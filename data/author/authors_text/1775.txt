Proceedings of the 12th Conference of the European Chapter of the ACL, pages 184?192,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning to Interpret Utterances Using Dialogue History
David DeVault
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292
devault@ict.usc.edu
Matthew Stone
Department of Computer Science
Rutgers University
Piscataway, NJ 08845-8019
Matthew.Stone@rutgers.edu
Abstract
We describe a methodology for learning a
disambiguation model for deep pragmatic
interpretations in the context of situated
task-oriented dialogue. The system accu-
mulates training examples for ambiguity
resolution by tracking the fates of alter-
native interpretations across dialogue, in-
cluding subsequent clarificatory episodes
initiated by the system itself. We illus-
trate with a case study building maxi-
mum entropy models over abductive in-
terpretations in a referential communica-
tion task. The resulting model correctly re-
solves 81% of ambiguities left unresolved
by an initial handcrafted baseline. A key
innovation is that our method draws exclu-
sively on a system?s own skills and experi-
ence and requires no human annotation.
1 Introduction
In dialogue, the basic problem of interpretation is
to identify the contribution a speaker is making to
the conversation. There is much to recognize: the
domain objects and properties the speaker is refer-
ring to; the kind of action that the speaker is per-
forming; the presuppositions and implicatures that
relate that action to the ongoing task. Neverthe-
less, since the seminal work of Hobbs et al (1993),
it has been possible to conceptualize pragmatic in-
terpretation as a unified reasoning process that se-
lects a representation of the speaker?s contribution
that is most preferred according to a background
model of how speakers tend to behave.
In principle, the problem of pragmatic interpre-
tation is qualitatively no different from the many
problems that have been tackled successfully by
data-driven models in NLP. However, while re-
searchers have shown that it is sometimes possi-
ble to annotate corpora that capture features of in-
terpretation, to provide empirical support for the-
ories, as in (Eugenio et al, 2000), or to build
classifiers that assist in dialogue reasoning, as in
(Jordan and Walker, 2005), it is rarely feasible
to fully annotate the interpretations themselves.
The distinctions that must be encoded are subtle,
theoretically-loaded and task-specific?and they
are not always signaled unambiguously by the
speaker. See (Poesio and Vieira, 1998; Poesio
and Artstein, 2005), for example, for an overview
of problems of vagueness, underspecification and
ambiguity in reference annotation.
As an alternative to annotation, we argue here
that dialogue systems can and should prepare
their own training data by inference from under-
specified models, which provide sets of candi-
date meanings, and from skilled engagement with
their interlocutors, who know which meanings are
right. Our specific approach is based on contribu-
tion tracking (DeVault, 2008), a framework which
casts linguistic inference in situated, task-oriented
dialogue in probabilistic terms. In contribution
tracking, ambiguous utterances may result in alter-
native possible contexts. As subsequent utterances
are interpreted in those contexts, ambiguities may
ramify, cascade, or disappear, giving new insight
into the pattern of activity that the interlocutor is
engaged in. For example, consider what happens
if the system initiates clarification. The interlocu-
tor?s answer may indicate not only what they mean
now but also what they must have meant earlier
when they used the original ambiguous utterance.
Contribution tracking allows a system to accu-
mulate training examples for ambiguity resolution
by tracking the fates of alternative interpretations
across dialogue. The system can use these ex-
amples to improve its models of pragmatic inter-
pretation. To demonstrate the feasibility of this
approach in realistic situations, we present a sys-
tem that tracks contributions to a referential com-
munication task using an abductive interpretation
184
model: see Section 2. A user study with this sys-
tem, described in Section 3, shows that this sys-
tem can, in the course of interacting with its users,
discover the correct interpretations of many poten-
tially ambiguous utterances. The system thereby
automatically acquires a body of training data in
its native representations. We use this data to build
a maximum entropy model of pragmatic interpre-
tation in our referential communication task. After
training, we correctly resolve 81% of the ambigu-
ities left open in our handcrafted baseline.
2 Contribution tracking
We continue a tradition of research that uses sim-
ple referential communication tasks to explore the
organization and processing of human?computer
and mediated human?human conversation, includ-
ing recently (DeVault and Stone, 2007; Gergle
et al, 2007; Healey and Mills, 2006; Schlangen
and Ferna?ndez, 2007). Our specific task is a two-
player object-identification game adapted from the
experiments of Clark and Wilkes-Gibbs (1986)
and Brennan and Clark (1996); see Section 2.1.
To play this game, our agent, COREF, inter-
prets utterances as performing sequences of task-
specific problem-solving acts using a combination
of grammar-based constraint inference and abduc-
tive plan recognition; see Section 2.2. Crucially,
COREF?s capabilities also include the ambiguity
management skills described in Section 2.3, in-
cluding policies for asking and answering clarifi-
cation questions.
2.1 A referential communication task
The game plays out in a special-purpose graphical
interface, which can support either human?human
or human?agent interactions. Two players work
together to create a specific configuration of ob-
jects, or a scene, by adding objects into the scene
one at a time. Their interfaces display the same set
of candidate objects (geometric objects that differ
in shape, color and pattern), but their locations are
shuffled. The shuffling undermines the use of spa-
tial expressions such as ?the object at bottom left?.
Figures 1 and 2 illustrate the different views.1
1Note that in a human?human game, there are literally
two versions of the graphical interface on the separate com-
puters the human participants are using. In a human?agent
interaction, COREF does not literally use the graphical inter-
face, but the information that COREF is provided is limited
to the information the graphical interface would provide to a
human participant. For example, COREF is not aware of the
locations of objects on its partner?s screen.
Present: [c4, Agent], Active: [] Skip this objectContinue (next object) or You (c4:) 
c4: brown diamond
c4: yesHistory  
Candidate Objects    Your scene    
Figure 1: A human user plays an object identifi-
cation game with COREF. The figure shows the
perspective of the user (denoted c4). The user is
playing the role of director, and trying to identify
the diamond at upper right (indicated to the user
by the blue arrow) to COREF.
Present: [c4, Agent], Active: [] Skip this object or You (Agent:) 
c4: brown diamond
c4: yesHistory  
Candidate Objects    Your scene    
Figure 2: The conversation of Figure 1 from
COREF?s perspective. COREF is playing the role
of matcher, and trying to determine which object
the user wants COREF to identify.
As in the experiments of Clark and Wilkes-
Gibbs (1986) and Brennan and Clark (1996), one
of the players, who plays the role of director,
instructs the other player, who plays the role of
matcher, which object is to be added next to the
scene. As the game proceeds, the next target ob-
ject is automatically determined by the interface
and privately indicated to the director with a blue
arrow, as shown in Figure 1. (Note that the corre-
sponding matcher?s perspective, shown in Figure
2, does not include the blue arrow.) The director?s
job is then to get the matcher to click on (their ver-
sion of) this target object.
To achieve agreement about the target, the two
players can exchange text through an instant-
messaging modality. (This is the only communi-
185
cation channel.) Each player?s interface provides
a real-time indication that their partner is ?Active?
while their partner is composing an utterance, but
the interface does not show in real-time what is
being typed. Once the Enter key is pressed, the
utterance appears to both players at the bottom of
a scrollable display which provides full access to
all the previous utterances in the dialogue.
When the matcher clicks on an object they be-
lieve is the target, their version of that object is pri-
vately moved into their scene. The director has no
visible indication that the matcher has clicked on
an object. However, the director needs to click the
Continue (next object) button (see Fig-
ure 1) in order to move the current target into the
director?s scene, and move on to the next target
object. This means that the players need to discuss
not just what the target object is, but also whether
the matcher has added it, so that they can coordi-
nate on the right moment to move on to the next
object. If this coordination succeeds, then after
the director and matcher have completed a series
of objects, they will have created the exact same
scene in their separate interfaces.
2.2 Interpreting user utterances
COREF treats interpretation broadly as a prob-
lem of abductive intention recognition (Hobbs et
al., 1993).2 We give a brief sketch here to high-
light the content of COREF?s representations, the
sources of information that COREF uses to con-
struct them, and the demands they place on disam-
biguation. See DeVault (2008) for full details.
COREF?s utterance interpretations take the
form of action sequences that it believes would
constitute coherent contributions to the dialogue
task in the current context. Interpretations are con-
structed abductively in that the initial actions in
the sequence need not be directly tied to observ-
able events; they may be tacit in the terminology
of Thomason et al (2006). Examples of such tacit
actions include clicking an object, initiating a clar-
ification, or abandoning a previous question. As
a concrete example, consider utterance (1b) from
the dialogue of Figure 1, repeated here as (1):
(1) a. COREF: is the target round?
b. c4: brown diamond
c. COREF: do you mean dark brown?
d. c4: yes
2In fact, the same reasoning interprets utterances, button
presses and the other actions COREF observes!
In interpreting (1b), COREF hypothesizes that the
user has tacitly abandoned the agent?s question in
(1a). In fact, COREF identifies two possible inter-
pretations for (1b):
i2,1= ? c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,saddlebrown(t7)]?
i2,2= ? c4:tacitAbandonTasks[2],
c4:addcr[t7,rhombus(t7)],
c4:setPrag[inFocus(t7)],
c4:addcr[t7,sandybrown(t7)]?
Both interpretations begin by assuming that
user c4 has tacitly abandoned the previous ques-
tion, and then further analyze the utterance as per-
forming three additional dialogue acts. When a di-
alogue act is preceded by tacit actions in an inter-
pretation, the speaker of the utterance implicates
that the earlier tacit actions have taken place (De-
Vault, 2008). These implicatures are an important
part of the interlocutors? coordination in COREF?s
dialogues, but they are a major obstacle to annotat-
ing interpretations by hand.
Action sequences such as i2,1 and i2,2 are coher-
ent only when they match the state of the ongoing
referential communication game and the seman-
tic and pragmatic status of information in the dia-
logue. COREF tracks these connections by main-
taining a probability distribution over a set of di-
alogue states, each of which represents a possi-
ble thread that resolves the ambiguities in the di-
alogue history. For performance reasons, COREF
entertains up to three alternative threads of inter-
pretation; COREF strategically drops down to the
single most probable thread at the moment each
object is completed. Each dialogue state repre-
sents the stack of processes underway in the ref-
erential communication game; constituent activi-
ties include problem-solving interactions such as
identifying an object, information-seeking interac-
tions such as question?answer pairs, and ground-
ing processes such as acknowledgment and clari-
fication. Dialogue states also represent pragmatic
information including recent utterances and refer-
ents which are salient or in focus.
COREF abductively recognizes the intention I
of an actor in three steps. First, for each dia-
logue state sk, COREF builds a horizon graph of
possible tacit action sequences that could be as-
sumed coherently, given the pending tasks (De-
Vault, 2008).
Second, COREF uses the horizon graph and
other resources to solve any constraints associ-
186
ated with the observed action. This step instanti-
ates any free parameters associated with the action
to contextually relevant values. For utterances,
the relevant constraints are identified by parsing
the utterance using a hand-built, lexicalized tree-
adjoining grammar. In interpreting (1b), the parse
yields an ambiguity in the dialogue act associated
with the word ?brown?, which may mean either
of the two shades of brown in Figure 1, which
COREF distinguishes using its saddlebrown
and sandybrown concepts.
Once COREF has identified a set of interpre-
tations {it,1, ..., it,n} for an utterance o at time t,
the last step is to assign a probability to each. In
general, we conceive of this following Hobbs et
al. (1993): the agent should weigh the different
assumptions that went into constructing each in-
terpretation.3 Ultimately, this process should be
made sensitive to the rich range of factors that
are available from COREF?s deep representation
of the dialogue state and the input utterance?this
is our project in this paper. However, in our initial
implemented prototype, COREF assigned these
probabilities using a simple hand-built model con-
sidering only NT, the number of tacit actions ab-
ductively assumed to occur in an interpretation:
P (I = it,j |o, St = sk) ?
1
NT(it,j) + 1
(1)
In effect, this is a ?null hypothesis? that assigns
relatively uniform weights to different abductive
hypotheses.
2.3 Interactive disambiguation
COREF uses its probabilistic model of context in
order to tolerate ambiguity as it moves forward
with its dialogues and to resolve ambiguity over
time. We have put particular effort into COREF?s
skills with three kinds of ambiguity: word-sense
ambiguities, where COREF finds multiple resolu-
tions for the domain concept evoked by the use
of a lexical item, as in the interaction (1) of Fig-
ure 1; referential ambiguities, where COREF takes
a noun phrase to be compatible with multiple ob-
jects from the display; and speech act ambiguities,
where alternative interpretations communicate or
implicate different kinds of contributions to the
ongoing task.
The resolution of ambiguity may involve some
combination of asking questions of the user, ag-
3Though note that Hobbs et al do not explicitly construe
their weights in terms of probabilities.
Figure 3: A human user plays the director role in
an object identification game with COREF. The
figure shows the perspective of the user.
gregating information provided by the user across
multiple turns of dialogue, and strategically drop-
ping threads of interpretation. For example,
COREF represents the context resulting from (1b)
in terms of two states: one from interpretation i2,1
and one from i2,2. COREF asks a clarification
question (1c); the user?s explicit answer yes allows
COREF to discard one of the possible states and
allocate all its probability mass to the other one.
The dialogue in (2) shows an alternative case.
(2) a. c1: color is orange
b. c1: shape is diamond
c. COREF: ok
The example is taken from the setting shown in
Figure 3. In this case, COREF finds two colors on
the screen it thinks the user could intend to evoke
with the word orange; the peachy orange of the di-
amond and circle on the top row and the brighter
orange of the solid and empty squares in the mid-
dle column. COREF responds to the ambiguity by
introducing two states which track the alternative
colors. Immediately COREF gets an additional
description from the user, and adds the constraint
that the object is a diamond. As there is no bright
orange diamond, there is no way to interpret the
user?s utterance in the bright orange state; COREF
discards this state and allocates all its probability
mass to the other one.
3 Inferring the fates of interpretations
Our approach is based on the observation that
COREF?s contribution tracking can be viewed as
assigning a fate to every dialogue state it enter-
tains as part of some thread of interpretation. In
187
particular, if we consider the agent?s contribution
tracking retrospectively, every dialogue state can
be assigned a fate of correct or incorrect, where a
state is viewed as correct if it or some of its descen-
dants eventually capture all the probability mass
that COREF is distributing across the viable sur-
viving states, and incorrect otherwise.
In general, there are two ways that a state can
end up with fate incorrect. One way is that the
state and all of its descendants are eventually de-
nied any probability mass due to a failure to in-
terpret a subsequent utterance or action as a co-
herent contribution from any of those states. In
this case, we say that the incorrect state was elimi-
nated. The second way a state can end up incorrect
is if COREF makes a strategic decision to drop the
state, or all of its surviving descendants, at a time
when the state or its descendants were assigned
nonzero probability mass. In this case we say that
the incorrect state was dropped. Meanwhile, be-
cause COREF drops all states but one after each
object is completed, there is a single hypothesized
state at each time t whose descendants will ulti-
mately capture all of COREF?s probability mass.
Thus, for each time t, COREF will retrospectively
classify exactly one state as correct.
Of course, we really want to classify interpre-
tations. Because we seek to estimate P (I =
it,j |o, St = sk), which conditions the probability
assigned to I = it,j on the correctness of state
sk, we consider only those interpretations arising
in states that are retrospectively identified as cor-
rect. For each such interpretation, we start from
the state where that interpretation is adopted and
trace forward to a correct state or to its last surviv-
ing descendant. We classify the interpretation the
same way as that final state, either correct, elimi-
nated, or dropped.
We harvested a training set using this method-
ology from the transcripts of a previous evaluation
experiment designed to exercise COREF?s ambi-
guity management skills. The data comes from
20 subjects?most of them undergraduates par-
ticipating for course credit?who interacted with
COREF over the web in three rounds of the ref-
erential communication each. The number of ob-
jects increased from 4 to 9 to 16 across rounds;
the roles of director and matcher alternated in each
round, with the initial role assigned at random.
Of the 3275 sensory events that COREF in-
terpreted in these dialogues, from the (retrospec-
N Percentage N Percentage
0 10.53 5 0.21
1 79.76 6 0.12
2 7.79 7 0.09
3 0.85 8 0.06
4 0.58 9 0.0
Figure 4: Distribution of degree of ambiguity in
training set. The table lists percentage of events
that had a specific number N of candidate inter-
pretations constructed from the correct state.
tively) correct state, COREF hypothesized 0 inter-
pretations for 345 events, 1 interpretation for 2612
events, and more than one interpretation for 318
events. The overall distribution in the number of
interpretations hypothesized from the correct state
is given in Figure 4.
4 Learning pragmatic interpretation
We capture the fate of each interpretation it,j in a
discrete variable F whose value is correct, elimi-
nated, or dropped. We also represent each inten-
tion it,j , observation o, and state sk in terms of
features. We seek to learn a function
P (F = correct | features(it,j),
features(o),
features(sk))
from a set of training examples E = {e1, ..., en}
where, for l = 1..n, we have:
el = ( F = fate(it,j), features(it,j),
features(o), features(sk)).
We chose to train maximum entropy models
(Berger et al, 1996). Our learning framework is
described in Section 4.1; the results in Section 4.2.
4.1 Learning setup
We defined a range of potentially useful features,
which we list in Figures 5, 6, and 7. These fea-
tures formalize pragmatic distinctions that plau-
sibly provide evidence of the correct interpreta-
tion for a user utterance or action. You might
annotate any of these features by hand, but com-
puting them automatically lets us easily explore a
much larger range of possibilities. To allow these
various kinds of features (integer-valued, binary-
valued, and string-valued) to interface to the max-
imum entropy model, these features were con-
verted into a much broader class of indicator fea-
tures taking on a value of either 0.0 or 1.0.
188
feature set description
NumTacitActions The number of tacit actions in it,j .
TaskActions These features represent the action type (function symbol) of
each action ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, as a
string.
ActorDoesTaskAction For each Ak : ak in it,j = ?A1 : a1, A2 : a2, ..., An : an?, a
feature indicates that Ak (represented as string ?Agent? or
?User?) has performed action ak (represented as a string
action type, as in the TaskActions features).
Presuppositions If o is an utterance, we include a string representation of each
presupposition assigned to o by it,j . The predicate/argument
structure is captured in the string, but any gensym identifiers
within the string (e.g. target12) are replaced with
exemplars for that identifier type (e.g. target).
Assertions If o is an utterance, we include a string representation of each
dialogue act assigned to o by it,j . Gensym identifiers are
filtered as in the Presuppositions features.
Syntax If o is an utterance, we include a string representation of the
bracketed phrase structure of the syntactic analysis assigned to
o by it,j . This includes the categories of all non-terminals in
the structure.
FlexiTaskIntentionActors Given it,j = ?A1 : a1, A2 : a2, ..., An : an?, we include a single
string feature capturing the actor sequence ?A1, A2, ..., An? in
it,j (e.g. ?User, Agent, Agent?).
Figure 5: The interpretation features, features(it,j), available for selection in our learned model.
feature set description
Words If o is an utterance, we include features that indicate the
presence of each word that occurs in the utterance.
Figure 6: The observation features, features(o), available for selection in our learned model.
feature set description
NumTasksUnderway The number of tasks underway in sk.
TasksUnderway The name, stack depth, and current task state for each task
underway in sk.
NumRemainingReferents The number of objects yet to be identified in sk.
TabulatedFacts String features representing each proposition in the
conversational record in sk (with filtered gensym identifiers).
CurrentTargetConstraints String features for each positive and negative constraint on the
current target in sk (with filtered gensym identifiers). E.g.
?positive: squareFigureObject(target)? or
?negative: solidFigureObject(target)?.
UsefulProperties String features for each property instantiated in the experiment
interface in sk. E.g. ?squareFigureObject?,
?solidFigureObject?, etc.
Figure 7: The dialogue state features, features(sk), available for selection in our learned model.
189
We used the MALLET maximum entropy clas-
sifier (McCallum, 2002) as an off-the-shelf, train-
able maximum entropy model. Each run involved
two steps. First, we applied MALLET?s feature
selection algorithm, which incrementally selects
features (as well as conjunctions of features) that
maximize an exponential gain function which rep-
resents the value of the feature in predicting in-
terpretation fates. Based on manual experimenta-
tion, we chose to have MALLET select about 300
features for each learned model. In the second
step, the selected features were used to train the
model to estimate probabilities. We used MAL-
LET?s implementation of Limited-Memory BFGS
(Nocedal, 1980).
4.2 Evaluation
We are generally interested in whether COREF?s
experience with previous subjects can be lever-
aged to improve its interactions with new sub-
jects. Therefore, to evaluate our approach, while
making maximal use of our available data set, we
performed a hold-one-subject-out cross-validation
using our 20 human subjects H = {h1, ..., h20}.
That is, for each subject hi, we trained a model
on the training examples associated with subjects
H \ {hi}, and then tested the model on the exam-
ples associated with subject hi.
To quantify the performance of the learned
model in comparison to our baseline, we adapt
the mean reciprocal rank statistic commonly used
for evaluation in information retrieval (Vorhees,
1999). We expect that a system will use the prob-
abilities calculated by a disambiguation model to
decide which interpretations to pursue and how to
follow them up through the most efficient interac-
tion. What matters is not the absolute probability
of the correct interpretation but its rank with re-
spect to competing interpretations. Thus, we con-
sider each utterance as a query; the disambigua-
tion model produces a ranked list of responses for
this query (candidate interpretations), ordered by
probability. We find the rank r of the correct in-
terpretation in this list and measure the outcome
of the query as 1r . Because of its weak assump-
tions, our baseline disambiguation model actually
leaves many ties. So in fact we must compute an
expected reciprocal rank (ERR) statistic that aver-
ages 1r over all ways of ordering the correct inter-
pretation against competitors of equal probability.
Figure 8 shows a histogram of ERR across
ERR range Hand-built
model
Learned
models
1 20.75% 81.76%
[12 , 1) 74.21% 16.35%
[13 ,
1
2) 3.46% 1.26%
[0, 13) 1.57% 0.63%
mean(ERR) 0.77 0.92
var(ERR) 0.02 0.03
Figure 8: For the 318 ambiguous sensory events,
the distribution of the expected reciprocal of rank
of the correct interpretation, for the initial, hand-
built model and the learned models in aggregate.
the ambiguous utterances from the corpus. The
learned models correctly resolve almost 82%,
while the baseline model correctly resolves about
21%. In fact, the learned models get much of this
improvement by learning weights to break the ties
in our baseline model. The overall performance
measure for a disambiguation model is the mean
expected reciprocal rank across all examples in the
corpus. The learned model improves this metric to
0.92 from a baseline of 0.77. The difference is un-
ambiguously significant (Wilcoxon rank sum test
W = 23743.5, p < 10?15).
4.3 Selected features
Feature selection during training identified a vari-
ety of syntactic, semantic, and pragmatic features
as useful in disambiguating correct interpretations.
Selections were made from every feature set in
Figures 5, 6, and 7. It was often possible to iden-
tify relevant features as playing a role in successful
disambiguation by the learned models. For exam-
ple, the learned model trained on H \ {c4} deliv-
ered the following probabilities for the two inter-
pretations COREF found for c4?s utterance (1b):
P (I = i2,1|o, S2 = s8923) = 0.665
P (I = i2,2|o, S2 = s8923) = 0.335
The correct interpretation, i2,1, hypothesizes that
the user means saddlebrown, the darker of the
two shades of brown in the display. Among the
features selected in this model is a Presupposi-
tions feature (see Figure 5) which is present just
in case the word ?brown? is interpreted as mean-
ing saddlebrown rather than some other shade.
This feature allows the learned model to prefer
to interpret c4?s use of ?brown? as meaning this
190
darker shade of brown, based on the observed lin-
guistic behavior of other users.
5 Results in context
Our work adds to a body of research learning deep
models of language from evidence implicit in an
agent?s interactions with its environment. It shares
much of its motivation with co-training (Blum and
Mitchell, 1998) in improving initial models by
leveraging additional data that is easy to obtain.
However, as the examples of Section 2.3 illustrate,
COREF?s interactions with its users offer substan-
tially more information about interpretation than
the raw text generally used for co-training. Closer
in spirit is AI research on learning vocabulary
items by connecting user vocabulary to the agent?s
perceptual representations at the time of utterance
(Oates et al, 2000; Roy and Pentland, 2002; Co-
hen et al, 2002; Yu and Ballard, 2004; Steels
and Belpaeme, 2005). Our framework augments
this information about utterance context with ad-
ditional evidence about meaning from linguistic
interaction. In general, dialogue coherence is an
important source of evidence for all aspects of lan-
guage, for both human language learning (Saxton
et al, 2005) as well as machine models. For exam-
ple, Bohus et al (2008) use users? confirmations
of their spoken requests in a multi-modal interface
to tune the system?s ASR rankings for recognizing
subsequent utterances.
Our work to date has a number of limitations.
First, although 318 ambiguous interpretations did
occur, this user study provided a relatively small
number of ambiguous interpretations, in machine
learning terms; and most (80.2%) of those that did
occur were 2-way ambiguities. A richer domain
would require both more data and a generative ap-
proach to model-building and search.
Second, this learning experiment has been per-
formed after the fact, and we have not yet inves-
tigated the performance of the learned model in a
follow-up experiment in which COREF uses the
learned model in interactions with its users.
A third limitation lies in the detection of
?correct? interpretations. Our scheme some-
times conflates the user?s actual intentions with
COREF?s subsequent assumptions about them. If
COREF decides to strategically drop the user?s
actual intended interpretation, our scheme may
mark another interpretation as ?correct?. Alterna-
tive approaches may do better at harvesting mean-
ingful examples of correct and incorrect interpre-
tations from an agent?s dialogue experience. Our
approach also depends on having clear evidence
about what an interlocutor has said and whether
the system has interpreted it correctly?evidence
that is often unavailable with spoken input or
information-seeking tasks. Thus, even when spo-
ken language interfaces use probabilistic inference
for dialogue management (Williams and Young,
2007), new techniques may be needed to mine
their experience for correct interpretations.
6 Conclusion
We have implemented a system COREF that
makes productive use of its dialogue experience by
learning to rank new interpretations based on fea-
tures it has historically associated with correct ut-
terance interpretations. We present these results as
a proof-of-concept that contribution tracking pro-
vides a source of information that an agent can
use to improve its statistical interpretation process.
Further work is required to scale these techniques
to richer dialogue systems, and to understand the
best architecture for extracting evidence from an
agent?s interpretive experience and modeling that
evidence for future language use. Nevertheless,
we believe that these results showcase how judi-
cious system-building efforts can lead to dialogue
capabilities that defuse some of the bottlenecks to
learning rich pragmatic interpretation. In particu-
lar, a focus on improving our agents? basic abilities
to tolerate and resolve ambiguities as a dialogue
proceeds may prove to be a valuable technique for
improving the overall dialogue competence of the
agents we build.
Acknowledgments
This work was sponsored in part by NSF CCF-
0541185 and HSD-0624191, and by the U.S.
Army Research, Development, and Engineering
Command (RDECOM). Statements and opinions
expressed do not necessarily reflect the position or
the policy of the Government, and no official en-
dorsement should be inferred. Thanks to our re-
viewers, Rich Thomason, David Traum and Jason
Williams.
191
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory, pages 92?100.
Dan Bohus, Xiao Li, Patrick Nguyen, and Geoffrey
Zweig. 2008. Learning n-best correction models
from implicit user feedback in a multi-modal local
search application. In The 9th SIGdial Workshop on
Discourse and Dialogue.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology, 22(6):1482?
1493.
Herbert H. Clark and DeannaWilkes-Gibbs. 1986. Re-
ferring as a collaborative process. In Philip R. Co-
hen, Jerry Morgan, and Martha E. Pollack, editors,
Intentions in Communication, pages 463?493. MIT
Press, Cambridge, Massachusetts, 1990.
Paul R. Cohen, Tim Oates, Carole R. Beal, and Niall
Adams. 2002. Contentful mental states for robot
baby. In Eighteenth national conference on Artifi-
cial intelligence, pages 126?131, Menlo Park, CA,
USA. American Association for Artificial Intelli-
gence.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue. In Pro-
ceedings of the 11th Workshop on the Semantics and
Pragmatics of Dialogue (Decalog 2007), pages 49?
56.
David DeVault. 2008. Contribution Tracking: Par-
ticipating in Task-Oriented Dialogue under Uncer-
tainty. Ph.D. thesis, Department of Computer Sci-
ence, Rutgers, The State University of New Jersey,
New Brunswick, NJ.
Barbara Di Eugenio, Pamela W. Jordan, Richmond H.
Thomason, and Johanna D. Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogue. International Journal of Human-Computer
Studies, 53:1017?1076.
Darren Gergle, Carolyn P. Rose?, and Robert E. Kraut.
2007. Modeling the impact of shared visual infor-
mation on collaborative reference. InCHI 2007 Pro-
ceedings, pages 1543?1552.
Patrick G. T. Healey and Greg J. Mills. 2006. Partic-
ipation, precedence and co-ordination in dialogue.
In Proceedings of Cognitive Science, pages 1470?
1475.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and
Paul Martin. 1993. Interpretation as abduction. Ar-
tificial Intelligence, 63:69?142.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. JAIR, 24:157?194.
Andrew McCallum. 2002. MALLET: A
MAchine learning for LanguagE toolkit.
http://mallet.cs.umass.edu.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Tim Oates, Zachary Eyler-Walker, and Paul R. Co-
hen. 2000. Toward natural language interfaces for
robotic agents. In Proc. Agents, pages 227?228.
Massimo Poesio and Ron Artstein. 2005. Annotating
(anaphoric) ambiguity. In Proceedings of the Cor-
pus Linguistics Conference.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Deb Roy and Alex Pentland. 2002. Learning words
from sights and sounds: A computational model.
Cognitive Science, 26(1):113?146.
Matthew Saxton, Carmel Houston-Price, and Natasha
Dawson. 2005. The prompt hypothesis: clarifica-
tion requests as corrective input for grammatical er-
rors. Applied Psycholinguistics, 26(3):393?414.
David Schlangen and Raquel Ferna?ndez. 2007. Speak-
ing through a noisy channel: Experiments on in-
ducing clarification behaviour in human?human di-
alogue. In Proceedings of Interspeech 2007.
Luc Steels and Tony Belpaeme. 2005. Coordinating
perceptually grounded categories through language.
a case study for colour. Behavioral and Brain Sci-
ences, 28(4):469?529.
Richmond H. Thomason, Matthew Stone, and David
DeVault. 2006. Enlightened update: A
computational architecture for presupposition and
other pragmatic phenomena. For the Ohio
State Pragmatics Initiative, 2006, available at
http://www.research.rutgers.edu/?ddevault/.
Ellen M. Vorhees. 1999. The TREC-8 question an-
swering track report. In Proceedings of the 8th Text
Retrieval Conference, pages 77?82.
Jason Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1:57?80.
192
Proceedings of NAACL HLT 2009: Short Papers, pages 53?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Natural Language Understanding of Partial Speech Recognition
Results in Dialogue Systems
Kenji Sagae and Gwen Christian and David DeVault and David R. Traum
Institute for Creative Technologies, University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292
{sagae,gchristian,devault,traum}@ict.usc.edu
Abstract
We investigate natural language understand-
ing of partial speech recognition results to
equip a dialogue system with incremental lan-
guage processing capabilities for more realis-
tic human-computer conversations. We show
that relatively high accuracy can be achieved
in understanding of spontaneous utterances
before utterances are completed.
1 Introduction
Most spoken dialogue systems wait until the user
stops speaking before trying to understand and re-
act to what the user is saying. In particular, in a
typical dialogue system pipeline, it is only once the
user?s spoken utterance is complete that the results
of automatic speech recognition (ASR) are sent on
to natural language understanding (NLU) and dia-
logue management, which then triggers generation
and synthesis of the next system prompt. While
this style of interaction is adequate for some appli-
cations, it enforces a rigid pacing that can be un-
natural and inefficient for mixed-initiative dialogue.
To achieve more flexible turn-taking with human
users, for whom turn-taking and feedback at the sub-
utterance level is natural and common, the system
needs to engage in incremental processing, in which
interpretation components are activated, and in some
cases decisions are made, before the user utterance
is complete.
There is a growing body of work on incremen-
tal processing in dialogue systems. Some of this
work has demonstrated overall improvements in sys-
tem responsiveness and user satisfaction; e.g. (Aist
et al, 2007; Skantze and Schlangen, 2009). Several
research groups, inspired by psycholinguistic mod-
els of human processing, have also been exploring
technical frameworks that allow diverse contextual
information to be brought to bear during incremen-
tal processing; e.g. (Kruijff et al, 2007; Aist et al,
2007).
While this work often assumes or suggests it is
possible for systems to understand partial user ut-
terances, this premise has generally not been given
detailed quantitative study. The contribution of this
paper is to demonstrate and explore quantitatively
the extent to which one specific dialogue system can
anticipate what an utterance means, on the basis of
partial ASR results, before the utterance is complete.
2 NLU for spontaneous spoken utterances
in a dialogue system
For this initial effort, we chose to look at incremental
processing of natural language understanding in the
SASO-EN system (Traum et al, 2008), a complex
spoken dialog system for which we have a corpus
of user data that includes recorded speech files that
have been transcribed and annotated with a semantic
representation. The domain of this system is a nego-
tiation scenario involving the location of a medical
clinic in a foreign country. The system is intended as
a negotiation training tool, where users learn about
negotiation tactics in the context of the culture and
social norms of a particular community.
2.1 The natural language understanding task
The NLU module must take the output of ASR as
input, and produce domain-specific semantic frames
as output. These frames are intended to capture
much of the meaning of the utterance, although a
53
dialogue manager further enriches the frame rep-
resentations with pragmatic information (Traum,
2003). NLU output frames are attribute-value ma-
trices, where the attributes and values are linked to a
domain-specific ontology and task model.
Complicating the NLU task of is the relatively
high word error rate (0.54) in ASR of user speech
input, given conversational speech in a complex do-
main and an untrained broad user population.
The following example, where the user attempts
to address complaints about lack of power in the pro-
posed location for the clinic, illustrates an utterance-
frame pair.
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.agent kirk
<s>.sem.event deliver
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
<s>.sem.theme power-generator
<s>.sem.type event
The original NLU component for this system was
described in (Leuski and Traum, 2008). For the pur-
poses of this experiment, we have developed a new
NLU module and tested on several different data
sets as described in the next section. Our approach
is to use maximum entropy models (Berger et al,
1996) to learn a suitable mapping from features de-
rived from the words in the ASR output to semantic
frames. Given a set of examples of semantic frames
with corresponding ASR output, a classifier should
learn, for example, that when ?generators? appears
in the output of ASR, the value power-generators is
likely to be present in the output frame. The specific
features used by the classifier are: each word in the
input string (bag-of-words representation of the in-
put), each bigram (consecutive words), each pair of
any two words in the input, and the number of words
in the input string.
0102030405060 1
23
45
67
89
1011
1213
1415
1617
1819
2021
2223
24
Length
 
n (word
s)
Number of utterances (bars)
050100150200250300350400450
Cumulative number of 
utterances (line)
Exact
ly n wo
rds
At mo
st n w
ords
Figure 1: Length of utterances in the development set.
2.2 Data
Our corpus consists of 4,500 user utterances spread
across a number of different dialogue sessions. Ut-
terances that were out-of-domain (13.7% of the cor-
pus) were assigned a ?garbage? frame, with no se-
mantic content. Approximately 10% of the utter-
ances were set aside for final testing, and another
10% was designated the development corpus for the
NLU module. The development and test sets were
chosen so that all the utterances in a session were
kept in the same set, but sessions were chosen at ran-
dom for inclusion in the development and test sets.
The training set contains 136 distinct frames,
each of which is composed of several attribute-value
pairs, called frame elements. Figure 1 shows the ut-
terance length distribution in the development set.
2.3 NLU results on complete ASR output
To evaluate NLU results, we look at precision, re-
call and f-score of frame elements. When the NLU
module is trained on complete ASR utterances in
the training set, and tested on complete ASR utter-
ances in the development set, f-score of frame ele-
ments is 0.76, with precision at 0.78 and recall at
0.74. To gain insight on what the upperbound on
the accuracy of the NLU module might be, we also
trained the classifier using features extracted from
gold-standard manual transcription (instead of ASR
output), and tested the accuracy of analyses of gold-
standard transcriptions (which would not be avail-
able at run-time in the dialogue system). Under
these ideal conditions, NLU f-score is 0.87. Training
on gold-standard transcriptions and testing on ASR
output produces results with a lower f-score, 0.74.
54
3 NLU on partial ASR results
Roughly half of the utterances in our training data
contain six words or more, and the average utter-
ance length is 5.9 words. Since the ASR module is
capable of sending partial results to the NLU mod-
ule even before the user has finished an utterance, in
principle the dialogue system can start understand-
ing and even responding to user input as soon as
enough words have been uttered to give the system
some indication of what the user means, or even
what the user will have said once the utterance is
completed. To measure the extent to which our NLU
module can predict the frame for an input utterance
when it sees only a partial ASR result with the first
n words, we examine two aspects of NLU with par-
tial ASR results. The first is correctness of the NLU
output with partial ASR results of varying lengths, if
we take the gold-standard manual annotation for the
entire utterance as the correct frame for any of the
partial ASR results for that utterance. The second is
stability: how similar the NLU output with partial
ASR results of varying lengths is to what the NLU
result would have been for the entire utterance.
3.1 Training the NLU module for analysis of
partial ASR results
The simplest way to performNLU of partial ASR re-
sults is simply to process the partial utterances using
the NLU module trained on complete ASR output.
However, better results may be obtained by train-
ing separate NLU models for analysis of partial ut-
terances of different lengths. To train these sepa-
rate NLU models, we first ran the audio of the utter-
ances in the training data through our ASR module,
recording all partial results for each utterance. Then,
to train a model to analyze partial utterances con-
taining n words, we used only partial utterances in
the training set containing n words (unless the entire
utterance contained less than n words, in which case
we simply used the complete utterance). In some
cases, multiple partial ASR results for a single utter-
ance contained the same number of words, and we
used the last partial result with the appropriate num-
ber of words 1. We trained separate NLU models for
1At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial ut-
terance of length n+ 1.
01020304050607080
1
2
3
4
5
6
7
8
9
10
all
Length
 
n (word
s)
F-score
Traine
d on a
ll data
Traine
d on p
artials
 up to
length
 
n
Traine
d on p
artials
 up to
length
 
n + c
ontex
t
Figure 2: Correctness for three NLU models on partial
ASR results up to n words.
n varying from one to ten.
3.2 Results
Figure 2 shows the f-score for frames obtained by
processing partial ASR results up to length n using
three NLU models. The dashed line is our baseline
NLU model, trained on complete utterances only
(model 1). The solid line shows the results obtained
with length-specific NLU models (model 2), and the
dotted line shows results for length-specific models
that also use features that capture dialogue context
(model 3). Models 1 and 2 are described in the previ-
ous sections. The additional features used in model
3 are unigram and bigram word features extracted
from the most recent system utterance.
As seen in Figure 2, there is a clear benefit to
training NLU models specifically tailored for partial
ASR results. Training a model on partial utterances
with four or five words allows for relatively high f-
score of frame elements (0.67 and 0.71, respectively,
compared to 0.58 and 0.66 when the same partial
ASR results are analyzed using model 1). Consider-
ing that half of the utterances are expected to have
more than five words (based on the length of the ut-
terances in the training set), allowing the system to
start processing user input when four or five-word
partial ASR results are available provides interesting
opportunities. Targeting partial results with seven
words or more is less productive, since the time sav-
ings are reduced, and the gain in accuracy is modest.
The context features used in model 3 did not pro-
vide substantial benefits in NLU accuracy. It is pos-
55
0102030405060708090100
1
2
3
4
5
6
7
8
9
10
Length
 
n of pa
rtial A
SR ou
tput us
ed in m
odel 2
Stability F-score
Figure 3: Stability of NLU results for partial ASR results
up to length n.
sible that other ways of representing context or di-
alogue state may be more effective. This is an area
we are currently investigating.
Finally, figure 3 shows the stability of NLU re-
sults produced by model 2 for partial ASR utter-
ances of varying lengths. This is intended to be an
indication of how much the frame assigned to a par-
tial utterance differs from the ultimate NLU output
for the entire utterance. This ultimate NLU output
is the frame assigned by model 1 for the complete
utterance. Stability is then measured as the F-score
between the output of model 2 for a particular partial
utterance, and the output of model 1 for the corre-
sponding complete utterance. A stability F-score of
1.0 would mean that the frame produced for the par-
tial utterance is identical to the frame produced for
the entire utterance. Lower values indicate that the
frame assigned to a partial utterance is revised sig-
nificantly when the entire input is available. As ex-
pected, the frames produced by model 2 for partial
utterances with at least eight words match closely
the frames produced by model 1 for the complete ut-
terances. Although the frames for partial utterances
of length six are almost as accurate as the frames for
the complete utterances (figure 2), figure 3 indicates
that these frames are still often revised once the en-
tire input utterance is available.
4 Conclusion
We have presented experiments that show that it
is possible to obtain domain-specific semantic rep-
resentations of spontaneous speech utterances with
reasonable accuracy before automatic speech recog-
nition of the utterances is completed. This allows for
interesting opportunities in dialogue systems, such
as agents that can interrupt the user, or even finish
the user?s sentence. Having an estimate of the cor-
rectness and stability of NLU results obtained with
partial utterances allows the dialogue system to es-
timate how likely its initial interpretation of an user
utterance is to be correct, or at least agree with its
ultimate interpretation. We are currently working on
the extensions to the NLU model that will allow for
the use of different types of context features, and in-
vestigating interesting ways in which agents can take
advantage of early interpretations.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Language and Robots: Proc. from the
Symposium (LangRo?2007). University of Aveiro, 12.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. of the
12th Conference of the European Chapter of the ACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of Intelli-
gent Virtual Agents Conference IVA-2008.
D. Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
56
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198?207,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Making Grammar-Based Generation Easier to Deploy in Dialogue Systems
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a development pipeline and asso-
ciated algorithms designed to make grammar-
based generation easier to deploy in imple-
mented dialogue systems. Our approach real-
izes a practical trade-off between the capabili-
ties of a system?s generation component and
the authoring and maintenance burdens im-
posed on the generation content author for a
deployed system. To evaluate our approach,
we performed a human rating study with sys-
tem builders who work on a common large-
scale spoken dialogue system. Our results
demonstrate the viability of our approach and
illustrate authoring/performance trade-offs be-
tween hand-authored text, our grammar-based
approach, and a competing shallow statistical
NLG technique.
1 Introduction
This paper gives an overview of a new example-
based generation technique that is designed to make
grammar-based generation easier to deploy in dia-
logue systems. Dialogue systems present several
specific requirements for a practical generation com-
ponent. First, the generator needs to be fast enough
to support real-time interaction with a human user.
Second, the generator must provide adequate cover-
age for the meanings the dialogue system needs to
express. What counts as ?adequate? can vary be-
tween systems, since the high-level purpose of a di-
alogue system can affect priorities regarding output
fluency, fidelity to the requested meaning, variety
of alternative outputs, and tolerance for generation
failures. Third, developing the necessary resources
for the generation component should be relatively
straightforward in terms of time and expertise re-
quired. This is especially important since dialogue
systems are complex systems with significant devel-
opment costs. Finally, it should be relatively easy
for the dialogue manager to formulate a generation
request in the format required by the generator.
Together, these requirements can reduce the at-
tractiveness of grammar-based generation when
compared to simpler template-based or canned text
output solutions. In terms of speed, off-the-
shelf, wide-coverage grammar-based realizers such
as FUF/SURGE (Elhadad, 1991) can be too slow for
real-time interaction (Callaway, 2003).
In terms of adequacy of coverage, in principle,
grammar-based generation offers significant advan-
tages over template-based or canned text output by
providing productive coverage and greater variety.
However, realizing these advantages can require sig-
nificant development costs. Specifying the neces-
sary connections between lexico-syntactic resources
and the flat, domain-specific semantic representa-
tions that are typically available in implemented sys-
tems is a subtle, labor-intensive, and knowledge-
intensive process for which attractive methodologies
do not yet exist (Reiter et al, 2003).
One strategy is to hand-build an application-
specific grammar. However, in our experience,
this process requires a painstaking, time-consuming
effort by a developer who has detailed linguistic
knowledge as well as detailed domain knowledge,
and the resulting coverage is inevitably limited.
Wide-coverage generators that aim for applicabil-
198
ity across application domains (White et al, 2007;
Zhong and Stent, 2005; Langkilde-Geary, 2002;
Langkilde and Knight, 1998; Elhadad, 1991) pro-
vide a grammar (or language model) for free. How-
ever, it is harder to tailor output to the desired word-
ing and style for a specific dialogue system, and
these generators demand a specific input format that
is otherwise foreign to an existing dialogue system.
Unfortunately, in our experience, the development
burden of implementing the translation between the
system?s available meaning representations and the
generator?s required input format is quite substan-
tial. Indeed, implementing the translation might re-
quire as much effort as would be required to build a
simple custom generator; cf. (Callaway, 2003; Buse-
mann and Horacek, 1998). This development cost is
exacerbated when a dialogue system?s native mean-
ing representation scheme is under revision.
In this paper, we survey a new example-based ap-
proach (DeVault et al, 2008) that we have devel-
oped in order to mitigate these difficulties, so that
grammar-based generation can be deployed more
widely in implemented dialogue systems. Our de-
velopment pipeline requires a system developer to
create a set of training examples which directly
connect desired output texts to available applica-
tion semantic forms. This is achieved through a
streamlined authoring task that does not require de-
tailed linguistic knowledge. Our approach then
processes these training examples to automatically
construct all the resources needed for a fast, high-
quality, run-time grammar-based generation compo-
nent. We evaluate this approach using a pre-existing
spoken dialogue system. Our results demonstrate
the viability of the approach and illustrate author-
ing/performance trade-offs between hand-authored
text, our grammar-based approach, and a competing
shallow statistical NLG technique.
2 Background and Motivation
The generation approach set out in this paper has
been developed in the context of a research pro-
gram aimed at creating interactive virtual humans
for social training purposes (Swartout et al, 2006).
Virtual humans are embodied conversational agents
that play the role of people in simulations or games.
They interact with human users and other virtual hu-
Figure 1: Doctor Perez.
mans using spoken language and non-verbal behav-
ior such as eye gaze, gesture, and facial displays.
The case study we present here is the genera-
tion of output utterances for a particular virtual hu-
man, Doctor Perez (see Figure 1), who is designed
to teach negotiation skills in a multi-modal, multi-
party, non-team dialogue setting (Traum et al, 2005;
Traum et al, 2008). The human trainee who talks
to the doctor plays the role of a U.S. Army captain
named Captain Kirk. We summarize Doctor Perez?s
generation requirements as follows.
In order to support compelling real-time conver-
sation and effective training, the generator must be
able to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Doctor Perez has a relatively rich internal men-
tal state including beliefs, goals, plans, and emo-
tions. As Doctor Perez attempts to achieve his con-
versational goals, his utterances need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Doctor Perez cur-
rently uses about 200 distinct output utterances in
the course of his dialogues.
Doctor Perez is designed to simulate a non-native
English speaker, so highly fluent output is not a ne-
cessity; indeed, a small degree of disfluency is even
desirable in order to increase the realism of talking
to a non-native speaker.
Finally, in reasoning about user utterances, dia-
logue management, and generation, Doctor Perez
199
26
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
addressee captain-kirk
dialogue-act
2
6
4
addressee captain-kirk
type assign-turn
actor doctor-perez
3
7
5
speech-act
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
actor doctor-perez
addressee captain-kirk
action assert
content
2
6
6
6
6
6
6
6
6
4
type state
polarity negative
time present
attribute resourceAttribute
value medical-supplies
object-id market
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
addressee captain-kirk
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
dialogue-act.actor doctor-perez
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.action assert
speech-act.content.type state
speech-act.content.polarity negative
speech-act.content.time present
speech-act.content.attribute resourceAttribute
speech-act.content.value medical-supplies
speech-act.content.object-id market
(a) Attribute-value matrix (b) Corresponding frame
Figure 2: An example of Doctor Perez?s representations for utterance semantics: Doctor Perez tells the captain that
there are no medical supplies at the market.
exploits an existing semantic representation scheme
that has been utilized in a family of virtual humans.
This scheme uses an attribute-value matrix (AVM)
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain, as well as other features such as po-
larity and modality. See (Traum, 2003) for some
more details and examples of this representation.
For ease of interprocess communication, and certain
kinds of statistical processing, this AVM structure is
linearized so that each non-recursive terminal value
is paired with a path from the root to the final at-
tribute. Thus, the AVM in Figure 2(a) is represented
as the ?frame? in Figure 2(b).
Because the internal representations that make up
Doctor Perez?s mental state are under constant de-
velopment, the exact frames that are sent to the gen-
eration component change frequently as new rea-
soning capabilities are added and existing capabil-
ities are reorganized. Additionally, while only hun-
dreds of frames currently arise in actual dialogues,
the number of potential frames is orders of magni-
tude larger, and it is difficult to predict in advance
which frames might occur.
In this setting, over a period of years, a number
of different approaches to natural language gener-
ation have been implemented and tested, including
hand-authored canned text, domain specific hand-
built grammar-based generators (e.g., (Traum et al,
2003)), shallow statistical generation techniques,
and the grammar-based approach presented in this
paper. We now turn to the details of our approach.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component.
The approach involves three primary steps: spec-
ification of training examples, grammar induction,
and search optimization. In this section, we present
the format that training examples take and then sum-
marize the subsequent automatic processing steps.
Due to space limitations, we omit the full details
of these automatic processing steps, and refer the
reader to (DeVault et al, 2008) for additional details.
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 3.
In this example, the generation content author
200
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
Semantics
we do n?t . . . . . . . . .
{
speech-act.action = assert
speech-act.content.polarity = negative
have . . . . . . . . . . . . . speech-act.content.attribute = resourceAttribute
medical supplies . . speech-act.content.value = medical-supplies
here . . . . . . . . . . . . . speech-act.content.object-id = market
captain . . . . . . . . . .
?
?
?
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 3: A generation training example for Doctor Perez.
suggests the output utterance u = we don?t have
medical supplies here captain. Each utterance u is
accompanied by syntax(u), a syntactic analysis in
Penn Treebank format (Marcus et al, 1994). In this
example, the syntax is a hand-corrected version of
the output of the Charniak parser (Charniak, 2001;
Charniak, 2005) on this sentence; we discuss this
hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
201
3.2 Automatic Grammar Induction and Search
Optimization
The first processing step is to induce a productive
grammar from the training examples. We adopt the
probabilistic tree-adjoining grammar (PTAG) for-
malism and grammar induction technique of (Chi-
ang, 2003). We induce our grammar from training
examples such as Figure 3 using heuristic rules to
assign derivations to the examples, as in (Chiang,
2003). Once derivations have been assigned, sub-
trees within the training example syntax are incre-
mentally detached. This process yields the reusable
linguistic resources in the grammar, as well as the
statistical model needed to compute operation prob-
abilities when the grammar is later used in genera-
tion. Figure 5 in the Appendix illustrates this pro-
cess by presenting the linguistic resources inferred
from the training example of Figure 3.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (200ms for Doctor Perez) and returning a
list of alternative outputs ranked by their derivation
probabilities.
The search space created by a grammar induced
in this way is too large to be searched exhaustively
in most applications. The second step of automated
processing, then, uses the training examples to learn
an effective search policy so that good output sen-
tences can be found in a reasonable time frame. The
solution we have developed employs a beam search
strategy that uses weighted features to rank alterna-
tive grammatical expansions at each step. Our al-
gorithm for selecting features and weights is based
on the search optimization algorithm of (Daum?
and Marcu, 2005), which decides to update feature
weights when mistakes are made during search on
training examples. We use the boosting approach of
(Collins and Koo, 2005) to perform feature selection
and identify good weight values.
4 Empirical Evaluation
In the introduction, we identified run-time speed, ad-
equacy of coverage, authoring burdens, and NLG re-
quest specification as important factors in the selec-
tion of a technology for a dialogue system?s NLG
component. In this section, we evaluate our tech-
nique along these four dimensions.
Hand-authored utterances. We collected a sam-
ple of 220 instances of frames that Doctor Perez?s
dialogue manager had requested of the generation
component in previous dialogues with users. Some
frames occurred more than once in this sample.
Each frame was associated with a single hand-
authored utterance. Some of these utterances arose
in human role plays for Doctor Perez; some were
written by a script writer; others were authored
by system builders to provide coverage for specific
frames. All were reviewed by a system builder for
appropriateness to the corresponding frame.
Training. We used these 220 (frame, utterance)
examples to evaluate both our approach and a shal-
low statistical method called sentence retriever (dis-
cussed below). We randomly split the examples
into 198 training and 22 test examples; we used the
same train/test split for our approach and sentence
retriever.
To train our approach, we constructed training ex-
amples in the format specified in Section 3.1. Syntax
posed an interesting problem, because the Charniak
parser frequently produces erroneous syntactic anal-
yses for utterances in Doctor Perez?s domain, but it
was not obvious how detrimental these errors would
be to overall generated output. We therefore con-
structed two alternative sets of training examples ?
one where the syntax of each utterance was the un-
corrected output of the Charniak parser, and another
where the parser output was corrected by hand (the
syntax in Figure 3 above is the corrected version).
Hand correction of parser output requires consider-
able linguistic expertise, so uncorrected output rep-
resents a substantial reduction in authoring burden.
The connections between surface expressions and
frame key-value pairs were identical in both uncor-
rected and corrected training sets, since they are in-
dependent of the syntax. For each training set, we
trained our generator on the 198 training examples.
We then generated a single (highest-ranked) utter-
ance for each example in both the test and training
sets. The generator sometimes failed to find a suc-
cessful utterance within the 200ms timeout; the suc-
cess rate of our generator was 95% for training ex-
202
amples and 80% for test examples. The successful
utterances were rated by our judges.
Sentence retriever is based on the cross-
language information retrieval techniques described
in (Leuski et al, 2006), and is currently in use for
Doctor Perez?s NLG problem. Sentence retriever
does not exploit any hierarchical syntactic analy-
sis of utterances. Instead, sentence retriever views
NLG as an information retrieval task in which a set
of training utterances are the ?documents? to be re-
trieved, and the frame to be expressed is the query.
At run-time, the algorithm functions essentially as a
classifier: it uses a relative entropy metric to select
the highest ranking training utterance for the frame
that Doctor Perez wishes to express. This approach
has been used because it is to some extent robust
against changes in internal semantic representations,
and against minor deficiencies in the training corpus,
but as with a canned text approach, it requires each
utterance to be hand-authored before it can be used
in dialogue. We trained sentence retriever on the 198
training examples, and used it to generate a single
(highest-ranked) utterance for each example in both
the test and training sets. Sentence retriever?s suc-
cess rate was 96% for training examples and 90%
for test examples. The successful utterances were
rated by our judges.
Figure 7 in the Appendix illustrates the alternative
utterances that were produced for a frame present in
the test data but not in the training data.
Run-time speed. Both our approach and sentence
retriever run within the available 200ms window.
Adequacy of Coverage. To assess output quality,
we conducted a study in which 5 human judges gave
overall quality ratings for various utterances Doctor
Perez might use to express specific semantic frames.
In total, judges rated 494 different utterances which
were produced in several conditions: hand-authored
(for the relevant frame), generated by our approach,
and sentence retriever.
We asked our 5 judges to rate each of the 494 ut-
terances, in relation to the specific frame for which
it was produced, on a single 1 (?very bad?) to 5
(?very good?) scale. Since ratings need to incorpo-
rate accuracy with respect to the frame, our judges
had to be able to read the raw system semantic rep-
resentations. This meant we could only use judges
who were deeply familiar with the dialogue system;
however, the main developer of the new generation
algorithms (the first author) did not participate as
a judge. Judges were blind to the conditions un-
der which utterances were produced. The judges
rated the utterances using a custom-built application
which presented a single frame together with 1 to 6
candidate utterances for that frame. The rating inter-
face is shown in Figure 6 in the Appendix. The order
of candidate utterances for each frame was random-
ized, and the order in which frames appeared was
randomized for each judge.
The judges were instructed to incorporate both
fluency and accuracy with respect to the frame into
a single overall rating for each utterance. While it
is possible to have human judges rate fluency and
accuracy independently, ratings of fluency alone are
not particularly helpful in evaluating Doctor Perez?s
generation component, since for Doctor Perez, a cer-
tain degree of disfluency can contribute to believ-
ability (as noted in Section 2). We therefore asked
judges to make an overall assessment of output qual-
ity for the Doctor Perez character.
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. Agreement between subsets of judges
ranged from ? = 0.802 for the most concordant pair
of judges to ? = 0.593 for the most discordant pair.
We also performed an ANOVA comparing three
conditions (generated, retrieved and hand-authored
utterances) across the five judges; we found sig-
nificant main effects of condition (F (2, 3107) =
55, p < 0.001) and judge (F (4, 3107) = 17, p <
0.001), but no significant interaction (F (8, 3107) =
0.55, p > 0.8). We therefore conclude that the indi-
vidual differences among the judges do not affect the
comparison of utterances across the different condi-
tions, so we will report the rest of the evaluation on
the mean ratings per utterance.
Due to the large number of factors and the dif-
ferences in the number of utterances correspond-
ing to each condition, we ran a small number
of planned comparisons. The distribution of rat-
ings across utterances is not normal; to validate
our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and signifi-
cance always fell in the same general range. We
found a significant difference between generated
203
Generated (N = 90)
Sentence retriever (N = 100)
Rating
Fr
eq
u
en
cy
(%
)
0
10
20
30
40
1 2 3 4 5
Figure 4: Observed ratings of generated (uncorrected
syntax) vs. retrieved sentences for test examples.
output for all examples, retrieved output for all ex-
amples, and hand-authored utterances (F (2, 622) =
16, p < 0.001); however, subsequent t-tests show
that all of this difference is due to the fact that hand-
authored utterances (mean rating 4.4) are better than
retrieved (t(376) = 3.7, p < 0.001) and gener-
ated (t(388) = 5.9, p < 0.001) utterances, whereas
the difference between generated (mean rating 3.8)
and retrieved (mean rating 4.0) is non-significant
(t(385) = 1.6, p > 0.1).
Figure 4 shows the observed rating frequencies
of sentence retriever (mean 3.0) and our approach
(mean 3.6) on the test examples. While this data
does not show a significant difference, it suggests
that retriever?s selected sentences are most fre-
quently either very bad or very good; this reflects
the fact that the classification algorithm retrieves
highly fluent hand-authored text which is sometimes
semantically very incorrect. (Figure 7 in the Ap-
pendix provides such an example, in which a re-
trieved sentence has the wrong polarity.) The qual-
ity of our generated output, by comparison, appears
more graded, with very good quality the most fre-
quent outcome and lower qualities less frequent. In
a system where there is a low tolerance for very
bad quality output, generated output would likely be
considered preferable to retrieved output.
In terms of generation failures, our approach had
poorer coverage of test examples than sentence re-
triever (80% vs. 90%). Note however that in this
study, our approach only delivered an output if it
could completely cover the requested frame. In the
future, we believe coverage could be improved, with
perhaps some reduction in quality, by allowing out-
puts that only partially cover requested frames.
In terms of output variety, in this initial study our
judges rated only the highest ranked output gener-
ated or retrieved for each frame. However, we ob-
served that our generator frequently finds several al-
ternative utterances of relatively high quality (see
Figure 7); thus our approach offers another poten-
tial advantage in output variety.
Authoring burdens. Both canned text and sen-
tence retriever require only frames and correspond-
ing output sentences as input. In our approach, syn-
tax and semantic links are additionally needed. We
compared the use of corrected vs. uncorrected syn-
tax in training. Surprisingly, we found no significant
difference between generated output trained on cor-
rected and uncorrected syntax (t(29) = 0.056, p >
0.9 on test items, t(498) = ?1.1, p > 0.2 on all
items). This is a substantial win in terms of reduced
authoring burden for our approach.
If uncorrected syntax is used, the additional bur-
den of our approach lies only in specifying the se-
mantic links. For the 220 examples in this study,
one system builder specified these links in about 6
hours. We present a detailed cost/benefit analysis of
this effort in (DeVault et al, 2008).
NLG request specification. Both our approach
and sentence retriever accept the dialogue manager?s
native semantic representation for NLG as input.
Summary. In exchange for a slightly increased
authoring burden, our approach yields a generation
component that generalizes to unseen test problems
relatively gracefully, and does not suffer from the
frequent very bad output or the necessity to author
every utterance that comes with canned text or a
competing statistical classification technique.
5 Conclusion and Future Work
In this paper we have presented an approach to spec-
ifying domain-specific, grammar-based generation
by example. The method reduces the authoring bur-
den associated with developing a grammar-based
NLG component for an existing dialogue system.
We have argued that the method delivers relatively
high-quality, domain-specific output without requir-
ing that content authors possess detailed linguistic
knowledge. In future work, we will study the perfor-
204
mance of our approach as the size of the training set
grows, and assess what specific weaknesses or prob-
lematic disfluencies, if any, our human rating study
identifies in output generated by our technique. Fi-
nally, we intend to evaluate the performance of our
generation approach within the context of the com-
plete, running Doctor Perez agent.
Acknowledgments
Thanks to Arno Hartholt, Susan Robinson, Thomas
Russ, Chung-chieh Shan, and Matthew Stone. This
work was sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM), and the content does not necessarily reflect
the position or the policy of the Government, and no
official endorsement should be inferred.
References
Stephen Busemann and Helmut Horacek. 1998. A flex-
ible shallow approach to text generation. In Proceed-
ings of INLG, pages 238?247.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. Proceedings of the
International Joint Conferences on Artificial Intelli-
gence.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL ?01: Proceedings of the
39th Annual Meeting on Association for Computa-
tional Linguistics, pages 124?131, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2005.
ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Rens
Bod, Remko Scha, and Khalil Sima?an, editors, Data
Oriented Parsing, pages 299?316. CSLI Publications,
Stanford.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Hal Daum?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML ?05: Proceed-
ings of the 22nd international conference on Machine
learning, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Fifth International Natural Language Generation
Conference (INLG).
Michael Elhadad. 1991. FUF: the universal unifier user
manual version 5.0. Technical Report CUCS-038-91.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12, pages 129?
154. Sage, Beverly Hills, CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In The 7th SIGdial Workshop
on Discourse and Dialogue.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
E. Reiter, S. Sripada, and R. Robertson. 2003. Acquir-
ing correct knowledge for natural language generation.
Journal of Artificial Intelligence Research, 18:491?
516.
William Swartout, Jonathan Gratch, Randall W. Hill, Ed-
uard Hovy, Stacy Marsella, Jeff Rickel, and David
Traum. 2006. Toward virtual humans. AI Mag.,
27(2):96?108.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. Nl generation for virtual humans in a complex
social environment. In Working Notes AAAI Spring
Symposium on Natural Language Generation in Spo-
ken and Written Dialogue, March.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul
Baghat, Susan Robinson, Andrew Marshall, Dagen
Wang, Sudeep Gandhe, and Anton Leuski. 2005.
Dealing with doctors: A virtual human for non-team
interaction. In SIGdial.
D. R. Traum, W. Swartout, J Gratch, and S Marsella.
2008. A virtual human dialogue model for non-team
interaction. In Laila Dybkjaer and Wolfgang Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In proceedings
of the International Workshop on Computational Se-
mantics, pages 380?394, January.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proc. Corpus Linguistics ?05
Workshop on Using Corpora for Natural Language
Generation.
205
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee =
captain-kirk
speech-act.addressee =
captain-kirk
Figure 5: The linguistic resources automatically inferred from the training example in Figure 3.
Figure 6: Human rating interface.
206
Input semantic form
addressee captain-kirk
dialogue-act.actor doctor-perez
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
speech-act.action assert
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.content.attribute acceptableAttribute
speech-act.content.object-id clinic
speech-act.content.time present
speech-act.content.type state
speech-act.content.value yes
Outputs
Hand-authored
the clinic is acceptable captain
Generated (uncorrected syntax)
Rank Time (ms)
1 16 the clinic is up to standard captain
2 94 the clinic is acceptable captain
3 78 the clinic should be in acceptable condition captain
4 16 the clinic downtown is currently acceptable captain
5 78 the clinic should agree in an acceptable condition captain
Generated (corrected syntax)
Rank Time (ms)
1 47 it is necessary that the clinic be in good condition captain
2 31 i think that the clinic be in good condition captain
3 62 captain this wont work unless the clinic be in good condition
Sentence retriever
the clinic downtown is not in an acceptable condition captain
Figure 7: The utterances generated for a single test example by different evaluation conditions. Generated outputs
whose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in this
paper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for the
same requested semantic form. Note how the output of sentence retriever has the opposite meaning to that of the input
frame.
207
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 11?20,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Can I finish? Learning when to respond to incremental interpretation
results in interactive dialogue
David DeVault and Kenji Sagae and David Traum
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,sagae,traum}@ict.usc.edu
Abstract
We investigate novel approaches to re-
sponsive overlap behaviors in dialogue
systems, opening possibilities for systems
to interrupt, acknowledge or complete a
user?s utterance while it is still in progress.
Our specific contributions are a method for
determining when a system has reached a
point of maximal understanding of an on-
going user utterance, and a prototype im-
plementation that shows how systems can
use this ability to strategically initiate sys-
tem completions of user utterances. More
broadly, this framework facilitates the im-
plementation of a range of overlap behav-
iors that are common in human dialogue,
but have been largely absent in dialogue
systems.
1 Introduction
Human spoken dialogue is highly interactive, in-
cluding feedback on the speech of others while
the speech is progressing (so-called ?backchan-
nels? (Yngve, 1970)), monitoring of addressees
and other listener feedback (Nakano et al, 2003),
fluent turn-taking with little or no delays (Sacks et
al., 1974), and overlaps of various sorts, including
collaborative completions, repetitions and other
grounding moves, and interruptions. Interrup-
tions can be either to advance the new speaker?s
goals (which may not be related to interpreting the
other?s speech) or in order to prevent the speaker
from finishing, which again can be for various rea-
sons. Few of these behaviors can be replicated by
current spoken dialogue systems. Most of these
behaviors require first an ability to perform in-
cremental interpretation, and second, an ability to
predict the final meaning of the utterance.
Incremental interpretation enables more rapid
response, since most of the utterance can be inter-
preted before utterance completion (Skantze and
Schlangen, 2009). It also enables giving early
feedback (e.g., head nods and shakes, facial ex-
pressions, gaze shifts, and verbal backchannels) to
signal how well things are being perceived, under-
stood, and evaluated (Allwood et al, 1992).
For some responsive behaviors, one must go be-
yond incremental interpretation and predict some
aspects of the full utterance before it has been
completed. For behaviors such as comply-
ing with the evocative function (Allwood, 1995)
or intended perlocutionary effect (Sadek, 1991),
grounding by demonstrating (Clark and Schaefer,
1987), or interrupting to avoid having the utter-
ance be completed, one must predict the semantic
content of the full utterance from a partial prefix
fragment. For other behaviors, such as timing a
reply to have little or no gap, grounding by saying
the same thing at the same time (called ?chanting?
by Hansen et al (1996)), performing collaborative
completions (Clark and Wilkes-Gibbs, 1986), or
some corrections, it is important not only to pre-
dict the meaning, but also the form of the remain-
ing part of the utterance.
We have begun to explore these issues in the
context of the dialogue behavior of virtual human
(Rickel and Johnson, 1999) or embodied conver-
sational agent (Cassell et al, 2000) characters for
multiparty negotiation role-playing (Traum et al,
2008b). In these kinds of systems, human-like be-
havior is a goal, since the purpose is to allow a user
to practice this kind of dialogue with the virtual
humans in training for real negotiation dialogues.
The more realistic the characters? dialogue behav-
ior is, the more kinds of negotiation situations can
be adequately trained for. We discuss these sys-
11
tems further in Section 2.
In Sagae et al (2009), we presented our first re-
sults at prediction of semantic content from partial
speech recognition hypotheses, looking at length
of the speech hypothesis as a general indicator of
semantic accuracy in understanding. We summa-
rize this previous work in Section 3.
In the current paper, we incorporate additional
features of real-time incremental interpretation to
develop a more nuanced prediction model that can
accurately identify moments of maximal under-
standing within individual spoken utterances (Sec-
tion 4). We demonstrate the value of this new
ability using a prototype implementation that col-
laboratively completes user utterances when the
system becomes confident about how the utter-
ance will end (Section 5). We believe such pre-
dictive models will be more broadly useful in im-
plementing responsive overlap behaviors such as
rapid grounding using completions, confirmation
requests, or paraphrasing, as well as other kinds of
interruptions and multi-modal displays. We con-
clude and discuss future work in Section 6.
2 Domain setting
The case study we present in this paper is taken
from the SASO-EN scenario (Hartholt et al, 2008;
Traum et al, 2008b). This scenario is designed
to allow a trainee to practice multi-party negoti-
ation skills by engaging in face to face negotia-
tion with virtual humans. The scenario involves
a negotiation about the possible re-location of a
medical clinic in an Iraqi village. A human trainee
plays the role of a US Army captain, and there are
two virtual humans that he negotiates with: Doctor
Perez, the head of the NGO clinic, and a local vil-
lage elder, al-Hassan. The doctor?s main objective
is to treat patients. The elder?s main objective is to
support his village. The captain?s main objective
is to move the clinic out of the marketplace, ide-
ally to the US base. Figure 1 shows the doctor and
elder in the midst of a negotiation, from the per-
spective of the trainee. Figure A-1 in the appendix
shows a sample dialogue from this domain.
The system has a fairly typical set of pro-
cessing components for virtual humans or dia-
logue systems, including ASR (mapping speech
to words), NLU (mapping from words to semantic
frames), dialogue interpretation and management
(handling context, dialogue acts, reference and de-
ciding what content to express), NLG (mapping
Figure 1: SASO-EN negotiation in the cafe: Dr.
Perez (left) looking at Elder al-Hassan.
2
6
6
6
6
6
6
6
4
mood : declarative
sem :
2
6
6
6
6
6
4
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
?
possibility : can
?
speech? act :
?
type : offer
?
3
7
7
7
7
7
5
3
7
7
7
7
7
7
7
5
Figure 2: AVM utterance representation.
frames to words), non-verbal generation, and syn-
thesis and realization. The doctor and elder use
the same ASR and NLU components, but have dif-
ferent modules for the other processing, including
different models of context and goals, and differ-
ent output generators. In this paper, we will often
refer to the characters with various terms, includ-
ing ?virtual humans?, ?agents?, or ?the system?.
In this paper, we are focusing on the NLU
component, looking at incremental interpretation
based on partial speech recognition results, and
the potential for using this information to change
the dialogue strategy where warranted, and pro-
vide responses before waiting for the final speech
result. The NLU output representation is an
attribute-value matrix (AVM), where the attributes
and values represent semantic information that
is linked to a domain-specific ontology and task
model (Hartholt et al, 2008). Figure 2 shows an
example representation, for an utterance such as
?we can provide you with power generators?. The
AVMs are linearized, using a path-value notation,
as shown in Figure 3.
To develop and test the new incremen-
tal/prediction models, we are using a corpus of
12
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
Figure 3: Example NLU frame.
utterances collected from people playing the role
of captain and negotiating with the virtual doctor
and elder. In contrast with Figure A-1, which
is a dialogue with one of the system designers
who knows the domain well, dialogues with naive
users are generally longer, and often have a fairly
high word error rate (average 0.54), with many
out of domain utterances. The system is robust to
these kinds of problems, both in terms of the NLU
approach (Leuski and Traum, 2008; Sagae et al,
2009) as well as the dialogue strategies (Traum
et al, 2008a). This is accomplished in part by
approximating the meaning of utterances. For
example, the frame in Figure 3 is also returned for
an utterance of we are prepared to give you guys
generators for electricity downtown as well as the
ASR output for this utterance, we up apparently
give you guys generators for a letter city don town.
3 Predicting interpretations from partial
recognition hypotheses
Our NLU module, mxNLU (Sagae et al, 2009), is
based on maximum entropy classification (Berger
et al, 1996), where we treat entire individual
frames as classes, and extract input features from
ASR. The training data for mxNLU is a corpus
of approximately 3,500 utterances, each annotated
with the appropriate frame. These utterances were
collected from user sessions with the system, and
the corresponding frames were assigned manually.
Out-of-domain utterances (about 15% of all utter-
ances in our corpus) could not be mapped to con-
cepts in our ontology and task model, and were
assigned a ?garbage? frame. For each utterance
in our corpus, we have both a manual transcrip-
tion and the output of ASR, although only ASR
is used by mxNLU (both at training and at run-
time). Each training instance for mxNLU consists
of a frame, paired with a set of features that rep-
resent the ASR output for user utterances. The
specific features used by the classifier are: each
word in the input string (bag-of-words representa-
tion of the input), each bigram (pairs of consec-
utive words), each pair of any two words in the
input, and the number of words in the input string.
In the 3,500-utterance training set, there are 136
unique frames (135 that correspond to the seman-
tics of different utterances in the domain, plus one
frame for out-of-domain utterances).1 The NLU
task is then framed as a multiclass classification
approach with 136 classes, and about 3,500 train-
ing examples.
Although mxNLU produces entire frames as
output, we evaluate NLU performance by look-
ing at precision and recall of the attribute-value
pairs (or frame elements) that compose frames.
Precision represents the portion of frame elements
produced by mxNLU that were correct, and re-
call represents the portion of frame elements in
the gold-standard annotations that were proposed
by mxNLU. By using precision and recall of
frame elements, we take into account that certain
frames are more similar than others and also al-
low more meaningful comparative evaluation with
NLU modules that construct a frame from sub-
elements or for cases when the actual frame is not
in the training set. The precision and recall of
frame elements produced by mxNLU using com-
plete ASR output are 0.78 and 0.74, respectively,
for an F-score (harmonic mean of precision and
recall) of 0.76.
3.1 NLU with partial ASR results
The simplest way to perform NLU of partial ASR
results is simply to process the partial utterances
using the NLU module trained on complete ASR
output. However, better results may be obtained
by training separate NLU models for analysis of
partial utterances of different lengths. To train
these separate NLU models, we first ran the au-
dio of the utterances in the training data through
our ASR module, recording all partial results for
each utterance. Then, to train a model to ana-
lyze partial utterances containing N words, we
used only partial utterances in the training set con-
taining N words (unless the entire utterance con-
tained less than N words, in which case we sim-
ply used the complete utterance). In some cases,
multiple partial ASR results for a single utterance
1In a separate development set of 350 utterances, anno-
tated in the same way as the training set, we found no frames
that had not appeared in the training set.
13
0
10
20
30
40
50
60
70
80
1 2 3 4 5 6 7 8 9 10 allLength n (words)
F
-
s
c
o
r
e
Trained on all data
Trained on partials up tolength nTrained on partials up tolength n + context
Figure 4: F-score for three NLU models on partial
ASR results up to N words.
contained the same number of words, and we used
the last partial result with the appropriate number
of words.2 We trained ten separate partial NLU
models for N varying from one to ten.
Figure 4 shows the F-score for frames obtained
by processing partial ASR results up to length N
using three variants of mxNLU. The dashed line is
our baseline NLU model, trained on complete ut-
terances only, and the solid line shows the results
obtained with length-specific NLU models. The
dotted line shows results for length-specific mod-
els that also use features that capture aspects of di-
alogue context. In these experiments, we used uni-
gram and bigram word features extracted from the
most recent system utterance to represent context,
but found that these context features did not im-
prove NLU performance. Our final NLU approach
for partial ASR hypotheses is then to train separate
models for specific lengths, using hypotheses of
that length during training (solid line in figure 4).
4 How well is the system understanding?
In this section, we present a strategy that uses
machine learning to more closely characterize the
performance of a maximum entropy based incre-
mental NLU module, such as the mxNLU mod-
ule described in Section 3. Our aim is to iden-
tify strategic points in time, as a specific utterance
is occurring, when the system might react with
confidence that the interpretation will not signif-
2At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial
utterance of length N + 1.
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 5: Incremental interpretation of a user ut-
terance.
icantly improve during the rest of the utterance.
This reaction could take several forms, including
providing feedback, or, as described in Section 5
an agent might use this information to opportunis-
tically choose to initiate a completion of a user?s
utterance.
4.1 Motivating example
Figure 5 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes cap-
tured audio in 200ms chunks. The figure shows
the partial ASR results after the ASR has pro-
cessed each 200ms of audio, along with the F-
14
score achieved by mxNLU on each of these par-
tials. Note that the NLU F-score fluctuates some-
what as the ASR revises its incremental hypothe-
ses about the user utterance, but generally in-
creases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the
agent needs to be able (in the right circumstances)
to make an assessment that it has already under-
stood the utterance ?well enough?, based on the
partial ASR results that are currently available. We
have implemented a specific approach to this as-
sessment which views an utterance as understood
?well enough? if the agent would not understand
the utterance any better than it currently does even
if it were to wait for the user to finish their utter-
ance (and for the ASR to finish interpreting the
complete utterance).
Concretely, Figure 5 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However,
in fact, mxNLU already achieves this maximal F-
score at the moment it interprets the partial ASR
result elder do you agree to move the at 1800ms.
The agent therefore could, in principle, initiate an
overlapping response at 1800ms without sacrific-
ing any accuracy in its understanding of the user?s
utterance.
Of course the agent does not automatically re-
alize that it has achieved a maximal F-score at
1800ms. To enable the agent to make this assess-
ment, we have trained a classifier, which we call
MAXF, that can be invoked for any specific par-
tial ASR result, and which uses various features of
the ASR result and the current mxNLU output to
estimate whether the NLU F-score for the current
partial ASR result is at least as high as the mxNLU
F-score would be if the agent were to wait for the
entire utterance.
4.2 Machine learning setup
To facilitate the construction of our MAXF clas-
sifier, we identified a range of potentially useful
features that the agent could use at run-time to as-
sess its confidence in mxNLU?s output for a given
partial ASR result. These features are exempli-
fied in the appendix in Figure A-2, and include:
K, the number of partial results that have been re-
ceived from the ASR; N , the length (in words) of
the current partial ASR result; Entropy, the en-
tropy in the probability distribution mxNLU as-
signs to alternative output frames (lower entropy
corresponds to a more focused distribution); Pmax,
the probability mxNLU assigns to the most prob-
able output frame; NLU, the most probable output
frame (represented for convenience as fI , where
I is an integer index corresponding to a specific
complete frame). We also define MAXF (GOLD),
a boolean value giving the ground truth about
whether mxNLU?s F-score for this partial is at
least as high as mxNLU?s F-score for the final par-
tial for the same utterance. In the example, note
that MAXF (GOLD) is true for each partial where
mxNLU?s F-score (F (K)) is ? 0.91, the value
achieved for the final partial (elder do you agree to
move the clinic downtown). Of course, the actual
F-score F (K) is not available at run-time, and so
cannot serve as an input feature for the classifier.
Our general aim, then, is to train a classifier,
MAXF, whose output predicts the value of MAXF
(GOLD) as a function of the input features. To
create a data set for training and evaluating this
classifier, we observed and recorded the values of
these features for the 6068 partial ASR results in
a corpus of ASR output for 449 actual user utter-
ances.3
We chose to train a decision tree using Weka?s
J48 training algorithm (Witten and Frank, 2005).4
To assess the trained model?s performance, we car-
ried out a 10-fold cross-validation on our data set.5
We present our results in the next section.
4.3 Results
We will present results for a trained decision
tree model that reflects a specific precision/recall
tradeoff. In particular, given our aim to enable
an agent to sometimes initiate overlapping speech,
while minimizing the chance of making a wrong
assumption about the user?s meaning, we selected
a model with high precision at the expense of
lower recall. Various precision/recall tradeoffs are
possible in this framework; the choice of a spe-
cific tradeoff is likely to be system and domain-
dependent and motivated by specific design goals.
We evaluate our model using several features
which are exemplified in the appendix in Fig-
ure A-3. These include MAXF (PREDICTED),
the trained MAXF classifier?s output (TRUE or
3This corpus was not part of the training data for mxNLU.
4Of course, other classification models could be used.
5All the partial ASR results for a given utterance were
constrained to lie within the same fold, to avoid training and
testing on the same utterance.
15
FALSE) for each partial; KMAXF, the first par-
tial number for which MAXF (PREDICTED) is
TRUE; ?F (K) = F (K) ? F (Kfinal), the ?loss?
in F-score associated with interpreting partial K
rather than the final partialKfinal for the utterance;
T (K), the remaining length (in seconds) in the
user utterance at each partial.
We begin with a high level summary of the
trained MAXF model?s performance, before dis-
cussing more specific impacts of interest in the di-
alogue system. We found that our trained model
predicts that MAXF = TRUE for at least one
partial in 79.2% of the utterances in our cor-
pus. For the remaining utterances, the trained
model predicts MAXF = FALSE for all partials.
The precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high
precision means that 88% of the time that the
model predicts that F-score is maximized at a spe-
cific partial, it really is. On the other hand, the
lower recall means that only 52% of the time that
F-score is in fact maximized at a given partial does
the model predict that it is.
For the 79.2% of utterances for which the
trained model predicts MAXF = TRUE at some
point, Figure 6 shows the amount of time in sec-
onds, T (KMAXF), that remains in the user utter-
ance at the time partialKMAXF becomes available
from the ASR. The mean value is 1.6 seconds; as
the figure shows, the time remaining varies from 0
to nearly 8 seconds per utterance. This represents
a substantial amount of time that an agent could
use strategically, for example by immediately ini-
tiating overlapping speech (perhaps in an attempt
to improve communication efficiency), or by ex-
ploiting this time to plan an optimal response to
the user?s utterance.
However, it is also important to understand the
cost associated with interpreting partial KMAXF
rather than waiting to interpret the final ASR result
Kfinal for the utterance. We therefore analyzed
the distribution in ?F (KMAXF) = F (KMAXF)?
F (Kfinal). This value is at least 0.0 if mxNLU?s
output for partial KMAXF is no worse than its out-
put for Kfinal (as intended). The distribution is
given in Figure 7. As the figure shows, 62.35% of
the time (the median case), there is no difference
in F-score associated with interpreting KMAXF
rather than Kfinal. 10.67% of the time, there is
a loss of -1, which corresponds to a completely
incorrect frame at KMAXF but a completely cor-
Utterance time remaining (seconds)
F
r
e
q
u
e
n
c
y
0 2 4 6 80
10
20
30
Figure 6: Distribution of T (KMAXF).
?F (KMAXF) range Percent of
utterances
-1 10.67%
(?1, 0) 17.13%
0 62.35%
(0, 1) 7.30%
1 2.52%
mean(?F (KMAXF)) -0.1484
median(?F (KMAXF)) 0.0000
Figure 7: The distribution in ?F (KMAXF), the
?loss? associated with interpreting partial KMAXF
rather than Kfinal.
rect frame at Kfinal. The converse also happens
2.52% of the time: mxNLU?s output frame is com-
pletely correct at the early partial but completely
incorrect at the final partial. The remaining cases
are mixed. While the median is no change in F-
score, the mean case is a loss in F-score of -0.1484.
This is the mean penalty in NLU performance that
could be paid in exchange for the potential gain in
communication efficiency suggested by Figure 6.
5 Prototype implementation
To illustrate one use of the techniques described in
the previous sections, we have implemented a pro-
totype module that performs user utterance com-
pletion. This allows an agent to jump in during a
user?s utterance, and say a completion of the utter-
ance before it is finished, at a point when the agent
16
thinks it understands what the user means. This
type of completion is often encountered in human-
human dialogue, and may be used, for example,
for grounding or for bringing the other party?s turn
to a conclusion.
We have equipped one of our virtual humans,
Doctor Perez, with an ability to perform comple-
tions as follows. The first step is for the agent to
recognize when it understands what the user wants
to say. As discussed in Sections 3 and 4, this often
happens before the user has completed the utter-
ance. NLU is performed on partial ASR hypothe-
ses as they become available, and MAXF decides
whether the agent?s understanding of the current
partial hypothesis is likely to improve given more
time. Once MAXF indicates that the agent?s un-
derstanding is likely to be already maximized for
the utterance, we take the current partial ASR hy-
pothesis and attempt to generate text to complete it
in a way that is fluent and agrees with the meaning
of the utterance the user has in mind.
The generation of the surface text for comple-
tions takes advantage of the manual transcriptions
in the corpus of utterances used to train the NLU
module. For each frame that the agent under-
stands, our training set contains several user utter-
ances that correspond to the meaning in that frame.
At the point where the agent is ready to formu-
late a completion, mxNLU has already predicted
a frame for the user?s utterance (even though it is
still incomplete). We then consider only the set
of known utterances that correspond to that frame
as possible sources of completions. As a simple
distance metric, we compute the word error rate
(WER) between the current partial hypothesis for
the user?s utterance and a prefix of each of these
known utterances. In our prototype, these prefixes
have the same length as the current partial ASR
hypothesis. We then select the utterance whose
prefix has the lowest WER against the current par-
tial ASR hypothesis. As a final step, we look in
the prefix of our selected utterance for the last oc-
currence of the last word in the partial ASR, and
if such a word is found, we take the remainder of
the utterance as the agent?s completion. Consid-
ering only the set of utterances that correspond to
the frame predicted by mxNLUmakes it likely that
the completion will have the appropriate meaning.
Since the completion is a suffix of a transcript of a
previous user utterance, and this suffix follows the
last word uttered by the user, it is likely to form a
fluent completion of the user?s partial utterance.
For 356 utterances in our development set, the
procedure above generated completions contain-
ing at least one word for 177 utterances. We list
below four representative examples of user utter-
ance completions generated by Doctor Perez:
1. Partial utterance: we need to
Predicted completion: move your clinic
Actual user completion: move the clinic
2. Partial utterance: I have orders
Predicted completion: to move you and this
clinic
Actual user completion: to help you in mov-
ing the clinic to a new location
3. Partial utterance: the market
Predicted completion: is not safe
Actual user completion: is not a safer loca-
tion
4. Partial utterance: we can also
Predicted completion: give you medical sup-
plies
Actual user completion: build you a well
Although agent completions for user utterances
are often acceptably accurate (examples 1-3), oc-
casionally they are incorrect (example 4). The
frequency of appropriate completions depends on
the accuracy of mxNLU and the chosen preci-
sion/recall tradeoff in MAXF. Finally, although
the agent has the ability to generate these com-
pletions, clearly it should not complete the user?s
utterance at every opportunity. Determining a pol-
icy that results in natural behavior with respect to
the frequency of completions for different types of
agents is a topic under current investigation.
6 Summary and future work
We have presented a framework for interpretation
of partial ASR hypotheses of user utterances, and
high-precision identification of points within user
utterances where the system already understands
the intended meaning. Our initial implementa-
tion of an utterance completion ability for a vir-
tual human serves to illustrate the capabilities of
this framework, but only scratches the surface of
the new range of dialogue behaviors and strategies
it allows.
Immediate future work includes the design of
policies for completions and interruptions that re-
17
sult in natural conversational behavior. Other ap-
plications of this work include the generation of
paraphrases that can be used for grounding, in ad-
dition to extra-linguistic behavior during user ut-
terances, such as head nods and head shakes.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily
reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred. We would also like to thank
Anton Leuski for facilitating the use of incremen-
tal speech results, and David Schlangen and the
ICT dialogue group, for helpful discussions.
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahlsen.
1992. On the semantics and pragmatics of linguistic
feedback. Journal of Semantics, 9.
Jens Allwood. 1995. An activity based approach to
pragmatics. Technical Report (GPTL) 75, Gothen-
burg Papers in Theoretical Linguistics, University of
G?teborg.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Justine Cassell, Joseph Sullivan, Scott Prevost, and
Elizabeth Churchill, editors. 2000. Embodied Con-
versational Agents. MIT Press, Cambridge, MA.
Herbert H. Clark and Edward F. Schaefer. 1987. Col-
laborating on contributions to conversation. Lan-
guage and Cognitive Processes, 2:1?23.
Herbert H. Clark and DeannaWilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition, 22:1?
39. Also appears as Chapter 4 in (Clark, 1992).
Herbert H. Clark. 1992. Arenas of Language Use.
University of Chicago Press.
B. Hansen, D. Novick, and S. Sutton. 1996. Prevention
and repair of breakdowns in a simple task domain.
In Proceedings of the AAAI-96 Workshop on De-
tecting, Repairing, and Preventing Human-Machine
Miscommunication, pages 5?12.
A. Hartholt, T. Russ, D. Traum, E. Hovy, and S. Robin-
son. 2008. A common ground for virtual humans:
Using an ontology in a natural language oriented
virtual human architecture. In Language Resources
and Evaluation Conference (LREC), May.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
Yukiko I. Nakano, Gabe Reinstein, Tom Stocky, and
Justine Cassell. 2003. Towards a model of face-to-
face grounding. In ACL, pages 553?561.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50:696?735.
M. D. Sadek. 1991. Dialogue acts are rational
plans. In Proceedings of the ESCA/ETR workshop
on multi-modal dialogue.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of
partial speech recognition results in dialogue sys-
tems. In Short Paper Proceedings of NAACL HLT.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, pages 745?753.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-
team interaction. In L. Dybkjaer and W. Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
D. R. Traum, S. Marsella, J. Gratch, J. Lee, and
A. Hartholt. 2008b. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents.
In Helmut Prendinger, James C. Lester, and Mitsuru
Ishizuka, editors, IVA, volume 5208 of Lecture Notes
in Computer Science, pages 117?130. Springer.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
Victor H. Yngve. 1970. On getting a word in edgewise.
In Papers from the Sixth Regional Meeting, pages
567?78. Chicago Linguistic Society.
18
A Appendix
1 C Hello Doctor Perez.
2 D Hello captain.
3 E Hello captain.
4 C Thank you for meeting me.
5 E How may I help you?
6 C I have orders to move this clinic to a camp near the US base.
7 E We have many matters to attend to.
8 C I understand, but it is imperative that we move the clinic out of this area.
9 E This town needs a clinic.
10 D We can?t take sides.
11 C Would you be willing to move downtown?
12 E We would need to improve water access in the downtown area, captain.
13 C We can dig a well for you.
14 D Captain, we need medical supplies in order to run the clinic downtown.
15 C We can deliver medical supplies downtown, Doctor.
16 E We need to address the lack of power downtown.
17 C We can provide you with power generators.
18 E Very well captain, I agree to have the clinic downtown.
19 E Doctor, I think you should run the clinic downtown.
20 D Elder, the clinic downtown should be in an acceptable condition before
we move.
21 E I can renovate the downtown clinic, Doctor.
22 D OK, I agree to run the clinic downtown, captain.
23 C Excellent.
24 D I must go now.
25 E I must attend to other matters.
26 C Goodbye.
26 D Goodbye.
26 E Farewell, sir.
Figure A-1: Successful negotiation dialogue between C, a captain (human trainee), D, a doctor (virtual
human), and E, a village elder (virtual human).
19
MAXF model training features
Partial ASR result F (K) K N Entropy Pmax NLU MAXF (GOLD)
(empty) 0.00 1 0 2.96 0.48 f82 FALSE
(empty) 0.00 2 0 2.96 0.48 f82 FALSE
all 0.00 3 1 0.82 0.76 f72 FALSE
elder 0.00 4 1 0.08 0.98 f39 FALSE
elder do you 0.83 5 3 1.50 0.40 f68 FALSE
elder to you d 0.50 6 3 1.31 0.75 f69 FALSE
elder do you agree 0.83 7 4 1.84 0.35 f68 FALSE
elder do you agree to 0.83 8 5 1.40 0.61 f68 FALSE
elder do you agree to move the 0.91 9 7 0.94 0.49 f10 TRUE
elder do you agree to move the 0.91 10 7 0.94 0.49 f10 TRUE
elder do you agree to move the clinic to 0.83 11 9 1.10 0.58 f68 FALSE
elder do you agree to move the clinic down 0.83 12 9 1.14 0.66 f68 FALSE
elder do you agree to move the clinic downtown 0.91 13 9 0.50 0.89 f10 TRUE
elder do you agree to move the clinic downtown 0.91 14 9 0.50 0.89 f10 TRUE
Figure A-2: Features used to train the MAXF model.
MAXF model evaluation features
K F (K) ?F (K) T (K) MAXF (PREDICTED)
1 0.00 -0.91 2.6 FALSE
2 0.00 -0.91 2.4 FALSE
3 0.00 -0.91 2.2 FALSE
4 0.00 -0.91 2.0 FALSE
5 0.83 -0.08 1.8 FALSE
6 0.50 -0.41 1.6 FALSE
7 0.83 -0.08 1.4 FALSE
8 0.83 -0.08 1.2 FALSE
9 (= KMAXF) 0.91 0.00 (=?F (KMAXF)) 1.0 TRUE
10 0.91 0.00 0.8 TRUE
11 0.83 -0.08 0.6 FALSE
12 0.83 -0.08 0.4 FALSE
13 0.91 0.00 0.2 TRUE
14 0.91 0.00 0.0 TRUE
Figure A-3: Features used to evaluate the MAXF model.
20
Interpreting Vague Utterances in Context
David DeVault and Matthew Stone
Department of Computer Science
Rutgers University
Piscataway NJ 08854-8019
David.DeVault@rutgers.edu, Matthew.Stone@rutgers.edu
Abstract
We use the interpretation of vague scalar predi-
cates like small as an illustration of how system-
atic semantic models of dialogue context en-
able the derivation of useful, fine-grained utter-
ance interpretations from radically underspeci-
fied semantic forms. Because dialogue context
suffices to determine salient alternative scales
and relevant distinctions along these scales,
we can infer implicit standards of comparison
for vague scalar predicates through completely
general pragmatics, yet closely constrain the in-
tended meaning to within a natural range.
1 Introduction
Modeling context and its effects on interpretation
may once have seemed to call for an open-ended in-
vestigation of people?s knowledge of the common-
sense world (Hobbs et al, 1993). But research on
the semantics of practical dialogue (Allen et al,
2001) now approaches dimensions of context sys-
tematically, through increasingly lightweight, fac-
tored models. The evolving state of real-world ac-
tivity proceeds predictably according to background
plans and principles of coordination (Rich et al,
2001). The status of the dialogue itself is defined
by circumscribed obligations to ground prior ut-
terances, follow up open issues, and advance real-
world negotiation (Larsson and Traum, 2000). Fi-
nally, the evolving state of the linguistic context
is a direct outgrowth of the linguistic forms inter-
locutors use and the linguistic relationships among
successive utterances (Ginzburg and Cooper, 2001;
Asher and Lascarides, 2003). These compatible
models combine directly to characterize an aggre-
gate information state that provides a general back-
ground for interpretation (Bunt, 2000).
We argue in this paper that such integrated mod-
els enable systems to calculate useful, fine-grained
utterance interpretations from radically underspec-
ified semantic forms. We focus in particular on
vague scalar predicates like small or long. These
predicates typify qualitative linguistic expression of
quantitative information, and are thus both chal-
lenging and commonplace. Building on a mul-
tidimensional treatment of dialogue context, we
develop and implement a theoretically-motivated
model of vagueness which is unique in treating
vague predicates as genuinely vague and genuinely
context-sensitive, yet amenable to general processes
of contextual and interpretive inference.
1.1 Semantic insights
We pursue our argument in the context of an imple-
mented drawing application, FIGLET, which allows
users to give English instructions to draw a carica-
ture of an expressive face. Figure 1 shows a repre-
sentative interaction with FIGLET; the user gives the
successive instructions in (1):
(1) a. Make two small circles.
b. Draw a long line underneath.
Like Di Eugenio and Webber (1996), we empha-
size that understanding such instructions requires
contextual inference combining linguistic, task and
domain knowledge. For example, consider the re-
sponse to (1a) of placing circles so as to form the
eyes of a new face. To recognize the possibility
of drawing eyes exploits knowledge of the ongoing
drawing task. To put the eyes where they belong
in the upper part of the new face exploits domain
knowledge. The response to (1b) adds the linguis-
tic context as another ingredient. To identify where
the line goes, the user uses the objects mentioned
recently in the interaction as the understood spatial
landmark for underneath. Figure 1 highlights the
importance of using multidimensional representa-
tions of dialogue context in understanding instruc-
tions for quantitative domains.
We leverage this background context in our com-
putational approach to vagueness. We model a
vague utterance like draw a long line as though it
meant draw a line with, you know, length. In this
approach, vague predicates are completely under-
specified; linguistic knowledge says nothing about
how long something long is. Instead, vague lan-
guage explicitly draws on the background knowl-
Initial blank figure state. After the user utters (1a):
Make two small circles.
After the user utters (1b):
Draw a long line underneath.
Figure 1: Motivating interaction: Vague instructions to draw a face.
edge already being applied in utterance interpre-
tation. The user?s motivation in using long is to
differentiate an intended interpretation, here an in-
tended action, from alternative possibilities in con-
text. Background knowledge already sets out the
relevant ways to draw a line; drawing a long line
means singling out some of them by the length of
that new line. This model recalls dynamic theo-
ries of vague scalar predicates, such as the seman-
tics of Kyburg and Morreau (2000), Barker (2002),
or Kennedy (2003), but it is exactly implemented
in FIGLET. The implementation capitalizes on the
richness of current models of context to recover
content for the you know of vagueness.
1.2 Overview
In Section 2, we motivate approaches to the seman-
tics of vague scalar predicates that associate them
with a presupposed standard of comparison. We il-
lustrate how context can be understood to supply
possible standards, and how pragmatic reasoning
from utterances allows interlocutors to infer them.
In Section 3, we establish a bridge to the general
treatment of practical dialogue, by showing how
multiple dimensions of context generally contribute
to recognizing possible interpretations for under-
specified utterances. Section 4 builds on Sections 2
and 3 to show how FIGLET exploits a rich model of
utterance context to respond cooperatively to vague
utterances like (1a) and (1b), while Section 5 de-
tails FIGLET?s actual implementation. We conclude
in Section 6 by suggesting further challenges that
vagueness still poses for computational semantics.
2 Vague standards in context
We adopt a view of vague predicates motivated by
linguistic theory, particularly Kennedy?s approach
(1999; 2003). We assume that gradable adjectives
are associated with measurement functions mapping
individuals to degrees on a scale. In FIGLET?s draw-
ing domain, the relevant measurements pertain to
spatial properties. For long, for example, the mea-
surement maps individuals to their spatial lengths;
for small, it maps individuals to degrees on an in-
verted scale of size.
Positive gradable adjectives compare measured
degrees against a standard on the scale which is de-
rived from context. For example, long says that an
object?s length exceeds the threshold set by the cur-
rent standard for length. Other forms, such as com-
parative adjectives or adjectives with explicit mea-
sure phrases, compare degrees differently.
Importantly, grammar says nothing about how
standards for positive gradable adjectives are de-
rived. In other words, contra Cresswell (1977) and
others, the interpretation of adjectives is not com-
puted relative to a grammatically-specified ?com-
parison class? of related objects. And, contra Oates
et al (2000) and Roy and Pentland (2002), the in-
terpretation of adjectives need not require statistical
knowledge about how objects with different mea-
surements on a scale tend to be described. Instead,
standards are derived directly from an evolving con-
text by the general principles that govern pragmatic
resolution of context dependence.
Kennedy synthesizes a range of evidence for this
claim. Here we go further, and provide a formal,
implemented model. We can sketch the evidence
and our model by considering two key examples.
First, we illustrate that vagueness depends di-
rectly on specific contextually-relevant distinctions.
Consider the session with FIGLET shown in Fig-
ure 2. The user has elected to draw two objects side-
by-side. The initial context just contains a square.
The user utters (2).
(2) Make a small circle.
To interpret (2) it doesn?t seem to help to appeal to
general knowledge about how big circles tend to be.
(It?s quite convoluted to even frame the idea in a
sensible way.) Graff (2000) observes that standards
often implicitly appeal to what we expect about par-
ticular individuals, not just what we know about
similar individuals. In context, here, the user just
seems to be asking for a circle vaguely smaller than
the square. This is the interpretation FIGLET builds;
to comply, FIGLET draws the circle an arbitrary but
representative possible size. The point is that salient
objects and actions inevitably set up meaningful dis-
Initial figure state. After the user utters (2). Initial figure state. After the user utters (3).
Figure 2: Taking standards from context in (2):
Make a small circle.
Figure 3: Disambiguating contextual standards
in (3): Make the small square a circle.
tinctions in the context. Interlocutors exploit these
distinctions in using vague adjectives.
Figure 3 illustrates that understanding vagueness
is part of a general problem of understanding utter-
ances. Figure 3 shows FIGLET?s action in a more
complex context, containing two squares of differ-
ent sizes. We consider the user?s instruction (3):
(3) Make the small square a circle.
FIGLET?s action changes the smaller of the two
squares. The standard behind this interpretation
is implicitly set to differentiate the contextually-
salient objects from one another; the natural reso-
lution of (3) does not require that either square be
definitely small (Kyburg and Morreau, 2000). In
Figure 3, for example, there are different potential
standards that would admit either both squares or
neither square as small. However, we can rule out
these candidate standards in interpreting (3). The
user?s communicative intention must explain how
a unique square from the context can be identified
from (3) using a presupposed small standard. If that
standard is too big, both squares will fit. If that stan-
dard is too small, neither square will fit. Only when
that standard falls between the sizes of the squares
does (3) identify a unique square.
The examples in Figures 2 and 3 show two ways
new standards can be established. Once established,
however, standards become part of the evolving
context (Barker, 2002). Old standards serve as de-
faults in interpreting subsequent utterances. Only
if no better interpretation is found will FIGLET go
back and reconsider its standard. This too is general
pragmatic reasoning (Stone and Thomason, 2003).
3 Dimensions of context in interpretation
To cash out our account of contextual reasoning
with vagueness, we need to characterize the con-
text for practical dialogue. Our account presupposes
a context comprising domain and situation knowl-
edge, task context and linguistic context. In this sec-
tion, we survey each of these dimensions of context,
and show how they converge in the resolution of un-
derspecification across a wide range utterances.
Domain and situation knowledge describes the
commonsense structure of the real-world objects
and actions under discussion. Practical dialogue re-
stricts this otherwise open-ended specification to the
circumscribed facts that are directly relevant to an
ongoing collaboration. For example, in our drawing
domain, individuals are categorized by a few types:
types of shape such as circles and squares; and types
of depiction such as eyes and heads. These types
come with corresponding constraints on individuals.
For example, the shape of a mouth may be a line, an
ellipse, or a rectangle, while the shape of a head can
only be an ellipse. These constraints contribute to
interpretation. For instance, a head can never be de-
scribed as a line, for example, since heads cannot
have this shape.
Task context tracks collaborators? evolving com-
mitment to shared goals and plans during joint ac-
tivity. In FIGLET?s drawing domain, available ac-
tions allow users to build figure parts by introducing
shapes and revising them. Our experience is that
users? domain plans organize these actions hierar-
chically into strategic patterns. For example, users
tend to complete the structures they begin drawing
before drawing elsewhere; and once they are satis-
fied with what they have, they proceed in natural
sequence to a new part nearby. Task context plays
a powerful role in finding natural utterance inter-
pretations. By recording a plan representation and
keeping track of progress in carrying it out, FIGLET
has access to a set of candidate next actions at each
point in an interaction. Matching the user?s utter-
ance against this candidate set restricts the interpre-
tation of instructions based on the drawing already
created and the user?s focus of attention within it.
For example, if the user has just drawn the right eye
onto an empty face, they are likely to turn to the left
eye next. This context suggests making a winking
left eye in response to draw a line, an interpretation
that might not otherwise be salient.
Linguistic context records the evolving status
of pragmatic distinctions triggered by grammatical
conventions. One role of the linguistic context is its
contribution to distinguishing the prominent entities
Initial figure state. After the user utters (4):
Draw a line underneath.
Figure 4: Context in instructions.
that can serve as the referents of pronouns and other
reduced expressions. To see this, note that, as far
as domain knowledge and task context go, the in-
struction make it bigger could apply to any object
currently being created. If the figure is hierarchi-
cal, there will be many possibilities. Yet we typi-
cally understand it to refer specifically to an object
mentioned saliently in the previous utterance. The
linguistic context helps disambiguate it.
Figure 4 illustrates how the three different dimen-
sions of context work together. It illustrates an inter-
action with FIGLET where the user has just issued an
instruction to create two eyes, resulting in the figure
state shown at the left in Figure 4. The user?s next
instruction is (4):
(4) Draw a line underneath.
We focus on how the context constrains the position
and orientation of the line.
Linguistic context indicates that underneath
should be understood as underneath the eyes. This
provides one constraint on the placement of the line.
Task context makes drawing the mouth a plausible
candidate next action. Domain knowledge shows
that the mouth can be a line, but only if further
constraints on position, orientation and length are
met. In understanding the instruction, FIGLET ap-
plies all these contextual constraints simultaneously.
The set of consistent solutions?drawing a horizon-
tal line at a range of plausible mouth positions be-
low the eyes?constitutes the utterance interpreta-
tion. FIGLET acts to create the result in Figure 4 by
choosing a representative action from this set.
4 Interpreting vague utterances in context
In our approach, the linguistic context stores agreed
standards for vague predicates. Candidate standards
are determined using information available from do-
main knowledge and the current task context. In
FIGLET?s drawing domain, possibilities include the
actual measurements of objects that have already
been drawn. They also include the default domain
measurements for new objects that task context says
could be added. Setting standards by a measure-
ment is our shorthand for adopting an implicit range
of compatible standards; these standards remain
vague, especially since many options are normally
available (Graff, 2000).
We treat the use of new candidate standards in in-
terpretation as a case of presupposition accommo-
dation (Bos, 2003). In presupposition accommo-
dation, the interpretation of an utterance must be
constructed using a context that differs from the ac-
tual context. When speakers use an utterance which
requires accommodation, they typically expect that
interlocutors will update the dialogue context to in-
clude the additional presumptions the utterance re-
quires. We assume that all accommodation is sub-
ject to two Gricean constraints. First, we assume
whenever possible that an utterance should have a
uniquely identifiable intended interpretation in the
context in which it is to be interpreted. Second, we
assume that when interpretations in alternative con-
texts are available, the speaker is committed to the
strongest one?compare Dalrymple et al (1998).
Inferring standards for vague predicates is a special
case of this general Gricean reasoning.
The principles articulated thus far in Sections 2?4
allow us to offer a precise explanation of FIGLET?s
behavior as depicted in Figure 1. The user starts
drawing a face with an empty figure. In this domain
and task context, make two circles fits a number of
possible actions. For example, it fits the action of
drawing a round head and its gaping mouth. How-
ever, in (1a), what the user actually says is make
two small circles. The interpretation for (1a) must
accommodate a standard for small and select from
the continuum of size possibilities two new circles
that meet this standard.
The standards in this context are associated with
the size distinctions among potential new objects.
The different qualitative behavior of these standards
in interpretation can be illustrated by the standards
set from possible new circular objects that are con-
sistent with the face-drawing task. We can set the
standard from the default size of an eye, from the
default size of a mouth (larger), or from the default
size of a head (larger still).1 Because each stan-
dard allows all smaller objects to be created next,
these standards lead to 1, 3, and 6 interpretations,
respectively. So we recover the standard from the
eye, which results in a unique interpretation.2
1Since the default sizes of new objects reflect the relative
dimensions of any other objects already in the figure, FIGLET?s
default sizes are not generally equivalent to static comparison
classes.
2Note that there are many potential sources of standards for
small that FIGLET does not currently pursue. E.g. the average
size of all objects already in the figure. We believe that general
In tandem with its response, FIGLET tracks the
changes to the context. The task context is updated
to note that the user has drawn the eyes and must
continue with the process of creating and revising
the features of the face. The linguistic context is
updated to include the new small standard, and to
place the eyes in focus.
This updated context provides the background
for (1b), the user?s next instruction draw a long
line underneath. In this context, as we saw with
Figure 4, context makes it clear that any response
to draw a line underneath must draw the mouth.
Thus, unlike in (1a), all the interpretations here have
the same qualitative form. Nevertheless, FIGLET?s
Gricean reasoning can still adjust the standard for
length to differentiate interpretations quantitatively,
and thereby motivate the user?s use of the word long
in the instruction. FIGLET bases its possible stan-
dards for length on both actual and potential ob-
jects. It can set the standard from an actual eye or
from the two eyes together; and it can set the stan-
dard from the default mouth or head. The mouth, of
course, must fit inside the head; the largest standard
is ruled out. All the other standards lead to unique
interpretations. Since the length of the two eyes to-
gether is the strictest of the remaining standards, it
is adopted. This interpretation leads FIGLET to the
response illustrated at the right in Figure 1.
5 Implementation
We have implemented FIGLET in Prolog using
CLP(R) real constraints (Jaffar and Lassez, 1987)
for metric and spatial reasoning. This section
presents a necessarily brief overview of this imple-
mentation; we highlight how FIGLET is able to ex-
actly implement the semantic representations and
pragmatic reasoning presented in Sections 2?4. We
offer a detailed description of our system and dis-
cuss some of the challenges of building it in DeVault
and Stone (2003).
5.1 Semantic representation
In FIGLET, we record the semantics of user instruc-
tions using constraints, or logical conjunctions of
open atomic formulas, to represent the contextual
requirements that utterances impose; we view these
constraints as presuppositions that speakers make in
using the utterance. We assume matches take the
form of instances that supply particular domain rep-
resentations as suitable values for variables. Stone
(2003) motivates this framework in detail.
methods for specifying domain knowledge will help provide
the meaningful task distinctions that serve as candidate stan-
dards for vague predicates on our approach, but pursuing this
hypothesis is beyond the scope of this paper.
In (5a-d), we show the presuppositions FIGLET
assigns to an utterance of Make two small circles,
arranged to show the contributions of each individ-
ual word. In (5e), we show the contribution made
by the utterance to an evolving dialogue; the effect
is to propose that an action be carried out.
(5) a. simple(A)? target(A,X)?fits plan(A)?
holds(result(A,now),visible(X))?
holds(now, invisible(X))?
b. number(X ,2)?
c. standard(small,S)?
holds(result(A,now),small(X ,S))?
d. number(X ,multiple)?
holds(result(A,now),shape(X ,circle))
e. propose(A)
We formulate these constraints in an expressive
ontology. We have terms and variables for ac-
tions, such as A; for situations, such as now and
result(A,now); for objects, such as X ; for stan-
dards for gradable vague predicates (scale-threshold
pairs), such as S; and for quantitative points and in-
tervals of varying dimensionality, as necessary.
5.2 Pragmatic reasoning
Constraint networks such as (5a-e) provide a uni-
form venue for describing the various contextual
dependencies required to arrive at natural utterance
interpretations. Thus, the contextual representation
and reasoning outlined in Sections 3 and 4 is real-
ized by a uniform mechanism in FIGLET: specifica-
tions of how to reason from context to find solutions
to these constraints.
For example, Section 3 described domain knowl-
edge that links particular object types like eyes and
heads with type-specific constraints. In our imple-
mentation, we specify real and finite constraints that
individuals of each type must satisfy. In order for an
individual e of type t to serve as part of a solution
to a constraint network like (5a-e), e must addition-
ally meet the constraints associated with type t. In
this way, FIGLET requires utterance interpretations
to respect domain knowledge.
Solving many of the constraints appearing in (5a-
e) requires contextual reasoning about domain ac-
tions and their consequences. Some constraints
characterize actions directly; thus simple(A) means
that A is a natural domain action rather than
an abstruse one. Constraints can describe the
effects of actions by reference to the state of
the visual display in hypothetical situations; thus
holds(result(A,now),shape(X ,circle)) means that
the individual X has a circular shape once action
A is carried out. Constraints can additionally char-
acterize causal relationships in the domain; thus
target(A,X) means that action A directly affects X ,
and the constraints of (5a-d) together mean that car-
rying out action A in the current situation causes two
small circles to become visible. These constraints
are proved in FIGLET by what is in effect a planner
that can find complex actions that achieve specified
effects via a repertoire of basic domain actions.
Task context is brought to bear on interpretation
through the fits plan(A) constraint of (5a). FIGLET
uses a standard hierarchical, partially ordered plan
representation to record the structure of a user?s
task. We specify the solutions to fits plan(A) to
be just those actions A that are possible next steps
given the user?s current state in achieving the task.
Since these task-appropriate actions can factor addi-
tional constraints into interpretation, enforcing the
fits plan(A) constraint can help FIGLET identify a
natural interpretation.
As discussed in Section 4, FIGLET records a
list of current standards for vague scalar adjec-
tives in the linguistic context. The constraint
standard(small,S) of (5c) connects the overall ut-
terance interpretation to the available standards for
small in the linguistic context. FIGLET interprets ut-
terances carrying semantic constraints of the form
standard(vague-predicate,S) in one or two stages.
In the first stage, the constraint is solved just in
case S is the prevailing standard for vague-predicate
in the linguistic context. If there is no prevailing
standard for an evoked vague property, or if this
stage does not yield a unique utterance interpreta-
tion, then FIGLET moves to a second stage in which
the constraint is solved for any standard that cap-
tures a relevant distinction for vague-predicate in
the context. If there is a strongest standard that re-
sults in a unique interpretation, it is adopted and in-
tegrated into the new linguistic context.
5.3 Parsing and Interpretation
Language understanding in FIGLET is mediated by a
bottom-up chart parser written in Prolog. As usual,
chart edges indicate the presence of recognized par-
tial constituents within the input sequence. In ad-
dition, edges now carry constraint networks that
specify the contextual reasoning required for under-
standing. In addition to finite instances (Schuler,
2001), these networks include real constraints that
formalize metric and spatial relationships. Interpre-
tation of these networks is carried out incremen-
tally, during parsing; each edge thus records a set
of associated candidate interpretations. Since do-
main reasoning can be somewhat time-intensive in
our current implementation, we adopt a strategy
of delaying the solution of certain constraints until
enough lexical material has accrued that the asso-
ciated problem-solving is judged tractable (DeVault
and Stone, 2003).
6 Assessment and Conclusion
In our approach, we specify a genuinely vague se-
mantics: vague words evoke a domain-specific scale
that can differentiate alternative domain individuals.
To find a unique interpretation for a vague utter-
ance, we leverage ordinary inference about the do-
main, task, and linguistic context to recover implicit
thresholds on this scale.
We believe that further methodological advances
will be required to evaluate treatments of vagueness
in indefinite reference, such as that considered here.
For example, obviously the very idea of a ?gold
standard? for resolution of vagueness is problem-
atic. We believe that the best argument for a theory
of vagueness in a language interface would show
that naive users of the interface are, on the whole,
likely to accept its vague interpretations and un-
likely to renegotiate them through clarification. But
the experiment would have to rule out confounding
factors such as poorly-modeled lexical representa-
tion and context tracking as sources for system in-
terpretations that users reject.
We intend to take up the methodological chal-
lenges necessary to construct such an argument in
future work. In the meantime, while our current im-
plementation of FIGLET exhibits the promising be-
havior discussed in this paper and illustrated in Fig-
ures 1?4, some minor engineering unrelated to lan-
guage understanding remains before a fruitful eval-
uation can take place. As alluded to above, the tight
integration of contextual reasoning and interpreta-
tion that FIGLET carries out can be expensive if not
pursued efficiently. While our initial implementa-
tion achieves a level of performance that we accept
as researchers (interpretation times of between one
and a few tens of seconds), evaluation requires us to
improve FIGLET?s performance to levels that exper-
imental participants will accept as volunteers. Our
analysis of FIGLET indicates that this performance
can in fact be achieved with better-regimented do-
main problem-solving.
Nevertheless, we emphasize the empirical and
computational arguments we already have in sup-
port of our model. Our close links with the linguis-
tic literature mean that major empirical errors would
be surprising and important across the language sci-
ences. Indeed, limited evaluations of treatments of
vague definite reference using standards of differ-
entiation or very similar ideas have been promising
(Gorniak and Roy, In Press). The computational ap-
peal is that all the expensive infrastructure required
to pursue the account is independently necessary.
Once this infrastructure is in place the account is
readily implemented with small penalty of perfor-
mance and development time. It is particularly at-
tractive that the approach requires minimal lexical
knowledge and training data. This means adding
new vague words to an interface is a snap.
Overall, our new model offers three contribu-
tions. Most importantly, of course, we have devel-
oped a computational model of vagueness in terms
of underspecified quantitative constraints. But we
have also presented a new demonstration of the im-
portance and the feasibility of using multidimen-
sional representations of dialogue context in under-
standing descriptions of quantitative domains. And
we have introduced an architecture for resolving un-
derspecification through uniform pragmatic mech-
anisms based on context-dependent collaboration.
Together, these developments allow us to circum-
scribe possible resolutions for underspecified utter-
ances, to zero in on those that the speaker and hearer
could adopt consistently and collaboratively, and
so to constrain the speaker?s intended meaning to
within a natural range.
Acknowledgments
We thank Kees van Deemter and our anonymous re-
viewers for valuable comments. This work was sup-
ported by NSF grant HLC 0308121.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2001. Towards conver-
sational human-computer interaction. AI Maga-
zine, 22(4):27?37.
N. Asher and A. Lascarides. 2003. Logics of Con-
versation. Cambridge.
C. Barker. 2002. The dynamics of vagueness. Lin-
guistics and Philosophy, 25(1):1?36.
J. Bos. 2003. Implementing the binding and
accommodation theory for anaphora resolution
and presupposition. Computational Linguistics,
29(2):179?210.
H. Bunt. 2000. Dialogue pragmatics and context
specification. In H. Bunt and W. Black, editors,
Abduction, Belief and Context in Dialogue, pages
81?150. Benjamin.
M. Cresswell. 1977. The semantics of degree. In
B. H. Partee, editor, Montague Grammar, pages
261?292. Academic.
M. Dalrymple, M. Kanazawa, Y. Kim, S. Mchombo,
and S. Peters. 1998. Reciprocal expressions and
the concept of reciprocity. Linguistics and Phi-
losophy, 21(2):159?210.
D. DeVault and M. Stone. 2003. Domain inference
in incremental interpretation. In Proc. ICoS.
B. Di Eugenio and B. Webber. 1996. Pragmatic
overloading in natural language instructions. Int.
Journal of Expert Systems, 9(2):53?84.
J. Ginzburg and R. Cooper. 2001. Resolving ellip-
sis in clarification. In Proc. ACL.
P. Gorniak and D. Roy. In Press. Grounded seman-
tic composition for visual scenes. Journal of Ar-
tificial Intelligence Research.
D. Graff. 2000. Shifting sands: An interest-
relative theory of vagueness. Philosophical Top-
ics, 28(1):45?81.
J. Hobbs, M. Stickel, D. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial In-
telligence, 63:69?142.
J. Jaffar and J.-L. Lassez. 1987. Constraint logic
programming. In Proc. POPL, pages 111?119.
C. Kennedy. 1999. Projecting the adjective: The
syntax and semantics of gradability and compar-
ison. Garland.
C. Kennedy. 2003. Towards a grammar of vague-
ness. Manuscript, Northwestern.
A. Kyburg and M. Morreau. 2000. Fitting words:
Vague words in context. Linguistics and Philos-
ophy, 23(6):577?597.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6:323?340.
T. Oates, M. D. Schmill, and P. R. Cohen. 2000.
Toward natural language interfaces for robotic
agents. In Proc. Agents, pages 227?228.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse the-
ory to human-computer interaction. AI Maga-
zine, 22(4):15?26.
D. Roy and A. Pentland. 2002. Learning words
from sights and sounds: A computational model.
Cognitive Science, 26(1):113?146.
W. Schuler. 2001. Computational properties of
environment-based disambiguation. In Proc.
ACL, pages 466?473.
M. Stone and R. H. Thomason. 2003. Coordinat-
ing understanding and generation in an abductive
approach to interpretation. In Proc. DiaBruck,
pages 131?138.
M. Stone. 2003. Knowledge representation for lan-
guage engineering. In A. Farghaly, editor, A
Handbook for Language Engineers, pages 299?
366. CSLI.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 1?4, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Information-State Approach to Collaborative Reference
David DeVault1 Natalia Kariaeva2 Anubha Kothari2 Iris Oved3 and Matthew Stone1
1Computer Science 2Linguistics 3Philosophy and Center for Cognitive Science
Rutgers University
Piscataway NJ 08845-8020
Firstname.Lastname@Rutgers.Edu
Abstract
We describe a dialogue system that works
with its interlocutor to identify objects.
Our contributions include a concise, mod-
ular architecture with reversible pro-
cesses of understanding and generation,
an information-state model of reference,
and flexible links between semantics and
collaborative problem solving.
1 Introduction
People work together to make sure they understand
one another. For example, when identifying an ob-
ject, speakers are prepared to give many alternative
descriptions, and listeners not only show whether
they understand each description but often help the
speaker find one they do understand (Clark and
Wilkes-Gibbs, 1986). This natural collaboration is
part of what makes human communication so robust
to failure. We aim both to explain this ability and to
reproduce it.
In this paper, we present a novel model of collab-
oration in referential linguistic communication, and
we describe and illustrate its implementation. As we
argue in Section 2, our approach is unique in com-
bining a concise abstraction of the dynamics of joint
activity with a reversible grammar-driven model of
referential language. In the new information-state
model of reference we present in Section 3, inter-
locutors work together over multiple turns to asso-
ciate an entity with an agreed set of concepts that
characterize it. On our approach, utterance planning
and understanding involves reasoning about how
domain-independent linguistic forms can be used
in context to contribute to the task; see Section 4.
Our system reduces to four modules: understanding,
update, deliberation and generation, together with
some supporting infrastructure; see Section 5. This
design derives the efficiency and flexibility of refer-
ential communication from carefully-designed rep-
resentation and reasoning in this simple architecture;
see Section 6. With this proof-of-concept implemen-
tation, then, we provide a jumping-off point for more
detailed investigation of knowledge and processes in
conversation.
2 Overview and Related Work
Our demonstration system plays a referential com-
munication game, much like the one that pairs of
human subjects play in the experiments of Clark and
Wilkes-Gibbs (1986). We describe each episode in
this game as an activity involving the coordinated
action of two participants: a director D who knows
the referent R of a target variable T and a matcher
M whose task is to identify R. Our system can play
either role, D or M, using virtual objects in a graph-
ical display as candidate targets and distractors, and
using text as its input and output. Our system uses
the same task knowledge and the same grammar
whichever role it plays. Of course, the system also
draws on private knowledge to decide how best to
carry out its role; for now it describes objects using
the domain-specific iteration proposed by Dale and
Reiter (1995). The knowledge we have formalized is
targeted to a proof-of-concept implementation, but
we see no methodological obstacle in adding to the
1
system?s resources.
We exemplify what our system does in (1).
(1) a. S: This one is a square.
b. U: Um-hm...
c. S: It?s light brown.
d. U: You mean like tan?
e. S: Yeah.
f. S: It?s solid.
g. U: Got it.
The system (S) and user (U) exchange seven utter-
ances in the course of identifying a tan solid square.
We achieve this interaction using the information-
state approach to dialogue system design (Larsson
and Traum, 2000). This approach describes dialogue
as a coordinated effort to maintain an agreed record
of the state of the conversation. Our model contrasts
with traditional plan-based models, as exemplified
by Heeman and Hirst?s model of goals and beliefs
in collaborative reference (1995). Our approach ab-
stracts away from such details of individuals? men-
tal states and cognitive processes, for principled rea-
sons (Stone, 2004a). We are able to capture these
details implicitly in the dynamics of conversation,
whereas plan-based models must represent them ex-
plicitly. Our representations are simpler than Hee-
man and Hirst?s but support more flexible dialogue.
For example, their approach to (1) would have in-
terlocutors coordinating on goals and beliefs about
a syntactic representation for the tan solid square;
for us, this description and the interlocutors? com-
mitment to it are abstract results of the underlying
collaborative activity.
Another important antecedent to our work is
Purver?s (2004) characterization of clarification of
names for objects and properties. We extend this
work to develop a treatment of referential descriptive
clarification. When we describe things, our descrip-
tions grow incrementally and can specify as much
detail as needed. Clarification becomes correspond-
ingly cumulative and open-ended. Our revised in-
formation state includes a model of cumulative and
open-ended collaborative activity, similar to that ad-
vocated by Rich et al (2001). We also benefit from
a reversible goal-directed perspective on descriptive
language (Stone et al, 2003).
3 Information State
Our information state (IS) models the ongoing col-
laboration using a stack of tasks. For a task of col-
laborative reference, the IS tracks how interlocutors
together set up and solve a constraint-satisfaction
problem to identify a target object. In any state, D
and M have agreed on a target variable T and a set of
constraints that the value of T must satisfy. When M
recognizes that these constraints identify R, the task
ends successfully. Until then, D can take actions
that contribute new constraints on R. Importantly,
what D says adds to what is already known about R,
so that the identification of R can be accomplished
across multiple sentences with heterogeneous syn-
tactic structure.
Our IS also allows subtasks of questioning or clar-
ification that interlocutors can use to maintain align-
ment. The same constraint-satisfaction model is
used not only for referring to displayed objects but
also for referring to abstract entities, such as actions
or properties. Our IS tracks the salience of entity
and property referents and, like Purver?s, maintains
the previous utterance for reference in clarification
questions. Note, however, that we do not factor
updates to the IS through an abstract taxonomy of
speech acts. Instead, utterances directly make do-
main moves, such as adding a constraint, so our ar-
chitecture allows utterances to trigger an open-ended
range of domain-specific updates.
4 Linguistic Representations
The way utterances signal task contributions is
through a collection of presupposed constraints. To
understand an utterance, we solve the utterance?s
grammatically-specified semantic constraints. An
interpretation is only feasible if it represents a
contextually-appropriate contribution to the ongoing
task. Symmetrically, to generate an utterance, we
use the grammar to formulate a set of constraints;
these constraints must identify the contribution the
system intends to make. We view interpreted lin-
guistic structures as representing communicative in-
tentions; see (Stone et al, 2003) or (Stone, 2004b).
As in (DeVault et al, 2004), a knowledge in-
terface mediates between domain-general meanings
and the domain-specific ontology supported in a par-
ticular application. This allows us to build inter-
2
pretations using domain-specific representations for
referents, for task moves, and for the domain prop-
erties that characterize referents.
5 Architecture
Our system is implemented in Java. A set of in-
terface types describes the flow of information and
control through the architecture. The representation
and reasoning outlined in Sections 3 and 4 is ac-
complished by implementations of these interfaces
that realize our approach. Modules in the architec-
ture exchange messages about events and their in-
terpretations. (1) Deliberation responds to changes
in the IS by proposing task moves. (2) Generation
constructs collaborative intentions to accomplish the
planned task moves. (3) Understanding infers col-
laborative intentions behind user actions. Genera-
tion and understanding share code to construct inten-
tions for utterances, and both carry out a form of in-
ference to the best explanation. (4) Update advances
the IS symmetrically in response to intentions sig-
naled by the system or recognized from the user;
the symmetric architecture frees the designer from
programming complementary updates in a symmet-
rical way. Additional supporting infrastructure han-
dles the recognition of input actions, the realization
of output actions, and interfacing between domain
knowledge and linguistic resources.
Our system is designed not just for users to inter-
act with, but also for demonstrating and debugging
the system?s underlying models. Processing can be
paused at any point to allow inspection of the sys-
tem?s representations using a range of visualization
tools. You can interactively explore the IS, including
the present state of the world, the agreed direction
of the ongoing task, and the representation of lin-
guistic distinctions in salience and information sta-
tus. You can test the grammar and other interpretive
resources. And you can visualize the search space
for understanding and generation.
6 Example
Let us return to dialogue (1). Here the system rep-
resents its moves as successively constraining the
shape, color and pattern of the target object. In gen-
erating (1c), the system iteratively elaborates its de-
scription from brown to light brown in an attempt
to identify the object?s color unambiguously. The
user?s clarification request at (1d) marks this de-
scription of color as problematic and so triggers a
nested instance of the collaborative reference task.
At (1e) the system adds the user?s proposed con-
straint and (we assume) solves this nested subtask.
The system returns to the main task at (1f) having
grounded the color constraint and continues by iden-
tifying the pattern of the target object.
Let us explore utterance (1c) in more detail. The
IS records the status of the identification process.
The system is the director; the user is the matcher.
The target is represented provisionally by a dis-
course referent t1, and what has been agreed so far
is that the current target is a square of the rele-
vant sort for this task, represented in the agent as
square-figure-object(t1). In addition, the system has
privately recorded that square o1 is the referent it
must identify. For this IS, it is expected that the
director will propose an additional constraint iden-
tifying t1. The discourse state represents t1 as being
in-focus, or available for pronominal reference.
Deliberation now gives the generator a specific
move for the system to achieve:
(2) add-constraint(t1,color-sandybrown(t1))
The content of the move in (2) is that the system
should update the collaborative reference task to in-
clude the constraint that the target is drawn in a par-
ticular, domain-specific color (RGB value F4-A4-60,
or XHTML standard ?sandy brown?). The system
finds an utterance that achieves this by exploring
head-first derivations in its grammar; it arrives at the
derivation of it?s light brown in (3).
(3)
brown [present predicative adjective]



H
HH
HH
it [subject] light [color degree adverb]
A set of presuppositions connect this linguistic
structure to a task domain; they are given in (4a).
The relevant instances in this task are shown in (4b).
(4) a. predication(M)?brown(C)? light(C)
b. predication(add-constraint)?
brown(color-sandybrown)?
light(color-sandybrown)
3
The utterance also uses it to describe a referent
X so presupposes that in-focus(X) holds. The
move effected by the utterance is schematized as
M(X ,C(X)). Given the range of possible task moves
in the current context, the constraints specified by
the grammar for (3) are modeled as determining the
instantiation in (2). The system realizes the utter-
ance and assumes, provisionally, that the utterance
achieves its intended effect and records the new con-
straint on t1.
Because the generation process incorporates en-
tirely declarative reasoning, it is normally reversible.
Normally, the interlocutor would be able to identify
the speaker?s intended derivation, associate it with
the same semantic constraints, resolve those con-
straints to the intended instances, and thereby dis-
cover the intended task move. In our example, this
is not what happens. Recognition of the user?s clari-
fication request is triggered as in (Purver, 2004). The
system fails to interpret utterance (1d) as an appro-
priate move in the main reference task. As an alter-
native, the system ?downdates? the context to record
the fact that the system?s intended move may be the
subject of explicit grounding. This involves push-
ing a new collaborative reference task on the stack
of ongoing activities. The system remains the direc-
tor, the new target is the variable C in interpretation
and the referent to be identified is the property color-
sandybrown. Interpretation of (1d) now succeeds.
7 Discussion
Our work bridges research on collaborative dialogue
in AI (Rich et al, 2001) and research on pragmat-
ics in computational linguistics (Stone et al, 2003).
The two traditions have a lot to gain from reconcil-
ing their assumptions, if as Clark (1996) suggests,
people?s language use is coextensive with their joint
activity. There are implications both ways.
For pragmatics, our model suggests that language
use requires collaboration in part because reaching
agreement about content involves substantive social
knowledge and coordination. Indeed, we suspect
that collaborative reference is only one of many rel-
evant social processes. For collaborative dialogue
systems, adopting rich declarative linguistic repre-
sentations enables us to directly interface the core
modules of a collaborative system with one another.
In language understanding, for example, we can col-
lapse together notional subprocesses like semantic
reconstruction, reference resolution, and intention
recognition and solve them in a uniform way.
Our declarative, reversible approach supports an
analysis of how the system?s specifications drive its
input-output behavior. The architecture of this sys-
tem thus provides the groundwork for further in-
vestigations into the interaction of social, linguis-
tic, cognitive and even perceptual and developmen-
tal processes in meaningful communication.
Acknowledgements
Supported in part by NSF HLC 0308121. Thanks to
Paul Tepper.
References
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
H. H. Clark. 1996. Using Language. Cambridge.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 18:233?263.
D. DeVault, C. Rich, and C. L. Sidner. 2004. Natural
language generation and discourse context: Comput-
ing distractor sets from the focus stack. In FLAIRS.
P. Heeman and G. Hirst. 1995. Collaborating on refer-
ring expressions. Comp. Ling., 21(3):351?382.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Eng., 6:323?340.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, Univ. of London.
C. Rich, C. L. Sidner, and N. Lesh. 2001. COL-
LAGEN: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22:15?25.
M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer.
2003. Microplanning with communicative intentions.
Comp. Intelligence, 19(4):311?381.
M. Stone. 2004a. Communicative intentions and conver-
sational processes. In J. Trueswell and M. K. Tanen-
haus, editors, Approaches to Studying World-Situated
Language Use, pages 39?70. MIT.
M. Stone. 2004b. Intention, interpretation and the com-
putational structure of language. Cognitive Science,
28(5):781?809.
4
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interpretation of Partial Utterances in Virtual Human Dialogue Systems
Kenji Sagae and David DeVault and David R. Traum
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
{sagae,devault,traum}@ict.usc.edu
Abstract
Dialogue systems typically follow a rigid pace
of interaction where the system waits until the
user has finished speaking before producing
a response. Interpreting user utterances be-
fore they are completed allows a system to
display more sophisticated conversational be-
havior, such as rapid turn-taking and appropri-
ate use of backchannels and interruptions. We
demonstrate a natural language understanding
approach for partial utterances, and its use in a
virtual human dialogue system that can often
complete a user?s utterances in real time.
1 Introduction
In a typical spoken dialogue system pipeline, the
results of automatic speech recognition (ASR) for
each user utterance are sent to modules that per-
form natural language understanding (NLU) and di-
alogue management only after the utterance is com-
plete. This results in a rigid and often unnatural pac-
ing where the system must wait until the user stops
speaking before trying to understand and react to
user input. To achieve more flexible turn-taking with
human users, for whom turn-taking and feedback at
the sub-utterance level is natural, the system needs
the ability to start interpretation of user utterances
before they are completed.
We demonstrate an implementation of techniques
we have developed for partial utterance understand-
ing in virtual human dialogue systems (Sagae et al,
2009; DeVault et al, 2009) with the goal of equip-
ping these systems with sophisticated conversational
behavior, such as interruptions and non-verbal feed-
back. Our demonstration highlights the understand-
ing of utterances before they are finished. It also
includes an utterance completion capability, where a
virtual human can make a strategic decision to dis-
play its understanding of an unfinished user utter-
ance by completing the utterance itself.
The work we demonstrate here is part of a grow-
ing research area in which new technical approaches
to incremental utterance processing are being de-
veloped (e.g. Schuler et al (2009), Kruijff et al
(2007)), new possible metrics for evaluating the per-
formance of incremental processing are being pro-
posed (e.g. Schlangen et al (2009)), and the ad-
vantages for dialogue system performance and us-
ability are starting to be empirically quantified (e.g.
Skantze and Schlangen (2009), Aist et al (2007)).
2 NLU for partial utterances
In previous work (Sagae et al, 2009), we presented
an approach for prediction of semantic content from
partial speech recognition hypotheses, looking at
length of the speech hypothesis as a general indi-
cator of semantic accuracy in understanding. In
subsequent work (DeVault et al, 2009), we incor-
porated additional features of real-time incremen-
tal interpretation to develop a more nuanced predic-
tion model that can accurately identify moments of
maximal understanding within individual spoken ut-
terances. This research was conducted in the con-
text of the SASO-EN virtual human dialogue sys-
tem (Traum et al, 2008), using a corpus of approxi-
mately 4,500 utterances from user sessions. The cor-
pus includes a recording of each original utterance, a
33
??
?
?
?
?
?
?
?
?
mood : declarative
sem :
?
?
?
?
?
?
?
?
type : event
agent : captain? kirk
event : deliver
theme : power ? generator
modal :
[
possibility : can
]
speech? act :
[
type : offer
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: AVM utterance representation.
manual transcription, and a gold-standard semantic
frame, allowing us to develop and evaluate a data-
driven NLU approach.
2.1 NLU in SASO-EN Virtual Humans
Our NLU module for the SASO-EN system,
mxNLU (Sagae et al, 2009), is based on maxi-
mum entropy classification (Berger et al, 1996) ,
where we treat entire individual semantic frames as
classes, and extract input features from ASR. The
NLU output representation is an attribute-value ma-
trix (AVM), where the attributes and values repre-
sent semantic information that is linked to a domain-
specific ontology and task model (Figure 1). The
AVMs are linearized, using a path-value notation, as
seen in the NLU input-output example below:
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.type event
<s>.sem.agent captain-kirk
<s>.sem.event deliver
<s>.sem.theme power-generator
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
When mxNLU is trained on complete ASR out-
put for approximately 3,500 utterances, and tested
on a separate set of 350 complete ASR utterances,
the F-score of attribute-value pairs produced by the
NLU is 0.76 (0.78 precision and 0.74 recall). These
figures reflect the use of ASR at run-time, and most
errors are caused by incorrect speech recognition.
2.2 NLU with partial ASR results (Sagae et al,
2009)
To interpret utterances before they are complete,
we use partial recognition hypotheses produced by
ASR every 200 milliseconds while the user is speak-
ing. To process these partial utterances produced by
ASR, we train length-specific models for mxNLU.
These models are trained using the partial ASR re-
sults we obtain by running ASR on the audio corre-
sponding to the utterances in the training data. The
NLU task is then to predict the meaning of the en-
tire utterance based only on a (noisy) prefix of the
utterance. On average, the accuracy of mxNLU on a
six-word prefix of an utterance (0.74 F-score) is al-
most as the same as the accuracy of mxNLU on en-
tire utterances. Approximately half of the utterances
in our corpus contain more than six words, creating
interesting opportunities for conversational behavior
that would be impossible under a model where each
utterance must be completed before it is interpreted.
2.3 Detecting points of maximal
understanding (DeVault et al, 2009)
Although length-specific NLU models produce ac-
curate results on average, more effective use of the
interpretation provided by these models might be
achieved if we could automatically gauge their per-
formance on individual utterances at run-time. To
that end, we have developed an approach (DeVault et
al., 2009) that aims to detect those strategic points in
time, as specific utterances are occurring, when the
system reaches maximal understanding of the utter-
ance, in the sense that its interpretation will not sig-
nificantly improve during the rest of the utterance.
Figure 2 illustrates the incremental output of
mxNLU as a user asks, elder do you agree to move
the clinic downtown? Our ASR processes captured
audio in 200ms chunks. The figure shows the par-
tial ASR results after the ASR has processed each
200ms of audio, along with the F-score achieved by
mxNLU on each of these partials. Note that the NLU
F-score fluctuates somewhat as the ASR revises its
incremental hypotheses about the user utterance, but
generally increases over time.
For the purpose of initiating an overlapping re-
sponse to a user utterance such as this one, the agent
needs to be able (in the right circumstances) to make
34
Utterance time (ms)
NL
U F
?sc
ore
(emp
ty)
(emp
ty)
 
all 
 
elde
r 
 
elde
r do
 you
 
 
elde
r to
 you
 d 
 
elde
r do
 you
 agr
ee 
 
elde
r do
 you
 agr
ee t
o 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic to
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
 
elde
r do
 you
 agr
ee t
o m
ove
 the
 clin
ic d
own
tow
n 
200 400 600 800 100
0
120
0
140
0
160
0
180
0
200
0
220
0
240
0
260
0
280
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Partial ASR result
Figure 2: Incremental interpretation of a user utterance.
an assessment that it has already understood the ut-
terance ?well enough?, based on the partial ASR re-
sults that are currently available. We have imple-
mented a specific approach to this assessment which
views an utterance as understood ?well enough? if
the agent would not understand the utterance any
better than it currently does even if it were to wait
for the user to finish their utterance (and for the ASR
to finish interpreting the complete utterance).
Concretely, Figure 2 shows that after the entire
2800ms utterance has been processed by the ASR,
mxNLU achieves an F-score of 0.91. However, in
fact, mxNLU already achieves this maximal F-score
at the moment it interprets the partial ASR result el-
der do you agree to move the at 1800ms. The agent
therefore could, in principle, initiate an overlapping
response at 1800ms without sacrificing any accuracy
in its understanding of the user?s utterance.
Of course the agent does not automatically realize
that it has achieved a maximal F-score at 1800ms.
To enable the agent to make this assessment, we
have trained a classifier, which we call MAXF, that
can be invoked for any specific partial ASR result,
and which uses various features of the ASR result
and the current mxNLU output to estimate whether
the NLU F-score for the current partial ASR result
is at least as high as the mxNLU F-score would be if
the agent were to wait for the entire utterance.
To facilitate training of a MAXF classifier, we
identified a range of potentially useful features that
the agent could use at run-time to assess its confi-
dence in mxNLU?s output for a given partial ASR
result. These features include: the number of par-
tial results that have been received from the ASR;
the length (in words) of the current partial ASR
result; the entropy in the probability distribution
mxNLU assigns to alternative output frames (lower
entropy corresponds to a more focused distribution);
the probability mxNLU assigns to the most probable
output frame; and the most probable output frame.
Based on these features, we trained a decision tree
to make the binary prediction that MAXF is TRUE
or FALSE for each partial ASR result. DeVault et al
(2009) include a detailed evaluation and discussion
of the classifier. To briefly summarize our results,
the precision/recall/F-score of the trained MAXF
model are 0.88/0.52/0.65 respectively. The high pre-
cision means that 88% of the time that the model
predicts that F-score is maximized at a specific par-
tial, it really is. Our demonstration, which we out-
line in the next section, highlights the utility of a
high-precision MAXF classifier in making the deci-
sion whether to complete a user?s utterance.
3 Demo script outline
We have implemented the approach for partial utter-
ance understanding described above in the SASO-
EN system (Traum et al, 2008), a virtual human
dialogue system with speech input and output (Fig-
ure 3), allowing us to demonstrate both partial utter-
ance understanding and some of the specific behav-
iors made possible by this capability. We divide this
demonstration in two parts: visualization of NLU
for partial utterances and user utterance completion.
35
Figure 3: SASO-EN: Dr. Perez and Elder al-Hassan.
Partial ASR result Predicted completion
we can provide transportation to move the patient there
the market is not safe
there are supplies where we are going
Table 1: Examples of user utterance completions.
3.1 Visualization of NLU for partial utterances
Because the demonstration depends on usage of the
system within the domain for which it was designed,
the demo operator provides a brief description of the
system, task and domain. The demo operator (or
a volunteer user) then speaks normally to the sys-
tem, while a separate window visualizes the sys-
tem?s evolving understanding. This display is up-
dated every 200 milliseconds, allowing attendees to
see partial utterance understanding in action. For
ease of comprehension, the display will summarize
the NLU state using an English paraphrase of the
predicted meaning (rather than displaying the struc-
tured frame that is the actual output of NLU). The
display will also visualize the TRUE or FALSE state
of the MAXF classifier, highlighting the moment the
system thinks it reaches maximal understanding.
3.2 User utterance completion
The demo operator (or volunteer user) starts to speak
and pauses briefly in mid-utterance, at which point,
if possible, one of the virtual humans jumps in and
completes the utterance (DeVault et al, 2009). Ta-
ble 1 includes a few examples of the many utterances
that can be completed by the virtual humans.
4 Conclusion
Interpretation of partial utterances, combined with
a way to predict points of maximal understanding,
opens exciting possibilities for more natural conver-
sational behavior in virtual humans. This demon-
stration showcases the NLU approach and a sample
application of the basic techniques.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
D. DeVault, K. Sagae, and D. Traum. 2009. Can I finish?
Learning when to respond to incremental interpreta-
tion results in interactive dialogue. In Proc. SIGDIAL.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Proc. LangRo?2007.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Short Paper Proceedings of NAACL HLT.
D. Schlangen, T. Baumann, and M. Atterer. 2009. In-
cremental reference resolution: The task, metrics for
evaluation, and a Bayesian filtering model that is sen-
sitive to disfluencies. In Proc. SIGDIAL, page 30?37.
W. Schuler, S. Wu, and L. Schwartz. 2009. A frame-
work for fast incremental interpretation during speech
decoding. Computational Linguistics, 35(3):313?343.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. EACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negoti-
ation for multi-modal virtual agents. In Proc. of the
Eighth International Conference on Intelligent Virtual
Agents.
36
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 25?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Incremental Speech Understanding in a Multi-Party Virtual Human
Dialogue System
David DeVault and David Traum
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{devault,traum}@ict.usc.edu
1 Extended Abstract
This demonstration highlights some emerging ca-
pabilities for incremental speech understanding and
processing in virtual human dialogue systems. This
work is part of an ongoing effort that aims to en-
able realistic spoken dialogue with virtual humans in
multi-party negotiation scenarios (Plu?ss et al, 2011;
Traum et al, 2008b). These scenarios are designed
to allow trainees to practice their negotiation skills
by engaging in face-to-face spoken negotiation with
one or more virtual humans.
An important component in achieving naturalistic
behavior in these negotiation scenarios, which ide-
ally should have the virtual humans demonstrating
fluid turn-taking, complex reasoning, and respond-
ing to factors like trust and emotions, is for the vir-
tual humans to begin to understand and in some
cases respond in real time to users? speech, as the
users are speaking (DeVault et al, 2011b). These re-
sponses could range from relatively straightforward
turn management behaviors, like having a virtual hu-
man recognize when it is being addressed by a user
utterance, and possibly turn to look at the user who
has started speaking, to more complex responses
such as emotional reactions to the content of what
users are saying.
The current demonstration extends our previous
demonstration of incremental processing (Sagae et
al., 2010) in several important respects. First, it
includes additional indicators, as described in (De-
Vault et al, 2011a). Second, it is applied to a new
domain, an extension of that presented in (Plu?ss et
al., 2011). Finally, it is integrated with the dialogue
Figure 1: SASO negotiation in the saloon: Utah (left)
looking at Harmony (right).
models (Traum et al, 2008a), such that each par-
tial interpretation is given a full pragmatic interpre-
tation by each virtual character, which can be used
to generate real-time incremental non-verbal feed-
back (Wang et al, 2011).
Our demonstration is set in an implemented multi-
party negotiation domain (Plu?ss et al, 2011) in
which two virtual humans, Utah and Harmony (pic-
tured in Figure 1), talk with two human negotiation
trainees, who play the roles of Ranger and Deputy.
The dialogue takes place inside a saloon in an Amer-
ican town in the Old West. In this negotiation sce-
nario, the goal of the two human role players is to
convince Utah and Harmony that Utah, who is cur-
rently employed as the local bartender, should take
on the job of town sheriff.
One of the research aims for this work is to
support natural dialogue interaction, an example of
which is the excerpt of human role play dialogue
shown in Figure 2. One of the key features of immer-
sive role plays is that people often react in multiple
ways to the utterances of others as they are speaking.
For example, in this excerpt, the beginning of the
25
Ranger We can?t leave this place and have it overrun by outlaws.
Uh there?s no way that?s gonna happen so we?re gonna
make sure we?ve got a properly deputized and equipped
sheriff ready to maintain order in this area.
00:03:56.660 - 00:04:08.830
Deputy Yeah and you know and and we?re willing to
00:04:06.370 - 00:04:09.850
Utah And I don?t have to leave the bar completely. I can still
uh be here part time and I can um we can hire someone to
do the like day to day work and I?ll do the I?ll supervise
them and I?ll teach them.
00:04:09.090 - 00:04:22.880
Figure 2: Dialogue excerpt from one of the role plays.
Timestamps indicate the start and end of each utterance.
Deputy?s utterance overlaps the end of the Ranger?s,
and then Utah interrupts the Deputy and takes the
floor a few seconds later.
Our prediction approach to incremental speech
understanding utilizes a corpus of in-domain spo-
ken utterances, including both paraphrases selected
and spoken by system developers, as well as spo-
ken utterances from user testing sessions (DeVault
et al, 2011b). An example of a corpus element is
shown in Figure 3. In previous negotiation domains,
we have found a fairly high word error rate in au-
tomatic speech recognition results for such sponta-
neous multi-party dialogue data; for example, our
average word error rate was 0.39 in the SASO-EN
negotiation domain (Traum et al, 2008b) with many
(15%) out of domain utterances. Our speech un-
derstanding framework is robust to these kinds of
problems (DeVault et al, 2011b), partly through
approximating the meaning of utterances. Utter-
ance meanings are represented using an attribute-
value matrix (AVM), where the attributes and val-
ues represent semantic information that is linked to
a domain-specific ontology and task model (Traum,
2003; Hartholt et al, 2008; Plu?ss et al, 2011). The
AVMs are linearized, using a path-value notation, as
seen in Figure 3. In our framework, we use this data
to train two data-driven models, one for incremen-
tal natural language understanding, and a second for
incremental confidence modeling.
The first step is to train a predictive incremental
understanding model. This model is based on maxi-
mum entropy classification, and treats entire individ-
ual frames as output classes, with input features ex-
tracted from partial ASR results, calculated in incre-
ments of 200 milliseconds (DeVault et al, 2011b).
? Utterance (speech): i?ve come here today to talk to you
about whether you?d like to become the sheriff of this town
? ASR (NLU input): have come here today to talk to you
about would the like to become the sheriff of this town
? Frame (NLU output):
<S>.mood interrogative
<S>.sem.modal.desire want
<S>.sem.prop.agent utah
<S>.sem.prop.event providePublicServices
<S>.sem.prop.location town
<S>.sem.prop.theme sheriff-job
<S>.sem.prop.type event
<S>.sem.q-slot polarity
<S>.sem.speechact.type info-req
<S>.sem.type question
Figure 3: Example of a corpus training example.
Each partial ASR result then serves as an incremen-
tal input to NLU, which is specially trained for par-
tial input as discussed in (Sagae et al, 2009). NLU
is predictive in the sense that, for each partial ASR
result, the NLU module produces as output the com-
plete frame that has been associated by a human an-
notator with the user?s complete utterance, even if
that utterance has not yet been fully processed by
the ASR. For a detailed analysis of the performance
of the predictive NLU, see (DeVault et al, 2011b).
The second step in our framework is to train a set
of incremental confidence models (DeVault et al,
2011a), which allow the agents to assess in real time,
while a user is speaking, how well the understand-
ing process is proceeding. The incremental confi-
dence models build on the notion of NLU F-score,
which we use to quantify the quality of a predicted
NLU frame in relation to the hand-annotated correct
frame. The NLU F-score is the harmonic mean of
the precision and recall of the attribute-value pairs
(or frame elements) that compose the predicted and
correct frames for each partial ASR result. By using
precision and recall of frame elements, rather than
simply looking at frame accuracy, we take into ac-
count that certain frames are more similar than oth-
ers, and allow for cases when the correct frame is
not in the training set.
Each of our incremental confidence models
makes a binary prediction for each partial NLU re-
sult as an utterance proceeds. At each time t dur-
26
Figure 4: Visualization of Incremental Speech Processing.
ing an utterance, we consider the current NLU F-
Score Ft as well as the final NLU F-Score Ffinal
that will be achieved at the conclusion of the ut-
terance. In (DeVault et al, 2009) and (DeVault
et al, 2011a), we explored the use of data-driven
decision tree classifiers to make predictions about
these values, for example whether Ft ? 12 (cur-
rent level of understanding is ?high?), Ft ? Ffinal
(current level of understanding will not improve),
or Ffinal ? 12 (final level of understanding will be
?high?). In this demonstration, we focus on the
first and third of these incremental confidence met-
rics, which we summarize as ?Now Understanding?
and ?Will Understand?, respectively. In an evalua-
tion over all partial ASR results for 990 utterances
in this new scenario, we found the Now Under-
standing model to have precision/recall/F-Score of
.92/.75/.82, and the Will Understand model to have
precision/recall/F-Score of .93/.85/.89. These incre-
mental confidence models therefore provide poten-
tially useful real-time information to Utah and Har-
mony about whether they are currently understand-
ing a user utterance, and whether they will ever un-
derstand a user utterance.
The incremental ASR, NLU, and confidence
models are passed to the dialogue managers for each
of the agents, Harmony and Utah. These agents then
relate these inputs to their own models of dialogue
context, plans, and emotions, to calculate pragmatic
interpretations, including speech acts, reference res-
olution, participant status, and how they feel about
what is being discussed. A subset of this informa-
tion is passed to the non-verbal behavior generation
module to produce incremental non-verbal listening
behaviors (Wang et al, 2011).
In support of this demonstration, we have ex-
tended the implementation to include a real-time vi-
sualization of incremental speech processing results,
which will allow attendees to track the virtual hu-
mans? understanding as an utterance progresses. An
example of this visualization is shown in Figure 4.
2 Demo script
The demonstration begins with the demo operator
providing a brief overview of the system design, ne-
gotiation scenario, and incremental processing capa-
bilities. The virtual humans Utah and Harmony (see
Figure 1) are running and ready to begin a dialogue
with the user, who will play the role of the Ranger.
As the user speaks to Utah or Harmony, attendees
can observe the real time visualization of speech
27
processing to observe changes in the incremental
processing results as the utterance progresses. Fur-
ther, the visualization interface enables the demo op-
erator to ?rewind? an utterance and step through the
incremental processing results that arrived each 200
milliseconds, highlighting how specific partial ASR
results can change the virtual humans? understand-
ing or confidence.
For example, Figure 4 shows the incremental
speech processing state at a moment 4.8 seconds into
a user?s 7.4 second long utterance, i?ve come here
today to talk to you about whether you?d like to be-
come the sheriff of this town. At this point in time,
the visualization shows (at top left) that the virtual
humans are confident that they are both Now Under-
standing and Will Understand this utterance. Next,
the graph (in white) shows the history of the agents?
expected NLU F-Score for this utterance (ranging
from 0 to 1). Beneath the graph, the partial ASR re-
sult (HAVE COME HERE TODAY TO TALK TO
YOU ABOUT...) is displayed (in white), along
with the currently predicted NLU frame (in blue).
For ease of comprehension, an English gloss (utah
do you want to be the sheriff?) for the NLU frame is
also shown (in blue) above the frame.
To the right, in pink, we show some of Utah and
Harmony?s agent state that is based on the current in-
cremental NLU results. The display shows that both
of the virtual humans believe that Utah is being ad-
dressed by this utterance, that utah has a positive at-
titude toward the content of the utterance while har-
mony does not, and that both have comprehension
and participation goals. Further, Harmony believes
she is a side participant at this moment. The demo
operator will explain and discuss this agent state in-
formation, including possible uses for this informa-
tion in response policies.
Acknowledgments
We thank all the members of the ICT Virtual Hu-
mans team. The project or effort described here
has been sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not
necessarily reflect the position or the policy of the
United States Government, and no official endorse-
ment should be inferred.
References
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL.
David DeVault, Kenji Sagae, and David Traum. 2011a.
Detecting the status of a predictive incremental speech
understanding model for real-time decision-making in
a spoken dialogue system. In Proceedings of Inter-
Speech.
David DeVault, Kenji Sagae, and David Traum. 2011b.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue & Dis-
course, 2(1).
Arno Hartholt, Thomas Russ, David Traum, Eduard
Hovy, and Susan Robinson. 2008. A common ground
for virtual humans: Using an ontology in a natural
language oriented virtual human architecture. In Pro-
ceedings of LREC, Marrakech, Morocco, may.
Brian Plu?ss, David DeVault, and David Traum. 2011.
Toward rapid development of multi-party virtual hu-
man negotiation scenarios. In Proceedings of SemDial
2011, the 15th Workshop on the Semantics and Prag-
matics of Dialogue.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language un-
derstanding of partial speech recognition results in dia-
logue systems. In Short Paper Proceedings of NAACL
HLT.
Kenji Sagae, David DeVault, and David R. Traum. 2010.
Interpretation of partial utterances in virtual human
dialogue systems. In Demonstration Proceedings of
NAACL-HLT.
D. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008a. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
David Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of IVA.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011.
Towards more comprehensive listening behavior: Be-
yond the bobble head. In Hannes Vilhjlmsson, Stefan
Kopp, Stacy Marsella, and Kristinn Thrisson, editors,
Intelligent Virtual Agents, volume 6895 of Lecture
Notes in Computer Science, pages 216?227. Springer
Berlin / Heidelberg.
28
Proceedings of NAACL-HLT 2013, pages 1092?1099,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A method for the approximation of incremental understanding of explicit
utterance meaning using predictive models in finite domains
David DeVault and David Traum
Institute for Creative Technologies, University of Southern California,
12015 Waterfront Dr., Playa Vista, CA 90094 USA
{devault,traum}@ict.usc.edu
Abstract
This paper explores the relationship between explicit
and predictive models of incremental speech under-
standing in a dialogue system that supports a finite
set of user utterance meanings. We present a method
that enables the approximation of explicit under-
standing using information implicit in a predictive
understanding model for the same domain. We show
promising performance for this method in a corpus
evaluation, and discuss its practical application and
annotation costs in relation to some alternative ap-
proaches.
1 Introduction
In recent years, there has been a growing interest among
researchers in methods for incremental natural language
understanding (NLU) for spoken dialogue systems; see
e.g. (Skantze and Schlangen, 2009; Sagae et al, 2009;
Schlangen et al, 2009; Heintze et al, 2010; DeVault et
al., 2011a; Selfridge et al, 2012). This work has gen-
erally been motivated by a desire to make dialogue sys-
tems more efficient and more natural, by enabling them to
provide lower latency responses (Skantze and Schlangen,
2009), human-like feedback such as backchannels that in-
dicate how well the system is understanding user speech
(DeVault et al, 2011b; Traum et al, 2012), and more in-
teractive response capabilities such as collaborative com-
pletions of user utterances (DeVault et al, 2011a), more
adaptive handling of interruptions (Buschmeier et al,
2012), and others.
This paper builds on techniques developed in previous
work that has adopted a predictive approach to incremen-
tal NLU (DeVault et al, 2011a). On this approach, at
specific moments while a user?s speech is in progress,
an attempt is made to predict what the full meaning of
the complete user utterance will be. Predictive models
can be contrasted with explicit approaches to incremen-
tal NLU. We use the term explicit understanding to refer
to approaches that attempt to determine the meaning that
has been expressed explicitly in the user?s partial utter-
ance so far (without predicting further aspects of mean-
ing to come). Explicit understanding of partial utterances
can be implemented using statistical classification or se-
quential tagging models (Heintze et al, 2010).
Both predictive and explicit incremental NLU capabil-
ities can be valuable in a dialogue system. Prediction
can support specific response capabilities, such as sys-
tem completion of user utterances (DeVault et al, 2011a)
and reduced response latency.1 However, explicit models
support additional and complementary capabilities. For
instance, depending on the application domain (Heintze
et al, 2010) and on the individual utterance (DeVault et
al., 2011b), it may be difficult for a system to predict a
user?s impending meaning with confidence. Neverthe-
less, it may often be possible for systems to determine
the meaning of what a user has said so far, and to take
action based on this partial understanding. As one exam-
ple, items in a user interface could be highlighted when
mentioned by a user (Bu? and Schlangen, 2011). An-
other capability would be to provide grounding feedback,
such as verbal back-channels or head nods (in embod-
ied systems), to indicate when the system is understand-
ing the user?s meaning (Traum et al, 2012). Explicit ut-
terance meanings also allow a system to distinguish be-
tween meaning that has been expressed and meaning that
is merely implied or inferred, which may be less reliable.
In the near future, as incremental processing capabilities
in dialogue systems grow, it may prove valuable for di-
alogue systems to combine both predictive and explicit
incremental understanding capabilities.
In this paper, we present a technique for approximating
a user?s explicit meaning using an existing predictive un-
derstanding framework (DeVault et al, 2011a). The spe-
cific new contributions in this paper are (1) to show that
1A simple approach to reducing response latency is to begin to plan
a response to the predicted meaning while the user is still speaking.
1092
an estimate of a user?s explicit utterance meaning can be
derived from this kind of predictive understanding model
(Section 2); (2) to quantify the performance of this new
method in a corpus evaluation (Section 3); (3) to provide
concrete examples and discussion of the annotation costs
associated with implementing this technique, in relation
to some alternative approaches to explicit understanding
(Section 4). Our results and discussion show that the
proposed method offers promising performance, has rela-
tively low annotation costs, and enables explicit and pre-
dictive understanding to be easily combined within a di-
alogue system. It may therefore be a useful incremental
understanding technique for some dialogue systems.
2 Technical Approach and Data Set
In Sections 2.1-2.3, we briefly summarize the data set and
approach to predictive incremental NLU (DeVault et al,
2011a) that serves as the starting point for the new work
in this paper. Sections 2.4 and 2.5 present our new ap-
proach to explicit understanding based on this approach.
2.1 Data set
For the experiments reported here, we use a corpus of
user utterances collected with the SASO-EN spoken dia-
logue system (Hartholt et al, 2008; Traum et al, 2008).
Briefly, this system is designed to allow a trainee to prac-
tice multi-party negotiation skills by engaging in face to
face negotiation with virtual humans. The scenario in-
volves a negotiation about the possible re-location of a
medical clinic in an Iraqi village. A human trainee plays
the role of a US Army captain, and there are two virtual
humans that he negotiates with: Doctor Perez, the head
of an NGO clinic, and a local village elder, al-Hassan.
The captain?s main objective is to convince the doctor and
the elder to move the clinic out of an unsafe marketplace
area.
The corpus used for the experiments in this paper in-
cludes 3,826 training and 449 testing utterances drawn
from user dialogues in this domain. The corpus and its se-
mantic annotation are described in (DeVault et al, 2010;
DeVault et al, 2011a). All user utterances have been au-
dio recorded, transcribed, and manually annotated with
the correct NLU output frame for the entire utterance.
(We discuss the cost of this annotation in Section 4.) Each
NLU output frame contains a set of attributes and values
that represent semantic information linked to a domain-
specific ontology and task model (Traum, 2003). Exam-
ples of the NLU output frames are included in Figures 2,
3, and 5.
2.2 Predictive incremental NLU
This approach uses a predictive incremental NLU mod-
ule, mxNLU (Sagae et al, 2009; DeVault et al, 2011a),
which is based on maximum entropy classification. The
approach treats entire individual frames as output classes,
and extracts input features from partial ASR results. To
define the incremental understanding problem, the audio
of the utterances in the training data were fed through
an ASR module, PocketSphinx (Huggins-Daines et al,
2006), in 200 millisecond chunks, and each partial ASR
result produced by the ASR was recorded. Each par-
tial ASR result then serves as an incremental input to
mxNLU. NLU is predictive in the sense that, for each
partial ASR result, the task of mxNLU is to produce as
output the complete frame that has been associated by a
human annotator with the user?s complete utterance, even
if that utterance has not yet been fully processed by the
ASR.
The human annotation defines a finite set S =
{S1, ..., SN} of possible NLU output frames, where each
frame Si = {e1, ..., en} is a set of key-value pairs or
frame elements. For notation, a user utterance u generally
creates a sequence of m partial ASR results ?r1, ..., rm?,
where each ASR result rj is a partial text such as we need
to move. Let Gu denote the correct (or ?gold?) frame for
the complete utterance u. For each result rj and for each
complete frame Si, the maximum entropy model pro-
vides P (Gu = Si|rj). The NLU output frame SNLUj is
the complete frame for which this probability is highest.
2.3 Performance of predictive incremental NLU
The performance of this predictive incremental NLU
framework has been evaluated using the training and
test portions of the SASO-EN data set described in Sec-
tion 2.1. Performance is quantified by looking at pre-
cision, recall, and F-score of the frame elements that
compose the predicted (SNLUj ) and correct (Gu) frames
for each partial ASR result. When evaluated over all
the 5,736 partial ASR results for the 449 test utterances,
the precision/recall/F-Score of this predictive NLU, in
relation to the complete frames, are 0.67/0.47/0.56, re-
spectively. When evaluated on only the ASR results
for complete test utterances, these scores increase to
0.81/0.71/0.76, respectively.
2.4 Assigning probability to frame elements
An interesting question is whether we can use this model
to attach useful probabilities not only to complete pre-
dicted frames but also to the individual frame elements
that make up those frames. To explore this, for each par-
tial ASR result rj in each utterance u, and for each frame
element e in SASO-EN, let us model the probability that
e will be part of the correct frame for the complete utter-
ance as:
P (e ? Gu|rj) =
?
Si:e?Si
P (Gu = Si|rj) (1)
1093
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Probability assigned to frame element
Re
lati
ve 
fre
que
ncy
 of 
fra
me
 ele
me
nt 
in c
orr
ect
 fra
me
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Re
lati
ve 
fre
que
ncy
 of 
pro
bab
ility
 be
ing
 as
sig
ned
 to 
a fr
am
e e
lem
en
tModel calibration (left axis)
Perfect calibration (left axis)
Relative frequency of
assigned probabilities(right axis)
Figure 1: Calibration of frame element probabilities.
This method derives the probability of frame elements
from the probabilities assigned to the possible frames that
contain them. Computing this sum is straightforward in a
finite semantic domain such as SASO-EN.
We computed this probability for all frame elements
e and all partial ASR results rj in our test set, yielding
approximately 478,000 probability values. We grouped
these probability values into bins of size 0.05, and cal-
culated the frequency with which the frame elements in
each bin were indeed present in the correct frame Gu for
the relevant utterance u. The results are presented in Fig-
ure 1, which shows that the probability values derived
from Equation (1) are relatively ?well calibrated?, in the
sense that the relative frequency with which a frame el-
ement is in the final frame is very close to the numeric
probability assigned by Equation (1). The figure also
shows how frequently the model assigns various proba-
bility ranges to frame elements (blue dotted line, plotted
against the secondary right axis). Note that most frame
elements are assigned very little probability for most par-
tial ASR results.
We conclude from these observations that the probabil-
ities assigned by (1) could indeed carry useful informa-
tion about the likelihood that individual key values will
be present in the complete utterance meaning.
2.5 Selecting probable frame elements
In exploring the model of frame element probabilities
given in Equation (1), we observed that often the reason
a frame element has lower probability, at a given point
within a user utterance, is that it is a prediction rather than
something that has been expressed explicitly. Building on
this observation, our technique for estimating the user?s
explicit meaning uses a probability threshold to select
those individual frame elements which are most likely to
be in the frame for a complete utterance, according to the
predictive model. That is, at each partial result rj , we
estimate the user?s explicit meaning using a constructed
frame:
SSUBj = {e|P (e ? Gu|rj) ? ?} (2)
This approximation could work well if, in practice, the
most probable frame elements prove to match fairly
closely the user?s non-incremental utterance meaning at
the point this frame is constructed. We evaluate this in
the next section.
Note that, in general, the returned subset of frame
elements may not be identical to any complete frame
Si ? S; rather it will correspond to parts of these com-
plete frames or ?subframes?.
3 Performance Evaluation
To evaluate this technique, we constructed subsets of
frame elements or ?explicit subframes? using Equation
(2) and various minimum probability thresholds ? for par-
tial ASR results in our test set. We then compared the
resulting subframes both to the final complete frame Gu
for each utterance u, and also to manually annotated sub-
1094
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: hello
0.813 <S>.sem.speechact.type greeting <S>.sem.speechact.type greeting
<S>.addressee doctor-perez
<S>.sem.speechact.type greeting
Partial ASR result: hello elder
0.945 <S>.sem.speechact.type greeting
0.934 <S>.addressee elder-al-hassan
<S>.sem.speechact.type greeting
<S>.addressee elder-al-hassan
<S>.sem.speechact.type greeting
<S>.addressee elder-al-hassan
Figure 2: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of hello elder.
frames that represent human judgments of explicit incre-
mental utterance meaning.
To collect these judgments, we hand-annotated a word-
meaning alignment for 50 random utterances in our test
set.2 To perform this annotation, successively larger pre-
fixes of each utterance transcript were mapped to succes-
sively larger subframes of the full frame for the complete
utterance. The annotated subframes for each utterance
prefix were selected to be explicit; they include only those
frame elements that are explicitly expressed in the corre-
sponding prefix of the user?s utterance. (We discuss the
cost of this annotation in Section 4.)
We provide a simple concrete example in Figure 2.
This example shows two partial ASR results during
an utterance of hello elder by a user. For each par-
tial ASR result, three frames are indicated horizon-
tally. At the right, labeled ?Annotated subframe?, we
show the human judgment of explicit incremental ut-
terance meaning for this partial utterance. Our hu-
man judge has indicated that the word hello corresponds
to the frame element <S>.sem.speechact.type
greeting, and that the words hello elder correspond
to an expanded frame that includes the frame element
<S>.addressee elder-al-hassan.
At the left, labeled ?Explicit subframe?, we show
the subframe selected by Equation (2) for each par-
tial ASR result, with threshold ? = 0.5. A relevant
background fact for this example is that in this sce-
nario, the user can generally address either of two vir-
tual humans who are present, Doctor Perez or Elder
Al-Hassan. After the user has said hello, the frame
element <S>.sem.speechact.type greeting is
assigned probability 0.813 by Equation (1), and only this
frame element appears in the explicit subframe.
In the middle, labeled ?Predicted complete frame?, the
figure also shows the full predicted frame from mxNLU
at each point. After the user has said hello, the full
predicted output includes an additional frame element,
<S>.addressee doctor-perez, indicating a pre-
diction that the addressee of this user utterance will be
Doctor Perez rather than Elder al-Hassan. However, the
2Note that no utterances in our training set were annotated.
probability assigned to this prediction by Equation (1) is
less than 0.5, and so this predicted frame element is ex-
cluded from the explicit subframe. And indeed, this is the
correct explicit representation of the meaning of hello in
this system.
This simple example illustrates how our proposed tech-
nique can enable a dialogue system to have access to both
explicit and predicted utterance meaning as a user?s ut-
terance progresses. An excerpt from a more complex
utterance is given in Figure 3. This example shows in-
cremental outputs for two partial ASR results during a
user utterance of we will provide transportation at no
cost. In this example, the explicit subframe for we
will includes frame elements that convey that the cap-
tain (i.e. the user) is promising to do something. This
subframe does not exactly match the human judgment
of explicit meaning at the right, which does not include
at this point the <S>.sem.agent captain-kirk
and <S>.sem.type event frame elements. How-
ever, the explicit subframe more closely matches the hu-
man judgment than does the predicted complete frame
from mxNLU (middle column), which includes an in-
correct prediction that the captain is promising to de-
liver medical supplies (represented by the key values
<S>.sem.event deliver and <S>.sem.theme
medical-supplies). For the next partial ASR re-
sult shown in the figure, the explicit subframe correctly
adds several additional frame elements which formalize
the meaning of the phrase provide transportation in this
scenario as having the army move the clinic out of the
market area.
To understand more quantitatively how well this tech-
nique works, we evaluated this technique in the SASO-
EN test corpus, using different probability thresholds in
the range [0.5,1.0). We present the results in Figure 4. To
understand the effect of the threshold ? , note that, in gen-
eral, the effect of selecting a higher threshold should be to
?cherry pick? those frame elements which are most likely
to appear in the complete frame Gu, thereby increasing
precision while decreasing recall of the frame elements in
SSUBj in relation to Gu. In the figure, we can see that this
is indeed the case. The lines marked ?(complete frame)?
1095
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: we will
0.856 <S>.mood declarative
0.824 <S>.sem.agent captain-kirk
0.663 <S>.sem.modal.intention will
0.663 <S>.sem.speechact.type promise
0.776 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event deliver
<S>.sem.modal.intention will
<S>.sem.speechact.type promise
<S>.sem.theme medical-supplies
<S>.sem.type event
<S>.mood declarative
<S>.sem.modal.intention will
<S>.sem.speechact.type promise
Partial ASR result: we will provide transportation
0.991 <S>.mood declarative
0.990 <S>.sem.agent captain-kirk
0.927 <S>.sem.event move
0.905 <S>.sem.instrument us-army
0.964 <S>.sem.modal.intention will
0.927 <S>.sem.source market
0.964 <S>.sem.speechact.type promise
0.928 <S>.sem.theme clinic
0.989 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.instrument us-army
<S>.sem.modal.intention will
<S>.sem.source market
<S>.sem.speechact.type promise
<S>.sem.theme clinic
<S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.instrument us-army
<S>.sem.modal.intention will
<S>.sem.source market
<S>.sem.speechact.type promise
<S>.sem.theme clinic
<S>.sem.type event
Figure 3: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we will provide
transportation at no cost.
0.5 0.6 0.7 0.8 0.9 1.0
0.0
0.2
0.4
0.6
0.8
1.0
threshold
o
o
o
Precision (complete frame)
Precision (annotated subframe)
Recall (complete frame)
Recall (annotated subframe)
F?Score (complete frame)
F?Score (annotated subframe)
Figure 4: The effect of threshold on precision, recall, and F-Score of explicit subframes. All scores are measured in relation to
complete utterance frames and annotated subframes.
1096
in the figure evaluate the returned subframes in relation
to the complete frameGu associated with the user?s com-
plete utterance. We see that this method enables us to
select subsets of frame elements that are most likely to
appear in Gu: by increasing the threshold, it is possible
to return subframes which are of increasingly higher pre-
cision in relation to the final frame Gu, but that also have
lower recall.
We also evaluated the returned subframes in relation to
the hand-annotated subframes, to assess its performance
at identifying the user?s explicit meaning. For an utter-
ance u that generates partial ASR results ?r1, ..., rm?,
we denote the hand-annotated subframe corresponding to
partial ASR result rj by GSUBj . In the lines marked ?(an-
notated subframe)?, we show the precision, recall, and
F-score of the explicit subframe for each ASR result rj
in relation to the annotated subframe GSUBj .
As a first observation, note that at any threshold level,
the explicit subframes do better at recalling the hand-
annotated subframe elements than they do at recalling the
complete frame elements. This means our new method is
better at recalling what has been said already by the user
than it is at predicting what will be said, as intended. We
have seen two examples of this already, for the partial
ASR result hello in Figure 2, and for the partial ASR re-
sult we will in Figure 3.
A second observation in Figure 4 is that precision re-
mains better against the complete utterance frame than
against the hand-annotated subframe (at all threshold lev-
els). This indicates that the explicit subframes are often
still predicting some aspects of the full frame. An exam-
ple of this is given in Figure 5, where the user?s partial
utterance we need to is assigned an explicit subframe that
includes frame elements describing an event of moving
the clinic, which the user has not said explicitly. This
happens because, in the SASO-EN domain, in fact there
is nothing else that the interlocutors need to do besides
move the clinic. So based on the NLU training data,
the data-driven probabilities assigned by Equation (1) de-
scribe the additional frame elements as about as probable
as the ones capturing the we need to part of the semantics
(given at the right).
Finally, a third observation is that overall, the preci-
sion, recall, and F-score results against the annotated sub-
frames using our method are surprisingly strong. For
example, when evaluating the explicit subframes over
all partial ASR results, an F-score of 0.75 is attained at
thresholds in the range 0.5-0.55. This F-score is sub-
stantially better than the F-score of our predictive NLU
in relation to the final full frames, which is 0.56 when
evaluated over all partial ASR results. This means that
our proposed model works better as an explicit incre-
mental NLU than mxNLU works as a predictive incre-
mental NLU. Further, we observe that this F-score of
0.75 against hand-annotated subframes is approximately
as good as the F-score of 0.76 that is achieved when
mxNLU is used to interpret complete utterances. We
therefore conclude that the proposed model is a promis-
ing and viable approach to explicit incremental NLU in
SASO-EN.
4 Discussion and Related Approaches
In this section, we discuss some of the practical aspects
of using the technique presented here, in relation to some
alternative approaches.
An important consideration for NLU techniques is the
cost, in both time and knowledge, of the annotation that
is needed. One attractive aspect of our technique is that
the only semantic annotation that is required is the asso-
ciation of complete user utterances with complete NLU
output frames. This task can be performed by anyone fa-
miliar with the scenario and the semantic frame format,
such as a system developer or scenario designer. In fact,
the annotation of the SASO-EN data set we use in this
paper has been described in (DeVault et al, 2010), which
reports that the overall corpus of 4678 token utterances
was semantically annotated at an average rate of about 10
seconds per unique utterance.
The model in Equation (2) is what (Heintze et al,
2010) call a hybrid output approach, in which larger and
larger frames are provided as partial input grows, but
in which a detailed alignment between surface text and
frames is not provided by the incremental NLU compo-
nent. They contrast hybrid output systems with tech-
niques that deliver either whole-frame output (like the
predictive mxNLU) or aligned output that connects indi-
vidual words to their meanings. A data-driven approach
to providing aligned outputs would involve preparing
a more detailed annotated corpus that aligns individ-
ual words and surface expressions to their corresponding
frame elements. Given such a word-aligned corpus, one
could train several kinds of models to produce the aligned
outputs incrementally. One strategy would be to use a se-
quential tagging model such as a CRF to tag partial utter-
ances with the frame elements that capture their explicit
meaning, as in (Heintze et al, 2010).
Using a machine learning approach that models a
more detailed alignment between surface text and frames
would be one way to more cleanly separate explicit from
predictive aspects of meaning. Preparing the training data
for such models, however, would create additional an-
notation costs. As part of creating the annotated sub-
frames for the evaluation presented in Section 3, we mea-
sured the time requirement for such annotation of word-
meaning alignments at about 30 seconds per unique ut-
terance. Performing full word-meaning alignment there-
fore takes about three times as much time as the com-
plete utterance annotation needed for our technique. Ad-
1097
Explicit subframe (with frame element probabilities) Predicted complete frame Annotated subframe
Partial ASR result: we
0.753 <S>.mood declarative
0.687 <S>.sem.agent captain-kirk
0.692 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event deliver
<S>.sem.modal.possibility can
<S>.sem.speechact.type offer
<S>.sem.theme medical-supplies
<S>.sem.type event
Partial ASR result: we need to
0.945 <S>.mood declarative
0.928 <S>.sem.agent captain-kirk
0.900 <S>.sem.event move
0.816 <S>.sem.modal.deontic must
0.900 <S>.sem.source market
0.900 <S>.sem.speechact.type statement
0.906 <S>.sem.theme clinic
0.930 <S>.sem.type event
<S>.mood declarative
<S>.sem.agent captain-kirk
<S>.sem.event move
<S>.sem.modal.deontic must
<S>.sem.source market
<S>.sem.speechact.type statement
<S>.sem.theme clinic
<S>.sem.type event
<S>.mood declarative
<S>.sem.modal.deontic must
<S>.sem.speechact.type statement
Figure 5: Explicit subframes and predicted complete frames for two partial ASR results in a user utterance of we need to move the
clinic.
ditionally, this task requires a greater degree of linguis-
tic knowledge and sophistication, as the annotator must
be able to segment the utterance and align specific sur-
face segments with potentially complex aspects of mean-
ing such as modality, polarity, speech act types, and
others. An example of the kinds of complexities that
arise is illustrated in Figure 3, where the relationship be-
tween specific words like ?provide? and ?transportation?
to frame elements like <S>.sem.event move and
<S>.sem.theme clinic is not transparent, even if
it is straightforward to mark the whole utterance as con-
veying that meaning in this domain. We have generally
found this alignment task challenging for people without
advanced linguistics training.
The reason we describe the method in this paper as an
approximation of explicit NLU is that, partly because it
is trained without detailed word-meaning alignments, it
can be expected to occasionally include some predictive
aspects of user utterance meaning. An example of this is
the method?s explicit subframe output for the phrase we
need to in Figure 5.
Another way to approximate explicit NLU would be
using the method (Heintze et al, 2010) call an ensem-
ble of classifiers; it involves training an individual clas-
sifier for each frame key. Like the method presented
here, an ensemble of classifiers can be easily trained to
predict those frame elements that will appear in the fi-
nal frame Gu for each utterance. And like our method,
prediction with an ensemble of classifiers does not re-
quire detailed annotation of word-meaning alignment in
the training data. One difference is that, with our method,
by selecting an appropriate threshold, it is easy to enforce
certain consistency properties on subframe outputs. In an
ensemble of classifiers approach, there is no immediate
guarantee that the output frame constructed by the inde-
pendent classifiers will be internally consistent from the
standpoint of downstream system modules (Heintze et al,
2010). For example, in the SASO-EN domain, an NLU
frame should not contain frame elements that mix aspects
of events and states in the SASO-EN ontology; e.g., the
frame element <S>.sem.type event should not co-
occur in an NLU output frame with the frame element
<S>.sem.object-id market (which would be ap-
propriate for a state frame but not for an event frame).
With the method proposed here, if we select a threshold
? that is greater than 0.5, and if none of the complete
NLU frames contain incompatible key values (which is
relatively easy to enforce as part of the annotation task),
then it will be mathematically impossible for two incom-
patible frame elements to be returned in a subframe.3
Ultimately, a classification method that is trained on
word-meaning aligned data and that uses additional tech-
niques to ensure that only valid, grammatical output
frames are produced could prove to be an attractive ap-
proach. In future work, we will explore such techniques,
and compare both their performance as well as their anno-
tation and development costs to the approximation tech-
nique presented here.
5 Conclusion
The analysis in this paper has explored a method of ap-
proximating explicit incremental NLU using predictive
3Suppose frame element ei is incompatible with ej , and that
P (ei ? Gu|rj) > 0.5. By stipulation, no complete frame S ? S
such that ei ? S will also contain ej . Since we know that the total
probability of all the frames containing ei must be greater than 0.5 in
order for ei to be selected, we can infer that the total probability of all
frames including ej must be less than 0.5, and thus that ej will not be
selected.
1098
techniques in finite semantic domains. We have shown
that an estimate of a user?s explicit utterance meaning
can be derived from an existing predictive understand-
ing model in an example domain. We have quantified
the performance of this new method in a corpus evalu-
ation, showing that the method returns incremental ex-
plicit subframes with performance ? as measured by pre-
cision, recall, and F-Score against hand-annotated sub-
frames ? that is competitive with a current statistical,
data-driven approach for understanding complete spoken
utterances in the same domain. We have provided ex-
amples that illustrate its strengths and weaknesses, and
discussed the annotation costs associated with imple-
menting this technique in relation to some alternative ap-
proaches. The method requires no additional annotation
beyond what is needed for training an NLU module to
understand complete spoken utterances. (Hand annota-
tion of word-meaning alignment for a small number of
utterances may be performed in order to tune the se-
lected threshold and evaluate explicit understanding per-
formance.) The method provides a free parameter that
can be used to target the most advantageous levels of pre-
cision and recall for a particular dialogue system applica-
tion. In future work, we will explore additional machine
learning models that leverage richer training data, and in-
vestigate further the combination of explicit and predic-
tive techniques.
Acknowledgments
The project or effort described here has been sponsored
by the U.S. Army Research, Development, and Engi-
neering Command (RDECOM). Statements and opinions
expressed do not necessarily reflect the position or the
policy of the United States Government, and no official
endorsement should be inferred. This material is based
upon work supported by the National Science Founda-
tion under Grant No. IIS-1219253. Any opinions, find-
ings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not neces-
sarily reflect the views of the National Science Founda-
tion.
References
Hendrik Buschmeier, Timo Baumann, Benjamin Dosch, Ste-
fan Kopp, and David Schlangen. 2012. Combining incre-
mental language generation and incremental speech synthe-
sis for adaptive information presentation. In Proceedings of
the 13th Annual Meeting of the Special Interest Group on
Discourse and Dialogue, pages 295?303, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
Okko Bu? and David Schlangen. 2011. Dium - an incremental
dialogue manager that can produce self-corrections. In Pro-
ceedings of the 15th Workshop on the Semantics and Prag-
matics of Dialogue (SemDial).
David DeVault, Susan Robinson, and David Traum. 2010.
IORelator: A graphical user interface to enable rapid seman-
tic annotation for data-driven natural language understand-
ing. In Fifth Joint ISO-ACL/SIGSEM Workshop on Interop-
erable Semantic Annotation.
David DeVault, Kenji Sagae, and David Traum. 2011a. Incre-
mental interpretation and prediction of utterance meaning for
interactive dialogue. Dialogue & Discourse, 2(1).
David DeVault, Kenji Sagae, and David R. Traum. 2011b. De-
tecting the status of a predictive incremental speech under-
standing model for real-time decision-making in a spoken
dialogue system. In Interspeech, pages 1021?1024.
Arno Hartholt, Thomas Russ, David Traum, Eduard Hovy,
and Susan Robinson. 2008. A common ground for vir-
tual humans: Using an ontology in a natural language ori-
ented virtual human architecture. In European Language
Resources Association (ELRA), editor, Proc. LREC, Mar-
rakech, Morocco, may.
Silvan Heintze, Timo Baumann, and David Schlangen. 2010.
Comparing local and sequential models for statistical incre-
mental natural language understanding. In The 11th Annual
Meeting of the Special Interest Group in Discourse and Dia-
logue (SIGDIAL 2010).
David Huggins-Daines, Mohit Kumar, Arthur Chan, Alan W.
Black, Mosur Ravishankar, and Alex I. Rudnicky. 2006.
Pocketsphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings of
ICASSP.
Kenji Sagae, Gwen Christian, David DeVault, and David R.
Traum. 2009. Towards natural language understanding of
partial speech recognition results in dialogue systems. In
NAACL HLT.
David Schlangen, Timo Baumann, and Michaela Atterer. 2009.
Incremental reference resolution: The task, metrics for eval-
uation, and a bayesian filtering model that is sensitive to dis-
fluencies. In SIGDIAL.
Ethan O. Selfridge, Iker Arizmendi, Peter A. Heeman, and Ja-
son D. Williams. 2012. Integrating incremental speech
recognition and pomdp-based dialogue systems. In Proceed-
ings of the 13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 275?279, Seoul, South
Korea, July. Association for Computational Linguistics.
Gabriel Skantze and David Schlangen. 2009. Incremental di-
alogue processing in a micro-domain. In Proceedings of
EACL 2009.
David Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, and
Arno Hartholt. 2008. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents. In Proc.
of Intelligent Virtual Agents Conference IVA-2008.
David Traum, David DeVault, Jina Lee, Zhiyang Wang, and
Stacy C. Marsella. 2012. Incremental dialogue understand-
ing and feedback for multi-party, multimodal conversation.
In The 12th International Conference on Intelligent Virtual
Agents (IVA), Santa Cruz, CA, September.
David Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In Proc. of the Interna-
tional Workshop on Computational Semantics, pages 380?
394, January.
1099
Practical Grammar-Based NLG from Examples
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a technique that opens up
grammar-based generation to a wider range
of practical applications by dramatically re-
ducing the development costs and linguis-
tic expertise that are required. Our method
infers the grammatical resources needed for
generation from a set of declarative exam-
ples that link surface expressions directly to
the application?s available semantic represen-
tations. The same examples further serve to
optimize a run-time search strategy that gener-
ates the best output that can be found within an
application-specific time frame. Our method
offers substantially lower development costs
than hand-crafted grammars for application-
specific NLG, while maintaining high output
quality and diversity.
1 Introduction
This paper presents a new example-based genera-
tion technique designed to reduce the development
costs and linguistic expertise needed to integrate a
grammar-based generation component into an ex-
isting application. We believe this approach will
broaden the class of applications in which grammar-
based generation may feasibly be deployed.
In principle, grammar-based generation offers
significant advantages for many applications, when
compared with simpler template-based or canned
text output solutions, by providing productive cov-
erage and greater output variety. However, realiz-
ing these advantages can require significant devel-
opment costs (Busemann and Horacek, 1998).
One possible strategy is to exploit a wide-
coverage realizer that aims for applicability in mul-
tiple application domains (White et al, 2007; Cahill
and van Genabith, 2006; Zhong and Stent, 2005;
Langkilde-Geary, 2002; Langkilde and Knight,
1998; Elhadad, 1991). These realizers provide
a sound wide-coverage grammar (or robust wide-
coverage language model) for free, but demand a
specific input format that is otherwise foreign to
an existing application. Unfortunately, the devel-
opment burden of implementing the translation be-
tween the system?s available semantic representa-
tions and the required input format can be quite sub-
stantial (Busemann and Horacek, 1998). Indeed, im-
plementing the translation might require as much ef-
fort as would be required to build a simple custom
generator; cf. (Callaway, 2003). Thus, there cur-
rently are many applications where using a wide-
coverage generator remains impractical.
Another strategy is for system builders to hand
craft an application-specific grammar for genera-
tion. This approach can be initially attractive to
system builders because it allows syntactic cover-
age and semantic modeling to be tailored directly
to application needs. However, writing grammati-
cal rules by hand ultimately requires a painstaking,
time-consuming effort by a developer who has de-
tailed linguistic knowledge as well as detailed appli-
cation knowledge. Further, the resulting coverage is
inevitably limited to the set of linguistic construc-
tions that have been selected for careful modeling.
A third strategy is to use an example-based ap-
proach (Wong and Mooney, 2007; Stone, 2003;
Varges and Mellish, 2001) in which the connection
77
between available application semantic representa-
tions and desired output utterances is specified by
example. Example-based approaches aim to allow
system builders to specify a productive generation
capacity while leaving the representations and rea-
soning that underlie that productive capacity mostly
implicit in a set of training examples. This method-
ology insulates system builders from the detailed ex-
pertise and technical infrastructure needed to imple-
ment the productive capacity directly, and has made
example-based approaches attractive not only in text
generation but also in related areas such as concate-
native speech synthesis and motion capture based
animation; see, e.g., (Stone et al, 2004).
The technique we present in this paper is a new
example-based approach to specifying application-
specific text generation. As in other hand-crafted
and example-based approaches, our technique al-
lows syntactic coverage and semantic modeling to
follow the needs and available semantic representa-
tions in an application. One contribution of our tech-
nique is to relieve the generation content author of
the burden of manual syntactic modeling by lever-
aging an off-the-shelf parser; defects in the syntax
provided by the parser are effectively overcome us-
ing a machine learning technique. Additionally, our
technique organizes the authoring task in a way that
relieves the generation author of carefully modeling
the connections between particular syntactic con-
structions and available semantic representations.
Together, we argue, these features dramatically
reduce the linguistic expertise and other develop-
ment costs that are required to integrate a grammar-
based generation component into an existing system.
In a case study application, we show that our ap-
proach allows an application developer who lacks
detailed linguistic knowledge to extend grammatical
coverage at an expense of less than one minute per
additional lexical entry.
2 Case Study: Doctor Perez
Our approach has been tested as a replacement for
the generation component of interactive virtual hu-
mans used for social training purposes (Swartout et
al., 2006). Virtual humans are embodied conversa-
tional agents that play the role of people in simula-
tions or games. The case study we present in this
paper is the generation of output utterances for a
particular virtual human, Doctor Perez, who is de-
signed to teach negotiation skills in a multi-modal,
multi-party, non-team dialogue setting (Traum et al,
2008). The human trainee who talks to the doctor
plays the role of a U.S. Army captain named Cap-
tain Kirk. The design goals for Doctor Perez create
a number of requirements for a practical NLG com-
ponent. We briefly summarize these requirements
here; see (DeVault et al, 2008) for more details.
Doctor Perez has a relatively rich internal mental
state including beliefs, goals, plans, and emotions.
He uses an attribute-value matrix (AVM) semantic
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain. To facilitate interprocess communi-
cation, and statistical processing, this AVM structure
is linearized into a ?frame? of key values in which
each non-recursive terminal value is paired with a
path from the root to the final attribute. Figure 1
shows a typical frame. See (Traum, 2003) for addi-
tional details and examples of this representation.
While only hundreds of frames currently arise in
actual dialogues, the number of potential frames is
orders of magnitude larger, and it is difficult to pre-
dict in advance which frames might occur. The ut-
terances that realize these frames need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Highly fluent out-
put is not a necessity for this character, since Doc-
tor Perez is designed to simulate a non-native En-
glish speaker. However, in order to support com-
pelling real-time conversational interaction and ef-
fective training, the generation module must be able
to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Finally, the development team for Doctor Perez?s
language capabilities includes approximately 10
programmers, testers, linguists, and computational
linguists. Wherever possible, it is better if any de-
veloper can improve any aspect of Doctor Perez?s
language processing; e.g., if a programmer discov-
78
ers a bug or disfluency in the NLG output, it is better
if she can fix it directly rather than requiring a (com-
putational) linguist to do so.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component. In particular, we leverage the
increasing availability of off-the-shelf parsers such
as (Charniak, 2001; Charniak, 2005) to automati-
cally (or semi-automatically) assign syntactic anal-
yses to a set of suggested output sentences. We
then draw on lexicalization techniques for statistical
language models (Magerman, 1995; Collins, 1999;
Chiang, 2000; Chiang, 2003) to induce a probabilis-
tic, lexicalized tree-adjoining grammar that supports
the derivation of all the suggested output sentences,
and many others besides.
The final step is to use the training examples to
learn an effective search policy so that our run-time
generation component can find good output sen-
tences in a reasonable time frame. In particular, we
use variants of existing search optimization (Daum?
and Marcu, 2005) and ranking algorithms (Collins
and Koo, 2005) to train our run-time component to
find good outputs within a specified time window;
see also (Stent et al, 2004; Walker et al, 2001). The
result is a run-time component that treats generation
as an anytime search problem, and is thus suitable
for applications in which a time/performance trade-
off is necessary (such as real-time dialogue).
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 1. In
this example, the generation content author suggests
the output utterance u = we don?t have medical
supplies here captain. Each utterance u is accom-
panied by syntax(u), a syntactic analysis in Penn
Treebank format (Marcus et al, 1994). In the fig-
ure, we show two alternative syntactic analyses that
might be specified: one is the uncorrected output of
the Charniak parser on this sentence, and the other
a hand-corrected version of that parse; we evaluate
the utility of this hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
79
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
cat: SA??
cat: S??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: VP??
cat: ADVP??
pos: RB??
here
pos: VBP??
captain
(corrected Charniak parse) or (uncorrected Charniak parse)
Semantics
we do n?t . . . . . . .
{
have . . . . . . . . . . . . .
medical supplies . .
here . . . . . . . . . . . . .
captain . . . . . . . .
?
?
?
semantic frame
speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
speech-act.content.value = medical-supplies
speech-act.content.object-id = market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 1: A generation training example for Doctor Perez. If uncorrected syntax is used, the generation content author
only writes the utterance and the links to the semantic frame.
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
3.2 Automatic Grammar Induction
We adopt essentially the probabilistic tree-adjoining
grammar (PTAG) formalism and grammar induc-
tion technique of (Chiang, 2003). Our approach
makes three modifications, however. First, while
Chiang?s model includes both full adjunction and
sister adjunction operations, our grammar has only
sister adjunction (left and right), exactly as in the
TAGLET grammar formalism of (Stone, 2002). Sec-
ond, to support lexicalization at an arbitrary gran-
ularity, we allow Chiang?s tree templates to be as-
sociated with more than one lexical anchor. Third,
to unify syntactic and semantic reasoning in search,
we augment lexical anchors with semantic informa-
tion. Formally, wherever Chiang?s model has a lex-
ical anchor w, ours has a pair (?w1, ..., wn?,M ?),
where M ? ? M is connected to lexical anchors
?w1, ..., wn? by the generation content author, as in
Figure 1. The result is that the derivation probabili-
ties the grammar assigns depend not only on the im-
plicated syntactic structures and lexical anchors but
also on the senses of those lexical anchors in appli-
cation terms.
We induce our grammar from training exam-
ples such as Figure 1 using heuristics to assign
derivations to the examples, exactly as in (Chiang,
2003). The process proceeds in two stages. In
the first stage, a collection of rules is used to au-
tomatically ?decorate? the training syntax with a
number of features. These include deciding the
lexical anchor(s) for each non-terminal constituent
and assigning complement/adjunct status for non-
terminals which are not on their parent?s lexical-
ization path; see (Magerman, 1995; Chiang, 2003;
Collins, 1999). In addition, we deterministically add
features to improve several grammatical aspects, in-
cluding (1) enforcing verb inflectional agreement in
derived trees, (2) enforcing consistency in the finite-
ness of VP and S complements, and (3) restricting
subject/direct object/indirect object complements to
play the same grammatical role in derived trees.
In the second stage, the complements and ad-
juncts in the decorated trees are incrementally re-
80
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 2: The linguistic resources inferred from the training example in Figure 1.
moved, yielding the reusable linguistic resources in
the grammar, as illustrated in Figure 2, as well as
the maximum likelihood estimates needed to com-
pute operation probabilities.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (for Doctor Perez: 200ms) and returning a
list of alternative outputs ranked by their derivation
probabilities.
3.3 Optimizing the Search Strategy
The search space created by a grammar induced in
this way is too large to be searched exhaustively in
most applications. The solution we have developed
is a beam search strategy that uses weighted features
to rank alternative grammatical expansions at each
step. In particular, the beam size and structure is op-
timized so that, with high probability, the beam can
be searched exhaustively before the timeout.1 The
second step of automated processing, then, is a train-
ing problem of finding weighted features such that
1For Doctor Perez, we use a wider beam for initial trees,
since the Doctor?s semantic representation is particularly im-
poverished at the level of main verbs. At search depths > 1, we
use beam size 1 (i.e. greedy search).
for every training problem, nodes that lead to good
generation output are ranked highly enough by those
features to make it into the beam.
We use domain-independent rules to automati-
cally define a set of features that could be heuris-
tically useful for a given induced grammar. These
include features for various syntactic structures and
operations, numbers of undesired and desired mean-
ings of different types added by an expansion,
derivation probabilities, etc. (For Doctor Perez,
this yields about 600 features.) Our training algo-
rithm is based on the search optimization algorithm
of (Daum? and Marcu, 2005), which updates fea-
ture weights when mistakes are made during search
on training examples. For the weight update step,
we use the boosting approach of (Collins and Koo,
2005), which performs feature selection and iden-
tifies weight values that improve the ranking of al-
ternative derivation steps when mistakes are made
during search. We discuss the resulting success rate
and quality in the next section.
4 Cost/Benefit Analysis
The motivation that underlies our technical approach
is to reduce the development costs and linguistic ex-
pertise needed to develop a grammar-based genera-
tion component for an existing system. In this sec-
tion, we assess the progress we have made by ana-
81
lyzing the use of our approach for Doctor Perez.
Method. We began with a sample of 220 in-
stances of frames that Doctor Perez?s dialogue man-
ager had requested of the generation component in
previous dialogues with users. Each frame was as-
sociated with a hand-authored target output utter-
ance. We then constructed two alternative training
examples, in the format specified in Section 3.1, for
each frame. One example had uncorrected output
of the Charniak parser for the syntax, and another
had hand-corrected parser output (see Figure 1). The
connections between surface expressions and frame
key-value pairs were identical in both uncorrected
and corrected training sets.
We then built two generators using the two sets
of training examples. We used 90% of the data for
training and held out 10% for testing. The genera-
tors sometimes failed to find a successful utterance
within the 200ms timeout. For example, the success
rate of the version of our generator trained on uncor-
rected syntax was 96.0% for training examples and
81.8% for test examples.
Quality of generated output. To assess output
quality, 5 system developers rated each of 494 utter-
ances, in relation to the specific frame for which it
was produced, on a single 1 (?very bad?) to 5 (?very
good?) scale. The 494 utterances included all of the
hand-authored (suggested) utterances in the training
examples. They also included all the top-ranked ut-
terances that were successfully generated by the two
generators. We asked our judges to make an over-
all assessment of output quality, incorporating both
accuracy and fluency, for the Doctor Perez charac-
ter. Judges were blind to the conditions under which
utterances were produced. We discuss additional de-
tails of this rating task in (DeVault et al, 2008).
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. We ran a small number of planned
comparisons on these ratings. Surprisingly, we
found no significant difference between generated
output trained on corrected and uncorrected syntax
(t(29) = 0.056, p > 0.9 on test items, t(498) =
?1.1, p > 0.2 on all items).2 However, we did
2The distribution of ratings across utterances is not normal;
to validate our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and significance always fell
Hand-authored (N = 1099)
Generated:
Training input (N = 949)
Test input (N = 90)
Rating
Fr
eq
ue
nc
y
(%
)
0
10
20
30
40
50
60
1 2 3 4 5
Figure 3: Observed rating frequencies for hand-authored
vs. generated utterances (uncorrected syntax).
find that hand-authored utterances (mean rating 4.4)
are significantly better (t(388) = 5.9, p < 0.001)
than generated utterances (mean rating 3.8 for un-
corrected syntax). These ratings are depicted in Fig-
ure 3. While the figure suggests a slight reduction in
quality for generated output for test frames vs. train-
ing frames, we did not find a significant difference
between the two (t(19) = 1.4228, p > 0.15).
Variety of generated output. In general our any-
time algorithm delivers a ranked list of alternative
outputs. While in this initial study our judges rated
only the highest ranked output generated for each
frame, we observed that many of the lower ranked
outputs are of relatively high quality. For example,
Figure 4 shows a variety of alternative outputs that
were generated for two of Doctor Perez?s training
examples. Many of these outputs are not present as
hand-authored utterances (for any frame); this illus-
trates the potential of our approach to provide a va-
riety of alternative outputs or paraphrases, which in
some applications may be useful even for meanings
for which an example utterance is hand-authored.
Figure 5 shows the overall distribution in the number
of outputs returned for Doctor Perez.
Development costs. The development costs in-
cluded implementation of the approach and specifi-
cation of Doctor Perez?s training set. Implementa-
in the same general range.
82
Rank Time (ms) Novel?
1 16 no the clinic is up to standard captain
2 94 no the clinic is acceptable captain (hand-authored for this input)
3 78 yes the clinic should be in acceptable condition captain
4 16 yes the clinic downtown is currently acceptable captain
5 78 yes the clinic should agree in an acceptable condition captain
1 94 no there are no medical supplies downtown captain
2 172 no we don?t have medical supplies downtown captain
3 125 yes well captain i do not think downtown there are medical supplies
4 16 yes i do not think there are medical supplies downtown captain
Figure 4: All the utterances generated (uncorrected syntax) for two examples. Rank is determined by derivation
probability. Outputs marked as novel are different from any suggested output for any training example.
Number of successful outputs
Fr
eq
ue
nc
y
(%
)
0
10
20
30
0 1 2 3 4 5 6 7 8 9
Figure 5: Variety of outputs for each input.
tion required an effort of approximately six person
months. The developer who carried out the imple-
mentation initially had no familiarity with the Doc-
tor Perez domain, so part of this time was spent un-
derstanding Doctor Perez and his available seman-
tic representations. The bulk of the development
time was spent implementing the grammar induction
and training processes. Grammar induction included
implementing the probabilistic grammar model and
writing about 40 rules that are used to extract gram-
matical entries from the training examples. Of these
40 rules, only 3 are specific to Doctor Perez.3 The
remainder are broadly applicable to syntactic anal-
yses in Penn Treebank format, and thus we expect
they would transfer to applications of our approach
in other domains. Similarly, the training algorithms
are entirely domain neutral and could be expected to
transfer well to additional domains.
Specification of Doctor Perez?s training data took
3These 3 rules compensate for frequent errors in Charniak
parser output for the words captain, elder, and imam, which are
often used to signal the addressee of Doctor Perez?s utterances.
about 6 hours, or about 1.6 minutes per training ex-
ample. This time included hand correction of syn-
tactic analyses generated by the Charniak parser and
definition of semantic links between surface expres-
sions and frame key-value pairs. Since we found
that hand-correcting syntax does not improve out-
put quality, this 1.6 minutes/example figure over-
estimates the authoring time required by our ap-
proach. The remaining work lies in defining the se-
mantic links. For Doctor Perez, approximately half
of the semantic links were automatically assigned
with simple ad hoc scripts.4 The semantic linking
process might be further sped up through a stream-
lined authoring interface offering additional automa-
tion, or even using a machine learning approach to
suggest appropriate links.
Linguistic expertise required. Since we found
that hand-correcting syntax does not improve output
quality, a developer who wishes to exploit our ap-
proach may use the Charniak parser to supply the
syntactic model for the domain. Thus, while one
developer with linguistic expertise is required to im-
plement the approach, anybody on the application
team can contribute by hand authoring additional ut-
terances and defining semantic links. The benefit of
this authoring effort is the ability to generate high
quality output for many novel semantic inputs.
Cost/benefit. The grammar induced from the 198
training examples (with uncorrected syntax) con-
tains 426 lexical entries of the type depicted in Fig-
ure 2. These 426 lexical entries were produced auto-
matically from about 6 hours worth of authoring ef-
4Time to compose these scripts is included in the 1.6 min-
utes/example.
83
fort together with domain-neutral algorithms. This
translates to a rate of grammar expansion of less
than 1 minute per lexical entry, on average, for this
small application-specific grammar. This constitutes
a dramatic improvement over our previous experi-
ence hand-crafting grammars. It would be challeng-
ing for an expert to specify a lexical entry such as
those in Figure 2 in under one minute (and probably
impossible for someone lacking detailed linguistic
knowledge). In our experience, however, the bulk
of development lies in additional time spent con-
sidering and investigating possible interactions be-
tween lexical entries in generation. Our technique
helps with both problems: the grammar induction
streamlines the specification of lexical entries, and
the training removes the need for a developer to
manually trace through the various complex inter-
actions between lexical entries during generation.
5 Limitations
Currently, we do not support semantic links from
non-contiguous expressions, which means a desired
output like ?we rely heavily on medical supplies?
would be difficult to annotate if rely...on corresponds
to a single semantic representation. This is not an in-
trinsic limitation to our general approach, but rather
a simplification in our initial implementation.
As discussed in Section 3.2, our grammar induc-
tion process adds syntactic features related to verb
inflection, finiteness, and grammatical role to the in-
ferred lexical entries. Such features improve the flu-
ency and accuracy of output derived with the gram-
mar. While we believe such features can always be
assigned using domain-independent rules, develop-
ing these rules requires linguistic expertise, and it
is likely that additional rules and features (not yet
implemented) would improve coverage of linguistic
phenomena such as control verbs, various kinds of
coordination, and relative clauses, inter alia.
A more entrenched limitation of our approach
is its assumption that the generator does not need
context as a separate input. This means, for ex-
ample, that our approach cannot generate referring
expressions (by selecting disambiguating semantic
properties); rather, all semantic properties must be
pre-selected and included in the generation request.
Generation of anaphoric expressions is also limited,
since contextual ambiguities are not considered.
6 Related Work
To our knowledge, this is the first implemented
generation technique that does all three of the fol-
lowing: directly interfaces to existing application
semantic representations, infers a phrase structure
grammar from examples, and does not require hand-
authored syntax as input. (Varges and Mellish,
2001) also aims to reduce the authoring burden of
domain-specific generation; however, they seem to
use a special purpose semantic annotation rather
than pre-existing application semantics, and their
task is defined in terms of the Penn Treebank, so
hand-authored syntax is used as input. (Wong and
Mooney, 2007) also interfaces to existing applica-
tion semantics, and does not require hand-authored
syntax as input. Their technique infers a syn-
chronous grammar in which the hierarchical linguis-
tic analysis is isomorphic to the hierarchy in the ap-
plication semantics, and differs from phrase struc-
ture. It would be interesting to compare their out-
put quality with ours; their automated alignment of
words to semantics might also provide a way to fur-
ther reduce the authoring burden of our approach.
7 Conclusion and Future Work
We have presented a new example-based approach
to specifying text generation for an existing appli-
cation. We have used a cost/benefit analysis to ar-
gue that our approach offers productive coverage
and high-quality output with less linguistic expertise
and lower development costs than building a hand-
crafted grammar. In future work, we will evaluate
our approach in additional application settings, and
study the performance of our approach as the size
and scope of the training set grows.
Acknowledgments
Thanks to our anonymous reviewers, Arno Hartholt,
Susan Robinson, Thomas Russ, Chung-chieh Shan,
andMatthew Stone. This work was sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
84
References
S. Busemann and H. Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings of INLG,
pages 238?247.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In ACL, pages 1033?1040.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. Proceedings of IJCAI.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL, pages 124?131, Morristown,
NJ, USA. Association for Computational Linguistics.
E. Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
456?463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
D. Chiang. 2003. Statistical parsing with an automat-
ically extracted tree adjoining grammar. In R. Bod,
R. Scha, and K. Sima?an, editors, Data Oriented Pars-
ing, pages 299?316. CSLI Publications, Stanford.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. dissertation, Uni-
versity of Pennsylvania.
H. Daum? and D. Marcu. 2005. Learning as search
optimization: approximate large margin methods for
structured prediction. In ICML ?05: Proceedings of
the 22nd international conference on Machine learn-
ing, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy in
dialogue systems. In Ninth SIGdial Workshop on Dis-
course and Dialogue (SIGdial).
M. Elhadad. 1991. FUF: the universal unifier user man-
ual version 5.0. Technical Report CUCS-038-91.
K. Krippendorff, 1980. Content Analysis: An Introduc-
tion to Its Methodology, chapter 12, pages 129?154.
Sage, Beverly Hills, CA.
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276?283, Morristown, NJ, USA. Association for
Computational Linguistics.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In ACL.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Ro-
driguez, Adrian Stere, Alyssa Lees, and Chris Bregler.
2004. Speaking with hands: creating animated con-
versational characters from recordings of human per-
formance. ACM Trans. Graph., 23(3):506?513.
M. Stone. 2002. Lexicalized grammar 101. In ACL
Workshop on Tools and Methodologies for Teaching
Natural Language Processing.
Matthew Stone. 2003. Specifying generation of referring
expressions by example. In AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 133?140.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96?108.
D. R. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
D. Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In proceedings of the
International Workshop on Computational Semantics,
pages 380?394, January.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In NAACL, pages
1?8.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proceedings of the
North American Meeting of the Association for Com-
putational Linguistics.
M. White, R. Rajkumar, and S. Martin. 2007. To-
wards broad coverage surface realization with CCG.
In Proc. of the Workshop on Using Corpora for NLG:
Language Generation and Machine Translation (UC-
NLG+MT).
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statistical
machine translation. In Proceedings of NAACL-HLT,
pages 172?179.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. In Proc. Corpus Linguistics ?05 Workshop on
Using Corpora for Natural Language Generation.
85
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 39?48,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Toward Learning and Evaluation of Dialogue Policies with Text Examples
David DeVault and Anton Leuski and Kenji Sagae
Institute for Creative Technologies
University of Southern California
Playa Vista, CA 90094
{devault,leuski,sagae}@ict.usc.edu
Abstract
We present a dialogue collection and enrich-
ment framework that is designed to explore
the learning and evaluation of dialogue poli-
cies for simple conversational characters us-
ing textual training data. To facilitate learning
and evaluation, our framework enriches a col-
lection of role-play dialogues with additional
training data, including paraphrases of user ut-
terances, and multiple independent judgments
by external referees about the best policy re-
sponse for the character at each point. As
a case study, we use this framework to train
a policy for a limited domain tactical ques-
tioning character, reaching promising perfor-
mance. We also introduce an automatic policy
evaluation metric that recognizes the validity
of multiple conversational responses at each
point in a dialogue. We use this metric to ex-
plore the variability in human opinion about
optimal policy decisions, and to automatically
evaluate several learned policies in our exam-
ple domain.
1 Introduction
There is a large class of potential users of dialogue
systems technology who lack the background for
many of the formal modeling tasks that typically
are required in the construction of a dialogue sys-
tem. The problematic steps include annotating the
meaning of user utterances in some semantic formal-
ism, developing a formal representation of informa-
tion state, writing detailed rules that govern dialogue
management, and annotating the meaning of system
utterances in support of language generation, among
other tasks.
In this paper, we explore data collection and ma-
chine learning techniques that enable the implemen-
tation of domain-specific conversational dialogue
policies through a relatively small data collection ef-
fort, and without any formal modeling. We present
a case study, which serves to illustrate some of
the possibilities in our framework. In contrast to
recent work on data-driven dialogue policy learn-
ing that learns dialogue behavior from existing data
sources (Gandhe and Traum, 2007; Jafarpour et al,
2009; Ritter et al, 2010), we address the task of au-
thoring a dialogue policy from scratch with a spe-
cific purpose, task and scenario in mind. We exam-
ine the data collection, learning and evaluation steps.
The contributions of this work include a data col-
lection and enrichment framework without formal
modeling, and the creation of dialogue policies from
the collected data. We also propose a framework for
evaluating learned policies. We show, for the sce-
nario in our case study, that these techniques deliver
promising levels of performance, and point to possi-
ble future developments in data-driven dialogue pol-
icy creation and evaluation.
2 Case study
For our case study we selected an existing dialogue
system scenario designed for Tactical Questioning
training (Traum et al, 2008). The character targeted
in our study, Amani, is modeled closely after the
Amani Tactical Questioning character described by
Gandhe et al (2009) and Artstein et al (2009). Tac-
tical Questioning dialogues are those in which small
unit military personnel, usually on patrol, hold con-
versations with individuals to produce information
of military value. A tactical questioning dialogue
39
system is a simulation training environment where
virtual characters play the role of a person being
questioned. Tactical questioning characters are de-
signed to be non-cooperative at times. They may
answer some of the interviewers questions in a coop-
erative manner, but may refuse to answer other ques-
tions, or intentionally provide incorrect answers.
Therefore the interviewer is encouraged to conduct
the interview in a manner that induces cooperation
from the character: building rapport with the char-
acter, addressing their concerns, making promises
and offers, as well as threatening or intimidating the
character; the purpose of the dialogue system is to
allow trainees to practice these strategies in a realis-
tic setting (Gandhe et al, 2009).
This type of scenario is a good testbed for our
proposed learning and evaluation framework, since
it involves both flexible conversational choices and
well-defined constraints regarding the disclosure of
specific information. In the Amani scenario, the user
plays the role of a commander of a small military
unit in Iraq whose unit had been attacked by sniper
fire. The user interviews a character named Amani
who was a witness to the incident and is thought to
have some information about the identity of the at-
tackers. Amani is willing to tell the interviewer ev-
erything she knows provided that the user promises
her safety, secrecy, and small monetary compensa-
tion for the information (Artstein et al, 2009).
An exhaustive formal definition of Amani?s ideal
dialogue policy might include a large number of
rules covering a wide range of user utterance types.
The key constraints for the training simulation, how-
ever, can be stated simply with a few rules governing
the release of five pieces of information that Amani
knows. Amani will only reveal one of these pieces of
information if a precondition is met. Table 1 shows
how certain information relates to each of the pre-
conditions in Amani?s dialogue policy. Amani can
only reveal a fact from the first column if the user
promised her an item from the second column. For
example, Amani can only tell the user the shooter?s
name if the user promised her safety. If the user
has not promised safety, Amani will ask him for
safety. If the user refuses to promise safety, Amani
will either decline to answer the question or lie to
the interviewer. Amani does keep track of the user?s
promises and once she is promised safety, she would
information precondition
about shooter?s name safety
about shooter?s description safety
about shooter?s location secrecy
about the occupant of the shop secrecy
about shooter?s daily routine money
Table 1: Amani?s dialogue policy.
not ask for it again.
While the key constraints for Amani?s policy, as
summarized in Table 1, may be easily expressed
in terms of rules involving dialogue-acts, the rest
of Amani?s behavior is more open-ended and un-
derspecified. Ideally, the system designers would
like for the character to obey conversational conven-
tions (such as responding appropriately to greetings,
thankings, etc.). Her responses to other user utter-
ances should match human intuition about what a
good response would be, but specific responses are
not generally dictated by the goals for the training
simulation. There is therefore room for some flex-
ibility, and also for the character to reply that she
does not understand. Of course, her conversational
repertoire is inevitably limited by the available au-
thoring and development effort as well as language
processing challenges.
3 Data collection
The exponential number of possible utterances and
dialogue paths in even a simple conversational dia-
logue scenario such as the Amani scenario suggests
that learning acceptable dialogue behavior from sur-
face text examples without annotation or formal
modeling would require a seemingly insurmount-
able quantity of dialogues to serve as training data.
We address this problem in a data collection frame-
work with four main characteristics: (1) we sidestep
the problem of learning natural language generation
by using a fixed predefined set of utterances for the
Amani character. This so-called ?utterance selec-
tion? approach has been used in a number of dia-
logue systems (Zukerman and Marom, 2006; Sell-
berg and Jnsson, 2008; Kenny et al, 2007, for ex-
ample) and often serves as a reasonable approxima-
tion to generation (Gandhe and Traum, 2010); (2)
we collect dialogues from human participants who
40
play the parts of Amani and the commander in a
structured role play framework (Section 3.1); (3) we
enrich the dialogues collected in the structured role
play step with additional paraphrases for the utter-
ances of the commander, in an attempt to deal with
large variability of natural language input, even for
a limited domain conversational dialogue scenario
(Section 3.2); (4) we further augment the existing
dialogue data by adding acceptable alternatives to
the dialogue acts of the Amani role through the use
of external referees (Section 3.3).
Our data collection procedure is designed to cap-
ture the necessary information for learning dialogue
policies and evaluating their quality by approxi-
mating the exponentially large dialogue variability
while keeping the data collection effort tractable.
3.1 Structured role play
To examine the hypothesis that dialogue policies
such as Amani?s can be learned from examples with-
out explicit rules or any kind of formal modeling,
we collected dialogue data through a constrained
form of role play, which we call structured role play,
where the person playing the role of Amani is en-
couraged, whenever possible, to only use utterances
from a fixed set. Each utterance in the available set
of Amani replies corresponds roughly to one of the
dialogue acts (consisting of an illocutionary force
and some semantic content) described by Artstein et
al. (2009) for their version of the Amani character.
The players in the roles of Amani and the com-
mander take turns producing one utterance at a time,
each in a separate terminal. The commander player,
who receives a natural language description of the
scenario and the goal of the commander, enters utter-
ances through a teletype (chat) interface. The Amani
player, who receives a natural language descrip-
tion of the scenario and of Amani?s dialogue policy,
chooses an utterance from a list for each dialogue
turn. The Amani player is encouraged to use an ut-
terance from this list whenever possible; however,
for user utterances that the Amani player judges can-
not possibly be handled by any existing response, a
new response can be authored (as English text) and
immediately used in the role play. Each player sees
the other?s utterance as text in their own terminal.
This closely resembles a Wizard-of-Oz setup, with
they key difference being that both dialogue partic-
ipants believe they are interacting with another per-
son, which is in fact the case, and the idea of a wiz-
ard controlling a system is not part of the exercise.
However, because the Amani player is encouraged
to limit Amani?s responses to a fixed utterance set,
and the dialogue is constrained to a strict turn-taking
setup that interleaves utterances from each partici-
pant, the situation also differs from conventional role
play.
We collected a total of 19 dialogues and 296 ut-
terances for Amani, for an average of 15.6 Amani
utterances per dialogue.
3.2 Paraphrase generation
The dialogues collected through structured role play
are intended for serving as training data from which
Amani?s dialogue policy can be learned. However,
to cover the natural language variability with which
dialogue acts from the commander can be expressed
would require a much larger number of dialogues
than it would be practical to collect, since a learned
system that deals only with the surface text in the
dialogues would need to deal both with the dia-
logue policy and natural language understanding for
the scenario. Instead, we require only that the di-
alogues collected cover the desired dialogue acts
for the player role in the scenario. To address the
language understanding problem (indirectly), we at-
tempt to cover the variability of expression of these
dialogue acts through the collection of paraphrases
for the commander utterances in the set of dialogues.
For each commander utterance in the 19 dialogues
we collected, we had annotators create a set of para-
phrases. In creating paraphrases, annotators were
asked to take not just the original utterance into ac-
count, but also its context in the dialogue. We did
not specify a fixed number of paraphrases per utter-
ance, but instead asked for as many paraphrases as
the annotator could quickly think of.
Figure 1 exemplifies the paraphrases created dur-
ing this process, for a target user utterance of can
you tell me what you know of the incident?. For this
utterance, a total of 6 paraphrases were generated.
We used a total of 9 annotators, who created para-
phrases for the 296 utterances in the 19 dialogues.
Most annotators were responsible for no more than
two dialogues, and took on average less than 30
minutes per dialogue. The average number of para-
41
Previous Dialogue History
Lieutenant: hi amani
Amani: hello.
Lieutenant: how are you doing?
Amani: fine thank you.
Lieutenant: thank you for meeting with me
Amani: you?re welcome.
Target User Utterance
Lieutenant: can you tell me what you know of the incident?
Paraphrases:
please tell me what information you have about the incident
could you please tell me what you saw?
what can you tell me about the incident?
can you tell me about the incident?
please, tell me what you know about the incident
tell me what you saw, please
System Response
Amani: i saw all the shooting from my window. what do you want to know about it?
External Referees:
(3 referees) i saw all the shooting from my window. what do you want to know about it?
(2 referees) i remember that the gun fire was coming from the window on the second floor of assad?s
shop. the shop is only one story but there are apartments on top of the shop.
(1 referee) what is it you want to know about the incident?
Figure 1: An enriched dialogue turn from an Amani structured role play.
phrases collected per user utterance was 5.5.
Our 9 annotators had differing backgrounds, rang-
ing from transcribers and summer interns to experi-
enced NLP researchers. It should be noted that all
had at least some experience working with natural
language processing technologies. In future work,
we would like to explore using less experienced an-
notators for paraphrasing.
3.3 External referee annotation
Although the paraphrase generation step helps with
coverage of the language used by the commander
in our scenario, the combination of the original di-
alogues collected through structured role play and
the paraphrases do not address one crucial issue in
learning of data-driven dialogue policies, and their
automated evaluation: at each turn, a dialogue par-
ticipant has multiple valid dialogue acts that can be
performed, not a single correct one. In other words,
given the same dialogue history up to a given point,
multiple human dialogue participants following the
same underspecified policy may choose different di-
alogue acts to continue the dialogue, and each of
these different choices may be perfectly acceptable
and coherent. This is one of main challenges in cre-
ation and evaluation of data-driven policies, since
the exponentially many acceptable dialogue paths
are both difficult to model explicitly, and difficult
to recognize automatically when performed during
testing. Of course, the degree to which this is a prac-
tical problem in a specific dialogue scenario depends
on several factors, including how underspecified the
targeted dialogue policy is. In our case study, the
policy has a high level of underspecification, since
only behaviors related to the information in Table 1
are mentioned directly, and even those are only de-
scribed in natural language, without formal rigor.
The rest of the policy dictates only that human play-
ers in the part of Amani act according to their com-
monsense in playing the role of the Amani character.
However, we limit the otherwise potentially infinite
possibilities for dialogue behavior by strongly en-
couraging the Amani player to perform only one of a
set of predefined utterances corresponding to certain
dialogue acts in the scenario. In our experiments, the
number of utterances available for Amani was 96.
We first investigate this issue by attempting to
characterize the amount of human variation in the
choice of one of the 96 available dialogue acts at
any given point in a dialogue. To this end, we intro-
duce the idea of the external referee, who essentially
provides a ?second opinion? for dialogue acts per-
formed by the original role player. The external ref-
eree annotation task works as follows: (1) Starting
with an existing dialogue containing n utterances
42
?u1, u2, ..., un? for the participant whose utterances
will be externally refereed (one of the dialogues
collected through structured role play, in our case
study, where we externally referee the Amani utter-
ances), produce n dialogue histories h1, h2, ..., hn,
with each hi consisting of every utterance from each
dialogue participant from the beginning of the dia-
logue down to, but not including, the ith utterance in
the dialogue. (2) For each dialogue history hi, the
external referee (who must not be the person who
played a part in the original dialogue) chooses an
utterance u?i from the choices available for the sce-
nario, without knowledge of the original utterance
ui in the dialogue from which the history was pro-
duced.
Figure 1 provides an example of the choices made
by 6 external referees for a single target user ut-
terance. Given the previous dialogue history and
the target user utterance (can you tell me what you
know of the incident?), each external referee inde-
pendently chose a single best utterance for the char-
acter to respond with. In the example in the figure,
it can be seen that 3 of the 6 external referees chose
the same response as the original Amani player, as-
serting that Amani did indeed witness the incident
and asking what the commander would like to know.
The other three chose alternative responses; two
of these selected a response asserting information
about where the gun fire was coming from, while
a third referee chose a response simply asking what
the commander would like to know. It is important
to note that all three of these alternative responses
would be acceptable from a design and training per-
spective.
In this annotation task, the task is not to pro-
vide alternative dialogues, but simply one charac-
ter response to each individual utterance, assuming
the fixed history of the original dialogue. In other
words, the annotator has no control or impact over
the dialogue history at any point, and provides only
additional reference utterances for possible immedi-
ate continuations for each dialogue history. It is for
this reason we call the annotator an external referee.
Annotations from multiple external referees for
the dialogues collected through structured role play
do not result in a representation of the lattice of the
many possible dialogue paths in the scenario, but
rather an approximation that represents the possible
options in the immediate future of a given dialogue
history. The main difference is that the available his-
tories are limited to those in the original dialogues
from structured role play. While this may be a lim-
iting factor if one attempts to model dialogue be-
havior based on entire dialogue histories, since the
available histories represent only a very sparse sam-
ple of the space of valid histories, it is possible that
good approximate models can be achieved with fac-
torization of dialogues by sequences of a fixed num-
ber of consecutive turns, e.g. a model that makes a
second-order Markov assumption, considering only
the previous two turns in the dialogue as an approx-
imation of the entire history (Gandhe and Traum,
2007). This is in a way the same approximation used
in n-gram language models, but at the level of gran-
ularity of sentences, rather than words.
We collected annotations from 6 different exter-
nal referees, with each individual referee annotating
the entire set of 19 dialogues, and taking on average
about two hours to complete the annotation of the
entire set. All of our external referees were very fa-
miliar with the design of the Amani character, and
most had natural language processing expertise.
4 Evaluation of dialogue policies with
multiple external referees
4.1 External referee agreement
The dialogues and external referee annotations col-
lected using the procedure described in Section 3
provide a way to characterize the targeted policy
with respect to human variability in choosing utter-
ances from a fixed set, since the annotations include
the choices made by multiple external referees.
From the annotations of utterances chosen for
Amani in our 19 dialogues, we see that human an-
notators agree only 49.2% of the time when choos-
ing an utterance in the external referee framework.
That is, given the same dialogue history, we expect
that two human role players would agree on average
slightly less than 50% of the time on what the next
utterance should be1.
Based on this level of pairwise agreement, one
might conclude that using these data for either policy
learning or policy evaluation is a lost cause. How-
1This represents the averaged agreement over all pairs of
external referees.
43
010
20
30
1 2 3 4 5 6
U
tte
ra
n
ce
co
u
n
t(
%
)
Number of distinct utterance choices
Figure 2: Distribution in number of distinct choices by
external referees
ever, this result does not necessarily indicate that hu-
man raters disagree on what the correct choice is; it
is more likely to reflect that there are in fact mul-
tiple ?correct? (acceptable) choices, which we can
capture through multiple annotators.
The annotations from multiple external referees
in our case study support this view: Figure 2 shows
the number of distinct utterance choices made by
each of the six external referees for each specific ut-
terance in the 19 dialogues collected through struc-
tured role play. Each external referee chooses only
one utterance (out of 96 options) per Amani turn in
the 19 dialogues. Over the 296 Amani utterances
in the entire set of dialogues, all six referees agreed
unanimously on their utterance choice only 23.3%
of the time. The most frequent case, totaling almost
30% of all utterances, was that the set composed by
the single choice from each of the six wizards for
an utterance had exactly two distinct elements. For
only 1.3% of the 296 utterances did that set contain
the maximum number of distinct elements (six), in-
dicating complete disagreement among the external
referees. We note that, in this case, very low agree-
ment to complete disagreement reflects a situation
in dialogue where it is likely that there are many di-
alogue act choices considered acceptable by the col-
lective body of external referees. In our scenario,
there were at most two choices from the six referees
for more than 50% of the Amani turns, indicating
that in the majority of the cases there is only a small
set of acceptable dialogue acts (from the 296 avail-
able), while five or more options were chosen for
less than 10% of all Amani turns.
For a more direct characterization of dialogue sce-
narios, and also for the purposes of evaluation, we
40
50
60
70
80
90
1 2 3 4 5 6 7
W
ea
k
ag
re
em
en
t(
%
)
Number of external referees
bc
bc
bc
bc
bc
Figure 3: Weak agreement between external referees
now define a metric that reflects overall agreement
in a group of external referees. Instead of compar-
ing one choice from a single referee to another single
choice, we instead check for membership of a single
choice cij from a single referee Ri for utterance uj
in the set of choices {ckj |k 6= i} from all of the
other referees {Rk|k 6= i}. In the positive case, we
say that Ri weakly agrees with the rest of the raters
{Rk|k 6= i} on the annotation of utterance uj . We
define the weak agreement agrn for a set of N ex-
ternal referees over a set of m utterances to be rate
at which each rater Ri weakly agrees with the n? 1
raters {Rk|k 6= i}, for all integer values of i ranging
from 1 to N , inclusive. Intuitively, weak agreement
reflects two important questions: (1) how often is
the choice of a referee supported by the choice of
at least one more referee? and (2) given a set of
n ? 1 referees, how much new information (in the
form of unseen choices) should I expect to see from
a new nth referee? Figure 3 addresses these ques-
tions for the scenario in our case study by showing
the weak agreement figures obtained for sets of in-
creasing numbers of external referees, from 2 to 6.
Each point in the graph corresponds to the average
of the weak agreement values obtained for all possi-
ble ways of holding out one external referee Ri, and
computing the weak agreement between Ri and the
other referees, assuming an overall pool containing
the given number of external referees.
We note that with the dialogue act choices of a
single person, coverage of the possible acceptable
options is quite poor, corresponding only to an aver-
age of 50% of the choices made by another person.
44
The coverage increases rapidly as two more external
referees are added, and more slowly, although still
steadily from there. The rightmost point in Figure 3
indicates that with a set of five external referee we
should expect to cover almost 80% of the choices of
a sixth referee.
4.2 Dialogue policy evaluation with multiple
external referees
The weak agreement metric defined in the previ-
ous section can be used to measure the quality of
automatically learned policies, and to provide in-
sight into how a learned policy compares to human-
level performance. Because it recognizes the valid-
ity of multiple responses, the weak agreement metric
can help distinguish true policy errors from policy
choices that are consistent with the intuitions of at
least some human referees about what the character
should say.
In particular, given the choices made by five exter-
nal referees for our 19 Amani dialogues, we can ex-
pect their choices to cover about 80% of the choices
a sixth person would make for what Amani should
say at each turn in these dialogues. (I.e., we know
that the weak agreement among a group of six hu-
man referees is about 80% for this Amani scenario.)
We proceed to rate the quality of an automatic
policy by computing a one-vs-others version of
weak agreement?intuitively treating our policy as
if it were such a ?sixth person?, and comparing it
to the other five. Instead of computing the average
weak agreement for referees randomly selected from
an entire group, as in the previous section, to eval-
uate a policy, we compute its weak agreement com-
pared to the combined set of human external refer-
ees, as follows. For every system utterance uj in our
set of role play dialogues, a given automatic policy
P is used to select a response c?j (corresponding to
a dialogue act in the domain). We then check for
membership of c?j in the set that contains only and
all dialogue act choices ckj for k ranging from 1 to
N , inclusive, where N is the number of external ref-
erees and ckj corresponds to the kth referee?s choice
for the jth utterance. Another way to interpret this
evaluation metric is to consider it a form of accuracy
that computes the number of correct choices made
by the policy divided by the total number of choices
made by the policy, where a choice is considered
?correct? if it matches any of the external referees?
choices for a specific utterance. For this reason, we
refer to this evaluation-focused one-vs-all version of
weak agreement as weak accuracy.
Based on the definition above, an automatic pol-
icy with quality indistinguishable from that of a
person choosing utterances for the Amani character
would have a weak accuracy of about 80% or higher
when measured using a set of five external referees.
We see then that this metric is far from perfect, since
it cannot rank two policies with weak accuracy lev-
els of, say, 80% and 90%. It is also possible for a
policy that results in dialogue behavior noticeably
inferior to that of a human referee to be rated at
the same weak accuracy value for a human referee
(80%). In practice, however, weak accuracy with
five or six external referees has far greater power for
discriminating between policies of varying quality,
and ranking them correctly, than a naive version of
accuracy, which corresponds to weak accuracy us-
ing a single referee. Furthermore, the addition of
only a few more external referees would very likely
increase the efficacy of the weak agreement metric.
Despite the shortcomings of weak accuracy as a
metric for evaluation of quality of dialogue poli-
cies, it opens up a wide range of opportunities for
development of learned policies. Without an auto-
mated metric, development of such techniques can
be only vaguely incremental, relying on either costly
or, more likely, infrequent human evaluations with
results that are difficult to optimize toward with cur-
rent machine learning techniques. The use of im-
perfect automated metrics in situations where ideal
metrics are unavailable or are impractical to deploy
is fairly common in natural language processing.
PARSEVAL (Abney et al, 1991), commonly used
for parser evaluation, and BLEU (Papineni et al,
2002), commonly used in machine translation, are
two examples of well-known imperfect metrics that
have been the subject of much criticism, but that are
widely agreed to have been necessary for much of
the progress enjoyed by their respective fields. Un-
like BLEU, however, which has been shown to cor-
relate with certain types of human judgment on the
quality of machine translation systems, our notion
of weak accuracy has not yet been demonstrated to
correlate with human judgments on the quality of di-
alogue policies, and as such it is only hypothesized
45
to have this property. We leave this important step
of validation as future work.
5 Learning dialogue policies from
examples without formal modeling
Equipped with a dataset with 19 dialogues in the
Amani scenario (including paraphrases for the un-
constrained commander utterances, and external ref-
eree annotations for the constrained Amani utter-
ances), and an automatic evaluation framework for
distinguishing quality differences in learned poli-
cies, we now describe our experiments on learning
dialogue policies from data collected in structured
role play sessions, and enriched with paraphrases
and external referee annotations.
In each of our experiments we attempt to learn
a dialogue policy as a maximum entropy classi-
fier (Berger et al, 1996) that chooses one utterance
out of the 96 possible utterances for Amani after
each commander utterance, given features extracted
from the dialogue history. This policy could be in-
tegrated in a dialogue system very easily, since it
chooses system utterances directly given previous
user and system utterances. We evaluate the dia-
logue policies learned in each experiment through
19-fold cross-validation of our set of 19 dialogues:
in each fold, we hold out one dialogue (and all of its
related information, such as external referee anno-
tations and user utterance paraphrases) and use the
remaining 18 dialogues as training data.
5.1 Learning from examples
Using only the dialogues collected in structured role
play sessions, and no additional information from
external referees or paraphrases, we train the maxi-
mum entropy classifier to choose a system utterance
si based on features extracted from the two previous
user utterances ui and ui?1 and the previous system
utterance si?1. The features extracted from these ut-
terances are the words present in each user utterance,
and the complete text of each system utterance. Low
frequency words occurring fewer than 5 times in the
corpus are excluded.
The weak accuracy for this simple policy is 43%,
a low value that indicates that for more than half its
turns the policy chooses an utterance that was not
chosen by any of the referees, giving us a reasonable
level of confidence that this policy is of poor quality.
5.2 Enhanced training with external referees
The next experiment expands the training set avail-
able to the maximum entropy classifier by adding
training instances based on the utterances chosen by
the external referees. For each of the training in-
stances (target utterance coupled with features from
ui, si?1 and ui?1) we add six new training instances,
each using the same features as the original train-
ing instance, but replacing the target class with the
choice made by an external referee. Note that this
creates identical training instances for cases when
the same utterance is chosen by multiple annotators,
which has the effect of weighting training examples.
With the additional information, weak accuracy for
this policy improves to 56%, which is a large gain
that still results in a mediocre dialogue policy.
5.3 Expanding training examples with
paraphrases
To help determine how much of difficulty in our
policy learning task is due to the related problem
of natural language understanding (NLU), and how
much is due to modeling dialogue behavior regard-
less of NLU, we performed manual annotation of
dialogue acts for the user utterances, and trained a
policy as in the previous section, but using manu-
ally assigned dialogue acts instead of the words for
user utterances in the dialogue history. With this
gold-standard NLU, weak accuracy improves from
56% to 67%, approaching the level of human perfor-
mance, and already at a level where two out of every
three choices made by the learned policy matches
the choice of a human referee.
To bridge the gap between learning purely from
surface text (with no formal modeling) and learn-
ing from manually assigned dialogue acts specifi-
cally designed to capture important information in
the scenario, we turn to the paraphrases collected
for user utterances in our 19 dialogues. These para-
phrases are used to create additional synthetic train-
ing material for the classifier, as follows: for each
training instance produced from a chosen system ut-
terance si and previous utterances ui, si?1 and ui?1
(see previous section), we create additional training
instances keeping the target system utterance si and
previous system utterance si?1 the same, but using
46
a paraphrase u?i in the place of ui, and a paraphrase
u?i?1 in the place of ui?1. Training instances are
added for all possible combinations of the available
paraphrases for ui and ui?1, providing some (arti-
ficial) coverage for parts of the space of possible
dialogue paths that would be otherwise completely
ignored during training.
Training the classifier with material from the ex-
ternal referees (see previous section) and additional
synthetic training examples from paraphrases as de-
scribed above produces a dialogue policy with weak
accuracy of 66%, at the same level as the policy
learned with manually assigned speech acts. It is
noteworthy that this was achieved through a very
simple and intuitive paraphrase annotation task that
requires no technical knowledge about dialogue sys-
tems, dialogue acts or domain modeling. As men-
tioned in section 3.2, paraphrases for each of the 19
dialogues were generated in less than 30 minutes on
average.
6 Conclusion and future work
We introduced a framework for collection and en-
richment of scenario-specific dialogues based only
on tasks that require no technical knowledge. Data
collected in this framework support novel ap-
proaches not just for learning dialogue policies,
but perhaps more importantly for evaluating learned
policies, which allows us to examine different tech-
niques using an objective automatic metric.
Although research on both learning and evalu-
ating dialogue policies is still in early stages, this
case study and proof-of-concept experiments serve
to illustrate the basic ideas of external referee and
paraphrase annotation, and the use of multiple refer-
ence dialogue act choices in evaluation of dialogue
policies, in a way similar to how multiple reference
translations are used in evaluation of machine trans-
lation systems. We do not consider this line of re-
search a replacement for or an alternative to for-
mal modeling of domains and dialogue behavior,
but rather as an additional tool in the community?s
collective arsenal. There are many unexplored av-
enues for including data-driven techniques within
rule-based frameworks and vice-versa.
In future work we intend to further validate the
ideas presented in this paper by performing addi-
tional collection of dialogues in the Amani domain
to serve as a virgin test set, and applying these
techniques to other dialogue domains and scenar-
ios. We also plan to refine the weak accuracy and
weak agreement metrics to take into account the
level of agreement within utterances to reflect that
some parts of dialogues may be more open-ended
than others. Finally, we will conduct human evalu-
ations of different policies to begin validating weak
accuracy as an automatic metric for evaluation of di-
alogue policies.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank Ron Artstein,
Sudeep Gandhe, Fabrizio Morbini, Angela Nazar-
ian, Susan Robinson, Michael Rushforth, and David
Traum.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In E. Black, editor, Proceedings of the
workshop on Speech and Natural Language, HLT ?91,
pages 306?311, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
Sudeep Gandhe and David R. Traum. 2007. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07, 08/2007.
Sudeep Gandhe and David R. Traum. 2010. I?ve said it
before, and i?ll say it again: An empirical investigation
47
of the upper bound of the selection approach to dia-
logue. In 11th annual SIGdial Meeting on Discourse
and Dialogue.
Sudeep Gandhe, Nicolle Whitman, David R. Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Sina Jafarpour, Chris Burges, and Alan Ritter. 2009. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
In Proceedings of the NIPS Workshop on Advances in
Ranking.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-
ton Leuski, and Albert A. Rizzo. 2007. Virtual pa-
tients for clinical therapist skills training. In Proceed-
ings of the 7th international conference on Intelligent
Virtual Agents, IVA ?07, pages 197?210, Berlin, Hei-
delberg. Springer-Verlag.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Linus Sellberg and Arne Jnsson. 2008. Using ran-
dom indexing to improve singular value decomposi-
tion for latent semantic analysis. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco, may.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-
based approach to help-desk response generation.
Computational Intelligence for Modelling, Control
and Automation, International Conference on, 1:23.
48
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 279?285,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Approach to the Automated Evaluation of Pipeline Architectures in
Natural Language Dialogue Systems
Eliza Margaretha? and David DeVault
USC Institute for Creative Technologies, 12015 Waterfront Dr., Playa Vista, CA 90094
elizam@coli.uni-saarland.de
devault@ict.usc.edu
Abstract
We present an approach to performing auto-
mated evaluations of pipeline architectures in
natural language dialogue systems. Our ap-
proach addresses some of the difficulties that
arise in such automated evaluations, includ-
ing the lack of consensus among human an-
notators about the correct outputs within the
processing pipeline, the availability of multi-
ple acceptable system responses to some user
utterances, and the complex relationship be-
tween system responses and internal process-
ing results. Our approach includes the devel-
opment of a corpus of richly annotated tar-
get dialogues, simulations of the pipeline pro-
cessing that could occur in these dialogues,
and an analysis of how system responses vary
based on internal processing results within the
pipeline. We illustrate our approach in two im-
plemented virtual human dialogue systems.
1 Introduction
Natural language dialogue systems are typically im-
plemented as complex modular systems, with a
range of internal modules performing tasks such
as automatic speech recognition (ASR), natural
language understanding (NLU), dialogue manage-
ment (DM), natural language generation (NLG),
and speech synthesis (TTS). A common design is
for systems to adopt a pipeline architecture. In a
pipeline, each user utterance is processed in a se-
ries of successive processing steps, with the output
of each module serving as the input of the next mod-
ule, until the system?s response is determined.
?Now at Saarland University, Germany.
While there are many approaches to dialogue sys-
tem evaluation (see e.g. (Walker et al, 1997; Eck-
ert et al, 1997; Walker, 2005)), in many ways, the
primary data for assessing the performance of a di-
alogue system comes from the collection of live in-
teractive dialogues between an implemented system
and members of its intended user population. Yet,
live dialogue-based evaluation suffers from a num-
ber of limitations and drawbacks. Each dialogue set
can be expensive and time-consuming to collect, and
may only reflect a specific version of a system under
active development. Additional effort is also gener-
ally necessary to identify specific system responses
as problematic or unacceptable. Further annotation
and analysis is then necessary to diagnose and pin-
point the cause of the problematic responses, so that
the relevant pipeline module(s) may be improved.
In this paper, we present and discuss an approach
to performing automated evaluations of pipeline ar-
chitectures. Our approach involves the development
of a corpus of annotated target dialogues, starting
from Wizard-of-Oz data. Our automated evaluation
assesses the support for these target dialogues in a
pipeline system architecture. It is not designed as a
substitute for live system evaluations, but rather as
a complement to them which may help to alleviate
some of these challenges to understanding system
performance and streamlining development. In par-
ticular, unlike the PARADISE framework (Walker
et al, 1997), which aims to evaluate dialogue agent
strategies ? by relating overall user satisfaction to
various other metrics (task success, efficiency mea-
sures, and qualitative measures) ? our approach
takes the agent?s dialogue strategy for granted (in
279
utterance
user
typed
responsesystem(speech act)NLU userspeech act DM
Figure 1: Simplified pipeline architecture.
the form of a set of target dialogues that exemplify
the desired strategy), and instead zooms in and aims
to directly evaluate the dialogue system?s module
pipeline. Specifically, our approach quantifies the
ability of the pipeline to replicate the processing
steps needed to reproduce a set of target responses.
In our analysis, we place a special emphasis on the
possible lack of consensus among human annotators
about what the processing results should be. We do
not aim to further analyze the system?s live dialogue
behavior in terms of user satisfaction, task success,
or other global measures.
2 Research Setting
The work presented in this paper has been designed
to support the dialogue behavior of two virtual hu-
man systems, the SimCoach and Tactical Ques-
tioning (TACQ) systems. SimCoach (Rizzo et al,
2011) is an on-going project aiming at empower-
ing military personnel and their significant others
with online healthcare assistance for Post-Traumatic
Stress Disorder (PTSD), depression, and family-
related problems. The SimCoach character encour-
ages users to talk about any concerns or problems
they may have. TACQ (Gandhe et al, 2008) is de-
signed to support simulation and training for tactical
questioning skills, and provides virtual humans who
have information but will not answer certain ques-
tions unless the user cooperates by agreeing to their
requests, offering promises in their favor, and so on.
In this work, we have developed target dialogues for
the Amani character, who has been an eyewitness of
a recent shooting incident.
For simplicity, in the experiments reported in this
paper, we have used simplified versions of these two
dialogue systems. The simplification removes ASR
from TACQ,1 and removes NLG and TTS from both
systems. This yields a simple two-module pipeline
architecture that we depict in Figure 1. Note that
the input to NLU is a typed English utterance, and
1SimCoach always uses an instant messaging style typed in-
put interface.
the output of the NLU module (also the input to the
DM module) is a speech act representation. The out-
put of the DM, which we treat here as the system?s
response to the user, is also a speech act represen-
tation. Both of these systems use statistical classi-
fication models for NLU (Leuski and Traum, 2010;
Sagae et al, 2009), and finite state machine models
for DM (Gandhe et al, 2008; Rizzo et al, 2011).
3 Target Dialogues
Target dialogues are annotated versions of dialogues
a system designer would like the system to support.
3.1 Developing Target Dialogues
Wizard-of-Oz (WoZ) and role play dialogues pro-
vide valuable data to designers of dialogue systems,
especially in the form of natural dialogue data and
insights into human-level performance and strate-
gies for the specific dialogue task. However, in prac-
tice, system builders may not be able to implement
all of the strategies and competences of the wizards
or role players, and simplifications may be needed.
SimCoach target dialogues were developed from
a collection of 10 WoZ dialogues in which clini-
cians (wizards) and veterans (users) interacted with
each other. We also built Amani target dialogues for
TACQ starting from 19 WoZ dialogues. Each user
utterance and wizard?s response was annotated with
a target NLU speech act and one or more target DM
speech acts (i.e., the system response).2 The 10 Sim-
Coach target dialogues contain 376 user utterances
and 547 target system response speech acts. The 19
Amani target dialogues contain 317 user utterances
and 354 target system response speech acts. For ex-
cerpts of the SimCoach and Amani target dialogues,
see Tables A.1 and A.2 in the Appendix.
To create our target dialogues, we adjusted the
WoZ dialogues to reflect a number of system de-
sign limitations as well as wizard deviations from
the desired dialogue policy. These changes included
removing unsupported wizard utterances and sub-
dialogues, inserting or reordering system responses
due to wizard mistakes, and introducing clarification
subdialogues for unsupported user utterances.
2For both SimCoach and TACQ, the DM may generate one
or multiple speech acts in response to a user utterance.
280
3.2 Formalizing Target Dialogues
Let P = ?p1, ..., pk? be the pipeline in a system con-
taining k modules. We use St to denote the pipeline
state, which includes the internal states of any mod-
ules that maintain an internal state, at time t.
For a user input xt that occurs at time t, when
the pipeline state is St, we write A(P, St, xt) =
?y1, ..., yk? to represent the actual sequence of out-
puts from the pipeline modules, where yi is the out-
put of module pi for i = 1...k.
For a variety of reasons, these actual module out-
puts may differ from the target module outputs for
this input and pipeline state. Let T (P, St, xt) =
?z1, ..., zk? be the target pipeline response to input
xt, i.e. the sequence of target outputs from each of
the pipeline modules.
A target dialogue D = ?(x1, T1), ..., (xN , TN )?,
then, is a sequence of user inputs and corresponding
target pipeline responses. Specifically, for time t =
1...N , Tt = T (P, S?t , xt) = ?z1, ..., zk? is the target
pipeline response to input xt, where S?t is the target
pipeline state at each time t.
An important detail is that the target pipeline state
S?t is the state that the pipeline would be in if all
previous user inputs had triggered exactly the tar-
get pipeline responses. Formally, let S?1 be the ini-
tial state of the dialogue system pipeline. Then, let
S?t+1 = update(S
?
t , xt, Tt), where we use an update
function to capture the effect on the internal state of
the pipeline of the target response Tt to xt. Note that
the target pipeline state may differ from the actual
pipeline state, if an actual pipeline response differs
from the target pipeline response. For example, if
a previous user utterance was misunderstood by an
NLU module, then at run-time, the actual informa-
tion state inside the DM module would reflect this
earlier misunderstanding, while the target pipeline
state would include a corrected version of the in-
formation state. Using corrected information states,
and corrected pipeline states more generally, enables
the utterances within a target dialogue to be consid-
ered independently in a pipeline evaluation.3
We can say that a pipeline P is compatible with
3It also highlights how our pipeline evaluation results do not
translate directly into performance metrics for live dialogues,
as deviations and errors in system responses in live dialogues
may affect the subsequent interaction in ways that are difficult
to predict and deviate substantially from the target dialogues.
User Utterance NLU Speech Act DM Response
Having difficulty
sleeping... bad
dreams.. Wake up
a few times every
night
answer.observable.
sleeping-problems
question.
depression-pre-
check-list.1
answer.observable.
wakeup-generic
question.
depression-pre-
check-list.1
answer.observable.
wakeup-nightmare
question.
ptsd-pre-checklist.1
Table 1: Sample of Different NLU Speech Acts
a target dialogue D = ?(x1, T1), ..., (xN , TN )? iff
A(P, S?t , xt)[k] = Tt[k] for all t = 1...N . In other
words, for every user utterance, the actual system
response, as emitted by the last (kth) module in the
pipeline, matches the target system response.4 Both
the SimCoach and TACQ pipelines are compatible
in this sense with their target dialogues (Section 3.1).
3.2.1 Addressing the Lack of Consensus
A considerable challenge in the improvement of
pipeline performance is the lack of consensus about
the desired internal processing steps: different sys-
tem designers or human annotators often disagree
about what the intermediate results should be. For
example, in a system such as TACQ or SimCoach,
there may be substantial disagreement among hu-
man annotators about the correct NLU output for
each utterance; see e.g. (Artstein et al, 2009). Table
1 exemplifies 3 different possible NLU speech act
annotations for a user utterance to SimCoach. Note
that for the first two, the DM outputs the same sys-
tem response (which incidentally is the target re-
sponse). However, the third speech act yields a
different response. In our automated evaluations,
rather than trying to resolve all disagreements, our
approach is to characterize the frequency with which
these kinds of phenomena occur in the pipeline.
To support this analysis, for a target dialogue
D = ?(x1, T1), ..., (xN , TN )?, we assume then that
each input xt is associated not only with the target
pipeline response Tt, but also with a collection of an-
notations At = ?a1, ..., ak?. These annotations may
be derived from a number of independent sources
4A technical detail: for both SimCoach and TACQ, the DM
sometimes emits multiple speech acts; to accommodate these
cases, for now we treat the target DM output as a set of speech
acts A, and count each actual output DM speech act as an in-
dependent match if it matches any speech act in A (ignoring
order). A more complex matching scheme could be employed.
281
S = {s1, ..., sl}, and we write ai(s) = wi to denote
the correct output wi for module pi according to an-
notation source s ? S . These independent ?anno-
tation sources? might be human annotators, or com-
peting module algorithms, for example.
We can then capture the hypothetical effect of us-
ing annotation source s in place of some module pi
within the pipeline. To do so, we consider the effect
of replacing the output of module pi with ai(s), and
using this as the input to subsequent modules in the
pipeline. Let P ki+1 = ?pi+1, ..., pk? be the remainder
of the pipeline, starting at module pi+1. For input
xt, we can notate the hypothetical pipeline response,
if module i were replaced by annotation source s,
by H(P ki+1, S
?
t , ai(s)) = ?yi+1, ..., yk?. We will write
hs\it for the hypothetical system response to the user
input at time t, if source s were substituted for the
output of module i: hs\it = H(P ki+1, S
?
t , ai(s))[k] =
yk. For a target dialogue of length N , we can sum-
marize the frequency with which the hypothetical
pipeline response would match the target system re-
sponse by a performance measure:
Pstrict =
1
N
N?
t=1
match(hs\it , Tt[k])
where match(x, y) = 1 if x = y and 0 otherwise.5
A second form of lack of consensus issue is the
existence of multiple acceptable system responses
within a system. Returning to the example in Ta-
ble 1, system designers might decide that either of
the two system responses here would be accept-
able. In some cases, actual NLU outputs which dif-
fer from the target NLU output will simply result in
the system giving alternative acceptable system re-
sponses, as in this example. In other cases, they may
lead to unacceptable system responses.
We measure the frequency with which these phe-
nomena occur as follows. For a target dialogue
D = ?(x1, T1), ..., (xN , TN )?, let each input xt be
associated with a set Rt = {r1, ..., rm} of system
responses which differ from the target system re-
sponse Tt[k], but are also acceptable in design terms.
Given these alternative responses, we can then de-
fine a more permissive performance measure:
Pmultiple =
1
N
N?
t=1
match(hs\it , Tt[k], Rt)
5This strict agreement measure can be easily generalized to
measure the proportion of matches in a set of target dialogues.
NLU
speech act
source
Percent of NLU
speech acts
identical to...
(N=317)
Percent of system
response speech
acts identical to...
(N=354)
the
target
NLU
speech
act
(target)
the target
or other
acceptable
NLU
speech act
(humanall)
a target
system
response
speech
act
a target or
acceptable
system
response
speech act
target 100% 100% 99.4% 100%
human1 79.3% 95.4% 84.2% 88.4%
human2 76.7% 99.7% 86.7% 93.8%
human3 59.3% 90.2% 69.6% 78.8%
NPCEditor 42.3% 50.5% 55.3% 57.4%
Table 2: TACQ Amani Evaluation Results
where
match(hs\it , Tt[k], Rt) =
?
??
??
1 if hs\it = Tt[k]
1 if hs\it ? Rt
0 otherwise
.
4 Results
4.1 Annotations and Results for TACQ
We collected a range of annotations for the 19 TACQ
Amani target dialogues, including 6 sources of NLU
speech acts for the 317 user utterances: target (the
target NLU speech act for each utterance); 3 inde-
pendent human annotations of the best NLU speech
act for each utterance; humanall (a set containing
all of the alternative acceptable NLU speech acts
for each utterance, according to the same single re-
searcher who prepared target); and NPCEditor, the
NLU speech act output from NPCEditor (Leuski and
Traum, 2010), the NLU module for TACQ.
We analyzed the effect of differing NLU speech
act sources on the responses given by the system.
We present the results in Table 2. (For a de-
tailed processing example, see Table A.2 in the Ap-
pendix.) The first (leftmost) column of numbers
shows the percentage of NLU speech acts from each
source that are identical to the target NLU speech
act. These results highlight how human annotators
do not always agree with each other, or with the
target. The agreement among the human annota-
tors themselves, measured by Krippendorf?s alpha
(Krippendorff, 2007) is 0.599 (see also (Artstein et
al., 2009)). In the second column of numbers, we
tabulate the frequency with which the NLU speech
acts are present in humanall. While these numbers
282
are higher, they do not reach 100% for the human
annotators, suggesting that a single annotator is un-
likely to be able to circumscribe all the NLU speech
acts that other annotators might find acceptable.
Despite the frequent disagreements among human
annotators, this evaluation shows that the impact on
the target system responses is less than might be ex-
pected. In the third column of numbers, we calculate
Pstrict which measures the effect of using each of
NLU sources, in place of the NLU module?s actual
output, on the pipeline?s ability to produce the tar-
get response. As the table implies, the pipeline often
produces the target system response (third column)
even when the NLU source disagrees with the target
(first column). Indeed, for all the NLU sources ex-
cept for target, the pipeline is significantly more
likely to produce the target system response than the
NLU source is to produce the target NLU speech act
(Wilcoxon test, p < 0.001 for each source).
We also calculate Pmultiple (last column) which
measures the effect of using each NLU source on
the pipeline?s ability to produce either the target or
any other acceptable system response. As the ta-
ble shows, the actual system responses are often ac-
ceptable when they differ from the target responses.
Although this effect seems weaker for NPCEditor,
Wilcoxon tests reveal that for every source other
than target, the differences between Pstrict and
Pmultiple are significant at p < 0.005. This evalu-
ation confirms that the pipeline is significantly more
likely to deliver an acceptable system response than
a target response, and helps quantify to what ex-
tent NLU outputs that differ from the target remain
problematic for the pipeline performance.
4.2 Annotations and Results for SimCoach
We gathered a set of annotations for the 10 Sim-
Coach target dialogues, including 3 sources of NLU
speech acts for the 376 user utterances: target,
human1, and mxNLU (the NLU speech act output
from mxNLU (Sagae et al, 2009), the NLU mod-
ule for SimCoach). We present the evaluation re-
sults in Table 3. As the table shows, our indepen-
dent human annotator often disagreed with the target
NLU speech act. Despite the 72.1% agreement rate,
the system?s response to the human NLU speech act
agreed with the target response 93.3% of the time.
In comparison, mxNLU shows somewhat higher
NLU speech
act source
NLU speech acts
identical to target
(N = 376)
System response
speech acts identical
to target (N = 547)
target 100% 100%
human1 72.1% 93.3%
mxNLU 75.3% 91.1%
Table 3: SimCoach Evaluation Results
agreement (75.3%) with the target NLU annotation.
While this might at first suggest ?super-human?
NLU performance, in reality it is because the target
NLU annotation was constructed in very close con-
sultation with the training data for mxNLU.6 Despite
showing higher agreement with target NLU speech
acts, the system responses were not more likely to
match the target system responses with mxNLU.
The explanation is that disagreements for mxNLU
were more serious, reflecting more misunderstand-
ings and failures to understand than occur with a hu-
man annotator, and more deviations from the target
responses. This highlights the value of looking be-
yond the performance of individual modules.
5 Conclusions and Future Work
We have presented an approach to performing au-
tomated evaluations of pipeline architectures, and
demonstrated its application in two implemented
virtual human dialogue systems. The pipeline eval-
uation provided several insights into the current
pipeline performance, including what performance
would be attainable if human-level NLU were possi-
ble. In future work, we would like to expand beyond
our simplified two-module pipeline, and investigate
the connection between our automated pipeline eval-
uations and performance in live dialogues.
Acknowledgments
We thank our reviewers, Sudeep Gandhe, Fab-
rizio Morbini, and David Traum. The project
effort described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
6The exact target dialogue utterances were not in the
mxNLU training data, but similar utterances were inspected in
constructing the target dialogues.
283
References
R. Artstein, S. Gandhe, M. Rushforth, and D. Traum.
2009. Viability of a simple dialogue act scheme for
a tactical questioning dialogue system. In SemDial
Workshop, pages 43?50.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User mod-
eling for spoken dialogue system evaluation. In Proc.
IEEE ASR Workshop, pages 80?87.
S. Gandhe, D. DeVault, A. Roque, B. Martinovski,
R. Artstein, A. Leuski, and et al 2008. From do-
main specification to virtual humans: An integrated
approach to authoring tactical questioning characters.
Proceedings of Interspeech-08.
K Krippendorff. 2007. Computing krippendorff?s alpha-
reliability, June.
A. Leuski and D. Traum. 2010. NPCEditor: A tool for
building question-answering characters. In LREC.
A. Rizzo, B. Lange, J.G. Buckwalter, E. Forbell, J. Kim,
K. Sagae, J. Williams, B.O. Rothbaum, J. Difede,
G. Reger, T. Parsons, and P. Kenny. 2011. An intel-
ligent virtual human system for providing healthcare
information and support. In et al Westwood, J.D., ed-
itor, Technology and Informatics. IOS Press.
K. Sagae, G. Christian, D. DeVault, and D. R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Short Paper Proceedings of NAACL HLT.
M. A. Walker, D. J. Litman, C. A. Kamm, A. A. Kamm,
and A. Abella. 1997. Paradise: A framework for eval-
uating spoken dialogue agents. pages 271?280.
M. A. Walker. 2005. Can we talk? methods for evalu-
ation and training of spoken dialogue systems. Lan-
guage Resources and Evaluation, 39(1):pp. 65?75.
284
Appendix
t User Utterance (xt) Target NLU Speech
Act (t1)
Target System
Response (t2)
Textual Version of Target System Response
9 my husband seems distant,
and we have been arguing a
lot more lately
answer.observable.
family-problem
question.bio-info.
has-kids
Does he have children?
10 yes, 2 answer.yes question.family-
pre-checklist.6
In his family, do people collaborate together to
find the best way to solve problems?
Table A.1: Excerpt from a SimCoach Target Dialogue.
t User Utterance (xt) Source of NLU
Speech Act
NLU Speech Act (gloss) System Response Speech Acts (gloss)
1 hi amani. NPCEditor hello hello
target NLU hello hello
2 i wanted to talk to you about
the recent shooting that
occurred
NPC Editor Tell me more about the_ incident location of the_incident is the_shop
target NLU Is amani willing to talk? amani is willing to talk
3 do you know who was
responsible?
NPC Editor What is perpetrator of the_ incident ? perpetrator of the_incident is Saif
target NLU What is name of strange_man ? player should offer ?give-safety?
Table A.2: Excerpt from a TACQ target dialogue, including pipeline module processing.
285
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 131?133,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Demonstration of Incremental Speech Understanding and Confidence
Estimation in a Virtual Human Dialogue System
David DeVault and David Traum
Institute for Creative Technologies
University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094
{devault,traum}@ict.usc.edu
1 Overview
This demonstration highlights some emerging ca-
pabilities for incremental speech understanding and
processing in virtual human dialogue systems. This
work is part of an ongoing effort that aims to en-
able realistic spoken dialogue with virtual humans in
multi-party negotiation scenarios (Plu?ss et al, 2011;
Traum et al, 2008). In these negotiation scenarios,
ideally the virtual humans should demonstrate fluid
turn-taking, complex reasoning, and appropriate re-
sponses based on factors like trust and emotions. An
important component in achieving this naturalistic
behavior is for the virtual humans to begin to un-
derstand and in some cases respond in real time to
users? speech, as the users are speaking (DeVault
et al, 2011b). These responses could include rel-
atively straightforward turn management behaviors,
like having a virtual human recognize when it is be-
ing addressed and turn to look at the user. They
could also include more complex responses such as
emotional reactions to what users are saying.
Our demonstration is set in an implemented ne-
gotiation domain (Plu?ss et al, 2011) in which two
virtual humans, Utah and Harmony (pictured in Fig-
ure 1), talk with two human negotiation trainees,
who play the roles of Ranger and Deputy. The di-
alogue takes place inside a saloon in an American
town in the Old West. In this scenario, the goal of the
two human role players is to convince Utah and Har-
mony that Utah, who is currently the local bartender,
should take on the job of town sheriff. We presented
a substantially similar demonstration of this scenario
in (DeVault and Traum, 2012).
Figure 1: SASO negotiation in the saloon: Utah (left)
looking at Harmony (right).
To support more natural behavior in such negotia-
tion scenarios, we have developed an approach to in-
cremental speech understanding. The understanding
models are trained using a corpus of in-domain spo-
ken utterances, including both paraphrases selected
and spoken by system developers, as well as spo-
ken utterances from user testing sessions (DeVault
et al, 2011b). Every utterance in the corpus is an-
notated with an utterance meaning, which is repre-
sented using a frame. Each frame is an attribute-
value matrix (AVM), where the attributes and val-
ues represent semantic information that is linked to
a domain-specific ontology and task model (Traum,
2003; Hartholt et al, 2008; Plu?ss et al, 2011). The
AVMs are linearized, using a path-value notation, as
seen at the lower left in Figure 2. Our framework
uses this corpus to train two data-driven models, one
for incremental natural language understanding, and
a second for incremental confidence modeling. We
briefly summarize these two models here; for addi-
tional details and motivation for this framework, and
discussion of alternative approaches, see (DeVault et
al., 2011b; DeVault et al, 2011a).
The first step is to train a predictive incremental
understanding model. This model is based on maxi-
131
mum entropy classification, and treats entire individ-
ual frames as output classes, with input features ex-
tracted from partial ASR results, calculated in incre-
ments of 200 milliseconds (DeVault et al, 2011b).
Each partial ASR result serves as an incremental in-
put to NLU, which is specially trained for partial
input as discussed in (Sagae et al, 2009). NLU is
predictive in the sense that, for each partial ASR re-
sult, the NLU module tries to output the complete
frame that a human annotator would associate with
the user?s complete utterance, even if that utterance
has not yet been fully processed by the ASR.
The second step in our framework is to train a set
of incremental confidence models (DeVault et al,
2011a), which allow the agents to assess in real time,
while a user is speaking, how well the understand-
ing process is proceeding. The incremental confi-
dence models build on the notion of NLU F-score,
which we use to quantify the quality of a predicted
NLU frame in relation to the hand-annotated correct
frame. The NLU F-score is the harmonic mean of
the precision and recall of the attribute-value pairs
(or frame elements) that compose the predicted and
correct frames for each partial ASR result.
Each of our incremental confidence models
makes a binary prediction for each partial NLU re-
sult as an utterance proceeds. At each time t dur-
ing an utterance, we consider the current NLU F-
Score Ft as well as the final NLU F-Score Ffinal
that will be achieved at the conclusion of the utter-
ance. In (DeVault et al, 2009) and (DeVault et al,
2011a), we explored the use of data-driven decision
tree classifiers to make predictions about these val-
ues, for example whether Ft ? 12 (current level ofunderstanding is ?high?), Ft ? Ffinal (current level
of understanding will not improve), or Ffinal ? 12(final level of understanding will be ?high?). In
this demonstration, we focus on the first and third
of these incremental confidence metrics, which we
summarize as ?Now Understanding? and ?Will Un-
derstand?, respectively.
The incremental ASR, NLU, and confidence out-
puts are passed to the dialogue managers for each of
the agents, Harmony and Utah. These agents then
relate these inputs to their own models of dialogue
context, plans, and emotions, to calculate pragmatic
interpretations, including speech acts, reference res-
olution, participant status, and how they feel about
what is being discussed. A subset of this informa-
tion is passed to the non-verbal behavior generation
module to produce incremental non-verbal listening
behaviors (Wang et al, 2011).
2 Demo script
The demonstration begins with the demo operator
providing a brief overview of the system design, ne-
gotiation scenario, and incremental processing capa-
bilities. The virtual humans Utah and Harmony (see
Figure 1) are running and ready to begin a dialogue
with the user, who will play the role of the Ranger.
The demonstration includes a real-time visualization
of incremental speech processing results, which will
allow attendees to track the virtual humans? under-
standing as an utterance progresses. An example of
this visualization is shown in Figure 2.
As the user speaks to Utah or Harmony, attendees
can observe the real time visualization of incremen-
tal speech processing. Further, the visualization in-
terface enables the demo operator to ?rewind? an ut-
terance and step through the incremental processing
results that arrived each 200 milliseconds.
For example, Figure 2 shows the incremental
speech processing state at a moment 4.8 seconds into
a user?s 7.4 second long utterance, i?ve come here
today to talk to you about whether you?d like to be-
come the sheriff of this town. At this point in time,
the visualization shows (at top left) that the virtual
humans are confident that they are Now Understand-
ing and also Will Understand this utterance. Next,
the graph (in white) shows the history of the agents?
expected NLU F-Score for this utterance (ranging
from 0 to 1). Beneath the graph, the partial ASR re-
sult (HAVE COME HERE TODAY TO TALK TO
YOU ABOUT...) is displayed (in white), along
with the currently predicted NLU frame (in blue).
For ease of comprehension, an English gloss (utah
do you want to be the sheriff?) for the NLU frame is
also shown (in blue) above the frame.
To the right, in pink, we show some of Utah and
Harmony?s agent state that is based on the current in-
cremental NLU results. The display shows that both
of the virtual humans believe that Utah is being ad-
dressed by this utterance, that utah has a positive at-
titude toward the content of the utterance while har-
mony does not, and that both have comprehension
132
Figure 2: Visualization of Incremental Speech Processing.
and participation goals. Further, Harmony believes
she is a side participant at this moment.
Acknowledgments
We thank the entire ICT Virtual Humans team. The
project or effort described here has been sponsored
by the U.S. Army Research, Development, and En-
gineering Command (RDECOM). Statements and
opinions expressed do not necessarily reflect the po-
sition or the policy of the United States Government,
and no official endorsement should be inferred.
References
David DeVault and David R. Traum. 2012. Incremen-
tal speech understanding in a multi-party virtual hu-
man dialogue system. In Demonstration Proceedings
of NAACL-HLT.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of SIGDIAL.
David DeVault, Kenji Sagae, and David Traum. 2011a.
Detecting the status of a predictive incremental speech
understanding model for real-time decision-making in
a spoken dialogue system. In Proceedings of Inter-
Speech.
David DeVault, Kenji Sagae, and David Traum. 2011b.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue & Dis-
course, 2(1).
Arno Hartholt, Thomas Russ, David Traum, Eduard
Hovy, and Susan Robinson. 2008. A common ground
for virtual humans: Using an ontology in a natural
language oriented virtual human architecture. In Pro-
ceedings of LREC, Marrakech, Morocco, may.
Brian Plu?ss, David DeVault, and David Traum. 2011.
Toward rapid development of multi-party virtual hu-
man negotiation scenarios. In Proceedings of Sem-
Dial.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language un-
derstanding of partial speech recognition results in dia-
logue systems. In Short Paper Proceedings of NAACL
HLT.
David Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In Proceedings of IVA.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics.
Zhiyang Wang, Jina Lee, and Stacy Marsella. 2011.
Towards more comprehensive listening behavior: Be-
yond the bobble head. In Proceedings of IVA.
133
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 137?139,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Mixed-Initiative Conversational Dialogue System for Healthcare
Fabrizio Morbini and Eric Forbell and David DeVault and Kenji Sagae and
David R. Traum and Albert A. Rizzo
Institute for Creative Technologies
University of Southern California
Los Angeles, CA 90094, USA
{morbini,forbell,devault,sagae,traum,rizzo}@ict.usc.edu
Abstract
We present a mixed initiative conversational
dialogue system designed to address primar-
ily mental health care concerns related to
military deployment. It is supported by a
new information-state based dialogue man-
ager, FLoReS (Forward-Looking, Reward
Seeking dialogue manager), that allows both
advanced, flexible, mixed initiative interac-
tion, and efficient policy creation by domain
experts. To easily reach its target population
this dialogue system is accessible as a web ap-
plication.
1 Introduction
The SimCoach project is motivated by the challenge
of empowering troops and their significant others in
regard to their healthcare, especially with respect to
issues related to the psychological toll of military
deployment. SimCoach virtual humans are not de-
signed to act as therapists, but rather to encourage
users to explore available options and seek treatment
when needed by fostering comfort and confidence in
a safe and anonymous environment where users can
express their concerns to an artificial conversational
partner without fear of judgment or possible reper-
cussions.
SimCoach presents a rich test case for all compo-
nents of a dialogue system. The interaction with the
virtual human is delivered via the web for easy ac-
cess. As a trade-off between performance and qual-
ity, the virtual human has access to a limited set of
pre-rendered animations.
The Natural Language Understanding (NLU)
module needs to cope with both chat and military
Figure 1: Bill Ford, a SimCoach character. SimCoach
virtual humans are accessible through a web browser.
The user enters natural language input in the text field
on the bottom of the screen. The simcoach responds with
text, speech and character animation. The text area to the
right shows a transcript of the dialogue.
slang and a broad conversational domain. The dia-
logue policy authoring module needs to support non-
dialogue experts given that important parts of the di-
alogue policy are contributed by experts in psycho-
metrics and mental health issues in the military, and
others with familiarity with the military domain.
The dialogue manager (DM) must be able to take
initiative when building rapport or collecting the in-
formation it needs, but also respond appropriately
when the user takes initiative.
2 Supporting Mixed Initiative Dialogues
There is often a tension between system initiative
and performance of the system?s decision-making
for understanding and actions. A strong system-
initiative policy reduces the action state space since
137
user actions are only allowed at certain points in
the dialogue. System initiative also usually makes
it easier for a domain expert to design a dialogue
policy that will behave as desired.1 Such systems
can work well if the limited options available to the
user are what the user wants to do, but can be prob-
lematic otherwise, especially if the user has a choice
of whether or not to use the system. In particular,
this approach may not be well suited to an appli-
cation like SimCoach. At the other extreme, some
systems allow the user to say anything at any time,
but have fairly flat dialogue policies, e.g., (Leuski et
al., 2006). These systems can work well when the
user is naturally in charge, such as in interviewing
a character, but may not be suitable for situations
in which a character is asking the user questions, or
mixed initiative is desired.
True mixed initiative is notoriously difficult for a
manually constructed call-flow graph, in which the
system might want to take different actions in re-
sponse to similar stimuli, depending on local utili-
ties. Reinforcement learning approaches (Williams
and Young, 2007; English and Heeman, 2005) can
be very useful at learning local policy optimizations,
but they require large amounts of training data and a
well-defined global reward structure, are difficult to
apply to a large state-space and remove some of the
control, which can be undesirable (Paek and Pierac-
cini, 2008).
Our approach to this problem is a forward-looking
reward seeking agent, similar to that described in
(Liu and Schubert, 2010), though with support for
complex dialogue interaction and its authoring. Au-
thoring involves design of local subdialogue net-
works with pre-conditions and effects, and also qual-
itative reward categories (goals), which can be in-
stantiated with specific reward values. The dialogue
manager, called FLoReS, can locally optimize pol-
icy decisions, by calculating the highest overall ex-
pected reward for the best sequence of subdialogues
from a given point. Within a subdialogue, authors
can craft the specific structure of interaction.
Briefly, the main modules that form FLoReS are:
? The information state, a propositional knowl-
1Simple structures, such as a call flow graph (Pieraccini and
Huerta, 2005) and branching narrative for interactive games
(Tavinor, 2009) will suffice for authoring.
edge base that keeps track of the current state
of the conversation. The information state sup-
ports missing or unknown information by al-
lowing atomic formulas to have 3 possible val-
ues: true, false and null.
? A set of inference rules that allows the sys-
tem to add new knowledge to its information
state, based on logical reasoning. Forward in-
ference facilitates policy authoring by provid-
ing a mechanism to specify information state
updates that are independent of the specific di-
alogue context.2
? An event handling system, that allows the in-
formation state to be updated based on user in-
put, system action, or other classes of author-
defined events (such as system timeouts).
? A set of operators. Operators represent lo-
cal dialogue structure (trees), and can also be
thought of as reusable subdialogues. Each state
within the subdialogue can include a reward
for reaching that state. Rewards are functions
of the goals of the system, and are the main
method used to decide what to do when there is
more than one applicable operator. Operators
have preconditions and effects. Effects specify
changes to the information state. The precondi-
tions define when an operator can be activated.
3 Sample Dialogue
In this demo, the user will interact with the Sim-
Coach character Bill Ford, using a standard web
browser and typing text. The virtual human, driven
by FLoReS, will respond using pre-rendered anima-
tions encoded as H.264 video, delivered via a stan-
dard web server. Table 1 shows an excerpt from a
sample conversation with Bill Ford that illustrates
some of the features of this dialogue manager.
The excerpt starts from a rapport building
smalltalk sub-dialogue on the topic of barbecuing
which is interrupted by a user question about con-
fidentiality. The system responds to the user inter-
ruption and then re-starts the interrupted smalltalk
because it is still the most valuable conversation con-
tinuation available at that moment.
2For example: every time the user says that s/he has night-
mares we want to update the information state to include that
s/he also has sleeping problems.
138
Dialogue transcript Notes
Ask anybody about me, and
they?ll tell you that I love to
BBQ
BBQ Smalltalk The character is equipped
with a few operators for
smalltalk about a few topics.
BBQ is one of them.
Is this conversation se-
cret?
We don?t share your info
with anyone who can per-
sonally identify you. The
techs can see what we say,
but just to tell that the site is
working. But they have no
idea who said it, just what
was said
Did that help you?
Yes it did.
Great.
Confidentiality QA
Here the system is inter-
rupted by a user question and
it decides that answering it is
the best course of action.
Like I was saying, I love to
BBQ
BBQ Smalltalk After answering the question,
the best course of action is to
awaken the paused operator
about the BBQ smalltalk.
What is PTSD?
PTSD, or post-traumatic
stress disorder is an anxiety
condition associated with
serious traumatic events.
It can come with survivor
guilt, reliving the trauma in
dreams, numbness, and lack
of involvement with reality.
What is PTSD QA
Again the BBQ smalltalk is
interrupted by another ques-
tion from the user.
So, is PTSD something
you?re worried about. I only
ask, because you?ve been
asking about it. ...
PTSD Topic Interest QA
After answering the second
question the system decides
to ignore the paused operator
and load a follow-up operator
related to the important topic
raised by the user?s question.
The selection is based on the
expected reward that talking
about PTSD can bring to the
system.
Table 1: An excerpt of a conversation with Bill Ford that
shows opportunistic mixed initiative behavior.
Next, the user asks a question about the impor-
tant topic of post-traumatic stress disorder (PTSD).
That allows operators related to the PTSD topic to
become available and at the next chance the most
rewarding operator is no longer the smalltalk sub-
dialogue but one that stays on the PTSD topic.
4 Conclusion
We described the SimCoach dialogue system which
is designed to facilitate access to difficult health con-
cerns faced by military personnel and their fami-
lies. To easily reach its target population, the sys-
tem is available on the web. The dialogue is driven
by FLoReS, a new information-state and plan-based
DM with opportunistic action selection based on ex-
pected rewards that supports non-expert authoring.
Acknowledgments
The effort described here has been sponsored by the
U.S. Army. Any opinions, content or information
presented does not necessarily reflect the position or
the policy of the United States Government, and no
official endorsement should be inferred.
References
M.S. English and P.A. Heeman. 2005. Learning mixed
initiative dialogue strategies by using reinforcement
learning on both conversants. In HLT-EMNLP.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, pages 18?
27.
Daphne Liu and Lenhart K. Schubert. 2010. Combin-
ing self-motivation with logical planning and inference
in a reward-seeking agent. In Joaquim Filipe, Ana
L. N. Fred, and Bernadette Sharp, editors, ICAART (2),
pages 257?263. INSTICC Press.
Tim Paek and Roberto Pieraccini. 2008. Automating
spoken dialogue management design using machine
learning: An industry perspective. Speech Commu-
nication, 50(89):716 ? 729. Evaluating new methods
and models for advanced speech-based interactive sys-
tems.
Roberto Pieraccini and Juan Huerta. 2005. Where do we
go from here? Research and commercial spoken dia-
log systems. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue, Lisbon, Portugal,
September.
Grant Tavinor. 2009. The art of videogames. New Di-
rections in Aesthetics. Wiley-Blackwell, Oxford.
J.D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
139
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 270?274,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Study in How NLU Performance Can Affect the Choice of Dialogue
System Architecture
Anton Leuski and David DeVault
USC Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094
{leuski,devault}@ict.usc.edu
Abstract
This paper presents an analysis of how the
level of performance achievable by an NLU
module can affect the optimal modular design
of a dialogue system. We present an evalua-
tion that shows how NLU accuracy levels im-
pact the overall performance of a system that
includes an NLU module and a rule-based di-
alogue policy. We contrast these performance
levels with the performance of a direct classifi-
cation design that omits a separate NLU mod-
ule. We conclude with a discussion of the po-
tential for a hybrid architecture incorporating
the strengths of both approaches.
1 Introduction
Recently computer-driven conversational characters
or virtual humans have started finding real-life ap-
plications ranging from education to health services
and museums (Traum et al, 2005; Swartout et al,
2006; Kenny et al, 2009; Jan et al, 2009; Swartout
et al, 2010). As proliferation of these systems in-
creases, there is a growing demand for the design
and construction of virtual humans to be made more
efficient and accessible to people without extensive
linguistics and computer science backgrounds, such
as writers, designers, and educators. We are specif-
ically interested in making the language processing
and dialogue management components in a virtual
human easier for such potential authors to develop.
Some system building steps that can be challenging
for such authors include annotating the meaning of
user and system utterances in a semantic formalism,
developing a formal representation of information
state, and writing detailed rules that govern dialogue
management.
We are generally interested in the extent to which
these various authoring steps are necessary in order
to achieve specific levels of system performance. In
this paper, we present a case study analysis of the
performance of two alternative architectures for a
specific virtual human. The two architectures, which
have been developed and evaluated in prior work
(DeVault et al, 2011b; DeVault et al, 2011a), differ
substantially in their semantic annotation and policy
authoring requirements. We describe these architec-
tures and our evaluation corpus in Section 2. We
focus our new analysis specifically on how the over-
all performance of one of the architectures, which
uses a natural language understanding (NLU) mod-
ule and hand-authored rules for the dialogue policy,
depends on the performance of the NLU module. In
Section 3, we describe our finding that, depending
on the attainable level of NLU accuracy, this modu-
lar approach may or may not perform better than a
simpler direct classification design that omits a sep-
arate NLU module and has a lower annotation and
rule authoring burden. In Section 4, we present an
initial exploration of whether a hybrid architecture
may be able to combine these approaches? strengths.
2 Summary of Data Set and Prior Results
This work is part of an ongoing research effort
into techniques for developing high quality dialogue
policies using a relatively small number of sample
dialogues and low annotation requirements (DeVault
et al, 2011b; DeVault et al, 2011a). This section
briefly summarizes our prior work and data set.
270
2.1 Data Set
For our experiments we use the dataset described
in (DeVault et al, 2011b). It contains 19 Wiz-
ard of Oz dialogues with a virtual human called
Amani (Gandhe et al, 2009). The user plays the
role of an Army commander whose unit has been at-
tacked by a sniper. The user interviews Amani, who
was a witness to the incident and has some informa-
tion about the sniper. Amani is willing to tell the
interviewer what she knows, but she will only re-
veal certain information in exchange for promises of
safety, secrecy, and money (Artstein et al, 2009).
Each dialogue turn in the data set includes a single
user utterance followed by the response chosen by a
human Amani role player. There are a total of 296
turns, for an average of 15.6 turns/dialogue. User
utterances are modeled using 46 distinct speech act
(SA) labels. The dataset alo defines a different set
of 96 unique SAs (responses) for Amani. Six ex-
ternal referees analyzed each user utterance and se-
lected a single character response out of the 96 SAs.
Thus the dataset defines a one-to-many mapping be-
tween user utterances and alternative system SAs.
2.2 Evaluation Metric
We evaluate the dialogue policies in our experi-
ments through 19-fold cross-validation of our 19 di-
alogues. In each fold, we hold out one dialogue and
use the remaining 18 as training data. To measure
policy performance, we count an automatically pro-
duced system SA as correct if that SA was chosen by
the original wizard or at least one external referee for
that dialogue turn. We then count the proportion of
the correct SAs among all the SAs produced across
all 19 dialogues, and use this measure of weak accu-
racy to score dialogue policies.
We can use the weak accuracy of one referee,
measured against all the others, to establish a per-
formance ceiling for this metric. This score is .79;
see DeVault et al (2011b).
2.3 Baseline Systems
We consider two existing baseline systems in our ex-
periments here. The first system (Rules-NLU-SA)
consists of a statistical NLU module that maps a user
utterance to a single user SA label, and a rule-based
dialogue policy hand-crafted by one of the authors.
The NLU uses a maximum-entropy model (Berger
et al, 1996) to classify utterances as one of the user
SAs using shallow text features. Training this model
requires a corpus of user utterances that have been
semantically annotated with the appropriate SA.
We developed our rule-based policy by manu-
ally writing the simple rules needed to implement
Amani?s dialogue policy. Given a user SA label
At for turn t, the rules for determining Amani?s re-
sponse Rt take one of three forms:
(a)ifAt = SAi thenRt = SAj
(b)ifAt = SAi ? ?kAt?k = SAl thenRt = SAj
(c)ifAt = SAi ? ??kAt?k = SAl thenRt = SAj
The first rule form specifies that a given user SA
should always lead to a given system response. The
second and third rule forms enable the system?s re-
sponse to depend on the user having previously per-
formed (or not performed) a specific SA. One the
system developers, who is also a computational lin-
guist, created the current set of 42 rules in about 2
hours. There are 30 rules of form (a), 6 rules of form
(b), and 6 rules of form (c).
The second baseline system (RM-Text) is a sta-
tistical classifier that selects system SAs by analyz-
ing shallow features of the user utterances and sys-
tem responses. We use the Relevance Model (RM)
approach pioneered by Lavrenko et al (2002) for
cross-lingual information retrieval and adapted to
question-answering by Leuski et al (2006). This
method does not require semantic annotation or rule
authoring; instead, the necessary training data is de-
fined by linking user utterances directly to the appro-
priate system responses (Leuski and Traum, 2010).
Table 1 summarizes the performance for the base-
line systems (DeVault et al, 2011a). The NLU mod-
ule accuracy is approximately 53%, and the weak
accuracy of .58 for the corresponding system (Rules-
NLU-SA) is relatively low when compared to the
RM system at .71. For comparison we provide a
third data point: for Rules-G-SA, we assume that
our NLU is 100% accurate and always returns the
correct (?gold?) SA label. We then run the rule-
based dialogue policy on those labels. The third
column (Rules-G-SA) shows the resulting weak ac-
curacy value, .79, which is comparable to the weak
accuracy score achieved by the human referees (De-
Vault et al, 2011b).
271
Rules-NLU-SA RM-Text Rules-G-SA
.58 .71 .79
Table 1: Weak accuracy results for baseline systems.
Rules-G
RM-Text
Rules-NLU-SA
50 60 70 80 90 100
55
60
65
70
75
80
Simulated NLU Accuracy H%L
W
e
a
k
A
c
c
u
r
a
c
y
H
%
L
Figure 1: Weak accuracy of the Rules system as a func-
tion of simulated NLU accuracy.
3 NLU Accuracy and System Performance
We conducted two experiments. In the first, we stud-
ied the effect of NLU accuracy on the performance
of the Rules-NLU-SA system. One of our goals was
to find how accurate the NLU would have to be for
the Rules-NLU-SA system to outperform RM-Text.
To investigate this, we simulated NLU perfor-
mance at different accuracy levels by repeatedly
sampling to create a mixture of the SAs from the
trained NLU classifier and from the correct (gold)
set of SAs. Specifically, we set a fixed value p rang-
ing from 0 to 1 and then iterate over all dialogue
turns in the held out dialogue, selecting the the cor-
rect SA label with probability p or the trained NLU
module?s output with probability 1 ? p. Using the
sampled set of SA labels, we compute the result-
ing simulated NLU accuracy, run the Rules dialogue
policy, and record the weak accuracy result. We re-
peat the process 25 times for each value of p. We let
p range from 0 to 1 in increments of .05 to explore a
range of simulated accuracy levels.
Figure 1 shows simulated NLU accuracy and the
corresponding dialogue policy weak accuracy as a
point in two dimensions. The points form a cloud
with a clear linear trend that starts at approximately
53% NLU accuracy where it intersects with the
Rules-NLU-SA system performance and then goes
up to the Rules-G performance at 100% NLU accu-
racy. The correlation is strong with R2 = 0.97.1
The existence of a mostly linear relationship com-
ports with the fact that most of the policy rules (30
of 42), as described in Section 2.3, are of form (a).
For such rules, each individual correct NLU speech
act translates directly into a single correct system
response, with no dependence on the system hav-
ing understood previous user utterances correctly.
In contrast, selecting system responses that comply
with rules in forms (b) and (c) generally requires
correct understanding of multiple user utterances.
Such rules create a nonlinear relationship between
policy performance and NLU accuracy, but these
rules are relatively few in number for Amani.
The estimated linear trend line (in purple) crosses
the RM-Text system performance at approximately
82% NLU accuracy. This result suggests that our
NLU component would need to improve from its
current accuracy of 53% to approximately 82% ac-
curacy for the Rules-NLU-SA system to outperform
the RM-Text classifier. This represents a very sub-
stantial increase in NLU accuracy that, in practice,
could be expected to require a significant effort in-
volving utterance data collection, semantic annota-
tion, and optimization of machine learning for NLU.
4 Hybrid System
In our second experiment we investigated the po-
tential to integrate the Rules-NLU-SA and RM-Text
systems together for better performance. Our ap-
proach draws on a confidence score ? from the NLU
maximum-entropy classifier; specifically, ? is the
probability assigned to the most probable user SA.
Figure 2 shows an analysis of NLU accuracy,
Rules-NLU-SA, and RM-Text that is restricted to
those subsets of utterances for which NLU confi-
dence ? is greater than or equal to some threshold ? .
Two important aspects of this figure are (1) that rais-
ing the minimum confidence threshold also raises
the NLU accuracy on the selected subset of utter-
ances; and (2) that there is a threshold NLU confi-
dence level beyond which Rules-NLU-SA seems to
1This type of analysis of dialogue system performance in
terms of internal component metrics is somewhat similar to the
regression analysis in the PARADISE framework (Walker et al,
2000). However, here we are not concerned with user satis-
faction, but are instead focused solely on the modular system?s
ability to reproduce a specific well-defined dialogue policy.
272
0.4 0.5 0.6 0.7 0.8 0.9 1.0
65
70
75
80
85
90
95
NLU Confidence H?L
A
c
c
u
r
a
c
y
H
%
L
RM-Text
Rules-NLU-SA H? ? ?L
NLU Accuracy H? ? ?L
Figure 2: Weak accuracy of Rules-NLU-SA and RM-
Text on utterance subsets for which NLU confidence
? ? ? . We also indicate the corresponding NLU accu-
racy at each threshold. In all cases a rolling average of 30
data points is shown to more clearly indicate the trends.
RM-Text
Rules-NLU-SA
0.70 0.75 0.80 0.85 0.90 0.95 1.00
55
60
65
70
75
0
15
30
45
60
NLU Confidence H? L
W
e
a
k
A
c
c
u
r
a
c
y
H
%
L
P
r
o
p
.
o
f
N
L
U
S
A
s
w
?
?
?
?
Mix
Hybrid
Figure 3: Weak accuracy of the Hybrid system as a func-
tion of the NLU confidence score.
outperform RM-Text. This confidence level is ap-
proximately 0.95, and it identifies a subset of user
utterances for which NLU accuracy is 83.3%. These
results therefore suggest that NLU confidence can
be useful in identifying utterances for which NLU
speech acts are more likely to be accurate and Rules-
NLU-SA is more likely to perform well.
To explore this further, we implemented a hy-
brid system that chooses between Rules-NLU-SA or
RM-Text as follows. If the confidence score is high
enough (? ? ? , for some fixed threshold ? ), the Hy-
brid system uses the NLU output to run the Rules
dialogue policy to select the system SA; otherwise,
it discards the NLU SA, and applies the RM classi-
fier to select the system response directly.
Figure 3 shows the plot of the Hybrid system per-
formance as a function of the threshold value ? .
We see that with sufficiently high threshold value
(? ? 0.95) the Hybrid system outperforms both
the Rules-NLU-SA and the RM-Text systems. The
second line, labeled ?Mix? and plotted against the
secondary (right) axis, shows the proportion of the
NLU SAs with the confidence score that exceed the
threshold (? ? ? ). It indicates how often the Hybrid
system prefers the Rules-NLU-SA output over the
RM-Text system output. We observe that approxi-
mately 42 of the NLU outputs over all 296 dialogue
turns (15%) have confidence values ? ? 0.95. How-
ever, for most of these dialogue turns the outputs for
the Rules-NLU-SA and RM-Text dialogue policies
are the same. While we observe a small improve-
ment in the Hybrid system weak accuracy values
over the RM-Text system at thresholds of 0.95 and
higher, the difference is not statistically significant.
Despite the lack of statistical significance in the
initial Hybrid results in this small data set, we inter-
pret the complementary evidence from both experi-
ments, which support the potential for Rules-NLU-
SA to perform well when NLU accuracy is high, and
the potential for a hybrid system to identify a subset
of utterances that are likely to be understood accu-
rately at run-time, as indicating that a hybrid design
is a promising avenue for future work.
5 Conclusions and Future Work
We presented a case study analysis of how the level
of performance that is achievable in an NLU module
can provide perspective on the design choices for a
modular dialogue system. We found that NLU accu-
racy must be substantially higher than it currently is
in order for the Rules-NLU-SA design, which car-
ries a greater annotation and rule authoring burden,
to deliver better performance than the simpler RM-
Text design. We also presented evidence that a hy-
brid architecture could be a promising direction.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred.
273
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43?50, Stockholm, Sweden, June.
Adam L. Berger, Stephen D. Della Pietra, and Vincent
J. D. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22(1):39?71.
David DeVault, Anton Leuski, and Kenji Sagae. 2011a.
An evaluation of alternative strategies for implement-
ing dialogue policies using statistical classification and
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1341?1345, Nov.
David DeVault, Anton Leuski, and Kenji Sagae. 2011b.
Toward learning and evaluation of dialogue policies
with text examples. In Proceedings of the 12th annual
SIGdial Meeting on Discourse and Dialogue, pages
39?48.
Sudeep Gandhe, Nicolle Whitman, David R. Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Dusan Jan, Antonio Roque, Anton Leuski, Jackie Morie,
and David R. Traum. 2009. A virtual tour guide for
virtual worlds. In Zso?fia Ruttkay, Michael Kipp, An-
ton Nijholt, and Hannes Ho?gni Vilhja?lmsson, editors,
IVA, volume 5773 of Lecture Notes in Computer Sci-
ence, pages 372?378. Springer.
Patrick G. Kenny, Thomas D. Parsons, and Albert A.
Rizzo. 2009. Human computer interaction in virtual
standardized patient systems. In Proceedings of the
13th International Conference on Human-Computer
Interaction. Part IV, pages 514?523, Berlin, Heidel-
berg. Springer-Verlag.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the 25th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 175?182, Tampere, Finland.
Anton Leuski and David Traum. 2010. NPCEditor: A
tool for building question-answering characters. In
Proceedings of The Seventh International Conference
on Language Resources and Evaluation (LREC).
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, Syd-
ney, Australia, July.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96?108.
William R. Swartout, David R. Traum, Ron Artstein,
Dan Noren, Paul E. Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, and
Diane Piepol. 2010. Ada and grace: Toward realis-
tic and engaging virtual museum guides. In Jan M.
Allbeck, Norman I. Badler, Timothy W. Bickmore,
Catherine Pelachaud, and Alla Safonova, editors, IVA,
volume 6356 of Lecture Notes in Computer Science,
pages 286?300. Springer.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenney, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul Bha-
gat, Susan Robinson, Andrew Marshall, Dagen Wang,
Sudeep Gandhe, and Anton Leuski. 2005. Dealing
with doctors: Virtual humans for non-team interac-
tion training. In Proceedings of ACL/ISCA 6th SIGdial
Workshop on Discourse and Dialogue, Lisbon, Portu-
gal, September.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usability
with PARADISE. Nat. Lang. Eng., 6(3-4):363?377.
274
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
