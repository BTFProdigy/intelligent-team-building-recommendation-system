UCSG: A Wide Coverage Shallow Parsing System
G. Bharadwaja Kumar and Kavi Narayana Murthy
Department of Computer and Information Sciences
University of Hyderabad
g vijayabharadwaj@yahoo.com, knmuh@yahoo.com
Abstract
In this paper, we propose an architecture,
called UCSG Shallow Parsing Architecture,
for building wide coverage shallow parsers by
using a judicious combination of linguistic
and statistical techniques without need for
large amount of parsed training corpus to
start with. We only need a large POS tagged
corpus. A parsed corpus can be developed
using the architecture with minimal manual
effort, and such a corpus can be used for
evaluation as also for performance improve-
ment. The UCSG architecture is designed to
be extended into a full parsing system but
the current work is limited to chunking and
obtaining appropriate chunk sequences for a
given sentence. In the UCSG architecture, a
Finite State Grammar is designed to accept
all possible chunks, referred to as word
groups here. A separate statistical compo-
nent, encoded in HMMs (Hidden Markov
Model), has been used to rate and rank the
word groups so produced. Note that we are
not pruning, we are only rating and ranking
the word groups already obtained. Then we
use a Best First Search strategy to produce
parse outputs in best first order, without
compromising on the ability to produce all
possible parses in principle. We propose a
bootstrapping strategy for improving HMM
parameters and hence the performance of
the parser as a whole.
A wide coverage shallow parser has been
implemented for English starting from the
British National Corpus, a nearly 100 Mil-
lion word POS tagged corpus. Note that the
corpus is not a parsed corpus. Also, there
are tagging errors, multiple tags assigned in
many cases, and some words have not been
tagged. A dictionary of 138,000 words with
frequency counts for each word in each tag
has been built. Extensive experiments have
been carried out to evaluate the performance
of the various modules. We work with large
data sets and performance obtained is
encouraging. A manually checked parsed
corpus of 4000 sentences has also been
developed and used to improve the parsing
performance further. The entire system has
been implemented in Perl under Linux.
Key Words:- Chunking, Shallow Parsing,
Finite State Grammar, HMM, Best First
Search
1 Introduction
In recent times, there has been an increasing interest
in wide coverage and robust but shallow parsing
systems. Shallow parsing is the task of recovering
only a limited amount of syntactic information from
natural language sentences. Often shallow parsing is
restricted to finding phrases in sentences, in which
case it is also called chunking. Steve Abney (Abney,
1991) has described chunking as finding syntactically
related non-overlapping groups of words. In CoNLL
chunking task (Tjong Kim Sang and Buchholz,
2000) chunking was defined as the task of divid-
ing a text into syntactically non-overlapping phrases.
Most of the shallow parsers and chunkers de-
scribed in literature (Tjong Kim Sang and Buchholz,
2000; Carreras and Marquez, 2003; Dejean, 2002;
Molina and Pla, 2002; Osborne, 2002; Sang, 2002;
Abney, 1996; Grefenstette, 1996; Roche, 1997)
have used either only rule based techniques or only
machine learning techniques. Hand-crafting rules in
the linguistic approach can be very laborious and
time consuming. Parsers tend to produce a large
number of possible parse outputs and in the absence
72
of suitable rating and ranking mechanisms, selecting
the right parse can be very difficult. Statistical
learning systems, on the other hand, require large
and representative parsed corpora for training, and
such training corpora are not always available.
Perhaps only a good combination of linguistic and
statistical approaches can give us the best results
with minimal effort.
Other important observations from literature that
motivated the present work are: 1) Most chunking
systems have so far been tested only on small scale
data 2) Good performance has been obtained only
under restricted conditions 3) Performance is often
evaluated in terms of individual chunks rather than
complete chunk sequences for a whole sentence, and
4) Many chunkers produce only one output, not all
possible outputs in some ranked order.
2 UCSG Shallow Parsing
Architecture
UCSG shallow parsing architecture is set within
the UCSG full parsing framework for parsing nat-
ural language sentences which was initiated in the
early 1990?s at University of Hyderabad by Kavi
Narayana Murthy (Murthy, 1995). In this paper,
the focus is only on chunking - identifying chunks or
word groups, handling ambiguities, and producing
parses (chunk sequences) for given sentences. This
can be extended to include thematic role assignment
and clause structure analysis leading towards a full
parser. Figure 1 shows the basic UCSG Shallow
Parsing Architecture (Kumar and Murthy, 2006).
Figure 1: UCSG Shallow Parsing Architecture
The input to the parsing system is one sentence,
either plain or POS tagged. Output is an ordered
set of parses. Here by parse we mean a sequence
of chunks that covers the given sentence with no
overlaps or gaps. The aim is to produce all possible
parses in ranked order hoping to get the best parse
to the top.
A chunk or a ?word group? as we prefer to call it
in UCSG, is ?a structural unit, a non-overlapping
and non-recursive sequence of words, that can as
a whole, play a role in some predication? (Murthy,
1995). Note that word groups do not include clauses
(relative clauses, for example) or whole sentences.
Every word group has a head which defines the
type of the group. These word groups thus seem
to be similar to chunks as generally understood
(Molina and Pla, 2002; Sang and Buchholz, 2000;
Megyesi, 2002). However, chunks in UCSG are
required to correspond to thematic roles, which
means for example, that prepositional phrases are
handled properly. Many chunkers do not even build
prepositional phrases - prepositions are treated as
individual chunks in their own right. Thematic roles
can be viewed from question-answering perspective.
For example, in the sentence ?I teach at University
of Hyderabad?, ?at University of Hyderabad? answers
the ?where? question and should therefore be treated
as a single chunk. It is well known that prepositional
phrase attachment is a hard problem and the task
we have set for ourselves here is thus significantly
more challenging. The parse outputs in UCSG
would be more semantic and hence should be better
suited for many NLP applications.
In UCSG, a Finite State Grammar-Parser system
generates all possible chunks in linear time. Chunk
level HMMs are then used to rate and rank the
chunks so produced. Finally, a kind of best first
search strategy is applied to obtain chunk sequences
hopefully in best first order. The aim is to develop
wide coverage, robust parsing systems without
need for a large scale parsed corpus to start with.
Only a large POS tagged corpus is needed and a
parsed corpus can be generated from within the
architecture with minimal manual effort. Such a
parsed corpus can be used for evaluation as also for
further performance improvements.
We will need a dictionary which includes the fre-
quency of occurrence of each word in each possible
tag. Such a dictionary can be developed using a large
POS tagged corpus.
73
2.1 Finite State Grammar-Parser
Here the task is only to recognize chunks and
not produce a detailed description of the internal
structure of chunks. Also, chunks by definition are
non-recursive in nature, only linear order, repetition
and optional items need to be considered. Finite
state grammars efficiently capture linear precedence,
repetition and optional occurrence of words in
word groups. Finite state machines are thus both
necessary and sufficient for recognizing word groups
(Murthy, 1995). It is also well known that Finite
State Machines are computationally efficient - linear
time algorithms exist for recognizing word groups.
All possible word groups can be obtained in a single
left-to-right scan of the given sentence in linear time
(Murthy, 1995). Finite state grammars are also
conceptually simple and easy to develop and test.
The Finite State module accepts a sentence (ei-
ther already POS tagged or tagged with all possible
categories using the dictionary) and produces an un-
ordered set of possible chunks taking into account all
lexical ambiguities.
2.2 HMMs for Rating and Ranking Chunks
The second module is a set of Hidden Markov
Models (HMMs) used for rating and ranking the
word groups already produced by the Finite State
Grammar-Parser. The hope is to get the best
chunks near the top. This way, we are not pruning
and yet we can hope to get the right chunks near
the top and push down the others.
Words are observation symbols and POS tags
are states in our HMMs. Formally, a HMM model
? = (pi,A,B) for a given chunk type can be de-
scribed as follows:
Number of States (N) = number of relevant POS
Categories
Number of Observation Symbols (M) = number of
Words of relevant categories in the language
The initial state probability
pii = P{q1 = i} (1)
where 1 ? i ? N , q1 is a category (state) starting a
particular word group type.
State transition probability
aij = P{qt+1 = j|qt = i} (2)
where 1 ? i, j ? N and qt denotes the category at
time t and qt+1 denotes the category at time t+1.
Observation or emission probability
bj(k) = P{ot = vk|qt = j} (3)
where 1 ? j ? N , 1 ? k ? M and vk denotes the
kth word, and qt the current state.
We first pass a large POS tagged corpus through
the Finite State module and obtain all possible
chunks. Taking these chunks to be equi-probable,
we estimate the HMM parameters by taking the
ratios of frequency counts. One HMM is developed
for each major category of chunks, say, one for
noun-groups, one for verb-groups, and so on. The B
matrix values are estimated from a dictionary that
includes frequency counts for each word in every
possible category. These initial models of HMMs
are later refined using a bootstrapping technique as
described later.
We simply estimate the probability of each chunk
using the following equation :
P (O,Q|?) = piq1bq1(o1)aq1,q2bq2(o2)aq2,q3 ? ? ?
aqt?1,qtbqt(ot)
where q1 ,q2, ? ? ?, qt is a state sequence, o1 , o2,? ? ?,
ot is an observation sequence. Note that no Viterbi
search involved here and the state sequence is also
known. Thus even Forward/Backward algorithm
is not required and rating the chunks is therefore
computationally efficient.
The aim here is to assign the highest rank for the
correct chunk and to push down other chunks. Since
a final parse is a sequence of chunks that covers the
given sentence with no overlaps or gaps, we evaluate
the alternatives at each position in the sentence in a
left-to-right manner.
Here, we use Mean Rank Score to evaluate the per-
formance of the HMMs. Mean Rank Score is the
mean of the distribution of ranks of correct chunks
produced for a given training corpus. Ideally, all cor-
rect chunks would be at the top and hence the score
would be 1. The aim is to get a Mean Rank Score as
close to 1 as possible.
2.3 Parse Generation and Ranking
Parsing is a computationally complex task and
generating all possible parses may be practically
difficult. That is why, a generate-and-test approach
74
where we first generate all possible parses and
then look for the correct parse among the parses
produced is impracticable. Simply producing all
or some parses in some random or arbitrary order
is also not of much practical use. Many chunkers
produce a single output which may or may not
be correct. Here we instead propose a best first
strategy wherein the very production of possible
parses is in best first order and so, hopefully, we
will get the correct parse within the top few and in
practice we need not actually generate all possible
parses at all. This way, we overcome the problems
of computational complexity and at the same time
avoid the risk of missing the correct parse if pruning
is resorted to. Performance can be measured not
only in terms of percentage of input sentences for
which a fully correct parse is produced but also in
terms of the rank of the correct parse in the top k
parses produced, for any chosen value of k.
It may be noted that although we have already
rated and ranked the chunks, simply choosing the
locally best chunks at each position in a given
sentence does not necessarily give us the best parse
(chunk sequence) in all cases. Hence, we have
mapped our parse selection problem into a graph
search problem and used best first search algorithm
to get the best parse for a given sentence.
Words and chunks in a sentence are referred to in
terms of the positions they occupy in the sentence.
Positions are marked between words, starting from
zero to the left of the first word. The positions in
the sentence are treated as nodes of the resulting
graph. If a sentence contains N words then the
graph contains N + 1 nodes corresponding to the
N + 1 positions in the sentence. Word group Wi,j is
represented as an edge form node i to node j. We
thus have a lattice structure. The cost of a given
edge is estimated from the probabilities given by
the HMMs. If and where a parsed training corpus is
available, we can also use the transition probability
from previous word group type to current word
group type. It is possible to use the system itself to
parse sentences and from that produce a manually
checked parsed corpus with minimal human effort.
We always start from the initial node 0. N is the
goal node. Now our parse selection problem for a
sentence containing N words becomes the task of
finding an optimal (lowest cost) path from node 0
to node N .
We use the standard best first search algorithm.
In best first search, we can inspect all the currently-
available nodes, rank them on the basis of our par-
tial knowledge and select the most promising of the
nodes. We then expand the chosen node to gener-
ate it successors. The worst case complexity of best
first search algorithm is exponential: O(bm), where
b is the branching factor (i.e., the average number of
nodes added to the open list at each level), and m is
the maximum length of any path in the search space.
As an example, a 40 word sentence has been shown
to produce more than 1015 different parses (Kumar,
2007). In practice, however, we are usually interested
in only the top k parses for some k and exhaustive
search is not called for.
2.4 Bootstrapping
The HMM parameters can be refined through boot-
strapping. We work with large data sets running
into many hundreds of thousands of sentences and
Baum-Welch parameter re-estimation would not be
very practical. Instead, we use parsed outputs to re-
build HMMs. By parsing a given sentence using the
system and taking the top few parses only as train-
ing data, we can re-build HMMs that will hopefully
be better. We can also simply use the top-ranked
chunks for re-building the HMMs. This would re-
duce the proportion of invalid chunks in the training
data and hence hopefully result in better HMM pa-
rameters. As can be seen from the results in the next
section, this idea actually works and we can signif-
icantly improve the HMM parameters and improve
parser performance as well.
3 Experiments and Results
The entire parsing system has been implemented in
Perl under Linux. Extensive experimentation has
been carried out to evaluate the performance of the
system. However, direct comparisons with other
chunkers and parsers are not feasible as the architec-
tures are quite different. All the experiments have
been carried out on a system with Pentium Core 2
DUO 1.86 GHz Processor and 1 GB RAM. Tran-
scripts from the implemented system have been in-
cluded in the next section.
3.1 Dictionary
We have developed a dictionary of 138,000 words in-
cluding frequency of occurrence for each tag for each
word. The dictionary includes derived words but not
inflected forms. The dictionary has been built from
the British National Corpus(BNC) (Burnard, 2000),
an English text corpus of about 100 Million words.
Closed class words have been manually checked. The
dictionary has a coverage of 98% on the BNC corpus
itself, 86% on the Reuters News Corpus (Rose et
75
al., 2002) (about 180 Million words in size), 96.36%
on the Susanne parsed corpus (Sampson, 1995) and
95.27% on the Link parser dictionary.
3.2 Sentence Boundary Detection
We have developed a sentence segmentation module
using the BNC corpus as training data. We have
used delimiter, prefix, suffix and after-word as fea-
tures and extracted patterns from the BNC corpus.
Decision Tree algorithms have been used and an av-
erage F-Measure of 98.70% has been obtained, com-
parable to other published results. See (Htay et al,
2006) for more details.
3.3 Tag Set
We have studied various tag sets including BNC C5,
BNC C7, Susanne and Penn Tree Bank tag sets.
Since our work is based on BNC 1996 edition with
C5 tag set, we have used C5 tag set and made some
extensions as required. We now have a total of 71
tags in our extended tag set (Kumar, 2007).
3.4 Manually Parsed Corpus
We have developed a manually checked parsed
corpus of 4000 sentences, covering a wide variety of
sentence structures. Of these, 1000 sentences have
been randomly selected from the BNC corpus, 1065
sentences from ?Guide to Patterns and Usage in
English? (Hornby, 1975) and 1935 sentences from
the CoNLL-2000 test data. This corpus is thus very
useful for evaluating the various modules of the
parsing architecture and also for bootstrapping.
This corpus was developed by parsing the sen-
tences using this UCSG shallow parser itself and then
manually checking the top parse and making correc-
tions where required. Our experience shows that this
way we can build manually checked parsed corpora
with minimal human effort.
3.5 Tagging
If a POS tagger is available, we can POS tag the
input sentences before sending them to the parser.
Otherwise, all possible tags from the dictionary may
be considered. In our work here, we have not used
any POS tagger. All possible tags are assigned from
our dictionary and a few major rules of inflectional
morphology of English, including plurals for nouns,
past tense, gerundial and participial forms of verbs
and degrees of comparison for adjectives are handled.
Unresolved words are assigned NP0 (Proper Name)
tag.
3.6 Finite State Grammar
We have developed a Finite State Grammar for
identifying English word groups. The Finite State
Machine has a total of 50 states of which 24 are final
states. See (Kumar, 2007) for further details.
The UCSG Finite State Grammar recognizes
verb-groups, noun-groups, adverbial-groups,
adjective-groups, to-infinitives, coordinate and
subordinate conjunctions. There are no separate
prepositional phrases - prepositions are treated as
surface case markers in UCSG - their primary role
is to indicate the relationships between chunks and
the thematic roles taken up by various noun groups.
Prepositional groups are therefore treated on par
with noun groups.
We have evaluated the performance of the FSM
module on various corpora - Susanne Parsed Corpus,
CoNLL 2000 test data set and on our manually
parsed corpus of 4000 sentences. The evaluation
criteria is Recall (the percentage of correct chunks
recognized) alone since the aim here is only to
include the correct chunks. We have achieved a high
recall of 99.5% on manually parsed corpus, 95.06%
on CoNLL test data and 88.02% on Susanne corpus.
The reason for the relatively low Recall on the Su-
sanne corpus is because of the variations in the def-
inition of phrases in Susanne corpus. For example,
Susanne corpus includes relative clauses into noun
groups. The reasons for failures on CoNLL test data
have been traced mainly to missing dictionary en-
tries and inability of the current system to handle
multi-token adverbs.
3.7 Building and Refining HMMs
HMMs were initially developed from 3.7 Million
POS-tagged sentences taken from the BNC corpus.
Sentences with more than 40 words were excluded.
Since we use an extended C5 tag set, POS tags had
to be mapped to the extended set where necessary.
HMM parameters were estimated from the chunks
produced by the Finite State grammar, taking all
chunks to be equi-probable. Separate HMMs were
built for noun groups, verb groups, adjective groups,
adverb groups, infinitive groups and one HMM for
all other chunk types.
The chunks produced by the FSM are ranked using
these HMMs. It is interesting to observe the Recall
and Mean Rank Score within the top k ranks, where
k is a given cutoff rank. Table 1 shows that there is
a clear tendency for the correct chunks to bubble up
76
close to the top. For example, more than 95% of the
correct chunks were found within the top 5 ranks.
Table 1: Performance of the HMM Module on the
Manually Parsed Corpus of 4000 sentences
Plain POS Tagged
Cut- Mean Cumulative Mean Cumulative
-off Rank Recall (%) Rank Recall (%)
1 1 43.06 1 62.74
2 1.38 69.50 1.28 86.97
3 1.67 84.72 1.43 95.64
4 1.85 91.69 1.50 98.31
5 1.96 95.13 1.54 99.25
We have also carried out some experiments to see
the effect of the size of training data used to build
HMMs. We have found that as we use more and
more training data, the HMM performance improves
significantly, clearly showing the need for working
with very large data sets. See (Kumar, 2007) for
more details.
3.7.1 Bootstrapping
To prove the bootstrapping hypothesis, we have
carried out several experiments. Plain text sentences
from BNC corpus, 5 to 20 words in length, have been
used. All possible chunks are obtained using the Fi-
nite State Grammar-Parser and HMMs built from
these chunks. In one experiment, only the chunks
rated highest by these very HMMs are taken as train-
ing data for bootstrapping. In a second experiment,
best first search is also carried out and chunks from
the top ranked parse alone are taken for bootstrap-
ping. In a third experiment, data from these two
sources have been combined. Best results were ob-
tained when the chunks from the top parse alone
were used for bootstrapping. Table 2 shows the ef-
fect of bootstrapping on the HMM module for plain
sentences.
Table 2: Effect of Bootstrapping: on 4000 sentences
from Manually Parsed Corpus containing a total of
27703 chunks
Cutoff Iteration-1 Iteration-2
Recall Mean Recall Mean
Rank Rank
1 45.52 1.0 47.25 1.0
2 71.43 1.36 72.81 1.35
3 85.22 1.63 85.95 1.60
4 91.75 1.80 92.20 1.77
5 94.94 1.90 95.30 1.87
It may be observed that both the Recall and Mean
Rank Scores have improved. Our experiments show
that there is also some improvement in the final parse
when the HMMs obtained through bootstrapping are
used. These observations, seen consistently for both
plain and POS tagged sentences, show the effective-
ness of the overall idea.
3.8 Parse Generation and Ranking
It may be noted that in principle the performance
of the parser in terms of its ability to produce the
correct parse is limited only by the Finite State
Grammar and the dictionary, since the other mod-
ules in the UCSG architecture do not resort to any
pruning. However, in practical usage we generally
impose a time limit or a cutoff and attempt to
produce only the top k parses. In this latter case,
the percentage of cases where the fully correct
parse is included would be a relevant performance
indicator. Percentage of correct chunks in the top
parse is another useful indicator.
When tested on untagged sentences, on the 1065
linguistically rich sentence corpus forming part of
the manually checked parsed corpus developed by
us, the parser could generate fully correct parse
within the top 5 parses in 930 cases, that is, 87.32%
of the cases. In 683 cases the correct parse was
the top parse, 146 correct parses were found in
position 2, 56 in position 3, 29 in position 4 and
16 in position 5. Thus the mean rank of the
correct parses is 1.44. There is a clear tendency
for the correct parses to appear close to the top,
thereby verifying the best first strategy. If top 10
parses are generated, correct parse is obtained in
52 more cases and the Mean Rank Score goes to 1.75.
We give below the performance on the whole of
our 4000 strong manually checked corpus. Plain sen-
tences and POS tagged sentences have been tested
separately. The results are summarized in table 3.
Here, we have restricted the parsing time taken by
the best first search algorithm to 3 epoch seconds for
each sentence.
77
Table 3: Performance of the Best First Search Mod-
ule - Test Data of 4000 Sentences
Rank No. of Correct Parses(Plain) (POS tagged)
1 1210 2193
2 352 495
3 157 164
4 83 129
5 68 91
% of Correct 46.75 76.80
Parses in Top 5
% of Correct 83.92 88.26
Chunks in
Best Parse
In about 77% of the cases, the fully correct parse
is found within the top 5 parses when the input
sentences are POS tagged. Given the nature of
chunks produced in UCSG, this is quite encouraging.
In fact the top parse is nearly correct in many cases.
Further experiments and manual evaluations are
planned.
We have also observed that 96.01% of the words
are assigned correct POS tags in the top parse. We
observe that most of the times the top parse given
by the parse generation module is almost correct.
Chunkers are usually evaluated just for the per-
centage of correct chunks they produce. We have
placed greater demands on ourselves and we expect
our parser to produce optimal chunk sequence for
the whole sentence. Further, we produce all (or top
few) combinations and that too in hopefully a best
first order. Also, the very nature of chunks in UCSG
makes the task more challenging. More over, we have
used a fairly fine grained tag set with more than 70
tags. The data we have started with, namely the
BNC POS tagged corpus, is far from perfect. Given
these factors, the performance we are able to achieve
both in terms of percentage of correct chunks in the
top parse and rank of the fully correct parse is very
encouraging.
4 Transcripts:
Here we give the actual transcripts from the system.
For want of space, only a very simple example has
been included. Stars have been added in the begin-
ning of lines containing correct alternatives.
Input: I am studying at University of Hyderabad.
Tags from the Dictionary: <PNN_CRD><i>##<VBB><am>##<VVG>
<studying>##<PRN_PRP_AVP><at>##<NN1><university>
##<PRN_PRF_AVP><of>##<NP0><Hyderabad>##
Chunks Recognized by the FSM:
<ng><0-1><CRD><i>
<ajg><0-1><CRD><i>
*<ng><0-1><PNN><i>
<vg><1-2><VBB><am>
*<vg><1-3><VBB><am>##<VVG><studying>
<vg><1-4><VBB><am>##<VVG><studying>##<AVP><at>
<vgs><2-3><VVG><studying>
<ng><2-3><VVG><studying>
<ajg><2-3><VVG><studying>
<vgs><2-4><VVG><studying>##<AVP><at>
<ng><2-5><VVG><studying>##<PRP><at>##<NN1><university>
<ng><2-7><VVG><studying>##<PRP><at>##<NN1><university>
##<PRF><of>##<NP0><hyderabad>
<part><3-4><AVP><at>
<ng><3-5><PRP><at>##<NN1><university>
*<ng><3-7><PRP><at>##<NN1><university>##<PRF><of>##
<NP0><hyderabad>
<ng><4-5><NN1><university>
<ng><4-7><NN1><university>##<PRF><of>##<NP0><hyderabad>
<part><5-6><AVP><of>
<ng><5-7><PRF><of>##<NP0><hyderabad>
<ng><6-7><NP0><hyderabad>
Ranking by HMMs:
*<ng><0-1><PNN><i><-3.2491231040407><1><3><1>
<ng><0-1><CRD><i><-9.56376400947296><2><3><1>
<ajg><0-1><CRD><i><-36.8109739544272><3><3><1>
<vg><1-2><VBB><am><-7.27367328109116><1><3><2>
*<vg><1-3><VBB><am>##<VVG><studying><-15.945895214915>
<2><3><2>
<vg><1-4><VBB><am>##<VVG><studying>##<AVP><at>
<-25.5608664628101><3><3><2>
<vgs><2-3><VVG><studying><-10.5328994260119><1><6><3>
<ng><2-3><VVG><studying><-12.7929752284183><2><6><3>
<vgs><2-4><VVG><studying>##<AVP><at><-20.147870673907>
<3><6><3>
<ng><2-5><VVG><studying>##<PRP><at>##<NN1><university>
<-30.3473074722636><4><6><3>
<ajg><2-3><VVG><studying><-32.767076078699><5><6><3>
<ng><2-7><VVG><studying>##<PRP><at>##<NN1><university>##
<PRF><of>##<NP0><hyderabad><-35.1643970692879><6><6><3>
<part><3-4><AVP><at><-7.99897865005313><1><3><4>
<ng><3-5><PRP><at>##<NN1><university><-15.7772256956695>
<2><3><4>
*<ng><3-7><PRP><at>##<NN1><university>##<PRF><of>##<NP0>
<hyderabad><-20.5943152926938><3><3><4>
<ng><4-5><NN1><university><-13.2259579687766><1><2><5>
<ng><4-7><NN1><university>##<PRF><of>##<NP0><hyderabad>
<-18.0430475658009><2><2><5>
<part><5-6><AVP><of><-3.87313237166961><1><2><6>
<ng><5-7><PRF><of>##<NP0><hyderabad><-19.0843146188301>
<2><2><6>
<ng><6-7><NP0><hyderabad><-3.43828759462479><1><1><7>
Final Parse:
*<ng>[<PNN><i>]</ng> <vg>[<VBB><am>##<VVG><studying>]</vg>
<ng>[<PRP><at>##<NN1><university>##<PRF><of>##<NP0>
<hyderabad>]</ng> -- -41.2629507152745
<ng>[<PNN><i>]</ng> <vg>[<VBB><am>]</vg> <ng>[<VVG>
<studying>]</ng><ng>[<PRP><at>##<NN1><university>##<PRF>
<of>##<NP0><hyderabad>]</ng> -- -46.7375549370651
<ng>[<PNN><i>]</ng> <vg>[<VBB><am>]</vg> <ng>[<VVG>
<studying>##<PRP><at>##<NN1><university>##<PRF><of>##
<NP0><hyderabad>]</ng> -- -47.1608105580448
<ng>[<CRD><i>]</ng> <vg>[<VBB><am>##<VVG><studying>]</vg>
<ng>[<PRP><at>##<NN1><university>##<PRF><of>##<NP0>
<hyderabad>]</ng> -- -47.5775916207068
<ng>[<PNN><i>]</ng> <vg>[<VBB><am>##<VVG><studying>##
<AVP><at>]</vg><ng>[<NN1><university>##<PRF><of>##
<NP0><hyderabad>]</ng> -- -48.3266542362767
78
5 Conclusions:
A hybrid architecture for developing wide coverage
shallow parsing systems, without need for a large
scale parsed corpus to start with, has been proposed
and its effectiveness demonstrated by developing a
wide coverage shallow parser for English. The sys-
tem has been built and tested on very large data sets,
covering a wide variety of texts, giving us confidence
that the system will perform well on new, unseen
texts. The system is general and not domain spe-
cific, but we can adapt and fine tune for any specific
domain to achieve better performance. We are con-
fident that wide coverage and robust shallow parsing
systems can be developed using the UCSG architec-
ture for other languages of the world as well. We
plan to continue our work on English parsing while
we also start our work on Telugu.
References
Steven P. Abney. 1991. Parsing by Chunks. Kluwer,
Principle-Based Parsing: Computation and Psy-
cholinguistics edition.
Steven P. Abney. 1996. Partial Parsing via Finite-
State Cascades. In Workshop on Robust Parsing,
8th European Summer School in Logic, Language
and Information, pages 8?15, Prag.
L. Burnard. 2000. The Users? Reference Guide for
the British National Corpus. Oxford University
Computing Services, Oxford.
Xavier Carreras and Lluys Marquez. 2003. Phrase
Recognition by Filtering and Ranking with Per-
ceptrons. In Proceedings of the International
Conference on Recent Advances in Natural Lan-
guage Processing, RANLP-2003, pages 127?132,
Borovets, Bulgaria.
Herve Dejean. 2002. Learning Rules and their Ex-
ceptions. In Journal of Machine Learning Re-
search, Volume 2, pages 669?693.
G. Grefenstette. 1996. Light Parsing as Finite State
Filtering. In Workshop on Extended Finite State
Models of Language, Budapest, Hungary.
A. S. Hornby. 1975. Guide to Patterns and Usage in
English. Oxford University Press.
Hla Hla Htay, G. Bharadwaja Kumar, and
Kavi Narayana Murthy. 2006. Constructing
English-Myanmar Parallel Corpora. In Proceed-
ings of Fourth International Conference on Com-
puter Applications, pages 231?238, Yangon, Myan-
mar.
G Bharadwaja Kumar and Kavi Narayana Murthy.
2006. UCSG Shallow Parser. Proceedings of CI-
CLING 2006, LNCS, 3878:156?167.
G. Bharadwaja Kumar. 2007. UCSG Shallow
Parser: A Hybrid Architecture for a Wide Cover-
age Natural Language Parsing System. Phd thesis,
University of Hyderabad.
B Megyesi. 2002. Shallow Parsing with PoS Taggers
and Linguistic Features. In Journal of Machine
Learning Research, Volume 2, pages 639?668.
Antonio Molina and Ferran Pla. 2002. Shallow Pars-
ing using Specialized HMMs. In Journal of Ma-
chine Learning Research, Volume 2, pages 595?
613.
Kavi Narayana Murthy. 1995. Universal Clause
Structure Grammar. Phd thesis, University of Hy-
derabad.
Miles Osborne. 2002. Shallow Parsing using Noisy
and Non-Stationary Training Material. In Journal
of Machine Learning Research, Volume 2, pages
695?719.
E. Roche. 1997. Parsing with Finite State Transduc-
ers. MIT Press, finite-State Language Processing
edition.
T.G. Rose, M. Stevenson, and M. Whitehead. 2002.
The Reuters Corpus Volume 1 - from Yesterday?s
News to Tomorrow?s Language Resources. In Pro-
ceedings of the Third International Conference on
Language Resources and Evaluation, Las Palmas
de Gran Canaria.
Geoffrey Sampson. 1995. English for the Computer.
Clarendon Press (The Scholarly Imprint of Oxford
University Press).
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-2000,
pages 127?132, Lisbon, Portugal.
Erik F. Tjong Kim Sang. 2002. Memory-Based Shal-
low Parsing. In Journal of Machine Learning Re-
search, Volume 2, pages 559?594.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 Shared Task:
Chunking. In Claire Cardie, Walter Daelemans,
Claire Nedellec, and Erik Tjong Kim Sang, edi-
tors, Proceedings of CoNLL-2000 and LLL-2000,
pages 127?132. Lisbon, Portugal.
79
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 41?50,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Named Entity Recognition for Telugu
P Srikanth and Kavi Narayana Murthy
Department of Computer and Information Sciences,
University of Hyderabad,
Hyderabad, 500 046,
email: patilsrik@yahoo.co.in, knmuh@yahoo.com
Abstract
This paper is about Named Entity Recogni-
tion (NER) for Telugu. Not much work has
been done in NER for Indian languages in
general and Telugu in particular. Adequate
annotated corpora are not yet available in
Telugu. We recognize that named entities
are usually nouns. In this paper we there-
fore start with our experiments in building
a CRF (Conditional Random Fields) based
Noun Tagger. Trained on a manually tagged
data of 13,425 words and tested on a test
data set of 6,223 words, this Noun Tagger
has given an F-Measure of about 92%. We
then develop a rule based NER system for
Telugu. Our focus is mainly on identify-
ing person, place and organization names.
A manually checked Named Entity tagged
corpus of 72,157 words has been developed
using this rule based tagger through boot-
strapping. We have then developed a CRF
based NER system for Telugu and tested
it on several data sets from the Eenaadu
and Andhra Prabha newspaper corpora de-
veloped by us here. Good performance has
been obtained using the majority tag con-
cept. We have obtained overall F-measures
between 80% and 97% in various experi-
ments.
Keywords: Noun Tagger, NER for Telugu, CRF,
Majority Tag.
1 Introduction
NER involves the identification of named entities
such as person names, location names, names of
organizations, monetary expressions, dates, numer-
ical expressions etc. In the taxonomy of Compu-
tational Linguistics, NER falls within the category
of Information Extraction which deals with the ex-
traction of specific information from given docu-
ments. NER emerged as one of the sub-tasks of the
DARPA-sponsored Message Understanding Confer-
ence (MUCs). The task has important significance in
the Internet search engines and is an important task
in many of the Language Engineering applications
such as Machine Translation, Question-Answering
systems, Indexing for Information Retrieval and Au-
tomatic Summarization.
2 Approaches to NER
There has been a considerable amount of work on
NER in English (Isozaki and Kazawa, 2002; Zhang
and Johnson, 2003; Petasis et al, 2001; Mikheev
et al, 1999). Much of the previous work on name
finding is based on one of the following approaches:
(1) hand-crafted or automatically acquired rules or
finite state patterns (2) look up from large name lists
or other specialized resources (3) data driven ap-
proaches exploiting the statistical properties of the
language (statistical models).
The earliest work in named-entity recognition in-
volved hand-crafted rules based on pattern matching
(Appelt et al, 1993). For instance, a sequence of
capitalized words ending in ?Inc.? is typically the
name of an organization in the US, so one could im-
plement a rule to that effect. Another example of
such a rule is: Title Capitalized word ? Title Per-
son name. Developing and maintaining rules and
dictionaries is a costly affair and adaptation to dif-
ferent domains is difficult.
41
In the second approach, the NER system recog-
nizes only the named entities stored in its lists, also
called gazetteers. This approach is simple, fast, lan-
guage independent and easy to re-target - just re-
create the lists. However, named entities are too
numerous and are constantly evolving. Even when
named entities are listed in the dictionaries, it is not
always easy to decide their senses. There can be
semantic ambiguities. For example, ?Washington?
refers to both person name as well as place name.
Statistical models have proved to be quite ef-
fective. Such models typically treat named-entity
recognition as a sequence tagging problem, where
each word is tagged with its entity type if it is part
of an entity. Machine learning techniques are rela-
tively independent of language and domain and no
expert knowledge is needed. There has been a lot
of work on NER for English employing the machine
learning techniques, using both supervised learning
and unsupervised learning. Unsupervised learning
approaches do not require labelled training data -
training requires only very few seed lists and large
unannotated corpora (Collins and Singer, 1999). Su-
pervised approaches can achieve good performance
when large amounts of high quality training data is
available. Statistical methods such as HMM (Bikel
et al, 1997; Zhou and Su, 2001), Decision tree
model (Baluja et al, 2000; Isozaki, 2001), and con-
ditional random fields (McCallum, 2003) have been
used. Generative models such as Hidden Markov
Models (Bikel et al, 1997; Zhou and Su, 2001) have
shown excellent performance on the Message Un-
derstanding Conference (MUC) data-set (Chinchor,
1997). However, developing large scale, high qual-
ity training data is itself a costly affair.
3 NER for Indian languages
NLP research around the world has taken giant leaps
in the last decade with the advent of effective ma-
chine learning algorithms and the creation of large
annotated corpora for various languages. However,
annotated corpora and other lexical resources have
started appearing only very recently in India. Not
much work has been done in NER in Indian lan-
guages in general and Telugu in particular. Here we
include a brief survey.
In (Eqbal, 2006), a supervised learning system
based on pattern directed shallow parsing has been
used to identify the named entities in a Bengali cor-
pus. Here the training corpus is initially tagged
against different seed data sets and a lexical con-
textual pattern is generated for each tag. The entire
training corpus is shallow parsed to identify the oc-
currence of these initial seed patterns. In a position
where the seed pattern matches wholly or in part,
the system predicts the boundary of a named entity
and further patterns are generated through bootstrap-
ping. Patterns that occur in the entire training corpus
above a certain threshold frequency are considered
as the final set of patterns learned from the training
corpus.
In (Li and McCallum, 2003), the authors have
used conditional random fields with feature induc-
tion to the Hindi NER task. The authors have iden-
tified those feature conjunctions that will signifi-
cantly improve the performance. Features consid-
ered here include word features, character n-grams
(n = 2,3,4), word prefix and suffix (length - 2,3,4)
and 24 gazetteers.
4 NER for Telugu
Telugu, a language of the Dravidian family, is spo-
ken mainly in southern part of India and ranks sec-
ond among Indian languages in terms of number of
speakers. Telugu is a highly inflectional and agglu-
tinating language providing one of the richest and
most challenging sets of linguistic and statistical fea-
tures resulting in long and complex word forms (Ku-
mar et al, June 2007). Each word in Telugu is in-
flected for a very large number of word forms. Tel-
ugu is primarily a suffixing Language - an inflected
word starts with a root and may have several suffixes
added to the right. Suffixation is not a simple con-
catenation and morphology of the language is very
complex. Telugu is also a free word order Language.
Telugu, like other Indian languages, is a resource
poor language - annotated corpora, name dictionar-
ies, good morphological analyzers, POS taggers etc.
are not yet available in the required measure. Al-
though Indian languages have a very old and rich
literary history, technological developments are of
recent origin. Web sources for name lists are avail-
able in English, but such lists are not available in
Telugu forcing the use of transliteration.
42
In English and many other languages, named en-
tities are signalled by capitalization. Indian scripts
do not show upper-case - lower-case distinction.
The concept of capitalization does not exist. Many
names are also common nouns. Indian names are
also more diverse i.e there are lot of variations for
a given named entity. For example ?telugude:s?aM?
is written as Ti.Di.pi, TiDipi, te.de.pa:, de:s?aM etc.
Developing NER systems is thus both challenging
and rewarding. In the next section we describe our
work on NER for Telugu.
5 Experiments and Results
5.1 Corpus
In this work we have used part of the LERC-UoH
Telugu corpus, developed at the Language Engineer-
ing Research Centre at the Department of Computer
and Information Sciences, University of Hyderabad.
LERC-UoH corpus includes a wide variety of books
and articles, and adds up to nearly 40 Million words.
Here we have used only a part of this corpus includ-
ing news articles from two of the popular newspa-
pers in the region. The Andhra Prabha (AP) cor-
pus consists of 1.3 Million words, out of which there
are approximately 200,000 unique word forms. The
Eenaadu (EE) corpus consists of 26 Million words
in all.
5.2 Evaluation Metrics
We use two standard measures, Precision, Recall.
Here precision (P) measures the number of correct
NEs in the answer file (Machine tagged data ) over
the total number of NEs in the answer file and re-
call (R) measures the number of correct NEs in the
answer file over the total number of NEs in the key
file (gold standard). F-measure (F) is the harmonic
mean of precision and recall: F = (?2+1)PR?2R+P when
?2 = 1. The current NER system does not handle
multi-word expressions - only individual words are
recognized. Partial matches are also considered as
correct in our analyses here. Nested entities are not
yet handled.
5.3 Noun Identification
Named entities are generally nouns and it is there-
fore useful to build a noun identifier. Nouns can
be recognized by eliminating verbs, adjectives and
closed class words. We have built a CRF based bi-
nary classifier for noun identification. Training data
of 13,425 words has been developed manually by
annotating each word as noun or not-noun. Next we
have extracted the following features for each word
of annotated corpus:
? Morphological features: Morphological an-
alyzer developed at University of Hyderabad
over the last many years has been used to ob-
tain the root word and the POS category for the
given word. A morphological analyzer is use-
ful in two ways. Firstly, it helps us to recog-
nize inflected forms (which will not be listed in
the dictionary) as not named entities. Secondly,
word forms not recognized by morphology are
likely to be named entities.
? Length: This is a binary feature whose value
is 1 if length of the given word is less than or
equal 3 characters, otherwise 0. This is based
on the observation that very short words are
rarely nouns.
? Stop words: A stop word list including func-
tion words has been collected from exist-
ing bi-lingual dictionaries. Bi-lingual dic-
tionaries used for our experiments include C
P Brown?s English-Telugu dictionary (Brown,
1997), Telugu-Hindi dictionary developed at
University of Hyderabad and the Telugu-
English dictionary developed by V Rao Ve-
muri. We have also extracted high frequency
words from our corpora. Initially words which
have occurred 1000 times or more were se-
lected, hand filtered and added to the stop word
list. Then, words which have occurred 500 to
1000 times were looked at, hand filtered and
added to the stop word list. The list now has
1731 words. If the given word belongs to this
list, the feature value is 1 otherwise 0.
? Affixes: Here, we use the terms prefix/suffix
to mean any sequence of first/last few charac-
ters of a word, not necessarily a linguistically
meaningful morpheme. The use of prefix and
suffix information is very useful for highly in-
flected languages. Here we calculate suffixes
of length from 4 characters down to 1 char-
acter and prefixes of length from 7 characters
43
down to 1 character. Thus the total number of
prefix/suffix features are 11. For example, for
the word ?virigiMdi? (broke), the suffixes are
?iMdi, Mdi, di, i? and the prefixes are ?virigiM,
virigi, virig, viri, vir, vi, v?. The feature values
are not defined (ND) in the following cases:
? If length of a word is less than or equal to
3 characters, all the affix values are ND.
? If length of a word is from 4 to 6 charac-
ters, initial prefixes will be ND.
? If the word contains special symbols or
digits, both the suffix and prefix values are
ND.
? Position: This is a binary feature, whose value
is 1 if the given word occurs at the end of the
sentence, otherwise 0. Telugu is a verb final
language and this feature is therefore signifi-
cant.
? POS: A single dictionary file is compiled from
the existing bi-lingual dictionaries. This file in-
cludes the head word and its Part of Speech. If
a given word is available in this file, then its
POS tag is taken as feature otherwise feature
value is 0.
? Orthographic information This is a binary
feature whose value is 1 if a given word con-
tains digits or special symbols otherwise the
feature value is 0.
? Suffixes A list of linguistic suffixes of verbs,
adjectives and adverbs were compiled from
(Murthy and J.P.L.Gywnn, 1985) to recognize
not-nouns in a given sentence. This feature
value is 1 if the suffix of the given word be-
longs to this list, otherwise it is 0.
A feature vector consisting of the above features
is extracted for each word in the annotated corpus.
Now we have training data in the form of (Wi, Ti),
where Wi is the ith word and its feature vector, and
Ti is its tag - NOUN or NOT-NOUN. The feature
template used for training CRF is shown in Table-1,
where wi is the current word, wi?1 is previous
word, wi?2 is previous to previous word, wi+1 is
next word and wi+2 is next to next word.
wi?2
wi?1
wi
wi+1
wi+2
combination of wi?1, wi
combination of wi, wi+1
feature vector of wi
morph tags of wi?2, wi?1, wi, wi+1 and wi+2
output tag of current and previous word (ti,ti?1)
Table 1: Feature Template used for Training CRF
based Noun Tagger
The inputs for training CRF consists of the train-
ing data and the feature template. The model learned
during training is used for testing. Apart from the
basic features described above, we have also experi-
mented by including varying amounts of contextual
information in the form of neighbouring words and
their morph features. Let us define:
? F1: [(wi), feature vector of wi, ti, ti?1].
? F2 : [wi?1, wi+1, (wi?1, wi), (wi, wi+1) and
the morph tags of wi?1 and wi+1].
? F3 : [wi?2, wi+2, morph tags of wi?2 and
wi+2]
The CRF trained with the basic template F1,
which consists of the current word, the feature vec-
tor of the current word and the output tag of the pre-
vious word as the features, was tested on a test data
of 6,223 words and an F-measure of 91.95% was
obtained. Next, we trained the CRF by taking the
combination of F1 and F2. We also trained using
combination of F1, F2 and F3. The performances
of all 3 combinations are shown in Table-2. It may
be seen that performance of the system is reducing
as we increase the number of neighbouring words as
features. Adding contextual features does not help.
5.4 Heuristic based NER system
Nouns which have already been identified in the
noun identification phase are now checked for
named entities. In this work, our main focus
is on identifying person, place and organization
names. Indian place names and person names often
44
Feature combinations Precision Recall F-measure
F1 91.64 92.28 91.95
F1+F2 91.46 92.28 91.86
F1+F2+F3 91.17 91.99 91.57
Table 2: Performance of the CRF based Noun tagger with different feature combinations
have some suffix or prefix clues. For example
?na:yuDu? is a person suffix clue for identifying
?ra:ma:na:yuDu? as a person entity and ?ba:d? is a
location suffix clue for identifying ?haidara:ba:d?,
?adila:ba:d? etc as place entities. We have manually
prepared a list of such suffixes for both persons and
locations as also a list of prefixes for person names.
List of organization names is also prepared manu-
ally. We have also prepared a gazetteer consisting
of location names and a gazetteer of person name
contexts since context lists are also very useful in
identifying person names. For example, it has been
observed that whenever a context word such as
?maMtri? appears, a person name is likely to follow.
Regular expressions are used to identify person
entities like ?en.rame:S? and organization entities
which are in acronym form such as ?Ti.Di.pi?,
?bi.je.pi? etc. Initially one file of the corpus is
tagged using these seed lists and patterns. Then
we manually check and tag the unidentified named
entities. These new named entities are also added
to the corresponding gazetteers and the relevant
contexts are added to their corresponding lists.
Some new rules are also observed during manual
tagging of unidentified names. Here is an example
of a rule:
?if word[i] is NOUN and word[i-1] belongs to
the person context list then word[i] is person name?.
Currently the gazetteers include 1346 location
names, 221 organization names, and small lists of
prefixes, suffixes and other contextual cues that sig-
nal the presence of named entities, their types, or
their beginning or ending. Using these lists and
rules, we then tag another file from the remain-
ing corpus. This process of semi-automatic tagging
is continued for several iterations. This way we
have developed a named entity annotated database
of 72,157 words, including 6,268 named entities
(1,852 place names, 3,201 person names and 1,215
organization names).
5.4.1 Issues in Heuristic NER
There are ambiguities. For example, ?ko:Tla? is
a person first name in ?ko:Tla vijaybha:skar? and
it is also a common word that exists in a phrase
such as ?padi ko:Tla rupa:yalu? (10 crore rupees).
There also exists ambiguity between a person entity
and place entity. For example, ?siMha:calaM? and
?raMga:reDDi? are both person names as well as
place names. There are also some problems while
matching prefixes and suffixes of named entities.
For example ?na:Du? is a useful suffix for match-
ing place names and the same suffix occurs with
time entities such as ?so:mava:raMna:Du?. Prefixes
like ?ra:j? can be used for identifying person enti-
ties such as ?ra:jkiran?, ?ra:jgo:pa:l?,?ra:js?e:khar?
etc. but the same prefix also occurs with common
words like ?ra:jaki:ya:lu?. Thus these heuristics are
not fool proof. We give below the results of our ex-
periments using our heuristic based NER system for
Telugu.
5.4.2 Experiment 1
Here, we have presented the performance of the
heuristic-based NER system over two test data sets
(AP-1 and AP-2). These test data sets are from the
AP corpus. Total number of words (NoW) and num-
ber of named entities in the test data sets AP-1 and
AP-2 are given in Table-3. Performance of the sys-
tem is measured in terms of F-measure. The rec-
ognized named entity must be of the correct type
(person, place or organization) for it to be counted
as correct. A confusion matrix is also given. The
notation used is as follows: PER - person; LOC -
location; ORG - organization; NN - not-name. The
results are depicted in Tables 4, 5 and 6.
45
AP-1 AP-2
PER LOC ORG PER LOC ORG
P (%) 83.44 97.5 97.40 60.57 87.93 87.5
R (%) 84.84 96.29 87.20 72.83 86.56 77.77
F (%) 84.13 96.89 92.01 66.13 87.23 82.34
Table 4: Performance of Heuristic based NER System
AP Corpus PER LOC ORG NoW
AP-1 296 81 86 3,537
AP-2 173 321 63 7,032
Table 3: Number of Entities in Test Data Sets
Actual/Obtained PER LOC ORG NN
PER 285 0 0 12
LOC 0 81 0 0
ORG 6 0 75 5
NN 63 3 3 3004
Table 5: Confusion Matrix for the Heuristic based
System on AP-1
Actual/Obtained PER LOC ORG NN
PER 126 0 0 47
LOC 2 277 0 41
ORG 0 0 49 14
NN 80 38 7 6351
Table 6: Confusion matrix of heuristic based system
on AP-2
5.5 CRF based NER system
Now that we have developed a substantial amount of
training data, we have also attempted supervised ma-
chine learning techniques for NER. In particular, we
have used CRFs. For the CRF based NER system,
the following features are extracted for each word
of the labelled training data built using the heuristic
based NER system.
? Class Suffixes/Prefixes This includes the fol-
lowing three features:
? Location suffix: If the given word contains
a location suffix, feature value is 1 other-
wise 0.
? Person suffix: If the given word contains a
person suffix, feature value is 1 otherwise
it is 0.
? Person prefix: If the given word contains a
person prefix, feature value is 1 otherwise
it is 0.
? Gazetteers Five different gazetteers have been
used. If the word belongs to the person first
name list, feature value is 1 else if the word be-
longs to person middle name list, feature value
is 2 else if the word belongs to person last name
list, feature value is 3 else if the word belongs
to location list, feature value is 4 else if the
word belongs to organization list, feature value
is 5 else feature value is 0.
? Context If the word belongs to person context
list, feature value is 1 else if the word belongs
to location context list, feature value is 2 else
if the word belongs to organization context list,
feature value is 3 else the feature value is 0.
? Regular Expression This includes two fea-
tures as follows:
? REP: This is regular expression used to
identify person names. The feature value
is 1 if the given word matches.
/([a-zA-Z:?]{1,3})\.(
[a-zA-Z:?]{1,3})?\.?(
[a-zA-Z:?]{1,3})?\.?
[a-zA-Z:??]{4,}/
? REO: This is regular expression used
to identify organization names men-
tioned in acronym format like ?bi.je.pi?,
?e.ai.Di.eM.ke?. etc. This feature value is
1, if the given word matches
/(.{1,3})\.(.{1,3})\.
(.{1,3})\.(.{1,3})?\.?
(.{1,3})?\.?/)/
46
? Noun tagger Noun tagger output is also used
as a feature value.
? Orthographic Information, Affixes, Mor-
phological feature, Position feature, Length
are directly extracted from ?Noun Identifica-
tion? process.
The training data used for training CRFs consists
of words, the corresponding feature vectors and the
corresponding name tags. We have used ?CRF++:
Yet another CRF toolkit? (Taku, ) for our experi-
ments. Models are built based on training data and
the feature template. Results are given in the next
subsection. These models are used to tag the test
data. The feature template used in these experiments
is as follows:
wi?3
wi?2
wi?1
wi
wi+1
wi+2
combination of wi?1, wi
combination of wi, wi+1
feature vector of wi
morph tags of wi?2, wi?1, wi, wi+1 and wi+2
output tag of the previous word ti?1
context information of the neighbour words
Table 7: Feature Template used for Training CRF
5.5.1 Experiment 2
In this experiment, we took 19,912 words of
training data (TR-1) and trained the CRF engine
with different feature combinations of the feature
template. Details of the training data (TR-1 ?
TR-2 ? TR-3) and test data sets used in these
experiments are given in Tables 8 and 9. Here the
experiments are performed by varying the number
of neighbouring words in the feature template. In
the first case, feature template consists of current
word (wi), feature vector of the current word, two
neighbours of the current word (wi?1, wi+1), morph
tags of the neighbour words, context information
of the neighbour words, combination of current
word and its neighbours and the output tag of the
previous word. A model is built by training the CRF
engine using this template. The model built is used
in testing data sets (AP-1 and AP-2). Similarly,
we repeated the same experiment by considering
4 and 6 neighbouring words of the current word
in the feature template. The results are shown in
Table-9 with varying number of neighbour words
represented as window-size. It is observed that there
is not much improvement in the performance of
the system by including more of the neighbouring
words as features.
Performance of the system without taking
gazetteer features is shown in Table-11. We see
that the performance of the system reduces when we
have not considered morph features and Noun tagger
output in the feature template as can be seen from
Table-12.
Finally, we have tested the performance of the
system on two new test data sets (EE-1 and EE-2)
from the EE corpus with varying amounts of training
data. Total number of words (NoW) and the number
of named entities in the test data sets EE-1 and EE-2
are depicted in Table-8. Performance of the system
in terms of F-measure is shown in table 13.
EE Corpus PER LOC ORG NoW
EE-1 321 177 235 6,411
EE-2 325 144 187 5221
Table 8: Number of Entities in Test Data Sets
AP corpus PER LOC ORG NoW
TR-1 804 433 175 19,912
TR-2 1372 832 388 34,116
TR-3 2555 1511 793 60,525
Table 9: Number of Entities in Training Data Sets
Gazetteers have a major role in performance while
morph is adding a bit. F-Measures of 74% to 93%
AP-1 AP-2 EE-1 EE-2
PER 93.76 79.36 70.91 69.84
LOC 96.81 89.78 81.84 70.91
ORG 80.27 91.66 71.73 80.75
Table 12: Performance of the CRF based NER Sys-
tem without Morph and Noun Tagger Features
47
Win- AP-1 AP-2
Size PER LOC ORG PER LOC ORG
2 99.62 100 98.41 90.07 93.55 98.21
P 4 99.62 100 96.96 89.36 93.53 98.21
6 99.62 100 96.96 90.71 93.55 98.21
2 89.86 93.82 72.09 72.15 85.98 87.30
R 4 89.86 93.82 74.41 71.59 85.66 87.30
6 89.52 93.82 74.41 72.15 85.98 87.30
2 94.49 96.81 83.22 80.12 89.61 92.43
F 4 94.49 96.81 84.21 79.49 89.43 92.43
6 94.30 96.81 84.21 80.37 89.61 92.43
Table 10: Performance of CRF based NER system with different window sizes
AP-1 AP-2
PER LOC ORG PER LOC ORG
P 90.86 97.95 97.91 89.05 96.88 96.15
R 57.09 59.25 54.65 69.31 67.91 79.36
F 70.12 73.84 70.14 77.95 79.85 86.95
Table 11: Performance of the CRF based NER system without Gazetteers
Test Data CLASS TR-1 TR-2 TR-3
PER 75.14 79.70 81.58
EE-1 LOC 81.84 80.66 81.45
ORG 76.76 78.46 79.89
PER 69.98 74.47 79.70
EE-2 LOC 70.91 70.96 71.2
ORG 82.13 82.82 83.69
Table 13: Performance of CRF based NER system
with varying amounts of Training Data on EE Test
Data
have been obtained. Effect of training corpus size
has been checked by using 19,912 words, 34,116
words and 60,525 words training corpora built from
the AP newspaper corpus. Test data was from EE
newspaper. It is clearly seen that larger the training
data, better is the performance. See table 13.
5.5.2 Experiment 3: Majority Tag as an
Additional Feature
There are some names like ?kRSNa:?, which can
refer to either person name, place name or a river
name depending up on the context in which they are
used. Hence, if the majority tag is incorporated as
a feature, a classifier can be trained to take into ac-
count the context in which the named entity is used,
as well as frequency information. In this experiment,
we have used an unlabelled data set as an additional
resource from the EE news corpus. The unlabelled
data set consists of 11,789 words.
Initially, a supervised classifier h1 is trained on
the labelled data (TR-3) of 60,525 words. Then this
classifier labels the unlabelled data set (U) (11,789
words) and produces a machine tagged data set U ?.
Although our NER system is not so robust, useful
information can still be gathered as we shall see be-
low.
Next, a majority tag list (L) is produced by ex-
tracting the list of named entities with their associ-
ated majority tags from the machine tagged data set
U ?. The process of extracting majority tag list (L) is
simple: We first identify possible name classes as-
signed for the named entities in U ? and we assign
the class that has occurred most frequently. Next, in
order to recover unidentified named entities (inflec-
tions of named entities already identified), we com-
pare the root words of those words whose class is as-
signed neither to person, place or organization with
the named entities already identified. If there is any
match with any of the named entities, the tag of the
identified named entity is assigned to the unidenti-
48
EE Without Majority Tag With Majority Tag
Corpus PER LOC ORG PER LOC ORG
P 96.99 98.4 99.36 97.02 98.38 98.78
R 70.40 69.49 66.80 71.02 68.92 68.93
F 81.58 81.45 79.89 82.01 81.06 81.20
Table 14: Performance of CRF based NER using Maj-tag on EE-1
EE Without Majority Tag With Majority Tag
Corpus PER LOC ORG PER LOC ORG
P 98.18 83.96 98.55 98.22 84.11 97.88
R 67.07 61.80 72.72 68 62.5 74.31
F 79.70 71.2 83.69 80.36 71.71 84.49
Table 15: Performance of CRF based NER using Maj-tag on EE-2
fied named entity. L thus consists of (NE, Maj-tag)
pairs, where Maj-tag is the name class that occurs
most frequently for the named entity (NE) in the ma-
chine tagged data set U ?.
Now, we add this Maj-tag as an additional feature
to labelled data (TR-3): if a word in labelled data
matches with a named entity in the majority tag list
(L), then the corresponding Maj-tag (name class) is
assigned as a feature value to that word in the la-
belled data. Finally, a classifier h2 is trained on the
labelled data (TR-3). We use this classifier (h2) to
tag the test data sets (EE-1 and EE-2). It can be
observed from tables 14 and 15 that including the
majority tag feature improves the performance a bit.
6 Conclusions
Not much work has been done in NER in Telugu
and other Indian languages so far. In this paper, we
have reported our work on Named Entity Recogni-
tion for Telugu. We have developed a CRF based
noun tagger, whose output is used as one of the
feature for the CRF based NER system. We have
also described how we have developed a substantial
training data using a heuristic based system through
boot-strapping. The CRF based system performs
better when compared with the initial heuristic based
system. We have also shown that performance of
the system can be improved by adding gazetteers as
features. Morphological analyser has shown a small
contribution to the performance of the system. It
is also observed that there is some increase in per-
formance of the system by using majority tag con-
cept. We have obtained F-measures between 80%
and 97% in various experiments. It may be observed
that we have not used any POS tagger or parser or
annotated corpora tagged with POS or syntactic in-
formation. Once adequate POS taggers and chun-
kers are developed, we may be able to do better. The
current work is limited to recognizing single word
NEs. We plan to consider multi-token named enti-
ties and nested structures in our future work.
References
D. Appelt, J. Hobbs, J. Bear, D. Israel, M. Kameyama,
A.Kehler, D. Martin, K.Meyers, and M. Tyson. 1993.
SRI international FASTUS system: MUC-6 test results
and analysis.
Shumeet Baluja, Vibhu O. Mittal, and Rahul Suk-
thankar. 2000. Applying Machine Learning for
High-Performance Named-Entity Extraction. Compu-
tational Intelligence, 16(4):586?596.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the fifth conference on Applied natural language pro-
cessing, pages 194?201, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Charles Philip Brown. 1997. Telugu-English dictionary.
New Delhi Asian Educational Services.
Nancy Chinchor. 1997. MUC-7 Named Entity Task Def-
inition (version 3.0). In Proceedings of the 7th Mes-
sage Understanding Conference (MUC-7).
49
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora.
Asif Eqbal. 2006. Named Entity Recognition for Ben-
gali. Satellite Workshop on Language, Artificial Intel-
ligence and Computer Science for Natural Language
Applications (LAICS-NLP), Department of Computer
Engineering Faculty of Engineering Kasetsart Univer-
sity, Bangkok, Thailand.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition.
In Proceedings of the 19th international conference
on Computational linguistics, pages 1?7, Morristown,
NJ, USA. Association for Computational Linguistics.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision tree
learning. In ACL ?01: Proceedings of the 39th An-
nual Meeting on Association for Computational Lin-
guistics, pages 314?321, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
G. Bharadwaja Kumar, Kavi Narayana Murthy, and
B.B.Chaudhari. June 2007. Statistical Analysis of
Telugu Text Corpora. IJDL,Vol 36, No 2, pages 71?
99.
Wei Li and Andrew McCallum. 2003. Rapid develop-
ment of Hindi named entity recognition using con-
ditional random fields and feature induction. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 2(3):290?294.
McCallum. 2003. Early results for Named Entity Recog-
nition with Conditional Random Fields, feature induc-
tion and web-enhanced lexicons. In Proceedings of
the seventh conference on Natural language learning
at HLT-NAACL 2003, pages 188?191, Morristown, NJ,
USA. Association for Computational Linguistics.
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named Entity Recognition without gazetteers. In Pro-
ceedings of the ninth conference on European chap-
ter of the Association for Computational Linguistics,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Bh.Krishna Murthy and J.P.L.Gywnn. 1985. A Grammar
of Modern Telugu. Oxford University Press, Delhi.
Georgios Petasis, Frantz Vichot, Francis Wolinski, Geor-
gios Paliouras, Vangelis Karkaletsis, and Constan-
tine D. Spyropoulos. 2001. Using machine learning
to maintain rule-based named-entity recognition and
classification systems. In ACL ?01: Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 426?433, Morristown, NJ,
USA. Association for Computational Linguistics.
Taku. http://crfpp.sourceforge.net/.
Tong Zhang and David Johnson. 2003. A Robust Risk
Minimization based Named Entity Recognition sys-
tem. In Proceedings of the seventh conference on Nat-
ural language learning at HLT-NAACL 2003, pages
204?207, Morristown, NJ, USA. Association for Com-
putational Linguistics.
GuoDong Zhou and Jian Su. 2001. Named Entity
Recognition using an HMM-based chunk tagger. In
ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
473?480, Morristown, NJ, USA. Association for Com-
putational Linguistics.
50
Myanmar Word Segmentation using Syllable level Longest Matching
Hla Hla Htay, Kavi Narayana Murthy
Department of Computer and Information Sciences
University of Hyderabad, India
hla hla htay@yahoo.co.uk, knmuh@yahoo.com
Abstract
In Myanmar language, sentences are
clearly delimited by a unique sentence
boundary marker but are written without
necessarily pausing between words with
spaces. It is therefore non-trivial to seg-
ment sentences into words. Word tokeniz-
ing plays a vital role in most Natural Lan-
guage Processing applications. We observe
that word boundaries generally align with
syllable boundaries. Working directly with
characters does not help. It is therefore
useful to syllabify texts first. Syllabification
is also a non-trivial task in Myanmar. We
have collected 4550 syllables from avail-
able sources . We have evaluated our syl-
lable inventory on 2,728 sentences spread
over 258 pages and observed a coverage of
99.96%. In the second part, we build word
lists from available sources such as dic-
tionaries, through the application of mor-
phological rules, and by generating syllable
n-grams as possible words and manually
checking. We have thus built list of 800,000
words including inflected forms. We have
tested our algorithm on a 5000 sentence
test data set containing a total of (35049
words) and manually checked for evaluat-
ing the performance. The program recog-
nized 34943 words of which 34633 words
were correct, thus giving us a Recall of
98.81%, a Precision of 99.11% and a F-
Measure is 98.95%.
Key Words:- Myanmar, Syllable, Words, Seg-
mentation, Syllabification, Dictionary
1 Introduction
Myanmar (Burmese) is a member of the Burmese-
Lolo group of the Sino-Tibetan language spoken by
about 21 Million people in Myanmar (Burma). It
is a tonal language, that is to say, the meaning of a
syllable or word changes with the tone. It has been
classified by linguists as a mono-syllabic or isolating
language with agglutinative features. According to
history, Myanmar script has originated from Brahmi
script which flourished in India from about 500 B.C.
to over 300 A.D (MLC, 2002). The script is syllabic
in nature, and written from left to right.
Myanmar script is composed of 33 consonants,
11 basic vowels, 11 consonant combination sym-
bols and extension vowels, vowel symbols, devow-
elizing consonants, diacritic marks, specified sym-
bols and punctuation marks(MLC, 2002),(Thu and
Urano, 2006). Myanmar script represents sequences
of syllables where each syllable is constructed from
consonants, consonant combination symbols (i.e.
Medials), vowel symbols related to relevant conso-
nants and diacritic marks indicating tone level.
Myanmar has mainly 9 parts of speech: noun,
pronoun, verb, adjective, adverb, particle , conjunc-
tion, post-positional marker and interjection (MLC,
2005), (Judson, 1842).
In Myanmar script, sentences are clearly delim-
ited by a sentence boundary marker but words are
not always delimited by spaces. Although there is
a general tendency to insert spaces between phrases,
inserting spaces is more of a convenience rather than
The 6th Workshop on Asian Languae Resources, 2008
41
a rule. Spaces may sometimes be inserted between
words and even between a root word and the associ-
ated post-position. In fact in the past spaces were
rarely used. Segmenting sentences into words is
therefore a challenging task.
Word boundaries generally align with syllable
boundaries and syllabification is therefore a useful
strategy. In this paper we describe our attempts on
syllabification and segmenting Myanmar sentences
into words. After a brief discussion of the corpus
collection and pre-processing phases, we describe
our approaches to syllabification and tokenization
into words.
Computational and quantitative studies in Myan-
mar are relatively new. Lexical resources available
are scanty. Development of electronic dictionaries
and other lexical resources will facilitate Natural
Language Processing tasks such as Spell Checking,
Machine Translation, Automatic Text summariza-
tion, Information Extraction, Automatic Text Cate-
gorization, Information Retrieval and so on (Murthy,
2006).
Over the last few years, we have developed mono-
lingual text corpora totalling to about 2,141,496
sentences and English-Myanmar parallel corpora
amounting to about 80,000 sentences and sentence
fragments, aligned at sentence and word levels. We
have also collected word lists from these corpora
and also from available dictionaries. Currently our
word list includes about 800,000 words including in-
flected forms.
2 Myanmar Words
Myanmar words are sequences of syllables. The syl-
lable structure of Burmese is C(G)V((V)C), which
is to say the onset consists of a consonant option-
ally followed by a glide, and the rhyme consists of
a monophthong alone, a monophthong with a con-
sonant, or a diphthong with a consonant 1. Some
representative words are:
? CV [mei] girl
? CVC [me ?] crave
? CGV [mjei] earth
? CGVC [mje ?] eye
1http://en.wikipedia.org/wiki/Burmese language
? CVVC [maun] (term of address for young men)
? CGVVC [mjaun] ditch
Words in the Myanmar language can be divided
into simple words, compound words and complex
words (Tint, 2004),(MLC, 2005),(Judson, 1842).
Some examples of compound words and loan words
are given below.
? Compound Words
? head [u:]   + pack [htou ?]  = hat [ou ?
htou ?]  
language [sa] + look,see [kji.] 	
 +
[tai ?] building 
 = library [sa kji. dai ?]
	


? sell [yaun:]  + buy [we]  = trading
 [yaun : we]
? Loan Words
? 
 [kun pju ta] computer
? 
 [hsa ? ko ma
?
ti] sub-committee
? 