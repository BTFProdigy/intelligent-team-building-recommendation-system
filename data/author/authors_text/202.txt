Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 657?664
Manchester, August 2008
A Discriminative Alignment Model for Abbreviation Recognition
Naoaki Okazaki
?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou
?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii
??
tsujii@is.s.u-tokyo.ac.jp
?
Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?
School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper presents a discriminative align-
ment model for extracting abbreviations
and their full forms appearing in actual
text. The task of abbreviation recognition
is formalized as a sequential alignment
problem, which finds the optimal align-
ment (origins of abbreviation letters) be-
tween two strings (abbreviation and full
form). We design a large amount of fine-
grained features that directly express the
events where letters produce or do not pro-
duce abbreviations. We obtain the optimal
combination of features on an aligned ab-
breviation corpus by using the maximum
entropy framework. The experimental re-
sults show the usefulness of the alignment
model and corpus for improving abbrevia-
tion recognition.
1 Introduction
Abbreviations present two major challenges in nat-
ural language processing: term variation and am-
biguity. Abbreviations substitute for expanded
terms (e.g., dynamic programming) through the
use of shortened term-forms (e.g., DP). At the
same time, the abbreviation DP appearing alone in
text is ambiguous, in that it may refer to different
concepts, e.g., data processing, dirichlet process,
differential probability. Associating abbreviations
and their full forms is useful for various applica-
tions including named entity recognition, informa-
tion retrieval, and question answering.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The task of abbreviation recognition, in which
abbreviations and their expanded forms appearing
in actual text are extracted, addresses the term vari-
ation problem caused by the increase in the num-
ber of abbreviations (Chang and Sch?utze, 2006).
Furthermore, abbreviation recognition is also cru-
cial for disambiguating abbreviations (Pakhomov,
2002; Gaudan et al, 2005; Yu et al, 2006), pro-
viding sense inventories (lists of abbreviation def-
initions), training corpora (context information of
full forms), and local definitions of abbreviations.
Hence, abbreviation recognition plays a key role in
abbreviation management.
Numerous researchers have proposed a variety
of heuristics for recognizing abbreviation defini-
tions, e.g., the use of initials, capitalizations, syl-
lable boundaries, stop words, lengths of abbrevia-
tions, and co-occurrence statistics (Park and Byrd,
2001; Wren and Garner, 2002; Liu and Fried-
man, 2003; Okazaki and Ananiadou, 2006; Zhou
et al, 2006; Jain et al, 2007). Schwartz and
Hearst (2003) implemented a simple algorithm that
finds the shortest expression containing all alpha-
numerical letters of an abbreviation. Adar (2004)
presented four scoring rules to choose the most
likely expanded form in multiple candidates. Ao
and Takagi (2005) designed more detailed condi-
tions for accepting or discarding candidates of ab-
breviation definitions.
However, these studies have limitations in dis-
covering an optimal combination of heuristic rules
from manual observations of a corpus. For exam-
ple, when expressions transcrip
:
tion factor 1 and
thyroid transcription factor 1 are full-form can-
didates for the abbreviation TTF-1
1
, an algorithm
should choose the latter expression over the shorter
1
In this paper, we use straight and
::::
wavy underlines to rep-
resent correct and incorrect origins of abbreviation letters.
657
expression (former). Previous studies hardly han-
dle abbreviation definitions where full forms (e.g.,
water activity) shuffle their abbreviation letters
(e.g., AW). It is also difficult to reject ?negative?
definitions in a text; for example, an algorithm
should not extract an abbreviation definition from
the text, ?the replicon encodes a large
:::
replic
:
ation
protein (RepA),? since RepA provides a descrip-
tion of the protein rather than an abbreviation.
In order to acquire the optimal rules from
the corpora, several researchers applied machine
learning methods. Chang and Sch?utze (2006) ap-
plied logistic regression to combine nine features.
Nadeau and Turney (2005) also designed seven-
teen features to classify candidates of abbrevia-
tion definitions into positive or negative instances
by using the Support Vector Machine (SVM).
Notwithstanding, contrary to our expectations, the
machine-learning approach could not report better
results than those with hand-crafted rules.
We identify the major problem in the previ-
ous machine-learning approach: these studies did
not model associations between abbreviation let-
ters and their origins, but focused only on indirect
features such as the number of abbreviation letters
that appear at the head of a full form. This was
probably because the training corpus did not in-
clude annotations on the exact origins of abbrevia-
tion letters but only pairs of abbreviations and full
forms. It was thus difficult to design effective fea-
tures for abbreviation recognition and to reuse the
knowledge obtained from the training processes.
In this paper, we formalize the task of abbrevi-
ation recognition as a sequential alignment prob-
lem, which finds the optimal alignment (origins of
abbreviation letters) between two strings (abbrevi-
ation and full form). We design a large amount
of features that directly express the events where
letters produce or do not produce abbreviations.
Preparing an aligned abbreviation corpus, we ob-
tain the optimal combination of the features by us-
ing the maximum entropy framework (Berger et
al., 1996). We report the remarkable improve-
ments and conclude this paper.
2 Proposed method
2.1 Abbreviation alignment model
We express a sentence x as a sequence of letters
(x
1
, ..., x
L
), and an abbreviation candidate y in the
sentence as a sequence of letters (y
1
, ..., y
M
). We
define a letter mapping a = (i, j) to indicate that
the abbreviation letter y
j
is produced by the letter
in the full form x
i
. A null mapping a = (i, 0) indi-
cates that the letter in the sentence x
i
is unused to
form the abbreviation. Similarly, a null mapping
a = (0, j) indicates that the abbreviation letter y
j
does not originate from any letter in x. We de-
fine a
(x)
and a
(y)
in order to represent the first and
second elements of the letter mapping a. In other
words, a
(x)
and a
(y)
are equal to i and j respec-
tively, when a = (i, j). Finally, an abbreviation
alignment a is defined as a sequence of letter map-
pings, a = (a
1
, ..., a
T
), where T represents the
number of mappings in the alignment.
Let us consider the following example sentence:
We investigate the effect of thyroid tran-
scription factor 1 (TTF-1).
This sentence contains an abbreviation candidate
TTF-1 in parentheses
2
. Figure 1 illustrates the
correct alignment a (bottom line) and its two-
dimensional representation for the example sen-
tence
3
; the abbreviation letters ?t,? ?t,? ?f,? ?-,? and
?1? originate from x
30
, x
38
, x
52
, nowhere (null
mapping), and x
59
respectively.
We directly model the conditional probability of
the alignment a, given x and y, using the maxi-
mum entropy framework (Berger et al, 1996),
P (a|x,y) =
exp {? ? F (a,x,y)}
?
a?C(x,y)
exp {? ? F (a,x,y)}
.
(1)
In Formula 1, F = {f
1
, ..., f
K
} is a global feature
vector whose elements present K feature func-
tions, ? = {?
1
, ..., ?
K
} denotes a weight vector
for the feature functions, and C(x,y) yields a set
of possible alignments for the given x and y. We
obtain the following decision rule to choose the
most probable alignment a? for given x and y,
a? = argmax
a?C(x,y)
P (a|x,y). (2)
Note that a set of possible alignments C(x,y)
always includes a negative alignment whose ele-
ments are filled with null-mappings (refer to Sec-
tion 2.3 for further detail). This allows the formula
to withdraw the abbreviation candidate y when any
expression in x is unlikely to be a definition.
2
Refer to Section 3.1 for text makers for abbreviations.
3
We ignore non-alphabetical letters in abbreviations.
658
    We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x:
a
            ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~ ~ 
8 words 4 lettersmin(|y|+5, 2|y|)
<NIL> T  T  F  -   1  <SF>
y:
0 0 0 0 0 0 0 0 0
1
2
3
5
91 13 16 2122 25 28 30 38 47 52 55 59 61
1
0
1
2
3
4
5
6
t =
i
 j
a =
a 
2 3 4 5 6 7 8 9 10 11 12 13 14
Null outsideOther positions Abbreviation Null inside Associate inside
a = ((9,0), (13,0), (16,0), (21,0), (22, 0), (25,0), (28,0), (30,1), (38,2), (47,0), (52,3), (55,0), (59,5), (61,6))
Figure 1: The correct alignment for the example sentence and its two dimensional representation.
2.2 Features
The main advantage of the discriminative align-
ment model is its ability to incorporate a wide
range of non-independent features. Inspired
by feature engineering for Conditional Random
Fields (CRFs) (Lafferty et al, 2001), we design
two kinds of features: unigram (state) features de-
fined on each letter mapping, and bigram (tran-
sition) features defined on each pair of adjacent
letter mappings. Given a triplet, a, x, and y, a
global feature function f
k
(a,x,y) ? F sums up
the boolean values (0 or 1) of the corresponding
local feature g
k
(a,x,y, t) at t ? {1, ..., T},
f
k
(a,x,y) =
T
?
t=1
g
k
(a,x,y, t). (3)
In other words, f
k
(a,x,y) counts the number of
times the local feature is fired in the alignment a.
A unigram feature corresponds to the observa-
tion at x
i
and y
j
associated by a mapping a
t
=
(i, j). A unigram feature encodes the condition
where the letter in the full form x
i
is chosen or
unchosen for producing the abbreviation letter y
j
.
For example, we may infer from the letter mapping
at a
8
= (30, 1) in Figure 1, that x
30
is mapped to
y
1
because: x
30
is at the head of the word, y
1
is a
capital letter, and both x
30
and y
1
are at the head
of the word and abbreviation.
Bigram features, combining two observations at
a
s
and a
t
(1 ? s < t ? T ), are useful in capturing
the common characteristics shared by an abbrevi-
ation definition. For instance, we may presume in
Figure 1 that the head letters in the full form might
be selectively used for producing the abbreviation,
based on the observations at a
8
= (30, 1) and
a
9
= (38, 2). In order to focus on the conditions
for consecutive non-null mappings, we choose the
previous position s for the given t.
s =
?
?
?
t? 1
(
a
t(y)
= 0 ? ?u : a
u(y)
= 0
)
max
1?u<t
{
u | a
u(y)
6= 0
}
(otherwise)
(4)
Formula 4 prefers the non-null mapping that is the
most adjacent to t over the previous mapping (t ?
1). In Figure 1, transitions a
9
?a
11
and a
11
?a
13
exist for this reason.
In this study, we express unigram and bi-
gram features with atomic functions (Table 1)
that encode observation events of x
a
t(x)
, y
a
t(y)
,
a
t
, x
a
s(x)
?x
a
t(x)
, and y
a
s(y)
?y
a
t(y)
. Atomic
functions x ctype, y ctype, x position, and
y position present common heuristics used by
previous studies. The function x word examines
the existence of stop words (e.g., the, of, in) to
prevent them from producing abbreviation letters.
We also include x pos (part-of-speech of the word)
since a number of full forms are noun phrases.
Functions x diff , x diff wd, and y diff are de-
signed specifically for bigram features, receiving
two positions s and t in their arguments. The
function x diff mainly deals with abbreviation def-
initions that include consecutive letters of their
full forms, e.g., amplifier (AMP). The function
659
Function Return value
x ctype
?
(a,x, t) x
a
t(x)
+?
is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter
x position
?
(a,x, t) x
a
t(x)
+?
is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word
x char
?
(a,x, t) The lower-cased letter of x
a
t(x)
+?
x word
?
(a,x, t) The lower-cased word (offset position ?) containing the letter x
a
t(x)
x pos
?
(a,x, t) The part-of-speech code of the word (offset position ?) containing the letter x
a
t(x)
y ctype(a,y, t) y
a
t(y)
is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter
y position(a,y, t) y
a
t(y)
is at the {N (NIL) H (head), T (tail), I (inner)} of the word
y char(a,y, t) The lower-cased letter of y
a
t(y)
a state(a,y, t) {SKIP (a
t(y)
= 0),MATCH (1 ? a
t(y)
? |y|),ABBR (a
t(y)
= |y|+ 1)}
x diff(a,x, s, t) (a
t(x)
? a
s(x)
) if letters x
a
t(x)
and x
a
s(x)
are in the same word, NONE otherwise
x diff wd(a,x, s, t) The number of words between x
a
t(x)
and x
a
s(x)
y diff(a,y, s, t) (a
t(y)
? a
s(y)
)
Table 1: Atomic functions to encode observation events in x and y
Combination Rules
unigram(t) xy unigram(t)? {a state(t)}
xy unigram(t) x unigram(t)? y unigram(t)?
(
x unigram(t)? y unigram(t)
)
x unigram(t) x state
0
(t)? x state
?1
(t)? x state
1
(t)
?
(
x state
?1
(t)? x state
0
(t)
)
?
(
x state
0
(t)? x state
1
(t)
)
y unigram(t)
{
y ctype(t), y position(t), y ctype(t)y position(t)
}
x state
?
(t)
{
x ctype
?
(t), x position
?
(t), x char
?
(t), x word
?
(t), x pos
?
(t), x ctype
?
(t)x position
?
(t),
x position
?
(t)x pos
?
(t), x pos
?
(t)x ctype
?
(t), x ctype
?
(t)x position
?
(t)x pos
?
(t)
}
bigram(s, t) xy bigram(s, t)? {a state(s)a state(t)}
xy bigram(s, t)
(
x state
0
(s)? x state
0
(t)? trans(s, t)
)
?
(
y unigram(s)? y unigram(t)? trans(s, t)
)
?
(
x state
0
(s)? y unigram(s)? x state
0
(t)? y unigram(t)? trans(s, t)
)
trans(s, t)
{
x diff(s, t), x diff wd(s, t), y diff(s, t)
}
Table 2: Generation rules for unigram and bigram features.
x diff wd measures the distance of two words.
The function y diff models the ordering of abbre-
viation letters; this function always returns non-
negative values if the abbreviation contains letters
in the same order as in its full form.
We express unigram and bigram features with
the atomic functions. For example, Formula 5 de-
fines a unigram feature for the event where the cap-
ital letter in a full-form word x
a
t(x)
produces the
identical abbreviation letter y
a
t(y)
.
g
k
(a,x,y, t) =
?
?
?
?
?
?
?
?
?
?
?
1 x ctype
0
(a,x, t) = U
? y ctype(a,y, t) = U
? a state(a,y, t) = MATCH
0 (otherwise)
(5)
For notation simplicity, we rewrite this boolean
function as (arguments a, x, and y are omitted),
1
{x ctype
0
(t)y ctype(t)a state(t)=U;U;MATCH}
. (6)
In this formula, 1
{v=v?}
is an indicator function that
equals 1 when v = v? and 0 otherwise. The term
v presents a generation rule for a feature, i.e., a
combination rule of atomic functions.
Table 2 displays the complete list of gener-
ation rules for unigram and bigram features
4
,
unigram(t) and bigram(s, t). For each genera-
tion rule in unigram(t) and bigram(s, t), we de-
fine boolean functions that test the possible values
yielded by the corresponding atomic function(s).
2.3 Alignment candidates
Formula 1 requires a sum over the possible align-
ments, which amounts to 2
LM
for a sentence (L
letters) with an abbreviation (M letters). It is
unrealistic to compute the partition factor of the
formula directly; therefore, the factor has been
computed by dynamic programing (McCallum et
al., 2005; Blunsom and Cohn, 2006; Shimbo and
Hara, 2007) or approximated by the n-best list of
highly probable alignments (Och and Ney, 2002;
Liu et al, 2005). Fortunately, we can prune align-
ments that are unlikely to present full forms, by in-
troducing the natural assumptions for abbreviation
definitions:
4
In Table 2, a set of curly brackets {} denotes a list (array)
rather than a mathematical set. Operators ? and ? present
concatenation and Cartesian product of lists. For instance,
when A = {a, b} and B = {c, d}, A?B = {a, b, c, d} and
A?B = {ac, ad, bc, bd}.
660
  investigate the effect of thyroid transcription factor 1
0   0  0    00  0  0 0       0        0    0  0   00   0  0    00  0  0 0       1        2    3  0   50   0  0    00  0  0 1       2        0    3  0   50   0  0    00  0  0 1       0        2    3  0   50   0  0    00  0  0 2       1        0    3  0   50   0  0    00  0  0 2       0        1    3  0   50   0  0    00  0  3 0       0        1    0  2   50   0  0    00  0  3 0       1        2    0  0   50   0  0    00  0  3 0       1        0    0  2   5
.   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   .
x: ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~
min(|y|+5, 2|y|) = 8 words, (|y| = 4; y = "TTF-1")
94 13 16 2122 25 28 30 38 47 52 55 59ia =
ShffleShffleShffleShffleShffle
#0
#1
#2
#3
#4
#5
#6
#7
#8
Figure 2: A part of the possible alignments for the
abbreviation TTF-1 in the example sentence.
1. A full form may appear min(m + 5, 2m)
words before its abbreviation in the same sen-
tence, where m is the number of alphanu-
meric letters in the abbreviation (Park and
Byrd, 2001).
2. Every alphanumeric letter in an abbreviation
must be associated with the identical (case-
insensitive) letter in its full form.
3. An abbreviation letter must not originate from
multiple letters in its full form; a full-form let-
ter must not produce multiple letters.
4. Words in a full form may be shuffled at most
d times, so that all alphanumeric letters in the
corresponding abbreviation appear in the re-
arranged full form in the same order. We de-
fine a shuffle operation as removing a series
of word(s) from a full form, and inserting the
removed word(s) to another position.
5. A full form does not necessarily exist in the
text span defined by assumption 1.
Due to the space limitation, we do not describe
the algorithm for obtaining possible alignments
that are compatible with these assumptions. Al-
ternatively, Figure 2 illustrates a part of possible
alignments C(x,y) for the example sentence. The
alignment #2 represents the correct definition for
the abbreviation TTF-1. We always include the
negative alignment (e.g., #0) where no abbrevia-
tion letters are associated with any letters in x.
The alignments #4?8 interpret the generation
process of the abbreviation by shuffling the words
in x. For example, the alignment #6 moves the
word ?of? to the position between ?factor? and
?1?. Shuffled alignments cover abbreviation defini-
tions such as receptor of estrogen (ER) and water
activity (AW). We call the parameter d, distortion
parameter, which controls the acceptable level of
reordering (distortion) for the abbreviation letters.
2.4 Parameter estimation
Parameter estimation for the abbreviation
alignment model is essentially the same as
for general maximum entropy models. Given
a training set that consists of N instances,
(
(a
(1)
,x
(1)
,y
(1)
), ..., (a
(N)
,x
(N)
,y
(N)
)
)
, we
maximize the log-likelihood of the conditional
probability distribution by using the maximum
a posterior (MAP) estimation. In order to avoid
overfitting, we regularize the log-likelihood with
either the L
1
or L
2
norm of the weight vector ?,
L
1
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
1
?
1
, (7)
L
2
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
2
2
2?
2
2
. (8)
In these formulas, ?
1
and ?
2
are regularization pa-
rameters for the L
1
and L
2
norms. Formulas 7
and 8 are maximized by the Orthant-Wise Limited-
memory Quasi-Newton (OW-LQN) method (An-
drew and Gao, 2007) and the Limited-memory
BFGS (L-BFGS) method (Nocedal, 1980)
5
.
3 Experiments
3.1 Aligned abbreviation corpus
The Medstract Gold Standard Corpus (Pustejovsky
et al, 2002) was widely used for evaluating abbre-
viation recognition methods (Schwartz and Hearst,
2003; Adar, 2004). However, we cannot use
this corpus for training the abbreviation alignment
model, since it lacks annotations on the origins of
abbreviation letters. In addition, the size of the
corpus is insufficient for a supervised machine-
learning method.
Therefore, we built our training corpus with
1,000 scientific abstracts that were randomly cho-
sen from the MEDLINE database. Although the
alignment model is independent of linguistic pat-
terns for abbreviation definitions, in the corpus we
found only three abbreviation definitions that were
described without parentheses. Hence, we em-
ployed parenthetical expressions, full-form ?(? ab-
breviation ?)?, to locate possible abbreviation def-
initions (Wren and Garner, 2002). In order to ex-
clude parentheses inserting clauses into passages,
5
We used Classias for parameter estimation:
http://www.chokkan.org/software/classias/
661
we consider the inner expression of parentheses as
an abbreviation candidate, only if the expression
consists of two words at most, the length of the ex-
pression is between two to ten characters, the ex-
pression contains at least an alphabetic letter, and
the first character is alphanumeric.
We asked a human annotator to assign refer-
ence abbreviation alignments for 1,420 parentheti-
cal expressions (instances) in the corpus. If a par-
enthetical expression did not introduce an abbre-
viation, e.g., ?... received treatment at 24 months
(RRMS),? the corresponding instance would have
a negative alignment (as #0 in Figure 2). Eventu-
ally, our aligned corpus consisted of 864 (60.8%)
abbreviation definitions (with positive alignments)
and 556 (39.2%) other usages of parentheses (with
negative alignments). Note that the log-likelihood
in Formula 7 or 8 increases only if the probabilistic
model predicts the reference alignments, regard-
less of whether they are positive or negative.
3.2 Baseline systems
We prepared five state-of-the-art systems of ab-
breviation recognition as baselines: Schwartz
and Hearst?s method (SH) (Schwartz and Hearst,
2003), SaRAD (Adar, 2004), ALICE (Ao and
Takagi, 2005), Chang and Sch?utze?s method
(CS) (Chang and Sch?utze, 2006), and Nadeau and
Turney?s method (NT) (Nadeau and Turney, 2005).
We utilized the implementations available on the
Web for SH
6
, CS
78
, and ALICE
9
, and we repro-
duced SaRAD and NT, based on their papers.
Our implementation of NT consists of a classi-
fier that discriminates between positive (true) and
negative (false) full forms, using all of the feature
functions presented in the original paper. Although
the original paper presented heuristics for gener-
ating full-form candidates, we replaced the candi-
date generator with the function C(x,y), so that
the classifier and our alignment model can receive
the same set of full-form candidates. The classi-
fier of the NT system was modeled by the LIB-
SVM implementation
10
with Radial Basis Func-
6
Abbreviation Definition Recognition Software:
http://biotext.berkeley.edu/software.html
7
Biomedical Abbreviation Server:
http://abbreviation.stanford.edu/
8
We applied a score cutoff of 0.14.
9
Abbreviation LIfter using Corpus-based Extraction:
http://uvdb3.hgc.jp/ALICE/ALICE index.html
10
LIBSVM ? A Library for Support Vector Machines:
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm/
System P R F1
Schwartz & Hearst (SH) .978 .940 .959
SaRAD .891 .919 .905
ALICE .961 .920 .940
Chang & Sch?utze (CS) .942 .900 .921
Nadeau & Turney (NT) .954 .871 .910
Proposed (d = 0; L
1
) .973 .969 .971
Proposed (d = 0; L
2
) .964 .968 .966
Proposed (d = 1; L
1
) .960 .981 .971
Proposed (d = 1; L
2
) .957 .976 .967
Table 3: Performance on our corpus.
tion (RBF) kernel
11
. If multiple full-form can-
didates for an abbreviation are classified as posi-
tives, we choose the candidate that yields the high-
est probability estimate.
3.3 Results
We trained and evaluated the proposed method on
our corpus by performing 10-fold cross valida-
tion
12
. Our corpus includes 13 out of 864 (1.5%)
abbreviation definitions in which the abbreviation
letters are shuffled. Thus, we have examined two
different distortion parameters, d = 0, 1 in this
experiment. The average numbers of candidates
produced by the candidate generator C(x,y) per
instance were 8.46 (d = 0) and 69.1 (d = 1), re-
spectively. The alignment model was trained in a
reasonable execution time
13
, ca. 5 minutes (d = 0)
and 1.5 hours (d = 1).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) on the basis of the number of cor-
rect abbreviation definitions recognized by each
system. The proposed method achieved the best
F1 score (0.971) of all systems. The inclusion of
distorted abbreviations (d = 1) gained the high-
est recall (0.981 with L
1
regularization). Base-
line systems with refined heuristics (SaRAD and
ALICE) could not outperform the simplest sys-
tem (SH). The previous approaches with machine
learning (CS and NT) were roughly comparable to
rule-based methods.
We also evaluated the alignment model on the
Medstract Gold Standard development corpus to
examine the adaptability of the alignment model
trained with our corpus (Table 4). Since the origi-
11
We tuned kernel parameters C = 128 and ? = 2.0 by
using the grid-search tool in the LIBSVM distribution.
12
We determined the regularization parameters as ?
1
= 3
and ?
2
= 3 after testing {0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10} for
the regularization parameters. The difference between the
highest and lowest F1 scores was 1.8%.
13
On Intel Dual-Core Xeon 5160/3GHz CPU, excluding
time for feature generation and data input/output.
662
System P R F1
Schwartz & Hearst (SH) .942 .891 .916
SaRAD .909 .859 .884
ALICE .960 .945 .953
Chang & Sch?utze (CS) .858 .852 .855
Nadeau & Turney (NT) .889 .875 .882
Proposed (d = 1; L
1
) .976 .945 .960
Table 4: Performance on Medstract corpus.
# Atomic function(s) F1
(1) x position + x ctype .905
(2) (1) + x char + y char .885
(3) (1) + x word + x pos .941
(4) (1) + x diff + x diff wd + y diff .959
(5) (1) + y position + y ctype .964
(6) All atomic functions .966
Table 5: Effect of atomic functions (d = 0; L
2
).
nal version of the Medstract corpus includes anno-
tation errors, we used the version revised by Ao
and Takagi (2005). For this reason, the perfor-
mance of ALICE might be over-estimated in this
evaluation; ALICE delivered much better results
than Schwartz & Hearst?s method on this corpus.
The abbreviation alignment model trained with
our corpus (d = 1; L
1
) outperformed the baseline
systems for all evaluation metrics. It is notable that
the model could recognize abbreviation definitions
with shuffled letters, e.g., transfer of single embryo
(SET) and inorganic phosphate (PI), without any
manual tuning for this corpus. In some false cases,
the alignment model yielded incorrect probability
estimates. For example, the probabilities of the
alignments prepubertal bipolarity, bi
:
polarity, and
non-definition (negative) for the abbreviation BP
were computed as 3.4%, 89.6%, and 6.7%, respec-
tively; but the first expression prepubertal bipolar-
ity is the correct definition for the abbreviation.
Table 5 shows F1 scores of the proposed method
trained with different sets of atomic functions. The
baseline setting (1), which built features only with
x position and x ctype functions, gained a 0.905
F1 score; further, adding more atomic functions
generally improves the score. However, the x char
and y char functions decreased the performance
since the alignment model was prone to overfit to
the training data, relying on the existence of spe-
cific letters in the training instances. Interestingly,
the model was flexible enough to achieve a high
performance with four atomic functions (5).
Table 6 demonstrates the ability for our ap-
proach to obtain effective features; the table shows
the top 10 (out of 850,009) features with high
# Feature ?
1 U: x position
0
=H;y ctype
0
=U;y position
0
=H/M 1.7370
2 B: y position
0
=I/y position
0
=I/x diff=1/M-M 1.3470
3 U: x ctype
?1
=L;x ctype
0
=L/S 0.96342
4 B: x ctype
0
=L/x ctype
0
=L/x diff wd=0/M-M 0.94009
5 U: x position
0
=I;x char
1
=?t?/S 0.91645
6 U: x position
0
=H;x pos
0
=NN;y ctype
0
=U/M 0.86786
7 U: x ctype
?1
=S;x
c
type
0
=L;M 0.86474
8 B: x char
0
=?o?/x ctype
0
=L/y diff=0/M-S 0.71262
9 U: x char
?1
=?o?;x ctype
0
=L/M 0.69764
10 B: x position
0
=H/x ctype
0
=U/y diff=1/M-M 0.66418
Table 6: Top ten features with high weights.
weights assigned by the MAP estimation with L
1
regularization. A unigram and bigram features
have prefixes ?U:? and ?B:? respectively; a feature
expresses conditions at s (bigram features only),
conditions at t, and mapping status (match or skip)
separated by ?/? symbols. For example, the #1 fea-
ture associates a letter at the head of a full-form
word with the uppercase letter at the head of its
abbreviation. The #4 feature is difficult to obtain
from manual observations, i.e., the bigram feature
suggests the production of two abbreviation letters
from two lowercase letters in the same word.
4 Conclusion
We have presented a novel approach for recogniz-
ing abbreviation definitions. The task of abbrevi-
ation recognition was successfully formalized as
a sequential alignment problem. We developed
an aligned abbreviation corpus, and obtained fine-
grained features that express the events wherein
a full forum produces an abbreviation letter. The
experimental results showed remarkable improve-
ments and usefulness of the alignment approach
for abbreviation recognition. We expect the use-
fullness of the discriminative model for building
an comprehensible abbreviation dictionary.
Future work would be to model cases in which
a full form yields non-identical letters (e.g., ?one?
? ?1? and ?deficient? ? ?-?), and to demonstrate
this approach with more generic linguistic patterns
(e.g., aka, abbreviated as, etc.). We also plan to
explore a method for training a model with an un-
aligned abbreviation corpus, estimating the align-
ments simultaneously from the corpus.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
663
References
Adar, Eytan. 2004. SaRAD: A simple and robust
abbreviation dictionary. Bioinformatics, 20(4):527?
533.
Andrew, Galen and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Ma-
chine Learning (ICML 2007), pages 33?40.
Ao, Hiroko and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (Coling-ACL 2006), pages 65?72.
Chang, Jeffrey T. and Hinrich Sch?utze. 2006. Abbre-
viations in biomedical text. In Ananiadou, Sophia
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Gaudan, Sylvain, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in Medline. Bioinformatics, 21(18):3658?
3664.
Jain, Alpa, Silviu Cucerzan, and Saliha Azzam. 2007.
Acronym-expansion recognition and ranking on the
web. In Proceedings of the IEEE International Con-
ference on Information Reuse and Integration (IRI
2007), pages 209?214.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning (ICML 2001), pages
282?289.
Liu, Hongfang and Carol Friedman. 2003. Mining ter-
minological knowledge in large biomedical corpora.
In the 8th Pacific Symposium on Biocomputing (PSB
2003), pages 415?426.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL 2005), pages 459?466.
McCallum, Andrew, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proceedings of the 21st Conference on Un-
certainty in Artificial Intelligence (UAI 2005), pages
388?395.
Nadeau, David and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intel-
ligence (AI?2005) (LNAI 3501), page 10 pages.
Nocedal, Jorge. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 295?302.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Build-
ing an abbreviation dictionary using a term recogni-
tion approach. Bioinformatics, 22(24):3089?3095.
Pakhomov, Serguei. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL 2002), pages 160?167.
Park, Youngja and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the 2001 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2001), pages 126?133.
Pustejovsky, James, Jos?e Casta?no, Roser Saur??, Anna
Rumshinsky, Jason Zhang, and Wei Luo. 2002.
Medstract: creating large-scale information servers
for biomedical libraries. In Proceedings of the ACL-
02 workshop on Natural language processing in the
biomedical domain, pages 85?92.
Schwartz, Ariel S. and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB 2003), pages 451?462.
Shimbo, Masashi and Kazuo Hara. 2007. A dis-
criminative learning model for coordinate conjunc-
tions. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 610?619.
Wren, Jonathan D. and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated construc-
tion of comprehensive acronym-definition dictionar-
ies. Methods of Information in Medicine, 41(5):426?
434.
Yu, Hong, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based
approach for automatically disambiguating biomedi-
cal abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
Zhou, Wei, Vetle I. Torvik, and Neil R. Smalheiser.
2006. ADAM: another database of abbreviations in
MEDLINE. Bioinformatics, 22(22):2813?2818.
664
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447?456,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Discriminative Candidate Generator for String Transformations
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka?
yoshimasa.tsuruoka@manchester.ac.uk
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
String transformation, which maps a source
string s into its desirable form t?, is related
to various applications including stemming,
lemmatization, and spelling correction. The
essential and important step for string trans-
formation is to generate candidates to which
the given string s is likely to be transformed.
This paper presents a discriminative approach
for generating candidate strings. We use sub-
string substitution rules as features and score
them using an L1-regularized logistic regres-
sion model. We also propose a procedure to
generate negative instances that affect the de-
cision boundary of the model. The advantage
of this approach is that candidate strings can
be enumerated by an efficient algorithm be-
cause the processes of string transformation
are tractable in the model. We demonstrate
the remarkable performance of the proposed
method in normalizing inflected words and
spelling variations.
1 Introduction
String transformation maps a source string s into its
destination string t?. In the broad sense, string trans-
formation can include labeling tasks such as part-
of-speech tagging and shallow parsing (Brill, 1995).
However, this study addresses string transformation
in its narrow sense, in which a part of a source string
is rewritten with a substring. Typical applications of
this task include stemming, lemmatization, spelling
correction (Brill and Moore, 2000; Wilbur et al,
2006; Carlson and Fette, 2007), OCR error correc-
tion (Kolak and Resnik, 2002), approximate string
matching (Navarro, 2001), and duplicate record de-
tection (Bilenko and Mooney, 2003).
Recent studies have formalized the task in the dis-
criminative framework (Ahmad and Kondrak, 2005;
Li et al, 2006; Chen et al, 2007),
t? = argmax
t?gen(s)
P (t|s). (1)
Here, the candidate generator gen(s) enumerates
candidates of destination (correct) strings, and the
scorer P (t|s) denotes the conditional probability of
the string t for the given s. The scorer was modeled
by a noisy-channel model (Shannon, 1948; Brill and
Moore, 2000; Ahmad and Kondrak, 2005) and max-
imum entropy framework (Berger et al, 1996; Li et
al., 2006; Chen et al, 2007).
The candidate generator gen(s) also affects the
accuracy of the string transformation. Previous stud-
ies of spelling correction mostly defined gen(s),
gen(s) = {t | dist(s, t) < ?}. (2)
Here, the function dist(s, t) denotes the weighted
Levenshtein distance (Levenshtein, 1966) between
strings s and t. Furthermore, the threshold ? requires
the distance between the source string s and a can-
didate string t to be less than ?.
The choice of dist(s, t) and ? involves a tradeoff
between the precision, recall, and training/tagging
speed of the scorer. A less restrictive design of these
factors broadens the search space, but it also in-
creases the number of confusing candidates, amount
of feature space, and computational cost for the
scorer. Moreover, the choice is highly dependent on
the target task. It might be sufficient for a spelling
447
correction program to gather candidates from known
words, but a stemmer must handle unseen words ap-
propriately. The number of candidates can be huge
when we consider transformations from and to un-
seen strings.
This paper addresses these challenges by explor-
ing the discriminative training of candidate genera-
tors. More specifically, we build a binary classifier
that, when given a source string s, decides whether
a candidate t should be included in the candidate set
or not. This approach appears straightforward, but it
must resolve two practical issues. First, the task of
the classifier is not only to make a binary decision
for the two strings s and t, but also to enumerate a
set of positive strings for the string s,
gen(s) = {t | predict(s, t) = 1}. (3)
In other words, an efficient algorithm is necessary
to find a set of strings with which the classifier
predict(s, t) yields positive labels for the string s.
Another issue arises when we prepare a training
set. A discriminative model requires a training set
in which each instance (pair of strings) is annotated
with a positive or negative label. Even though some
existing resources (e.g., inflection table and query
log) are available for positive instances, such re-
sources rarely contain negative instances. Therefore,
we must generate negative instances that are effec-
tive for discriminative training.
To address the first issue, we design features that
express transformations from a source string s to its
destination string t. Feature selection and weight-
ing are performed using an L1-regularized logistic
regression model, which can find a sparse solution
to the classification model. We also present an al-
gorithm that utilizes the feature weights to enumer-
ate candidates of destination strings efficiently. We
deal with the second issue by generating negative
instances from unlabeled instances. We describe a
procedure to choose negative instances that affect
the decision boundary of the classifier.
This paper is organized as follows. Section 2 for-
malizes the task of the candidate generator as a bi-
nary classification modeled by logistic regression.
Features for the classifier are designed using the
rules of substring substitution. Therefore, we can
obtain, efficiently, candidates of destination strings
and negative instances for training. Section 3 re-
ports the remarkable performance of the proposed
method in various applications including lemmati-
zation, spelling normalization, and noun derivation.
We briefly review previous work in Section 4, and
conclude this paper in Section 5.
2 Candidate generator
2.1 Candidate classification model
In this section, we first introduce a binary classifier
that yields a label y ? {0, 1} indicating whether a
candidate t should be included in the candidate set
(1) or not (0), given a source string s. We express
the conditional probability P (y|s, t) using a logistic
regression model,
P (1|s, t) =
1
1 + exp (??TF (s, t))
, (4)
P (0|s, t) = 1? P (1|s, t). (5)
In these equations, F = {f1, ..., fK} denotes a vec-
tor of the Boolean feature functions; K is the num-
ber of feature functions; and ? = {?1, ..., ?K}
presents a weight vector of the feature functions.
We obtain the following decision rule to choose
the most probable label y? for a given pair ?s, t?,
y? = argmax
y?{0,1}
P (y|s, t) =
{
1
(
?TF (s, t) > 0
)
0 (otherwise)
.
(6)
Finally, given a source string s, the generator func-
tion gen(s) is defined to collect all strings to which
the classifier assigns positive labels:
gen(s) = {t | P (1|s, t) > P (0|s, t)}
= {t | ?TF (s, t) > 0}. (7)
2.2 Substitution rules as features
The binary classifier can include any arbitrary fea-
ture. This is exemplified by the Levenshtein dis-
tance and distributional similarity (Lee, 1999) be-
tween two strings s and t. These features can im-
prove the classification accuracy, but it is unrealistic
to compute these features for every possible string,
as in equation 7. For that reason, we specifically
examine substitution rules, with which the process
448
^oestrogen$
^estrogen$
^anaemia$
^anemia$
^studies$
^study$
('o', ''), ('^o', '^'), ('oe', 'e'),
('^oe', '^e'), ('^oes', '^es'), ...
('a', ''), ('na', 'n'), ('ae', 'e'),
('ana', 'an'), ('nae', 'ne'), ('aem', 'em'),
...
('ies', 'y'), ('dies', 'dy'), ('ies$', 'y$'),
('udies', 'udy'), ('dies$', 'dy$'), ...
S:
t:
S:
t:
S:
t:
(1)
(2)
(3)
Figure 1: Generating substitution rules.
of transforming a source string s into its destination
form t is tractable.
In this study, we assume that every string has a
prefix ??? and postfix ?$?, which indicate the head
and tail of a string. A substitution rule r = (?, ?)
replaces every occurrence of the substring ? in a
source string into the substring ?. Assuming that a
string s can be transformed into another string twith
a single substitution operation, substitution rules ex-
press the different portion between strings s and t.
Equation 8 defines a binary feature function with
a substitution rule between two strings s and t,
fk(s, t) =
{
1 (rule rk can convert s into t)
0 (otherwise)
.
(8)
We allow multiple substitution rules for a given pair
of strings. For instance, substitution rules (?a?,
??), (?na?, ?n?), (?ae?, ?e?), (?nae?, ?ne?), etc.
form feature functions that yield 1 for strings s =
??anaemia$? and t = ??anemia$?. Equation
6 produces a decision based on the sum of feature
weights, or scores of substitution rules, representing
the different portions between s and t.
Substitution rules for the given two strings s and
t are obtained as follows. Let l denote the longest
common prefix between strings s and t, and r the
longest common postfix. We define cs as the sub-
string in s that is not covered by the longest common
prefix l and postfix r, and define ct for t analogously.
In other words, strings s and t are divided into three
regions, lcsr and lctr, respectively. For strings s =
??anaemia$? and t = ??anemia$? in Figure 1
(2), we obtain cs = ?a? and ct = ?? because l =
??an? and r = ?emia$?.
Because substrings cs and ct express different
portions between strings s and t, we obtain the mini-
mum substitution rule (cs, ct), which can convert the
string s into t by replacing substrings cs in s with
ct; the minimum substitution rule for the same ex-
ample is (?a?, ??). However, replacing letters ?a?
in ??anaemia$? into empty letters does not pro-
duce the correct string ??anemia$? but ??nemi$?.
Furthermore, the rule might be inappropriate for ex-
pressing string transformation because it always re-
moves the letter ?a? from every string.
Therefore, we also obtain expanded substitution
rules, which insert postfixes of l to the head of min-
imum substitution rules, and/or append prefixes of
r to the rules. For example, we find an expanded
substitution rule (?na?, ?n?), by inserting a postfix
of l = ??an? to the head of the minimum substitu-
tion rule (?a?, ??); similarly, we obtain an expanded
substitution rule (?ae?, ?e?), by appending a prefix
of r = ?emia$? to the tail of the rule (?a?, ??).
Figure 1 displays examples of substitution rules
(the right side) for three pairs of strings (the left
side). Letters in blue, green, and red respectively
represent the longest common prefixes, longest com-
mon postfixes, and different portions. In this study,
we expand substitution rules such that the number of
letters in rules is does not pass a threshold ?1.
2.3 Parameter estimation
Given a training set that consists of N instances,
D =
(
(s(1), t(1), y(1)), ..., (s(N), t(N), y(N))
)
, we
optimize the feature weights in the logistic regres-
sion model by maximizing the log-likelihood of the
conditional probability distribution,
L? =
N?
i=1
logP (y(i)|s(i), t(i)). (9)
The partial derivative of the log-likelihood with re-
spect to a feature weight ?k is given as equation 10,
?L?
??k
=
N?
i=1
{
y(i) ? P (1|s(i), t(i))
}
fk(s
(i), t(i)).
(10)
The maximum likelihood estimation (MLE) is
known to suffer from overfitting the training set. The
1The number of letters for a substitution rule r = (?, ?) is
defined as the sum of the quantities of letters in ? and ?, i.e.,
|?|+ |?|. We determined the threshold ? = 12 experimentally.
449
common approach for addressing this issue is to use
the maximum a posteriori (MAP) estimation, intro-
ducing a regularization term of the feature weights
?, i.e., a penalty on large feature weights. In addi-
tion, the generation algorithm of substitution rules
might produce inappropriate rules that transform a
string incorrectly, or overly specific rules that are
used scarcely. Removing unnecessary substitution
rules not only speeds up the classifier but also the
algorithm for candidate generation, as presented in
Section 2.4.
In recent years, L1 regularization has received in-
creasing attention because it produces a sparse so-
lution of feature weights in which numerous fea-
ture weights are zero (Tibshirani, 1996; Ng, 2004).
Therefore, we regularize the log-likelihood with the
L1 norm of the weight vector ? and define the final
form the objective function to be minimized as
E? = ?L? +
|?|
?
. (11)
Here, ? is a parameter to control the effect of L1
regularization; the smaller the value we set to ?,
the more features the MAP estimation assigns zero
weights to: it removes a number of features from the
model. Equation 11 is minimized using the Orthant-
Wise Limited-memory Quasi-Newton (OW-LQN)
method (Andrew and Gao, 2007) because the second
term of equation 11 is not differentiable at ?k = 0.
2.4 Candidate generation
The advantage of our feature design is that we can
enumerate strings to which the classifier is likely to
assign positive labels. We start by observing the nec-
essary condition for t in equation 7,
?TF (s, t) > 0? ?k : fk(s, t) = 1 ? ?k > 0.
(12)
The classifier might assign a positive label to strings
s and t when at least one feature function whose
weight is positive can transform s to t.
Let R+ be a set of substitution rules to which
MAP estimation has assigned positive feature
weights. Because each feature corresponds to a sub-
stitution rule, we can obtain gen(s) for a given string
s by application of every substitution rule r ? R+,
gen(s) = {r(s) | r ? R+ ??TF (s, r(s)) > 0}.
(13)
Input: s = (s1, ..., sl): an input string s (series of letters)
Input: D: a trie dictionary containing positive features
Output: T : gen(s)
T = {};1
U = {};2
foreach i ? (1, ..., |s|) do3
F ? D.prefix search(s, i);4
foreach f ? F do5
if f /? U then6
t? f .apply(s);7
if classify(s, t) = 1 then8
add t to T ;9
end10
add f to U ;11
end12
end13
end14
return T ;15
Algorithm 1: A pseudo-code for gen(s).
Here, r(s) presents the string to which the substitu-
tion rule r transforms the source string s. We can
compute gen(s) with a small computational cost if
the MAP estimation with L1 regularization reduces
the number of active features.
Algorithm 1 represents a pseudo-code for obtain-
ing gen(s). To search for positive substitution rules
efficiently, the code stores a set of rules in a trie
structure. In line 4, the code obtains a set of positive
substitution rules F that can rewrite substrings start-
ing at offset #i in the source string s. For each rule
f ? F , we obtain a candidate string t by application
of the substitution rule f to the source string s (line
7). The candidate string t is qualified to be included
in gen(s) when the classifier assigns a positive label
to strings s and t (lines 8 and 9). Lines 6 and 11 pre-
vent the algorithm from repeating evaluation of the
same substitution rule.
2.5 Generating negative instances
The parameter estimation requires a training set D
in which each instance (pair of strings) is annotated
with a positive or negative label. Negative instances
(counter examples) are essential for penalizing in-
appropriate substitution rules, e.g. (?a?, ??). Even
though some existing resources (e.g. verb inflection
table) are available for positive instances, such re-
sources rarely contain negative instances.
A common approach for handling this situation
is to assume that every pair of strings in a resource
450
Input: D+ = [(s1, t1), ..., (sl, tl)]: positive instances
Input: V : a suffix array of all strings (vocabulary)
Output: D?: negative instances
Output: R: substitution rules (features)
D? = [];1
R = {};2
foreach d ? D+ do3
foreach r ? features(d) do4
add r to R;5
end6
end7
foreach r ? R do8
S ? V .search(r.src);9
foreach s ? S do10
t? r.apply(s);11
if (s, t) /? D+ then12
if t ? V then13
append (s, t) to D?;14
end15
end16
end17
end18
return D?, R;19
Algorithm 2: Generating negative instances.
is a negative instance; however, negative instances
amount to ca. V (V ? 1)/2, where V represents the
total number of strings. Moreover, substitution rules
expressing negative instances are innumerable and
sparse because the different portions are peculiar to
individual negative instances. For instance, the min-
imum substitution rule for unrelated words anaemia
and around is (?naemia?, ?round?), but the rule
cannot be too specific to generalize the conditions
for other negative instances.
In this study, we generate negative instances so
that they can penalize inappropriate rules and settle
the decision boundary of the classifier. This strat-
egy is summarized as follows. We consider every
pair of strings as candidates for negative instances.
We obtain substitution rules for the pair using the
same algorithm as that described in Section 2.2 if a
string pair is not included in the dictionary (i.e., not
in positive instances). The pair is used as a nega-
tive instance only when any substitution rule gener-
ated from the pair also exists in the substitution rules
generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-
ments the strategy for generating negative instances
efficiently. First, we presume that we have positive
instances D+ = [(s1, t1), ..., (sl, tl)] and unlabeled
Table Description # Entries
LRSPL Spelling variants 90,323
LRNOM Nominalizations (derivations) 14,029
LRAGR Agreement and inflection 910,854
LRWD Word index (vocabulary) 850,236
Table 1: Excerpt of tables in the SPECIALIST Lexicon.
Data set # + # - # Rules
Orthography 15,830 33,296 11,098
Derivation 12,988 85,928 5,688
Inflection 113,215 124,747 32,278
Table 2: Characteristics of datasets.
strings V . For example, positive instance D+ repre-
sent orthographic variants, and unlabeled strings V
include all possible words (vocabulary). We insert
the vocabulary into a suffix array, which is used to
locate every occurrence of substrings in V .
The algorithm first generates substitution rules R
only from positive instances D+ (lines 3 to 7). For
each substitution rule r ? R, we enumerate known
strings S that contain the source substring r.src (line
9). We apply the substitution rule to each string s ?
S and obtain its destination string t (line 11). If the
pair of strings ?s, t? is not included in D+ (line 12),
and if the destination string t is known (line 13), the
substitution rule r might associate incorrect strings
s and t, which do not exist in D+. Therefore, we
insert the pair to the negative set D? (line 14).
3 Evaluation
3.1 Experiments
We evaluated the candidate generator using three
different tasks: normalization of orthographic vari-
ants, noun derivation, and lemmatization. The
datasets for these tasks were obtained from the
UMLS SPECIALIST Lexicon2, a large lexicon that
includes both commonly occurring English words
and biomedical vocabulary. Table 1 displays the list
of tables in the SPECIALIST Lexicon that were used
in our experiments. We prepared three datasets, Or-
thography, Derivation, and Inflection.
The Orthography dataset includes spelling vari-
ants (e.g., color and colour) in the LRSPL table. We
2UMLS SPECIALIST Lexicon:
http://specialist.nlm.nih.gov/
451
chose entries as positive instances in which spelling
variants are caused by (case-insensitive) alphanu-
meric changes3. The Derivation dataset was built di-
rectly from the LRNOM table, which includes noun
derivations such as abandon ? abandonment. The
LRAGR table includes base forms and their inflec-
tional variants of nouns (singular and plural forms),
verbs (infinitive, third singular, past, past participle
forms, etc), and adjectives/adverbs (positive, com-
parative, and superlative forms). For the Inflection
dataset, we extracted the entries in which inflec-
tional forms differ from their base forms4, e.g., study
? studies.
For each dataset, we applied the algorithm de-
scribed in Section 2.5 to generate substitution rules
and negative instances. Table 2 shows the number of
positive instances (# +), negative instances (# -), and
substitution rules (# Rules). We evaluated the per-
formance of the proposed method in two different
goals of the tasks: classification (Section 3.2) and
normalization (Section 3.3).
3.2 Experiment 1: Candidate classification
In this experiment, we measured the performance
of the classification task in which pairs of strings
were assigned with positive or negative labels.
We trained and evaluated the proposed method
by performing ten-fold cross validation on each
dataset5. Eight baseline systems were prepared
for comparison: Levenshtein distance (LD), nor-
malized Levenshtein distance (NLD), Dice coef-
ficient on letter bigrams (DICE) (Adamson and
Boreham, 1974), Longest Common Substring Ra-
tio (LCSR) (Melamed, 1999), Longest Common
Prefix Ratio (PREFIX) (Kondrak, 2005), Porter?s
stemmer (Porter, 1980), Morpha (Minnen et al,
2001), and CST?s lemmatiser (Dalianis and Jonge-
3LRSPL table includes trivial spelling variants that can be
handled using simple character/string operations. For example,
the table contains spelling variants related to case sensitivity
(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).
4LRAGR table also provides agreement information even
when word forms do not change. For example, the table con-
tains an entry indicating that the first-singular present form of
the verb study is study, which might be readily apparent to En-
glish speakers.
5We determined the regularization parameter ? = 5 experi-
mentally. Refer to Figure 2 for the performance change.
jan, 2006)6.
The five systems LD, NLD, DICE, LCSR, and
PREFIX employ corresponding metrics of string
distance or similarity. Each system assigns a posi-
tive label to a given pair of strings ?s, t? if the dis-
tance/similarity of strings s and t is smaller/larger
than the threshold ? (refer to equation 2 for distance
metrics). The threshold of each system was chosen
so that the system achieves the best F1 score.
The remaining three systems assign a positive la-
bel only if the system transforms the strings s and
t into the identical string. For example, a pair of
two words studies and study is classified as positive
by Porter?s stemmer, which yields the identical stem
studi for these words. We trained CST?s lemmatiser
for each dataset to obtain flex patterns that are used
for normalizing word inflections.
To examine the performance of the L1-
regularized logistic regression as a discriminative
model, we also built two classifiers based on the
Support Vector Machine (SVM). These SVM
classifiers were implemented by the SVMperf 7 on
a linear kernel8. An SVM classifier employs the
same feature set (substitution rules) as the proposed
method so that we can directly compare the L1-
regularized logistic regression and the linear-kernel
SVM. Another SVM classifier incorporates the five
string metrics; this system can be considered as our
reproduction of the discriminative string similarity
proposed by Bergsma and Kondrak (2007).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) based on the number of correct de-
cisions for positive instances. The proposed method
outperformed the baseline systems, achieving 0.919,
0.888, and 0.984 of F1 scores, respectively. Porter?s
stemmer worked on the Inflection set, but not on
the Orthography set, which is beyond the scope of
the stemming algorithms. CST?s lemmatizer suf-
fered from low recall on the Inflection set because
it removed suffixes of base forms, e.g., (cloning,
clone) ? (clone, clo). Morpha and CST?s lemma-
6We used CST?s lemmatiser version 2.13:
http://www.cst.dk/online/lemmatiser/uk/
index.html
7SVM for Multivariate Performance Measures (SVMperf ):
http://svmlight.joachims.org/svm_perf.html
8We determined the parameter C = 500 experimentally; it
controls the tradeoff between training error and margin.
452
System Orthography Derivation Inflection
P R F1 P R F1 P R F1
Levenshtein distance (? = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565
Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646
Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673
Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645
LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645
PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645
Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881
Morpha (Minnen et al, 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902
CST?s lemmatiser (Dalianis et al 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290
Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984
Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983
+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983
Table 3: Performance of candidate classification
Rank Src Dst Weight Examples
1 uss us 9.81 focussing
2 aev ev 9.56 mediaeval
3 aen en 9.53 ozaena
4 iae$ ae$ 9.44 gadoviae
5 nni ni 9.16 prorennin
6 nne ne 8.84 connexus
7 our or 8.54 colour
8 aea ea 8.31 paean
9 aeu eu 8.22 stomodaeum
10 ooll ool 7.79 woollen
Table 4: Feature weights for the Orthography set
tizer were not designed for orthographic variants and
noun derivations.
Levenshtein distance (? = 1) did not work for
the Derivation set because noun derivations often
append two or more letters (e.g., happy ? happi-
ness). No string similarity/distance metrics yielded
satisfactory results. Some metrics obtained the best
F1 scores with extreme thresholds only to classify
every instance as positive. These results imply the
difficulty of the string metrics for the tasks.
The L1-regularized logistic regression was com-
parable to the SVM with linear kernel in this exper-
iment. However, the presented model presents the
advantage that it can reduce the number of active
features (features with non-zero weights assigned);
the L1 regularization can remove 74%, 48%, and
82% of substitution rules in each dataset. The
performance improvements by incorporating string
metrics as features were very subtle (less than 0.7%).
What is worse, the distance/similarity metrics do not
specifically derive destination strings to which the
classifier is likely to assign positive labels. There-
fore, we can no longer use the efficient algorithm
as a candidate generator (in Section 2.4) with these
features.
Table 4 demonstrates the ability of our approach
to obtain effective features; the table shows the top
10 features with high weights assigned for the Or-
thography data. An interesting aspect of the pro-
posed method is that the process of the orthographic
variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for
the Inflection data when we change the number of
active features (x-axis) by controlling the regular-
ization parameter ? from 0.001 to 100. The larger
the value we set for ?, the better the classifier per-
forms, generally, with more active features. In ex-
treme cases, the number of active features drops to
97 with ? = 0.01; nonetheless, the classifier still
achieves 0.961 of the F1 score. The result suggests
that a small set of substitution rules can accommo-
date most cases of inflectional variations.
3.3 Experiment 2: String transformation
The second experiment examined the performance
of the string normalization tasks formalized in equa-
tion 1. In this task, a system was given a string s and
was required to yield either its transformed form t?
(s 6= t?) or the string s itself when the transforma-
tion is unnecessary for s. The conditional probabil-
ity distribution (scorer) in equation 1 was modeled
453
System Orthography Derivation Inflection XTAG morph 1.5
P R F1 P R F1 P R F1 P R F1
Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688
CST?s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587
Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827
Table 5: Performance of string transformation
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0 1000 2000 3000 4000 5000 6000 7000
F1 s
core
Number of active features (with non-zero weights)
Spelling variation
Figure 2: Number of active features and performance.
by the maximum entropy framework. Features for
the maximum entropy model consist of: substitution
rules between strings s and t, letter bigrams and tri-
grams in s, and letter bigrams and trigrams in t.
We prepared four datasets, Orthography, Deriva-
tion, Inflection, and XTAG morphology. Each
dataset is a list of string pairs ?s, t? that indicate
the transformation of the string s into t. A source
string s is identical to its destination string t when
string s should not be changed. These instances
correspond to the case where string s has already
been lemmatized. For each string pair (s, t) in LR-
SPL9, LRNOM, and LRAGR tables, we generated
two instances ?s, t? and ?t, t?. Consequently, a sys-
tem is expected to leave the string t unchanged. We
also used XTAG morphology10 to perform a cross-
domain evaluation of the lemmatizer trained on the
Inflection dataset11. The entries in XTAG morphol-
9We define that s precedes t in dictionary order.
10XTAG morphology database 1.5:
ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.
5/morph-1.5.tar.gz
11We found that XTAG morphology contains numerous in-
ogy that also appear in the Inflection dataset were
39,130 out of 317,322 (12.3 %). We evaluated
the proposed method and CST?s lemmatizer by per-
forming ten-fold cross validation.
Table 5 reports the performance based on the
number of correct transformations. The proposed
method again outperformed the baseline systems
with a wide margin. It is noteworthy that the pro-
posed method can accommodate morphological in-
flections in the XTAG morphology corpus with no
manual tuning or adaptation.
Although we introduced no assumptions about
target tasks (e.g. a known vocabulary), the aver-
age number of positive substitution rules relevant
to source strings was as small as 23.9 (in XTAG
morphology data). Therefore, the candidate gen-
erator performed 23.9 substitution operations for a
given string. It applied the decision rules (equa-
tion 7) 21.3 times, and generated 1.67 candidate
strings per source string. The experimental results
described herein demonstrated that the candidate
generator was modeled successfully by the discrim-
inative framework.
4 Related work
The task of string transformation has a long history
in natural language processing and information re-
trieval. As described in Section 1, this task is re-
lated closely to various applications. Therefore, we
specifically examine several prior studies that are
relevant to this paper in terms of technical aspects.
Some researchers have reported the effectiveness
of the discriminative framework of string similarity.
MaCallum et al (2005) proposed a method to train
the costs of edit operations using Conditional Ran-
dom Fields (CRFs). Bergsma and Kondrak (2007)
correct comparative and superlative adjectives, e.g., unpopular
? unpopularer ? unpopularest and refundable ? refundabler
? refundablest. Therefore, we removed inflection entries for
comparative and superlative adjectives from the dataset.
454
presented an alignment-based discriminative string
similarity. They extracted features from substring
pairs that are consistent to a character-based align-
ment of two strings. Aramaki et al (2008) also used
features that express the different segments of the
two strings. However, these studies are not suited for
a candidate generator because the processes of string
transformations are intractable in their discrimina-
tive models.
Dalianis and Jongejan (2006) presented a lem-
matiser based on suffix rules. Although they pro-
posed a method to obtain suffix rules from a training
data, the method did not use counter-examples (neg-
atives) for reducing incorrect string transformations.
Tsuruoka et al (2008) proposed a scoring method
for discovering a list of normalization rules for dic-
tionary look-ups. However, their objective was to
transform given strings, so that strings (e.g., studies
and study) referring to the same concept in the dic-
tionary are mapped into the same string (e.g., stud);
in contrast, this study maps strings into their destina-
tion strings that were specified by the training data.
5 Conclusion
We have presented a discriminative approach for
generating candidates for string transformation.
Unlike conventional spelling-correction tasks, this
study did not assume a fixed set of destination
strings (e.g. correct words), but could even generate
unseen candidate strings. We used anL1-regularized
logistic regression model with substring-substitution
features so that candidate strings for a given string
can be enumerated using the efficient algorithm. The
results of experiments described herein showed re-
markable improvements and usefulness of the pro-
posed approach in three tasks: normalization of or-
thographic variants, noun derivation, and lemmati-
zation.
The method presented in this paper allows only
one region of change in string transformation. A
natural extension of this study is to handle mul-
tiple regions of changes for morphologically rich
languages (e.g. German) and to handle changes
at the phrase/term level (e.g., ?estrogen receptor?
and ?receptor of oestrogen?). Another direction
would be to incorporate the methodologies for semi-
supervised machine learning to accommodate situa-
tions in which positive instances and/or unlabeled
strings are insufficient.
Acknowledgments
This work was partially supported by Grants-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and for Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character struc-
ture to identify semantically related pairs of words and
document titles. Information Storage and Retrieval,
10(7-8):253?260.
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 955?962.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Machine
Learning (ICML 2007), pages 33?40.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), pages 48?55.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
656?663.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining (KDD 2003), pages 39?48.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on the As-
sociation for Computational Linguistics (ACL 2000),
pages 286?293.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
455
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the Sixth International Conference on
Machine Learning and Applications (ICMLA 2007),
pages 166?171.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 181?189.
Hercules Dalianis and Bart Jongejan. 2006. Hand-
crafted versus machine-learned inflectional rules: The
euroling-siteseeker stemmer and cst?s lemmatiser. In
In Proceedings of the 6th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 663?666.
Okan Kolak and Philip Resnik. 2002. OCR error correc-
tion using a noisy channel model. In Proceedings of
the second international conference on Human Lan-
guage Technology Research (HLT 2002), pages 257?
262.
Grzegorz Kondrak. 2005. Cognates and word alignment
in bitexts. In Proceedings of the Tenth Machine Trans-
lation Summit (MT Summit X), pages 305?312.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 1999),
pages 25?32.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association for
Computational Linguistics (Coling-ACL 2006), pages
1025?1032.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI 2005), pages 388?395.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys (CSUR),
33(1):31?88.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning (ICML 2004), pages 78?85.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27(3):379?423.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-
niadou. 2008. Normalizing biomedical terms by min-
imizing ambiguity and variability. BMC Bioinformat-
ics, Suppl 3(9):S2.
W. John Wilbur, Won Kim, and Natalie Xie. 2006.
Spelling correction in the PubMed search engine. In-
formation Retrieval, 9(5):543?564.
456
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 643?650,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Term Recognition Approach to Acronym Recognition
Naoaki Okazaki ?
Graduate School of Information
Science and Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
113-8656 Japan
okazaki@mi.ci.i.u-tokyo.ac.jp
Sophia Ananiadou
National Centre for Text Mining
School of Informatics
Manchester University
PO Box 88, Sackville Street, Manchester
M60 1QD United Kingdom
Sophia.Ananiadou@manchester.ac.uk
Abstract
We present a term recognition approach
to extract acronyms and their definitions
from a large text collection. Parentheti-
cal expressions appearing in a text collec-
tion are identified as potential acronyms.
Assuming terms appearing frequently in
the proximity of an acronym to be
the expanded forms (definitions) of the
acronyms, we apply a term recognition
method to enumerate such candidates and
to measure the likelihood scores of the
expanded forms. Based on the list of
the expanded forms and their likelihood
scores, the proposed algorithm determines
the final acronym-definition pairs. The
proposed method combined with a letter
matching algorithm achieved 78% preci-
sion and 85% recall on an evaluation cor-
pus with 4,212 acronym-definition pairs.
1 Introduction
In the biomedical literature the amount of terms
(names of genes, proteins, chemical compounds,
drugs, organisms, etc) is increasing at an astound-
ing rate. Existing terminological resources and
scientific databases (such as Swiss-Prot1, SGD2,
FlyBase3, and UniProt4) cannot keep up-to-date
with the growth of neologisms (Pustejovsky et al,
2001). Although curation teams maintain termino-
logical resources, integrating neologisms is very
difficult if not based on systematic extraction and
?Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
1http://www.ebi.ac.uk/swissprot/
2http://www.yeastgenome.org/
3http://www.flybase.org/
4http://www.ebi.ac.uk/GOA/
collection of terminology from literature. Term
identification in literature is one of the major bot-
tlenecks in processing information in biology as it
faces many challenges (Ananiadou and Nenadic,
2006; Friedman et al, 2001; Bodenreider, 2004).
The major challenges are due to term variation,
e.g. spelling, morphological, syntactic, semantic
variations (one term having different termforms),
term synonymy and homonymy, which are all cen-
tral concerns of any term management system.
Acronyms are among the most productive type
of term variation. Acronyms (e.g. RARA)
are compressed forms of terms, and are used
as substitutes of the fully expanded termforms
(e.g., retinoic acid receptor alpha). Chang and
Schu?tze (2006) reported that, in MEDLINE ab-
stracts, 64,242 new acronyms were introduced in
2004 with the estimated number being 800,000.
Wren et al (2005) reported that 5,477 documents
could be retrieved by using the acronym JNK
while only 3,773 documents could be retrieved by
using its full term, c-jun N-terminal kinase.
In practice, there are no rules or exact patterns
for the creation of acronyms. Moreover, acronyms
are ambiguous, i.e., the same acronym may re-
fer to different concepts (GR abbreviates both glu-
cocorticoid receptor and glutathione reductase).
Acronyms also have variant forms (e.g. NF kappa
B, NF kB, NF-KB, NF-kappaB, NFKB factor for
nuclear factor-kappa B). Ambiguity and variation
present a challenge for any text mining system,
since acronyms have not only to be recognised, but
their variants have to be linked to the same canon-
ical form and be disambiguated.
Thus, discovering acronyms and relating them
to their expanded forms is important for terminol-
ogy management. In this paper, we present a term
recognition approach to construct an acronym dic-
643
tionary from a large text collection. The proposed
method focuses on terms appearing frequently in
the proximity of an acronym and measures the
likelihood scores of such terms to be the expanded
forms of the acronyms. We also describe an algo-
rithm to combine the proposed method with a con-
ventional letter-based method for acronym recog-
nition.
2 Related Work
The goal of acronym identification is to extract
pairs of short forms (acronyms) and long forms
(their expanded forms or definitions) occurring in
text5. Currently, most methods are based on let-
ter matching of the acronym-definition pair, e.g.,
hidden markov model (HMM), to identify short/-
long form candidates. Existing methods of short-
/long form recognition are divided into pattern
matching approaches, e.g., exploring an efficient
set of heuristics/rules (Adar, 2004; Ao and Takagi,
2005; Schwartz and Hearst, 2003; Wren and Gar-
ner, 2002; Yu et al, 2002), and pattern mining ap-
proaches, e.g., Longest Common Substring (LCS)
formalization (Chang and Schu?tze, 2006; Taghva
and Gilbreth, 1999).
Schwartz and Hearst (2003) implemented an al-
gorithm for identifying acronyms by using paren-
thetical expressions as a marker of a short form.
A character matching technique was used, i.e. all
letters and digits in a short form had to appear in
the corresponding long form in the same order, to
determine its long form. Even though the core al-
gorithm was very simple, the authors report 99%
precision and 84% recall on the Medstract gold
standard6.
However, the letter-matching approach is af-
fected by the expressions in the source text and
sometimes finds incorrect long forms such as
acquired syndrome and a patient with human
immunodeficiency syndrome7 instead of the cor-
rect one, acquired immune deficiency syndrome
for the acronym AIDS. This approach also en-
counters difficulties finding a long form whose
short form is arranged in a different word order,
e.g., beta 2 adrenergic receptor (ADRB2). To
5This paper uses the terms ?short form? and ?long form?
hereafter. ?Long form? is what others call ?definition?,
?meaning?, ?expansion?, and ?expanded form? of acronym.
6http://www.medstract.org/
7These examples are obtained from the actual MED-
LINE abstracts submitted to Schwartz and Hearst?s algorithm
(2003). An author does not always write a proper definition
with a parenthetic expression.
improve the accuracy of long/short form recogni-
tion, some methods measure the appropriateness
of these candidates based on a set of rules (Ao and
Takagi, 2005), scoring functions (Adar, 2004), sta-
tistical analysis (Hisamitsu and Niwa, 2001; Liu
and Friedman, 2003) and machine learning ap-
proaches (Chang and Schu?tze, 2006; Pakhomov,
2002; Nadeau and Turney, 2005).
Chang and Schu?tze (2006) present an algorithm
for matching short/long forms with a statistical
learning method. They discover a list of abbrevia-
tion candidates based on parentheses and enumer-
ate possible short/long form candidates by a dy-
namic programming algorithm. The likelihood of
the recognized candidates is estimated as the prob-
ability calculated from a logistic regression with
nine features such as the percentage of long-form
letters aligned at the beginning of a word. Their
method achieved 80% precision and 83% recall on
the Medstract corpus.
Hisamitsu and Niwa (2001) propose a method
for extracting useful parenthetical expressions
from Japanese newspaper articles. Their method
measures the co-occurrence strength between the
inner and outer phrases of a parenthetical expres-
sion by using statistical measures such as mutual
information, ?2 test with Yate?s correction, Dice
coefficient, log-likelihood ratio, etc. Their method
deals with generic parenthetical expressions (e.g.,
abbreviation, non abbreviation paraphrase, supple-
mentary comments), not focusing exclusively on
acronym recognition.
Liu and Friedman (2003) proposed a method
based on mining collocations occurring before the
parenthetical expressions. Their method creates a
list of potential long forms from collocations ap-
pearing more than once in a text collection and
eliminates unlikely candidates with three rules,
e.g., ?remove a set of candidates Tw formed by
adding a prefix word to a candidate w if the num-
ber of such candidates Tw is greater than 3?. Their
approach cannot recognise expanded forms occur-
ring only once in the corpus. They reported a pre-
cision of 96.3% and a recall of 88.5% for abbrevi-
ations recognition on their test corpus.
3 Methodology
3.1 Term-based long-form identification
We propose a method for identifying the long
forms of an acronym based on a term extrac-
tion technique. We focus on terms appearing fre-
644
factor 1 (TTF-1)transcription
transciptiontransription
thyroid
thyroidtissue specific nkx2
thyroidthyroid
expression of
co-expression ofregulation of thecontainingexpressedstained foridentification of
encodinggene
examinedexploreincreasedstudiedits...... ............
............
............................................................
............
............
............ ......
216 218213209
11
33
1
1
1
1
1
1
1
1
1
1
1
1
factor5one1protein11
4 231 factor2
1
nuclearthyroid...... 1
* These candidates are spelling mistakes    found in the MEDLINE abstracts.
Figure 1: Long-form candidates for TTF-1.
quently in the proximity of an acronym in a text
collection. More specifically, if a word sequence
co-occurs frequently with a specific acronym and
not with other surrounding words, we assume that
there is a relationship8 between the acronym and
the word sequence.
Figure 1 illustrates our hypothesis taking the
acronym TTF-1 as an example. The tree consists
of expressions collected from all sentences with
the acronym in parentheses and appearing before
the acronym. A node represents a word, and a path
from any node to TTF-1 represents a long-form
candidate9. The figure above each node shows
the co-occurrence frequency of the corresponding
long-form candidate. For example, long-form can-
didates 1, factor 1, transcription factor 1, and thy-
roid transcription factor 1 co-occur 218, 216, 213,
and 209 times respectively with the acronym TTF-
1 in the text collection.
Even though long-form candidates 1, factor
1 and transcription factor 1 co-occur frequently
with the acronym TTF-1, we note that they
also co-occur frequently with the word thyroid.
Meanwhile, the candidate thyroid transcription
factor 1 is used in a number of contexts (e.g.,
expression of thyroid transcription factor 1,
expressed thyroid transcription factor 1, gene
encoding thyroid transcription factor 1, etc.).
Therefore, we observe this to be the strongest
relationship between acronym TTF-1 and its
8A sequence of words that co-occurs with an acronym
does not always imply the acronym-definition relation. For
example, the acronym 5-HT co-occurs frequently with the
term serotonin, but their relation is interpreted as a synony-
mous relation.
9The words with function words (e.g., expression of, reg-
ulation of the, etc.) are combined into a node. This is due
to the requirement for a long-form candidate discussed later
(Section 3.3).
A large collection of text
Contextual sentencesfor acronyms
Acronym dictionary
Short-formmining
Long-formminingLong-formvalidation
Raw text
Sentences witha specific acronym
All sentences withany acronyms
Acronyms andexpanded forms
Figure 2: System diagram of acronym recognition
long-form candidate thyroid transcription factor 1
in the tree. We apply a number of validation rules
(described later) to the candidate pair to make
sure that it has an acronym-definition relation. In
this example, the candidate pair is likely to be
an acronym-definition relation because the long
form thyroid transcription factor 1 contains all
alphanumeric letters in the short form TTF-1.
Figure 1 also shows another notable character-
istic of long-form recognition. Assuming that the
term thyroid transcription factor 1 has an acronym
TTF-1, we can disregard candidates such as tran-
scription factor 1, factor 1, and 1 since they lack
the necessary elements (e.g., thyroid for all can-
didates; thyroid transcription for candidates fac-
tor 1 and 1; etc.) to produce the acronym TTF-
1. Similarly, we can disregard candidates such
as expression of thyroid transcription factor 1 and
encoding thyroid transcription factor 1 since they
contain unnecessary elements (i.e., expression of
and encoding) attached to the long-form. Hence,
once thyroid transcription factor 1 is chosen as
the most likely long form of the acronym TTF-
1, we prune the unlikely candidates: nested can-
didates (e.g., transcription factor 1); expansions
(e.g., expression of thyroid transcription factor 1);
and insertions (e.g., thyroid specific transcription
factor 1).
3.2 Extracting acronyms and their contexts
Before describing in detail the formalization of
long-form identification, we explain the whole
process of acronym recognition. We divide the
acronym extraction task into three steps (Figure
2):
1. Short-form mining: identifying and extract-
ing short forms (i.e., acronyms) in a collec-
tion of documents
2. Long-form mining: generating a list of
ranked long-form candidates for each short
645
Acronym Contextual sentence
... .... .... .. . .... ..
HML Hard metal lung diseases (HML) are rare, and complex
to diagnose.
HMM Heavy meromyosin (HMM) from conditioned hearts
had a higher Ca++-ATPase activity than from controls.
HMM Heavy meromyosin (HMM) and myosin subfragment 1
(S1) were prepared from myosin by using low concen-
trations of alpha-chymotrypsin.
HMM Hidden Markov model (HMM) techniques are used to
model families of biological sequences.
HMM Hexamethylmelamine (HMM) is a cytotoxic agent
demonstrated to have broad antitumor activity.
HMN Hereditary metabolic neuropathies (HMN) are marked
by inherited enzyme or other metabolic defects.
... ... .. ..... .. ....... . .......
Table 1: An example of extracted acronyms and
their contextual sentences.
form by using a term extraction technique
3. Long-form validation: extracting short/long
form pairs recognized as having an acronym-
definition relation and eliminating unneces-
sary candidates.
The first step, short-form mining, enumerates all
short forms in a target text which are likely to be
acronyms. Most studies make use of the follow-
ing pattern to find candidate acronyms (Wren and
Garner, 2002; Schwartz and Hearst, 2003):
long form ?(? short form ?)?
Just as the heuristic rules described in Schwartz
and Hearst (Schwartz and Hearst, 2003), we con-
sider short forms to be valid only if they consist of
at most two words; their length is between two to
ten characters; they contain at least an alphabetic
letter; and the first character is alphanumeric. All
sentences containing a short form in parenthesis
are inserted into a database, which returns all con-
textual sentences for a short form to be processed
in the next step. Table 1 shows an example of the
database content.
3.3 Formalizing long-form mining as a term
extraction problem
The second step, long-form mining, generates a
list of long-form candidates and their likelihood
scores for each short form. As mentioned previ-
ously, we focus on words or word sequences that
co-occur frequently with a specific acronym and
not with any other surrounding words. We deal
with the problem of extracting long-form candi-
dates from contextual sentences for an acronym
in a similar manner as the term recognition task
which extracts terms from the given text. For that
purpose, we used a modified version of the C-
value method (Frantzi and Ananiadou, 1999).
C-value is a domain-independent method for
automatic term recognition (ATR) which com-
bines linguistic and statistical information, empha-
sis being placed on the statistical part. The lin-
guistic analysis enumerates all candidate terms in
a given text by applying part-of-speech tagging,
candidate extraction (e.g., extracting sequences
of adjectives/nouns based on part-of-speech tags),
and a stop-list. The statistical analysis assigns
a termhood (likelihood to be a term) to a candi-
date term by using the following features: the fre-
quency of occurrence of the candidate term; the
frequency of the candidate term as part of other
longer candidate terms; the number of these longer
candidate terms; and the length of the candidate
term.
The C-value approach is characterized by the
extraction of nested terms which gives preference
to terms appearing frequently in a given text but
not as a part of specific longer terms. This is a de-
sirable feature for acronym recognition to identify
long-form candidates in contextual sentences. The
rest of this subsection describes the method to ex-
tract long-form candidates and to assign scores to
the candidates based on the C-value approach.
Given a contextual sentence as shown in Ta-
ble 1, we tokenize a contextual sentence by
non-alphanumeric characters (e.g., space, hyphen,
colon, etc.) and apply Porter?s stemming algo-
rithm (Porter, 1980) to obtain a sequence of nor-
malized words. We use the following pattern to
extract long-form candidates from the sequence:
[:WORD:].*$ (1)
Therein: [:WORD:] matches a non-function
word; .* matches an empty string or any word(s)
of any length; and $ matches a short form of the
target acronym. The extraction pattern accepts a
word or word sequence if the word or word se-
quence begins with any non-function word, and
ends with any word just before the corresponding
short form in the contextual sentence. We have
defined 113 function words such as a, the, of, we,
and be in an external dictionary so that long-form
candidates cannot begin with these words.
Let us take the example of a contextual sen-
tence, ?we studied the expression of thyroid tran-
scription factor-1 (TTF-1)?. We extract the fol-
lowing substrings as long form candidates (words
are stemmed): 1; factor 1; transcript factor 1; thy-
roid transcript factor 1; expression of thyroid tran-
script factor 1; and studi the expression of thyroid
646
Candidate Length Freq Score Valid
adriamycin 1 727 721.4 o
adrenomedullin 1 247 241.7 o
abductor digiti minimi 3 78 74.9 o
doxorubicin 1 56 54.6 L
effect of adriamycin 3 25 23.6 E
adrenodemedullated 1 19 17.7 o
acellular dermal matrix 3 17 15.9 o
peptide adrenomedullin 2 17 15.1 E
effects of adrenomedullin 3 15 13.2 E
resistance to adriamycin 3 15 13.2 E
amyopathic dermatomyositis 2 14 12.8 o
vincristine (vcr) and adriamycin 4 11 10.0 E
drug adriamycin 2 14 10.0 E
brevis and abductor digiti minimi 5 11 9.8 E
minimi 1 83 5.8 N
digiti minimi 2 80 3.9 N
right abductor digiti minimi 4 4 2.5 E
automated digital microscopy 3 1 0.0 m
adrenomedullin concentration 2 1 0.0 N
Valid = { o: valid, m: letter match, L: lacks necessary letters, E: expansion,
N: nested, B: below the threshold }
Table 2: Long-form candidates for ADM.
transcript factor 1. Substrings such as of thyroid
transcript factor 1 (which begins with a function
word) and thyroid transcript (which ends prema-
turely before the short form) are not selected as
long-form candidates.
We define the likelihood LF(w) for candidate w
to be the long form of an acronym:
LF(w) = freq(w)?
?
t?Tw
freq(t)? freq(t)freq(Tw) . (2)
Therein: w is a long-form candidate; freq(x) de-
notes the frequency of occurrence of a candidate
x in the contextual sentences (i.e., co-occurrence
frequency with a short form); Tw is a set of nested
candidates, long-form candidates each of which
consists of a preceding word followed by the can-
didate w; and freq(Tw) represents the total fre-
quency of such candidates Tw.
The first term is equivalent to the co-occurrence
frequency of a long-form candidate with a short
form. The second term discounts the co-
occurrence frequency based on the frequency dis-
tribution of nested candidates. Given a long-form
candidate t ? Tw, freq(t)freq(Tw) presents the occurrence
probability of candidate t in the nested candidate
set Tw. Therefore, the second term of the formula
calculates the expectation of the frequency of oc-
currence of a nested candidate accounting for the
frequency of candidate w.
Table 2 shows a list of long-form candidates for
acronym ADM extracted from 7,306,153 MED-
LINE abstracts10. The long-form mining step
10 52GB XML files (from medline05n0001.xml to
medline05n0500.xml)
extracted 10,216 unique long-form candidates
from 1,319 contextual sentences containing the
acronym ADM in parentheses. Table 2 arranges
long-form candidates with their scores in de-
sending order. Long-form candidates adriamycin
and adrenomedullin co-occur frequently with the
acronym ADM.
Note the huge difference in scores between
the candidates abductor digiti minimi and minimi.
Even though the candidate minimi co-occurs more
frequently (83 times) than abductor digiti minimi
(78 times), the co-occurrence frequency is mostly
derived from the longer candidate, i.e., digiti min-
imi. In this case, the second term of Formula
2, the occurrence-frequency expectation of expan-
sions for minimi (e.g., digiti minimi), will have a
high value and will therefore lower the score of
candidate minimi. This is also true for the can-
didate digiti minimi, i.e., the score of candidate
digiti minimi is lowered by the longer candidate
abductor digiti minimi. In contrast, the candidate
abductor digiti minimi preserves its co-occurrence
frequency since the second term of the formula is
low, which means that each expansion (e.g, brevis
and abductor digiti minimi, right abductor digiti
minimi, ...) is expected to have a low frequency of
occurrence.
3.4 Validation rules for long-form candidates
The final step of Figure 2 validates the extracted
long-form candidates to generate a final set of
short/long form pairs. According to the score
in Table 2, adriamycin is the most likely long-
form for acronym ADM. Since the long-form
candidate adriamycin contains all letters in the
acronym ADM, it is considered as an authentic
long-form (marked as ?o? in the Valid field). This
is also true for the second and third candidate
(adrenomedullin and abductor digiti minimi).
The fourth candidate doxorubicin looks inter-
esting, i.e., the proposed method assigns a high
score to the candidate even though it lacks the let-
ters a and m, which are necessary to form the cor-
responding short form. This is because doxoru-
bicin is a synonymous term for adriamycin and de-
scribed directly with its acronym ADM. In this pa-
per, we deal with the acronym-definition relation
although the proposed method would be applica-
ble to mining other types of relations marked by
parenthetical expressions. Hence, we introduce a
constraint that a long form must cover all alphanu-
647
# [ V a r i a b l e s ]
# s f : t h e t a r g e t s h o r t?form .
# c a n d i d a t e s : long?form c a n d i d a t e s .
# r e s u l t : t h e l i s t o f d e c i s i v e long?f o rms .
# t h r e s h o l d : t h e t h r e s h o l d o f cu t?o f f .
# S o r t long?form c a n d i d a t e s i n d e s c e n d i n g o r d e r
c a n d i d a t e s . s o r t ( # o f s c o r e s .
key=lambda l f : l f . s c o r e , r e v e r s e =True )
# I n i t i a l i z e r e s u l t l i s t as empty .
r e s u l t = [ ]
# Pick up a lo ng form one by one from c a n d i d a t e s .
f o r l f in c a n d i d a t e s :
# Apply a cu t?o f f based on termhood s c o r e .
# Al low c a n d i d a t e s w i t h l e t t e r match ing . . . . . ( a )
i f l f . s c o r e < t h r e s h o l d and not l f . match :
c o n t in u e
# A long?form must c o n t a i n a l l l e t t e r s . . . . . . ( b )
i f l e t t e r r e c a l l ( s f , l f ) < 1 :
c o n t in u e
# Apply p r u n i n g o f r e d u n d a n t l ong form . . . . . . ( c )
i f r e d u n d a n t ( r e s u l t , l f ) :
c o n t in u e
# I n s e r t t h i s l ong form t o t h e r e s u l t l i s t .
r e s u l t . append ( l f )
# Outpu t t h e d e c i s i v e long?f o rms .
p r i n t r e s u l t
Figure 3: Pseudo-code for long-form validation.
meric letters in the short form.
The fifth candidate effect of adriamycin is an
expansion of a long form adriamycin, which has
a higher score than effect of adriamycin. As we
discussed previously, the candidate effect of adri-
amycin is skipped since it contains unnecessary
word(s) to form an acronym. Similarly, we prune
the candidate minimi because it forms a part of an-
other long form abductor digiti minimi, which has
a higher score than the candidate minimi. The like-
lihood score LF (w) determines the most appro-
priate long-form among similar candidates sharing
the same words or lacking some words.
We do not include candidates with scores be-
low a given threshold. Therefore, the proposed
method cannot extract candidates appearing rarely
in the text collection. It depends on the applica-
tion and considerations of the trade-off between
precision and recall, whether or not an acronym
recognition system should extract such rare long
forms. When integrating the proposed method
with e.g., Schwartz and Hearst?s algorithm, we
treat candidates recognized by the external method
as if they pass the score cut-off. In Table 2, for
example, candidate automated digital microscopy
is inserted into the result set whereas candidate
adrenomedullin concentration is skipped since it
is nested by candidate adrenomedullin.
Figure 3 is a pseudo-code for the long-form val-
idation algorithm described above. A long-form
Rank Parenthetic phrase # contextual # unique
sentence long-forms
1 CT 30,982 171
2 PCR 25,387 39
3 HIV 19,566 13
4 LPS 18,071 51
5 MRI 16,966 18
6 ELISA 16,527 25
7 SD 15,760 165
8 BP 14,860 145
9 DA 14,518 129
10 CSF 14,035 34
11 CNS 13,573 47
12 IL 13,423 60
13 PKC 13,414 11
14 TNF-ALPHA 12,228 14
15 HPLC 12,211 16
16 ER 12,155 140
17 RT-PCR 12,153 21
18 TNF 12,145 13
19 LDL 11,960 24
20 5-HT 11,836 20
.. .... ... ..
? (overall 50 acronyms) 600,375 4,212
Table 3: Statistics on our evaluation corpus.
candidate is considered valid if the following con-
ditions are met: (a) it has a score greater than
a threshold or is nominated by a letter-matching
algorithm; (b) it contains all letters in the corre-
sponding short form; and (c) it is not nested, ex-
pansion, or insertion of the previously chosen long
forms.
4 Evaluation
Several evaluation corpora for acronym recogni-
tion are available. The Medstract Gold Standard
Evaluation Corpus, which consists of 166 alias
pairs annotated to 201 MEDLINE abstracts, is
widely used for evaluation (Chang and Schu?tze,
2006; Schwartz and Hearst, 2003). However, the
amount of the text in the corpus is insufficient for
the proposed method, which makes use of statisti-
cal features in a text collection. Therefore, we pre-
pared an evaluation corpus with a large text collec-
tion and examined how the proposed algorithm ex-
tracts short/long forms precisely and comprehen-
sively.
We applied the short-form mining described
in Section 3 to 7,306,153 MEDLINE abstracts10.
Out of 921,349 unique short-forms recognized by
the short-form mining, top 50 acronyms11 appear-
ing frequently in the abstracts were chosen for our
11We have excluded several parenthetical expressions such
as II (99,378 occurrences), OH (37,452 occurrences), and
P<0.05 (23,678 occurrences). Even though they are enclosed
within parentheses, they do not introduce acronyms. We have
also excluded a few acronyms such as RA (18,655 occur-
rences) and AD (15,540 occurrences) because they have many
variations of their expanded forms to prepare the evaluation
corpus manually.
648
evaluation corpus. We asked an expert in bio-
informatics to extract long forms from 600,375
contextual sentences with the following criteria:
a long form with minimum necessary elements
(words) to produce its acronym is accepted; a long
form with unnecessary elements, e.g., magnetic
resonance imaging unit (MRI) or computed x-ray
tomography (CT), is not accepted; a misspelled
long-form, e.g., hidden markvov model (HMM),
is accepted (to separate the acronym-recognition
task from a spelling-correction task). Table 3
shows the top 20 acronyms in our evaluation cor-
pus, the number of their contextual sentences, and
the number of unique long-forms extracted.
Using this evaluation corpus as a gold standard,
we examined precision, recall, and f-measure12 of
long forms recognized by the proposed algorithm
and baseline systems. We compared five sys-
tems: the proposed algorithm with Schwartz and
Hearst?s algorithm integrated (PM+SH); the pro-
posed algorithm without any letter-matching algo-
rithm integrated (PM); the proposed algorithm but
using the original C-value measure for long-form
likelihood scores (CV+SH); the proposed algo-
rithm but using co-occurrence frequency for long-
form likelihood scores (FQ+SH); and Schwartz
and Hearst?s algorithm (SH). The threshold for the
proposed algorithm was set to four.
Table 4 shows the evaluation result. The best-
performing configuration of algorithms (PM+SH)
achieved 78% precision and 85% recall. The
Schwartz and Hearst?s (SH) algorithm obtained a
good recall (93%) but misrecognized a number
of long-forms (56% precision), e.g., the kinetics
of serum tumour necrosis alpha (TNF-ALPHA)
and infected mice lacking the gamma interferon
(IFN-GAMMA). The SH algorithm cannot gather
variations of long forms for an acronym, e.g.,
ACE as angiotensin-converting enzyme level, an-
giotensin i-converting enzyme gene, angiotensin-
1-converting enzyme, angiotensin-converting, an-
giotensin converting activity, etc. The proposed
method combined with the Schwartz and Hearst?s
algorithm remedied these misrecognitions based
on the likelihood scores and the long-form vali-
dation algorithm. The PM+SH also outperformed
other likelihood measures, CV+SH and FQ+SH.
12We count the number of unique long forms, i.e., count
once even if short/long form pair ?HMM, hidden markov
model? occurs more than once in the text collection. The
Porter?s stemming algorithm was applied to long forms be-
fore comparing them with the gold standard.
Method Precision Recall F-measure
PM+SH 0.783 0.849 0.809
CV+SH 0.722 0.838 0.765
FQ+SH 0.716 0.800 0.747
SH 0.555 0.933 0.681
PM 0.815 0.140 0.216
Table 4: Evaluation result of long-form recogni-
tion.
The proposed algorithm without Schwartz and
Hearst?s algorithm (PM) identified long forms the
most precisely (81% precision) but misses a num-
ber of long forms in the text collection (14% re-
call). The result suggested that the proposed likeli-
hood measure performed well to extract frequently
used long-forms in a large text collection, but
could not extract rare acronym-definition pairs.
We also found the case where PM missed a set of
long forms for acronym ER which end with rate,
e.g., eating rate, elimination rate, embolic rate,
etc. This was because the word rate was used with
a variety of expansions (i.e., the likelihood score
for rate was not reduced much) while it can be
also interpreted as the long form of the acronym.
Even though the Medstract corpus is insuffi-
cient for evaluating the proposed method, we ex-
amined the number of long/short pairs extracted
from 7,306,153 MEDLINE abstracts and also ap-
pearing in the Medstract corpus. We can neither
calculate the precision from this experiment nor
compare the recall directly with other acronym
recognition methods since the size of the source
texts is different. Out of 166 pairs in Medstract
corpus, 123 (74%) pairs were exactly covered by
the proposed method, and 15 (83% in total) pairs
were partially covered13. The algorithm missed 28
pairs because: 17 (10%) pairs in the corpus were
not acronyms but more generic aliases, e.g., alpha
tocopherol (Vitamin E); 4 (2%) pairs in the cor-
pus were incorrectly annotated (e.g, long form in
the corpus embryo fibroblasts lacks word mouse to
form acronym MEFS); and 7 (4%) long forms are
missed by the algorithm, e.g., the algorithm recog-
nized pair protein kinase (PKR) while the correct
pair in the corpus is RNA-activated protein kinase
(PKR).
13Medstract corpus leaves unnecessary elements attached
to some long-forms such as general transcription factor iib
(TFIIB), whereas the proposed algorithm may drop the un-
necessary elements (i.e. general) based on the frequency. We
regard such cases as partly correct.
649
5 Conclusion
In this paper we described a term recognition ap-
proach to extract acronyms and their definitions
from a large text collection. The main contribution
of this study has been to show the usefulness of
statistical information for recognizing acronyms in
large text collections. The proposed method com-
bined with a letter matching algorithm achieved
78% precision and 85% recall on the evaluation
corpus with 4,212 acronym-definition pairs.
A future direction of this study would be to
incorporate other types of relations expressed
with parenthesis such as synonym, paraphrase,
etc. Although this study dealt with the acronym-
definition relation only, modelling these relations
will also contribute to the accuracy of the acronym
recognition, establishing a methodology to distin-
guish the acronym-definition relation from other
types of relations.
References
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527?
533.
Sophia Ananiadou and Goran Nenadic. 2006. Auto-
matic terminology management in biomedicine. In
Sophia Ananiadou and John McNaught, editors, Text
Mining for Biology and Biomedicine, pages 67?97.
Artech House, Inc.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): Integrating biomedical ter-
minology. Nucleic Acids Research, 32:267?270.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99?119. Artech House, Inc.
Katerina T. Frantzi and Sophia Ananiadou. 1999. The
C-value / NC-value domain independent method for
multi-word term extraction. Journal of Natural Lan-
guage Processing, 6(3):145?179.
Carol Friedman, Hongfang Liu, Lyuda Shagina,
Stephen Johnson, and George Hripcsak. 2001.
Evaluating the UMLS as a source of lexical knowl-
edge for medical language processing. In AMIA
Symposium, pages 189?193.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extract-
ing useful terms from parenthetical expression by
combining simple rules and statistical measures: A
comparative evaluation of bigram statistics. In Di-
dier Bourigault, Christian Jacquemin, and Marie-
C L?Homme, editors, Recent Advances in Compu-
tational Terminology, pages 209?224. John Ben-
jamins.
Hongfang Liu and Carol Friedman. 2003. Mining
terminological knowledge in large biomedical cor-
pora. In 8th Pacific Symposium on Biocomputing
(PSB 2003), pages 415?426.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identifica-
tion. In 8th Canadian Conference on Artificial In-
telligence (AI?2005) (LNAI 3501), page 10 pages.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbrevia-
tion normalization in medical texts. In 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 126?133.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
James Pustejovsky, Jose? Castan?o, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym meaning pairs from
MEDLINE databases. MEDINFO 2001, pages 371?
375.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451?462.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191?198.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426?434.
Jonathan D. Wren, Jeffrey T. Chang, James Puste-
jovsky, Eytan Adar, Harold R. Garner, and Russ B.
Altman. 2005. Biomedical term mapping
databases. Database Issue, 33:D289?D293.
Hong Yu, George Hripcsak, and Carol Friedman. 2002.
Mapping abbreviations to full forms in biomedical
articles. Journal of the American Medical Informat-
ics Association, 9(3):262?272.
650
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
A Methodology for Terminology-based 
Knowledge Acquisition and Integration 
 
Hideki Mima1
?
, Sophia Ananiadou2, Goran Nenadic2 and Junichi Tsujii1 
 
1Dept. of Information Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{mima, tsujii}@is.s.u-tokyo.ac.jp 
2Computer Science, University of Salford 
Newton Building, Salford M5 4WT, UK 
{S.Ananiadou, G.Nenadic}@salford.ac.uk 
 
                                                
? Current affiliation: Dept. of Engineering, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113- 8656, Japan 
Abstract  
In this paper we propose an integrated knowledge 
management system in which terminology-based 
knowledge acquisition, knowledge integration, 
and XML-based knowledge retrieval are 
combined using tag information and ontology 
management tools. The main objective of the 
system is to facilitate knowledge acquisition 
through query answering against XML-based 
documents in the domain of molecular biology. 
Our system integrates automatic term recognition, 
term variation management, context-based 
automatic term clustering, ontology-based 
inference, and intelligent tag information retrieval. 
Tag-based retrieval is implemented through 
interval operations, which prove to be a powerful 
means for textual mining and knowledge 
acquisition. The aim is to provide efficient access 
to heterogeneous biological textual data and 
databases, enabling users to integrate a wide 
range of textual and non-textual resources 
effortlessly. 
Introduction 
With the recent increasing importance of 
electronic communication and data sharing over 
the Internet, there exist an increasingly growing 
number of publicly accessible knowledge sources, 
both in the form of documents and factual 
databases. These knowledge sources (KSs) are 
intrinsically heterogeneous and dynamic. They 
are heterogeneous since they are autonomously 
developed and maintained by independent 
organizations for different purposes. They are 
dynamic since constantly new information is 
being revised, added and removed. Such an 
heterogeneous and dynamic nature of KSs 
imposes challenges on systems that help users to 
locate and integrate knowledge relevant to their 
needs. 
   Knowledge, encoded in textual documents, is 
organised around sets of specialised (technical) 
terms (e.g. names of proteins, genes, acids). 
Therefore, knowledge acquisition relies heavily 
on the recognition of terms. However, the main 
problems that make term recognition difficult are 
the lack of clear naming conventions and 
terminology variation (cf. Jacquemin and 
Tzoukermann (1999)), especially in the domain 
of molecular biology. Therefore, we need a 
scheme to integrate terminology management as 
a key prerequisite for knowledge acquisition and 
integration. 
   However, automatic term extraction is not the 
ultimate goal itself, since the large number of 
new terms calls for a systematic way to access 
and retrieve the knowledge represented through 
them. Therefore, the extracted terms need to be 
placed in an appropriate framework by 
discovering relations between them, and by 
establishing the links between the terms and 
different factual databases. 
   In order to solve the problem, several 
approaches have been proposed. MeSH Term in 
MEDLINE (2002) and Gene Ontology (2002) 
provide a top-down controlled ontology 
framework, which aims to describe and constrain 
the terminology in the domain of molecular 
biology. On the other hand, automatic term 
acquisition approaches have been developed in 
order to address a dynamic and corpus-driven 
knowledge acquisition methodology (Mima et al, 
1999; 2001a).  
   Different approaches to linking relevant 
resources have also been suggested. The 
Semantic Web framework (Berners-Lee (1998)) 
aims to link relevant Web resources in bottom-up 
manner using the Resource Description 
Framework (RDF) (Bricklet and Guha, 2000) and 
an ontology. However, although the Semantic 
Web framework is powerful to express content of 
resources to be semantically retrieved, some 
manual description is expected using the 
RDF/ontology. Since no solution to the 
well-known difficulties in manual ontology 
development, such as the ontology 
conflictions/mismatches (Visser et al, 1997) is 
provided, an automated ontology management is 
required for the efficient and consistent 
knowledge acquisition and integration. TAMBIS 
(Baker et al, 1998) tried to provide a filter from 
biological information services by building a 
homogenising layer on top of the different 
sources using the classical mediator/wrapper 
architecture. It intended to provide source 
transparency using a mapping from terms placed 
in a conceptual knowledge base of molecular 
biology onto terms in external sources.  
   In this paper we introduce TIMS, an integrated 
knowledge management system in the domain of 
molecular biology, where terminology-based 
knowledge acquisition (KA), knowledge 
integration (KI), and XML-based knowledge 
retrieval are combined using tag information and 
ontology management tools. The management of 
knowledge resources, similarly to the Semantic 
Web, is based on XML, RDF, and 
ontology-based inference. However, our aim is to 
facilitate the KA and KI tasks not only by using 
manually defined resource descriptions, but also 
by exploit ing NLP techniques such as automatic 
term recognition (ATR) and automatic term 
clustering (ATC), which are used for automatic 
and systematic ontology population.  
    
   The paper is organised as follows: in section 1 
we present the overall TIMS architecture and 
briefly describe the components incorporated in 
the system, while section 2 gives the details of the 
proposed method for KA and KI. In the last 
section we present results, evaluation and 
discussion. 
1 TIMS ? system architecture 
XML-based Tag Information Management 
System (TIMS) is a core machinery for managing 
XML tag information obtained from sub 
functional components. Its main aim is to 
facilitate an efficient mechanism for KA and KI 
through a query answering system for 
XML-based documents in the domain of 
molecular biology, by using a tag information 
database.  
   Figure 1 shows the system architecture of 
TIMS. It integrates the following modules via  
XML-based data exchange: JTAG ? an 
annotation tool, ATRACT ? an automatic term 
recognition and clustering workbench, and the 
LiLFeS abstract machine, which we briefly 
describe in this section. ATRACT and LiLFeS 
play a central role in the knowledge acquisition 
process, which includes term recognition, 
ontology population, and ontology-based 
inference. In addition to these modules, TIMS 
implements an XML-data manager and a TIQL 
query processor (see Section 2).  
1.1 JTAG 
JTAG is an XML-based manual annotation and 
resource description aid tool. Its purpose is to 
support manual annotation (e.g. semantic 
tagging), adjusting term recognition results, 
developing RDF logic, etc. In addition, ontology 
information described in XML can also be 
developed and modified using the tool. All the 
annotations can be managed via a GUI.  
1.2 ATRACT 
In the domain of molecular biology, there is an 
increasing amount of new terms that represent 
newly created concepts. Since existing term 
Figure 1: System architecture of TIMS 
 
XML Data 
Retrieval 
TIMS
Tag Information Database  
XML Data 
Management 
ATRACT 
Automatic Term 
Recognition 
and Term 
Clustering 
XML data
XML data
XML data
Document/
Database
Retriever
L iLFeS 
Syntactic and 
Semantic Parser / 
RDF and Ontology 
Manager 
JTAG 
Manual Resource 
Description 
Aid Interface XML data
dictionaries cannot cover the needs of specialists, 
automatic term extraction tools are important for 
consistent term discovery. ATRACT (Mima et al, 
2001a) is a terminology management workbench 
that integrates ATR and ATC. Its main aim is to 
help biologists to gather and manage terminology 
in the domain. The module retrieves and 
classifies terms on the fly and sends the results as 
XML tag information to TIMS.  
   The ATR method is based on the C/NC-value 
method (Frantzi et al, 2000). The original 
method has been augmented with acronym 
acquisition and term variation management 
(Nenadic et al 2002), in order to link different 
terms that denote the same concept. Term 
variation management is based on term 
normalisation as an integral part of the ATR 
process. All orthographic, morphological and 
syntactic term variations and acronym variants (if 
any) are conflated prior to the statistical analysis, 
so that term candidates comprise all variants that 
appear in a corpus. 
   Besides term recognition, term clustering is an 
indispensable component in a knowledge 
management process (see figure 2). Since 
terminological opacity and polysemy are very 
common in molecular biology, term clustering is 
essential for the semantic integration of terms, 
the construction of domain ontology and for 
choosing the appropriate semantic information.  
   The ATC method is based on Ushioda?s AMI 
(Average Mutual Information)-hierarchical 
clustering method (Ushioda, 1996). Our 
implementation uses parallel symmetric 
processing for high speed clustering and is built 
on the C/NC-value results. As input, we use 
co-occurrences of automatically recognised 
terms and their contexts, and the output is a 
dendrogram of hierarchical term clusters (like a 
thesaurus). The calculated term cluster 
information is stored in LiLFeS (see below) and 
combined with a predefined ontology according 
to the term classes automatically assigned. 
1.3 LiLFeS 
LiLFeS (Miyao et al, 2000) is a Prolog-like 
programming language and language processor 
used for defining definite clause programs with 
typed feature structures. Since typed feature 
structures can be used like first order terms in 
Prolog, the LiLFeS language can describe 
various kinds of applications based on feature 
structures. Examples include HPSG parsers, 
HPSG-based grammars and compilers from 
HPSG to CFG. Furthermore, other NLP modules 
can be easily developed because feature structure 
processing can be directly written in the LiLFeS 
language. Within TIMS, LiLFeS is used to: 1) 
infer similarity between terms using hierarchical 
matching, and 2) parse sentences using 
HPSG-based parsers and convert the results into 
an XML-based formalism. 
 
2 Knowledge Integration and Management  
 
Knowledge integration and management in 
TIMS is organised by integrating XML-data 
management (section 2.1) and tag- and 
ontology-based information extraction (section 
2.2). Figure 3 illustrates a model of the 
knowledge management based on the knowledge 
integration and question-answering process 
within TIMS. In this scenario, a user formulates a 
query, which is processed by a query manager. 
The tag data manager retrieves the relevant data 
from the collection of documents via a tag 
database and ontology-based inference (such as 
POS Tagger 
Acronym Recognition 
C-value ATR 
Orthographic Variants 
Morphological Variants 
Syntactic Variants 
NC-value Analyzer 
Term Clustering  
(Semantic Analyzer) 
XML Documents Including 
Term Tags and Term 
Variation/Class Information 
Input Documents 
Figure 2. Term Ontology Development 
Recognition of Term 
Variations (synonyms) 
Recognition of Term 
Classes (Similar Terms) 
hierarchical matching of term classes).  
2.1 XML-tag data management 
Communication within TIMS is based on 
XML-data exchange.  TIMS initially parses the 
XML documents (which contain relevant 
terminology information generated automatically 
by ATRACT) and ?de-tags? them. Then, like in 
the TIPSTER architecture (Grishman, 1995), 
every tag information is stored separately from 
the original documents and managed by an 
external database software. This facility allows, 
as shown in figure 4, different types of tags (POS, 
syntactic, semantic, etc.) for the same document 
to be supported. 
2.2 Tag- and ontology-based IE 
The key feature of KA and KI within TIMS is a 
facility to logically retrieve data that is 
represented by different tags. This feature is 
implemented via interval operations. The main 
assumption is that the XML tags specify certain 
intervals within documents. Interval operations 
are XML specific text/data retrieval operations, 
which operate on such textual intervals. Each 
interval operation takes two sets of intervals as 
input and returns a set of intervals according to 
the specified logical operations. Currently, we 
define four types of logical operations: 
? Intersection ??? returns intersected intervals 
of all the intervals given. 
? Union ??? returns merged intervals of all the 
intersected intervals. 
? Subtraction ?y? returns differences in 
intervals of all the intersected intervals. 
? Concatenation ?+? returns concatenated 
intervals of all the continuous intervals. 
 
For example, the interval operation 1 
<VP>?(<V>?<term>) describes all verb 
(<V>)-term (<term>) pairs within a verb phrase 
(<VP>). Similarly, suppose X denotes a set of 
intervals of manually annotated tags for a 
document and Y denotes a set of intervals of 
automatically annotated tags for the same 
document. The interval operation ((X?Y) 
?{X?Y}) results in the differences between 
human and machine annotations (see figure 5). 
Interval operations are powerful means for 
textual mining from different sources using tag 
information.  In addition, LiLFeS enables tag 
(interval) retrieval to process not only regular 
                                                
1 ??? denotes a merged set of all the elements. 
 
Figure 3: Question-answering process in TIMS 
Database 
A
  
A
  
A
  
A
  
A
  
A
  
XML / HTML 
Knowledge 
Sources 
Tag Data 
Language 
Analyzer 
 
 
Tag Data 
Manager 
TIQL 
Processor
NLP Components 
TIMS  
Query to TIQL
Translator 
Query 
ATRACT 
Ontology 
Data 
A
  
A
  
A
  
A
  
A
  
A
  
 
LiLFeS 
Knowledge Acquisition 
Knowledge Integration 
 ? 
 
26 15 VERB 
? 
 
110 100 ADJ 
? 150 140 DNA 
? ? ? ?.. 
? 209 203 RNA 
? 10 5 NOUN
. . . end start Tag 
? 
 
35 15 VP 
? 
 
100 35 PP 
? 
 
150 100 VP 
? ? ? ?.. 
? 209 203 NP 
? 
 
10 5 NP 
. . . end start Tag 
Figure 4: Tag data management 
Part-of-speech tags 
? 
 
80 40 PROTEIN 
? 
 
180 160 DNA 
? 
 
220 200 DNA 
? ? ? ?.. 
? 
 
260 240 RNA 
? 
 
18 5 DNA 
. . . end start Tag 
Semantic tags 
Syntactic tags 
 
X = {                                                           }
Y = {                                                            }
 
X?Y  = {                                                              } 
?{X?Y}={                                                        
} 
 
(X?Y) ?{X?Y}={                                             
                                                                                               }
Figure 5. (X?Y) ?{X ?Y} 
pattern/string matching using tag information, 
but also the ontological hierarchy matching to 
subordinate classes using either predefined or 
automatically derived term ontology. Thus, 
semantically-based tag information retrieval can 
be achieved. For example, the interval operation2 
<VP>?<nucleic_acid*> will retrieve all 
subordinate terms/classes of nucleic acid, which 
are contained within a VP. 
   The interval operations can be performed over 
the specified documents and/or tag sets (e.g. 
syntactic, semantic tags, etc.) simultaneously or 
in batch mode, by selecting the documents/tag 
sets from a list. This accelerates the process of 
KA, as users are able to retrieve information from 
multiple KSs simultaneously. 
2.3 TIQL - Tag Information Query Language  
In order to integrate and expand the above 
components, we have developed a tag 
information query language (TIQL). Using this 
language, a user can specify the interval 
operations to be performed on selected 
documents (including the ontology inference to 
expand queries). The basic expression in TIQL 
has the following form: 
 
SELECT [n-tuple variables]  
FROM [XML document(s)] 
WHERE [interval operation] 
      FROM [XML document(s)] 
WHERE [interval operation] 
                     ?? 
where, [n-tuple variables] specifies the 
table output format, [XML document(s)] 
denotes the document(s) to be processed, and 
[interval operation] denotes an interval 
operation to be performed over the corresponding 
document with variables of each interval to be 
bound. 
For example, the following expression: 
 
SELECT   x1, x2  
 FROM   ?paper-1.xml? 
    WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
  FROM   ?paper-2.xml? 
 WHERE  
<VP>?{x1:<EVENT*>?x2:<nucleic_acid*>} 
 
                                                
2 ?*? denotes hierarchical matching. 
extracts all the hierarchically subordinate classes 
matched to (<EVENT>, <nucleic_acid>) pair 
within a VP from the specified XML-documents,  
and then automatically builds a table to display 
the results (see figure 6).  
   Since formulating an appropriate TIQL 
expression using interval operations might be 
cumbersome, in particular for novice users, 
TIMS was augmented with a capability of 
?recycling? predefined queries and macros. 
3 Evaluation and discussion 
We have conducted preliminary experiments 
using the proposed framework. In this paper we 
briefly present the quality of automatic term 
recognition and similarity measure calculation 
via automatically clustered terms. After that, we 
discuss the practical performance of tag 
manipulation in TIMS compared to string-based 
XML tag manipulation to show the advantage of 
the tag information management scheme.  
   The term recognition evaluation was performed 
on the NACSIS AI-domain corpus (Koyama et 
al., 1998), which includes 1800 abstracts and on a 
set of MEDLINE abstracts. Table 1 shows a 
sample of extracted terms and term variants. The 
ATR precisions of the top 100 intervals range 
from 93% to 98% (see figure 7; for detailed 
evaluation, see Mima et al (2001b) and Nenadic 
et al (2002)).  
 
 Title 
Background 
                                      
                  
........<DNA>androgen 
receptor gene</DNA>   
............... 
                       
         
                      
paper-2.xml 
Title 
Background 
                                      
                  
........<RNA>HB-EGF 
mRNA</RNA>   
............... 
                       
         
                            
paper-1.xml 
nucleic_acid 
 nucleic acid EVENT 
EVENT 
androgen receptor 
gene acid HB-EGF mRNA 
... 
activate 
bind 
... 
Figure 6. Ontology-based Tagged 
Information Retrieval 
 
   terms (and term variants) term-hood 
retinoic acid receptor                                              
     retinoic acid receptor 
     retinoic acid receptors 
     RAR, RARs 
6.33 
nuclear receptor  
     nuclear receptor 
     nuclear receptors 
     NR, NRs 
6.00 
all-trans retionic acid 
     all trans retionic acid 
     all-trans-retinoic acids 
     ATRA, at-RA, atRA 
4.75 
9-cis-retinoic acid 
     9-cis retinoic acid 
     9cRA, 9-c-RA 
4.25 
 
Table 1: Sample of recognised terms  
85
90
95
100
2.65-3.99 4.00-5.99 6.00-Top
C-value
pr
ec
is
io
n
 
Figure 7: ATR interval precision 
 
   For term clustering and tag manipulation 
performance we used the GENIA resources 
(GENIA corpus, 2002), which include 1,000 
MEDLINE abstracts (MEDLINE, 2002), with 
overall 40,000 (16,000 distinct) semantic tags 
annotated for terms in the domain of nuclear 
receptors. We used the similarity measure 
calculation as the central computing mechanism 
for inferring the relevance between the XML tags 
and tags specified in the TIQL/interval operation,  
determining the most relevant tags in the 
XML-based KS(s). As a gold standard, we used 
similarities between the terms that were 
calculated according to the hierarchy of the 
clustered terms according to the GENIA 
ontology. In this experiment, we have adopted a 
semantic similarity calculation method for 
measuring the similarity between terms described 
in (Oi et al, 1997). The three major sets of 
classes (namely, nucleic_acid, amino_acid, 
SOURCE) of manually classified terms from 
GENIA ontology (GENIA corpus, 2002) were 
used to calculate the average similarities (AS) of 
the elements. ASs of the elements within the 
same classes were greater than the ASs between 
elements from different classes, which proves 
that the terms were clustered reliably according 
to their semantic features. 
   In order to examine the tag manipulation 
performance of TIMS, we measured the 
processing times consumed for executing an 
interval operation in TIMS compared to the time 
needed by using string-based regular expression 
matching (REM). We focused on measuring the 
interval operation ??? with intervals (tags) 
<title> and <term> (i.e. extracting all terms 
within titles).   In the evaluation process, we used 
5 different samples to examine IE performances 
according to their size (namely the number of 
tags and file size in Kb).  
 
 Sample1 Sample2 Sample3 Sample4 Sample5 
TIMS 
(millisec.) 16 28 40 44 62 
REM 
(millisec.) 24 38 58 80 104 
# of tags 1146 2383 3730 4799 5876 
Size  
(K bytes) 92 191 298 382 470 
 
Table 2: TIMS - practical performance 
 
Table 2 and Figure 8 show the results: the 
processing times of TIMS were about 1.4-1.8 
times faster (depending on number of tags and 
corpus length) than those of REM. Therefore, we 
assume that the TIMS tag information 
management scheme can be considered as an 
efficient mechanism to facilitate knowledge 
acquisition and information extraction process. 
0
20
40
60
80
100
120
0 2000 4000 6000
# of tags
tim
e 
(m
ill
i s
ec
.)
TIMS
REM
Figure 8. IE performance (TIMS vs. REM) 
Conclusion 
In this paper, we presented a methodology for 
KA and KI over large KSs. We described TIMS, 
an XML-based integrated KA aid system, in 
which we have integrated automatic term 
recognition, term clustering, tagged data 
management and ontology-based knowledge  
retrieval. TIMS allows users to search and 
combine information from various sources. An 
important source of information in the system is 
derived from terminological knowledge, which is 
provided automatically in the XML format. 
Tag-based retrieval is implemented through 
interval operations, which ? in combination with 
hierarchical matching ? prove to be powerful 
means for textual mining and knowledge 
acquisition. 
   The system has been tested in the domain of 
molecular biology. The preliminary experiments 
show that the TIMS tag information management 
scheme is an efficient methodology to facilitate 
KA and IE in specialised fields. 
   Important areas of future research will involve 
expanding the scalability of the system to real 
WWW knowledge acquisition tasks and 
experiments with fine-grained term 
classification. 
References  
Baker P. G., Brass A., Bechhofer S., Goble C., Paton 
N. and Stevens R. (1998) TAMBIS: Transparent 
Access to Multiple Bioinformatics Information 
Sources. An Overview in Proc. of the Sixth 
International Conference on Intelligent Systems for 
Molecular Biology, ISMB98, Montreal. 
Berners-Lee, T. (1998) The Semantic Web as a 
longuage of logic, available at: http://www.w3.org/ 
DesignIssues/Logic.html 
Brickle, D. and Guha R. (2000) Resource Description 
Framework (RDF) Schema Specification 1.0, W3C 
Candidate Recommendation, available at 
http://www.w3.org/TR/rdf-schema 
Frantzi K. T., Ananiadou S. and Mima H. (2000) 
Automatic Recognition of Multi-Word Terms: the 
C-value/NC-value method, in International Journal 
on Digital Libraries, Vol. 3, No. 2, 115?130. 
Gene Ontology Consortium (2002) GO ontology. 
available at  http:// www.geneontology.org/ 
GENIA corpus (2002) GENIA project home page. 
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 
Grishman R (1995) TIPSTER Phase II Architecture 
Design Document. New York University, available 
at http://www.tipster.org/arch.htm 
Jacquemin C. and Tzoukermann E. (1999) NLP for 
Term Variant Extraction: A Synergy of Morphology, 
Lexicon and Syntax. In T. Strzalkowski (editor), 
Natural Language Information Retrieval, Kluwer, 
Boston, pp. 25-74. 
Koyama T., Yoshioka M. and Kageura K. (1998) The 
Construction of a Lexically Motivated Corpus - The 
Problem with Defining Lexical Unit. In Proceedings 
of LREC 1998, Granada, Spain, pp. 1015?1019. 
MEDLINE (2002) National Library of Medicine, 
http://www.ncbi.nlm.nih.gov/PubMed/ 
Mima H., Ananiadou S. and Nenadic G. (2001a) 
ATRACT Workbench: An Automatic Term 
Recognition and Clustering of Te rms, in Text, 
Speech and Dialogue - TSD2001, Lecture Notes in 
AI 2166, Springer Verlag 
Mima H. and Ananiadou S. (2001b) An Application 
and Evaluation of the C/NC-value Approach for the 
Automatic term Recognition of Multi-Word units in 
Japanese, in International Journal on Terminology, 
Vol. 6(2), pp 175-194. 
Mima H., Ananiadou S. and Tsujii J. (1999) A 
Web-based integrated knowledge mining aid 
system using term-oriented natural language 
processing, in Proceedings of The 5th Natural 
Language Processing Pacific Rim Symposium, 
NLPRS'99, pp. 13?18. 
Miyao Y., Makino T., Torisawa K. and Tsujii J. 
(2000) The LiLFeS abstract machine and its 
evaluation with the LinGO grammar. Journal of 
Natural Language Engineering, Cambridge 
University Press, Vol. 6(1), pp.47-62. 
Nenadic G., Spasic I. and Ananiadou S. (2002) 
Automatic Acronym Acquisition and Term 
Variation Management within Domain Specific 
Texts, in Proc. of LREC 2002, Las Palmas, Spain, 
pp. 2155-2162. 
Oi K., Sumita E. and Iida H. (1997) Document 
Retrieval Method Using Semantic Similarity and 
Word Sense Disambiguation (in Japanese), in 
Journal of Natural Language Processing, Vol.4, 
No.3, pp.51-70. 
Visser P.R.S., Jones D.M., Bench-Capon T.J.M. and 
Shave M.J.R. (1997) An Analysis of Ontology 
Mismatches; Heterogeneity versus Interoperability. 
In AAAI 1997 Spring Symposium on Ontological 
Engineering, Stanford University, California, USA. 
Ushioda A. (1996) Hierarchical Clustering of Words. 
In Proc. of COLING ?96, Copenhagen 
Identifying Terms by their Family and Friends 
Diana Maynard  
Dept. of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St 
Sheffield, $1 4DP, UK 
d. maynard0dcs, shef. ac. uk 
Sophia Anan iadou 
Computer Science, School of Sciences 
University of Saltbrd, Newton Building 
Saltbrd, M5 4WT, U.K. 
s. ananiadou@salf ord. ac. uk 
Abstract 
Multi-word terms are traditionally identified using 
statistical techniques or, more recently, using hybrid 
techniques combining statistics with shallow linguis- 
tic information. Al)proaches to word sense disam- 
biguation and machine translation have taken ad- 
vantage of contextual information in a more mean- 
ingflfl way, but terminology has rarely followed suit. 
We present an approach to term recognition which 
identifies salient parts of the context and measures 
their strength of association to relevant candidate 
terms. The resulting list of ranked terms is shown 
to improve on that produced by traditional method- 
s, in terms of precision and distribution, while the 
information acquired in the process can also be used 
for a variety of other applications, such as disam- 
biguation, lexical tuning and term clustering. 
1 Introduction 
Although statistical approaches to automatic term 
recognition, e.g. (Bourigault, 1992; Daille et al, 
1994; Enguehard and Pantera, 1994; 3usteson and 
Katz, 1995; Lauriston, 1996), have achieved rela- 
tive success over the years, the addition of suitable 
linguistic information has the potential to enhance 
results still further, particularly in the case of small 
corpora or very specialised omains, where statis- 
tical information may not be so accurate. One of 
the main reasons for the current lack of diversity in 
approaches to term recognition lies in the difficul- 
ty of extracting suitable semantic information from 
speeialised corpora, particularly in view of the lack 
of appropriate linguistic resources. The increasing 
development of electronic lexieal resources, coupled 
with new methods for automatically creating and 
fine-tuning them from corpora, has begun to pave 
the way for a more dominant appearance of natural 
language processing techniques in the field of termi- 
nology. 
The TRUCKS approach to term recognition (Ter- 
m Recognition Using Combined Knowledge Sources) 
focuses on identifying relevant contextual informa- 
tion from a variety of sources, in order to enhance 
traditional statistical techniques of term recognition. 
Although contextual information has been previous- 
ly used, e.g. in general language (Grefenstette, 1994) 
mid in the NC-Value method for term recognition 
(Frantzi, 1998; Frantzi and Ananiadou, 1999), only 
shallow syntactic information is used in these cas- 
es. The TRUCKS approach identifies different; el- 
ements of the context which are combined to form 
the Information Weight, a measure of how strong- 
ly related the context is to a candidate term. The 
hffbrmation Weight is then combined with the sta- 
tistical information about a candidate term and its 
context, acquired using the NC-Value method, to 
form the SNC-Value. Section 2 describes the NC- 
Value method. Section 3 discusses the importance 
of contextual information and explains how this is 
acquired. Sections 4 and 5 describe the hffbrmation 
Weight and the SNC-VMue respectively. We finish 
with an evaluation of the method and draw some 
conclusions about the work and its fllture. 
2 The NC-Value method 
The NC-Value method uses a combination of lin- 
guistic and statistical information. Terms are first 
extracted from a corpus using the C-Value method 
(Frantzi and Ananiadou, 1999), a measure based on 
frequency of occurrence and term length. This is 
defined formally as: 
is not nested 
C-Value(a) = Zo.q~l(~l l~('n,) ~b~T~ f(b)) 
a is nested 
where 
a is the candidate string, 
f(a) is its frequency in the corpus, 
eT, is the set of candidate terms that contain a, 
P(Ta) is the number of these candidate terms. 
Two different cases apply: one for terms that are 
found as nested, and one for terms that are not. If a 
candidate string is not found as nested, its termhood 
is calculated from its total frequency and length. If 
it is found as nested, termhood is calculated from its 
total frequency, length, frequency as a nested string, 
530 
and the tmmber of longer candidate terms it; ai)l)ears 
in. 
The NC-Value metho(1 builds oil this by incorl)o- 
rating contextual information in the form of a con- 
text factor for each candidate term. A context word 
can be any noun, adjective or verb apI)earing with- 
in a fixed-size window of tim candidate term. Each 
context word is assigned a weight, based on how fre- 
quently it appears with a ca lldidate term. Ttmse 
weights m'e titan SUllslned for all colltext words rel- 
ative to a candidate term. The Context l"actor is 
combined with the C-Value to form tlm NC-Value: 
NCvaluc(a) = 0.8 * Cvalue(a) + 0.2 * C l,'(a) (1) 
where 
a is tile candidate term, 
Cvahte(a) is the Cvalue fin' tlm candidate term, 
CF(a) is the context factor tbr the candidate 
term. 
3 Contextua l  In fo rmat ion :  a Term's  
Social Life 
Just as a person's social life can provide valuable 
clues al)out their i)ersonality, so we can gather much 
information about the nature of a term by investi- 
gating the coral)any it keeps. We acquire this knowl- 
edge by cxtra{:ting three different ypes of contextual 
information: 
1. syntactic; 
2. terminologic~fl; 
3. semantic. 
3.1 Syntact i c  knowledge  
Syntactic knowledge is based on words in the con- 
text which occur immediately t)efore or afl;er a can- 
didatc term, wtfich we call boundary words. Follow- 
ing "barrier word" al)proaches to term recoglfition 
(Bourigault, 1992; Nelson et al, 1995), where par- 
titular syntactic ategories are used to delimit era> 
didate terms, we develop this idea fllrther by weight- 
ing boundary words according to tlmir category. The 
weight for each category, shown in Table 1, is all{)- 
cate(1 according to its relative likelihood of occur- 
ring with a term as opposed to a non-term. A verb, 
therefore, occurring immediately before or after a 
candidate, term, is statistically a better indicator of 
a term than an adjective is. By "a better indica- 
tor", we mean that a candidate term occurring with 
it is more likely to be valid. Each candidate term is 
assigned a syntactic weight, calculated by summing 
the category weights tbr the context bomsdary words 
occurring with it. 
Category Weight 
Verb 1.2 
Prep 1.1 
Noun 0.9 
Adj 0.7 
Table 1: We.ights for categories of boundary words 
3.2 Termino log ica l  knowledge  
Ternfinological knowledge concerns the terminologi- 
cal sta.tus of context words. A context word whicll 
is also a term (whicll we call a context erm) is like- 
ly to 1)e a better indicator than one wlfich is not. 
The terminological status is determined by applying 
the NC-Value at)proach to the corlms, and consider- 
ing tile top third of the list; of ranked results as valid 
terms. A context erm (CT) weight is then produced 
fin" each candidate term, based on its total frequency 
of occurrence with all relewmt context terms. The 
CT weight is formally described as follows: 
where 
a is the candidate term, 
7', is the set: of context erms of a, 
d is a word from Ta, 
fa(d) is the frequency of d as a context term of a. 
3.3 Semant ic  knowledge  
Semantic knowledge is obtained about context erms 
using the UMLS Metathesaurus and Semantic Net- 
work (NLM, 1997). The former provides a seman- 
tic tag for each term, such as Acquired Abnormality. 
The latte, r provides a hierarchy of semantic type- 
s, from wlfich we compute the similarity between a 
candidate term and the context I;erms it occurs with. 
An example of part of tim network is shown in Figure 
\]. 
Similarity is measured because we believe that a 
context erm which is semantically similar to a can- 
didate term is more likely to be significant han one 
wlfieh is less similar. We use tim method for seman- 
tic distance described in (M~\ynard and Ananiadou, 
1999a), wtfich is based on calculating the vertical 
position and horizontal distance between odes in a 
hierarchy. Two weights are cMculated: 
? positionah measured by the combined istance 
from root to each node 
? commonality: measured by the number of 
shared common ancestors multiplied by the 
munber of words (usuMly two). 
Similarity between the nodes is calculated by divid- 
ing tim commomflity weight by the 1)ositional weight 
to t)roduce a figure between 0 and 1, I being the ease 
531 
1'1'1 
\['rM 
ENTII'? 
\[ 'rAi l  
PIIYSICM, ()IHECr 
/ ,  
/ 
\[TAIII 
OIIGANISM 
ITAIItl rrAtl21 
PI,ANT I"UN(;US 
ITAIIlll 
ALGA 
\['rlq 
EVI,:NT 
\[TA2I 
CONCEI~I'UAI, ~N'I'I'I'Y 
ITAI21 
ANATOMII2AL STIIUCTURI,: 
/ /  
ITAI211 \[TAI221 
EMIIRYONIC ANA'I'OM \[IUA 1, 
STllUC'I'UItE AIINOILMALrI'Y 
Figure 1: Fragment of the Semantic Network 
where tile two nodes are identical, and 0 being the 
case where there is no common ancestor. This is 
formally defined as follows: 
sim(w,. . .w, , )  - com(w,...w,,) (3) 
pOS(~Ul...Wn) 
where 
corn(w1 ...w,~) is the commonality weight of words 
1. . .n  
pos('wl...w,~) is the positional weight of words 
l...n. 
Let us take an example from the UMLS. The sim- 
ilarity between a term t)elonging to the semantic 
category Plant and one belonging to the category 
Fungus would be calculated as follows:- 
? Plant has the semantic ode TA l l l  and Fungus 
has the semantic ode TAl l2.  
? The commonality weight is the number of nodes 
in common, multiplied by the number of terms 
we are considering. TA l l l  and TA l l2  have 4 
nodes in common (T, TA, TA1 and TAl l ) .  So 
the weight will be 4 * 2 = 8. 
? The positional weight is the total height of each 
of the terms (where tile root node has a height of 
1). TA l l l  has a height of 5 (T, TA, TA1, TA l l  
and TAl l1) ,  and TAl12 also has a height of 5 
(T, TA, TA1, TA l l  and TAl l2) .  The weight 
will therefore be 5 + 5 = 10. 
? The similarity weight is tile comlnonality 
weight divided by the positional weight, i.e. 
8/10 = 0.8. 
4 The  In fo rmat ion  Weight  
The three individual weights described above are 
calculated for all relevant context words or context 
terms. The total weights for the context are then 
combined according to the following equation: 
IW(a) = ~ .syria(b) + ~ f,(d) . sim,(d) (4) 
beC. (l~7~ 
where 
a is the candidate term, 
Cais the set of context words of a, 
b is a word from C,,  
f,(b) is tlm frequency of b as a context word of a, 
syn~(b) is the syntactic weight of b as a context 
word of a, 
T. is the set of context terms of a, 
d is a word fl'om T., 
fi,(d) is the frequency of d as a context erm of a, 
sims(d) is the similarity weight of d as a context 
term of a. 
This basically means that the Infornlation Weight 
is composed of the total terminological weight, 511151- 
tiplied by tile total semantic weight, and then added 
to the total syntactic weight of all the context words 
or context erms related to the candidate term. 
5 The  SNC-Va lue  
Tile Information Weight gives a score for each candi- 
date term based on the ilnt)ortance of the contextual 
intbrmation surrounding it. To obtain the final SNC- 
Value ranking, the Information Weight is combined 
with the statistical information obtained using the 
NC-Vahm nmthod, as expressed formally below: 
SlVCV,a.,c(a) = NCVal~u~(a) + IW(a) (5) 
where  
a is the candidate term 
NCValue(a) is the NC-Value of a 
IW is the Inqmrtance Weight of a 
For details of the NC-Value, see (l:5'antzi and Ana- 
niadou, 1999). 
An example of the final result is shown in Table 
2. This corot)ares tile top 20 results from the SNC- 
Value list with the top 20 from the NC-Value list. 
The terms in italics are those which were considered 
as not valid. We shall discuss the results in more de- 
tail in the next section, but we can note here three 
points. Firstly, the weights for the SNC-Value are 
substantially greater than those for the NC-Vahm. 
This, in itself, is not important, since it, is the posi- 
tion in the list, i.e. the relative weight, rather than 
the absolute weight, which is important. Secondly, 
we can see that there are more valid terms in the 
SNC-Value results than in the NC-Value results. It 
532 
Term SNC '\].L'rm NC 
l)owlllall ~S_lllelllbralle 
\]nalignant_melanoma 
hyaline_fibrous_tissue 
planes_of_section 
tral) ecularJneshwork 
keratinous_del)ris 
l)ruch~s_inenll)r &lie 
plane_of_section= 
mclanoma_of_choroid 
lymphocytieAnfiltration 
ciliary_processes 
cellularAibrous_tissue 
squamous_ct)ithelium 
oI)tic_nerve_head 
l)Ul)illary_border 
(:orlmal_el)ithelium 
seleraldnw~sion 
granulation_tissue 
stratified_squamous_epithelium 
ocular~structures 
605782 
231237 
215843 
170016 
157353 
101644 
94996.2 
90109.4 
71.615.1 
53822 
52355.7 
51486.8 
46928.9 
39054.5 
36510.8 
31.335.9 
31017.4 
28010.1 
27445.5 
26143.6 
pla'ne_@section 
dencelnel;~s_ill(~.llll)r~/iEe 
basal_cell_carcinoma 
stump_of_optic_nerve 
1)asal_cell_l)at)illoma 
planc_of_section= 
rnclano,na_of_ch, oroid 
pla'ncs_@scction 
malignant _melanoma 
optic_nerveAmad 
ciliaryq)rocesses 
1)ruth's_membrane 
keratinous_eyst 
ellipse_of_skin 
wcdgc_of_lid_ma~yin 
scaT"_tT'ack 
conImctive_tissue 
vertical_plane 
carcinoma_of_lid 
excision_biopsy 
1752.71 
1.345.76 
1.268.21 
993.15 
616.614 
506.517 
497.673 
453.716 
448.591 
422.211 
421.204 
413.027 
392.944 
267.636 
211.41.4 
228.217 
167.053 
167.015 
164 
155.257 
Table 2: Top 20 results for the SNC-VaIue and NC-Value 
in hard to make flu:ther judgements based on this 
list alone, 1)ecause we cmmot s~3; wlmther on(; ter- 
\]u is 1)etter than another, if tiE(; two terms are both 
valid. Thirdly, we can nee that more of the top 20 
terms are valid tin' tim SNC-Vahm than for the NC- 
Value: 17 (851X,) as ot)t)osed to 10 (50%). 
6 Eva luat ion  
The SNC-Value method wan initially t(;sted on a eor- 
l)US of 800,000 eye t)athoh)gy reI)ortn , which had 
1)een tagged with the Brill t)art-of-nl)eeeh tagger 
(Brill, 1992). The ca.ndidate terms we,'e first ex- 
tracted using the NC-Value method (lhantzi, 1998), 
and the SNC-Value was then (:alculated. To exvdu- 
ate the results, we examined the p(.'rformanee of the 
similarity weight alone, and the overall 1)erformance 
of the system. 
6.1 Evaluation methods 
The main evaluation i)rocedure was carried out with 
resl)ect o a manual assessment of tim list of terms 
l)y 2 domain exI)erts. There are, however, 1)roblems 
associated with such an evaluation. Firstly, there ix 
no gold standm:d of evaluation, and secondly, man- 
ual evaluation is both fallil)le and sul)jective. To 
avoid this 1)rol)lem, we measure the 1)erformance of
the system ill relative termn rather than in abso- 
lute terms, by measuring the improveln(mt over the 
results of tile NC-Value as eomt)ared with mmmal 
evahlation. Although we could have used the list 
of terms 1)rovided in the UMLS, instead of a manu~ 
ally evahlated list, we found that there was a huge 
discrei)an(:y 1)etween this lint and the lint validated 
by the manual experts (only 20% of the terms they 
judged valid were fOtlEl(1 ill the UMLS). There are 
also further limitations to the UMLS, such as the 
fact that it is only nl)e(:ific to medicine in general, 
1)ut not to eye t)athology, and the fact that it; is or- 
ganised ill nllch a way that only the preferred terms, 
and not lexical variants, m'e actively and (:onnistent- 
ly 1)r(~sent. 
We first evaluate the similarity weight individu- 
ally, since this is the main 1)rinciple on which the 
SNC-\Sflue method relies. We then ewduate the 
SNC-VaIue as a whole t)y comparing it with the NC- 
Value, so I;hat we can ewfluate the impact of tile ad- 
dition of the deel)er forms of linguistic information 
incorl)orated in {:he hnI)ortance Weight. 
6.2 Similarity Weight 
One of the 1)roblems with our method of calculat- 
ing similarity is that it relies on a 1)re-existing lexi- 
(:al resource, which Eneans it is 1)rone to errors and 
omissions. Bearing in mind its innate inadequacies, 
we can nevertheless evaluate the expected theoretical 
performance of tilt measure by concerning ourselves 
only with what is covered by the thesaurus. This 
means that we assume COml)leteness (although we 
know that this in not the case) and evahtate it ac- 
cordingly, ignoring anything which may be inissing. 
The semantic weight ix based on the premise that 
tile more similar a context term is to the candidate 
term it occurs with, the better an indicator that con- 
text term is. So the higher the total semantic weight 
533 
Section Term Non-Term 
top set 76% 24% 
middle set 56% 44% 
bottom set 49% 51% 
Table 3: Semantic weights of terms and non-terms 
for the candidate term, the higher the ranking of the 
term and the better the chance that the candidate 
term is a valid one. To test the performmme of the 
semantic weight, we sorted the terms in descending 
order of their semantic weights and divided the list 
into 3, such that the top third contained the terms 
with the highest semantic weights, and the bottom 
third contained those with the lowest. We then com- 
pared how many valid and non-valid terms (accord- 
ing to the manual evaluation) were contained in each 
section of the list,. 
Tile results, depicted in Table 3, can be interpret- 
ed as follows. In the top third of the list;, 76% were 
terms and 24% were non-terms, whilst in the middle 
third, 56% were terms and 44% were non-terms, and 
so on. This means that most of the valid terms are 
contained in the top third of tile list mid the fewest 
valid terms are contained in the bottom third of the 
list. Also, the proportion of terms to non-terms in 
tile top of tile list is such that there are more terms 
than non-terms, whereas in the bottom of the list; 
there are more non-terms than ternis. This there- 
fore demonstrates two things: 
? more of' the terms with the highest semantic 
weights are valid, and fewer of those with the 
lowest semmitic weights are valid; 
? more valid terms have high semantic weights 
than non-terms, mid more non-terms have lower 
semantic weights than valid terms. 
We also tested the similarity measure to see 
whether adding sosne statistical information would 
improve its results, and regulate any discrepancies 
in tile uniformity of the hierarchy. The method- 
s which intuitively seem most plausible are based 
on information content, e.g.(Resnik, 1995; Smeaton 
and Quigley, 1996). The informatiosl content of a n- 
ode is related to its probability of occurrence in the 
corpus. Tile snore fi'equently it appears, the snore 
likely it is to be important in terms of conveying 
information, and therefore the higher weighting it 
should receive. We performed experiments to cosn- 
pare two such methods with our similarity measure. 
The first considers the probability of the MSCA of 
the two terms (the lowest node which is an ancestor 
of both), whilst the second considers the probability 
of the nodes of the terms being colnpared. However, 
the tindings showed a negligible difference between 
the three methods, so we conchlde that there is no 
SNC-Value NC-Vahm 
Section Valid Precision Valid Precision 
1 163 64% 160 62% 
2 84 aa% 98 38% 
3 89 35% 69 27% 
4 89 35% 78 30% 
5 76 30% 87 34% 
6 57 22% 78 30% 
7 66 26% 92 36% 
8 75 29% 100 39% 
9 70 27% 42 16% 
10 59 23% 68 27% 
Table 4: Precision of SNC-Vahle and NC-Value 
advantage to be gained by adding statistical int'or- 
mation, fbr this particular corpus. It; is possible that 
with a larger corlms or different hierarchy, this might 
slot be the case. 
6.3 Overall Evaluat ion of the SNC-Value 
We first; compare the precision rates for the SNC- 
Value and the NC-Value (Table 4), by dividing tile 
ranked lists into 10 equal sections. Each section con- 
tains 250 terms, marked as valid or invalid by the 
manual experts. In the top section, the precision is 
higher for the SNC-Value, and in the bottom section, 
it is lower. This indicates that the precision span is 
greater fl~r the SNC-Value, and therefore that the 
ranking is improved. The distribution of valid terms 
is also better for the SNC-Value, since of the valid 
terms, more appear at the top of the list than at the 
bottom. 
Looking at Figure 2, we can see that the SNC- 
Value graph is smoother than that of the NC-Vahle. 
We can compare the graphs niore accurately using 
a method we call comparative upward trend. Be- 
cruise there is no one ideal graph, we instead mea- 
sure how much each graph deviates from a mono- 
tonic line downwards. This is calculated by dividing 
the total rise in precision percentage by the length 
of the graph. A graph with a lower upward trend 
will therefore be better than a graph with a higher 
upward trend. If we compare the upward trends of 
the two graphs, we find that the trend for the SNC- 
Value is 0.9, whereas the trend for the NC-Value is 
2.7. This again shows that the SNC-Value rmiking 
is better thmi the NC-Value ranking, since it is more 
consistent. 
Table 5 shows a more precise investigation of the 
top portion of the list, (where it is to be expected 
that ternis are most likely to be wflid, and which 
is therefore the inost imi)ortant part of the list) We 
see that the precision is most iml)roved here, both 
in terms of accuracy and in terms of distribution 
of weights. At the I)ottom of the top section, the 
534 
9O 
U{} 
71} 
60 
PlccJshm 50 
,111 
30 
211 
10 
SN{" Vah,c 
. . . .  NC-Vah,c 
\ 
\ 
T ~  T T I 
I 3 4 ~ 6 7 8 9 10 
Scct iono l l i s t  
Figure 2: Precision of SNC-Value and NC-Vatue 
SNC-\Sflue 
Section Valid I Precision 
1 21 184% 
2 19 176% 
3 ~" '68% i i 
4: 16 164% 
5 1.8 172% 
6 12 148% 
7 13 152% 
8 : 7 : 68{/{) 
9 \] 3 I 52% 
10 \] 4 i 56% 
\] N C-Value 
Valid Precision 
z 
19 76% 
23 92% 
21 84% 
13 52% 
13 52% 
19 76% 
18 72% 
14 56% 
10 40% 
8 32% 
Table 5: Precision of SNC-\Sdue and NC-Vahm for 
top 250 terms 
precision is much higher for the SNC-Value. This is 
important because ideally, all the terms in this part 
of the list should be valid, 
7 Conc lus ions  
In this paper, we have described a method for multi- 
word term extraction which improves on traditional 
statistical at)proaches by incorporating more specific 
contextual information. It focuses particularly on 
measuring the strength of association (in semantic 
terms) l)etween a candidate term and its context. 
Evahlation shows imi)rovement over the NC-Vahm 
approach, although the percentages are small. This 
is largely l)ecmlse we have used a very small corpus 
for testing. 
The contextuM information acquired can also be 
used for a mmlber of other related tasks, such as 
disambiguation and clustering. At present, the se- 
mantic information is acquired from a 1)re-existing 
domain-slmcitic thesaurus, but there m:c 1)ossibili- 
tics for creating such a thesaurus automatically, or 
entrancing an existing one, using the contextual in- 
formation we acquire (Ushioda, 1996; MaynaM and 
Anmfiadou, 1999b). 
There is much scope tbr filrther extensions of this 
research. Firstly, it; could be extended to other (lo- 
mains and larger corpora, in order to see the true 
benefit of such a.n apl)roach. Secondly, the thesaurus 
could be tailored to the corpus, as we have men- 
tioncd. An incremental approach might be possible, 
whereby the similarity measure is combined with s- 
tatistical intbrmation to tune an existing ontology. 
Also, the UMLS is not designed as a linguistic re- 
source, but as an information resource. Some kind 
of integration of the two types of resource would be 
usefifl so that, for example, lexical variation could 
be more easily handled. 
References  
D. Bourigault. 1992. Surface grammatical analysis 
for tile extraction of terminological noun phras- 
es. In Proc. of l~th International Co~@rcncc 
on Computational Linguistics (COL\[NG), pages 
977-981, Nantes, bYance. 
Eric Brill. 1992. A simple rule-based part of speech 
tagger. In Pwc. of 3rd Confc~vnce of Applied Nat- 
ural Language Processing. 
B. l)aille, E. Gaussicr, and J.M. Lang5. 1994. To- 
wards automatic extraction of monolingual and 
t)ilingual terminology. In Proc. of iSth Interna- 
tional Conference on Computational Linguistics 
(COLIN(;), pages 515-521. 
Chantal Enguehard and Lmu'ent Pantera. 1994. 
Autoumtic natural a(:quisition of a terminology. 
Journal of Quantitative Linguistics, 2(1):27-32. 
K.T. li'r;mtzi and S. Ananiadou. 1.999. The C- 
Value/NC-Vahm domain independent method ~br 
multi-word term extraction. Journal of Natural 
Language PTvccssing, 6(3):1.45 179. 
K.T. Frantzi. 1.998. Automatic Recognition of 
Multi-Word Terms. Ph.D. thesis, Manchester 
Metropolitan University, England. 
G. Grefenstette. 1994. E:rplorations in Automatic 
Thesaurus Discovcry. Kluwer Aca(temic Publish- 
ers .  
J.S. Justcson and S.M. Katz. 1995. Technical ter- 
minology: some linguistic properties and an algo- 
rithm for identification in text. Natural Language 
Engineering, 1:9-27. 
Andy Lauriston. 1996. Automatic term recognition: 
performance of lin9uistic and statistical learning 
techniques. Ph.D. thesis, UMIST, Manchester, 
UK. 
D.G. Maynard and S. Anmfiadou. 1999a. hlentify- 
ing contextual information tbr term extraction. In 
i}Tvc, of 5th International Congress on 7~rminol- 
535 
ogy and Knowlc@c Engineering (TKE '99), pt~ges 
212-221, Innsbruck, Austria. 
D.G. Maynard and S. Anmfiadou. 1999b. A linguis- 
tic ~I)proach to context clustering. In Proc. of Nat- 
n~nl Language Proecssinfl Pacific \]~im Symposium 
(NLPRS), pages 346-351, Beijing, China. 
S.J. Nelson, N.E. Olson, L. Fuller, M.S. Turtle, W.G. 
Cole, and D.D. Sherertz. 1995. Identifying con- 
cepts in medical knowledge. In Proc. of 8th World 
Congress on Medical Informatics (MEDINFO), 
1)~ges 33-36. 
NLM, 1997. UMLS K?wwlcdgc Sourccs. National 
Library of Medicine, U.S. Dept. of Health and Hu- 
man Services, 8th edition, January. 
P. Resnik. 1995. Disambiguating noun groupings 
with respect o WordNet senses. In Proc. of 3rd 
Workshop on Very Large Corpora. MIT. 
A. Smeaton and I. Quigley. 1996. Experiments on 
using semantic distances between words in image 
caption retrieval. In Proc. of 19t.h htternationaI 
Conferc'ncc on Research and Development i~. I'n- 
formation Retrieval, Zurich, Switzerland. 
Akira Ushioda. 1996. IIierarchical clustering of 
words. In Proc. of 16th I'ntcrnational ConfcT~cncc 
on Computational Linguistics (COLING), pages 
1159 1162. 
536 
Enhancing automatic term recognition through recognition of variation  
Goran Nenadi?*  
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
G.Nenadic@umist.ac.uk 
Sophia Ananiadou* 
Computer Science 
University of Salford 
Salford, UK, M5 4WT  
S.Ananiadou@salford.ac.uk 
John McNaught* 
Department of Computation 
UMIST 
Manchester, UK, M60 1QD 
J.McNaught@umist.ac.uk 
 
                                                     
* Co-affiliation: National Centre for Text Mining, Manchester, UK 
Abstract 
Terminological variation is an integral part of the 
linguistic ability to realise a concept in many ways, 
but it is typically considered an obstacle to 
automatic term recognition (ATR) and term 
management. We present a method that integrates 
term variation in a hybrid ATR approach, in which 
term candidates are recognised by a set of 
linguistic filters and termhood assignment is based 
on joint frequency of occurrence of all term 
variants. We evaluate the effectiveness of 
incorporating specific types of term variation by 
comparing it to the performance of a baseline 
method that treats term variants as separate terms. 
We show that ATR precision is enhanced by 
considering joint termhoods of all term variants, 
while recall benefits by the introduction of new 
candidates through consideration of different 
variation types. On a biomedical test corpus we 
show that precision can be increased by 20?70% 
for the top ranked terms, while recall improves 
generally by 2?25%. 
1 Introduction 
Terminological processing has long been 
recognised as one of the crucial aspects of 
systematic knowledge acquisition and of many 
NLP applications (IR, IE, corpus querying, etc.). 
However, term variation has been under-discussed 
and is rarely accounted for in such applications.  
When naming a new concept, scientists and 
specialists usually follow some predefined term 
formation patterns, a process which does not 
exclude the usage of term variations or alternative 
names for concepts. Term variations are very 
frequent: approximately one third of term 
occurrences are variants (Jacquemin, 2001). They 
occur not only in text, but also in controlled, 
manually curated terminological resources (e.g. 
UMLS (NLM, 2004)).  
The task of an automatic term recognition (ATR) 
system is not only to suggest the most likely 
candidate terms from text, but also to correlate 
them with synonymous term variants. In this paper, 
we briefly present an analysis of term variation 
phenomena, whose results are subsequently 
incorporated into a corpus-based ATR method in 
order to enhance its performance.  
The paper is organised as follows. In Section 2, 
we analyse the main types of term variation, and 
briefly examine how existing ATR systems treat 
them. Our approach to incorporating variants into 
ATR is presented in Section 3. In Section 4, we 
evaluate our approach by comparing it to a 
baseline method (the method without variation re-
cognition), and we conclude the paper in Section 5. 
2 Background 
Terms are linguistic units that are assigned to 
concepts and used by domain specialists to 
describe and refer to specific concepts in a domain. 
In this sense, terms are preferred designators of 
concepts. In text, however, concepts are frequently 
denoted by different surface realisations of 
preferred terms, which we denote as their term 
variants. Consequently, a concept can be 
linguistically represented using any of the surface 
forms that are variants of the corresponding 
preferred term. We consider the following types of 
term variation: 
 
(i) orthographic: e.g. usage of hyphens and slashes 
(amino acid and amino-acid), lower and upper 
cases (NF-KB and NF-kb), spelling variations 
(tumour and tumor), different Latin/Greek 
transcriptions (oestrogen and estrogen), etc. 
(ii) morphological: the simplest variations are 
related to inflectional phenomena (e.g. singular, 
plural). Derivational transformations can lead to 
variants in some cases (cellular gene and cell 
gene), but not always (activated factor vs. 
activating factor); 
(iii) lexical: genuine lexical synonyms, which may 
be interchangeably used (carcinoma and 
cancer, haemorrhage and blood loss); 
(iv) structural: e.g. possessive usage of nouns 
using prepositions (clones of human and human 
clones), prepositional variants (cell in blood, 
cell from blood), term coordinations (adrenal 
glands and gonads); 
(v) acronyms and abbreviations: very frequent 
term variation phenomena in technical 
sublanguages, especially in biomedicine; 
sometimes they may be even preferred terms 
(DNA for deoxyribonucleic acid).  
 
Note that variation types (i) ? (iii) affect 
individual constituents, while (iv) and (v) involve 
variation in structure of the preferred term. In any 
case, they do not ?change? the meaning as they 
refer to the same concept. Daille et al (1996) and 
Jacquemin (1999, 2001) further identified types of 
variation that modified the meaning of terms.  
Although many authors mention the problems 
related to term variation, few have dealt with 
linking the corresponding term variants. Also, the 
recognition of variants is typically performed as a 
separate operation, and not as part of ATR.  
The simplest technique to handle some types of 
term variation (e.g. morphological) is based on 
stemming: if two term forms share a stemmed 
representation, they are considered as mutual 
variants (Jacquemin and Tzoukermann, 1999; 
Ananiadou et al, 2000). However, stemming may 
result in ambiguous denotations related to ?over-
stemming? (i.e. resulting in the conflation of terms 
which are not real variants) and ?under-stemming? 
(i.e. resulting in the failure to link real term 
variants).  
Other approaches to the recognition of term 
variants use preferred terms and known synonyms 
from existing term dictionaries and approximate 
string matching techniques to link or generate 
different term variants (Krauthammer et al, 2001; 
Tsuruoka and Tsujii, 2003).  
Jacquemin (2001) presents a rule-based system, 
FASTR, which supports several hundred meta-
rules dealing with morphological, syntactic (i.e. 
structural) and semantic term variation. Term 
variation recognition is based on the 
transformation of basic term structures into variant 
structures. However, the variants recognised by 
FASTR are more conceptual variants than 
terminological ones, as non-terminological units 
(such as verb phrases, extended insertions, etc.) are 
also linked to terms in order to improve indexing 
and retrieval.   
 
3 Incorporating term variation into ATR 
Our approach to ATR combines the C-value 
method (Frantzi et al, 2000) with the recognition 
of term variation, which is incorporated as an 
integral part of the term extraction process.  
C-value is a hybrid approach combining term 
formation patterns with corpus-based statistical 
measures. Term formation patterns act as linguistic 
filters to a POS tagged corpus: filtered sequences 
are considered as potential realisations of domain 
concepts (term candidates). They are subsequently 
assigned termhoods (i.e. likelihood to represent 
terms) according to a statistical measure. The 
measure amalgamates four corpus-based 
characteristics of a term candidate, namely its 
frequency of occurrence, its frequency of 
occurrence as a form nested within other candidate 
terms, the number of candidate terms inside which 
it is nested, and the number of words it contains.   
The original C-value method treats term variants 
that correspond to the same concept as separate 
term candidates. Consequently, by providing 
separate frequencies of occurrence for individual 
variants instead of a single frequency of 
occurrence calculated for a term candidate unifying 
all variants, the corpus-based measures and 
termhoods are distributed across different variants. 
Therefore, we aim at enhancing the statistical 
evaluation of termhoods through conflation of 
different surface representations of a given term, 
and through joint frequencies of occurrence of all 
equivalent surface forms that correspond to a 
single concept.  
In order to conflate equivalent surface 
expressions, we carry out linguistic normalisation 
of individual term candidates (see examples in 
Table 1). Firstly, each term candidate is mapped to 
a specific canonical representative (CR) by 
semantically isomorphic transformations. Then, we 
establish an equivalence relation, where two term 
candidates are related iff they share the same CR. 
The partitions of this relation are denoted as 
synterms: a synterm contains surface term 
representations sharing the same CR.  
 
synterm canonical representative 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma } human cancer 
Table 1: Term normalisation examples 
 
Our aim is to form synterms prior to the syntactic 
estimation of termhoods for term candidates. 
Therefore, after the extraction of individual term 
candidates, we subsequently normalise them in 
order to generate synterms, where the 
normalisation is performed according to the 
typology of variations described in Section 2. More 
precisely, we consider separately the normalisation 
of variations that affect term candidate constituents 
and variations that involve structural changes. The 
general architecture of our ATR approach is 
presented in Figure 1. 
 
 
P O S  tagger 
 
In flec tiona l n orm alisa tio n  
S tructura l  n orm alisation  
O rthographic  no rm alisa tion  
E x trac ted  syn term s 
Inp u t d ocu m ents 
T erm h ood  es tim ation  
E xtrac tio n  of term  cand id ates  
A cron ym  acq uis ition  
 
Figure 1: The architecture of the ATR process 
 
3.1 Normalising term constituent variation 
In the case of variations that do not affect the 
structure of terms, the formation of CRs is based 
on a POS tagger (for inflectional variation) and 
simple heuristics (for orthographic normalisation). 
For example, different transcriptions of 
neoclassical combining forms are treated by 
replacements of specific character combinations 
(ae ? e, ph ? f) in such forms (and only in such 
forms). Inflectional normalisation is based on POS 
tagging: a canonical term candidate form is a 
singular form containing no possessives (Down?s 
syndrome ? down syndrome). 
In order to address lexical variants, one can use 
dictionaries of synonyms where the preferred terms 
are used for normalisation purposes ({hepatic 
microsomes, liver microsomes} ? liver 
microsomes). In experiments reported here, we did 
not attempt to normalise lexical variation. 
 
3.2 Normalising term structure variation 
Variations affecting term structure are less frequent 
but more complex. Here we consider two types of 
term variation: prepositional term candidates and 
coordinated term candidates (for a detailed analysis 
of these variations see (Nenadic et al, 2004)). 
Prepositional term candidates are normalised by 
transformation into corresponding expressions 
without prepositions. Using prepositions of, in, for 
and by as anchors, we generate semantically 
isomorphic CRs by inversion. For example, the 
candidate nuclear factor of activated T cell is 
transformed into activated T cell nuclear factor. 
Here is a simplified example of a rule describing 
the transformation of a term candidate that 
contains the preposition of: 
 
if  structure of  term candidate is   
   (A|N)1* N1 Prep(of) (A|N)2* N2  
then   CR = (A|N)2* N2 (A|N)1* N1 
 
In order to address the problems of determining 
the boundaries of term constituents in text (to the 
right and left of prepositions), for each 
prepositional term candidate we generate all 
possible nested candidates? and their corresponding 
CRs. For example, for the candidate regulation of 
gene expression, we generate both gene regulation 
and gene expression regulation. Since this 
approach also generates a number of false 
candidates, additional heuristics are used to 
enhance precision, such as removing adverbials 
and determiners, using a stop list of 
terminologically irrelevant prepositional 
expressions (e.g. number of ..., list of ..., case of ..., 
in presence of ...), etc.  
A similar approach is used for the recognition of 
coordinated term candidates: coordinating 
conjunctions (and, or, but not, as well as, etc.) are 
used as anchors, and when a coordinating structure 
is recognised in text, the corresponding CRs of the 
candidate terms involved are generated.  
We differentiate between head coordination 
(where term heads are coordinated, e.g. adrenal 
glands and gonads) and argument coordination 
(where term arguments/modifiers are coordinated, 
e.g. SMRT and Trip-1 mRNAs). 
The recognition and extraction of coordinated 
terms is highly ambiguous even for human 
specialists, since coordinated terms and term 
conjunctions share the same structures (see Table 
2). Also, similar patterns cover both argument and 
head coordinations, which makes it difficult to 
extract coordinated constituents (i.e. terms). Not 
only is the recognition of term coordinations and 
their subtypes ambiguous, but also internal 
boundaries of coordinated terms are blurred. In a 
separate study, we have shown that 
morphosyntactic features are insufficient both for 
the successful recognition of coordinations and for 
the extraction of coordinated terms: in many cases, 
the correct interpretation and decoding of term 
coordinations is only possible with sufficient 
background knowledge (Nenadic et al, 2004). 
                                                     
? Each constituent extracted from a nested pre-
positional term candidate has to follow a pattern used 
for the extraction of individual candidate terms. 
 
example adrenal  glands and gonads 
head 
coordination [adrenal [glands and gonads]] 
term  
conjunction  [adrenal glands] and [gonads] 
Table 2: Ambiguities within coordinated structures 
In order to address the problems of structural 
ambiguities and boundaries of coordinated terms, 
we also generate all possible nested coordination 
expressions and corresponding term candidates. 
For example, from a candidate coordination viral 
gene expression and replication we generate two 
pairs of coordinated term candidates: 
 
viral gene expression  and  viral gene replication 
viral gene expression  and  viral replication 
 
Patterns for the extraction of term candidates 
from coordinations have been acquired semi-
manually for a subset of term coordinations. For 
each pattern, we define a procedure for the 
extraction of coordinated term candidates and 
generation of the corresponding CRs (see Table 3 
for examples). The generated candidates from 
coordinated structures are subsequently treated as 
individual term candidates. 
 
3.3 Normalising acronym variation  
We treat acronym extraction as part of the ATR 
process (see Figure 1). In (Nenadic et al, 2002) we 
suggested a simple procedure for acquiring 
acronyms and their expanded forms (EFs), which 
was mainly based on using orthographic and 
syntactic features of contexts where acronyms 
were introduced. The model is based on three types 
of patterns: acronym patterns (defining common 
internal acronym structures and forms), definition 
patterns (based on syntactic patterns which 
describe typical contexts where acronyms are 
introduced in text) and matching patterns (the set 
of matching rules between acronyms and their 
corresponding EFs).  
Acronyms also exhibit variation (e.g. RAR alpha, 
RAR-alpha, RARA, RARa, RA receptor alpha etc. 
are all acronyms for retinoic acid receptor alpha). 
Therefore, in addition to extracting acronyms, we 
further gather all acronym variants and their EFs, 
and we map them into a single CR. Since in this 
paper acronyms are taken as term variants, we  
?replace? acronym occurrences by the CR of their 
EFs. In order to bypass the problem of acronym 
ambiguity, we replace/normalise only acronyms 
that are introduced in a given document. 
 
(N|A)1 & (N|A)2 (N+)3 
candidate1 = (N|A)2 (N+)3 
candidate2 = (N|A)1 nested(N+3 ) 
 
e.g.   B and T cell antigen    
          T cell antigen    
          B cell antigen, B antigen 
N1 & N2 A3 N+4 
candidate1 = N2 A3 N+4 
candidate2 = N1 A3 N+4 
 
e.g.  function or surface antigenic profile 
     surface antigenic profile   
     function antigenic profile 
N+1 N2 & (N|A)3 
candidate1 = N+1 N2  
candidate2 = nested(N+1) (N|A)3  
 
e.g.  breast cancer therapy and prevention 
     breast cancer therapy  
     breast caner prevention, breast  prevention 
N+1 (A+)2  A3 &  A4 
candidate1 = N+1 (A+)2 A3  
candidate2 = N+1 (A+)2 A4  
 
e.g.  RNA polymerases II and III 
      RNA polymerasis II 
      RNA polymerasis III 
Table 3: Examples of patterns used for the 
extraction of term candidates from coordinations  
(nested denotes the generation of all possible 
linearly nested substrings)  
3.4 Calculating termhoods with variants 
Term variants sharing the same CR are grouped 
together into synterms, and the calculation of C-
values (i.e. termhoods) is performed for the whole 
synterm rather than for individual term candidates. 
The main reason for doing this is to avoid the 
distribution of frequencies of occurrence of term 
candidates across different variants, as these 
frequencies have a significant impact on estimating 
termhoods. Instead of providing separate 
frequencies of occurrence and obtaining termhoods 
for individual term candidates, we provide a single 
frequency of occurrence and joint termhood 
calculated for a synterm, which unifies all variants. 
Similarly to the estimation of C-values for 
individual term candidates (Frantzi et al, 2000), 
the formula for calculating the termhoods for 
synterms is as follows: 
 
 
??
??
?
?
??
=
?
?
nestednot  is CR),(||log
nested is CR,))(||
1)((||log  )value(-C
2
2
CRfCR
bfTCRfCRc CRTbCR
 
where c denotes a synterm whose elements share a 
canonical representative (denoted as CR in the 
formula), f(CR) corresponds to the cumulative 
frequency with which all term candidates from the 
synterm c occur in a given corpus, |CR| denotes 
the average length of the term candidates (the 
number of constituents), and TCR is a set of all 
synterms whose CRs contain the given CR as a 
nested substring. 
This approach ensures that all term variants are 
naturally dealt with jointly, thus supporting the fact 
that they denote the same concept. As a 
consequence, we expect that precision would be 
enhanced by considering joint frequencies of 
occurrence and termhoods for all variants of 
candidate terms, while recall would benefit by the 
introduction of new candidates through 
consideration of different variation types. 
 
4 Evaluation and discussion 
In order to assess the effectiveness of incorporating 
specific types of term variation into ATR, we 
compared the performance of the baseline C-value 
method (without considering variations) with the 
approach including recognition and conflation of 
term variants. Here we are not interested in an 
absolute measure of the ATR performance, but 
rather in the comparison of results obtained 
through handling different variation types.  
We conducted two sets of experiments: in the 
first experiment, we analysed the incorporation of 
term candidates resulting from considering term 
variations individually, while, in the second, we 
experimented with the integration of combined 
variations in the ATR process. 
The evaluation was carried out using the GENIA 
corpus (GENIA, 2004), which contains 2,000 
abstracts in the biomedical domain with 76,592 
manually marked occurrences of terms. These 
occurrences (which include different term variants) 
correspond to 29,781 different, unique terms. Each 
occurrence of a term in the corpus (except 
occurrences of acronyms) is linked to the 
corresponding ?normalised? term (typically a 
singular form), while coordinated terms are 
identified, marked and normalised within term 
coordinations. A third of occurrences of GENIA 
terms are affected by inflectional variations, and 
almost half of GENIA terms have inflectional 
variants appearing in the corpus. On the other 
hand, only 0.5% of terms contain a preposition, 
while 2% of all term occurrences are coordinated, 
involving 9% of distinct GENIA terms (for a 
detailed analysis of GENIA terms see (Nenadic et 
al., 2004)). 
We used the list of GENIA terms as a gold 
standard for the evaluation. Since our ATR method 
produces a ranked list of suggested synterms, we 
considered precision at fixed rank cut-offs 
(intervals): precision was calculated as the ratio 
between the number of correctly recognised terms 
and the total number of entities recognised in a 
given interval (where an interval included all terms 
from the top ranked synterms).? The baseline 
method (original C-value) was treated in the same 
way, as term candidates suggested by the original 
C-value could be seen as singleton synterms. In 
order to estimate the influence on recall, we also 
used all variants from suggested synterms.  
The incorporation of individual variations 
affecting term constituents into ATR had 
considerable positive effects, especially on the 
most frequently occurring terms (see Figures 2a 
and 2b): for some intervals, inflectional variants, 
for example, improved precision by almost 50%. 
Similarly, the integration of acronyms improved 
precision, in particular for frequent terms (up to 
70%), as acronyms are typically introduced for 
such terms. As one would expect, the combined 
constituent-level variations further improved 
interval precisions compared both to the baseline 
method and individual variations (see Figure 2c). 
However, the incorporation of structural variants 
(in particular for prepositional terms) negatively 
influenced precision compared to the baseline 
method, as many false candidates were introduced.  
In order to assess the quality of extracted 
prepositional term candidates, we evaluated a set 
of the 117 most frequently occurring candidates 
with prepositions: 80% of suggested expressions 
were deemed relevant by domain experts, although 
they were not included in the gold GENIA 
standard (such as expression of genes or binding of 
NF kappa B). Still, the recognition of prepositional 
term candidates is difficult as they are infrequent 
and there are no clear morphosyntactic cues that 
can differentiate between terminologically relevant 
and irrelevant prepositional phrases. 
The incorporation of coordinated term candidates 
had only marginal influence on precision, mainly 
because they were not frequent in the GENIA 
corpus. Furthermore, simple term conjunctions 
                                                     
? It was an open question whether to count the 
recognition of each term form (e.g. singular and plural 
forms, an acronym and its EF, prepositional and non-
prepositional forms) separately (i.e. as two positive 
?hits?) or as one positive ?hit? (see also (Church, 
1995)). Since the evaluation of the baseline method 
(original C-value) typically counts such hits separately, 
we decided to follow this approach, and consequently 
count all positive hits from synterms.  
 
were far more frequent than term coordinations, 
which made their extraction highly ambiguous. 
Still, using only the patterns from Table 3, we have 
correctly extracted 35.76% of all GENIA 
coordinated terms, with more than a half of all 
suggested candidates being found among those that 
appeared exclusively in coordinations. However, 
these patterns also generated a number of false 
coordination expressions, and consequently a 
number of false term candidates. 
The integration of term variants was also useful 
for re-ranking of true positive term candidates: the 
combined rank was typically higher than the 
separate ranks of term variants. Furthermore, some 
terms, not suggested by the baseline method at all, 
were ranked highly when variants were conflated 
(for example, the term T-lymphocyte was 
recognised only as a coordinated term candidate, 
while replication of HIV-1 was extracted only by 
considering prepositional term candidates). In 
order to estimate the overall influence on recall of 
ATR, we used all terms from the respective 
synterms (see Table 4 for the detailed results). In 
general, the incorporation of inflectional variants 
increased recall by ?, while acronyms improved 
recall by almost ? when only the most frequent 
terms were considered. It is interesting that 
acronym acquisition can further improve recall by 
extracting variants that have more complex internal 
structures (such as EFs containing prepositions 
(REA = repressor of estrogen activity) and/or 
coordinations (SMRT = silencing mediator of 
retinoic and thyroid receptor)). Prepositional and 
coordination candidate terms had some influence 
on recall, in particular as they increased the 
likelihood of some candidates to be suggested as 
terms. Low recall of term coordinations may be 
increased by adding more patterns (which would 
probably negatively affect precision).  
Summarising, experiments performed on the 
GENIA corpus have shown that the incorporation 
of term variations into the ATR process resulted in 
significantly better precision and recall. In general, 
acronyms and inflectional unification are the most 
important variation types (at least in the domain of 
biomedicine). Individually, they increased 
precision by 20?70% for the top ranked synterm 
intervals, while recall is generally improved, in 
some cases up to 25%. Other term variations had 
only marginal influence on the performance, 
mainly because they were infrequent in the test 
corpus (compared to the total number of term 
occurrences, and not only with regard to specific 
individual candidates, but also in general). For 
these variations, larger-scale corpora may show 
their stronger influence.  
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2a: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 5) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
prepositions inflectional acronyms
Figure 2b: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of individual term variants 
(terms with frequency > 0) 
0
1
2
50 100 150 250 500 100
0
150
0
300
0
500
0
100
00 all
inflectional infl & acro all
Figure 2c: Comparison of interval ATR precision 
of the baseline method (=1) to ATR precisions with 
integrated recognition of combined term variants 
(terms with frequency > 0) 
 
term sets prep. coord. infl. acro. 
freq. ? 5 +5.30% +12.42% +17.52% +60.49%
freq. > 0 +2.36% +2.53% +25.25% +8.52% 
Table 4: Improvement in recall when variations 
are considered as an integral part of ATR 
 
5 Conclusion 
In this paper we discussed possibilities for the 
extraction and conflation of different types of 
variation of term candidates. We demonstrated that 
the incorporation of treatment of term variation 
enhanced the performance of an ATR system, and 
that tackling term variation phenomena was an 
essential step for ATR. In our case, precision was 
boosted by considering joint frequencies of 
occurrence and termhoods for all candidate terms 
from candidate synterms, while recall benefited 
from the introduction of new candidates through 
consideration of different variation types. Although 
we experimented with a biomedical corpus, our 
techniques are general and can be applied to other 
domains.  
Variations affecting single term candidate 
constituents are the most frequent phenomena, and 
also straightforward for implementation as part of 
an ATR process. The conflation of such term 
candidate variants can be further tuned for a 
specific domain by using lists of combining forms 
and affixes. The incorporation of acronyms had a 
significant high positive effect, in particular on 
more frequent terms (since acronyms are 
introduced for terms that are used more 
frequently). 
However, more complex structural phenomena 
had a moderate positive influence on recall, but, in 
general, the negative effect on precision. The main 
reason for such performances is structural and 
terminological ambiguity of these expressions, in 
addition to their low frequency of occurrence 
(compared to the total number of term 
occurrences). For handling such complex variants, 
a knowledge-intensive and domain-specific 
approach is needed, as coordinated term candidates 
or candidates with prepositions need to be 
additionally semantically analysed in order to 
suggest more reliable term candidates, and to 
introduce fewer false candidates.  
Apart  from being useful for boosting precision 
and recall, the integration of term variation into 
ATR is particularly important for smaller corpora 
(where linking related occurrences is vital for 
successful terminology management) as well as for 
many text-mining tasks (such as IR, IE, term or 
document clustering and classification, etc.). 
Finally, as future work, we plan to investigate 
more knowledge intensive, domain-specific 
treatment of prepositional and coordinated terms, 
as well as pronominal term references. 
6 Acknowledgements 
This research has been partially supported by the 
JISC-funded National Centre for Text Mining 
(NaCTeM), Manchester, UK. 
References  
S. Ananiadou, S. Albert and D. Schuhmann. 2000. 
Evaluation of Automatic Term Recognition of 
Nuclear Receptors from Medline. Genome 
Informatics Series, vol. 11. 
K.W. Church. 1995. One Term or Two? Proc. of 
SIGIR-95, pp. 310-318. 
B. Daille, B. Habert and C. Jacquemin. 1996. 
Empirical Observation of Term Variation and 
Principles for Their Description. Terminology 
3(2), pp. 197?258. 
K. Frantzi, S. Ananiadou and H. Mima. 2000. 
Automatic Recognition of Multi-Word Terms: 
the C/NC value method. International Journal of 
Digital Libraries, vol. 3:2, pp. 115?130. 
C. Jacquemin. 1999. Syntagmatic and paradigmatic 
representations of term variation. Proc. of 37th 
Annual Meeting of ACL, pp. 341?348. 
C. Jacquemin. 2001. Spotting and Discovering 
Terms through NLP. MIT Press, Cambridge MA. 
C. Jacquemin and E. Tzoukermann. 1999. NLP for 
Term Variant Extraction: A Synergy of 
Morphology, Lexicon and Syntax, in T. 
Strzalkowski (ed.), Natural Language 
Information Retrieval, Kluwer, pp. 25-74 
M. Krauthammer, A. Rzhetsky, P. Morozov, and 
C. Friedman. 2001. Using BLAST for 
identifying gene and protein names in journal 
articles. Gene, 259(1?2): pp. 245?52. 
GENIA. 2004. GENIA resources. Available at 
http://www.tsujii.is.u-tokyo.ac.jp/~Genia/ 
G. Nenadic, I. Spasic and S. Ananiadou. 2002. 
Automatic Acronym Acquisition and Term 
Variation Management within Domain-Specific 
Texts. Proc. of LREC 2002, pp. 2155?2162. 
G. Nenadic, I. Spasic and S. Ananiadou. 2004. 
Mining Biomedical Abstracts: What?s in a 
Term? Proc. of IJC-NLP, pp. 247-254.  
NLM. 2004. National Library of Medicine, Unified 
Medical Language System. 
Y. Tsuruoka and J. Tsujii. 2003. Probabilistic 
Term Variant Generator for Biomedical Terms. 
Proc. of 26th Annual ACM SIGIR Conference. 
243
244
245
246
247
248
249
250
Automatic Discovery of Term Similarities Using Pattern Mining
Goran NENADI?, Irena SPASI? and Sophia ANANIADOU
Computer Science, University of Salford
Salford, M5 4WT, UK
{G.Nenadic, I.Spasic, S.Ananiadou}@salford.ac.uk
Abstract
Term recognition and clustering are key topics in automatic knowledge acquisition and text mining. In
this paper we present a novel approach to the automatic discovery of term similarities, which serves as
a basis for both classification and clustering of domain-specific concepts represented by terms. The
method is based on automatic extraction of significant patterns in which terms tend to appear. The
approach is domain independent: it needs no manual description of domain-specific features and it is
based on knowledge-poor processing of specific term features. However, automatically collected
patterns are domain specific and identify significant contexts in which terms are used. Beside features
that represent contextual patterns, we use lexical and functional similarities between terms to define a
combined similarity measure. The approach has been tested and evaluated in the domain of molecular
biology, and preliminary results are presented.
Introduction
In a knowledge intensive discipline such as
molecular biology, the vast and constantly
increasing amount of information demands
innovative techniques to gather and systematically
structure knowledge, usually available only from
text/document resources. In order to discover new
knowledge, one has to identify main concepts,
which are linguistically represented by domain
specific terms (Maynard and Ananiadou (2000)).
There is an increased amount of new terms that
represent newly created concepts. Since existing
term dictionaries usually do not meet the needs of
specialists, automatic term extraction tools are
indispensable for efficient term discovery and
dynamic update of term dictionaries.
   However, automatic term recognition (ATR) is
not the ultimate aim: terms recognised should be
related to existing knowledge and/or to each other.
This entails the fact that terms should be classified
or clustered so that semantically similar terms are
grouped together. Classification and/or clustering
of terms are indispensable for improving
information extraction, knowledge acquisition, and
document categorisation. Classification can also be
used for efficient term management and populating
and updating existing ontologies in a consistent
manner. Both classification and clustering methods
are built on top of a specific similarity measure.
The notion of term similarity has been defined and
considered in different ways: terms can have
functional and/or structural similarities, though
they can be correlated by different relationships
(Grefenstette (1994), Maynard and Ananiadou
(2000)). In this paper we suggest a novel, domain-
independent method for the automatic discovery of
term similarities, which can serve as a basis for
both classification and clustering of terms. The
method is mainly based on the automatic discovery
of significant term features through pattern mining.
Automatically collected patterns are domain
dependent and they identify significant contexts in
which terms tend to appear. In addition, the
measure combines lexical and syntactical
similarities between terms.
   The paper is organised as follows. In Section 1
we overview term management approaches.
Section 2 introduces the term similarity measure
and Section 3 presents results and experiments.
1   Terminology Management
Since vast amount of knowledge still remains
unexplored, several systems have been proposed to
help scientists to acquire relevant knowledge from
scientific literature. For example, GENIES
(Friedman et al (2001)) uses a semantic grammar
and substantial syntactic knowledge in order to
extract comprehensive information about signal-
transduction pathways. Some of the systems are
terminology-based, since technical terms
semantically characterise documents and therefore
represent starting place for knowledge acquisition
tasks. For example, Mima et al (2002) introduce
TIMS, a terminology-based knowledge acquisition
system, which integrates automatic term
recognition, term variation management, context-
based automatic term clustering, ontology-based
inference, and intelligent tag information retrieval.
The system?s aim is to provide efficient access and
integration of heterogeneous biological textual data
and databases.
   There are numerous approaches to ATR. Some
methods (Bourigault (1992), Ananiadou (1994))
rely purely on linguistic information, namely
morpho-syntactic features of term candidates.
Recently, hybrid approaches combining linguistic
and statistical knowledge are becoming
increasingly used (Frantzi et al (2000), Nakagawa
et al (1998)).
   There is a range of clustering and classification
approaches that are based on statistical measures of
word co-occurrences (e.g. Ushioda (1996)), or
syntactic information derived from corpora (e.g.
Grefenstette  (1994)). However, few of them deal
with term clustering: Maynard and Ananiadou
(2000) present a method that uses manually
defined semantic frames for specific classes,
Hatzivassiloglou et al (2001) use machine learning
techniques to disambiguate names of proteins,
genes and RNAs, while Friedman et al (2001)
describe extraction of specific molecular pathways
from journal articles.
   In our previous work, an integrated knowledge
mining system in the domain of molecular biology,
ATRACT, has been developed (Mima et al
(2001)). ATRACT (Automatic Term Recognition
and Clustering for Terms) is a part of the ongoing
BioPath1 project, and its main aim is to facilitate an
efficient expert-computer interaction during term-
based knowledge acquisition. Term management is
based on integration of automatic term recognition
and automatic term clustering (ATC). ATR is
based on the C/NC-value method (Frantzi et al
                                                          
1 BioPath is a Eureka funded project, coordinated by
LION BioScience (http://www.lionbioscience.com) and
funded by the German Ministry of Research.
(2000)), a hybrid approach combining linguistic
knowledge (term formation patterns) and statistical
knowledge (term length, frequency of occurrence,
etc). The extension of the method handles
orthographic, morphological and syntactic term
variants and acronym recognition as an integral
part of the ATR process (Nenadi? et al (2002a)),
providing that all term occurrences of a term are
considered. The ATC method is based on the
Ushioda?s AMI (Average Mutual Information)
hierarchical clustering method (Ushioda (1996)).
Co-occurrence based term similarities are used as
input, and a dendrogram of terms is generated.2
2   Term Similarity Measures
In this section we introduce a novel hybrid method
to measure term similarity. Our method
incorporates three types of similarity measures,
namely contextual, lexical and syntactical
similarity. We use a linear combination of the three
similarities in order to estimate similarity between
terms. In the following subsections we describe
each of the three similarity measures.
2.1   Contextual Similarity
Determining the similarity of terms based on their
contexts is a standard approach based on the
hypothesis that similar terms tend to appear in
similar contexts. Contextual similarity, however,
may be determined in a number of ways depending
on the way in which the context is defined. For
example, some approaches consider only terms
that appear in a close proximity to each other
(Maynard and Ananiadou (2000)), while in other
approaches, grammatical roles such as object or
subject are taken into account (Grefenstette
(1994)).
   Our approach to contextual similarity is based on
automatic pattern mining. The aim is to
automatically identify and learn the most important
context patterns in which terms appear. Context
pattern (CP) is a generalised regular expression
that corresponds to either left or right context of a
term. 3 The following example shows a sample left
context pattern of the term high affinity:
                                                          
2 For the evaluation of the ATR and ATC methods
incorporated in ATRACT, see Mima et al (2001).
3 Left and right contexts are treated separately.
V:bind TERM:rxr_heterodimers PREP:with
   Let us now describe the process of constructing
CPs and determining their importance. First, we
collect concordances for all automatically
recognised terms. Context constituents, which we
consider important for discriminating terms (e.g.
noun and verb phrases, prepositions, and terms
themselves) are identified by a tagger and by
appropriate local grammars, which define syntactic
phrases (e.g. NPs, VPs). The grammatical and
lexical information attached to the context
constituents is used to construct CPs.  In the
simplest case, contexts are mapped into the
syntactic categories of their constituents. However,
the lemmatised form for each of the syntactic
categories can be used as well. For example, when
encountered in a context, the preposition with can
be either mapped to its POS tag, i.e. PREP, or
instead, the lemma can be added, in which case we
have an instantiated chunk: PREP:with.  Further,
some of the syntactic categories can be removed
from the context patterns, as not all syntactic
categories are equally significant in providing
useful contextual information (Maynard and
Ananiadou (2000)). Such CPs will be regarded as
normalised CPs. In our approach, one can define
which categories to instantiate and which to
remove. In the examples provided later in the
paper (Section 3) we decided to remove the
following categories: adjectives (that are not part
of a term), adverbs, determiners and so-called
linking words (e.g. however, moreover, etc.).
Also, we instantiated terms and either verbs or
prepositions, as these categories are significant for
discriminating terms.
   Once we have normalised CPs, we calculate the
values of a measure called CP-value in order to
estimate the importance of the CPs. CP-value is
defined similarly to the C/NC-value for terms
(Frantzi et al (2000)). It assesses a CP (p)
according to its total frequency (f(p)), its length
(|p|, as the number of constituents) and the
frequency of its occurrence within other CPs (|Tp|,
where Tp is a set of all CPs that contain p):
The CPs whose CP-value is above a chosen
threshold are deemed important. Note that these
patterns are domain-specific and that they are
automatically extracted from a domain specific
corpus. Tables 1 and 2 show samples of significant
left context patterns extracted from a MEDLINE
corpus (MEDLINE (2002)).
CPs CP-value
PREP   NP 272.65
PREP   NP   PREP 186.47
.   .   . .   .   .
PREP  NP   V:stimulate 9.32
V:indicate   NP 5.00
PREP   NP   PREP   V:involve NP 4.64
PREP   TERM:transcriptional_activity 4.47
V:require   NP   PREP 4.38
PREP TERM:nuclear_receptor PREP 4.00
Table 1: Sample of left CPs
(only terms and most frequent verbs are instantiated)
CPs CP-value
PREP:of   NP 121.49
V  NP 71.42
PREP:of   NP   V 62.83
NP   PREP:of   NP 59.72
PREP:in   NP 59.55
NP   PREP:of 43.37
PREP:of   NP  V   NP 37.64
PREP:of  TERM:transcriptional_activity 36.60
Table 2: Sample of left CPs
(only terms and prepositions are instantiated)
   At this point, each term is associated with a set of
the most characteristic patterns in which it occurs.
We treat CPs as term features, and we use a feature
contrast model (Santini and Jain (1999)) to
calculate similarity between terms as a function of
both common and distinctive features. Let us now
formally define the contextual similarity measure.
Let C1 and C2 be two sets of CPs associated with
terms t1 and t2 respectively. Then, the contextual
similarity (CS) between t1 and t2 corresponds to the
ratio between the number of common and
distinctive contexts:
2.2   Lexical Similarity
We also examine the lexical similarity between
words that constitute terms. For example, if terms
share the same head, they are assumed to have the













	


??
?
=

?
; otherwise
nestedis not
)(||
1)(log
);(log
)(
2
2
Tpbp
bfTpfp
ppfp
pCP
|\||\|||2
||2),(
212121
21
21 CCCCCC
CCttCS
++?
?
=
same concept as an (in)direct hypernym (e.g.
progesterone receptor and oestrogen
receptor). Further, if one of such terms has
additional modifiers, this may indicate concept
specialisation (e.g. nuclear receptor and
orphan nuclear receptor). Bearing that in
mind, we base the definition of lexical similarity
on having a common head and/or modifier(s).
Formally, if t1 and t2 are terms, H1 and H2 their
heads, and M1 and M2 the sets of the stems of their
modifiers, then their lexical similarity (LS) is
calculated according to the following formula:
where a and b are weights such that a > b, since we
give higher priority to shared heads over shared
modifiers.
   Note that the lexical similarity between two
different terms can have a positive value only if at
least one of them is a multiword term. Also, when
calculating lexical similarity between terms that
are represented by corresponding acronyms, we
use normalised expanded forms. 4
2.3   Syntactical Similarity
By analysing the distribution of similar terms in
corpora, we observed that some general (i.e.
domain independent) lexico-syntactic patterns
indicate functional similarity between terms. For
instance, the following example:
... steroid receptors such as
estrogen receptor, glucocorticoid
receptor,and progesterone receptor.
suggests that all the terms involved are highly
correlated, since they appear in an enumeration
(represented by the such-as pattern) which
indicates their similarity (based on the is_a
relationship). Some of these patterns have been
previously used to discover hyponym relations
between words (Hearst (1992)). We generalised
                                                          
4 For our approach to acronym acquisition and term
normalisation, see Nenadic et al (2002).
the approach by taking into account patterns in
which the terms are used concurrently within the
same context. We hypothesise that the parallel
usage of terms within the same context, as a
specific type of co-occurrence, shows their
functional similarity. Namely, all the terms within
a parallel structure have the same syntactic
function within the sentence (e.g. object or subject)
and are used in combination with the same verb or
preposition. This fact is used as an indicator of
their semantic similarity.
   In our approach, several types of lexico-
syntactical patterns are considered: enumeration
expressions, coordination, apposition, and
anaphora. However, currently we do not
discriminate between different similarity
relationships among terms (which are represented
by different patterns), but instead, we consider
terms appearing in the same syntactical roles as
highly semantically correlated.
   A sample of enumeration patterns is shown in
Table 3. 5 Manually defined patterns are applied as
syntactic filters in order to retrieve sets of similar
terms. These patterns provide relatively good recall
and precision. We also used coordination patterns
(Klavans et al (1997)) as another type of parallel
syntactic structure. Two types of argument
coordination and two types of head coordination
patterns were considered (see Table 4). However,
not all the sequences that match the coordination
patterns are coordinated structures (see Table 5).
Therefore, these patterns provide relatively good
recall, but not high precision if one wants to
retrieve terms involved in such expression.6
However, both term coordination and (nominal)
conjunction of terms indicate their similarity.
   Based on co-occurrence of terms in these parallel
lexico-syntactical patterns, we define the
syntactical similarity (SS) measure for a pair of
terms as 1 if the two terms appear together in any
of the patterns, and 0 otherwise.
                                                          
5 Non-terminal syntactic categories are given in angle
brackets. Non-terminal <&> denotes a conjunctive word
sequence, i.e. the following regular expression: (as
well as)| (and[/or])|(or[/and]). Special
characters (, ), [, ], |, and * have the usual
interpretation in regular expression notation.
6 In the experiments that we have performed, the
precision of expanding terms from coordinated
structures was 70%.
( +?
+
= ||1),( 21*21 HHabattLS
)|\||\|||2
||2
212121
21* MMMMMM
MMb
++?
?
+
<TERM>([(](such as)|like|(e.g.[,])) <TERM> (,<TERM>)* [[,] <&> <TERM>] [)]
<TERM> (,<TERM>)* [,] <&> other <TERM>
<TERM> [,] (including|especially) <TERM> (,<TERM>)* [[,] <&> <TERM>]
both <TERM> and <TERM>
either <TERM> or <TERM>
neither <TERM> nor <TERM>
Table 3: Sample of enumeration lexico-syntactic patterns
(<N>|<Adj>) (,(<N>|<Adj>))* [,] <&> (<N>|<Adj>) <TERM>
(<N>|<Adj>)/(<N>|<Adj>) <TERM>
(<N>|<Adj>) <TERM> (,<TERM>)* [,] <&> <TERM>
(<N>|<Adj>) <TERM>/<TERM>
Table 4: Sample of coordination patterns
head coordination [adrenal [glands and gonads]]
term conjunction [adrenal glands] and [gonads]
Table 5: Ambiguities of coordinated structures
2.4   Hybrid CLS Similarity
None of the similarities introduced so far is
sufficient on its own to reliably estimate similarity
between two arbitrary terms. For example, if a
term appears infrequently or within very specific
CPs, the number of its significant CPs will
influence its contextual similarity to other terms.
Further, there are concepts that have idiosyncratic
names (e.g. a protein named Bride of
sevenless), which thus cannot be classified
relying exclusively on lexical similarity. Our
experiments also show that syntactical similarity
provides high precision, but low recall when used
on its own, as not all terms appear in a parallel
lexico-syntactical expression.
   Therefore, we introduce a hybrid term similarity
measure, called the CLS similarity, as a linear
combination of the three similarity measures:
CLS(t1, t2) = ? CS(t1, t2) + ? LS(t1, t2) + ? SS(t1, t2)
The choice of the weights ?, ?, and ? in the
previous formula is not a trivial problem. In our
preliminary experiments (Section 3) we used
manually chosen values. However, the parameters
have also been fine-tuned automatically by
supervised learning method based on a genetic
algorithm approach (Spasi? et al (2002)). A
domain specific ontology has been used to evaluate
the generated similarity measures and to set the
direction of their convergence. The differences
between results based on the various parameters
are presented in the following section.
3   Results, Evaluation and Discussion
The CLS measure was tested on a corpus of 2008
abstracts retrieved from MEDLINE database
(MEDLINE (2002)) with manually chosen values
0.3, 0.3 and 0.4 for ?, ?, and ? respectively.
Random samples of results have been evaluated by
a domain expert, and the combined measure
proved to be a good indicator of semantic
similarity. Table 6 shows the similarity of term
retinoic acid receptor to a number of
terms. The examples point out the importance of
combining different types of term similarities. For
instance, the low value of contextual similarity7 for
retinoid X receptor is balanced out by the
other two similarity values, thus correctly
indicating it as a term similar to term retinoic
acid receptor. Similarly, the high value of the
contextual similarity for signal transduction
pathway is neutralised by the other two similarity
                                                          
7 The low value is caused by relatively low frequency of
the term?s occurrences in the corpus.
values, hence preventing it as being labelled as
similar to retinoic acid receptor.
Term CS SS LS CLS
nuclear receptor 0.58 1.00 0.50 0.72
retinoid X receptor 0.32 1.00 0.33 0.60
retinoic acid 0.31 0.00 1.00 0.39
receptor complex 0.52 0.00 0.50 0.31
progesteron receptor 0.35 0.00 0.50 0.25
signal transduction
pathway
0.75 0.00 0.00 0.22
Table 6: Similarity values between retinoic
acid receptor and other terms
   The combined measure also proved to be
consistent in the sense that similar terms share the
same "friends" (Maynard and Ananiadou (2000)).
For example, the similarity values of two similar
terms glucocorticoid receptor and
estrogen receptor (the value of their
similarity is 0.68) with respect to other terms are
mainly approximate (Table 7).
Term glucocotricoid
receptor
estrogen
receptor
steroid receptor 0.66 0.64
progesterone receptor 0.55 0.59
human estrogen
t
0.28 0.37
retinoid x receptor 0.27 0.36
nuclear receptor 0.30 0.33
receptor complex 0.31 0.33
retinoic acid receptor 0.27 0.28
retinoid nuclear
t
0.26 0.26
Table 7: Similarity values for glucocorticoid
receptor and estrogen receptor
  The supervised learning of parameters resulted in
the values 0.13, 0.81 and 0.06 for ?, ?, and ?
respectively (see Spasi? et al (2002)). The
measure with these values showed a higher degree
of stability relative to the ontology-based similarity
measure. Note that the lexical similarity appears to
be the most important and the syntactical similarity
to be insignificant. The ontology used as a seed for
learning term similarities contained well-
structured, standardised and preferred terms which
resulted in promoting the lexical similarity as the
most significant. On the other hand, the SS
similarity is corpus-dependent: the size of the
corpus and the frequency with which the
concurrent lexico-syntactic patterns are realised in
it, affect the syntactical similarity. In the training
corpus such patterns occurred infrequently relative
to the number of terms, which indicates that a
bigger corpus is needed in the training phase. In
order to increase the number of concurrent
patterns, we also aim at including additional
patterns that describe appositions and
implementing procedures for resolution of co-
referring terms. We also plan to experiment with
parametrising the values of syntactical similarity
depending on the number and type of patterns in
which two terms appear simultaneously.
   The main purpose of discovering term
similarities is to produce a similarity matrix to
identify term clusters. In Nenadi? et al (2002b) we
present some preliminary results on term clustering
using the CLS hybrid term similarity measure. Two
different methods (namely the nearest neighbour
and the Ward?s method) have been used, and both
achieved around 70% precision in clustering
semantically similar terms.
Conclusions and Further Research
In this paper we have presented a novel method for
the automatic discovery of term similarities. The
method is based on the combination of contextual,
lexical and syntactical similarities between terms.
Lexical similarity exposes the resemblance
between the words that constitute terms, while
syntactical similarity is based on mutual co-
occurrence in parallel lexico-syntactic patterns.
Contextual similarity is based on the automatic
discovery of significant contexts through
contextual pattern mining.   Although the approach
is domain independent and knowledge-poor,
automatically collected patterns are domain
specific and they identify significant contexts in
which terms tend to appear. However, in order to
learn domain-appropriate term similarity
parameters, we need to customise the method by
incorporating domain-specific knowledge. For
example, we have used an ontology to represent
such knowledge.
   The preliminary results in the domain of
molecular biology have shown that the measure
proves to be a good indicator of semantic similarity
between terms. Furthermore, the similarity
measure is consistent at assigning weights: similar
terms tend to share the same ?friends?, i.e. there is
a significant degree of overlapping between terms
that are similar. These results are encouraging, as
terms are grouped reliably according to their
contextual, syntactical and lexical similarities.
   Besides term clustering (presented in Nenadi? et
al. (2002b)), the similarity measure can be used for
several term-oriented knowledge management
tasks.  Our future work will focus on the term
classification and the consistent population and
update of ontologies. However, specific term
relationship identification that will direct placing
terms in a hierarchy is needed. Further, term
similarities can be used for term sense
disambiguation as well, which is essential for
resolving terminological confusion occurring in
many domains.
Acknowledgement
We would like to thank Dr. Sylvie Albert and Dr.
Dietrich Schuhmann from LION Bioscience for the
evaluation of the results.
References
Ananiadou S. (1994): A Methodology for Automatic
Term Recognition. Proceedings of COLING-94,
Kyoto, Japan.
Bourigault D. (1992): Surface Grammatical Analysis for
the Extraction of Terminological Noun Phrases.
Proceedings of 14th International Conference on
Computational Linguistics, Nantes, France, pp. 977-
981.
Frantzi K.T., Ananiadou S. and Mima H. (2000):
Automatic Recognition of Multi-Word Terms: the C-
value/NC-value method. International Journal on
Digital Libraries, 3/2, pp. 115-130.
Friedman C., Kra P., Yu H., Krauthammer M. and
Rzhetsky A. (2001): GENIES: A Natural Language
Processing System for the Extraction of Molecular
Pathways from Journal Articles. Bioinformatics, 17/1,
pp. S74-S82.
Grefenstette G. (1994): Exploration in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Massachusetts, p. 302.
Hatzivassiloglou V., Duboue P. and Rzetsky A. (2001):
Disambiguating Proteins, Genes, and RNA in Text: A
Machine Learning Approach. Bioinformatics, 17/1,
pp. S97-S106
Hearst M.A. (1992): Automatic acquisition of hyponyms
from large text corpora. Proceedings of the 14th
International Conference on Computational
Linguistics, Nantes, France.
Klavans J. L., Tzoukermann E. and Jacquemin C.
(1997): A Natural Language Approach to Multi-Word
Term Conflation. Proceedings of Workshop DELOS,
Zurich, pp. 33-40.
Maynard D. and Ananiadou S. (2000): Identifying
Terms by Their Family and Friends.  Proceedings of
COLING 2000, Luxembourg, pp.530-536.
MEDLINE (2002): National Library of Medicine.
http://www.ncbi.nlm.nih.gov/PubMed/
Mima H., Ananiadou S. and Nenadi? G. (2001):
ATRACT Workbench: An Automatic Term
Recognition and Clustering of Terms. Text, Speech
and Dialogue - TSD 2001, LNAI 2166, Springer-
Verlag, Berlin, pp. 126-133.
Mima H., Ananiadou S., Nenadi? G. and Tsujii J.
(2002): A Methodology for Terminology-based
Knowledge Acquisition and Integration. Proceedings
of COLING 2002, Taiwan
Nakagawa H. and Mori, T. (2000): Nested Collocation
and Compound Noun for Term Recognition.
Proceedings of the First Workshop on Computational
Terminology COMPUTERM 98, pp. 64-70.
Nenadi? G., Spasi? I. and Ananiadou S. (2002a):
Automatic Acronym Acquisition and Term Variation
Management within Domain-specific Texts.
Proceedings of LREC 2002, Las Palmas, Spain, pp.
2155-2162.
Nenadi? G., Spasi? I. and Ananiadou S. (2002b): Term
Clustering using a Corpus-Based Similarity Measure.
Text, Speech and Dialogue - TSD 2002, LNAI series,
Springer-Verlag, Berlin
Santini S. and Jain R. (1999): Similarity Measures.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 21/9, pp. 871-88
Spasi? I., Nenadi? G. and Ananiadou S. (2002):
Supervised Learning of Term Similarities. IDEAL
2002, LNAI series, Springer-Verlag, Berlin
Ushioda A. (1996): Hierarchical Clustering of Words.
Proceedings of COLING ?96, Copenhagen, pp. 1159-
1162.
Using Domain-Specific Verbs for Term Classification 
 
Irena Spasi? 
Computer Science 
University of Salford, UK 
I.Spasic@salford.ac.uk
Goran Nenadi? 
Department of Computing 
UMIST, UK 
G.Nenadic@umist.ac.uk
Sophia Ananiadou 
Computer Science  
University of Salford, UK 
S.Ananiadou@salford.ac.uk
 
Abstract 
In this paper we present an approach to 
term classification based on verb com-
plementation patterns. The complementa-
tion patterns have been automatically 
learnt by combining information found in 
a corpus and an ontology, both belonging 
to the biomedical domain. The learning 
process is unsupervised and has been im-
plemented as an iterative reasoning pro-
cedure based on a partial order relation 
induced by the domain-specific ontology. 
First, term recognition was performed by 
both looking up the dictionary of terms 
listed in the ontology and applying the 
C/NC-value method. Subsequently, do-
main-specific verbs were automatically 
identified in the corpus. Finally, the 
classes of terms typically selected as ar-
guments for the considered verbs were in-
duced from the corpus and the ontology. 
This information was used to classify 
newly recognised terms. The precision of 
the classification method reached 64%. 
1 Introduction 
Basic notions used when describing a specific 
problem domain are concepts, classes and attrib-
utes (or features). The identification of concepts, 
linguistically represented by domain-specific terms 
(Maynard and Ananiadou, 2000), is a basic step in 
the automated acquisition of knowledge from tex-
tual documents. Textual documents describing new 
knowledge in an intensively expanding domain are 
swamped by new terms representing newly identi-
fied or created concepts. Dynamic domains, such 
as biomedicine, cannot be represented by static 
models, since new discoveries give rise to the ap-
pearance of new terms. This makes the automatic 
term recognition (ATR) tools essential assets for 
efficient knowledge acquisition.  
However, ATR itself is not sufficient when it 
comes to organizing newly acquired knowledge. 
Concepts are natively assorted into groups and a 
well-formed model of a domain, represented 
through terms and their relations, needs to reflect 
this property consistently. Dynamic domain mod-
els should be able to adapt to the advent of new 
terms representing newly discovered or identified 
concepts. In other words, newly extracted terms 
need to be incorporated into an existing model by 
associating them with one another and with already 
established terms preferably in an automated man-
ner. This goal may be achieved by relying on term 
clustering (the process of linking semantically 
similar terms together) and term classification (the 
process of assigning terms to classes from a pre-
defined classification scheme). In particular, classi-
fication results can be used for efficient and consis-
tent term management through populating and 
updating existing ontologies in expanding domains 
such as biomedicine. In this paper, we compare 
some of the term classification approaches and in-
troduce another approach to this problem.  
The paper is organised as follows. In Section 2 
we provide a brief overview of the existing term 
classification approaches and suggest the main idea 
of our approach to this problem. Section 3 de-
scribes the learning phase of our classification 
method. Further, Section 4 provides details on the 
classification algorithm. Finally, in Section 5 we 
describe the evaluation strategy and provide the 
results, after which we conclude the paper. 
2 Term Classification Approaches 
Similarly to general classification algorithms, the 
existing term classification approaches typically 
rely on learning techniques. These techniques are 
most often statistically based (e.g. hidden Markov 
models, naive Bayesian learning, etc.). Other tech-
niques include decision trees, inductive rule learn-
ing, support-vector machines (SVMs), etc. We, on 
the other hand, suggest the use of a genetic algo-
rithm as a learning engine for the classification 
task. Let us now discuss some approaches to the 
automatic classification of biomedical terms. 
Nobata et al (2000) implemented a statistical 
method for term classification. In their approach, 
each class was represented by a list of (single) 
words. The first step was to estimate the condi-
tional probability P(c | w) of each word w being 
assigned to a specific class c, based on the assump-
tion that each word occurrence is independent of 
its context and position in the text. Further, yet an-
other strong restriction was made by assuming that 
there was one-to-one correspondence between 
terms and their classes. In addition, this approach 
is not applicable to ?unknown? terms, i.e. terms 
containing words for which no classification prob-
abilities had been determined. A special class, re-
ferring to ?other?, was introduced to cover such 
words. Bearing in mind the increasing number of 
new terms, such an approach is bound to produce 
skewed results, where many of the terms would 
simply be classified as ?other?. 
While Nobata et al (2000) statistically proc-
essed the information found inside the terms, Col-
lier et al (2001) applied statistical techniques to 
the information found outside the terms. A hidden 
Markov model based on n-grams (assuming that a 
term?s class may be induced from the previous n-1 
lexical items and their classes) was used as a theo-
retical basis for their classification method. The 
method relied on the orthographic features includ-
ing numerals, capital and Greek letters, special 
characters (such as `-`, `/`, `+`, etc.), parenthesis, 
etc. In the biomedical domain, such features often 
provide hints regarding the class of a specific term. 
Each unclassified term was assigned a class of the 
most similar (with respect to the orthographic fea-
tures) term from the training set. This approach 
encountered the minority class prediction problem. 
Namely, the best classification results in terms of 
recall and precision were achieved for the most 
frequent class of terms in their training corpus, 
while the worst results were those achieved for the 
least frequent class. 
Hatzivassiloglou et al (2001) proposed a 
method for unsupervised learning of weights for 
context elements (including words as context con-
stituents and the corresponding positional and 
morphological information) of known terms and 
using these weights for term classification. Three 
well-known learning techniques were used: naive 
Bayesian learning, decision trees, and inductive 
rule learning. Simplified classification experiments 
in which a classification algorithm was choosing 
between two or three options respectively were 
conducted. The precision of binary classification 
was around 76% for all three learning algorithms, 
and the precision dropped to approximately 67% 
when choosing between three options. If the pro-
posed techniques were to be applied for general 
classification where the number of options is arbi-
trary, the precision is expected to decrease even 
further. 
Nenadic et al (2003b) conducted a series of 
large-scale experiments with different types of fea-
tures for a multi-class SVM. These features in-
cluded document identifiers, single words, their 
lemmas and stems, and automatically recognised 
terms. The results indicated that the performance 
was approximately the same (around 60% in the 
best case) when using single words, lemmas or 
stems. On the other side, terms proved to be better 
(more than 90% precision) than single words at 
lower recall points (less than 10%), which means 
that terms as features can improve the precision for 
minority classes. The best results were achieved 
with document identifiers, but such features cannot 
be used on the fly in new documents.  
Spasic et al (2002) used a genetic algorithm 
(GA) based on a specific crossover operator to ex-
plore the relationships between verbs and the terms 
complementing them. The GA performed reason-
ing about term classes allowed to be combined 
with specific verbs by using an existing ontology 
as a seed for learning. In this paper, we use the re-
sults of the proposed methodology as a platform 
for term classification. In the following section we 
briefly overview the method for the acquisition of 
verb complementation patterns. 
3 Verb Complementation Patterns 
By looking at the context of an isolated verb occur-
rence it is difficult to predict all term classes that 
can be combined with the given verb. On the other 
hand, the whole ?population? of terms comple-
menting a specific verb is likely to provide a cer-
tain conclusion about that verb with respect to its 
complementation patterns. This was a primary mo-
tivation for Spasic et al (2002) to use a GA as it 
operates on a population of individuals as opposed 
to a single individual. This fact also makes the ap-
proach robust, since it does not rely on every spe-
cific instance of verb-term combination to be 
correctly recognised.  
As not all verbs are equally important for the 
term classification task, we are primarily interested 
in domain-specific verb complementation patterns. 
In our approach, a complementation pattern of a 
domain-specific verb is defined as a disjunction of 
terms and/or their classes that are used in combina-
tion with the given verb. The automatic acquisition 
of these patterns is performed in the following 
steps: term recognition, domain-specific verb ex-
traction, and the learning of complementation pat-
terns. Let us describe each of these steps in more 
detail. 
 
3.1   Term Recognition 
 
First, a corpus is terminologically processed: both 
terms present in the ontology and the terms recog-
nised automatically are tagged. Terms already 
classified in the ontology are used to learn the 
classes allowed by the domain-specific verbs, 
while the new terms are yet to be classified based 
on the learnt classes. New terms are recognized by 
the C/NC-value method (Frantzi et al, 2000), 
which extracts multi-word terms. This method rec-
ognises terms by combining linguistic knowledge 
and statistical analysis. Linguistic knowledge is 
used to propose term candidates through general 
term formation patterns. Each term candidate t is 
then quantified by its termhood C-value(t) calcu-
lated as a combination of its numerical characteris-
tics: length |t| as the number of words, absolute 
frequency f(t) and two types of frequency relative 
to the set S(t) of candidate terms containing a 
nested candidate term t (frequency of occurrence 
nested inside other candidate terms and the number 
of different term candidates containing a nested 
candidate term): 
 
??
??
?
????
?=?
=? ?
?
)( if  ,))(|)(|
1)((||ln
)( if  ),(||ln
)(
)(
tSsftStft
tStft
tvalueC
tSs
 
 
Obviously, the higher the frequency of a candi-
date term the greater its termhood. The same holds 
for its length. On the other side, the more fre-
quently the candidate term is nested in other term 
candidates, the more its termhood is reduced. 
However, this reduction decreases with the in-
crease in the number of different host candidate 
terms as it is hypothesised that the candidate term 
is more independent if the set of its host terms is 
more versatile. 
Term distribution in top-ranked candidate terms 
is further improved by taking into account their 
context. The relevant context words, including 
nouns, verbs and adjectives, are extracted and as-
signed weights based on how frequently they co-
occur with top-ranked term candidates. Subse-
quently, context factors are assigned to candidate 
terms according to their co-occurrence with top-
ranked context words. Finally, new termhood esti-
mations (NC-values) are calculated as a linear 
combination of the C-values and context factors.  
Nenadic et al (2003a) modified the C/NC-value 
to recognise acronyms as a special type of single-
word terms, and, thus, enhanced the recall of the 
method. On the other hand, the modified version 
incorporates the unification of term variants into 
the linguistic part of the method, which also im-
proved the precision, since the statistical analysis is 
more reliable when performed over classes of 
equivalent term variants instead of separate terms. 
 
3.2   Domain-Specific Verb Recognition 
 
Verbs are extracted from the corpus and ranked 
based on the frequency of occurrence and the fre-
quency of their co-occurrence with terms. A stop 
list of general verbs frequently mentioned in scien-
tific papers independently of the domain (e.g. ob-
serve, explain, etc.) was used to filter out such 
verbs. The top ranked verbs are selected and 
considered to be domain-specific. Moreover, these 
verbs are also corpus-specific (e.g. activate, 
bind, etc.). Table 3 provides a list of such verbs, 
which were used in the experiments. 
 
3.3   Complementation Pattern Learning 
 
In order to learn a verb complementation pattern 
for each of the selected verbs separately, terms are 
collected from the corpus by using these verbs as 
anchors. A GA has been implemented as an itera-
tive reasoning procedure based on a partial order 
relation induced by the domain-specific ontology.1 
In each iteration pairs of verb complementation 
patterns represented as sets of terms and term 
classes are merged. This operation involves the 
substitution of less general terms/classes by their 
more general counterparts, if there is a path in the 
ontology connecting them. Otherwise, the disjunc-
tion of the terms is formed and passed to the next 
iteration. Figure 1 depicts the process of learning a 
verb complementation pattern. 
Since the partial order relation induced by the 
ontology is transitive, the order in which terms are 
processed is of no importance. The final verb com-
plementation patterns are minimal in the sense that 
the number of terms in a verb complementation 
pattern and the depth of each individual term in the 
ontology are minimised. 
 
 
Figure 1. Learning the complementation pattern 
                    for the verb bind 
4 Term Classification Method 
The verb complementation patterns have been ob-
tained by running the GA on a set of terms some of 
which were present in an ontology, which is used 
                                                           
1 The partial order relation is based on the hierarchy of 
terms/classes: term/class t1 is in relation with t2, if there is a 
path in the ontology from t2 to t1. In that case, we say that t2  is 
more general than t1. 
during the learning process. The newly recognised 
terms (i.e. the ones not found in the ontology) will 
remain included in the final verb complementation 
patterns as non-classified terms, since at this point 
it is not known which classes could replace them. 
All elements of the final verb complementation 
patterns can be thus divided into two groups based 
on the criterion of their (non)existence in the on-
tology. The elements already present in the ontol-
ogy are candidate classes for the newly recognised 
terms. Let us now describe the classification 
method in more detail. 
Let V = {v1, v2, ... , vn} be a set of automatically 
identified domain-specific verbs. During the phase 
of learning verb complementation patterns, each of 
these verbs is associated with a set of classes and 
terms it co-occurs with. Let Ci = {ci,1, ci,2, ... , ci,mi} 
denote a set of classes assigned automatically to 
the verb vi (1 ? i ? n) by a learning algorithm based 
on the information found in the corpus and the 
training ontology. As indicated earlier, we define 
such set to be a verb complementation pattern for 
the given verb.  
 
4.1   Statistical Analysis 
 
As we planned to use verb complementation pat-
terns for term classification, we modified the origi-
nal learning algorithm (Spasic et al, 2002) by 
attaching the frequency information to terms and 
their classes. When substituting a less general class 
by its more general counterpart, 2  the frequency 
information is updated by summing the two 
respective frequencies of occurrence. In the final 
verb complementation pattern, each class ci,j has 
the frequency feature fi,j, which aggregates the fre-
quency of co-occurrence with vi (1 ? i ? n; 1 ? j ?  
mi) for the given class and its subclasses. The fre-
quency information is used to estimate the class 
probabilities given a verb, P(ci,j | vi): 
 
?
=
=
lm
l
li
ji
ji
f
fp
1
,
,
,  
                                                           
2 The ontology used for learning allowed multiple inheritance 
only at the leaf level, that way incurring no ambiguities when 
substituting subclass by its superclass. The multiple inheri-
tance at the leaf level was resolved by mapping each term to 
all its classes, which were then processed by a GA. 
Unclassified terms remain present in the final 
verb complementation patterns, and, like classes, 
they are also assigned the information on the fre-
quency of co-occurrence with the given verb. 
When classifying a specific term, this information 
is used to select the verb based on whose pattern 
the term will be classified. Precisely, the verb the 
given term most frequently co-occurs with is cho-
sen, as it is believed to be the most indicative one 
for the classification purpose. 
 
4.2   Term Similarity Measure 
 
A complementation pattern associated with the 
chosen verb typically contain several classes. In 
order to link the newly recognised terms to specific 
candidate classes, we used a hybrid term similarity 
measure, called the CLS similarity measure. It 
combines contextual, lexical and syntactic proper-
ties of terms in order to estimate their similarity 
(Nenadic et al, 2002).  
Lexical properties used in the CLS measure re-
fer to constituents shared by the compared terms. 
The rationale behind the lexical term similarity 
involves the following hypotheses: (1) Terms shar-
ing a head are likely to be hyponyms of the same 
term (e.g. progesterone receptor and oes-
trogen receptor). (2) A term derived by modi-
fying another term is likely to be its hyponym (e.g. 
nuclear receptor and orphan nuclear re-
ceptor). Counting the number of common con-
stituents is a simple and straightforward approach 
to measuring term similarity, but it falls short when 
it comes to single-word terms and those introduced 
in an ad-hoc manner. Thus, properties other than 
lexical need to be included.  
We use syntactic properties in the form of spe-
cific lexico-syntactical patterns indicating parallel 
usage of terms (e.g. both Term and Term). All 
terms used within a parallel structure have identi-
cal syntactic features and are used in combination 
with the same verb, preposition, etc., and, hence, 
can be regarded as similar with high precision. 
However, patterns used as syntactic properties of 
terms have relatively low frequency of occurrence 
compared to the total number of terms, and in or-
der to have a good recall, a large-size corpus is 
needed. In order to remedy for small-size corpora, 
other contextual features are exploited.  
Context patterns (CPs) in which terms appear 
are used as additional features for term compari-
son. CPs consist of the syntactic categories and 
other grammatical and lexical information (e.g. 
PREP NP V:stimulate). They are ranked ac-
cording to a measure called CP-value  (analogue to 
C-value for ATR). The ones whose CP-value is 
above a chosen threshold are deemed significant 
and are used to compare terms. Each term is asso-
ciated with a set of its CPs, and contextual similar-
ity between terms is then measured by comparing 
the corresponding sets. Automatically collected 
CPs are indeed domain-specific, but the method for 
their extraction is domain independent. 
 
4.3   Term-Class Similarity 
 
The CLS similarity measure applies to pairs of 
terms. However, in case of multiple choices pro-
vided by the verb complementation patterns, we 
need to compare terms to classes. In order to do so, 
we use the similarity between the given term and 
the terms belonging to the classes. The selection of 
terms to be compared is another issue. One possi-
bility is to use the full or random set of terms (be-
longing to the given class) that occur in the corpus. 
Alternatively, some ontologies provide a set of 
prototypical instances for each class, which can be 
used for comparison of terms and classes.3 More 
formally, if c is a class, e1, e2,..., ek are terms repre-
senting the class, and t is a term, then the similarity 
between the term t and the class c is calculated in 
the following way: 
?
=
?
=
k
j
j
i
ki
etCLS
etCLSctEx
1
2
},...,1{
),(
),(max),(  
 
This example-based similarity measure maxi-
mises the value of the CLS measure between the 
term and the instances representing the class. In 
addition, the values of the CLS measure are 
mapped into the interval (0,1) by performing vec-
tor normalisation in order to make them compara-
ble to the class probability estimations.  
 
4.4   Term Classification 
 
Finally, given the term t and the verb vi it most  
frequently co-occurs with, a score is calculated for 
                                                           
3 For example, in the UMLS ontology each class is assigned a 
number of its prototypical examples represented by terms. 
each class ci,j from the set Ci according to the fol-
lowing formula: 
 
),()1(),( ,,, jijiji ctExapactC ??+?=    (1) 
 
where a (0 ? a ? 1) is a parameter, which balances 
the impact of the class probabilities and the simi-
larity measure.4 A class with the highest C(t, ci,j) 
score is used to classify the term t. Alternatively, 
multiple classes may be suggested by setting a 
threshold for C(t, ci,j). 
At this point, let us reiterate that the final verb 
complementation patterns are minimal in the sense 
that the number of terms in a verb complementa-
tion pattern and the depth of each individual term 
in the ontology are minimised. The latter condition 
may cause the classification to be crude, that is ? 
new terms will be assigned to classes close to the 
root of the ontology. For more fine-grained classi-
fication results, the classes placed close to the root 
of the ontology should be either removed from the 
initial verb complementation patterns, thus being 
unable to override the classes found lower in the 
hierarchy or in other way prevented from substitut-
ing less general terms. The depth up to which the 
terms are to be blocked may be empirically deter-
mined. 
5 Experiments and Evaluation 
5.1   Resources 
 
The resources used for the experiments include an 
ontology and a corpus, both belonging to the do-
main of biomedicine. We used an ontology, which 
is a part of the UMLS (Unified Medical Language 
System) knowledge sources (UMLS, 2002). 
UMLS integrates biomedical information from a 
variety of sources and is regularly updated. 
Knowledge sources maintained under the UMLS 
project include: METATHESAURUS linking term 
variants referring to the same concepts; 
SPECIALIST LEXICON providing syntactic informa-
tion for terms, their component words, and general 
                                                           
4 Note that when a = 0, the classification method resembles 
the nearest neighbour classification method, where the exam-
ples are used as a training set. On the other hand, when a = 1, 
the method is similar to naive Bayesian learning. However, in 
both cases the method represents a modification of the men-
tioned approaches, as the classes used in formula (1) are not 
all classes, but the ones learned by the GA. 
English words; and SEMANTIC NETWORK contain-
ing information about the classes to which all 
METATHESAURUS concepts have been assigned. 
The knowledge sources used in our term classi-
fication experiments include METATHESAURUS 
and SEMANTIC NETWORK. As the number of terms 
in  METATHESAURUS was too large (2.10 million 
terms) and the classification scheme too broad 
(135 classes) for the preliminary experiments, we 
made a decision to focus only on terms belonging 
to a subtree of the global hierarchy of the 
SEMANTIC NETWORK. The root of this subtree re-
fers to substances, and it contains 28 classes. 
The corpus used in conjunction with the above 
ontology consisted of 2082 abstracts on nuclear 
receptors retrieved from the MEDLINE database 
(MEDLINE, 2003). The majority of terms found in 
the corpus were related to nuclear receptors and 
other types of biological substances, as well as the 
domain-specific verbs extracted automatically 
from the corpus in the way described in Section 3.  
 
5.2   Evaluation Framework 
 
When retrieving terms found in the context of do-
main-specific verbs (see Section 3 for details) both 
terms found in the ontology and terms recognised 
on the fly by the C/NC-value method should be 
extracted. However, for the purpose of evaluation, 
only terms classified in the ontology were used. In 
that case, it was possible to automatically verify 
whether such terms were correctly classified by 
comparing the classes suggested by the classifica-
tion method to the original classification informa-
tion found in the ontology. 
During the phase of retrieving the verb-term 
combinations, some of the terms were singled out 
for testing. Namely, for each verb, 10% of the re-
trieved terms were randomly selected for testing, 
and the union of all such terms formed a testing set 
(138 terms) for the classification task. The remain-
ing terms constituted a training set (1618 terms) 
and were used for the learning of complementation 
patterns.  
 
5.3   Results 
 
Based on the training set, domain-specific verbs 
were associated with the complementation patterns 
given (see Table 1 for examples). Then, each term 
from the training set was associated with the verb 
it most frequently co-occurred with. The comple-
mentation pattern learnt for that verb was used to 
classify the term in question.  
 
Verb Complementation pattern 
activate
bind
Immunologic Factor 
Receptor   
Enzyme 
Hormone 
Organic Chemical 
Hazardous or Poisonous Substance 
Pharmacologic Substance 
Table 1. Learnt verb complementation patterns 
 
Since the UMLS ontology contains a number of 
prototypical examples for each class, we have used 
these class representatives to compare unclassified 
terms to their potential classes as indicated in Sec-
tion 4. Table 2 shows the results for some of the 
terms from the testing set and compares them to 
the correct classifications obtained from the ontol-
ogy. 
  
Term Suggested 
class 
Correct classes 
4 hydroxy-
tamoxifen
Organic   
   chemical Organic chemical 
benzoic
acid
Organic  
   chemical 
Organic chemical 
Pharmacologic      
   substance 
testoster-
one
Pharmacologic 
   substance 
Steroid  
Pharmacologic 
   substance 
Hormone 
Table 2. Examples of the classification results 
 
Note that in UMLS one term can be assigned to 
multiple classes. We regarded a testing term to be 
correctly classified if the automatically suggested 
class was among these classes. Table 3 provides 
information on the performance of the classifica-
tion method for each of the considered verbs sepa-
rately and for the combined approach in which the 
verb most frequently co-occurring with a given 
term was used for its classification. The combined 
approach provided considerably higher recall 
(around 50%) and a slight improvement in preci-
sion (around 64%) compared to average values 
obtained with the same method for each of the 
verbs separately. The classification precision did 
not tend to very considerably, and was not affected 
by the recall values. The recall could be improved 
by taking into account more domain-specific verbs, 
while the improvement of precision depends on 
proper tuning of: (1) the module for learning the 
verb complementation patterns, and (2) the similar-
ity measure used for the classification. Another 
possibility is to generalize the classification 
method by relying on domain-specific lexico-
syntactic patterns instead of verbs. Such patterns 
would have higher discriminative power than verbs 
alone. Moreover, they could be acquired automati-
cally. For instance, the CP-value method can be 
used for their extraction from a corpus (Nenadic et 
al., 2003a). 
 
Verb Recall Precision F-measure 
activate 19.28 66.59 29.90 
bind 29.30 66.53 40.68 
compete   3.58 63.16   6.78 
conserve   2.41 61.82   4.64 
inhibit 16.62 62.81 26.28 
interact 13.16 64.31 21.85 
mediate 11.68 62.75 19.69 
modulate 10.44 64.13 17.96 
repress   6.18 62.91 11.25 
stimulate   9.39 63.25 16.35 
Average: 12.20 63.83 20.48 
Combined: 49.88 64.18 56.13 
Table 3. The performance of the classification  
                  method 
 
The values for precision and recall provided in 
Table 3 refer to the classification method itself. If 
it were to be used for the automatic ontology up-
date, then the success rate of such update would 
also depend on the performance of the term recog-
nition method, as the classification module would 
operate on its output. We used the C/NC-value 
method for ATR; still any other method may be 
used for this purpose. We have chosen the C/NC-
value method because it is constantly improving 
and is currently performing around 72% recall and 
98% precision (Nenadic et al, 2002). 
6 Conclusion 
Efficient update of the existing knowledge re-
positories in many rapidly expanding domains is a 
burning issue. Due to an enormous number of 
terms and the complex structure of the terminol-
ogy, manual update approaches are prone to be 
both inefficient and inconsistent. Thus, it has be-
come absolutely essential to implement efficient 
and reliable term recognition and term classifica-
tion methods as means of maintaining the knowl-
edge repositories. In this paper, we have suggested 
a domain independent classification method as a 
way of incorporating automatically recognised 
terms into an existing ontology. For the prelimi-
nary experiments, we used the UMLS ontology in 
the domain of biomedicine, but the method can be 
easily adapted to use other ontologies in any other 
domain.  
The classification method makes use of the 
contextual information. Not all word types found 
in the context are of equal importance in the 
process of reasoning about the terms: the most in-
formative are verbs, noun phrases (especially 
terms) and adjectives. The presented term 
classification approach revolves around domain-
specific verbs. These verbs are used to collect 
unclassified terms and to suggest their potential 
classes based on the automatically learnt verb 
complementation patterns.  
Note that not every term appearing in a corpus 
is guaranteed to be classified by the proposed clas-
sification method due to the fact that a term need 
not occur as a complement of a domain-specific 
verb. Still, for a large number of terms the classifi-
cation method is expected to obtain the classifica-
tion information, as it is highly probable (though 
not certain) for a term to occur in a context of a 
domain-specific verb. The main goal of the method 
is to provide aid for the automatic ontology update 
by populating newly recognised terms into an ex-
isting ontology, rather than classifying arbitrary 
term occurrences in the corpus.  
The presented classification method can be eas-
ily modified to use lexical classes other than verbs 
as a criterion for classification. Even more, it can 
be further generalised to use a combination of lexi-
cal classes, which can be specified as a set of 
lexico-syntactic patterns. Further experiments with 
the generalisation of the classification method by 
basing it on a set of domain-specific lexico-
syntactic patterns instead of domain-specific verbs 
are expected to demonstrate better performance in 
terms of recall and precision. These facts suggest 
that our classification approach, in combination 
with the C/NC-value method, could be reliably 
used as a (semi)automatic ontology maintenance 
procedure.  
References 
Nigel Collier, Chikashi Nobata and Junichi Tsujii. 2001. 
Automatic Acquisition and Classification of Termi-
nology Using a Tagged Corpus in the Molecular Bi-
ology Domain. Journal of Terminology, John 
Benjamins. 
Katerina Frantzi, Sophia Ananiadou and Hideki Mima. 
2000. Automatic Recognition of Multi-Word Terms: 
the C-value/NC-value Method. International Journal 
on Digital Libraries 3(2):115-130. 
Vasileios Hatzivassiloglou, Pablo Duboue and Andrey 
Rzhetsky. 2001. Disambiguating Proteins, Genes, 
and RNA in Text: A Machine Learning Approach. 
Bioinformatics, 1(1):1-10. 
Diana Maynard and Sophia Ananiadou. 2000. Identify-
ing Terms by their Family and Friends. Proceedings 
of COLING 2000, Saarbrucken, Germany, 530-536.  
MEDLINE. 2003. National Library of Medicine. Avail-
able at: http://www.ncbi.nlm.nih.gov/PubMed/ 
Goran Nenadic, Irena Spasic and Sophia Ananiadou. 
2002. Automatic Acronym Acquisition and Term 
Variation Management within Domain-Specific 
Texts. Proceedings of LREC-3, Las Palmas, Spain, 
2155-2162.  
Goran Nenadic, Irena Spasic and Sophia Ananiadou. 
2003a. Automatic Discovery of Term Similarities Us-
ing Pattern Mining. To appear in Terminology. 
Goran Nenadic, Simon Rice, Irena Spasic, Sophia 
Ananiadou and Benjamin Stapley. 2003b. Selecting 
Features for Text-Based Classification: from Docu-
ments to Terms. Proceedings of ACL Workshop on 
Natural Language Processing in Biomedicine, 
Sapporo, Japan. 
Chikashi Nobata, Nigel Collier and Junichi Tsujii. 2000. 
Automatic Term Identification and Classification in 
Biology Texts. Proceedings of the Natural Language 
Pacific Rim Symposium (NLPRS?2000), 369-375. 
Irena Spasic, Goran Nenadic and Sophia Ananiadou. 
2002. Tuning Context Features with Genetic Algo-
rithms. Proceedings of 3rd International Conference 
on Language,  Resources and Evaluation, Las Pal-
mas, Spain, 2048-2054. 
UMLS. 2002. UMLS Knowledge Sources. National 
Library of Medicine, 13th edition.  
 
Selecting Text Features for Gene Name Classification:  
from Documents to Terms 
 
Goran Nenadi?1,2, Simon Rice2, Irena Spasi?3, Sophia Ananiadou3, Benjamin Stapley2 
 
1Dept. of Computation 
UMIST 
Manchester, M60 1QD 
 
2Dept. of BioMolecular Sciences 
UMIST 
Manchester, M60 1QD 
 
3Computer Science 
University of Salford 
Salford, M5 4WT 
Abstract 
In this paper we discuss the performance 
of a text-based classification approach by 
comparing different types of features. We 
consider the automatic classification of 
gene names from the molecular biology 
literature, by using a support-vector ma-
chine method. Classification features 
range from words, lemmas and stems, to 
automatically extracted terms. Also, sim-
ple co-occurrences of genes within docu-
ments are considered. The preliminary 
experiments performed on a set of 3,000 
S. cerevisiae gene names and 53,000 
Medline abstracts have shown that using 
domain-specific terms can improve the 
performance compared to the standard 
bag-of-words approach, in particular for 
genes classified with higher confidence, 
and for under-represented classes.  
1 Introduction 
Dynamic development and new discoveries in the 
domain of biomedicine have resulted in the huge 
volume of the domain literature, which is con-
stantly expanding both in the size and thematic 
coverage (Blaschke et al, 2002). The literature, 
which is still the most relevant and the most useful 
knowledge source, is swamped by newly coined 
terms and relationships representing and linking 
newly identified or created compounds, genes, 
drugs, reactions, etc., which makes the existing 
terminological resources rarely up-to-date. There-
fore, domain knowledge sources need to frequently 
adapt to the advent of such terms by assorting them 
into appropriate classes, in order to allow biolo-
gists to rapidly acquire, analyse and visualise enti-
ties or group of entities (Stapley et al, 2002).  
Naming conventions solely cannot be used as 
reliable classification criteria, since they typically 
do not systematically reflect any particular func-
tional property or relatedness between biological 
entities. On the other hand, it has proved surpris-
ingly difficult to automatically predict classes for 
some types of biological entities based solely on 
experimental data (e.g. the prediction of protein 
cellular locations from sequences (Eisenhaber and 
Bork, 1998) or the amino acid composition of pro-
teins (Nishikawa and Ooi, 1982)).  
In order to overcome this problem, several lit-
erature-based classification methods have been 
developed (Collier et al 2001; Hatzivassiloglou et 
al., 2001). Classification methods typically rely on 
supervised machine learning techniques that ex-
amine the wider context in which terms are used. 
For example, Raychaudhuri et al (2002) used 
document-based word counts and naive Bayesian 
classification, maximum entropy modelling and 
nearest-neighbour classification to assign the GO 
ontology codes to a set of genes. Recently, sup-
port-vector machines (SVMs, (Vapnik, 1995)) 
have been widely used as fast, effective and reli-
able means for text-based classification, both for 
document classification (Joachims, 1998) and clas-
sification of specific named entities (Stapley et al, 
2002; Kazama et al, 2002). 
Regardless of the learning approach and target 
entities (documents or terms), different types of 
text features have been employed for the classifica-
tion task. For example, a bag-of-words approach 
was used by Stapley et al (2002) to classify pro-
teins, while Collier et al (2001) used orthographic 
features to classify different biological entities. On 
the other hand, Hatzivassiloglou et al (2001) ex-
perimented with morphological, distributional and 
shallow-syntactic information to discriminate be-
tween proteins, genes and RNAs.  
In this paper we analyse the impact of different 
types of features on the performance of an SVM-
based classifier. More precisely, we discuss the 
multi-class SVM performance with respect to the 
type of features used, ranging from document iden-
tifiers, through words, lemmas and stems, to auto-
matically extracted terms.   
The paper is organised as follows. After pre-
senting the related work on feature selection in 
Section 2, the methods used for engineering fea-
tures in our approach are explained in Section 3. 
Section 4 discusses the experiments and results. 
2 Related work 
An SVM is a binary classification method that 
combines statistical learning and optimisation tech-
niques with kernel mapping (Vapnik, 1995). The 
main idea of the method is to automatically learn a 
separation hyperplane from a set of training 
examples, which splits classified entities into two 
subsets according to a certain classification prop-
erty. The optimisation part is used to maximise the 
distance (called the margin) of each of the two 
subsets from the hyperplane.     
The SVM approach has been used for different 
classification tasks quite successfully, in particular 
for document classification, where the method out-
performed many alternative approaches (Joachims, 
1998). Similarly, SVMs have been used for term 
classification. For example, a bag-of-simple-words 
approach with idf-like weights was used to learn a 
multi-class SVM classifier for protein cellular lo-
cation classification (Stapley et al, 2002). Proteins 
were represented by feature vectors consisting of 
simple words co-occurring with them in a set of 
relevant Medline abstracts. The precision of the 
method was better than that of a classification 
method based on experimental data, and similar to 
a rule-based classifier.  
Unlike many other classification methods that 
have difficulties coping with huge dimensions, one 
of the main advantages of the SVM approach is 
that its performance does not depend on the dimen-
sionality of the space where the hyperplane separa-
tion takes place. This fact has been exploited in the 
way that many authors have suggested that ?there 
are few irrelevant features? and that ?SVMs elimi-
nate the need for feature selection? (Joachims, 
1998). It has been shown that even the removal of 
stop-words is not necessary (Leopold and Kinder-
mann, 2002). 
Few approaches have been undertaken only re-
cently to tune the original SVM approach by se-
lecting different features, or by using different 
feature weights and kernels, mostly for the docu-
ment classification task. For example, Leopold and 
Kindermann (2002) have discussed the impact of 
different feature weights on the performance of 
SVMs in the case of document classification in 
English and German. They have reported that an 
entropy-like weight was generally performing bet-
ter than idf, in particular for larger documents. 
Also, they suggested that, if using single words as 
features, the lemmatisation was not necessary, as it 
had no significant impact on the performance.   
Lodhi et al (2002) have experimented with dif-
ferent kernels for document classification. They 
have shown that a string kernel (which generates 
all sub-sequences of a certain number of charac-
ters) could be an effective alternative to linear ker-
nel SVMs, in particular in the sense of efficiency.  
In the case of term classification, Kazama et al 
(2002) used a more exhaustive feature set contain-
ing lexical information, POS tags, affixes and their 
combinations in order to recognise and classify 
terms into a set of general biological classes used 
within the GENIA project (GENIA, 2003). They 
investigated the influence of these features on the 
performance. For example, they claimed that suffix 
information was helpful, while POS and prefix 
features did not have clear or stable influence.  
While each of these studies used some kind of 
orthographical and/or lexical indicators to generate 
relevant features, we wanted to investigate the us-
age of semantic indicators (such as domain-
specific terms) as classification features, and to 
compare their performance with the classic lexi-
cally-based features. 
3 Feature selection and engineering 
The main aim while selecting classification fea-
tures is to find (and use) textual attributes that can 
improve the classification accuracy and accelerate 
the learning phase. In our experiments we exam-
ined the impact of different types of features on the 
performance of an SVM-based gene name classifi-
cation task. The main objective was to investigate 
whether additional linguistic pre-processing of 
documents could improve the SVM results, and, in 
particular, whether semantic processing (such as 
terminological analysis) was beneficial for the 
classification task. In other words, we wanted to 
see which textual units should be generated as in-
put feature vectors, and what level of pre-
processing was appropriate in order to produce 
more accurate predictions. 
We have experimented with two types of tex-
tual features: in the first case, we have used a clas-
sic bag-of-single-words approach, with different 
levels of lexical pre-processing (i.e. single words, 
lemmas, and stems). In the second case, features 
related to semantic pre-processing of documents 
have been generated: a set of automatically ex-
tracted multi-word terms (other than gene names to 
be classified) has been used as a feature set. Addi-
tionally, we have experimented with features 
reflecting simple gene-gene co-occurrences within 
the same documents.  
3.1 Single words as features 
The first set of experiments included a classic bag-
of-single-words approach. All abstracts (from a 
larger collection, see Section 4) that contained at 
least one occurrence of a given gene or its aliases 
have been selected as documents relevant for that 
gene. These documents have been treated as a sin-
gle virtual document pertinent to the given gene. 
All words co-occurring with a given gene in any of 
the abstracts were used as its features.  
A word has been defined as an alphanumeric 
sequence between two standard separators, with all 
numeric expressions that were not part of other 
words filtered out. In addition, a standard list of 
around 300 stop-words has been used to exclude 
some frequent non-content words. 
An idf-like measure has been used for feature 
weights: the weight of a word w for gene g is given 
by 
(1)                        |)|1(
)(1
log
gw
Rj
j
RN
wf
g
+
+ ?
?
 
where Rg  is a set of relevant documents for the 
gene g,  fj(w) is the frequency of w in document j, 
and Nw is the global frequency of w. Gene vectors, 
containing weights for all co-occurring words, 
have been used as input for the SVM. 
It is widely accepted that rare words do not 
have any significant influence on accuracy (cf. 
(Leopold and Kindermann, 2002)), neither do 
words appearing only in few documents. In our 
experiments (demonstrated in Section 4), we com-
pared the performance between the ?all-words ap-
proach? and an approach featuring words appearing 
in at least two documents. In the latter case, the 
dimension of the problem (expressed as the num-
ber of features) was significantly reduced (with 
factor 3), and consequently the training time was 
shortened (see Section 4). 
Since many authors claimed that the biomedical 
literature contained considerably more linguistic 
variations than text in general (cf. Yakushiji et al, 
2001), we applied two standard transformations in 
order to reduce the level of lexical variability. In 
the first case, we used the EngCG POS tagger 
(Voutilainen and Heikkila, 1993) to generate lem-
mas, so that lemmatised words were used as fea-
tures, while, in the second case, we generated 
stems by the Porter?s algorithm (Porter, 1980). 
Analogously to words, the same idf-based measure 
was used for weights, and experiments were also 
performed with all features and with the features 
appearing in no less than two documents. 
3.2 Terms as features 
Many literature-mining techniques rely heavily on 
the identification of main concepts, linguistically 
represented by domain specific terms (Nenadic et 
al., 2002b). Terms represent the most important 
concepts in a domain and have been used to char-
acterise documents semantically (Maynard and 
Ananiadou, 2002). Since terms are semantic indi-
cators used in scientific discourse, we hypothesised 
that they might be useful classification features.  
The high neology rate for terms makes existing 
glossaries incomplete for active and time-limited 
research, and thus automatic term extraction tools 
are needed for efficient terminological processing. 
In order to automatically generate term as features, 
we have used an enhanced version of the C-value 
method (Frantzi et al, 2000), which assigns term-
hoods to automatically extracted multi-word term 
candidates. The method combines linguistic forma-
tion patterns and statistical analysis. The linguistic 
part includes part-of-speech tagging, syntactic pat-
tern matching and the use of a stop list to eliminate 
frequent non-terms, while statistical termhoods 
amalgamate four numerical characteristic of a can-
didate term, namely: the frequency of occurrence, 
the frequency of occurrence as a nested element, 
the number of candidate terms containing it as a 
nested element, and term?s length.   
Due to the extensive term variability in the do-
main, the same concept may be designated by 
more than one term. Therefore, term variants con-
flation rules have been added to the linguistic part 
of the C-value method, in order to enhance the re-
sults of the statistical part. When term variants are 
processed separately by the statistical module, their 
termhoods are distributed across different variants 
providing separate frequencies for individual vari-
ants instead of a single frequency calculated for a 
term candidate unifying all of its variants. Hence, 
in order to make the most of the statistical part of 
the C-value method, all variants of the candidate 
terms are matched to their normalised forms by 
applying rule-based transformations and treated 
jointly as a term candidate  (Nenadic et al, 2002a). 
In addition, acronyms are acquired prior to the se-
lection of the term candidates and also mapped to 
their expanded forms, which are normalised in the 
same manner as other term candidates.  
Once a corpus has been terminologically proc-
essed, each target gene is assigned a set of terms 
appearing in the corresponding set of documents 
relevant to the given gene. Thus, in this case, gene 
vectors used in the SVM classifier contain co-
occurring terms, rather than single words. As term 
weights, we have used a formula analogous to (1). 
Also, similarly to single-word features, we have 
experimented with terms appearing in at least two 
documents. 
3.3 Combining word and term features 
The C-value method extracts only multi-word 
terms, which may be enriched during the normali-
sation process with some single-word terms, sourc-
ing from e.g. acronyms or orthographic variations. 
In order to assess impact of both single and multi-
word terms as features, we experimented with 
combining single-word based features with multi-
word terms by using a simple kernel modification 
that concatenates the corresponding feature vec-
tors. Thus, gene vectors used in this case contain 
both words and terms that genes co-occur with. 
3.4 Document identifiers as features 
Term co-occurrences have been traditionally used 
as an indication of their similarity (Ushioda, 1986), 
with documents considered as bags of words in the 
majority of approaches. For example, Stapley et al 
(2000) used document co-occurrence statistics of 
gene names in Medline abstracts to predict their 
connections. The co-occurrence statistics were rep-
resented by the reciprocal Dice coefficient. Similar 
approach has been undertaken by Jenssen et al 
(2001): they identified co-occurrences of gene 
names within abstracts, and assigned weights to 
their ?relationship? based on frequency of co-
occurrence.  
In our experiments, abstract identifiers (Pub-
Med identifiers, PMIDs) have been used as fea-
tures for classification, where the dimensionality of 
the feature space was equal to the number of 
documents in the document set. As feature 
weights, binary values (i.e. a gene is present/absent 
in a document) were used.  
We would like to point out that ? contrary to 
other features ? this approach is not a general 
learning approach, as document identifiers are not 
classification attributes that can be learnt and used 
against other corpora. Instead, this approach can be 
only used to classify new terms that appear in a 
closed corpus used for training. 
4 Experiments and discussions 
An experimental environment was set up by using 
the following resources:  
a) corpus: a set of documents has been ob-
tained by collecting Medline abstracts (NLM, 
2003) related to the baker?s yeast (S. cerevisiae), 
resulting in 52,845 abstracts; this set, containing 
almost 5 million word occurrences, was used as 
both training and testing corpus. 
b) classification entities: a set of 5007 S. cere-
visiae gene names has been retrieved from the 
SGD (Saccharomyces Genome Database) gene 
registry1, which also provided synonyms and ali-
ases of genes; 2975 gene names appearing in the 
corpus have been used for the classification task. 
c) classification scheme: each gene name has 
been classified according to a classification scheme 
based on eleven categories (see Table 1) of the up-
                                                           
1 http://genome-www.stanford.edu/Saccharomyces/registry.html  
per part of the GO ontology (Ashburner et al, 
2000)2. 
d) training and testing sets: positive examples 
for each class were split evenly between the train-
ing and testing sets, and, also, the number of nega-
tive examples in the training set was set equal to 
the number of positive examples within each class. 
The only exception was the metabolism class, 
which had far more positive than negatives exam-
ples. Therefore, in this case, we have evenly split 
negative examples between the training and testing 
sets. Table 1 presents the distribution of positive 
and negative examples for each class. 
d) SVM engine: for training the multi-class 
SVM, we used SVM Light package v3.50 
(Joachims, 1998) with a linear kernel function with 
the regulation parameter calculated as avg(<x,x>)-1.  
  
 
examples Category 
(GO code) training testing 1 testing 2 
autophagy 
(GO:0006914) 12/12 11/2940 11/11 
cell organisation 
(GO:0016043) 379/379 378/1839 378/378 
cell cycle 
(GO:0007049) 226/226 225/2298 225/225 
intracellular 
protein transport 
(GO:0006886) 
135/135 134/2571 134/134 
ion homeostasis 
(GO:0006873) 37/37 37/2864 37/37 
meiosis 
(GO:0007126) 45/45 44/2841 44/44 
metabolism 
(GO:0008152) 1118/370 1117/370 370/370 
signal  
transduction 
(GO:0007165) 
68/68 68/2771 68/68 
sporulation (sc) 
(GO:0007151) 27/27 27/2894 27/27 
response to 
stress 
(GO:0006950) 
91/91 91/2702 91/91 
transport 
(GO:0006810) 284/284 284/2123 284/284 
  
Table 1. Classification categories and the number 
of examples in the training and the testing sets 
                                                           
2 The January 2003 release of the GO ontology was used. A 
similar classification scheme was used in (Raychaudhuri et al, 
2002). 
Features have been generated according to the 
methods explained in Section 3 (Table 2 shows the 
number of features generated). As indicated earlier, 
the experiments have been performed by using ei-
ther all features or by selecting only those that ap-
peared in at least two documents. As a rule, there 
were no significant differences in the classification 
performance between the two.  
 
feature no. of all features 
no. of features 
appearing in >1 docs 
words 160k 60k 
lemmas 150k 54k 
stems 140k 50k 
terms 127k 62k 
  
Table 2. The number of features generated 
 
To evaluate the classification performance we 
have firstly generated precision/recall plots for 
each class. In the majority of classes, terms have 
demonstrated the best performance (cf. Figures 1 
and 2). However, the results have shown a wide 
disparity in performance across the classes, de-
pending on the size of the training set. The classes 
with fairly large number of training entities (e.g. 
metabolism) have been predicted quite accurately 
(regardless of the features used), while, on the 
other hand, under-represented classes (e.g. sporu-
lation) performed quite modestly (cf. Figure 1).   
 
 
Figure 1. Precision/recall plots for some classes  
using words and terms  
 
Comparison between performances on different 
classes is difficult if the classes contain fairly dif-
ferent ratios of positive/negative examples in the 
testing sets, as it was the case in our experiments 
(see Table 1, column testing 1). Therefore, we re-
evaluated the results by selecting ? for each class ? 
the same number of positive and negative exam-
ples (see Table 1, column testing 2), so that we 
could compare relative performance across classes. 
The results shown in Figure 2 actually indicate 
which classes are ?easier? to learn (only the per-
formance of single-words and terms are presented). 
To assess the global performance of classifica-
tion methods, we employed micro-averaging of the 
precision/recall data presented in Figure 2. In mi-
cro-averaging (Yang, 1997), the precision and re-
call are averaged over the number of entities that 
are classified (giving, thus, an equal weight to the 
performance on each gene). In other words, micro-
average shows the performance of the classifica-
tion system on a gene selected randomly from the 
testing set. 
The comparison of micro-averaging results for 
words, lemmas and stems has shown that there was 
no significant difference among them. This out-
come matches the results previously reported for 
the document classification task (Leopold and 
Kindermann, 2002), which means that there is no 
need to pre-process documents. 
Figure 3 shows the comparison of micro-
averaging plots for terms and lemmas. Terms per-
form generally much better at lower recall points, 
while there is just marginal difference between the 
two at the higher recall points. Very high precision 
points at lower recall mean that terms may be use-
ful classification features for precise predictions 
for genes classified with the highest confidence. 
 
 
 
Figure 2. Precision/recall plots for the 11 classes using words and terms  
(horizontal lines indicate the performance of a random classifier) 
 
Figure 3. Micro-averaging plot for 11 classes using 
lemmas and terms 
 
The results obtained by combining terms and 
words have not shown any improvements over us-
ing only terms as classification features. We be-
lieve that adding more features has introduced 
additional noise that derogated the overall per-
formance of terms. 
Finally, Figure 4 presents the comparison of 
classification results using terms and abstract iden-
tifiers. Although PMIDs outperformed terms, we 
reiterate that ? while other features allow learning 
more general properties that can be applied on 
other corpora ? PMIDs can be only used to classify 
new terms that appear in a closed training/testing 
corpus. 
 
 
 
Figure 4. Micro-averaging plot for 11 classes using 
PMIDs and terms 
 
 
5 Conclusion 
Due to an enormous number of terms and the 
complex and inconsistent structure of the biomedi-
cal terminology, manual update of knowledge re-
positories are prone to be both inefficient and 
inconsistent (Nenadic et al, 2002b; Stapley et al, 
2002). Therefore, automatic text-based classifica-
tion of biological entities (such as gene and protein 
names) is essential for efficient knowledge man-
agement and systematic approach that can cope 
with huge volume of the biomedical literature. Fur-
thermore, classified terms irrefutably have a posi-
tive impact on improving the results of IE/IR, 
knowledge acquisition, document classification 
and terminology management (Blaschke et al, 
2002).  
In this paper we have examined the procedures 
for engineering text-based features at various lev-
els of linguistic pre-processing, and considered 
their impacts on the performance of an SVM-based 
gene name classifier. The experiments have shown 
that simple linguistic pre-processing (such as lem-
matisation and stemming) does not have significant 
influence on the performance, i.e. there is no need 
to pre-process documents. Also, reducing the fea-
ture space by selecting only features that appear in 
more documents does not result in decrease of the 
performance, but can significantly reduce the time 
needed for training. PMID-based classification has 
shown very good performance, but a PMID-based 
classifier can be applied only on the training set of 
documents. 
The experiments have also shown that using 
semantic indicators (represented by dynamically 
extracted domain-specific terms) can improve the 
performance compared to the standard bag-of-
words approach, in particular at lower recall 
points, and for rare classes. This means that terms 
can be used as reliable features for classifying 
genes with higher confidence, and for under-
represented classes. However, terminological 
analysis requires considerable pre-processing time.  
Our further research will focus on generating 
the biological interpretation and justification of the 
classification results by using terms (that have 
been used as key distinguishing features for classi-
fication) as semantic indicators of the correspond-
ing classes.  
 
References 
 
M. Ashburner, et al. 2000. Gene Ontology: Tool for the 
Unification of Biology. Nature, 25:25-29.  
C. Blaschke, L. Hirschman and A. Valencia. 2002. In-
formation Extraction in Molecular Biology. Briefings 
in Bioinformatics, 3(2):154-165. 
N. Collier, C. Nobata and J. Tsujii. 2001. Automatic 
Acquisition and Classification of Terminology Using 
a Tagged Corpus in the Molecular Biology Domain. 
Journal of Terminology, John Benjamins. 
F. Eisenhaber and P. Bork. 1998. Wanted: Subcellular 
Localization of Proteins Based on Sequences. Trends 
Cell Biology, 8(4):169-170. 
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic 
Recognition of Multi-Word Terms: the C-value/NC-
value Method. International Journal on Digital Li-
braries 3(2):115-130. 
GENIA project. 2003. GENIA resources. Available at:  
http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/ 
V. Hatzivassiloglou, P. Duboue and A. Rzhetsky. 2001. 
Disambiguating Proteins, Genes, and RNA in Text: A 
Machine Learning Approach. Bioinformatics, 1(1):1-
10. 
T. Jenssen, A. Laegreid, J. Komorowski and E. Hovig. 
2001. A literature Network of Human Genes for 
High-throughput Analysis of Gene Expressions. Na-
ture Genetics, 28: 21-28. 
T. Joachims. 1998. Text Categorization with Support 
Vector Machines: Learning Many Relevant Features. 
Proceedings of 10th European Conference on Ma-
chine Learning, Springer-Verlag, Heidelberg, 137-
142. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named 
Entity Recognition. Proceedings of the Workshop 
NLP in Biomedicine, ACL 2002. 
E. Leopold and J. Kindermann. 2002. Text Categoriza-
tion with Support Vector Machines. How to Repre-
sent Texts in Input Space? Machine Learning, 
46:423-444. 
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini 
and C. Watkins. 2002. Text Classification using 
String Kernels. Journal of Machine Learning Re-
search, 2:419-444. 
D. Maynard and S. Ananiadou. 2000. Identifying Terms 
by their Family and Friends. Proceedings of 
COLING 2000, Saarbrucken, Germany, 530-536.  
 
 
K. Nishikawa and T. Ooi. 1982. Correlation of the 
Amino Acid Composition of a Protein to its Struc-
tural and Biological Characters. Journal of Bio-
chemistry (Tokyo), 91(5):1281-1824. 
G. Nenadic, I. Spasic and S. Ananiadou. 2002a. Auto-
matic Acronym Acquisition and Term Variation 
Management within Domain-Specific Texts. Proceed-
ings of LREC-3, Las Palmas, Spain, 2155-2162.  
G. Nenadic, H. Mima, I. Spasic, S. Ananiadou and J. 
Tsujii. 2002b. Terminology-based Literature Mining 
and Knowledge Acquisition in Biomedicine. Interna-
tional Journal of Medical Informatics, 67(1-3):33-48. 
NLM, National Library of Medicine. 2003. Medline. 
Available at http://www.ncbi.nlm.nih.gov/PubMed/ 
M. Porter. 1980: An Algorithm for Suffix Stripping. Pro-
gram, 14(1):130-137. 
S. Raychaudhuri, J. Chang, P. Sutphin and R. Altman. 
2002. Associating Genes with Gene Ontology Codes 
Using a Maximum Entropy Analysis of Biomedical 
Literature. Genome Research, 12:203-214. 
B. Stapley and G. Benoit. 2000. Bibliometrics: Informa-
tion Retrieval and Visualization from Co-occurrence 
of Gene Names in Medline Abstracts. Proceedings of 
the Pacific Symposium on Bio-computing, PSB 2000 
B. Stapley, L. Kelley and M. Sternberg. 2002. Predict-
ing the Sub-Cellular Location of Proteins from Text 
Using Support Vector Machines. Proceedings of the 
Pacific Symposium on Bio-computing, PSB 2002. 
A. Ushioda. 1996. Hierarchical Clustering of Words. 
Proceedings of COLING 96. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer Verlag, Heidelberg. 
A. Voutilainen and J. Heikkila. 1993. An English Con-
straint Grammar (ENGCG) a Surface-Syntactic 
Parser of English. In Fries, U et al (Eds.): Creating 
and Using English Language Corpora, Rodopi, Am-
sterdam/Atlanta, 189-199. 
A. Yakushiji, Y. Tateisi, Y. Miyao and J. Tsujii. 2001. 
Event Extraction From Biomedical Papers Using a 
Full Parser. Proceedings PSB 2001, Hawaii, USA, 
408-419. 
Y. Yang. 1997. An Evaluation of Statistical Approaches 
to Text Categorization. Information Retrieval, 
1(1/2):69-90. 
Morpho-syntactic Clues for Terminological Processing in Serbian 
Goran Nenadi? 
Department of Computing 
UMIST, UK 
G.Nenadic@umist.ac.uk
Irena Spasi? 
Computer Science 
University of Salford, UK 
I.Spasic@salford.ac.uk
Sophia Ananiadou 
Computer Science  
University of Salford, UK 
S.Ananiadou@salford.ac.uk
 
 
Abstract 
In this paper we discuss morpho-syntactic 
clues that can be used to facilitate termi-
nological processing in Serbian. A 
method (called SRCE) for automatic ex-
traction of multiword terms is presented. 
The approach incorporates a set of ge-
neric morpho-syntactic filters for recogni-
tion of term candidates, a method for 
conflation of morphological variants and 
a module for foreign word recognition. 
Morpho-syntactic filters describe general 
term formation patterns, and are imple-
mented as generic regular expressions. 
The inner structure together with the 
agreements within term candidates are 
used as clues to discover the boundaries 
of nested terms. The results of the termi-
nological processing of a textbook corpus 
in the domains of mathematics and com-
puter science are presented.  
1 Introduction 
An overwhelming amount of textual information 
presented in newswire, scientific literature, legal 
texts, etc., makes it difficult for a human to effi-
ciently localise the information of interest. In 
particular, it is doubtful that anybody could proc-
ess such huge amount of information without an 
automated help, especially when the information 
content spans across domains. The amount of e-
documents and their fuzzy structure require 
effective tools that can help users to 
systematically gather and make use of the 
information encoded in text documents. For these 
reasons, different text and/or literature mining 
techniques have been developed recently (e.g. 
(Hearst et al, 2000; Grobelnik et al, 2000)) in 
order to facilitate efficient discovery of knowl-
cient discovery of knowledge contained in large 
scientific or legal text collections. The main goal 
is to retrieve the knowledge ?buried? in a text 
and to present it to users in a digested form.  
The discovery (and transfer) of knowledge re-
lies heavily on the identification of relevant con-
cepts, which are linguistically represented by 
domain specific terms. Terms represent the most 
important notions in a domain and characterise 
documents semantically, and thus should be used 
as a basis for sophisticated knowledge acquisi-
tion. Still, few text-mining systems incorporate 
deep and dynamic terminology processing, al-
though there is an increasing amount of new 
terms that represent newly created concepts in 
rapidly developing fields. Existing term diction-
aries and standardised terminologies offer only a 
partial solution, as they are almost never up-to-
date. Although naming conventions do exist for 
some types of concepts (e.g. gene and protein 
names in biomedicine), these are only guidelines 
and as such do not impose restrictions to domain 
experts, who frequently introduce ad-hoc terms. 
Thus, the lack of clear naming conventions 
makes the automatic term recognition (ATR) task 
difficult even for languages that are not morpho-
logically and derivationally rich.  
ATR tools have been developed for English 
(Frantzi et al, 2000), French (Jacquemin, 2001), 
Japanese (Nakagawa and Mori, 2000), etc. Some 
methods rely purely on linguistic information, 
namely morpho-syntactic features of term candi-
dates (Ananiadou, 1994). Hybrid approaches 
combining linguistic patterns and statistical 
measures (e.g. (Frantzi et al, 2000)) and ma-
chine-learning techniques (e.g. (Hatzivassiloglou 
et al, 2001)) have been also used.  
However, few studies have been done for 
morphologically rich Slavic languages. For ex-
ample, Vintar (2000) presented two methods for 
extraction of terminological collocations in order 
to assist the translation process in Slovene. The 
statistical approach was based on the mutual ex-
pectation and LocalMax measures, and involved 
collocation extraction from raw text. The ex-
tracted collocations were filtered with a stop-
word list, and only collocations containing sin-
gle-word terms (devised previously by bilingual 
alignment) were accepted as relevant. In another 
approach, she used regular expression patterns to 
extract term collocations from a morpho-
syntactically tagged corpus. However, these pat-
terns are too general, and consequently not all 
extracted phrases were terminologically relevant.  
In this paper we discuss automatic terminology 
recognition in Serbian, in particular, the extrac-
tion of multiword terms, which are very frequent1 
in certain domains (e.g. natural sciences, mathe-
matics, etc.). Since Serbian is a highly inflective 
and morphologically and derivationally rich lan-
guage, morpho-syntactic clues are indispensable 
in the ATR process. Our hybrid approach (called 
SRCE ? Serbian C-value) combines morpho-
syntactic features of term candidates and statisti-
cal analysis of their occurrences in text. In addi-
tion, since terms appear in texts in many different 
forms due to their morphological and derivational 
variations, the necessity of taking these variations 
into account becomes particularly apparent. 
Therefore, the SRCE method incorporates generic 
morpho-syntactic patterns, a term normalisation 
approach and a foreign word detection method.  
The paper is organised as follows: in Section 2 
we present an overview of the core term extrac-
tion method, called the C-value method. In Sec-
tion 3 we discuss morpho-syntactic clues, the 
normalisation approach and the foreign word 
recognition that are used for singling out terms in 
Serbian. The experiments and evaluation are de-
scribed in Section 4. 
 
2 Automatic Term Recognition: the core 
C-value method 
Our approach to ATR is based on the C-value 
method (Frantzi et al, 2000), which extracts 
multi-word terms. It is a general term recognition 
approach in the sense that it is not limited to spe-
cific classes of concepts. The approach is hybrid: 
the method combines linguistic knowledge (term 
                                                           
1 In English, more than 85% of domain-specific terms are 
multi-words (Nakagawa and Mori, 2000).  
formation patterns) and statistical analysis. Lin-
guistic knowledge is used to single out term can-
didates, while their statistical features are used to 
measure the likelihood of term candidates being 
?real? terms. The method uses a POS tagged text 
as input, and outputs a list of extracted terms 
ranked according to their termhoods. Termhood 
is a numeric estimation of the degree to which a 
given linguistic unit (a multiword compound) is 
related to a domain-specific concept. However, 
the values are not normalised in the sense that a 
multiword, having a termhood value 10, is 10 
times more likely to be a term than a term candi-
date with a termhood value 1.  
In general, the C-value method enhances the 
commonly used baseline method that extracts 
most frequent term candidates (assuming that 
termhoods directly correspond to frequencies of 
occurrence) by making it sensitive to a particular 
type of terms ? nested terms2.   
The method is implemented as a two-step pro-
cedure. In the first step, term candidates are ex-
tracted using a set of morpho-syntactic filters, 
which describe general term formation patterns in 
a given language. As a rule, terms form a proper 
subset of noun phrases (NPs). For example, a set 
of general filters for English may include the fol-
lowing patterns:3 
 
Noun+ Noun  
(Adj | Noun)+ Noun  
(Adj | Noun)+| ((Adj | Noun)* Prep?) (Adj | Noun)* Noun  
 
Although these patterns are regular expressions, 
the filters are implemented as unification-like 
LR(1) rules (Mima et al, 1995) in order to facili-
tate processing of grammatical agreements (if 
any) within term candidates.  
For each term candidate extracted by a filter, a 
set of nested term candidates is generated (see 
Table 1 for an example in English). The proce-
dure for the generation of nested term candidates 
is implemented via transformation rules for each 
morpho-syntactic filter that is used to extract 
                                                           
2 For example, nuclear receptor is a nested term in hormone 
nuclear receptor. Similarly, baza podataka (Engl. database) 
is a nested term in a?uriranje baze podataka (Engl. update of 
database).  
3 Noun, Adj and Prep denote POS tags that correspond to 
nouns, adjectives and prepositions respectively. These filters 
were used for ATR from newswire corpora and in biomedi-
cine (Frantzi et al, 2000; Nenadi? et al, 2002).  
term candidates. The main indicator that a nested 
term candidate might be a real term is that it also 
appears on its own in the corpus. 
 
Term Term candidate: 
      steroid hormone receptor factor + 
Nested term candidates: 
steroid hormone receptor 
hormone receptor factor 
steroid hormone 
hormone receptor  
receptor factor 
 
+ 
- 
+ 
+ 
- 
Table 1: Nested term candidates 
 
In the second step, the term candidates are as-
signed termhoods (referred to as C-values) ac-
cording to a statistical measure. The measure 
amalgamates four numerical corpus-based char-
acteristic of a candidate term, namely the fre-
quency of occurrence, the frequency of occurring 
as nested within other candidate terms, the num-
ber of candidate terms inside which the given 
candidate term is nested, and the number of 
words contained in the candidate term. Formally,  
where a denotes a term candidate, f(a) corre-
sponds to its frequency, |a| denotes the number of 
words in a, and Ta is a set of terms that contain 
term a as a nested term. Term candidates are 
ranked according to their C-values, and terms 
whose C-values are higher than a chosen thresh-
old are presented as terms. 
Evaluation of the C-value method for English 
has shown that using additional statistical infor-
mation (frequency of ?nestedness?) improves the 
precision with slight loss on recall (Frantzi et al, 
2000). Also, systematic term normalisation may 
further improve precision and recall of the 
method (Nenadi? et al, 2002). 
3 Morpho-syntactic clues for extraction 
of terms in Serbian 
In order to adjust the core C-value method for 
Serbian, we have defined an appropriate set of 
morpho-syntactic filters and rules for inflectional 
normalisation of term candidates, and, addition-
ally, a module for foreign word recognition.  
3.1 Term formation patterns 
As a rule, the vast majority of multiword terms in 
Serbian match the following general formation 
pattern:4 
 
(1)           (Adj | ProAdj | Num | Noun )+ Noun 
 
which has been used for recognition of NPs in 
Serbian (Nenadi? and Vitas, 1998a). Of course, 
not all NPs that follow this pattern are terms.5 
Moreover, when applied to an initially POS 
tagged text6, this pattern may be too general even 
for description of NPs, as not all word sequences 
in a text that match this pattern are valid NPs. For 
example, in a sequence koji se naziva relacioni 
model (Engl. which is called the relational 
model), a word naziva can be initially tagged ei-
ther as a noun naziv (Engl. name) or a verb na-
zivati (Engl. call), although, in this sentence, only 
the latter is correct. Thus, without further POS 
disambiguation, the string naziva relacioni model 
follows the pattern (1), although it is not a valid 
NP. This means that classical regular expressions 
are not sufficient for the representation of such 
constraints, and that we need more expressive 
means to model constraints related to the NP 
structure and agreements of multiword constitu-
ents on case, number and gender. We used the 
notion of generic patterns as an extension of 
regular expressions (Nenadi? and Vitas, 1998b). 
For example, a generic pattern 
 
(2)      Adj.x1y1z1  Noun.x1y1z1   Adj.x2y2g   Noun.x2y2g 
 
models obligatory agreements that each NP from 
a specific class has to fulfil: both first and second 
pairs of adjectives and nouns must have the same 
values for certain morphological features (i.e. 
values for gender, number and case denoted by xi, 
                                                           
4 ProAdj and Num denote possessive adjectives and numbers 
respectively. 
5 For example, ovaj na?in (Engl. this way), veliki deo (Engl. 
large part), etc. This is a reason why we need additional 
processing to recognise semantically relevant NPs. 
6 Initially (or lexically) tagged POS text is a text in which 
every word occurrence is associated with all of its possible 
lexical and grammatical interpretations. The initial POS 
tagging is intrinsically ambiguous as each word is analysed 
separately, without considering neighbouring words (Ne-
nadi? and Vitas, 1998a). Thus, as a result of initial tagging, a 
lot of lexical ambiguities arise resulting in highly ambiguous 
word sequences. See Section 4 for further discussion. 
? ? ? 
? ? ? 
? 
? 
? 
= ? ? ?
otherwise                                   
        )), ( | | 
1 )( ( | | log 
nested, not   is                ), ( | | log 
) ( 2 
2 
a a Tb b fT a f a
a af a
a value C 
yi and zi respectively), while these values may be 
different for each respective pair. The last adjec-
tive and noun are ?frozen? in the genitive case 
(g), while the case (z1) in the first pair is ?free?. 
By defining generic patterns one can model the 
agreements within various lexical structures in a 
highly inflective language such as Serbian (Ne-
nadi? and Vitas, 1998b). As a result, these 
agreements can be used to detect the boundaries 
of the structures in questions. 
A set of generic patterns has been used to 
model the most frequent term formation patterns 
in Serbian. The set is mainly based on patterns 
used to model NPs in Serbian. Table 2 presents 
some of them. First four patterns describe NPs 
containing a nested NP whose lexical properties 
(such as case and/or number) are invariant in all 
inflected forms of the host NP. As a rule, the fro-
zen part is in genitive. Depending on NP con-
stituents, some agreements are obligatory within 
frozen part (see, for example, the third pattern ? 
agreements between an adjective and the corre-
sponding noun), or not (see the fourth pattern ? 
no necessary agreement between the last two 
nouns in gender, number). The fifth pattern (Ta-
ble 2) corresponds to NPs that do not have in-
variant parts. 
 
Generic patterns Examples 
1 N1   N gen baza podataka nejednakost trougla 
2 A1   N1   N gen manipulativni aspekt modela grani?na vrednost niza 
3 N1   A gen  N gen operacija prirodnog spajanja niz realnih brojeva 
4 N1   N 2;gen N gen integritet baze podataka kriterijum konvergencije niza 
5 A1+  N1 pro?ireni relacioni model  kompletan metri?ki  prostor 
Table 2: Frequent term formation patterns7 
 
While these patterns are used to single out 
term candidates from an initially tagged text, 
agreements within NPs are used to generate pos-
sible nested structures. While the rules for nested 
structures are more ?blurred? in English (since 
                                                           
7 In order to improve readability of filters, the generic pat-
terns in this table are encoded using the following syntax: A 
and N stand for Adj and Noun respectively, while X1 stands 
for X.x1y1z1 , Xgen stands for X.xyg and X2;gen stands for 
X.x2y2g (for X ? {A, N}). Also, invariant parts are underlined 
in the given examples. 
 
nouns are usually used as modifiers), ?nested-
ness? in Serbian has to preserve the necessary 
structure and inner agreements, which are spe-
cific for the NP class in question. Therefore, gen-
eration of nested term candidates depends on the 
type of host term candidates (consider examples 
in Table 3). Nested structures that are not them-
selves NPs are not considered as term candidates. 
 
     Nested term candidates  NP Term 
 
2 
 
manipulativni aspekt modela 
   manipulativni aspekt  
   aspekt modela 
+ 
+ 
+ 
+ 
- 
- 
 
3 
 
operacija prirodnog spajanja  
   operacija prirodnog 
   prirodnog spajanja 
+ 
- 
+ 
+ 
- 
+ 
 
4 
 
integritet baze podataka 
 integritet baze  
 baze podataka 
+ 
+ 
+ 
+ 
- 
+ 
 
5 
 
kompletan metri?ki prostor 
  kompletan metri?ki  
  metri?ki prostor 
+ 
- 
+ 
+ 
- 
+ 
Table 3: Nested term candidates (in Serbian) 
3.2 Conflating morphological variants 
If we aim at systematic recognition of terms, then 
handling term variation has to be treated as an 
essential part of terminology retrieval. Term 
variation ranges from simple orthographic (e.g. 
oestrogen ? estrogen, vitamin ? vitamine) and 
morphological variants (e.g. clone ? clones) to 
more complex semantic variation (e.g. eye sur-
gery ? ophthalmologic surgery).  
Several methods for term variation manage-
ment have been developed. For example, the 
BLAST system (Krauthammer et al, 2000) used 
approximate text string matching techniques and 
dictionaries to recognise spelling variations in 
gene and protein names. FASTR (Jacquemin, 
2001) handles morphological and syntactic varia-
tions by means of meta-rules used to describe 
term normalisation, while semantic variants are 
handled via WordNet.  
The necessity of taking term variants into ac-
count as part of ATR process becomes particu-
larly apparent in highly inflective languages. In 
Serbian, for example, the simplest morphological 
variations generally give rise to 14 possible vari-
ants of a single term (seven cases and two num-
bers (singular and plural) ? see Table 4). If the 
core C-value method were to be applied without 
conflating morphological variants, then term-
hoods would be distributed across different mor-
phological variants providing separate 
frequencies for individual variants instead of a 
single frequency calculated for a term candidate 
unifying all of its variants. In addition, the ?nest-
ing? factor of the C-value method would cause 
skewed results, since the case property of nested 
terms does not have normal distribution. Namely, 
as indicated previously (see Table 2), the major-
ity of nested terms in Serbian are in genitive case, 
which means that the termhood for a term candi-
date in genitive case would differ significantly 
from its counterparts in other cases. Moreover, 
this deviation cannot be remedied later by sum-
ming up individual termhoods, since C-value is 
not an additive measure. Hence, in order for the 
C-value method to be applied correctly in a 
highly inflective language, term candidates must 
be (at least inflectionally) normalised prior to the 
calculation of termhoods.  
 
Canonical form: 
operacija prirodnog spajanja (nom. sing. = ns) 
 
Morphological variants: 
operacija prirodnog spajanja (ns;gp) 
operacije prirodnog spajanja (gs;np;ap;vp) 
operaciji  prirodnog spajanja (ds;ls) 
operaciju prirodnog spajanja (as) 
operacijo prirodnog spajanja (vs) 
operacijom prirodnog spajanja (is) 
operacijama prirodnog spajanja (dp;ip;lp) 
Normalised form: 
    operacija (ns) prirodno (nsm) spajanje (ns)  
 
Table 4: Variants and normalisation of term 
candidates ? an example for term operacija prirod-
nog spajanja (Engl. natural join operation) 
 
Our approach to morphological normalisation 
of term variants is based on the normalisation of 
individual term constituents. Namely, each word 
that is a part of a term candidate is mapped onto 
its lemma, and term candidates are treated as se-
quences of lemmas. At the end of the ATR proc-
ess, terms are converted into their canonical form 
(singular, nominative case), which is not neces-
sarily identical to the normalised form (the se-
quence of the corresponding singular words in 
singular, nominative case). The normalisation 
process is illustrated in Table 4. 
At this point, the usage of generic patterns in 
order to check the agreements in case, number 
and gender during the phase of filtering of term 
candidates might seem unnecessary, since all 
these features are subsequently normalised. How-
ever, in order to enhance the precision of the 
SRCE method, it is important for term candidates 
to be correctly recognised prior to the statistical 
analysis. This means that the necessary agree-
ments between NP constituents have to be 
checked. Once the term candidates are identified, 
they are normalised in order to make the most of 
the statistical part of the method.   
3.3 Foreign word detection 
Despite the efforts to rely mostly on Serbian vo-
cabulary when building a terminology, many of 
the terms used in specific scientific domains bor-
row some of their building blocks from lan-
guages other than Serbian at various levels. For 
example, at morphological level, foreign suffixes, 
mostly originating from Latin and Greek, are of-
ten ?preferred? to their Serbian counterparts in, 
for example, the biomedical domain, even when 
they are used to modify a root that is in fact Ser-
bian (e.g. amino-kiselina (Engl. amino acid)). 
Similarly, at lexical level, words of foreign origin 
are used to form multi-word terms (e.g. redun-
dantan atribut (Engl. redundant attribute)). This 
is particularly obvious in fairly recently expanded 
disciplines such as computer science, where, for 
many of the original terms used in English, it has 
not been simple to adapt new terms in Serbian. 
Consequently, many of the terms have been sim-
ply transcribed into Serbian or, even worse, they 
are still used in their original form. Not only do 
foreign words appear as ?valid? parts of terms, 
but they have also proved to be good indicators 
of terms. It is, thus, necessary to develop proce-
dures for their detection.  
In our approach, the recognition of foreign 
words has been integrated into the ATR process 
for Serbian. The following morphological fea-
tures are used to indicate occurrences of potential 
foreign words (Spasi?, 1996): 
 
? characters (e.g. x, y, q) that do not belong to 
Serbian graphemic system, 
? successive vowel occurrences, 
? exception to the palatalisation rule,   
? exception to the assimilation rules, 
? occurrence of atypical consonant bi/tri-grams  
? occurrence of bi-grams or tri-grams typical 
for other languages (especially Latin and 
English), and 
? foreign affixes. 
 
The words satisfying some of the above crite-
ria are not necessarily foreign words. The preci-
sion of these rules varies from one to another. For 
example, the first rule is the strongest indicator of 
the presence of foreign words, since the alpha-
betical system used is not Serbian. Other rules 
may be tuned to a certain extent in order to in-
crease their precision.  
Let us, for instance, consider the second rule. 
The successive usage of vowels is fairly frequent 
in Serbian, but the majority of such cases follow 
certain restrictions8 under which they can occur. 
Moreover, these restrictions can be described by 
regular expressions. Any other occurrence of 
successive vowels can be used to indicate a po-
tential foreign word. 
Foreign word detection has been incorporated 
into the ATR process in two ways: during the 
selection of term candidates and for the calcula-
tion of termhoods. First, it is used before the ini-
tial POS tagging process in order to locate 
foreign words, which are tagged accordingly. 
Otherwise, foreign words would be typically 
considered as unknown. As explained earlier, it is 
very likely for foreign words in Serbian scientific 
and technical texts to be related to domain-
specific concepts, and their mishandling would 
significantly decrease the recall of the ATR 
method. This information is used by the linguistic 
part of the SRCE-method, where we introduced a 
special category corresponding to foreign words.  
In the second step, that is - once the term can-
didates have been selected - the information 
about foreign origin is used to increase the term-
hood of term candidates containing such words. 
This time, foreign word recognition is used to 
improve the precision of the ATR method. 
                                                           
8 For example, verbs in the paste tense, masculine gender 
always end with a pair of vowels (e.g. ispitivao (Engl. exam-
ined)). Further, some adjectives in masculine gender (e.g. 
beo (Engl. white)), as well as some nouns in masculine gen-
der (e.g. smisao (Engl. sense)) also end with a pair of vow-
els. The usage of prefixes is another example where vowels 
may occur successively (e.g. za+ustaviti (Engl. to stop)). 
4 Experiments and discussion 
The preliminary ATR experiments were con-
ducted using the SRCE system on a corpus con-
taining samples from university textbooks in 
mathematics9 and computer science10 (altogether 
120k words). 
Texts were pre-processed, i.e. initially tagged, 
by a system of electronic dictionaries (e-
dictionaries) containing simple nominal words 
for Serbian (Vitas, 1993). E-dictionaries contain 
exhaustive description of morpho-syntactic 
characteristics and are used for lexical 
recognition and initial lemmatisation of words 
that occur in a text.  This process is realised by e-
dictionary look-up, which results in an initially 
tagged text: each textual word is associated with 
its lemma(s) and corresponding morpho-syntactic 
categories (tags) retrieved from the  
e-dictionary. In general, e-dictionaries cannot 
resolve lexical ambiguities that result from the 
fact that there is no one-to-one correspondence 
between word forms and their morpho-syntactic 
features. There are different methods to resolve 
ambiguities (e.g. cache-dictionaries or local 
grammars), but in our experiments no disam-
biguation techniques were applied.  
In order to extract a list of term candidates, the 
set of morpho-syntactic filters described in 3.1 
was applied to the initially tagged corpus. We 
performed two sets of experiments. 
In the first experiment, we did not use any 
stoplist to discard unwanted constituents of term 
candidates. For each term candidate, we gener-
ated a canonical form (nominative, singular), a 
morphologically normalised form (list of normal-
ised words comprising the term candidate) and a 
list of nested term candidates (see Table 3 for 
examples).  In the next step, C-values for term 
candidates were calculated using statistics based 
on occurrences of normalised forms, and all term 
candidates with C-values above an empirically 
chosen threshold were selected as terms.  
Table 5 gives some examples of the recognised 
terms. In order to calculate the precision, we ex-
                                                           
9 N. La?eti?, Matematika II/1, Nau?na knjiga, Beograd, 
1994 
10 G. Pavlovi?-La?eti?, Osnove relacionih baza podataka, 
Vesta - Matemati?ki fakultet, Beograd, 1996. We would like 
to thank the authors of both textbooks for giving us permis-
sion to use their texts for experiments. 
amined separately interval precisions in sub-
corpora in mathematical analysis and computer 
science (see Table 6). Intervals are sets of recog-
nised terms that are placed at certain positions 
within the list. For example, interval 1-50 con-
tains top 50 terms, while the interval over 150 
contains all terms whose positions in the list are 
above 150. Terms have been inspected by the 
first two authors, who are Serbian native speakers 
and are specialists in both computer science and 
mathematics. 
 
Term  C-value 
metri?ki prostor   
topolo?ki prostor 
otvoren skup      
normiran prostor  
Ko?ijev niz 
zatvoren skup  
vektorski prostor  
prirodan broj 
nejednakost trougla    
neprekidnost preslikavanja 
Hausdorfov topolo?ki prostor     
633.55 
175.13 
93.20 
88.00 
68.11 
59.20 
53.13 
44.41 
33.98 
28.02 
19.43 
Table 5: Top ranked terms in the domain of  
mathematical analysis 
 
Interval Mathematical analysis 
Computer 
science 
1 ? 50 98% 90% 
50 ? 100 88% 70% 
100 ? 150 52% 58% 
> 150 69% 68% 
Table 6: Precision of the ATR method  
(without the usage of a stoplist) 
 
In the first 50 terms for the domain of mathe-
matical analysis, there was only one false term 
candidate (specijalna klasa neprekidnih pres-
likavanja), which contained an ?unwanted? adjec-
tive specijalna (Engl. special). The reason for the 
significant drop in the precision in the second and 
third intervals is mainly the same: apart from few 
true negatives11, the majority of false term candi-
dates contained common ?unwanted? constitu-
ents, which are sampled in Table 7. The results 
for the computer science sub-corpus were slightly 
worse since the mathematical language seems to 
be more consistent and restricted. 
                                                           
11 Such as: toplo?ka ta?ka gledi?ta, kompletnost prostora igra, 
kod preslikavnja. 
In the second experiment, we used a stoplist 
containing the words detected as frequent 
?wrong? constituents in the previous experi-
ments. The results are summarised in Table 8. 
 
prozvoljan 
tra?en 
specijalan 
va?an 
odgovaraju?i 
definisan 
op?ti 
dokazan 
globalan 
jedinstven 
poznat 
veliki 
pojam 
specifi?nost 
svojstvo 
slu?aj 
posledica 
gledi?te 
Table 7: A sample of normalised stop-words 
 
Interval Mathematical analysis 
Computer 
science 
1 ? 50 100% 94% 
50 ? 100 92% 92% 
100 ? 150 80% 74% 
> 150 74% 70% 
Table 8: Precision of the ATR method 
 (with the usage of a stoplist) 
 
The majority of remaining errors originate 
from the ambiguous POS tagging (more than 
50%, problematic words being naziv(a), igra, 
kod, etc.). Since no further processing of text has 
been performed, another source of problems is 
the detection of boundaries of frozen parts in 
prepositional phrases (e.g. na osnovu (Engl. 
based on), u slu?aju (Engl. in the case of)), 
which may be resolved by using a set of corre-
sponding local grammars (Nenadi? and Vitas, 
1998b). In addition, for the computer science 
domain, some of the false terms were related to a 
specific application area (the text intensively 
used examples from a university information sys-
tem, so candidates such as zvanje nastavnika 
(Engl. lecturer position), godina studija (Engl. 
year of study), etc. were wrongly suggested as 
computer science terms). 
5 Conclusion 
In this paper we have presented an approach to 
automatic extraction of terminology in a morpho-
logically rich language, such as Serbian. Terms 
extracted automatically may be used as semantic 
indicators for a range of classic IR/IE tasks. 
The approach is hybrid: it combines morpho-
syntactic filters for extraction of term candidates, 
and statistical analysis that ranks term candidates 
according to their termhood.  
Extraction of term candidates is based on the 
recognition of proper NPs. In order to enhance 
both the precision and recall of the ATR method, 
it is inevitable to incorporate significant linguistic 
knowledge. Since describing NPs by means of 
regular expressions is not sufficient for modelling 
agreements between NP constituents, we have 
used generic morpho-syntactic patterns. Further, 
since not all NPs are terms that semantically 
characterise documents, we have used a statisti-
cal measure in order to estimate semantic signifi-
cance of term candidates. Also, once the term 
candidates are correctly identified, they are nor-
malised in order to make the most of the statisti-
cal part of the method. Term candidates 
suggested as terms by the statistical part of the 
SRCE method are finally mapped into the canoni-
cal form of the original term. 
The preliminary experiments show that the 
precision is in line with the results for English, 
and that for the top ranked terms the precision is 
well above 90%. The analysis of errors shows 
that the majority of them appear due to lexical 
ambiguity of the input text. Certainly, if the cor-
pora were lexically disambiguated, we would 
have better precision.  
In order to improve the recall, additional mor-
pho-syntactic filters need to be identified. In par-
ticular, we plan to study terms that contain 
prepositions, as this is a common formation pat-
tern in many domains. Further, the broader han-
dling of term variants (e.g. dialectic variants, 
acronyms, derivational variants) may also im-
prove both precision and recall. Currently we 
deal only with inflectional variants by mapping 
them to a canonical form. Term variants unifica-
tion and normalisation also provide a broader 
basis for further IR and IE tasks, as queries can 
be expanded by referring to a class of synony-
mous terms as opposed to a single term.  
 
References  
Ananiadou S. 1994.  Methodology for Automatic Term 
Recognition. In Proceedings of COLING-94, 
Kyoto, Japan 
Frantzi K.T., Ananiadou S. and Mima H. 2000. Auto-
matic Recognition of Multi-word Terms: the C-
value/NC-value Method. Int. J. on Digital Libraries, 
3/2, pp. 115-130. 
Grobelnik M., Mladeni? D. and Mili?-Frayling N. 
2000. Text Mining as Integration of Several Re-
lated Research Areas, KDD 2000 Workshop on 
Text Mining, Boston, USA 
Hatzivassiloglou V., Duboue P. and Rzetsky A. 2001. 
Disambiguating Proteins, Genes, and RNA in Text: 
A Machine Learning Approach. Bioinformatics, 
17/1, pp. S97-S106 
Hearst M. 2000. Text Mining Tools: Instruments for 
Scientific Discovery, in IMA Text Mining Work-
shop, Institute for Mathematics and its Applica-
tions, Minneapolis, USA, 2000 
Jacquemin C. 2001. Spotting and discovering terms 
through NLP. MIT Press, Cambridge MA, 378 p. 
Krauthammer M., Rzhetsky A., Morozov P. and 
Friedman C. 2000. Using BLAST for identifying 
gene and protein names in journal articles. Gene, 
259, pp. 245-252. 
Mima H., Ando K. and Aoe J. 1995: Incremental 
Generation of LR(1) Parse Tables. In Proceedings 
of NLPRS?95, Pacific-Rim Symp., Seoul, Korea 
Nakagawa H. and Mori T. 2000. Nested Collocation 
and Compound Noun for Term Recognition. Proc. 
of COMPUTERM 98, pp. 64?70 
Nenadi? G. and Vitas D. 1998a. Formal Model of 
Noun Phrases in Serbo-Croatian. BULAG 23, 
Universite Franche-Compte, Besan?on, France. 
Nenadi? G. and Vitas D. 1998b. Using Local Gram-
mars for Agreement Modelling in Highly Inflective 
Languages. In Proceedings of TSD 98. Masaryk 
University, Brno, pp. 91-96. 
Nenadi? G., Mima H., Spasi? I., Ananiadou S. and 
Tsujii J. 2002. Terminology-driven Literature Min-
ing and Knowledge Acquisition in Biomedicine. In-
ternational Journal of Medical Informatics, 1-16. 
Spasi? I. 1996. Automatic Foreign Words Recognition 
in a Serbian Scientific or Technical Text. In Pro-
ceedings of Standardisation of Terminology, Bel-
grade, Yugoslavia, 1996 
Vintar ?. 2000. Extracting Terms and Terminological 
Collocations from the ELAN Slovene-English Par-
allel Corpus. In Proceedings of the 5th EAMT 
Workshop, Ljubljana, Slovenia, 2000 
Vitas D. 1993. Mathematical Model of Serbo-
Croatian Morphology (Nominal Inflection). PhD 
thesis. Faculty of Mathematics, Belgrade. 
 
Design and Implementation of a Terminology-based 
Literature Mining and Knowledge Structuring System 
 
Hideki Mima 
School of Engineering 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 
113-0033, Japan  
mima@biz-model.t.u-tokyo.ac.jp 
Sophia Ananiadou 
School of Computing, Science and 
Engineering, University of Salford, 
Salford M5 4WT, UK 
National Centre for Text Mining 
S.Ananiadou@salford.ac.uk 
Katsumori Matsushima 
School of Engineering 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, 
Tokyo 113-0033, Japan  
matsushima@naoe.t.u-tokyo.ac.jp
 
 
Abstract 
The purpose of the study is to develop an 
integrated knowledge management system for the 
domains of genome and nano-technology, in 
which terminology-based literature mining, 
knowledge acquisition, knowledge structuring, 
and knowledge retrieval are combined. The system 
supports integrating different databases (papers 
and patents, technologies and innovations) and 
retrieving different types of knowledge 
simultaneously. The main objective of the system 
is to facilitate knowledge acquisition from 
documents and new knowledge discovery through 
a terminology-based similarity calculation and a 
visualization of automatically structured 
knowledge. Implementation issues of the system 
are also mentioned. 
Key Words: Structuring knowledge, knowledge 
acquisition, information extraction, natural 
language processing, automatic term recognition, 
terminology 
1. Introduction 
The growing number of electronically 
available knowledge sources (KSs) 
emphasizes the importance of developing 
flexible and efficient tools for automatic 
knowledge acquisition and structuring in 
terms of knowledge integration. Different 
text and literature mining techniques have 
been developed recently in order to facilitate 
efficient discovery of knowledge contained 
in large textual collections. The main goal of 
literature mining is to retrieve knowledge 
that is ?buried? in a text and to present the 
distilled knowledge to users in a concise 
form. Its advantage, compared to ?manual? 
knowledge discovery, is based on the 
assumption that automatic methods are 
able to process an enormous amount of 
texts. It is doubtful that any researcher 
could process such huge amount of 
information, especially if the knowledge 
spans across domains. For these reasons, 
literature mining aims at helping scientists 
in collecting, maintaining, interpreting and 
curating information. 
In this paper, we introduce a knowledge 
integration and structuring system (KISS) 
we designed, in which terminology-driven 
knowledge acquisition (KA), knowledge 
retrieval (KR) and knowledge visualization 
(KV) are combined using automatic term 
recognition, automatic term clustering and 
terminology-based similarity calculation is 
explained. The system incorporates our 
proposed automatic term recognition / 
clustering and a visualization of retrieved 
knowledge based on the terminology, which 
allow users to access KSs visually though 
sophisticated GUIs. 
2. Overview of the system  
The main purpose of the knowledge 
structuring system is 1) accumulating 
knowledge in order to develop huge 
knowledge bases, 2) exploiting the 
accumulated knowledge efficiently. Our 
approach to structuring knowledge is based 
on: 
z automatic term recognition (ATR) 
z automatic term clustering (ATC) as an 
ontology1 development 
z ontology-based similarity calculation 
z visualization of relationships among 
documents (KSs) 
One of our definitions to structuring 
knowledge is discovery of relevance between 
documents (KSs) and its visualization. In 
order to achieve real time processing for 
structuring knowledge, we adopt 
terminology / ontology-based similarity 
calculation, because knowledge  can also be 
represented as textual documents or 
passages (e.g. sentences, subsections) which 
are efficiently characterized by sets of 
specialized (technical) terms. Further details 
of our visualization scheme will be 
mentioned in Section 4. 
                                                   
1  Although, definition of ontology is domain-
specific, our definition of ontology is the 
collection and classification of (technical) terms 
to recognize their semantic relevance. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 83
The system architecture is modular, and 
it integrates the following components 
(Figure 1):  
- Ontology Development Engine(s) (ODE) ? 
components that carry out the automatic 
ontology development which includes 
recognition and structuring of domain 
terminology; 
- Knowledge Data Manager (KDM) ? stores 
index of KSs and ontology in a ontology 
information database (OID) and provides 
the corresponding interface; 
- Knowledge Retriever (KR) ? retrieves KSs 
from TID and calculates similarities 
between keywords and KSs. Currently, we 
adopt tf*idf based similarity calculation; 
- Similarity Calculation Engine(s) (SCE) ? 
calculate similarities between KSs 
provided from KR component using 
ontology developed by ODE in order to 
show semantic similarities between each 
KSs. Semantic clusters of KSs are also 
provided. 
- Graph Visualizer ? visualizes knowledge 
structures based on graph expression in 
which relevance links between provided 
keywords and KSs, and relevance links 
between the KSs themselves can be 
shown. 
Linguistic pre-processing within the 
system is performed in two steps. In the 
first step, POS tagging2, i.e. the assignment 
of basic parts of speech (e.g. noun, verb, 
etc.) to words, is performed. In the second 
step, an ontology development engine is 
used to perform ATR and ATC. We also 
used feature structure-based parsing for 
English and Japanese for linguistic filter of 
the ATR. 
 
                                                   
2 We use EngCG tagger[4] in English and 
JUMAN / Chasen morphological analyzers in 
Japanese. 
3. Terminological processing 
as an ontology development 
The lack of clear naming 
standards in a domain (e.g. 
biomedicine) makes ATR a non-
trivial problem [1]. Also, it typically 
gives rise to many-to-many 
relationships between terms and 
concepts. In practice, two problems 
stem from this fact: 1) there are 
terms that have multiple meanings 
(term ambiguity), and, conversely, 2) 
there are terms that refer to the 
same concept (term variation). Generally, 
term ambiguity has negative effects on IE 
precision, while term variation decreases IE 
recall. These problems point out the 
impropriety of using simple keyword-based 
IE techniques. Obviously, more 
sophisticated techniques, identifying groups 
of different terms referring to the same (or 
similar) concept(s), and, therefore, could 
benefit from relying on efficient and 
consistent ATR/ATC and term variation 
management methods are required. These 
methods are also important for organising 
domain specific knowledge, as terms should 
not be treated isolated from other terms. 
They should rather be related to one another 
so that the relations existing between the 
corresponding concepts are at least partly 
reflected in a terminology. 
Terminological processing in our system 
is carried out based on C / NC-value method 
[2,3] for ATR, and average mutual 
information based ATC (Figure 2). 
3.1. Term recognition 
The ATR method used in the system is 
based on the C / NC-value methods [2,3]. 
The C-value method recognizes terms by 
combining linguistic knowledge and 
statistical analysis. The method extracts 
multi-word terms3 and is not limited to a 
specific class of concepts. It is implemented 
as a two-step procedure. In the first step, 
term candidates are extracted by using a set 
of linguistic filters, implemented using a 
LFG-based GLR parser, which describe 
general term formation patterns. In the 
second step, the term candidates are 
assigned termhoods (referred to as C-values) 
according to a statistical measure. The 
measure amalgamates four numerical 
corpus-based characteristics of a candidate 
                                                   
3 More than 85% of domain-specific terms are 
multi-word terms [3]. 
Figure 1: The system architecture 
 Br
o
w
s
e
r 
G
U
I 
KSs 
PDF, Word, HTML, 
XML, CSV 
Data ReaderDocument 
Viewer 
Ontology Data 
Manager 
Knowledge 
Retriever 
Similarity 
Manager 
 
 ?????  
 
Similarity 
Calculation 
Engine 
Similarity 
Graph 
Visualizer 
     
Ontology
Development
Engine 
Summarizer 
Browser 
Interface 
Knowledge Data
Manager 
Ontology Information 
Database Database 
Similarity Processing Ontology Development 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology84
term, namely the frequency of occurrence, 
the frequency of occurrence as a substring 
of other candidate terms, the number of 
candidate terms containing the given 
candidate term as a substring, and the 
number of words contained in the 
candidate term. 
The NC-value method further improves 
the C-value results by taking into account 
the context of candidate terms. The relevant 
context words are extracted and assigned 
weights based on how frequently they 
appear with top-ranked term candidates 
extracted by the C-value method. 
Subsequently, context factors are assigned 
to candidate terms according to their co-
occurrence with top-ranked context words. 
Finally, new termhood estimations, referred 
to as NC-values, are calculated as a linear 
combination of the C-values and context 
factors for the respective terms. Evaluation 
of the C/NC-methods [3] has shown that 
contextual information improves term 
distribution in the extracted list by placing 
real terms closer to the top of the list. 
3.2. Term variation management 
Term variation and ambiguity are 
causing problems not only for ATR but for 
human experts as well. Several methods for 
term variation management have been 
developed. For example, the BLAST system 
[5] used approximate text string matching 
techniques and dictionaries to recognize 
spelling variations in gene and protein 
names. FASTR [6] handles morphological 
and syntactic variations by means of meta-
rules used to describe term normalization, 
while semantic variants are handled via 
WordNet. 
The basic C-value method has been 
enhanced by term variation management 
[2]. We consider a variety of sources from 
which term variation problems originate. In 
particular, we deal with orthographical, 
morphological, syntactic, lexico-semantic 
and pragmatic phenomena. Our approach 
to term variation management is based on 
term normalization as an integral part of 
the ATR process. Term variants  (i.e. 
synonymous terms) are dealt with in the 
initial phase of ATR when term candidates 
are singled out, as opposed to other 
approaches (e.g. FASTR handles variants 
subsequently by applying transformation 
rules to extracted terms). Each term variant 
is normalized (see table 1 as an example) 
and term variants having the same 
normalized form are then grouped into 
classes in order to link each term candidate 
to all of its variants. This way, a list of 
normalized term candidate classes, rather 
than a list of single terms is statistically 
processed. The termhood is then calculated 
for a whole class of term variants, not for 
each term variant separately. 
Table 1: Automatic term normalization 
Term variants  Normalised term 
human cancers 
cancer in humans 
human?s cancer 
human carcinoma 
} ?  human cancer 
3.3. Term clustering 
Beside term recognition, term clustering 
is an indispensable component of the 
literature mining process. Since 
terminological opacity and polysemy are 
very common in molecular biology and 
biomedicine, term clustering is essential for 
the semantic integration of terms, the 
construction of domain ontologies and 
semantic tagging.  
ATC in our system is performed using a 
hierarchical clustering method in which 
clusters are merged based on average 
mutual information measuring how strongly 
terms are related to one another [7]. Terms 
automatically recognized by the NC-value 
method and their co-occurrences are used 
as input, and a dendrogram of terms is 
produced as output. Parallel symmetric 
processing is used for high-speed clustering. 
The calculated term cluster information is 
encoded and used for calculating semantic 
similarities in SCE component. More 
precisely, the similarity between two 
individual terms is determined according to 
their position in a dendrogram. Also a 
commonality measure is defined as the 
number of shared ancestors between two 
terms in the dendrogram, and a positional 
 
P O S  ta g g e r  
A c r o n y m  r e c o g n i t io n  
C - v a lu e  A T R  
O r th o g r a p h ic  v a r i a n t s  
M o r p h o lo g ic a l  v a r i a n t s  
S y n ta c t i c  v a r i a n t s  
N C - v a lu e  A T R  
T e r m  c lu s te r in g   
X M L  d o c u m e n ts  i n c lu d in g  
t e r m  ta g s  a n d  t e r m  
v a r i a t io n /c l a s s  in f o r m a t io n  
I n p u t  d o c u m e n ts  
R e c o g n i t i o n  
o f  t e r m s  
S t r u c t u r in g  
o f  t e r m s  
Figure 2: Ontology development 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 85
measure as a sum of their distances from 
the root. Similarity between two terms 
corresponds to a ratio between 
commonality and positional measure.   
Further details of the methods and their 
evaluations can be referred in [2,3]. 
4. Structuring knowledge 
Literature mining can be regarded as a 
broader approach to IE/KA. IE and KA in 
our system are implemented through the 
integration of ATR, ATC, and ontology-
based semantic similarity calculation. 
Graph-based visualization for globally 
structuring knowledge is also provided to 
facilitate KR and KA from documents. 
Additionally, the system supports 
combining different databases (papers and 
patents, technologies and innovations) and 
retrieves different types of knowledge 
simultaneously and crossly. This feature 
can accelerate knowledge discovery by 
combining existing knowledge. For example, 
discovering new knowledge on industrial 
innovation by structuring knowledge of 
trendy scientific paper database and past 
industrial innovation report database can 
be expected. Figure 3 shows an example of 
visualization of knowledge structures in the 
domain of innovation and engineering. In 
order to structure knowledge, the system 
draws a graph in which nodes indicate 
relevant KSs to keywords given and each 
links between KSs indicates semantic 
similarities dynamically calculated using 
ontology information developed by our ATR 
/ ATC components. Since characterization 
for KSs using terminology is thought to be 
the most efficient and ultimate 
summarization to KSs, achieving a fast and 
just-in-time processing for structuring 
knowledge can be expected.  
5. Conclusion 
In this paper, we presented a system for 
literature mining and knowledge 
structuring over large KSs. The system is a 
terminology-based integrated KA system, in 
which we have integrated ATR, ATC, IR, 
similarity calculation, and visualization for 
structuring knowledge. It allows users to 
search and combine information from 
various sources. KA within the system is 
terminology-driven, with terminology 
information provided automatically. 
Similarity based knowledge retrieval is 
implemented through various semantic 
similarity calculations, which, in 
combination with hierarchical, ontology- 
 
Figure 3: Visualization 
based matching, offers powerful means for 
KA through visualization-based literature 
mining. 
Preliminary experiments we conducted 
show that the system?s knowledge 
management scheme is an efficient 
methodology to facilitate KA and new 
knowledge discovery in the field of genome 
and nano-technology[2]. 
Important areas of future research will 
involve integration of a manually curated 
ontology with the results of automatically 
performed term clustering. Further, we will 
investigate the possibility of using a term 
classification system as an alternative 
structuring model for knowledge deduction 
and inference (instead of an ontology). 
References 
[1] K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi, 
Toward information extraction: identifying 
protein names from biological papers, Proc. 
of PSB-98, Hawaii, 1998, pp. 3:705-716. 
[2] H. Mima, S. Ananiadou, G. Nenadic, ATRACT 
workbench: an automatic term recognition 
and clustering of terms, in: V. Matou?ek, P. 
Mautner, R. Mou?ek, K. Tau?er (Eds.) Text, 
Speech and Dialogue, LNAI 2166, Springer 
Verlag, 2001, pp. 126-133. 
[3] H. Mima, S. Ananiadou, An application and 
evaluation of the C/NC-value approach for 
the automatic term recognition of multi-word 
units in Japanese, Int. J. on Terminology 6/2 
(2001), pp. 175-194. 
[4] A. Voutilainen, J. Heikkila, An English 
Constraint Grammar (ENGCG) a surface-
syntactic parser of English, in: U. Fries et al 
(Eds.) Creating and Using English language 
corpora, Rodopi, Amsterdam, Atlanta, 1993, 
pp. 189-199. 
[5] M. Krauthammer, A. Rzhetsky, P. Morozov, 
C. Friedman, Using BLAST for identifying 
gene and protein names in journal articles, 
in: Gene 259 (2000), pp. 245-252. 
[6] C. Jacquemin, Spotting and discovering 
terms through NLP, MIT Press, Cambridge 
MA, 2001, p. 378. 
[7] A. Ushioda, Hierarchical clustering of words, 
Proc. of COLING ?96, Copenhagen, Denmark, 
1996, pp. 1159-1162. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology86
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 25?31, Detroit, June 2005. c?2005 Association for Computational Linguistics
A Machine Learning Approach to Acronym Generation
Yoshimasa Tsuruoka
 
 
CREST
Japan Science and Technology Agency
Japan
Sophia Ananiadou
School of Computing
Salford University
United Kingdom
tsuruoka@is.s.u-tokyo.ac.jp
S.Ananiadou@salford.ac.uk
tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii  

Department of Computer Science
The University of Tokyo
Japan
Abstract
This paper presents a machine learning
approach to acronym generation. We for-
malize the generation process as a se-
quence labeling problem on the letters in
the definition (expanded form) so that a
variety of Markov modeling approaches
can be applied to this task. To con-
struct the data for training and testing, we
extracted acronym-definition pairs from
MEDLINE abstracts and manually anno-
tated each pair with positional informa-
tion about the letters in the acronym. We
have built an MEMM-based tagger using
this training data set and evaluated the
performance of acronym generation. Ex-
perimental results show that our machine
learning method gives significantly bet-
ter performance than that achieved by the
standard heuristic rule for acronym gen-
eration and enables us to obtain multi-
ple candidate acronyms together with their
likelihoods represented in probability val-
ues.
1 Introduction
Technical terms and named-entities play important
roles in knowledge integration and information re-
trieval in the biomedical domain. However, spelling
variations make it difficult to identify the terms con-
veying the same concept because they are written
in different manners. Acronyms constitute a major
part of spelling variations (Nenadic et al, 2002), so
proper management of acronyms leads to improved
performance of the information systems in this do-
main.
As for the methods for recognizing acronym-
definition pairs from running text, there are many
studies reporting high performance (e.g. over 96%
accuracy and 82% recall) (Yoshida et al, 2000; Ne-
nadic et al, 2002; Schwartz and Hearst, 2003; Za-
hariev, 2003; Adar, 2004). However, another aspect
that we have to consider for efficient acronym man-
agement is to generate acronyms from the given def-
inition (expanded form).
One obvious application of acronym generation
is to expand the keywords in information retrieval.
As reported in (Wren et al, 2005), for example,
you can retrieve only 25% of the documents con-
cerning the concept of ?JNK? by using the key-
word ?c-jun N-terminal kinase?. In more than 33%
of the documents the concept is written with its
acronym ?JNK?. To alleviate this problem, some
research efforts have been devoted to constructing
a database containing a large number of acronym-
definition pairs from running text of biomedical doc-
uments (Adar, 2004).
However, the major problem of this database-
building approach is that building the database offer-
ing complete coverage is nearly impossible because
not all the biomedical documents are publicly avail-
able. Although most of the abstracts of biomedical
papers are publicly available on MEDLINE, there
is still a large number of full-papers which are not
available.
In this paper, we propose an alternative approach
25
to providing acronyms from their definitions so
that we can obtain acronyms without consulting
acronym-definition databases.
One of the simplest way to generate acronyms
from definitions would be to choose the letters at the
beginning of each word and capitalize them. How-
ever, there are a lot of exceptions in the acronyms
appearing in biomedical documents. The followings
are some real examples of the definition-acronym
pairs that cannot be created with the simple heuristic
method.
RNA polymerase (RNAP)
antithrombin (AT)
melanoma cell adhesion molecule (Mel-CAM)
the xenoestrogen 4-tert-octylphenol (t-OP)
In this paper we present a machine learning ap-
proach to automatic generation of acronyms in order
to capture a variety of mechanisms of acronym gen-
eration. We formalize this problem as a sequence
labeling task such as part-of-speech tagging, chunk-
ing and other natural language tagging tasks so that
common Markov modeling approaches can be ap-
plied to this task.
2 Acronym Generation as a Sequence
Labeling Problem
Given the definition (expanded form), the mecha-
nism of acronym generation can be regarded as the
task of selecting the appropriate action on each letter
in the definition.
Figure 1 illustrates an example, where the defini-
tion is ?Duck interferon gamma? and the generated
acronym is ?DuIFN-gamma?. The generation pro-
ceeds as follows:
The acronym generator outputs the first
two letters unchanged and skips the fol-
lowing three letters. Then the generator
capitalizes ?i? and skip the following four
letters...
By assuming that an acronym is made up of alpha-
numeric letters, spaces and hyphens, the actions be-
ing taken by the generator are classified into the fol-
lowing five classes.
  SKIP
The generator skips the letter.
  UPPER
If the target letter is uppercase, the generator
outputs the same letter. If the target letter is
lowercase, the generator coverts the letter into
the corresponding upper letter.
  LOWER
If the target letter is lowercase, the generator
outputs the same letter. If the target letter is
uppercase, the generator coverts the letter into
the corresponding lowercase letter.
  SPACE
The generator convert the letter into a space.
  HYPHEN
The generator convert the letter into a hyphen.
From the probabilistic modeling point of view,
this task is to find the sequence of actions 
that maximizes the following probability given the
observation 	
	  	 


	
 (1)
Observations are the letters in the definition and
various types of features derived from them. We de-
compose the probability in a left-to-right manner.






	



ffCombining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 761?768
Manchester, August 2008
Event Frame Extraction Based on a Gene Regulation Corpus 
Yutaka Sasaki 1    Paul Thompson 1    Philip Cotter 1    John McNaught 1, 2 
Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
Yutaka.Sasaki@manchester.ac.uk 
 
 Abstract 
This paper describes the supervised ac-
quisition of semantic event frames  based 
on a corpus of biomedical abstracts, in 
which the biological process of E. coli 
gene regulation has been linguistically 
annotated by a group of biologists in the 
EC research project "BOOTStrep". Gene 
regulation is one of the rapidly advancing 
areas for which information extraction 
could boost research. Event frames are an 
essential linguistic resource for extraction 
of information from biological literature.  
This paper presents a specification for 
linguistic-level annotation of gene regu-
lation events, followed by novel methods 
of automatic event frame extraction from 
text.  The event frame extraction per-
formance has been evaluated with 10-
fold cross validation.  The experimental 
results show that a precision of nearly 
50% and a recall of around 20% are 
achieved.  Since the goal of this paper is 
event frame extraction, rather than event 
instance extraction, the issue of low re-
call could be solved by applying the 
methods to a larger-scale corpus. 
1 Introduction 
This paper describes the automatic extraction of 
linguistic event frames based on a corpus of 
MEDLINE abstracts that has been annotated 
with gene regulation events by a group of do-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
main experts. Annotation is centred on both 
verbs and nominalised verbs that describe rele-
vant events. For each event, semantic arguments 
that occur within the same sentence are marked 
and labelled with semantic roles and named en-
tity (NE) types. 
The focus of the paper is the extraction of 
event frames on the basis of the annotated corpus 
using machine learning techniques. Event frames 
are linguistic specifications concerning the be-
haviour of verbs and nominalised verbs, in terms 
of the number and types of semantic arguments 
with which they typically co-occur in texts. Our 
eventual goal is to exploit such information to 
improve information extraction. Event frame ex-
traction is different to event instance extraction 
(or template filling). Our event frames are des-
tined for incorporation in the BOOTStrep 
BioLexicon to support identification of relevant 
event instances and  discovery of event instance 
participants by NLP systems. 
2 Background 
There are several well-established, large-scale 
repositories of semantic frames for general lan-
guage, e.g., VerbNet (Kipper-Schuler, 2005), 
PropBank (Palmer et al, 2005) and FrameNet 
(Rupenhoffer et al 2006). These all aim to char-
acterise verb behaviour in terms of the semantic 
arguments with which verbs occur but differ in 
how they represent semantic arguments and 
groupings of verbs.  
In VerbNet, the semantic roles of arguments 
come from frame-independent roles, e.g. Agent, 
Patient, Location and Instrument.  
In contrast, PropBank and FrameNet use a 
mixture of role types: some are common amongst 
a number of frames; others are specific to par-
ticular frames.  
Whilst FrameNet and VerbNet differ in their 
treatment of semantic roles, they both specify  
761
semantic frames that correspond to groups of 
verbs with similar behaviour. However, frames 
in PropBank correspond to individual verbs. 
   Biology-specific extensions have been at-
tempted both for PropBank (Wattarujeekrit et al, 
2004) and FrameNet (Dolbey et al, 2006). How-
ever, to our knowledge, there has been no such 
attempt at extending VerbNet into the biological 
domain. 
In common with VerbNet, our work is focus-
sed on producing event frames that use a set of 
frame-independent semantic roles. However, we 
adopt a smaller set of roles tailored to the domain. 
This use of frame-independent roles allows lin-
guistic generalisations to be captured more easily 
(Cohen and Hunter, 2006). Also, the use of such 
roles is more suitable for direct exploitation by 
NLP systems (Zaphirain et al, 2008).  
Unlike VerbNet, we aim to produce a set of 
frames that are verb-specific (rather than frames 
that apply to groups of verbs). Verb-specific 
frames are able to provide more detailed argu-
ment specifications?particularly important in 
the biomedical field, where phrases that identify 
information such as location, manner, timing and 
condition are essential for correct interpretation 
of events (Tsai et al 2007).  
3 Annotated corpus 
To aid semantic event frame extraction, we need 
a corpus annotated with event-level information.  
Several already exist for biology.  Some target 
extraction of PropBank-style frames (e.g. Chou 
et al (2006), Kulick et al (2004)). The corpus 
produced by Kim et al (2008) uses frame-
independent roles. However, only a few semantic 
argument types are annotated.  
The target of our event frame extraction is a 
set of semantic frames which specify all potential 
arguments of gene regulation events. For this 
purpose, we had to produce our own annotated 
corpus, using a larger set of event-independent 
semantic roles than Kim et al (2008). Our roles 
had to cover sufficiently wide scope to allow an-
notation and characterization of all instantiated 
arguments of relevant events within texts. To our 
knowledge, this makes our scheme unique within 
the biomedical field. 
In contrast to many other comparable re-
sources, annotated events are centred on both 
verbs and nominalised verbs, such as transcrip-
tion and control. Nominalised verbs play an im-
portant and possibly dominant role in biological 
texts (Cohen and Hunter, 2006). Our own corpus 
confirms this, in that the nominalised verb ex-
pression is the most commonly annotated word 
on which gene regulation events are centred. By 
annotating events centred on nominalised verbs 
in a similar way to verbs, it becomes possible to 
extract separate event frames for nominalised 
verbs. This enables their potentially idiosyncratic 
behaviour to be accounted for.  
Role Name Description Example (bold = semantic argument, italics = focussed verb)  
AGENT Drives/instigates event The narL gene product activates the nitrate reductase operon 
THEME a) Affected by/results from event 
b) Focus of events describing states 
recA protein was induced by UV radiation 
The FNR protein resembles CRP 
MANNER Method/way in which event is car-
ried out 
cpxA gene increases the levels of csgA transcription by dephosphoryla-
tion of CpxR 
INSTRUMENT Used to carry out event EnvZ functions through OmpR to control NP porin gene expression in 
Escherichia coli K-12. 
LOCATION Where complete event takes place Phosphorylation of OmpR modulates expression of the ompF and ompC 
genes in Escherichia coli 
SOURCE Start point of event A transducing lambda phage was isolated from a strain harboring a 
glpD??lacZ fusion  
DESTINATION End point of event Transcription of gntT is activated by binding of the cyclic AMP (cAMP)-
cAMP receptor protein (CRP) complex to a CRP binding site 
TEMPORAL Situates event in time w.r.t another 
event 
The Alp protease activity is detected in cells after introduction of plas-
mids carrying the alpA gene 
CONDITION Environmental conditions/changes 
in conditions 
Strains carrying a mutation in the crp structural gene fail to repress ODC 
and ADC activities in response to increased cAMP 
RATE Change of level or rate marR mutations elevated inaA expression by  10-  to 20-fold over that of 
the wild-type. 
DESCRIPTIVE-
AGENT 
Provides descriptive information 
about the AGENT of the event 
It is likely that HyfR acts as a formate-dependent regulator of the hyf 
operon 
DESCRIPTIVE-
THEME 
Provides descriptive information 
about the AGENT of the event 
The FNR protein resembles CRP. 
PURPOSE Purpose/reason for the event occur-
ring 
The fusion strains were used to study the regulation of the cysB gene by 
assaying the fused lacZ gene product 
Table 1. Semantic Roles 
762
Our annotated corpus consists of 677 MED-
LINE abstracts on E. Coli. Within them, a total 
of 4770 gene regulation events have been anno-
tated. 
3.1 Semantic Roles 
Based on the observations of Tsai et al(2007) 
regarding the most important types of informa-
tion specified for biomedical events, together 
with detailed examination of a large number of 
relevant events within our corpus, in discussion 
with biologists, we defined a set of 13 frame-
independent semantic roles that are suitable for 
the domain.   
 Certain roles within the set are domain-
independent, and are based on those used in 
VerbNet, e.g. AGENT, THEME, and LOCA-
TION. To these, we have added a number of do-
main-dependent roles, e.g. CONDITION and 
MANNER. The size of the role set attempts to 
balance the need for a sufficiently wide-ranging 
set of roles with the need for one that is as small 
and general as possible, to reduce the burden on 
annotators, whilst also helping to ensure consis-
tency across extracted verb frames. The full set 
of semantic roles used is shown in Table 1.  
3.2  Named Entity Categorisation 
 Although our semantic roles are rather general, 
the annotation scheme allows more detailed in-
formation about semantic arguments to be en-
coded in the corpus through the assignment of 
named entity (NE) tags. Unlike other corpus pro-
jects, we do not annotate all entities within each 
abstract, but just those entities that occur as se-
mantic arguments of annotated gene regulation 
events. 
Our set of NE tags goes beyond the traditional 
view of NEs,  in that labelling is extended to in-
clude events represented by nominalised verbs 
(e.g. repression). A total of 61 NE classes have 
been defined as being relevant to the gene regu-
lation field, which are divided into four entity-
specific super-classes (DNA, PROTEIN, EX-
PERIMENTAL and ORGANISMS) and one 
event-specific super-class (PROCESSES). The 
NEs within each of these classes are hierarchi-
cally-structured. Table 2 provides definitions of 
each of these five super-classes. The NEs corre-
spond to classes in the Gene Regulation Ontol-
ogy (Splendiani et al 2007), which has been de-
veloped as part of the BOOTStrep project in 
which this work has been carried out. The Gene 
Regulation Ontology integrates parts of other 
established bio-ontologies, such as Gene Ontol-
ogy (Ashburner et al, 2000) and Sequence On-
tology (Eilbeck,2005). 
3.3 Annotation process 
Annotation was carried out over a period of three 
months by seven PhD students with experience 
in gene regulation and with native or near-native 
competence in English. 
 Prior to annotation, each abstract was auto-
matically processed. Firstly, linguistic pre-
processing (i.e. morphological analysis, POS 
tagging and syntactic chunking)1 was carried out.  
 Secondly, all occurrences from a list of 700 
biologically relevant verbs were automatically 
marked. Annotators then considered each marked 
verb within an abstract. If the verb denoted a 
gene regulation event, annotators then: 
a. Identified all semantic arguments of the 
verb within the sentence 
b. Assigned a semantic role to each identi-
fied argument 
                                                 
1 Each abstract to be annotated is first pre-processed with 
the GENIA tagger (Tsuruoka et al 2005). 
NE class Definition 
DNA 
Entities chiefly composed of nucleic 
acids and their structural or positional 
references. This includes the physical 
structure of all DNA-based entities 
and the functional roles associated 
with regions thereof. 
PROTEIN 
Entities chiefly composed of amino 
acids and their positional references. 
This includes the physical structure 
and functional roles associated with 
each type. 
EXPERIMENTAL 
Both physical and methodological 
entities, either used, consumed or 
required for a reaction to take place. 
ORGANISMS 
Entities representing individuals or 
collections of living things and their 
component parts. 
PROCESSES A set of event classes used to label biological processes described in text.  
Table 2. Description of NE super-classes  Table 3. Most commonly annotated verbs and 
nominalised verbs 
Word Count Type 
expression 409 NV 
encode 351 V 
transcription 125 NV 
bind 110 V 
require 100 V 
express 93 V 
regulate 91 V 
synthesis 90 NV 
contain 80 V 
induce 78 V 
763
c. If appropriate, assigned named entity 
categories to (parts of) the semantic ar-
gument span 
d. If the argument corresponded to a nomi-
nalised verb, repeated steps a?c to iden-
tify its own arguments. 
Syntactic chunks were made visible to annota-
tors. In conjunction with annotation guidelines, 
the chunks were used to help ensure consistency 
of annotated semantic arguments. For example, 
the guidelines state that semantic arguments 
should normally consist of complete (and pref-
erably single) syntactic chunks.  The annotation 
was performed using a customised version of 
WordFreak (Morton and LaCivita, 2003), a Java-
based linguistic annotation tool.  
3.4  Corpus statistics 
The corpus is divided into 2 parts, i.e. 
1) 597 abstracts, each annotated by a single 
annotator, containing a total of 3612 
events, 
2) 80 pairs of double-annotated documents, 
allowing checking of inter-annotator 
agreement and consistency, and contain-
ing 1158 distinct events.  
 
 In the corpus, 277 distinct verbs were annotated 
as denoting gene regulation events, of which 73 
were annotated 10 times or more. In addition, 
annotation has identified 135 relevant nominal-
ised verbs, of which 22 were annotated 10 times 
or more. The most commonly annotated verbs 
and nominalised verbs are shown in Table 3.  
3.5 Inter-annotator agreement 
Inter-annotator agreement statistics for the 80 
pairs of duplicate-annotated abstracts are shown 
in Table 4.  
The figures shown in Table 4 are direct 
agreement rates. Whilst the Kappa statistic is 
very familiar for calculating inter-annotator 
agreement, we follow Wilbur et al (2006) and 
Pyysalo (2007) in choosing not to use it, because 
it is not appropriate or possible to calculate it for 
all of the above statistics. For instance: 
 
1. For some tasks, like annotation of events and 
arguments spans, deciding how to calculate 
random agreement is not clear. 
2. The Kappa statistic assumes that annotation 
categories are discrete and mutually exclu-
sive. This is not the case for the NE catego-
ries, which are hierarchically structured.   
 
 Table 4 shows that, in terms of identifying 
events  (i.e. determining which verbs denote gene 
regulation events), agreement between annotators 
is reached about half the time. The main reason 
for this relatively low figure is that reaching a 
consensus on the specific types of events to be 
annotated under the heading of ?gene regulation? 
required a large amount of discussion. Thus, par-
ticularly towards the start of the annotation phase, 
annotators tended to either under- or over-
annotate the events. 
Greater amounts of consistency seem to be 
achievable for other sub-tasks of the annotation, 
with agreement rates for the identification and 
subsequent labelling of semantic arguments be-
ing achieved in around three quarters of cases.  
Comparable, but slightly lower rates of agree-
ment were achieved in the identification of NEs. 
In terms of assigning categories to them, the 
agreement rate for exact category matches is a 
little lower (62%). However, if we relax the 
matching conditions by exploiting the hierarchi-
cal structure of the NE categories (i.e. if we 
count as a match the cases where the category 
assigned by one annotator was the ancestor of the 
category assigned by the other annotator), then 
the agreement increases by around 11%.  
The large number of NE categories (61), 
makes the decision of the most appropriate cate-
gory rather complex; this was verified by the an-
notators themselves. Based on this, we will con-
sider the use of a more coarse-grained scheme 
when carrying out further annotation of this type. 
However, in the current corpus, the hierarchical 
structuring of the NE categories means that it 
would be possible to use a smaller set of catego-
ries by mapping the specific categories to more 
general ones.   
4 Corpus Format 
For the purposes of event frame extraction, the 
annotations in the corpus were converted to an 
XML-style inline format consisting of three dif-
ferent types of element: 
 
Table 4. Inter-annotator agreement rates  
AGREEMENT RATE VALUE 
Event identification 0.49 
Argument identification (partial span match) 0.73 
Semantic role assignment 0.78 
NE identification (partial span match) 0.68 
NE category assignment (exact) 0.62 
NE category assignment (including parent) 0.65 
NE category assignment (including ancestors) 0.73 
  
764
EVENT ? surrounds text spans (i.e. verb 
phrases and nominalised verbs) on which 
events are centred. 
SLOT ? surrounds spans corresponding to se-
mantic arguments (i.e. slots) of events.  The 
head verb/nominalised verb of the event is also 
treated as a SLOT, with role type Verb. The 
eventid attribute links each slot with its respec-
tive event, whilst the Role attribute indicates 
the semantic role assigned to the slot.  
NE ? surrounds text spans annotated as named 
entities. The cat attribute stores the NE cate-
gory assigned. 
 
Where there are several annotations over some 
text span, elements are embedded inside each 
other. If more than one annotation begins at a 
particular offset, then the ordering of the embed-
ding is fixed, so that SLOT elements are embed-
ded inside EVENT elements, and that NE ele-
ments are embedded inside SLOT elements. An 
example of the annotation for the sentence "TaqI 
restriction endonuclease has been subcloned 
downstream from an inducible phoA promoter" 
is shown below: 
 
<SLOT argid="4" eventid="5" Role="Theme">  
<NE cat="ENZYME">TaqI restriction endonucle-
ase</NE></SLOT> <EVENT id="5"> 
has been <SLOT argid="6" eventid="5" 
Role="Verb">subcloned </SLOT></EVENT>  
<SLOT argid="8" eventid="5" 
Role="Location">downstream from  
<NE cat="PROMOTER">an inducible phoA pro-
moter</NE></SLOT>. 
 
The EVENT created over the VP chunk has 
been subcloned has been annotated as having 2 
semantic arguments (SLOTs), i.e. a THEME,  
TaqI restriction endonuclease and a LOCATION, 
i.e. downstream from an inducible phoA pro-
moter. A 3rd SLOT element corresponds to the 
head verb in the VP chunk. Named entity tags 
have also been assigned to the THEME span and 
part of the LOCATION span.  
5 Event Patterns and Event Frames 
This section defines event patterns and event 
frames.  Event patterns are syntactic patterns of 
sequences of surface words, NEs, and semantic 
roles, whilst event frames are the record-like data 
structures consisting of event slots and event slot 
values. 
5.1 Event Patterns 
Event patterns are fragments of event annotations 
in which semantic arguments are generalized to 
their semantic role and NE categories, if present. 
An event pattern is extracted for each unique 
event id within an abstract. An event annotation 
span begins with the earliest SLOT span, and 
ends with the latest SLOT assigned to the event. 
An example event span is as follows: 
 
<SLOT eventid="9" Role="Agent">  
<NE cat="OPERON"> transfer operon</NE></SLOT> 
<EVENT id="9"><SLOT eventid="9" Role="Verb"> 
expression </SLOT></EVENT></SLOT> of  
<SLOT eventid="9" Role="Theme">  
<NE cat="DNA_FRAGMENT"> F-like plasmids 
</NE></SLOT> 
 
For each event, each event span is generalized 
into an event pattern as follows:  
? ?Verb? role slots of the event are converted 
into a tuple consisting of the role type, part-
of-speech and surface form, i.e., 
[Verb:POS:verb].  
? Other semantic role slots and their NE slots 
for the event are generalized to tuples con-
sisting of the role and NE super class, i.e., 
[role:NE_super_class]. 
? Other XML tags are removed. 
 
The above example event span is thus general-
ized to the following event pattern: 
 
[Agent:DNA] [Verb:NN:expression] of [Theme:DNA]. 
 
5.2 Event frames 
Event frames are directly extracted from event 
patterns, and take the following general form: 
 
event_frame_name( 
     slot_name => slot_value, 
     ? 
     slot_name => slot_value). 
where 
? event_frame_name is the base form of the 
event verb or nominalized verb; 
? slot_names are  the names of the semantic 
roles within the event pattern; 
? slot_values are NE categories, if present 
within the event pattern. 
 
For example, the event frame corresponding to 
the event pattern shown in the previous section is 
as follows: 
expression( Agent=>DNA, 
            Theme=>DNA ). 
 
765
6 Event Frame Extraction 
Our event frame extraction is a fusion of sequen-
tial labelling based on Conditional Random 
Fields (CRF), and event pattern matching. Event 
frames are extracted in three steps.  Firstly, a 
CRF-based Named Entity Recognizer (NER) 
assigns biological NEs to word sequences. Sec-
ondly, a CRF-based semantic role labeller deter-
mines the semantic roles of word sequences with 
NE labels.  Thirdly, word sequences are com-
pared with event patterns derived from the cor-
pus.  Only those event frames whose semantic 
roles, NEs, and verb POS satisfy event pattern 
conditions will be extracted. 
6.1 Biological NER  
Since it is costly and time-consuming to create a 
large-scale training corpus annotated by biolo-
gists, we need to concede to use coarse-grained 
biological NE categories. That is, the NER com-
ponent is trained on the five NE super classes, 
i.e., Protein, DNA, Experimental, Organisms, 
and Processes. 
The NER models are trained by CRFs 
(Lafferty et al, 2001) using the standard IOB2 
labelling method.  That is, the label ``B-NE'' is 
given to the first token of the target NE sequence, 
?I-NE? to each remaining token in the target se-
quence,  and ``O'' to other tokens. 
Features used are as follows: 
? word feature 
- orthographic features: 
 the first letter and the last four letters of the 
word form, in which capital letters in a word are 
normalized to ?A?, lower case letters are normal-
ized to ?a?, and digits are replaced by ?0?. For 
example, the word form ?IL-2? is normalised to 
?AA-0?. 
- postfix features:  the last two and four let-
ters 
? POS feature 
 
We applied first-order CRFs using the above fea-
tures for the tokens within a window size of  ?2 
of the current token. 
6.2 Semantic Role Labelling  
First of all, each NE token sequence identified by 
B and I labels is merged into a single token with 
the NE category name. Then, the semantic role 
labelling models are trained by CRFs in a similar 
way to NER.  That is, the label ``B-Role'' is given 
to the first token of the target Role sequence, ?I-
Role? to each remaining token in the target se-
quence, and ?O? to other tokens. 
Features used here are as follows: 
? word feature 
?  base form feature 
? POS feature 
? NE feature 
 
The window size was ?2 of the current token. 
6.3 Event pattern matching  
When a new sentence is given, sequential label-
ling models decide NE and semantic role labels 
of tokenized input sentences. Then, the token 
sequences are converted into the following token 
sequences with POS, semantic role, and NE in-
formation (called augmented token sequences): 
 
1. Each token sequence labelled by IOB seman-
tic role labels is merged into a token labelled 
with the role. 
2. Verbs and nominalized verbs are converted 
to [Verb:POS:surface_form]. 
3. Tokens with semantic role label and NE su-
per-class are converted into the form 
[Role:NE_super_class]. 
4. Other tokens with O label are converted to 
surface tokens. 
 
Then, event patterns are generalized: 
5. Event patterns are modified so that elements 
corresponding to verbs and nominalized 
verbs will match any words with the same 
POS, e.g., [Verb:POS:*]. 
 
Finally, each event pattern is applied to aug-
mented token sequences one by one:  
6. By matching the generalized event patterns 
with augmented token sequences, i.e. when 
verbs or nominalized verbs and the surround-
ing semantic roles and NEs satisfy the event 
pattern conditions, then successfully unified 
event patterns are extracted as new event pat-
terns. 
7. The newly obtained event patterns are con-
verted into event frames in the same way as 
described in Section 5.2.  
7 Experimental Results 
The aim of this section is to evaluate semantic 
frame extraction performance, given a set of an-
notated training data. 
The annotated corpus was randomly separated 
into 10 document groups and their event patterns 
766
and event frames were segmented into 10 groups 
according to the document separation. 
We conducted 10-fold cross validation based 
on the 10 document groups.  Named entity rec-
ognizers and semantic role labellers were trained 
using 9 groups of annotated documents.  Event 
frames were then extracted from the remaining 
group of documents.  Micro-average precision 
and recall for the set of event frames extracted 
from all the folds were evaluated. 
Table 5 shows the event frame extraction per-
formance.  #TP, #FN, and #FP indicate the num-
ber of true positives, false negatives, and false 
positives, respectively.   
Named entity recognition performance was 
also evaluated (Table 6).  Since the training data 
size is small, the performance is between ap-
proximately 20-60% F-measure. However, this 
will not cause a problem for the event frame ex-
traction task.  This is because, if a particular 
event frame occurs multiple times in a corpus, it 
is sufficient to extract only a single occurrence of 
the event description. So, whilst the NE and se-
mantic role labelling may not be successful for 
all occurrences of the event frame, there is a 
good chance that at least one occurrence of the 
event will be realized in the text in such a way as 
to allow the labelling to be carried out success-
fully, thus allowing the extraction of an appro-
priate event frame.  
8 Discussion 
Linguistic-level event annotation of biological 
events is an inherently difficult task.  This is 
supported by the fact that the inter-annotator 
agreement level for the identification of events 
was 0.49 (see Table 4).  Therefore, in terms of 
event extraction performance, a precision of 
49.0% on 10-fold cross validation is almost 
comparable to human experts. The low recall of 
18.6% may not be an issue, as the recall is likely 
to improve with the size of the target corpus.   
The precision may additionally be underesti-
mated in the evaluation due to inconsistencies in 
the annotation.  We found that the average preci-
sion of our event frame extraction over 10 folds 
is around 30%, despite the fact that the precision 
of all event frames extracted from 10 folds is 
almost 50% compared with the annotated event 
frames in the whole corpus.  This happens be-
cause some events not annotated in a particular 
fold are annotated in the rest of corpus.  From 
this insight, our conjecture is that the true preci-
sion against the whole corpus would be some-
what higher (potentially 70-80%) if we were us-
ing an annotated corpus 10 times larger for the 
evaluation. 
The automatic NER performance was also 
comparable to human annotators. 
There are several approaches to the generation 
of information extraction patterns (e.g. Soderland 
et al, 1995; Califf et al, 1997; Kim and Moldo-
van, 1995).  Our event patterns are similar to in-
formation extraction rules used in conventional 
IE systems.  However, the goal of this paper is 
not event instance extraction but event (or se-
mantic) frame extraction. We also combined 
CRF-based NER and semantic role labelling 
tuned for gene regulation with event extraction 
from sentences so that the clues of gene regula-
tion event frames could be assigned automati-
cally to un-annotated text. 
9 Conclusion  
This paper has presented linguistic annotation of 
gene regulation events in MEDLINE abstracts, 
and automatic event frame extraction based on 
the annotated corpus. Semantic event frames are 
linguistic resources effective in bridging between 
domain knowledge and text in IE tasks. 
Although biological event annotations carried 
out by domain experts is a challenging task, ex-
perimental results on event frame extraction 
demonstrate a precision of almost 50%, which is 
close to the inter-annotator agreement rate of 
human annotators. 
The extracted event frames will be included in 
the BOOTStrep BioLexicon, which will be made 
available for research purposes. 
Acknowledgement 
This research is supported by EC IST project 
FP6-028099 (BOOTStrep), whose Manchester 
team is hosted by the JISC/BBSRC/EPSRC 
sponsored National Centre for Text Mining. 
 
Table 5. 10-fold cross validation results 
 Score #TP #FN #FP 
Recall  0.186 165 730  
Precision 0.490 165  172 
 
Table 6.  NE identification performance 
NE Type Recall Precision F 
DNA 0.627  0.660  0.643  
Protein 0.525  0.633  0.574  
Experimental 0.224  0.512  0.312  
Processes 0.125  0.337  0.182  
Organisms 0.412  0.599  0.488  
 
767
References 
Califf, Mary E. and Raymond J. Mooney (1997).  
Relational Learning of Pattern-Match Rules for In-
formation Extraction, In Proceedings of the ACL-
97 Workshop in Natural Language Learning, pp 9?
15. 
Chou, Wen-Chi., Richard T.H. Tsai, Ying-Shan Su, 
Wei Ku, Ting-Yi Sung and Wen-Lian Hsu (2006). 
A Semi-Automatic Method for Annotating a Bio-
medical Proposition Bank. In Proceedings of the 
Workshop on Frontiers in Linguistically Annotated 
Corpora 2006, pp 5?12. 
Cohen, K. Bretonnel and Laurence Hunter (2006). A 
critical review of PASBio's argument structures for 
biomedical verbs. BMC Bioinformatics 7 (Suppl. 3), 
S5.  
Dolbey, Andrew, Michael Ellsworth and Jan 
Scheffczykx (2006). BioFrameNet: A Domain-
Specific FrameNet Extension with Links to Bio-
medical Ontologies. In O. Bodenreider (Ed.), In 
Proceedings of KR-MED, pp 87?94. 
Eilbeck, Karen, Suzanna .E Lewis., Christopher J. 
Mungall, Mark Yandell, Lincoln Stein, Richard 
Durbin and Michael Ashburner. (2005) The Se-
quence Ontology: A tool for the unification of ge-
nome annotations. Genome Biology 6:R44 
Kim, Jin-Dong,  Tomoko Ohta and Jun?ichi Tsujii 
(2008).  Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics 9:10.   
Kim, Jun-Tae and Dan I. Moldovan (1995).  Acquisi-
tion of Linguistic Patterns for Knowledge-Based 
Information Extraction. IEEE Transaction on 
Knowledge and Data Engineering (IEEE TKDE), 
7(5), pp.713?724.   
Kipper-Schuler, Karen (2005). VerbNet: A broad-
coverage, comprehensive verb lexicon. PhD Thesis. 
Computer and Information Science Dept., Univer-
sity of Pennsylvania. Philadelphia, PA. 
Kulick Seth, Ann Bies, Mark Liberman, Mark Mandel,  
Ryan McDonald, Martha Palmer, Andrew Schein, 
and Lyle Ungar  (2004) Integrated Annotation for 
Biomedical Information Extraction. In HLT-
NAACL 2004 Workshop: BioLink 2004, Linking 
Biological Literature, Ontologies and Databases, 
pp 61?68.  
Lafferty John, Andrew McCallum and Fernando 
Pereira (2001).  Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data. In Proceedings of the Eighteenth In-
ternational Conference on    Machine Learning 
(ICML-2001), pp 282?289.  
Morton, Thomas and Jeremy LaCivita (2003). Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pp 17?18. 
Palmer Martha, Paul Kingsbury and Daniel Gildea 
(2005). The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31(1), pp 71?106. 
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen and  Tapio 
Salakoski (2007). BioInfer: a corpus for informa-
tion extraction in the biomedical domain?.  BMC 
Bioinformatics 8:50. 
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L. 
Petruck, Christopher R. Johnson, and Jan  
Scheffczyk (2006).   FrameNet II: Extended The-
ory and Practice. Available online at 
http://framenet.icsi.berkeley.edu/ 
Soderland, Steven, David Fisher, Jonathan Aseltine 
and  Wendy Lenert (1995). CRYSTAL: Inducing a 
Conceptual Dictionary, In Proceedings of The 13th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-95). pp.1314?1319. 
The Gene Ontology Consortium. (2000). Gene Ontol-
ogy: tool for the unification of biology. Nature Ge-
netetics 25, pp 25?29. 
Tsai Richard T.H, Wen-Chi Chou, Ying-San Su, Yu-
Chun Lin, Chen-Lung Sung, Hong-Jie Dai, Irene 
T.H Yeh, Wei Ku, Ting-Yi Sung and Wen-Lian 
Hsu (2007). BIOSMILE: A semantic role labeling 
system for biomedical verbs using a maximum-
entropy model with automatically generated tem-
plate features, BMC Bioinformatics 8:325  
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii (2005). Developing a Robust 
Part-of-Speech Tagger for Biomedical Text, In Ad-
vances in Informatics - 10th Panhellenic Confer-
ence on Informatics, pp 382?392. 
Wattarujeekrit, Tuangthong, Parantu K. Shah and 
Nigel Collier (2004). PASBio: predicate-argument 
structures for event extraction in molecular biology, 
BMC Bioinformatics 5:155. 
Wilbur, W.John, Andrey Rzhetsky, and Hagit Shatkay 
(2006). New Directions in Biomedical Text Anno-
tations: Definitions. Guidelines and Corpus Con-
struction. BMC Bioinformatics. 7:356 
Zapirain, Be?at, Eneko Agirre, Llu?s M?rquez (2008). 
A Preliminary Study on the Robustness and Generali-
zation of Role Sets for Semantic Role Labeling. In 
Alexander F. Gelbukh (Ed.), Computational Linguis-
tics and Intelligent Text Processing, 9th International 
Conference, CICLing 2008. 
 
768
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513?1522,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Classifying Relations for Biomedical Named Entity Disambiguation
Xinglong Wang
??
Jun?ichi Tsujii
???
Sophia Ananiadou
??
?
School of Computer Science, University of Manchester, UK
?
National Centre for Text Mining, UK
?
Department of Computer Science, University of Tokyo, Japan
{xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Named entity disambiguation concerns
linking a potentially ambiguous mention
of named entity in text to an unambigu-
ous identifier in a standard database. One
approach to this task is supervised classifi-
cation. However, the availability of train-
ing data is often limited, and the avail-
able data sets tend to be imbalanced and,
in some cases, heterogeneous. We pro-
pose a new method that distinguishes a
named entity by finding the informative
keywords in its surrounding context, and
then trains a model to predict whether each
keyword indicates the semantic class of
the entity. While maintaining a compara-
ble performance to supervised classifica-
tion, this method avoids using expensive
manually annotated data for each new do-
main, and thus achieves better portability.
1 Introduction
While technology on named entity recognition
(NER) matures, many researchers in the field of
information extraction (IE) gradually shifted their
focus to more complex tasks such as named en-
tity disambiguation and relation extraction. Both
tasks are particularly important for biomedical text
mining, which concerns automatically extracting
facts from the exponentially growing biomedical
literature (Hunter and Cohen, 2006). One type of
facts is relations between biomedical named en-
tities, such as disease-drug relation, gene-disease
relation, protein-protein interaction (PPI), etc. To
automatically extract these facts, advanced natu-
ral language processing techniques such as parsing
have been adopted to analyse the syntactic and se-
mantic structure of text. The idea is that linguistic
structures between the interacting biological enti-
ties may have common characteristics that can be
exploited by similarity measures or machine learn-
ing algorithms. For example, Erkan et al (2007)
used the shortest path between two genes accord-
ing to edit distance in a dependency tree to de-
fine a kernel function for extracting gene interac-
tions. Miwa et al (2008) comparably evaluated a
number of kernels for incorporating syntactic fea-
tures, including the bag-of-word kernel, the subset
tree kernel (Moschitti, 2006) and the graph ker-
nel (Airola et al, 2008), and they concluded that
combining all kernels achieved better results than
using any individual one. Miyao et al (2008)
used syntactic paths as one of the features to train
a support vector machines (SVM) model for PPIs
and also discussed how different parsers and out-
put representations affected the end results.
Another crucial IE task is named entity disam-
biguation, which concerns grounding mentions of
named entities in text to unambiguous concepts as
defined in some standard dictionary or database.
For instance, given a search term Python, users
may like to see the results grouped into the fol-
lowing categories: a type of snake, a programming
language, or a film (Bunescu and Pas?ca, 2006).
One approach to such lexical disambiguation tasks
is supervised classification. However, such tech-
niques suffer from the knowledge acquisition bot-
tleneck, meaning that manually annotating train-
ing data is costly and can never satisfy the need by
the machine learning algorithms. In addition, su-
pervised techniques may not yield reliable results
when the distributions of the semantic classes are
different in the training and test datasets (Agirre
and Martinez, 2004; Koeling et al, 2005). For ex-
ample, on the task of word sense disambiguation,
a model trained on a dataset where the predom-
inant sense of the word star is ?heavenly body?,
may not work well on text mainly composed of
entertainment news. Such problems are also ma-
jor concerns when developing a system to disam-
biguate biomedical named entities (e.g., protein,
1513
gene, and disease), for which some researchers
rely on hand-crafted rules in addition to a small
amount of training data (Morgan and Hirschman,
2007; Hakenberg et al, 2008).
This paper proposes a new disambiguation
method that, instead of classifying each individual
occurrence of an entity, it classifies pair-wise re-
lations between the entity mention in question and
the ?cue words? in its adjacent context, where each
cue word is assumed to bear a semantic class. We
then select the cue word that has a positive rela-
tion with the entity, and pass its semantic tag to it.
While an individual entity mention may belong to
a large number of semantic classes, a relation can
only take one of two values: positive or negative,
hence transforming a complex multi-classification
problem into a less complicated binary classifica-
tion task. The remainder of the paper is organised
as follows: Section 2 proposes the disambigua-
tion method and Section 3 introduces the task of
disambiguating the model organisms of biomedi-
cal named entities. Section 4 describes in detail
our proposed method and also a number of base-
line systems for comparison purposes. Section 5
shows the evaluation results and discusses the ad-
vantages and drawback of our system, and we fi-
nally conclude in Section 6.
2 Disambiguation as Relation
Classification
The named entity disambiguation task is defined
as follows: given a mention of a named entity in
text, we automatically assign a semantic tag d to
it, where d ? D, and D is a pre-compiled dic-
tionary with |D| entries. When |D| is small, the
problem can be approached by supervised classi-
fication. For example, to determine whether an
occurrence of an entity is a protein, a gene or an
RNA, Hatzivassiloglou et al (2001) compared
performance of 3 supervised classification meth-
ods and reported results near the human agree-
ment rate. Nevertheless, when |D| is large (e.g.,
> 100), the performance of classification may de-
crease, especially when the distribution of d in
training dataset differs from that in the test set. In
other words, when |D| is large, named entity dis-
ambiguation becomes a multi-class classification
task on heterogeneous and imbalanced datasets,
which is challenging for a machine learning model
to learn to discriminate enough between the se-
mantic classes (Japkowicz, 2000).
We propose an alternative method for named
entity disambiguation. Intuitively, in the surround-
ing context of an ambiguous entity, one can of-
ten find ?cue words? that are informative indica-
tors of the entity?s semantic category. These cue
words are provided by authors to remind readers
the semantic identity of a named entity. For ex-
ample, in an article about protein p53, phrase ?hu-
man protein p53? may be mentioned, where both
human and protein contain semantic information
regarding p53: human indicates the model organ-
ism of p53, and protein suggests the type of this
entity. Such cue words may occur infrequently in
the training data, making it difficult for machine
learning classifiers to capture.
Our method exploits this observation. Given a
sentence, let E be the set of ?target? entities (e.g.,
p53) and W of the ?cue? words (e.g., human) that
co-occur in a sentence, we define a relation as a
pair r = ?e, w?, where e ? E and w ? W , and
r is a positive relation if e belongs to the semantic
class indicated by w, and is a negative one if not.
Then we can disambiguate e by accomplishing the
following steps: 1) identify W and build a set
of relations R = {?e, w
i
?|w
i
?W, i = 1, 2, .., n},
where n is the size of W ; and 2) classify every
r ? R and assign the semantic tag of w
j
to e such
that r
j
= ?e, w
j
? is positive. The first task can be
tackled by a dictionary lookup, or by an NER sys-
tem, if manually annotated data is available. The
second is essentially a binary relation classifica-
tion task, and in this work, we use an SVM model
exploiting bag-of-word and syntactic features.
3 Species Disambiguation
We show the performance of the proposed method
on a task of resolving one major source of am-
biguity in protein and gene entities: model or-
ganisms. Model organisms are species studied to
understand particular biological phenomena. Bi-
ological experiments are often conducted on one
species, with the expectation that the discover-
ies will provide insight into the workings of oth-
ers, including humans, which are more difficult
to study directly. From viruses, prokaryotes, to
plants and animals, there are dozens of organ-
isms commonly used in biological studies, such
as E. coli, Drosophila, Homo sapiens, and hun-
dreds more are frequently mentioned in biologi-
cal research papers. In biomedical articles, entities
of different species are commonly referred to us-
1514
ing the same name, causing great ambiguity. For
example, searching a protein sequence database,
RefSeq
1
with query ?tumor protein p53? resulted
in over 100 proteins, as the name is shared by
many organisms.
The importance of distinguishing model organ-
isms has been recognised by the community of
biomedical text mining. Chen et al (2005) col-
lected gene names from various source databases
and calculated intra- and inter-species ambigui-
ties. Overall, only 25 (0.02%) official symbols
were ambiguous within the organisms. However,
when official symbols from 21 organisms were
combined, the ambiguity increased substantially
to 21, 279 (14.2%) symbols. Hakenberg et al
(2008) showed that species disambiguation is one
of the most important steps for term normalisa-
tion and identification, which concerns automat-
ically associating mentions of biomedical enti-
ties in text to unique database identifiers (Mor-
gan et al, 2008). Also, the task of extracting
PPIs in the recent BioCreative Challenge II work-
shop (Hirschman et al, 2007) requires protein
pairs to be recognised and normalised, which in-
evitably involves species disambiguation.
More specifically, given a text, in which men-
tions of biomedical named entities are annotated,
a species disambiguation system automatically as-
signs a species identifier, as in a standard database
of model organisms, to every entity mention. The
types of biomedical named entities concerned in
this study are protein, gene, protein complex and
mRNA/cDNA, and we used identifiers from the
NCBI Taxonomy of model organisms.
2
The work
focuses on species disambiguation and assumes
that the entities are already identified. In practice,
an automated named entity recogniser (e.g., AB-
NER (Settles, 2005)) should be used before apply-
ing the systems.
4 Approaches
This section describes a number of approaches to
species disambiguation, highlighting the relation
classification method proposed in Section 2.
4.1 Heuristics Baselines
The cue words for species are words denoting
names of model organisms (e.g., mouse as in
1
http://www.ncbi.nlm.nih.gov/RefSeq
2
http://www.ncbi.nlm.nih.gov/sites/
entrez?db=taxonomy
phrase ?mouse p53?). Another clue is the pres-
ence of the species-indicating prefixes in gene and
protein names. For instance, prefix ?h? in en-
tity ?hSos-1? suggests that it is a human protein.
Throughout this paper, we refer to such cue words
(e.g., mouse, hSos-1) as ?species words?. Note
that a species ?word? may contain multiple tokens
(e.g., E. Coli).
We encoded this knowledge in a rule-based
species tagging system (Wang and Grover, 2008).
The system takes a 2-step approach. First, it marks
up species words in the document using a species-
word detection program,
3
which searches every
word in a dictionary of model organisms and as-
signs a species ID to the word if a match is found.
The dictionary was built using the NCBI taxon-
omy
4
and the UniProt controlled vocabulary of
species,
5
and in total it contains 420,224 species
words for 324,157 species IDs. When species
words are identified, we disambiguate an entity
mention using one of the following rules:
1. previous species word: If the word preceding an entity
is a species word, assign the species ID indicated by
that word to the entity.
2. species word in the same sentence: If a species word
and an entity appear in the same sentence, assign its
species ID to the entity. When more than one species
word co-occurs in the sentence, priority is given to the
species word to the entity?s left with the smallest dis-
tance. If all species words occur to the right of the en-
tity, take the nearest one.
3. majority vote: assign the most frequently occurring
species ID in the document to all entity mentions.
It is expected that the first rule would produce
good precision. However, it can only disam-
biguate the fraction of entities that happen to have
a species word to their immediate left. The second
rule relaxes the first by allowing an entity to take
the species indicated by its nearest species word
in the same sentence, which should increase recall
but decrease precision. Statistics from our dataset
(see Section 5.1) show that only 5.68% entities can
potentially be resolved by rule 1 and 22.16% by
rule 2, while the majority rule can tackle every en-
tity mention in the dataset.
3
The species word detector identifies the cue words and
was used in all the systems studied in this paper. We could
not properly evaluate the detector due to the lack of man-
ually annotated data. Its performance, however, would not
affect the comparative evaluation results, and improvement
to species word detection should increase the performance of
these disambiguation systems.
4
ftp://ftp.ncbi.nih.gov/pub/taxonomy/
5
http://www.expasy.ch/cgi-bin/speclist
1515
4.2 Supervised Classification
The disambiguation problem can be approached as
a classification task. Given an entity mention and
its surrounding context, a machine learning model
classifies the entity into one of the classes, where
each class corresponds to a species ID. We car-
ried out experiments with two classification meth-
ods: multi-class classification and one-class clas-
sification, where a maximum entropy model
6
was
used for the former and SVM-light
7
for the lat-
ter. In one-class classification, we trained a se-
ries of binary SVM classifiers, each constructing
a separating hyperplane that maximises the mar-
gin between the instances of one specific species
(i.e., the target class) and a set of randomly se-
lected instances of other species (i.e., the outlier
class). We used equal numbers of instances for
both classes in training. The following types of
features were used in both multi-class and one-
class experiments, where the values of n were
set empirically by cross-validation on the training
data:
? leftContext The n word lemmas to the left of the entity
(n = 200).
? rightContext The n word lemmas to the right of the
entity (n = 200).
? leftSpeciesIDs The n species IDs to the left of the entity
(with order, n = 5).
? rightSpeciesIDs The n species IDs to the right of the
entity (with order, n = 5).
? leftNouns The n nouns to the left of the entity (with
order, n = 2).
? leftAdjs The n adjectives to the left of the entity (with
order, n = 2).
? leftSpeciesWords The n species word forms to the left
of the entity (n = 5).
? rightSpeciesWords The n species word forms to the
right of the entity (n = 5).
? firstLetter The first character of the entity itself (e.g.,
?h? in hP53).
? documentSpeciesIDs All species IDs that occur in the
document in question.
? useStopWords filter out function words.
? useStopPattern filter out words consisting only of digits
and punctuation characters.
Feature selection was also carried out for the
one-class classification experiments. We com-
pared two feature selection methods that report-
edly work well on the task of text classification:
information gain (IG) (Yang and Pedersen, 1997)
6
http://homepages.inf.ed.ac.uk/
s0450736/maxent_toolkit.html
7
http://svmlight.joachims.org/
The ARG1 ARG1 ARG2
ARG1 ARG2ARG1
Drosophila Kip3 isorthologue of Klp67A.
Figure 1: Predicate argument structure (PAS).
and Bi-Normal separation (BNS) (Forman, 2003).
IG measures the decrease in entropy when the
feature is given vs. absent, and is defined as:
IG(Y |X) = H(Y ) ? H(Y |X) where H(Y ) is
the uncertainty about the value of Y (i.e., Y ?s en-
tropy), and H(Y |X) is Y ?s conditional entropy
given X . The BNS is defined as: |F
?1
(x) ?
F
?1
(y)|, where F
?1
is the standard Normal distri-
bution?s inverse cumulative probability function,
namely, z-score; x is the ratio between the number
of positive cases containing the feature in ques-
tion, and the total number of positive cases; and y
is the ratio between the number of negative cases
containing the feature, and the total number of
negative cases.
We computed a weight for each feature and then
ranked the features according to their weight, with
respect to each feature selection method. The top
10% features were used in training. Given a test
instance, the one-class classification method first
counts the species words in the document that the
instance appears in, and then applies in sequence
the binary models of each occurring species, start-
ing from the most frequent one. For example, if
a document contains 5 occurrences of human and
3 mouse, we first apply the human species model
to judge whether an entity mention is of human
species, and only if not, the mouse model was ap-
plied. The most-frequent species in the document
was used as backup when none of the binary mod-
els gives positive answers.
4.3 Relation Classification
4.3.1 Overview
As for the proposed relation classification method,
in the training phase, we first selected the sen-
tences in which an entity mention and a species
word co-occur, and constructed pair-wise entity-
species relations. We then assigned each relation a
binary label: a relation is positive if the species ID
inferred from the species word matches the gold-
standard species annotation on the entity, and is
negative otherwise. For example, for the sentence
shown in Figure 1, where Drosophila is a species
word, and Kip3 and Klp67A are proteins, relation
?Kip3, Drosophila? is a negative instance and the
1516
pair ?Klp67A, Drosophila? is a positive one.
8
For each relation, a vector of features were ex-
tracted. We followed the PPI extraction method
described in (Miyao et al, 2008), where two types
of features were used for a SVM classifier. The
first was bag-of-word features, i.e., the words be-
fore, between and after the pair of entities, where
the words were lemmatised. We added an ad-
ditional feature of the distance between the en-
tity and the cue word. The other type was syn-
tactic features obtained from parsers. For bag-
of-word features, a linear kernel was used, and
for syntactic ones, a subset tree kernel (Mos-
chitti, 2006) was adopted. The syntactic features
were represented in a flat tree format. Figure 2
shows such a feature for the negative instance
?Kip3, Drosophila? from Figure 1. Note that all
species words (e.g., Drosophila) were normalised
to ?SPECIESWORD?, and entities (e.g., Kip3) to
?ENTITY?, which not only reduces the noise in
the feature set, but also makes the model more
species-generic. From the training dataset (see
Section 5.1), 25, 413 relations were extracted, of
which 63.3% were positive.
(ENJU(noun arg1(SPECIESWORD orthologue))
(prep arg12(of orthologue))
(prep arg12(of ENTITY)))
Figure 2: A syntactic feature obtained from the ENJU
parser.
To identify the species of an entity in unseen
text, we first parsed the sentence, and then listed
all pairs of species words and entities as relations.
Having extracted the bag-of-word and syntactic
features from the instance, the trained model was
applied to judge whether each species-entity rela-
tion was positive. The entity mention in a positive
relation would be tagged with the ID indicated by
the species word, while the mentions in negative
relations would be left untagged. The next section
describes in detail how we extracted the syntactic
features from text.
4.3.2 Syntactic Features
Given a sentence, a natural language parser au-
tomatically recognises its syntactic structure and
outputs a parse tree, in which nodes represent
words or syntactic constituents. A path between
8
Orthologues are genes/proteins in different species but
have similar sequences. In this example it implies that
Klp67A is a Drosophila protein but Kip3 is not.
Parser Input Output
C&C POS-tagged GR
ENJU POS-tagged PAS
ENJU-Genia POS-tagged PAS
Minipar Sentence-detected Minipar
RASP Tokenised GR
Stanford POS-tagged SD
Stanford-Genia POS-tagged SD
Table 1: Parsers and their input and output format
a pair of nodes can be interpreted as a syntactic re-
lation between sentence units, which was proved
useful to infer biological relations (e.g., Airola et
al., 2008; Miwa et al, 2008).
We experimented with the following parsers
(summarised in Table 1):
? Dependency parsers identify one word as the head
of a sentence and all other words are either a depen-
dent of that word, or else dependent on some other
word that connects to the headword through a sequence
of dependencies. We used Minipar (Lin, 1998) and
RASP (Briscoe et al, 2006) for the experiments;
? Constituent-structured parsers split a sentence into
syntactic constituents such as noun phrases or verb
phrases. We used the Stanford parser (Klein and Man-
ning, 2003), and also a variant of the Stanford parser
(i.e., Stanford-Genia), which was trained on the GE-
NIA treebank (Tateisi et al, 2005) for biomedical text;
? Deep parsers aim to compute in-depth syntactic and
semantic structures based on syntactic theories such as
HPSG (Pollard and Sag, 1994) and CCG (Steedman,
2000). We used the C&C parser (Clark and Curran,
2007), ENJU (Miyao and Tsujii, 2008), and a variant
of ENJU (Hara et al, 2007) adapted for the biomedical
domain (i.e., ENJU-Genia);
There were a number of practical issues to con-
sider when using parsers for this task. Firstly, be-
fore parsing, the text needs to be linguistically pre-
processed, and the quality of this process has a sig-
nificant impact on parsers? performance. The pre-
processing steps include sentence boundary detec-
tion, tokenisation and part-of-speech (POS) tag-
ging, all of which can be tricky especially when
applied to biomedical text (Grover et al, 2003).
To avoid the noise that can be introduced in the
pre-processing steps and to concentrate on evalu-
ating the performance of the parsers, we used the
same pre-processing tools (Alex et al, 2008a)
9
whenever possible. The middle column in Ta-
ble 1 shows how the input text was linguisti-
cally pre-processed with respect to each parser.
A POS-tagged text implies that it was also sen-
tence boundary detected and tokenised Except for
9
These particular tools were chosen because they were
adopted to pre-process the ITI-TXM dataset, which we used
in our study.
1517
RASP and Minipar, all parsers took POS-tagged
text as input. RASP requires POS tags and punctu-
ation labels that were derived from the CLAWS-7
tagset,
10
whereas our dataset uses POS labels from
the Penn Treebank tagset (Marcus et al, 1994).
As RASP does not recognise the Penn tagset, we
used its build-in POS tagger. Minipar, on the other
hand, does not support input of tokenised or POS-
tagged text, and therefore took split sentences as
input.
Secondly, the output representations of the
parsers are different and we preferred a format
that depicts relations between words instead of
syntactic constituents. In total, 4 representations
were used: grammatical relation (GR) (Briscoe et
al., 2006), Stanford typed dependency (SD) (de
Marneffe et al, 2006), Minipar?s own representa-
tion (Lin, 1998), and ENJU?s predicate-argument
structure (PAS). All the above representations de-
fine relations of words in triples, where a depen-
dency triple (i.e., GR, SD and Minipar) consists
of head, dependent and relation, and a PAS triple
contains predicate, argument, and relation. Fig-
ure 1 shows a sentence parsed by ENJU in PAS
representation. The right-most column in Table 1
lists the output representation of each parser. A
syntactic path between an entity and a species
word was represented by a sequence of triples,
each following the order of head-dependent or
predicate-argument. These paths were used as
syntactic features for the SVM classifier.
4.4 Spreading Strategies
Except for the majority vote rule, the approaches
described in Sections 4.1 and 4.3 were expected
to yield low recall, because they can only detect
intra-sentential relations, and therefore only be ap-
plied to the entities having at least one species
word appearing in the same sentence.
Since our aim is to disambiguate as many entity
mentions as possible, we would like to ?spread?
the decisions from the disambiguated mentions to
their ?relatives? in the same document. We define
an entity mention e? as another mention e?s rela-
tive under either of the following conditions: a)
if e? has the same surface form with e; or, b) if
e? is an abbreviation or an antecedent of e, where
abbreviation/antecedent pairs were detected using
the algorithm described in (Schwartz and Hearst,
10
http://ucrel.lancs.ac.uk/claws7tags.
html
2003). Given the set of disambiguated mentions,
we then ?spread? their species IDs to their rela-
tives in the same document. After this process, the
mentions that do not have any disambiguated rela-
tives would still be missed by the system. In such
cases, we used a ?default? species, as determined
by the rule of majority vote (see Section 4.1).
5 Evaluation
5.1 Data and Ontology
The species disambiguation experiments were
conducted using the ITI-TXM corpus (Alex et al,
2008b), a collection of full-length biomedical re-
search articles manually annotated with linguistic
and biomedical information for developing auto-
matic information extraction systems. The cor-
pus contains two datasets covering slightly dif-
ferent domains: enriched protein-protein interac-
tion (EPPI) and tissue expression (TE). When-
ever possible, protein, protein complex, gene, and
mRNA/cDNA entities were tagged with NCBI
Taxonomy IDs, denoting their species, and it was
the species annotation that this study used.
The EPPI and TE datasets have different distri-
butions of species. The entities in EPPI belong to
118 species with human being the most frequent at
51.98%. In TE, the entities are across 67 species
and mouse is the most frequent at 44.67%.
11
The
inter-annotator agreement of species annotation on
EPPI and TE are 86.45% and 95.11%, respectively.
The species disambiguation systems were de-
veloped on the training portions of the EPPI and
TE corpora, each containing 221 articles, and eval-
uated on a dataset combining the development
test (DEVTEST) datasets of EPPI and TE, contain-
ing 58 and 48 articles, respectively. The com-
bined training dataset contains 96, 992 entity men-
tions belonging to 138 model organisms, while the
DEVTEST dataset contains 23, 118 entities of 54
species. The diversity of model organisms in this
corpus highlights the fact that a primary consid-
eration when developing a species disambiguation
system is its ability to distinguish a wide range of
species with minimal additional manual effort.
5.2 Results
5.2.1 Evaluation Metrics
The evaluation was carried out on the DEVTEST
dataset, and the systems are compared using av-
11
These figures were obtained from the training split of the
datasets.
1518
micro-avg. macro-avg.
Maxent 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
SVM 62.24 / 59.35 / 60.76 14.70 / 17.11 / 15.01
SVM (IG) 65.20 / 61.06 / 63.06 14.90 / 19.53 / 16.09
SVM (BNS) 43.61 / 42.63 / 43.11 11.99 / 10.05 / 9.34
Table 2: Evaluation results of the classification systems on
DEVTEST (precision/recall/F1-score, in %)
eraged precision, recall and F1 scores over all
species. In more detail, for each model organism
that appears in the DEVTEST dataset, we collect
two lists of entity mentions of that species: one
from the gold-standard DEVTEST dataset, and the
other from the output of a disambiguation system.
Then the list of system output is compared against
the gold-standard list to obtain precision, recall
and F1 score. For each system, the scores ob-
tained from all species are averaged using micro-
average and macro-average. The micro-average is
the mean of the summation of contingency metrics
for all model organisms, so that scores of the more
frequent species influence the mean more than
those of less frequent ones. The macro-average is
the mean of precision, recall, or F1 over all labels,
thus attributing equal weights to each species, and
measuring a system?s adaptability across different
model organisms.
5.2.2 Evaluation Results
First of all, Table 2 shows the results of the clas-
sification methods described in Section 4.2. The
multi-classification system using a maximum en-
tropy model (Maxent) yielded the highest overall
micro-averaged F1. Among the SVM-based sys-
tems, the one using IG feature selection achieved
better performance. In particular, it outperformed
the Maxent model in term of macro-averages. The
performance of the SVM model with BNS feature
selection is disappointing, perhaps because the oc-
currences of a feature in each instance are not nor-
mally distributed. As the Maxent system obtained
better results, it was used to compare with other
disambiguation systems.
Table 3 shows the results of a number of meth-
ods described in the previous sections. The meth-
ods are categorised into 4 groups: rule-based
baseline systems, a Maxent classification model,
relation-classification methods, and a hybrid sys-
tem. The difference between the relation classifi-
cation systems is the features adopted. Rel-Context
was trained on only bag-of-word and distance fea-
tures, whereas each other system also used syn-
tactic features provided by a specific parser. For
example, the Rel-RASP system identifies an entity?s
species by finding positive relations between the
entity and its neighbouring species words, using
features including bag-of-word, distance, and de-
pendency paths generated by RASP. The hybrid
system (Hbrd) ran the Rel-ENJU-Genia system on top
of the outcome of Maxent. When a conflict oc-
curs, the species ID is chosen by Rel-ENJU-Genia.
The idea is that the relation classification system
is more accurate than Maxent when it is applica-
ble, and hence would improve precision on dis-
ambiguating the species with few or no training
instances.
Without spreading (shown in the ?NO SPRD?
columns of Table 3), most of the rule-based and re-
lation classification systems only work on a subset
of DEVTEST, resulting in low recall: Rule-Sp works
on the small proportion of entities (5.68%) with a
preceding species word, while the other systems
only work on the collection of sentences contain-
ing at least one species word and one entity, which
covers 4.60% sentences and 22.16% entity men-
tions. Rule-Majority, Maxent, and Hbrd, on the other
hand, apply to all entity mentions, and therefore
they are only compared against the others when
spreading was applied.
The results shown in the ?NO SPRD? columns
can be viewed as a comparative evaluation of
the usefulness of the syntactic features supplied
by the parsers on this particular task. The rule-
based systems set high baselines: Rule-Sp pro-
duced good precision and Rule-SpSent achieved the
highest micro-averaged F1, thanks to its high
coverage, which is also an upperbound of recall
for the relation classification systems. Neverthe-
less, it is encouraging that the relation classifica-
tion systems obtained higher precision than Rule-
SpSent, which is important, considering the de-
cisions will be transfered to the untagged entity
mentions across the document. Indeed, as shown
in the SPRD columns in Table 3, most relation
classification systems outperformed the Rule-SpSent
baseline when spreading was used. The scores
of the systems using different parser outputs only
vary slightly. Rel-Context, on the other hand, sur-
passed others in terms of micro-averaged preci-
sion, while sacrificing micro-averaged recall and
macro-averaged scores.
Next, the SPRD columns in Table 3 show the re-
sults when the spreading rules were applied, which
1519
METHOD NO SPRD (micro-avg) NO SPRD (macro-avg) SPRD (micro-avg) SPRD (macro-avg)
Rule-Majority N/A N/A 66.14 / 61.99 / 64.00 16.76 / 21.75 / 18.08
Rule-Sp 88.96 / 5.02 / 9.51 33.77 / 8.55 / 10.18 66.96 / 63.41 / 65.13 28.25 / 30.65 / 27.00
Rule-SpSent 80.82 / 16.88 / 27.93 43.16 / 28.85 / 24.73 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Maxent N/A N/A 70.48 / 70.48 / 70.48 10.07 / 10.00 / 9.85
Rel-Context 90.04 / 3.71 / 6.13 15.23 / 4.45 / 4.90 67.34 / 63.22 / 65.21 22.65 / 26.42 / 23.10
Rel-C&C 82.79 / 16.14 / 27.02 43.97 / 29.56 / 25.60 66.59 / 63.64 / 65.08 32.29 / 33.20 / 29.14
Rel-ENJU 83.39 / 15.87 / 26.66 46.89 / 29.88 / 25.95 68.28 / 65.02 / 66.61 31.82 / 34.08 / 29.67
Rel-ENJU-Genia 83.54 / 15.74 / 26.49 44.13 / 29.93 / 25.78 68.91 / 65.45 / 67.13 32.00 / 34.87 / 30.21
Rel-Minipar 81.82 / 16.27 / 27.14 43.63 / 27.88 / 24.15 67.98 / 63.77 / 65.81 31.83 / 33.93 / 29.44
Rel-RASP 81.67 / 16.10 / 26.90 43.95 / 28.92 / 25.03 66.62 / 64.08 / 65.33 32.66 / 33.54 / 29.80
Rel-Stanford 82.75 / 16.10 / 26.95 44.05 / 29.49 / 25.92 66.81 / 63.81 / 65.28 32.67 / 33.03 / 29.45
Rel-Stanford-Genia 82.22 / 16.04 / 26.84 43.37 / 29.40 / 25.22 66.85 / 63.64 / 65.21 32.72 / 32.29 / 28.64
Hbrd N/A N/A 74.15 / 73.26 / 73.70 43.98 / 37.47 / 31.80
Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)
effectively improved recall (see Section 5.2.3
for discussion on statistical significance tests on
the results). The Maxent system achieved very
good micro-averaged precision, but low macro-
averaged scores. In fact, as shown in Table 4, Max-
ent can only disambiguate 7 species (out of a total
of 54) that have relatively large amount of train-
ing instances,
12
and failed completely on other
species. This suggests that Maxent may not be able
to generate good micro-averaged scores when ap-
plied to a dataset where the dominant species are
different from those in the training set. On the
other hand, the relation-classification approaches
have a clear advantage over Maxent as measured
by macro-averaged scores. As shown in Table 4,
Rel-ENJU-Genia worked well on most of the species,
displaying its good adaptability, while achieving
comparable micro-averaged F1 to Maxent. Over-
all, Hbrd, which combines the strengths of relation
classification and the Maxent classification model,
obtained the highest points as measured by every
metric.
5.2.3 Statistical Significance
To see whether our methods significantly im-
proved the baseline systems, we performed ran-
domisation tests (Noreen, 1989; Yeh, 2000) on
some of the results shown in Table 3. The in-
tuition of randomisation test is as follows: when
comparing two systems (e.g., A and B), we erase
the labels ?output of A? or ?output of B? from all
observations. The null hypothesis is that there is
no difference between A and B, and thus any re-
sponse produced by one of the systems could have
as likely come from the other. We shuffle these re-
12
The following 7 species occur most frequently in the
training set: H. sapiens (43.25%), M. musculus (27.05%),
R. norvegicus (5.35%), S. cerevisiae (3.98%), X. tropicalis
(3.56%), D. melanogaster (3.33%) and C. elegans (0.94%).
Species Name Pct Mxt Rel Hbrd
H. sapiens 50.13% 76.25 65.33 79.51
M. musculus 13.99% 66.41 58.29 68.27
X. tropicalis 7.35% 64.80 77.72 71.39
D. melanogaster 6.34% 93.17 78.46 95.15
S. cerevisiae 4.79% 90.12 83.32 87.68
R. norvegicus 2.97% 44.04 38.69 51.77
T. aestivum 2.62% 0.00 89.68 23.35
P. americana 2.27% 0.00 98.50 7.76
C. elegans 2.08% 96.83 95.88 97.50
H. herpesvirus 5 1.58% 0.00 54.46 4.27
R. virus 1.45% 0.00 28.54 6.45
H. spumaretrovirus 1.17% 0.00 99.37 2.49
... ... ... ... ...
Macro-average 9.85 30.21 31.80
Micro-average 70.48 67.13 73.70
Table 4: The micro-averaged F1 scores (%) of Maxent
(Mxt), Rel-ENJU-Genia with spreading (Rel), and Hbrd with
respect to each of the most frequent 12 species in DEVTEST.
sponses R times, reassign each response to A or
B and see how likely such a shuffle produces a
difference in the metric of interest that is at least
as large as the difference observed when using A
and B on the test data. Let r denote the number
of times that such a difference occurred, then as
R ? ?,
r+1
R+1
approaches the significance level.
In our case, the metrics tested were micro- and
macro-averaged precision, recall and F1.
Following this procedure, we tested whether the
improvements made by a relation classification
based system (i.e., Rel-ENJU-Genia with SPRD) and
the hybrid system (i.e., Hbrd) over the baseline sys-
tems were statistically significant. We carried out
approximate randomisation with 10,000 shuffles
and the test results are shown in Table 5. The nu-
merical figures in the cells are differences in pre-
cision, recall and F1 between a pair of systems.
The significance levels (i.e., p-values) are indi-
cated by superscript marks, whose correspond-
ing values are displayed in Table 6. For exam-
1520
Rule-Majority Rule-Sp Rule-SpSent Maxent
Rel
micro-avg 2.77
?
/3.46
?
/3.13
?
1.95
?
/2.04
?
/2.00
?
1.57
?
/2.22
?
/1.92
?
-1.57
?
/ -5.02
?
/ -3.35
?
macro-avg 15.24
?
/13.12
?
/12.13
?
3.75
a
/4.21
a
/3.20
a
9.35
?
/8.44
?
/7.10
?
21.92
?
/24.87
?
/20.35
?
Hbrd
micro-avg 8.01
?
/11.27
?
/9.70
?
7.19
?
/9.85
?
/8.57
?
6.81
?
/10.04
?
/8.49
?
3.67
?
/2.78
?
/2.82
b
macro-avg 27.22
?
/15.72
c
/13.72
d
15.73
?
/6.82
e
/4.80
f
21.33
?
/11.05
g
/ 8.70
h
33.91
i
/27.47
?
/21.95
?
Table 5: Results of paired randomisation tests on whether Rel-ENJU-Genia with SPRD (Rel) and Hbrd significantly im-
proved the baseline systems. The numerical figures in the cells show the differences between the two systems as measured by
precision/recall/F1 in percentage. The superscript marks indicate the significance levels and are explained in Table 6.
ple, the difference in micro-averaged precision be-
tween Rel-ENJU-Genia and Rule-Majority on the test
data was 2.77%, and in 10,000 approximate ran-
domisation trials, there was zero times
13
that Rel-
ENJU-Genia?s micro-averaged precision is greater
than Rule-Majority?s by at least 2.77% (p < 0.0001).
MARK VALUE MARK VALUE
* p < 0.0001 a p < 0.06
b p < 0.002 c p < 0.0003
d p < 0.0002 e p < 0.03
f p < 0.05 g p < 0.003
h p < 0.005 i p < 0.07
Table 6: p-values.
The test results confirmed that, the improve-
ments made by Hbrd are statistically significant
with at least 95% confidence as measured by all
metrics except for macro-averaged precision. The
relation classification approach achieved signifi-
cantly lower performance than Maxent in terms of
micro-averaged scores (hence the ?-? sign in the
corresponding cell in Table 5), but in all other
cases it can reject the null hypothesis with very
high confidence (i.e., p < 0.0001).
6 Conclusions and Future Work
This paper proposes a method that tackles a com-
plex disambiguation problem by breaking it into
two cascaded simpler tasks of cue word discov-
ery and binary relation classification. We evalu-
ated the method on the task of disambiguating the
model organisms of biomedical named entities,
along with a number of other approaches. As mea-
sured by micro-averaged F1 score, a supervised
classification approach (Maxent) yielded the second
best result. However, it can only disambiguate
a small number of species that have abundant
training instances. With spreading rules, a rela-
tion classification system (Rel-ENJU-Genia) trained
on word and syntactic features from ENJU-Genia
also obtained good micro-averaged F1, while sur-
13
The numbers of times are not shown in Table5 for
brevity.
passing Maxent significantly in terms of macro-
averaged scores. Combining these two systems
achieved the best overall performance. Neverthe-
less, we combined the two methods in a rather
crude way, leaving ample room for exploring bet-
ter strategies in the future.
One drawback of the relation classification sys-
tems is that they can not cover all entity mentions
but only the ones with informative keywords co-
occurring in the same sentence. We overcame the
drawback by using spreading rules. For some ap-
plications, however, it may be sufficient to make
predictions exclusively for cases where the sys-
tems are applicable. Also, the predictions with
high confidence can be used as seed training ma-
terial for automatically harvesting more training
data.
Acknowledgments
The work reported in this paper is funded by Pfizer
Ltd.. The UK National Centre for Text Mining is
funded by JISC. The ITI-TXM corpus used in the
experiments was developed at School of Informat-
ics, University of Edinburgh, in the TXM project,
which was funded by ITI Life Sciences, Scotland.
References
E. Agirre and D. Martinez. 2004. Unsupervised WSD based
on automatically retrieved examples: The importance of
bias. In Proceedings of EMNLP.
A. Airola, S. Pyysalo, J. Bj?orne, T. Pahikkala, F. Ginter, and
T. Salakoski. 2008. A graph kernel for protein-protein
interaction extraction. In Proceedings of BioNLP.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008a.
Assisted curation: does text mining really help? In Pro-
ceedings of the Pacific Symposium on Biocomputing.
B. Alex, C. Grover, B. Haddow, M. Kabadjov, E. Klein,
M. Matthews, S. Roebuck, R. Tobin, and X. Wang. 2008b.
The ITI TXM corpus: Tissue expression and protein-
protein interactions. In Proceedings of the Workshop on
Building and Evaluating Resources for Biomedical Text
Mining at LREC.
E. Briscoe, J. Carroll, and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL Interactive Presentation Sessions.
1521
R. Bunescu and M. Pas?ca. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In Proceedings of
EACL.
L. Chen, H. Liu, and C. Friedman. 2005. Gene name
ambiguity of eukaryotic nomenclatures. Bioinformatics,
21(2):248?256.
S. Clark and J. R. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models. Com-
putational Linguistics, 33(4).
M-C de Marneffe, B. MacCartney, and C. D. Manning. 2006.
Generating typed dependency parses from phrase struc-
ture. In Proceedings of LREC.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interaction
sentences using dependency parsing. In Proceedings of
the Joint Conference of EMNLP and CoNLL.
G. Forman. 2003. An extensive empirical study of feature se-
lection metrics for text classification. Journal of Machine
Learning Research, 3:1289?1305.
C. Grover, M. Lapata, and A. Ascarides. 2003. A compar-
ison of parsing technologies for the biomedical domain.
Natural Language Engineering, 1(1):1?38.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of gene
mentions with GNAT. Bioinformatics, 24(16).
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating impact
of re-training a lexical disambiguation model on domain
adaptation of an HPSG parser. In Proceedings of the 10th
International Conference on Parsing Technology.
V. Hatzivassiloglou, PA Dubou?e, and A. Rzhetsky. 2001.
Disambiguating proteins, genes, and RNA in text: a ma-
chine learning approach. Bioinformatics, 17(Suppl 1).
L. Hirschman, M. Krallinger, J. Wilbur, and A. Valencia, ed-
itors. 2007. The BioCreative II - Critical Assessment
for Information Extraction in Biology Challenge, volume
9(Suppl 2). Genome Biology.
L. Hunter and K. B. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed. Molecular Cell,
21(5):589?594.
N. Japkowicz. 2000. Learning from imbalanced data sets: a
comparison of various strategies. In Proceedings of AAAI
Workshop on Learning from Imbalanced Data Sets.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of ACL.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of HLT/EMNLP.
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Miwa, R. Satre, Y. Miyao, T. Ohta, and J. Tsujii. 2008.
Combining multiple layers of syntactic information for
protein-protein interaction extraction. In Proceedings of
SMBM.
Y. Miyao and J. Tsujii. 2008. Feature forest models for prob-
abilistic HPSG parsing. Computational Linguistics, 34(1).
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsujii.
2008. Task-oriented evaluation of syntactic parsers and
their representations. In Proceedings of ACL-08: HLT.
A. A. Morgan and L. Hirschman. 2007. Overview of
BioCreAtIvE II gene normalisation. In Proceedings of the
BioCreAtIvE II Workshop, Madrid.
A. A. Morgan, Z. Lu, X. Wang, A. M. Cohen, J. Fluck,
P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Haken-
berg, C. Sun, H. Liu, R. Torres, M. Krauthammer, W. W.
Lau, H. Liu, C. Hsu, M. Schuemie, K. B. Cohen, and
L. Hirschman. 2008. Overview of BioCreAtIvE II gene
normalization. Genome Biology, 9(Suppl 2).
A. Moschitti. 2006. Making tree kernels practical for natural
language learning. In Proceedings of EACL.
E. W. Noreen. 1989. Computer Intensive Methods for Test-
ing Hypothesis. John Wiley & Sons.
C. Pollard and I. A. Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
A. S. Schwartz and M. A. Hearst. 2003. Identifying abbrevi-
ation definitions in biomedical text. In Proceedings of the
Pacific Symposium on Biocomputing.
B. Settles. 2005. ABNER: An open source tool for automat-
ically tagging genes, proteins, and other entity names in
text. Bioinformatics, 21(14):3191?3192.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syn-
tax annotation for the GENIA corpus. In Proceedings of
IJCNLP.
X. Wang and C. Grover. 2008. Learning the species of
biomedical named entities from annotated corpora. In
Proceedings of LREC.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proceedings of
ICML.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of COLING.
1522
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790?798,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Fast Full Parsing by Linear-Chain Conditional Random Fields
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper presents a chunking-based dis-
criminative approach to full parsing. We
convert the task of full parsing into a series
of chunking tasks and apply a conditional
random field (CRF) model to each level
of chunking. The probability of an en-
tire parse tree is computed as the product
of the probabilities of individual chunk-
ing results. The parsing is performed in a
bottom-up manner and the best derivation
is efficiently obtained by using a depth-
first search algorithm. Experimental re-
sults demonstrate that this simple parsing
framework produces a fast and reasonably
accurate parser.
1 Introduction
Full parsing analyzes the phrase structure of a sen-
tence and provides useful input for many kinds
of high-level natural language processing such as
summarization (Knight and Marcu, 2000), pro-
noun resolution (Yang et al, 2006), and infor-
mation extraction (Miyao et al, 2008). One of
the major obstacles that discourage the use of full
parsing in large-scale natural language process-
ing applications is its computational cost. For ex-
ample, the MEDLINE corpus, a collection of ab-
stracts of biomedical papers, consists of 70 million
sentences and would require more than two years
of processing time if the parser needs one second
to process a sentence.
Generative models based on lexicalized PCFGs
enjoyed great success as the machine learning
framework for full parsing (Collins, 1999; Char-
niak, 2000), but recently discriminative models
attract more attention due to their superior accu-
racy (Charniak and Johnson, 2005; Huang, 2008)
and adaptability to new grammars and languages
(Buchholz and Marsi, 2006).
A traditional approach to discriminative full
parsing is to convert a full parsing task into a series
of classification problems. Ratnaparkhi (1997)
performs full parsing in a bottom-up and left-to-
right manner and uses a maximum entropy clas-
sifier to make decisions to construct individual
phrases. Sagae and Lavie (2006) use the shift-
reduce parsing framework and a maximum en-
tropy model for local classification to decide pars-
ing actions. These approaches are often called
history-based approaches.
A more recent approach to discriminative full
parsing is to treat the task as a single structured
prediction problem. Finkel et al (2008) incor-
porated rich local features into a tree CRF model
and built a competitive parser. Huang (2008) pro-
posed to use a parse forest to incorporate non-local
features. They used a perceptron algorithm to op-
timize the weights of the features and achieved
state-of-the-art accuracy. Petrov and Klein (2008)
introduced latent variables in tree CRFs and pro-
posed a caching mechanism to speed up the com-
putation.
In general, the latter whole-sentence ap-
proaches give better accuracy than history-based
approaches because they can better trade off deci-
sions made in different parts in a parse tree. How-
ever, the whole-sentence approaches tend to re-
quire a large computational cost both in training
and parsing. In contrast, history-based approaches
are less computationally intensive and usually pro-
duce fast parsers.
In this paper, we present a history-based parser
using CRFs, by treating the task of full parsing as
a series of chunking problems where it recognizes
chunks in a flat input sequence. We use the linear-
790
Estimated  volume  was   a   light  2.4  million  ounces  .
VBN         NN    VBD DT  JJ    CD     CD NNS   .
QPNP
Figure 1: Chunking, the first (base) level.
volume          was   a   light    million       ounces .
NP             VBD DT  JJ          QP            NNS   .
NP
Figure 2: Chunking, the 2nd level.
chain CRF model to perform chunking.
Although our parsing model falls into the cat-
egory of history-based approaches, it is one step
closer to the whole-sentence approaches because
the parser uses a whole-sequence model (i.e.
CRFs) for individual chunking tasks. In other
words, our parser could be located somewhere
between traditional history-based approaches and
whole-sentence approaches. One of our motiva-
tions for this work was that our parsing model
may achieve a better balance between accuracy
and speed than existing parsers.
It is also worth mentioning that our approach is
similar in spirit to supertagging for parsing with
lexicalized grammar formalisms such as CCG and
HPSG (Clark and Curran, 2004; Ninomiya et al,
2006), in which significant speed-ups for parsing
time are achieved.
In this paper, we show that our approach is in-
deed appealing in that the parser runs very fast
and gives competitive accuracy. We evaluate our
parser on the standard data set for parsing exper-
iments (i.e. the Penn Treebank) and compare it
with existing approaches to full parsing.
This paper is organized as follows. Section 2
presents the overall chunk parsing strategy. Sec-
tion 3 describes the CRF model used to perform
individual chunking steps. Section 4 describes the
depth-first algorithm for finding the best derivation
of a parse tree. The part-of-speech tagger used in
the parser is described in section 5. Experimen-
tal results on the Penn Treebank corpus are pro-
vided in Section 6. Section 7 discusses possible
improvements and extensions of our work. Sec-
tion 8 offers some concluding remarks.
volume          was                    ounces          .
NP             VBD                    NP           .
VP
Figure 3: Chunking, the 3rd level.
volume                           was                   .
NP                               VP                .
S
Figure 4: Chunking, the 4th level.
2 Full Parsing by Chunking
This section describes the parsing framework em-
ployed in this work.
The parsing process is conceptually very sim-
ple. The parser first performs chunking by iden-
tifying base phrases, and converts the identified
phrases to non-terminal symbols. It then performs
chunking for the updated sequence and converts
the newly recognized phrases into non-terminal
symbols. The parser repeats this process until the
whole sequence is chunked as a sentence
Figures 1 to 4 show an example of a parsing pro-
cess by this framework. In the first (base) level,
the chunker identifies two base phrases, (NP Es-
timated volume) and (QP 2.4 million), and re-
places each phrase with its non-terminal symbol
and head1. In the second level, the chunker iden-
tifies a noun phrase, (NP a light million ounces),
and converts it into NP. This process is repeated
until the whole sentence is chunked at the fourth
level. The full parse tree is recovered from the
chunking history in a straightforward way.
This idea of converting full parsing into a se-
ries of chunking tasks is not new by any means?
the history of this kind of approach dates back to
1950s (Joshi and Hopely, 1996). More recently,
Brants (1999) used a cascaded Markov model to
parse German text. Tjong Kim Sang (2001) used
the IOB tagging method to represent chunks and
memory-based learning, and achieved an f-score
of 80.49 on the WSJ corpus. Tsuruoka and Tsu-
jii (2005) improved upon their approach by using
1The head word is identified by using the head-
percolation table (Magerman, 1995).
791
 0
 1000
 2000
 3000
 4000
 5000
 0  5  10  15  20  25  30
# 
se
nt
en
ce
s
Height
Figure 5: Distribution of tree height in WSJ sec-
tions 2-21.
a maximum entropy classifier and achieved an f-
score of 85.9. However, there is still a large gap
between the accuracy of chunking-based parsers
and that of widely-used practical parsers such as
Collins parser and Charniak parser (Collins, 1999;
Charniak, 2000).
2.1 Heights of Trees
A natural question about this parsing framework is
how many levels of chunking are usually needed to
parse a sentence. We examined the distribution of
the heights of the trees in sections 2-21 of the Wall
Street Journal (WSJ) corpus. The result is shown
in Figure 5. Most of the sentences have less than
20 levels. The average was 10.0, which means we
need to perform, on average, 10 chunking tasks to
obtain a full parse tree for a sentence if the parsing
is performed in a deterministic manner.
3 Chunking with CRFs
The accuracy of chunk parsing is highly depen-
dent on the accuracy of each level of chunking.
This section describes our approach to the chunk-
ing task.
A common approach to the chunking problem
is to convert the problem into a sequence tagging
task by using the ?BIO? (B for beginning, I for
inside, and O for outside) representation. For ex-
ample, the chunking process given in Figure 1 is
expressed as the following BIO sequences.
B-NP I-NP O O O B-QP I-QP O O
This representation enables us to use the linear-
chain CRF model to perform chunking, since the
task is simply assigning appropriate labels to a se-
quence.
3.1 Linear Chain CRFs
A linear chain CRF defines a single log-linear
probabilistic distribution over all possible tag se-
quences y for the input sequence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x),
where fk(t, yt, yt?1,x) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x).
This model allows us to define features on states
and edges combined with surface observations.
The weights of the features are determined in
such a way that they maximize the conditional log-
likelihood of the training data:
L? =
N
?
i=1
log p(y(i)|x(i)) + R(?),
where R(?) is introduced for the purpose of regu-
larization which prevents the model from overfit-
ting the training data. The L1 or L2 norm is com-
monly used in statistical natural language process-
ing (Gao et al, 2007). We used L1-regularization,
which is defined as
R(?) = 1C
K
?
k=1
|?k|,
where C is the meta-parameter that controls the
degree of regularization. We used the OWL-QN
algorithm (Andrew and Gao, 2007) to obtain the
parameters that maximize the L1-regularized con-
ditional log-likelihood.
3.2 Features
Table 1 shows the features used in chunking for
the base level. Since the task is basically identical
to shallow parsing by CRFs, we follow the feature
sets used in the previous work by Sha and Pereira
(2003). We use unigrams, bigrams, and trigrams
of part-of-speech (POS) tags and words.
The difference between our CRF chunker and
that in (Sha and Pereira, 2003) is that we could
not use second-order CRF models, hence we could
not use trigram features on the BIO states. We
792
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Word Unigrams h?2, h?1, h0, h+1, h+2
Word Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Word Trigrams h?1h0h+1
Table 1: Feature templates used in the base level chunking. s represents a terminal symbol (i.e. POS tag)
and the subscript represents a relative position. h represents a word.
found that using second order CRFs in our task
was very difficult because of the computational
cost. Recall that the computational cost for CRFs
is quadratic to the number of possible states. In
our task, we need to consider the states for all non-
terminal symbols, whereas their work is only con-
cerned with noun phrases.
Table 2 shows feature templates used in the non-
base levels of chunking. In the non-base levels of
chunking, we can use a richer set of features than
the base-level chunking because the chunker has
access to the information about the partial trees
that have been already created. In addition to the
features listed in Table 1, the chunker looks into
the daughters of the current non-terminal sym-
bol and use them as features. It also uses the
words and POS tags around the edges of the re-
gion covered by the current non-terminal symbol.
We also added a special feature to better capture
PP-attachment. The chunker looks at the head of
the second daughter of the prepositional phrase to
incorporate the semantic head of the phrase.
4 Searching for the Best Parse
The probability for an entire parse tree is com-
puted as the product of the probabilities output by
the individual CRF chunkers:
score =
h
?
i=0
p(yi|xi), (1)
where i is the level of chunking and h is the height
of the tree. The task of full parsing is then to
choose the series of chunking results that maxi-
mizes this probability.
It should be noted that there are cases where
different derivations (chunking histories) lead to
the same parse tree (i.e. phrase structure). Strictly
speaking, therefore, what we describe here as the
probability of a parse tree is actually the proba-
bility of a single derivation. The probabilities of
the derivations should then be marginalized over
to produce the probability of a parse tree, but in
this paper we ignore this effect and simply focus
only on the best derivation.
We use a depth-first search algorithm to find the
highest probability derivation. Figure 6 shows the
algorithm in pseudo-code. The parsing process is
implemented with a recursive function. In each
level of chunking, the recursive function first in-
vokes a CRF chunker to obtain chunking hypothe-
ses for the given sequence. For each hypothesis
whose probability is high enough to have possibil-
ity of constituting the best derivation, the function
calls itself with the sequence updated by the hy-
pothesis. The parsing process is performed in a
bottom up manner and this recursive process ter-
minates if the whole sequence is chunked as a sen-
tence.
To extract multiple chunking hypotheses from
the CRF chunker, we use a branch-and-bound
algorithm rather than the A* search algorithm,
which is perhaps more commonly used in previous
studies. We do not give pseudo code, but the ba-
sic idea is as follows. It first performs the forward
Viterbi algorithm to obtain the best sequence, stor-
ing the upper bounds that are used for pruning in
branch-and-bound. It then performs a branch-and-
bound algorithm in a backward manner to retrieve
possible candidate sequences whose probabilities
are greater than the given threshold. Unlike A*
search, this method is memory efficient because it
is performed in a depth-first manner and does not
require priority queues for keeping uncompleted
hypotheses.
It is straightforward to introduce beam search in
this search algorithm?we simply limit the num-
ber of hypotheses generated by the CRF chunker.
We examine how the width of the beam affects the
parsing performance in the experiments.
793
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Head Unigrams h?2, h?1, h0, h+1, h+2
Head Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Head Trigrams h?1h0h+1
Symbol & Daughters s0d01, ... s0d0m
Symbol & Word/POS context s0wj?1, s0pj?1, s0wk+1 , s0pk+1
Symbol & Words on the edges s0wj , s0wk
Freshness whether s0 has been created in the level just below
PP-attachment h?1h0m02 (only when s0 = PP)
Table 2: Feature templates used in the upper level chunking. s represents a non-terminal symbol. h
represents a head percolated from the bottom for each symbol. d0i is the ith daughter of s0. wj is the
first word in the range covered by s0. wj?1 is the word preceding wj . wk is the last word in the range
covered by s0. wk+1 is the word following wk. p represents POS tags. m02 represents the head of the
second daughter of s0.
Word Unigram w?2, w?1, w0, w+1, wi+2
Word Bigram w?1w0, w0w+1, w?1w+1
Prefix, Suffix prefixes of w0
suffixes of w0
(up to length 10)
Character features w0 has a hyphen
w0 has a number
w0 has a capital letter
w0 is all capital
Normalized word N(w0)
Table 3: Feature templates used in the POS tagger.
w represents a word and the subscript represents a
relative position.
5 Part-of-Speech Tagging
We use the CRF model also for POS tagging.
The CRF-based POS tagger is incorporated in the
parser in exactly the same way as the other lay-
ers of chunking. In other words, the POS tagging
process is treated like the bottom layer of chunk-
ing, so the parser considers multiple probabilistic
hypotheses output by the tagger in the search al-
gorithm described in the previous section.
5.1 Features
Table 3 shows the feature templates used in the
POS tagger. Most of them are standard features
commonly used in POS tagging for English. We
used unigrams and bigrams of neighboring words,
prefixes and suffixes of the current word, and some
characteristics of the word. We also normalized
the current word by lowering capital letters and
converting all the numerals into ?#?, and used the
normalized word as a feature.
6 Experiments
We ran parsing experiments using the Wall Street
Journal corpus. Sections 2-21 were used as the
training data. Section 22 was used as the devel-
opment data, with which we tuned the feature set
and parameters for learning and parsing. Section
23 was reserved for the final accuracy report.
The training data for the CRF chunkers were
created by converting each parse tree in the train-
ing data into a list of chunking sequences like
the ones presented in Figures 1 to 4. We trained
three CRF models, i.e., the POS tagging model,
the base chunking model, and the non-base chunk-
ing model. The training took about two days on a
single CPU.
We used the evalb script provided by Sekine and
Collins for evaluating the labeled recall/precision
of the parser outputs2. All experiments were car-
ried out on a server with 2.2 GHz AMD Opteron
processors and 16GB memory.
6.1 Chunking Performance
First, we describe the accuracy of individual
chunking processes. Table 4 shows the results
for the ten most frequently occurring symbols on
the development data. Noun phrases (NP) are the
2The script is available at http://nlp.cs.nyu.edu/evalb/. We
used the parameter file ?COLLINS.prm?.
794
1: procedure PARSESENTENCE(x)
2: PARSE(x, 1, 0)
3:
4: function PARSE(x, p, q)
5: if x is chunked as a complete sentence
6: return p
7: H ? PERFORMCHUNKING(x, q/p)
8: for h ? H in descending order of their
probabilities do
9: r ? p? h.probability
10: if r > q then
11: x? ? UPDATESEQUENCE(x, h)
12: s? PARSE(x?, r, q)
13: if s > q then
14: q ? s
15: return q
16:
17: function PERFORMCHUNKING(x, t)
18: perform chunking with a CRF chunker and
19: return a set of chunking hypotheses whose
20: probabilities are greater than t.
21:
22: function UPDATESEQUENCE(x, h)
23: update sequence x according to chunking
24: hypothesis h and return the updated
25: sequence.
Figure 6: Searching for the best parse with a
depth-first search algorithm. This pseudo-code il-
lustrates how to find the highest probability parse,
but in the real implementation, the function needs
to keep track of chunking histories as well as prob-
abilities.
most common symbol and consist of 55% of all
phrases. The accuracy of noun phrases recognition
was relatively high, but it may be useful to design
special features for this particular type of phrase,
considering the dominance of noun phrases in the
corpus. Although not directly comparable, Sha
and Pereira (2003) report almost the same level
of accuracy (94.38%) on noun phrase recognition,
using a much smaller training set. We attribute
their superior performance mainly to the use of
second-order features on state transitions. Table 4
also suggests that adverb phrases (ADVP) and ad-
jective phrases (ADJP) are more difficult to recog-
nize than other types of phrases, which coincides
with the result reported in (Collins, 1999).
It should be noted that the performance reported
in this table was evaluated using the gold standard
sequences as the input to the CRF chunkers. In the
Symbol # Samples Recall Prec. F-score
NP 317,597 94.79 94.16 94.47
VP 76,281 91.46 91.98 91.72
PP 66,979 92.84 92.61 92.72
S 33,739 91.48 90.64 91.06
ADVP 21,686 84.25 85.86 85.05
ADJP 14,422 77.27 78.46 77.86
QP 14,308 89.43 91.16 90.28
SBAR 11,603 96.42 96.97 96.69
WHNP 8,827 95.54 97.50 96.51
PRT 3,391 95.72 90.52 93.05
: : : : :
all 579,253 92.63 92.62 92.63
Table 4: Chunking performance (section 22, all
sentences).
Beam Recall Prec. F-score Time (sec)
1 86.72 87.83 87.27 16
2 88.50 88.85 88.67 41
3 88.69 89.08 88.88 61
4 88.72 89.13 88.92 92
5 88.73 89.14 88.93 119
10 88.68 89.19 88.93 179
Table 5: Beam width and parsing performance
(section 22, all sentences).
real parsing process, the chunkers have to use the
output from the previous (one level below) chun-
ker, so the quality of the input is not as good as
that used in this evaluation.
6.2 Parsing Performance
Next, we present the actual parsing performance.
The first set of experiments concerns the relation-
ship between the width of beam and the parsing
performance. Table 5 shows the results obtained
on the development data. We varied the width of
the beam from 1 to 10. The beam width of 1 cor-
responds to deterministic parsing. Somewhat un-
expectedly, the parsing accuracy did not drop sig-
nificantly even when we reduced the beam width
to a very small number such as 2 or 3.
One of the interesting findings was that re-
call scores were consistently lower than precision
scores throughout all experiments. A possible rea-
son is that, since the score of a parse is defined
as the product of all chunking probabilities, the
parser could prefer a parse tree that consists of
a small number of chunk layers. This may stem
795
from the history-based model?s inability of prop-
erly trading off decisions made by different chun-
kers.
Overall, the parsing speed was very high. The
deterministic version (beam width = 1) parsed
1700 sentences in 16 seconds, which means that
the parser needed only 10 msec to parse one sen-
tence. The parsing speed decreases as we increase
the beam width.
The parser was also memory efficient. Thanks
to L1 regularization, the training process did not
result in many non-zero feature weights. The num-
bers of non-zero weight features were 58,505 (for
the base chunker), 263,889 (for the non-base chun-
ker), and 42,201 (for the POS tagger). The parser
required only 14MB of memory to run.
There was little accuracy difference between the
beam width of 4 and 5, so we adopted the beam
width of 4 for the final accuracy report on the test
data.
6.3 Comparison with Previous Work
Table 6 shows the performance of our parser on
the test data and summarizes the results of previ-
ous work. Our parser achieved an f-score of 88.4
on the test data, which is comparable to the accu-
racy achieved by recent discriminative approaches
such as Finkel et al (2008) and Petrov & Klein
(2008), but is not as high as the state-of-the-art
accuracy achieved by the parsers that can incor-
porate global features such as Huang (2008) and
Charniak & Johnson (2005). Our parser was more
accurate than traditional history-based approaches
such as Sagae & Lavie (2006) and Ratnaparkhi
(1997), and was significantly better than previous
cascaded chunking approaches such as Tsuruoka
& Tsujii (2005) and Tjong Kim Sang (2001).
Although the comparison presented in the table
is not perfectly fair because of the differences in
hardware platforms, the results show that our pars-
ing model is a promising addition to the parsing
frameworks for building a fast and accurate parser.
7 Discussion
One of the obvious ways to improve the accuracy
of our parser is to improve the accuracy of in-
dividual CRF models. As mentioned earlier, we
were not able to use second-order features on state
transitions, which would have been very useful,
due to the problem of computational cost. Incre-
mental feature selection methods such as grafting
(Perkins et al, 2003) may help us to incorporate
such higher-order features, but the problem of de-
creased efficiency of dynamic programming in the
CRF would probably need to be addressed.
In this work, we treated the chunking problem
as a sequence labeling problem by using the BIO
representation for the chunks. However, semi-
Markov conditional random fields (semi-CRFs)
can directly handle the chunking problem by
considering all possible combinations of subse-
quences of arbitrary length (Sarawagi and Cohen,
2004). Semi-CRFs allow one to use a richer set
of features than CRFs, so the use of semi-CRFs
in our parsing framework should lead to improved
accuracy. Moreover, semi-CRFs would allow us to
incorporate some useful restrictions in producing
chunking hypotheses. For example, we could nat-
urally incorporate the restriction that every chunk
has to contain at least one symbol that has just
been created in the previous level3. It is hard for
the normal CRF model to incorporate such restric-
tions.
Introducing latent variables into the CRF model
may be another promising approach. This is the
main idea of Petrov and Klein (2008), which sig-
nificantly improved parsing accuracy.
A totally different approach to improving the
accuracy of our parser is to use the idea of ?self-
training? described in (McClosky et al, 2006).
The basic idea is to create a larger set of training
data by applying an accurate parser (e.g. rerank-
ing parser) to a large amount of raw text. We can
then use the automatically created treebank as the
additional training data for our parser. This ap-
proach suggests that accurate (but slow) parsers
and fast (but not-so-accurate) parsers can actually
help each other.
Also, since it is not difficult to extend our parser
to produce N-best parsing hypotheses, one could
build a fast reranking parser by using the parser as
the base (hypotheses generating) parser.
8 Conclusion
Although the idea of treating full parsing as a se-
ries of chunking problems has a long history, there
has not been a competitive parser based on this
parsing framework. In this paper, we have demon-
strated that the framework actually enables us to
3For example, the sequence VBD DT JJ in Figure 2 can-
not be a chunk in the current level because it would have been
already chunked in the previous level if it were.
796
Recall Precision F-score Time (min)
This work (deterministic) 86.3 87.5 86.9 0.5
This work (search, beam width = 4) 88.2 88.7 88.4 1.7
Huang (2008) 91.7 Unk
Finkel et al (2008) 87.8 88.2 88.0 >250*
Petrov & Klein (2008) 88.3 3*
Sagae & Lavie (2006) 87.8 88.1 87.9 17
Charniak & Johnson (2005) 90.6 91.3 91.0 Unk
Tsuruoka & Tsujii (2005) 85.0 86.8 85.9 2
Collins (1999) 88.1 88.3 88.2 39**
Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk
Charniak (2000) 89.6 89.5 89.5 23**
Ratnaparkhi (1997) 86.3 87.5 86.9 Unk
Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the
training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the
parsers.
build a competitive parser if we use CRF mod-
els for each level of chunking and a depth-first
search algorithm to search for the highest proba-
bility parse.
Like other discriminative learning approaches,
one of the advantages of our parser is its general-
ity. The design of our parser is very generic, and
the features used in our parser are not particularly
specific to the Penn Treebank. We expect it to be
straightforward to adapt the parser to other projec-
tive grammars and languages.
This parsing framework should be useful when
one needs to process a large amount of text or
when real time processing is required, in which
the parsing speed is of top priority. In the deter-
ministic setting, our parser only needed about 10
msec to parse a sentence.
Acknowledgments
This work described in this paper has been
funded by the Biotechnology and Biological Sci-
ences Research Council (BBSRC; BB/E004431/1)
and the European BOOTStrep project (FP6 -
028099). The research team is hosted by the
JISC/BBSRC/EPSRC sponsored National Centre
for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
Thorsten Brants. 1999. Cascaded markov models. In
Proceedings of EACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL 2000,
pages 132?139.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING 2004, pages 282?
288.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08:HLT, pages 586?594.
Aravind K. Joshi and Phil Hopely. 1996. A parser
from antiquity. Natural Language Engineering,
2(4):291?294.
797
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI/IAAI, pages 703?710.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL.
Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08:HLT, pages 46?54.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP 2006,
pages 155?163.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems 20
(NIPS), pages 1153?1160.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP 1997, pages 1?10.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 691?698.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proceedings of NIPS.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL.
Erik Tjong Kim Sang. 2001. Transforming a chunker
to a parser. In J. Veenstra W. Daelemans, K. Sima?an
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000, pages 177?188. Rodopi.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of IWPT, pages
133?140.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic features. In Proceedings of COLING/ACL,
pages 41?48.
798
Identifying Sections in Scientific Abstracts using Conditional Random Fields
Kenji Hirohata?
hirohata@nii.ac.jp
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
OBJECTIVE: The prior knowledge about
the rhetorical structure of scientific abstracts
is useful for various text-mining tasks such
as information extraction, information re-
trieval, and automatic summarization. This
paper presents a novel approach to cate-
gorize sentences in scientific abstracts into
four sections, objective, methods, results,
and conclusions. METHOD: Formalizing
the categorization task as a sequential label-
ing problem, we employ Conditional Ran-
dom Fields (CRFs) to annotate section la-
bels into abstract sentences. The train-
ing corpus is acquired automatically from
Medline abstracts. RESULTS: The pro-
posed method outperformed the previous
approaches, achieving 95.5% per-sentence
accuracy and 68.8% per-abstract accuracy.
CONCLUSION: The experimental results
showed that CRFs could model the rhetor-
ical structure of abstracts more suitably.
1 Introduction
Scientific abstracts are prone to share a similar
rhetorical structure. For example, an abstract usu-
ally begins with the description of background in-
formation, and is followed by the target problem,
solution to the problem, evaluation of the solution,
and conclusion of the paper. Previous studies ob-
served the typical move of rhetorical roles in sci-
entific abstracts: problem, solution, evaluation, and
conclusion (Graetz, 1985; Salanger-Meyer, 1990;
Swales, 1990; Ora?san, 2001). The American Na-
tional Standard Institute (ANSI) recommends au-
thors and editors of abstracts to state the purpose,
methods, results, and conclusions presented in the
documents (ANSI, 1979).
The prior knowledge about the rhetorical structure
of abstracts is useful to improve the performance of
various text-mining tasks. Marcu (1999) proposed
an extraction method for summarization that cap-
tured the flow of text, based on Rhetorical Struc-
ture Theory (RST). Some extraction methods make
use of cue phrases (e.g., ?in conclusion?, ?our in-
vestigation has shown that ...?), which suggest that
the rhetorical role of sentences is to identify im-
portant sentences (Edmundson, 1969; Paice, 1981).
We can survey the problems, purposes, motivations,
and previous approaches of a research field by read-
ing texts in background sections of scientific papers.
Tbahriti (2006) improved the performance of their
information retrieval engine, giving more weight to
sentences referring to purpose and conclusion.
In this paper, we present a supervised machine-
learning approach that categorizes sentences in sci-
entific abstracts into four sections, objective, meth-
ods, results, and conclusions. Figure 1 illustrates
the task of this study. Given an unstructured ab-
stract without section labels indicated by boldface
type, the proposed method annotates section labels
of each sentence. Assuming that this task is well
formalized as a sequential labeling problem, we use
Conditional Random Fields (CRFs) (Lafferty et al,
2001) to identify rhetorical roles in scientific ab-
stracts.The proposed method outperforms previous
approaches to this problem, achieving 95.5% per-
381
OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) pro-
duction in humans. METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml. After phlebotomy, they
received (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9%
sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor ago-
nist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14
mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min). RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p ?
0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparable
with placebo. Creatinine clearance was significantly increased during dobutamine treatment. Urinary cyclic adenosine monophos-
phate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased. Plasma renin activity
was significantly increased during dobutamine and fenoterol infusion. CONCLUSIONS: This study shows in a model of con-
trolled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not the
beta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans. The result can be interpreted as
a hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta
1-adrenergic receptors. It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causally
linked to the control of erythropoietin production in this experimental setting.
Figure 1: An abstract with section labels indicated by boldface type (Gleiter et al, 1997).
sentence accuracy and 68.8% per-abstract accuracy.
This paper is organized as follows. Section 2
describes previous approaches to this task. For-
malizing the task as a sequential-labeling problem,
Section 3 designs a sentence classifier using CRFs.
Training corpora for the classifier are acquired au-
tomatically from the Medline abstracts. Section 4
reports considerable improvements in the proposed
method over the baseline method using Support Vec-
tor Machine (SVM) (Cortes and Vapnik, 1995). We
conclude this paper in Section 5.
2 Related Work
The previous studies regarded the task of identify-
ing section names as a text-classification problem
that determines a label (section name) for each sen-
tence. Various classifiers for text categorization,
Na??ve Bayesian Model (NBM) (Teufel and Moens,
2002; Ruch et al, 2007), Hidden Markov Model
(HMM) (Wu et al, 2006; Lin et al, 2006), and Sup-
port Vector Machines (SVM) (McKnight and Arini-
vasan, 2003; Shimbo et al, 2003; Ito et al, 2004;
Yamamoto and Takagi, 2005) were applied.
Table 1 summarizes these approaches and perfor-
mances. All studies target scientific abstracts except
for Teufel and Moens (2002) who target scientific
full papers. Field classes show the set of section
names that each study assumes: background (B),
objective/aim/purpose (O), method (M), result (R),
conclusion (C), and introduction (I) that combines
the background and objective. Although we should
not compare directly the performances of these stud-
ies, which use a different set of classification labels
and evaluation corpora, SVM classifiers appear to
yield better results for this task. The rest of this sec-
tion elaborates on the previous studies with SVMs.
Shimbo et al (2003) presented an advanced text
retrieval system for Medline that can focus on a
specific section in abstracts specified by a user.
The system classifies sentences in each Medline ab-
stract into four sections, objective, method, results,
and conclusion. Each sentence is represented by
words, word bigrams, and contextual information of
the sentence (e.g., class of the previous sentence,
relative location of the current sentence). They
reported 91.9% accuracy (per-sentence basis) and
51.2% accuracy (per-abstract basis1) for the clas-
sification with the best feature set for quadratic
SVM. Ito et al (2004) extended the work with a
semi-supervised learning technique using transduc-
tive SVM (TSVM).
Yamamoto and Takagi (2005) developed a sys-
tem to classify abstract sentences into five sections,
background, purpose, method, result, and conclu-
sion. They trained a linear-SVM classifier with fea-
tures such as unigram, subject-verb, verb tense, rel-
ative sentence location, and sentence score (average
TF*IDF score of constituent words). Their method
achieved 68.9%, 63.0%, 83.6%, 87.2%, 89.8% F-
scores for classifying background, purpose, method,
result, and conclusion sentences respectively. They
also reported the classification performance of intro-
duction sentences, which combines background and
purpose sentences, with 91.3% F-score.
1An abstract is considered correct if all constituent sentences
are correctly labeled.
382
Methods Model Classes Performance (reported in papers)
Teufel and Moens (2002) NBM (7 classes) 44% precision and 65% recall for aim sentences
Ruch et al (2007) NBM O M R C 85% F-score for conclusion sentences
Wu et al (2006) HMM B O M R C 80.54% precision
Lin et al (2006) HMM I M R C 88.5%, 84.3%, 89.8%, 89.7% F-scores
McKnight and Srinivasan (2003) SVM I M R C 89.2%, 82.0%, 82.1%, 89.5% F-scores
Shimbo et al (2003) SVM B O M R C 91.9% accuracy
Ito et al (2004) TSVM B O M R C 66.0%, 51.0%, 49.3%, 72.9%, 67.7% F-scores
Yamamoto and Takagi (2005) SVM I (B O) M R C 91.3% (68.9%, 63.0%), 83.6%, 87.2%, 89.8% F-scores
Table 1: Approaches and performances of previous studies on section identification
3 Proposed method
3.1 Section identification as a sequence labeling
problem
The previous work saw the task of labeling as a text
categorization that determines the class label yi for
each sentence xi. Even though some work includes
features of the surrounding sentences for xi, e.g.
?class label of xi?1 sentence,? ?class label of xi+1
sentence,? and ?unigram in xi?1 sentence,? the clas-
sifier determines the class label yi for each sentence
xi independently. It has been an assumption for text
classification tasks to decide a class label indepen-
dently of other class labels.
However, as described in Section 1, scientific ab-
stracts have typical moves of rhetorical roles: it
would be very peculiar if result sentences appear-
ing before method sentences were described in an
abstract. Moreover, we would like to model the
structure of abstract sentences rather than model-
ing just the section label for each sentence. Thus,
the task is more suitably formalized as a sequence
labeling problem: given an abstract with sentences
x = (x1, ..., xn), determine the optimal sequence of
section names y = (y1, ..., yn) of all possible se-
quences.
Conditional Random Fields (CRFs) have been
successfully applied to various NLP tasks includ-
ing part-of-speech tagging (Lafferty et al, 2001) and
shallow parsing (Sha and Pereira, 2003). CRFs de-
fine a conditional probability distribution p(y|x) for
output and input sequences, y and x,
p(y|x) =
1
Z?(x)
exp {? ? F (y,x)} . (1)
Therein: function F (y,x) denotes a global feature
vector for input sequence x and output sequence y,
F (y,x) =
?
i
f(y,x, i), (2)
i ranges over the input sequence, function f(y,x, i)
is a feature vector for input sequence x and output
sequence y at position i (based on state features and
transition features), ? is a vector where an element
?k represents the weight of feature Fk(y,x), and
Z?(x) is a normalization factor,
Z?(x) =
?
y
exp {? ? F (y,x)} . (3)
The optimal output sequence y? for an input se-
quence x,
y? = argmax
y
p(y|x), (4)
is obtained efficiently by the Viterbi algorithm. The
optimal set of parameters ? is determined efficiently
by the Generalized Iterative Scaling (GIS) (Darroch
and Ratcliff, 1972) or Limited-memory Broyden-
Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal and
Wright, 1999) method.
3.2 Features
We design three kinds of features to represent each
abstract sentence for CRFs. The contributions of
these features will be evaluated later in Section 4.
Content (n-gram) This feature examines the exis-
tence of expressions that characterize a specific sec-
tion, e.g. ?to determine ...,? and ?aim at ...? for stat-
ing the objective of a study. We use features for sen-
tence contents represented by: i) words, ii) word bi-
grams, and iii) mixture of words and word bigrams.
Words are normalized into their base forms by the
GENIA tagger (Tsuruoka and Tsujii, 2005), which
is a part-of-speech tagger trained for the biomedical
383
Rank OBJECTIVE METHOD RESULTS CONCLUSIONS
1 # to be measure % ) suggest that
2 be to be perform ( p may be
3 to determine n = p < # these
4 study be be compare ) . should be
5 this study be determine % . these result
Table 2: Bigram features with high ?2 values (?#? stands for a beginning of a sentence).
domain. We measure the co-occurrence strength (?2
value) between each feature and section label. If a
feature appears selectively in a specific section, the
?2 value is expected to be high. Thus, we extract the
top 200,000 features2 that have high ?2 values to re-
duce the total number of features. Table 3.2 shows
examples of the top five bigrams that have high ?2
values.
Relative sentence location An abstract is likely to
state objective of the study at the beginning and its
conclusion at the end. The position of a sentence
may be a good clue for determining its section la-
bel. Thus, we design five binary features to indicate
relative position of sentences in five scales.
Features from previous/next w sentences This
reproduces features from previous and following w
sentences to the current sentence (w = {0, 1, 2}),
so that a classifier can make use of the content of
the surrounding sentences. Duplicated features have
prefixes (e.g. PREV_ and NEXT_) to distinguish
their origins.
3.3 Section labels
It would require much effort and time to prepare a
large amount of abstracts annotated with section la-
bels. Fortunately, some Medline abstracts have sec-
tion labels stated explicitly by its authors. We ex-
amined section labels in 7,811,582 abstracts in the
whole Medline3, using the regular-expression pat-
tern:
?[A-Z]+([ ][A-Z]+){0,3}:[ ]
A sentence is qualified to have a section name if it
begins with up to 4 uppercase token(s) followed by
2We chose the number of features based on exploratory ex-
periments.
3The Medline database was up-to-date on March 2006.
a colon ?:?. This pattern identified 683,207 (ca. 9%)
abstracts with structured sections.
Table 3 shows typical moves of sections in Med-
line abstracts. The majority of sequences in this
table consists of four sections compatible with the
ANSI standard, purpose, methods, results, and con-
clusions. Moreover, the most frequent sequence
is ?OBJECTIVE ? METHOD(S) ? RESULTS
? CONCLUSION(S),? supposing that AIM and
PURPOSE are equivalent to OBJECTIVE. Hence,
this study assumes four sections, OBJECTIVE,
METHOD, RESULTS, and CONCLUSIONS.
Meanwhile, it is common for NP chunking tasks
to represent a chunk (e.g., NP) with two labels,
the begin (e.g., B-NP) and inside (e.g., I-NP) of
a chunk (Ramshaw and Marcus, 1995). Although
none of the previous studies employed this repre-
sentation, attaching B- and I- prefixes to section la-
bels may improve a classifier by associating clue
phrases (e.g., ?to determine?) with the starts of sec-
tions (e.g., B-OBJECTIVE). We will compare clas-
sification performances on two sets of label repre-
sentations: namely, we will compare four section
labels and eight labels with BI prefixes attached to
section names.
4 Evaluation
4.1 Experiment
We constructed two sets of corpora (?pure? and ?ex-
panded?), each of which contains 51,000 abstracts
sampled from the abstracts with structured sections.
The ?pure? corpus consists of abstracts that have the
exact four section labels. In other words, this cor-
pus does not include AIM or PURPOSE sentences
even though they are equivalent to OBJECTIVE sen-
tences. The ?pure? corpus is useful to compare the
performance of this study with the previous work.
384
Rank # abstracts (%) Section sequence
1 111,617 (17.6) OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
2 107,124 (16.9) BACKGROUND(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
3 40,083 (6.3) PURPOSE?METHOD(S)? RESULT(S)? CONCLUSION(S)
4 20,519 (3.2) PURPOSE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
5 16,705 (2.6) AIM(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
6 16,400 (2.6) BACKGROUND? OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
7 12,227 (1.9) OBJECTIVE? STUDY DESIGN? RESULT(S)? CONCLUSION(S)
8 11,483 (1.8) BACKGROUND?METHOD(S) AND RESULT(S)? CONCLUSION(S)
9 8,866 (1.4) OBJECTIVE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
10 8,537 (1.3) PURPOSE? PATIENT AND METHOD(S)? RESULT(S)? CONCLUSION(S)
.. ... ... ...
Total 683,207 (100.0)
Table 3: Typical sequences of sections in Medline abstracts
Representative Equivalent section labels
OBJECTIVE AIM, AIM OF THE STUDY, AIMS, BACKGROUND/AIMS, BACKGROUND/PURPOSE, BACK-
GROUND, BACKGROUND AND AIMS, BACKGROUND AND OBJECTIVE, BACKGROUND AND
OBJECTIVES, BACKGROUND AND PURPOSE, CONTEXT, INTRODUCTION, OBJECT, OBJEC-
TIVE, OBJECTIVES, PROBLEM, PURPOSE, STUDY OBJECTIVE, STUDY OBJECTIVES, SUM-
MARY OF BACKGROUND DATA
METHOD ANIMALS, DESIGN, DESIGN AND METHODS, DESIGN AND SETTING, EXPERIMENTAL DE-
SIGN,INTERVENTION, INTERVENTION(S), INTERVENTIONS, MATERIAL AND METHODS, MA-
TERIALS AND METHODS, MEASUREMENTS, METHOD, METHODOLOGY, METHODS, METH-
ODS AND MATERIALS, PARTICIPANTS, PATIENT(S), PATIENTS, PATIENTS AND METHODS,
PROCEDURE, RESEARCH DESIGN AND METHODS, SETTING, STUDY DESIGN, STUDY DESIGN
AND METHODS, SUBJECTS, SUBJECTS AND METHODS
RESULTS FINDINGS, MAIN RESULTS, RESULT, RESULT(S), RESULTS
CONCLUSIONS CONCLUSION, CONCLUSION(S), CONCLUSIONS, CONCLUSIONS AND CLINICAL RELE-
VANCE, DISCUSSION, IMPLICATIONS, INTERPRETATION, INTERPRETATION AND CONCLU-
SIONS
Table 4: Representative section names and their expanded sections
In contrast, the ?expanded? corpus includes sen-
tences in equivalent sections: AIM and PURPOSE
sentences are mapped to the OBJECTIVE. Table 4
shows the sets of equivalent sections for representa-
tive sections. We created this mapping table man-
ually by analyzing the top 100 frequent section la-
bels found in the Medline. The ?expanded? corpus
is close to the real situation in which the proposed
method annotates unstructured abstracts.
We utilized FlexCRFs4 implementation to build
a classifier with linear-chain CRFs. As a baseline
method, we also prepared an SVM classifier5 with
the same features.
4Flexible Conditional Random Field Toolkit (FlexCRFs):
http://flexcrfs.sourceforge.net/
5We used SVMlight implementation with the linear kernel,
which achieved the best accuracy through this experiment:
http://svmlight.joachims.org/
 20
 30
 40
 50
 60
 70
 80
 1000  10000  100000
Pe
r-a
bs
tra
ct
 a
cc
ur
ac
y 
(%
)
Number of abstracts for training
CRF
SVM
Figure 2: Training curve
4.2 Results
Given the number of abstracts for training n, we ran-
domly sampled n abstracts from a corpus for train-
ing and 1,000 abtracts for testing. Content (n-gram)
features were generated for each trainig set. We
385
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 88.7 (42.4) 81.5 (19.1) 85.7 (33.0) 83.3 (23.4)
n-gram + position 93.4 (59.7) 88.2 (35.5) 92.4 (55.4) 89.6 (39.4)
n-gram + surrounding (w = 1) 93.3 (60.4) 89.9 (42.2) 92.1 (52.8) 90.0 (42.0)
n-gram + surrounding (w = 2) 93.7 (61.1) 91.8 (49.4) 92.8 (54.3) 91.8 (47.0)
Full 94.3 (62.9) 93.3 (55.5) 93.3 (56.1) 92.9 (52.2)
Table 5: Classification performance (accuracy) on ?pure? corpus (n = 10, 000)
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 87.7 (35.6) 78.5 (14.5) 81.9 (21.0) 80.0 (16.2)
n-gram + position 92.6 (54.3) 87.1 (31.2) 91.4 (48.7) 88.1 (31.2)
n-gram + surrounding (w = 1) 92.3 (52.0) 88.5 (37.6) 89.9 (44.0) 88.4 (37.1)
n-gram + surrounding (w = 2) 92.4 (52.5) 90.1 (41.1) 91.2 (46.6) 90.4 (41.6)
Full 93.0 (55.0) 92.0 (47.3) 92.5 (50.9) 91.7 (44.0)
Table 6: Classification performance (accuracy) on ?expanded? corpus (n = 10, 000)
measured the classification accuracy of sentences
(per-sentence accuracy) and abstracts (per-abstract
accuracy). In per-abstract accuracy, an abstract is
considered correct if all constituent sentences are
correctly labeled.
Trained with n = 50, 000 abstracts from ?pure?
corpus, the proposed method achieved 95.5% per-
sentence accuracy and 68.8% per-abstract accuracy.
The F-score for each section label was 98.7% (O),
95.8% (M), 95.0% (R), and 94.2% (C). The pro-
posed method performed this task better than the
previous studies by a great margin. Figure 2 shows
the training curve for the ?pure? corpus with all fea-
tures presented in this paper. CRF and SVM meth-
ods performed better with more abstracts used for
training. This training curve demonstrated that, with
less than half the number of training corpus, the pro-
posed method could achieve the same accuracy as
the baseline method.
Tables 5 and 6 report the performance of the
proposed and baseline methods on ?pure? and ?ex-
panded? corpora respectively (n = 10, 000). These
tables show per-sentence accuracy followed by per-
abstract accuracy in parentheses with different con-
figurations of features (row) and label representa-
tions (column). For example, the proposed method
obtained 94.3% per-sentence accuracy and 62.9%
per-abstract accuracy with 10,000 training abstracts
from ?pure? corpus, all features, and BI prefixes for
class labels.
The proposed method outperformed the baseline
method in all experimental configurations. This
suggests that CRFs are more suitable for modeling
moves of rhetorical roles in scientific abstracts. It
is noteworthy that the CRF classifier gained higher
per-abstract accuracy than the SVM. For example,
both the CRF classifier with features from surround-
ing sentences (w = 1), and SVM classifier with full
features, obtained 93.3% per-sentence accuracy in
Table 5. Nevertheless, the per-abstract accuracies of
the former and latter were 60.4% and 55.5% respec-
tively: the CRF classifier had roughly 5% advantage
on per-abstract accuracy over SVM. This analysis
reflects the capability of CRFs to determine the op-
timal sequence of section names.
Additional features such as sentence position and
surrounding sentences improved the performance by
ca. 5?10%. The proposed method achieved the best
results with all features. Another interesting discus-
sion arises with regard to the representations of sec-
tion labels. The BI representation always boosted
the per-abstract accuracy of CRF classifiers by ca.
4?14%. In contrast, the SVM classifier could not
leverage the BI representation, and in some configu-
rations, even degraded the accuracy.
386
5 Conclusion
This paper presented a novel approach to identifying
rhetorical roles in scientific abstracts using CRFs.
The proposed method achieved more successful re-
sults than any other previous reports. The CRF clas-
sifier had roughly 5% advantage on per-abstract ac-
curacy over SVM. The BI representation of section
names also boosted the classification accuracy by
5%. In total, the proposed method gained more than
10% improvement on per-abstract accuracy.
We have evaluated the proposed method only on
medical literatures. In addition to improving the
classification performance, a future direction for this
study would be to examine the adaptability of the
proposed method to include other types of texts. We
are planning to construct a summarization system
using the proposed method.
References
ANSI. 1979. American national standard for writing
abstracts. Z39.14-1979, American National Standards
Institute (ANSI).
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
John N. Darroch and Douglas Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Harold P. Edmundson. 1969. New methods in automatic
extracting. Journal of the Association for Computing
Machinery, 16(2):264?285.
Christoph H. Gleiter, Tilmann Becker, Katharina H.
Schreeb, Stefan Freudenthaler, and Ursula Gundert-
Remy. 1997. Fenoterol but not dobutamine increases
erythropoietin production in humans. Clinical Phar-
macology & Therapeutics, 61(6):669?676.
Naomi Graetz. 1985. Teaching EFL students to extract
structural information from abstracts. In Jan M. Ulijn
and Anthony K. Pugh, editors, Reading for Profes-
sional Purposed: Methods and Materials in Teaching
Languages, pages 123?135. Acco, Leuven, Belgium.
Takahiko Ito, Masashi Simbo, Takahiro Yamasaki, and
Yuji Matsumoto. 2004. Semi-supervised sentence
classification for medline documents. In IPSJ SIG
Technical Report, volume 2004-ICS-138, pages 141?
146.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML-2001), pages 282?289.
Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,
and Sanjeev Khudanpur. 2006. Generative con-
tent models for structural analysis of medical ab-
stracts. In Proceedings of the HLT/NAACL 2006
Workshop on Biomedical Natural Language Process-
ing (BioNLP?06), pages 65?72, New York City, USA.
Daniel Marcu. 1999. Discourse trees are good indicators
of importance in text. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Larry McKnight and Padmini Arinivasan. 2003. Cate-
gorization of sentence types in medical abstracts. In
AMIA 2003 Symposium Proceedings, pages 440?444.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer-Verlag, New York, USA.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of Corpus Linguistics 2001 Confer-
ence, pages 433 ? 443, Lancaster University, Lan-
caster, UK.
Chris D. Paice. 1981. The automatic generation of litera-
ture abstracts: an approach based on the identification
of self-indicating phrases. In SIGIR ?80: Proceedings
of the 3rd annual ACM conference on Research and
development in information retrieval, pages 172?191,
Kent, UK. Butterworth & Co.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the ACL 3rd Workshop on Very Large Cor-
pora, pages 82?94.
Patrick Ruch, Celia Boyer, Christine Chichester, Imad
Tbahriti, Antoine Geissbu?hler, Paul Fabry, Julien Gob-
eill, Violaine Pillet, Dietrich Rebholz-Schuhmann,
Christian Lovis, and Anne-Lise Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. International Journal of Medical Infor-
matics, 76(2?3):195?200.
Franc?oise Salanger-Meyer. 1990. Discoursal flaws
in medical english abstracts: A genre analysis per
research- and text-type. Text, 10(4):365?384.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 134?
141, Edmonton, Canada.
387
Masashi Shimbo, Takahiro Yamasaki, and Yuji Mat-
sumoto. 2003. Using sectioning information for text
retrieval: a case study with the medline abstracts. In
Proceedings of Second International Workshop on Ac-
tive Mining (AM?03), pages 32?41.
John M. Swales, 1990. Genre Analysis: English in aca-
demic and research settings, chapter 6. Cambridge
University Press, UK.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2006. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the medline digital
library. International Journal OF Medical Informat-
ics, 75(6):488?495.
Simone Teufel and Marc Moens. 2002. Summa-
rizing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467?474, Vancouver, British Columbia, Canada.
Jien-Chen Wu, Yu-Chia Chang, Hsien-Chin Liou, and
Jason S. Chang. 2006. Computational analysis of
move structures in academic abstracts. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions, pages 41?44, Sydney, Australia.
Yasunori Yamamoto and Toshihisa Takagi. 2005. A sen-
tence classification system for multi-document sum-
marization in the biomedical domain. In Proceedings
of the International Workshop on Biomedical Data En-
gineering (BMDE2005), pages 90?95.
388
TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477?485,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Stochastic Gradient Descent Training for
L1-regularized Log-linear Models with Cumulative Penalty
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Stochastic gradient descent (SGD) uses
approximate gradients estimated from
subsets of the training data and updates
the parameters in an online fashion. This
learning framework is attractive because
it often requires much less training time
in practice than batch training algorithms.
However, L1-regularization, which is be-
coming popular in natural language pro-
cessing because of its ability to pro-
duce compact models, cannot be effi-
ciently applied in SGD training, due to
the large dimensions of feature vectors
and the fluctuations of approximate gra-
dients. We present a simple method to
solve these problems by penalizing the
weights according to cumulative values for
L1 penalty. We evaluate the effectiveness
of our method in three applications: text
chunking, named entity recognition, and
part-of-speech tagging. Experimental re-
sults demonstrate that our method can pro-
duce compact and accurate models much
more quickly than a state-of-the-art quasi-
Newton method for L1-regularized log-
linear models.
1 Introduction
Log-linear models (a.k.a maximum entropy mod-
els) are one of the most widely-used probabilistic
models in the field of natural language process-
ing (NLP). The applications range from simple
classification tasks such as text classification and
history-based tagging (Ratnaparkhi, 1996) to more
complex structured prediction tasks such as part-
of-speech (POS) tagging (Lafferty et al, 2001),
syntactic parsing (Clark and Curran, 2004) and se-
mantic role labeling (Toutanova et al, 2005). Log-
linear models have a major advantage over other
discriminative machine learning models such as
support vector machines?their probabilistic out-
put allows the information on the confidence of
the decision to be used by other components in the
text processing pipeline.
The training of log-liner models is typically per-
formed based on the maximum likelihood crite-
rion, which aims to obtain the weights of the fea-
tures that maximize the conditional likelihood of
the training data. In maximum likelihood training,
regularization is normally needed to prevent the
model from overfitting the training data,
The two most common regularization methods
are called L1 and L2 regularization. L1 regular-
ization penalizes the weight vector for its L1-norm
(i.e. the sum of the absolute values of the weights),
whereas L2 regularization uses its L2-norm. There
is usually not a considerable difference between
the two methods in terms of the accuracy of the
resulting model (Gao et al, 2007), but L1 regu-
larization has a significant advantage in practice.
Because many of the weights of the features be-
come zero as a result of L1-regularized training,
the size of the model can be much smaller than that
produced by L2-regularization. Compact models
require less space on memory and storage, and en-
able the application to start up quickly. These mer-
its can be of vital importance when the application
is deployed in resource-tight environments such as
cell-phones.
A common way to train a large-scale L1-
regularized model is to use a quasi-Newton
method. Kazama and Tsujii (2003) describe a
method for training a L1-regularized log-linear
model with a bound constrained version of the
BFGS algorithm (Nocedal, 1980). Andrew and
Gao (2007) present an algorithm called Orthant-
Wise Limited-memory Quasi-Newton (OWL-
QN), which can work on the BFGS algorithm
without bound constraints and achieve faster con-
vergence.
477
An alternative approach to training a log-linear
model is to use stochastic gradient descent (SGD)
methods. SGD uses approximate gradients esti-
mated from subsets of the training data and up-
dates the weights of the features in an online
fashion?the weights are updated much more fre-
quently than batch training algorithms. This learn-
ing framework is attracting attention because it of-
ten requires much less training time in practice
than batch training algorithms, especially when
the training data is large and redundant. SGD was
recently used for NLP tasks including machine
translation (Tillmann and Zhang, 2006) and syn-
tactic parsing (Smith and Eisner, 2008; Finkel et
al., 2008). Also, SGD is very easy to implement
because it does not need to use the Hessian infor-
mation on the objective function. The implemen-
tation could be as simple as the perceptron algo-
rithm.
Although SGD is a very attractive learning
framework, the direct application of L1 regular-
ization in this learning framework does not result
in efficient training. The first problem is the inef-
ficiency of applying the L1 penalty to the weights
of all features. In NLP applications, the dimen-
sion of the feature space tends to be very large?it
can easily become several millions, so the appli-
cation of L1 penalty to all features significantly
slows down the weight updating process. The sec-
ond problem is that the naive application of L1
penalty in SGD does not always lead to compact
models, because the approximate gradient used at
each update is very noisy, so the weights of the
features can be easily moved away from zero by
those fluctuations.
In this paper, we present a simple method for
solving these two problems in SGD learning. The
main idea is to keep track of the total penalty and
the penalty that has been applied to each weight,
so that the L1 penalty is applied based on the dif-
ference between those cumulative values. That
way, the application of L1 penalty is needed only
for the features that are used in the current sample,
and also the effect of noisy gradient is smoothed
away.
We evaluate the effectiveness of our method
by using linear-chain conditional random fields
(CRFs) and three traditional NLP tasks, namely,
text chunking (shallow parsing), named entity
recognition, and POS tagging. We show that our
enhanced SGD learning method can produce com-
pact and accurate models much more quickly than
the OWL-QN algorithm.
This paper is organized as follows. Section 2
provides a general description of log-linear mod-
els used in NLP. Section 3 describes our stochastic
gradient descent method for L1-regularized log-
linear models. Experimental results are presented
in Section 4. Some related work is discussed in
Section 5. Section 6 gives some concluding re-
marks.
2 Log-Linear Models
In this section, we briefly describe log-linear mod-
els used in NLP tasks and L1 regularization.
A log-linear model defines the following prob-
abilistic distribution over possible structure y for
input x:
p(y|x) = 1Z(x) exp
?
i
wifi(y,x),
where fi(y,x) is a function indicating the occur-
rence of feature i, wi is the weight of the feature,
and Z(x) is a partition (normalization) function:
Z(x) =
?
y
exp
?
i
wifi(y,x).
If the structure is a sequence, the model is called
a linear-chain CRF model, and the marginal prob-
abilities of the features and the partition function
can be efficiently computed by using the forward-
backward algorithm. The model is used for a va-
riety of sequence labeling tasks such as POS tag-
ging, chunking, and named entity recognition.
If the structure is a tree, the model is called a
tree CRF model, and the marginal probabilities
can be computed by using the inside-outside algo-
rithm. The model can be used for tasks like syn-
tactic parsing (Finkel et al, 2008) and semantic
role labeling (Cohn and Blunsom, 2005).
2.1 Training
The weights of the features in a log-linear model
are optimized in such a way that they maximize
the regularized conditional log-likelihood of the
training data:
Lw =
N
?
j=1
log p(yj |xj ;w)?R(w), (1)
where N is the number of training samples, yj is
the correct output for input xj , and R(w) is the
478
regularization term which prevents the model from
overfitting the training data. In the case of L1 reg-
ularization, the term is defined as:
R(w) = C
?
i
|wi|,
where C is the meta-parameter that controls the
degree of regularization, which is usually tuned by
cross-validation or using the heldout data.
In what follows, we denote by L(j,w)
the conditional log-likelihood of each sample
log p(yj |xj ;w). Equation 1 is rewritten as:
Lw =
N
?
j=1
L(j,w)? C
?
i
|wi|. (2)
3 Stochastic Gradient Descent
SGD uses a small randomly-selected subset of the
training samples to approximate the gradient of
the objective function given by Equation 2. The
number of training samples used for this approx-
imation is called the batch size. When the batch
size is N , the SGD training simply translates into
gradient descent (hence is very slow to converge).
By using a small batch size, one can update the
parameters more frequently than gradient descent
and speed up the convergence. The extreme case
is a batch size of 1, and it gives the maximum
frequency of updates and leads to a very simple
perceptron-like algorithm, which we adopt in this
work.1
Apart from using a single training sample to
approximate the gradient, the optimization proce-
dure is the same as simple gradient descent,2 so
the weights of the features are updated at training
sample j as follows:
wk+1 = wk + ?k
?
?w (L(j,w)?
C
N
?
i
|wi|),
where k is the iteration counter and ?k is the learn-
ing rate, which is normally designed to decrease
as the iteration proceeds. The actual learning rate
scheduling methods used in our experiments are
described later in Section 3.3.
1In the actual implementation, we randomly shuffled the
training samples at the beginning of each pass, and then
picked them up sequentially.
2What we actually do here is gradient ascent, but we stick
to the term ?gradient descent?.
3.1 L1 regularization
The update equation for the weight of each feature
i is as follows:
wik+1 = wik + ?k
?
?wi
(L(j,w)? CN |wi|).
The difficulty with L1 regularization is that the
last term on the right-hand side of the above equa-
tion is not differentiable when the weight is zero.
One straightforward solution to this problem is to
consider a subgradient at zero and use the follow-
ing update equation:
wik+1 = wik + ?k
?L(j,w)
?wi
? CN ?ksign(w
k
i ),
where sign(x) = 1 if x > 0, sign(x) = ?1 if x <
0, and sign(x) = 0 if x = 0. In this paper, we call
this weight updating method ?SGD-L1 (Naive)?.
This naive method has two serious problems.
The first problem is that, at each update, we need
to perform the application of L1 penalty to all fea-
tures, including the features that are not used in
the current training sample. Since the dimension
of the feature space can be very large, it can sig-
nificantly slow down the weight update process.
The second problem is that it does not produce
a compact model, i.e. most of the weights of the
features do not become zero as a result of train-
ing. Note that the weight of a feature does not be-
come zero unless it happens to fall on zero exactly,
which rarely happens in practice.
Carpenter (2008) describes an alternative ap-
proach. The weight updating process is divided
into two steps. First, the weight is updated with-
out considering the L1 penalty term. Then, the
L1 penalty is applied to the weight to the extent
that it does not change its sign. In other words,
the weight is clipped when it crosses zero. Their
weight update procedure is as follows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ?
C
N ?k),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i +
C
N ?k).
In this paper, we call this update method ?SGD-
L1 (Clipping)?. It should be noted that this method
479
-0.1
-0.05
 0
 0.05
 0.1
 0  1000  2000  3000  4000  5000  6000
W
ei
gh
t
Updates
Figure 1: An example of weight updates.
is actually a special case of the FOLOS algorithm
(Duchi and Singer, 2008) and the truncated gradi-
ent method (Langford et al, 2009).
The obvious advantage of using this method is
that we can expect many of the weights of the
features to become zero during training. Another
merit is that it allows us to perform the applica-
tion of L1 penalty in a lazy fashion, so that we
do not need to update the weights of the features
that are not used in the current sample, which leads
to much faster training when the dimension of the
feature space is large. See the aforementioned pa-
pers for the details. In this paper, we call this effi-
cient implementation ?SGD-L1 (Clipping + Lazy-
Update)?.
3.2 L1 regularization with cumulative
penalty
Unfortunately, the clipping-at-zero approach does
not solve all problems. Still, we often end up with
many features whose weights are not zero. Re-
call that the gradient used in SGD is a crude ap-
proximation to the true gradient and is very noisy.
The weight of a feature is, therefore, easily moved
away from zero when the feature is used in the
current sample.
Figure 1 gives an illustrative example in which
the weight of a feature fails to become zero. The
figure shows how the weight of a feature changes
during training. The weight goes up sharply when
it is used in the sample and then is pulled back
toward zero gradually by the L1 penalty. There-
fore, the weight fails to become zero if the feature
is used toward the end of training, which is the
case in this example. Note that the weight would
become zero if the true (fluctuationless) gradient
were used?at each update the weight would go
up a little and be pulled back to zero straightaway.
Here, we present a different strategy for apply-
ing the L1 penalty to the weights of the features.
The key idea is to smooth out the effect of fluctu-
ating gradients by considering the cumulative ef-
fects from L1 penalty.
Let uk be the absolute value of the total L1-
penalty that each weight could have received up
to the point. Since the absolute value of the L1
penalty does not depend on the weight and we are
using the same regularization constant C for all
weights, it is simply accumulated as:
uk =
C
N
k
?
t=1
?t. (3)
At each training sample, we update the weights
of the features that are used in the sample as fol-
lows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ? (uk + qk?1i )),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i + (uk ? qk?1i )),
where qki is the total L1-penalty that wi has actu-
ally received up to the point:
qki =
k
?
t=1
(wt+1i ? w
t+ 12
i ). (4)
This weight updating method penalizes the
weight according to the difference between uk and
qk?1i . In effect, it forces the weight to receive the
total L1 penalty that would have been applied if
the weight had been updated by the true gradients,
assuming that the current weight vector resides in
the same orthant as the true weight vector.
It should be noted that this method is basi-
cally equivalent to a ?SGD-L1 (Clipping + Lazy-
Update)? method if we were able to use the true
gradients instead of the stochastic gradients.
In this paper, we call this weight updating
method ?SGD-L1 (Cumulative)?. The implemen-
tation of this method is very simple. Figure 2
shows the whole SGD training algorithm with this
strategy in pseudo-code.
480
1: procedure TRAIN(C)
2: u? 0
3: Initialize wi and qi with zero for all i
4: for k = 0 to MaxIterations
5: ? ? LEARNINGRATE(k)
6: u? u + ?C/N
7: Select sample j randomly
8: UPDATEWEIGHTS(j)
9:
10: procedure UPDATEWEIGHTS(j)
11: for i ? features used in sample j
12: wi ? wi + ? ?L(j,w)?wi
13: APPLYPENALTY(i)
14:
15: procedure APPLYPENALTY(i)
16: z ? wi
17: if wi > 0 then
18: wi ? max(0, wi ? (u + qi))
19: else if wi < 0 then
20: wi ? min(0, wi + (u? qi))
21: qi ? qi + (wi ? z)
22:
Figure 2: Stochastic gradient descent training with
cumulative L1 penalty. z is a temporary variable.
3.3 Learning Rate
The scheduling of learning rates often has a major
impact on the convergence speed in SGD training.
A typical choice of learning rate scheduling can
be found in (Collins et al, 2008):
?k =
?0
1 + k/N , (5)
where ?0 is a constant. Although this scheduling
guarantees ultimate convergence, the actual speed
of convergence can be poor in practice (Darken
and Moody, 1990).
In this work, we also tested simple exponential
decay:
?k = ?0??k/N , (6)
where ? is a constant. In our experiments, we
found this scheduling more practical than that
given in Equation 5. This is mainly because ex-
ponential decay sweeps the range of learning rates
more smoothly?the learning rate given in Equa-
tion 5 drops too fast at the beginning and too
slowly at the end.
It should be noted that exponential decay is not
a good choice from a theoretical point of view, be-
cause it does not satisfy one of the necessary con-
ditions for convergence?the sum of the learning
rates must diverge to infinity (Spall, 2005). How-
ever, this is probably not a big issue for practition-
ers because normally the training has to be termi-
nated at a certain number of iterations in practice.3
4 Experiments
We evaluate the effectiveness our training algo-
rithm using linear-chain CRF models and three
NLP tasks: text chunking, named entity recogni-
tion, and POS tagging.
To compare our algorithm with the state-of-the-
art, we present the performance of the OWL-QN
algorithm on the same data. We used the publicly
available OWL-QN optimizer developed by An-
drew and Gao.4 The meta-parameters for learning
were left unchanged from the default settings of
the software: the convergence tolerance was 1e-4;
and the L-BFGS memory parameter was 10.
4.1 Text Chunking
The first set of experiments used the text chunk-
ing data set provided for the CoNLL 2000 shared
task.5 The training data consists of 8,936 sen-
tences in which each token is annotated with the
?IOB? tags representing text chunks such as noun
and verb phrases. We separated 1,000 sentences
from the training data and used them as the held-
out data. The test data provided by the shared task
was used only for the final accuracy report.
The features used in this experiment were uni-
grams and bigrams of neighboring words, and un-
igrams, bigrams and trigrams of neighboring POS
tags.
To avoid giving any advantage to our SGD al-
gorithms over the OWL-QN algorithm in terms of
the accuracy of the resulting model, the OWL-QN
algorithm was used when tuning the regularization
parameter C. The tuning was performed in such a
way that it maximized the likelihood of the heldout
data. The learning rate parameters for SGD were
then tuned in such a way that they maximized the
value of the objective function in 30 passes. We
first determined ?0 by testing 1.0, 0.5, 0.2, and 0.1.
We then determined ? by testing 0.9, 0.85, and 0.8
with the fixed ?0.
3This issue could also be sidestepped by, for example,
adding a small O(1/k) term to the learning rate.
4Available from the original developers? websites:
http://research.microsoft.com/en-us/people/galena/ or
http://research.microsoft.com/en-us/um/people/jfgao/
5http://www.cnts.ua.ac.be/conll2000/chunking/
481
Passes Lw/N # Features Time (sec) F-score
OWL-QN 160 -1.583 18,109 598 93.62
SGD-L1 (Naive) 30 -1.671 455,651 1,117 93.64
SGD-L1 (Clipping + Lazy-Update) 30 -1.671 87,792 144 93.65
SGD-L1 (Cumulative) 30 -1.653 28,189 149 93.68
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.622 23,584 148 93.66
Table 1: CoNLL-2000 Chunking task. Training time and accuracy of the trained model on the test data.
-2.4
-2.2
-2
-1.8
-1.6
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 3: CoNLL 2000 chunking task: Objective
 0
 50000
 100000
 150000
 200000
 0  10  20  30  40  50
# 
Ac
tiv
e 
fe
at
ur
es
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 4: CoNLL 2000 chunking task: Number of
active features.
Figures 3 and 4 show the training process of
the model. Each figure contains four curves repre-
senting the results of the OWL-QN algorithm and
three SGD-based algorithms. ?SGD-L1 (Cumu-
lative + ED)? represents the results of our cumu-
lative penalty-based method that uses exponential
decay (ED) for learning rate scheduling.
Figure 3 shows how the value of the objec-
tive function changed as the training proceeded.
SGD-based algorithms show much faster conver-
gence than the OWL-QN algorithm. Notice also
that ?SGD-L1 (Cumulative)? improves the objec-
tive slightly faster than ?SGD-L1 (Clipping)?. The
result of ?SGD-L1 (Naive)? is not shown in this
figure, but the curve was almost identical to that
of ?SGD-L1 (Clipping)?.
Figure 4 shows the numbers of active features
(the features whose weight are not zero). It is
clearly seen that the clipping-at-zero approach
fails to reduce the number of active features, while
our algorithms succeeded in reducing the number
of active features to the same level as OWL-QN.
We then trained the models using the whole
training data (including the heldout data) and eval-
uated the accuracy of the chunker on the test data.
The number of passes performed over the train-
ing data in SGD was set to 30. The results are
shown in Table 1. The second column shows the
number of passes performed in the training. The
third column shows the final value of the objective
function per sample. The fourth column shows
the number of resulting active features. The fifth
column show the training time. The last column
shows the f-score (harmonic mean of recall and
precision) of the chunking results. There was no
significant difference between the models in terms
of accuracy. The naive SGD training took much
longer than OWL-QN because of the overhead of
applying L1 penalty to all dimensions.
Our SGD algorithms finished training in 150
seconds on Xeon 2.13GHz processors. The
CRF++ version 0.50, a popular CRF library de-
veloped by Taku Kudo,6 is reported to take 4,021
seconds on Xeon 3.0GHz processors to train the
model using a richer feature set.7 CRFsuite ver-
sion 0.4, a much faster library for CRFs, is re-
ported to take 382 seconds on Xeon 3.0GHz, using
the same feature set as ours.8 Their library uses the
OWL-QN algorithm for optimization. Although
direct comparison of training times is not impor-
6http://crfpp.sourceforge.net/
7http://www.chokkan.org/software/crfsuite/benchmark.html
8ditto
482
tant due to the differences in implementation and
hardware platforms, these results demonstrate that
our algorithm can actually result in a very fast im-
plementation of a CRF trainer.
4.2 Named Entity Recognition
The second set of experiments used the named
entity recognition data set provided for the
BioNLP/NLPBA 2004 shared task (Kim et al,
2004).9 The training data consist of 18,546 sen-
tences in which each token is annotated with the
?IOB? tags representing biomedical named enti-
ties such as the names of proteins and RNAs.
The training and test data were preprocessed
by the GENIA tagger,10 which provided POS tags
and chunk tags. We did not use any information on
the named entity tags output by the GENIA tagger.
For the features, we used unigrams of neighboring
chunk tags, substrings (shorter than 10 characters)
of the current word, and the shape of the word (e.g.
?IL-2? is converted into ?AA-#?), on top of the
features used in the text chunking experiments.
The results are shown in Figure 5 and Table
2. The trend in the results is the same as that of
the text chunking task: our SGD algorithms show
much faster convergence than the OWL-QN algo-
rithm and produce compact models.
Okanohara et al (2006) report an f-score of
71.48 on the same data, using semi-Markov CRFs.
4.3 Part-Of-Speech Tagging
The third set of experiments used the POS tag-
ging data in the Penn Treebank (Marcus et al,
1994). Following (Collins, 2002), we used sec-
tions 0-18 of the Wall Street Journal (WSJ) corpus
for training, sections 19-21 for development, and
sections 22-24 for final evaluation. The POS tags
were extracted from the parse trees in the corpus.
All experiments for this work, including the tun-
ing of features and parameters for regularization,
were carried out using the training and develop-
ment sets. The test set was used only for the final
accuracy report.
It should be noted that training a CRF-based
POS tagger using the whole WSJ corpus is not a
trivial task and was once even deemed impractical
in previous studies. For example, Wellner and Vi-
lain (2006) abandoned maximum likelihood train-
9The data is available for download at http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/ERtask/report.html
10http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
-3.8
-3.6
-3.4
-3.2
-3
-2.8
-2.6
-2.4
-2.2
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 5: NLPBA 2004 named entity recognition
task: Objective.
-2.8
-2.7
-2.6
-2.5
-2.4
-2.3
-2.2
-2.1
-2
-1.9
-1.8
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 6: POS tagging task: Objective.
ing because it was ?prohibitive? (7-8 days for sec-
tions 0-18 of the WSJ corpus).
For the features, we used unigrams and bigrams
of neighboring words, prefixes and suffixes of
the current word, and some characteristics of the
word. We also normalized the current word by
lowering capital letters and converting all the nu-
merals into ?#?, and used the normalized word as a
feature.
The results are shown in Figure 6 and Table 3.
Again, the trend is the same. Our algorithms fin-
ished training in about 30 minutes, producing ac-
curate models that are as compact as that produced
by OWL-QN.
Shen et al, (2007) report an accuracy of 97.33%
on the same data set using a perceptron-based bidi-
rectional tagging model.
5 Discussion
An alternative approach to producing compact
models for log-linear models is to reformulate the
483
Passes Lw/N # Features Time (sec) F-score
OWL-QN 161 -2.448 30,710 2,253 71.76
SGD-L1 (Naive) 30 -2.537 1,032,962 4,528 71.20
SGD-L1 (Clipping + Lazy-Update) 30 -2.538 279,886 585 71.20
SGD-L1 (Cumulative) 30 -2.479 31,986 631 71.40
SGD-L1 (Cumulative + Exponential-Decay) 30 -2.443 25,965 631 71.63
Table 2: NLPBA 2004 Named entity recognition task. Training time and accuracy of the trained model
on the test data.
Passes Lw/N # Features Time (sec) Accuracy
OWL-QN 124 -1.941 50,870 5,623 97.16%
SGD-L1 (Naive) 30 -2.013 2,142,130 18,471 97.18%
SGD-L1 (Clipping + Lazy-Update) 30 -2.013 323,199 1,680 97.18%
SGD-L1 (Cumulative) 30 -1.987 62,043 1,777 97.19%
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.954 51,857 1,774 97.17%
Table 3: POS tagging on the WSJ corpus. Training time and accuracy of the trained model on the test
data.
problem as a L1-constrained problem (Lee et al,
2006), where the conditional log-likelihood of the
training data is maximized under a fixed constraint
of the L1-norm of the weight vector. Duchi et
al. (2008) describe efficient algorithms for pro-
jecting a weight vector onto the L1-ball. Although
L1-regularized and L1-constrained learning algo-
rithms are not directly comparable because the ob-
jective functions are different, it would be inter-
esting to compare the two approaches in terms
of practicality. It should be noted, however, that
the efficient algorithm presented in (Duchi et al,
2008) needs to employ a red-black tree and is
rather complex.
In SGD learning, the need for tuning the meta-
parameters for learning rate scheduling can be an-
noying. In the case of exponential decay, the set-
ting of ? = 0.85 turned out to be a good rule
of thumb in our experiments?it always produced
near best results in 30 passes, but the other param-
eter ?0 needed to be tuned. It would be very useful
if those meta-parameters could be tuned in a fully
automatic way.
There are some sophisticated algorithms for
adaptive learning rate scheduling in SGD learning
(Vishwanathan et al, 2006; Huang et al, 2007).
However, those algorithms use second-order infor-
mation (i.e. Hessian information) and thus need
access to the weights of the features that are not
used in the current sample, which should slow
down the weight updating process for the same
reason discussed earlier. It would be interesting
to investigate whether those sophisticated learning
scheduling algorithms can actually result in fast
training in large-scale NLP tasks.
6 Conclusion
We have presented a new variant of SGD that can
efficiently train L1-regularized log-linear models.
The algorithm is simple and extremely easy to im-
plement.
We have conducted experiments using CRFs
and three NLP tasks, and demonstrated empiri-
cally that our training algorithm can produce com-
pact and accurate models much more quickly than
a state-of-the-art quasi-Newton method for L1-
regularization.
Acknowledgments
We thank N. Okazaki, N. Yoshinaga, D.
Okanohara and the anonymous reviewers for their
useful comments and suggestions. The work de-
scribed in this paper has been funded by the
Biotechnology and Biological Sciences Research
Council (BBSRC; BB/E004431/1). The research
team is hosted by the JISC/BBSRC/EPSRC spon-
sored National Centre for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
484
Bob Carpenter. 2008. Lazy sparse stochastic gradient
descent for regularized multinomial logistic regres-
sion. Technical report, Alias-i.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of COLING 2004, pages 103?110.
Trevor Cohn and Philip Blunsom. 2005. Semantic role
labeling with tree conditional random fields. In Pro-
ceedings of CoNLL, pages 169?172.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. The Jour-
nal of Machine Learning Research (JMLR), 9:1775?
1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Christian Darken and John Moody. 1990. Note on
learning rate schedules for stochastic optimization.
In Proceedings of NIPS, pages 832?838.
Juhn Duchi and Yoram Singer. 2008. Online and
batch learning using forward-looking subgradients.
In NIPS Workshop: OPT 2008 Optimization for Ma-
chine Learning.
Juhn Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto
the l1-ball for learning in high dimensions. In Pro-
ceedings of ICML, pages 272?279.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Han-Shen Huang, Yu-Ming Chang, and Chun-Nan
Hsu. 2007. Training conditional random fields by
periodic step size adaptation for large-scale text min-
ing. In Proceedings of ICDM, pages 511?516.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of EMNLP
2003.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA), pages
70?75.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. The
Journal of Machine Learning Research (JMLR),
10:777?801.
Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y.
Ng. 2006. Efficient l1 regularized logistic regres-
sion. In Proceedings of AAAI-06, pages 401?408.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In Proceedings
of COLING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In Proceedings of ACL, pages 760?767.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP, pages 145?156.
James C. Spall. 2005. Introduction to Stochastic
Search and Optimization. Wiley-IEEE.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of COLING/ACL, pages 721?728.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL, pages 589?
596.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In Proceedings of ICML, pages
969?976.
Ben Wellner and Marc Vilain. 2006. Leveraging
machine readable dictionaries in discriminative se-
quence models. In Proceedings of LREC 2006.
485
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
How to Make the Most of NE Dictionaries in Statistical NER
Yutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1,2 Sophia Ananiadou1,2
1 National Centre for Text Mining
2 School of Computer Science, University of Manchester
MIB, 131 Princess Street, Manchester, M1 7DN, UK
Abstract
When term ambiguity and variability are very
high, dictionary-based Named Entity Recogni-
tion (NER) is not an ideal solution even though
large-scale terminological resources are avail-
able. Many researches on statistical NER have
tried to cope with these problems. However,
it is not straightforward how to exploit exist-
ing and additional Named Entity (NE) dictio-
naries in statistical NER. Presumably, addi-
tion of NEs to an NE dictionary leads to bet-
ter performance. However, in reality, the re-
training of NER models is required to achieve
this. We have established a novel way to im-
prove the NER performance by addition of
NEs to an NE dictionary without retraining.
We chose protein name recognition as a case
study because it most suffers the problems re-
lated to heavy term variation and ambiguity.
In our approach, first, known NEs are identi-
fied in parallel with Part-of-Speech (POS) tag-
ging based on a general word dictionary and
an NE dictionary. Then, statistical NER is
trained on the tagger outputs with correct NE
labels attached. We evaluated performance of
our NER on the standard JNLPBA-2004 data
set. The F-score on the test set has been im-
proved from 73.14 to 73.78 after adding the
protein names appearing in the training data to
the POS tagger dictionary without any model
retraining. The performance further increased
to 78.72 after enriching the tagging dictionary
with test set protein names. Our approach
has demonstrated high performance in pro-
tein name recognition, which indicates how
to make the most of known NEs in statistical
NER.
1 Introduction
The accumulation of online biomedical informa-
tion has been growing at a rapid pace, mainly at-
tributed to a rapid growth of a wide range of repos-
itories of biomedical data and literature. The auto-
matic construction and update of scientific knowl-
edge bases is a major research topic in Bioinformat-
ics. One way of populating these knowledge bases
is through named entity recognition (NER). Unfortu-
nately, biomedical NER faces many problems, e.g.,
protein names are extremely difficult to recognize
due to ambiguity, complexity and variability. A fur-
ther problem in protein name recognition arises at
the tokenization stage. Some protein names include
punctuation or special symbols, which may cause to-
kenization to lose some word concatenation infor-
mation in the original sentence. For example, IL-2
and IL - 2 fall into the same token sequence IL
- 2 as usually dash (or hyphen) is designated as a
token delimiter.
Research into NER is centred around three ap-
proaches: dictionary-based, rule-based and machine
learning-based approaches. To overcome the usual
NER pitfalls, we have opted for a hybrid approach
combining dictionary-based and machine learning
approaches, which we call dictionary-based statisti-
cal NER approach. After identifying protein names
in text, we link these to semantic identifiers, such as
UniProt accession numbers. In this paper, we focus
on the evaluation of our dictionary-based statistical
NER.
2 Methods
Our dictionary-based statistical approach consists of
two components: dictionary-based POS/PROTEIN
tagging and statistical sequential labelling. First,
63
dictionary-based POS/PROTEIN tagging finds can-
didates for protein names using a dictionary. The
dictionary maps strings to parts of speech (POS),
where the POS tagset is augmented with a tag
NN-PROTEIN. Then, sequential labelling applies
to reduce false positives and false negatives in the
POS/PROTEIN tagging results. Expandability is
supported through allowing a user of the NER tool to
improve NER coverage by adding entries to the dic-
tionary. In our approach, retraining is not required
after dictionary enrichment.
Recently, Conditional Random Fields (CRFs)
have been successfully applied to sequence labelling
problems, such as POS tagging and NER, and have
outperformed other machine learning techniques.
The main idea of CRFs is to estimate a conditional
probability distribution over label sequences, rather
than over local directed label sequences as with Hid-
den Markov Models (Baum and Petrie, 1966) and
Maximum Entropy Markov Models (McCallum et
al., 2000). Parameters of CRFs can be efficiently
estimated through the log-likelihood parameter esti-
mation using the forward-backward algorithm, a dy-
namic programming method.
2.1 Training and test data
Experiments were conducted using the training and
test sets of the JNLPBA-2004 data set(Kim et al,
2004).
Training data The training data set used in
JNLPBA-2004 is a set of tokenized sentences with
manually annotated term class labels. The sentences
are taken from the Genia corpus (version 3.02) (Kim
et al, 2003), in which 2,000 abstracts were manu-
ally annotated by a biologist, drawing on a set of
POS tags and 36 biomedical term classes. In the
JNLPBA-2004 shared task, performance in extract-
ing five term classes, i.e., protein, DNA, RNA, cell
line, and cell type classes, were evaluated.
Test Data The test data set used in JNLPBA-2004
is a set of tokenized sentences extracted from 404
separately collected MEDLINE abstracts, where the
term class labels were manually assigned, following
the annotation specification of the Genia corpus.
2.2 Overview of dictionary-based statistical
NER
Figure 1 shows the block diagram of dictionary-
based statistical NER. Raw text is analyzed by
a POS/PROTEIN tagger based on a CRF tagging
Figure 1: Block diagram of dictionary-based statistical
NER
Figure 2: Block diagram of training procedure
model and dictionary, and then converted into to-
ken sequences. Strings in the text that match with
protein names in the dictionary will be tagged as
NN-PROTEIN depending on the context around the
protein names. Since it is not realistic to enumer-
ate all protein names in the dictionary, due to their
high variability of form, instead previously unseen
forms are predicted to be protein names by statisti-
cal sequential labelling. Finally, protein names are
identified from the POS/PROTEIN tagged token se-
quences via a CRF labelling model.
Figure 2 shows the block diagram of the train-
ing procedure for both POS/PROTEIN tagging and
sequential labelling. The tagging model is created
using the Genia corpus (version 3.02) and a dic-
tionary. Using the tagging model, MEDLINE ab-
stracts used for the JNLPBA-2004 training data set
are then POS/PROTEIN-tagged. The output token
sequences over these abstracts are then integrated
with the correct protein labels of the JNLPBA-2004
training data. This process results in the preparation
of token sequences with features and correct protein
labels. A CRF labelling model is finally generated
by applying a CRF tool to these decorated token se-
quences.
64
IL/NNP
-/- 2/CD
-/-
mediated/VVD
mediated/VVN
activation/NN
IL-2/NN-PROTEIN
IL-2/NN-PROTEIN
-/-
2/CD
mediated/VVN
mediated/VVD
mediate/VVP
mediate/VV
activation/NN
IL/NNP
IL-2-mediated activation ...
POS/PROTEIN tagging
Lexicon
Figure 3: Dictionary based approach
2.2.1 Dictionary-based POS/PROTEIN tagging
The dictionary-based approach is beneficial when
a sentence contains some protein names that con-
flict with general English words. Otherwise, if the
POS tags of sentences are decided without consider-
ing possible occurrences of protein names, POS se-
quences could be disrupted. For example, in ?met
proto-oncogene precursor?, met might be falsely
recognized as a verb by a non dictionary-based tag-
ger.
Given a sentence, the dictionary-based approach
extracts protein names as follows. Find all word se-
quences that match the lexical entries, and create a
token graph (i.e., trellis) according to the word order.
Estimate the score of every path using the weights of
node and edges estimated by training using Condi-
tional Random Fields. Select the best path.
Figure 3 shows an example of our dictionary-
based approach. Suppose that the input is ?IL-
2-mediated activation?. A trellis is created based
on the lexical entries in a dictionary. The se-
lection criteria for the best path are determined
by the CRF tagging model trained on the Genia
corpus. In this example, IL-2/NN-PROTEIN
-/- mediated/VVN activation/NN is se-
lected as the best path. Following Kudo et al (Kudo
et al, 2004), we adapted the core engine of the
CRF-based morphological analyzer, MeCab1, to our
POS/PROTEIN tagging task. MeCab?s dictionary
databases employ double arrays (Aoe, 1989) which
enable efficient lexical look-ups.
The features used were:
? POS
? PROTEIN
1http://sourceforge.net/project/showfiles.php?group id=177856/
? POS-PROTEIN
? bigram of adjacent POS
? bigram of adjacent PROTEIN
? bigram of adjacent POS-PROTEIN
During the construction of the trellis, white space
is considered as the delimiter unless otherwise stated
within dictionary entries. This means that unknown
tokens are character sequences without spaces.
2.2.2 Dictionary construction
A dictionary-based approach requires the dictio-
nary to cover not only a wide variety of biomedical
terms but also entries with:
? all possible capitalization
? all possible linguistic inflections
We constructed a freely available, wide-coverage
English word dictionary that satisfies these condi-
tions. We did consider the MedPost pos-tagger
package2 which contains a free dictionary that has
downcased English words; however, this dictionary
is not well curated as a dictionary and the number of
entries is limited to only 100,000, including inflec-
tions.
Therefore, we started by constructing an English
word dictionary. Eventually, we created a dictionary
with about 266,000 entries for English words (sys-
tematically covering inflections) and about 1.3 mil-
lion entries for protein names.
We created the general English part of the dictio-
nary from WordNet by semi-automatically adding
POS tags. The POS tag set is a minor modifica-
tion of the Penn Treebank POS tag set3, in that pro-
tein names are given a new POS tag, NN-PROTEIN.
Further details on construction of the dictionary now
follow.
Protein names were extracted from the BioThe-
saurus4. After selecting only those terms
clearly stated as protein names, 1,341,992 pro-
tein names in total were added to the dictionary.
2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/
3ftp://ftp.cis.upenn.edu/pub/treebank/
doc/tagguide.ps.gz
4http://pir.georgetown.edu/iprolink/
biothesaurus/
65
Nouns were extracted from WordNet?s noun list.
Words starting with lower case and upper case
letters were determined as NN and NNP, re-
spectively. Nouns in NNS and NNPS cate-
gories were collected from the results of POS
tagging articles from Plos Biology Journal5
with TreeTagger6.
Verbs were extracted from WordNet?s verb list. We
manually curated VBD, VBN, VBG and VBZ
verbs with irregular inflections based on Word-
Net. Next, VBN, VBD, VBG and VBZ forms
of regular verbs were automatically generated
from the WordNet verb list.
Adjectives were extracted from WordNet?s adjec-
tive list. We manually curated JJ, JJR and JJS
of irregular inflections of adjectives based on
the WordNet irregular adjective list. Base form
(JJ) and regular inflections (JJR, JJS) of adjec-
tives were also created based on the list of ad-
jectives.
Adverbs were extracted from WordNet?s adverb
list. Both the original and capitalised forms
were added as RB.
Pronouns were manually curated. PRP and PRP$
words were added to the dictionary.
Wh-words were manually curated. As a result,
WDT, WP, WP$ and WRB words were added
to the dictionary.
Words for other parts of speech were manually
curated.
2.2.3 Statistical prediction of protein names
Statistical sequential labelling was employed to
improve the coverage of protein name recognition
and to remove false positives resulting from the pre-
vious stage (dictionary-based tagging).
We used the JNLPBA-2004 training data, which
is a set of tokenized word sequences with
IOB2(Tjong Kim Sang and Veenstra, 1999) protein
labels. As shown in Figure 2, POSs of tokens re-
sulting from tagging and tokens of the JNLPBA-
2004 data set are integrated to yield training data for
sequential labelling. During integration, when the
single token of a protein name found after tagging
5http://biology.plosjournals.org/
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html/
corresponds to a sequence of tokens from JNLPBA-
2004, its POS is given as NN-PROTEIN1, NN-
PROTEIN2,..., according to the corresponding token
order in the JNLPBA-2004 sequence.
Following the data format of the JNLPBA-2004
training set, our training and test data use the IOB2
labels, which are ?B-protein? for the first token of
the target sequence, ?I-protein? for each remaining
token in the target sequence, and ?O? for other to-
kens. For example, ?Activation of the IL 2 precursor
provides? is analyzed by the POS/PROTEIN tagger
as follows.
Activation NN
of IN
the DT
IL 2 precursor NN-PROTEIN
provides VVZ
The tagger output is given IOB2 labels as follows.
Activation NN O
of IN O
the DT O
IL NN-PROTEIN1 B-protein
2 NN-PROTEIN2 I-protein
precursor NN-PROTEIN3 I-protein
provides VVZ O
We used CRF models to predict the IOB2 la-
bels. The following features were used in our ex-
periments.
? word feature
? orthographic features
? the first letter and the last four letters of
the word form, in which capital letters in
a word are normalized to ?A?, lower case
letters are normalized to ?a?, and digits are
replaced by ?0?, e.g., the word form of IL-
2 is AA-0.
? postfixes, the last two and four letters
? POS feature
? PROTEIN feature
The window size was set to ?2 of the current to-
ken.
3 Results and discussion
66
Table 1: Experimental Rusults
Tagging R P F
Full 52.91 43.85 47.96
(a) POS/PROTEIN tagging Left 61.48 50.95 55.72
Right 61.38 50.87 55.63
Sequential Labelling R P F
Full 63.23 70.39 66.62
(b) Word feature Left 68.15 75.86 71.80
Right 69.88 77.79 73.63
Full 77.17 67.52 72.02
(c) (b) + orthographic feature Left 82.51 72.20 77.01
Right 84.29 73.75 78.67
Full 76.46 68.41 72.21
(d) (c) + POS feature Left 81.94 73.32 77.39
Right 83.54 74.75 78.90
Full 77.58 69.18 73.14
(e) (d) + PROTEIN feature Left 82.69 73.74 77.96
Right 84.37 75.24 79.54
Full 79.85 68.58 73.78
(f) (e) + after adding protein names in the Left 84.82 72.85 78.38
training set to the dictionary Right 86.60 74.37 80.02
3.1 Protein name recognition performance
Table 1 shows our protein name recognition results,
showing the differential effect of various combina-
tions of strategies. Results are expressed accord-
ing to recall (R), precision (P), and F-measure (F),
which here measure how accurately our various ex-
periments determined the left boundary (Left), the
right boundary (Right), and both boundaries (Full)
of protein names. The baseline for tagging (row
(a)) shows the protein name detection performance
of our dictionary-based tagging using our large pro-
tein name dictionary, where no training for protein
name prediction was involved. The F-score of this
baseline tagging method was 47.96.
The baseline for sequential labelling (row (b))
shows the prediction performance when using only
word features where no orthographic and POS fea-
tures were used. The F-score of the baseline la-
belling method was 66.62. When orthographic fea-
ture was added (row (c)), the F-score increased by
5.40 to 72.02. When the POS feature was added
(row (d)), the F-score increased by 0.19 to 72.21.
Using all features (row (e)), the F-score reached
73.14. Surprisingly, adding protein names appear-
ing in the training data to the dictionary further im-
proved the F-score by 0.64 to 73.78, which is the
second best score for protein name recognition us-
ing the JNLPBA-2004 data set.
Table 2: After Dictionary Enrichment
Method R P F
Tagging Full 79.02 61.87 69.40
(+test set Left 82.28 64.42 72.26
protein names) Right 80.96 63.38 71.10
Labelling full 86.13 72.49 78.72
(+test set Left 89.58 75.40 81.88
protein names) Right 90.23 75.95 82.47
Tagging and labelling speeds were measured us-
ing an unloaded Linux server with quad 1.8 GHz
Opteron cores and 16GB memory. The dictionary-
based POS/PROTEIN tagger is very fast even
though the total size of the dictionary is more than
one million. The processing speed for tagging and
sequential labelling of the 4,259 sentences of the test
set data took 0.3 sec and 7.3 sec, respectively, which
means that in total it took 7.6 sec. for recognizing
protein names in the plain text of 4,259 sentences.
3.2 Dictionary enrichment
The advantage of the dictionary-based statistical ap-
proach is that it is versatile, as the user can easily
improve its performance with no retraining. We as-
sume the following situation as the ideal case: sup-
pose that a user needs to analyze a large amount of
text with protein names. The user wants to know
67
the maximum performance achievable for identify-
ing protein names with our dictionary-based statis-
tical recognizer which can be achieved by adding
more protein names to the current dictionary. Note
that protein names should be identified in context.
That is, recall of the NER results with the ideal dic-
tionary is not 100%. Some protein names in the ideal
dictionary are dropped during statistical tagging or
labelling.
Table 2 shows the scores after each step of dic-
tionary enrichment. The first block (Tagging) shows
the tagging performance after adding protein names
appearing in the test set to the dictionary. The sec-
ond block (Labelling) shows the performance of the
sequence labelling of the output of the first step.
Note that tagging and the sequence labelling mod-
els are not retrained using the test set.
3.3 Discussion
It is not possible in reality to train the recognizer
on target data, i.e., the test set, but it would be pos-
sible for users to add discovered protein names to
the dictionary so that they could improve the overall
performance of the recognizer without retraining.
Rule-based and procedural approaches are taken
in (Fukuda et al, 1998; Franzen et al, 2002). Ma-
chine learning-based approaches are taken in (Col-
lier et al, 2000; Lee et al, 2003; Kazama et al,
2002; Tanabe and Wilbur, 2002; Yamamoto et al,
2003; Tsuruoka, 2006; Okanohara et al, 2006).
Machine learning algorithms used in these studies
are Naive Bayes, C4.5, Maximum Entropy Models,
Support Vector Machines, and Conditional Random
Fields. Most of these studies applied machine learn-
ing techniques to tokenized sentences.
Table 3 shows the scores reported by other sys-
tems. Tsai et al (Tsai et al, 2006) and Zhou and
Su (Zhou and Su, 2004) combined machine learning
techniques and hand-crafted rules. Tsai et al (Tsai
et al, 2006) applied CRFs to the JNLPBA-2004
data. After applying pattern-based post-processing,
they achieved the best F-score (75.12) among those
reported so far. Kim and Yoon(Kim and Yoon, 2007)
also applied heuristic post-processing. Zhou and Su
(Zhou and Su, 2004) achieved an F-score of 73.77.
Purely machine learning-based approaches have
been investigated by several researchers. The
GENIA Tagger (Tsuruoka, 2006) is trained on
the JNLPBA-2004 Corpus. Okanohara et al
(Okanohara et al, 2006) employed semi-Markov
CRFs whose performance was evaluated against the
JNLPBA-2004 data set. Yamamoto et al (Ya-
mamoto et al, 2003) used SVMs for character-
based protein name recognition and sequential la-
belling. Their protein name extraction performance
was 69%. This paper extends the machine learning
approach with a curated dictionary and CRFs and
achieved high F-score 73.78, which is the top score
among the heuristics-free NER systems. Table 4
shows typical recognition errors found in the recog-
nition results that achieved F-score 73.78. In some
cases, protein name boundaries of the JNLPBA-
2004 data set are not consistent. It is also one of
the reasons for the recognition errors that the data
set contains general protein names, such as domain,
family, and binding site names as well as anaphoric
expressions, which are usually not covered by pro-
tein name repositories. Therefore, our impression on
the performance is that an F-score of 73.78 is suffi-
ciently high.
Furthermore, thanks to the dictionary-based ap-
proach, it has been shown that the upper bound per-
formance using ideal dictionary enrichment, with-
out any retraining of the models, has an F-score of
78.72.
4 Conclusions
This paper has demonstrated how to utilize known
named entities to achieve better performance in sta-
tistical named entity recognition. We took a two-
step approach where sentences are first tokenized
and tagged based on a biomedical dictionary that
consists of general English words and about 1.3 mil-
lion protein names. Then, a statistical sequence
labelling step predicted protein names that are not
listed in the dictionary and, at the same time, re-
duced false negatives in the POS/PROTEIN tagging
results. The significant benefit of this approach is
that a user, not a system developer, can easily en-
hance the performance by augmenting the dictio-
nary. This paper demonstrated that the state-of-
the-art F-score 73.78 on the standard JNLPBA-2004
data set was achieved by our approach. Further-
more, thanks to the dictionary-based NER approach,
the upper bound performance using ideal dictionary
enrichment, without any retraining of the models,
yielded F-score 78.72.
5 Acknowledgments
This research is partly supported by EC IST project
FP6-028099 (BOOTStrep), whose Manchester team
is hosted by the JISC/BBSRC/EPSRC sponsored
National Centre for Text Mining.
68
Table 3: Conventional results for protein name recognition
Authors R P F
Tsai et al(Tsai et al, 2006) 71.31 79.36 75.12
Our system 79.85 68.58 73.78
Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77
Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34
Okanohara et al(Okanohara et al, 2006) 77.74 68.92 73.07
Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79
Finkel et al(Finkel et al, 2004) 77.40 68.48 72.67
Settles(Settles, 2004) 76.1 68.2 72.0
Song et al(Song et al, 2004) 65.50 73.04 69.07
Ro?ssler(Ro?ssler, 2004) 72.9 62.0 67.0
Park et al(Park et al, 2004) 69.71 59.37 64.12
References
J. Aoe, An Efficient Digital Search Algorithm by Using
a Double-Array Structure, IEEE Transactions on Soft-
ware Engineering, 15(9):1066?1077, 1989.
L.E. Baum and T. Petrie, Statistical inference for proba-
bilistic functions of finite state Markov chains, The An-
nals of Mathematical Statistics, 37:1554?1563, 1966.
J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding
Gene and Protein names one Word at a Time, Bioin-
formatics, Vol. 20, pp. 216-225, 2004.
N. Collier, C. Nobata, J. Tsujii, Extracting the Names
of Genes and Gene Products with a Hidden Markov
Model, Proc. of the 18th International Conference
on Computational Linguistics (COLING?2000), Saar-
brucken, 2000.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-
sim, Gail Sinclair and Christopher Manning, Exploit-
ing Context for Biomedical Entity Recognition: From
Syntax to the Web, Proc. of the Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-2004), pp. 88?91, 2004.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,
and J. Koster, Protein Names and How to Find Them,
Int. J. Med. Inf., Vol. 67, pp. 49?61, 2002.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,
Toward information extraction: identifying protein
names from biological papers, PSB, pp. 705-716,
1998.
J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support
Vector Machines for Biomedical Named Entity Recog-
nition, Proc. of ACL-2002 Workshop on Natural Lan-
guage Processing in the Biomedical Domain, pp. 1?8,
2002.
J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus
- semantically annotated corpus for bio-textmining,
Bioinformatics 2003, 19:i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, Introduction to the Bio-Entity Recogni-
tion Task at JNLPBA, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 70?75, 2004.
S. Kim, J. Yoon: Experimental Study on a Two Phase
Method for Biomedical Named Entity Recognition,
IEICE Transactions on Informaion and Systems 2007,
E90-D(7):1103?1120.
Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,
Applying Conditional Random Fields to Japanese
Morphological Analysis, Proc. of Empirical Methods
in Natural Language Processing (EMNLP), pp. 230?
237, 2004.
J. Lafferty, A. McCallum, and F. Pereira, Conditional
Random Fields: Probabilistic Models for Segment-
ing and Labeling Sequence Data, Proc. of ICML-2001,
pp.282?289, 2001
K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase
Biomedical NE Recognition based on SVMs, Proc. of
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, Sapporo, 2003.
McCallum A, Freitag D, Pereira F.: Maximum entropy
Markov models for information extraction and seg-
mentation, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, 2000:591-
598.
Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka and Jun?ichi Tsujii, Improving the Scalability of
Semi-Markov Conditional Random Fields for Named
Entity Recognition, Proc. of ACL 2006, Sydney, 2006.
Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and
Hae-Chang Rim. Boosting Lexical Knowledge for
Biomedical Named Entity Recognition, Proc. of the
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pp.
76-79, 2004.
Marc Ro?ssler, Adapting an NER-System for German to
the Biomedical Domain, Proc. of the Joint Workshop
on Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), pp. 92?95, 2004.
Burr Settles, Biomedical Named Entity Recognition Us-
ing Conditional Random Fields and Novel Feature
69
Table 4: Error Analysis
False positives
Cause Correct extraction Identified term
1 dictionary - protein, binding sites
2 prefix word trans-acting factor common trans-acting factor
3 unknown word - ATTTGCAT
4 sequential labelling error - additional proteins
5 test set error - Estradiol receptors
False negatives
Cause Correct extraction Identified term
1 anaphoric (the) receptor, (the) binding sites -
2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B
3 prefix word activation protein-1 protein-1
catfish STAT STAT
4 postfix word nuclear factor kappa B complex nuclear factor kappa B
5 plural protein tyrosine kinase(s) protein tyrosine kinase
6 family name, biding site, T3 binding sites -
and domain residues 639-656 -
7 sequential labelling error PCNA -
Chloramphenicol acetyltransferase -
8 test set error superfamily member -
Sets, Proc. of the Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
(JNLPBA-2004), pp. 104?1007, 2004.
Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-
kee Yi, POSBIOTM-NER in the shared task of
BioNLP/NLPBA 2004, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 100-103, 2004.
L. Tanabe and W. J. Wilbur, Tagging Gene and Protein
Names in Biomedical Text, Bioinformatics, 18(8), pp.
1124?1132, 2002.
E.F. Tjong Kim Sang and J. Veenstra, Representing Text
Chunks,EACL-99, pp. 173-179, 1999.
Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.
Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic
Knowledge into a Conditional Random Field Frame-
work to Identify Biomedical Named Entities, Expert
Systems with Applications, 30 (1), 2006.
Yoshimasa Tsuruoka, GENIA Tagger 3.0,
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/, 2006.
K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,
Protein Name Tagging for Biomedical Annotation in
Text, in Proc. of ACL-2003 Workshop on Natural Lan-
guage Processing in Biomedicine, Sapporo, 2003.
Guofeng Zhou and Jian Su, Exploring Deep Knowledge
Resources in Biomedical Name Recognition, Proceed-
ings of the Joint Workshop on Natural Language Pro-
cessing of Biomedicine and its Applications (JNLPBA-
2004), pp. 96-99, 2004.
70
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 22?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrated NLP Evaluation System for Pluggable Evaluation Metrics 
with Extensive Interoperable Toolkit 
 
 
Yoshinobu Kano1   Luke McCrohon1   Sophia Ananiadou2   Jun?ichi Tsujii1,2 
 
1 Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester and National Centre for 
Text Mining, 131 Princess St, M1 7DN, UK 
  
[kano,tsujii]@is.s.u-tokyo.ac.jp 
luke.mccrohon@gmail.com 
sophia.ananiadou@manchester.ac.uk 
 
  
 
Abstract 
To understand the key characteristics of NLP 
tools, evaluation and comparison against dif-
ferent tools is important. And as NLP applica-
tions tend to consist of multiple semi-
independent sub-components, it is not always 
enough to just evaluate complete systems, a 
fine grained evaluation of underlying compo-
nents is also often worthwhile. Standardization 
of NLP components and resources is not only 
significant for reusability, but also in that it al-
lows the comparison of individual components 
in terms of reliability and robustness in a wid-
er range of target domains.  But as many eval-
uation metrics exist in even a single domain, 
any system seeking to aid inter-domain eval-
uation needs not just predefined metrics, but 
must also support pluggable user-defined me-
trics. Such a system would of course need to 
be based on an open standard to allow a large 
number of components to be compared, and 
would ideally include visualization of the dif-
ferences between components. We have de-
veloped a pluggable evaluation system based 
on the UIMA framework, which provides vi-
sualization useful in error analysis. It is a sin-
gle integrated system which includes a large 
ready-to-use, fully interoperable library of 
NLP tools. 
1 Introduction 
When building NLP applications, the same sub-
tasks tend to appear frequently while construct-
ing different systems.  Due to this, the reusability 
of tools designed for such subtasks is a common 
design consideration; fine grained interoperabili-
ty between sub components, not just between 
complete systems. 
In addition to the benefits of reusability, inte-
roperability is also important in evaluation of 
components. Evaluations are normally done by 
comparing two sets of data, a gold standard data 
and test data showing the components perfor-
mance. Naturally this comparison requires the 
two data sets to be in the same data format with 
the same semantics. Comparing of "Apples to 
Apples" provides another reason why standardi-
zation of NLP tools is beneficial. Another advan-
tage of standardization is that the number of gold 
standard data sets that can be compared against is 
also increased, allowing tools to be tested in a 
wider range of domains. 
The ideal is that all components are standar-
dized to conform to an open, widely used intero-
perability framework. One possible such frame-
work is UIMA; Unstructured Information Man-
agement Architecture (Ferrucci et al, 2004), 
which is an open project of OASIS and Apache. 
We have been developing U-Compare (Kano et 
al., 2009)1, an integrated testing an evaluation 
platform based on this framework. 
                                                 
1 Features described in this paper are integrated as U-
Compare system, publicly available from:  
http://u-compare.org/ 
22
Although U-Compare already provided a wide 
range of tools and NLP resources, its inbuilt 
evaluation mechanisms were hard coded into the 
system and were not customizable by end users. 
Furthermore the evaluation metrics used were 
based only on simple strict matchings which se-
verely limited its domains of application. We 
have extended the evaluation mechanism to al-
low users to define their own metrics which can 
be integrated into the range of existing evalua-
tion tools.  
The U-Compare library of interoperable tools 
has also been extended; especially with regard to 
resources related to biomedical named entity ex-
traction. U-Compare is currently providing the 
world largest library of type system compatible 
UIMA components. 
In section 2 of this paper we first look at the 
underlying technologies, UIMA and 
U-Compare. Then we describe the new plugga-
ble evaluation mechanism in section 3 and our 
interoperable toolkit with our type system in sec-
tion 4 and 5. 
 
2 Background 
2.1 UIMA 
UIMA is an open framework specified by OA-
SIS2. Apache UIMA provides a reference im-
plementation as an open source project, with 
both a pure java API and a C++ development kit . 
UIMA itself is intended to be purely a frame-
work, i.e. it does not intend to provide specific 
tools or type system definitions. Users should 
develop such resources themselves. In the fol-
lowing subsections, we briefly describe the basic 
concepts of UIMA, and define keywords used to 
explain our system in later sections. 
2.1.1 CAS and Type System 
The UIMA framework uses the ?stand-off anno-
tation? style (Ferrucci et al, 2006). The underl-
ing raw text of a document is generally kept un-
changed during analysis, and the results of 
processing the text are added as new stand-off 
annotations with references to their positions in 
the raw text. A Common Analysis Structure 
(CAS) holds a set of such annotations. Each of 
which is of a given type as defined in a specified 
                                                 
                                                
2 http://www.oasis-open.org/committees/uima/ 
hierarchical type system. Annotation3 types may 
define features, which are themselves typed. 
Apache UIMA provides definitions of a range of 
built in primitive types, but a more complete type 
system should be specified by developers. The 
top level Apache UIMA type is referred to as 
TOP, other primitive types include. int, String, 
Annotation and FSArray (an array of any annota-
tions). 
2.1.2 Component and Capability 
UIMA components receive and update CAS one 
at a time. Each UIMA component has a capabili-
ty property, which describes what types of anno-
tations it takes as input and what types of anno-
tations it may produce as output.  
UIMA components can be deployed either lo-
cally, or remotely as SOAP web services. Re-
motely deployed web service components and 
locally deployed components can be freely com-
bined in UIMA workflows. 
2.1.3 Aggregate Component and Flow Con-
troller 
UIMA components can be either primitive or 
aggregate. Aggregate components include other 
components as subcomponents. Subcomponents 
may themselves be aggregate. In the case where 
an aggregate has multiple subcomponents these 
are by default processed in linear order. This or-
dering can be customized by implementing a 
custom flow controller. 
2.2 U-Compare 
U-Compare is a joint project of the University of 
Tokyo, the Center for Computational Pharma-
cology at the University of Colorado School of 
Medicine, and the UK National Centre for Text 
Mining. 
U-Compare provides an integrated platform 
for users to construct, edit and compare 
workflows compatible with any UIMA compo-
nent. It also provides a large, ready-to-use toolkit 
of interoperable NLP components for use with 
any UIMA based system. This toolkit is currently 
the world largest repository of type system com-
patible components. These all implement the U-
Compare type system described in section 3. 
 
3  In the UIMA framework, Annotation is a base 
type which has begin and end offset values. In this paper 
we call any objects (any subtype of TOP) as annotations. 
23
2.2.1 Related Works 
There also exist several other public UIMA 
component repositories: CMU UIMA component 
repository, BioNLP UIMA repository (Baum-
gartner et al, 2008), JCoRe (Hahn et al, 2008), 
Tsujii Lab Component Repository at the Univer-
sity of Tokyo (Kano et al, 2008a), etc. Each 
group uses their own type system, and so com-
ponents provided by each group are incompatible. 
Unlike U-Compare these repositories are basical-
ly only collections of UIMA components, U-
Compare goes further by providing a fully inte-
grated set of UIMA tools and utilities. 
2.2.2 Integrated Platform 
U-Compare provides a variety of features as part 
of an integrated platform. The system can be 
launched with a single click in a web browser; all 
required libraries are downloaded and updated 
automatically in background.  
The Workflow Manager GUI helps users to 
create workflows in an easy drag-and-drop fa-
shion. Similarly, import/export of workflows, 
running of workflows and saving results can all 
be handled via a graphical interface.  
U-Compare special parallel aggregate compo-
nents allow combinations of specified compo-
nents to be automatically combined and com-
pared based on their I/O capabilities (Kano et al, 
2008b). When workflows are run, U-Compare 
shows statistics and visualizations of results ap-
propriate to the type of workflow. For example 
when workflows including parallel aggregate 
components are run comparison statistics be-
tween all possible parallel component combina-
tions are given. 
3 Integrated System for Pluggable 
Evaluation Metrics  
While U-Compare already has a mechanism to 
automatically create possible combinations of 
components for comparison from a specified 
workflow, the comparison (evaluation) metric 
itself was hard coded into the system. Only com-
parison based on simple strict matching was 
possible. 
However, many different evaluation metrics 
exist, even for the same type of annotations. For 
example, named entity recognition results are 
often evaluated based on several different anno-
tation intersection criteria: exact match, left/right 
only match, overlap, etc. Evaluation metrics for 
nested components can be even more complex 
(e.g. biomedical relations, deep syntactic struc-
tures). Sometimes new metrics are also required 
for specific tasks. Thus, a mechanism for plugg-
able evaluation metrics in a standardized way is 
seen as desirable. 
3.1 Pluggable Evaluation Component 
Our design goal for the evaluation systems is to 
do as much of the required work as possible and 
to provide utilities to reduce developer?s labor. 
We also want our design to be generic and fix 
within existing UIMA standards. 
The essential process of evaluation can be ge-
neralized and decomposed as follows: 
 
(a) prepare a pair of annotation sets which 
will be used for comparison, 
(b) select annotations which should be in-
cluded in the final evaluation step, 
(c) compare selected annotations against 
each other and mark matched pairs. 
For example, in the case of the Penn Treebank 
style syntactic bracket matching, these steps cor-
respond to (a) prepare two sets of constituents 
and tokens, (b) select only the constituents (re-
moving null elements if required), (c) compare 
constituents between the sets and return any 
matches. 
In our new design, step (a) is performed by the 
system, (b) and (c) are performed by an evalua-
tion component. The evaluation component is 
just a normal UIMA component, pluggable based 
on the UIMA standard. This component is run on 
a CAS which was constructed by the system dur-
ing step (a). This CAS includes an instance of 
ComparisonSet type and its features GoldAnno-
tationGroup and TestAnnotationGroup. Corres-
ponding to step (b), based on this input the com-
parison component should make a selection of 
annotations and store them as FSArray for both 
GoldAnnotations and TestAnnotations. Finally 
for step (c), the component should perform a 
matching and store the results as MatchedPair 
instances in the MatchedAnnotations feature of 
the ComparisonSet. 
Precision, recall, and F1 scores are calculated 
by U-Compare based on the outputted Compari-
sonSet. These calculation can be overridden and 
customized if the developer so desires. 
Implementation of the compare() method of 
the evaluation component is recommended. It is 
used by the system when showing instance based 
evaluations of what feature values are used in 
24
matching, which features are matched, and which 
are not. 
3.2 Combinatorial Evaluation and Er-
ror Analysis 
By default, evaluation statistics are calculated by 
simply counting the numbers of gold, test, 
matched annotations in the returned Compari-
sonSet instance. Then precision, recall, and F1 
scores for each CAS and for the complete set of 
CASes are calculated. Users can specify which 
evaluation metrics are used for each type of an-
notations based on the input specifications they 
set for supplied evaluation components. 
Normally, precision, recall, and F1 scores are 
the only evaluation statistics used in the NLP 
community. It is often the case in many research 
reports that a new tool A performs better than 
another tool B, increasing the F1 score by 1%. In 
such cases it is important to analysis what pro-
portion of annotations are shared between A, B, 
and the gold standard. Is A a strict 1% increase 
over B? Or does it cover 2% of instances B 
doesn?t but miss a different 1%? Our system 
provides these statistics as well. 
Further, our standardized evaluation system 
makes more advanced evaluation available. 
Since the evaluation metrics themselves are more 
or less arbitrary, we should carefully observe the 
results of evaluations. When two or more metrics 
are available for the same type of annotations, 
we can compare the results of each to analyze 
and validate the individual evaluations. 
An immediate application of such comparison 
would be in a voting system, which takes the 
results of several tools as input and selects com-
mon overlapping annotations as output. 
U-Compare also provides visualizations of 
evaluation results allowing instance-based error 
analysis. 
4 U-Compare Type System 
U-Compare currently provides the world largest 
set of type system compatible UIMA compo-
nents. We will describe some of these in section 
5. In creating compatible components in UIMA a 
key task is their type system definitions. 
The U-Compare type system is designed in a 
hierarchical fashion with distinct types to achieve 
a high level of interoperability. It is intended to 
be a shared type system capable of mapping 
types originally defined as part of independent 
type systems (Kano et al, 2008c). In this section 
we describe the U-Compare type system in detail. 
4.1 Basic Types 
While most of the U-Compare types are inherit-
ing a UIMA built-in type, Annotation (Figure 1), 
there are also types directly extending the TOP 
type; let us call these types as metadata types.  
AnnotationMetadata holds a confidence value, 
which is common to all of the U-Compare anno-
tation types as a feature of BaseAnnotation type. 
BaseAnnotation extends DiscontinuousAnnota-
tion, in which fragmental annotations can be 
stored as a FSArray of Annotations, if any.  
ExternalReference is another common meta-
data type where namespace and ID are stored, 
referring to an external ontology entity outside 
UIMA/U-Compare. Because it is not realistic to 
represent everything like such a detailed ontolo-
gy hierarchy in a UIMA type system, this meta-
data is used to recover original information, 
which are not expressed as UIMA types. Refe-
renceAnnotation is another base annotation type, 
which holds an instance of this ExternalRefe-
rence.  
UniqueLabel is a special top level type for ex-
plicitly defined finite label sets, e.g. the Penn 
Treebank tagset. Each label in such a tagset is 
mapped to a single type where UniqueLabel as its 
BaseAnnotation 
<AnnotationMetadata> 
SyntacticAnnotation 
Token
POSToken
<POS> 
RichToken
<String>base
Sentence Dependency 
<DependencyLabel> 
Stanford 
Dependency 
TreeNode 
<TOP>parent 
<FSArray>children
AbstractConstituent
NullElement 
<NullElementLabel>
<Constituent> 
Constituent 
<ConstituentLabel>
FunctionTaggedConstituent 
<FunctionLabel> 
TemplateMappedConstituent 
<Constituent> 
TOP
Coordinations
<FSArray>
Figure 2. Syntactic Types in U-Compare. 
25
ancestor, putting middle level types if possible 
(e.g. Noun type for the Penn Treebank POS tag-
set). These types are omitted in the figure.  
4.2 Syntactic Types 
SyntacticAnnotation is the base type of all syn-
tactic types (Figure 2). POSToken holds a POS 
label, RichToken additionally holds a base form. 
Dependency is used by dependency parsers, 
while TreeNode is for syntactic tree nodes. Con-
stituent, NullElement, FunctionTaggedConsti-
tiuent, TemplateMappedConstituent are designed 
to fully represent all of the Penn Treebank style 
annotations. Coordination is a set of references to 
coordinating nodes (currently used by the Genia 
Treebank). We are planning on extending the set 
of syntactic types to cover the outputs of several 
deep parsers. 
4.3 Semantic Types  
SemanticAnnotation is the base type for semantic 
annotations; it extends ReferenceAnnotation by 
holding the original reference.  
SemanticClassAnnotation is a rather complex 
type designed to be somewhat general. In many 
cases, semantic annotations may reference other 
semantic annotations, e.g. references between 
biological events. Such references are often la-
beled with their roles which we express with the 
ExternalReference type. Such labeled references 
are expressed by LinkingAnnotationSet. As a role 
may refer to more than one annotation, Linkin-
gAnnotationSet has an FSArray of SemanticAn-
notation as a feature. 
There are several biomedical types included in 
Figure 3, e.g. DNA, RNA, Protein, Gene, Cel-
lLine, CellType, etc. It is however difficult to 
decide which ontological entities should be in-
cluded in such a type system. One reason for this 
is that such concepts are not always distinct; dif-
ferent ontologies may give overlapping defini-
tions of these concepts. Further, the number of 
possible substance level entities is infinite; caus-
ing difficult in their expression as individual 
types. The current set of biomedical types in the 
U-Compare type system includes types which are 
frequently used for evaluation in the BioNLP 
research. 
4.4 Document Types  
DocumentAnnotation is the base type for docu-
ment related annotations (Figure 4). It extends 
DocumentClassAnnotation 
<FSArray:DocumentAttribute> 
<FSArray:ReferenceAnnotation> 
DocumentAttribute 
<ExternalReference> 
DocumentAnnotation 
DocumentReferenceAttribute
<ReferenceAnnotation>
DocumentValueAttribute
<String>value 
ReferenceAnnotation TOP
Figure 4. Document types in the U-Compare type system. 
SemanticAnnotation
ReferenceAnnotation
SemanticClassAnnotation 
<FSArray:LinkedAnnotationSet>
NamedEntity EventAnnotation
CellType CellLine GeneOrGeneProductRNADNAProper 
Name
Title 
Place Protein GenePerson 
ProteinRegion
DNARegion
LinkingAnnotationSet 
<ExternalReference> 
<FSArray:SemanticAnnotation>
CoreferenceAnnotation DiscourseEntity Expression
Negation 
TOP 
Speculation
Figure 3. Semantic types in the U-Compare type system. 
26
ReferenceAnnotation to reference the full exter-
nal type in the same way as SemanticAnnotation.  
187 
The document length in bytes is 
output in the first line (end with 
new line),  
DocumentClassAnnotation together with Do-
cumentAttribute are intended to express XML 
style data. XML tags may have fields storing 
their values, and/or idref fields refering to other 
tags. DocumentValueAttiributerepresents simple 
value field, while DocumentReferenceAttribute 
represents idref type fields. A DocumentClas-
sAnnotation corresponds to the tag itself. 
then the raw text follows as is 
(attaching a new line in the end), 
finally annotations follow line by 
line. 
0 187 Document id="u1" 
0 3 POSToken id="u2" pos="DT" 
.... 
Although these types can represent most doc-
ument structures, we still plan to add several 
specific types such as Paragraph, Title, etc. 
Figure 5. An example of the U-Compare simple I/O 
format. 
 
5 Interoperable Components and Utili-
ties 
In this section, we describe our extensive toolkit 
of interoperable components and the set of utili-
ties integrated into the U-Compare system. All of 
the components in our toolkit are compatible 
with the U-Compare type system described in the 
previous section. 
5.1 Corpus Reader Components  
In the UIMA framework, a component which 
generates CASes is called a Collection Reader. 
We have developed several collection readers 
which read annotated corpora and generates an-
notations using the U-Compare type system.  
Because our primary target domain was bio-
medical field, there are corpus readers for the 
biomedical corpora; Aimed corpus (Bunescu et 
al., 2006) reader and BioNLP ?09 shared task 
format reader generate event annotations like 
protein-protein interaction annotations; Readers 
for BIO/IOB format, Bio1 corpus (Tateisi et al, 
2000), BioCreative (Hirschman et al, 2004) task 
1a format, BioIE corpus (Bies et al, 2005), 
NLPBA shared task dataset (Kim et al, 2004), 
Texas Corpus (Bunescu et al, 2005), Yapex 
Corpus (Kristofer Franzen et al, 2002), generate 
biomedical named entities, and Genia Treebank 
corpus (Tateisi et al, 2005) reader generates 
Penn Treebank (Marcus et al, 1993) style brack-
eting and part-of-speech annotations. Format 
readers require users to prepare annotated data, 
while others include corpora themselves, auto-
matically downloaded as an archive on users? 
demand. 
In addition, there is File System Collection 
Reader from Apache UIMA which reads files as 
plain text. We have developed an online interac-
tive text reader, named Input Text Reader. 
5.2 Analysis Engine Components  
There are many tools covering from basic syn-
tactic annotations to the biomedical annotations. 
Some of the tools are running as web services, 
but users can freely mix local services and web 
services. 
For syntactic annotations: sentence detectors 
from GENIA, LingPipe, NaCTeM, OpenNLP 
and Apache UIMA; tokenizers from GENIA tag-
ger (Tsuruoka et al, 2005), OpenNLP, Apache 
UIMA and Penn Bio Tokenizer; POS taggers 
from GENIA tagger, LingPipe, OpenNLP and 
Stepp Tagger; parsers from OpenNLP (CFG), 
Stanford Parser (dependency) (de Marneffe et al, 
2006), Enju (HPSG) (Miyao et al, 2008). 
For semantic annotations: ABNER (Settles, 
2005) for NLPBA/BioCreative trained models, 
GENIA Tagger, NeMine, MedT-NER, LingPipe 
and OpenNLP NER, for named entity recogni-
tions. Akane++ (S?tre et al, 2007) for protein-
protein interaction detections. 
5.3 Components for Developers  
Although Apache UIMA provides APIs in both 
Java and C++ to help users develop UIMA com-
ponents, a level of understanding of the UIMA 
framework is still required. Conversion of exist-
ing tools to the UIMA framework can also be 
difficult, particularly when they are written in 
other programming languages. 
We have designed a simple I/O format to 
make it easy for developers who just want to 
provide a UIMA wrapper for existing tools.  
Input of this format consists of two parts: raw 
text and annotations The first line of the raw text 
section is an integer of byte count of the length 
of the text. The raw text then follows with a new-
line character appended at the end. Annotations 
are then included; one annotation per line, some-
times referring another annotation by assigned 
ids (Figure 5). A line consists of begin position, 
27
end position, type name, unique id, and feature 
values if any. Double newlines indicates an end 
of a CAS. 
Output of the component is lines of annota-
tions if any created by the component. 
U-Compare provides a wrapper component 
which uses this I/O format, communicating with 
wrapped tools via standard I/O streams. 
5.4 Type System Converters 
As U-Compare is a joint project, the U-Compare 
toolkit includes UIMA components originally 
developed using several different type systems. 
In order to integrate these components into the 
U-Compare type system, we have developed 
type system converter components for each ex-
ternal type system. 
The CCP team at the University of Colorado 
made a converter between their CCP type system 
and our type system. We also developed conver-
ters for OpenNLP components and Apache UI-
MA components. These converters remove any 
original annotations not compatible with the U-
Compare type system. This prevents duplicated 
converters from translating external annotation 
multiple times in the same workflow. 
We are providing such non U-Compare com-
ponents by aggregating with type system conver-
ters, so users do not need to aware of the type 
system conversions. 
5.5 Utility Tools  
We have developed and integrated several utility 
tools, especially GUI tools for usability and error 
analysis. 
Figure 6 is showing our workflow manager 
GUI, which provides functions to create a user 
workflow by an easy drag-and-drop way. By 
clicking ?Run Workflow? button in that manager 
window, statistics will be shown (Figure 8).  
Figure 6. A s
There are also a couple of annotation visuali-
zation tools. Figure 7 is showing a viewer for 
tree structures and HPSG feature structures. Fig-
ure 9 is showing a general annotation viewer, 
when annotations have complex inter-
dependencies. 
6 Summary and Future Directions  
We have designed and developed a pluggable 
evaluation system based on the UIMA frame-
work. This evaluation system is integrated with 
the U-Compare combinatorial comparison me-
chanism which makes evaluation of many factors 
available automatically. 
creenshot of Workflow Manager 
GUI and Component Library. 
Since the system behavior is dependent on the 
type system used, we have carefully designed the 
U-Compare type system to cover a broad range 
of concepts used in NLP applications. Based di-
rectly on this type system, or using type system 
converters, we have developed a large toolkit of 
type system compatible interoperable UIMA 
component. All of these features are integrated 
into U-Compare. 
Figure 7. A screenshot of HPSG feature structure 
viewer, showing a skeleton CFG tree, feature values 
and head/semhead links. 
28
In future we are planning to increase the num-
ber of components available, e.g. more syntactic 
parsers, corpus readers, and resources for lan-
guages other than English. This will also re-
quired enhancements to the existing type system 
to support additional components. Finally we 
also hope to add integration with machine learn-
ing tools in the near future. 
 
Acknowledgments 
onal Centre for Text Mining is 
Figure 8. A screenshot of a comparison statistics showing number of instances (gold, test, and
matched), F1, precision, and recall scores of two evaluation metrics on the same data. 
 
We wish to thank Dr. Lawrence Hunter?s text 
mining group at Center for Computational Phar-
macology, University of Colorado School of 
Medicine, for helping build the type system and 
for making their tools available for this research. 
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT, 
Japan). The Nati
funded by JISC. 
W.
ning sys-
tems. J Biomed Discov Collab, 3(1), 1. 
An
ie in the Sky, ACL, Ann Arbor, 
Michigan, USA. 
Ra
tificial Intelligence in Medi-
cine, 33(2), 139-155. 
References  
 A. Baumgartner, Jr., K. B. Cohen, and L. Hunter. 
2008. An open-source framework for large-scale, 
flexible evaluation of biomedical text mi
n  Bies, Seth Kulick, and Mark Mandel. 2005. Pa-
rallel entity and treebank annotation. In Proceed-
ings of the the Workshop on Frontiers in Corpus 
Annotations II: P
zvan  Bunescu, Ruifang Ge, Rohit J. Kate, Edward 
M. Marcotte, Raymond J. Mooney, Arun Kumar 
Ramani, et al 2005. Comparative experiments on 
learning information extractors for proteins and 
their interactions. ArFigure 9. A screenshot of a visualization of com-
plex annotations. 
29
Razvan Bunescu, and Raymond Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. In Y. 
Weiss, B. Scholkopf and J. Platt (Eds.), Advances 
in Neural Information Processing Systems 18 (171-
-178). Cambridge, MA: MIT Press. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. 
In Proceedings of the the 5th International Confe-
rence on Language Resources and Evaluation 
(LREC 2006). 
David Ferrucci, and Adam Lally. 2004. Building an 
example application with the Unstructured Infor-
mation Management Architecture. Ibm Systems 
Journal, 43(3), 455-475. 
David Ferrucci, Adam Lally, Daniel Gruhl, and Ed-
ward Epstein. 2006. Towards an Interoperability 
Standard for Text and Multi-Modal Analytics. 
U. Hahn, E. Buyko, R. Landefeld, M. M?hlhausen, M.  
Poprat, K.  Tomanek, et al 2008, May. An Over-
view of JCoRe, the JULIE Lab UIMA Component 
Repository. In Proceedings of the LREC'08 Work-
shop, Towards Enhanced Interoperability for Large 
HLT Systems: UIMA for NLP, Marrakech, Moroc-
co. 
Lynette Hirschman, Alexander Yeh, Christian 
Blaschke, and Antonio Valencia. 2004. Overview 
of BioCreAtIvE: critical assessment of information 
extraction for biology. BMC Bionformatics, 
6(Suppl 1:S1). 
Yoshinobu Kano, William A Baumgartner, Luke 
McCrohon, Sophia Ananiadou, Kevin B Cohen, 
Lawrence Hunter, et al 2009. U-Compare: share 
and compare text mining tools with UIMA. Bioin-
formatics, accepted. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Keiichi-
ro Fukamachi, Kazuhiro Yoshida, Yusuke Miyao, 
et al 2008c, January. Sharable type system design 
for tool inter-operability and combinatorial com-
parison. In Proceedings of the the First Internation-
al Conference on Global Interoperability for Lan-
guage Resources (ICGL), Hong Kong. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Keiichiro Fukamachi, Yusuke Miyao, 
et al 2008b, January. Towards Data And Goal 
Oriented Analysis: Tool Inter-Operability And 
Combinatorial Comparison. In Proceedings of the 
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP), Hyderabad, India. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Yusuke Miyao, Yoshimasa Tsuruoka, 
et al 2008a, January. Filling the gaps between 
tools and users: a tool comparator, using protein-
protein interaction as an example. In Proceedings 
of the Pacific Symposium on Biocomputing (PSB), 
Hawaii, USA. 
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, 
Yuka Tateisi, and Nigel Collier. 2004. Introduction 
to the Bio-Entity Recognition Task at JNLPBA. In 
Proceedings of the International Workshop on Nat-
ural Language Processing in Biomedicine and its 
Applications (JNLPBA-04), Geneva, Switzerland. 
Kristofer Franzen, Gunnar Eriksson, Fredrik Olsson, 
Lars Asker, Per Liden, and Joakim Coster. 2002. 
Protein names and how to find them. International 
Journal of Medical Informatics, 67(1-3), 49-61. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the penn treebank. Com-
putational Linguistics, 19(2), 313-330. 
Yusuke Miyao, and Jun'ichi Tsujii. 2008. Feature 
Forest Models for Probabilistic HPSG Parsing. 
Computational Linguistics, 34(1), 35-80. 
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi, and To-
moko Ohta. 2007, April. AKANE System: Protein-
Protein Interaction Pairs in BioCreAtIvE2 Chal-
lenge, PPI-IPS subtask. In Proceedings of the 
Second BioCreative Challenge Evaluation Work-
shop. 
Burr Settles. 2005. ABNER: an open source tool for 
automatically tagging genes, proteins and other 
entity names in text. Bioinformatics, 21(14), 3191-
3192. 
Yuka Tateisi, Tomoko Ohta, Nigel Collier, Chikashi 
Nobata, and Jun'ichi Tsujii. 2000, August. Building 
an Annotated Corpus from Biology Research Pa-
pers. In Proceedings of the COLING 2000 Work-
shop on Semantic Annotation and Intelligent Con-
tent, Luxembourg. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun'ichi Tsujii. 2005, October. Syntax Annotation 
for the GENIA Corpus. In Proceedings of the the 
Second International Joint Conference on Natural 
Language Processing (IJCNLP '05), Companion 
volume, Jeju Island, Korea. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin Dong Kim, 
Tomoko Ohta, J. McNaught, Sophia Ananiadou, et 
al. 2005. Developing a robust part-of-speech tag-
ger for biomedical text. In Advances in Informatics, 
Proceedings (Vol. 3746, 382-392). Berlin: Sprin-
ger-Verlag Berlin. 
 
30
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 30?37,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Accelerating the Annotation of Sparse Named Entities
by Dynamic Sentence Selection
Yoshimasa Tsuruoka1, Jun?ichi Tsujii1,2,3 and Sophia Ananiadou1,3
1 School of Computer Science, The University of Manchester, UK
2 Department of Computer Science, The University of Tokyo, Japan
3 National Centre for Text Mining (NaCTeM), Manchester, UK
yoshimasa.tsuruoka@manchester.ac.uk
tsujii@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
Abstract
This paper presents an active learning-like
framework for reducing the human effort for
making named entity annotations in a corpus.
In this framework, the annotation work is per-
formed as an iterative and interactive process
between the human annotator and a proba-
bilistic named entity tagger. At each itera-
tion, sentences that are most likely to con-
tain named entities of the target category are
selected by the probabilistic tagger and pre-
sented to the annotator. This iterative anno-
tation process is repeated until the estimated
coverage reaches the desired level. Unlike ac-
tive learning approaches, our framework pro-
duces a named entity corpus that is free from
the sampling bias introduced by the active
strategy. We evaluated our framework by
simulating the annotation process using two
named entity corpora and show that our ap-
proach could drastically reduce the number
of sentences to be annotated when applied to
sparse named entities.
1 Introduction
Named entities play a central role in conveying im-
portant domain specific information in text, and
good named entity recognizers are often required
in building practical information extraction systems.
Previous studies have shown that automatic named
entity recognition can be performed with a reason-
able level of accuracy by using various machine
learning models such as support vector machines
(SVMs) or conditional random fields (CRFs) (Tjong
Kim Sang and De Meulder, 2003; Settles, 2004;
Okanohara et al, 2006).
However, the lack of annotated corpora, which are
indispensable for training machine learning models,
makes it difficult to broaden the scope of text mining
applications. In the biomedical domain, for exam-
ple, several annotated corpora such as GENIA (Kim
et al, 2003), PennBioIE (Kulick et al, 2004), and
GENETAG (Tanabe et al, 2005) have been created
and made publicly available, but the named entity
categories annotated in these corpora are tailored to
their specific needs and not always sufficient or suit-
able for text mining tasks that other researchers need
to address.
Active learning is a framework which can be used
for reducing the amount of human effort required to
create a training corpus (Dagan and Engelson, 1995;
Engelson and Dagan, 1996; Thompson et al, 1999;
Shen et al, 2004). In active learning, samples that
need to be annotated by the human annotator are
picked up by a machine learning model in an iter-
ative and interactive manner, considering the infor-
mativeness of the samples. Active learning has been
shown to be effective in several natural language
processing tasks including named entity recognition.
The problem with active learning is, however, that
the resulting annotated data is highly dependent on
the machine learning algorithm and the sampling
strategy employed, because active learning anno-
tates only a subset of the given corpus. This sam-
pling bias is not a serious problem if one is to use the
annotated corpus only for their own machine learn-
ing purpose and with the same machine learning al-
gorithm. However, the existence of bias is not desir-
able if one also wants the corpus to be used by other
applications or researchers. For the same reason, ac-
30
tive learning approaches cannot be used to enrich an
existing linguistic corpus with a new named entity
category.
In this paper, we present a framework that enables
one to make named entity annotations for a given
corpus with a reduced cost. Unlike active learn-
ing approaches, our framework aims to annotate all
named entities of the target category contained in
the corpus. Obviously, if we were to ensure 100%
coverage of annotation, there is no way of reducing
the annotation cost, i.e. the human annotator has to
go through every sentence in the corpus. However,
we show in this paper that it is possible to reduce
the cost by slightly relaxing the requirement for the
coverage, and the reduction can be drastic when the
target named entities are sparse.
We should note here that the purpose of this pa-
per is not to claim that our approach is superior to
existing active learning approaches. The goals are
different?while active learning aims at optimizing
the performance of the resulting machine learning-
based tagger, our framework aims to help develop
an unbiased named entity-annotated corpus.
This paper is organized as follows. Section 2 de-
scribes the overall annotation flow in our framework.
Section 3 presents how to select sentences using the
output of a probabilistic tagger. Section 4 describes
how to estimate the coverage during the course of
annotation. Experimental results using two named
entity corpora are presented in section 5. Section 6
describes related work and discussions. Concluding
remarks are given in section 7.
2 Annotating Named Entities by Dynamic
Sentence Selection
Figure 1 shows the overall flow of our annotation
framework. The framework is an iterative process
between the human annotator and a named entity
tagger based on CRFs. In each iteration, the CRF
tagger is trained using all annotated sentences avail-
able and is applied to the unannotated sentences to
select sentences that are likely to contain named
entities of the target category. The selected sen-
tences are then annotated by the human annotator
and moved to the pool of annotated sentences.
This overall flow of annotation framework is very
similar to that of active learning. In fact, the only
1. Select the first n sentences from the corpus and
annotate the named entities of the target cate-
gory.
2. Train a CRF tagger using all annotated sen-
tences.
3. Apply the CRF tagger to the unannotated sen-
tences in the corpus and select the top n sen-
tences that are most likely to contain target
named entities.
4. Annotate the selected sentences.
5. Go back to 2 (repeat until the estimated cover-
age reaches a satisfactory level).
Figure 1: Annotating named entities by dynamic sentence
selection.
differences are the criterion of sentence selection
and the fact that our framework uses the estimated
coverage as the stopping condition. In active learn-
ing, sentences are selected according to their infor-
mativeness to the machine learning algorithm. Our
approach, in contrast, selects sentences that are most
likely to contain named entities of the target cate-
gory. Section 3 elaborates on how to select sentences
using the output of the CRF-based tagger.
The other key in this annotation framework is
when to stop the annotation work. If we repeat the
process until all sentences are annotated, then obvi-
ously there is not merit of using this approach. We
show in section 4 that we can quite accurately esti-
mate how much of the entities in the corpus are al-
ready annotated and use this estimated coverage as
the stopping condition.
3 Selecting Sentences using the CRF
tagger
Our annotation framework takes advantage of the
ability of CRFs to output multiple probabilistic hy-
potheses. This section describes how we obtain
named entity candidates and their probabilities from
CRFs in order to compute the expected number of
named entities contained in a sentence 1.
1We could use other machine learning algorithms for this
purpose as long as they can produce probabilistic output. For
31
3.1 The CRF tagger
CRFs (Lafferty et al, 2001) can be used for named
entity recognition by representing the spans of
named entities using the ?BIO? tagging scheme, in
which ?B? represents the beginning of a named en-
tity, ?I? the inside, and ?O? the outside (See Table 2
for example). This representation converts the task
of named entity recognition into a sequence tagging
task.
A linear chain CRF defines a single log-linear
probabilistic distribution over the possible tag se-
quences y for a sentence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt),
where fk(t, yt, yt?1,xt) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt).
This modeling allows us to define features on states
(?BIO? tags) and edges (pairs of adjacent ?BIO?
tags) combined with observations (e.g. words and
part-of-speech (POS) tags).
The weights of the features are determined
in such a way that they maximize the condi-
tional log-likelihood of the training data2 L(?) =
?N
i=1 log p?(y(i)|x(i)). We use the L-BFGS algo-
rithm (Nocedal, 1980) to compute those parameters.
Table 1 lists the feature templates used in the CRF
tagger. We used unigrams of words/POS tags, and
prefixes and suffixes of the current word. The cur-
rent word is also normalized by lowering capital let-
ters and converting all numerals into ?#?, and used
as a feature. We created a word shape feature from
the current word by converting consecutive capital
letters into ?A?, small letters ?a?, and numerals ?#?.
example, maximum entropy Markov models are a possible al-
ternative. We chose the CRF model because it has been proved
to deliver state-of-the-art performance for named entity recog-
nition tasks by previous studies.
2In the actual implementation, we used L2 norm penalty for
regularization.
Word Unigram wi, wi?1, wi+1 & yi
POS Unigram pi, pi?1, pi+1 & yi
Prefix, Suffix prefixes of wi & yi
suffixes of wi & yi
(up to length 3)
Normalized Word N(wi) & yi
Word Shape S(wi) & yi
Tag Bi-gram true & yi?1yi
Table 1: Feature templates used in the CRF tagger.
3.2 Computing the expected number of named
entities
To select sentences that are most likely to contain
named entities of the target category, we need to
obtain the expected number of named entities con-
tained in each sentence. CRFs are well-suited for
this task as the output is fully probabilistic.
Suppose, for example, that the sentence is ?Tran-
scription factor GATA-1 and the estrogen receptor?.
Table 2 shows an example of the 5-best sequences
output by the CRF tagger. The sequences are rep-
resented by the aforementioned ?BIO? representa-
tion. For example, the first sequence indicates that
there is one named entity ?Transcription factor? in
the sequence. By summing up these probabilistic se-
quences, we can compute the probabilities for pos-
sible named entities in a sentence. From the five se-
quences in Table 2, we obtain the following three
named entities and their corresponding probabilities.
?Transcription factor? (0.677 + 0.242 = 0.916)
?estrogen receptor? (0.242 + 0.009 = 0.251)
?Transcription factor GATA-1? (0.012 + 0.009 =
0.021)
The expected number of named entities in this
sentence can then be calculated as 0.916 + 0.251 +
0.021 = 1.188.
In this example, we used 5-best sequences as an
approximation of all possible sequences output by
the tagger, which are needed to compute the exact
expected number of entities. One possible way to
achieve a good approximation is to use a large N for
N -best sequences, but there is a simpler and more
efficient way 3, which directly produces the exact
3We thank an anonymous reviewer for pointing this out.
32
Probability Transcription factor GATA-1 and the estrogen receptor
0.677 B I O O O O O
0.242 B I O O O B I
0.035 O O O O O O O
0.012 B I I O O O O
0.009 B I I O O B I
: : : : : : : :
Table 2: N-best sequences output by the CRF tagger.
expected number of entities. Recall that named enti-
ties are represented with the ?BIO? tags. Since one
entity always contains one ?B? tag, we can compute
the number of expected entities by simply summing
up the marginal probabilities for the ?B? tag on each
token in the sentence4.
Once we compute the expected number of enti-
ties for every unannotated sentence in the corpus,
we sort the sentences in descending order of the ex-
pected number of entities and choose the top n sen-
tences to be presented to the human annotator.
4 Coverage Estimation
To ensure the quality of the resulting annotated cor-
pus, it is crucial to be able to know the current cov-
erage of annotation at each iteration in the annota-
tion process. To compute the coverage, however,
one needs to know the total number of target named
entities in the corpus. The problem is that it is not
known until all sentences are annotated.
In this paper, we solve this dilemma by using
an estimated value for the total number of entities.
Then, the estimated coverage can be computed as
follows:
(estimated coverage) = mm + ?i?U Ei
(1)
where m is the number of entities actually annotated
so far and Ei is the expected number of entities in
sentence i, and U is the set of unannotated sentences
in the corpus. At any iteration, m is always known
and Ei is obtained from the output of the CRF tagger
as explained in the previous section.
4The marginal probabilities on each token can be computed
by the forward-backward algorithm, which is much more effi-
cient than computing N -best sequences for a large N .
# Entities Sentences (%)
CoNLL: LOC 7,140 5,127 (36.5%)
CoNLL: MISC 3,438 2,698 (19.2%)
CoNLL: ORG 6,321 4,587 (32.7%)
CoNLL: PER 6,600 4,373 (31.1%)
GENIA: DNA 2,017 5,251 (28.3%)
GENIA: RNA 225 810 ( 4.4%)
GENIA: cell line 835 2,880 (15.5%)
GENIA: cell type 1,104 5,212 (28.1%)
GENIA: protein 5,272 13,040 (70.3%)
Table 3: Statistics of named entities.
5 Experiments
We carried out experiments to see how our method
can improve the efficiency of annotation process
for sparse named entities. We evaluate our method
by simulating the annotation process using existing
named entity corpora. In other words, we use the
gold-standard annotations in the corpus as the anno-
tations that would be made by the human annotator
during the annotation process.
5.1 Corpus
We used two named entity corpora for the exper-
iments. One is the training data provided for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), which consists of 14,041 sen-
tences and includes four named entity categories
(LOC, MISC, ORG, and PER) for the general do-
main. The other is the training data provided for
the NLPBA shared task (Kim et al, 2004), which
consists of 18,546 sentences and five named entity
categories (DNA, RNA, cell line, cell type, and pro-
tein) for the biomedical domain. This corpus is cre-
ated from the GENIA corpus (Kim et al, 2003) by
merging the original fine-grained named entity cate-
gories.
33
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 2: Annotation of LOC in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 3: Annotation of MISC in the CoNLL corpus.
Table 3 shows statistics of the named entities in-
cluded in the corpora. The first column shows the
number of named entities for each category. The
second column shows the number of the sentences
that contain the named entities of each category. We
can see that some of the named entity categories are
very sparse. For example, named entities of ?RNA?
appear only in 4.4% of the sentences in the corpus.
In contrast, named entities of ?protein? appear in
more than 70% of the sentences in the corpus.
In the experiments reported in the following sec-
tions, we do not use the ?protein? category because
there is no merit of using our framework when most
sentences are relevant to the target category.
5.2 Results
We carried out eight sets of experiments, each of
which corresponds to one of those named entity cat-
egories shown in Table 3 (excluding the ?protein?
category). The number of sentences selected in each
iteration (the value of n in Figure 1) was set to 100
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 4: Annotation of ORG in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 5: Annotation of PER in the CoNLL corpus.
throughout all experiments.
Figures 2 to 5 show the results obtained on the
CoNLL data. The figures show how the coverage
increases as the annotation process proceeds. The
x-axis shows the number of annotated sentences.
Each figure contains three lines. The normal line
represents the coverage actually achieved, which is
computed as follows:
(coverage) = entities annotatedtotal number of entities . (2)
The dashed line represents the coverage estimated
by using equation 1. For the purpose of comparison,
the dotted line shows the coverage achieved by the
baseline annotation strategy in which sentences are
selected sequentially from the beginning to the end
in the corpus.
The figures clearly show that our method can
drastically accelerate the annotation process in com-
parison to the baseline annotation strategy. The im-
provement is most evident in Figure 3, in which
34
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 6: Annotation of DNA in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 7: Annotation of RNA in the GENIA corpus.
named entities of the category ?MISC? are anno-
tated.
We should also note that coverage estimation was
surprisingly accurate. In all experiments, the differ-
ence between the estimated coverage and the real
coverage was very small. This means that we can
safely use the estimated coverage as the stopping
condition for the annotation work.
Figures 6 to 9 show the experimental results on
the GENIA data. The figures show the same char-
acteristics observed in the CoNLL data. The accel-
eration by our framework was most evident for the
?RNA? category.
Table 4 shows how much we can save the annota-
tion cost if we stop the annotation process when the
estimated coverage reaches 99%. The first column
shows the coverage actually achieved and the second
column shows the number and ratio of the sentences
annotated in the corpus. This table shows that, on
average, we can achieve a coverage of 99.0% by an-
notating 52.4% of the sentences in the corpus. In
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 8: Annotation of cell line in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 9: Annotation of cell type in the GENIA corpus.
other words, we could roughly halve the annotation
cost by accepting the missing rate of 1.0%.
As expected, the cost reduction was most drastic
when ?RNA?, which is the most sparse named entity
category (see Table 3), was targeted. The cost reduc-
tion was more than seven-fold. These experimental
results confirm that our annotation framework is par-
ticularly useful when applied to sparse named enti-
ties.
Table 4 also shows the timing information on the
experiments 5. One of the potential problems with
this kind of active learning-like framework is the
computation time required to retrain the tagger at
each iteration. Since the human annotator has to
wait while the tagger is being retrained, the compu-
tation time required for retraining the tagger should
not be very long. In our experiments, the worst
case (i.e. DNA) required 443 seconds for retrain-
ing the tagger at the last iteration, but in most cases
5We used AMD Opteron 2.2GHz servers for the experiments
and our CRF tagger is implemented in C++.
35
Coverage Sentences Annotated (%) Cumulative Time (second) Last Interval (second)
CoNLL: LOC 99.1% 7,600 (54.1%) 3,362 92
CoNLL: MISC 96.9% 5,400 (38.5%) 1,818 61
CoNLL: ORG 99.7% 8,900 (63.4%) 5,201 104
CoNLL: PER 98.0% 6,200 (44.2%) 2,300 75
GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443
GENIA: RNA 99.2% 2,500 (13.5%) 822 56
GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284
GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295
Average 99.0% - (52.4%) - -
Table 4: Coverage achieved when the estimated coverage reached 99%.
the training time for each iteration was kept under
several minutes.
In this work, we used the BFGS algorithm for
training the CRF model, but it is probably possible to
further reduce the training time by using more recent
parameter estimation algorithms such as exponenti-
ated gradient algorithms (Globerson et al, 2007).
6 Discussion and Related Work
Our annotation framework is, by definition, not
something that can ensure a coverage of 100%. The
seriousness of a missing rate of, for example, 1% is
not entirely clear?it depends on the application and
the purpose of annotation. In general, however, it
is hard to achieve a coverage of 100% in real an-
notation work even if the human annotator scans
through all sentences, because there is often ambi-
guity in deciding whether a particular named entity
should be annotated or not. Previous studies report
that inter-annotator agreement rates with regards to
gene/protein name annotation are f-scores around
90% (Morgan et al, 2004; Vlachos and Gasperin,
2006). We believe that the missing rate of 1% can be
an acceptable level of sacrifice, given the cost reduc-
tion achieved and the unavoidable discrepancy made
by the human annotator.
At the same time, we should also note that our
framework could be used in conjunction with ex-
isting methods for semi-supervised learning to im-
prove the performance of the CRF tagger, which
in turn will improve the coverage. It is also pos-
sible to improve the performance of the tagger by
using external dictionaries or using more sophis-
ticated probabilistic models such as semi-Markov
CRFs (Sarawagi and Cohen, 2004). These enhance-
ments should further improve the coverage, keeping
the same degree of cost reduction.
The idea of improving the efficiency of annota-
tion work by using automatic taggers is certainly not
new. Tanabe et al (2005) applied a gene/protein
name tagger to the target sentences and modified
the results manually. Culotta and McCallum (2005)
proposed to have the human annotator select the
correct annotation from multiple choices produced
by a CRF tagger for each sentence. Tomanek et
al. (2007) discuss the reusability of named entity-
annotated corpora created by an active learning ap-
proach and show that it is possible to build a cor-
pus that is useful to different machine learning algo-
rithms to a certain degree.
The limitation of our framework is that it is use-
ful only when the target named entities are sparse
because the upper bound of cost saving is limited
by the proportion of the relevant sentences in the
corpus. Our framework may therefore not be suit-
able for a situation where one wants to make an-
notations for named entities of many categories si-
multaneously (e.g. creating a corpus like GENIA
from scratch). In contrast, our framework should be
useful in a situation where one needs to modify or
enrich named entity annotations in an existing cor-
pus, because the target named entities are almost al-
ways sparse in such cases. We should also note that
named entities in full papers, which recently started
to attract much attention, tend to be more sparse than
those in abstracts.
7 Conclusion
We have presented a simple but powerful framework
for reducing the human effort for making name en-
tity annotations in a corpus. The proposed frame-
work allows us to annotate almost all named entities
36
of the target category in the given corpus without
having to scan through all the sentences. The frame-
work also allows us to know when to stop the anno-
tation process by consulting the estimated coverage
of annotation.
Experimental results demonstrated that the frame-
work can reduce the number of sentences to be anno-
tated almost by half, achieving a coverage of 99.0%.
Our framework was particularly effective when the
target named entities were very sparse.
Unlike active learning, this work enables us to
create a named entity corpus that is free from the
sampling bias introduced by the active learning strat-
egy. This work will therefore be especially useful
when one needs to enrich an existing linguistic cor-
pus (e.g. WSJ, GENIA, or PennBioIE) with named
entity annotations for a new semantic category.
Acknowledgment
This work is partially supported by BBSRC grant
BB/E004431/1. The UK National Centre for Text
Mining is sponsored by the JISC/BBSRC/EPSRC.
References
Aron Culotta and Andrew McCallum. 2005. Reducing
labeling effort for structured prediction tasks. In Pro-
ceedings of AAAI-05, pages 746?751.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL, pages 319?326.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proceedings of ICML, pages 305?
312.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (Suppl. 1):180?182.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein, and
Lyle Ungar. 2004. Integrated annotation for biomed-
ical information extraction. In Proceedings of HLT-
NAACL 2004 Workshop: Biolink 2004, pages 61?68.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37:396?410.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In COLING 2004 International Joint workshop
on Natural Language Processing in Biomedicine and
its Applications (NLPBA/BioNLP) 2004, pages 107?
110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL,
pages 589?596, Barcelona, Spain.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl 1):S3.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of ICML, pages 406?414.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of EMNLP-CoNLL,
pages 486?495.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
37
Coling 2010: Poster Volume, pages 851?859,
Beijing, August 2010
Imbalanced Classification Using Dictionary-based Prototypes and
Hierarchical Decision Rules for Entity Sense Disambiguation
Tingting Mu
National Centre for Text Mining
University of Manchester
tingting.mu@man.ac.uk
Xinglong Wang
National Centre for Text Mining
University of Manchester
xinglong.wang@man.ac.uk
Jun?ichi Tsujii
Department of Computer Science
University of Tokyo
tsujii@is.s.u-tokyo.ac.jp
Sophia Ananiadou
National Centre for Text Mining
University of Manchester
Sophia.Ananiadou@man.ac.uk
Abstract
Entity sense disambiguation becomes dif-
ficult with few or even zero training in-
stances available, which is known as im-
balanced learning problem in machine
learning. To overcome the problem, we
create a new set of reliable training in-
stances from dictionary, called dictionary-
based prototypes. A hierarchical classifi-
cation system with a tree-like structure is
designed to learn from both the prototypes
and training instances, and three different
types of classifiers are employed. In addi-
tion, supervised dimensionality reduction
is conducted in a similarity-based space.
Experimental results show our system out-
performs three baseline systems by at least
8.3% as measured by macro F1 score.
1 Introduction
Ambiguities in terms and named entities are a
challenge for automatic information extraction
(IE) systems. The problem is particularly acute
for IE systems targeting the biomedical domain,
where unambigiously identifying terms is of fun-
damental importance. In biomedical text, a term
(or its abbreviation (Okazaki et al, 2010)) may
belong to a wide variety of semantic categories
(e.g., gene, disease, etc.). For example, ER may
denote protein estrogen receptor in one context,
but cell subunit endoplasmic reticulum in another,
not to mention it can also mean emergency room.
In addition, same terms (e.g., protein) may be-
long to many model organisms, due to the nomen-
clature of gene and gene products, where genes
in model organisms other than human are given,
whenever possible, the same names as their hu-
man orthologs (Wain et al, 2002). On the other
hand, public biological databases keep species-
specific records for the same protein or gene,
making species disambiguation an inevitable step
for assigning unique database identifiers to entity
names in text (Hakenberg et al, 2008; Krallinger
et al, 2008).
One way to entity disambiguation is classify-
ing an entity into pre-defined semantic categories,
based on its context (e.g., (Bunescu and Pas?ca,
2006)). Existing classifiers, such as maximum
entropy model, achieved satisfactory results on
the ?majority? classes with abundant training in-
stances, but failed on the ?minority? ones with few
or even zero training instances, i.e., the knowl-
edge acquisition bottleneck (Agirre and Martinez,
2004). Furthermore, it is often infeasible to cre-
ate enough training data for all existing semantic
classes. In addition, too many training instances
for certain majority classes lead to increased com-
putational complexity for training, and a biased
system ignoring the minority ones. These corre-
spond to two previously addressed difficulties in
imbalanced learning: ?... either (i) you have far
more data than your algorithms can deal with,
851
and you have to select a sample, or (ii) you have
no data at all and you have to go through an in-
volved process to create them? (Provost, 2000).
Given an entity disambiguation task with imbal-
anced data, this paper explores how to create more
informative training instances for minority classes
and how to improve the large-scale training for
majority classes.
Previous research has shown that words denot-
ing class information in the surrounding context of
an entity can be an informative indicator for dis-
ambiguation (Krallinger et al, 2008; Wang et al,
2010). Such words are refered to as ?cue words?
throughout this paper. For example, to disam-
biguate the type of an entity, that is, whether it
is a protein, gene, or RNA, looking at words such
as protein, gene and RNA are very helpful (Hatzi-
vassiloglou et al, 2001). Similarly, for the task
of species disambiguation (Wang et al, 2010),
the occurrence of mouse p53 strongly suggests
that p53 is a mouse protein. In many cases, cue
words are readily available in dictionaries. Thus,
for the minority classes, instead of creating arti-
ficial training instances by commonly used sam-
pling methods (Haibo and Garcia, 2009), we pro-
pose to create a new set of real training instances
by modelling cue words from a dictionary, called
dictionary-based prototypes. To learn from both
the original training instances and the dictionary-
based prototypes, a hierarchical classification sys-
tem with a tree-like structure is designed. Further-
more, to cope with the large number of features
representing each instance, supervised orthogo-
nal locality preserving projection (SOLPP) is con-
ducted for dimensionality reduction, by simulta-
neously preserving the intrinsic structures con-
structed from both the features and labels. A new
set of lower-dimensional embeddings with better
discriminating power is obtained and used as in-
put to the classifier. To cope with the large num-
ber of training instances in some majority classes,
we propose a committee machine scheme to ac-
celerate training speed without sacrificing classi-
fication accuracy. The proposed method is evalu-
ated on a species disambiguation task, and the em-
pirical results are encouraging, showing at least
8.3% improvement over three different baseline
systems.
2 Related Work
Construction of a classification model using su-
pervised learning algorithms is popular for entity
disambiguation. A number of researchers have
tackled entity disambiguation in general text us-
ing wikipedia as a resource to learn classifica-
tion models (Bunescu and Pas?ca, 2006). Hatzi-
vassiloglou et al (2001) studied disambiguating
proteins, genes, and RNA in text by training var-
ious classifiers using entities with class informa-
tion provided by adjacent cue words. Wang et
al. (2010) proposed a ?hybird? system for species
disambiguation, which heuristically combines re-
sults obtained from classifying the context, and
those from modeling relations between cue words
and entities. Although satisfactory performance
was reported, their system incurs higher computa-
tional cost due to syntactic parsing and the binary
relation classifier.
Many imbalanced learning techniques, as re-
viewed by Haibo and Garcia (2009), can also be
used to achieve the same purpose. However, to
our knowledge, there is little research in apply-
ing these machine learning (ML) techniques to en-
tity disambiguation. It is worth mentioning that
although these ML techniques can improve the
learning performance to some extent, they only
consider the information contained in the origi-
nal training instances. The created instances do
not add new information, but instead utilize the
original training information in a more sophisti-
cated way. This motivates us to pursue a differ-
ent method of creating new training instances by
using information from a related and easily ob-
tained source (e.g., a dictionary), similar to trans-
fer learning (Pan and Yang, 2009).
3 Task and Corpus
In this work, we develop an entity disambiguation
technique with the use of cue words, as well as a
general ML algorithm for imbalanced classifica-
tion using a set of newly created dictionary-based
prototypes. These prototypes are represented with
different features from those used by the original
training instances. The proposed method is eval-
uated on a species disambiguation task: given a
text, in which mentions of biomedical named en-
852
tities are annotated, we assign a species identi-
fier to every entity mention. The types of entities
studied in this work are genes and gene products
(e.g., proteins), and we use the NCBI Taxonomy1
(taxon) IDs as species tags and to build the proto-
types. Note that this paper focuses on the task of
species disambiguation and makes the assumption
that the named entities are already recognised.
Consider the following sentence as an exam-
ple: if one searches the proteins (i.e., the under-
lined term) in a protein database, he/she will find
they belong to many model organisms. However,
in this particular context, CD200R-CD4d3+4 is
human and mouse protein, while rCD4d3+4 is
a rat one.2 We call such a task of assigning
species identifiers to entities, according to context,
as species disambiguation.
The amounts of human and mouse
CD200R-CD4d3+4 and rCD4d3+4
protein on the microarray spots were
similar as visualized by the red fluo-
rescence of OX68 mAb recognising
the CD4 tag present in each of the
recombinant proteins.
The informative cue words (e.g., mouse) used
to help species disambiguation are called species
words. In this work, species words are defined as
any word that indicates a model organism and also
appears in the organism dictionaries we use. They
may have various parts-of-speech, and may also
contain multiple tokens (despite the name species
word). For example, ?human?, ?mice?, ?bovine?
and ?E. Coli? are all species words. We detect
these words by automatic dictionary lookup: a
word is annotated as a species word if it matches
an entry in a list of organism names. Each entry in
the list contains a species word and its correspond-
ing taxon ID, and the list is merged from two dic-
tionaries: the NCBI Taxonomy and the UniProt
controlled vocabulary of species.3 The NCBI por-
tion is a flattened NCBI Taxonomy (i.e., without
hierarchy) including only the identifiers of genus
and species ranks. In total, the merged list con-
1http://www.ncbi.nlm.nih.gov/sites/entrez?db= taxon-
omy
2Prefix ?r? in ?rCD4d3+4? indicates that it is a rat protein.
3http://www.expasy.ch/cgi-bin/speclist
tains 356,387 unique species words and 272,991
unique species IDs. The ambiguity in species
words is low: 3.86% of species words map to mul-
tiple IDs, and on average each word maps to 1.043
IDs.
The proposed method was evaluated on the
corpus developed in (Wang et al, 2010), con-
taining 6, 223 genes and gene products, each of
which was manually assigned with either a taxon
ID or an ?Other? tag, with human being the
most frequent at 50.30%. With the extracted
features and the species ID tagged by domain
experts, each occurrence of named entities can
be represented as a d-dimensional vector with
a label. Species disambiguation can be mod-
elled as a multi-classification task: Given n train-
ing instances {xi}ni=1, their n ? d feature ma-
trix X = [xij ] and n-dimensional label vector
y = [y1, y2, . . . , yn]T are used to train a clas-
sifier C(?), where xi = [xi1, xi2, . . . , xid]T , yi ?
{1, 2, . . . , c}, and c denotes the number of ex-
isting species in total. Given m different query
instances {x?i}mi=1, their m ? d feature matrix
X? = [x?ij ] are used as the input to the trained
classifier, so that their labels can be predicted by
{C(x?i)}mi=1.
We used relatively simple contextual features
because this work was focused on developing a
ML framework. In more detail, we used the fol-
lowing features: 1) 200 words surrounding the en-
tity in question; 2) two nouns and two adjectives
at the entity?s left and right; 3) 5 species words
at the entity?s left and right. In addition, function
words and words that consist of only digits and
punctuations are filtered out. The final numeri-
cal dataset consists of 6,227 instances, each rep-
resented by 16,851 binary features and belonging
to one of the 13 classes. The dataset is highly im-
balanced: among the 13 classes, the numbers of
instances in the four majority classes vary from
449 to 3,220, while no more than 20 instances are
contained in the eight minority classes (see Table
1).
853
4 Proposed Method
4.1 Dictionary-based Prototypes
For each existing species, we create a b-
dimensional binary vector, given as pi =
[pi1, pi2, . . . , pib]T , using b different species
words listed in the dictionary as features, which
is called dictionary-based prototype. The binary
value pij denotes whether the jth species word
belongs to the ith species in the dictionary. This
leads to a c ? b feature matrix P = [pij ] for c
species.
Considering that the species words preceding
and appearing in the same sentence as an en-
tity can be informative indicators for the possible
species of this entity, we create two morem?b bi-
nary feature matrices for the query instances with
the same b species words as features: X?1 = [x?(1)ij ]
and X?2 = [x?(2)ij ], where x?(1)ij denotes whether the
jth species word is the preceding word of the ith
entity, and x?(2)ij denotes whether the jth species
word appears in the same sentence as the ith en-
tity but is not preceding word. Thus, the similar-
ity between each query entity and existing species
can be simply evaluated by calculating the inner-
product between the entity instance and the cor-
responding prototype. This leads to the following
m? c similarity matrix S? = [s?ij ]:
S? = ?X?1PT + (1? ?)X?2PT , (1)
where 0 ? ? ? 1 is a user-defined parameter con-
trolling the degree of indicating reliability of the
preceding word and the same-sentence word. The
n?c similarity matrix S = [sij ] between the train-
ing instances and the species can be constructed in
exactly the same way. Based on empirical expe-
rience, the preceding word indicates the entity?s
species more accurately than the same-sentence
word. Thus, ? is preferred to be set as greater
than 0.5. The obtained similarity matrix will be
used in the nearest neighbour classifier (see Sec-
tion 4.2.1).
Both the original training instances X and the
newly created prototypes P are used to train the
proposed hierarchical classification system. Sub-
ject to the nature of the classifier employed, it is
convenient to construct one single feature matrix
Nearest Neighbor Classifier(IT1)
Minority Classes
Majority Classes
SOLPP-FLDA Classifier(IT2)
Small-scale Majority Classes
Large-scale Majority Classes
Committee Classifier(END)
Yes
No Yes
No
Output: Instances with predicted labels belonging to MI Output: Instances with predicted labels belonging to SMA
Output: Instances with predicted labels belonging to LMA
Note: Definition of the minority,majority, small-scale majority, large-scale majority classes, as well as theIF-THEN rule 1 (IT1) and IF-THEN rule2 (IT2) are provided in the paper.
Figure 1: Structure of the proposed hierarchical
classification system
instead of using X and P individually. Aiming at
keeping the same similarity values between each
entity instance and the species prototype, we con-
struct the following (n+c)?(d+b) feature matrix
for both the training instances and prototypes:
F =
[ X ?X1 + (1? ?)X2
0 P
]
, (2)
where X1 and X2 are constructed in the same way
as X?1 and X?2 but for training instances. Their cor-
responding label vector is l = [yT , 1, 2, . . . , c]T .
4.2 Hierarchical Classification
Multi-stage or hierarchical classification (Giusti
et al, 2002; Podolak, 2007; Kurzyn?ski, 1988)
is widely used in many complex multi-category
classification tasks. Existing research shows such
techniques can potentially achieve right trade-off
between accuracy and resource allocation (Giusti
et al, 2002; Podolak, 2007). Our proposed hier-
archical system has a tree-like structure with three
different types of classifier at nodes (see Figure 1).
Different classes are organized in a hierarchical
order to be classified based on the corresponding
numbers of available training instances. Letting
ni denote the number of training instances avail-
able in the ithe class excluding the created proto-
types, we categorize the classes as follows:
? Minority Classes (MI): Classes with less
training instances than the threshold: MI =
{i : nin < ?1, i ? {1, 2 . . . , c}}.
854
? Majority Classes (MA): Classes with more
training instances than the threshold: MA =
{i : nin ? ?1, i ? {1, 2 . . . , c}}.
? Small-scale Majority Classes (SMA): Ma-
jority Classes with less training instances
than the threshold: SMA = {i : nin <
?2, i ? MA}.
? Large-scale Majority Classes (LMA): Ma-
jority Classes with more training instances
than the threshold: LMA = {i : nin ?
?2, i ? MA}.
Here, 0 < ?1 < 1 and 0 < ?2 < 1 are size
thresholds set by users. We have MI ?MA = ?,
SMA ? LMA = ?, and SMA ? LMA = MA.
The tree-like hierarchical structure of our sys-
tem is determined by MI, MA, SMA, and LMA.
We propose two IF-THEN rules to control the sys-
tem: Given a query instance x?i, the level 1 clas-
sifier C1 is used to predict whether x?i belongs to
MA or a specific class in MI, which wer call IF-
THEN rule 1 (IT1). If x?i belongs to MA, the level
2 classifier C2 is used to predict whether x?i be-
longs to LMA or a specific class in SMA, called
IF-THEN rule 2 (IT2). If x?i belongs to LMA, the
level 3 classifier C3 finally predicts the specific
class in LMA x?i belongs to. We explain in the
following sections how the classifiers C1, C2, and
C3 work in detail.
4.2.1 Nearest Neighbour Classifier
The goal of the nearest neighbour classifier, de-
noted by C1, is to decide whether the nearest-
neighbour prototype of the query instance be-
longs to MI. The only used training instances are
our created dictionary-based prototypes {pi}ci=1
with the label vector [1, 2, . . . , c]T . The nearest-
neighbour prototype of the query instance x?i pos-
sesses the maximum similarity to x?i:
NN(x?i) = arg maxj=1, 2, ..., c s?ij , (3)
where s?ij is obtained by Eq. (1). Consequently,
the output of the classifier C1 is given as
C1(x?i) =
{
NN(x?i), If NN(x?i) ? MI,
0, Otherwise.
(4)
The IF-THEN rule 1 can then be expressed as
Action(IT1) =
{ Go to C2, If C1(x?i) = 0,
Stop, Otherwise.
4.2.2 SOLPP-FLDA Classifier
The goal of the SOLPP-FLDA classifier, de-
noted by C2, is to predict whether the query in-
stance belongs to LMA or a specific class in SMA.
In this classifier, the used training instances are
the original training entities and the dictionary-
based prototypes, both belonging to MA. The fea-
ture matrix F and the label vector l defined in Sec-
tion 4.1 are used, but with instances from MI re-
moved (we use n? to denote the number of remain-
ing training instances, and the same symbol F for
feature matrix). The used label vector l? to train C2
should be re-defined as l?i = li if li ? SMA, and 0
otherwise.
First, we propose to implement orthog-
onal locality preserving projection (OLPP)
(Kokiopoulou and Saad, 2007) in a supervised
manner, leading to SOLPP, to obtain a smaller set
of more powerful features for classification. Also,
we conduct SOLPP in a similarity-based feature
space computed from (d + 2b) original features
by employing dot-product based similarity, given
by FFT . As explained later, to compute the
new features from FFT instead of the original
features F achieves reduced computational cost.
An n??k projection matrix V = [vij ] is optimized
in this n-dimensional similarity-based feature
space. The optimal projections are obtained by
minimizing the weighted distances between the
lower-dimensional embeddings so that ?similar?
instances are mapped together in the projected
feature space. Mathematically, this leads to the
following constrained optimization problem:
min
V?Rn??k,
VT V=Ik?k
tr[VTFTF(D?W)FFTV], (5)
where W = [wij ] denotes the n ? n weight ma-
trix with wij defining the degree of ?closeness? or
?similarity? between the ith and jth instances, D
is a diagonal matrix with {di =?n?j=1wij}n?i=1 as
the diagonal elements.
Usually, the weight matrix W is defined by
an adjacency graph constructed from the original
855
data, e.g. for OLPP. One common way to define
the adjacency is by including the K-nearest neigh-
bors (KNN) of a given node to its adjacency list,
which is also called the KNN-graph (Kokiopoulou
and Saad, 2007). There are two common ways to
define the weight matrix: constant value, where
wij = 1 if the ith and jth samples are adjacent,
while wij = 0 otherwise, and Gaussian kernel.
We will denote in the rest of the paper such a
weight matrix computed only from the features
as WX . Ideally, if the features can accurately
describe all the discriminating characteristics, the
samples that are close or similar enough to each
other should have the same label vectors. How-
ever, when processing real dataset, what may hap-
pen is that, in the d-dimensional feature space,
the data points that are close to each other may
belong to different categories, while on the con-
trary, the data points that are in a distant to each
other may belong to the same category. In the k-
dimensional projected feature space obtained by
OLPP, one may have the same problem. Because
OLPP solves the constrained optimization prob-
lem in Eq. (5) using WX : if two instances are
close or similar to each other in the original fea-
ture space, they will be the same close or simi-
lar to each other in the projected space. To solve
this problem, we decide to modify the ?closeness?
or ?similarity? between instances in the projected
feature space by considering the label informa-
tion. The following computation of a supervised
weight matrix is used for our SOLPP:
W = (1? ?)WX + ?LLT , (6)
where 0 ? ? ? 1 is a user-defined parameter
controlling the tradeoff between the label-based
and feature-based neighborhood structures, and
L = [lij ] is an n? ? c binary label matrix with
lij = 1 if the ith instance belongs to the jth class,
and lij = 0 otherwise.
The optimal solution of Eq. (5) is the top
(k + 1)th eigenvectors of the n? ? n? symmetric
matrix FTF(D ? W)FFT , corresponding to the
k + 1 smallest eigenvalues, but with the top one
eigenvector removed, denoted by V?. It is worth
to mention that if the original feature matrix F is
used as the input of SOLPP, one needs to com-
pute the eigen-decomposition of the (d + b) ?
(d+ b) symmetric matrix FT (D?W)F. The cor-
responding computation complexity increases in
O((d + b)3), which is unacceptable in practical
when d + b  n?. The projected features for the
training instances are computed by
Z = FFTV?. (7)
Given a different set of m query instances with an
m? (d+ b) feature matrix,
F? = [X?, ?X?1 + (1? ?)X?2], (8)
their embeddings can be easily obtained by
Z? = F?F?TV?. (9)
Then, the projected feature matrix Z and label
vector l? are used to train a multi-class classifier.
By employing the one-against-all scheme, differ-
ent binary classifiers {C(2)i }i?SMA?{0} with label
space {+1, ?1} are trained. For the ith class
(i ? SMA?{0}), the training instances belonging
to it are labeled as positive, otherwise negative. In
each binary classifier C(2)i , a separating function
f (2)i (x) = xTw
(2)
i + b
(2)
i (10)
is constructed, of which the optimal values of the
weight vector w(2)i and bias b(2)i are computed us-
ing Fisher?s linear discriminant analysis (FLDA)
(Fisher, 1936; Mu, 2008). Finally, the output of
the classifier C2 can be obtained by assigning the
most confident class label to the query instance x?i,
with the confidence value indicated by the value of
separating function:
C2(x?i) = arg max
j?SMA?{0}
f (2)j (x?i). (11)
The IF-THEN rule 2 can then be expressed as
Action(IT2) =
{ Go to C3, If C2(x?i) = 0,
Stop, Otherwise.
4.2.3 Committee Classifier
The goal of the committee classifier, denoted
by C3, is to predict the specific class in LMA
the query instance belongs to. The used training
856
instances are entities and dictionary-based proto-
types only belonging to LMA. With the same one-
against-all scheme, there are large number of pos-
itive and negative training instances to train a bi-
nary classifier for a class in LMA. To accelerate
the training procedure without sacrificing the ac-
curacy, the following scheme is designed.
Letting ne denote the number of experts in
committee, all the training instances are averagely
divided into ne+1 groups each containing similar
numbers of training instances from the same class.
The instances in the ith and the (i+1)th groups are
used to train the ith expert classifier. This achieves
overlapped training instances between expert clas-
sifiers. The output value of C(3)i is not the class in-
dex as used in C2, but the value of the separating
function of the most confident class, denoted by
f (3)i . Different from the commonly used majority
voting rule, we only trust the most confident ex-
pert. Thus, the output of C3 for a query instance
x?i can be obtained by
C3(x?i) = arg maxj=1, 2, ..., ne f
(3)
j (x?i). (12)
By using C3, different expert classifiers can be
trained in parallel. The total training time is equal
to that of the slowest expert classifier. The more
expert classifiers are used, the faster the system is,
however, the less accurate the system may become
due to the decrease of used training instances for
each expert, especially the positive instances in
the case of imbalanced classification. This is also
the reason we do not apply the committee scheme
to SMA classes.
5 Experiments
5.1 System Evaluation and Baseline
We evaluate the proposed method using 5-fold
cross validation, with around 4,980 instances for
training, and 1,245 instances for test in each trial.
We compute the F1 score for each species, and
employ macro- and micro- average scheme to
compute performance for all species. Three base-
lines for comparison include:
? Baseline 1 (B1) : A maximum entropy
model trained with training data only.
? Baseline 1 (B2) : Combination of B1 and
the species dictionary using rules employed
in Wang et al (2010).
? Baseline 2 (B3): The ?hybrid? system com-
bining B1, the dictionary and a relation
model 4 using rules (Wang et al, 2010).
Our hierarchical classification system were imple-
mented in two ways:
? HC: Only the training data on its own is used
to train the system.
? HC/D: Both the training data and the
dictionary-based prototypes are used to train
the system.
5.2 Results and Analysis
The proposed system was implemented with ? =
0.8, ? = 0.8, ne = 4, and k = 1000. The species
9606, 10090, 7227, and 4932 were categorized as
LMA, the species 10116 as SMA, and the rest sep-
cies as MI. To compute the supervised weight ma-
trix, the percentage of the used KNN in the KNN-
graph was 0.6. Parameters were not fine tuned, but
set based on our empirical experience on previous
classification research. As shown in Table 1: HC
and B1 were trained with the same instances and
features, and HC outperformed B1 in both macro
and micro F1. Both HC and B1 obtained zero F1
scores for most minority species, showing that it is
nearly impossible to correctly label the query in-
stances of minority classes, due to lack of training
data. By learning from a related resource, HC/D,
B2, and B3 yielded better macro performance. In
particular, while HC/D and B2 learned from the
same dictionary and training data, HC/D outper-
formed B2 by 19.1% in macro and 2.5% in mi-
cro F1. B3 aimed at improving the macro perfor-
mance by employing computationally expensive
syntactic parsers and also by training an extra re-
lation classifier. With the same goal, HC/D inte-
grated the cue word information into the ML clas-
sifier in a more general way, and yielded an 8.3%
improvement over B3, as measured by macro-F1.
4This is an SVM model predicting relations between en-
tities and nearby species words with positive output indicates
species words bear the semantic label of entities.
857
Species Name Cat. No. HC HC/D B1 B2 B3
Homo sapiens (9606) LMA 3220 87.39 87.48 86.06 85.43 86.48
Mus musculus (10090) LMA 1709 79.99 79.98 79.59 80.00 80.41
Drosophila melanogaster (7227) LMA 641 86.62 86.35 87.96 87.02 87.37
Saccharomyces cerevisiae (4932) LMA 499 90.24 90.24 83.35 81.64 84.64
Rattus norvegicus (10116) SMA 50 55.07 69.23 48.42 64.41 59.41
Escherichia coli K-12 (83333) MI 18 0.00 0.00 0.00 0.00 0.00
Xenopus tropicalis (8364) MI 8 0.00 40.00 0.00 41.67 36.36
Caenorhabditis elegans (6239) MI 7 0.00 22.22 0.00 28.57 22.22
Oryctolagus cuniculus (9986) MI 3 0.00 0.00 0.00 20.00 0.00
Bos taurus (9913) MI 3 0.00 50.00 0.00 0.00 100.00
Arabidopsis thaliana (3702) MI 2 0.00 0.00 0.00 0.00 66.67
Arthropoda (6656) MI 1 0.00 100.00 0.00 50.00 0.00
Martes zibellina (36722) MI 1 0.00 50.00 0.00 28.57 0.00
Micro-average N/A N/A 85.03 85.13 83.59 83.04 83.80
Macro-average N/A N/A 30.72 51.96 29.42 43.64 47.97
Table 1: Performance is compared in F1 (%), where ?No.? denotes the number of training instances
and ?Cat.? denotes the category of species class as defined in Section 4.2.
6 Conclusions and Future Work
Disambiguating bio-entities presents a challenge
for traditional supervised learning methods, due
to the high number of semantic classes and lack of
training instances for some classes. We have pro-
posed a hierarchical framework for imbalanced
learning, and evaluated it on the species disam-
biguation task. Our method automatically builds
training instances for the minority or missing
classes from a cue word dictionary, under the as-
sumption that cue words in the surrounding con-
text of an entity strongly indicate its semantic cat-
egory. Compared with previous work (Wang
et al, 2010; Hatzivassiloglou et al, 2001), our
method provides a more general way to integrate
the cue word information into a ML framework
without using deep linguistic information.
Although the species disambiguation task is
specific to bio-text, the difficulties caused by im-
balanced frequency of different senses are com-
mon in real application of sense disambiguation.
The proposed technique can also be applied to
other domains, providing the availability of a cue
word dictionary that encodes semantic informa-
tion regarding the target semantic classes. Build-
ing such a dictionary from scratch can be chal-
lenging, but may be easier compared to manual
annotation. In addition, such dictionaries may al-
ready exist in specialised domains.
Acknowledgment
The authors would like to thank the biologists who
annotated the species corpus, and National Cen-
tre for Text Mining. Funding: Pfizer Ltd.; Joint
Information Systems Committee (to UK National
Centre for Text Mining)
References
Agirre, E. and D. Martinez. 2004. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP.
Bunescu, R. and M. Pas?ca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL.
Fisher, R. A. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7(2):179?188.
Giusti, N., F. Masulli, and A. Sperduti. 2002. Theoret-
ical and experimental analysis of a two-stage system
for classification. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 24(7):893?904.
Haibo, H. and E. A. Garcia. 2009. Learning from
imbalanced data. IEEE Trans. on Knowledge and
Data Engineering, 21(9):1263?1284.
858
Hakenberg, J., C. Plake, R. Leaman, M. Schroeder, and
G. Gonzalez. 2008. Inter-species normalization of
gene mentions with GNAT. Bioinformatics, 24(16).
Hatzivassiloglou, V., PA Duboue?, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics,
17(Suppl 1).
Kokiopoulou, E. and Y. Saad. 2007. Orthogonal
neighborhood preserving projections: A projection-
based dimensionality reduction technique. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 29(12):2143?2156.
Krallinger, M., A. Morgan, L. Smith, F. Leitner,
L. Tanabe, J. Wilbur, L. Hirschman, and A. Valen-
cia. 2008. Evaluation of text-mining systems for
biology: overview of the second biocreative com-
munity challenge. Genome Biology, 9(Suppl 2).
Kurzyn?ski, M. W. 1988. On the multistage bayes clas-
sifier. Pattern Recognition, 21(4):355?365.
Mu, T. 2008. Design of machine learning algorithms
with applications to breast cancer detection. Ph.D.
thesis, University of Liverpool.
Okazaki, N., S. Ananiadou, and J. Tsujii. 2010.
Building a high quality sense inventory for im-
proved abbreviation disambiguation. Bioinformat-
ics, doi:10.1093/bioinformatics/btq129.
Pan, S. J. and Q. Yang. 2009. A survey on transfer
learning. IEEE Trans. on Knowledge and Data En-
gineering.
Podolak, I. T. 2007. Hierarchical rules for a hierarchi-
cal classifier. Lecture Notes in Computer Science,
4431:749?757.
Provost, F. 2000. Machine learning from imbalanced
data sets 101. In Proc. of Learning from Imbalanced
Data Sets: Papers from the Am. Assoc. for Artificial
Intelligence Workshop. (Technical Report WS-00-
05).
Wain, H., E. Bruford, R. Lovering, M. Lush,
M. Wright, and S. Povey. 2002. Guidelines for
human gene nomenclature. Genomics, 79(4):464?
470.
Wang, X., J. Tsujii, and S. Ananiadou. 2010. Dis-
ambiguating the species of biomedical named enti-
ties using natural language parsers. Bioinformatics,
26(5):661667.
859
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2270?2279, Dublin, Ireland, August 23-29 2014.
Comparable Study of Event Extraction in Newswire and Biomedical
Domains
Makoto Miwa
?,?
Paul Thompson
?
Ioannis Korkontzelos
?
Sophia Ananiadou
?
?
National Centre for Text Mining and School of Computer Science,
University of Manchester, United Kingdom
?
Graduate School of Engineering, Toyota Technological Institute, Japan
{makoto.miwa, paul.thompson, ioannis.korkontzelos, sophia.ananiadou}@manchester.ac.uk
Abstract
Event extraction is a popular research topic in natural language processing. Several event extrac-
tion tasks have been defined for both the newswire and biomedical domains. In general, different
systems have been developed for the two domains, despite the fact that the tasks in both domains
share a number of characteristics. In this paper, we analyse the commonalities and differences
between the tasks in the two domains. Based on this analysis, we demonstrate how an event
extraction method originally designed for the biomedical domain can be adapted for application
to the newswire domain. The performance is state-of-the-art for both domains, with F-scores of
52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primary
evaluation metrics.
1 Introduction
Research into event extraction was initially focussed on the general language domain, largely driven by
the Message Understanding Conferences (MUC) series (e.g., Chinchor (1998)) and the Automated Con-
tent Extraction (ACE) evaluations
1
. More recently, the focus of research has been widened to the biomed-
ical domain, motivated by the ongoing series of biomedical natural language processing (BioNLP) shared
tasks (STs) (e.g., Kim et al. (2013)).
Although the textual characteristics and the types of relevant events to be extracted can vary consid-
erably between domains, the same general features of events normally hold across domains. An event
usually consists of a trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nom-
inalised verb that denotes the presence of the event in the text, while the arguments are usually entities.
In general, arguments are assigned semantic roles that characterise their contribution towards the event
description.
Until now, however, there has been little, if any, effort by researchers working on event extraction in
different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008))
and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006))
and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit cross-
domain features of events to develop more adaptable event extraction systems is an under-studied area.
Consequently, although there is a large number of published studies on event extraction, proposing many
different methods, no work has previously been reported that aims to adapt an event extraction method
developed for one domain to a new domain.
In response to the above, we have investigated the feasibility of adapting an event extraction method
developed for the biomedical domain to the newswire domain. To facilitate this, we firstly carry out a
detailed static analysis of the differences that hold between event extraction tasks in the newswire and
biomedical domains. Specifically, we consider the ACE 2005 event extraction task (Walker et al., 2006)
for the newswire domain and the Genia Event Extraction task (GENIA) in BioNLP ST 2013 (Kim et al.,
2013) for the biomedical domain. Based on the results of this analysis, we adapt the biomedical event
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
itl.nist.gov/iad/mig/tests/ace
2270
Jim McMahon was body slammed to the ground in the mid 80's about five seconds after he had released a pass.
PER_Individual Conflict_Attack ?? timex2 PER_Individual
timex2
Target Time-Within
Time-At-End
Figure 1: ACE 2005 event example (ID: MARKBACKER 20041220.0919)
p300 immunoprecipitated Foxp3 when both proteins were overexpressed in HEK 293T cells
Pro Binding Pro +Reg
+Reg
Gene expression
Gene expressionTheme Theme2 Cause
CauseTheme
Theme
Theme
Theme
Figure 2: GENIA event example (ID: PMC-1447668-08-Results)
extraction method to the task of extracting events in the newswire domain, according to the specification
of the ACE 2005 event extraction task. The original method consists of a classification pipeline that has
previously been applied to extract events according to task descriptions that are similar to GENIA. In
order to address the differences between this task and the ACE task, we have made a number of changes
to the original method, including modifications to the classification labels assigned, the pipeline itself
and the features used. We retrained the model of the adapted system on the ACE task, compared the
performance, and empirically analysed the differences between the two tasks in terms of entity-related
information. We demonstrate that the resulting system achieves state-of-the-art performance for tasks in
both domains.
2 Related Work
In this section, we introduce the two domain specific event extraction tasks on which we will focus, i.e.,
the ACE 2005 event extraction task, which concerns events in the newswire domain, and the GENIA
event task from the BioNLP ST 2013, which deals with biomedical event extraction. We also examine
state-of-the-art systems that have been developed to address each task.
2.1 Newswire Event Extraction
The extraction of events from news-related texts has been widely researched, largely due to motivation
from the various MUC and ACE shared tasks. Whilst MUC focussed on filling a single event template
on a single topic by gathering information from different parts of a document, ACE defined a more
comprehensive task, involving the recognition of multiple fine-grained and diverse types of entities and
associated intra-sentential events within each document.
A common approach to tackling the MUC template filling task has involved the employment of
pattern-based methods, e.g., Riloff (1996). In contrast, supervised learning approaches have constituted
a more popular means of approaching the ACE tasks
2
. In this paper, we choose to focus on adapting
our biomedical-focussed event extraction method to the ACE 2005 task. Our choice is based on the task
definition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than the
MUC event template task definition.
In terms of the characteristics of state-of-the-art event extraction systems designed according to the
ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006).
Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role
types, and event triggers. This pipeline approach has been further extended in several subsequent studies.
For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence
of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to
ensure cross-entity consistency.
2
Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for
the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper.
2271
Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments
(together with their role types) using a structured perceptron model. The system outperformed the best
results reported for the ACE 2005 task in the literature, without the use of any external resources.
2.2 Biomedical Event Extraction
The task of event extraction has received a large amount of attention from BioNLP researchers in recent
years. Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained through
the organisation of further STs in 2011 and 2013. The STs consist of a number of different sub-tasks, the
majority of which concern the extraction of events from biomedical papers from the PubMed database.
Events generally concern interactions between biomedical entities, such as proteins, cells and chemicals.
Similarly to newswire event extraction systems, pipeline-based methods have constituted a popular
approach to extracting events in the biomedical domain (Bj?orne and Salakoski, 2013; Miwa et al., 2012).
The pipeline developed by Miwa et al. (2012) consists of a number of modules, which sequentially
detect event triggers, event arguments, event structures and hedges (i.e., speculations and negations).
The system has been applied to several event extraction tasks, and has achieved the best performance on
most of these, in comparison to other systems. It should be noted that the ordering of the components
in biomedical event extraction pipelines often differs from pipelines designed for news event extraction,
e.g., Grishman et al. (2005), which was described above.
As in newswire event detection, some joint (non pipeline-based) approaches have also been proposed
for biomedical event extraction. For example, McClosky et al. (2012) used a stacking model to combine
the results of applying two different methods to event extraction. The first method is a joint method,
similar to Li et al. (2013), that detects triggers, arguments and their roles. However, in contrast to
the structured perceptron employed in Li et al. (2013), McClosky et al. (2012) use a dual-decomposition
approach for the detection. The second method is based on dependency parsing and treats event structures
as dependency trees.
3 Adaptation of Biomedical Event Extraction to Newswire Event Extraction
In this section, we firstly analyse the differences between the domain-specific ACE 2005 and GENIA
event extraction tasks. Based on our findings, we propose an approach to adapting an existing event ex-
traction method, originally developed for biomedical event extraction, to the ACE 2005 task, by resolving
the observed differences between the two task definitions.
3.1 Differences in event extraction tasks
Both the ACE 2005 and GENIA tasks concern the task of event extraction, i.e., the identification of
relationships between entities. For both tasks, the requirement is to extract events from text that conform
to the general event description introduced earlier, i.e., a trigger and its arguments, each of which is
assigned a semantic role. Despite this high-level similarity between the tasks, their finer-grained details
diverge in a number of ways. Apart from the different textual domain, the tasks adopt varying annotation
schemes. The exact kinds of annotations provided at training time are also different, as are the evaluation
settings.
Several variants of the official task setting for the ACE 2005 corpus have been defined. This is partly
due to the demanding nature of the official task definition, which requires the detection of events from
scratch, including the recognition of named entities participating in events, together with the resolution
of coreferences. Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010)))
generally simplify the official task definition, e.g., by omitting the requirement to perform coreference
resolution. A further issue is that the test data sets for the official task setting have not been made publicly
available. As a result of the multiple existing variations of the ACE 2005 task definition that have been
employed by different research efforts, direct comparison of our results with those obtained by other
state-of-the art systems is problematic. The solution we have chosen is to adopt the same ACE 2005
event extraction task specification that has been adopted in recent research, by Hong et al. (2011) and Li
et al. (2013). For GENIA, we follow the specification of the original GENIA event extraction task.
2272
ACE 2005 GENIA
# of entity types 13 (type) / 53 (subtype) 2
Argument Entity/Nominal/Value/Time Entity
# of event types 8 (type) / 33 (subtype) 13
# of argument role types 35 7
Max # of arguments for an event 11 4
Nested events None Possible
Overlaps of events None Possible
Correspondences of arguments None Possible
Entity Available (Given) Available (Partially given)
Entity attributes Available (Given) Not available
Event attributes Available (Not given) Available (Not given)
Entity coreference Available (Given) Available (Not given)
Event coreference Available (Not given) Not available
Evaluation Trigger/Role Event
Table 1: Comparison of event definitions and event extraction tasks. ?Available annotations? are annota-
tions available in the corresponding corpus, while ?Given annotations? are annotations provided during
(training and) prediction. ?Given annotations? do not need to be predicted during event extraction.
Event annotation examples for ACE 2005 and GENIA are shown in Figures 1 and 2, respectively.
Table 1 summarises the following comparison between the two event extraction tasks.
Semantic types There are more event, role and entity types and a greater potential number of arguments
in ACE 2005 events than in GENIA events. There is also a hierarchy of event types and entity types
in ACE 2005. For example, the Life event type has Be-Born, Marry, Divorce, Injure, Die event
subtypes. Some GENIA event types can also be arranged to have a hierarchy but they are limited.
Events in ACE 2005 can take non-entity arguments, e.g., Time.
Nested events/Overlapping events Event structures are flat in ACE 2005, but they can be nested in
GENIA, i.e., an event can take other events as its arguments. Events in GENIA can also be over-
lapping, in the sense that a particular word or phrase can be a trigger for multiple events. Figure 2
illustrates both nesting and overlapping in GENIA events. These properties of GENIA events are
not addressed by methods developed for event extraction according to the ACE 2005 specification,
making direct application of these methods to the GENIA task impossible.
Links amongst arguments A specific feature of the GENIA event extraction task, which is completely
absent from the ACE 2005 task, is that links amongst arguments sometimes have to be identified.
For example, the Binding event type in the GENIA task can take the following argument role types:
Theme, Theme2, Site and Site2. The number 2 is attached to differentiate specific linkages between
arguments: Site is the location of Theme, while Site2 is the location of Theme2.
Entities, events and their attributes Entities in ACE 2005 have rich attributes associated with them.
For example, the Time entity type has an attribute to store a normalised temporal format (e.g., 2003-
03-04 for entities ?20030304?, ?March 4? and ?Tuesday?) while the GPE (Geo-Political Entity)
type has attributes such as subtypes (e.g., Nation), mention type (proper name, common noun or
pronoun), roles (location of a group or person) and style (literal or metonymic). In contrast, GENIA
entities have no attributes
3
. In ACE 2005, all entities are provided (gold) in the training and test
data and they do not need to be predicted. In GENIA, some named entities (i.e., Proteins) are also
provided, but other types of named or non-named entities that can constitute event arguments, such
as locations and sites of proteins, are not provided in the test data and thus need to be predicted
as part of the extraction process. Events in both corpora also have associated attributes: modality,
3
Types are not counted as attributes in this paper.
2273
polarity, genericity and tense in ACE 2005 and negation and speculation in GENIA. The GENIA
task definition requires event attributes to be predicted, but the ACE 2005 task definition does not.
Coreference Both entity and event coreference are annotated in ACE 2005, but only entity coreference is
annotated in GENIA. Events in ACE 2005 can take non-entity mentions, such as pronouns, as their
arguments. However, events in GENIA can take only entity mentions as arguments. Thus, instead
of non-entity mentions, coreferent entity mentions that are the closest to triggers are annotated as
arguments in GENIA. For example, in Figure 2, ?p300? and ?Foxp3? are annotated as Themes of
Gene expression events instead of ?both proteins?.
Evaluation In ACE 2005, the accuracy of extracted events is evaluated at the level of individual ar-
guments and their roles. Completeness of events is not taken into consideration (Li et al., 2013),
presumably because each event can take many arguments. Evaluation is performed by taking into
account the 33 event subtypes, rather than the 8 coarser-grained event types. In contrast, evaluation
of events according to the GENIA specification considers only the correctness of complete events,
after nested events have been broken down.
In summary, the ACE 2005 task is in some respects more complex than the GENIA task, because it
concerns a greater number event types, whose arguments may constitute a greater range of entity types,
and whose semantic roles are drawn from a larger set, some of which are specific to particular event
types and entities. In other respects, the task is more straightforward than the GENIA task, because of
the simpler nature of the event structures in ACE 2005, i.e., there are no nested or overlapping event
structures.
3.2 Adaptation of event extraction method
Since event structures are simpler in ACE 2005 than GENIA, we choose to adapt a biomedical event
extraction method to the ACE 2005 task rather than the other way around. The inverse adaptation,
starting from a newswire event extraction method, is considered more complex, since we would need to
extend the method to capture the more complex event structures required in the GENIA task. It would
additionally be inappropriate to employ domain adaptation methods (Daum?e III and Marcu, 2006; Pan
and Yang, 2010) to allow GENIA-trained models to be applied to the ACE 2005 tasks. This is because
such methods require that there is at least a certain degree of overlap between the target information
types, which is not the case in this scenario.
We employ the biomedical event extraction pipeline method described in Miwa et al. (2012) as our
starting point. Our motivation is that, due to their modular nature, pipeline approaches are often easier
to adapt to other task settings than joint approaches, e.g., (McClosky et al., 2012; Li et al., 2013).
In addition, the method has previously been shown to achieve state-of-the-art performance in several
biomedical event extraction tasks (Miwa et al., 2012).
The pipeline consists of four detectors, i.e., trigger/entity, event role, event structure, and hedge de-
tectors. The trigger/entity detector finds triggers and entities in text. The event role detector determines
which triggers/entities constitute arguments of events, links them to the appropriate event trigger and as-
signs semantic roles to the arguments. The event structure detector merges trigger-argument pairs into all
possible complete event structures, and determines which of these structures constitute actual events. The
same detector determines links between arguments, such as Theme2 and Site2. The hedge detector finds
negation and speculation information associated with events. Each detector solves multi-label multi-
class classification problems using lexical and syntactic features obtained from multiple parsers. These
features include character n-grams, word n-grams, and shortest paths between triggers and participants
within parse structures. More detailed information can be found in Miwa et al. (2012).
We have updated the original method by simplifying the format of the classification labels used by
both the event role detector and event structure detector modules. We refer to this method as BioEE,
which we have applied to the GENIA task. We use only the role types (e.g., Theme) as classification
labels for instances in the event role detector, instead of the more complex labels used in the original
version of the module, which combined event types, roles and semantic entity types of arguments (e.g.,
2274
Binding:Theme-Protein). Similarly, in the event structure detector, we use only two labels (?EVENT?
or ?NOT-EVENT?), instead of the previously used composite labels, which consisted of the event type,
together with the roles and semantic entity types of all arguments of the event (e.g., Regulation:Cause-
Protein:Theme-Protein.) We employed the simplified labels, since they increase the number of training
instances for each label. The use of such labels, compared to the more complex ones, could reduce the
potential of carrying out detailed modelling of specific aspects of the task. However, this was found not
to be an issue, since the use of the simplified labels improved the performance of the pipeline in detecting
events within the GENIA development data set (about 1% improvement in F-score). The simplification of
the set of classification labels was also vital to ensure the tractability of the classification problems within
the context of the ACE 2005 task. For example, using the same conventions to formulate classification
labels as in the original system would result in 345 possible labels (compared to 91 in GENIA) to be
predicted by the event role detector (and an even greater number of labels for the event structure detector),
based on event-role-semantic type combinations found in the ACE training/development sets.
In order to adapt the system to extract events according to the ACE 2005 specification, we modified
BioEE in several ways, making changes to both the pipeline itself and the features employed by the
different modules. We refer to this method as Adapted BioEE, and we applied this method to the ACE
2005 task. These changes were made in an attempt to address the two major differences between the
GENIA and ACE 2005 tasks, i.e., the simpler event structures and the availability of entity attribute and
coreference information in ACE.
The pipeline-based modifications consisted of removing certain modules from the original pipeline,
such that only two modules remained, i.e., the trigger/entity and event role detectors. The other two
modules of the original pipeline, i.e., the event structure and hedge detectors, were designed to deal with
problems that do not exist in the ACE 2005 extraction task, and thus their usage would be redundant.
Instead of using the event structure detector to piece the different elements of an event, we simply aggre-
gate all the arguments of the same trigger into a single event structure, after the event role detector has
been applied.
As mentioned above, the ACE 2005 task definition includes rich information about entities, including
attributes and coreference information. Existing systems developed to address this task have exploited
this information to generate rich feature sets for classification (Liao and Grishman, 2010; Li et al.,
2013). Based on the demonstrated utility of this information within the context of event extraction, we
also choose to use it, by adding binary feature that indicate the presence of base forms, entity subtypes,
and attributes of the entities and their coreferent entities to features in both detectors above. We choose
to use base forms, since surface forms of entities are not used by most biomedical event extraction
systems, including BioEE. We also add the features for Brown clusters (Brown et al., 1992) following Li
et al. (2013). Further details can be found in Li et al. (2013).
4 Evaluation
4.1 Evaluation settings
To assess the performance of Adapted BioEE on the ACE 2005 task, we followed the evaluation process
and settings used in previously reported studies (Hong et al., 2011; Li et al., 2013). ACE 2005 consists
of 599 documents. In order to facilitate direct comparison with other systems trained on the same data,
we conducted a blind test on the same 40 newswire documents that were used for evaluation in (Ji and
Grishman, 2008; Li et al., 2013), and used the remaining documents as training/development sets. We
use precision (P), recall (R) and F-score (F) to report the performance of the adapted system in classifying
triggers and argument roles. We use the latter F-score as our primary metric for comparing our system
with other systems, since this score better reflects the performance of the extraction of event structures.
GENIA consists of 34 full paper articles (Kim et al., 2013). To evaluate the performance of BioEE
on the GENIA task, we followed the task setting in BioNLP ST 2013 and used the official evaluation
systems provided by the organisers. We also used the same partitioning of data that was employed in
the official BioNLP ST 2013 evaluation, with 20 articles being used as the training/development set, and
the remaining 14 articles being held back as the test set. For brevity, we show the only the primary P,
2275
Arg. Role Decomposition Event Detection
P R F P R F (%)
BioEE 71.76 47.44 57.12 64.36 44.62 52.71
BioEE (+Entity) 69.47 46.94 56.02 61.81 44.11 51.48
EVEX 64.30 48.51 55.30 58.03 45.44 50.97
TEES-2.1 62.69 49.40 55.26 56.32 46.17 50.74
Table 2: Overall performance of BioEE on the GENIA data set
Trigger Classification Arg. Role Classification Event Detection
P R F P R F P R F (%)
Adapted BioEE 59.9 72.6 65.7 54.2 50.2 52.1 20.7 21.7 21.2
Adapted BioEE (-Entity) 57.9 71.5 64.0 51.0 48.1 49.5 19.7 19.3 19.5
Li et al. (2013) 73.7 62.3 67.5 64.7 44.4 52.7 - - -
Hong et al. (2011) 72.9 64.3 68.3 51.6 45.5 48.4 - - -
Table 3: Overall performance of Adapted BioEE on the ACE 2005 data set
R and F scores in the shared task, i.e., the EVENT TOTAL results obtained using the approximate span
& recursive evaluation method, as recommended by the organisers. The method individually evaluates
each complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxed
span matching, after nested events have been broken down as explained in Section 3.1. Note that the
scores do not count the non-named entities, hedges, and links between arguments, since only core events
are considered in the official evaluation.
We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagae
and Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for the
GENIA task. We also employed the GENIA sentence splitter (S?tre et al., 2007) for sentence splitting,
and the snowball (Porter2) stemmer
4
for stemming. We did not make use of any other external resources,
such as dictionaries, since this would hinder direct comparison of the two versions of the system.
4.2 Evaluation on GENIA
The ?Event Detection? column in Table 2 shows evaluation results of BioEE on GENIA. The effects
on performance by including entity-related features, i.e., entity base forms and Brown clustering, as
introduced in Section 3.2, are shown as ?BioEE (+Entity)?. The inclusion of these features slightly
degrades the performance.
For completeness, we also show in Table 2 the best and second best performing systems that took
part in the official BioNLP 2013 ST evaluation: EVEX (Hakala et al., 2013) and TEES-2.1 (Bj?orne and
Salakoski, 2013). TEES-2.1 consists of a modular pipeline similar to BioEE, but it uses a different set
of features. EVEX enhances the output of TEES-2.1, by using information obtained from the results of
large-scale event extraction. The comparison shows that BioEE achieves state-of-the-art event extraction
performance on the GENIA task.
4.3 Evaluation on ACE 2005
The ?Trigger Classification? and ?Arg. Role Classification? columns of Table 3 summarise the evaluation
results of the Adapted BioEE system (as described in Section 3.2) on the ACE 2005 task.
We analysed the effects of incorporating features based on entity-related information into the extrac-
tion process, by repeating the experiments with such features omitted (-Entity). As can be observed in
Table 3, the removal of entity-related features led to 3% performance decrease in F-score.
For completeness, Table 3 also illustrates the results of state-of-the-art systems that were specifi-
cally developed for ACE 2005: the system based on a joint approach (Li et al., 2013) and the pipeline-
based system enhanced with web-gathered information (Hong et al., 2011). The difference between the
4
snowball.tartarus.org
2276
Adapted BioEE and the best system is small and insignificant and the Adapted BioEE achieved perfor-
mance that is comparable to or better than these other systems, in terms of the F-scores in argument role
classification.
5 Discussion
To further investigate the differences in performance of the BioEE and Adapted BioEE systems on the
two tasks, we evaluate the scores achieved for each task using the evaluation criteria originally designed
for the other task. Specifically, we apply the ACE 2005 argument role classification criteria to the out-
put of GENIA task, and we apply the complete event-based evaluation, originally used to evaluate the
GENIA task, to the events extracted for the ACE 2005 task. The ?Arg. Role Decomposition? column of
Table 2 depicts the former evaluation, while the ?Event Detection? column of Table 3 shows the latter.
Table 2 also shows the performance of the other biomedical event extraction systems introduced above
in carrying out argument role classification, since such information was provided as ?Decomposition?
within the results of the original task evaluation
5
. Although the results shown for ?Arg. Role Decompo-
sition? in Table 2 are not directly comparable to those shown for ?Arg. Role Classification? in Table 3
(given the different characteristics of GENIA and ACE 2005 tasks), the scores are broadly comparable.
This demonstrates that the task of argument role classifications is equally challenging for both tasks.
The ?Event Detection? column of Table 3 illustrates event-based evaluation scores on ACE 2005.
The event structure detector was added to the pipeline to facilitate comparison of the results of the two
different tasks in a similar setting, and performance was evaluated according to the GENIA evaluation
criteria. Evaluation scores on ACE 2005 are unexpectedly low compared to those in Table 2. Considering
that the performance of argument role classification is similar in both tasks, this low performance is likely
to be due to the large number of potential event arguments in ACE 2005. This means that, in comparison
to GENIA events, which have a small number of possible argument types, there is a greater chance that
some arguments of more complex ACE 2005 events will fail to be detected. According to the GENIA
evaluation criteria, even if the majority of arguments has been correctly identified, the complete event
structure will still be evaluated as incorrect. This helps to explain why such evaluation criteria may have
been deemed inappropriate in the original ACE 2005 evaluations.
Subsequently, we analysed the effects of utilising entity-related features. We show the results obtained
by adding entity information (+Entity) in Table 2 and the results obtained by removing entity information
(-Entity) in Table 3. The positive or negative effect on performance of adding or removing these features
is consistent across all subtask evaluations shown in the two tables, although the exact level of perfor-
mance improvement or degradation depends on the subtask under evaluation. Overall, the inclusion of
the features degraded the performance of BioEE on the GENIA task, but improved the performance of
Adapted BioEE on the ACE 2005 task. These differences may be due to the increased richness of en-
tity information in the ACE 2005 corpus, suggesting that enriching entities in the GENIA corpus with
attribute information could be a possible way to further improve the performance of the system on this
task.
6 Conclusions and Future Work
In this paper, we have described our adaptation of a biomedical event extraction method to the newswire
domain. We firstly evaluated the method on a biomedical event extraction task (GENIA), and showed
that its performance was superior to other state-of-the-art systems designed for the task. We then adapted
the method to a newswire event extraction task (ACE 2005), by addressing the major differences between
the tasks. With only a small number of adaptations, the resulting system was also able to achieve state-of-
the-art performance on the newswire extraction task. These results show that there is no need to develop
separate systems for event extraction tasks in different domains, as long as the types of tasks being
addressed exhibit domain-independent features. However, further discussion and evaluation is needed to
better understand how different potential methods for adapting such tools from one domain to another
can be used and/or combined effectively.
5
bionlp-st.dbcls.jp/GE/2013/results
2277
As future work, we intend to further investigate the adaptation of alternative methods proposed for
use in one domain to another domain. Several interesting approaches have been described, such as the
utilisation of contextual information beyond the boundaries of individual sentences in the newswire do-
main (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in the
biomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet been
investigated. We also intend to investigate the possibility of discovering and utilising shared information
between the two domains (Goldwasser and Roth, 2013). Encouraging greater levels of communication
between researchers working on NLP tasks in different domains will help to stimulate such new direc-
tions of research, both for event extraction and for other related information extraction tasks, such as
relation extraction and coreference resolution.
Acknowledgements
This work was supported by the Arts and Humanities Research Council (AHRC) [grant number
AH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Commu-
nity?s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-in-
Aid for Young Scientists (B) [grant number 25730129].
References
David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning
about Time and Events, pages 1?8, Sydney, Australia, July. ACL.
Jari Bj?orne and Tapio Salakoski. 2013. Tees 2.1: Automated annotation scheme learning in the bionlp 2013 shared
task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16?25, Sofia, Bulgaria, August. ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2. In Proceedings of the 7th Message Understanding
Conference (MUC-7/MET-2).
Hal Daum?e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Simple information extraction (sie): A portable
and effective ie system. In Proceedings of the Workshop on Adaptive Text Extraction and Mining (ATEM 2006),
pages 9?16, Trento, Italy, April. Association for Computational Linguistics.
Dan Goldwasser and Dan Roth. 2013. Leveraging domain-independent information in semantic parsing. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 462?466, Sofia, Bulgaria, August. Association for Computational Linguistics.
Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU?s english ACE 2005 system description. In
Proceedings of ACE 2005 Evaluation Workshop, Washington, US.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. Evex in st?13:
Application of a large-scale text mining resource to event extraction and network construction. In Proceedings
of the BioNLP Shared Task 2013 Workshop, pages 26?34, Sofia, Bulgaria, August. ACL.
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity in-
ference to improve event extraction. In Proceedings of the 49th ACL-HLT, pages 1127?1136, Portland, Oregon,
USA, June. ACL.
Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings
of ACL-08: HLT, pages 254?262, Columbus, Ohio, June. ACL.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori. 2013. The genia event extraction shared task, 2013 edition
- overview. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15, Sofia, Bulgaria, August.
ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proceedings of the 51st ACL, pages 73?82, Sofia, Bulgaria, August. ACL.
2278
Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction.
In Proceedings of the 48th ACL, pages 789?797, Uppsala, Sweden, July. ACL.
Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
835?844, Jeju Island, Korea, July. Association for Computational Linguistics.
David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher Manning. 2012. Com-
bining joint models for biomedical event extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational
Linguistics, 34(1):35?80, March.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
Longhua Qian and Guodong Zhou. 2012. Tree kernel-based protein?protein interaction extraction from biomedi-
cal literature. Journal of biomedical informatics, 45(3):535?543.
Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the
national conference on artificial intelligence, pages 1044?1049.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, YusukeMiyao, Yuichiro Matsubayashi, and Tomoko Ohta. 2007.
AKANE System: Protein-protein interaction pairs in BioCreAtIvE2 Challenge, PPI-IPS subtask. In Proceed-
ings of the Second BioCreative Challenge Evaluation Workshop, pages 209?212, CNIO, Madrid, Spain, April.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. ACL.
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training
corpus. Linguistic Data Consortium.
2279
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701?1712,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining String and Context Similarity
for Bilingual Term Alignment from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
Automatically compiling bilingual dictio-
naries of technical terms from comparable
corpora is a challenging problem, yet with
many potential applications. In this paper,
we exploit two independent observations
about term translations: (a) terms are of-
ten formed by corresponding sub-lexical
units across languages and (b) a term and
its translation tend to appear in similar lex-
ical context. Based on the first observa-
tion, we develop a new character n-gram
compositional method, a logistic regres-
sion classifier, for learning a string similar-
ity measure of term translations. Accord-
ing to the second observation, we use an
existing context-based approach. For eval-
uation, we investigate the performance of
compositional and context-based methods
on: (a) similar and unrelated languages,
(b) corpora of different degree of compa-
rability and (c) the translation of frequent
and rare terms. Finally, we combine the
two translation clues, namely string and
contextual similarity, in a linear model and
we show substantial improvements over
the two translation signals.
1 Introduction
Bilingual dictionaries of technical terms are re-
sources useful for various tasks, such as computer-
aided human translation (Dagan and Church,
1994; Fung and McKeown, 1997), Statistical Ma-
chine Translation (Och and Ney, 2003) and Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997). In the last two decades, researchers
have focused on automatically compiling bilingual
term dictionaries either from parallel (Smadja et
al., 1996; Van der Eijk, 1993) or comparable cor-
pora (Rapp, 1999; Fung and Yee, 1998). While
parallel corpora contain the same sentences in two
languages, comparable corpora consist of bilin-
gual pieces of text that share some features, only,
such as topic, domain, or time period. Comparable
corpora can be constructed more easily than paral-
lel corpora. Freely available, up-to-date, on-line
resources (e.g., Wikipedia) can be employed.
In this paper, we exploit two different sources
of information to extract bilingual terminology
from comparable corpora: the compositional and
the contextual clue. The compositional clue is
the hypothesis that the representations of a term
in any pair of languages tend to consist of cor-
responding lexical or sub-lexical units, e.g., pre-
fixes, suffices and morphemes. In order to cap-
ture associations of textual units across languages,
we investigate three different character n-gram ap-
proaches, namely a Random Forest (RF) classifier
(Kontonatsios et al., 2014), Support Vector Ma-
chines with an RBF kernel (SVM-RBF) and a Lo-
gistic Regression (LogReg) classifier. Whilst the
previous approaches take as an input monolingual
features and then try to find cross-lingual map-
pings, our proposed method (LogReg classifier)
considers multilingual features, i.e., tuples of co-
occurring n-grams.
The contextual clue is the hypothesis that mu-
tual translations of a term tend to occur in similar
lexical context. Context-based approaches are un-
supervised methods that compare the context dis-
tributions of a source and a target term. A bilin-
gual seed dictionary is used to map context vec-
tor dimensions of two languages. Li and Gaussier
(2010) suggested that the seed dictionary can be
used to estimate the degree of comparability of a
bilingual corpus. Given a seed dictionary, the cor-
pus comparability is the expectation of finding for
each word of the source corpus, its translation in
the target part of the corpus. The performance of
context-based methods has been shown to depend
on the frequency of terms to be translated and the
1701
corpus comparability. In this work, we use an ex-
isting distributional semantics approach to locate
term translations.
Furthermore, we hypothesise that the compo-
sitional and contextual clue are orthogonal, since
the former considers the internal structure of terms
while the latter exploits the surrounding lexical
context. Based on the above hypothesis, we com-
bine the two translation clues in a linear model.
For experimentation, we construct compara-
ble corpora for four language pairs (English-
Spanish, English-French, English-Greek and
English-Japanese) of the biomedical domain.
We choose this domain because a large propor-
tion of the medical terms tends to composition-
ally translate across languages (Lovis et al., 1997;
Namer and Baud, 2007). Additionally, given the
vast amount of newly introduced terms (neolo-
gisms) in the medical domain (Pustejovsky et al.,
2001), term alignment methods are needed in or-
der to automatically update existing resources.
We investigate the following aspects of term
alignment: (a) the performance of compositional
methods on closely related and on distant lan-
guages, (b) the performance of context vectors and
compositional methods when translating frequent
or rare terms, (c) the degree to which the corpus
comparability affects the performance of context-
based and compositional methods (d) the improve-
ments that we can achieve when we combine the
compositional and context clue.
Our experiments show that the performance of
compositional methods largely depends on the dis-
tance between the two languages. The perfor-
mance of the context-based approach is greatly
affected by corpus-specific parameters (the fre-
quency of occurrence of the terms to be translated
and the degree of corpora comparability). It is also
shown that the combination of compositional and
contextual methods performs better than each of
the clues, separately. Combined systems can be
deployed in application environments with differ-
ent language pairs, comparable corpora and seeds
dictionaries.
The LogReg, dictionary extraction method de-
scribed in this paper is freely available
1
.
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/LogReg-TermAlign.tar.gz
2 Related Work
Context-based methods (Fung and Yee, 1998;
Rapp, 1999) adapt the Distributional Hypothesis
(Harris, 1954), i.e., words that occur in similar
lexical context tend to have the same meaning, in
a multilingual environment. They represent the
context of each term t as a context vector, usu-
ally following the bag-of-words model. Each di-
mension of the vector corresponds to a context
word occurring within a predefined window, while
the corresponding value is computed by a corre-
lation metric, e.g., Log-Likelihood Ratio (Morin
et al., 2007; Chiao and Zweigenbaum, 2002) or
Point-wise Mutual Information (Andrade et al.,
2010). A general bilingual dictionary is then used
to translate/project the target context vectors into
the source language. As a result, the source and
target context vectors become directly compara-
ble. In a final step, candidate translations are being
ranked according to a distance metric, e.g., cosine
similarity (Tamura et al., 2012) or Jaccard index
(Zanzotto et al., 2010; Apidianaki et al., 2012).
Whilst context-based methods have become a
common practise for bilingual dictionary extrac-
tion from comparable corpora, nonetheless, their
performance is subject to various factors, one of
which is the quality of the comparable corpus. Li
and Gaussier (2010) introduced the corpus com-
parability metric and showed that it is related to
the performance of context vectors. The higher
the corpus comparability is, the higher the perfor-
mance of context vectors is. Furthermore, context
vector approaches are sensitive to the frequency of
terms. For frequent terms, distributional seman-
tics methods exhibit robust performance since the
corresponding context is more informative. Chiao
and Zweigenbaum (2002) reported an accuracy of
91% for the top 20 candidates when translating
terms that occur 100 times or more. However,
the performance of context vectors drastically de-
creases for lower frequency terms (Kontonatsios et
al., 2014; Morin and Daille, 2010).
Our work is more closely related to a second
class of term alignment methods that exploits the
internal structure of terms between a source and
a target language. Compositional translation al-
gorithms are based on the principal of composi-
tionality (Keenan and Faltz, 1985), which claims
that the translation of the whole is a function of
the translation of its parts. Lexical (Morin and
Daille, 2010; Daille, 2012; Robitaille et al., 2006;
1702
Tanaka, 2002) and sub-lexical (Delpech et al.,
2012) compositional algorithms are knowledge-
rich approaches that proceed in two steps, namely
generation and selection. In the generation step,
an input source term is segmented into basic trans-
lation units: words (lexical compositional meth-
ods) or morphemes (sub-lexical methods). Then
a pre-compiled, seed dictionary of words or mor-
phemes is used to translate the components of the
source term. Finally, a permutation function gen-
erates candidate translations using the list of the
translated segments. In the selection step, candi-
date translations are ranked according to their fre-
quency (Morin and Daille, 2010; Robitaille et al.,
2006) or their context similarity with the source
term (Tanaka, 2002). The performance of the
compositional translation algorithms is bound to
the coverage of the seed dictionary (Daille, 2012).
Delpech et al. (2012) noted that 30% of untrans-
lated terms were due to the low coverage of the
seed dictionary.
Kontonatsios et al. (2014) introduced a Random
Forest (RF) classifier that learns correspondences
of character n-grams between a source and target
language. Unlike lexical and sub-lexical compo-
sitional methods, a RF classifier does not require
a bilingual dictionary of translation units. The
model is able to automatically build correlation
paths between source and target sub-lexical seg-
ments that best discriminate translation from non-
translation pairs. However, being a supervised
method, it still requires a seed bilingual dictio-
nary of technical terms for training. The RF classi-
fier was previously applied on an English-Spanish
comparable corpus and it was shown to signifi-
cantly outperform context-based approaches.
3 Methods
In this section we describe the character n-gram
models, the context vector method and the hybrid
system. The lexicon induction task is formalised
as a two-class classification problem. Given a pair
of terms in a source and a target language, the out-
put is a prediction of whether the terms are mutual
translations are not. Furthermore, each term align-
ment method implements a ranking function that
calculates a similarity score between a source and
a target term. The methods rank target terms ac-
cording to the similarity score and select the top N
ranked terms as candidate translations. The rank-
ing functions will be discussed in the following
subsections.
3.1 Character n-gram models
Let s be a source term containing p character n-
grams (s={s
1
, s
2
, ..., s
p
} s
i
? S, ?i ? [1, p])
and t a target term of q n-grams (t={t
1
, t
2
, ..., t
q
}
t
i
? T , ?i ? [1, q]). We extract charac-
ter n-grams by considering any contiguous, non-
linguistically motivated sequence of characters
that occurs within a window size of [2 ? 5]
2
) for
English, French and Greek. For Japanese, uni-
grams are included (window size of [1 ? 5] be-
cause Japanese terms often contain Kanji (Chi-
nese) characters.
Given the two lists of source and target n-grams,
our objective is to find an underlying relationship
between S and T that best discriminates trans-
lation from non-translation pairs. The RF clas-
sifier was previously shown to exhibit such be-
haviour (Kontonatsios et al., 2014). An RF clas-
sifier (Breiman, 2001) is a collection of decision
trees voting for the most popular class. For a pair
of source and target terms ?s, t?, the RF method
creates feature vectors of a fixed size 2r, i.e., first
order feature space. The first r features are ex-
tracted from the source term, while the last r fea-
tures from the target term. Each feature has a
boolean value (0 or 1) that designates the pres-
ence/absence of the corresponding n-gram in the
input instance.
The ability of the RF to detect latent associa-
tions between S and T relies on the decision trees.
The internal nodes of a decision tree represent the
n-gram features that are linked together in the tree-
hierarchy. Each leaf node of the trees is labelled as
translation or non-translation indicating whether
the parent path of n-gram features is positively or
negatively associated. The classification margin
that we use to rank the candidate translations is
given by a margin function (Breiman, 2001):
mg(X,Y ) = av(I(x) = 1)?av(I(x)) = 0) (1)
where x is an instance ?s, t?, y ? Y = {0, 1} the
class label, I(?) : (s, t) ?? {0, 1} is the indicator
function of a decision tree and av(I(?)) the aver-
age number of trees voting for the same class la-
bel. In our experiments, we used the same settings
as the ones reported in Kontonatsios et al. (2014).
2
we have experiments with larger and narrower window
sizes but this setting resulted in better translation accuracy
1703
We used 140 decision trees and log
2
|2q| + 1 ran-
dom features. For training an RF model, we used
the WEKA platform (Hall et al., 2009).
The second class of machine learning algo-
rithms that we investigate is Support Vector Ma-
chines (SVMs). The simplest version of SVMs
is a linear classifier (linear-SVM) that tries to
place a hyperplane, a decision boundary, that sepa-
rates translation from non-translation instances. A
linear-SVM is a feature agnostic method since the
model only exploits the position of the vectors in
the hyperspace to achieve class separation (Hastie
et al., 2009).
The first order feature representation used with
the RF classifier does not model associations be-
tween S and T . Hence, intuitively, a first or-
der feature space is not linearly separable, i.e.,
there exists no decision boundary that divides the
data points into translations and non-translations.
3
. To solve non-linear classification problems,
SVMs employ non-linear kernels. A kernel func-
tion projects input instances into a higher dimen-
sional space to discover non-linear associations
between the initial features. In this new, projected
feature space, the SVM attempts to define a sep-
arating plane. For training an non-linear SVM on
the first order feature space, we used the LIBSVM
package (Chang and Lin, 2011) with a radial ba-
sis function (RBF) kernel. For ranking candidate
translations, we used the decision value given by
LIBSVM which represents the distance between
an instance and the hyperplane. To translate a
source term, the method ranks candidate transla-
tions by decision value and suggests as best trans-
lation the candidate with the maximum distance
(maximum margin).
While the first order models try to find cross-
lingual mappings between monolingual features,
our proposed method follows a different approach.
It models cross-lingual links between the source
and target character n-grams and uses them as
second order features to train a linear classifier.
A second order feature is a tuple of n-grams in
S and T , respectively, that co-occur in a train-
ing, translation instance. Second order feature
3
We applied a linear-SVM with the first order feature
representation on the four comparable corpora for English-
French, English-Spanish, English-Greek and English-
Japanese. In all cases, the best accuracies achieved were close
to zero. Additionally, the ranked list of candidate translations
was the same for all source terms. Hence, we can empiri-
cally suggest that the linear-SVM cannot exploit a first order
feature space.
values are boolean. Given a translation instance
?s, t? of p source and q target n-grams, there are
p?q second order features. For dimensionality re-
duction, we consider as second order features the
most frequent out of all possible first order feature
combinations, only. Experiments indicate that a
large number of features needs to be considered
to achieve robust performance. To cope with the
high dimensional second order space, we use LI-
BLINEAR (Fan et al., 2008), which is designed
to solve large-scale, linear classifications prob-
lems. LIBLINEAR implements two linear clas-
sification algorithms: LogReg and linear-SVM.
Both models solve the same optimisation problem,
i.e., determine the optimal separating plane, but
they adopt different loss functions. Since LIBLIN-
EAR does not support decision value estimations
for the linear-SVM, we only experimented with
LogReg. Similarly to SVM-RBF, LogReg ranks
candidate translations by classification margin.
3.2 Context vectors
We follow a standard approach to calculate context
similarity of source and target terms (Rapp, 1999;
Morin and Daille, 2010; Morin and Prochasson,
2011a; Delpech et al., 2012). Context vectors
of candidate terms in the source and target lan-
guage are populated after normalising each bilin-
gual corpus, separately. Normalisation consists
of stop-word filtering, tokenisation, lemmatisa-
tion and Part-of-Speech (PoS) tagging. For En-
glish, Spanish and French we used the TreeTagger
(Schmid, 1994) while for Greek we used the ILSP
toolkit (Papageorgiou et al., 2000). The Japanese
corpus was segmented and PoS-tagged using Ju-
man (Kurohashi and Kawahara, 2005).
In succession, monolingual context vectors are
compiled by considering all lexical units that oc-
cur within a window of 3 words before or af-
ter a term (a seven-word window). Only lexical
units (seeds) that occur in a bilingual dictionary
are retained The values in context vectors are Log-
Likelihood Ratio associations (Dunning, 1993) of
the term and a seed lexical unit occurring in it. In
a second step, we use the translations in the seed
dictionary to map target context vectors into the
source vector space. If there are several transla-
tions for a term, they are all considered with equal
weights. Finally, candidate translations are ranked
in descending order of the cosine of the angle be-
tween the mapped target vectors and the source
1704
Trainingcorpus
Testcorpus
character n-grammodel context vectors
hybrid model
Annotate Annotate
Train Project
seed termdictionary seed worddictionary
Figure 1: Architecture of the hybrid term align-
ment system.
vector.
3.3 Hybrid term alignment system
Figure 1 illustrates a block diagram of our term
alignment system. We use two bilingual seed dic-
tionaries: (a) a dictionary of term translation pairs
to train the n-gram models and (b) a dictionary of
word-to-word correspondences to translate target
context vectors. The n-gram and context vector
methods are used separately to score term pairs.
The n-gram model computes the value of the com-
positional clue while the context vector estimates
the score of the contextual clue. The hybrid model
combines both methods by using the correspond-
ing scores as features to train a linear classifier.
For this, we used a linear-SVM of the LIBSVM
package with default values for all parameters.
4 Data
Following previous research (Prochasson and
Fung, 2011; Irvine and Callison-Burch, 2013;
Klementiev et al., 2012), we construct compara-
ble biomedical corpora using Wikipedia as a freely
available resource.
Starting with a list of 4K biomedical English
terms (query-terms), we collected 4K English
Wikipedia articles, by matching query-terms to the
topic signatures of articles. Then, we followed
the Wikipedia interlingual links to retrieve the-
matically related articles in each target language.
Since not all English articles contain links for all
four target languages (Spanish, French, Greek and
Japanese), we used a different list of query-terms
for each language pair. Corpora were randomly
divided into training and testing parts. For train-
ing we used 3K documents and for testing the re-
maining 1K. Table 1 shows the size of corpora in
terms of numbers of source (SW) and target words
(TW).
4.1 Seed dictionaries
As shown in Figure 1, the term alignment methods
require two seed bilingual dictionaries: a term and
a word dictionary. The character n-gram models
rely on a bilingual term dictionary to learn asso-
ciations of n-grams that appear often in technical
terms. The dictionary may contain both single-
word and multi-word terms. For English-Spanish
and English-French we used UMLS (Bodenreider,
2004) while for English-Japanese we used an elec-
tronic dictionary of medical terms (Denshika and
Kenkyukai, 1991).
An English-Greek biomedical dictionary was
not available at the time of conducting these ex-
periments, thus we automatically compiled a dic-
tionary from a parallel corpus. For this, we trained
a standard Statistical Machine Translation system
(Koehn et al., 2007) on EMEA (Tiedemann, 2009),
a biomedical parallel corpus containing sentence-
aligned documents from the European Medicines
Agency. Then, we extracted all English-Greek
pairs for which: (a) the English sequence was
listed in UMLS and (b) the translation probability
was equal or higher to 0.7.
The sizes of the seed term dictionaries vary sig-
nificantly, e.g., 500K entries for English-French
but only 20K entries for English-Greek. How-
ever, the character n-gram models require a rela-
tively small portion of the corresponding dictio-
nary to converge. In the reported experiments,
we used 10K translation pairs as positive, train-
ing instances. In addition, we generated an equal
number of pseudo-negative instances by randomly
matching non-translation terms.
Morin and Prochasson (2011b) showed that the
translation accuracy of context vectors is higher
when using bilingual dictionaries that contain both
general language entries and technical terms rather
than general or domain-specific dictionaries, sep-
1705
Training corpus Test Corpus
# SW # TW # SW # TW
en-fr 4.8M 2.2M 1.9M 1.1M
en-es 4.9M 2.5M 1.8M 0.9M
en-el 10.2M 2.4M 3.3M 1.3M
en-jpn 5.3M 2.4M 2.3M 1.2M
Table 1: Statistics of the English-French (en-
fr), Engish-Spanish (en-es), English-Greek (en-
el) and English-Japanese (en-jpn) Wikipedia com-
parable corpora. SW: source words, TW: target
words
Corpus Seed words
Comparability in dictionary
en-fr 0.71 66K
en-es 0.75 40K
en-el 0.68 22K
en-jpn 0.49 57K
Table 2: Corpus comparability and number of fea-
tures of the seed word dictionaries
arately. In a mixed dictionary, lexical units are
either single-word technical terms, such as ?dis-
ease? and ?patient?, or general language words,
such as ?occur? and ?high?. Note that we have
already compiled a seed term dictionary for each
pair of languages. Following the suggestion of
Morin and Prochasson (2011b), we attempt to en-
rich the seed term dictionaries with general lan-
guage entries. For this, we extracted bilingual
word dictionaries for English-Spanish, English-
French and English-Greek by applying GIZA++
(Och and Ney, 2003) on the EMEA corpus. We
then concatenated the word with the term dictio-
naries to obtain enhanced seeds for the three lan-
guage pairs. For English-Japanese, we only used
the term dictionary to translate the target context
vectors.
Once the word dictionaries have been compiled,
we compute the corpus comparability measure. Li
and Gaussier (2010) define corpus comparability
as the percentage of words that can be translated
bi-directionally, given a seed dictionary.
Table 2 shows corpus comparability scores of
the four corpora accompanied with the number
of English, single words in the seed dictionar-
ies. It can be observed that seed dictionary sizes
are not necessarily proportional to the correspond-
ing corpus comparability scores. As expected, for
English-Japanese, corpus comparability is low be-
cause the dictionary contains single-word terms,
only. The English-Spanish dictionary is smaller
than the English-French but achieved higher cor-
pus comparability, i.e., a higher percentage of
words can be bi-directionally translated using the
corresponding seed dictionary. A possible ex-
planation is that the comparable corpora were
constructed using different lists of query-terms.
Hence, the query-terms used for English-Spanish
retrieved a more coherent corpus. The resulting
values of corpus comparability indicate that the
context vectors will perform the best for English-
Spanish while for English-Japanese the perfor-
mance is expected to be substantially lower.
4.2 Training and evaluation datasets
For evaluation, we construct a test dataset of
single-word terms, in particular nouns or adjec-
tives. The dataset contains 1K terms that occur
more frequently than 20 but not more than 200
times and are listed in the English part of the
UMLS. In order to extract candidate translations,
we considered all nouns or adjectives that occur
at least 5 times in the target part of the corpus.
Furthermore, we do not constraint the evaluation
datasets only to those terms whose corresponding
translation occurs in the corpus.
The hybrid model that combines the composi-
tional and context clue, is based on a two-feature
model. Therefore, the model converges using only
a few hundred instances. For training a hybrid
model, we used 1K translation instances that oc-
curred in the training comparable corpora. Sim-
ilarly, to the character n-gram models, pseudo-
negative instances were generated by randomly
coupling non-translation terms. The ratio of posi-
tive to negative instances is 1 : 1.
5 Experiments
In this section, we present three experiments con-
ducted to evaluate the character n-gram, con-
text vector and hybrid methods. Firstly, we
examine the performance of the n-gram mod-
els on closely related language pairs (English-
French, English-Spanish), on a distant language
pair (English-Greek) and on an unrelated language
pair (English-Japanese). English and Greek are
not unrelated because they are members of the
same language family, but also not closely re-
lated because they use different scripts. Secondly,
1706
we compare the character n-gram methods against
context vectors when translating frequent or rare
terms and on comparable corpora of similar lan-
guage pairs (English-French, English-Spanish) but
of different corpus comparability scores. Thirdly,
we evaluate the hybrid method on all four com-
parable corpora and investigate the improvement
margin of combining the contextual with the com-
positional clue.
As evaluation metrics, we adopt the top-N
translation accuracy, following most previous ap-
proaches (Rapp, 1999; Chiao and Zweigenbaum,
2002; Morin et al., 2007; Tamura et al., 2012). The
top-N translation accuracy is defined as the per-
centage of source terms for which a given method
has output the correct translation among the top N
candidate translations.
5.1 Character n-gram models
In the first experiment, we investigate the perfor-
mance of the character n-gram models consider-
ing an increasing number of features. The features
were sorted in order of decreasing frequency of oc-
currence. Starting from the top of the list, more
features were incrementally added and translation
accuracy was recorded.
Figure 2 shows the top-20 translation accu-
racy of single-word terms on an increasing num-
ber of first and second order features. With re-
gards to the first order models (Subfigure 2a),
the Random Forest (RF) classifier outperforms
our baseline method (SVM-RBF) for all four lan-
guage pairs. The largest margin between RF and
SVM-RBF can be observed for the English-Greek
dataset while for closely related language pairs,
i.e., English-French and English-Spanish, the mar-
gin is smaller. Furthermore, it can be noted that
using only a small number of first order features,
1K features (500 for the source and 500 for the
target language, both n-gram models reach a sta-
ble performance.
In contrast to the first order models, the Lo-
gReg classifier requires a large number of sec-
ond order features to achieve a robust performance
(Subfigure 2b). Starting from 100K features, the
translation accuracy continuously increases. The
best performance is observed for a total number
of 4M second order features when considering
the English-French, English-Spanish and English-
Greek datasets. For English-Japanese, the best
performance is achieved for 2M features. Beyond
this point, translation accuracy decreases slightly.
After feature selection is performed, we directly
compare all the character n-gram models. Table 3
summarises performance achieved by the LogReg,
RF and SVM-RBF models. It can be noted that
LogReg and RF performed similarly for closely
related languages (no statistically significant dif-
ferences were observed) while both methods out-
performed the SVM-RBF. However, for English-
Greek and English-Japanese, LogReg achieved
a statistically significant improvement over the
translation accuracy of RF and SVM-RBF. Lo-
gReg outperformed RF by 7% for English-Greek,
while for English-Japanese the improvement was
10% and 17% percent for top-1 and top-20 accu-
racy, respectively. Finally, it can be observed that
the more distant the language pair is, the lower the
performance.
5.2 N-gram methods and context vectors
In this experiment, we compare the n-gram meth-
ods against context vectors with regards to two pa-
rameters: (a) the frequency of source terms to be
translated and (b) corpus comparability. English-
French and English-Spanish are similar language
pairs but the corresponding corpora are of dif-
ferent corpus comparability scores. To investi-
gate how performance is affected by term occur-
rence frequency, we compiled an additional test
dataset of 1K rare English terms in the frequency
range [10, 20]. Our intuition is, that character n-
gram methods will perform similarly for all set-
tings since character n-grams are corpus indepen-
dent features.
We compare (a) the character n-gram models
(LogReg, RF and SVM-RBF) with (b) the con-
text vector method (context) and (c) an upper
bound. The latter represents the percentage of
source terms for which a reference translation ac-
tually occurs in the target corpus. Hence, the up-
per bound is the maximum performance achiev-
able according to the reference evaluation.
Figure 3a shows the top-20 translation accu-
racy for high and medium frequency terms, within
the frequency range [20, 200]. Context vectors
achieved a robust performance of 52% and 45%
for English-Spanish and English-French, respec-
tively. The difference in corpus comparability
can explain this 7% margin between these perfor-
mances. As shown in Table 2, the corpus com-
parability scores for English-Spanish and English-
1707
0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 400 600 800 1000
trans
lation
 accu
racy 
@ 20
# first order features
RF (en-jpn)SVM-RBF (en-jpn)RF (en-el)SVM-RBF (en-el)
RF (en-fr)SVM-RBF (en-fr)RF (en-es)SVM-RBF (en-es)
(a) First order n-gram models
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
100 200 300 400 500 1000 2000 3000 4000
trans
lation
 accu
racy 
@ 20
# second order features (x10^3)
en-jpn en-el en-fr en-es
(b) Second order n-gram model
Figure 2: Top-20 translation accuracy of models trained on (a) first and (b) second order features
English-French English-Spanish English-Greek English-Japanese
acc@1 acc@20 acc@1 acc@20 acc@1 acc@20 acc@1 acc@20
LogReg 0.45 0.61 0.42 0.62 0.3 0.48 0.25 0.41
RF 0.47 0.58 0.43 0.59 0.23 0.41 0.15 0.24
SVM-RBF 0.38 0.51 0.33 0.53 0.1 0.25 0.06 0.16
Table 3: Top-1 (acc@1) and top-20 (acc@20) translation accuracy of LogReg, RF and SVM-RBF
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(a) Test terms with frequency [20, 200]
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(b) Test terms with frequency [10, 20]
Figure 3: Top-20 translation accuracy of terms in the frequency range of [10, 200] and [10, 20]
French are 0.75 and 0.71, respectively. In contrast
to context vectors, the character n-gram methods
performed comparably.
A second factor that affects the performance of
context vectors, is the frequency of the terms to
be translated. The translation of rare terms has
been shown to be a challenging case for context
vectors. For example, Morin and Daille (2010)
reported low accuracy (21% for the top-20 can-
didates) of context vectors for terms occurring 20
times or less. In our experiments, Figure 3b illus-
trates accuracies achieved for less frequent terms
([10, 20]). The performance of context vectors is
significantly lower, 26% for English-Spanish and
21% for English-French. Furthermore, the trans-
lation accuracy of the n-gram methods decreases
slightly (? 5% to 8%). This can be explained
by the decrease of the upper bound for lower fre-
quency terms (? 3% to 6%).
5.3 Combining internal and contextual
similarity
We have hypothesised that the compositional and
contextual clue are orthogonal, i.e., they convey
1708
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(a) Top-20 accuracy (acc@20)
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(b) Top-1 accuracy (acc@1)
Figure 4: Overall performance. Top-20 and top-1 translation accuracy
different and possibly complimentary information.
To investigate this intuition, we evaluate the hybrid
model on all four comparable corpora, for term oc-
currence frequencies in [20, 200].
Figure 4a illustrates top-20 translation accu-
racy scores for (a) the character n-gram models,
(b) the context vector method and (c) the hy-
brid models, i.e., LogReg+Context, RF+Context,
SVM-RBF+Context. We observe that the com-
bination of the compositional and contextual clue
improved the performance of all methods. The hy-
brid model largely improved the performance of
the SVM-RBF (? 14% to 20%). With regards
to the combined signals the translation accuracy
of LogReg and RF increased by ? 4% for the
English-Japanese corpus and ? 8% for all other
corpora.
For the top 1 candidate translation, we observe
in Figure 4 smaller improvements achieved by the
hybrid model in comparison to the top-20 accu-
racy. Interestingly, the RF classifier performed
slightly better on its own for English-French,
English-Spanish and English-Japanese. This in-
dicates that the hybrid method ranks more correct
translations in the top 20 candidates but it does not
always assign the best score to the correct answer.
6 Discusion and Future work
In this paper, we investigated a compositional
and a context-based approach useful for compil-
ing bilingual dictionaries of terms automatically
from comparable corpora. Compositional transla-
tion methods exploit the internal structure of terms
across languages while context-based approaches
investigate the surrounding lexical context.
We proposed a character n-gram composi-
tional method, i.e., a Logistic Regression clas-
sifier, which uses a multilingual representation,
i.e., source and target terms. Experimental evi-
dence showed that the LogReg classifier signifi-
cantly outperformed the baseline methods on dis-
tant languages. For closely related languages, Lo-
gReg performed comparably to an existing n-gram
method based on a Random Forest classifier.
Furthermore, we compared the n-gram models
against a context-based approach under different
corpus-specific parameters: (a) corpus compara-
bility, which is relevant to the seed dictionary, and
(b) the occurrence frequency of the terms to be
translated. It was shown that the performance of
n-gram methods was not affected by different pa-
rameter settings. Only small fluctuations were ob-
served, since the n-gram methods are based on
corpus-independent features, only. In contrast,
the context-based method was affected by corpus
comparability scores. The corresponding transla-
tion accuracy declined significantly for rare terms.
Finally, we hypothesised that the n-gram and
context-based methods provide complimentary in-
formation. To test this hypothesis, we developed a
hybrid method that combines compositional and
contextual similarity scores as features in a lin-
ear classifier. The hybrid model achieved signif-
icantly better top-20 translation accuracy than the
two methods separately but minor improvements
were observed in terms of top-1 accuracy.
As future work, we plan to improve the qual-
ity of the extracted dictionary further by exploiting
additional translation signals. For example, previ-
ous works (Schafer and Yarowsky, 2002; Klemen-
tiev et al., 2012) have reported that the temporal
and topic similarity are clues that indicate transla-
tion equivalence. It would be interesting to investi-
gate the contribution of different clues for various
1709
experimental parameters, e.g., domain, distance of
languages, types of comparable corpora.
Acknowledgements
The authors would like to thank Dr. Danushka
Bollegala for providing feedback on this paper
and the three anonymous reviewers for their useful
comments and suggestions. This work was funded
by the European Community?s Seventh Frame-
work Program (FP7/2007-2013) [grant number
318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages 19?27. Asso-
ciation for Computational Linguistics.
Marianna Apidianaki, Nikola Ljube?sic, and Darja
Fi?ser. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267?
D270.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Ido Dagan and Ken Church. 1994. Termight: Identi-
fying and translating technical terminology. In Pro-
ceedings of the fourth conference on Applied natural
language processing, pages 34?40. Association for
Computational Linguistics.
Emmanuel Morin B??eatrice Daille. 2012. Revising the
compositional method for terminology acquisition
from comparable corpora. COLING 2012, 1810.
Estelle Delpech, B?eatrice Daille, Emmanuel Morin,
and Claire Lemaire. 2012. Extraction of domain-
specific bilingual lexicon from comparable corpora:
Compositional translation and ranking. In COLING,
pages 745?762.
Igakuyo Denshika and Jisho Kenkyukai. 1991.
250,000 medical term dictionary (in japanese).
Nichigai Associates, Inc.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational lin-
guistics, 19(1):61?74.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Pascale Fung and Kathleen McKeown. 1997. A
technical word-and term-translation aid using noisy
parallel corpora across language groups. Machine
Translation, 12(1-2):53?87.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Z.S. Harris. 1954. Distributional structure. Word.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of NAACL-
HLT, pages 518?523.
Edward L Keenan and Leonard M Faltz. 1985.
Boolean semantics for natural language, volume 23.
Springer.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
1710
pages 177?180. Association for Computational Lin-
guistics.
G. Kontonatsios, I. Korkontzelos, J. Tsujii, and S. Ana-
niadou. 2014. Using a random forest classifier
to compile bilingual dictionaries of technical terms
from comparable corpora. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 111?116. Association for Com-
putational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2005.
Japanese morphological analysis system juman ver-
sion 5.1 manual.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 644?652. Association for Computational
Linguistics.
Christian Lovis, R Baud, PA Michel, JR Scherrer, and
AM Rassinoux. 1997. Building medical dictionar-
ies for patient encoding systems: A methodology. In
Artificial Intelligence in Medicine, pages 373?380.
Springer.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011a.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34. Association for Computational Lin-
guistics.
Emmanuel Morin and Emmanuel Prochasson. 2011b.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2):226?
233.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Harris Papageorgiou, Prokopis Prokopidis, Voula
Giouli, and Stelios Piperidis. 2000. A unified pos
tagging architecture and its application to greek. In
Proceedings of the 2nd Language Resources and
Evaluation Conference, pages 1455?1462, Athens,
June. European Language Resources Association.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned compara-
ble documents. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1327?1335. Association for Computational
Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling french-japanese terminologies from the
web. In EACL.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In proceedings of the 6th con-
ference on Natural language learning-Volume 20,
pages 1?7. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Frank Smadja, Kathleen R McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational linguistics, 22(1):1?38.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Takaaki Tanaka. 2002. Measuring the similarity be-
tween compound nouns in different languages us-
ing non-parallel corpora. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, pages 1?7. Association for
Computational Linguistics.
1711
J?org Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248.
Pim Van der Eijk. 1993. Automating the acquisition of
bilingual terminology. In Proceedings of the sixth
conference on European chapter of the Association
for Computational Linguistics, pages 113?119. As-
sociation for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1263?1271, Stroudsburg, PA,
USA. Association for Computational Linguistics.
1712
Proceedings of the EACL 2009 Demonstrations Session, pages 61?64,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
  
Three BioNLP Tools Powered by a Biological Lexicon 
 
Abstract 
In this paper, we demonstrate three NLP 
applications of the BioLexicon, which is a 
lexical resource tailored to the biology 
domain. The applications consist of a 
dictionary-based POS tagger, a syntactic 
parser, and query processing for biomedical 
information retrieval.  Biological 
terminology is a major barrier to the 
accurate processing of literature within 
biology domain. In order to address this 
problem, we have constructed the 
BioLexicon using both manual and semi-
automatic methods. We demonstrate the 
utility of the biology-oriented lexicon 
within three separate NLP applications. 
1 Introduction 
Processing of biomedical text can frequently be 
problematic, due to the huge number of technical 
terms and idiosyncratic usages of those terms.  
Sometimes, general English words are used in 
different ways or with different meanings in 
biology literature. 
There are a number of linguistic resources 
that can be use to improve the quality of 
biological text processing.  WordNet (Fellbaum, 
1998) and the NLP Specialist Lexicon 1  are 
dictionaries commonly used within biomedical 
NLP. 
WordNet is a general English thesaurus which 
additionally covers biological terms. However, 
since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are missing.   
The Specialist Lexicon is a syntactic lexicon 
of biomedical and general English words, 
providing linguistic information about individual 
vocabulary items (Browne et al, 2003).  Whilst 
it contains a large number of biomedical terms, 
                                                 
1 http://SPECIALIST.nlm.hih.gov 
its focus is on medical terms. Therefore some 
biology-specific terms, e.g., molecular biology 
terms, are not the main target of the lexicon.  
In response to this, we have constructed the 
BioLexicon (Sasaki et al, 2008), a lexical 
resource tailored to the biology domain.  We will 
demonstrate three applications of the BioLexicon, 
in order to illustrate the utility of the lexicon 
within the biomedical NLP field.  
The three applications are: 
 
? BLTagger: a dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
2. Summary of the BioLexicon 
In this section, we provide a summary of the 
BioLexicon (Sasaki et al, 2008). It contains 
words belonging to four part-of-speech 
categories: verb, noun, adjective, and adverb.  
Quochi et al(2008) designed the database 
model of the BioLexicon which follows the 
Lexical Markup Framework (Francopoulo et al, 
2008).    
2.1 Entries in the Biology Lexicon 
The BioLexicon accommodates both general 
English words and terminologies. Biomedical 
terms were gathered from existing biomedical 
databases. Detailed information regarding the 
sources of biomedical terms can be found in  
(Rebholz-Schuhmann et al, 2008). The lexicon 
entries consist of the following: 
 
(1) Terminological verbs: 759 base forms (4,556 
inflections) of terminological verbs with 
automatically extracted verb 
subcategorization frames 
 
Yutaka Sasaki 1   Paul Thompson1   John McNaught 1, 2   Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
{Yutaka.Sasaki,Paul.Thompson,John.McNaught,Sophia.Ananiadou}@manchester.ac.uk 
61
(2)Terminological adjectives: 1,258 
terminological adjectives.   
(3) Terminological adverbs: 130 terminological 
adverbs. 
(4) Nominalized verbs: 1,771  nominalized verbs.   
(5) Biomedical terms: Currently, the BioLexicon 
contains biomedical terms in the categories of 
cell (842 entries, 1,400 variants), chemicals 
(19,637 entries, 106,302 variants), enzymes 
(4,016 entries, 11,674 variants), diseases 
(19,457 entries, 33,161 variants), genes and 
proteins (1,640,608 entries, 3,048,920 
variants), gene ontology concepts (25,219 
entries, 81,642 variants), molecular role 
concepts (8,850 entries, 60,408 variants), 
operons (2,672 entries, 3,145 variants), 
protein complexes (2,104 entries, 2,647 
variants), protein domains (16,940 entries, 
33,880 variants), Sequence ontology concepts 
(1,431 entries, 2,326 variants), species 
(482,992 entries, 669,481 variants), and 
transcription factors (160 entries, 795 
variants).   
In addition to the existing gene/protein names, 
70,105 variants of gene/protein names have been 
newly extracted from 15 million MEDLINE 
abstracts. (Sasaki et al, 2008) 
2.2. Comparison to existing lexicons 
This section focuses on the words and 
derivational relations of words that are covered 
by our BioLexicon but not by comparable 
existing resources. Figures 1 and 2 show the 
percentage of the terminological words and 
derivational relations (such as the word 
retroregulate and the derivational relation 
retroregulate ? retroregulation) in our lexicon 
that are also found in WorNet and the Specialist 
Lexicion. 
Since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are not included.   
Because the Specialist Lexicon is a 
biomedical lexicon and the target is broader than 
our lexicon, some biology-oriented words and 
relations are missing.  For example, the 
Specialist Lexicon includes the term retro-
regulator but not retro-regulate. This means that 
derivational relations of retro-regulate are not 
covered by the Specialist Lexicon.  
3. Application 1: BLTagger 
Dictionary-based POS tagging is advantageous 
when a sentence contains technical terms that 
conflict with general English words. If the POS 
tags are decided without considering possible 
occurrences of biomedical terms, then POS 
errors could arise.  
For example, in the protein name ?met proto-
oncogene precursor?, met might be incorrectly 
recognized as a verb by a non dictionary-based 
tagger.   
Input sentence: 
?IL-2-mediated activation of ??
IL/NP
IL-2/NN-BIOMED
-/-
2/CD
mediated/VVD
IL-2-mediated/UNKNOWN
IL/NP
2/CD
IL-2/NN-BIOMED
??????????
mediated/VVD
mediate/VVP
mediate/VV
of/IN
mediated/VVN
-/-
-/-
mediated/VVNdictionary-based tagging of/IN
Fig. 3 BLTagger example 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
Fig. 1  Comparison with WordNet 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
 
Fig. 2  Comparison with Specialist Lexicon 
62
In the dictionary, biomedical terms are given 
POS tag "NN-BIOMED". Given a sentence, the 
dictionary-based POS tagger works as follows.  
 
? Find all word sequences that match the 
lexical entries, and create a token graph (i.e., 
trellis) according to the word order.  
? Estimate the score of every path using the 
weights of the nodes and edges, through 
training using Conditional Random Fields.  
? Select the best path. 
 
Figure 3 shows an example of our dictionary-
based POS tagger BLTagger. 
Suppose that the input is ?IL-2-mediated 
activation of?. A trellis is created based on the 
lexical entries in the dictionary. The selection 
criteria for the best path are determined by the 
CRF tagging model trained on the Genia corpus 
(Kim et al, 2003). In this example,  
 
IL-2/NN-BIOMED -/- mediated/VVN 
activation/NN of/IN 
 
is selected as the best path.  
Following Kudo et al (2004), we adapted the 
core engine of the CRF-based morphological 
analyzer, MeCab2, to our POS tagging task.  
The features used were: 
 
? POS 
? BIOMED 
? POS-BIOMED 
? bigram of adjacent POS 
? bigram of adjacent BIOMED 
? bigram of adjacent POS-BIOMED 
 
During the construction of the trellis, white 
space is considered as the delimiter unless 
otherwise stated within dictionary entries. This 
means that unknown tokens are character 
sequences without spaces. 
As the BioLexicon associates biomedical 
semantic IDs with terms, the BLTagger attaches 
semantic IDs to the tokenizing/tagging results. 
4. Application 2: Enju full parser with the 
BioLexicon 
Enju (Miyao, et al, 2003) is an HPSG parser, 
which is tuned to the biomedical domain.  
Sentences are parsed based on the output of the 
                                                 
2 http://sourceforge.net/project/showfiles.php?group 
id=177856/ 
Stepp POS tagger, which is also tuned to the 
biomedical domain. 
To further tune Enju to the biology domain, 
(especially molecular biology), we have 
modified Enju to parse sentences based on the 
output of the BLTagger. 
As the BioLexicon contains many multi-word 
biological terms, the modified version of Enju 
parses token sequences in which some of the 
tokens are multi-word expressions.  This is 
effective when very long technical terms (e.g., 
more than 20 words) are present in a sentence. 
To use the dictionary-based tagging for 
parsing, unknown words should be avoided as 
much as possible. In order to address this issue, 
we added entries in WordNet and the Specialist 
Lexicion to the dictionary of BLTagger. 
The enhancement in the performance of Enju 
based on these changes is still under evaluation. 
However, we demonstrate a functional, modified 
version of Enju. 
5. Application 3: Query processing for IR 
It is sometimes the case that queries for 
biomedical IR systems contain long technical 
terms that should be handled as single multi-
word expressions.  
We have applied BLTagger to the TREC 2007 
Genomics Track data (Hersh et al, 2007).  The 
goal of the TREC Genomics Track 2007 was to 
generate a ranked list of passages for 36 queries 
that relate to biological events and processes.    
Firstly, we processed the documents with a 
conventional tokenizer and standard stop-word 
remover, and then created an index containing 
the words in the documents. Queries are 
processed with the BLTagger and multi-word 
expressions are used as phrase queries.  Passages 
are ranked with Okapi BM25 (Robertson et al, 
1995). 
Table 1 shows the preliminary Mean Average 
Precision (MAP) scores of applying the 
BLTagger to the TREC data set.   
By adding biology multi-word expressions 
identified by the BLTagger to query terms (row 
(a)), we were able to obtain a slightly better 
Passage2 score. As the BLTagger outputs 
semantic IDs which are defined in the 
BioLexicon, we tried to use these semantic IDs 
for query expansion (rows (b) and (d)).  However, 
the MAP scores degraded. 
63
6. Conclusions 
We have demonstrated three applications of the 
BioLexicon, which is a resource comprising 
linguistic information, targeted for use within 
bio-text mining applications.   
We have described the following three 
applications that will be useful for processing of 
biological literature. 
 
? BLTagger: dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
 
Our future work will include further intrinsic 
and extrinsic evaluations of the BioLexicon in 
NLP, including its  application to information 
extraction tasks in the biology domain. The 
BioLexicon is available for non-commercial 
purposes under the Creative Commons license. 
Acknowledgements 
This research has been supported by the EC IST 
project FP6-028099 (BOOTStrep), whose 
Manchester team is hosted by the 
JISC/BBSRC/EPSRC sponsored National Centre 
for Text Mining.   
References 
Browne, A.C., G. Divita, A.R. Aronson, and A.T. 
McCray. 2003. UMLS Language and Vocabulary 
Tools. In Proc. of AMIA Annual Symposium 2003, 
p.798. 
Dietrich Rebholz-Schuhmann, Piotr Pezik, Vivian Lee, 
Jung-Jae Kim, Riccardo del Gratta, Yutaka Sasaki, 
Jock McNaught, Simonetta Montemagni, Monica 
Monachini, Nicoletta Calzolari, Sophia Ananiadou, 
BioLexicon: Towards a Reference Terminological 
Resource in the Biomedical Domain, the 16th 
Annual International Conference on Intelligent 
Systems for Molecular Biology (ISMB-2008) 
(Poster), Toronto, Canada, 2008. 
(http://www.ebi.ac.uk/Rebholz-srv/BioLexicon/ 
BioLexicon_Poster_EBI_UoM_ILC.pdf) 
Fellbaum, C., editor. 1998. WordNet: An Electronic 
Lexical Database.  MIT Press, Cambridge, MA.. 
Francopoulo, G., M. George, N. Calzolari, M. 
Monachini, N. Bel, M. Pet, and C. Soria. 2006. 
Lexical Markup Framework (LMF). In Proc. of  
LREC 2006, Genova, Italy. 
Hersh, W., Aaron Cohen, Lynn Ruslen, and Phoebe 
Roberts, TREC 2007 Genomics Track Overview, 
TREC-2007, 2007. 
Kim, J-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.  
GENIA Corpus - Semantically Annotated Corpus 
for Bio-Text Mining. Bioinformatics, 19:i180-i182. 
Kudo T., Yamamoto K., Matsumoto Y., Applying 
Conditional Random Fields to Japanese Mor- 
phological Analysis. In Proc. of Empirical 
Methods in Natural Language Processing 
(EMNLP-04), pp. 230?237, 2004. 
Lafferty, J., A. McCallum, and F. Pereira. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labelling Sequence Data. In 
Proc. of the Eighteenth International Conference 
on Machine Learning (ICML-2001), pages 282-289.  
Miyao, Y. and J. Tsujii, 2003. Probabilistic modeling 
of argument structures including non-local 
dependencies. In Proc. of the Conference on 
Recent Advances in Natural Language Processing 
(RANLP 2003), pages 285-291. 
Quochi, V., Monachini, M., Del Gratta, R., Calzolari, 
N., A lexicon for biology and bioinformatics: the 
BOOTStrep experience. In Proc. of LREC 2008, 
Marrakech, 2008. 
Robertson, S.E., Walker S., Jones, S., Hancock-
Beaulieu M.M., and Gatford, M., 1995. Okapi at 
TREC-3. In Proc of Overview of the Third Text 
REtrieval Conference (TREC-3), pp. 109?126. 
Yutaka Sasaki, Simonetta Montemagni, Piotr Pezik, 
Dietrich Rebholz-Schuhmann, John McNaught, 
and Sophia Ananiadou, BioLexicon: A Lexical 
Resource for the Biology Domain, In Proc. of the 
Third International Symposium on Semantic 
Mining in Biomedicine (SMBM 2008), 2008. 
Table 1 Preliminary MAP scores for TREC Genomics Track 2007 data 
 
Query expansion method Passage2 MAP Aspect MAP Document MAP 
(a) BioLexicon terms 0.0702 0.1726 0.2158 
(b) BioLexicon terms 
 + semantic IDs 
0.0696 0.1673 0.2148 
(c) no query expansion  (baseline) 0.0683 0.1726 0.2183 
(d) semantic IDs 0.0677 0.1670 0.2177 
 
64
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102?107,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
BRAT: a Web-based Tool for NLP-Assisted Text Annotation
Pontus Stenetorp1? Sampo Pyysalo2,3? Goran Topic?1
Tomoko Ohta1,2,3 Sophia Ananiadou2,3 and Jun?ichi Tsujii4
1Department of Computer Science, The University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, UK
3National Centre for Text Mining, University of Manchester, Manchester, UK
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,goran,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We introduce the brat rapid annotation tool
(BRAT), an intuitive web-based tool for text
annotation supported by Natural Language
Processing (NLP) technology. BRAT has
been developed for rich structured annota-
tion for a variety of NLP tasks and aims
to support manual curation efforts and in-
crease annotator productivity using NLP
techniques. We discuss several case stud-
ies of real-world annotation projects using
pre-release versions of BRAT and present
an evaluation of annotation assisted by se-
mantic class disambiguation on a multi-
category entity mention annotation task,
showing a 15% decrease in total annota-
tion time. BRAT is available under an open-
source license from: http://brat.nlplab.org
1 Introduction
Manually-curated gold standard annotations are
a prerequisite for the evaluation and training of
state-of-the-art tools for most Natural Language
Processing (NLP) tasks. However, annotation is
also one of the most time-consuming and finan-
cially costly components of many NLP research
efforts, and can place heavy demands on human
annotators for maintaining annotation quality and
consistency. Yet, modern annotation tools are
generally technically oriented and many offer lit-
tle support to users beyond the minimum required
functionality. We believe that intuitive and user-
friendly interfaces as well as the judicious appli-
cation of NLP technology to support, not sup-
plant, human judgements can help maintain the
quality of annotations, make annotation more ac-
cessible to non-technical users such as subject
?These authors contributed equally to this work
Figure 1: Visualisation examples. Top: named en-
tity recognition, middle: dependency syntax, bot-
tom: verb frames.
domain experts, and improve annotation produc-
tivity, thus reducing both the human and finan-
cial cost of annotation. The tool presented in
this work, BRAT, represents our attempt to realise
these possibilities.
2 Features
2.1 High-quality Annotation Visualisation
BRAT is based on our previously released open-
source STAV text annotation visualiser (Stene-
torp et al 2011b), which was designed to help
users gain an understanding of complex annota-
tions involving a large number of different se-
mantic types, dense, partially overlapping text an-
notations, and non-projective sets of connections
between annotations. Both tools share a vector
graphics-based visualisation component, which
provide scalable detail and rendering. BRAT in-
tegrates PDF and EPS image format export func-
tionality to support use in e.g. figures in publica-
tions (Figure 1).
102
Figure 2: Screenshot of the main BRAT user-interface, showing a connection being made between the
annotations for ?moving? and ?Citibank?.
2.2 Intuitive Annotation Interface
We extended the capabilities of STAV by imple-
menting support for annotation editing. This was
done by adding functionality for recognising stan-
dard user interface gestures familiar from text ed-
itors, presentation software, and many other tools.
In BRAT, a span of text is marked for annotation
simply by selecting it with the mouse by ?drag-
ging? or by double-clicking on a word. Similarly,
annotations are linked by clicking with the mouse
on one annotation and dragging a connection to
the other (Figure 2).
BRAT is browser-based and built entirely using
standard web technologies. It thus offers a fa-
miliar environment to annotators, and it is pos-
sible to start using BRAT simply by pointing a
standards-compliant modern browser to an instal-
lation. There is thus no need to install or dis-
tribute any additional annotation software or to
use browser plug-ins. The use of web standards
also makes it possible for BRAT to uniquely iden-
tify any annotation using Uniform Resource Iden-
tifiers (URIs), which enables linking to individual
annotations for discussions in e-mail, documents
and on web pages, facilitating easy communica-
tion regarding annotations.
2.3 Versatile Annotation Support
BRAT is fully configurable and can be set up to
support most text annotation tasks. The most ba-
sic annotation primitive identifies a text span and
assigns it a type (or tag or label), marking for e.g.
POS-tagged tokens, chunks or entity mentions
(Figure 1 top). These base annotations can be
connected by binary relations ? either directed or
undirected ? which can be configured for e.g. sim-
ple relation extraction, or verb frame annotation
(Figure 1 middle and bottom). n-ary associations
of annotations are also supported, allowing the an-
notation of event structures such as those targeted
in the MUC (Sundheim, 1996), ACE (Doddington
et al 2004), and BioNLP (Kim et al 2011) In-
formation Extraction (IE) tasks (Figure 2). Addi-
tional aspects of annotations can be marked using
attributes, binary or multi-valued flags that can
be added to other annotations. Finally, annotators
can attach free-form text notes to any annotation.
In addition to information extraction tasks,
these annotation primitives allow BRAT to be
configured for use in various other tasks, such
as chunking (Abney, 1991), Semantic Role La-
beling (Gildea and Jurafsky, 2002; Carreras
and Ma`rquez, 2005), and dependency annotation
(Nivre, 2003) (See Figure 1 for examples). Fur-
ther, both the BRAT client and server implement
full support for the Unicode standard, which al-
low the tool to support the annotation of text us-
ing e.g. Chinese or Devana?gar?? characters. BRAT
is distributed with examples from over 20 cor-
pora for a variety of tasks, involving texts in seven
different languages and including examples from
corpora such as those introduced for the CoNLL
shared tasks on language-independent named en-
tity recognition (Tjong Kim Sang and De Meul-
der, 2003) and multilingual dependency parsing
(Buchholz and Marsi, 2006).
BRAT also implements a fully configurable sys-
tem for checking detailed constraints on anno-
tation semantics, for example specifying that a
TRANSFER event must take exactly one of each
of GIVER, RECIPIENT and BENEFICIARY argu-
ments, each of which must have one of the types
PERSON, ORGANIZATION or GEO-POLITICAL
ENTITY, as well as a MONEY argument of type
103
Figure 3: Incomplete TRANSFER event indicated
to the annotator
MONEY, and may optionally take a PLACE argu-
ment of type LOCATION (LDC, 2005). Constraint
checking is fully integrated into the annotation in-
terface and feedback is immediate, with clear vi-
sual effects marking incomplete or erroneous an-
notations (Figure 3).
2.4 NLP Technology Integration
BRAT supports two standard approaches for inte-
grating the results of fully automatic annotation
tools into an annotation workflow: bulk anno-
tation imports can be performed by format con-
version tools distributed with BRAT for many
standard formats (such as in-line and column-
formatted BIO), and tools that provide standard
web service interfaces can be configured to be in-
voked from the user interface.
However, human judgements cannot be re-
placed or based on a completely automatic analy-
sis without some risk of introducing bias and re-
ducing annotation quality. To address this issue,
we have been studying ways to augment the an-
notation process with input from statistical and
machine learning methods to support the annota-
tion process while still involving human annotator
judgement for each annotation.
As a specific realisation based on this approach,
we have integrated a recently introduced ma-
chine learning-based semantic class disambigua-
tion system capable of offering multiple outputs
with probability estimates that was shown to be
able to reduce ambiguity on average by over 75%
while retaining the correct class in on average
99% of cases over six corpora (Stenetorp et al
2011a). Section 4 presents an evaluation of the
contribution of this component to annotator pro-
ductivity.
2.5 Corpus Search Functionality
BRAT implements a comprehensive set of search
functions, allowing users to search document col-
Figure 4: The BRAT search dialog
lections for text span annotations, relations, event
structures, or simply text, with a rich set of search
options definable using a simple point-and-click
interface (Figure 4). Additionally, search results
can optionally be displayed using keyword-in-
context concordancing and sorted for browsing
using any aspect of the matched annotation (e.g.
type, text, or context).
3 Implementation
BRAT is implemented using a client-server ar-
chitecture with communication over HTTP using
JavaScript Object Notation (JSON). The server is
a RESTful web service (Fielding, 2000) and the
tool can easily be extended or adapted to switch
out the server or client. The client user interface is
implemented using XHTML and Scalable Vector
Graphics (SVG), with interactivity implemented
using JavaScript with the jQuery library. The
client communicates with the server using Asyn-
chronous JavaScript and XML (AJAX), which
permits asynchronous messaging.
BRAT uses a stateless server back-end imple-
mented in Python and supports both the Common
Gateway Interface (CGI) and FastCGI protocols,
the latter allowing response times far below the
100 ms boundary for a ?smooth? user experience
without noticeable delay (Card et al 1983). For
server side annotation storage BRAT uses an easy-
to-process file-based stand-off format that can be
converted from or into other formats; there is no
need to perform database import or export to in-
terface with the data storage. The BRAT server in-
104
Figure 5: Example annotation from the BioNLP Shared Task 2011 Epigenetics and Post-translational
Modifications event extraction task.
stallation requires only a CGI-capable web server
and the set-up supports any number of annotators
who access the server using their browsers, on any
operating system, without separate installation.
Client-server communication is managed so
that all user edit operations are immediately sent
to the server, which consolidates them with the
stored data. There is no separate ?save? operation
and thus a minimal risk of data loss, and as the
authoritative version of all annotations is always
maintained by the server, there is no chance of
conflicting annotations being made which would
need to be merged to produce an authoritative ver-
sion. The BRAT client-server architecture also
makes real-time collaboration possible: multiple
annotators can work on a single document simul-
taneously, seeing each others edits as they appear
in a document.
4 Case Studies
4.1 Annotation Projects
BRAT has been used throughout its development
during 2011 in the annotation of six different cor-
pora by four research groups in efforts that have
in total involved the creation of well-over 50,000
annotations in thousands of documents compris-
ing hundreds of thousands of words.
These projects include structured event an-
notation for the domain of cancer biology,
Japanese verb frame annotation, and gene-
mutation-phenotype relation annotation. One
prominent effort making use of BRAT is the
BioNLP Shared Task 2011,1 in which the tool was
used in the annotation of the EPI and ID main
task corpora (Pyysalo et al 2012). These two
information extraction tasks involved the annota-
tion of entities, relations and events in the epige-
netics and infectious diseases subdomains of biol-
ogy. Figure 5 shows an illustration of shared task
annotations.
Many other annotation efforts using BRAT are
still ongoing. We refer the reader to the BRAT
1http://2011.bionlp-st.org
Mode Total Type Selection
Normal 45:28 13:49
Rapid 39:24 (-6:04) 09:35 (-4:14)
Table 1: Total annotation time, portion spent se-
lecting annotation type, and absolute improve-
ment for rapid mode.
website2 for further details on current and past an-
notation projects using BRAT.
4.2 Automatic Annotation Support
To estimate the contribution of the semantic class
disambiguation component to annotation produc-
tivity, we performed a small-scale experiment in-
volving an entity and process mention tagging
task. The annotation targets were of 54 dis-
tinct mention types (19 physical entity and 35
event/process types) marked using the simple
typed-span representation. To reduce confound-
ing effects from annotator productivity differ-
ences and learning during the task, annotation was
performed by a single experienced annotator with
a Ph.D. in biology in a closely related area who
was previously familiar with the annotation task.
The experiment was performed on publication
abstracts from the biomolecular science subdo-
main of glucose metabolism in cancer. The texts
were drawn from a pool of 1,750 initial candi-
dates using stratified sampling to select pairs of
10-document sets with similar overall statistical
properties.3 Four pairs of 10 documents (80 in to-
tal) were annotated in the experiment, with 10 in
each pair annotated with automatic support and 10
without, in alternating sequence to prevent learn-
ing effects from favouring either approach.
The results of this experiment are summarized
in Table 1 and Figure 6. In total 1,546 annotations
were created in normal mode and 1,541 annota-
2http://brat.nlplab.org
3Document word count and expected annotation count,
were estimated from the output of NERsuite, a freely avail-
able CRF-based NER tagger: http://nersuite.nlplab.org
105
0500
1000
1500
2000
2500
3000
Normal Mode Rapid Mode
Tim
e(
se
co
nd
s)
Figure 6: Allocation of annotation time. GREEN
signifies time spent on selecting annotation type
and BLUE the remaining annotation time.
tions in rapid mode; the sets are thus highly com-
parable. We observe a 15.4% reduction in total
annotation time, and, as expected, this is almost
exclusively due to a reduction in the time the an-
notator spent selecting the type to assign to each
span, which is reduced by 30.7%; annotation time
is otherwise stable across the annotation modes
(Figure 6). The reduction in the time spent in se-
lecting the span is explained by the limiting of the
number of candidate types exposed to the annota-
tor, which were decreased from the original 54 to
an average of 2.88 by the semantic class disam-
biguation component (Stenetorp et al 2011a).
Although further research is needed to establish
the benefits of this approach in various annotation
tasks, we view the results of this initial experi-
ment as promising regarding the potential of our
approach to using machine learning to support an-
notation efforts.
5 Related Work and Conclusions
We have introduced BRAT, an intuitive and user-
friendly web-based annotation tool that aims to
enhance annotator productivity by closely inte-
grating NLP technology into the annotation pro-
cess. BRAT has been and is being used for several
ongoing annotation efforts at a number of aca-
demic institutions and has so far been used for
the creation of well-over 50,000 annotations. We
presented an experiment demonstrating that inte-
grated machine learning technology can reduce
the time for type selection by over 30% and over-
all annotation time by 15% for a multi-type entity
mention annotation task.
The design and implementation of BRAT was
informed by experience from several annotation
tasks and research efforts spanning more than
a decade. A variety of previously introduced
annotation tools and approaches also served to
guide our design decisions, including the fast an-
notation mode of Knowtator (Ogren, 2006), the
search capabilities of the XConc tool (Kim et al
2008), and the design of web-based systems such
as MyMiner (Salgado et al 2010), and GATE
Teamware (Cunningham et al 2011). Using ma-
chine learning to accelerate annotation by sup-
porting human judgements is well documented in
the literature for tasks such as entity annotation
(Tsuruoka et al 2008) and translation (Mart??nez-
Go?mez et al 2011), efforts which served as in-
spiration for our own approach.
BRAT, along with conversion tools and exten-
sive documentation, is freely available under the
open-source MIT license from its homepage at
http://brat.nlplab.org
Acknowledgements
The authors would like to thank early adopters of
BRAT who have provided us with extensive feed-
back and feature suggestions. This work was sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan), the UK Biotechnology
and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event
Extraction from the Literature for Drug Discov-
ery (reference number: BB/G013160/1), and the
Royal Swedish Academy of Sciences.
106
References
Steven Abney. 1991. Parsing by chunks. Principle-
based parsing, 44:257?278.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Stuart K. Card, Thomas P. Moran, and Allen Newell.
1983. The psychology of human-computer interac-
tion. Lawrence Erlbaum Associates, Hillsdale, New
Jersey.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic Role
Labeling. In Proceedings of the 9th Conference on
Natural Language Learning, pages 152?164. Asso-
ciation for Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, Angus
Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation, pages 837?
840.
Roy Fielding. 2000. REpresentational State Trans-
fer (REST). Architectural Styles and the Design
of Network-based Software Architectures. Univer-
sity of California, Irvine, page 120.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformatics,
9(1):10.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 1?6, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical
report, Linguistic Data Consortium.
Pascual Mart??nez-Go?mez, Germa?n Sanchis-Trilles,
and Francisco Casacuberta. 2011. Online learn-
ing via dynamic reranking for computer assisted
translation. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 93?105. Springer Berlin / Heidelberg.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, pages 149?160.
Philip V. Ogren. 2006. Knowtator: A prote?ge? plug-in
for annotated corpus construction. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Companion Volume:
Demonstrations, pages 273?275, New York City,
USA, June. Association for Computational Linguis-
tics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Junichi Tsujii, and Sophia Ananiadou. 2012.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(suppl.
8):S2.
David Salgado, Martin Krallinger, Marc Depaule,
Elodie Drula, and Ashish V Tendulkar. 2010.
Myminer system description. In Proceedings of the
Third BioCreative Challenge Evaluation Workshop
2010, pages 157?158.
Pontus Stenetorp, Sampo Pyysalo, Sophia Ananiadou,
and Jun?ichi Tsujii. 2011a. Almost total recall: Se-
mantic category disambiguation using large lexical
resources and approximate string matching. In Pro-
ceedings of the Fourth International Symposium on
Languages in Biology and Medicine.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011b. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Beth M. Sundheim. 1996. Overview of results of
the MUC-6 evaluation. In Proceedings of the Sixth
Message Understanding Conference, pages 423?
442. Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2008. Accelerating the annotation of
sparse named entities by dynamic sentence selec-
tion. BMC Bioinformatics, 9(Suppl 11):S8.
107
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111?116,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Using a Random Forest Classifier to Compile Bilingual Dictionaries of
Technical Terms from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We describe a machine learning approach,
a Random Forest (RF) classifier, that is
used to automatically compile bilingual
dictionaries of technical terms from com-
parable corpora. We evaluate the RF clas-
sifier against a popular term alignment
method, namely context vectors, and we
report an improvement of the translation
accuracy. As an application, we use the
automatically extracted dictionary in com-
bination with a trained Statistical Machine
Translation (SMT) system to more accu-
rately translate unknown terms. The dic-
tionary extraction method described in this
paper is freely available
1
.
1 Background
Bilingual dictionaries of technical terms are im-
portant resources for many Natural Language
Processing (NLP) tasks including Statistical Ma-
chine Translation (SMT) (Och and Ney, 2003) and
Cross-Language Information Retrieval (Balles-
teros and Croft, 1997). However, manually cre-
ating and updating such resources is an expensive
process. In addition to this, new terms are con-
stantly emerging. Especially in the biomedical
domain, which is the focus of this work, there is
a vast number of neologisms, i.e., newly coined
terms, (Pustejovsky et al., 2001).
Early work on bilingual lexicon extraction
focused on clean, parallel corpora providing
satisfactory results (Melamed, 1997; Kay and
R?oscheisen, 1993). However, parallel corpora are
expensive to construct and for some domains and
language pairs are scarce resources. For these rea-
sons, the focus has shifted to comparable corpora
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/RF-TermAlign.tar.gz
that are more readily available, more up-to-date,
larger and cheaper to construct than parallel data.
Comparable corpora are collections of monolin-
gual documents in a source and target language
that share the same topic, domain and/or docu-
ments are from the same period, genre and so
forth.
Existing methods for bilingual lexicon extrac-
tion from comparable corpora are mainly based
on the same principle. They hypothesise that a
word and its translation tend to appear in simi-
lar lexical context (Fung and Yee, 1998; Rapp,
1999; Morin et al., 2007; Chiao and Zweigen-
baum, 2002). Context vector methods are reported
to achieve robust performance on terms that occur
frequently in the corpus. Chiao and Zweigenbaum
(2002) achieved a performance of 94% accuracy
on the top 20 candidates when translating high fre-
quency, medical terms (frequency of 100 or more).
In contrast, Morin and Daille (2010) reported an
accuracy of 21% for multi-word terms occurring
20 times or less, noting that translating rare terms
is a challenging problem for context vectors.
Kontonatsios et al. (2013) introduced an RF
classifier that is able to automatically learn as-
sociation rules of textual units between a source
and target language. However, they applied their
method only on artificially constructed datasets
containing an equal number of positive and neg-
ative instances. In the case of comparable cor-
pora, the datasets are highly unbalanced (given n,
m source and target terms respectively, we need to
classify n?m instances). In this work, we incor-
porate the classification margin into the RF model,
to allow the method to cope with the skewed dis-
tribution of positive and negative instances that oc-
curs in comparable corpora.
Our proposed method ranks candidate transla-
tions using the classification margin and suggests
as the best translation the candidate with the max-
imum margin. We evaluate our method on an
111
English-Spanish comparable corpus of Wikipedia
articles that are related to the medical sub-domain
of ?breast cancer?. Furthermore, we show that dic-
tionaries extracted from comparable corpora can
be used to dynamically augment an SMT sys-
tem in order to better translate Out-of-Vocabulary
(OOV) terms.
2 Methodology
A pair of terms in a source and target language is
represented as a feature vector where each dimen-
sion corresponds to a unique character n-gram.
The value of each dimension is 0 or 1 and desig-
nates the occurrence of the corresponding n-gram
in the input terms. The feature vectors that we
use contain 2q dimensions where the first q dimen-
sions correspond to the n-gram features extracted
from the source terms and the last q dimensions to
those from the target terms. In the reported experi-
ments, we use the 600 (300 source and 300 target)
most frequently occurring n-grams.
The underlying mechanism that allows the RF
method to learn character gram mappings between
terms of a source and target language is the de-
cision trees. A node in the decision tree is a
unique character n-gram. The nodes are linked
through the branches of the trees and therefore the
two sub-spaces of q source and q target charac-
ter grams are combined. Each decision tree in the
forest is constructed as follows: every node is split
by considering |?| random n-gram features of the
initial feature set ?, and a decision tree is fully
grown. This process is repeated |? | times and con-
structs |? | decision trees. We tuned the RF clas-
sifier using 140 random trees where we observed
a plateau in the classification performance. Fur-
thermore, we set the number of random features
using |?| = log
2
|?|+ 1 as suggested by Breiman
(2001).
The classification margin that we use to rank
the candidate translations is calculated by simply
subtracting the average number of trees predicting
that the input terms are not translations from the
average number of decision trees predicting that
the terms are mutual translations. A larger classi-
fication margin means that more decision trees in
the forest classify an instance as a translation pair.
For training an RF model, we use a bilingual
dictionary of technical terms. When the dictionary
lists more than one translation for an English term,
we randomly select only one. Negative instances
are created by randomly matching non-translation
pairs of terms. We used an equal number of posi-
tive and negative instances for training the model.
Starting from 20, 000 translation pairs we gener-
ated a training dataset of 40, 000 positive and neg-
ative instances.
2.1 Baseline method
The context projection method was first pro-
posed by (Fung and Yee, 1998; Rapp, 1999) and
since then different variations have been suggested
(Chiao and Zweigenbaum, 2002; Morin et al.,
2007; Andrade et al., 2010; Morin and Prochas-
son, 2011). Our implementation more closely
follows the context vector method introduced by
(Morin and Prochasson, 2011).
As a preprocessing step, stop words are re-
moved using an online list
2
and lemmatisation
is performed using TreeTagger (Schmid, 1994) on
both the English and Spanish part of the compa-
rable corpus. Afterwards, the method proceeds
in three steps. Firstly, for each source and target
term of the comparable corpus, i.e., i, we collect
all lexical units that: (a) occur within a window
of 3 words around i (a seven-word window) and
(b) are listed in the seed bilingual dictionary. The
lexical units that satisfy the above two conditions
are the dimensions of the context vectors. Each
dimension has a value that indicates the correla-
tion between the context lexical unit and the term
i. In our approach, we use the log-likelihood ra-
tio. In the second step, the seed dictionary is used
to translate the lexical units of the Spanish context
vectors. In this way the Spanish and English vec-
tors become comparable. When several transla-
tions are listed in the seed dictionary, we consider
all of them. In the third step, we compute the con-
text similarity, i.e., distance metric, between the
vector of an English term to be translated with ev-
ery projected, Spanish context vector. For this we
use the cosine similarity.
3 Experiments
In this section, we evaluate the two dictionary ex-
traction methods, namely context vectors and RF,
on a comparable corpus of Wikipedia articles.
For the evaluation metric, we use the top-k
translation accuracy
3
and the mean reciprocal
2
http://members.unine.ch/jacques.savoy/clef/index.html
3
the percentage of English terms whose top k candidates
contain a correct translation
112
rank (MRR)
4
as in previous approaches (Chiao
and Zweigenbaum, 2002; Chiao and Zweigen-
baum, 2002; Morin and Prochasson, 2011; Morin
et al., 2007; Tamura et al., 2012). As a refer-
ence list, we use the UMLS metathesaurus
5
. In
addition to this, considering that in several cases
the dictionary extraction methods retrieved syn-
onymous translations that do not appear in the ref-
erence list, we manually inspected the answers.
Finally, unlike previous approaches (Chiao and
Zweigenbaum, 2002), we do not restrict the test
list only to those English terms whose Spanish
translations are known to occur in the target cor-
pus. In such cases, the performance of dictionary
extraction methods have been shown to achieve a
lower performance (Tamura et al., 2012).
3.1 Data
We constructed a comparable corpus of Wikipedia
articles. For this, we used Wikipedia?s search en-
gine
6
and submitted the queries ?breast cancer?
and ?c?ancer de mama? for English and Spanish
respectively. From the returned list of Wikipedia
pages, we used the 1, 000 top articles for both lan-
guages.
The test list contains 1, 200 English single-word
terms that were extracted by considering all nouns
that occur more than 10 but not more than 200
times and are listed in UMLS. For the Spanish part
of the corpus, we considered all nouns as candi-
date translations (32, 347 in total).
3.2 Results
Table 1 shows the top-k translation accuracy and
the MRR of RF and context vectors.
Acc
1
Acc
10
Acc
20
MRR
RF 0.41 0.57 0.59 0.47
Cont.
Vectors 0.1 0.21 0.26 0.11
Table 1: top-k translation accuracy and MRR of
RF and context vectors on 1, 200 English terms
We observe that the proposed RF method
achieves a considerably better top-k translation ac-
4
MRR =
1
|Q|
?
Q
i=1
1
rank
i
where |Q| is the number of
English terms for which we are extracting translations and
rank
i
is the position of the first correct translation from re-
turned list of candidates
5
nlm.nih.gov/research/umls
6
http://en.wikipedia.org/wiki/Help:Searching
curacy and MRR than the baseline method. More-
over, we segmented the 1, 200 test terms into 7
frequency ranges
7
, from high-frequency to rare
terms. Figure 1 shows the translation accuracy at
top 20 candidates for the two methods. We note
Figure 1: Translation accuracy of top 20 candi-
dates on different frequency ranges
that for high frequency terms, i.e. [100,200] range,
the performance achieved by the two methods is
similar (53% and 52% for the RF and context vec-
tors respectively). However, for lower frequency
terms, the translation accuracy of the context vec-
tors continuously declines. This confirms that con-
text vectors do not behave robustly for rare terms
(Morin and Daille, 2010). In contrast, the RF
slightly fluctuates over different frequency ranges
and presents approximately the same translation
accuracy for both frequent and rare terms.
4 Application
As an application of our method, we use the pre-
viously extracted dictionaries to on-line augment
the phrase table of an SMT system and observe
the translation performance on test sentences that
contain OOV terms. For the translation probabil-
ities in the phrase table, we use the distance met-
ric given by the dictionary extraction methods i.e.,
classification margin and cosine similarity of RF
and context vectors respectively, normalised by
the uniform probability (if a source term has m
candidate translations, we normalise the distance
metric by dividing by m as in (Wu et al., 2008) .
4.1 Data and tools
We construct a parallel, sentence-aligned corpus
from the biomedical domain, following the pro-
cess described in (Wu et al., 2011; Yepes et al.,
2013). The parallel corpus comprises of article ti-
tles indexed by PubMed in both English and Span-
ish. We collect 120K parallel sentences for train-
7
each frequency range contains 100 randomly sampled
terms
113
ing the SMT and 1K sentences for evaluation. The
test sentences contain 1, 200 terms that do not ap-
pear in the training parallel corpus. These terms
occur in the Wikipedia comparable corpus. Hence,
the previously extracted dictionaries list a possible
translation. Using the PubMed parallel corpus, we
train Moses (Koehn et al., 2007), a phrase-based
SMT system.
4.2 Results
We evaluated the translation performance of the
SMT that uses the dictionary extracted by the RF
against the following baselines: (i) Moses using
only the training parallel data (Moses), (ii) Moses
using the dictionary extracted by context vectors
(Moses+context vector). The evaluation metric is
BLEU (Papineni et al., 2002).
Table 2 shows the BLEU score achieved by the
SMT systems when we append the top-k transla-
tions to the phrase table.
BLEU
on top-k translations
1 10 20
Moses 24.22 24.22 24.22
Moses+
RF 25.32 24.626 24.42
Moses+
Context Vectors 23.88 23.69 23.74
Table 2: Translation performance when adding
top-k translations to the phrase table
We observe that the best performance is
achieved by the RF when we add the top 1 trans-
lation with a total gain of 1.1 BLEU points over
the baseline system. In contrast, context vec-
tors decreased the translation performance of the
SMT system. This indicates that the dictionary ex-
tracted by the context vectors is too noisy and as
a result the translation performance dropped. Fur-
thermore, it is noted that the augmented SMT sys-
tems achieve the highest performance for the top 1
translation while for k greater than 1, the transla-
tion performance decreases. This behaviour is ex-
pected since the target language model was trained
only on the training Spanish sentences of the par-
allel corpus. Hence, the target language model
does not have a prior knowledge of the OOV trans-
lations and as a result it cannot choose the correct
translation among k candidates.
To further investigate the effect of the language
model on the translation performance of the aug-
mented SMT systems, we conducted an oracle ex-
periment. In this ideal setting, we assume a strong
language model, that is trained on both training
and test Spanish sentences of the parallel corpus,
in order to assign a higher probability to a correct
translation if it exists in the deployed dictionary.
As we observe in Table 3, a strong language model
can more accurately select the correct translation
among top-k candidates. The dictionary extracted
by the RF improved the translation performance
by 2.5 BLEU points for the top-10 candidates and
context vectors by 0.45 for the top-20 candidates.
BLEU
on top-k translations
1 10 20
Moses 28.85 28.85 28.85
Moses+
RF 30.98 31.35 31.2
Moses+
Context Vectors 28.18 29.17 29.3
Table 3: Translation performance when adding
top-k translations to the phrase table. SMT sys-
tems use a language model trained on training and
test Spanish sentences of the parallel corpus.
5 Discussion
In this paper, we presented an RF classifier that
is used to extract bilingual dictionaries of techni-
cal terms from comparable corpora. We evaluated
our method on a comparable corpus of Wikipedia
articles. The experimental results showed that our
proposed method performs robustly when translat-
ing both frequent and rare terms.
As an application, we used the automatically
extracted dictionary to augment the phrase table of
an SMT system. The results demonstrated an im-
provement of the overall translation performance.
As future work, we plan to integrate the RF clas-
sifier with context vectors. Intuitively, the two
methods are complementary considering that the
RF exploits the internal structure of terms while
context vectors use the surrounding lexical con-
text. Therefore, it will be interesting to investigate
how we can incorporate the two feature spaces in
a machine learner.
114
6 Acknowledgements
This work was funded by the European Commu-
nity?s Seventh Framework Program (FP7/2007-
2013) [grant number 318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 19?
27, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
Martin Kay and Martin R?oscheisen. 1993. Text-
translation alignment. computational Linguistics,
19(1):121?142.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Georgios Kontonatsios, Ioannis Korkontzelos, Jun?ichi
Tsujii, and Sophia Ananiadou. 2013. Using ran-
dom forest to recognise translation equivalents of
biomedical terms across languages. In Proceed-
ings of the Sixth Workshop on Building and Using
Comparable Corpora, pages 95?104. Association
for Computational Linguistics, August.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 305?312. Association for
Computational Linguistics.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 993?1000. Association for Computational
Linguistics.
115
Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.
2011. Statistical machine translation for biomedical
text: are we there yet? In AMIA Annual Sympo-
sium Proceedings, volume 2011, page 1290. Ameri-
can Medical Informatics Association.
Antonio Jimeno Yepes,
?
Elise Prieur-Gaston, and
Aur?elie N?ev?eol. 2013. Combining medline and
publisher data to create parallel corpora for the auto-
matic translation of biomedical text. BMC bioinfor-
matics, 14(1):146.
116
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 121?126,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Building trainable taggers in a web-based, UIMA-supported NLP
workbench
Rafal Rak, BalaKrishna Kolluru and Sophia Ananiadou
National Centre for Text Mining
School of Computer Science, University of Manchester
Manchester Interdisciplinary Biocentre
131 Princess St, M1 7DN, Manchester, UK
{rafal.rak,balakrishna.kolluru,sophia.ananiadou}@manchester.ac.uk
Abstract
Argo is a web-based NLP and text mining
workbench with a convenient graphical user
interface for designing and executing process-
ing workflows of various complexity. The
workbench is intended for specialists and non-
technical audiences alike, and provides the
ever expanding library of analytics compliant
with the Unstructured Information Manage-
ment Architecture, a widely adopted interop-
erability framework. We explore the flexibil-
ity of this framework by demonstrating work-
flows involving three processing components
capable of performing self-contained machine
learning-based tagging. The three components
are responsible for the three distinct tasks of 1)
generating observations or features, 2) train-
ing a statistical model based on the generated
features, and 3) tagging unlabelled data with
the model. The learning and tagging compo-
nents are based on an implementation of con-
ditional random fields (CRF); whereas the fea-
ture generation component is an analytic ca-
pable of extending basic token information to
a comprehensive set of features. Users de-
fine the features of their choice directly from
Argo?s graphical interface, without resorting
to programming (a commonly used approach
to feature engineering). The experimental re-
sults performed on two tagging tasks, chunk-
ing and named entity recognition, showed that
a tagger with a generic set of features built
in Argo is capable of competing with task-
specific solutions.
1 Introduction
The applications of automatic recognition of cate-
gories, or tagging, in natural language processing
(NLP), range from part of speech tagging to chunk-
ing to named entity recognition and complex scien-
tific discourse analyses. Currently, there is a variety
of tools capable of performing these tasks. A com-
monly used approach involves the use of machine
learning to first build a statistical model based on a
manually or semi-automatically tagged sample data
and then to tag new data using this model. Since
the machine learning algorithms for building mod-
els are well established, the challenge shifted to fea-
ture engineering, i.e., developing task-specific fea-
tures that form the basis of these statistical models.
This task is usually accomplished programmatically
which pose an obstacle to a non-technically inclined
audience. We alleviate this problem by demonstrat-
ing Argo1, a web-based platform that allows the user
to build NLP and other text analysis workflows via
a graphical user interface (GUI) available in a web
browser. The system is equipped with an ever grow-
ing library of text processing components ranging
from low-level syntactic analysers to semantic an-
notators. It also allows for including user-interactive
components, such as an annotation editor, into oth-
erwise fully automatic workflows. The interoper-
ability of processing components is ensured in Argo
by adopting Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004) as
the system?s framework. In this work we explore the
capabilities of this framework to support machine
1http://nactem.ac.uk/Argo
121
learning components for tagging textual content.
In the following section we present related work.
Section 3 provides background information on Argo
and its relationship to UIMA. The details of the three
machine learning components are discussed in Sec-
tion 4. Section 5 provides evaluation, whereas Sec-
tion 6 concludes the paper.
2 Related work
Language processing tools with machine learning
capabilities for tagging textual content have been
distributed by various groups in form of either stan-
dalone applications or application programming in-
terfaces (API). Packages such as Lingpipe2, Mal-
let3, Stanford NLP tools4 and OpenNLP5 have been
extensively used by the NLP and text mining com-
munities (Kolluru et al, 2011; Corbett and Murray-
Rust, 2006). However, such tools inherently impose
inconveniences on users, such as a lack of GUI, of-
ten arduous manual installation procedures, profi-
ciency in programming or familiarity with the de-
tails of machine learning algorithms.
These limitations are overcome by GUI-equipped,
workflow-supporting platforms that often directly
use the solutions provided by the former tools. The
notable examples of such platforms designed specif-
ically for NLP and text mining tasks are GATE
(Cunningham et al, 2002), a suite of text process-
ing and annotation tools, and U-Compare (Kano et
al., 2010), a standalone application supporting the
UIMA framework that formed the inspiration for
Argo.
Although the GUI platforms provide machine
learning solutions, these are usually limited to us-
ing pre-trained models and providing a rich set of
features for training requires resorting to program-
ming. Argo, on the other hand, allows the users to
train their own models with either a generic set of
features or customisable features without having to
write a single line of code. This capability is pro-
vided in Argo entirely through its GUI.
2http://alias-i.com/lingpipe
3http://mallet.cs.umass.edu
4http://nlp.stanford.edu/software/index.shtml
5http://opennlp.apache.org
Figure 1: Screen capture of Argo?s web-based inter-
face.
3 Argo and UIMA
Argo?s main user interface consists of three panels
as shown in Figure 1. The left-hand panel includes
user-owned or shared storable objects; the middle
panel is a drawing space for constructing workflows
and the right-hand panel displays context-dependent
information. The storable objects are categorised
into workflows, represented as block diagrams of
interconnected processing components, documents
that represent the user?s space intended for upload-
ing resources and saving processing results, and ex-
ecutions that provide past and live workflow exe-
cution details and access points to user-interactive
components should such be present in a workflow.
Component interoperability in Argo is ensured by
UIMA which defines common structures and inter-
faces. A typical UIMA processing pipeline consists
of a collection reader, a set of analysis engines and a
consumer. The role of a collection reader is to fetch
a resource (e.g., a text document) and deposit it in
a common annotation structure, or CAS, as the sub-
ject of annotation. Analysis engines then process the
subject of annotation stored in the CAS and populate
the CAS with their respective annotations. The con-
sumer?s role is to transform some or all of the an-
notations and/or the subject of annotation from the
CAS and serialise it into some storable format.
Readers, analysers and consumers are represented
graphically in Argo as blocks with incoming only,
incoming and outgoing, and outgoing only ports, re-
spectively, visible in the middle of Figure 1.
122
(a) Training (b) Tagging
Figure 2: Two generic workflows demonstrating
the use of the Feature Generator component for (a)
training and (b) tagging.
4 Machine learning components in Argo
In order to ensure flexibility in building workflows,
we split the machine learning capability into three
distinct processing components, namely feature gen-
erator, model trainer and tagger. The trainer and
the tagger are intrinsic machine learning compo-
nents, whereas the feature generator is a convenient
and customisable processing component capable of
building a feature space for a user-defined domain.
From UIMA?s perspective, the feature generator
and the tagger are both analysis engines whose pur-
pose is to analyse the incoming CASes and en-
rich them with additional annotations; whereas the
trainer is a consumer that transforms the information
stored in CASes into a statistical model.
A typical use of the three components is shown
in Figure 2. The three components are repre-
sented as the Feature Generator, CRF++ Trainer and
CRF++ Tagger blocks. Figure 2a shows a pro-
cess of building a statistical model supported by
a document reader, common, well-established pre-
processing components (in this case, to establish
boundaries of sentences and tokens), and the previ-
ously mentioned editor for manually creating anno-
tations6. The manual annotations serve to generate
tags/labels which are used in the training process to-
gether with the features produced by Feature Gener-
ator. The trained model is then used in the workflow
shown in Figure 2b to tag new resources. Although
the tagging workflow automatically recognises the
labels of interest (based on the model supplied in
CRF++ Tagger), in practice, the labels need further
correction, hence the use of Annotation Editor after
the tagger.
4.1 Training and tagging
At present, our implementation of the training and
tagging components is based on the conditional ran-
dom fields (CRF) (Lafferty et al, 2001). Our choice
is dictated by the fact that CRF models are currently
one of the best models for tagging and efficient algo-
rithms to compute marginal probabilities and n-best
sequences are freely available.
We used the CRF++ implementation7 and
wrapped it into two UIMA-compatible components,
CRF++ Trainer and CRF++ Tagger. The trainer
deals with the optimisation of feature parameters,
whereas word observations are produced by Feature
Generator, as described in the following section.
4.2 From annotations to features
The Feature Generator component is an intermedi-
ary between annotations stored in CASes and the
training component. This component is customis-
able via the component?s settings panel, parts of
which are shown in Figure 3. The panel allows the
user to 1) identify the stream of tokens8 (Figure 3a),
2) identify the stream of token sequences (usually
6The preprocessing and manual annotation components
could be replaced with CAS Reader, a component capable of
supplying the workflow with a previously annotated set of doc-
uments.
7http://code.google.com/p/crfpp/
8The definition of token depends on the selected UIMA an-
notation type. It may range from a simple span of text to a
complex lexical or semantic structure.
123
(a) Selecting a token annotation type
(b) Defining features
Figure 3: Feature Generator settings panel allows
the user to (a) select labels for machine learning and
(b) define features.
sentences), and 3) define features or token observa-
tions (Figure 3b).
Each feature definition consists of a name, a token
field, an optional list of token field transformations,
and an optional set of context windows. The name
is only for the user?s convenience of identifying in-
dividual feature definitions. The token field is the
primary subject of transformations (if any) and it is
one of the data fields of the selected token annota-
tion type. For instance, the token annotation type
may define data fields such as part of speech, chunk,
or lemma. By default, the system selects ?covered
text?, i.e., the span of text covered by an annotation,
since this data field is available for any annotation.
If no transformation is declared, the string rep-
Figure 4: UML diagram of transformation types
resentation of the token field?s value ultimately be-
comes the value of the generated feature. If the
user declares one or more transformations then these
are applied on the token field?s value in sequence,
i.e., an outcome of the preceding transformation be-
comes an input of the following one. Figure 4 shows
the various transformations currently available in the
system.
Context windows allow for enriching the current
token?s feature set by introducing observations from
surrounding tokens as n-grams. For example, the
selected feature definition in Figure 3b, ?surface has
symbols?, declares the covered text as the feature?s
basis and defines two transformations and two con-
text windows. The two transformations will first
transform the covered text to a collapsed shape (e.g.,
?NF-kappa? will become ?A#a?) and then produce
?Y? or ?N? depending on whether the collapsed
shape matches the simple regular expression ?#?
(e.g., ?A#a? will become ?Y?). The two context win-
dows define six unigrams and four bigrams, which
will ultimately result in this single feature defini-
tion?s producing ten observations for training.
5 Evaluation
We show the performance of taggers trained with
two distinct sets of features, basic and extended.
The basic set of features uses token fields such as
the covered text and the part of speech without any
transformations or context n-grams. The extended
set makes the full use of Feature Generator?s settings
and enriches the basic set with various transforma-
tions and context n-grams. The transformations in-
124
Dataset Setup P R F
CoNLL Best 94.29 94.01 94.13
L2 IOBES 92.20 93.43 92.81
L2 IOB 92.14 93.27 92.70
L1 IOBES 91.95 93.17 92.55
L1 IOB 91.83 93.11 92.46
Baseline 72.58 82.14 77.07
BioNLP/ Best 76.00 69.40 72.6
NLPBA L1 IOBES 66.22 65.06 65.63
L2 IOB 66.06 64.87 65.46
L1 IOB 66.05 64.61 65.32
L2 IOBES 65.77 64.79 65.28
Baseline 52.60 43.60 47.70
Table 1: Performance of various setups (L1 vs L2,
and IOB vs IOBES) on the chunking and NER tasks.
The setups are ordered by F-score.
Dataset Setup P R F
CoNLL Basic 73.80 84.50 78.78
Extended 92.20 93.43 92.81
BioNLP/ Basic 37.06 48.13 41.88
NLPBA Extended 66.22 65.06 65.63
Table 2: Comparison of setups with basic and ex-
tended features for the chunking and NER tasks.
clude surface shape, length, prefixes, suffixes, and
the presence of various combinations of letters, dig-
its and symbols. The context n-grams include uni-
grams for all feature definitions and bigrams for se-
lected ones. Figure 3b shows a sample of the actual
extended set.
We use two datasets, one prepared for the CoNLL
2000 shared task (Tjong et al, 2000) and another
prepared for the BioNLP/NLPBA 2004 shared task
(Kim et al, 2004). They represent two different
tagging tasks, chunking and named entity recog-
nition, respectively. The CoNLL 2000 chunking
dataset involves 10 labels and comes pre-tokenised
with 211,727 tokens in the training set and 47,377
tokens in the test set. The dataset alo provides part-
of-speech tags for each token. The BioNLP/NLPBA
2004 named entity recognition dataset involves five
biology-related labels and consists of 472,006 and
96,780 tokens in the training and testing sets, re-
spectively. Contrary to the former dataset, there is
no other information supporting the tokens in the
BioNLP/NLPBA dataset. To compensate for it we
automatically generated part of speech and chunk la-
bels for each token.
The chosen datasets/tasks are by no means an
exhaustive set of representative comparative-setup
datasets available. Our goal is not to claim the su-
periority of our approach over the solutions reported
in the respective shared tasks. Instead, we aim to
show that our generic setup is comparable to those
task-tuned solutions.
We further explore the options of both Feature
Generator and CRF++ Trainer by manipulating la-
belling formats (IOB vs IOBES (Kudo and Mat-
sumoto, 2001)) for the former and parameter esti-
mation algorithms (L2- vs L1-norm regularisation)
for the latter. Ultimately, there are 32 setups as the
result of the combinations of the two feature sets, the
two datasets, the two labelling formats and the two
estimation algorithms.
5.1 Results
Table 1 shows the precision, recall and f-scores of
our extended-feature setups against each other as
well as with reference to the best and baseline solu-
tions as reported in the respective shared tasks. The
gap to the best performing solution for the chunking
task is about 1.3% points in F-score, ahead of the
baseline by 15.7% points. Respectively for the NER
task, our best setup stands behind the best reported
solution by about 7% points, ahead of the baseline
by about 18% points. In both instances our solution
would be placed in the middle of the reported rank-
ings, which is a promising result, especially that our
setups are based solely on the tokens? surface form,
part of speech, and (in the case of the NER task)
chunk. In contrast, the best solutions for the NER
task involve the use of dictionaries and advanced
analyses such as acronym resolution.
The tested combinations of the labelling formats
and parameter estimation algorithms showed to be
inconclusive, with a difference between the best and
worst setups of only 0.35% points for both tasks.
The advantage of using the extended set of fea-
tures over the basic set is clearly illustrated in Table
2. The performance of the basic set on the chunking
dataset is only at the level of the baseline, whereas
for the NER task it falls nearly 6% points behind the
125
Dataset Setup L2 L1
CoNLL Extended IOB 555 187
Basic IOB 134 70
Extended IOBES 528 209
Basic IOBES 139 72
BioNLP/ Extended IOB 865 179
NLPBA Basic IOB 226 72
Extended IOBES 860 201
Basic IOBES 217 79
Table 3: Number of iterations needed for the optimi-
sation algorithm to converge.
baseline (which comes as no surprise given that the
baseline system is a string match of entities found in
the training set).
Table 3 shows the number of iterations9 needed
for the optimisation algorithm of the trainer to con-
verge. The advantage of the L1 regularisation is
apparent with nearly two to five times less itera-
tions needed when compared to the L2 regularisa-
tion. Given the close F-scores achieved by the two
family of setups, the L1 regularisation becomes a
clear winner in our experimentation setup.
6 Conclusions
Argo?s strength is manifested by its online avail-
ability, an intuitive graphical user interface available
from a web browser, convenience in building even
most complex text processing workflows, and the
availability of trainable machine learning compo-
nents. The Feature Generator component, customis-
able entirely through a GUI, provides the flexibility
needed to extend the basic set of features without
resorting to programming. The experiment results
showed that an extended, yet generic, set of features
can be taken to competitive levels in terms of effec-
tiveness.
7 Acknowledgements
This work was partially supported by Biotechnol-
ogy and Biological Sciences Research Council (BB-
9We do not report detailed CPU times due to experimenting
on resource-shared machines. Such a setup makes direct side-
by-side comparisons largely skewed. As a reference we note
that the workflows completed in 15 minutes to about 11 hours
depending on a feature space size and machine load.
SRC BB/G53025X/1 From Text to Pathways) and
Korea Institute of Science and Technology Informa-
tion (KISTI Text Mining and Pathways).
References
P. Corbett and P. Murray-Rust. 2006. High-throughput
identification of chemistry in life science texts. Comp
Life, pages 107?118. LNBI 4216.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proc. of the 40th Anniversary Meeting
of the Association for Computational Linguistics.
D. Ferrucci and A. Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Process-
ing in the Corporate Research Environment. Natural
Language Engineering, 10(3-4):327?348.
Y. Kano, R. Dorado, L. McCrochon, S. Ananiadou, and
J. Tsujii. 2010. U-Compare: An integrated language
resource evaluation platform including a comprehen-
sive UIMA resource library. In Proc. of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), pages 428?434.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at jnlpba. In Proc. of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications, JNLPBA ?04, pages
70?75, Geneva, Switzerland. Association for Compu-
tational Linguistics.
B. Kolluru, S. Nakjang, R. P. Hirt, A. Wipat, and S. Ana-
niadou. 2011. Automatic extraction of microorgan-
isms and their habitats from free text using text min-
ing workflows. Journal of Integrative Bioinformatics,
8(2):184.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proc. of the second meeting
of the North American Chapter of the Association for
Computational Linguistics on Language technologies,
NAACL ?01, pages 1?8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
J. Lafferty, A. Mccallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. 18th
International Conf. on Machine Learning, pages 282?
289. Morgan Kaufmann, San Francisco, CA.
K. S. Tjong, F. Erik, and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: chunking. In
Proc. of the 2nd workshop on Learning language in
logic and the 4th Conference on Computational nat-
ural language learning, pages 127?132, Morristown,
NJ, USA. Association for Computational Linguistics.
126
Proceedings of the ACL Student Research Workshop, pages 38?45,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
What causes a causal relation?
Detecting Causal Triggers in Biomedical Scientific Discourse
Claudiu Miha?ila? and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science,
The University of Manchester,
131 Princess Street, Manchester M1 7DN, UK
claudiu.mihaila@manchester.ac.uk
sophia.ananiadou@manchester.ac.uk
Abstract
Current domain-specific information extrac-
tion systems represent an important resource
for biomedical researchers, who need to pro-
cess vaster amounts of knowledge in short
times. Automatic discourse causality recog-
nition can further improve their workload by
suggesting possible causal connections and
aiding in the curation of pathway models. We
here describe an approach to the automatic
identification of discourse causality triggers in
the biomedical domain using machine learn-
ing. We create several baselines and experi-
ment with various parameter settings for three
algorithms, i.e., Conditional Random Fields
(CRF), Support Vector Machines (SVM) and
Random Forests (RF). Also, we evaluate the
impact of lexical, syntactic and semantic fea-
tures on each of the algorithms and look at er-
rors. The best performance of 79.35% F-score
is achieved by CRFs when using all three fea-
ture types.
1 Introduction
The need to provide automated, efficient and accu-
rate means of retrieving and extracting user-oriented
biomedical knowledge has significantly increased
according to the ever-increasing amount of knowl-
edge pusblished daily in the form of research ar-
ticles (Ananiadou and McNaught, 2006; Cohen
and Hunter, 2008). Biomedical text mining has
seen significant recent advancements in recent years
(Zweigenbaum et al, 2007), including named en-
tity recognition (Fukuda et al, 1998), coreference
resolution (Batista-Navarro and Ananiadou, 2011;
Savova et al, 2011) and relation (Miwa et al, 2009;
Pyysalo et al, 2009) and event extraction (Miwa
et al, 2012b; Miwa et al, 2012a). Using biomed-
ical text mining technology, text can now be en-
riched via the addition of semantic metadata and
thus can support tasks such as analysing molecu-
lar pathways (Rzhetsky et al, 2004) and semantic
searching (Miyao et al, 2006).
However, more complex tasks, such as question
answering and automatic summarisation, require the
extraction of information that spans across several
sentences, together with the recognition of relations
that exist across sentence boundaries, in order to
achieve high levels of performance.
The notion of discourse can be defined as a co-
herent sequence of clauses and sentences. These
are connected in a logical manner by discourse re-
lations, such as causal, temporal and conditional,
which characterise how facts in text are related. In
turn, these help readers infer deeper, more com-
plex knowledge about the facts mentioned in the
discourse. These relations can be either explicit
or implicit, depending whether or not they are ex-
pressed in text using overt discourse connectives
(also known as triggers). Take, for instance, the case
in example (1), where the trigger Therefore signals
a justification between the two sentences: because
?a normal response to mild acid pH from PmrB re-
quires both a periplasmic histidine and several glu-
tamic acid residues?, the authors believe that the
?regulation of PmrB activity could involve protona-
tion of some amino acids?.
(1) In the case of PmrB, a normal response to mild
acid pH requires not only a periplasmic histidine
38
but also several glutamic acid residues.
Therefore, regulation of PmrB activity may in-
volve protonation of one or more of these amino
acids.
Thus, by identifying this causal relation, search
engines become able to discover relations between
biomedical entities and events or between experi-
mental evidence and associated conclusions. How-
ever, phrases acting as causal triggers in certain con-
texts may not denote causality in all cases. There-
fore, a dictionary-based approach is likely to pro-
duce a very high number of false positives. In
this paper, we explore several supervised machine-
learning approaches to the automatic identification
of triggers that actually denote causality.
2 Related Work
A large amount of work related to discourse pars-
ing and discourse relation identification exists in the
general domain, where researchers have not only
identified discourse connectives, but also developed
end-to-end discourse parsers (Pitler and Nenkova,
2009; Lin et al, 2012). Most work is based on
the Penn Discourse Treebank (PDTB) (Prasad et al,
2008), a corpus of lexically-grounded annotations of
discourse relations.
Until now, comparatively little work has been car-
ried out on causal discourse relations in the biomed-
ical domain, although causal associations between
biological entities, events and processes are central
to most claims of interest (Kleinberg and Hripcsak,
2011). The equivalent of the PDTB for the biomed-
ical domain is the BioDRB corpus (Prasad et al,
2011), containing 16 types of discourse relations,
e.g., temporal, causal and conditional. The number
of purely causal relations annotated in this corpus is
542. There are another 23 relations which are a mix-
ture between causality and one of either background,
temporal, conjunction or reinforcement relations. A
slightly larger corpus is the BioCause (Miha?ila? et
al., 2013), containing over 850 manually annotated
causal discourse relations in 19 full-text open-access
journal articles from the infectious diseases domain.
Using the BioDRB corpus as data, some re-
searchers explored the identification of discourse
connectives (Ramesh et al, 2012). However, they
do not distinguish between the types of discourse
relations. They obtain the best F-score of 75.7% us-
ing CRF, with SVM reaching only 65.7%. These
results were obtained by using only syntactic fea-
tures, as sematic features were shown to lower the
performance. Also, they prove that there exist dif-
ferences in discourse triggers between the biomedi-
cal and general domains by training a model on the
BioDRB and evaluating it against PDTB and vice-
versa.
3 Methodology
In this section, we describe our data and the features
of causal triggers. We also explain our evaluation
methodology.
3.1 Data
The data for the experiments comes from the Bio-
Cause corpus. BioCause is a collection of 19 open-
access full-text journal articles pertaining to the
biomedical subdomain of infectious diseases, manu-
ally annotated with causal relationships. Two types
of spans of text are marked in the text, namely causal
triggers and causal arguments. Each causal relation
is composed of three text-bound annotations: a trig-
ger, a cause or evidence argument and an effect argu-
ment. Some causal relations have implicit triggers,
so these are excluded from the current research.
Figure 1 shows an example of discourse causality
from BioCause, marking the causal trigger and the
two arguments with their respective relation. Named
entities are also marked in this example.
BioCause contains 381 unique explicit triggers in
the corpus, each being used, on average, only 2.10
times. The number decreases to 347 unique triggers
when they are lemmatised, corresponding to an av-
erage usage of 2.30 times per trigger. Both count
settings show the diversity of causality-triggering
phrases that are used in the biomedical domain.
3.2 Features
Three types of features have been employed in the
development of this causality trigger model, i.e., lex-
ical, syntactic and semantic. These features are cat-
egorised and described below.
3.2.1 Lexical features
The lexical features are built from the actual to-
kens present in text. Tokenisation is performed by
39
Figure 1: Causal relation in the BioCause.
the GENIA tagger (Tsuruoka et al, 2005) using the
biomedical model. The first two features represent
the token?s surface expression and its base form.
Neighbouring tokens have also been considered.
We included the token immediately to the left and
the one immediately to the right of the current to-
ken. This decision is based on two observations.
Firstly, in the case of tokens to the left, most trig-
gers are found either at the beginning of the sentence
(311 instances) or are preceded by a comma (238 in-
stances). These two left contexts represent 69% of
all triggers. Secondly, for the tokens to the right, al-
most 45% of triggers are followed by a determiner,
such as the, a or an, (281 instances) or a comma (71
instances).
3.2.2 Syntactic features
The syntax, dependency and predicate argument
structure are produced by the Enju parser (Miyao
and Tsujii, 2008). Figure 2 depicts a partial lexical
parse tree of a sentence which starts with a causal
trigger, namely Our results suggest that. From the
lexical parse trees, several types of features have
been generated.
The first two features represent the part-of-speech
and syntactic category of a token. For instance,
the figure shows that the token that has the part-of-
speech IN. These features are included due to the
fact that either many triggers are lexicalised as an
adverb or conjunction, or are part of a verb phrase.
For the same reason, the syntactical category path
from the root of the lexical parse tree to the token is
also included. The path also encodes, for each par-
ent constituent, the position of the token in its sub-
tree, i.e., beginning (B), inside (I) or end (E); if the
token is the only leaf node of the constituent, this is
marked differently, using a C. Thus, the path of that,
highlighted in the figure, is I-S/I-VP/B-CP/C-CX.
Secondly, for each token, we extracted the pred-
Figure 2: Partial lexical parse tree of a sentence starting
with a causal trigger.
icate argument structure and checked whether a re-
lation exista between the token and the previous and
following tokens. The values for this feature repre-
sent the argument number as allocated by Enju.
Thirdly, the ancestors of each token to the third
degree are instantiated as three different features. In
the case that such ancestors do not exist (i.e., the
root of the lexical parse tree is less than three nodes
away), a ?none? value is given. For instance, the
token that in Figure 2 has as its first three ancestors
the constituents marked with CX, CP and VP.
Finally, the lowest common ancestor in the lexi-
cal parse tree between the current token and its left
neighbour has been included. In the example, the
lowest common ancestor for that and suggest is VP.
These last two feature types have been produced
on the observation that the lowest common ancestor
for all tokens in a causal trigger is S or VP in over
70% of instances. Furthermore, the percentage of
cases of triggers with V or ADV as lowest common
ancestor is almost 9% in each case. Also, the aver-
40
age distance to the lowest common ancestor is 3.
3.2.3 Semantic features
We have exploited several semantic knowledge
sources to identify causal triggers more accurately,
as a mapping to concepts and named entities acts as
a back-off smoothing, thus increasing performance.
One semantic knowledge source is the BioCause
corpus itself. All documents annotated for causal-
ity in BioCause had been previously manually an-
notated with biomedical named entity and event in-
formation. This was performed in the context of var-
ious shared tasks, such as the BioNLP 2011 Shared
Task on Infectious Diseases (Pyysalo et al, 2011).
We therefore leverage this existing information to
add another semantic layer to the model. More-
over, another advantage of having a gold standard
annotation is the fact that it is now possible to sepa-
rate the task of automatic causal trigger recognition
from automatic named entity recognition and event
extraction. The named entity and event annotation
in the BioCause corpus is used to extract informa-
tion about whether a token is part of a named entity
or event trigger. Furthermore, the type of the named
entity or event is included as a separate feature.
The second semantic knowledge source is Word-
Net (Fellbaum, 1998). Using this resource, the hy-
pernym of every token in the text has been included
as a feature. Only the first sense of every token has
been considered, as no sense disambiguation tech-
nique has been employed.
Finally, tokens have been linked to the Unified
Medical Language System (UMLS) (Bodenreider,
2004) semantic types. Thus, we included a feature
to say whether a token is part of a UMLS type and
another for its semantic type if the previous is true.
3.3 Experimental setup
We explored with various machine learning algo-
rithms and various settings for the task of identifying
causal triggers.
On the one hand, we experimented with CRF
(Lafferty et al, 2001), a probabilistic modelling
framework commonly used for sequence labelling
tasks. In this work, we employed the CRFSuite im-
plementation1.
1http://www.chokkan.org/software/
crfsuite
On the other hand, we modelled trigger detection
as a classification task, using Support Vector Ma-
chines and Random Forests. More specifically, we
employed the implementation in Weka (Hall et al,
2009; Witten and Frank, 2005) for RFs, and Lib-
SVM (Chang and Lin, 2011) for SVMs.
4 Results and discussion
Several models have been developed and 10-fold
cross-evaluated to examine the complexity of the
task, the impact of various feature types (lexical,
syntactic, semantic). Table 1 shows the performance
evaluation of baseline systems and other classifiers.
These are described in the following subsections. It
should be noted that the dataset is highly skewed,
with a ratio of positive examples to negative exam-
ples of approximately 1:52.
Classifier P R F1
Ba
sel
ine Dict 8.36 100 15.43Dep 7.51 76.66 13.68
Dict+Dep 14.30 75.33 24.03
2-w
ay CRF 89.29 73.53 79.35SVM 81.62 61.05 69.85
RandFor 78.16 66.96 72.13
3-w
ay CRF 89.13 64.04 72.87SVM 74.21 56.82 64.36
RandFor 73.80 60.95 66.76
Table 1: Performance of various classifiers in identifying
causal connectives
4.1 Baseline
Several baselines have been devised. The first base-
line is a dictionary-based heuristic, named Dict. A
lexicon is populated with all annotated causal trig-
gers and then this is used to tag all instances of its
entries in the text as connectives. The precision of
this heuristic is very low, 8.36%, which leads to an
F-score of 15.43%, considering the recall is 100%.
This is mainly due to triggers which are rarely used
as causal triggers, such as and, by and that.
Building on the previously mentioned observation
about the lowest common ancestor for all tokens in a
causal trigger, we built a baseline system that checks
all constituent nodes in the lexical parse tree for the
S, V, VP and ADV tags and marks them as causal
41
triggers. The name of this system is Dep. Not only
does Dep obtain a lower precision than Dict, but it
also performs worse in terms of recall. The F-score
is 13.68%, largely due to the high number of inter-
mediate nodes in the lexical parse tree that have VP
as their category.
The third baseline is a combination of Dict and
Dep: we consider only constituents that have the
necessary category (S, V, VP or ADV) and include
a trigger from the dictionary. Although the recall
decreases slightly, the precision increases to almost
twice that of both Dict and Dep. This produces a
much better F-score of 24.03%.
4.2 Sequence labelling task
As a sequence labelling task, we have modelled
causal trigger detection as two separate tasks.
Firstly, each trigger is represented in the B-I-O for-
mat (further mentioned as the 3-way model). Thus,
the first word of every trigger is tagged as B (be-
gin), whilst the following words in the trigger are
tagged as I (inside). Non-trigger words are tagged
as O (outside).
The second model is a simpler version of the pre-
vious one: it does not distinguish between the first
and the following words in the trigger. In other
words, each word is tagged either as being part of
or outside the trigger, further known as the 2-way
model. Hence, a sequence of contiguous tokens
marked as part of a trigger form one trigger.
CRF performs reasonably well in detecting causal
triggers. In the 3-way model, it obtains an F-score of
almost 73%, much better than the other algorithms.
It also obtains the highest precision (89%) and recall
(64%). However, in the 2-way model, CRF?s perfor-
mance is slightly lower than that of Random Forests,
achieving only 79.35%. Its precision, on the other
hand, is the highest in this model. The results from
both models were obtained by combining features
from all three feature categories.
Table 2 show the effect of feature types on both
models of CRFs. As can be observed, the best per-
formances, in terms of F-score, including the previ-
ously mentioned ones, are obtained when combin-
ing all three types of features, i.e., lexical, syntactic
and semantic. The best precision and recall, how-
ever, are not necessarily achieved by using all three
feature types. In the two-way model, the best preci-
Features P R F1
2-w
ay
Lex 88.99 67.09 73.59
Syn 92.20 68.68 75.72
Sem 87.20 63.30 69.36
Lex-Syn 87.76 73.29 78.73
Lex+Sem 89.54 69.10 75.61
Syn+Sem 87.48 72.62 78.13
Lex-Syn-Sem 89.29 73.53 79.35
3-w
ay
Lex 85.87 56.34 65.18
Syn 87.62 61.44 70.22
Sem 80.78 51.43 59.39
Lex+Syn 87.80 63.04 72.59
Lex+Sem 85.50 58.11 66.80
Syn+Sem 84.83 64.94 72.41
Lex-Syn-Sem 89.13 64.04 72.87
Table 2: Effect of feature types on the sequence labelling
task, given in percentages.
sion is obtained by using the syntactic features only,
reaching over 92%, almost 3% higher than when all
three feature types are used. In the three-way model,
syntactic and semantic features produce the best re-
call (almost 65%), which is just under 1% higher
than the recall when all features are used.
4.3 Classification task
As a classification task, an algorithm has to decide
whether a token is part of a trigger or not, similarly
to the previous two-way subtask in the case of CRF.
Firstly, we have used RF for the classification
task. Various parameter settings regarding the num-
ber of constructed trees and the number of random
features have been explored.
The effect of feature types on the performance of
RF is shown in Table 3. As can be observed, the
best performance is obtained when combining lexi-
cal and semantic features. Due to the fact that causal
triggers do not have a semantic mapping to concepts
in the named entity and UMLS annotations, the trees
in the random forest classifier can easily produce
rules that distinguish triggers from non-triggers. As
such, the use of semantic features alone produce a
very good precision of 84.34%. Also, in all cases
where semantic features are combined with other
feature types, the precision increases by 0.5% in the
case of lexical features and 3.5% in the case of syn-
tactic features. However, the recall of semantic fea-
42
tures alone is the lowest. The best recall is obtained
when using only lexical features.
Features P R F1
Lex 78.47 68.30 73.03
Syn 68.19 62.36 65.15
Sem 84.34 56.83 67.91
Lex+Syn 77.11 65.92 71.09
Lex+Sem 79.10 67.91 73.08
Syn+Sem 71.83 64.45 67.94
Lex+Syn+Sem 77.98 67.31 72.25
Table 3: Effect of feature types on Random Forests.
Secondly, we explored the performance of SVMs
in detecting causal triggers. We have experimented
with two kernels, namely polynomial (second de-
gree) and radial basis function (RBF) kernels. For
each of these two kernels, we have evaluated vari-
ous combinations of parameter values for cost and
weight. Both these kernels achieved similar results,
indicating that the feature space is not linearly sepa-
rable and that the problem is highly complex.
The effect of feature types on the performance of
SVMs is shown in Table 4. As can be observed,
the best performance is obtained when combining
the lexical and semantic feature types (69.85% F-
score). The combination of all features produces the
best precision, whilst the best recall is obtained by
combining lexical and semantic features.
Features P R F1
Lex 80.80 60.94 69.47
Syn 82.94 55.60 66.57
Sem 85.07 56.51 67.91
Lex+Syn 86.49 53.63 66.81
Lex+Sem 81.62 61.05 69.85
Syn+Sem 84.49 55.31 66.85
Lex+Syn+Sem 87.70 53.96 66.81
Table 4: Effect of feature types on SVM.
4.4 Error analysis
As we expected, the majority of errors arise from se-
quences of tokens which are only used infrequently
as non-causal triggers. This applies to 107 trigger
types, whose number of false positives (FP) is higher
than the number of true positives (TP). In fact, 64
trigger types occur only once as a causal instance,
whilst the average number of FPs for these types is
14.25. One such example is and, for which the num-
ber of non-causal instances (2305) is much greater
than that of causal instances (1). Other examples
of trigger types more commonly used as causal trig-
gers, are suggesting (9 TP, 54 FP), indicating (8 TP,
41 FP) and resulting in (6 TP, 14 FP). For instance,
example (2) contains two mentions of indicating, but
neither of them implies causality.
(2) Buffer treated control cells showed intense
green staining with syto9 (indicating viabil-
ity) and a lack of PI staining (indicating no
dead/dying cells or DNA release).
5 Conclusions and Future Work
We have presented an approach to the automatic
identification of triggers of causal discourse rela-
tions in biomedical scientific text. The task has
proven to be a highly complex one, posing many
challenges. Shallow approaches, such as dictionary
matching and lexical parse tree matching, perform
very poorly, due to the high ambiguity of causal
triggers (with F-scores of approximately 15% each
and 24% when combined). We have explored vari-
ous machine learning algorithms that automatically
classify tokens into triggers or non-triggers and we
have evaluated the impact of multiple lexical, syn-
tactic and semantic features. The performance of
SVMs prove that the task of identifying causal trig-
gers is indeed complex. The best performing classi-
fier is CRF-based and combines lexical, syntactical
and semantical features in order to obtain an F-score
of 79.35%.
As future work, integrating the causal relations in
the BioDRB corpus is necessary to check whether a
data insufficiency problem exists and, if so, estimate
the optimal amount of necessary data. Furthermore,
evaluations against the general domain need to be
performed, in order to establish any differences in
expressing causality in the biomedical domain. One
possible source for this is the PDTB corpus. A more
difficult task that needs attention is that of identify-
ing implicit triggers. Finally, our system needs to be
extended in order to identify the two arguments of
43
causal relations, the cause and effect, thus allowing
the creation of a complete discourse causality parser.
Acknowledgements
This work was partially funded by the Engineer-
ing and Physical Sciences Research Council [grant
number EP/P505631/1].
References
Sophia Ananiadou and John McNaught, editors. 2006.
Text Mining for Biology And Biomedicine. Artech
House, Inc.
Riza Theresa B. Batista-Navarro and Sophia Anani-
adou. 2011. Building a coreference-annotated corpus
from the domain of biochemistry. In Proceedings of
BioNLP 2011, pages 83?91.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (UMLS): integrating biomedical termi-
nology. Nucleic Acids Research, 32(suppl 1):D267?
D270.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Kevin Bretonnel Cohen and Lawrence Hunter. 2008.
Getting started in text mining. PLoS Computational
Biology, 4(1):e20, 01.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Ken-ichiro Fukuda, Tatsuhiko Tsunoda, Ayuchi Tamura,
and Toshihisa Takagi. 1998. Toward information ex-
traction: Identifying protein names from biological pa-
pers. In Proceedings of the Pacific Symposium on Bio-
computing, volume 707, pages 707?718.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Samantha Kleinberg and George Hripcsak. 2011. A re-
view of causal inference for biomedical informatics.
Journal of Biomedical Informatics, 44(6):1102 ? 1112.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
pdtb-styled end-to-end discourse parser. Natural Lan-
guage Engineering, FirstView:1?34, 10.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating and
analysing causality in the biomedical domain. BMC
Bioinformatics, 14(1):2, January.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and Jun?ichi
Tsujii. 2009. Protein-protein interaction extraction by
leveraging multiple kernels and parsers. International
Journal of Medical Informatics, 78(12):e39?e46, June.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012a. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics, 28(13):1759?1765.
Makoto Miwa, Paul Thompson, John McNaught, Dou-
glas B. Kell, and Sophia Ananiadou. 2012b. Extract-
ing semantically enriched events from biomedical lit-
erature. BMC Bioinformatics, 13:108.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Computa-
tional Linguistics, 34(1):3580, March.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ninomiya,
and Jun?ichi Tsujii. 2006. Semantic retrieval for the
accurate identification of relational concepts in mas-
sive textbases. In ACL.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
ACL/AFNLP (Short Papers), pages 13?16.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,
and Daniel Tapias, editors, In Proceedings of the 6th
International Conference on language Resources and
Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical discourse
relation bank. BMC Bioinformatics, 12(1):188.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, BioNLP ?09, pages 1?
9, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the infectious diseases (ID) task of
BioNLP shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 26?35,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
44
Polepalli Balaji Ramesh, Rashmi Prasad, Tim Miller,
Brian Harrington, and Hong Yu. 2012. Automatic dis-
course connective detection in biomedical text. Jour-
nal of the American Medical Informatics Association.
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Morris,
Hong Yu, Ariel Pablo Duboue?, Wubin Weng, W.John
Wilbur, Vasileios Hatzivassiloglou, and Carol Fried-
man. 2004. Geneways: a system for extracting, ana-
lyzing, visualizing, and integrating molecular pathway
data. Journal of Biomedical Informatics, 37(1):43 ?
53.
Guergana K Savova, Wendy W Chapman, Jiaping Zheng,
and Rebecca S Crowley. 2011. Anaphoric rela-
tions in the clinical narrative: corpus creation. Jour-
nal of the American Medical Informatics Association,
18(4):459?465.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances
in Informatics - 10th Panhellenic Conference on In-
formatics, volume 3746 of LNCS, pages 382?392.
Springer-Verlag, Volos, Greece, November.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques (Second
Edition). Morgan Kaufmann.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. 2007. Frontiers of biomedical
text mining: current progress. Briefings in Bioinfor-
matics, 8(5):358?375.
45
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115?120,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Development and Analysis of NLP Pipelines in Argo
Rafal Rak, Andrew Rowley, Jacob Carter, and Sophia Ananiadou
National Centre for Text Mining
School of Computer Science, University of Manchester
Manchester Institute of Biotechnology
131 Princess St, M1 7DN, Manchester, UK
{rafal.rak,andrew.rowley,jacob.carter,sophia.ananiadou}@manchester.ac.uk
Abstract
Developing sophisticated NLP pipelines
composed of multiple processing tools
and components available through differ-
ent providers may pose a challenge in
terms of their interoperability. The Un-
structured Information Management Ar-
chitecture (UIMA) is an industry stan-
dard whose aim is to ensure such in-
teroperability by defining common data
structures and interfaces. The architec-
ture has been gaining attention from in-
dustry and academia alike, resulting in a
large volume of UIMA-compliant process-
ing components. In this paper, we demon-
strate Argo, a Web-based workbench for
the development and processing of NLP
pipelines/workflows. The workbench is
based upon UIMA, and thus has the poten-
tial of using many of the existing UIMA
resources. We present features, and show
examples, of facilitating the distributed de-
velopment of components and the analysis
of processing results. The latter includes
annotation visualisers and editors, as well
as serialisation to RDF format, which en-
ables flexible querying in addition to data
manipulation thanks to the semantic query
language SPARQL. The distributed devel-
opment feature allows users to seamlessly
connect their tools to workflows running
in Argo, and thus take advantage of both
the available library of components (with-
out the need of installing them locally) and
the analytical tools.
1 Introduction
Building NLP applications usually involves a se-
ries of individual tasks. For instance, the ex-
traction of relationships between named entities
in text is preceded by text segmentation, part-of-
speech recognition, the recognition of named enti-
ties, and dependency parsing. Currently, the avail-
ability of such atomic processing components is
no longer an issue; the problem lies in ensur-
ing their compatibility, as combining components
coming from multiple repositories, written in dif-
ferent programming languages, requiring different
installation procedures, and having incompatible
input/output formats can be a source of frustration
and poses a real challenge for developers.
Unstructured Information Management Archi-
tecture (UIMA) (Ferrucci and Lally, 2004) is a
framework that tackles the problem of interoper-
ability of processing components. Originally de-
veloped by IBM, it is currently an Apache Soft-
ware Foundation open-source project1 that is also
registered at the Organization for the Advance-
ment of Structured Information Standards (OA-
SIS)2. UIMA has been gaining much interest from
industry and academia alike for the past decade.
Notable repositories of UIMA-compliant tools
include U-Compare component library3, DKPro
(Gurevych et al, 2007), cTAKES (Savova et
al., 2010), BioNLP-UIMA Component Reposi-
tory (Baumgartner et al, 2008), and JULIE Lab?s
UIMA Component Repository (JCoRe) (Hahn et
al., 2008).
In this work we demonstrate Argo4, a Web-
based (remotely-accessed) workbench for collabo-
rative development of text-processing workflows.
We focus primarily on the process of development
and analysis of both individual processing com-
ponents and workflows composed of such compo-
nents.
The next section demonstrates general features
of Argo and lays out several technical details about
1http://uima.apache.org
2http://www.oasis-open.org/committees/uima
3http://nactem.ac.uk/ucompare/
4http://argo.nactem.ac.uk
115
UIMA that will ease the understanding of the re-
maining sections. Sections 3?5 discuss selected
features that are useful in the development and
analysis of components and workflows. Section 6
mentions related efforts, and Section 7 concludes
the paper.
2 Overview of Argo
Argo comes equipped with an ever-growing li-
brary of atomic processing components that can be
put together by users to form meaningful pipelines
or workflows. The processing components range
from simple data serialisers to complex text an-
alytics and include text segmentation, part-of-
speech tagging, parsing, named entity recognition,
and discourse analysis.
Users interact with the workbench through a
graphical user interface (GUI) that is accessible
entirely through a Web browser. Figure 1 shows
two views of the interface: the main, resource
management window (Figure 1(a)) and the work-
flow diagramming window (Figure 1(b)). The
main window provides access to emphdocuments,
workflows, and processes separated in easily ac-
cessible panels.
The Documents panel lists primarily user-
owned files that are uploaded (through the GUI)
by users into their respective personal spaces on
the remote host. Documents may also be gener-
ated as a result of executing workflows (e.g., XML
files containing annotations), in which case they
are available for users to download.
The Workflows panel lists users? workflows,
i.e., the user-defined arrangements of processing
components together with their settings. Users
compose workflows through a flexible, graphi-
cal diagramming editor by connecting the com-
ponents (represented as blocks) with lines signi-
fying the flow of data between components (see
Figure 1(b)). The most common arrangement is to
form a pipeline, i.e., each participating component
has at most one incoming and at most one out-
going connection; however, the system also sup-
ports multiple branching and merging points in the
workflow. An example is shown in Figure 2 dis-
cussed farther in text. For ease of use, components
are categorized into readers, analytics, and con-
sumers, indicating what role they are set to play in
a workflow. Readers are responsible for delivering
data for processing and have only an outgoing port
(represented as a green triangle). The role of an-
(a) Workflow management view
(b) Worflow diagram editor view
Figure 1: Screenshots of Argo Web browser con-
tent.
alytics is to modify incoming data structures and
pass them onto following components in a work-
flow, and thus they have both incoming and outgo-
ing ports. Finally, the consumers are responsible
for serialising or visualising (selected or all) anno-
tations in the data structures without modification,
and so they have only an incoming port.
The Processes panel lists resources that are cre-
ated automatically when workflows are submit-
ted for execution by users. Users may follow the
progress of the executing workflows (processes) as
well as manage the execution from this panel. The
processing of workflows is carried out on remote
servers, and thus frees users from using their own
processing resources.
2.1 Argo and UIMA
Argo supports and is based upon UIMA and thus
can run any UIMA-compliant processing compo-
nent. Each such component defines or imports
type systems and modifies common annotation
structures (CAS). A type system is the represen-
116
tation of a data model that is shared between com-
ponents, whereas a CAS is the container of data
whose structure complies with the type system. A
CAS stores feature structures, e.g., a token with
its text boundaries and a part-of-speech tag. Fea-
ture structures may, and often do, refer to a sub-
ject of annotation (Sofa), a structure that (in text-
processing applications) stores the text. UIMA
comes with built-in data types including primitive
types (boolean, integer, string, etc.), arrays, lists,
as well as several complex types, e.g., Annotation
that holds a reference to a Sofa the annotation is
asserted about, and two features, begin and end,
for marking boundaries of a span of text. A devel-
oper is free to extend any of the complex types.
2.2 Architecture
Although the Apache UIMA project provides an
implementation of the UIMA framework, Argo
incorporates home-grown solutions, especially in
terms of the management of workflow processing.
This includes features such as workflow branching
and merging points, user-interactive components
(see Section 4), as well as distributed processing.
The primary processing is carried out on a
multi-core server. Additionally, in order to in-
crease computing throughput, we have incorpo-
rated cloud computing capabilities into Argo,
which is designed to work with various cloud
computing providers. As a proof of concept,
the current implementation uses HTCondor, an
open-source, high-throughput computing software
framework. Currently, Argo is capable of switch-
ing the processing of workflows to a local cluster
of over 3,000 processor cores. Further extensions
to use the Microsoft Azure5 and Amazon EC26
cloud platforms are also planned.
The Argo platform is available entirely us-
ing RESTful Web services (Fielding and Taylor,
2002), and therefore it is possible to gain access
to all or selected features of Argo by implement-
ing a compliant client. In fact, the ?native? Web
interface shown in Figure 1 is an example of such
a client.
3 Distributed Development
Argo includes a Generic Listener component that
permits execution of a UIMA component that is
running externally of the Argo system. It is pri-
5http://www.windowsazure.com
6http://aws.amazon.com/ec2
marily intended to be used during the develop-
ment of processing components, as it allows a de-
veloper to rapidly make any necessary changes,
whilst continuing to make use of the existing com-
ponents available within Argo, which may other-
wise be unavailable if developing on the devel-
oper?s local system. Any component that a user
wishes to deploy on the Argo system has to un-
dergo a verification process, which could lead to
a slower development lifecycle without the avail-
ability of this component.
Generic Listener operates in a reverse manner
to a traditional Web service; rather than Argo con-
necting to the developer?s component, the compo-
nent connects to Argo. This behaviour was de-
liberately chosen to avoid network-related issues,
such as firewall port blocking, which could be-
come a source of frustration to developers.
When a workflow, containing a Generic Lis-
tener, is executed within Argo, it will continue
as normal until the point at which the Generic
Listener receives its first CAS object. Argo will
prompt the user with a unique URL, which must
be supplied to the client component run by the
user, allowing it to connect to the Argo workflow
and continue its execution.
A skeleton Java project has been provided to as-
sist in the production of such components. It con-
tains a Maven structure, Eclipse IDE project files,
and required libraries, in addition to a number of
shell scripts to simplify the running of the compo-
nent. The project provides both a command-line
interface (CLI) and GUI runner applications that
take, as arguments, the name of the class of the lo-
cally developed component and the URL provided
by Argo, upon each run of a workflow containing
the remote component.
An example of a workflow with a Generic Lis-
tener is shown in Figure 2. The workflow is de-
signed for the analysis and evaluation of a solu-
tion (in this case, the automatic extraction of bio-
logical events) that is being developed locally by
the user. The reader (BioNLP ST Data Reader)
provides text documents together with gold (i.e.,
manually created) event annotations prepared for
the BioNLP Shared Task7. The annotations are
selectively removed with the Annotation Remover
and the remaining data is sent onto the Generic
Listener component, and consequently, onto the
developer?s machine. The developer?s task is to
7http://2013.bionlp-st.org/
117
Figure 2: Example of a workflow for development,
analysis, and evaluation of a user-developed solu-
tion for the BioNLP Shared Task.
connect to Argo, retrieve CASes from the run-
ning workflow, and for each CAS recreate the re-
moved annotations as faithfully as possible. The
developer can then track the performance of their
solution by observing standard information ex-
traction measures (precision, recall, etc.) com-
puted by the Reference Evaluator component that
compares the original, gold annotations (coming
from the reader) against the developer?s annota-
tions (coming from the Generic Listener), and
saves these measures for each document/CAS into
a tabular-format file. Moreover, the differences
can be tracked visually though the interactive Brat
BioNLP ST Comparator component, discussed in
the next section.
4 Annotation Analysis and Manipulation
Traditionally, NLP pipelines (including existing
UIMA-supporting platforms), once set up, are
executed without human involvement. One of
the novelties in Argo is an introduction of user-
interactive components, a special type of analytic
that, if present in a workflow, cause the execu-
tion of the workflow to pause. Argo resumes the
execution only after receiving input from a user.
This feature allows for manual intervention in the
otherwise automatic processing by, e.g., manipu-
lating automatically created annotations. Exam-
ples of user-interactive components include Anno-
tation Editor and Brat BioNLP ST Comparator.
The Brat BioNLP ST Comparator component
Figure 3: Example of an annotated fragment of
a document visualised with the Brat BioNLP ST
Comparator component. The component high-
lights (in red and green) differences between two
sources of annotations.
Figure 4: Example of manual annotation with the
user-interactive Annotation Editor component.
expects two incoming connections from compo-
nents processing the same subject of annotation.
As a result, using brat visualisation (Stenetorp et
al., 2012), it will show annotation structures by
laying them out above text and mark differences
between the two inputs by colour-coding missing
or additional annotations in each input. A sam-
ple of visualisation coming from the workflow in
Figure 2 is shown in Figure 3. Since in this par-
ticular workflow the Brat BioNLP ST Comparator
receives gold annotations (from the BioNLP ST
Data Reader) as one of its inputs, the highlighted
differences are, in fact, false positives and false
negatives.
Annotation Editor is another example of a user-
interactive component that allows the user to add,
delete or modify annotations. Figure 4 shows the
editor in action. The user has an option to cre-
ate a span-of-text annotation by selecting a text
fragment and assigning an annotation type. More
complex annotation types, such as tokens with
part-of-speech tags or annotations that do not re-
fer to the text (meta-annotations) can be created
or modified using an expandable tree-like struc-
ture (shown on the right-hand side of the figure),
which makes it possible to create any annotation
118
(a) Select query
neAText neACat neBText neBCat count
Ki-67 Protein p53 Protein 85
DC CellType p53 Protein 61
DC CellType KCOT Protein 47
(b) Results (fragment)
(c) Insert query
Figure 5: Example of (a) a SPARQL query that returns biological interactions; (b) a fragment of retrieved
results; and (c) a SPARQL query that creates new UIMA feature structures. Namespaces and data types
are omitted for brevity.
structure permissible by a given type system.
5 Querying Serialised Data
Argo comes with several (de)serialisation com-
ponents for reading and storing collections of
data, such as a generic reader of text (Document
Reader) or readers and writers of CASes in XMI
format (CAS Reader and CAS Writer). One of
the more useful in terms of annotation analysis
is, however, the RDF Writer component as well
as its counterpart, RDF Reader. RDF Writer se-
rialises data into RDF files and supports several
RDF formats such as RDF/XML, Turtle, and N-
Triple. A resulting RDF graph consists of both the
data model (type system) and the data itself (CAS)
and thus constitutes a self-contained knowledge
base. RDF Writer has an option to create a graph
for each CAS or a single graph for an entire collec-
tion. Such a knowledge base can be queried with
languages such as SPARQL8, an official W3C
Recommendation.
Figure 5 shows an example of a SPARQL query
that is performed on the output of an RDF Writer
in the workflow shown in Figure 1(b). This work-
flow results in several types of annotations in-
cluding the boundaries of sentences, tokens with
part-of-speech tags and lemmas, chunks, as well
as biological entities, such as DNA, RNA, cell
line and cell type. The SPARQL query is meant
to retrieve pairs of seemingly interacting biolog-
ical entities ranked according to their occurrence
in the entire collection. The interaction here is
(na??vely) defined as co-occurrence of two entities
in the same sentence. The query includes pat-
terns for retrieving the boundaries of sentences
(syn:Sentence) and two biological entities
(sem:NamedEntity) and then filters out the
crossproduct of those by ensuring that the two en-
8http://www.w3.org/TR/2013/REC-sparql11-overview-
20130321/
119
tities are enclosed in a sentence. As a result, the
query returns a list of biological entity pairs ac-
companied by their categories and the number of
appearances, as shown in Figure 5(b). Note that
the query itself does not list the four biological cat-
egories; instead, it requests their common seman-
tic ancestor sem:NamedEntity. This is one of
the advantages of using semantically-enabled lan-
guages, such as SPARQL.
SPARQL also supports graph manipulation.
Suppose a user is interested in placing the re-
trieved biological entity interactions from our run-
ning example into the UIMA structure Relation-
ship that simply defines a pair of references to
other structures of any type. This can be accom-
plished, without resorting to programming, by is-
suing a SPARQL insert query shown in Figure
5(c). The query will create triple statements com-
pliant with the definition of Relationship. The re-
sulting modified RDF graph can then be read back
to Argo by the RDF Reader component that will
convert the new RDF graph back into a CAS.
6 Related Work
Other notable examples of NLP platforms that
provide graphical interfaces for managing work-
flows include GATE (Cunningham et al, 2002)
and U-Compare (Kano et al, 2010). GATE is
a standalone suite of text processing and annota-
tion tools and comes with its own programming
interface. In contrast, U-Compare?similarly to
Argo?uses UIMA as its base interoperability
framework. The key features of Argo that distin-
guish it from U-Compare are the Web availabil-
ity of the platform, primarily remote processing
of workflows, a multi-user, collaborative architec-
ture, and the availability of user-interactive com-
ponents.
7 Conclusions
Argo emerges as a one-stop solution for develop-
ing and processing NLP tasks. Moreover, the pre-
sented annotation viewer and editor, performance
evaluator, and lastly RDF (de)serialisers are in-
dispensable for the analysis of processing tasks
at hand. Together with the distributed develop-
ment support for developers wishing to create their
own components or run their own tools with the
help of resources available in Argo, the workbench
becomes a powerful development and analytical
NLP tool.
Acknowledgments
This work was partially funded by the MRC Text
Mining and Screening grant (MR/J005037/1).
References
W A Baumgartner, K B Cohen, and L Hunter. 2008.
An open-source framework for large-scale, flexible
evaluation of biomedical text mining systems. Jour-
nal of biomedical discovery and collaboration, 3:1+.
H Cunningham, D Maynard, K Bontcheva, and
V Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics.
D Ferrucci and A Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
R T Fielding and R N Taylor. 2002. Principled de-
sign of the modern Web architecture. ACM Trans.
Internet Technol., 2(2):115?150, May.
I Gurevych, M Mu?hlha?user, C Mu?ller, J Steimle,
M Weimer, and T Zesch. 2007. Darmstadt knowl-
edge processing repository based on uima. In Pro-
ceedings of the First Workshop on Unstructured
Information Management Architecture, Tu?bingen,
Germany.
U Hahn, E Buyko, R Landefeld, M Mu?hlhausen,
M Poprat, K Tomanek, and J Wermter. 2008. An
Overview of JCORE, the JULIE Lab UIMA Compo-
nent Repository. In Language Resources and Eval-
uation Workshop, Towards Enhanc. Interoperability
Large HLT Syst.: UIMA NLP, pages 1?8.
Y Kano, R Dorado, L McCrochon, S Ananiadou, and
J Tsujii. 2010. U-Compare: An integrated language
resource evaluation platform including a compre-
hensive UIMA resource library. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 428?434.
G K Savova, J J Masanz, P V Ogren, J Zheng, S Sohn,
K C Kipper-Schuler, and C G Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction
System (cTAKES): architecture, component evalua-
tion and applications. Journal of the American Med-
ical Informatics Association, 17(5):507?513.
P Stenetorp, S Pyysalo, G Topic?, T Ohta, S Ananiadou,
and J Tsujii. 2012. brat: a web-based tool for nlp-
assisted text annotation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 102?107,
Avignon, France.
120
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 132?140,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards Event Extraction from Full Texts on Infectious Diseases
Sampo Pyysalo? Tomoko Ohta? Han-Cheol Cho? Dan Sullivan?
Chunhong Mao? Bruno Sobral? Jun?ichi Tsujii??? Sophia Ananiadou??
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, USA
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{smp,okap,priancho,tsujii}@is.s.u-tokyo.ac.jp
{dsulliva,cmao,sobral}@vbi.vt.edu
Sophia.Ananiadou@manchester.ac.uk
Abstract
Event extraction approaches based on ex-
pressive structured representations of ex-
tracted information have been a significant
focus of research in recent biomedical nat-
ural language processing studies. How-
ever, event extraction efforts have so far
been limited to publication abstracts, with
most studies further considering only the
specific transcription factor-related subdo-
main of molecular biology of the GENIA
corpus. To establish the broader relevance
of the event extraction approach and pro-
posed methods, it is necessary to expand
on these constraints. In this study, we pro-
pose an adaptation of the event extraction
approach to a subdomain related to infec-
tious diseases and present analysis and ini-
tial experiments on the feasibility of event
extraction from domain full text publica-
tions.
1 Introduction
For most of the previous decade, biomedical In-
formation Extraction (IE) efforts have focused pri-
marily on tasks that allow extracted information
to be represented as simple pairs of related enti-
ties. This representation is applicable to many IE
targets of interest, such as gene-disease associa-
tions (Chun et al, 2006) and protein-protein inter-
actions (Ne?dellec, 2005; Krallinger et al, 2007).
However, it has limited applicability to advanced
applications such as semantic search, Gene On-
tology term annotation, and pathway extraction,
tasks for which and relatively few resources or sys-
tems (e.g. (Rzhetsky et al, 2004)) have been intro-
duced. A number of recent studies have proposed
more expressive representations of extracted in-
formation, introducing resources supporting ad-
vanced IE approaches (Pyysalo et al, 2007; Kim
et al, 2008; Thompson et al, 2009; Ananiadou
et al, 2010a). A significant step in the develop-
ment of domain IE methods capable of extract-
ing this class of representations was taken in the
BioNLP?09 shared task on event extraction, where
24 teams participated in an IE task setting requir-
ing the extraction of structured representations of
multi-participant biological events of several types
(Kim et al, 2009).
While the introduction of structured event ex-
traction resources and methods has notably ad-
vanced the state of the art in biomedical IE rep-
resentations, the focus of event extraction studies
carries other limitations frequently encountered in
domain IE efforts. Specifically, resources anno-
tated for biomedical events contain exclusively
texts from publication abstracts, typically further
drawn from small subdomains of molecular biol-
ogy. These choices constrain not only the types of
texts but also the types of events considered, re-
stricting the applicability of event extraction. This
paper presents results from one ongoing effort to
extend an event extraction approach over these
boundaries, toward event extraction from full text
documents in the domain of infectious diseases.
In this study, we consider the subdomain related
to Type IV secretion systems as a model subdo-
main of interest within the broad infectious dis-
eases domain. Type IV secretion systems (T4SS)
are mechanisms for transferring DNA and pro-
teins across cellular boundaries. T4SS are found
in a broad range of Bacteria and in some Ar-
chaea. These translocation systems enable gene
transfer across cellular membranes thus contribut-
ing to the spread of antibiotic resistance and viru-
132
Figure 1: Event representation example. Inhibition of binding caused by phosphorylation is represented
using three events. The shaded text background identifies the text bindings of the events and entities.
lence genes making them an especially important
mechanism in infectious disease research (Juhas et
al., 2008). Type IV secretion systems are found in
plant pathogens, such as Agrobacterium tumefa-
ciens, the cause of crown gall disease as well as in
animal pathogens, such as Helicobacter pylori, a
cause of severe gastric disease. The study of T4SS
has been hampered by the lack of consistent termi-
nology to describe genes and proteins associated
with the translocation mechanism thus motivating
the use of natural language processing techniques
to enhance information retrieval and information
extraction from relevant literature.
2 Event Extraction for the T4SS Domain
This section presents the application of an event
extraction approach to the T4SS domain.
2.1 Event Extraction
We base our information extraction approach on
the model introduced in the BioNLP?09 shared
task on event extraction. Central to this approach
is the event representation, which can capture
the association of multiple participants in varying
roles and numbers and treats events as primary ob-
jects of annotation, thus allowing events to be par-
ticipants in other events. Further, both entities and
events are text-bound, i.e. anchored to specific ex-
pressions in text (Figure 1).
The BioNLP?09 shared task defined nine event
types and five argument types (roles): Theme spec-
ifies the core participant(s) that an event affects,
Cause the cause of the the event, Site a specific
domain or region on a participant involved in the
event, and ToLoc and AtLoc locations associated
with localization events (Table 1). Theme and
Cause arguments may refer to either events or
gene/gene product entities, and other arguments
refer to other physical entities. The Theme ar-
gument is always mandatory, while others can be
omitted when a relevant participant is not stated.
The event types were originally defined to cap-
ture statements of biologically relevant changes in
Event type Args Example
Gene expression T 5-LOX is coexpressed
Transcription T IL-4 transcription
Protein catabolism T IkB-A proteolysis
Localization T,L translocation of STAT6
Phosphorylation T,S NF90 was phosphorylated
Binding T+,S+ Nmi interacts with STAT
Regulation T,C,S IL-4 gene control
Positive regulation T,C,S IL-12 induced binding
Negative regulation T,C,S suppressed dimerization
Table 1: Event types targeted in the BioNLP?09
shared task and their arguments, with minimal
examples of each event type. Arguments ab-
breviate for (T)heme, (C)ause, (S)ite and L for
ToLoc/AtLoc, with ?+? identifying arguments
than can occur multiple times. The expression
marked as triggering the event shown in italics.
the state of entities in a target subdomain involv-
ing transcription factors in human blood cells. In
adapting the approach to new domains, some ex-
tension of the event types is expected to be nec-
essary. By contrast, the argument types and the
general design of the representation are intended
to be general, and to maintain compatibility with
existing systems we aim to avoid modifying these.
2.2 T4SS Domain
A corpus of full-text publications relating to the
T4SS subdomain of the infectious diseases do-
main annotated for biological entities and terms of
interest to domain experts was recently introduced
by (Ananiadou et al, 2010b). In the present study,
we use this corpus as a reference standard defin-
ing domain information needs. In the following
we briefly describe the corpus annotation and the
view it provides of the domain.
The T4SS corpus annotation covers four classes
of tagged entities and terms: Bacteria, Cellular
components, Biological Processes, and Molecular
functions. The latter three correspond to the three
Gene Ontology (GO) (Ashburner et al, 2000) top-
level sub-ontologies, and terms of these types were
annotated with reference to both GO and relevance
to the interests of domain experts, with guidelines
133
Bacterium
A. tumefaciens 32.7%
H. pylori 20.0%
L. pneumophila 16.3%
E. coli 12.3%
B. pertussis 3.0%
Cell component
T4SS 5.2%
Ti plasmid 5.1%
outer membrane 4.2%
membrane 3.5%
genome 3.4%
Biological process
virulence 14.1%
conjugation 7.9%
localization 6.1%
nuclear import 5.8%
transfer 5.1%
Molecular function
nucleotide-binding 20.3%
ATPase activity 17.3%
NTP-binding 14.7%
ATP-binding 12.2%
DNA-binding 9.1%
Table 2: Most frequently tagged terms (after normalization) and their relative frequencies of all tagged
entities of each of the four types annotated in the T4SS corpus.
Type Annotations
Bacteria 529
Cellular component 2237
Biological process 1873
Molecular function 197
Table 3: Statistics for the existing T4SS corpus
annotation.
requiring that marked terms be both found in GO
and associated with T4SS. These constraints as-
sure that the corpus is relevant to the informa-
tion needs of biologists working in the domain and
that it can be used as a reference for the study of
automatic GO annotation. In the work introduc-
ing the corpus, the task of automatic GO anno-
tation was studied as facilitating improved infor-
mation access, such as advanced search function-
ality: GO annotation can allow for search by se-
mantic classes or co-occurrences of terms of speci-
fied classes. The event approach considered in this
study further extends on these opportunities in in-
troducing a model allowing e.g. search by specific
associations of the concepts of interest.
The previously created annotation of the T4SS
corpus covers 27 full text publications totaling
15143 pseudo-sentences (text sentences plus table
rows, references, etc.) and 244942 tokens.1 A to-
tal of nearly 5000 entities and terms are annotated
in these documents; Table 2 shows the most fre-
quently tagged terms of each type after basic nor-
malization of different surface forms, and Table 3
gives the per-class statistics. Domain characteris-
tics are clearly identifiable in the first three tagged
types, showing disease-related bacteria, their ma-
jor cellular components, and processes related to
movement, reproduction and infection. The last
term type is dominated by somewhat more generic
binding-type molecular functions.
In addition to the four annotated types it was
1While the document count is modest compared to that
of abstract-based corpora, we estimate that in terms of the
amount of text (tokens) the corpus corresponds to over 1000
abstracts, comparable in size to e.g. the GENIA event corpus
(Kim et al, 2008).
recognized during the original T4SS corpus anno-
tation that genes and gene products are centrally
important for domain information needs, but their
annotation was deferred to focus on novel cate-
gories. As part of the present study, we introduce
annotation for gene/gene product (GGP) mentions
(Section 3.2), and in the following discussion of
applying an event extraction approach to the do-
main the availability of this class annotation as an
additional category is assumed.
2.3 Adaptation of the Event Model
The event model involves two primary categories
of representation: physical entities such as genes
and proteins are elementary (non-structured) and
their mentions annotated as typed spans of text,2
and events and processes (?things that happen?)
are represented using the structured event repre-
sentation described in Section 2.1. This division
applies straightforwardly to the T4SS annotations,
suggesting an approach where bacteria and cell
components retain their simple tagged-term repre-
sentation and the biological processes and molec-
ular functions are given an event representation.
In the following, we first analyze correspondences
between the latter two classes and BioNLP?09
shared task events, and then proceed to study the
event arguments and their roles as steps toward a
complete event model for the domain.
Molecular functions, the smallest class tagged
in the T4SS corpus, are highly uniform: almost
75% involve binding, immediately suggesting rep-
resentation using the Binding class of events de-
fined in the applied event extraction model. The
remaining functions are ATPase activity, together
with its exact GO synonyms (e.g. ATP hydrolase
activity) accounting for 19% of the terms, the gen-
eral type hydrolysis (4.5%), and a small number
of rare other functions. While these have no cor-
respondence with previously defined event types,
2Normalization identifying e.g. the Uniprot entry corre-
sponding to a protein mention may also be necessary, but here
excluded from consideration an independent issue.
134
Class Category Freq
Location
Transfer 27.6%
Localization 15.6%
Import/export 14.5%
Virulence 14.1%
High-level Assembly 8.7%
process Conjugation 8.3%
Secretion 8.1%
(Other) 1.8%
Table 4: Categorization of T4SS corpus biologi-
cal processes and relative frequency of mentions
of each category of the total tagged.
their low overall occurrence counts make them of
secondary interest as extraction targets.
The biological processes are considerably more
diverse. To identify general categories, we per-
formed a manual analysis of the 217 unique nor-
malized terms annotated in the corpus as biologi-
cal processes (Table 4). We find that the majority
of the instances (58%) relate to location or move-
ment. As related types of statements are anno-
tated as Localization events in the applied model,
we propose to apply this event type and differen-
tiate between the specific subtypes on the basis of
the event arguments. A further 39% are of cate-
gories that can be viewed as high-level processes.
These are distinct from the events considered in
the BioNLP?09 shared task in involving coarser-
grained events and larger-scale participants than
the GGP entities considered in the task: for ex-
ample, conjugation occurs between bacteria, and
virulence may involve a human host.
To analyze the role types and arguments char-
acteristic of domain events, we annotated a small
sample of tagged mentions for the most fre-
quent types in the broad classification discussed
above: Binding for Molecular function, Transfer
for Location-related, and Virulence for High-level
process. The statistics of the annotated 65 events
are shown in Tables 5, 6 and 7. For Binding, we
find that while an estimated 90% of events in-
volve a GGP argument, the other participant of
the binding is in all cases non-GGP, most fre-
quently of Nucleotide type (e.g. NTP/ATP). While
only GGP Binding arguments were considered in
the shared task events, the argument structures are
typical of multi-participant binding and this class
of expressions are in scope of the original GE-
NIA Event corpus annotation (Kim et al, 2008).
Event annotations could thus potentially be de-
rived from existing data. Localization event
arguments show substantially greater variety and
Freq Arguments
78% Theme: GGP, Theme: Nucleotide
5.5% Theme: GGP, Theme: DNA
5.5% Theme: GGP, Theme: Sugar
5.5% Theme: Protein family, Theme: DNA
5.5% Theme: Protein, Theme: Nucleotide
Table 5: Binding event arguments.
Freq Arguments
16% Theme: DNA, From/To: Organism
16% Theme: DNA
16% Theme: Cell component
12% Theme: DNA, To: Organism
8% Theme: Protein family, From/To: Organism
4% Theme: GGP
4% Theme: GGP, To: Organism
4% Theme: GGP, From: Organism
4% Theme: Protein family, From: Organism
4% Theme: Protein family
4% Theme: Organism, To: Cell component
4% Theme: DNA From: Organism, To: Cell component
4% (no arguments)
Table 6: Localization (Transfer) event arguments.
Freq Arguments
64% Cause: GGP
16% Theme:Organism, Cause: GGP
8% Cause: Organism
8% (no arguments)
4% Cause: Protein family
Table 7: Process (Virulence) arguments.
some highly domain-specific argument combina-
tions, largely focusing on DNA and Cell compo-
nent (e.g. phagosome) transfer, frequently involv-
ing transfer between different organisms. While
the participants are almost exclusively of types
that do not appear in Localization events in exist-
ing annotations, the argument structures are stan-
dard and in our judgment reasonably capture the
analyzed statements, supporting the applicability
of the general approach. Finally, the argument
analysis shown in Table 7 supports the previous
tentative observation that the high-level biologi-
cal processes are notably different from previously
considered event types: for over 80% of these pro-
cesses no overtly stated Theme could be identified.
We take this to indicate that the themes ? the core
participants that the processes concern ? are ob-
vious in the discourse context and their overt ex-
pression would be redundant. (For example, in
the context virulence obviously involves a host and
conjugation involves bacteria.) By contrast, in the
corpus the entities contributing to these processes
are focused: a participant we have here analyzed
as Cause is stated in over 90% of cases. This
135
Sentences Tokens
Abstracts 150 3789
Full texts 448 13375
Total 598 17164
Table 8: Statistics for the selected subcorpus.
novel pattern of event arguments suggests that the
event model should be augmented to capture this
category of high-level biological processes. Here,
we propose an event representation for these pro-
cesses that removes the requirement for a Theme
and substitutes instead a mandatory Cause as the
core argument. In the event annotation and exper-
iments, we focus on this newly proposed class.
3 Annotation
This section describes the new annotation intro-
duced for the T4SS corpus.
3.1 Text Selection
The creation of exhaustive manual annotation for
the full T4SS corpus represents a considerable an-
notation effort. Due to resource limitations, for
this study we did not attempt full-scope annota-
tion but instead selected a representative subset of
the corpus texts. We aimed to select texts that pro-
vide good coverage of the text variety in the T4SS
corpus and can be freely redistributed for use in re-
search. We first selected for annotation all corpus
documents with at least a freely available PubMed
abstract, excluding 3 documents. As the corpus
only included a single freely redistributable Open
Access paper, we extended full text selection to
manuscripts freely available as XML/HTML (i.e.
not only PDF) via PubMed Central. While these
documents cannot be redistributed in full, their
text can be reliably combined with standoff anno-
tations to recreate the annotated corpus.
In selected full-text documents, to focus anno-
tation efforts on sections most likely to contain re-
liable new information accessible to natural lan-
guage processing methods, we further selected the
publication body text, excluding figures and tables
and their captions, and removed Methods and Dis-
cussion sections. We then removed artifacts such
as page numbers and running heads and cleaned
remaining errors from PDF conversion of the orig-
inal documents. This selection produced a subcor-
pus of four full-text documents and 19 abstracts.
The statistics for this corpus are shown in Table 8.
GGP GGP/sentence
Abstracts 124 0.82
Full texts 394 0.88
Total 518 0.87
Table 9: Statistics for the GGP annotation.
3.2 Gene/Gene Product Annotation
As gene and gene product entities are central to
domain information needs and the core entities of
the applied event extraction approach, we first in-
troduced annotation for this entity class. We cre-
ated manual GGP annotation following the an-
notation guidelines of the GENIA GGP Corpus
(Ohta et al, 2009). As this corpus was the source
of the gene/protein entity annotation provided as
the basis of the BioNLP shared task on event ex-
traction, adopting its annotation criteria assures
compatibility with recently introduced event ex-
traction methods. Briefly, the guidelines spec-
ify tagging for minimal continuous spans of spe-
cific gene/gene product names, without differen-
tiating between DNA/RNA/protein. A ?specific
name? is understood to be a a name that allows
a domain expert to identify the entry in a rele-
vant database (Entrez gene/Uniprot) that the name
refers to. Only GGP names are tagged, excluding
descriptive references and the names of related en-
tities such as complexes, families and domains.
The annotation was created on the basis of an
initial tagging created by augmenting the output
of the BANNER tagger (Leaman and Gonzalez,
2008) by dictionary- and regular expression-based
tagging. This initial high-recall markup was then
corrected by a human annotator. To confirm that
the annotator had correctly identified subdomain
GGPs and to check against possible error intro-
duced through the machine-assisted tagging, we
performed a further verification of the annotation
on approx. 50% of the corpus sentences: we com-
bined the machine- and human-tagged annotations
as candidates, removed identifying information,
and asked two domain experts to identify the cor-
rect GGPs. The two sets of independently pro-
duced judgments showed very high agreement:
holding one set of judgments as the reference stan-
dard, the other would achieve an f-score of 97%
under the criteria presented in Section 4.2. We
note as one contributing factor to the high agree-
ment that the domain has stable and systematically
applied GGP naming criteria. The statistics of the
full GGP annotation are shown in Table 9.
136
Events Event/sentence
Abstracts 15 0.1
Full texts 5 0.01
Additional 80 2.2
Total 100 0.16
Table 10: Statistics for the event annotation.
3.3 Event Annotation
Motivated by the analysis described in Section 2.3,
we chose to focus on the novel category of asso-
ciations of GGP entities in high-level processes.
Specifically, we chose to study biological pro-
cesses related to virulence, as these are the most
frequent case in the corpus and prototypical of the
domain. We adopted the GENIA Event corpus an-
notation guidelines (Kim et al, 2008), marking as-
sociations between specific GGPs and biological
processes discussed in the text even when these
are stated speculatively or their existence explic-
itly denied. As the analysis indicated this category
of processes to typically involve a single stated
participant in a fixed role, annotations were ini-
tially recorded as (GGP, process) pairs and later
converted into an event representation.
During annotation, the number of annotated
GGP associations with the targeted class of pro-
cesses in the T4SS subcorpus was found to be too
low to provide material for both training and test-
ing a supervised learning-based event extraction
approach. To extend the source data, we searched
PubMed for cases where a known T4SS-related
protein co-occurred with an expression known to
relate to the targeted process class (e.g. virulence,
virulent, avirulent, non-virulent) and annotated a
further set of sentences from the search results for
both GGPs and their process associations. As the
properties of these additional examples could not
be assured to correspond to those of the targeted
domain texts, we used these annotations only as
development and training data, performing evalu-
ation on cases drawn from the T4SS subcorpus.
As the annotation target was novel, we per-
formed two independent sets of judgments for all
annotated cases, jointly resolving disagreements.
Although initial agreement was low, for a final set
of judgments we measured high agreement, corre-
sponding to 93% f-score when holding one set of
judgments as the gold standard. The statistics of
the annotation are shown in Table 10. Annotations
are sparse in the T4SS subcorpus and, as expected,
very dense in the targeted additional data.
4 Experiments
4.1 Methods
For GGP tagging experiments, we applied a state-
of-the-art tagger with default settings as reference
and a custom tagger for adaptation experiments.
As the reference tagger, we applied a recent re-
lease of BANNER (Leaman and Gonzalez, 2008)
trained on the GENETAG corpus (Tanabe et al,
2005). The corpus is tagged for gene and protein-
related entities and its texts drawn from a broad
selection of PubMed abstracts. The current revi-
sion of the tagger3 achieves an f-score of 86.4%
on the corpus, competitive with the best result re-
ported in the BioCreative II evaluation (Wilbur et
al., 2007), 87.2%. The custom tagger4 follows the
design of BANNER in both the choice of Con-
ditional Random Fields (Lafferty et al, 2001) as
the applied learning method and the basic feature
design, but as a key extension can further adopt
features from external dictionaries as both positive
and negative indicators of tagged entities. Tagging
experiments were performed using a document-
level 50/50 split of the GGP-annotated subcorpus.
For event extraction, we applied an adapta-
tion of the approach of the top-ranking system in
the BioNLP?09 shared task (Bjo?rne et al, 2009):
all sentences in the input text were parsed with
the McClosky-Charniak (2008) parser and the re-
sulting phrase structure analyses then converted
into the Stanford Dependency representation us-
ing conversion included in the Stanford NLP tools
(de Marneffe et al, 2006). Trigger recognition
was performed with a simple regular expression-
based tagger covering standard surface form vari-
ation. Edge detection was performed using a su-
pervised machine learning approach, applying the
LibSVM (Chang and Lin, 2001) Support Vector
Machine implementation with a linear kernel and
the feature representation of Bjo?rne et al (2009),
building largely around the shortest dependency
path connecting a detected trigger with a candi-
date participant. The SVM regularization parame-
ter was selected by a sparse search of the parame-
ter space with evaluation using cross-validation on
the training set. As the class of events targeted for
extraction in this study are of a highly restricted
type, each taking only of a single mandatory Cause
argument, the construction of events from detected
3http://banner.sourceforge.net
4http://www-tsujii.is.s.u-tokyo.ac.jp/
NERsuite/
137
Precision Recall F-score
Abstracts 68.1% 89.5% 77.3%
Full texts 56.9% 80.7% 66.7%
Total 59.4% 82.8% 69.2%
Table 11: Initial GGP tagging results.
triggers and edges could be implemented as a sim-
ple deterministic rule.
4.2 Evaluation Criteria
For evaluating the performance of the taggers we
apply a relaxed matching criterion that accepts a
match between an automatically tagged and a gold
standard entity if the two overlap at least in part.
This relaxation is adopted to focus on true tagging
errors. The GENETAG entity span guidelines dif-
fer from the GENIA GGP guidelines adopted here
in allowing the inclusion of e.g. head nouns when
names appear in modifier position, while the an-
notation guidelines applied here require marking
only the minimal name.5 When applying strict
matching criteria, a substantial number of errors
may trace back to minor boundary differences
(Wang et al, 2009), which we consider of sec-
ondary interest to spurious or missing tags. Over-
all results are microaverages, that is, precision, re-
call and f-score are calculated from the sum of true
positive etc. counts over individual documents.
For event extraction, we applied the BioNLP?09
shared task event extraction criteria (Kim et al,
2009) with one key change: to make it possible
to evaluate the extraction of the high-level pro-
cess participants, we removed the requirement that
all events must define a Theme as their core argu-
ment.
4.3 Gene/Gene Product Tagging
The initial GGP tagging results using BANNER
are shown in Table 11. We find that even for the
relaxed overlap matching criterion, the f-score is
nearly 10% points lower than reported on GENE-
TAG in the evaluation on abstracts. For full texts,
performance is lower yet by a further 10% points.
In both cases, the primary problem is the poor
precision of the tagger, indicating that many non-
GGPs are spuriously tagged.
To determine common sources of error, we per-
formed a manual analysis of 100 randomly se-
lected falsely tagged strings (Table 12). We find
5GENETAG annotations include e.g. human ets-1 protein,
whereas the guidelines applied here would require marking
only ets-1.
Category Freq Examples
GGP family or group 34% VirB, tmRNA genes
Figure/table 26% Fig. 1B, Table 1
Cell component 10% T4SS, ER vacuole
Species/strain 9% E. coli, A348deltaB4.5
Misc. 9% step D, Protocol S1
GGP domain or region 4% Pfam domain
(Other) 8% TrIP, LGT
Table 12: Common sources of false positives in
GGP tagging.
Precision Recall F-score
Abstracts 90.5% 95.7% 93.1%
Full texts 90.0% 93.2% 91.6%
Total 90.1% 93.8% 91.9%
Table 13: GGP tagging results with domain adap-
tation.
that the most frequent category consists of cases
that are arguably correct by GENETAG annota-
tion criteria, which allow named protein families
of groups to be tagged. A similar argument can
be made for domains or regions. Perhaps not sur-
prisingly, a large number of false positives relate
to features common in full texts but missing from
the abstracts on which the tagger was trained, such
as figure and table references. Finally, systematic
errors are made for entities belonging to other cat-
egories such as named cell components or species.
To address these issues, we applied a domain-
adapted custom tagger that largely replicates the
features of BANNER, further integrating infor-
mation from the UMLS Metathesaurus,6 which
provides a large dictionary containing terms cov-
ering 135 different semantic classes, and a cus-
tom dictionary of 1081 domain GGP names, com-
piled by (Ananiadou et al, 2010b). The non-GGP
UMLS Metathesaurus terms provided negative in-
dicators for reducing spurious taggings, and the
custom dictionary positive indicators. Finally, we
augmented the GENETAG training data with 10
copies7 of the training half of the T4SS GGP cor-
pus as in-domain training data.
Table 13 shows the results with the domain-
adapted tagger. We find dramatically improved
performance for both abstracts and full texts,
showing results competitive with the state of the
art performance on GENETAG (Wilbur et al,
2007). Thus, while the performance of an un-
adapted tagger falls short of both results reported
6http://www.nlm.nih.gov/research/umls/
7As the GENETAG corpus is considerably larger than the
T4SS GGP corpus, replication was used to assure that suffi-
cient weight is given to the in-domain data in training.
138
Precision Recall F-score
Co-occurrence 65% 100% 78%
Machine learning 81% 85% 83%
Table 14: Event extraction results.
on GENETAG and levels necessary for practi-
cal application, adaptation addressing common
sources of error through the adoption of general
and custom dictionaries and the use of a small
set of in-domain training data was successful in
addressing these issues. The performance of the
adapted tagger is notably high given the modest
size of the in-domain data, perhaps again reflect-
ing the consistent GGP naming conventions of the
subdomain.
4.4 Event Extraction
We performed an event extraction experiment fol-
lowing the training and test split described in Sec-
tion 3.3. Table 14 shows the results of the ap-
plied machine learning-based method contrasted
with a co-occurrence baseline replacing the edge
detection with a rule that extracts a Cause edge for
all trigger-GGP combinations co-occurring within
sentence scope. This approach achieves 100% re-
call as the test data was found to only contain
events where the arguments are stated in the same
sentence as the trigger.
The results show that the machine learning ap-
proach achieves very high performance, matching
the best results reported for any single event type
in the BioNLP?09 shared task (Kim et al, 2009).
The very high co-occurrence baseline result sug-
gests that the high performance largely reflects the
relative simplicity of the task. With respect to
the baseline result, the machine-learning approach
achieves a 21% relative reduction in error.
While this experiment is limited in both scope
and scale, it suggests that the event extraction ap-
proach can be beneficially applied to detect do-
main events represented by novel argument struc-
tures. As a demonstration of feasibility the result
is encouraging for both the applicability of event
extraction to this specific new domain and for the
adaptability of the approach to new domains in
general.
5 Discussion and Conclusions
We have presented a study of the adaptation of an
event extraction approach to the T4SS subdomain
as a step toward the introduction of event extrac-
tion to the broader infectious diseases domain. We
applied a previously introduced corpus of subdo-
main full texts annotated for mentions of bacte-
ria and terms from the three top-level Gene On-
tology subontologies as a reference defining do-
main information needs to study how these can
be met through the application of events defined
in the BioNLP?09 Shared Task on event extrac-
tion. Analysis indicated that with minor revision
of the arguments, the Binding and Localization
event types could account for the majority of both
biological processes and molecular functions of
interest. We further identified a category of ?high-
level? biological processes such as the virulence
process typical of the subdomain, which necessi-
tated extension of the considered event extraction
model.
Based on argument analysis, we proposed a rep-
resentation for high-level processes in the event
model that substitutes Cause for Theme as the
core argument. We further produced annotation
allowing an experiment on the extraction of the
dominant category of virulence processes with
gene/gene product (GGP) causes, annotating 518
GGP mentions and 100 associations between these
and the processes. Experiments indicated that with
annotated in-domain resources both the GGP enti-
ties and their associations with processes could be
extracted with high reliability.
In future work we will extend the model and
annotation proposed in this paper to the broader
infectious diseases domain, introducing annotated
resources and extraction methods for advanced in-
formation access. All annotated resources intro-
duced in this study are available from the GENIA
project homepage.8
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
the National Institutes of Health, grant number
HHSN272200900040C, and the Joint Information
Systems Committee (JISC, UK).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010a. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
139
Sophia Ananiadou, Dan Sullivan, Gina-Anne Levow,
Joseph Gillespie, Chunhong Mao, Sampo Pyysalo,
Jun?ichi Tsujii, and Bruno Sobral. 2010b. Named
entity recognition for bacterial type IV secretion sys-
tems. (manuscript in review).
M Ashburner, CA Ball, JA Blake, D Botstein, H But-
ler, JM Cherry, AP Davis, K Dolinski, SS Dwight,
JT Eppig, MA Harris, DP Hill, L Issel-Tarver,
A Kasarskis, S Lewis, JC Matese, JE Richardson,
M Ringwald, GM Rubin, and G Sherlock. 2000.
Gene ontology: tool for the unification of biology.
Nature genetics, 25:25?29.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Hong-Woo Chun, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dic-
tionaries and machine learning. In Proceedings of
the Pacific Symposium on Biocomputing (PSB?06),
pages 4?15.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Mario Juhas, Derrick W. Crook, and Derek W. Hood.
2008. Type IV secretion systems: tools of bacterial
horizontal gene transfer and virulence. Cellular mi-
crobiology, 10(12):2377?2386.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the Second BioCreative
PPI task: Automatic Extraction of Protein-Protein
Interactions. In L. Hirschman, M. Krallinger, and
A. Valencia, editors, Proceedings of Second BioCre-
ative Challenge Evaluation Workshop, pages 29?39.
John D. Lafferty, Andrew McCallum, and Fernando
C . N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML ?01: Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
R. Leaman and G. Gonzalez. 2008. Banner: an ex-
ecutable survey of advances in biomedical named
entity recognition. Pacific Symposium on Biocom-
puting, pages 652?663.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in
Logic - Genic Interaction Extraction Challenge. In
J. Cussens and C. Ne?dellec, editors, Proceedings
of the 4th Learning Language in Logic Workshop
(LLL05), pages 31?37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur. 2005. Genetag: a tagged
corpus for gene/protein named entity recognition.
BMC Bioinformatics, 6(Suppl 1):S3.
Paul Thompson, Syed Iqbal, John McNaught, and
Sophia Ananiadou. 2009. Construction of an anno-
tated corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10(1):349.
Yue Wang, Jin-Dong Kim, Rune Saetre, Sampo
Pyysalo, and Jun?ichi Tsujii. 2009. Investigat-
ing heterogeneous protein annotations toward cross-
corpora utilization. BMC Bioinformatics, 10(1):403.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, ed-
itors, Proceedings of Second BioCreative Challenge
Evaluation Workshop, pages 7?16.
140
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 69?77,
Uppsala, July 2010.
Evaluating a Meta-Knowledge Annotation Scheme for Bio-Events 
 
 
Raheel Nawaz1 Paul Thompson1,2 Sophia Ananiadou1,2 
1School of Computer Science, University of Manchester, UK 
2National Centre for Text Mining, University of Manchester, UK  
E-mail: nawazr@cs.man.ac.uk, paul.thompson@manchester.ac.uk, 
sophia.ananiadou@manchester.ac.uk 
 
  
 
Abstract 
The correct interpretation of biomedical texts 
by text mining systems requires the recogni-
tion of a range of types of high-level informa-
tion (or meta-knowledge) about the text. Ex-
amples include expressions of negation and 
speculation, as well as pragmatic/rhetorical in-
tent (e.g. whether the information expressed 
represents a hypothesis, generally accepted 
knowledge, new experimental knowledge, 
etc.) Although such types of information have 
previously been annotated at the text-span 
level (most commonly sentences), annotation 
at the level of the event is currently quite 
sparse. In this paper, we focus on the evalua-
tion of the multi-dimensional annotation 
scheme that we have developed specifically 
for enriching bio-events with meta-knowledge 
information. Our annotation scheme is in-
tended to be general enough to allow integra-
tion with different types of bio-event annota-
tion, whilst being detailed enough to capture 
important subtleties in the nature of the meta-
knowledge expressed in the text. To our 
knowledge, our scheme is unique within the 
field with regards to the diversity of meta-
knowledge aspects annotated for each event, 
whilst the evaluation results have confirmed 
its feasibility and soundness.  
1 Introduction 
The ability to recognise high-level information 
(or meta-knowledge) relating to the interpreta-
tion of texts is an important task for text mining 
systems. There are several types of meta-
knowledge that fall under this category. For ex-
ample, the detection of expressions of specula-
tion and negation is important across all do-
mains, although the way in which these phenom-
ena are expressed may be domain-specific. In 
scientific texts, it is also important to be able to 
determine other types of information, such as the 
author?s rhetorical/pragmatic intent (de Waard et 
al., 2009). This would correspond to whether the 
information expressed represents a hypothesis, 
accepted knowledge, new experimental knowl-
edge, etc.  
The ability to distinguish between these dif-
ferent types of information can be important for 
tasks such as  building and updating models of 
biological processes, like pathways (Oda et al, 
2008), and curation of biological databases 
(Ashburner et al, 2000). Central to both of these 
tasks is the identification of new knowledge that 
can enhance these resources, e.g. to build upon 
an existing, but incomplete model of a biological 
process (Lisacek et al, 2005) or to ensure that 
the database is kept up to date. Any new knowl-
edge added should be supported though evi-
dence, which could include linking hypotheses 
with experimental findings. It is also important to 
take into account inconsistencies and contradic-
tions reported in the literature. 
The production of annotated corpora can help 
to train text mining systems to recognise types of 
meta-knowledge, such as the above. Although a 
number of such corpora have already been pro-
duced, different annotation schemes are required 
according to the exact domain under considera-
tion, as well as the types of task that will be un-
dertaken by the text mining system.  
The work described in this paper is focused on 
the design and evaluation of the meta-knowledge 
annotation scheme described in Nawaz et al, 
(2010). The annotation scheme has been specifi-
cally designed to recognise a range of meta-
knowledge types for events extracted from bio-
medical texts (henceforth bio-events). The aim is 
to facilitate the development of more useful sys-
tems in the context of various biomedical infor-
mation extraction (IE) and textual inference (TI) 
tasks. Although the scheme has been designed 
69
for application to existing bio-event corpora, it is 
intended to be applied to any type of bio-relation 
corpora, and can easily be tailored for other types 
of relations/events within the domain. 
1.1  Bio-Event Representation of Text 
Searching for relevant information in electronic 
documents is most commonly carried out by en-
tering keywords into a search engine. However, 
such searches will normally return a huge num-
ber of documents, many of which will be irrele-
vant to the user?s needs.  
A more promising and efficient way of search-
ing is over events that have been extracted from 
texts through the application of natural language 
processing methods. An event is a structured rep-
resentation of a certain piece of information con-
tained within the text, which is usually anchored 
to a particular word in the text (typically a verb 
or noun) that is central to the description of the 
event. Events are often represented by a tem-
plate-like structure with slots that are filled by 
the event participants. Each event participant is 
also assigned a role within the event. These par-
ticipants can be entities, concepts or even other 
events. This kind of event representation allows 
the information contained in a text to be repre-
sented as a collection of nested events.  
A bio-event is an event specialised for the 
biomedical domain. Kim et al (2008) define a 
bio-event as a dynamic bio-relation involving 
one or more participants. These participants can 
be bio-entities or (other) bio-events, and are each 
assigned a semantic role/slot like theme and 
cause etc. Each bio-event is typically assigned a 
type/class from a chosen bio-event taxon-
omy/ontology, e.g., the GENIA Event Ontology 
(Kim et al, 2008). Similarly, the bio-entities are 
also assigned types/classes from a chosen bio-
term taxonomy/ontology, e.g., the Gene Ontol-
ogy (Ashburner et al, 2000). 
As an example, consider the simple sentence 
shown in Figure 1. 
This sentence contains a single bio-event, an-
chored to the verb activates. Figure 2 shows a 
typical structured representation of this bio-
event. 
The fact that the verb is anchored to the verb 
activates allows the event-type of positive regu-
lation to be assigned. The event has two slots, 
i.e. theme and cause whose labels help to charac-
terise the contribution that the slot filler makes 
towards the meaning of the event. In this case, 
the slots are filled by the subject and object of 
the verb activates, both of which correspond to 
different types of bio-entities (i.e. operon and 
protein).  
IE systems trained to extract bio-events from 
texts allow users to formulate semantic queries 
over the extracted events. Such queries can  
specify semantic restrictions on the events in 
terms of event types, semantic role labels and 
named entity types etc. (Miyao et al, 2006), in 
addition to particular keywords. For example, it 
would be possible to search only for those texts 
containing bio-events of type nega-
tive_regulation where the cause is an entity of 
type protein. Such queries provide a great deal 
more descriptive power than traditional keyword 
searches over unstructured documents.  Bio-
medical corpora that have been manually anno-
tated with event level information (e.g., Pyysalo 
et al, 2007; Kim et al, 2008; Thompson et al, 
2009) facilitate the training of systems such as 
those described above.  
Whilst event-based querying has advantages 
for efficient searching, the extracted events have 
little practical use if they are not accompanied by 
meta-knowledge information to aid in their inter-
pretation.  
1.2 Existing Meta-knowledge Annotation 
Various corpora of biomedical literature (ab-
stracts and/or full papers) have been produced 
that feature some degree of meta-knowledge an-
notation. These corpora vary in both the richness 
of the annotation added, and the type/size of the 
units at which the meta-knowledge annotation 
has been performed. Taking the unit of annota-
tion into account, we can distinguish between 
annotations that apply to continuous text-spans, 
and annotations that have been performed at the 
event level. 
Text-Span Annotation: Such annotations have 
mostly been carried out at the sentence level. 
They normally concentrate on a single aspect (or 
The results suggest that the narL gene product 
activates the nitrate reductase operon. 
 
Figure 1. A Simple Sentence from a Biomedi-
cal Abstract 
Figure 2. Typical Structured Representation 
of the Bio-Event mentioned in Figure 1 
EVENT-TRIGGER: activates 
EVENT-TYPE: positive_regulation 
THEME: nitrate reductase operon: operon 
CAUSE: narL gene product: protein 
 
70
dimension) of meta-knowledge, normally either 
speculation/certainty level, (e.g., Light et al, 
2004; Medlock & Briscoe, 2007; Vincze et al, 
2008) or general information content/rhetorical 
intent, e.g., background, methods, results, in-
sights. This latter type of annotation has been 
attempted both on abstracts, (e.g., McKnight & 
Srinivasan, 2003; Ruch et al, 2007) and full pa-
pers, (e.g. Teufel et al, 1999; Langer et al, 2004; 
Mizuta & Collier, 2004), with the number of dis-
tinct annotation categories varying between 4 
and 14.  
Despite the availability of these corpora, anno-
tation at the sentence level can often be too 
granular. In terms of information content, a sen-
tence may describe, for example, both an ex-
perimental method and its results. The situation 
becomes more complicated if a sentence contains 
an expression of speculation. If this is only 
marked at the sentence level, there may be con-
fusion about which part(s) of the sentence are 
affected by the speculative expression.  
Certain corpora and associated systems have 
attempted to address these issues. The BioScope 
corpus (Vincze et al, 2008) annotates the scopes 
of negative and speculative keywords, whilst 
Morante & Daelemans (2009) have trained a sys-
tem to undertake this task. The scheme described 
by Wilbur et al (2006) applies annotation to 
fragments of sentences, which are created on the 
basis of changes in the meta-knowledge ex-
pressed. The scheme consists of multiple annota-
tion dimensions which capture aspects of both 
certainty and rhetorical/pragmatic intent, 
amongst other things. Training a system to auto-
matically annotate these dimensions is shown to 
be highly feasible (Shatkay et al, 2008). 
Event-Level Annotation: Explicit annotation of 
meta-knowledge at the event-level is currently 
rather minimal within biomedical corpora. 
Whilst several corpora contain annotations to 
distinguish positive and negative events (e.g. 
Sanchez-Graillet & Poesio, 2007; Pyysalo et al, 
2007), the annotation of the GENIA Event Cor-
pus (Kim et al, 2008) is slightly more extensive, 
in that it additionally annotates certainty level. 
To our knowledge, no existing bio-event corpus 
has attempted annotation that concerns rhetori-
cal/pragmatic intent.  
 
1.3 The Need for an Event-Centric Meta-
Knowledge Annotation Scheme 
In comparison to meta-knowledge annotation 
carried out at the text-span level, the amount of 
annotation carried out at the event level is quite 
sparse. The question thus arises as to whether it 
is possible to use systems trained on text-span 
annotated corpora to assign meta-knowledge to 
bio-events, or whether new annotation at the 
event level is required.  
Some corpora seem better suited to this pur-
pose than others ? whilst sentence-level annota-
tions are certainly too granular for an event-
centric view of the text, sentence fragments, such 
as those identified by Wilbur et al (2006), are 
likely to correspond more closely to the extent of 
text that describes an event and its slots. Like-
wise, knowing the scopes of negative and specu-
lative keywords within a sentence may be a use-
ful aid in determining whether they affect the 
interpretation of a particular event.   
However, the information provided in these 
corpora is still not sufficiently precise for event-
level meta-knowledge annotation. Even within a 
text fragment, there may be several different bio-
events, each with slightly different meta-
knowledge interpretations. In a similar way, not 
all events that occur within the scope of a nega-
tion or speculation keyword are necessarily af-
fected by it.  
  Based on these observations, we have devel-
oped a meta-knowledge annotation scheme that 
is specifically tailored to bio-events. Our scheme 
annotates various different aspects or dimensions 
of meta-knowledge. A close examination of a 
large number of relevant bio-events has resulted 
in a scheme that has some similarities to previ-
ously proposed schemes, but has a number of 
differences that seem especially relevant when 
dealing with events, e.g. the annotation of the 
manner of the event. The scheme is intended to 
be general enough to allow integration with ex-
isting bio-event annotation schemes, whilst being 
detailed enough to capture important subtleties in 
the nature of the meta-knowledge expressed 
about the event.  
1.4 Lexical Markers of Meta-Knowledge 
Most of the existing corpora mentioned above 
annotate text spans or events with particular 
categories (e.g. certainty level or general infor-
mation type) in different meta-knowledge di-
mensions. However, what they do not normally 
do is to annotate lexical clues or keywords used 
to determine the correct values.  
A number of previous studies have demon-
strated the importance of lexical markers (i.e., 
words or phrases) that can accompany statements 
in scientific articles in determining the intended 
71
interpretation of the text (e.g. Hyland, 1996; Ri-
zomilioti 2006). We also performed a similar 
study (Thompson et al, 2008) although, in con-
trast to other studies, we took a multi-
dimensional approach to the categorisation of 
such lexical items, acknowledging that several 
types of important information may be expressed 
through different words in the same sentence. As 
an example, let us consider the example sentence 
in Figure 3.  
The author?s pragmatic/rhetorical intent to-
wards the statement that the catalytic role of 
these side chains is associated with their interac-
tion with the DNA substrate is encoded by the 
word indicate, which shows that the statement 
represents an analysis of the evidence stated at 
the beginning of the sentence, i.e., that the muta-
tions at positions 849 and 668 have DNA-
binding properties. Furthermore, the author?s 
certainty level (i.e., their degree of confidence) 
towards this analysis is shown by the word may. 
Here, the author is uncertain about the validity of 
their analysis. 
Whilst our previous work served to demon-
strate that the different aspects of meta-
knowledge that can be specified lexically within 
texts require a multi-dimensional analysis to cor-
rectly capture their subtleties, it showed that the 
presence of particular lexical items is not the 
only important feature for determining meta-
knowledge categories. In particular, their pres-
ence does not guarantee that the ?expected? in-
terpretation can be assumed (S?ndor, 2007). In 
addition, not all types of meta-knowledge are 
indicated through explicit markers. Mizuta & 
Collier (2004) note that  rhetorical zones may be 
indicated not only through explicit lexical mark-
ers, but also through features such as the main 
verb in the clause and the position of the sen-
tence within the article or abstract. 
For these reasons, we perform annotation on 
all relevant instances, regardless of the presence 
of lexical markers. This will allow systems to be 
trained that can learn to determine the correct 
meta-knowledge category, even when lexical 
markers are not present. However, due to the 
proven importance of lexical markers in deter-
mining certain meta-knowledge dimensions, our 
annotation scheme annotates such markers, 
whenever they are present. 
2 Annotation Scheme 
The annotation scheme we present here is a 
slightly modified version of our original meta-
knowledge annotation scheme (Nawaz et al, 
2010). The modified scheme consists of five 
meta-knowledge dimensions, each with a set of 
complete and mutually-exclusive categories, i.e., 
any given bio-event belongs to exactly one cate-
gory in each dimension. Our chosen set of anno-
tation dimensions has been motivated by the 
major information needs of biologists discussed 
earlier, i.e., the ability to distinguish between 
different intended interpretations of events. 
In order to minimise the annotation burden, 
the number of possible categories within each 
dimension has been kept as small as possible, 
whilst still respecting important distinctions in 
meta-knowledge that have been observed during 
our corpus study.     
The advantage of using a multi-dimensional 
scheme is that the interplay between different 
values of each dimension can reveal both subtle 
and substantial differences in the types of meta-
knowledge expressed in the surrounding text. 
Therefore, in most cases, the exact rhetori-
cal/pragmatic intent of an event can only be de-
termined by considering a combination of the 
values of different dimensions. This aspect of our 
scheme is further discussed in section 3. 
 
Figure 4 provides an overview of the annota-
tion scheme. The boxes with the light-coloured 
(grey) background correspond to information 
that is common to most bio-event annotation 
schemes, i.e., the participants in the event, to-
gether with an indication of the class or type of 
Figure 4. Bio-Event Annotation 
 
Figure 3. Example Sentence 
 
The DNA-binding properties of mutations at posi-
tions 849 and 668 may indicate that the catalytic 
role of these side chains is associated with their 
interaction with the DNA substrate. 
 
72
the event. The boxes with the darker (green) 
backgrounds correspond to our proposed meta-
knowledge annotation dimensions and their pos-
sible values. The remainder of this section pro-
vides brief details of each annotation dimension.  
2.1 Knowledge Type (KT) 
This dimension is responsible for capturing the 
general information content of the event. Whilst 
less detailed than some of the previously pro-
posed sentence-level schemes, its purpose is to 
form the basis of distinguishing between the 
most critical types of rhetorical/pragmatic intent, 
according to the needs of biologists. Each event 
is thus classified into one of the following four 
categories: 
Investigation: Enquiries or investigations, which 
have either already been conducted or are 
planned for the future, typically marked by lexi-
cal clues like examined, investigated and studied, 
etc.  
Observation: Direct observations, often repre-
sented by lexical clues like found, observed and 
report, etc.  Simple past tense sentences typically 
also describe observations. Such events represent 
experimental knowledge.  
Analysis: Inferences, interpretations, specula-
tions or other types of cognitive analysis, typi-
cally expressed by lexical clues like suggest, in-
dicate, therefore and conclude etc. Such events, 
if they are interpretations or reliable inferences 
based on experimental results, can also constitute 
another type of (indirect) experimental knowl-
edge. Weaker inferences or speculations, how-
ever, may be considered as hypotheses which 
need further proof through experiments.  
General: Scientific facts, processes, states or 
methodology. This is the default category for the 
knowledge type dimension. 
2.2 Certainty Level (CL) 
The value of this dimension is almost always 
indicated through the presence/absence of an ex-
plicit lexical marker. In scientific literature, it is 
normally only applicable to events whose KT 
corresponds either to Analysis or General. In the 
case of Analysis events, CL encodes confidence 
in the truth of the event, whilst for General 
events, there is a temporal aspect, to account for 
cases where a particular process is explicitly 
stated to occur most (but not all) of the time, us-
ing a marker such as normally, or only occasion-
ally, using a marker like sometimes.  Events cor-
responding to direct Observations are not open to 
judgements of certainty, nor are Investigation 
events, which refer to things which have not yet 
happened or have not been verified.  
Regarding the choice of values for the CL di-
mension, there is an ongoing discussion as to 
whether it is possible to partition the epistemic 
scale into discrete categories (Rubin, 2007). 
However, the use of a number of distinct catego-
ries is undoubtedly easier for annotation pur-
poses and has been proposed in a number of pre-
vious schemes. Although recent work has sug-
gested the use of  four or more categories (Shat-
kay et al, 2008; Thompson et al, 2008), our ini-
tial analysis of bio-event corpora has shown that 
only three levels of certainty seem readily distin-
guishable for bio-events. This is in line with 
Hoye (1997), whose analysis of general English 
showed that there are at least three articulated 
points on the epistemic scale.  
We have chosen to use numerical values for 
this dimension, in order to reduce potential anno-
tator confusions or biases that may be introduced 
through the use of labels corresponding to par-
ticular lexical markers of each category, such as 
probable or possible, and also to account for the 
fact that slightly different interpretations apply to 
the different levels, according to whether the 
event has a KT value of Analysis or General.  
L3: No expression of uncertainty or speculation 
(default category)  
L2: High confidence or slight speculation.  
L1: Low confidence or considerable speculation; 
typical lexical markers include may, might and 
perhaps.  
2.3 Source 
The source of experimental evidence provides 
important information for biologists. This is 
demonstrated by its annotation during the crea-
tion of the Gene Ontology (Ashburner et al, 
2000) and in the corpus created by Wilbur et al 
(2006). The Source dimension can also help in 
distinguishing new experimental knowledge 
from previously reported knowledge. Our 
scheme distinguishes two categories, namely: 
Other: The event is attributed to a previous 
study. In this case, explicit clues (citations or 
phrases like previous studies etc.) are normally 
present. 
Current: The event makes an assertion that can 
be (explicitly or implicitly) attributed to the cur-
rent study. This is the default category, and is 
assigned in the absence of explicit lexical or con-
textual clues. 
73
2.4 Polarity 
This dimension identifies negated events. Al-
though certain bio-event corpora are annotated 
with this information, it is still missing from oth-
ers. The indication of whether an event is ne-
gated is vital, as the interpretation of a negated 
event instance is completely opposite to the in-
terpretation of a non-negated (positive) instance 
of the same event.  
We define negation as the absence or non-
existence of an entity or a process. Negation is 
typically expressed by the adverbial not and the 
nominal no. However, other lexical devices like 
negative affixals (un- and in-, etc.), restrictive 
verbs (fail, lack, and unable, etc.), restrictive 
nouns (exception, etc.), certain adjectives (inde-
pendent, etc.), and certain adverbs (without, etc.) 
can also be used. 
2.5 Manner 
Events may be accompanied by a word or phrase 
which provides an indication of the rate, level, 
strength or intensity of the interaction. We refer 
to this as the Manner of the event. Information 
regarding manner is absent from the majority of 
existing bio-event corpora, but yet the presence 
of such words can be significant in the correct 
interpretation of the event. Our scheme distin-
guishes 3 categories of Manner, namely:  
High: Typically expressed by adverbs and adjec-
tives like strongly, rapidly and high, etc.  
Low: Typically expressed by adverbs and adjec-
tives like weakly, slightly and slow, etc.  
Neutral: Default category assigned to all events 
without an explicit indication of manner. 
3 Hyper-Dimensions 
Determining the pragmatic/rhetorical intent be-
hind an event is not completely possible using 
any one of our explicitly annotated dimensions. 
Although the Knowledge Type value forms the 
basis for this, it is not in itself sufficient. How-
ever, a defining feature of our annotation scheme 
is that additional information can be inferred by 
considering combinations of some of the explic-
itly annotated dimensions. We refer to this addi-
tional information as ?latent? or ?hyper? dimen-
sions of our scheme. We have identified two 
such hyper-dimensions. 
3.1 New Knowledge 
The isolation of events describing new knowl-
edge can be important in certain tasks undertaken 
by biologists, as explained earlier. Events with 
the Knowledge Type of Observation could corre-
spond to new knowledge, but only if they repre-
sent observations from the current study, rather 
than observations cited from elsewhere. In a 
similar way, an Analysis drawn from experimen-
tal results in the current study could be treated as 
new knowledge, but generally only if it repre-
sents a straightforward interpretation of results, 
rather than something more speculative.  
 Hence, we consider New Knowledge to be a 
hyper-dimension of our scheme. Its value (either 
Yes or No) is inferred by considering a combina-
tion of the value assignments for the KT, Source 
and CL dimensions.  
Table 1 shows the inference table that can be 
used to obtain the value for the New Knowledge 
hyper-dimension from the assigned values of the 
Source, KT and CL dimensions. The symbol ?X? 
indicates a ?don?t care condition?, meaning that 
this value does not have any impact on the result.  
 
Source 
(Annotated) 
KT 
(Annotated) 
CL 
(Annotated) 
New  
Knowledge 
(Inferred) 
Other X X No 
X X L2 No 
X X L1 No 
Current Observation L3 Yes 
Current Analysis L3 Yes 
X General X No 
X Investigation X No 
 
Table 1. Inference-Table for New Knowledge 
Hyper-Dimension 
 
3.2 Hypothesis 
A further hyper-dimension of our scheme is Hy-
pothesis. The binary value of this hyper-
dimension can be inferred by considering the 
values of KT and CL. Events with a KT value of 
Investigation can always be assumed to be a hy-
pothesis, However, if the KT value is Analysis, 
then only those events with a CL value of L1 or 
L2 (speculative inferences made on the basis of 
results) should be considered as hypothesis, to be 
matched with more definite experimental evi-
dence when available. A value of L3 in this in-
stance would normally be classed as new knowl-
edge, as explained in the previous section.   
Table 2 shows the inference table that can be 
used to get the value for the Hypothesis hyper-
dimension.  
 
74
KT 
(Annotated) 
CL 
(Annotated) 
Hypothesis 
(Inferred) 
General X No 
Observation X No 
Analysis L3 No 
Analysis L2 Yes 
Analysis L1 Yes 
Investigation X Yes 
 
Table 2. Inference-Table for Hypothesis 
Hyper-Dimension 
4 Evaluation 
The annotation scheme has been evaluated 
through a small annotation experiment. We ran-
domly choose 70 abstracts from the GENIA 
Pathway Corpus, which collectively contain over 
2600 annotated bio-events. Two of the authors 
independently annotated these bio-events using a 
set of annotation guidelines. These guidelines 
were developed following an analysis of the 
various bio-event corpora and the output of the 
initial case study (Nawaz et al, 2010). 
The highly favourable results of this experi-
ment further confirmed the feasibility and 
soundness of the annotation scheme. The re-
mainder of this section discusses the results in 
more detail. 
 
Dimension Cohen?s Kappa 
Knowledge Type 0.9017 
Certainty Level 0.9329 
Polarity 0.9059 
Manner 0.8944 
Source 0.9520 
Table 3. Inter-Annotator Agreement 
4.1 Inter-Annotator Agreement 
We have used the familiar measure of Cohen?s 
kappa (Cohen, 1960) for assessing the quality of 
annotation. Table 3 shows the kappa values for 
each annotated dimension. The highest value of 
kappa was achieved for the Source dimension, 
while the KT dimension yielded the lowest kappa 
value. Nevertheless, the kappa scores for all an-
notation dimensions were in the good region 
(Krippendorff, 1980).  
4.2 Category Distribution 
Knowledge Type:  The most prevalent category 
found in this dimension was Observation, with 
45% of all annotated events belonging to this 
category. Only a small fraction (4%) of these 
events was represented by an explicit lexical clue 
(mostly sensory verbs).  In most cases the tense, 
local context (position within the sentence) or 
global context (position within the document) 
were found to be important factors. 
The second most common category (37% of 
all annotated events) was General. We discov-
ered that most (64%) of the events belonging to 
this category were processes or states embedded 
in noun phrases (such as c-fos expression). More 
than a fifth of the General events (22%) ex-
pressed known scientific facts, whilst a smaller 
fraction (14%) expressed experimental/scientific 
methods (such as stimulation and incubation 
etc.). Explicit lexical clues were found only for 
facts, and even then in only 1% of cases. 
Analysis was the third most common category, 
comprising 16% of all annotated events. Of the 
events belonging to this category, 44% were de-
ductions (CL=L1), whilst the remaining 54% 
were hedged interpretations (CL=L2/L3). All 
Analysis events were marked with explicit lexical 
clues. 
The least common category was Investigation 
(1.5% of all annotated events). All Investigation 
events were marked with explicit lexical clues. 
Certainty Level: L3 was found to be the most 
prevalent category, corresponding to 93% of all 
events. The categories L2 and L1 occurred with 
frequencies of 4.3% and 2.5%, respectively. The 
relative scarcity of speculative sentences in sci-
entific literature is a well documented phenome-
non (Thompson et al, 2008; Vincze et al, 2008). 
Vincze et al (2008) found that less than 18% of 
sentences occurring in biomedical abstracts are 
speculative. Similarly, we found that around 20% 
of corpus events belong to speculative sentences. 
Since speculative sentences contain non-
speculative events as well, the frequency of 
speculative events is expected to be much less 
than the frequency of speculative sentences. In 
accordance with this hypothesis, we found that 
only 7% of corpus events were expressed with 
some degree of speculation. We also found that 
almost all speculated events had explicit lexical 
clues.  
Polarity:  Our event-centric view of negation 
showed just above 3% of the events to be ne-
gated. Similarly to speculation, the expected fre-
75
quency of negated events is lower than the fre-
quency of negated sentences. Another reason for 
finding fewer negated events is the fact that, in 
contrast to previous schemes, we draw a distinc-
tion between events that are negated and events 
expressed with Low manner. For example, cer-
tain words like limited and barely are often con-
sidered as negation clues. However, we consider 
them as clues for Low manner. In all cases, nega-
tion was expressed through explicit lexical clues. 
Manner: Whilst only a small fraction (4%) of 
events contains an indication of Manner, we 
found that where present, manner conveys vital 
information about the event. Our results also re-
vealed that indications of High manner are three 
times more frequent than the indications of Low 
manner. We also noted that both High and Low 
manners were always indicated through the use 
of explicit clues. 
Source: Most (99%) of the events were found to 
be of the Current category. This is to be ex-
pected, as authors tend to focus on current work 
in within abstracts. It is envisaged, however, that 
this dimension will be more useful for analyzing 
full papers. 
Hyper-dimensions: Using the inference tables 
shown in section 3, we calculated that almost 
57% of the events represent New Knowledge, and 
just above 8% represent Hypotheses.  
5 Conclusion and Future Work 
We have evaluated a slightly modified version of 
our meta-knowledge annotation scheme for bio-
events, first presented in Nawaz et al (2010). 
The scheme captures key information regarding 
the correct interpretation of bio-events, which is 
not currently annotated in existing bio-event cor-
pora, but which we have shown to be critical in a 
number of text mining tasks undertaken by bi-
ologists. The evaluation results have shown high 
inter-annotator agreement and a sufficient num-
ber of annotations along each category in every 
dimension. These results have served to confirm 
the feasibility and soundness of the annotation 
scheme, and provide promising prospects for its 
application to existing and new bio-event cor-
pora. 
We are currently working on a large scale an-
notation effort, involving multiple independent 
annotators. Although our main objective is to 
enrich the entire GENIA event corpus with meta-
knowledge information, we also plan to create a 
small corpus of full papers enriched with bio-
event and meta-knowledge annotations. 
Acknowledgments 
The work described in this paper has been 
funded by the Biotechnology and Biological Sci-
ences Research Council through grant numbers 
BBS/B/13640, BB/F006039/1 (ONDEX) 
References  
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. 
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. 
S. Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. 
Issel-Tarver, A. Kasarskis, S. Lewis, J. C. Matese, 
J. E. Richardson, M. Ringwald, G. M. Rubin and 
G. Sherlock.  2000. Gene ontology: tool for the 
unification of biology.  Nature Genetics 25:25-29. 
J. Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement 20: 37?46. 
A. de Waard, B. Shum, A. Carusi, J. Park, M. Sam-
wald and ?. S?ndor. 2009. Hypotheses, Evidence 
and Relationships: The HypER Approach for Rep-
resenting Scientific Knowledge Claims. In Pro-
ceedings of the Workshop on Semantic Web Appli-
cations in Scientific Discourse. Available at:  
http://oro.open.ac.uk/18563/ 
L. Hoye. 1997. Adverbs and Modality in English. 
London & New York: Longman 
K. Hyland. 1996. Talking to the Academy: Forms of 
Hedging in Science Research Articles. Written 
Communication 13(2):251-281. 
K. Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. London: Continuum 
J. Kim, T. Ohta and J. Tsujii. 2008. Corpus annotation 
for mining biomedical events from literature. BMC 
Bioinformatics 9:10 
K. Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Beverly Hills: Sage 
Publications 
H. Langer, H. Lungen and P. S. Bayerl. 2004. Text 
type structure and logical document structure. In 
Proceedings of the ACL Workshop on Discourse 
Annotation, pages 49-56 
M. Light, X. T. Qui and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and 
statements in between. In Proceedings of the Bio-
Link 2004 Workshop on Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users, 
pages 17-24. 
F. Lisacek, C. Chichester, A. Kaplan and A. Sandor. 
2005. Discovering Paradigm Shift Patterns in Bio-
medical Abstracts: Application to Neurodegenera-
tive Diseases. In Proceedings of SMBM 2005, 
pages 212-217 
76
L. McKnight and P. Srinivasan. 2003. Categorization 
of sentence types in medical abstracts. In Proceed-
ings of the 2003 Annual Symposium of AMIA, 
pages 440-444. 
B. Medlock and T. Briscoe. 2007. Weakly supervised 
learning for hedge classification in scientific litera-
ture. In Proceedings of ACL 2007, pages 992- 999. 
Y. Miyao, T. Ohta, K. Masuda, Y. Tsuruoka, K. Yo-
shida, T. Ninomiya and J. Tsujii. 2006. Semantic 
Retrieval for the Accurate Identification of Rela-
tional Concepts in Massive Textbases. In Proceed-
ings of COLING-ACL 2006, pages 1017-1024. 
Y. Mizuta and N. Collier. 2004. Zone identification in 
biology articles as a basis for information extrac-
tion. In Proceedings of the joint NLPBA/BioNLP 
Workshop on Natural Language for Biomedical 
Applications, pages 119-125. 
R. Morante and W. Daelemans. 2009. A metalearning 
approach to processing the scope of negation. In 
Proceedings of CoNLL 2009, pages 21-29. 
R. Nawaz, P. Thompson, J. McNaught and S. 
Ananiadou. 2010. Meta-Knowledge Annotation of 
Bio-Events. In Proceedings of LREC 2010, pages 
2498-2507. 
K. Oda, J. Kim, T. Ohta, D. Okanohara, T. Matsuzaki,  
Y. Tateisi and J. Tsujii. 2008. New challenges for 
text mining: mapping between text and manually 
curated pathways. BMC Bioinformatics 9(Suppl 3): 
S5. 
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjorne, J. 
Boberg, J. Jarvinen and T. Salakoski. 2007. BioIn-
fer: a corpus for information extraction in the bio-
medical domain. BMC Bioinformatics 8:50. 
V. Rizomilioti. 2006. "Exploring Epistemic Modality 
in Academic Discourse Using Corpora." Informa-
tion Technology in Languages for Specific Pur-
poses 7, pages 53-71 
V. L. Rubin. 2007. Stating with certainty or stating 
with doubt: Intercoder reliability results for manual 
annotation of epistemically modalized statements. 
In Proceedings of NAACL-HLT 2007, Companion 
Volume,  pages 141-144. 
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. 
Geissb?hler, P. Fabry, J. Gobeill, V. Pillet, D. 
Rebholz-Schuhmann and C. Lovis. 2007. Using 
argumentation to extract key sentences from bio-
medical abstracts. International Journal of Medical 
Informatics 76(2-3):195-200. 
O. Sanchez-Graillet and M. Poesio. 2007. Negation of 
protein-protein interactions: analysis and extrac-
tion. Bioinformatics 23(13):i424-i432 
?. S?ndor. 2007. Modeling metadiscourse conveying 
the author?s rhetorical strategy in biomedical re-
search abstracts. Revue Fran?aise de Linguistique 
Appliqu?e 200(2):97-109. 
H. Shatkay, F. Pan, A. Rzhetsky and W. J. Wilbur.  
2008. Multi-dimensional classification of biomedi-
cal text: toward automated, practical provision of 
high-utility text to diverse users. Bioinformatics 
24(18): 2086-2093. 
S. Teufel, J. Carletta and M. Moens. 1999. An annota-
tion scheme for discourse-level argumentation in 
research articles. In Proceedings of EACL 1999, 
pages  110-117. 
S. Teufel, A. Siddharthan and C. Batchelor. 2009. 
Towards discipline-independent argumentative 
zoning: Evidence from chemistry and computa-
tional linguistics. In Proceedings of EMNLP-09, 
pages 1493-1502 
P. Thompson, S. Iqbal, J. McNaught and S. 
Ananiadou. 2009. Construction of an annotated 
corpus to support biomedical information extrac-
tion. BMC Bioinformatics 10: 349. 
P. Thompson, G. Venturi, J. McNaught, S. Monte-
magni and S. Ananiadou. 2008. Categorising Mo-
dality in Biomedical Texts. In Proceedings of the 
LREC 2008 Workshop on Building and Evaluating 
Resources for Biomedical Text Mining, pages 27-
34. 
V. Vincze, G. Szarvas, R. Farkas, G. Mora and J. 
Csirik. 2008. The BioScope corpus: biomedical 
texts annotated for uncertainty, negation and their 
scopes. BMC Bioinformatics 9(Suppl 11): S9. 
W. J. Wilbur, A. Rzhetsky and H. Shatkay. 2006. 
New directions in biomedical text annotations: 
definitions, guidelines and corpus construction. 
BMC Bioinformatics 7: 356. 
 
77
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 83?91,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Building a Coreference-Annotated Corpus
from the Domain of Biochemistry
Riza Theresa Batista-Navarro1,2,3,? and Sophia Ananiadou1,2,??
1National Centre for Text Mining, University of Manchester, United Kingdom
2School of Computer Science, University of Manchester, United Kingdom
3Department of Computer Science, University of the Philippines Diliman, Philippines
?batistar@cs.man.ac.uk, ??sophia.ananiadou@manchester.ac.uk
Abstract
One of the reasons for which the resolution
of coreferences has remained a challenging
information extraction task, especially in the
biomedical domain, is the lack of training
data in the form of annotated corpora. In or-
der to address this issue, we developed the
HANAPIN corpus. It consists of full-text ar-
ticles from biochemistry literature, covering
entities of several semantic types: chemical
compounds, drug targets (e.g., proteins, en-
zymes, cell lines, pathogens), diseases, or-
ganisms and drug effects. All of the co-
referring expressions pertaining to these se-
mantic types were annotated based on the an-
notation scheme that we developed. We ob-
served four general types of coreferences in
the corpus: sortal, pronominal, abbreviation
and numerical. Using the MASI distance
metric, we obtained 84% in computing the
inter-annotator agreement in terms of Krip-
pendorff?s alpha. Consisting of 20 full-text,
open-access articles, the corpus will enable
other researchers to use it as a resource for
their own coreference resolution methodolo-
gies.
1 Introduction
Coreferences are linguistic expressions referring to
the same real-world entity (Jurafsky and Martin,
2009). The process of grouping all co-referring ex-
pressions in text into respective coreference chains is
known as coreference resolution. It was introduced
as one of the tasks of the sixth Message Understand-
ing Conference (MUC-6) in 1995 (Grishman and
Sundheim, 1995) and is one of the information ex-
traction tasks which have remained a challenge to
this day. One of the reasons it is still considered
an unresolved problem especially in the biomedical
domain is the lack of coreference-annotated corpora
which are needed for developing coreference resolu-
tion systems.
There exist only a handful of biomedical corpora
which are annotated with coreference information.
We have conducted a review of each of them, tak-
ing into consideration their sizes, document compo-
sition, domain, types of markable entities, types of
coreference annotated, availability, and reliability in
terms of inter-annotator agreement. Of these, only
two corpora have been used in coreference resolu-
tion systems developed outside the research group
that annotated them: MEDSTRACT (Castano et al,
2002), and the MEDCo1 corpus of abstracts which
was used by the different teams who participated
in the Coreference Supporting Task of the BioNLP
2011 Shared Task2. These two corpora are widely
used, despite the fact that they are composed only of
abstracts.
Previous studies have shown the advantages of
utilising full-text articles rather than abstracts in
information extraction systems (Shah et al, 2003;
Schumie et al, 2004; Cohen et al, 2010a). Further-
more, recent research on fact extraction (McIntosh
and Curran, 2009) has demonstrated the need for
processing full-text articles when identifying coref-
erent expressions pertaining to biomedical entities.
1http://nlp.i2r.a-star.edu.sg/medco.html
2http://sites.google.com/site/bionlpst/home/protein-gene-
coreference-task
83
However, coreference-annotated corpora composed
of full-text articles are not readily accessible. Cur-
rently, only the FlySlip corpus (Gasperin et al,
2007) is available for download. In this corpus,
only gene-related entities were considered for coref-
erence annotation. Thus, there is a need for devel-
oping full-text corpora with coreference annotations
for more semantic types. This is currently being ad-
dressed by the CRAFT project (Cohen et al, 2010b)
which seeks to develop a corpus of full-text articles
with coreference annotations for more types of en-
tities; it was not explicitly stated, however, exactly
which types are being covered. Similarly, we are
developing a corpus of full-text articles with corefer-
ence annotations, but to further the aim of covering
as many semantic types as possible, we selected a
domain that covers a variety of semantic concepts.
Research literature from this biochemistry subdo-
main, marine natural products chemistry, contains
references pertaining to chemical compounds, or-
ganisms, drug targets such as proteins, enzymes, nu-
cleic acids, tissues, cells, cell components, cell lines
and pathogens, drug effects, as well as diseases. We
cover a number of entity types with the intention of
providing more insight into how to disambiguate co-
referring expressions of different semantic types.
An annotation scheme was developed, taking into
consideration the coreference types which have been
observed from the corpus, namely: sortal, pronom-
inal, numerical and abbreviation. Three chemistry
graduates were employed to annotate the corpus. To
determine the reliability of the resulting annotations,
we measured inter-annotator agreement in terms of
Krippendorff?s alpha.
2 Related Work
Coreference is often associated with the phe-
nomenon of anaphora which is characterised by
an expression (called an anaphor) that points back
to an entity previously mentioned in the same dis-
course (called antecedent). Anaphora resolution
is the process of determining the antecedent of an
anaphor. While the output of anaphora resolution
is a set of anaphor-antecedent pairs, that of corefer-
ence resolution is a set of coreference chains which
can be treated as equivalence classes. Despite this
difference, an overlap between them may be ob-
served in several cases. Often, a number of anaphor-
antecedent pairs from a discourse are coreferential
or refer to the same entity in the same domain,
and may be placed in the same coreference chain.
For this reason, we also included in our review
of biomedical corpora those which were annotated
with anaphora information and refer to them hence-
forth as coreference-annotated corpora.
We determined the types of coreference anno-
tated in each corpus we have reviewed, adapting
Mitkov?s classification of anaphora (Mitkov et al,
2000) which is also applicable to coreference. Nom-
inal coreference is characterised by co-referring ex-
pressions pertaining to a noun. It is further divided
into pronominal coreference and sortal coreference
which use a pronoun and a lexical noun phrase,
respectively, as co-referring expressions. Unlike
nominal coreference, verbal coreference is char-
acterised by co-referring expressions pertaining to
verbs. Both nominal and verbal coreference can
be broadly categorised according to the kind of
relation as direct or indirect. In direct corefer-
ence, co-referring expressions are related by iden-
tity, synonymy or specialisation; in indirect corefer-
ence, they are related by associative relations such as
meronymy or holonymy for nouns, and troponymy
or entailment for verbs. Annotation of indirect
coreference is usually more challenging as it re-
quires more specialised domain knowledge.
Presently, there are five (5) different biomedical
corpora which are annotated with coreference in-
formation: MEDSTRACT (Castano et al, 2002),
MEDCo3, FlySlip (Gasperin et al, 2007), the Col-
orado Richly Annotated Full Text (CRAFT) cor-
pus (Cohen et al, 2010b) and DrugNerAr (Segura-
Bedmar et al, 2009).
The MEDCo corpus has two subsets, one consist-
ing of abstracts (which we shall refer to as MEDCo-
A) and another consisting of full papers (MEDCo-
B). The results of our review of all five corpora
are presented in Table 1. Included in the last row
(HANAPIN) are the attributes of the corpus that we
have developed for comparison with existing cor-
pora.
Three of them, MEDSTRACT, MEDCo and
DrugNerAr, adapted an annotation scheme similar
3http://nlp.i2r.a-star.edu.sg/medco.html
84
Ta
bl
e
1:
C
om
pa
ri
so
n
of
B
io
m
ed
ic
al
C
or
po
ra
w
it
h
C
or
ef
er
en
ce
A
nn
ot
at
io
ns
C
or
pu
s
S
ch
em
e
D
oc
um
en
t
D
om
ai
n/
C
or
ef
er
en
ce
A
va
il
ab
il
it
y
Fo
rm
at
R
el
ia
bi
li
ty
A
da
pt
ed
C
om
po
si
ti
on
M
ar
ka
bl
es
Ty
pe
s
M
E
D
S
T
R
A
C
T
M
U
C
C
S
10
0
ab
st
ra
ct
s
m
ol
ec
ul
ar
bi
ol
og
y/
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
un
kn
ow
n
U
M
L
S
ty
pe
s
av
ai
la
bl
e
M
E
D
C
o-
A
M
U
C
C
S
19
99
ab
st
ra
ct
s
hu
m
an
bl
oo
d
ce
ll
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
tr
an
sc
ri
pt
io
n
fa
ct
or
s/
av
ai
la
bl
e
83
%
on
15
ab
st
ra
ct
s
G
E
N
IA
Te
rm
O
nt
ol
og
y
ty
pe
s
M
E
D
C
o-
B
M
U
C
C
S
43
fu
ll
pa
pe
rs
hu
m
an
bl
oo
d
ce
ll
di
re
ct
no
m
in
al
cu
rr
en
tl
y
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
tr
an
sc
ri
pt
io
n
fa
ct
or
s/
un
av
ai
la
bl
e
80
.7
%
on
2
fu
ll
pa
pe
rs
G
E
N
IA
Te
rm
O
nt
ol
og
y
ty
pe
s
F
ly
S
li
p
do
m
ai
n-
5
fu
ll
pa
pe
rs
fr
ui
tfl
y
ge
no
m
ic
s/
di
re
ct
an
d
pu
bl
ic
ly
X
M
L
K
ap
pa
sc
or
e:
sp
ec
ifi
c
ge
ne
ti
c
en
ti
ti
es
in
di
re
ct
av
ai
la
bl
e
gr
ea
te
r
th
an
83
%
so
rt
al
on
ea
ch
pa
pe
r
C
R
A
F
T
O
nt
oN
ot
es
97
fu
ll
pa
pe
rs
m
ou
se
ge
no
m
ic
s/
di
re
ct
no
m
in
al
cu
rr
en
tl
y
S
G
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
al
le
nc
ou
nt
er
ed
an
d
ve
rb
al
an
d
un
av
ai
la
bl
e
61
.9
%
on
10
fu
ll
pa
pe
rs
D
ru
gN
er
A
r
M
U
C
C
S
49
D
ru
gB
an
k
dr
ug
-d
ru
g
in
te
ra
ct
io
ns
/
di
re
ct
no
m
in
al
pu
bl
ic
ly
X
M
L
un
kn
ow
n
te
xt
s
dr
ug
s
av
ai
la
bl
e
H
A
N
A
P
IN
M
E
D
C
o
20
fu
ll
pa
pe
rs
m
ar
in
e
na
tu
ra
l
di
re
ct
no
m
in
al
,
cu
rr
en
tl
y
X
M
L
K
ri
pp
en
do
rf
f?
s
al
ph
a:
pr
od
uc
ts
ch
em
is
tr
y/
nu
m
er
ic
al
&
un
av
ai
la
bl
e
75
%
av
er
ag
ed
ch
em
ic
al
co
m
po
un
ds
,
ab
br
ev
ia
ti
on
(t
o
be
re
le
as
ed
ov
er
20
pa
pe
rs
;
or
ga
ni
sm
s,
dr
ug
pu
bl
ic
ly
)
84
%
us
in
g
th
e
M
A
S
I
ta
rg
et
s,
dr
ug
di
st
an
ce
m
et
ri
c
ef
fe
ct
s,
di
se
as
es
85
to that of the Message Understanding Conference
scheme or MUCCS (Hirschman, 1997). Using the
Standard Generalized Markup Language (SGML) as
annotation format, MUCCS creates a link between
co-referring expressions by setting the value of an
attribute of the referring element to the ID of the ref-
erent.
The same mechanism is used in the annotation
of MEDSTRACT, MEDCo and DrugNerAr, but
with respective extensions to account for more spe-
cific relations (e.g., appositive relation in the case
of MEDCo). On the contrary, rather than link-
ing the referring expression to its referent, an an-
notator explicitly places co-referring expressions in
the same coreference chain with OntoNotes, the
scheme adapted in annotating the CRAFT corpus.
FlySlip can be considered unique in terms of its
annotation scheme as it adapted a domain-specific
scheme which was necessary since indirect corefer-
ences were annotated. All corpora are available in
the form of a mark-up language (SGML or XML).
The five corpora can be grouped into three accord-
ing to general domain: molecular biology (MED-
STRACT and MEDCo), genomics (FlySlip and
CRAFT), and pharmacology (DrugNerAr). MED-
STRACT and MEDCo both have coreference an-
notations for semantic types from the UMLS and
the GENIA ontology, respectively, which can be
broadly categorised into compound, organism, pro-
tein, gene and cell. Each of the FlySlip and
DrugNerAr corpora, on the other hand, have anno-
tations for only one general semantic type: gene-
related entities and drugs, respectively. CRAFT is
unique in this respect as its developers seek to anno-
tate all co-referring expressions regardless of seman-
tic type; the semantic types that have been encoun-
tered so far have not yet been reported, however.
In terms of coreference types for which annota-
tions have been added, CRAFT is the only corpus
with annotations for verbal coreference; all the rest
have annotations only for pronominal and/or sortal
coreference. With respect to coreference types ac-
cording to relation, FlySlip is the only corpus with
annotations for indirect coreference.
MEDCo-B, FlySlip and CRAFT are three exist-
ing corpora which are comprised of full-text arti-
cles. Among them, only FlySlip is currently publicly
available.
The corpus that we have developed, which we call
the HANAPIN corpus, is also intended for public
release in the near future and covers five general
semantic types. In the annotation scheme which
was designed and used in HANAPIN, two addi-
tional coreference types were considered: abbrevi-
ations and numerical coreferences which are com-
monly used in chemistry research literature. These
coreference types and the annotation scheme are fur-
ther described in the succeeding section.
3 Methodology
3.1 Composition of Corpus Documents
Taking into consideration that the corpus should
consist of full-text articles which can be distributed
to the public, we gathered full-text articles from the
journal Marine Drugs4 which is under the PubMed
Central Open Access subset5. The said journal cov-
ers subject areas such as marine natural products,
medicine analysis, marine pharmacology, pharma-
ceutical biology, marine drugs development and ma-
rine biotechnology, among many others. From all
of its articles from 2003 to 2009, we randomly se-
lected twenty (20) which seemed to be a reason-
able size considering that only five months were al-
located for the annotation of the corpus, and that
a previous study on biomedical corpora (Cohen et
al., 2005) has shown that a corpus can possibly be
widely used despite its small size. The experimen-
tal sections of the articles were not annotated as
they contain very detailed descriptions of the meth-
ods carried out by the authors; according to a study
(Shah et al, 2003), these usually contain technical
data, instruments and measurements ? types of in-
formation which are currently not of much interest
to researchers doing biomedical information extrac-
tion, although they may be in the future. The corpus
contains a total of 1,027 sentences or 27, 358 words.
3.2 Coreference Types
The coreferences observed in the corpus were cat-
egorised into four general nominal types: pronom-
inal, sortal, numerical and abbreviation. Table 2
presents the subtypes of sortal and pronominal
coreference, as well as examples for all types. We
4http://www.mdpi.com/journal/marinedrugs
5http://www.ncbi.nlm.nih.gov/pmc/about/openftlist.html
86
Table 2: Coreference Types with Examples
General Coreference Type Subtype Examples
pronominal
demonstrative this, that, these, those
personal it, they, its, their, theirs
indefinite another, few, other, some, all, any
distributive both, such, each, either, neither
relative which, that, whose
sortal
definite the loihichelins
indefinite an alkaloid, a mycalamide
demonstrative this metabolite, these compounds
distributive both compounds
predicate nominative ?Galactans are polysaccharides...?
appositive ?Radiosumin, an N-methyl dipeptide...?
numerical
N.A. ?The structures of 1 and 2...?
?Compounds 1-3 inhibit...?
abbreviation
N.A. ?...as a membrane type 1 matrix
metalloproteinase (MT1-MMP) inhibitor.
Compound 1 inhibited MT1-MMP with...?
have decided not to take into account verbal and in-
direct coreferences; only nominal and direct coref-
erences have been considered for the first release of
the corpus.
3.2.1 Pronominal Coreference
This type of coreference is characterised by a pro-
noun referring to a noun phrase. The pronoun is used
as a substitute to a noun. We have further identified
the following subtypes of pronominal coreference:
demonstrative, personal, indefinite, distributive and
relative.
3.2.2 Sortal Coreference
Also referred to as lexical noun phrase corefer-
ence, sortal coreference is characterised by a noun
phrase consisting of a head noun and its modifiers.
The subtypes of sortal coreference which have been
identified include: definite, indefinite, demonstra-
tive, distributive, predicate nominative and apposi-
tive.
3.2.3 Numerical Coreference
In chemistry research literature, a number is con-
ventionally used to refer to a chemical entity which
was introduced using the same number. Oftentimes,
a range of numbers is also used to refer to a number
of compounds previously mentioned.
3.2.4 Abbreviation
In annotating the HANAPIN corpus, abbrevia-
tions were also considered as co-referring expres-
sions. We distinguish them from the other corefer-
ence types to make the corpus of benefit to develop-
ers of abbreviation identification algorithms as well.
3.3 Annotation Scheme and Procedure
The annotation scheme used in MEDCo (which was
based on MUCCS) was adapted and modified for
the annotation of the HANAPIN corpus. We have
selected the MEDCo scheme as it already differen-
tiates between the pronominal and identity (equiva-
lent to sortal) types, whereas MUCCS has only the
identity type. There was a need, however, to extend
the MEDCo scheme to further specialise the corefer-
ence types. The XML Concordancer (XConc) tool6
was used in annotating the corpus. Configuring the
said tool for our needs is straightforward as it only
involved the customisation of a Document Type Def-
inition (DTD) file.
3.3.1 Term Annotations
As a preliminary step, the scheme required that
all terms which can be categorised into any of the
6http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?
page=XConc+Suite
87
Figure 1: Sample annotations as shown in the XConc annotation tool. The sentences in this example come from one
of the documents in the HANAPIN corpus, the Marine Drugs article with PubMed ID 19841723. For illustrative
purposes, the first sentence in the example was slightly modified to demonstrate the use of the cons element.
following semantic types be annotated:
1. chemical compound
2. organism
3. drug effect
4. disease
5. drug target (further categorised into: protein,
enzyme, nucleic acid, tissue, cell, cell compo-
nent, cell line, pathogen)
For each markable, the annotator creates a term
element which is assigned an ID and one of the se-
mantic types above. The scheme supports the anno-
tation of embedded terms, as well as terms in a dis-
continuous text region. The former entails placing
a term element within another. The latter is done
by dividing the discontinuous text into fragments
and annotating each fragment in the same manner
as an ordinary term element. The fragment elements
are then grouped together as a constituent element
(cons). Figure 1 presents a sample annotation of
a discontinuous term (constituent C5) as viewed in
XConc.
3.3.2 Co-referring Expressions
An annotator proceeds to the annotation of co-
referring expressions after annotating all terms
within a document. If an expression was found to
be co-referring with another term, the annotator as-
signs the ID of the latter as the value of the idref
attribute of the former. If the referring expression,
however, is a noun phrase and not a term that was
previously annotated during term annotation, it is
marked as a ref element and then linked to its ref-
erent. Annotators delimit these expressions by in-
cluding the necessary modifiers of the co-referring
element (e.g., the new jaspamide derivatives instead
of just jaspamide derivatives). A coreference type
which could be any of pronominal, numerical, ab-
breviation, and sortal (further categorised into def-
inite, indefinite, demonstrative, distributive, predi-
cate nominative and appositive) is also assigned as
the value of the type attribute of each link created.
We decided not to further divide pronominal coref-
erence into its subtypes as it became apparent dur-
ing the annotation dry runs that there is only a hand-
ful of pronominal coreferences. Figure 1 shows co-
referring expressions (connected by arrows) linked
by the mechanism just described.
Listed below are some of the main points of the
annotation guidelines:
1. A referring expression may be linked to multi-
ple referents.
2. The more specific one between two co-
referring expressions is considered as the ref-
erent. This means that there might be cases
when the referent occurs later than the refer-
ring expression. For example, R30:the new
natural products is the co-referring ex-
pression and C5:jaspamide Q and R is
the referent in Figure 1.
3. In cases where there are multiple choices for
the referent of a referring expression, the clos-
est one may be chosen as long as it is (or will
be) linked to the other choice expressions.
4. There are cases when more than one type of
coreference applies. For example, in Figure 1,
the new natural products is both an appositive
and a definite noun phrase. In such cases, the
appositive and predicate nominative types take
precedence over the other sortal types.
88
Figure 2: XML code generated by XConc for the sample annotations in Figure 1.
One could process the XML code (provided in
Figure 2 for the reader?s reference) to obtain the fol-
lowing coreference chains:
1. {R30:the new natural products,
C5:jaspamide Q and R, R10:the
new jaspamide derivatives,
R11:which, R12:both}
2. {T66:jaspamide Q, R34:2}
3. {T67:jaspamide R, R35:3}
4. {T70:jaspamide, R36:1}
The complete annotation guidelines will be pub-
licly released together with the annotated corpus.
4 Results
The three annotators were asked to complete the
coreference annotations within five months. A bi-
weekly meeting was held to address questions and
issues which could not be addressed or resolved by
means of the online project forum.
4.1 Statistics
As the HANAPIN corpus is the first of its kind from
the biochemistry domain and aims to cover several
semantic as well as coreference types, it is of interest
to determine which of the types are most prevalent.
To do this we computed statistics over the annota-
tions (Figure 3). For each type, we obtained the av-
erage over the annotations from the three coders.
There is a total of 395 coreference chains (not
including singleton chains or those with only one
mention) in the entire corpus. The coreference
chains are of the following semantic types: chemical
compounds (70.89%), drug targets (12.66% that ac-
counts for proteins, cell lines, pathogens, enzymes,
cells, cell parts, nucleic acids and tissues), organ-
isms (9.87%), drug effects (3.29%), and diseases
(3.29%). Among the drug targets, the most preva-
lent are proteins, cell lines and pathogens.
A total of 760 coreference links have been found
in the corpus. The most common among the types
is the numerical one (46%), followed by the sortal
type (33% that accounts for the definite, indefinite,
demonstrative, appositive, predicate nominative and
distributive types). Less common are the pronomi-
nal type (11%) and abbreviation (10%). Among the
sortal coreferences, the most common are the def-
inite and indefinite types, followed by the demon-
strative type.
89
Sheet5
Page 1
280
drug target (50) 50
organism (39) 39
drug effect (13) 13
disease (13) 13
395
numerical (352) 352
pronominal (83) 83
abbreviation (74) 74
definite (64) 64
indefinite (58) 58
demonstrative (42) 42
appositive (31) 31
pred. nom. (28) 28
distributive (28) 28
760
chem (280)
Semantic Types
chem (280)
drug target 
(50)
organism (39)
drug effect 
(13)
disease (13)
Coreference Types
numerical 
(352)
pronominal 
(83)
abbreviation 
(74)
definite (64)
indefinite (58)
demonstrative 
(42)
appositive 
(31)
pred. nom. 
(28)
distributive 
(28)
Figure 3: Distribution of semantic and coreference types in the HANAPIN corpus.
4.2 Corpus Reliability
Following Passoneau?s proposed method for com-
puting reliability for coreference annotation (Pas-
soneau, 2004), we computed for the reliability of
the corpus in terms of Krippendorff?s alpha, a co-
efficient of agreement that allows for partial dis-
agreement with the use of a distance metric based
on the similarity between coreference chains. Pas-
soneau?s first proposed distance metric (dP ) assigns
0 for identity, 0.33 for subsumption, 0.67 for inter-
section and 1 for disjunction. There are, however,
alternative distance metrics that consider the sizes
of the coreference chains, such as Jaccard?s coeffi-
cient of community (dJ ) and Dice?s coincidence in-
dex (dD) which can be computed as follows (Art-
stein and Peosio, 2004):
dJ = 1?
|A ?B|
|A ?B|
dD = 1?
2|A ?B|
|A|+ |B|
A new distance metric called Measuring Agree-
ment on Set-valued Items (MASI) was then later
proposed by Passoneau. It is obtained by getting the
product of the original distance metric dP and Jac-
card?s coefficient dJ .
Initially using Passoneau?s first proposed distance
metric dP in computing for Krippendorff?s alpha,
we obtained an average of 75% over all documents
in the HANAPIN corpus. Computing for alpha us-
ing the MASI distance metric gives 84%. Though
there is no value of alpha that has been established
to be an absolute indication of high agreement, pre-
vious works cited by Krippendorff have shown that
values of alpha less than 67% indicate unreliability
(Krippendorff, 1980). We can therefore regard the
obtained values of alpha as satisfactory.
5 Conclusion and Future Work
A coreference-annotated corpus from the domain
of biochemistry, consisting of full-text articles, has
been developed. It was annotated following guide-
lines which covered coreference and semantic types
that have not been covered in other biomedical cor-
pora before. This was done to further the aim of pro-
viding researchers with more insight into the phe-
nomenon of coreference in a cross-disciplinary do-
main. Results show that in this biochemistry do-
main, the most common types of coreference being
used by authors are the numerical and sortal types.
Verbal and indirect coreferences, however, have not
been considered at this stage; the annotation of these
types can be explored as part of future work on the
corpus.
To measure reliability of the corpus, we deter-
mined inter-annotator agreement on all documents
by computing for the value of Krippendorff?s al-
pha. Using Passoneau?s first proposed distance met-
ric and the MASI distance metric, we obtained sat-
isfactory values of 75% and 84%, respectively. The
corpus and annotation guidelines will be released to
the public to encourage and enable more researchers
to develop improved biomedical coreference resolu-
90
tion methodologies.
Acknowledgements
The UK National Centre for Text Mining is funded
by the UK Joint Information Systems Committee
(JISC). The authors would also like to acknowledge
the Office of the Chancellor, in collaboration with
the Office of the Vice-Chancellor for Research and
Development, of the University of the Philippines
Diliman for funding support through the Outright
Research Grant.
The authors also thank Paul Thompson for his
feedback on the annotation guidelines, and the
anonymous reviewers for their helpful comments.
References
Ron Artstein and Massimo Poesio. 2004. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555-596.
Jose? Castan?o, Jason Zhang and James Pustejovsky. 2002.
Anaphora resolution in biomedical literature. Pro-
ceedings of the International Symposium on Reference
Resolution for NLP.
K. Bretonnel Cohen, Philip V. Ogren, Lynne Fox and
Lawrence E. Hunter. 2005. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. AMIA Annual Symposium Proceedings, pages
156-160.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, Lawrence E. Hunter. 2010. The
structural and content aspects of abstracts versus bod-
ies of full text journal articles are different. BMC
Bioinformatics, 11(1):492.
K. Bretonnel Cohen, Arrick Lanfranchi, William Cor-
vey, William A. Baumgartner Jr., Christophe Roeder,
Philip V. Ogren, Martha Palmer and Lawrence E.
Hunter. 2010. Annotation of all coreference in
biomedical text: Guideline selection and adaptation.
Proceedings of the Second Workshop on Building
and Evaluating Resources for Biomedical Text Mining
(BioTxtM 2010), LREC 2010.
Caroline Gasperin, Nikiforos Karamanis and Ruth Seal.
2007. Annotation of anaphoric relations in biomedical
full-text articles using a domain-relevant scheme. Pro-
ceedings of the 6th Discourse Anaphora and Anaphor
Resolution Colloquium (DAARC 2007).
Ralph Grishman and Beth Sundheim. 1995. Design of
the MUC-6 Evaluation. MUC ?95: Proceedings of the
6th Message Understanding Conference, pages 1-11.
Lynette Hirschman. 1997. MUC-7 Coreference Task
Definition. Message Understanding Conference 7
Proceedings.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice-Hall, 2nd edition.
Klaus H. Krippendorff. 1980. Content Analysis: An
Introduction to Its Methodology. Beverly Hills, CA:
Sage Publications.
Tara McIntosh and James R.Curran. 2009. Chal-
lenges for automatically extracting molecular inter-
actions from full-text articles. BMC Bioinformatics,
10(1):311.
Ruslan Mitkov, Richard Evans, Constantin Orasan,
Catalina Barbu, Lisa Jones and Violeta Sotirova. 2005.
Coreference and anaphora: developing annotating
tools, annotated resources and annotation strategies.
Proceedings of the Discourse Anaphora and Anaphora
Resolution Colloquium (DAARC 2000), pages 49-58.
Rebecca J. Passoneau. 2004. Computing reliability for
coreference annotation. Proceedings of the Interna-
tional Conference on Language Resouces (LREC).
M. Schumie, M. Weeber, B. Schijvenaars, E. van Mul-
ligen, C. van der Eijk, R. Jelier, B. Mons and J.
Kors. 2004. Distribution of information in biomedi-
cal abstracts and full-text publications. Bioinformat-
ics, 20(16):2597-2604.
Isabel Segura-Bedmar, Mario Crespo, Ce?sar de Pablo-
Sa?nchez and Paloma Mart??nez. 2009. Resolving
anaphoras for the extraction of drug- drug interactions
in pharmacological documents. BMC Bioinformatics,
11(Suppl 2):S1.
Parantu K. Shah, Carolina Perez-Iratxeta, Peer Bork and
Miguel A. Andrade. 2003. Information extraction
from full text scientific articles: Where are the key-
words? BMC Bioinformatics, 4(1): 20.
91
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 44?53,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Enrichment and Structuring of Archival Description Metadata
Kalliopi Zervanou?, Ioannis Korkontzelos?, Antal van den Bosch? and Sophia Ananiadou?
? Tilburg centre for Cognition and Communication (TiCC), University of Tilburg
Warandelaan 2 - PO Box 90153, 5000 LE Tilburg, The Netherlands
{K.Zervanou, Antal.vdnBosch}@uvt.nl
? National Centre for Text Mining, University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{Ioannis.Korkontzelos, Sophia.Ananiadou}@manchester.ac.uk
Abstract
Cultural heritage institutions are making their
digital content available and searchable on-
line. Digital metadata descriptions play an im-
portant role in this endeavour. This metadata
is mostly manually created and often lacks de-
tailed annotation, consistency and, most im-
portantly, explicit semantic content descrip-
tors which would facilitate online browsing
and exploration of available information. This
paper proposes the enrichment of existing
cultural heritage metadata with automatically
generated semantic content descriptors. In
particular, it is concerned with metadata en-
coding archival descriptions (EAD) and pro-
poses to use automatic term recognition and
term clustering techniques for knowledge ac-
quisition and content-based document classi-
fication purposes.
1 Introduction
The advent of the digital age has long changed the
processes and the media which cultural heritage in-
stitutions (such as libraries, archives and museums)
apply for describing and cataloguing their objects:
electronic cataloguing systems support classification
and search, while cultural heritage objects are asso-
ciated to digital metadata content descriptions. The
expansion of the web and the increasing engagement
of web users throughout the world has brought about
the need for cultural heritage institutions to make
their content available and accessible to a wider au-
dience online.
In this endeavour, cultural heritage institutions
face numerous challenges. In terms of metadata,
different metadata standards currently exist for de-
scribing various types of objects, both within the
same institution and across different institutions.
Moreover, metadata object descriptions have been
typically both created by and addressed to librar-
ian and archivist experts who have been expected
to assist visitors in their search. For this reason,
they primarily refer to bibliographic descriptions
(e.g. author/creator, title, etc.), or physical descrip-
tions (e.g. size, shape, material, etc.), and location.
The lack of semantic descriptors in this type of meta-
data makes it difficult for potential online visitors to
browse and explore available information based on
more intuitive content criteria.
Work on metadata in cultural heritage institu-
tions has been largely focused on the issue of meta-
data heterogeneity. There have been efforts towards
the development and adoption of collection-specific
metadata standards, such as MARC 21 (Library of
Congress, 2010) and EAD (Library of Congress,
2002), for library and archival material respectively,
which are intended to standardise metadata descrip-
tions across different institutions. To address the is-
sue of heterogeneity across different types of object
collections, generic metadata schemas have been
proposed, such as the Dublin Core Metadata Initia-
tive (DCMI, 2011). Moreover, current research has
attempted to integrate diverse metadata schemas by
mappings across existing schemas (Bountouri and
Gergatsoulis, 2009), or mappings of existing meta-
data to ontologies, either based on ad-hoc manually
developed ontologies (Liao et al, 2010), or on ex-
isting standard ontologies for cultural heritage pur-
poses (Lourdi et al, 2009), such as the CIDOC Con-
44
ceptual Reference Model (CIDOC, 2006). Other
approaches attempt to address the issue of meta-
data heterogeneity from a pure information retrieval
perspective and discard the diverse metadata struc-
tures in favour of the respective text content descrip-
tions for full text indexing (Koolen et al, 2007).
Zhang and Kamps (2009) attempt to exploit the ex-
isting metadata XML structure for XML-based re-
trieval, thus targeting individual document compo-
nents. Similarly to our approach, they investigate
metadata describing archive collections.
The work presented in this paper focuses on meta-
data for textual objects, such as archive documents,
and on the issue of explicit, semantic, content de-
scriptors in this metadata, rather than heterogene-
ity. In particular, we are concerned with the lack
of explicit content descriptors which would support
exploratory information search. For this purpose,
we attempt to automatically enrich manually cre-
ated metadata with content information. We view
the problem from an unsupervised, text mining per-
spective, whereby multi-word terms recognised in
free text are assumed to indicate content. In turn,
the respective inter-relationships among the recog-
nised terms in the hierarchy are assumed to reveal
the knowledge structure of the document collection.
In this paper, we start with a description of our
EAD dataset and the challenges which our dataset
poses in text processing. Subsequently, we discuss
our approach to the enrichment and structuring of
these archival descriptions and present our experi-
ments. We conclude with a discussion on our results
and our considerations for future work.
2 EAD and Challenges in Text Processing
The Encoded Archival Description (EAD) was con-
ceived as ?a nonproprietary encoding standard for
machine-readable finding aids such as inventories,
registers, indexes, and other documents created by
archives, libraries, museums, and manuscript repos-
itories to support the use of their holdings? (Li-
brary of Congress, 2002). It is intended to be a data
communication format based on SGML/XML syn-
tax, aiming at supporting the accessibility to archival
resources across different institutions and focusing
on the structural content of the archival descrip-
tion, rather than its presentation. For this reason,
the EAD schema is characterised by a hierarchi-
cal informational structure, where the deepest lev-
els in the schema may inherit descriptive informa-
tion defined in the upper levels. The schema de-
fines a total of 146 elements. The three highest level
elements are <eadheader>, <frontmatter>,
and <archdesc>. <eadheader> is an ele-
ment containing bibliographic and descriptive in-
formation about the metadata document, while
<frontmatter> is an optional element describ-
ing the creation, publication, or use of the metadata
document (Library of Congress, 2002). Both these
two upper level elements do not contain information
about the archival material itself. The designated el-
ement for this purpose is <archdesc> which de-
scribes ?the content, context, and extent of a body
of archival materials, including administrative and
supplemental information that facilitates use of the
materials? (Library of Congress, 2002).
EAD metadata files can be lengthy and com-
plex in structure, with deep nesting of the XML
hierarchy elements. As Zhang and Kamps (2009)
also observe, the EAD elements may be of three
types:
i. atomic units (or text content elements) which
contain only text and no XML elements;
ii. composite units (or nested elements) which
contain as nested other XML elements;
iii. mixed elements which contain both atomic and
composite units.
The EAD documents used in this study describe
archival collections of the International Institute of
Social History (IISH). They are of varying length
and are often characterised by long spans of non-
annotated, free text. The degree of annotation, es-
pecially within mixed element types is inconsistent.
For example, some names may be annotated in one
element and others not, while quite often repeated
mentions of the same name may not be annotated.
Moreover, the text within an annotated element may
include annotator comments (e.g., translations, alter-
nate names, questions, notes, etc.), either in square
brackets or parentheses, again in an inconsistent
manner. The multilingual text content poses another
challenge. In particular, the languages used in the
description text vary, not only within a single EAD
document, but often also within an element (mixed
or atomic). In our approach, the former is addressed
45
by identifying the language at element level (cf. Sec-
tion 3.2). However, the issue of mixed languages
within an element is not addressed. This introduces
errors, especially for multilingual elements of short
text length.
3 Enrichment and Structuring Method
The overall rationale behind our method for the en-
richment of EAD metadata with semantic content in-
formation is based on two hypotheses:
i. multi-word terms recognised in free text are
valid indicators of content, and
ii. the respective term inter-relationships reflect
the knowledge structure of the collection.
Thus, automatic term recognition and subsequent
term clustering constitute the two core components
of our EAD processing. In particular, as illustrated
in Figure 1, we start with a pre-processing phase,
where the EAD input SGML/XML files are first
parsed, in order to retrieve the respective text con-
tent snippets, and then classified, based on language.
Subsequently, terms are recognised automatically.
The resulting terms are clustered as a hierarchy and,
finally, the documents are classified according to the
term hierarchy, based on the terms that they contain.
To evaluate our term recognition process, we exploit
knowledge from two sources: existing annotations
in the EAD files, such as entity annotation residing
in mixed elements (cf. Section 2) and entity and sub-
ject term information originating from the respective
cultural heritage institution Authority files, namely
the library files providing standard references for en-
tities and terms that curators should use in their ob-
ject descriptions. In this section, we discuss in more
detail the methodology for each of the components
of our approach.
3.1 EAD Text Element Extraction
In our processing of the EAD metadata XML, we
focused on the free text content structured below
the <archdesc> root element. As discussed in
Section 2, it is the only top element which con-
tains information about the archival material itself.
In the text element extraction process, we parse
the EAD XML and, from the hierarchically struc-
tured elements below <archdesc>, we select the
text contained in <abstract>, <bioghist>,
<scopecontent>, <odd>, <note> , <dsc>
and <descgrp> and their nested elements.
Among these elements, the <dsc> (Description
of Subordinate Components) provides information
about the hierarchical groupings of the materials be-
ing described, whereas <descgrp> (DSC Group)
defines nested encoded finding aids. They were se-
lected because they may contain nested information
of interest. The rest of the elements were selected
because they contain important free text information
related to the archive content:
- <bioghist>: describing the archive creator
e.g. the life of the individual or family, or
the administrative history of the organisation
which created the archive;
- <scopecontent>: referring to the range
and topical coverage of the described materials,
often naming significant organisations, individ-
uals, events, places, and subjects represented;
- <odd>: other descriptive data;
- <note>: referring to archivist comments and
explanations;
- <abstract>: brief summaries of all the
above information.
All other elements not referring to the archive se-
mantic content, such as administrative information,
storage arrangement, physical location, etc. were ig-
nored. Moreover, atomic or composite elements
without free text descriptions were not selected, be-
cause the descriptive information therein is assumed
to be already fully structured.
3.2 Language Identification
As mentioned in Section 2, the languages used in
the description text of the EAD documents vary, not
only within a single EAD document, but often also
within an EAD element. In our approach, the objec-
tive of the language identification process is to de-
tect the language of the text content snippets, i.e. the
output of the text element extraction process, and
classify these snippets accordingly (cf. Figure 1).
Language identification is a text categorisation
task, whereby identifiers attempt to learn the mor-
phology of a language based on training text and,
subsequently, use this information to classify un-
known text accordingly. For this reason, training a
language identification component requires a train-
ing corpus for each language of interest.
46
Figure 1: Block diagram of EAD metadata enrichment and structuring process
Computational approaches to language identifi-
cation can be coarsely classified into information-
theoretic, word-based, and N-gram-based.
Information-theoretic approaches compare the
compressibility of the input text to the compress-
ibility of text in the known languages. Measuring
compressibility employs mutual information mea-
sures (Poutsma, 2002). Word-based approaches
consider the amount of common words or special
characters between the input text and a known
language. Finally, N-gram-based approaches con-
struct language models beyond word boundaries,
based on the occurrence statistics of N-grams up
to some predefined length N (Dunning, 1994).
The subsequent language identification in unknown
text is based on the similarity of the unknown text
N-gram model to each training language model.
As evidenced by these approaches, language iden-
tification relies on some form of comparison of the
unknown text to known languages. For this reason,
the respective text categorisation into a given lan-
guage suffers when the input text is not long enough:
the shorter the input text is, the fewer the available
features for comparison against known language
models. Moreover, errors in the categorisation pro-
cess are also introduced, when the language models
under comparison share the same word forms.
In our approach, we have opted for the most pop-
ular language identification method, the one based
on N-grams. Nevertheless, any other language iden-
tification method could have been applied.
3.3 Term Recognition
The objective of term recognition is the identifica-
tion of linguistic expressions denoting specialised
concepts, namely domain or scientific terms. For in-
formation management and retrieval purposes, the
automatic identification of terms is of particular im-
portance because these specialised concept expres-
sions reflect the respective document content.
Term recognition approaches largely rely on the
identification of term formation patterns. Linguistic
approaches use either syntactic (Justeson and Katz,
1995; Hearst, 1998), or morphological (Heid, 1998)
rule patterns, often in combination with termino-
logical or other lexical resources (Gaizauskas et al,
2000) and are typically language and domain spe-
cific.
Statistical approaches typically combine linguis-
tic information with statistical measures. These
measures can be coarsely classified into two
categories: unithood-based and termhood-based.
Unithood-based approaches measure the attachment
strength among the constituents of a candidate
term. For example, some unithood-based mea-
sures are frequency of co-occurrence, hypothesis
testing statistics, log-likelihood ratios test (Dunning,
1993) and pointwise mutual information (Church
and Hanks, 1990). Termhood-based approaches at-
tempt to measure the degree up to which a candidate
expression is a valid term, i.e. refers to a specialised
concept. They attempt to measure this degree by
considering nestedness information, namely the fre-
47
quencies of candidate terms and their subsequences.
Examples of such approaches are C-Value and NC-
Value (Frantzi et al, 2000) and the statistical barrier
method (Nakagawa, 2000).
It has been experimentally shown that termhood-
based approaches to automatic term extraction out-
perform unithood-based ones and that C-Value
(Frantzi et al, 2000) is among the best perform-
ing termhood-based approaches (Korkontzelos et
al., 2008). For this reason, we choose to employ
the C-Value measure in our pipeline. C-Value ex-
ploits nestedness and comes together with a com-
putationally efficient algorithm, which scores can-
didate multi-word terms according to the measure,
considering:
- the total frequency of occurrence of the candi-
date term;
- the frequency of the candidate term as part of
longer candidate terms;
- the number of these distinct longer candidates;
- the length of the candidate term (in tokens).
These arguments are expressed in the following
nestedness formula:
N(?) =
?
?
?
f(?), if ? is not nested
f(?)?
1
|T?|
?
b?T?
f(b), otherwise (1)
where ? is the candidate term, f(?) is its frequency,
T? is the set of candidate terms that contain ? and
|T?| is the cardinality of T?. In simple terms, the
more frequently a candidate term appears as a sub-
string of other candidates, the less likely it is to be a
valid term. However, the greater the number of dis-
tinct term candidates in which the target term can-
didate occurs as nested, the more likely it is to be
a valid term. The final C-Value score considers the
length (|?|) of each candidate term (?) as well:
C-value(?) = log2 |?| ?N(?) (2)
The C-Value method requires linguistic pre-
processing in order to detect syntactic term for-
mation patterns. In our approach, we used Lex-
Tagger (Vasilakopoulos, 2003), which combines
transformation-based learning with decision trees
and we adapted its respective lexicon to our domain.
We also included WordNet lemma information in
our processing, for text normalisation purposes. Lin-
guistic pre-processing is followed by the computa-
tion of C-Value on the candidate terms, in length or-
der, longest first. Candidates that satisfy a C-Value
threshold are sorted in decreasing C-Value order.
3.4 Hierarchical Agglomerative Clustering
In our approach, term recognition provides content
indicators. In order to make explicit the knowl-
edge structure of the EAD, our method requires
some form of concept classification and structuring.
The process of hierarchical agglomerative cluster-
ing serves this objective.
Agglomerative algorithms are very popular in
the field of unsupervised concept hierarchy induc-
tion and are typically employed to produce unla-
belled taxonomies (King, 1967; Sneath and Sokal,
1973). Hierarchical clustering algorithms are based
on measuring the distance (dissimilarity) between
pairs of objects. Given an object distance metric D,
the similarity of two clusters, A and B, can be de-
fined as a function of the distance D between the
objects that the clusters contain. According to this
similarity, also called linkage criterion, the choice
of which clusters to merge or split is made. In our
approach, we have experimented with the three most
popular criteria, namely:
Complete linkage (CL): The similarity of two clus-
ters is the maximum distance between their elements
simCL(A,B) = max
x?A,y?B
D(x, y) (3)
Single linkage (SL): The similarity of two clusters
is the minimum distance between their elements
simSL(A,B) = min
x?A,y?B
D(x, y) (4)
Average linkage (AL): The similarity of two clusters
is the average distance between their elements
simAL(A,B) =
1
|A| ? |B|
?
x?A
?
y?B
D(x, y) (5)
To estimate the distance metric D we use either
the document co-occurrence or the lexical similar-
ity metric. The chosen distance metric D and link-
age criterion are employed to derive a hierarchy of
terms by agglomerative clustering.
Our document co-occurrence (DC) metric is de-
fined as the number of documents (d) in the collec-
tion (R) in which both terms (t1 and t2) co-occur:
DC =
1
|R|
|{d : (d ? R) ? (t1 ? d) ? (t2 ? d)}| (6)
48
The above metric accepts that the distance between
two terms is inversely proportional to the number of
documents in which they co-occur.
Lexical Similarity (LS), as defined in Nenadic?
and Ananiadou (2006), is based on shared term con-
stituents:
LS =
|P (h1) ? P (h2)|
|P (h1)|+ |P (h2)|
+
|P (t1) ? P (t2)|
|P (t1)|+ |P (t2)|
(7)
where t1 and t2 are two terms, h1 and h2 their heads,
P (h1) and P (h2) their set of head words, and P (t1)
and P (t2) their set of constituent words, respec-
tively.
3.5 Document Classification
The term hierarchy is used in our approach for se-
mantic classification of documents. In this process,
we start by assigning to each leaf node of the term
hierarchy the set of EAD documents in which the
corresponding term occurs. Higher level nodes are
assigned the union of the document sets of their
daughters. The process is bottom-up and applied it-
eratively, until all hierarchy nodes are assigned a set
of documents.
Document classification, i.e. the assignment of
document sets to term hierarchy nodes, is use-
ful, among others, for structured search and index-
ing purposes. Moreover, it provides a direct soft-
clustering of documents based on semantics, given
the number of desired clusters, C. C corresponds
to a certain horizontal cut of the term hierarchy, so
that C top nodes appear, instead of one. The doc-
ument sets assigned to these C top nodes represent
the C desired clusters. This document clustering ap-
proach is soft, since each document can occur in one
or more clusters.
3.6 Evaluation Process
The automatic evaluation process, illustrated in Fig-
ure 1, serves the purpose of evaluating the term
recognition accuracy. Since the objective of term
recognition tools is the detection of linguistic ex-
pressions denoting specialised concepts, i.e. terms,
the results evaluation would ideally require input
from the respective domain experts. This is a la-
borious and time consuming process which also en-
tails finding the experts willing to dedicate effort
and time for this task. In response to this issue,
we decided to exploit the available domain-specific
knowledge resources and automate part of the eval-
uation process by comparing our results to this ex-
isting information. Thus, the automatic evaluation
process is intended to give us an initial estimate
of our performance and reduce the amount of re-
sults requiring manual evaluation. The available re-
sources used are of two types:
i. entity annotations in the EAD documents (i.e.
names of persons, organisations and geograph-
ical locations);
ii. entity and subject terms originating from the
cultural heritage institution Authority files.
The entity annotations in the EAD documents
were not considered during our term recognition.
The entity and subject terms of the respective Au-
thority file records are encoded in MARC21/XML
format (Library of Congress, 2010). MARC
(MAchine-Readable Cataloging) is a standard initi-
ated by the US Library of Congress and concerns
the representation of bibliographic information and
related data elements used in library catalogues. The
MARC21 Authority files resource used in our eval-
uation provides, among other information, the stan-
dard references for entities and the respective pos-
sible entity reference variations, such as alternate
names or acronyms, etc., that curators should use
in their object descriptions. The subject term Au-
thority records provide mappings between a legacy
subject term thesaurus which is no longer used for
classification, and current library records.
In the evaluation process the EAD SGML/XML
and the MARC21/XML Authority files are first
parsed by the respective parsers in order to extract
the XML elements of interest. Subsequently, the
text-content of the elements is processed for nor-
malisation and variant generation purposes. In this
process, normalisation involves cleaning up the text
from intercepted comments and various types of
inconsistent notes, such as dates, aliases and al-
ternate names, translations, clarifications, assump-
tions, questions, lists, etc. Variant generation in-
volves detecting the acronyms, abbreviated names
and aliases mentioned in the element text and cre-
ating the reversed variants for, e.g., [Last Name,
First Name] sequences. The results of this pro-
cess, from both EAD and Authority files, are merged
into a single list for every respective category (or-
49
language snippets language snippets
Dutch 50,363 Spanish 3,430
German 41,334 Danish 2,478
English 19,767 Italian 1,100
French 6,182 Swedish 699
Table 1: Number of snippets per identified language.
ganisations, persons, geographic locations and sub-
ject terms) and are compared to our term results list.
4 Experimental Setting
For training the language identification component,
we used the European Parliament Proceedings Par-
allel Corpus (Europarl) which covers the proceed-
ings of the European Parliament from 1996 to 2006
(Koehn, 2005). The corpus size is 40 million words
per language and is translated in Danish, German,
Greek, English, Spanish, Finnish, French, Italian,
Dutch, Portuguese and Swedish. In our experiments,
we take as input for subsequent term recognition
only the snippets identified as English text.
In the experiments reported in this work, we ac-
cept as term candidates morpho-syntactic pattern se-
quences which consist of adjectives and nouns, and
end with a noun. The C-Value algorithm (cf. Sec-
tion 3.3) was implemented under two different set-
tings:
i. one only considering as term candidates adjec-
tive and noun sequences that appear at least
once as non-nested in other candidate terms;
and
ii. one that considers all adjective and noun se-
quences, even if they never occur as non-
nested.
Considering that part-of-speech taggers usually suf-
fer high error rates when applied on specialty do-
mains, the former setting is expected to increase pre-
cision, whereas the latter to increase recall (cf. Sec-
tion 5).
We accepted as valid terms all term candidates
whose C-Value score exceeds a threshold, which
was set to 3.0 after experimentation. In the subse-
quent hierarchical agglomerative clustering process,
we experimented with all six combinations of the
three linkage criteria (i.e. complete, single and aver-
age) with the two distance metrics (i.e. document
co-occurrence and lexical similarity) described in
Figure 2: Length of snippets per identified language.
Section 3.4.
5 Results
The EAD document collection used for this study
consisted of 3, 093 SGML/XML files. As shown on
Table 1, according to our language identifier, the ma-
jority of the text snippets of the selected EAD XML
elements were in Dutch, followed by German and
English. We selected for later processing 19, 767
snippets classified as English text, corresponding to
419, 857 tokens. A quantitative evaluation of the
language identifier results has not been performed.
However, our observation of the term recognition re-
sults showed that there were some phrases, mostly
Dutch and German entity names (organisations and
persons mostly) classified as English. This might be
due to these entities appearing in their original lan-
guage within English text, as it is often the case in
our EAD files. Moreover, manual inspection of our
results showed that other languages classified as En-
glish, e.g. Turkish and Czech, were not covered by
Europarl.
As mentioned in Section 3.2, short text snip-
pets may affect language identification performance.
Figure 2 illustrates the snippet length per identified
language. We observe that the majority of text snip-
pets is below 10 tokens, few fall within an average
length of 20 to 50 tokens approximately, and very
few are above 100 tokens.
Figure 3 shows the results of our automatic evalu-
ation for the term recognition process. In this graph,
the upper, red curve shows the percentage of cor-
rect terms for the C-Value setting considering as
term candidates adjective and noun sequences that
appear at least once as non-nested in other candi-
date terms. The lower, blue curve shows the per-
50
Figure 3: Term coverage for each C-Value setting based
on EAD & Authority entity and subject term evaluation.
centage of correct terms for the C-Value setting con-
sidering all adjective and noun sequences, even if
they never occur as non-nested. In this automatic
evaluation, correct terms are, as presented in Sec-
tion 3.6, those candidate terms matching the com-
bined lists of entity and subject terms acquired by
the respective EAD and MARC21 Authority files.
We observe that the C-Value setting which considers
only noun phrase patterns occurring at least once as
non-nested, displays precision up to approximately
70% for the top terms in the ranked list, whereas the
other setting considering all noun phrase sequences,
reaches a maximum of 49%. The entire result set
above the 3.0 C-Value threshold amounts to 1, 345
and 2, 297 terms for each setting, and reaches pre-
cision of 42.01% and 28.91% respectively. Thus,
regarding precision, the selective setting clearly out-
performs the one considering all noun phrases, but it
also reaches a lower recall, as indicated by the ac-
tual terms within the threshold. We also observe
that precision drops gradually below the threshold,
an indication that the ranking of the C-Value mea-
sure is effective in promoting valid terms towards
the top. This automatic evaluation considers as erro-
neous unknown terms which may be valid. Further
manual evaluation by domain experts is required for
a more complete picture of the results.
Figure 4 shows six dendrograms, each represent-
ing the term hierarchy produced by the respective
combination of linkage criterion to distance metric.
The input for these experiments consists of all terms
exceeding the C-Value threshold, and by considering
only noun phrase sequences appearing at least once
as non-nested. Since the hierarchies contain 1, 345
terms, the dendrograms are very dense and difficult
to inspect thoroughly. However, we include them
based on the fact that the overall shape of the den-
drogram can indicate how much narrow or broad the
corresponding hierarchy is and indirectly its quality.
Narrow here characterises hierarchies whose most
non-terminal nodes are parents of one terminal and
one non-terminal node. Narrow hierarchies are deep
while broader hierarchies are shallower.
Broad and shallow hierarchies are, in our case, of
higher quality, since terms are expected to be related
to each other and form distinct groups. In this view,
average linkage leads to richer hierarchies (Figures
4(c), 4(f)), followed by single linkage (Figures 4(b),
4(e)) and, finally, complete linkage (Figures 4(a),
4(d)). The hierarchy of higher quality seems to
be the result of average linkage and document co-
occurrence combination (Figure 4(c)), followed by
the combination of average linkage and lexical sim-
ilarity (Figure 4(f)). Clearly, these two hierarchies
need to be investigated manually and closely to ex-
tract further conclusions. Moreover, an application-
based evaluation could investigate whether different
clustering settings suit different tasks.
6 Conclusion and Future Work
In this paper, we have presented a methodology for
semantically enriching archival description meta-
data and structuring the metadata collection. We
consider that terms are indicators of content seman-
tics. In our approach, we perform term recogni-
tion and then hierarchically structure the recognised
terms. Finally, we use the term hierarchy to classify
the metadata documents. We also propose an auto-
matic evaluation of the recognised terms, by com-
paring them to domain knowledge resources.
For term recognition, we used the C-Value al-
gorithm and found that considering noun phrases
which appear at least once independently, outper-
forms considering all noun phrases. Regarding hier-
archical clustering, we observe that the average link-
age criterion combined with a distance metric based
on document co-occurrence produces a rich broad
hierarchy. A more thorough evaluation of these re-
sults is required. This should include a manual eval-
uation of recognised terms by domain experts and
an application-based evaluation of the resulting doc-
ument classification.
51
(a) Complete linkage - DC (b) Single linkage - DC (c) Average linkage - DC
(d) Complete linkage - LS (e) Single linkage - LS (f) Average linkage - LS
Figure 4: Dendrograms showing the results of agglomerative clustering for all linkage criteria and distance metrics,
document co-occurrence (DC) and Lexical Similarity (LS).
References
Lina Bountouri and Manolis Gergatsoulis. 2009. Inter-
operability between archival and bibliographic meta-
data: An EAD to MODS crosswalk. Journal of Li-
brary Metadata, 9(1-2):98?133.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
CIDOC. 2006. The CIDOC Conceptual Reference
Model. CIDOC Documentation Standards Working
Group, International Documentation Committee, In-
ternational Council of Museums. http://www.
cidoc-crm.org/.
DCMI. 2011. The Dublin Core Metadata Initiative.
http://dublincore.org/.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Ted Dunning. 1994. Statistical identification of lan-
guage. MCCS 94-273. Technical report, Computing
Research Laboratory, New Mexico State University.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. International Journal on
Digital Libraries, 3(2):115?130.
Robert Gaizauskas, George Demetriou, and Kevin
Humphreys. 2000. Term recognition in biological sci-
ence journal articles. In Proc. of the NLP 2000 Work-
shop on Computational Terminology for Medical and
Biological Applications, pages 37?44, Patras, Greece.
Marti Hearst. 1998. Automated discovery of WordNet
relations. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 131?153. MIT
Press.
Ulrich Heid. 1998. A linguistic bootstrapping approach
to the extraction of term candidates from german text.
Terminology, 5(2):161?181.
John Justeson and Slava Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering,
1(1):9?27.
Benjamin King. 1967. Step-Wise clustering proce-
dures. Journal of the American Statistical Association,
62(317):86?101.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Marijn Koolen, Avi Arampatzis, Jaap Kamps, Vincent
de Keijzer, and Nir Nussbaum. 2007. Unified access
to heterogeneous data in cultural heritage. In Proc. of
RIAO ?07, pages 108?122, Pittsburgh, PA, USA.
Ioannis Korkontzelos, Ioannis Klapaftis, and Suresh
Manandhar. 2008. Reviewing and evaluating auto-
matic term recognition techniques. In Bengt Nord-
stro?m and Aarne Ranta, editors, Proc. of GoTAL ?08,
volume 5221 of LNCS, pages 248?259, Gothenburg,
Sweden. Springer.
Shu-Hsien Liao, Hong-Chu Huang, and Ya-Ning Chen.
2010. A semantic web approach to heterogeneous
metadata integration. In Jeng-Shyang Pan, Shyi-Ming
Chen, and Ngoc Thanh Nguyen, editors, Proc. of
ICCCI ?10, volume 6421 of LNCS, pages 205?214,
Kaohsiung, Taiwan. Springer.
Library of Congress. 2002. Encoded archival descrip-
tion (EAD), version 2002. Encoded Archival Descrip-
tion Working Group: Society of American Archivists,
52
Network Development and MARC Standards Office,
Library of Congress. http://www.loc.gov/
ead/.
Library of Congress. 2010. MARC standards. Network
Development and MARC Standards Office, Library of
Congress, USA. http://www.loc.gov/marc/
index.html.
Irene Lourdi, Christos Papatheodorou, and Martin Doerr.
2009. Semantic integration of collection description:
Combining CIDOC/CRM and Dublin Core collections
application profile. D-Lib Magazine, 15(7/8).
Hiroshi Nakagawa. 2000. Automatic term recognition
based on statistics of compound nouns. Terminology,
6(2):195?210.
Goran Nenadic? and Sophia Ananiadou. 2006. Min-
ing semantically related terms from biomedical liter-
ature. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 5(1):22?43.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45:179?189.
Peter Sneath and Robert Sokal. 1973. Numerical taxon-
omy: the principles and practice of numerical classifi-
cation. Freeman, San Francisco, USA.
Argyris Vasilakopoulos. 2003. Improved unknown word
guessing by decision tree induction for POS tagging
with tbl. In Proc. of CLUK ?03, Edinburgh, UK.
Junte Zhang and Jaap Kamps. 2009. Focused search
in digital archives. In Gottfried Vossen, Darrell D. E.
Long, and Jeffrey Xu Yu, editors, Proc. of WISE
?09, volume 5802 of LNCS, pages 463?471, Poznan,
Poland. Springer.
53
Proceedings of BioNLP Shared Task 2011 Workshop, pages 26?35,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011
Sampo Pyysalo? Tomoko Ohta? Rafal Rak?? Dan Sullivan? Chunhong Mao?
Chunxia Wang? Bruno Sobral? Jun?ichi Tsujii? Sophia Ananiadou??
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Virginia Bioinformatics Institute, Virginia Tech, Blacksburg, Virginia, USA
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
?Microsoft Research Asia, Beijing, China
{smp,okap}@is.s.u-tokyo.ac.jp jtsujii@microsoft.com
{dsulliva,cmao,cwang,sobral}@vbi.vt.edu
{rafal.rak,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper presents the preparation, resources,
results and analysis of the Infectious Diseases
(ID) information extraction task, a main task
of the BioNLP Shared Task 2011. The ID
task represents an application and extension
of the BioNLP?09 shared task event extrac-
tion approach to full papers on infectious dis-
eases. Seven teams submitted final results to
the task, with the highest-performing system
achieving 56% F-score in the full task, com-
parable to state-of-the-art performance in the
established BioNLP?09 task. The results in-
dicate that event extraction methods general-
ize well to new domains and full-text publi-
cations and are applicable to the extraction of
events relevant to the molecular mechanisms
of infectious diseases.
1 Introduction
The Infectious Diseases (ID) task of the BioNLP
Shared Task 2011 (Kim et al, 2011a) is an infor-
mation extraction task focusing on the biomolecu-
lar mechanisms of infectious diseases. The primary
target of the task is event extraction (Ananiadou et
al., 2010), broadly following the task setup of the
BioNLP?09 Shared Task (BioNLP ST?09) (Kim et
al., 2009).
The task concentrates on the specific domain of
two-component systems (TCSs, or two-component
regulatory systems), a mechanism widely used by
bacteria to sense and respond to the environment
(Thomason and Kay, 2000). Typical TCSs con-
sist of two proteins, a membrane-associated sensor
kinase and a cytoplasmic response regulator. The
sensor kinase monitors changes in the environment
while the response regulator mediates an adaptive
response, usually through differential expression of
target genes (Mascher et al, 2006). TCSs have many
functions, but those of particular interest for infec-
tious disease researchers include virulence, response
to antibiotics, quorum sensing, and bacterial cell at-
tachment (Krell et al, 2010). Not all TCS functions
are well known: in some cases, TCSs are involved
in metabolic processes that are difficult to precisely
characterize (Wang et al, 2010). TCSs are of in-
terest also as drugs designed to disrupt TCSs may
reduce the virulence of bacteria without killing it,
thus avoiding the potential selective pressure of an-
tibiotics lethal to some pathogenic bacteria (Gotoh
et al, 2010). Information extraction techniques may
support better understanding of these fundamental
systems by identifying and structuring the molecu-
lar processes underlying two component signaling.
The ID task seeks to address these opportuni-
ties by adapting the BioNLP ST?09 event extraction
model to domain scientific publications. This model
was originally introduced to represent biomolecu-
lar events relating to transcription factors in human
blood cells, and its adaptation to a domain that cen-
trally concerns both bacteria and their hosts involves
a variety of novel aspects, such as events concerning
whole organisms, the chemical environment of bac-
teria, prokaryote-specific concepts (e.g. regulons as
elements of gene expression), as well as the effects
of biomolecules on larger-scale processes involving
hosts such as virulence.
26
2 Task Setting
The ID task broadly follows the task definition and
event types of the BioNLP ST?09, extending it with
new entity categories, correspondingly broadening
the scope of events, and introducing a new class of
events, high-level biological processes.
2.1 Entities
The ID task defines five core types of entities:
genes/gene products, two-component systems, reg-
ulons/operons, chemicals, and organisms. Follow-
ing the general policy of the BioNLP Shared Task,
the recognition of the core entities is not part of
the ID task. As named entity recognition (NER)
is considered in other prominent domain evaluations
(Krallinger et al, 2008), we have chosen to isolate
aspects of extraction performance relating to NER
from the main task of interest, event extraction, by
providing participants with human-created gold an-
notations for core entities. These annotations are
briefly presented in the following.
Mentions of names of genes and their products
(RNA and proteins) are annotated with a single
type, without differentiating between subtypes, fol-
lowing the guidelines of the GENIA GGP corpus
(Ohta et al, 2009). This type is named PRO-
TEIN to maintain consistency with related tasks
(e.g. BioNLP ST?09), despite slight inaccuracy
for cases specifically referencing RNA or DNA
forms. Two-component systems, consisting of two
proteins, frequently have names derived from the
names of the proteins involved (e.g. PhoP-PhoR
or SsrA/SsrB). Mentions of TCSs are annotated as
TWO-COMPONENT-SYSTEM, nesting PROTEIN an-
notations if present. Regulons and operons are col-
lections of genes whose expression is jointly regu-
lated. Like the names of TCSs, their names may de-
rive from the names of the involved genes and pro-
teins, and are annotated as embedding PROTEIN an-
notations when they do. The annotation does not
differentiate between the two, marking both with a
single type REGULON-OPERON.
In addition to these three classes relating to genes
and proteins, the core entity annotation recognizes
the classes CHEMICAL and ORGANISM. All men-
tions of formal and informal names of atoms, inor-
ganic compounds, carbohydrates and lipids as well
as organic compounds other than amino acid and nu-
cleic acid compounds (i.e. gene/protein-related com-
pounds) are annotated as CHEMICAL. Mentions of
names of families, genera, species and strains as
well as non-name references with comparable speci-
ficity are annotated as ORGANISM.
Finally, the non-specific type ENTITY1 is defined
for marking entities that specify additional details of
events such as the binding site in a BINDING event or
the location an entity moves to in a LOCALIZATION
event. Unlike the core entities, annotations of the
generic ENTITY type are not provided for test data
and must be detected by participants addressing the
full task.
2.2 Relations
The ID task involves one relation, EQUIV, defin-
ing entities (of any of the core types) to be equiv-
alent. This relation is used to annotate abbreviations
and local aliases and it is not a target of extraction,
but provided for reference and applied in evaluation,
where references to any of a set of equivalent entities
are treated identically.
2.3 Events
The primary extraction targets of the ID task are the
event types summarized in Table 1. These are a su-
perset of those targeted in the BioNLP ST?09 and its
repeat, the 2011 GE task (Kim et al, 2011b). This
design makes it possible to study aspects of domain
adaptation by having the same extraction targets in
two subdomains of biomedicine, that of transcrip-
tion factors in human blood cells (GE) and infectious
diseases. The events in the ID task extend on those
of GE in the inclusion of additional entity types
as participants in previously considered event types
and the introduction of a new type, PROCESS. We
next briefly discuss the semantics of these events,
defined (as in GE) with reference to the community-
standard Gene Ontology (Ashburner et al, 2000).
We refer to (Kim et al, 2008; Kim et al, 2009) for
the ST?09/GE definitions.
1In terms of the GENIA ontology, ENTITY is used to mark
e.g. PROTEIN DOMAIN OR REGION references. Specific types
were applied in manual annotation, but these were replaced
with the generic ENTITY in part to maintain consistency with
BioNLP ST?09 data and to reduce the NER-related demands
on participating systems by not requiring the assignment of de-
tailed types.
27
Type Core arguments Additional arguments
GENE EXPRESSION Theme(PROTEIN or REGULON-OPERON)
TRANSCRIPTION Theme(PROTEIN or REGULON-OPERON)
PROTEIN CATABOLISM Theme(PROTEIN)
PHOSPHORYLATION Theme(PROTEIN) Site(ENTITY)
LOCALIZATION Theme(Core entity) AtLoc(ENTITY), ToLoc(ENTITY)
BINDING Theme(Core entity)+ Site(ENTITY)+
PROCESS Participant(Core entity)?
REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
POSITIVE REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
NEGATIVE REGULATION Theme(Core entity / Event), Cause(Core entity / Event)? Site(ENTITY), CSite(ENTITY)
Table 1: Event types and their arguments. The type of entity allowed as argument is specified in parenthesis. ?Core en-
tity? is any of PROTEIN, TWO-COMPONENT-SYSTEM, REGULON-OPERON, CHEMICAL, or ORGANISM. Arguments
that can be filled multiple times marked with ?+?, non-mandatory core arguments with ??? (all additional arguments
are non-mandatory).
The definitions of the first four types in Table 1
are otherwise unchanged from the ST?09 definitions
except that GENE EXPRESSION and TRANSCRIP-
TION extend on the former definition in recogniz-
ing REGULON-OPERON as an alternative unit of ex-
pression. LOCALIZATION, taking only PROTEIN
type arguments in the ST?09 definition, is allowed
to take any core entity argument. This expanded
definition remains consistent with the scope of the
corresponding GO term (GO:0051179). BINDING
is similarly extended, giving it a scope largely con-
sistent with GO:0005488 (binding) but also encom-
passing GO:0007155 (cell adhesion) (e.g. a bac-
terium binding another) and protein-organism bind-
ing. The three regulation types (REGULATION,
POSITIVE REGULATION, and NEGATIVE REGULA-
TION) likewise allow the new core entity types as
arguments, but their definitions are otherwise un-
changed from those in ST?09, that is, the GENIA on-
tology definitions. As in these resources, regulation
types are used not only for the biological sense but
also to capture statements of general causality (Kim
et al, 2008). As in ST?09, all events of types dis-
cussed above require a Theme argument: only events
involving an explicitly stated theme (of an appropri-
ate type) should be extracted. All other arguments
are optional.
The PROCESS type, new to ID, is used to annotate
high-level processes such as virulence, infection and
resistance that involve infectious organisms. This
type differs from the others in that it has no manda-
tory arguments: the targeted processes should be ex-
tracted even if they have no explicitly stated partici-
pants, reflecting that they are of interest even without
the further specification. When stated, the involved
participants are captured using the generic role type
Participant. Figure 1 shows an illustration of some
of the the ID task extraction targets.
We term the first five event types in Table 1 taking
exactly one Theme argument as their core argument
simple events. In analysis we further differentiate
non-regulation events (the first seven) and regulation
(the last three), which is known to represent partic-
ular challenges for extraction in involving events as
arguments, thus creating nested event structures.
2.4 Event modifications
The ID task defines two event modification ex-
traction targets, NEGATION and SPECULATION.
These modifications mark events as being explic-
itly negated (e.g. virB is not expressed) or stated in
a speculative context (e.g. virB may be expressed).
Both may apply simultaneously. The modification
definitions are identical to the ST?09 ones, includ-
ing the representation in which modifications (un-
like events) are not assigned text bindings.
3 Data
The ID task data were newly annotated for the
BioNLP Shared Task and are not based on any previ-
ously released resource. Annotation was performed
by two teams, one in Tsujii laboratory (University
of Tokyo) and one in Virginia Bioinformatics Insti-
tute (Virginia Tech). The entity and event annotation
28
Figure 1: Example event annotation. The association of a TCS with an organism is captured through an event structure
involving a PROCESS (?virulence?) and POSITIVE REGULATION. Regulation types are used to capture also statements
of general causality such as ?is essential for? here. (Simplified from PMC ID 2358977)
Journal # Published
PLoS Pathogens 9 2006?2010
PLoS One 7 2008?2010
BMC Genomics 3 2008?2010
PLoS Genetics 2 2007?2010
Open Microbiology J. 2 2008?2010
BMC Microbiology 2 2008?2009
Other 5 2007?2008
Table 2: Corpus composition. Journals in which selected
articles were published with number of articles (#) and
publication years.
design was guided by previous studies on NER and
event extraction in a closely related domain (Pyysalo
et al, 2010; Ananiadou et al, 2011).
3.1 Document selection
The training and test data were drawn from the pri-
mary text content of recent full-text PMC open ac-
cess documents selected by infectious diseases do-
main experts (Virginia Tech team) as representative
publications on two-component regulatory systems.
Table 2 presents some characteristics of the corpus
composition. To focus efforts on natural language
text likely to express novel information, we excluded
tables, figures and their captions, as well as methods
sections, acknowledgments, authors? contributions,
and similar meta-content.
3.2 Annotation
Annotation was performed in two primary stages,
one for marking core entities and the other for events
and secondary entities. As a preliminary processing
step, initial sentence segmentation was performed
with the GENIA Sentence Splitter2. Segmentation
errors were corrected during core entity annotation.
Core entity annotation was performed from the
basis of an automatic annotation created using se-
lected existing taggers for the target entities. The
2http://www-tsujii.is.s.u-tokyo.ac.jp/
?y-matsu/geniass/
Entity type prec. rec. F
PROTEIN 54.64 39.64 45.95
CHEMICAL 32.24 19.05 23.95
ORGANISM 90.38 47.70 62.44
TWO-COMPONENT-SYSTEM 87.69 47.24 61.40
Table 3: Automatic core entity tagging performance.
following tools and settings were adopted, with pa-
rameters tuned on initial annotation for two docu-
ments:
PROTEIN: NeMine (Sasaki et al, 2008) trained on
the JNLPBA data (Kim et al, 2004) with threshold
0.05, filtered to only GENE and PROTEIN types.
ORGANISM: Linnaeus (Gerner et al, 2010) with
?variant matching? for species names variants.
CHEMICAL: OSCAR3 (Corbett and Murray-Rust,
2006) with confidence 90%.
TWO-COMPONENT-SYSTEM: Custom regular ex-
pressions.
Initial automatic tagging was not applied for en-
tities of the REGULON-OPERON type or the generic
ENTITY type (for additional event arguments). All
automatically generated annotations were at least
confirmed through manual inspection, and the ma-
jority of the automatic annotations were revised in
manual annotation. Table 3 summarizes the tag-
ging performance of the automatic tools as measured
against the final human-annotated training and de-
velopment datasets.3
Annotation for the task extraction targets ? events
and event modifications ? was created entirely man-
ually without automatic annotation support to avoid
any possible bias toward specific extraction meth-
ods or approaches. The Tsujii laboratory team orga-
3It should be noted that these results are low in part due to
differences in annotation criteria (see e.g. (Wang et al, 2009))
and to data tagged using the ID task annotation guidelines not
being applied for training; training on the newly annotated data
is expected to allow notably more accurate tagging.
29
Item Train Devel Test Total
Articles 15 5 10 30
Sentences 2,484 709 1,925 5118
Words 74,439 21,225 57,489 153,153
Core entities 6,525 1,976 4,239 12,740
Events 2,088 691 1,371 4150
Modifications 95 45 74 214
Table 4: Statistics of the ID corpus.
nized the annotation effort, with a coordinating an-
notator with extensive experience in event annota-
tion (TO) leading annotator training and annotation
scheme development. Detailed annotation guide-
lines (Pyysalo et al, 2011) extending on the GE-
NIA annotation guidelines were developed jointly
with all annotators and refined throughout the an-
notation effort. Based on measurements of inter-
annotator consistency between annotations indepen-
dently created by the two teams, made throughout
annotator training and primary annotation (exclud-
ing final corpus cleanup), we estimate the consis-
tency of the final entity annotation to be no lower
than 90% F-score and that of the event annotation to
be no lower than 75% F-score for the primary eval-
uation criteria (see Section 4).
3.3 Datasets and statistics
Initial annotation was produced for the selected sec-
tions (see Section 3.1) in 33 full-text articles, of
which 30 were selected for the final dataset as repre-
sentative of the extraction targets. These documents
were split into training, development and test sets of
15, 5 and 10 documents, respectively. Participants
were provided with all training and development set
annotations and test set core entity annotations. The
overall statistics of the datasets are given in Table 4.
As the corpus consists of full-text articles, it con-
tains a somewhat limited number of articles, but in
other terms it is of broadly comparable size to the
largest of the BioNLP ST corpora: the corpus word
count, for example, corresponds to that of a cor-
pus of approximately 800 PubMed abstracts, and the
core entity count is comparable to that in the ST?09
data. However, for reasons that may relate in part to
the domain, the event count is approximately a third
of that for the ST?09 data. In addition to having less
training data, the entity/event ratio is thus consider-
ably higher (i.e. there are more candidates for each
true target), suggesting that the ID data could be ex-
pected to provide a more challenging extraction task.
4 Evaluation
The performance of participating systems was
evaluated in terms of events using the standard
precision/recall/F-score metrics. For the primary
evaluation, we adopted the standard criteria defined
in the BioNLP?09 shared task. In brief, for deter-
mining whether a reference annotation and a pre-
dicted annotation match, these criteria relax exact
matching for event triggers and arguments in two
ways: matching of text-bound annotation (event
triggers and ENTITY type entities) allows limited
boundary variation, and only core arguments need to
match in nested event arguments for events to match.
For details of the matching criteria, please refer to
Kim et al (2009).
The primary evaluation for the task requires the
extraction of all event arguments (both core and ad-
ditional; see Table 1) as well as event modifications
(NEGATION and SPECULATION). This is termed
the full task. We additionally report extraction re-
sults for evaluation where both the gold standard ref-
erence data and the submission events are reduced
to only core arguments, event modifications are re-
moved, and resulting duplicate events removed. We
term this the core task. In terms of the subtask divi-
sion applied in the BioNLP?09 Shared Task and the
GE task of 2011, the core task is analogous to sub-
task 1 and the full task analogous to the combination
of subtasks 1?3.
5 Results
5.1 Participation
Final results to the task were successfully submitted
by seven participants. Table 5 summarizes the in-
formation provided by the participating teams. We
note that full parsing is applied in all systems, with
the specific choice of the parser of Charniak and
Johnson (2005) with the biomedical domain model
of McClosky (2009) and conversion into the Stan-
ford Dependency representation (de Marneffe et al,
2006) being adopted by five participants. Further,
five of the seven systems are predominantly machine
learning-based. These can be seen as extensions of
trends that were noted in analysis of the BioNLP
30
NLP Events Other resources
Rank Team Org Word Parse Trig. Arg. Group. Modif. Corpora Other
1 FAUST 3NLP
CoreNLP,
SnowBall
McCCJ + SD (UMass+Stanford as features) GE word clusters
2 UMass 1NLP
CoreNLP,
SnowBall
McCCJ + SD Joint, dual dec.+MIRA 1-best - GE -
3 Stanford 3NLP CoreNLP McCCJ + SD MaxEnt Joint, MSTParser - GE word clusters
4 ConcordU 2NLP - McCCJ + SD dict rules rules rules -
triggers and
hedge words
5 UTurku 1BI Porter McCCJ + SD SVM SVM SVM SVM - hedge words
6 PNNL
1CS, 1NLP,
2BI
Porter Stanford SVM SVM rules - GE UMLS, triggers
7 PredX 1CS, 1NLP LGP LGP dict rules rules - - UMLS, triggers
Table 5: Participants and summary of system descriptions. Abbreviations: Trig./Arg./Group./Modif.=event trigger
detection/argument detection/argument grouping/modification detection, BI=Bioinformatician, NLP=Natural Lan-
guage Processing researcher, CS=Computer scientist, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snow-
ball=Snowball stemmer McCCJ=McClosky-Charniak-Johnson parser, LGP=Link Grammar Parser, SD=Stanford De-
pendency conversion, UMLS=UMLS resources (e.g. lexicon, metamap)
ST?09 participation. In system design choices, we
note an indication of increased use of joint models
as opposed to pure pipeline designs, with the three
highest-ranking systems involving a joint model.
Several participants compiled dictionaries of
event trigger words and two dictionaries of hedge
words from the data. Four teams, including the three
top-ranking, used the GE task corpus as supplemen-
tary material, indicating that the GE annotations are
largely compatible with ID ones (see detailed results
below). This is encouraging for future applications
of the event extraction approach: as manual annota-
tion requires considerable effort and time, the ability
to use existing annotations is important for the feasi-
bility of adaptation of the approach to new domains.
While several participants made use of support-
ing syntactic analyses provided by the organizers
(Stenetorp et al, 2011), none applied the analyses
for supporting tasks, such as coreference or entity
relation extraction results ? at least in cases due to
time constraints (Kilicoglu and Bergler, 2011).
5.2 Evaluation results
Table 6 presents the primary results by event type,
and Table 7 summarizes these results. The full
task requires the extraction of additional arguments
and event modifications and involves multiple novel
challenges from previously addressed domain tasks
including a new subdomain, full-text documents,
several new entity types and a new event category.
Team recall prec. F-score
FAUST 48.03 65.97 55.59
UMass 46.92 62.02 53.42
Stanford 46.30 55.86 50.63
ConcordU 49.00 40.27 44.21
UTurku 37.85 48.62 42.57
PNNL 27.75 52.36 36.27
PredX 22.56 35.18 27.49
Table 7: Primary evaluation results.
Nevertheless, extraction performance for the top
systems is comparable to the state-of-the-art results
for the established BioNLP ST?09 task (Miwa et al,
2010) as well as its repetition as the 2011 GE task
(Kim et al, 2011b), where the highest overall result
for the primary evaluation criteria was also 56% F-
score for the FAUST system (Riedel et al, 2011).
This result is encouraging regarding the ability of
the extraction approach and methods to generalize
to new domains as well as their applicability specifi-
cally to texts on the molecular mechanisms of infec-
tious diseases.
We note that there is substantial variation in the
relative performance of systems for different en-
tity types. For example, Stanford (McClosky et
al., 2011) has relatively low performance for simple
events but achieves the highest result for PROCESS,
while UTurku (Bjo?rne and Salakoski, 2011) results
show roughly the reverse. This suggests further po-
tential for improvement from system combinations.
31
FAUST UMass Stanford ConcordU UTurku PNNL PredX Size
GENE EXPRESSION 70.68 66.43 54.00 56.57 64.88 53.33 0.00 512
TRANSCRIPTION 69.66 68.24 60.00 70.89 57.14 0.00 53.85 77
PROTEIN CATABOLISM 75.00 72.73 20.00 66.67 33.33 11.76 0.00 33
PHOSPHORYLATION 64.00 66.67 40.00 54.55 60.61 64.29 40.00 69
LOCALIZATION 33.33 14.29 31.58 20.00 66.67 20.69 0.00 49
Simple event total 68.47 63.55 52.72 56.78 62.67 43.87 18.18 740
BINDING 31.30 34.62 23.44 40.00 22.22 20.00 28.28 156
PROCESS 65.69 62.26 73.57 67.17 41.57 51.04 53.27 901
Non-regulation total 63.78 60.68 63.59 62.43 46.39 47.34 43.65 1797
REGULATION 35.44 30.49 17.67 19.43 22.96 0.00 2.16 267
POSITIVE REGULATION 47.50 49.49 34.78 23.41 41.28 24.60 21.02 455
NEGATIVE REGULATION 58.86 60.45 44.44 47.96 52.11 25.70 9.49 260
Regulation total 47.07 46.65 33.02 28.87 39.49 18.45 9.71 982
Subtotal 57.28 55.03 52.09 46.60 43.33 37.53 28.38 2779
NEGATION 0.00 0.00 0.00 22.92 32.91 0.00 0.00 96
SPECULATION 0.00 0.00 0.00 3.23 15.00 0.00 0.00 44
Modification total 0.00 0.00 0.00 11.82 26.89 0.00 0.00 140
Total 55.59 53.42 50.63 44.21 42.57 36.27 27.49 2919
Table 6: Primary evaluation F-scores by event type. The ?size? column gives the number of annotations of each type
in the given data (training+development). Best result for each type shown in bold.
The best performance for simple events and for
PROCESS approaches or exceeds 70% F-score, ar-
guably approaching a sufficient level for user-facing
applications of the extraction technology. By con-
trast, BINDING and regulation events, found chal-
lenging in ST?09 and GE, remain problematic also
in the ID task, with best overall performance below
50% F-score. Only two teams, UTurku and Con-
cordU (Kilicoglu and Bergler, 2011), attempted to
extract event modifications, with somewhat limited
performance. The difficulty of correct extraction of
event modifications is related in part to the recursive
nature of the problem (similarly as for nested reg-
ulation events): to extract a modification correctly,
the modified event must also be extracted correctly.
Further, only UTurku predicted any instances of sec-
ondary arguments. Thus, teams other than UTurku
and ConcordU addressed only the core task extrac-
tion targets. With the exception of ConcordU, all
systems clearly favor precision over recall (Table 7),
in many cases having over 15% point higher preci-
sion than recall. This a a somewhat unexpected in-
version, as the ConcordU system is one of the two
rule-based in the task, an approach typically associ-
ated with high precision.
The five top-ranking systems participated also in
the GE task (Kim et al, 2011b), which involves a
subset of the ID extraction targets. This allows ad-
ditional perspective into the relative performance of
the systems. While there is a 13% point spread in
overall results for the top five systems here, in GE
all these systems achieved F-scores ranging between
50?56%. The results for FAUST, UMass and Stan-
ford were similar in both tasks, while the ConcordU
result was 6% points higher for GE and the UTurku
result over 10% points higher for GE, ranking third
after FAUST and UMass. These results suggest that
while the FAUST and UMass systems in particular
have some systematic (e.g. architectural) advantage
at both tasks, much of the performance difference
observed here between the top three systems and
those of ConcordU and UTurku is due to strengths
or weaknesses specific to ID. Possible weaknesses
may relate to the treatment of multiple core entity
types (vs. only PROTEIN in GE) or challenges re-
lated to nested entity annotations (not appearing in
GE). A possible ID-specific strength of the three
top-ranking systems is the use of GE data for train-
ing: Riedel and McCallum (2011) report an esti-
mated 7% point improvement and McClosky et al
(2011) a 3% point improvement from use of this
data; McGrath et al (2011) estimate a 1% point im-
provement from direct corpus combination. The in-
tegration strategies applied in training these systems
32
Team recall prec. F-score ?
FAUST 50.62 66.06 57.32 1.73
UMass 49.45 62.11 55.06 1.64
Stanford 48.87 56.03 52.20 1.57
ConcordU 50.77 43.25 46.71 2.50
UTurku 38.79 49.35 43.44 0.87
PNNL 29.36 52.62 37.69 1.42
PredX 23.67 35.18 28.30 0.81
Table 8: Core task evaluation results. The ? column
gives the F-score difference to the corresponding full task
(primary) result.
could potentially be applied also with other systems,
an experiment that could further clarify the relative
strengths of the various systems. The top-ranking
five systems all participated also in the EPI task
(Ohta et al, 2011), for which UTurku ranked first
with FAUST having comparable performance for the
core task. While this supports the conclusion that
ID performance differences do not reflect a simple
universal ranking of the systems, due to many sub-
stantial differences between the ID and EPI setups it
is not straightforward to identify specific reasons for
relative differences to performance at EPI.
Table 8 summarizes the core task results. There
are only modest and largely consistent differences to
the corresponding full task results, reflecting in part
the relative sparseness of additional arguments: in
the training data, for example, only approximately
3% of instances of event types that can potentially
take additional arguments had at least one additional
argument. While event modifications represent a
further 4% of full task extraction targets not required
for the core task, the overall low extraction perfor-
mance for additional arguments and modifications
limits the practical effect of these annotation cate-
gories on the performance difference between sys-
tems addressing only the core targets and those ad-
dressing the full task.
6 Discussion and Conclusions
We have presented the preparation, resources, re-
sults and analysis of the Infectious Diseases (ID)
task of the BioNLP Shared Task 2011. A corpus
of 30 full-text publications on the two-component
systems subdomain of infectious diseases was cre-
ated for the task in a collaboration of event annota-
tion and domain experts, adapting and extending the
BioNLP?09 Shared Task (ST?09) event representa-
tion to the domain.
Seven teams submitted final results to the ID task.
Despite the novel challenges of full papers, four new
entity types, extension of event scopes and the intro-
duction of a new event category for high-level pro-
cesses, the highest results for the full ID task were
comparable to the state-of-the-art performance on
the established ST?09 data, showing that the event
extraction approach and present systems generalize
well and demonstrating the feasibility of event ex-
traction for the infectious diseases domain. Analy-
sis of results suggested further opportunities for im-
proving extraction performance by combining the
strengths of various systems and the use of other
event resources.
The task design takes into account the needs
of supporting practical applications, and its results
and findings will be adopted in future development
of the Pathosystems Resource Integration Center4
(PATRIC). Specifically, PATRIC will combine do-
main named entity recognition and event extraction
to mine the virulence factor literature and integrate
the results with literature search and retrieval ser-
vices, protein feature analysis, and systems such as
Disease View.5 Present and future advances at the
ID event extraction task can thus assist biologists in
efforts of substantial public health interest.
The ID task will be continued as an open
shared task challenge with data, supporting re-
sources, and evaluation tools freely available from
the shared task site, http://sites.google.
com/site/bionlpst/.
Acknowledgments
This work was supported by Grant-in-Aid for Spe-
cially Promoted Research (MEXT, Japan). This
project has been funded in whole or in part with Fed-
eral funds from the National Institute of Allergy and
Infectious Diseases, National Institutes of Health,
Department of Health and Human Services, under
Contract No. HHSN272200900040C, awarded to
BWS Sobral.
4http://patricbrc.org
5See for example http://patricbrc.org/portal/
portal/patric/DiseaseOverview?cType=
taxon&cId=77643
33
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Peter Corbett and Peter Murray-Rust. 2006. High-
throughput identification of chemistry in life science
texts. Computational Life Sciences II, pages 107?118.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Martin Gerner, Goran Nenadic, and Casey M. Bergman.
2010. LINNAEUS: a species name identification sys-
tem for biomedical literature. BMC bioinformatics,
11(1):85+, February.
Yasuhiro Gotoh, Yoko Eguchi, Takafumi Watanabe, Sho
Okamoto, Akihiro Doi, and Ryutaro Utsumi. 2010.
Two-component signal transduction as potential drug
targets in pathogenic bacteria. Current Opinion in Mi-
crobiology, 13(2):232?239. Cell regulation.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biological
event extraction. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier, editors. 2004. Intro-
duction to the bio-entity recognition task at JNLPBA,
Geneva, Switzerland.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
Tino Krell, Jess Lacal, Andreas Busch, Hortencia Silva-
Jimnez, Mara-Eugenia Guazzaroni, and Juan Luis
Ramos. 2010. Bacterial sensor kinases: Diversity in
the recognition of environmental signals. Annual Re-
view of Microbiology, 64(1):539?559.
Thorsten Mascher, John D. Helmann, and Gottfried Un-
den. 2006. Stimulus perception in bacterial signal-
transducing histidine kinases. Microbiol. Mol. Biol.
Rev., 70(4):910?938.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing
for bionlp 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
David McClosky. 2009. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Liam McGrath, Kelly Domico, Courtney Corley, and
Bobbie-Jo Webb-Robertson. 2011. Complex biologi-
cal event extraction from full text using signatures of
linguistic and semantic features. In Proceedings of
34
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In Proceedings of COL-
ING?10, pages 779?787.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sulli-
van, Chunhong Mao, Chunxia Wang, Bruno Sobral,
Jun?ichi Tsujii, and Sophia Ananiadou. 2011. An-
notation guidelines for infectious diseases event cor-
pus. Technical report, Tsujii Laboratory, University of
Tokyo. To appear.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decompo-
sition and minimal domain adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in bionlp 2011. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Yutaka Sasaki, Yoshimasa Tsuruoka, John McNaught,
and Sophia Ananiadou. 2008. How to make the most
of NE dictionaries in statistical NER. BMC bioinfor-
matics, 9 Suppl 11.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Peter Thomason and Rob Kay. 2000. Eukaryotic sig-
nal transduction via histidine-aspartate phosphorelay.
J Cell Sci, 113(18):3141?3150.
Yue Wang, Jin-Dong Kim, Rune S?tre, Sampo Pyysalo,
and Jun?ichi Tsujii. 2009. Investigating heteroge-
neous protein annotations toward cross-corpora uti-
lization. BMC Bioinformatics, 10(403).
Chunxia Wang, Jocelyn Kemp, Isabel O. Da Fonseca,
Raymie C. Equi, Xiaoyan Sheng, Trevor C. Charles,
and Bruno W. S. Sobral. 2010. Sinorhizobium
meliloti 1021 loss-of-function deletion mutation in
chvi and its phenotypic characteristics. Molecular
Plant-Microbe Interactions, 23(2):153?160.
35
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 82?90,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
PubMed-Scale Event Extraction for Post-Translational Modifications,
Epigenetics and Protein Structural Relations
Jari Bjo?rne 1,2, Sofie Van Landeghem 3,4, Sampo Pyysalo 5, Tomoko Ohta 5,
Filip Ginter 2, Yves Van de Peer 3,4, Sophia Ananiadou 5 and Tapio Salakoski 1,2
1Turku Centre for Computer Science (TUCS), Joukahaisenkatu 3-5B, 20520 Turku, Finland
2Department of Information Technology, 20014 University of Turku, Finland
3Department of Plant Systems Biology, VIB, Technologiepark 927, 9052 Gent, Belgium
4Department of Plant Biotechnology and Bioinformatics, Ghent University, Gent, Belgium
5National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre,131 Princess Street, Manchester, UK
Abstract
Recent efforts in biomolecular event extrac-
tion have mainly focused on core event types
involving genes and proteins, such as gene
expression, protein-protein interactions, and
protein catabolism. The BioNLP?11 Shared
Task extended the event extraction approach
to sub-protein events and relations in the Epi-
genetics and Post-translational Modifications
(EPI) and Protein Relations (REL) tasks. In
this study, we apply the Turku Event Ex-
traction System, the best-performing system
for these tasks, to all PubMed abstracts and
all available PMC full-text articles, extract-
ing 1.4M EPI events and 2.2M REL relations
from 21M abstracts and 372K articles. We
introduce several entity normalization algo-
rithms for genes, proteins, protein complexes
and protein components, aiming to uniquely
identify these biological entities. This nor-
malization effort allows direct mapping of
the extracted events and relations with post-
translational modifications from UniProt, epi-
genetics from PubMeth, functional domains
from InterPro and macromolecular structures
from PDB. The extraction of such detailed
protein information provides a unique text
mining dataset, offering the opportunity to fur-
ther deepen the information provided by ex-
isting PubMed-scale event extraction efforts.
The methods and data introduced in this study
are freely available from bionlp.utu.fi.
1 Introduction
Biomedical domain information extraction has in re-
cent years seen a shift from focus on the extraction
of simple pairwise relations (Pyysalo et al, 2008;
Tikk et al, 2010) towards the extraction of events,
represented as structured associations of arbitrary
numbers of participants in specific roles (Ananiadou
et al, 2010). Domain event extraction has been pop-
ularized in particular by the BioNLP Shared Task
(ST) challenges in 2009 and 2011 (Kim et al, 2009;
Kim et al, 2011). While the BioNLP ST?09 em-
phasized protein interactions and regulatory rela-
tionships, the expressive event formalism can also
be applied to the extraction of statements regarding
the properties of individual proteins. Accordingly,
the EPI (Epigenetics and Post-Translational Modi-
fications) subchallenge of the BioNLP ST?11 pro-
vided corpora and competitive evaluations for the
detection of epigenetics and post-translational mod-
ification (PTM) events, while the REL (Entity Re-
lations) subchallenge covers structural and complex
membership relations of proteins (Ohta et al, 2011b;
Pyysalo et al, 2011). The complex memberships
and domains define the physical nature of an indi-
vidual protein, which is closely linked to its func-
tion and biological activity. Post-translational mod-
ifications alter and regulate this activity via struc-
tural or chemical changes induced by the covalent
attachment of small molecules to the protein. In
epigenetic regulation, gene expression is controlled
by the chemical modification of DNA and the his-
tone proteins supporting chromosomal DNA. All of
these aspects are important for defining the biologi-
cal role of a protein, and thus the EPI and REL tasks
enable the development of text mining systems that
can extract a more complete picture of the biomolec-
ular reactions and relations than previously possible
(cf. Table 1). Furthermore, previous work has shown
promising results for improving event extraction by
82
integration of ?static? entity relations (Pyysalo et al,
2009), in particular for the previously only available
PTM event, phosphorylation (Van Landeghem et al,
2010).
Information on protein modifications is avail-
able in general-purpose protein databases such as
UniProt, and there are also a number of dedicated
database resources covering such protein modifica-
tions (Wu and others, 2003; Lee et al, 2006; Li et
al., 2009). While the automatic extraction of PTMs
from text has also been considered in a number of
earlier studies, these have primarily involved single
PTM reactions extracted with special-purpose meth-
ods (Hu et al, 2005; Yuan et al, 2006; Lee et al,
2008). The EPI task and associated work (Ohta et
al., 2010) were the first to target numerous PTM re-
actions in a general framework using retrainable ex-
traction methods. The automatic detection of mod-
ification statements using keyword matching-based
methods has been applied also in support of DNA
methylation DB curation (Ongenaert et al, 2008;
Fang et al, 2011). However, as for PTM, the EPI
task and its preparatory efforts (Ohta et al, 2011a)
were the first to consider DNA methylation using the
general event extraction approach. To the best of our
knowledge, the present study is the first to extend the
event extraction approach to PTM and DNA methy-
lation event extraction to the scale of the entire avail-
able literature.
The Turku Event Extraction System (TEES), first
introduced for the BioNLP ST?09 (Bjo?rne et al,
2009), was updated and generalized for participa-
tion in the BioNLP ST?11, in which it had the best
performance on both the EPI and REL challenges
(Bjo?rne and Salakoski, 2011). With an F-score of
53.33% for the EPI and 57.7% for the REL task, it
performed over 16 pp better than the next best sys-
tems, making it well suited for our study. We apply
this system to the extraction of EPI events and REL
relations from all PubMed abstracts and all PMC
open access articles, using a pipeline of open source
text mining tools introduced in Bjo?rne et al (2010).
We further process the result using a recently
created bibliome-scale gene normalization dataset1.
This normalization effort connects protein and gene
mentions in text to their database IDs, a prerequi-
1Data currently under review.
site for effective use of text mining results for most
bioinformatics applications. In addition to protein
names, the EPI and REL challenges refer to the
protein substructures, modifications and complexes,
which we also need to normalize in order to deter-
mine the biological context of these events. In this
work, we develop a number of rule-based algorithms
for the normalization of such non-protein entities.
With both proteins and other entities normalized,
we can align the set of events extracted from the
literature with biological databases containing an-
notations on protein features, such as UniProt. We
can determine how many known and unknown fea-
tures we have extracted from text, and what percent-
age of various protein feature annotations our text
mining results cover. This association naturally also
works in the other direction, as we can take a gene or
protein and find yet unannotated post-translational
modifications, domains, or other features from sci-
entific articles, a promising use case for supporting
biomedical database curation.
2 Methods
2.1 PMC preprocessing
PMC full texts are distributed in an XML format that
TEES cannot use directly for event extraction. We
convert this XML into a flat ASCII text format with
a pipeline built on top of BioNLP ST?11 supporting
resource tools (Stenetorp et al, 2011). This process-
ing resolves embedded LATEX expressions, separates
blocks of text content (titles, sections, etc.) from
others, maps non-ASCII characters to corresponding
ASCII sequences, and normalizes whitespace. Re-
solving non-ASCII characters avoids increased error
rates from NLP tools trained on ASCII-only data.
2.2 Event Extraction
We use the Turku Event Extraction System for ex-
tracting both REL relations and EPI events. TEES is
a modular event extraction pipeline, that has recently
been extended for all the subtasks of the BioNLP?11
ST, including EPI and REL (Bjo?rne and Salakoski,
2011). TEES performs all supported tasks using
a shared graph scheme, which can represent both
events and relations (Figure 1 D). The system also
provides confidence scores enabling selection of the
most likely correct predictions. Before event extrac-
83
Event/relation type Example
Hydroxylation HIF-alpha proline hydroxylation
Phosphorylation (D) siRNA-mediated ATM depletion blocks p53 Serine-15 phosphorylation.
Ubiquitination K5 ubiquitinates BMPR-II on a Membrane-proximal Lysine
DNA methylation RUNX3 is frequently inactivated by P2 methylation in solid tumors.
Glycosylation Also, two asparagine residues in alpha-hCG were glycosylated.
Acetylation This interaction was regulated by Tat acetylation at lysine 50.
Methylation Methylation of lysine 37 of histone H2B is conserved.
Catalysis GRK2 catalyzed modest phosphorylation of BAC1.
Protein-Component Three enhancer elements are located in the 40 kb intron of the GDEP gene.
Subunit-Complex The most common form is a heterodimer composed of the p65/p50 subunits.
Table 1: Sentences with examples of the eight EPI event and two REL relation types, with highlighted triggers, and
protein and site arguments. Relations have no trigger and Catalysis takes as an argument another event.
Protein
Serine
Phosphorylation
of
Catalysis
is
Protein
mediated by CKI .
Cause>
REL detectionD
C
B
parsing
phosphorylation T-bet
Entity
<Theme
<Site
E
named entity recognition and normalization BANNER + GenNorm
McCJ-parser + Stanford Conversion
TEES
sentence splitting GENIA Sentence Splitter
PubMed Article Data
conversion to ST format and database import
A
Theme>
Serine of is mediated by CKI .phosphorylation T-betProteinEntity
<Protein-Component
Serine of is mediated by CKI .phosphorylation T-betNN VBN NN .NNNN VBZIN IN
<nn prep_of>
<nsubjpass
<auxpass agent>
Serine of is mediated by CKI .phosphorylation T-betProtein Protein
57765 27373
Serine of is mediated by CKI .phosphorylation T-bet
EPI detection
REL EPI
Figure 1: Event and relation extraction. Article text is
split into sentences (A), where gene/protein entities are
detected and normalized to their Entrez Gene IDs (B).
Each sentence with at least one entity is then parsed
(C). EPI events and REL relations are extracted from
the parsed sentences (D) and following conversion to
the BioNLP ST format are imported into a database (E).
(Adapted from Bjo?rne and Salakoski (2011)).
tion, protein/gene names are detected and sentences
are parsed. TEES handles all these preprocessing
steps via a pipeline of tool wrappers for the GE-
NIA Sentence Splitter (Kazama and Tsujii, 2003),
the BANNER named entity recognizer (Leaman and
Gonzalez, 2008), the McClosky-Charniak-Johnson
(McCCJ) parser (Charniak and Johnson, 2005; Mc-
Closky, 2010) and the Stanford tools (de Marneffe
et al, 2006). For a detailed description of TEES
we refer to Bjo?rne and Salakoski (2011) and for the
computational requirements of PubMed-scale event
extraction to Bjo?rne et al (2010).
2.3 Entity normalization
The extraction of events and relations as described in
the previous sections is purely text-based and does
not rely on any domain information from external
resources. This ensures generalizability of the meth-
ods to new articles possibly describing novel inter-
actions. However, practical use cases often require
integration of text mining results with external re-
sources. To enable such an integration, it is crucial to
link the retrieved information to known gene/protein
identifiers. In this section, we describe how we link
text mining data to biomolecular databases by pro-
viding integration with Entrez Gene, UniProt, Inter-
Pro and the Protein Data Bank.
2.3.1 Protein annotations
A crucial step for integrating statements in do-
main text with data records is gene name normaliza-
tion As part of a recent PubMed-scale effort,2 gene
2Data currently under review.
84
normalizations were produced by the GenNorm sys-
tem (Wei and Kao, 2011), assigning unique Entrez
Gene identifiers (Sayers and others, 2010) to am-
biguous gene/protein symbols. The GenNorm sys-
tem represents the state-of-the-art in gene normal-
ization, having achieved first rank by several evalua-
tion criteria in the BioCreative III Challenge (Lu and
others, 2011).
For practical applications, the Entrez Gene iden-
tifiers have been mapped to UniProt (The UniProt
Consortium, 2011) through conversion tables pro-
vided by the NCBI. As Entrez Gene and UniProt
are two of the most authoritative resources for gene
and protein identification, these annotations ensure
straightforward integration with other databases.
2.3.2 Complex annotations
The REL task Subunit-Complex relations all in-
volve exactly one protein complex and one of its
subunits, but the same complex may be involved in
many different Subunit-Complex relations (Pyysalo
et al, 2011). A key challenge for making use
of these relations thus involves retrieving a unique
identification of the correct complex. To identify
protein complexes, we use the Protein Data Bank
(PDB), an archive of structural data of biological
macromolecules (Berman et al, 2000). This re-
source currently contains more than 80,000 3-D
structures, and each polymer of a structure is anno-
tated with its respective UniProt ID.
To assign a unique PDB ID to an entity involved
in one or more Subunit-Complex relations, there
is usually no other lexical context than the protein
names in the sentence, e.g. ?the Rad9-Hus1-Rad1
complex?. Consequently, we rely on the normal-
ized protein names (Section 2.3.1) to retrieve a list
of plausible complexes, using data downloaded from
UniProt to link proteins to PDB entries. Ambiguity
is resolved by selecting the complex with the high-
est number of normalized proteins and giving pref-
erence to so-called representative chains. A list of
representative chains is available at the PDB web-
site, and they are determined by clustering similar
protein chains3 and taking the most confident ones
based on resolution quality.
Each assignment of a PDB identifier is annotated
with a confidence value between 0 and 1, express-
3Requiring at least 40% sequence similarity.
ing the percentage of proteins in the complex that
could be retrieved and normalized in text. For ex-
ample, even if one out of three UniProt identifiers is
wrongly assigned for a mention, the correct complex
might still be assigned with 0.66 confidence.
2.3.3 Domain annotations
Protein-Component relations define a relation be-
tween a gene/protein and one of its components,
such as a gene promoter or a protein domain. To
identify at least a substantial subset of these di-
verse relations, we have integrated domain knowl-
edge extracted from InterPro. InterPro is a rich re-
source on protein families, domains and functional
sites, integrating data from databases like PROSITE,
PANTHER, Pfam, ProDom, SMART and TIGR-
FAMs (Hunter and others, 2012). Over 23,000 dis-
tinct InterPro entries were retrieved, linking to more
than 16.5 million protein identifiers.
To assign an InterPro ID to an entity involved in
one or more Protein-Component relations, a set of
candidates is generated by inspecting the InterPro
associations of each of the proteins annotated with
that domain in text. For each such candidate, the
description of the InterPro entry is matched against
the lexical context around the entity by comparing
the number of overlapping tokens, excluding gen-
eral words, such as domain, and prepositions. The
amount of overlap is normalized against the length
of the InterPro description and expressed as a per-
centage, creating confidence values between 0 and 1.
Additionally, a simple pattern matching algorithm
recognizes statements expressing an amino acid in-
terval, e.g. ?aristaless domain (aa 527-542)?. When
such expressions are found, the intervals as anno-
tated in InterPro are matched against the retrieved
interval from text, and the confidence values express
the amount of overlap between the two intervals.
2.3.4 PTM site normalization
Six of the eight4 EPI event types refer to
post-translational modification of proteins. These
events are Hydroxylation, Phosphorylation, Ubiq-
uitination, Glycosylation, Acetylation and (Protein)
Methylation. To evaluate the events predicted
4As we are interested in PTM sites, we make no distinc-
tion between ?additive? PTMs such as Acetylation and their ?re-
verse? reactions such as Deacetylation.
85
from text, we compare these to annotated post-
translational modifications in UniProt. UniProt is
one of the largest manually curated databases for
protein knowledge, and contains annotations corre-
sponding to each of the EPI PTM event types.
We use the reviewed and manually annotated
UniProtKB/Swiss-Prot dataset (release 2012 02) in
XML format. We take for each protein all feature
elements of types modified residue, cross-link and
glycosylation site. Each of these feature elements
defines the site of the modification, either a single
amino acid, or a sequence of amino acids. We select
only annotations based on experimental findings,
that is, features that do not have a non-experimental
status (potential, probable or by similarity) to avoid
e.g. features only inferred from the sequence.
The modified residue feature type covers the event
types Hydroxylation, Phosphorylation, Acetylation
and Methylation. We determine the class of the mod-
ification with the UniProt controlled vocabulary of
post-translational modifications5. The description
attribute is the ID attribute of an entry in the vocabu-
lary, through which we can determine the more gen-
eral keyword (KW) for that description, if defined.
These keywords can then be connected to the corre-
sponding event types in the case of Hydroxylation,
Phosphorylation, Acetylation and Methylation. For
Ubiquitination events, we look for the presence of
the string ?ubiquitin? in the description attribute of
cross-link features. Finally, features corresponding
to Glycosylation events are determined by their fea-
ture element having the type glycosylation site.
The result of this selection process is a list of in-
dividual modification features, which contain a type
corresponding to one of the EPI PTM event types,
the UniProt ID of the protein, and the position and
amino acid(s) of the modification site. This data can
be compared with extracted events, using their type,
normalized protein arguments and modification site
arguments. However, we also need to normalize the
modification site arguments.
PTM sites are defined with a modification type
and the numbered target amino acid residue. In EPI
events, these residues are defined in the site argu-
ment target entities. To convert these into a form
that can be aligned with UniProt, we apply a set
5http://www.uniprot.org/docs/ptmlist/
Event Type Extracted PMC (%)
Hydroxylation 14,555 34.17
Phosphorylation 726,757 44.00
Ubiquitination 74,027 70.46
DNA methylation 140,531 52.27
Glycosylation 154,523 42.31
Acetylation 114,585 69.40
Methylation 122,015 74.86
Catalysis 45,763 67.86
Total EPI 1,392,756 51.53
Protein-Component 1,613,170 52.59
Subunit-Complex 537,577 51.18
Total REL 2,150,747 52.23
Table 2: Total number of EPI events and REL relations
extracted from PubMed abstracts and PMC full-text arti-
cles, with the fractions extracted from PMC.
of rules that try to determine whether a site is an
amino acid. We start from the main site token, and
check whether it is of the form AA#, where AA is an
amino acid name, or a one or three letter code, and
# an optional site number, which can also be in a to-
ken following the amino acid. For cases where the
site entity is the word ?residue? or ?residues?, we
look for the amino acid definition in the preceding
and following tokens. All strings are canonicalized
with removal of punctuation, hyphens and parenthe-
sis before applying the rules. In total, of the 177,994
events with a site argument, 75,131 could be nor-
malized to an amino acid, and 60,622 of these to a
specific residue number.
3 Results
The source for extraction in this work is the set of 21
million PubMed abstracts and 372 thousand PMC
open-access full-text articles. From this dataset,
1.4M EPI events and 2.2M REL relations were ex-
tracted (Table 2). For both tasks, about half of the
results were extracted from PMC, confirming that
full-text articles are an important source of infor-
mation for these extraction targets. The total num-
bers of events and relations are considerably lower
than e.g. the 21.3M events extracted for the GENIA
task from PubMed abstracts (Bjo?rne et al, 2010;
Van Landeghem et al, 2012), likely relating to the
comparatively low frequency with which EPI and
REL extraction targets are discussed with respect to
the basic GENIA biomolecular reactions.
86
Event type UniProt Events Match Coverage Events (site) Match Coverage
Hydroxylation 1,587 14,555 1,526 19 4,298 130 5
Phosphorylation 57,059 726,757 286,978 4,795 86,974 9,732 748
Ubiquitination 792 74,027 4,994 143 10,562 54 20
Glycosylation 6,708 154,523 18,592 897 22,846 68 31
Acetylation 6,522 114,585 15,470 764 25,689 158 30
Methylation 1,135 122,015 2,178 113 27,625 36 10
Total 73,803 1,206,462 329,738 6,731 177,994 10,178 844
Table 3: PTM events. PTMs that are not marked with non-experimental qualifiers are taken from UniProt. The
Events column lists the total number of predicted events, and the Events (site) the number of events that also have a
predicted site-argument. For these groups, Match is the number of events that matches a known PTM from UniProt,
and Coverage the number of UniProt PTMs for which at least one match exists. For Events matching takes into account
the PTM type and protein id, for Events (site) also the amino acid and position of the modified residue.
Event type AA UP # Highest confidence event Article ID
Phosphorylation S9 ? 2 p53 isolated from ML1, HCT116 and RKO cells, after short
term genotoxic stress, were phosphorylated on Ser 6, Ser 9
PMC:2777442
Acetylation S15 4 phosphorylated (Ser15), acetylated p53(Lys382) PMC:2557062
Methylation S15 1 phosphorylation of p53 at serine 15 and acetylation PM:10749144
Phosphorylation S15 ? 238 Chk2, as well as p53 Ser(15) phosphorylation and its PM:16731759
Phosphorylation T18 ? 12 p53 stabilization and its phosphorylation in Thr18 PMC:3046209
Phosphorylation S20 ? 45 that phosphorylation of p53 at Ser20 leads to PMC:3050855
Phosphorylation S33 ? 14 phosphorylation of p53 at serine 33 may be part of PMC:35361
Phosphorylation S37 ? 20 serine 33 of p53 in vitro when serine 37 is already PMC:35361
Phosphorylation S46 ? 55 phosphorylation of p53, especially at Serine 46 by PMC:2634840
Phosphorylation T55 ? 7 that phosphorylation of p53 at Thr55 inhibits its PMC:3050855
Phosphorylation S99 ? 0
Phosphorylation S183 ? 0
Phosphorylation S269 ? 0
Phosphorylation T284 ? 0
Ubiquitination K291 ? 0
Acetylation K292 ? 0
Ubiquitination K292 ? 0
Acetylation K305 ? 0
Phosphorylation S313 ? 1 hyperphosphorylation of p53, particularly of Ser313 PM:8649812
Phosphorylation S314 ? 0
Phosphorylation S315 ? 6 to require phosphorylation of p53 at serine 315 (35) PMC:2532731
Methylation K370 ? 6 by methylating lysine 370 of p53 PMC:1636665
Acetylation K372 1 for lysine 372 and 383 acetylated p53 (Upstate, PMC:1315280
Methylation K372 ? 5 methylation of p53 by the KMT7(SET7/9) methyltransferase
enzyme on Lys372
PMC:2794343
Acetylation K373 ? 16 p53 and acetylated p53 (lysine-373 and lysine-382) PMC:1208859
Methylation K373 ? 4 EHMT1-mediated p53 methylation at K373 PM:20588255
Acetylation K381 ? 0
Acetylation K382 ? 82 p53 acetylation at lysine 382 was found not PM:17898049
Methylation K382 ? 6 SET8 specifically monomethylates p53 at lysine 382 PM:17707234
Methylation K386 ? 1 that sumoylation of p53 at K386 blocks subsequent PM:19339993
Phosphorylation S392 ? 35 and phosphorylation of p53 at S392 PM:17237827
Table 4: Extracted and known PTM sites of p53. The type and site of the modification are in the first two columns.
UP indicates whether the PTM is present in the UniProt annotation for p53. Column # shows the number of extracted
events, followed by the event with the highest confidence score and the PubMed abstract or PMC full-text article it has
been extracted from.
87
3.1 Extracted PTMs compared to UniProt
The EPI PTM events were compared to annotated
PTMs from UniProt (Table 3). The majority of ex-
tracted PTM events (85%) have only a protein ar-
gument, and no information about the modification
site, so these can only be compared by the protein
id and PTM type. For the subset of proteins that
also have a site, which can be normalized to an
amino acid position, we can make a detailed com-
parison with UniProt. Finding a match for these
normalized amino acids is more difficult, and for
both categories, only a small fraction of proteins
from UniProt is covered. In part this may be due
to the limitations of the gene name normalization, as
finding the exact species-specific protein ID remains
a challenging task (Lu and others, 2011). How-
ever, even if the overall coverage is limited, well-
known protein modifications can be assigned to spe-
cific residues, as we show in the next section.
3.2 Extracted PTMs for a single protein
For an in-depth example of PTM modifications, we
study the protein p53, a central tumor suppressor
protein that is the subject of many studies. p53 is
also among the proteins with the most UniProt PTM
sites for which EPI events were predicted, making it
a good example for a case study (see Table 4).
We take from UniProt all known p53 PTMs corre-
sponding to our EPI event types and list the number
of predicted events for them (see Table 4). When
the number of predicted events is high, the most
confident prediction is usually a correctly extracted,
clear statement about the PTM. All events for PTMs
known in UniProt are correct except for the type
of K386. For events not in UniProt, the two S15
ones are false positives, and K372 acetylation, while
correctly extracted, is most likely a typo in the arti-
cle. For the PTMs for which no event was extracted,
we checked the reference article from UniProt an-
notation. K291, K292 ubiquitination, and K305 are
from abstracts, and thus missed events. S183, S269
and T284 are from a non-open access PMC article,
while S99, K292 acetylation, K305, S314 and K381
are from Excel or PDF format supplementary tables,
sources outside our extraction input.
In total, we have extracted 561 PTM events re-
lated to p53, 554 of which correspond to a PTM an-
Item PubMeth Extracted Recall
PMID+UPID 2776 1698 61.2%
UPID 392 363 92.6%
PMID 1163 1120 96.3%
Table 5: Evaluation of DNA methylation event extraction
recall against PubMeth.
notated in UniProt. Of the 28 EPI-relevant PTMs on
p53, 17 have at least one predicted event. The high-
est confidence events are about equally often from
abstracts as from full texts.
3.3 DNA methylation analysis
Two recently introduced databases, PubMeth (On-
genaert et al, 2008) and MeInfoText (Fang et al,
2011) provide manually curated information on
DNA methylation, primarily as it relates to cancer.
To evaluate the coverage of DNA methylation event
extraction, we focus here on PubMeth, as the full
content of this database could be directly used. Each
PubMeth DB record provides the primary name of
the methylated gene and the PMID of the publica-
tion supporting the curation of the record. We used
these two pieces of information to evaluate the recall
6 of DNA methylation event extraction.
We mapped PubMeth entries to UniProt iden-
tifiers (UPIDs), and extracted all unique (PMID,
UPID) pairs from both PubMeth and the automat-
ically extracted DNA methylation/demethylation
events. The results of comparison of these sets of
ID pairs are given in Table 5. We find that for over
60% of PubMeth entries, the system is able to re-
cover the specific (document, gene) pair. This result
is broadly in line with the recall of the system as
evaluated in the BioNLP ST. However, if the match-
ing constraint is relaxed, asking either 1) can the sys-
tem extract the methylation of each gene in PubMeth
somewhere in the literature or, inversely, 2) can the
system detect some DNA methylation event in each
document included in PubMeth as evidence, recall
is over 90%. In particular, the evaluation indicates
that the system shows very high recall for identify-
ing documents discussing DNA methylation.
6As PubMeth does not aim for exhaustive coverage, preci-
sion cannot be directly estimated in this way. For example, Pub-
Meth covers fewer than 2,000 documents and DNA methylation
events were extracted from over 20,000, but due to differences
in scope, this does not suggest precision is below 10%.
88
REL Type Extracted Match (p) Match (e)
Prot-Cmp 1613.1K 561.8K 150.7K
SU-Cmplx 537.6K 226.5K 99.6K
Table 6: Numbers of extracted entity relations, with the
protein (p) or both protein and entity (e) identified.
3.4 REL statistics
Table 6 presents the amount of extracted entity re-
lations and the coverage of the normalization algo-
rithms assigning protein, domain and complex iden-
tifiers. From a total of 537.6K Subunit-Complex re-
lations, 226.5K (42%) involve a protein that could be
unambiguously identified (Section 2.3.1). From this
subset, 99.6K relations (44%) could be assigned to a
PDB complex identifier (Section 2.3.2), accounting
for 3800 representative 3D protein structures.
The Protein-Component relations are much more
frequent in the data (1.6M relations) and here 35%
of the relations (561.8K) involve a normalized pro-
tein mention. The assignment of InterPro domains
to these Protein-Component relations (Section 2.3.3)
further covers 150.7K relations in this subset (27%),
identifying 5500 distinct functional domains. The
vast majority of these annotations (99%) are pro-
duced by matching the lexical context against the
InterPro descriptions, and only a few cases (200)
matched against the amino-acid pattern.
4 Conclusions
We have combined state-of-the-art methods for
gene/protein name normalization together with the
best available methods for event-based extraction
of protein post-translational modifications, reactions
relating to the epigenetic control of gene expres-
sion, and part-of relations between genes/proteins,
their components, and complexes. These methods
were jointly applied to the entire available litera-
ture, both PubMed abstracts and PMC full-text doc-
uments, creating a text mining dataset unique in both
scope and breadth of analysis. We further performed
a comprehensive analysis of the results of this au-
tomatic extraction process against major biological
database resources covering various aspects of the
extracted information. This analysis indicated that
text mining results for protein complexes, substruc-
tures and epigenetic DNA methylation provides al-
ready quite extensive coverage of relevant proteins.
For post-translational modifications, we note that
coverage still needs to be improved, but conclude
that the extracted events already provide a valuable
link to PTM related literature. In future work we
hope to further extend the event types extracted by
our PubMed-scale approach. The extraction meth-
ods as well as all data introduced in this study are
freely available from bionlp.utu.fi.
Acknowledgments
We thank the Academy of Finland, the Research
Foundation Flanders (FWO) and the UK BBSRC
(reference number: BB/G013160/1) for funding,
and CSC ? IT Center for Science Ltd for compu-
tational resources.
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Helen M. Berman, John Westbrook, Zukang Feng,
Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N.
Shindyalov, and Philip E. Bourne. 2000. The protein
data bank. Nucleic Acids Research, 28(1):235?242.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 183?191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Scaling up biomedical
event extraction to the entire PubMed. In Proceedings
of the BioNLP 2010 Workshop, pages 28?36.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
ACL, pages 173?180.
Y.C. Fang, P.T. Lai, H.J. Dai, and W.L. Hsu. 2011. Me-
infotext 2.0: gene methylation and cancer relation ex-
traction from biomedical literature. BMC bioinformat-
ics, 12(1):471.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
89
Sarah Hunter et al 2012. Interpro in 2011: new devel-
opments in the family and domain prediction database.
Nucleic Acids Research, 40(D1):D306?D312.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP 2003,
pages 137?144.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP 2009, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of the BioNLP Shared Task 2011, pages 1?6.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedical
named entity recognition. Pacific Symposium on Bio-
computing, pages 652?663.
Tzong-Yi Lee, Hsien-Da Huang, Jui-Hung Hung, Hsi-
Yuan Huang, Yuh-Shyong Yang, and Tzu-Hao Wang.
2006. dbPTM: an information repository of pro-
tein post-translational modification. Nucleic acids re-
search, 34(suppl 1):D622?D627.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein lig-
ases. Nucl. Acids Res., 36(suppl.2):W416?422.
Hong Li, Xiaobin Xing, Guohui Ding, Qingrun Li, Chuan
Wang, Lu Xie, Rong Zeng, and Yixue Li. 2009.
SysPTM: A Systematic Resource for Proteomic Re-
search on Post-translational Modifications. Molecular
& Cellular Proteomics, 8(8):1839?1849.
Zhiyong Lu et al 2011. The gene normalization task
in BioCreative III. BMC Bioinformatics, 12(Suppl
8):S2+.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of LREC-06, pages 449?454.
David McClosky. 2010. Any domain parsing: auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Department of Computer Science,
Brown University.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2011a. Event extraction for
DNA methylation. Journal of Biomedical Semantics,
2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii.
2011b. Overview of the epigenetics and post-
translational modifications (EPI) task of BioNLP
Shared Task 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 16?25.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methy-
lation database combining text-mining and expert
annotation. Nucl. Acids Res., 36(suppl 1):D842?846.
Sampo Pyysalo, Antti Airola, Juho Heimonen, and Jari
Bjo?rne. 2008. Comparative analysis of five protein-
protein interaction corpora. BMC Bioinformatics,
9(Suppl. 3):S6.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece in the
biomedical information extraction puzzle. In Proceed-
ings of the BioNLP 2009 Workshop, pages 1?9.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii. 2011.
Overview of the entity relations (REL) supporting task
of BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 83?88.
Eric W. Sayers et al 2010. Database resources of the na-
tional center for biotechnology information. Nucleic
Acids Research, 38(suppl 1):D5?D16.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Bionlp shared task 2011: Supporting resources. In
Proceedings of BioNLP Shared Task 2011 Workshop,
pages 112?120.
The UniProt Consortium. 2011. Ongoing and future de-
velopments at the universal protein resource. Nucleic
Acids Research, 39(suppl 1):D214?D219.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehen-
sive benchmark of kernel methods to extract protein-
protein interactions from literature. PLoS Comput
Biol, 6(7):e1000837, 07.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of BioNLP?10, pages 144?152.
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Ginter.
2012. Exploring biomolecular literature with EVEX:
Connecting genes through events, homology and indi-
rect associations. Advances in Bioinformatics.
Chih-Hsuan Wei and Hung-Yu Kao. 2011. Cross-species
gene normalization by species inference. BMC bioin-
formatics, 12(Suppl 8):S5.
Cathy H. Wu et al 2003. The Protein Information Re-
source. Nucl. Acids Res., 31(1):345?347.
X. Yuan, ZZ Hu, HT Wu, M. Torii, M. Narayanaswamy,
KE Ravikumar, K. Vijay-Shanker, and CH Wu. 2006.
An online literature mining tool for protein phospho-
rylation. Bioinformatics, 22(13):1668.
90
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 100?108,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
New Resources and Perspectives for Biomedical Event Extraction
Sampo Pyysalo1, Pontus Stenetorp2, Tomoko Ohta1, Jin-Dong Kim3 and Sophia Ananiadou1
1National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK
2Tokyo University, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
3Database Center for Life Science, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan
Abstract
Event extraction is a major focus of re-
cent work in biomedical information extrac-
tion. Despite substantial advances, many chal-
lenges still remain for reliable automatic ex-
traction of events from text. We introduce a
new biomedical event extraction resource con-
sisting of analyses automatically created by
systems participating in the recent BioNLP
Shared Task (ST) 2011. In providing for the
first time the outputs of a broad set of state-of-
the-art event extraction systems, this resource
opens many new opportunities for studying
aspects of event extraction, from the identifi-
cation of common errors to the study of ef-
fective approaches to combining the strengths
of systems. We demonstrate these opportuni-
ties through a multi-system analysis on three
BioNLP ST 2011 main tasks, focusing on
events that none of the systems can success-
fully extract. We further argue for new per-
spectives to the performance evaluation of do-
main event extraction systems, considering a
document-level, ?off-the-page? representation
and evaluation to complement the mention-
level evaluations pursued in most recent work.
1 Introduction
Biomedical information extraction efforts are in-
creasingly focusing on event extraction using struc-
tured representations that allow associations of arbi-
trary numbers of participants in specific roles (e.g.
Theme, Cause) to be captured (Ananiadou et al,
2010). Domain event extraction has been advanced
in particular by the BioNLP Shared Task (ST) events
(Kim et al, 2011a; Kim et al, 2011b), which have
introduced common task settings, datasets, and eval-
uation criteria for event extraction. Participants in
these shared tasks have introduced dozens of sys-
tems for event extraction, and the resulting methods
have been applied to automatically analyse the entire
available domain literature (Bjo?rne et al, 2010) and
applied in support of applications such as semantic
literature search (Ohta et al, 2010; Van Landeghem
et al, 2011b) and pathway curation support (Kemper
et al, 2010).
It is possible to assess recent advances in event ex-
traction through results for a task considered both in
the BioNLP ST 2009 and 2011. By the primary eval-
uation criteria, the highest performance achieved in
the 2009 task was 51.95% F-score, and a 57.46% F-
score was reached in the comparable 2011 task (Kim
et al, 2011b). These results demonstrate significant
advances in event extraction methods, but also indi-
cate that the task continues to hold substantial chal-
lenges. This has led to a call from task participants
for further analysis of the data and results, accompa-
nied by a proposal to release analyses from individ-
ual systems to facilitate such analysis (Quirk et al,
2011).
In this study, we explore new perspectives into the
analyses and performance of event extraction meth-
ods. We build primarily on a new resource compiled
with the support of the majority of groups participat-
ing in the BioNLP ST 2011, consisting of analyses
from systems for the three main tasks sharing the
text-bound event representation. We demonstrate
the use of this resource through an evaluation fo-
cusing on events that cannot be extracted even by
the union of combined systems, identifying partic-
ular remaining challenges for event extraction. We
further propose and evaluate an alternate, document-
level perspective to event extraction, demonstrat-
ing that when only unique events are considered for
100
Figure 1: Example event annotations. The ?crossed-out? event type identifies an event marked as negated. Event
illustrations created using the STAV visualization tool (Stenetorp et al, 2011).
each document, the measured performance and even
ranking of systems participating in the shared task is
notably altered.
2 Background
In this work, we focus on the definition of the
event extraction task first introduced in the BioNLP
Shared Task 2009.1 The task targets the extrac-
tion of events, represented as n-ary associations of
participants (entities or other events), each marked
as playing a specific role such as Theme or Cause
in the event. Each event is assigned a type such
as BINDING or PHOSPHORYLATION from a fixed,
task-specific set. Events are further typically associ-
ated with specific trigger expressions that state their
occurrence in text. As physical entities such as pro-
teins are also identified in the setting with specific
spans referring to the real-world entities in text, the
overall task is ?text-bound? in the sense of requiring
not only the extraction of targeted statements from
text, but also the identification of specific regions of
text expressing each piece of extracted information.
Events can further be marked with modifiers iden-
tifying additional features such as being explicitly
negated or stated in a speculative context. Figure 1
shows an illustration of event annotations.
This BioNLP ST 2009 formulation of the event
extraction task was followed also in three 2011 main
tasks: the GE (Kim et al, 2011c), ID (Pyysalo et al,
2011a) and EPI (Ohta et al, 2011) tasks. A vari-
ant of this representation that omits event triggers
was applied in the BioNLP ST 2011 bacteria track
(Bossy et al, 2011), and simpler, binary relation-
type representations were applied in three support-
ing tasks (Nguyen et al, 2011; Pyysalo et al, 2011b;
Jourde et al, 2011). Due to the challenges of con-
sistent evaluation and processing for tasks involv-
1While far from the only formulation proposed in the litera-
ture, this specific task setting is the most frequently considered
and arguably a de facto standard for domain event extraction.
ing different representations, we focus in this work
specifically on the three 2011 main tasks sharing a
uniform representation: GE, ID and EPI.
3 New Resources for Event Extraction
In this section, we present the new collection of au-
tomatically created event analyses and demonstrate
one use of the data through an evaluation of events
that no system could successfully extract.
3.1 Data Compilation
Following the BioNLP ST 2011, the MSR-NLP
group called for the release of outputs from various
participating systems (Quirk et al, 2011) and made
analyses of their system available.2 Despite the ob-
vious benefits of the availability of these resources,
we are not aware of other groups following this ex-
ample prior to the time of this publication.
To create the combined resource, we approached
each group that participated in the three targeted
BioNLP ST 2011 main tasks to ask for their support
to the creation of a dataset including analyses from
their event extraction systems. This suggestion met
with the support of all but a few groups that were
approached.3 The groups providing analyses from
their systems into this merged resource are summa-
rized in Table 1, with references to descriptions of
the systems used to create the included analyses. We
compiled for each participant and each task both the
final test set submission and a comparable submis-
sion for the separate development set.
As the gold annotations for the test set are only
available for evaluation through an online interface
(in order to avoid overfitting and assure the compa-
rability of results), it is important to provide also de-
velopment set analyses to permit direct comparison
2http://research.microsoft.com/bionlp/
3We have yet to hear back from a few groups, but none has
yet explicitly denied the release of their data. Should any re-
maining group accept the release of their data, we will release a
new, extended version of the resource.
101
Task System
Team GE EPI ID BB BI CO REL REN description
UTurku 1 1 1 1 1 1 1 1 Bjo?rne and Salakoski (2011)
ConcordU 1 1 1 1 1 1 Kilicoglu and Bergler (2011)
UMass 1 1 1 Riedel and McCallum (2011)
Stanford 1 1 1 McClosky et al (2011)
FAUST 1 1 1 Riedel et al (2011)
MSR-NLP 1 1 Quirk et al (2011)
CCP-BTMG 1 1 Liu et al (2011)
BMI@ASU 1 Emadzadeh et al (2011)
TM-SCS 1 Bui and Sloot (2011)
UWMadison 1 Vlachos and Craven (2011)
HCMUS 1 1 Le Minh et al (2011)
PredX 1 -
VIBGhent 1 Van Landeghem et al (2011a)
Table 1: BioNLP ST 2011 participants contributing to the combined resource.
Events
Task Gold FN Recall
GE (task 1) 3250 1006 69.05%
EPI (CORE task) 601 129 78.54%
ID (CORE task) 691 183 73.52%
Table 2: Recall for the union of analyses from systems
included in the combined dataset.
against gold annotations. The inclusion of both de-
velopment and test set annotations also allows e.g.
the study of system combination approaches where
the combination parameters are estimated on devel-
opment data for final testing on the test set (Kim et
al., 2011a).
3.2 Evaluation
We demonstrate the use of the newly compiled
dataset through a manual evaluation of GE, EPI and
ID main task development set gold standard events
that are not extracted by any of the systems for
which analyses were available.4 We perform eval-
uation on the GE subtask 1 and the EPI and ID
task CORE subtasks, as all participating systems ad-
dressed the extraction targets of these subtasks.
We first evaluated each of the analyses against the
development set of the respective task using the of-
ficial shared task evaluation software, using options
for the evaluation tools to list the sets of true posi-
tive (TP), false positive (FP) and false negative (FN)
4The final collection includes analyses from the systems of
two groups that agreed to the release of their data after the com-
pletion of this analysis, but we expect the results to largely hold
also for the final collection.
events. We then selected for each of the three tasks
the set of events that were included in the FN list
for all systems. This gives the results for the re-
call of the union of all systems shown in Table 2.
The recall of the system union is approximately 30%
points higher than that of any individual GE system
(Kim et al, 2011c) and 25% points higher for EPI
and ID (Ohta et al, 2011; Pyysalo et al, 2011a),
suggesting potential remaining benefits from system
combination. Nevertheless, a substantial fraction of
the total set of gold events remains inaccessible also
to this system union.
We then selected a random set of 100 events from
each of the three sets of events that were not re-
covered by any system (i.e. 300 events in total) and
performed a manual evaluation to identify frequent
properties of these events that could contribute to
extraction failures. In brief, we first performed a
brief manual evaluation to identify common charac-
teristics of these events, and then evaluated the 300
events individually to identify the set of these char-
acteristics that apply to each event.
The results of the evaluation for common cases
are shown in Table 3. We find that the most fre-
quent property of the unrecoverable events is that
they involve implicit arguments (Gerber and Chai,
2010), a difficult challenge that has not been ex-
tensively considered in domain event extraction. A
closely related issue are events involving arguments
in a sentence different from that containing the trig-
ger (?cross-sentence?), connected either implicitly
or through explicit coreference (?coreference?). Al-
102
Type GE EPI ID Total
Implicit argument 18 33 15 66
Cross-sentence 14 40 4 58
Weak trigger 28 14 11 53
Coreference 12 20 18 50
Static Relation 6 28 6 40
Error in gold 17 4 9 30
Ambiguous type 2 9 11 22
Shared trigger 2 12 1 15
Table 3: Manual evaluation results for features of events
that could not be recovered by any system.
though coreference was considered as as separate
task in BioNLP ST 2011 (Nguyen et al, 2011), it is
clear that it involves many remaining challenges for
event extraction systems. Similarly, events where
explicit arguments are connected to other arguments
through ?static? relations such as part-of (e.g. ?A
binds the X domain of B?) represent a known chal-
lenge (Pyysalo et al, 2011b). These results sug-
gest that further advances in event extraction perfor-
mance could be gained by the integration of systems
for the analysis of coreference and static relations,
approaches for which some success has already been
demonstrated in recent efforts (Van Landeghem et
al., 2010; Yoshikawa et al, 2011; Miwa et al, 2012).
?Weak? trigger expressions that must be inter-
preted in context to determine whether they express
an event, as well as a related class of events whose
type must be disambiguated with reference to con-
text (?ambiguous type?) are comparatively frequent
in the three tasks, while EPI in particular involves
many cases where a trigger is shared between mul-
tiple events ? an issue for approaches that assume
each token can be assigned at most a single class.
Finally, we noted a number of cases that we judged
to be errors in the gold annotation; the number
is broadly in line with the reported inter-annotator
agreement for the data (see e.g. Ohta et al (2011)).
While there is an unavoidable subjective com-
ponent to evaluations such as this, we note that a
similar evaluation performed following the BioNLP
Shared Task 2009 using test set data reached broadly
comparable results (Kim et al, 2011a). The newly
compiled dataset represents the first opportunity for
those without direct access to the test set data and
submissions to directly assess the task results, as
demonstrated here. We hope that this resource will
encourage further exploration of both the data, the
system analyses and remaining challenges in event
extraction.
4 New Perspectives to Event Extraction
As discussed in Section 2, the BioNLP ST event ex-
traction task is ?text-bound?: each entity and event
annotation is associated with a specific span of text.
Contrasted to the alternative approach where anno-
tations are document-level only, this approach has
a number of important benefits, such as allowing
machine learning methods for event extraction to
be directly trained on fully and specifically anno-
tated data without the need to apply frequently error-
prone heuristics (Mintz et al, 2009) or develop ma-
chine learning methods addressing the mapping be-
tween text expressions and document-level annota-
tions (Riedel et al, 2010). Many of the most suc-
cessful event extraction approaches involve direct
training of machine learning methods using the text-
bound annotations (Riedel and McCallum, 2011;
Bjo?rne and Salakoski, 2011; McClosky et al, 2011).
However, while the availability of text-bound anno-
tations in data provided to task participants is clearly
a benefit, there are drawbacks to the choice of ex-
clusive focus on text-bound annotations in system
output, including issues relating to evaluation and
the applicability of methods to the task. In the fol-
lowing section, we discuss some of these issues and
propose alternatives to representation and evaluation
addressing them.
4.1 Evaluation
The evaluation of the BioNLP ST is instance-based
and text-bound: each event in gold annotation and
each event extracted by a system is considered in-
dependently, separating different mentions of the
?same? real-world event. This is the most detailed
(sensitive) evaluation setting permitted by the data,
and from a technical perspective a reasonable choice
for ranking systems performing the task.
However, from a practical perspective, this eval-
uation setting arguably places excessively strict de-
mands on systems, and may result in poor correla-
tion between measured performance and the practi-
cal value of systems. Our motivating observations
are that specific real-world events tend to be men-
103
tioned multiple times in a single publication ? espe-
cially the events that are of particular importance in
the study ? and that there are few practical applica-
tions for which it is necessary to find each such re-
peated mention. For example, in literature search for
e.g. pathway or database curation support, one typi-
cal information need is to identify biomolecular re-
actions involving a specific protein. Event extraction
can support such needs either by summarizing all
events involving the protein that could be extracted
from the literature (Van Landeghem et al, 2011b), or
by retrieving documents (perhaps showing relevant
text snippets) containing such events (Ohta et al,
2010). For the former to meet the information need,
it may be sufficient that each different event is ex-
tracted once from the entire literature; for the latter,
once from each relevant document. For uses such
as these, there is no obvious need for, or, indeed,
no very obvious benefit from the ability of extrac-
tion systems to separately enumerate every mention
of every event in every publication. It is easy to en-
vision other practical use cases where instance-level
extraction performance is at best secondary and, we
argue, difficult to identify ones where it is of critical
importance.
For applications such as these, the important
question is the reliability of the system at identify-
ing events either on the level of documents or on the
level of (a relevant subset of) the literature, rather
than on the level of individual mentions. For a more
complete and realistic picture of the practical value
of event extraction methods, measures other than
instance-level should thus also be considered.
4.2 Task setting
While applications can benefit from the ability of
IE systems to identify a specific span of text sup-
porting extracted information,5 the requirement of
the BioNLP ST setting that the output of event ex-
traction systems must identify specific text spans for
each entity and event makes it complex or impossi-
ble to address the task using a number of IE methods
that might otherwise represent feasible approaches
to event extraction.
5For example, for curation support tasks, this allows the hu-
man curator to easily check the correctness of extracted infor-
mation and helps to select ?evidence sentences?, as included in
many databases.
For example, Patwardhan and Riloff (2007) and
Chambers and Jurafsky (2011) consider an IE ap-
proach where the extraction targets are MUC-4 style
document-level templates (Sundheim, 1991), the
former a supervised system and the latter fully un-
supervised. These methods and many like them for
tasks such as ACE (Doddington et al, 2004) work
on the document level, and can thus not be readily
applied or evaluated against the existing annotations
for the BioNLP shared tasks. Enabling the appli-
cation of such approaches to the BioNLP ST could
bring valuable new perspectives to event extraction.
4.3 Alternative evaluation
We propose a new mode of evaluation that otherwise
follows the primary BioNLP ST evaluation criteria,
but incorporates the following two exceptions:
1. remove the requirement to match trigger spans
2. only require entity texts, not spans, to match
The first alternative criterion has also been previ-
ously considered in the GE task evaluation (Kim et
al., 2011c); the latter has, to the best of our knowl-
edge, not been previously considered in domain
event extraction. We additionally propose to con-
sider only the minimal set of events that are unique
on the document level (under the evaluation criteria),
thus eliminating effects from repeated mentions of a
single event on evaluated performance. We created
tools implementing this mode of evaluation with ref-
erence to the BioNLP ST 2011 evaluation tools.
While this type of evaluation has, to the best of
our knowledge, not been previously applied specif-
ically in biomedical event extraction, it is closely
related (though not identical) to evaluation criteria
applied in MUC, ACE, and the in-domain PPI re-
lation extraction tasks in BioCreative (Krallinger et
al., 2008).
4.4 Alternative representation
A true conversion to a document-level, ?off the
page? representation would require manual anno-
tation efforts to identify the real-world entities and
events referred to in text (Doddington et al, 2004).
However, it is possible to reasonably approximate
such a representation through an automatic heuristic
conversion.
104
BioNLP Shared Task
T1 Protein 0 5 CIITA
T2 Protein 21 28 TAFII32
T3 Binding 6 15 interacts
E1 Binding:T3 Theme:T1 Theme2:T2
T4 Protein 54 61 TAFII32
T5 Protein 66 71 CIITA
T6 Binding 33 45 interactions
E2 Binding:T6 Theme:T4 Theme2:T5
Document level
T1 Protein CIITA
T2 Protein TAFII32 
E1 Binding Theme:T1 Theme2:T2
CIITA interacts with TAFII32 ... interactions between TAFII32 and CIITA are
Pro Binding Protein Binding Protein ProTh Th2 Theme
Theme2
...
Figure 2: Illustration of BioNLP Shared Task annotation format and the proposed document-level (?off-the-page?)
format.
We first introduce a non-textbound annotation for-
mat that normalizes over differences in e.g. argu-
ment order and eliminates duplicate events. The for-
mat largely follows that of the shared task but re-
moves any dependencies and references to text off-
sets (see Figure 2). The conversion process into this
representation involves a number of steps. First, we
merge duplicate pairs of surface strings and types,
as different mentions of the same entity in different
parts of the text are no longer distinguishable in the
representation. In the original format, equivalence
relations (Kim et al, 2011a) are annotated only for
specific mentions. When ?raising? the annotations
to the document level, equivalence relations are rein-
terpreted to cover the full document by extending
the equivalence to all mentions that share the surface
form and type with members of existing equivalence
classes. Finally, we implemented an event equiv-
alence comparison to remove duplicate annotations
from each document. The result of the conversion
to this alternate representation is thus an ?off-the-
page? summary of the unique set of events in the
document.
This data can then be used for training and com-
parison of methods analogously to the original anno-
tations, but without the requirement that all analyses
include text-bound annotations.
4.5 Experimental Results
We next present an evaluation using the alternative
document-level event representation and evaluation,
comparing its results to those for the primary shared
task evaluation criteria. As comparatively few of the
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 49.41 64.75 56.04 53.10 67.56 59.46
UMass 48.49 64.08 55.20 52.55 66.57 58.74
UTurku 49.56 57.65 53.30 54.23 60.11 57.02
MSR-NLP 48.64 54.71 51.50 53.55 58.24 55.80
ConcordU 43.55 59.58 50.32 47.42 60.85 53.30
UWMadison 42.56 61.21 50.21 46.09 62.50 53.06
Stanford 42.36 61.08 50.03 46.48 63.22 53.57
BMI@ASU 36.91 56.63 44.69 41.15 61.44 49.29
CCP-BTMG 31.57 58.99 41.13 34.82 66.89 45.80
TM-SCS 32.73 45.84 38.19 38.02 50.87 43.51
HCMUS 10.12 27.17 14.75 14.50 40.05 21.29
Table 4: Comparison of BioNLP ST 2011 GE task 1 re-
sults.
shared task participants attempted subtasks 2 and 3
for GE or the FULL task setting for EPI and ID, we
consider only GE subtask 1 and the EPI and ID task
CORE extraction targets in these experiments. We
refer to the task overviews for the details of the sub-
tasks and the primary evaluation criteria (Kim et al,
2011c; Pyysalo et al, 2011a; Ohta et al, 2011).
Tables 4, 5 and 6 present the results for the
GE, EPI and ID tasks, respectively. For GE, we
see consistently higher F-scores for the new crite-
ria, in most cases reflecting primarily an increase
in recall, but also involving increases in precision.
The F-score differences range between 3-4% for
most high-ranking systems, with more substantial
increases for lower-ranking systems. Notable in-
creases in precision are seen for some systems (e.g.
HCMUS), indicating that the systems comparatively
frequently extract correct information, but associ-
ated with the wrong spans of text.
105
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
UTurku 68.51 69.20 68.86 74.20 69.14 71.58
FAUST 59.88 80.25 68.59 67.04 76.82 71.60
MSR-NLP 55.70 77.60 64.85 59.24 77.66 67.21
UMass 57.04 73.30 64.15 65.76 69.65 67.65
Stanford 56.87 70.22 62.84 62.74 67.12 64.86
CCP-BTMG 45.06 63.37 52.67 54.62 63.17 58.58
ConcordU 40.28 76.71 52.83 48.41 76.57 59.32
Table 5: Comparison of BioNLP ST 2011 EPI CORE
task results.
For EPI (Table 5), we find comparable differences
in F-score to those for GE, but there is a signifi-
cant difference in the precision-recall balance: the
majority of systems show over 5% points higher re-
call under the new criteria, but many show substan-
tial losses in precision, while for GE precision was
also systematically increased. This effect was not
unexpected: we judge this to reflect primarily the
increased number of opportunities to extract each
unique event (higher recall) combined with the com-
paratively higher effect from errors from the reduc-
tion in the total number of unique correct extraction
targets (lower precision). It is not clear from our
analysis why a comparable effect was not seen for
GE. Interestingly, most systems show a better pre-
cision/recall balance under the new criteria than the
old, despite not optimizing for these criteria.
For ID (Table 6), we find a different effect also on
F-score, with all but one system showing reduced
performance under the new criteria, with some very
clear drops in performance; the only system to ben-
efit is UTurku. Analysis suggests that this effect
traces primarily to a notable reduction in the number
of simple PROCESS events that take no arguments6
when considering unique events on the document
level instead of each event mention independently.7
Conversely, the Stanford system, which showed the
highest instance-level performance in the extraction
of PROCESS type events (see Pyysalo et al (2011a)),
shows a clear loss in precision.
6The ID task annotation criteria call for mentions of some
high-level biological processes such as ?infection? to be anno-
tated as PROCESS even if no explicit participants are mentioned
(Pyysalo et al, 2011a).
7It is interesting to note that there was an error in the
UTurku system implementation causing it to fail to output any
events without arguments (Jari Bjo?rne, personal communica-
tion), likely contributing to the effect seen here.
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 50.84 66.35 57.57 50.11 65.33 56.72
UMass 49.67 62.39 55.31 49.34 60.98 54.55
Stanford 49.16 56.37 52.52 42.00 50.80 45.98
ConcordU 50.91 43.37 46.84 43.42 37.18 40.06
UTurku 39.23 49.91 43.93 48.03 51.84 49.86
PredX 23.67 35.18 28.30 20.94 30.69 24.90
Table 6: Comparison of BioNLP ST 2011 ID CORE task
results.
The clear differences in performance and the
many cases in which the system rankings under the
two criteria differ demonstrate that the new evalua-
tion criteria can have a decisive effect in which ap-
proaches to event extraction appear preferred. While
there may be cases for which the original shared task
criteria are preferred, there is at the very minimum
a reasonable argument to be made that the emphasis
these criteria place on the extraction of each instance
of simple events is unlikely to reflect the needs of
many practical applications of event extraction.
While these experimental results demonstrate that
the new evaluation criteria emphasize substantially
different aspects of the performance of the systems
than the original criteria, they cannot per se serve
as an argument in favor of one set of criteria over
another. We hope that these results and the accom-
panying tools will encourage increased study and
discussion of evaluation criteria for event extraction
and more careful consideration of the needs of spe-
cific applications of the technology.
5 Discussion and Conclusions
We have presented a new resource combining analy-
ses from the systems participating in the GE, ID and
EPI main tasks of the BioNLP Shared Task 2011,
compiled with the collaboration of groups partic-
ipating in these tasks. We demonstrated one use
of the resource through an evaluation of develop-
ment set events that none of the participating sys-
tems could recover, finding that events involving
implicit arguments, coreference and participants in
more than once sentence continue to represent chal-
lenges to the event extraction systems that partici-
pated in these tasks.
We further argued in favor of new perspectives to
the evaluation of domain event extraction systems,
106
emphasizing in particular the need for document-
level, ?off-the-page? representations and evaluation
to complement the text-bound, instance-level eval-
uation criteria that have so far been applied in the
shared task evaluation. We proposed a variant of
the shared task standoff representation for support-
ing such evaluation, and introduced evaluation tools
implementing the proposed criteria. An evaluation
supported by the introduced resources demonstrated
that the new criteria can in cases provide substan-
tially different results and rankings of the systems,
confirming that the proposed evaluation can serve
as an informative complementary perspective into
event extraction performance.
In future work, we hope to further extend the cov-
erage of the provided system outputs as well as their
analysis to cover all participants of all tasks in the
BioNLP Shared Task 2011. We also aim to use the
compiled resource in further study of appropriate
criteria for the evaluation of event extraction meth-
ods and deeper analysis of the remaining challenges
in event extraction.
To encourage further study of all aspects of event
extraction, all resources and tools introduced in this
study are provided freely to the community from
http://2011.bionlp-st.org.
Acknowledgments
We wish to thank the members of all groups con-
tributing to the combined resource, and in particular
the members of the MSR-NLP group for providing
both the initial suggestion for its creation as well as
the first publicly released analyses from their sys-
tem. We would also like to thank the anonymous
reviewers for their many insightful comments.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Maarten
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 56?64.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extract-
ing biological events from text using simple syntactic
patterns. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 143?146.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the ACL-HLT 2011, pages 976?986.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 153?154.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal predi-
cates. In Proceedings of ACL 2010, pages 1583?1592.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task
2011 ? Bacteria gene interactions and renaming. In
Proceedings of BioNLP Shared Task 2011 Workshop,
pages 65?73.
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka,
Yoshimasa Tsuruoka, Hiroaki Kitano, Sophia Anani-
adou, and Jun?ichi Tsujii. 2010. PathText: a text min-
ing integrator for biological pathway visualizations.
Bioinformatics, 26(12):i374?i381.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biologi-
cal event extraction. In Proceedings of the BioNLP
Shared Task 2011 Workshop.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011a. Extracting
bio-molecular events from literature - the BioNLP?09
shared task. Computational Intelligence, 27(4):513?
540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011b.
107
Overview of BioNLP Shared Task 2011. In Proceed-
ings of BioNLP Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori
Yonezawa. 2011c. Overview of the Genia Event task
in BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, Alfonso Valencia, et al 2008. Overview
of the protein-protein interaction annotation extrac-
tion task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for biomedical event anno-
tation. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 149?150.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP Shared Task 2011
Workshop.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing.
In Proceedings of ACL-HLT 2011, pages 1626?1635.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009, pages 1003?1011.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. Overview of BioNLP 2011 Protein Coreference
Shared Task. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 74?82.
Tomoko Ohta, Takuya Matsuzaki, Naoaki Okazaki,
Makoto Miwa, Rune S?tre, Sampo Pyysalo, and
Jun?ichi Tsujii. 2010. Medie and info-pubmed: 2010
update. BMC Bioinformatics, 11(Suppl 5):P7.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP Shared Task 2011
Workshop.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of EMNLP-
CoNLL 2007, pages 717?727.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the entity relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 83?88.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of BioNLP
Shared Task 2011 Workshop, pages 155?163.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP 2011, pages 1?12.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. Machine Learning and Knowledge Dis-
covery in Databases, pages 148?163.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in BioNLP 2011. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Beth M. Sundheim. 1991. Third message understanding
evaluation and conference (MUC-3): Phase 1 status
report. In Proceedings of the Speech and Natural Lan-
guage Workshop, pages 301?305.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of BioNLP 2010, pages 144?152.
Sofie Van Landeghem, Thomas Abeel, Bernard De Baets,
and Yves Van de Peer. 2011a. Detecting entity rela-
tions as a supporting task for bio-molecular event ex-
traction. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 147?148.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011b. Evex: a pubmed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
Andreas Vlachos and Mark Craven. 2011. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 36?40.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference based event-argument relation extraction
on biomedical text. Journal of Biomedical Semantics,
2(Suppl 5):S6.
108
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 47?56, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Bridging the Gap Between Scope-based and Event-based
Negation/Speculation Annotations: A Bridge Not Too Far
Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3
Sophia Ananiadou2,3 and Jun?ichi Tsujii2,3,4
1Department of Computer Science, University of Tokyo, Tokyo, Japan
2School of Computer Science, University of Manchester, Manchester, United Kingdom
3National Centre for Text Mining, University of Manchester, Manchester, United Kingdom
4Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,smp,okap}@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
jtsujii@microsoft.com
Abstract
We study two approaches to the marking of
extra-propositional aspects of statements in
text: the task-independent cue-and-scope rep-
resentation considered in the CoNLL-2010
Shared Task, and the tagged-event representa-
tion applied in several recent event extraction
tasks. Building on shared task resources and
the analyses from state-of-the-art systems rep-
resenting the two broad lines of research, we
identify specific points of mismatch between
the two perspectives and propose ways of ad-
dressing them. We demonstrate the feasibility
of our approach by constructing a method that
uses cue-and-scope analyses together with a
small set of features motivated by data anal-
ysis to predict event negation and speculation.
Evaluation on BioNLP Shared Task 2011 data
indicates the method to outperform the nega-
tion/speculation components of state-of-the-
art event extraction systems.
The system and resources introduced in this
work are publicly available for research pur-
poses at: https://github.com/ninjin/eepura
1 Introduction
Understanding extra-propositional aspects of texts
is key to deeper understanding of statements con-
tained in natural language texts. Extra-propositional
aspects such as the polarity of key statements have
long been acknowledged to be critical for user-
facing applications such as information retrieval
(Friedman et al, 1994; Hersh, 1996). In recogni-
tion of this need, a number of recent information
extraction (IE) resources involving structured repre-
sentations of text statements have explicitly included
some marking of certainty and polarity (LDC, 2005;
Kim et al, 2009; Saur and Pustejovsky, 2009; Kim
et al, 2011a; Thompson et al, 2011).
Although extra-propositional aspects are recog-
nised as important, there is no clear consensus on
how to address their annotation and extraction from
text. Some comparatively early efforts focused on
the detection of negation cue phrases associated with
specific (previously detected) terms through regu-
lar expression-based rules (Chapman et al, 2001).
A number of later efforts identified the scope of
negation cues with phrases in constituency analy-
ses in sentence structure (Huang and Lowe, 2007).
Drawing in part on this work, the BioScope corpus
(Vincze et al, 2008) applied a representation where
both cues and their associated scopes are marked as
contiguous spans of text (Figure 1 bottom). This ap-
proach was also applied in the CoNLL-2010 Shared
Task (Farkas et al, 2010), in which 13 participat-
ing groups proposed approaches for Task 2, which
required the identification of uncertainty cues and
their associated scopes in text. In the following,
we will term this task-independent, linguistically-
motivated approach as the cue-and-scope represen-
tation (please see Vincze et al (2008) for details re-
garding the representation).
For IE efforts, more task-oriented representations
are commonly applied. In an effort to formalise
and drive research for extracting structured repre-
sentations of statements regarding molecular biol-
ogy, the ongoing series of BioNLP shared tasks
have addressed biomedical Event Extraction (EE)
(Kim et al, 2009; Kim et al, 2011a). The extra-
propositional targets of negation and speculation
47
Figure 1: Example illustrating cue-and-scope and
event-based negation marking. ?Crossing-out?
marks events as negated. PRO, TH and NEG are ab-
breviations for PROTEIN, THEME and NEGATION,
respectively.
of extracted events were already included in the
first task in the series, using a representation where
events can be assigned ?flags? to mark them as being
negated, speculated, or both (Figure 1 upper). Due
to space limitations we refer the reader to Kim et al
(2009) for a detailed explanation of the representa-
tion; similar representations have been applied also
in previous event extraction tasks (LDC, 2005).
There are a number of ways in which task-
oriented, event-based approaches could benefit from
the existing linguistically-oriented cue-and-scope
methods for identifying extra-propositional aspects
of text statements. However, there has been sur-
prisingly little work exploring the combination of
the approaches, and comparatively few methods ad-
dressing the latter task in detail. Only three out
of the 24 participants in the BioNLP Shared Task
2009 submitted results for the non-mandatory nega-
tion/speculation task, and although negation and
speculation were also considered in three main tasks
for the 2011 follow-up event (Kim et al, 2011a),
the trend continued, with only two participants ad-
dressing the negation/speculation aspects of the task.
We are aware of only two studies exploring the rela-
tionship between the cue-and-scope and event-based
representations: in a manual analysis of scope over-
lap with tagged events, Vincze et al (2011) identi-
fied a number of issues and mismatches in annota-
tion scope and criteria, which may explain in part
the lack of methods combining these two lines of
research. Kilicoglu and Bergler (2010) approached
the problem from the opposite direction and used an
existing EE system to extract cue-and-scope annota-
tions in the CoNLL-2010 Shared Task.
In this work, we take a high-level perspective,
seeking to bridge the linguistically oriented frame-
work and the more application-oriented event frame-
work to overcome the mismatches demonstrated
by Vincze et al (2011). Specifically, we aim to
determine how cue-and-scope recognition systems
can be used to produce a state-of-the-art nega-
tion/speculation detection system for the EE task.
2 Resources
Several existing resources can support the investiga-
tion of the relationship between the linguistically-
oriented and task-oriented perspectives on nega-
tion/speculation detection. In this study, we make
use of the following resources.
First, we study the three BioNLP 2011 Shared
Task corpora that include annotation for negation
and speculation: the GE, EPI and ID main task cor-
pora (Table 1). Second, we make use of support-
ing analyses provided for these corpora in response
to a call sent by the BioNLP Shared Task organis-
ers to the developers of third-party systems (Stene-
torp et al, 2011). Specifically, we use the output
of the BiographTA NeSp Scope Labeler (here re-
ferred to as CLiPS-NESP) (Morante and Daelemans,
2009; Morante et al, 2010) provided by the Univer-
sity of Antwerp CLiPS center. This system provides
cue-and-scope analyses for negation and speculation
and was demonstrated to have state-of-the-art per-
formance at the relevant CoNLL-2010 Shared Task.
Finally, we make use of the event analyses created
by systems that participated in the BioNLP Shared
Task, made available to the research community for
the majority of the shared task submissions (Pyysalo
et al, 2012). These analyses represent the state-
of-the-art in event extraction and their capability to
detect event structures as well as marking them for
negation and speculation.
The above three resources present us with many
opportunities to relate scope-based annotations to
three highly relevant event-based corpora containing
negation/speculation annotations.
3 Manual Analysis
To gain deeper insight into the data and the chal-
lenges in combining the cue-and-scope and event-
oriented perspectives, we performed a manual anal-
ysis of the corpus annotations using the manually
48
Name Negated Events Speculated Events Negated Spans Speculated Spans Publication
EPI 103 (5.6%) 70 (3.8%) 561 1,032 Ohta et al (2011)
GE 759 (7.4%) 623 (6.0%) 1,308 1,968 Kim et al (2011b)
ID 69 (3.3%) 26 (1.2%) 415 817 Pyysalo et al (2011)
Table 1: Corpora used for our experiments along with annotation statistics for their respective training sets.
The parenthesised values are the relative proportion of negated/speculated event annotations.
Occ. (Ratio) EPI ID
Covered 26 (15.03%) 52 (56.52%)
Not-covered 135 (78.03%) 38 (41.30%)
Error-in-gold 12 (6.94%) 2 (2.18%)
Morphological 48 (27.75%) 11 (11.96%)
Hypothesis 44 (25.43%) 15 (16.30%)
Ellipsis 5 (2.89%) 0 (0.00%)
Argument-only 2 (1.16%) 10 (10.87%)
Table 2: Results from the Manual Data Analysis of
the EPI and ID test sets.
created BioNLP Shared Task training data event an-
notations, and the automatic annotations created for
this data by the CLiPS-NESP system. The test
data was held out and was not directly examined
at any point of our study. We performed the anal-
ysis specifically on the EPI and ID corpora, as the
GE corpus training set texts overlap with the train-
ing data for the CLiPS-NESP system (BioScope cor-
pus), and results on this data would thus not reflect
the performance of the system on unseen data, and
a comparison of the GE and BioScope gold anno-
tations was previously performed by Vincze et al
(2011).
The analysis was performed by an experienced
annotator with a doctoral degree in a related field
in biology, who individually examined each of the
events marked as negated and speculated in the
EPI and ID training corpora. For the analysis,
the CLiPS-NESP system output was super-imposed
onto the BioNLP Shared Task event annotations.
The annotator was asked to assign three primary
flags for each event that was marked as negated or
speculated: Covered if the event trigger was covered
by span(s) of the correct type with a correct cue in
the cue-and-span analysis, Not-covered if not Cov-
ered, and Error-in-gold if the negation/speculation
flag on the event annotation was itself incorrect. We
also identified a number of additional properties that
initial analysis suggested to frequently characterise
instances where the coverage of the cue-and-scope
system is lacking: Morphological was assigned if
the negation/speculation of an event could be in-
ferred only from the morphology of the word ex-
pressing the event, rather than from cue words in its
context (e.g. unphosphorylated, non-glycosylated);
Hypothesis for cases where speculation is marked
for events stated as hyphotheses1 under consider-
ation, e.g. ?We analysed the methylation status of
MGMT?; Ellipsis for cases where the modified ex-
pression is elided (e.g. ?A was phosphorylated but B
was not?); and Argument-only if the CLiPS-NESP
output had marked the argument of an event as
negated rather than the event trigger (we use argu-
ment in the sense it is used in the BioNLP Shared
Tasks, for example, in Figure 1 upper, the two argu-
ments of the event are ?fMimR? and ?fimA?).
The results of the analysis are summarised in Ta-
ble 2. We find that that the system shows a clear dif-
ference in coverage depending on the dataset. For
the ID dataset, a majority of the annotations are cov-
ered by the appropriate spans, while only a small mi-
nority are covered for EPI. Instead, the EPI dataset
contains a significant portion of events where extra-
propositional aspects can only be distinguished by
the morphology of the word expressing the event
(all Morphological cases were negation) as well as
events marked as speculated due to being expressed
as hypotheses under study.
The analysis thus identified specific ways in
which the applicability of negation-detection sys-
tems using a span-and-scope representation could be
improved for some tasks.
1While it is arguable whether such cases represent specula-
tion (Vincze et al, 2008), separation from affirmatively made
claims is clearly motivated for many applications.
49
Event-based
Scope-based
Negation/speculationdetection
Eventextraction
Oursystem
Figure 2: An illustration of our approach.
4 Methods
We next introduce the methods we apply for as-
signing negation and speculation flags to extracted
events.
4.1 Approach
To focus on the extra-propositional aspects of event
extraction, we only consider the assignment of the
negation and speculation flags, not the extraction of
the event structures that these mark. To our knowl-
edge, no previous work studying this subtask in iso-
lation from event extraction exists. Thus, in order to
be able to relate the performance of the methods we
consider to the performance of previously proposed
approaches, it is necessary to base the negation and
speculation detection on an event extraction analy-
sis. For this reason, we construct our methods us-
ing system outputs for systems participating in the
BioNLP Shared Task 2011, in effect creating a nega-
tion/speculation processing stage for a pipeline sys-
tem where the previous stage is the completion of
event analysis without negation/speculation detec-
tion (Figure 2).
Our methods thus take extracted events as input
and attempt to enrich the output with negation and
speculation annotations. This enables us to produce
a general system with the potential to be applied
together with any existing event extraction system.
Additionally, this allows us to directly compare our
system output with that of the negation/speculation
components of previously proposed monolithic sys-
tems by removing the existing negation and spec-
ulation output from submissions including this and
recreating these annotations using our methods.
4.2 Rule-based Methods
The most straightforward way of carrying over in-
formation from scope-based to event-based annota-
tions is to consider any event structure for which the
word or words stating the event (i.e. the event trig-
ger) is within the scope of a negation or speculation
be negated or speculated (respectively). We imple-
mented this simple heuristic as our initial rule-based
method.
One relatively common category of cases where
this heuristic fails that was identified in analysis re-
lates to events that take other events as arguments.
Consider, for example, the case illustrated in Fig-
ure 3. The speculation span is correctly identified as
covering the statement ?FimR modulates mfa1 ex-
pression?, and the event expressed through ?mod-
ulates? is identified as speculated. However, the
nested event, the expression of mfa1, is not spec-
ulated. To cover this case, we implemented what
we refer to as the root-heuristic, which prevents the
propagation of negation/speculation marking from
scopes to events that are the arguments of another
event contained in the same scope. The second rule-
based method we consider incorporates this addi-
tional heuristic.
Preliminary development set experiments indi-
cated that while the root-heuristic could improve
precision, the performance of the rule-based meth-
ods remained poor, in particular on the EPI dataset.
The results of the manual analysis (Section 3) sug-
gested this to trace in particular to two main issues,
namely differences between annotation criteria be-
tween BioScope and the shared task data (as noted
also by Vincze et al (2011)) and events which are
negated not by external cues but by morphological
alternations of the event trigger, such as ?unphos-
phorylated? expressing the absence of phosphory-
lation. As it would have been difficult to system-
atically incorporate both morphology and context
into the rule-based method without compromising
the generality of the approach, we opted to move to a
machine learning framework for further method de-
velopment. This allows us to continue to make use
of the existing cue-and-scope annotations while ex-
ploring the effects of other aspects of the text and
maintaining generality through retraining.
4.3 Machine Learning-based Methods
In developing a machine learning-based approach to
the negation/speculation task, we aimed to identify
and evaluate a minimal set of features directly mo-
50
Feature Example Value(s)
Heuristic ROOT/NON-ROOT
Heuristic-Cue possibility
Heuristic-Span One, possibility, . . .
Trigger-Text non-phosphorylated
Trigger-Prefixes no, non, non-, . . .
Trigger-Preceding-Context is, that, . . .
Trigger-Proceeding-Context mfa1, expression, . . .
Table 3: Machine learning features. The fea-
tures are categorised into three groups: features
based on cue-and-scope based heuristics (top), non-
contextual features derived from the event trigger
(middle), and features derived from the context of
the event trigger (bottom). These three feature sets
are abbreviated as E, M and C, respectively.
Figure 3: Example of a speculation span containing
two events, of which only one is speculated (marked
by a dashed border).
tivated by the analysis of the data and to use the
cue-and-scope analyses as much as possible. In par-
ticular, we wanted to avoid features requiring com-
putationally expensive analyses such as full pars-
ing or replicating the type of analyses performed by
the CLiPS-NESP system, focusing rather on specific
points where its output does not meet the needs of
the event-based approach.
We introduced features representing the heuristics
described in Section 4.2, marking each case as be-
ing either a root or non-root event in its scope (if
any). Drawing further on the cue-and-scope analy-
sis, we included as features the cue word and bag-of-
words features for all tokens in the scope (using sim-
ple white-space tokenisation). To address the issues
identified in manual analysis, we introduced features
for the event trigger text as well as character-based
prefixes of lengths 2 to 7 of the, intended primarily
to capture morphological negation.
All features presented above are derived only
from those parts of the sentence already marked ei-
ther by the event extraction or the cue-and-scope
system. However, due to the differences in anno-
tation guidelines for speculation annotations, we ex-
pect that the scope-based system will fail to mark
a significant portion of the speculation annotations.
To allow the system to learn to detect these, we in-
troduce a minimal set of contextual features, limited
to a bag-of-words representation of the three words
preceding and following the event trigger.
5 Experiments
We perform two sets of experiments, the first to eval-
uate our approach on gold annotations to give a fair
upper-limit to how well our negation/speculation de-
tection system could perform under ideal settings,
and the second to enrich the output of an event ex-
traction system with negation and speculation an-
notations, to evaluate real-world performance and
to allow direct comparison of our methods with
those incorporated in monolithic event extraction
and negation/speculation detection systems.
5.1 Corpora
For our experiments we used the GE, EPI and ID
corpora of the BioNLP Shared Task 2011 (Table 1).
We note that while the GE training set texts overlap
with the BioScope corpus used to train the CLiPS-
NESP system, the GE test set does not, and thus test
set results are not expected to be overfit.
We noted when performing development set
experiments that training machine learning-based
methods on the negation/speculation annotations of
the event-annotated corpora was problematic due to
the sparseness of these flags in the annotation. To
address this issue, we merge the training data of the
three corpora in all experiments with machine learn-
ing methods.
5.2 Baseline methods
We use the event analyses created by the UTurku
(Bjo?rne and Salakoski, 2011) and UConcordia (Kil-
icoglu and Bergler, 2011) systems for the BioNLP
2011, the only systems that included negation and
speculation analyses. To investigate the impact on a
system that did not include a negation/speculation
component, we further consider analyses created
51
Negation (R/P/F) EPI GE ID
H 29.23/31.67/30.40 53.92/52.84/53.38 44.00/31.88/36.97
HR 27.69/32.73/30.00 53.24/71.89/61.18 44.00/37.93/40.74
M 47.69/20.00/28.18 43.00/25.25/31.82 46.00/26.74/33.82
ME 60.00/66.10/62.90 58.36/70.08/63.69 54.00/69.23/60.67
MC 40.00/74.29/52.00 58.36/76.34/66.15 52.00/61.90/56.52
MCE 58.46/73.08/64.96 61.77/83.03/70.84 58.00/70.73/63.74
Table 4: Results for Negation for our two heuristics and the four combinations of machine learning features.
Speculation (R/P/F) EPI GE ID
H 13.46/6.48/8.75 33.77/18.12/23.58 54.17/6.50/11.61
HR 11.54/5.66/7.59 32.79/29.45/31.03 54.17/7.98/13.90
M 1.92/0.62/0.93 25.65/10.84/15.24 45.83/10.58/17.19
ME 3.85/12.50/5.88 22.08/42.24/29.00 29.17/28.00/28.57
MC 51.92/52.94/52.43 27.27/50.30/35.37 37.50/31.03/33.96
MCE 48.08/51.02/49.50 31.82/53.85/40.00 33.33/42.11/37.21
Table 5: Results for Speculation for our two heuristics and the four combinations of ML features.
by the FAUST system, which achieved the high-
est performance at two of the three tasks consid-
ered (Riedel et al, 2011). The UTurku system is
a pipeline ML-based EE system, while the UCon-
cordia system is strictly rule-based. FAUST is an
ML-based model combination system incorporating
information from the parser-based Stanford system
(McClosky et al, 2011) and the jointly-modelled
UMass system (Riedel and McCallum, 2011).
We also performed preliminary experiments for
the other released submissions to the BioNLP 2011
Shared Task, but due to space limitations focus only
on the three above-mentioned systems.
5.3 Evaluation criteria
We use the primary evaluation criteria of the
BioNLP 2011 Shared Task (Kim et al, 2011a) to
assure comparability, reporting all results using the
standard precision, recall and their harmonic mean
(F-score).
5.4 Methods
We apply the rule-based simple heuristic method
and its root extension (Section 4.2) as well as Sup-
port Vector Machines (SVM) trained with the fea-
tures introduced in Section 4.3. For the SVM, we
separately evaluate models based on all permuta-
tions of the feature sets introduced in Table 3. In the
results tables we abbreviate the feature set names as
done in Table 3 and use H for the heuristic method
and R for its root extension. As our machine learn-
ing component we use LIBLINEAR (Fan et al,
2008) with a L2-regularised L2-loss SVM model.
We optimise the SVM regularisation parameter C
using 10-fold cross-validation on the training data.
We use the training, development and test set par-
tition provided by the shared task organisers. In line
with standard ML methodology the test set was held
out during development and was only used when
carrying out the final experiments prior to submit-
ting the manuscript.
6 Results and Discussion
Our initial experiments, building on gold event data
(Tables 4 and 5), support our manual analysis, show-
ing nearly uniform performance improvement with
additional features. First, we find that the root-
heuristic gives an improvement over the original
heuristic in four out of six cases. To justify our us-
age of the cue-and-scope based heuristic feature (E)
we find that adding it as a feature improves on the M
feature set and the MC feature set, showing that even
given context, the cue-and-scope perspective is still
useful. The only anomaly is for speculation on the
EPI dataset, where adding this heuristic feature ac-
tually hampers performance, possibly relating to the
52
Negation (R/P/F) EPI GE ID
UConcordia 16.92/61.11/26.51 18.43/43.44/25.88 22.00/23.91/22.92
UConcordia* 20.00/70.59/31.17 20.14/42.96/27.42 28.00/31.58/29.68
UTurku 12.31/38.10/18.60 22.87/48.85/31.15 26.00/44.83/32.91
UTurku* 43.08/48.28/45.53 21.16/38.56/27.33 26.00/41.94/32.10
FAUST* 29.23/59.38/39.18 21.50/41.18/28.25 28.00/46.67/35.00
Table 6: Results of the Negation enrichment experiment.
Speculation (R/P/F) EPI GE ID
UConcordia 5.77/8.33/6.82 21.10/38.46/27.25 8.33/2.00/3.23
UConcordia* 1.92/4.55/2.70 12.99/29.20/17.98 8.33/2.22/3.51
UTurku 30.77/48.48/37.65 17.86/32.54/23.06 12.50/18.75/15.00
UTurku* 46.15/47.06/46.60 11.04/26.56/15.60 8.33/3.33/4.76
FAUST* 36.54/48.72/41.76 10.39/26.50/14.93 12.50/12.50/12.50
Table 7: Results of the Speculation enrichment experiment.
(R/P/F) EPI ID
UConcordia 20.83/42.14/27.88 49.00/40.27/44.21
UConcordia* 20.83/42.94/28.05 49.20/41.78/45.19
UTurku 52.69/53.98/53.33 37.85/48.62/42.57
UTurku* 54.72/53.86/54.29 37.79/47.76/42.19
FAUST 28.88/44.51/35.03 48.03/65.97/55.59
FAUST* 31.64/45.17/37.21 49.20/64.66/55.88
Table 8: Overall scores for the EPI and ID data sets.
sparseness of useful annotations due to the differing
annotation guidelines, as noted in manual analysis.
The numbers from these initial experiments serve as
an upper bound when we proceed to our enrichment
experiments, as they do not suffer from the possibil-
ity of producing false positives negation/speculation
annotations for false positive event structures.
In addition to the above in preliminary experi-
ments we also considered two features inspired by
findings made by Vincze et al (2011). A distance-
based feature, measuring the distance in tokens be-
tween the cue-word and the event trigger, and also
trigger suffixes to capture some cases of morpholog-
ical speculation (?induced? vs. ?inducible?). How-
ever, we failed to establish any consistent benefits
from these features and only for the EPI dataset did
the suffix features improve performance.
For the enrichment evaluation, adding nega-
F EPI GE ID
UConcordia 57.43 60.68 67.28
UTurku 81.31 66.27 55.84
FAUST 74.91 66.14 67.13
Table 9: Estimated F-score upper-bound for an ora-
cle system precision assigning negation/speculation
annotations to events predicted by an up-stream EE
system.
tion/speculation flags to the output of event extrac-
tion systems (Tables 6 and 7), our results are some-
what more modest. For negation we see an improve-
ment in four out of six cases, and for speculation in
two out of six. Despite the fact that a major limi-
tation to our approach are the false positive events
that are propagated from the original EE system, we
manage to improve the global score for all data sets
where a global score is provided by the organisers
(Table 8). We improve a full point in F-score for
UTurku on EPI, but only sub-percentage for Faust
on ID, the latter most likely since ID contains fewer
negation and speculation annotations and the global
scores are microaverages over all annotations.
As a final analysis we estimate the upper-bound
in F-score performance for all three EE systems
(Table 9). We do so by assuming that the recall
for events marked by negation and speculation is
53
equal to that of the overall recall of the up-stream
EE system and that negation/speculation annotations
assigned by an oracle. What we can see is that
there is still room for improvement, both for our
enrichment approach and for the EE system?s inter-
nal negation/speculation components, although re-
call of the EE output is a limiting factor we can
expect further efforts towards improving the extra-
propositional aspects of the system to yield perfor-
mance improvements.
7 Conclusions and Future Work
In this study, we have considered two broad lines
of research on extra-propositional aspects of key
statements in text, one using the task-independent,
linguistically-motivated cue-and-scope representa-
tion applied in the recent CoNLL-2010 Shared Task,
and the other using the task-oriented flagged-event
representation applied e.g. in the ACE and BioNLP
Shared Task evaluations. We presented a detailed
manual analysis exploring points of disagreement
and evaluated in detail rule-based and machine
learning-based methods joining state-of-the-art sys-
tems representing the two approaches.
Our manual analysis identified a number of phe-
nomena that limit the applicability of existing cue-
and-scope based systems to the event extraction
task, such as negation expressed through morpho-
logical change of words expressing events (e.g. un-
phosphorylated). To address these issues, we pro-
posed a combination of heuristics and simple lexical
features, carefully selected to address differences in
perspective between the cue-and-scope and event-
based frameworks and aiming to complement cue-
and-scope analyses for creating task-oriented out-
puts.
To test our approach, we created a method suit-
able for use as a component of an event extraction
pipeline that incorporates information from a previ-
ously proposed state-of-the-art cue-and-scope based
negation/speculation detection system and a mini-
mal set of features in an SVM-based system that was
shown to enhance and in several cases improve upon
the output of existing EE systems. Experiments on
the BioNLP Shared Task 2011 EPI and ID datasets
demonstrated that the combined approach could im-
prove the results of the best-performing systems at
the original task in 5 out of 6 cases, outperforming
the highest results reported for any system for these
two tasks.
There exist several potential targets for future
work on improving our introduced system and
to join cue-and-scope and event-based approaches.
Since none of the existing EE corpora was con-
structed with the aim to solely cover negation and
speculation annotations and taking into account our
finding that merging datasets to compensate for data
sparseness is beneficial, it might be worth consid-
ering other possible corpora or resources and how
they can be used for training our machine learning
system.
Also, it would be worthwhile to attempt to com-
bine an existing EE system capable of detect-
ing negation/speculation with our proposed method.
Combining the two could yield an ensemble, im-
proving upon an already strong system by bridging
the differences in perspectives and tapping into the
potential benefits of both approaches.
The system and all resources introduced in this
work are publicly available for research purposes at:
https://github.com/ninjin/eepura
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their many insightful comments and sug-
gestions for improvements.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 183?191.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of biomedi-
cal informatics, 34(5):301?310.
54
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12.
Carol Friedman, Philip O. Alderson, John H.M. Austin,
James J. Cimino, and Stephen B. Johnson. 1994. A
general natural-language text processor for clinical ra-
diology. Journal of the American Medical Informatics
Association, 1(2):161?174.
William R. Hersh. 1996. Information retrieval: a health
care perspective. Springer.
Yuang Huang and Henry J. Lowe. 2007. A novel hybrid
approach to automated negation detection in clinical
radiology reports. Journal of the American Medical
Informatics Association, 14(3):304?311.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and their
Scopes. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
70?77.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
pages 173?182.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 Shared Task on Event Extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP Shared Task 2011. In Proceed-
ings of the BioNLP 2011 Workshop Companion Vol-
ume for Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 7?15.
LDC. 2005. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Events. Technical re-
port, Linguistic Data Consortium.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of BioNLP 2011,
pages 41?45.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 28?
36.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scopes of hedge cues. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning ? Shared Task, CoNLL 2010: Shared
Task, pages 40?47.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 16?25.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 26?35.
Sampo Pyysalo, Pontus Stenetorp, Tomoka Ohta, Jin-
Dong Kim, and Sophia Ananiadou. 2012. New Re-
sources and Perspectives for Biomedical Event Extrac-
tion. In Proceedings of BioNLP 2012 Workshop. to
appear.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, pages 46?50.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, pages 51?55.
Roser Saur and James Pustejovsky. 2009. Fact-
Bank: a corpus annotated with event factuality.
Language Resources and Evaluation, 43:227?268.
10.1007/s10579-009-9089-9.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, pages 112?120.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedical
event corpus with meta-knowledge annotation. BMC
Bioinformatics, 12(1):393.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
55
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
Veronika Vincze, Gyorgy Szarvas, Gyorgy Mora,
Tomoko Ohta, and Richard Farkas. 2011. Linguis-
tic scope-based and biological event-based specula-
tion and negation annotations in the BioScope and Ge-
nia Event corpora. Journal of Biomedical Semantics,
2(Suppl 5):S8.
56
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 27?36,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Open-domain Anatomical Entity Mention Detection
Tomoko Ohta 1 Sampo Pyysalo 1 Jun?ichi Tsujii 2 Sophia Ananiadou 1
1National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK
2Microsoft Research Asia, Beijing, China
okap.tiffany@gmail.com, sampo.pyysalo@gmail.com
jtsujii@microsoft.com, sophia.ananiadou@manchester.ac.uk
Abstract
Anatomical entities such as kidney, muscle
and blood are central to much of biomedical
scientific discourse, and the detection of men-
tions of anatomical entities is thus necessary
for the automatic analysis of the structure of
domain texts. Although a number of resources
and methods addressing aspects of the task
have been introduced, there have so far been
no annotated corpora for training and evaluat-
ing systems for broad-coverage, open-domain
anatomical entity mention detection. We in-
troduce the AnEM corpus, a domain- and
species-independent resource manually anno-
tated for anatomical entity mentions using a
fine-grained classification system. The cor-
pus texts are selected randomly from citation
abstracts and full-text papers with the aim of
making the corpus representative of the en-
tire available biomedical scientific literature.
We demonstrate the use of the corpus through
an evaluation of the broad-coverage MetaMap
tagger and a CRF-based system trained on the
corpus data, considering also a combination
of these two methods. The combined sys-
tem demonstrates a promising level of per-
formance, approaching 80% F-score for men-
tion detection for a relaxed matching criterion.
The corpus and other introduced resources are
available under open licences from http://
www.nactem.ac.uk/anatomy/.
1 Introduction
Entity mention detection is a prerequisite for most
efforts to systematically analyse and represent the
structure of scientific discourse. In the life sciences,
a comprehensive analysis must include entities at
multiple levels of biological organization, from the
molecular to the organism level. The detection of
references to anatomical entities such as ?kidney?
and ?blood? is thus required for the automatic struc-
tured analysis of biomedical scientific text.
Although a wealth of lexical and ontological re-
sources covering anatomical entities are available
(Rosse and Mejino, 2003; Smith et al, 2007; Boden-
reider, 2004; Haendel et al, 2009), such resources
do not alone confer the ability to reliably detect
mentions of anatomical entities in natural language
(Gerner et al, 2010a; Travillian et al, 2011; Pyysalo
et al, 2012b). To support the development and eval-
uation of reliable anatomical entity mention detec-
tion methods, corpus resources annotated specifi-
cally for the task are necessary.
In this study, we aim to create a reference standard
for evaluating methods for anatomical entity men-
tion detection and for training machine learning-
based methods for the task. We seek to select
a set of texts that are representative of the rele-
vant scientific literature, i.e. open-domain in the
sense of avoiding bias toward, for example, specific
species, levels of biological organization (e.g. sub-
cellular or gross anatomy), parts of documents (e.g.
abstracts), or subdomains of life science. In sup-
port of our annotation, we draw on a granularity-
based, species-independent upper-level ontology of
anatomy as well as relevant species-specific onto-
logical resources.
The overall aim of our efforts is to create methods
and resources for comprehensive event-based anal-
ysis (Ananiadou et al, 2010) of biomedical scien-
tific discourse involving anatomy-level entities and
processes. In aiming to establish a stable basis
for anatomical entity mention detection, the present
study is an important step toward this goal.
27
Label Ontology classes Examples
A
na
to
m
ic
al
en
ti
ty
A
na
to
m
ic
al
st
ru
ct
ur
e ORGANISM SUBDIVISION organism subdivision CARO head, limb
ANATOMICAL SYSTEM anatomical system CARO vascular system
ORGAN compound organ CARO liver, heart
MULTI-TISSUE STRUCTURE multi-tissue structure CARO artery
TISSUE portion of tissue CARO epithelium
CELL cell CARO epithelial cell
DEVELOPING ANATOMICAL STRUCTURE developing anatomical structure UBERON embryo
CELLULAR COMPONENT cellular component GO mitochondrion
ORGANISM SUBSTANCE portion of organism substance CARO blood
IMMATERIAL ANATOMICAL ENTITY immaterial anatomical entity CARO lumen
PATHOLOGICAL FORMATION - carcinoma
Table 1: Annotations targets with applied label, corresponding ontology classes, and common examples.
2 Corpus Annotation
2.1 Ontological Basis
Following our previous efforts on anatomical en-
tity classification (Pyysalo et al, 2012b), we base
our definition of annotated mention scope, the sub-
division of anatomical entities into classes, and
the class labels applied in our annotation primar-
ily on the Common Anatomy Reference Ontology
(CARO) (Haendel et al, 2008). CARO is a small,
species-independent ontology of anatomical entities
based on the upper-level structure of the Founda-
tional Model of Anatomy (FMA) ontology of hu-
man anatomy (Rosse and Mejino, 2003; Rosse and
Mejino, 2008). CARO has been proposed as a stan-
dard for unifying the upper-level structure of the
various existing species-specific ontologies and is
adopted by many of the over 40 ontologies involv-
ing the anatomy domain in the Open Biomedical
Ontologies (OBO) foundry1 (Smith et al, 2007).
CARO adheres to disjoint classes and single inher-
itance, and divides anatomical structures primarily
by granularity (Kumar et al, 2004), a systematic no-
tion familiar to those working in the life sciences.
Although we draw primarily on CARO, we fol-
low the well-established cellular component subon-
tology of the Gene Ontology (GO) (Ashburner et
al., 2000) in grouping sub-cellular structures under
a single upper-level category. For developing struc-
tures that resist granularity-based categorization due
to occupying different levels at different stages of
development, we adopt a separate DEVELOPING
ANATOMICAL STRUCTURE category, as done also
in e.g. Uberon (Haendel et al, 2009).
1http://obofoundry.org/
2.2 Annotation Scope
We diverge from the scope of anatomy ontologies in
two important aspects in our annotation.
First, ontologies of anatomy commonly incorpo-
rate everything from molecules to whole organisms
within their scope. However, in entity mention de-
tection, many molecular level anatomical entities
fall within the scope of the established gene/protein
mention detection tasks (e.g. (Kim et al, 2004; Tan-
abe et al, 2005)), and whole organism mentions
similarly largely within what is covered by existing
methods and resources for organism mention detec-
tion (Gerner et al, 2010b; Naderi et al, 2011). To
avoid overlap with established tasks and to focus on
the novel aspects of anatomical entity mention de-
tection, we exclude biological macromolecules and
mentions of organism names from the scope of our
annotation, as argued in (Pyysalo et al, 2012b).
Second, these ontologies typically represent
canonical anatomy, an idealized state that is rarely
(if ever) encountered in reality (Bada and Hunter,
2011). As our annotation is intended to cover ref-
erences to real-world anatomy, we explicitly include
in the scope of our annotation also healthy as well as
pathological variants of canonical anatomy. We in-
clude also entities derived from these anatomical en-
tities through (planned) processing such as surgical
or laboratory procedures, even when these processed
entities are no longer properly part of the original
organism. Finally, we annotate pathological forma-
tions such as scars and carcinomas that are part of
individual organisms but have no correspondence in
canonical anatomy (Smith et al, 2005).
Table 1 presents the class labels applied in the an-
notation with the corresponding ontology classes.
28
In contrast, the 3 cases of metastatic cancer of the GB had no blood flow signal in the wall of the GB
Pathological form Organ OSubst MTS OrganPart-ofPart-of
Figure 1: Example sentence with annotation. OSUBST and MTS abbreviate for ORGANISM SUBSTANCE and MULTI-
TISSUE STRUCTURE, respectively.
2.3 Representation
The primary corpus annotation marks mentions of
anatomical entities as contiguous spans of characters
in text, each of which is assigned a type (Figure 1).
As the CARO-based categorization has comprehen-
sive coverage and disjoint classes, each annotation
can be assigned exactly one type (class label).
In addition to identifying and typing anatomical
entity mentions, we further apply binary attributes
(?flags?) marking the following characteristics of
each mention:
DEVELOPING developing variant of anatomical
entity, e.g. fetal liver
PATHOLOGICAL pathological variant of anatomi-
cal entity, e.g. carcinoma cell
PLANT anatomical entity that is part of a plant
(member of the Viridiplantae kingdom), e.g.
roots, leaf
PROCESSED variant of anatomical entity that has
undergone planned processing, e.g. tissue spec-
imen
Any combination of attributes can apply to a single
mention. These attributes allow the identification of
subsets of annotations that may be out of scope for
some efforts (e.g. pathological or processed entities)
and facilitate the analysis of mention detection sys-
tem performance by identifying particular problem-
atic categories.
2.4 Annotation Criteria
In very brief summary, we annotate spans of text that
refer to anatomical entities as defined above. Men-
tions that involve only metaphorical senses of such
entities (?on the other hand?) or artificial analogues
(?artificial heart?) are not annotated.
The primary targets of our annotation are anatom-
ical entity names (e.g. ?lymphocyte?) and nominal
mentions of anatomical entities (e.g. ?muscle tis-
sue?). Both names and nominal mentions are anno-
tated similarly, without distinction. We exclude pro-
nouns (it, that) from annotation even when they un-
cytoplasm of phagocytic microglia
Organism substance CellPart-of
thyroid and eye muscle membranes
Tissue TissueFrag
Figure 2: Part-of relation marking entity mention span-
ning a prepositional phrase (above) and Frag relation
marking coordination with ellipsis (below).
ambiguously refer to an anatomical entity; we con-
sider the identification and resolution of such men-
tions part of the distinct coreference resolution task
(see e.g. Pradhan et al (2011)).
In addition to names and nominal mentions, we
mark adjectives that have an unambiguous sense
of relating to a specific anatomical entity. Thus,
for example, both ?kidney? and ?renal? (relating to
the kidneys) are annotated as ORGAN in expres-
sions such as ?kidney failure? and ?renal failure?.
The choice to annotate adjectival references is mo-
tivated by the expected needs of applications mak-
ing use of automatically detected anatomical entity
mentions. For example, for semantic search target-
ing documents relating to organ failure, a document
discussing ?renal failure? is obviously relevant and
should be recovered.
Syntactically, annotations mainly cover base
noun phrases without determiners, i.e. nouns with
premodifiers relevant to identifying the specific
anatomical entity referred to. We exclude noun
phrase postmodifiers such as prepositional phrases
from the span of single annotations, but apply a
separate level of annotation for part-of relations
that allow such alternate spans to be recovered
when they identify an anatomical entity (Figure 2
top). Similarly, we decompose coordinated ref-
erences to anatomical entities involving ellipsis to
non-overlapping spans, but mark the cases using a
frag(ment) relation type (Figure 2 bottom). (Due to
space considerations, we omit detailed discussion of
these relation annotations.) Together with the prop-
erties described in Section 2.3, these constraints as-
sure that any single token is assigned at most one
class label and allow the annotation to be repre-
29
Matching criterion
Task Strict Left boundary Right boundary
Mention detection (single class) 89.2%/ 82.0%/ 85.4% 93.0%/ 85.5%/ 89.1% 94.6%/ 86.9%/ 90.6%
Detection and classification (multi-class) 85.6%/ 78.7%/ 82.0% 87.0%/ 80.0%/ 83.3% 90.2%/ 82.9%/ 86.4%
Table 2: Inter-annotator agreement results (precision / recall / F-score).
sented in the standard BIO format and to be straight-
forwardly applied with many existing entity mention
taggers.
By contrast to previously introduced domain re-
sources for e.g. molecular entity and organism men-
tion detection (Tanabe et al, 2005; Gerner et al,
2010b), we do not incorporate any specificity con-
straints in our annotation criteria. That is, non-
specific expressions such as ?tissue? and ?organ? are
marked identically to specific ones such as ?epithe-
lium? and ?heart?. This choice seeks to assure the
generality of the task and methods for addressing it.
2.5 Text Selection
Texts for the corpus were drawn from two sources:
the PubMed2 database of publication abstracts, and
the PubMed Central3 (PMC) Open Access subset
of full-text publications. PubMed, containing more
than 20 million citations, has a very broad coverage
of domain scientific texts but is limited to publica-
tion abstracts, while PMC has lower coverage but
does provide over 400,000 full-text documents un-
der open licenses. By sampling both sources, we
seek to assure the corpus is relevant to IE efforts re-
gardless of their choice of texts.
To avoid bias toward e.g. subdomains of biol-
ogy or specific species, we selected texts from both
sources by random sampling. For PubMed, we sim-
ply selected a random set of citations and extracted
their abstract and title texts. For PMC, we initially
extracted all non-overlapping section texts (PMC
XML <sec> elements) as well as caption texts
(<caption> elements), and then selected a ran-
dom set of extracts. This selection seeks to maxi-
mize the diversity of the texts in the full-text sec-
tion of the corpus, and the selection of extracts larger
than isolated sentences aims to allow the corpus to
be used to study methods making use of broader
context, e.g. by incorporating constraints such as
one sense per discourse (Gale et al, 1992).
2http://pubmed.com
3http://www.ncbi.nlm.nih.gov/pmc/
We selected a total of 500 documents using this
protocol, half from PubMed and half from PMC
document extracts. (Descriptive statistics of the ab-
stracts and full-text extracts subcorpora are given
later in Table 3.)
2.6 Annotation Process
Primary annotation was created by a PhD biologist
with extensive experience in domain information ex-
traction and text annotation (TO). The use of any rel-
evant resources, such as the full article being anno-
tated or species-specific anatomy ontologies in the
OBO foundry, was encouraged for resolving unclear
or ambiguous cases during annotation. Initial anno-
tation was produced entirely manually. To further
assure the quality of the annotation, a series of au-
tomatic tests was performed and used as the basis
of a further manual round of revision.4 Annotation
guidelines were initially created based on those cre-
ated by our previous domain-specific effort (Pyysalo
et al, 2012a) and revised throughout the annotation
effort to document specific decisions made during
annotation. The annotations were created using the
BRAT annotation tool (Stenetorp et al, 2012).
To evaluate the annotation consistency, we per-
formed an inter-annotator agreement (IAA) exper-
iment. After brief training with annotation guide-
lines provided by the primary annotator, a random
10% of the corpus was independently annotated by
a PhD computer scientist with experience in domain
text annotation and anatomy ontologies (SP). IAA
was evaluated using the same criteria as applied in
experiments (see Section 3.4), holding the primary
annotation as gold. The results are shown in Table 2.
We find very good agreement both for mention de-
tection (ignoring classification) as well as for the full
task, indicating that the task is well defined and the
annotation consistency high.
4No automatically suggested annotations were incorporated
into the corpus without manual verification.
30
3 Methods
We next present the methods applied in our anatomi-
cal entity mention detection experiments. We aim to
evaluate the capacity of the newly annotated corpus
to support reliable mention detection and to estab-
lish initial baseline results for the newly introduced
resource, and thus focus only on relatively straight-
forward applications of existing methods.
3.1 MetaMap
MetaMap5 (Aronson, 2001) is a tool capable of
detecting mentions of concepts from the exten-
sive UMLS Metathesaurus (Bodenreider, 2004)
in text. The metathesaurus and MetaMap have
broad coverage of concepts relevant to biology
and medicine and provide a categorization of
concepts into 133 semantic types, ranging from
Amino Acid to Health Care Activity to
Vertebrate, many directly relevant to anatomi-
cal entities. MetaMap is a key component of the
process used by the National Library of Medicine
(NLM) to index publications in the PubMed
database and has been applied in numerous other in-
formation extraction and information retrieval tasks
(Aronson and Lang, 2010).
In initial experiments, we applied MetaMap to
training set documents to identify the subset of the
133 semantic classes relevant to anatomy, select-
ing 14 classes (including e.g. Cell, Tissue and
Body Substance) for final experiments.6 Dur-
ing testing, we used command-line arguments to re-
strict output to the selected semantic classes. The
core tagging functionality of MetaMap is rule-based,
and it does not support training on tagged data
for concept mention detection. With the exception
of the semantic class selection, the evaluation of
MetaMap reflects an ?off-the-shelf? application of
the general-purpose tool.
3.2 CRF tagging
Conditional Random Fields (CRF) (Lafferty et al,
2001) are graphical models that are frequently ap-
5http://metamap.nlm.nih.gov/
6In brief, we tagged the training data with MetaMap, ex-
tracted the subset of semantic classes giving more than 5%
precision against the gold annotations, and manually analysed
these to select this subset. The selected classes are detailed in
supplementary material available on the project webpage.
plied to sequence labeling tasks, and CRFs form
the basis of state-of-the-art methods for many en-
tity mention tagging tasks. We performed experi-
ments using the NERsuite entity mention recogni-
tion toolkit, based on the CRFsuite implementation
of CRFs (Okazaki, 2007). NERsuite provides an
extensive set of features applied in entity mention
detection, allowing the tool to achieve performance
competitive with state-of-the-art methods for many
biomedical domain tasks through retraining with-
out task-specific adaptation7. Retraining the tool for
new tasks is also straightforward, allowing applica-
tion to new tasks with modest effort.
We set the L2 regularization parameter of the
learning method using held-out evaluation with
training set data, picking out of a set of values 2n
(n ? Z) the one giving best performance.8 Other
learning method parameters were left at default val-
ues.
3.3 System combination
As a third system, we apply a straightforward com-
bination of the MetaMap and CRF tagging systems,
where we initially tag the data using MetaMap and
then incorporate the classes assigned by MetaMap
as features for training and testing with NERsuite
(stacking). More specifically, we create a BIO-
tagged version of MetaMap output segmented to
match NERsuite tokenization, and assign each token
the BIO tag based on the MetaMap semantic type
code (e.g. B-cell) as a feature.
Excepting for the addition of these MetaMap-
derived features, NERsuite is applied as described
above (Section 3.2).
3.4 Experimental setting
We split the corpus data into two primary parts: a
training set consisting of 60% of the documents and
a test set of the remaining 40%. The data splits
were performed independently for the two subcor-
pora (abstracts and full-text extracts), using strati-
fied sampling to assure broadly comparable statisti-
cal properties between the sets. The test set was held
out during development and only applied for the fi-
nal experiments.
7http://nersuite.nlplab.org/
8Specifically, C2 = 2?5 was selected.
31
Dataset
Source Item Train Test Total
Abst.
Document 150 100 250
Word 28,960 18,199 47,159
Entity 1,182 764 1,946
FTE
Document 150 100 250
Word 26,306 17,955 44,261
Entity 697 492 1,189
Total
Document 300 200 500
Word 55,266 36,154 91,420
Entity 1,879 1,256 3,135
Table 3: Overall corpus statistics. Statistics given sepa-
rately for the abstracts (abst.) and full-text extracts (FTE)
subcorpora as well as for the total.
We perform experiments in two settings: a single-
class setting where the task is restricted to the detec-
tion of anatomical entity mentions without classifi-
cation, and a multi-class setting where the correct
class label must further be assigned to each detected
mention. As MetaMap uses UMLS semantic classes
that do not fully align with the applied CARO-based
classes, MetaMap is only applied in the single-class
setting.
For evaluation, we adopted the protocol, crite-
ria and metrics of the established BioNLP/JNLPBA
shared task 2004 (Kim et al, 2004). To assure com-
patibility, we created our evaluation tool on the ba-
sis of the shared task evaluation script. The eval-
uation is thus based on entity-wise (microaverage)
precision/recall/F-score metrics, and tagging perfor-
mance is separately evaluated under strict match, left
boundary match and right boundary match criteria.
In the former setting, a predicted entity must exactly
match the extent of a gold standard entity, while in
the latter two settings, it is enough that the left/right
boundary matches.
3.5 Format
The annotation is distributed in the standard column-
based BIO format applied for e.g. CoNLL 2003
(Tjong Kim Sang and De Meulder, 2003) and
JNLPBA (Kim et al, 2004) data, among other es-
tablished datasets.
4 Results
4.1 Corpus statistics
Table 3 presents the overall corpus statistics. We
note that the abstracts and full-text extracts (FTE)
Type Count
CELL 776
MULTI-TISSUE STRUCTURE 639
ORGAN 381
PATHOLOGICAL FORMATION 368
ORGANISM SUBSTANCE 291
CELLULAR COMPONENT 199
TISSUE 169
ORGANISM SUBDIVISION 162
IMMATERIAL ANATOMICAL ENTITY 60
ANATOMICAL SYSTEM 51
DEVELOPING ANATOMICAL STRUCTURE 39
Table 4: Annotation statistics by type.
subcorpora are of comparable size in terms of their
word counts, but the number of annotations is 1.6
times higher in the abstracts subcorpus (1.5 cor-
recting for number of words). This difference in
anatomical entity mention density between abstracts
and full texts parallels the findings of Cohen et al
(2010) on the relative density of gene, drug and dis-
ease mentions. We further note that the estimated
density of anatomical entity mentions in abstracts
(approx. 41 per 1000 words) and full texts (27 per
1000) are broadly comparable to the gene mention
density estimates of Cohen et al (61 and 47 for ab-
stracts and full texts, respectively).
Table 4 presents a breakdown by annotation type.
There are large differences in the number of anno-
tations by type, with the majority class CELL out-
numbering the rarest type 20-fold. While the total
number of annotated examples is likely to be suf-
ficient for training machine learning-based taggers
and most of the classes contain a respectable num-
ber of examples, the statistics suggest that the least
frequently annotated types may represent challenges
for learning.
4.2 Entity Mention Detection
Table 5 presents the experimental results for anatom-
ical entity mention detection (single-class). In terms
of F-score, we find the same ranking of the three
methods for all three criteria, with the CRF-based
tagger outperforming the rule-based MetaMap, and
the combination method outperforming its compo-
nents. Although it is not surprising that a dedicated
machine learning-based system is capable of outper-
forming a general-purpose, largely rule-based sys-
tem, this result does reflect positively on both the
32
Matching criterion
Method Strict Left boundary Right boundary
MetaMap 50.78% / 64.49% / 56.82% 54.67% / 69.43% / 61.17% 58.18% / 73.89% / 65.10%
NERsuite 77.98% / 52.15% / 62.50% 81.43% / 54.46% / 65.27% 90.00% / 60.19% / 72.14%
MetaMap + NERsuite 82.09% / 62.42% / 70.92% 84.61% / 64.33% / 73.09% 90.68% / 68.95% / 78.34%
Table 5: Overall single-class anatomical entity mention detection results (precision / recall / F-score).
Matching criterion
Method Strict Left boundary Right boundary
NERsuite 72.07% / 42.12% / 53.17% 72.75% / 42.52% / 53.67% 85.69% / 50.08% / 63.22%
MetaMap + NERsuite 75.41% / 51.75% / 61.38% 76.45% / 52.47% / 62.23% 83.99% / 57.64% / 68.37%
Table 6: Overall anatomical entity mention detection and classification results (precision / recall / F-score).
consistency of the annotation as well as the suffi-
ciency of the size of the newly introduced corpus.
In this application, we find that MetaMap tends to
favor recall over precision ? perhaps reflecting its
focus on IR applications (Aronson and Lang, 2010)
? while the trained machine learning-based models
are clearly biased in favor of high precision.
As expected on the basis of the results of previous
evaluations using similar experimental setups (Kim
et al, 2004), results are notably better under the re-
laxed matching criteria. In particular, requiring only
the right boundaries of annotations to match yields
F-scores nearly 10% points higher than under strict
matching. Recalling that the annotations primar-
ily mark base noun phrases, this suggests that the
systems comparatively frequently identify the head
word of an anatomical entity mention correctly but
differ from gold annotation regarding the choice of
premodifiers included in the span of the annotation.
As limited variation in premodifier selection is ar-
guably acceptable for many applications and relaxed
matching criteria are frequently applied in domain
tagging tasks (Kim et al, 2004; Wilbur et al, 2007),
we propose to consider performance under the re-
laxed right boundary match criterion as the primary
result for evaluation using the new corpus.
Table 6 presents the results for anatomical entity
mention detection and classification using the 11-
class categorization used in annotation.9 While per-
formance in terms of F-score is approximately 10%
points lower than for the single-class task, this drop
is comparatively modest given the large number of
9Note that evaluation using MetaMap only is not possible as
its semantic classes differ from those used in the annotation.
distinct classes, indicating that the number of an-
notations of most individual classes is sufficient for
learning.
While these initial results are not as high as for
established entity mention detection tasks in the do-
main (Wilbur et al, 2007; Rebholz-Schuhmann et
al., 2011), we consider the level of performance
quite good given the many new challenges relat-
ing to the task. Further, as the mention detection
methods were also applied with only modest specific
adaptation to the task, we believe there remain many
opportunities for further development of methods
for the task.
4.3 Discussion
Many commonly targeted mention types in both
the ?general? and the biological domain are fre-
quently characterized by obvious surface features:
the names of people and locations are capitalized in
many languages, as are genera in scientific species?
names, and many gene and chemical names have
comparable features distinguishing them from com-
mon nouns (consider e.g. p53, IgE, c-myc, Ca2+,
H2SO4). By contrast, many typical anatomical en-
tity mentions are common noun compounds lacking
obvious distinguishing surface features. This fact
likely contributes to the comparatively low perfor-
mance of the CRF-based tagger when applied with-
out support from lexical resources.
A further challenge that arises comparatively fre-
quently in anatomical entity mention detection is
ambiguity between entity mentions and other words
sharing the same surface form. For example, while
Barack Obama, Sweden, p53 and H2SO4 can be
33
safely identified as mentions of a person, country,
gene, and chemical without reference to context,
face should not be marked as an anatomical entity
mention in face the facts, nor should Airways in
British Airways. Thus, approaches relying on simple
matching against lexical resources will not suffice
for accurate anatomical entity mention detection.
Our evaluation results demonstrated a clear ad-
vantage to combining detection based on lexical re-
sources with machine learning-based tagging, an ap-
proach we believe will be key to the further develop-
ment of reliable anatomical entity mention tagging
that we will seek to explore in detail in future work.
To facilitate analysis of the performance of the meth-
ods, we provide the predictions of each method in
supplementary data on the project homepage.
5 Related work
A number of domain corpora such as GENIA (Ohta
et al, 2002), BioInfer (Pyysalo et al, 2007), and the
recently introduced CellFinder corpus (Neves et al,
2012) include annotation for at least some classes
of anatomical entities. However, such corpora typ-
ically cover only specific subdomains of the litera-
ture, such as transcription factors in human blood
cells (GENIA), protein-protein interactions (BioIn-
fer), or stem cells (CellFinder). To the best of our
knowledge, this is the first effort introducing a cor-
pus annotated for anatomical entity mentions that
specifically aims to be representative of the entire
available literature. We note that there is a well-
established precedent to this goal: sentences for
the de facto standard corpus for gene/protein name
recognition, GENETAG (Tanabe et al, 2005), were
similarly selected from PubMed abstracts without
domain restrictions.
The BioNLP/JNLPBA shared task 2004 (Kim et
al., 2004) targeted the detection of mentions of five
types of biological entities, including two that would
fall within in the scope of our CELL annotation
(?Cell type? and ?Cell line?). Other than this com-
paratively early shared task, collaborative domain
efforts such as BioCreative (Krallinger et al, 2008)
and CALBC (Rebholz-Schuhmann et al, 2011) have
not targeted anatomical entity mentions.
Some recent studies have considered the use of
ontological resources for the detection of anatomi-
cal entity mentions in natural language expressions.
In previous work (Pyysalo et al, 2012b), we studied
the classification of isolated noun phrases extracted
from PubMed to identify anatomy terms. Travillian
et al (2011) considered two lexical matching appli-
cations to detect anatomical entities from two OBO
resources in user-provided terms. However, these
efforts have not involved the annotation or detection
of mentions in context, which we view as critical for
real-world entity mention detection method devel-
opment and evaluation.
6 Conclusions
We have introduced a manually annotated corpus for
open-domain anatomical entity mention detection,
consisting of 500 documents (over 90,000 words)
drawn from publication abstracts and full texts. The
primary corpus annotation consists of the identifi-
cation of over 3,000 references to both healthy and
pathological anatomical entities, marked using a de-
tailed 11-class categorization based on established
biomedical domain ontologies. We demonstrated
the use of the new corpus through a comparative
evaluation of MetaMap, a general semantic class
tagger; NERsuite, a CRF-based machine learning
system; and a stacked combination of the two, find-
ing that under a relaxed matching criterion, the com-
bination approaches 80% F-score at mention detec-
tion and 70% F-score at mention detection and clas-
sification. This level of performance is encourag-
ing for a first application and suggests that reliable
open-domain anatomical entity mention detection is
not an unrealistic target.
We hope that the introduced corpus can serve as a
reference standard for the further development and
evaluation of methods for anatomical entity men-
tion detection. This corpus, the introduced evalua-
tion tools, and other resources created in this study
are made available under open licences from http:
//www.nactem.ac.uk/anatomy/.
Acknowledgments
This work was funded by UK Biotechnology and Bi-
ological Sciences Research Council (BBSRC) under
project Automated Biological Event Extraction from
the Literature for Drug Discovery (reference num-
ber: BB/G013160/1).
34
References
S. Ananiadou, S. Pyysalo, J. Tsujii, and D.B. Kell. 2010.
Event extraction for systems biology by text mining
the literature. Trends in Biotechnology, 28(7):381?
390.
A.R. Aronson and F.M. Lang. 2010. An overview of
MetaMap: historical perspective and recent advances.
Journal of the American Medical Informatics Associa-
tion, 17(3):229?236.
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of AMIA, pages 17?21.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
M. Bada and L. Hunter. 2011. Desiderata for ontologies
to be used in semantic annotation of biomedical docu-
ments. Journal of Biomedical Informatics, 44(1):94?
101.
O. Bodenreider. 2004. The unified medical language
system (UMLS): integrating biomedical terminology.
Nucleic acids research, 32(suppl 1):D267?D270.
K.B. Cohen, H. Johnson, K. Verspoor, C. Roeder, and
L. Hunter. 2010. The structural and content aspects of
abstracts versus bodies of full text journal articles are
different. BMC bioinformatics, 11(1):492.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. One
sense per discourse. In Proceedings of the workshop
on Speech and Natural Language, pages 233?237.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010a. An
exploration of mining gene expression mentions and
their anatomical locations from biomedical text. In
BioNLP?10, pages 72?80.
M. Gerner, G. Nenadic, and C.M. Bergman. 2010b.
LINNAEUS: a species name identification system
for biomedical literature. BMC bioinformatics,
11(1):85+.
M.A. Haendel, F. Neuhaus, D. Osumi-Sutherland, P.M.
Mabee, J.L.V. Mejino, C.J. Mungall, and B. Smith.
2008. CARO?the common anatomy reference ontol-
ogy. Anatomy Ontologies for Bioinformatics, pages
327?349.
M.A. Haendel, G.G. Gkoutos, S.E. Lewis, and
C. Mungall. 2009. Uberon: towards a comprehensive
multi-species anatomy ontology. Nature precedings.
J-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task at
JNLPBA. In Proceedings JNLPBA?04.
M. Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-
abe, J. Wilbur, L. Hirschman, and A. Valencia.
2008. Evaluation of text-mining systems for biology:
overview of the Second BioCreative community chal-
lenge. Genome biology, 9(Suppl 2):S1.
A. Kumar, B. Smith, and D.D. Novotny. 2004. Biomed-
ical informatics and granularity. Comparative and
functional genomics, 5(6-7):501?508.
J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289.
N. Naderi, T. Kappler, C.J.O. Baker, and R. Witte.
2011. OrganismTagger: Detection, normalization, and
grounding of organism entities in biomedical docu-
ments. Bioinformatics.
M. Neves, A. Damaschun, A. Kurtz, and U. Leser. 2012.
Annotating and evaluating text for stem cell research.
In Third Workshop on Building and Evaluation Re-
sources for Biomedical Text Mining (BioTxtM 2012).
(to appear).
T Ohta, Y Tateisi, H Mima, and J Tsujii. 2002. GE-
NIA corpus: an annotated research abstract corpus in
molecular biology domain. Proceedings of the Human
Language Technology Conference (HLT 2002), pages
73?77.
N. Okazaki. 2007. CRFsuite: a fast imple-
mentation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
ontonotes. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1?27.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. BioInfer: A cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, T. Ohta, M. Miwa, H-C. Cho, J. Tsujii, and
S. Ananiadou. 2012a. Event extraction across mul-
tiple levels of biological organization. (manuscript in
review).
S. Pyysalo, T. Ohta, J. Tsujii, and S. Ananiadou. 2012b.
Learning to classify anatomical entities using open
biomedical ontologies. Journal of Biomedical Seman-
tics. (to appear).
D. Rebholz-Schuhmann, A. Yepes, C. Li, S. Kafkas,
I. Lewin, N. Kang, P. Corbett, D. Milward, E. Buyko,
E. Beisswanger, K. Hornbostel, A. Kouznetsov,
R. Witte, J. Laurila, C. Baker, C. Kuo, S. Clematide,
F. Rinaldi, R. Farkas, G. Mora, K. Hara, L.I. Fur-
long, M. Rautschka, M. Neves, A. Pascual-Montano,
35
Q. Wei, N. Collier, M. Chowdhury, A. Lavelli,
R. Berlanga, R. Morante, V. Van Asch, W. Daelemans,
J. Marina, E. van Mulligen, J. Kors, and U. Hahn.
2011. Assessment of NER solutions against the first
and second calbc silver standard corpus. Journal of
Biomedical Semantics, 2(Suppl 5):S11.
C. Rosse and J.L.V. Mejino. 2003. A reference on-
tology for biomedical informatics: the foundational
model of anatomy. Journal of Biomedical Informat-
ics, 36(6):478?500.
C. Rosse and J.L.V. Mejino. 2008. The foundational
model of anatomy ontology. Anatomy Ontologies for
Bioinformatics, pages 59?117.
B. Smith, A. Kumar, W. Ceusters, and C. Rosse. 2005.
On carcinomas and other pathological entities. Com-
parative and functional genomics, 6(7-8):379?387.
B. Smith, M. Ashburner, C. Rosse, J. Bard, W. Bug,
W. Ceusters, L. J Goldberg, K. Eilbeck, A. Ireland,
C.J. Mungall, N. Leontis, P. Rocca-Serra, A. Rut-
tenberg, S-A Sansone, R.H. Scheuermann, N. Shah,
P.L. Whetzel, and S. Lewis. 2007. The OBO
Foundry: coordinated evolution of ontologies to sup-
port biomedical data integration. Nature biotechnol-
ogy, 25(11):1251?1255.
P. Stenetorp, S. Pyysalo, G. Topic?, T. Ohta, S. Ananiadou,
and J. Tsujii. 2012. brat: a web-based tool for NLP-
assisted text annotation. In Proceedings of the EACL
2012 Demonstrations, pages 102?107.
L. Tanabe, N. Xie, L. Thom, W. Matten, and W.J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC bioinfor-
matics, 6(Suppl 1):S3.
E.F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003, pages 142?147.
R. Travillian, T. Adamusiak, T. Burdett, M. Gruenberger,
J. Hancock, A-M. Mallon, J. Malone, P. Schofield, and
H. Parkinson. 2011. Anatomy ontologies and poten-
tial users: bridging the gap. Journal of Biomedical
Semantics, 2(Suppl 4):S3.
J. Wilbur, L. Smith, and L. Tanabe. 2007. BioCre-
ative 2 Gene Mention Task. In Proceedings of Second
BioCreative Challenge Evaluation Workshop, pages
7?16.
36
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37?46,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
A Three-Way Perspective on Scientific Discourse Annotation for       Knowledge Extraction   Maria Liakata Aberystwyth University, UK / EMBL-EBI, UK liakata@ebi.ac.uk Paul Thompson University of Manchester, UK paul.thompson@manchester.ac.uk Anita de Waard Elsevier Labs, USA / UiL-OTS, Universiteit Utrecht, NL a.dewaard@elsevier.com    Raheel Nawaz University of Manchester, UK raheel.nawaz@cs.man.ac.uk Henk Pander Maat UiL-OTS, Universiteit Utrecht, NL h.l.w.pandermaat@uu.nl Sophia Ananiadou University of Manchester, UK sophia.ananiadou@manchester.ac.uk   Abstract 
This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme.  1 Introduction The literature boom in the life sciences over the past few years has sparked increasing interest into text mining tools, which facilitate the automatic extraction of useful knowledge from text (Ananiadou et al, 2006; Ananiadou  &  McNaught, 2006; Zweigenbaum et al, 2007; Cohen  &  Hunter, 2008). Most of these tools have focussed on entity recognition and relation extraction and with few exceptions, e.g., (Hyland, 1996; Light et al, 2004; S?ndor, 2007; Vincze et al, 2008), do not take into account the discourse context of the knowledge extracted. However, failure to take this context into account results in the loss of information vital for the correct interpretation of extracted knowledge, e.g. the scope of the relations, or the level of certainty with which they are expressed. A particular piece of 
knowledge may represent, e.g., an accepted fact, hypothesis, results of an experiment, analysis based on experimental results, factual or speculative statements etc. Furthermore, this knowledge may represent the author's current work, or work reported elsewhere. The ability to recognise different discourse elements automatically provides crucial information for the correct interpretation of extracted knowledge, allowing scientific claims to be linked to experimental evidence, or newly reported experimental knowledge to be isolated. The importance of categorising such knowledge becomes more pronounced as analysis moves from abstracts to full papers, where the content is richer and linguistic constructions are more complex (Cohen et al, 2010). Analysis of full papers is extremely important, since less than 8% of scientific claims occur in abstracts (Blake, 2010). Various different schemes for annotating discourse elements in scientific texts have been proposed. The schemes vary along several axes, including perspective, motivation, complexity and the granularity of the units of text to which the scheme is applied. Faced with such variety, it is important to be able to select the best scheme(s) for the purpose at hand. Answers to questions such as the following can help in the selection process: 1. What are the relative merits of the different schemes? 2. What are the similarities and differences between schemes? 3. Can annotation according to multiple schemes provide enhanced information?  
37
Category Description Hypothesis An unconfirmed statement which is a stepping stone of the investigation Motivation The reasons behind an investigation Background Generally accepted background knowledge and previous work Goal A target state of the investigation where intended discoveries are made Object-New An entity which is a product or main theme of the investigation Object-New-Advantage Advantage of an object Object-New-Disadvantage Disadvantage of an object Method-New Means by which authors seek to achieve a goal of the investigation Method-New-Advantage Advantage of a Method Method-New-Disadvantage Disadvantage of a Method Method-Old A method mentioned pertaining to previous work Method-Old-Advantage Advantage of a Method Method-Old-Disadvantage Disadvantage of a Method Experiment An experimental method Model A statement about a theoretical model or framework Observation The data/phenomena recorded in an investigation Result Factual statements about the outputs, interpretation of observations Conclusion Statements inferred from observations & results  Table 1. The CoreSC Annotation scheme: layers 1 & 2 	 ?4. Is there any advantage in merging annotation schemes or is it better to allow complementary and different dimensions of scientific discourse annotation?	 ?As a starting point to addressing such questions, we provide a comparison of three different schemes for the annotation of discourse elements within scientific papers. Each scheme has a different perspective and motivation:, one is content-driven, seeking to identify the main components of a scientific investigation, another is driven by the need to describe events of biomedical relevance and the third focusses on how epistemic knowledge is conveyed in discourse.  These different viewpoints mean that the schemes vary in both the type and complexity of the discourse elements identified, as well as the types of units to which the annotation is applied, i.e. complete sentences, segments of sentences, or specific relations/events occurring within these sentences. To facilitate the comparison, we have annotated three full papers according to each of the schemes. The analysis resulting from this three-way annotation considers mappings between schemes, their relative merits, and how the information annotated by the different schemes can 
complement each other to provide enriched details about knowledge extracted from the texts. In the following sections, we firstly provide a description of the three schemes, and then explain how they have been used in our corpus annotation. Finally we discuss the results from the comparison, and the features of each scheme. 2 Sentence annotation: CoreSC scheme  The reasoning behind this scheme is that a paper is the human-readable representation of a scientific investigation. Therefore, the goal of the annotation is to retrieve the content model of scientific investigations as reflected within scientific discourse. The hypothesis is that there is a set of core scientific concepts (CoreSC), which constitute the key components of a scientific investigation. CoreSCs consist of 11 concepts originating from the CISP (Core Information about Scientific Papers) meta-data (Soldatova  &  Liakata, 2007), which are a subset of classes from the EXPO ontology for the description of scientific experiments (Soldatova  &  King, 2006). The CoreSCs are: Motivation, Goal, Object, Background, Hypothesis, Method, Model, Experiment, Observation, Result and Conclusion. 
38
Figure 1.  Bio-Event Representation 
The CoreSC scheme (Liakata et al, 2010; Liakata et al, 2012) implements the above-mentioned concepts as a 3-layered sentence-based annotation scheme. This means that each sentence in a document is assigned one of the 11 CoreSC concepts. The scheme also considers a layer designated to properties of the concepts (e.g. New Method vs Old Method) as well as identifiers which link instances of the same concept across sentences. A short definition of CoreSC categories and their properties can be found in Table 1.  The CoreSC scheme is accompanied by 47-page annotation guidelines, and has been used by 16 domain experts to annotate a corpus of 265 full papers from physical chemistry & biochemistry (Liakata  &  Soldatova, 2009; Liakata et al, 2010). This corpus consists of 40,000 sentences, containing over 1 million words and was developed in three phases (for details see Liakata et al (2012)). Inter-annotator agreement between experts was measured in terms of Cohen?s kappa (Cohen, 1960) on 41 papers and ranged between 0.5 and 0.7. Machine learning classifiers have been trained on the CoreSC corpus, achieving > 51% accuracy across the eleven categories. The most accurately predicted category is Experiment, the category describing experimental methods (Liakata et al, 2012). Classifiers trained on 1000 Biology abstracts annotated with CoreSC have obtained an accuracy of over 80% (Guo et al, 2010). Models trained on the CoreSC corpus papers have been used to create automatic summaries of the papers, which have been evaluated in a question answering task (Liakata et al, 2012). Lastly, the CoreSC scheme was used to annotate 50 papers from Pubmed Central pertaining to Cancer Risk Assessment. A web tool (SAPIENTA 1 ) allows users to annotate their full papers with Core Scientific concepts, and can be combined with manual annotation. A UIMA framework 2 implementation of this code for large-scale annotation of CoreSC concepts is in progress. 3 Event annotation: Meta-knowledge for bio-events The motivation for this annotation scheme is to allow the training of more sophisticated event-                                                1 http://www.sapientaproject.com/software 2 http://uima.apache.org/ 
based information extraction systems. In contrast to the sentence-based scheme described in section 2, this scheme is applied at the level of events (Ananiadou et al, 2010), of which there may be several within a single sentence. 3.1 Bio-Events Events are template-like, structured representations of pieces of knowledge contained within sentences. Normally, events are ?anchored? to a trigger (typically a verb or noun) around which the knowledge expressed is organised. Each event has one of more participants, which describe different aspects of the event. Participants can correspond to entities or other events, and are often labelled with semantic roles, e.g., CAUSE, THEME, LOCATION, etc. The work described here focusses specifically on bio-events, which are complex structured relations representing fine-grained relations between bio-entities and their modifiers. Figure 1 provides some examples of bio-events. Event extraction systems (Bj?rne et al, 2009; Miwa et al, 2010; Miwa et al, 2012; Quirk et al, 2011) are typically trained on text corpora, in which events and their participants have been manually annotated by domain experts. Research into bio-event extraction has been boosted by the two recent shared tasks at BioNLP 2009/2011 (Kim et al, 2011; Pyysalo et al, In Press). Several gold standard event annotated corpora exist; examples include the GENIA Event Corpus (Kim et al, 2008), GREC (Thompson et al, 2009) and BioInfer (Pyysalo et al, 2007), in addition to the corpora produced for the shared tasks. 
3.2 Meta-knowledge Annotation Until recently, the only attempts to recognise information relating to the correct interpretation of events were restricted to sparse details regarding negation and speculation (Kim et al, 2011). 
39
In order to address this problem, a multi-dimensional annotation scheme especially tailored to bio-events was developed (Nawaz et al, 2010; Thompson et al, 2011). The scheme identifies and categorises several different types of contextual details regarding events (termed meta-knowledge), including discourse information. Different types of meta-knowledge are encoded through five distinct dimensions (Figure 2). The advantage of using multiple dimensions is that the interplay between the assigned values in each dimension can reveal both subtle and substantial differences in the types of meta-knowledge expressed. In the majority of cases, meta-knowledge is expressed through the presence of particular ?clue? words or phrases, although other features can also come into play, such as the tense of the event trigger, or the relative position within the text. 
Figure 2: Meta-knowledge annotation 	 ?The annotation task consists of assigning an appropriate value from a fixed set for each dimension, as well as marking the textual evidence for this assignment. The five meta-knowledge dimensions and their values are as follows: Knowledge Type (KT): Captures the general information content of the event. Each event is classified as one of: Investigation (enquiries and examinations, etc.), Observation (direct experimental observations), Analysis (inferences, interpretations and conjectures, etc.), Fact (known facts), Method (methods) or Other (general events that provide incomplete information or do not fit into any other category).  Certainty Level (CL): Encodes the confidence or certainty level ascribed to the event in the given text. The epistemic scale is partitioned into three distinct levels: L3 (no expression of uncertainty), 
L2 (high confidence or slight speculation) and L1 (low confidence or considerable speculation). Polarity: Identifies negated events. Negation is defined as the absence or non-existence of an entity or a process. Manner: Captures information about the rate, level, strength or intensity of the event, using three values: High, Low, or Neutral (no indication of rate/intensity). Source:  Encodes the source of the knowledge being expressed by the event as Current (the current study) or Other (any other source).      Of these five dimensions, only KT, CL and Source were considered during the comparison with the other two schemes, since they are directly related to discourse analysis.  The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al, 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3  (Thompson et al, 2011). Inter-annotator agreement rates ranged between 0.84?0.93 (Cohen?s Kappa).  Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al, In Press).  In addition, the EventMine-MK service (Miwa et al, In Press), based on EventMine (Miwa et al, 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP?09 Shared Task corpus (Kim et al 2011). EventMine-MK is available as a component of the U-Compare interoperable text mining system4 (Kano et al, 2011). 4 Clause annotation: Segments for epistemic knowledge The third scheme we consider uses a Discourse Segment Type classification of segments at, roughly, a clause level, i.e., each segment has a main verb. This means that the level of granularity of argumentational elements in this scheme lies between the other two schemes, i.e. it is usually more granular than CoreSC, but sometimes less granular than the event-based scheme.                                                  3 http://www.nactem.ac.uk/meta-knowledge/ 4 http://www.nactem.ac.uk/ucompare/ 
40
 Table 2:  Discourse Segment Types 	 ?The segment annotation scheme identifies a taxonomy of discourse segment types that seem to be exclusive and useful (de Waard & Pander Maat, 2009). Three classes of segment types are defined:  ? Basic segment types: segments referring directly to the topic of study ? see Table 2.  ? ?Other?-segment types: segments referring to conceptual or experimental work in other research papers than the current one ? Regulatory segment types: ?regulatory? clauses that control and introduce other segments.  A list of segment types is presented in Table 2; further details, including a list of all segment types and correlations with verb tense can be found in de Waard  &  Pander Maat (2009). The focus of this work is to identify linguistic features that characterise these discourse segment types, according to three aspects: ? Verb tense, aspect, mood and voice ? Semantic verb class ? Epistemic modality markers So far, 6 full-text papers (comprising about 2300 segments) have been manually annotated with segment types and correlated with the above features. A first automated validation was promising (de Waard, Buitelaar and Eigener, 2009). The need for parsing at a clause level is especially prominent in biological text, since specific semantic roles are played by particular clause types. We give four examples of typical 
clause constructions that play a specific rhetorical role: firstly, reporting clauses are often sentence-initial ?that? matrix clauses (1a): 1. a.  This suggests that  1.b. miR-372 and miR-373 caused the observed  selective growth advantage. Secondly, descriptions confirming certain accepted characteristics of biological entities are often given as nonrestrictive relative clauses (2b):  2.a. We also generated BJ/ET cells expressing the  RASV12-ERTAM chimera gene,  2. b. which is only active when tamoxifen is added  Thirdly, a subordinate gerund clause is often used to describe a method (3a), with a main (finite) clause describing a result (3b) and fourthly, experimental goals are often given as a (mostly sentence-initial) clause with a to-infinitive (4a) often preceding a past-tense methods clause (4b). 3. a. Using fluorescence microscopy and luciferase assays, b. we observed potent and specific miRNA activity expressed from each miR-Vec (Figure S2). 4. a. To identify miRNAs that can interfere with this process  4. b. we transduced BJ/ET fibroblasts with miR-Lib  However, the lack of simple robust clause parsers has prevented the automated identification of semantic roles at the clause level. Therefore, this scheme has so far only been manually 
Segment Description Examples  Fact knowledge accepted to be true, a known fact. mature miR-373 is a homolog of miR-372,  Hypothesis  a proposed idea, not supported by evidence This could for instance be a result of high mdm2 levels  Problem unresolved, contradictory, or unclear issue However, further investigation is required to demonstrate the exact mechanism of LATS2 action Goal research goal To identify novel functions of miRNAs, Method  experimental method Using fluorescence microscopy and luciferase assays,  Result a restatement of the outcome of an experiment all constructs yielded high expression levels of mature miRNAs   Implication  an interpretation of the results, in light of data our procedure is sensitive enough to detect mild growth differences   Other-Hypothesis an idea proposed by others [It is generally believed that] transcription factors are the final common pathway driving differentiation] Regulatory-Hypothesis a matrix clause introducing a hypothesis It is generally believed that [transcription factors are the final common pathway driving differentiation] 
41
implemented. Despite being less widely implemented than the other two schemes, we believe that the segment scheme offers some useful pointers for linguistic features that can identify particular rhetorical classes in the text, and secondly, offers an interesting perspective on the fact that in biological text, several rhetorical moves are made within a single sentence.  5 Data and methods Three papers already annotated according to the GENIA event annotation scheme (Kim et al, 2008), were further annotated according to the three annotation schemes described above. We obtained all corresponding CoreSCs, events and segments per sentence. Each sentence has a single CoreSC annotation and one or more segment annotations (depending on the number of clauses). Event annotations in a sentence may range from zero to multiple, according to whether any relevant biomedical events are described in the sentence.  Events within a sentence are mapped to segments by identifying which segment contains the trigger for a particular event. The three meta-knowledge dimensions for events considered in this comparison, i.e., KT, CL and Source, result in 16 different combinations of values encountered in the three papers. The numbers for CoreSC and Segment labels encountered were 12 and 22, respectively. Confusion matrices were obtained for each paper and for each pair of annotation schemes. Note that, as bio-events are largely unconcerned with describing methodology, the Methods sections of these papers do not contain event annotation or meta-knowledge annotation. The pairwise confusion matrices from each paper were combined, resulting in three matrices (Tables 3, 4 and 5), which describe the associations between the annotation schemes in the three papers examined. We have highlighted the highest frequencies per row and where appropriate also the highest values per column. The use of two different colours aims to facilitate readability. 6 Results and Discussion We present the results from analysing the pairwise confusion matrices for the three schemes and discuss the merits of each scheme. 
6.1 Event Meta-knowledge v. CoreSC In Tables 3 (and 5), the meta-knowledge categories combine KT, CL and Source ((O)ther) values. Table 3 shows some straightforward and expected mappings, e.g.,Method (Met,L3) events are almost always found within CoreSC Experiment or Method sentences, whilst Investigation events (Inv,L3) occur most frequently within CoreSC Goal or Motivation sentences.  For other categories, information from the two schemes can complement each other in different ways. For example, KT and Source information about events can help to distinguish different types of information within CoreSC Background sentences (top left corner of Table 3). Such information mainly corresponds to facts, observations from previous studies, or analyses of information. Conversely, information from the CoreSC scheme can help to further classify the interpretation of events. For example, events with an analytical interpretation (Ana,L1,L2,L3) may occur as background information to a study (Bac), as hypotheses (Hyp),  as part of observations (Obs), when reporting the results of the current study (Res) or when making concluding remarks about the study (Con). CoreSCs can also help to further refine events relating to outcomes (Obs,L3) according to whether they pertain to (Obs)ervations, (Res)ults or (Con)clusions. CoreSC Conclusion, Result and Observation sentences contain mainly Observation events concerned with the current study. However, such sentences often also include an analytical part, with varying levels of certainty, which event information can help to isolate. The CL annotated for events is also useful in helping to determine the confidence with which information is stated in CoreSC Conclusion and Hypothesis sentences.  Due to the nature of bio-event annotation, only a small number of events correspond to methods. Thus, CoreSC provides a more detailed characterisation of method-related sentences, i.e., Experiment, Method_New, Model and Object. 6.2 Discourse Segments v. CoreSC In most cases, there seems to be natural mapping between the two schemes (See Table 4). CoreSC Observation maps to Result, CoreSC Method and Experiment map to Method, CoreSC Hypothesis maps to Hypothesis, CoreSC Goal maps to Goal, 
42
CoreSC Conclusion maps to Implication and Hypothesis, CoreSC Result maps to Implication and Result, and Problem is equivalent to CoreSC Motivation. The bulk of CoreSC Background maps to Fact and Other-Implication, but the ?Other? Segment categories provide a substantial refinement of the CoreSC Background category.   
 Table 3. Event Meta-knowledge vs CoreSC    On the other hand, CoreSC refines Method, Result and Implication segments. CoreSC Result may include both Fact and Method clauses, which can be captured by the Segment scheme, since annotation is performed at the clause level. CoreSC Conclusion maps to both Implication and Hypothesis segments, suggesting that there may be differences in the certainty levels of these conclusions. This is supported by preliminary classification experiments (paper in progress).  	 ?6.3 Discourse Segments v. Event Meta-Knowledge	 ? Some straightforward mappings exist between segment and event meta-knowledge categories (Table 5). For example, Investigation events (Inv, L3) are generally found within Goal and Problem segments; Method events (Met,L3) are normally found within Method segments, Observation events (Obs,L3) are found mainly within Result, Fact and Implication segments and (Ana,L1,L2) events correspond mainly to Hypotheses and Implications. Whilst these are similar findings to the comparison between event meta-knowledge and CoreSCs, the variance of the distribution is often smaller when mapping from Events to Segments. This is to be expected ? the information encoded by many events has the scope of roughly a clause, which corresponds closely to the scope of 
discourse segments. This could permit cleaner one-to-one mappings between categories. 
 Table 4: Segments vs CoreSC 	 ?  Hypothesis and Implication segments mainly contain (Ana)lysis events. The differing certainty levels of events can help to refine information about the statements made within these segments. Likewise, these segment types could help to refine the nature of the analysis described by the event.   Similarly to the CoreSC scheme, the results suggest that Result segments could be refined by the meta-knowledge scheme to distinguish between results emerging from direct experimental observations, and those obtained through analysis of experimental observations. Another interesting result is that Fact segments can contain Fact, (Ana)lysis or (Obs)ervation events. This may suggest that Fact segments are actually a rather general category, containing a range of different information. Few events occur within the Regulatory segments, as these mainly introduce content-bearing segments.  The majority of Method segments and a significant number of the Result segments do not correspond to events, as none of the methods sections have been annotated with event information, for reasons explained previously.  
	 ?Table 5: Segments vs Event Meta-Knowledge 
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs Res0 42 24 49 7 7 25 1 13 6 7 47 54Obs,L3,O 166 0 0 0 0 0 3 0 12 0 0 2Ana,L3,O 33 1 0 0 0 0 0 1 0 0 0 0Ana,L2,O 3 0 0 0 0 0 0 0 0 0 0 0Fact,L3,O 7 0 0 0 0 0 0 0 0 0 0 0Fact,L3 24 1 0 1 0 0 0 0 5 3 0 2Oth,L3 125 30 0 8 16 5 3 2 8 3 9 42Ana,L1 2 10 0 0 6 0 0 0 1 0 0 6Ana,L2 30 15 0 1 14 0 0 2 1 0 8 33Ana,L3 11 11 0 0 2 1 2 0 3 0 14 28Met,L3 4 1 15 1 0 5 0 0 0 0 2 6Inv,L2 0 0 0 0 0 0 0 0 0 0 1 1Inv,L3 5 3 1 6 2 4 3 0 8 0 1 1Inv,L3,O 0 0 0 0 2 0 1 0 0 0 0 0Obs,L1 1 0 0 0 0 0 0 0 0 0 0 0Obs,L2 1 0 0 0 0 0 0 0 0 0 1 0Obs,L3 31 34 3 1 10 3 0 2 7 1 59 87
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs ResFact 118 3 0 3 7 0 0 1 15 7 5 34OtherFact 70 4 0 0 0 0 0 3 1 0 0 0OtherGoal 2 1 0 0 0 0 0 0 0 0 0 0OtherHypothesis 14 0 0 0 0 0 0 0 0 0 0 0OtherImplication 124 1 0 0 3 0 0 1 5 0 0 1OtherMethod 5 0 0 0 0 0 3 0 0 0 0 2OtherProblem 1 0 0 0 0 0 0 0 0 0 0 0OtherResult 64 1 0 0 0 0 6 0 0 3 0 9RegFact 1 3 0 0 0 0 0 0 0 0 0 2Implication 13 58 0 0 2 0 0 3 1 0 3 80RegImplication 5 6 0 1 0 0 0 0 0 0 1 10Method 6 2 54 2 2 32 0 6 1 0 8 13Goal 2 0 5 12 6 9 2 2 4 0 0 5RegGoal 0 1 0 0 0 0 0 0 0 0 0 0Hypothesis 24 31 0 5 34 1 0 5 0 0 0 12RegHypothesis 6 4 0 0 2 0 0 1 0 0 0 2Problem 7 6 0 0 0 0 2 0 11 0 0 2RegProblem 0 3 0 0 0 0 0 0 0 0 0 0Result 13 6 1 1 2 0 0 2 8 0 112 75RegResult 1 0 0 0 0 0 0 0 0 1 1 2Intertextual 4 0 7 0 1 0 0 0 0 0 0 3Intratextual 2 0 1 0 0 0 0 2 0 0 8 4
Sheet1
Page 1
0 Ana Ana Ana Ana Ana Fact Fact Met Oth Inv Inv Inv ObsObsObsObsL1 L2 L2,OL3 L3,OL3 L3,O L3 L3 L2 L3 L3,OL1 L2 L3 L3,OHypothesis 8 18 26 1 0 0 0 0 1 39 0 4 1 0 0 14 0Implication 22 2 30 0 34 2 2 0 0 38 2 1 0 0 0 27 0OtherHypothesis 0 0 3 1 0 0 0 0 0 9 0 1 0 0 0 0 0OtherImplication 8 1 6 1 4 28 0 3 3 27 0 2 0 1 0 5 46RegImplication 11 0 2 0 0 1 0 0 0 5 0 1 0 0 0 3 0RegHypothesis 1 0 0 0 0 0 0 0 0 6 0 1 0 0 0 7 0Fact 15 0 18 0 6 0 28 0 0 55 0 1 0 0 1 44 25RegFact 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0OtherGoal 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0OtherProblem 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0Method 80 0 1 0 0 0 0 0 23 9 0 2 0 0 0 8 3OtherMethod 7 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0Goal 13 0 0 0 0 0 1 0 0 18 0 11 1 0 0 3 0RegGoal 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Problem 9 4 0 0 2 0 0 0 0 5 0 8 0 0 0 0 0RegProblem 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Result 51 0 14 0 20 0 0 0 6 18 0 0 0 0 1 103 7OtherResult 11 0 1 0 0 1 0 1 0 10 0 0 0 0 0 12 47OtherFact 4 0 1 0 0 2 5 3 0 7 0 0 0 0 0 2 54RegResult 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Intertextual 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0Intratextual 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0280 4 35 0 28 3 34 4 29 127 0 24 2 0 2 178 136
43
7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al (2005) in order to facilitate information extraction, and more recently Teufel et al (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012).  Modality and negation in text have also been the focus of recent workshops (Farkas et al(2010), Morante & Sporleder (2012)). Finally, Shatkay et al(2008) define a multi-dimensional scheme, which combines several of the above-mentioned aspects.      Recent work has compared schemes to discover mappings and relative merits. Liakata et al (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al (2008), a cut-down version of the                                                 5 http://www.nactem.ac.uk/medie/ 
scheme proposed by Teufel et al (2009) and CoreSC (1st layer), from general to specific.	 ?8  Conclusion We have compared three different schemes, each taking a different perspective to the annotation of scientific discourse. The comparison shows that the three schemes are complementary, with different strengths and points of focus. CoreSC offers a fine-grained characterisation of methods, outcomes and objectives. It has been used to annotate a collection of 265 full papers, and subsequently CoreSC recognition has been fully automated, creating the online SAPIENTA tool. The discourse segment annotation scheme can help to provide a finer-grained characterisation of background work, and could also help to split multi-clause CoreSC sentences into appropriate segments. Recognition of event meta-knowledge has been fully automated in the U-Compare framework, and the KT values of the scheme can help to provide a finer-grained analysis of certain segment and sentence types. The CL dimension also allows confidence values to be ascribed to the Conclusion, Result, Implication and Hypothesis categories of the other two schemes.   Future work will focus on annotating texts with several discourse perspectives to investigate the advantages of the schemes. Ideally we would like to propose a unified approach for scientific discourse annotation, but recognize that choices such as the unit of annotation are often task-oriented, and that users should be able to mix and match discourse segments as required. This said, the analysis in this paper paves the way for potential harmonisation, revealing points of union and intersection between the schemes. Acknowledgements This work has been supported through funding for Maria  Liakata by JISC, the Leverhulme Trust and EBI-EMBL. It has also been supported by the BBSRC through grant number BB/G013160/1UK (Automated Biological Event Extraction from the Literature for Drug Discovery), the MetaNet4U project (ICT PSP Programme, Grant Agreement: No. 270893) and the JISC-funded ISHER project.  
44
References  Ananiadou, S., Kell, D.B. and Tsujii, J. (2006). Text mining and its potential applications in systems biology. Trends Biotechnol, 24(12): 571-9. Ananiadou, S. and McNaught, J., Eds. (2006). Text Mining for Biology and Biomedicine. Boston / London, Artech House. Ananiadou, S., Pyysalo, S., Tsujii, J. and Kell, D.B. (2010). Event extraction for systems biology by text mining the literature. Trends Biotechnol, 28(7): 381-90. Bj?rne, J., Heimonen, J., Ginter, F., Airola, A., Pahikkala, T. and Salakoski, T. (2009). Extracting Complex Biological Events with Rich Graph-Based Feature Sets. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pp.  10-18. Blake, C. (2010). Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles. Journal of Biomedical Informatics, 43(2): 173-189. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20: 37-46. Cohen, K.B. and Hunter, L. (2008). Getting started in text mining. PLoS Comput Biol, 4(1): e20. Cohen, K.B., Johnson, H.L., Verspoor, K., Roeder, C. and Hunter, L.E. (2010). The structural and content aspects of abstracts versus bodies of full text journal articles are different. BMC Bioinformatics, 11: 492. de Waard, A., Buitelaar, P., Eigner, T. (2009). Identifying the epistemic value of discourse segments in biology texts. Proceedings of the Eighth International Conference on Computational Semantics, pp. 351-354 de Waard, A. and Pander Maat, H. (2009). Categorizing Epistemic Segment Types in Biology Research Articles. In Proceedings of the Workshop on Linguistic and Psycholinguistic Approaches to Text Structuring (LPTS 2009) de Waard, A. and Pander Maat, H. (2012). Knowledge Attribution in Scientific Discourse: A Taxonomy of Types and Overview of Features, In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse (DSDD), ACL 2012. Farkas, R.	 ?Vincze, V., M?ra, G., Csirik, J. and Szarvas, G. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden. 
Association for Computational Linguistics, pp. 1- 12. Guo, Y., Korhonen, A., Liakata, M., Silins, I., LiSun, L. and Stenius, U. (2010). Identifying the information structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP 2010, pp.  99-107. Hirohata, K., Okazaki, N., Ananiadou, S. and Ishizuka, M. (2008). Identifying Sections in Scientific Abstracts using Conditional Random Fields. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pp.  381-388. Hyland, K. (1996). Writing without conviction? Hedging in science research articles. Applied Linguistics, 17(4): 433-454. Kano, Y., Miwa, M., Cohen, K.B., Hunter, L.E., Ananiadou, S. and Tsujii, J. (2011). U-Compare: A modular NLP workflow construction and evaluation system. IBM Journal of Research and Development, 55(3): 11:1-11:10. Kilicoglu, H. and Bergler, S. (2008). Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9(Suppl 11): S10. Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9(10). Kim, J.D., Ohta, T., Pyysalo, S., Kano, Y. and Tsujii, J. (2011). Extracting Bio-Molecular Events from Literature - The BioNLP'09 Shared Task. Computational Intelligence, 27(4): 513-540. Liakata, M., Saha, S., Dobnik, S., Batchelor, C. and Rebholz-Schuhmann, D. (2012). Automatic recognition of conceptualisation zones in scientific articles and two life science applications. Bioinformatics, 28 (7). Liakata, M. and Soldatova, L.N. (2009). The ART corpus. Technical Report. Aberystwth University. Liakata, M., Teufel, S., Siddharthan, A. and Batchelor, C. (2010). Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC, pp.  2054-2061. Light, M., Qiu, X.Y. and Srinivasan, P. (2004). The language of bioscience: Facts, speculations, and statements in between. In Proceedings of the BioLink 2004 Workshop at HLT/NAACL, pp.  17?24. McKnight, L. and Srinivasan, P. (2003). Categorization of sentence types in medical abstracts. In AMIA Annu Symp Proc, pp.  440-4. Miwa, M., Saetre, R., Kim, J.D. and Tsujii, J. (2010). Event extraction with complex event 
45
classification using rich features. J Bioinform Comput Biol, 8(1): 131-46. Miwa, M., Thompson, P. and Ananiadou, S. (2012). Boosting automatic event extraction from the literature using domain adaptation and coreference resolution. Bioinformatics.  Miwa, M., Thompson, P., McNaught, J, Kell, D.B and Ananiadou, S. (In Press). Extracting semantically enriched events from biomedical literature. BMC Bioinformatics.  Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J. (2006). Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases. In Proceedings of ACL, pp.  1017-1024. Mizuta, Y., Korhonen, A., Mullen, T. and Collier, N. (2005). Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics,75(6): 468-487. Morante R., and Sporleder C, (2012). Modality and negation: An introduction to the special issue. Computational Linguistics, 38(2): 1?38. Nawaz, R., Thompson, P. and Ananiadou, S. (In Press). Identification of Manner in Bio-Events. Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 2012). Nawaz, R., Thompson, P., McNaught, J. and Ananiadou, S. (2010). Meta-Knowledge Annotation of Bio-Events. In Proceedings of LREC 2010, pp.  2498-2507. Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., Boberg, J., Jarvinen, J. and Salakoski, T. (2007). BioInfer: a corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8: 50. Pyysalo, S., Ohta, T., Rak, R., Sullivan, D., Mao, C., Wang, C., Sobral, B., Tsujii, J. and Ananiadou, S. (In Press). Overview of the ID, EPI and REL tasks of BioNLP Shared Task 2011. BMC Bioinformatics. Quirk, C., Choudhury, P., Gamon, M. and Vanderwende, L. (2011). MSR-NLP Entry in BioNLP Shared Task 2011. In Proceedings of BioNLP Shared Task 2011 Workshop, pp.  155-163. Ruch, P., Boyer, C., Chichester, C., Tbahriti, I., Geissbuhler, A., Fabry, P., Gobeill, J., Pillet, V., Rebholz-Schuhmann, D., Lovis, C. and Veuthey, A.L. (2007). Using argumentation to extract key sentences from biomedical abstracts. Int J Med Inform, 76(2-3): 195-200. S?ndor, ?. (2007). Modeling metadiscourse conveying the author?s rhetorical strategy in biomedical 
research abstracts. Revue Fran?aise de Linguistique Appliqu?e, 200(2): 97-109. Shatkay, H., Pan, F., Rzhetsky, A. and Wilbur, W.J. (2008). Multi-dimensional classification of biomedical text: toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 24(18): 2086-2093. Soldatova, L.N. and King, R.D. (2006). An ontology of scientific experiments. Journal of the Royal Society Interface, 3(11): 795-803. Soldatova, L.N. and Liakata, M. (2007). An ontology methodology and cisp-the proposed core information about scientific papers., Aberystwyth University. Technical Report JISC Project Report. Teufel, S. (2010). The Structure of Scientific Articles: Applications to Citation Indexing and Summarization. Stanford, CA, CSLI Publications. Teufel, S., Carletta, J. and Moens, M. (1999). An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL, pp.  110-117. Teufel, S. and Moens, M. (2002). Summarizing scientific articles: experiments with relevance and rhetorical status. Computational Linguistics, 28(4): 409-445. Teufel, S., Siddharthan, A. and Batchelor, C. (2009). Towards discipline-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proceedings of EMNLP 2009, pp.  1493-1502. Thompson, P., Iqbal, S.A., McNaught, J. and Ananiadou, S. (2009). Construction of an annotated corpus to support biomedical information extraction. BMC Bioinformatics, 10: 349. Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S. (2011). Enriching a biomedical event corpus with meta-knowledge annotation. BMC Bioinformatics, 12: 393. Vincze, V., Szarvas, G., Farkas, R., Mora, G. and Csirik, J. (2008). The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11): S9. Zweigenbaum, P., Demner-Fushman, D., Yu, H. and Cohen, K.B. (2007). Frontiers of biomedical text mining: current progress. Briefings in Bioinformatics, 8(5): 358-375.  
46
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 58?66,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of the Cancer Genetics (CG) task of BioNLP Shared Task 2013
Sampo Pyysalo Tomoko Ohta Sophia Ananiadou
National Centre for Text Mining and School of Computer Science, University of Manchester
sampo.pyysalo@gmail.com, tomoko.ohta@manchester.ac.uk,
sophia.ananiadou@manchester.ac.uk
Abstract
We present the design, preparation, results
and analysis of the Cancer Genetics (CG)
event extraction task, a main task of the
BioNLP Shared Task (ST) 2013. The CG
task is an information extraction task tar-
geting the recognition of events in text,
represented as structured n-ary associa-
tions of given physical entities. In addition
to addressing the cancer domain, the CG
task is differentiated from previous event
extraction tasks in the BioNLP ST series
in addressing a wide range of pathological
processes and multiple levels of biological
organization, ranging from the molecular
through the cellular and organ levels up to
whole organisms. Final test set submis-
sions were accepted from six teams. The
highest-performing system achieved an F-
score of 55.4%. This level of performance
is broadly comparable with the state of
the art for established molecular-level ex-
traction tasks, demonstrating that event ex-
traction resources and methods generalize
well to higher levels of biological orga-
nization and are applicable to the analy-
sis of scientific texts on cancer. The CG
task continues as an open challenge to
all interested parties, with tools and re-
sources available from http://2013.
bionlp-st.org/.
1 Introduction
Despite decades of focused research efforts, can-
cer remains one of the leading causes of death
worldwide. It is now well understood that cancer
is a broad class of diseases with a complex genetic
basis, involving changes in multiple molecular
pathways (Hanahan and Weinberg, 2000; Haber
et al, 2011). The scientific literature on cancer is
enormous, and our understanding of cancer is de-
veloping rapidly: a query of the PubMed literature
database for cancer returns 2.7 million scien-
tific article citations, with 140,000 citations from
2012. To build and maintain comprehensive, up-
to-date knowledge bases on cancer genetics, auto-
matic support for managing the literature is thus
required.
The BioNLP Shared Task (ST) series has been
instrumental in encouraging the development of
methods and resources for the automatic extrac-
tion of bio-processes from text, but efforts within
this framework have been almost exclusively fo-
cused on normal physiological processes and on
molecular-level entities and events (Kim et al,
2011a; Kim et al, 2011b). To be relevant to can-
cer biology, event extraction technology must be
generalized to be able to address also pathologi-
cal processes as well as physical entities and pro-
cesses at higher levels of biological organization,
including e.g. mutation, cell proliferation, apop-
tosis, blood vessel development, and metastasis.
The CG task aims to advance the development of
such event extraction methods and the capacity for
automatic analysis of texts on cancer biology.
The CG task introduces a novel corpus cover-
ing multiple subdomains of cancer biology, based
in part on a previously introduced angiogenesis
subdomain resource (Pyysalo et al, 2012a). To
extend event extraction to upper levels of biolog-
ical organization and pathological processes, the
task defines a set of 18 entity and 40 event types
based on domain ontologies such as the Com-
mon Anatomy Reference Ontology and Gene On-
tology, more than doubling the number of entity
and event types from those considered in previous
BioNLP ST extraction tasks.
This paper presents the design of the CG task,
introduces the groups and systems taking part in
the task, and presents evaluation results and anal-
ysis.
58
Gene or gene product Gene expression Positive regulation CarcinogenesisTheme ThemeCause
treatment with L-NAME inhibited growth of adenocarcinoma
Planned process Simple chemical Negative regulation Growth CancerThemeInstrument Theme
Cause
Figure 1: Examples of CG task entities and event structures. Visualizations generated using the BRAT
tool (Stenetorp et al, 2012).
2 Task definition
The CG task goal is the automatic extraction of
events (Ananiadou et al, 2010) from text. The
applied representation and task setting extend on
those first established in the BioNLP ST 2009
(Kim et al, 2011a). Each event has a type such as
GROWTH or METASTASIS and is associated with
a specific span of characters expressing the event,
termed the event trigger. Events can take any num-
ber of arguments, each of which is identified as
participating in the event in a specific role (e.g.
Theme or Cause). Event arguments may be either
(physical) entities or other events, allowing com-
plex event structures that capture e.g. one event
causing or preventing another. Finally, events may
be marked by flags identifying extra-propositional
aspects such as occurrence in a speculative or neg-
ative context. Examples of CG task extraction tar-
gets are shown in Figure 1.
The following sections present the categories
of annotation and the specific annotated types in-
volved in the CG task: entities, relations, events,
and event modifications. To focus efforts on novel
challenges, the CG task follows the general con-
vention of the BioNLP ST series of only requiring
participants to extract events and their modifica-
tions. For other categories of annotation, correct
(gold standard) annotations are provided also for
test data.
2.1 Entities
The entity types defined in the CG task are shown
in Table 1. The molecular level entity types largely
match the scope of types such as PROTEIN and
CHEMICAL included in previous ST tasks (Kim et
al., 2012; Pyysalo et al, 2012b). However, the CG
types are more fine grained, and the types PRO-
TEIN DOMAIN OR REGION and DNA DOMAIN OR
REGION are used in favor of the non-specific type
ENTITY, applied in a number of previous tasks
for additional event arguments (see Section 2.3).
The definitions of the anatomical entity types are
Type
ORGANISM
Anatomical entity
ORGANISM SUBDIVISION
ANATOMICAL SYSTEM
ORGAN
MULTI-TISSUE STRUCTURE
TISSUE
DEVELOPING ANATOMICAL STRUCTURE
CELL
CELLULAR COMPONENT
ORGANISM SUBSTANCE
IMMATERIAL ANATOMICAL ENTITY
PATHOLOGICAL FORMATION
CANCER
Molecular entity
GENE OR GENE PRODUCT
PROTEIN DOMAIN OR REGION
DNA DOMAIN OR REGION
SIMPLE CHEMICAL
AMINO ACID
Table 1: Entity types. Indentation corresponds to
is-a structure. Labels in gray identify groupings
defined for organization only, not annotated types.
progression of chronic myeloid leukemia (CML)
Development Cancer CancerEquivTheme
Figure 2: Example Equiv relation.
drawn primarily from the Common Anatomy Ref-
erence Ontology (Haendel et al, 2008), a small,
species-independent upper-level ontology based
on the Foundational Model of Anatomy (Rosse
and Mejino Jr, 2003). We refer to Ohta et al
(2012) for more detailed discussion of the anatom-
ical entity type definitions.
2.2 Relations
The CG task does not target the extraction of
any standalone relations. However, following the
model of past BioNLP ST tasks, the CG corpus is
annotated by Equiv (equivalence) relations, sym-
metric, transitive relations that identify two entity
mentions as referring to the same entity (Figure 2).
These relations primarily mark local aliases and
are applied only in evaluation. When determining
whether a predicted event matches a gold event,
59
Type Core arguments Additional arguments
Anatomical
DEVELOPMENT Theme (Anatomy)
BLOOD VESSEL DEVELOPMENT Theme?(Anatomy) AtLoc?
GROWTH Theme (Anatomy)
DEATH Theme (Anatomy)
CELL DEATH Theme?(CELL)
BREAKDOWN Theme (Anatomy)
CELL PROLIFERATION Theme (CELL)
CELL DIVISION Theme (CELL)
CELL DIFFERENTIATION Theme (CELL) AtLoc?
REMODELING Theme (TISSUE)
REPRODUCTION Theme (ORGANISM)
Pathological
MUTATION Theme (GGP) AtLoc?, Site?
CARCINOGENESIS Theme?(Anatomy) AtLoc?
CELL TRANSFORMATION Theme (CELL) AtLoc?
METASTASIS Theme?(Anatomy) ToLoc
INFECTION Theme?(Anatomy), Participant?(ORGANISM)
Molecular
METABOLISM Theme (Molecule)
SYNTHESIS Theme (SIMPLE CHEMICAL)
CATABOLISM Theme (Molecule)
AMINO ACID CATABOLISM Theme?(Molecule)
GLYCOLYSIS Theme?(Molecule)
GENE EXPRESSION Theme+(GGP)
TRANSCRIPTION Theme (GGP)
TRANSLATION Theme (GGP)
PROTEIN PROCESSING Theme (GGP)
PHOSPHORYLATION Theme (Molecule) Site?
(other chemical modifications defined similarly to PHOSPHORYLATION)
PATHWAY Participant (Molecule)
General
BINDING Theme+(Molecule) Site?
DISSOCIATION Theme (Molecule) Site?
LOCALIZATION Theme+(Molecule) AtLoc?, FromLoc?, ToLoc?
REGULATION Theme (Any), Cause?(Any)
POSITIVE REGULATION Theme (Any), Cause?(Any)
NEGATIVE REGULATION Theme (Any), Cause?(Any)
PLANNED PROCESS Theme*(Any), Instrument*(Entity)
Table 2: Event types and their arguments. Nesting corresponds to ontological structure (is-a/part-of ).
The affixes ?, *, and + denote zero or one, zero or more, and one or more, respectively. GGP abbreviates
for GENE OR GENE PRODUCT. For brevity, additional argument types are not shown in table: Loc
arguments take an anatomical entity type, and Site PROTEIN/DNA DOMAIN OR REGION.
differences in references to equivalent entities are
ignored, so that e.g. an event referring to CML
as its Theme instead of chronic myeloid leukemia
would be considered to match the event shown in
Figure 2.
2.3 Events
Table 2 summarizes the event types defined in the
CG task. As in most previous BioNLP ST task
settings, the event types are defined primarily with
reference to the Gene Ontology (GO) (Ashburner
et al, 2000). However, GO explicitly excludes
from its scope pathological processes, which are
critically important to the CG task. To capture
pathological processes, we systematically expand
the scope GO-based event types to include also
analogous processes involving pathological enti-
ties. For example, statements such as ?cancer
growth? are annotated with GROWTH events by
analogy to processes such as ?organ growth?. Sec-
ond, we introduce a number of event types ex-
plicitly accounting for pathological processes with
no analogous normal physiological process, such
as METASTASIS. Finally, many important effects
are discussed in the literature through statements
involving experimenter action such as transfect
and treat (Figure 1). To capture such state-
ments, we introduce the general PLANNED PRO-
CESS type, defined with reference to the Ontol-
ogy for Biomedical Investigations (Brinkman et
al., 2010).
The event argument roles largely match those
60
Domain Documents Query terms
Carcinogenesis 150 cell transformation, neoplastic AND (proteins OR genes)
Metastasis 100 neoplasm metastasis AND (proteins OR genes)
Apoptosis 50 apoptosis AND (proteins OR genes)
Glucose metabolism 50 (glucose/metabolism OR glycolysis) AND neoplasms
Table 3: Queries for document selection. All query terms were restricted to MeSH Term matches only
(e.g. "apoptosis"[MeSH Terms])
established in previous BioNLP ST tasks (Kim et
al., 2012; Pyysalo et al, 2012b): Theme identifies
the arguments undergoing the primary effects of
the event, Cause those that are responsible for its
occurrence, and Participant those whose precise
role is not stated. Site is used to identify specific
parts of Theme entities affected (e.g. phosphory-
lated residues) and the Loc roles entities where the
event takes place (AtLoc) and start and end points
of movement (FromLoc and ToLoc).
2.4 Event modifications
The CG task follows many previous BioNLP ST
tasks in including the event modification types
NEGATION and SPECULATION in its extraction
targets. These modifications apply to events,
marking them as explicitly negated and specula-
tively stated, respectively (Kim et al, 2011a).
2.5 Evaluation
The CG task evaluation follows the criteria orig-
inally defined in the BioNLP ST?09, requiring
events extracted by systems to otherwise match
gold standard events exactly, but allowing trigger
spans to differ from gold spans by single words
(approximate span matching) and not requiring
matching of additional arguments (see Table 2) for
events referred from other events (approximate re-
cursive matching). These criteria are discussed in
detail by Kim et al (2011a).
3 Corpus
3.1 Document selection
The corpus texts are the titles and abstracts of pub-
lications from the PubMed literature database, se-
lected on the basis of relevance to cancer genet-
ics, specifically with respect to major subdomains
relating to established hallmarks of cancer (Hana-
han and Weinberg, 2000). Of the 600 documents
forming the CG task corpus, 250 were previously
released as part of the MLEE corpus (Pyysalo
et al, 2012a) involving the angiogenesis subdo-
main. The remaining 350 were selected by iter-
Item Train Devel Test Total
Documents 300 100 200 600
Words 66 082 21 732 42 064 129 878
Entities 11 034 3 665 6 984 21 683
Relations 466 176 275 917
Events 8 803 2 915 5 530 17 248
Modifications 670 214 442 1 326
Table 4: Corpus statistics
atively formulating PubMed queries consisting of
MeSH terms relevant to subdomains such as apop-
tosis and metastasis (Table 3). Following initial
query formulation, random sets of abstracts were
selected from each domain and manually exam-
ined to select a final set of documents that specifi-
cally discuss both the target process and its molec-
ular foundations.
3.2 Annotation process
The corpus annotation was created using the BRAT
annotation tool (Stenetorp et al, 2012) by a single
PhD biologist with extensive experience in event
annotation (Tomoko Ohta). For the entity anno-
tation, we created preliminary annotation using
the following automatic named entity and entity
mention taggers: BANNER (Leaman and Gonza-
lez, 2008) trained on the GENETAG corpus (Tan-
abe et al, 2005) for GENE OR GENE PRODUCT
entities, Oscar4 (Jessop et al, 2011) for SIMPLE
CHEMICAL and AMINO ACID entities, NERsuite1
trained on the AnEM corpus (Ohta et al, 2012)
for anatomical entities, and LINNAEUS (Gerner
et al, 2010) for ORGANISM mentions. Process-
ing was performed on a custom pipeline originally
developed for the BioNLP ST?11 (Stenetorp et al,
2011). Following preliminary automatic annota-
tion, all entity annotations were manually revised
to create the final entity annotation.
By contrast to entity annotation, no automatic
preprocessing was applied for event annotation to
avoid any possibility of bias introduced by ini-
tial application of automatic methods. The event
annotation extended the guidelines and manual
1http://nersuite.nlplab.org
61
Team Institution Members
TEES-2.1 University of Turku 1 BI (Bjo?rne and Salakoski, 2013)
NaCTeM National Centre for Text Mining 1 NLP (Miwa and Ananiadou, 2013)
NCBI National Center for Biotechnology Information 3 BI (Liu et al, 2013)
RelAgent RelAgent Private Ltd. 1 LI, 1 CS (Ramanan and Nathan, 2013)
UET-NII
University of Engineering and Technology, Vietnam
6 CS (Tran et al, 2013)
and National Institute of Informatics, Japan
ISI Indian Statistical Institute 2 ML, 2 NLP -
Table 5: Participating teams and references to system descriptions. Abbreviations: BI=Bioinformatician,
NLP=Natural Language Processing researcher, CS=Computer Scientist, LI=Linguist, ML=Machine
Learning researcher.
NLP methods Events Resources
Team Lexical Syntactic Trigger Arg Group Modif. Corpora Other
TEES-2.1 Porter McCCJ + SD SVM SVM SVM SVM GE hedge words
NaCTeM Snowball Enju, GDep SVM SVM SVM SVM - triggers
NCBI MedPost, BLem McCCJ + SD Joint, subgraph matching - GE, EPI -
RelAgent Brill fnTBL, custom rules rules rules rules - -
UET-NII Porter Enju SVM MaxEnt Earley - - triggers
ISI CoreNLP CoreNLP NERsuite Joint, MaltParser - - -
Table 6: Summary of system architectures. Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Porter
stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson
parser, Charniak=Charniak parser, SD=Stanford Dependency conversion
annotation process introduced by Pyysalo et al
(2012a). Following the initial annotation, a num-
ber of revision passes were made to further im-
prove the consistency of the annotation using a va-
riety of automatically supported methods.2
3.3 Corpus statistics
Table 4 summarizes the corpus statistics for the
training, development and test sets, representing
50%, 17%, and 33% of the documents, respec-
tively. The CG task corpus is the largest of the
BioNLP ST 2013 corpora by most measures, in-
cluding the number of annotated events.
4 Participation
Final results to the CG task were successfully sub-
mitted by six teams, from six different academic
groups and one company, representing a broad
range of expertise ranging from biology to ma-
chine learning, natural language processing, and
linguistics (Table 5).
The characteristics of the participating systems
are summarized in Table 6. There is an interesting
spread of extraction approaches, with two systems
applying SVM-based pipeline architectures shown
2There was no opportunity to train a second annotator in
order to evaluate IAA specifically for the new CG corpus an-
notation. However, based on our previous evaluation using
the same protocol (Pyysalo et al, 2012a), we expect the con-
sistency of the final annotation to fall in the 70-80% F-score
range (primary task evaluation criteria).
successful in previous BioNLP ST events, one
applying a joint pattern matching approach, one
a rule-based approach, and two systems parsing-
based approaches to event extraction. Together,
these systems represent all broad classes of ap-
proaches applied to event extraction in previous
BioNLP ST events. Three of the six systems ad-
dressed also the event modification (negation and
speculation) extraction aspects of the task.
Although all systems perform syntactic analy-
sis of input texts, there is a fair amount of vari-
ety in the applied parsers, which include the parser
of Charniak and Johnson (2005) with the biomed-
ical domain model of McClosky (2009) and the
Stanford Dependency conversion (de Marneffe
et al, 2006) ? the choice in many systems in
BioNLP ST?11 ? as well as Enju (Miyao and Tsu-
jii, 2008), GDep (Sagae and Tsujii, 2007), Stan-
ford CoreNLP3, and a custom parser by RelAgent
(Ramanan and Nathan, 2013). Simple stemming
algorithms such as that of Porter (1980) remain
popular for word-level processing, with just the
NCBI system using a dedicated biomedical do-
main lemmatizer (Liu et al, 2012).
The task setting explicitly allows the use of any
external resources, including other corpora, and
previously released event resources contain sig-
nificant numbers of annotations that are relevant
3http://nlp.stanford.edu/software/
corenlp.shtml
62
Team recall prec. F-score
TEES-2.1 48.76 64.17 55.41
NaCTeM 48.83 55.82 52.09
NCBI 38.28 58.84 46.38
RelAgent 41.73 49.58 45.32
UET-NII 19.66 62.73 29.94
ISI 16.44 47.83 24.47
Table 7: Primary evaluation results
to the molecular level events annotated in the CG
task. Nevertheless, only the TEES and NCBI
teams made use of corpora other than the task
data, both using the GE corpus (Kim et al, 2012)
and NCBI using also the EPI corpus (Pyysalo et
al., 2012b). In addition to corpora annotated for
events, lexical resources derived from such cor-
pora, containing trigger and hedge expressions,
were applied by three teams.
We refer to the descriptions presented by each
of the participating teams (see Table 5) for further
detail on the systems and their implementations.
5 Results
The primary evaluation results are summarized in
Table 7. The highest performance is achieved by
the established machine learning-based TEES sys-
tem, with an F-score of 55%. Previous versions
of the same system achieved the highest perfor-
mance in the BioNLP ST?09 (52% F-score) and
in four out of eight tasks in BioNLP ST?11 (53%
F-score for the comparable GE task) (Bjo?rne and
Salakoski, 2011). The performance of the system
ranked second, EventMine (Miwa et al, 2012),
is likewise broadly comparable to the results for
the same system on the GE task considered in
BioNLP ST?09 and ?11. The NCBI submis-
sion also extends a system that participated in the
ST?11 GE task, then achieving a somewhat lower
F-score of 41.13% (Liu et al, 2011). By con-
trast, the RelAgent, UET-NII and ISI submissions
involve systems that were not previously applied
in BioNLP ST events. Thus, in each case where
system performance for previously proposed event
extraction tasks is known, the results indicate that
the systems generalize to CG task extraction tar-
gets without loss in performance.
These parallels with results for previously intro-
duced tasks involving molecular-level events are
interesting, in particular considering that the CG
task involves more than twice the number of en-
tity and event types included in previously con-
sidered BioNLP ST tasks. The results suggest
not only that event extraction methods generalize
well to higher levels of biological organization,
but also that overall performance is not primar-
ily limited by the number of targeted types. It is
also notable that the complexity of the task set-
ting does not exclude rule-based systems such as
that of RelAgent, which scores within 10% points
of the highest-ranking system. While the parser-
based systems of UET-NII and ISI perform be-
low others here, it should be noted that related ap-
proaches have achieved competitive performance
in previous BioNLP ST tasks (McClosky et al,
2011), suggesting that further development could
lead to improvements for systems based on these
architectures. As is characteristic for event extrac-
tion systems in general, all systems show notably
higher precision than recall, with the performance
of the UET-NII and ISI systems in particular pri-
marily limited by low recall.
The F-score results are shown separately for
each event type in Table 8. As suggested by the
overall results, the novel categories of events in-
volving anatomical and pathological entities are
not particularly challenging for most systems,
with results roughly mirroring performance for
molecular level events; the best results by event
category are 77% F-score for anatomical, 68%
for pathological, and 73% for molecular. Of
the newly introduced CG event categories, only
planned processes involving intentional human in-
tervention appear to represent difficulties, with the
best-performing system for PLANNED PROCESS
reaching only 41% F-score. Two previously es-
tablished categories of events remain challenging:
general events ? best 53% F-score ? including
BINDING (often taking multiple arguments) and
LOCALIZATION (frequent additional arguments),
and regulation category events, which often form
complex event structures by involving events as ar-
guments. Event modifications, addressed by three
of the six participating teams, show comparatively
low levels of extraction performance, with a best
result of 40% F-score for NEGATION and 30%
for SPECULATION. However, as in previous tasks
(Kim et al, 2011a), this is in part due to the com-
pound nature of the problem: for an event modifi-
cation attribute to be extracted correctly, the event
that it attaches to must also be correct.
Further details on system performance and anal-
yses are available on the shared task home page.
63
Event TEES-2.1 NaCTeM NCBI RelAgent UET-NII ISI
DEVELOPMENT 71.43 64.77 67.33 66.31 61.72 53.66
BLOOD VESSEL DEVELOPM 85.28 78.82 81.92 79.60 21.49 13.56
GROWTH 75.97 59.85 66.67 76.92 70.87 65.52
DEATH 81.74 73.17 74.07 64.71 77.78 63.16
CELL DEATH 73.30 75.18 78.05 66.98 25.17 7.35
CELL PROLIFERATION 80.00 78.33 72.73 64.39 71.43 57.40
CELL DIVISION 0.00 0.00 0.00 0.00 0.00 0.00
CELL DIFFERENTIATION 56.34 48.48 48.98 54.55 59.26 24.14
REMODELING 30.00 22.22 21.05 40.00 20.00 23.53
REPRODUCTION 100.00 100.00 100.00 100.00 100.00 100.00
Anatomical total 77.20 71.31 73.68 70.82 50.04 38.86
MUTATION 38.00 41.05 25.11 27.36 27.91 9.52
CARCINOGENESIS 77.94 72.18 67.14 64.12 35.96 24.72
CELL TRANSFORMATION 81.56 82.54 71.13 67.07 57.14 32.39
BREAKDOWN 76.74 70.13 76.54 42.42 58.67 50.70
METASTASIS 70.91 51.05 52.69 47.79 56.41 26.20
INFECTION 69.57 76.92 69.23 33.33 11.76 0.00
Pathological total 67.51 59.78 54.19 48.14 46.90 25.17
METABOLISM 83.87 70.27 74.29 80.00 68.75 71.43
SYNTHESIS 78.26 71.11 78.26 53.57 64.71 48.65
CATABOLISM 63.64 36.36 38.10 23.08 20.00 36.36
GLYCOLYSIS 0.00 100.00 95.45 97.78 0.00 0.00
AMINO ACID CATABOLISM 0.00 66.67 66.67 66.67 0.00 0.00
GENE EXPRESSION 78.21 79.96 73.69 69.45 58.01 53.28
TRANSCRIPTION 37.33 42.86 51.55 28.12 32.00 20.93
TRANSLATION 40.00 22.22 0.00 0.00 0.00 0.00
PROTEIN PROCESSING 100.00 100.00 100.00 0.00 100.00 100.00
ACETYLATION 100.00 100.00 66.67 100.00 66.67 66.67
GLYCOSYLATION 100.00 100.00 100.00 100.00 100.00 100.00
PHOSPHORYLATION 63.33 70.37 53.12 64.15 58.33 50.00
UBIQUITINATION 100.00 100.00 0.00 100.00 0.00 100.00
DEPHOSPHORYLATION 0.00 80.00 100.00 100.00 0.00 0.00
DNA METHYLATION 66.67 66.67 30.30 42.11 32.43 33.33
DNA DEMETHYLATION 0.00 0.00 0.00 0.00 0.00 0.00
PATHWAY 71.30 59.07 51.14 34.29 18.31 35.64
Molecular total 72.60 72.77 67.33 60.72 49.35 46.70
BINDING 45.35 43.93 37.89 32.69 33.94 11.92
DISSOCIATION 0.00 0.00 0.00 0.00 0.00 0.00
LOCALIZATION 54.83 57.20 47.58 45.22 44.94 35.94
General total 52.20 53.08 44.70 40.89 41.76 29.59
REGULATION 32.66 28.73 14.19 26.48 5.51 4.57
POSITIVE REGULATION 45.89 44.18 34.70 38.40 13.00 12.33
NEGATIVE REGULATION 47.79 43.17 33.20 40.47 10.30 12.16
Regulation total 43.08 39.79 29.21 35.58 10.30 10.29
PLANNED PROCESS 39.43 40.51 34.28 28.57 22.74 21.22
Sub-total 56.75 53.50 48.56 46.37 31.72 25.90
NEGATION 40.00 29.55 0.00 34.64 0.00 0.00
SPECULATION 27.14 30.35 0.00 25.90 0.00 0.00
Modification total 34.66 29.95 0.00 30.88 0.00 0.00
Total 55.41 52.09 46.38 45.32 29.94 24.47
Table 8: Primary evaluation F-scores by event type
6 Discussion and conclusions
We have presented the Cancer Genetics (CG) task,
an information extraction task introduced as a
main task of the BioNLP Shared Task (ST) 2013.
The task is motivated by the needs of maintain-
ing up-to-date knowledge bases of the enormous
and fast-growing literature on cancer genetics, and
extends previously proposed BioNLP ST tasks in
several aspects, including the inclusion of enti-
ties and events at levels of biological organiza-
tion above the molecular and the explicit inclusion
of pathological and planned processes among ex-
traction targets. To address these extraction goals,
we introduced a new corpus covering various sub-
domains of cancer genetics, annotated for 18 en-
tity and 40 event types and marking over 17,000
manually annotated events in 600 publication ab-
stracts.
Final submissions to the CG task were received
from six groups, who applied a variety of ap-
proaches including machine learning-based clas-
64
sifier pipelines, parsing-based approaches, and
pattern- and rule-based systems. The best-
performing system achieved an F-score of 55.4%,
a level of performance comparable to the state of
the art in established molecular level event extrac-
tion tasks. The results indicate that event extrac-
tion methods generalize well across the novel as-
pects introduced in the CG task and that event ex-
traction is applicable to the automatic processing
of the cancer literature.
Following convention in the BioNLP Shared
Task series, the Cancer Genetics task will con-
tinue as an open challenge available to all inter-
ested participants. The CG task corpus, supporting
resources and evaluation tools are available from
http://2013.bionlp-st.org/.
Acknowledgments
We wish to thank the BioNLP ST 2013 CG task
participants and supporting resource providers for
their invaluable contributions to making this task a
success. This work was supported by the Biotech-
nology and Biological Sciences Research Council
(BBSRC) [BB/G53025X/1].
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii,
and Douglas B. Kell. 2010. Event extraction
for systems biology by text mining the literature.
Trends in Biotechnology, 28(7):381?390.
Michael Ashburner, Catherine A Ball, Judith A Blake,
David Botstein, et al 2000. Gene ontology: tool
for the unification of biology. Nature genetics,
25(1):25?29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
BioNLP?11, pages 183?191.
Jari Bjo?rne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bioNLP
2013 shared task. In Proceedings of BioNLP Shared
Task 2013.
Ryan R Brinkman, Me?lanie Courtot, Dirk Derom, Jen-
nifer M Fostel, et al 2010. Modeling biomedical
experimental processes with OBI. J Biomed Seman-
tics, 1(Suppl 1):S7.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL?05, pages 173?
180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Martin Gerner, Goran Nenadic, and Casey M Bergman.
2010. Linnaeus: a species name identification sys-
tem for biomedical literature. BMC bioinformatics,
11(1):85.
Daniel A Haber, Nathanael S Gray, and Jose Baselga.
2011. The evolving war on cancer. Cell, 145(1):19?
24.
Melissa A Haendel, Fabian Neuhaus, David Osumi-
Sutherland, Paula M Mabee, Jos LV Mejino Jr,
Chris J Mungall, and Barry Smith. 2008. CARO?
the common anatomy reference ontology. pages
327?349.
Douglas Hanahan and Robert A Weinberg. 2000. The
hallmarks of cancer. Cell, 100(1):57?70.
David M Jessop, Sam E Adams, Egon L Willigha-
gen, Lezan Hawizy, and Peter Murray-Rust. 2011.
Oscar4: a flexible architecture for chemical text-
mining. Journal of Cheminformatics, 3(1):1?12.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011a. Ex-
tracting bio-molecular events from literature - the
BioNLP?09 shared task. Computational Intelli-
gence, 27(4):513?540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011b. Overview
of BioNLP Shared Task 2011. In Proceedings of
BioNLP?11.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC bioinformatics,
13(Suppl 11):S1.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: an executable survey of advances in biomedi-
cal named entity recognition. Proceedings of the Pa-
cific Symposium on Biocomputing (PSB?08), pages
652?663.
Haibin Liu, Ravikumar Komandur, and Karin Ver-
spoor. 2011. From graphs to events: A subgraph
matching approach for information extraction from
biomedical text. In Proceedings of BioNLP?11,
pages 164?172.
Haibin Liu, Tom Christiansen, William A Baumgart-
ner Jr, Karin Verspoor, et al 2012. Biolemmatizer:
a lemmatization tool for morphological processing
of biomedical text. Journal of biomedical seman-
tics, 3(3).
Haibin Liu, Karin Verspoor, Donald Comeau, Andrew
MacKinlay, and W John Wilbur. 2013. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of BioNLP Shared Task
2013 Workshop.
65
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event extraction as depen-
dency parsing for bionlp 2011. In Proceedings
BioNLP?11, pages 41?45.
David McClosky. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.
Makoto Miwa and Sophia Ananiadou. 2013. NaCTeM
EventMine for bioNLP 2013 CG and PC tasks. In
Proceedings of BioNLP Shared Task 2013 Work-
shop.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34(1):35?80.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and
Sophia Ananiadou. 2012. Open-domain anatomical
entity mention detection. In Proceedings of DSSD
2012, pages 27?36.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130?137.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-
Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou.
2012a. Event extraction across multiple levels of bi-
ological organization. Bioinformatics, 28(18):i575?
i581.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2012b.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC bioinformatics, 13(Suppl
11):S2.
SV Ramanan and P. Senthil Nathan. 2013. Perfor-
mance and limitations of the linguistically motivated
cocoa/peaberry system in a broad biological domain.
In Proceedings of BioNLP Shared Task 2013 Work-
shop.
Cornelius Rosse and Jose? LV Mejino Jr. 2003. A refer-
ence ontology for biomedical informatics: the foun-
dational model of anatomy. Journal of biomedical
informatics, 36(6):478?500.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP?11.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of EACL 2012,
pages 102?107.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne
Matten, and John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recog-
nition. BMC Bioinformatics, 6(Suppl 1):S3.
Mai-Vu Tran, Nigel Collier, Hoang-Quynh Le, Van-
Thuy Phi, and Thanh-Binh Pham. 2013. Adapting
a probabilistic earley parser for event decomposi-
tion in biomedical texts. In Proceedings of BioNLP
Shared Task 2013 Workshop.
66
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 67?75,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of the Pathway Curation (PC) task of BioNLP Shared Task 2013
Tomoko Ohta 1, Sampo Pyysalo 1, Rafal Rak 1, Andrew Rowley1, Hong-Woo Chun2,
Sung-Jae Jung 2,3, Chang-Hoo Jeong 2 Sung-Pil Choi 2,3, Jun?ichi Tsujii 4,Sophia Ananiadou 1
1National Centre for Text Mining and School of Computer Science, University of Manchester
2Software Research Center, Korea Institute of Science and Technology Information (KISTI)
3Department of Applied Information Science, University of Science and Technology (UST)
4Microsoft Research Asia, Beijing, China
Abstract
We present the Pathway Curation (PC)
task, a main event extraction task of
the BioNLP shared task (ST) 2013.
The PC task concerns the automatic ex-
traction of biomolecular reactions from
text. The task setting, representation
and semantics are defined with respect
to pathway model standards and ontolo-
gies (SBML, BioPAX, SBO) and docu-
ments selected by relevance to specific
model reactions. Two BioNLP ST 2013
participants successfully completed the
PC task. The highest achieved F-
score, 52.8%, indicates that event extrac-
tion is a promising approach to support-
ing pathway curation efforts. The PC
task continues as an open challenge with
data, resources and tools available from
http://2013.bionlp-st.org/
1 Introduction
Following developments in molecular biology, bi-
ological phenomena are increasingly understood
on the molecular level, as the products of complex
systems of molecular reactions. Pathway mod-
els formalizing biomolecules and their reactions
in machine readable representations are a key way
of sharing and communicating human understand-
ing of these phenomena and of developing com-
putational models of biological systems (Kitano,
2002). Many pathway models integrate knowl-
edge from hundreds or thousands of scientific pub-
lications, and their curation requires substantial
manual effort. To support this effort, we have de-
veloped PathText (Kemper et al, 2010) which pro-
vides a seamless environment integrating a path-
way visualizer, text mining systems and annota-
tion tools. Furthermore, automatic processing of
the domain literature could thus potentially play
pyruvate kinase catalyzes the conversion of PEP to pyruvate.
GGP +Regulation Conversion Chem ChemicalThemeCause Theme Product
Figure 1: Event representation for a conversion re-
action.
an important role in the support of pathway cura-
tion.
Information extraction targeting biomolecular
reactions has been a major focus of efforts in
biomedical natural language processing, with sev-
eral tasks, resources, and tools addressing in par-
ticular protein-protein interactions (Krallinger et
al., 2007; Pyysalo et al, 2008; Tikk et al, 2010).
However, most such efforts have employed sim-
ple representations, such as entity pairs, that are
not sufficient for capturing molecular reactions to
the level of detail required to support the curation
of pathway models. Additionally, previous efforts
have not directly involved the semantics (e.g. re-
action type definitions) of such models. Perhaps
in part due to these reasons, natural language pro-
cessing and information extraction methods have
not been widely embraced by biomedical pathway
curation communities (Ohta et al, 2011c; Ohta et
al., 2011a).
We believe that the extraction of structured
event representations (Figure 1) pursued in the
BioNLP Shared Tasks offers many opportuni-
ties to make significant contributions to support
the development, evaluation and maintenance of
biomolecular pathways. The Pathway Curation
(PC) task, a main task of the BioNLP Shared Task
2013, is proposed as a step toward realizing these
opportunities. The PC task aims to evaluate the ap-
plicability of event extraction systems to pathway
curation and to encourage the further development
of methods for related tasks. The design of the
task aims to address current issues in information
extraction for pathway curation by explicitly bas-
ing its representation and extraction targets on ma-
67
GTP GDP
GAPs
re1
re1 Protein Molecule MoleculeReactantModifier ProductConversion GAPs catalyze the hydrolysis of GTP to GDP.GGP +Reg Conversion Chem Chem
Cause ThemeTheme Product
(a) CONVERSION
p38 gamma Pp38 gamma
MKK6
re1
re1
MKK6 phosphorylates p38 gamma.Protein Protein
Protein
Modifier Reactant
Product
Phosphorylation MKK6 phosphorylates p38 gamma.GGP Phosphorylation GGP
Cause Theme
(b) PHOSPHORYLATION
NF-kappaB
p65
p50
p65
p50re1
p65 binds to p50.
GGP Bind GGPTheme Theme2
p65-p50 complex formation.
Complex BindingProduct
p65 and p50 form p65-p50 complex.
Protein Protein NC binding ComplexReactant2 Product
Reactant
(c) BINDING
Figure 2: Illustration of pathway reaction (left), matching representation as an idealized text-bound event
structure (middle) and applied event representation for statements actually appearing in text (right).
jor standards developed in the biomolecular path-
way curation community, such as SBML (Hucka
et al, 2003) and BioPAX (Mi et al, 2011), and
ontologies such as the Systems Biology Ontology1
(SBO) (Courtot et al, 2011). Further, The corpus
texts are selected on the basis of relevance to a se-
lection of pathway models from PANTHER Path-
way DB2 (Mi and Thomas, 2009) and BioMod-
els3 (Li et al, 2010) repositories. The PC task set-
ting and its document selection protocol aim to ac-
count for both signalling and metabolic pathways,
the latter of which has received comparatively lit-
tle attention in recent domain IE efforts (Li et al,
2013).
2 Task setting
The PC task is formulated as an event extraction
task (Ananiadou et al, 2010) following the general
representation and task setting first introduced in
the BioNLP ST 2009 (Kim et al, 2011). The pri-
mary aim is the extraction of event structures, or
events, each of which can involve any number of
physical entities or other events in specific roles.
The event representation is sufficiently expres-
sive to allow the definition of event structures that
closely parallel the definition of reactions in path-
way representations such as SBML and BioPAX.
These pathway representations differentiate be-
tween three primary groups of reaction partici-
pants: reactants (?inputs?), products (?outputs?),
and modifiers, where the specific roles of modi-
fiers can be further identified to differentiate e.g.
1http://www.ebi.ac.uk/sbo/main/
2http://www.pantherdb.org/pathway/
3http://www.ebi.ac.uk/biomodels-main/
reaction catalysts from inhibitors. Correspond-
ingly, the PC task applies the Theme role defined
in previous BioNLP ST tasks to capture reactants,
introduces a new Product role for products, and
applies the previously defined Cause role and reg-
ulatory events to capture modifiers (Figure 2; see
also Section 2.3).
It is important to note that while the event repre-
sentation allows a one-to-one mapping to reactions
in principle, an annotation scheme cannot guar-
antee that actual statements in text map to fully
specified reactions: in free-form text, authors fre-
quently omit mention of some entities taking part
in reactions, perhaps most typically to avoid re-
dundancies such as in ?p38? is phosphorylated
into phospho-p38?? (Figure 2b). Representations
extracted from explicit statements in text will thus
in some cases omit aspects of the corresponding
complete reactions in pathway models.
Systems addressing the PC task are expected to
extract events of specific types given 1) free-form
text and 2) gold standard annotation for mentions
of physical entities in that text. The task annota-
tions also include equivalence relations and event
modifications, a secondary extraction target. The
annotation types are detailed below.
2.1 Entities
The entity annotation marks mentions of physical
entities using start and end offsets in text (contigu-
ous span) and a type selected from a fixed set. The
following four entity types are marked in the PC
task: SIMPLE CHEMICAL, annotated with refer-
ence to the Chemical Entities of Biological Inter-
est (ChEBI) resource (Degtyarenko et al, 2008);
68
Entity type Scope Reference Ontology ID
SIMPLE CHEMICAL simple, non-repetitive chemical entities ChEBI SBO:0000247
GENE OR GENE PRODUCT genes, RNA and proteins gene/protein DBs SBO:0000246
COMPLEX entities of non-covalently linked components complex DBs SBO:0000253
CELLULAR COMPONENT parts of cell and extracellular environment GO-CC SBO:0000290
Table 1: Entity types, definitions, and reference resources.
Event type Core arguments Additional arguments Ontology ID
CONVERSION Theme:Molecule, Product:Molecule SBO:0000182
PHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000216
DEPHOSPHORYLATION Theme:Molecule, Cause:Molecule Site:SIMPLE CHEMICAL SBO:0000330
(Other modifications, such as ACETYLATION, defined similarly.)
LOCALIZATION Theme:Molecule At/From/ToLoc:CELL. COMP. GO:0051179
TRANSPORT Theme:Molecule From/ToLoc:CELL. COMP. SBO:0000185
GENE EXPRESSION Theme:GENE OR GENE PRODUCT GO:0010467
TRANSCRIPTION Theme:GENE OR GENE PRODUCT SBO:0000183
TRANSLATION Theme:GENE OR GENE PRODUCT SBO:0000184
DEGRADATION Theme:Molecule SBO:0000179
BINDING Theme:Molecule, Product:COMPLEX SBO:0000177
DISSOCIATION Theme:COMPLEX, Product:Molecule SBO:0000180
REGULATION Theme:ANY, Cause:ANY GO:0065007
POSITIVE REGULATION Theme:ANY, Cause:ANY
GO:0048518,
GO:0044093
ACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
NEGATIVE REGULATION Theme:ANY, Cause:ANY
GO:0048519,
GO:0044092
INACTIVATION Theme:Molecule, Cause:ANY SBO:0000412
PATHWAY Participant:Molecule SBO:0000375
Table 2: Event types and arguments. ?Molecule? refers to an entity annotation of any of the types
SIMPLE CHEMICAL, GENE OR GENE PRODUCT, or COMPLEX, and ?ANY? refers to an annotation of
any type, either entity or event. The indentation corresponds to ontological relationships between the
event types: for example, PHOSPHORYLATION is-a CONVERSION and TRANSCRIPTION part-of
GENE EXPRESSION.
GENE OR GENE PRODUCT, annotated with refer-
ence to gene and protein databases such as UniProt
(Consortium, 2011), Entrez Gene (Maglott et al,
2005) and PFam (Finn et al, 2010); COMPLEX,
annotated with reference to database resources
covering complexes; and CELLULAR COMPO-
NENT, annotated following the scope of the Gene
Ontology cellular component subontology
(Ashburner et al, 2000) (Table 1). For discussion
of the relation between these types and the repre-
sentations applied in pathway models, we refer to
Ohta et al (2011c).
In terms of mention types in text, the annotation
for SIMPLE CHEMICAL, GENE OR GENE PROD-
UCT and COMPLEX covers entity name mentions
only, while the annotation for CELLULAR COM-
PONENT covers entity name mentions, nominal
mentions, and adjectival references (e.g. ?mito-
chondrial?).
2.2 Relations
The PC task defines one relation type, Equiv
(equivalence), which can hold between entity
mitogen-activated protein kinase (MAPK, also known as ERK)
Gene or gene product GGP GGPEquivEquiv
Figure 3: Example Equiv annotation.
mentions of the same type and specifies that they
refer to the same real-world entity (Figure 3).
These relations are only applied to determine if
two events match during evaluation, where entities
connected by an Equiv relation are considered in-
terchangeable. Gold standard Equiv relations are
applied also for test data, and systems participat-
ing in the task are not expected to extract these
relations.
2.3 Events
The event annotation marks references to reac-
tions, processes and comparable associations in
scope of the annotation using the event represen-
tation. For the definition and scope of the event
annotation, we rely primarily on the Systems Biol-
ogy Ontology (SBO), drawing some general types
not in scope of this ontology from the Gene Ontol-
ogy (GO). Table 2 presents the event types anno-
69
Pathway Repository ID Publication
mTOR BioModels MODEL1012220002 (Caron et al, 2010)
mTORC1 upstream regulators BioModels MODEL1012220003 (Caron et al, 2010)
TLR BioModels MODEL2463683119 (Oda and Kitano, 2006)
Yeast Cell Cycle BioModels MODEL1011020000 (Kaizu et al, 2010)
Rb BioModels MODEL4132046015 (Calzone et al, 2008)
EGFR BioModels MODEL2463576061 (Oda et al, 2005)
Human Metabolic Network BioModels MODEL6399676120 (Duarte et al, 2007)
NF-kappaB pathway - - (Oda et al, 2008)
p38 MAPK PANTHER DB P05918 -
p53 PANTHER DB P00059 -
p53 feedback loop pathway PANTHER DB P04392 -
Wnt signaling pathway PANTHER DB P00057 -
Table 3: Pathway models used to select documents for the task, with pathway repository model identifiers
and publications presenting each model (when applicable).
tated in the PC task and their arguments. We refer
again to Ohta et al (2011c) for detailed discussion
of the relation between these types and other rep-
resentations applied in pathway models.
The role in which each event argument (entity
or other event) participates in an event is specified
as one of the following:
Theme entity/event that undergoes the effects of
the event. For example, the entity that is tran-
scribed in a TRANSCRIPTION event or transported
in a TRANSPORT event.
Cause entity/event that is causally active in the
event. Marks, for example, ?P1? in ?P1 inhibits P2
expression?.
AtLoc,FromLoc,ToLoc : location in which the
Theme entity of a LOCALIZATION event is local-
ized (At) in LOCALIZATION events not involving
movement or is transported (or moves) from/to
(From/To) in LOCALIZATION and TRANSPORT
events involving movement.
Site site on the Theme entity that is modified in
the event. Can be specified for modification events
such as PHOSPHORYLATION.
Participant general role type identifying an en-
tity that participates in some underspecified way in
a high-level process. Only applied for the PATH-
WAY type.
2.4 Event modifications
In addition to events, the PC task defines a sec-
ondary extraction target, event modifications. Two
modification types are defined: NEGATION and
SPECULATION. Both are binary flags that mod-
ify events, the former marking an event as be-
ing explicitly stated as not occurring (e.g. ?P is
not phosphorylated?) and the latter as being stated
in a speculative context (?P may be phosphory-
lated.?). Both are defined in terms of annotation
scope and semantics identically as in the BioNLP
ST?09 (Kim et al, 2009).
2.5 Evaluation
The PC task evaluation applies the standard evalu-
ation criteria established in the BioNLP ST 2009.
These criteria relax exact matching between gold
and predicted events in two aspects: approximate
trigger boundary matching, and approximate re-
cursive event matching. The former allows pre-
dicted event triggers to differ from gold triggers
by one word, and the latter requires recursively re-
ferred events to only match in their core arguments
(see Table 2). We refer to Kim et al (2011) for a
detailed definition of these criteria.
3 Corpus
This section presents the PC task corpus and its
annotation process.
3.1 Document selection
To assure that the documents annotated for the PC
task corpus are relevant to pathway reactions, we
applied two complementary approaches, both se-
lecting documents on the basis of relevance to a
specific pathway reaction. First, we selected from
the BioModels repository those pathway models
with the largest numbers of manually created an-
notations referencing a specific PubMed document
identifier. For each of these models, we extracted
literature references, selected a random subset,
downloaded the documents, and manually filtered
to select abstracts that explicitly discuss relevant
molecular reactions. Second, as only a small sub-
set of models include explicit references to the
70
literature providing evidence for specific pathway
reactions, we applied an alternative strategy where
reactions from a selection of PANTHER DB mod-
els were entered into the PathText system (Kem-
per et al, 2010),4 which is capable of suggest-
ing documents relevant to given reactions based
on an SBML model. We then selected a random
set of reactions to query the system, and manually
evaluated the highest-ranking documents to iden-
tify those whose abstracts explicitly discuss the se-
lected reaction. We refer to Miwa et al (2013a)
for a detailed description of this approach. Table 3
presents the pathway models on which the docu-
ment selection was based.
3.2 Annotation process
The base entity annotation for the PC corpus was
created automatically using state-of-the-art entity
mention taggers for each of the targeted entity
types. For SIMPLE CHEMICAL tagging, the OS-
CAR4 system (Jessop et al, 2011) trained on
the chemical named entity recognition corpus of
Corbett and Copestake (2008) was applied. For
GENE OR GENE PRODUCT mention detection, the
NERsuite5 system trained on the BioCreative 2
Gene Mention task (Wilbur et al, 2007) corpus
was used. NERsuite was also applied for CEL-
LULAR COMPONENT mention detection, for this
task trained on the Anatomical Entity Mention
(AnEM) corpus (Ohta et al, 2012). Finally, COM-
PLEX annotations were created using a combi-
nation of a dictionary and heuristics making use
of the GENE OR GENE PRODUCT annotation (for
mentions such as ?cyclin E/CDK2 complex?). To
support the curation process, these tools were in-
tegrated into the NaCTeM text-analysis workflow
system Argo (Rak et al, 2012).
Based on the evaluations of each of these tools
in the studies presenting them, we expected initial
automatic tagging performance to be in the range
80-90% in both precision and recall. Following
initial automatic annotation, the entity mention an-
notation was manually revised to improve quality
and consistency. As the entity annotation is not
itself a target of extraction in the shared task, we
did not separately evaluate the consistency of the
revised entity mention annotation.
To assure that the quality and consistency of
the event annotation are as high as possible, ini-
4http://nactem.ac.uk/pathtext/
5http://nersuite.nlplab.org/
Item Train Devel Test Total
Documents 260 90 175 525
Words 53811 18579 35966 108356
Entities 7855 2734 5312 15901
Events 5992 2129 4004 12125
Modifications 317 80 174 571
Table 4: PC corpus statistics
tial event annotation was created entirely man-
ually, without automatic support. This annota-
tion effort was carried out using the BRAT anno-
tation tool (Stenetorp et al, 2012) by a group of
biologists in collaboration between NaCTeM and
KISTI. Following initial annotator training and re-
finement of guidelines based on the event type def-
initions provided by the reference ontologies, the
primary event annotation was created by three bi-
ologists. To evaluate and maintain annotation con-
sistency, a random 20% of documents were an-
notated redundantly by all annotators, and these
overlapping annotations were periodically evalu-
ated and differences in annotation were discussed
between the annotators and annotation coordina-
tors. Following initial annotation, a round of semi-
automatic consistency checks were applied using
BRAT. Evaluation of the redundantly annotated
documents using the primary task evaluation cri-
teria gave an inter-annotator agreement of 61.0%
in F-score. For the final corpus, the redundantly
annotated documents were evaluated separately by
an annotation coordinator to select the best of each
set.6
The overall statistics of the corpus are summa-
rized in Table 4. We note that the among the
previous BioNLP ST corpora, only the GENIA
(GE) task corpus has a larger number of annotated
events than the PC corpus.
4 Results
4.1 Participation
Two groups submitted final results to the PC
task, one from the National Centre for Text Min-
ing (NaCTeM) and one from the University of
Turku BioNLP group (TEES-2.1) (Table 5). Both
participants applied their well-established, state-
of-the-art event extraction systems, EventMine7
(Miwa et al, 2012) (NaCTeM) and the Turku
6This selection implies that the consistency of the event
annotation of the final corpus is expected to exceed the 61%
F-score of the IAA experiment. Consistency after selection
was not separately evaluated.
7http://nactem.ac.uk/EventMine/
71
NLP Events Other resources
Rank Team Org Word Parse Trig. Arg. Group. Modif. Corpora Other
1 NaCTeM 1NLP Snowball Enju, GDep SVM SVM SVM SVM (see text) triggers
2 TEES-2.1 1BI Porter McCCJ + SD SVM SVM SVM SVM GE hedge words
Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician,
NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Char-
niak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.
Team recall prec. F-score
NaCTeM 52.23 53.48 52.84
TEES-2.1 47.15 55.78 51.10
Table 6: Primary evaluation results
Event Extraction System8 (Bjo?rne et al, 2011)
(TEES). The two systems share the same over-
all architecture, a one-best pipeline with SVM-
based stages for event trigger detection, trigger-
argument relation detection, argument grouping
into event structures, and modification prediction.
The feature representations of both systems draw
on substructures of dependency-like representa-
tions of sentence syntax, derived from full parses
of input sentences. TEES applies the Charniak
and Johnson (2005) parser with the McClosky
(2009) biomedical model, converting the phrase-
structure parses into dependencies using the Stan-
ford tools (de Marneffe et al, 2006). By contrast,
EventMine uses a combination of the predicate-
argument structure analyses created by the deep
parser Enju (Miyao and Tsujii, 2008) and the out-
put of the the GDep best-first shift-reduce depen-
dency parser (Sagae and Tsujii, 2007). All three
parsers have models trained in part on the biomed-
ical domain GENIA treebank (Tateisi et al, 2005).
Interestingly, both systems make use of the GE
task data, but the application of EventMine ex-
tends on this considerably by applying a stacked
model (Miwa et al, 2013b) with predictions also
from models trained on the BioNLP ST 2011 EPI
and ID tasks (Pyysalo et al, 2012) as well as from
four corpora introduced outside of the shared tasks
by Thompson et al (2011), Pyysalo et al (2011),
Ohta et al (2011b) and Ohta et al (2011c).
4.2 Evaluation results
Table 6 summarizes the primary evaluation results.
The two systems demonstrate broadly similar per-
formance in terms of F-scores, with NaCTeM
achieving an 1.7% point higher overall result.
8http://jbjorne.github.io/TEES/
However, the systems show quite different per-
formance in terms of the precision/recall balance:
while the NaCTeM system has little difference
between precision and recall, TEES-2.1 shows a
clear preference for precision, with 8.6% lower re-
call than precision.
Results are shown separately for each event type
in Table 7. The results largely mirror the over-
all performance, with the NaCTeM system show-
ing better performance for 13 out of the 21 event
types present in the test data and more balanced
precision and recall than TEES-2.1, which em-
phasizes precision over recall for almost all event
types. Although the results do not include evalu-
ation of EventMine with a reduced set of stacked
models in training, the modest difference in per-
formance suggests that comprehensive use of pre-
viously released event resources in EventMine did
not confer a decisive advantage, perhaps in part
due to differences in the event definitions between
the PC task and previous resources.
Overall, the two systems appear quite similar
not only in architecture but also performance, with
the clearest systematic difference observed being
the different emphases on precision vs. recall. As
both systems are based on machine learning meth-
ods with real-valued outputs, it would be relatively
straightforward to use prediction confidences to
analyse performance over the entire precision-
recall curve instead of a single fixed point. Such
analysis could provide further insight into the rel-
ative strengths and weaknesses of these two sys-
tems.
5 Discussion
Although participation in this initial run of the PC
task was somewhat limited, the two participating
systems have been applied to a large variety of
event extraction tasks over the last years and have
shown consistently competitive performance with
the state of the art (Bjo?rne and Salakoski, 2011;
Miwa et al, 2012). It is thus reasonable to as-
sume that the higher performance achieved by the
72
NaCTeM TEES-2.1
Event recall prec. F-score recall prec. F-score
CONVERSION 34.33 35.48 34.90 35.82 42.86 39.02
PHOSPHORYLATION 62.46 55.94 59.02 53.40 66.00 59.03
DEPHOSPHORYLATION 45.00 56.25 50.00 35.00 77.78 48.28
ACETYLATION 69.57 72.73 71.11 82.61 76.00 79.17
DEACETYLATION 33.33 33.33 33.33 0.00 0.00 0.00
METHYLATION 42.86 60.00 50.00 57.14 80.00 66.67
DEMETHYLATION 100.00 100.00 100.00 100.00 100.00 100.00
UBIQUITINATION 52.94 64.29 58.06 58.82 76.92 66.67
DEUBIQUITINATION 100.00 100.00 100.00 100.00 100.00 100.00
LOCALIZATION 42.25 61.22 50.00 43.66 54.39 48.44
TRANSPORT 65.52 61.29 63.33 56.55 59.85 58.16
GENE EXPRESSION 90.65 83.15 86.74 84.55 79.39 81.89
TRANSCRIPTION 71.15 82.22 76.29 57.69 73.17 64.52
TRANSLATION 0.00 0.00 0.00 50.00 100.00 66.67
Simple-total 66.42 64.80 65.60 60.40 67.87 63.92
DEGRADATION 78.57 89.19 83.54 78.57 78.57 78.57
ACTIVATION 78.54 70.96 74.56 72.06 72.06 72.06
INACTIVATION 44.62 55.77 49.57 38.46 45.45 41.67
BINDING 64.96 47.30 54.74 53.96 53.96 53.96
DISSOCIATION 38.46 46.88 42.25 35.90 45.16 40.00
PATHWAY 84.91 75.50 79.93 70.94 75.50 73.15
General-total 69.07 62.69 65.72 61.16 65.74 63.37
REGULATION 33.33 33.97 33.65 29.73 39.51 33.93
POSITIVE REGULATION 35.49 42.81 38.81 34.51 45.45 39.23
NEGATIVE REGULATION 45.75 50.64 48.07 41.02 47.37 43.97
Regulation-total 37.73 42.79 40.10 35.17 44.76 39.39
Sub-total 53.47 53.96 53.72 48.23 56.22 51.92
NEGATION 24.52 35.87 29.13 25.16 41.30 31.27
SPECULATION 15.79 22.22 18.46 0.00 0.00 0.00
Modification-total 23.56 34.65 28.05 22.41 40.00 28.73
Total 52.23 53.48 52.84 47.15 55.78 51.10
Table 7: Primary evaluation results by event type.
task participants, a balanced F-score of 52.8%, is
a good estimate of the performance level that can
be attained for this task by present event extraction
technology.
The results achieved by the two systems are
broadly comparable to the best results achieved by
any system in similar previously introduced event
extraction tasks (Kim et al, 2012; Pyysalo et al,
2012). Given the novelty of the task domain and
reference resource and the broad selection of doc-
uments, we find the results highly encouraging re-
garding the applicability of event extraction tech-
nology to supporting the development, evaluation,
and maintenance of pathway models.
6 Conclusions
This paper presented the Pathway Curation (PC)
task, a main event extraction task of the BioNLP
ST 2013. The task was organized in collaboration
between groups with an interest in pathway cura-
tion with the aim of evaluating and advancing the
state of the art in event extraction toward methods
for developing, evaluating and maintaining formal
pathway models in representations such as SBML
and BioPAX. We introduced an event extraction
task setting with reference to pathway model stan-
dards and the Systems Biology Ontology, selected
a set of 525 publication abstracts relevant to spe-
cific model reactions, and created fully manual
73
event annotation marking over 12,000 event struc-
tures in the corpus.
Two participants in the BioNLP ST 2013 sub-
mitted final predictions to the PC task, applying
established, state-of-the-art event extraction sys-
tems, EventMine and the Turku Event Extrac-
tion System. Both systems achieved F-scores
over 50%, with the EventMine system achiev-
ing the best overall result of 52.8%. This level
of performance is broadly comparable with re-
sults achieved in comparable previously proposed
tasks, indicating that current event extraction tech-
nology is applicable to the projected pathway cu-
ration support tasks.
To allow the further development and evalua-
tion of event extraction methods for the task, the
PC task continues as an open challenge to all inter-
ested participants, with the annotated corpus data,
supporting resources, and evaluation tools avail-
able under open licenses from the task homepage,
http://2013.bionlp-st.org/
Acknowledgments
We would like to thank Yonghwa Jo, Hyeyeon
Choi, Jeong-Ik Lee and Ssang-Goo Cho of
Konkuk University for their contribution to the de-
velopment of the relevance judgment annotation
criteria. We also wish to thank Hyun Uk Kim,
Jinki Kim and Kyusang Hwang of KAIST for
their efforts in producing the PC task annotation.
This work is a part of joint research of KISTI and
NaCTeM, and partially supported by the Biotech-
nology and Biological Sciences Research Council
(BBSRC) [BB/G53025X/1].
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and Dou-
glas B. Kell. 2010. Event extraction for systems biology
by text mining the literature. Trends in Biotechnology,
28(7):381?390.
Michael Ashburner, Catherine A. Ball, Judith A. Blake,
David Botstein, Heather Butler, J. Michael Cherry, Al-
lan P. Davis, Kara Dolinski, et al 2000. Gene ontology:
tool for the unification of biology. Nature genetics, 25:25?
29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 183?191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio
Pahikkala, and Tapio Salakoski. 2011. Extracting contex-
tualized complex biological events with rich graph-based
feature sets. Computational Intelligence, 27(4):541?557.
Laurence Calzone, Ame?lie Gelay, Andrei Zinovyev, Franc?ois
Radvanyl, and Emmanuel Barillot. 2008. A comprehen-
sive modular map of molecular interactions in rb/e2f path-
way. Molecular systems biology, 4(1).
Etienne Caron, Samik Ghosh, Yukiko Matsuoka, Dariel
Ashton-Beaucage, Marc Therrien, Se?bastien Lemieux,
Claude Perreault, Philippe P Roux, and Hiroaki Kitano.
2010. A comprehensive map of the mtor signaling net-
work. Molecular systems biology, 6(1).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
n-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL?05, pages 173?180.
The UniProt Consortium. 2011. Ongoing and future devel-
opments at the universal protein resource. Nucleic Acids
Research, 39(suppl 1):D214?D219.
Peter Corbett and Ann Copestake. 2008. Cascaded classifiers
for confidence-based chemical named entity recognition.
BMC Bioinformatics, 9(Suppl 11):S4.
Me?lanie Courtot, Nick Juty, Christian Knu?pfer, Dagmar Wal-
temath, Anna Zhukova, Andreas Dra?ger, Michel Dumon-
tier, Andrew Finney, Martin Golebiewski, Janna Hastings,
et al 2011. Controlled vocabularies and semantics in sys-
tems biology. Molecular systems biology, 7(1).
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of
LREC, volume 6, pages 449?454.
Kirill Degtyarenko, Paula De Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, Alan Mcnaught, Rafael
Alca?ntara, Michael Darsow, Mickae?l Guedj, and Michael
Ashburner. 2008. Chebi: a database and ontology for
chemical entities of biological interest. Nucleic acids re-
search, 36(suppl 1):D344?D350.
Natalie C Duarte, Scott A Becker, Neema Jamshidi, Ines
Thiele, Monica L Mo, Thuy D Vo, Rohith Srivas, and
Bernhard ? Palsson. 2007. Global reconstruction of
the human metabolic network based on genomic and bib-
liomic data. Proceedings of the National Academy of Sci-
ences, 104(6):1777?1782.
Robert D. Finn, Jaina Mistry, John Tate, Penny Coggill, An-
dreas Heger, Joanne E. Pollington, O. Luke Gavin, Prasad
Gunasekaran, et al 2010. The Pfam protein families
database. Nucleic Acids Research, 38(suppl 1):D211?
D222.
Michael Hucka, Andrew Finney, Herbert M Sauro, Hamid
Bolouri, John C Doyle, Hiroaki Kitano, Adam P Arkin,
Benjamin J Bornstein, et al 2003. The systems biology
markup language (SBML): a medium for representation
and exchange of biochemical network models. Bioinfor-
matics, 19(4):524?531.
David M. Jessop, Sam Adams, Egon L. Willighagen, Lezan
Hawizy, and Peter Murray-Rust. 2011. Oscar4: a flexible
architecture for chemical text-mining. Journal of chemin-
formatics, 3(1):1?12.
Kazunari Kaizu, Samik Ghosh, Yukiko Matsuoka, Hisao
Moriya, Yuki Shimizu-Yoshida, and Hiroaki Kitano.
2010. A comprehensive molecular interaction map of the
budding yeast cell cycle. Molecular systems biology, 6(1).
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka, Yoshi-
masa Tsuruoka, Hiroaki Kitano, Sophia Ananiadou, and
Jun?ichi Tsujii. 2010. Pathtext: a text mining integra-
tor for biological pathway visualizations. Bioinformatics,
26(12):i374?i381.
74
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
Shared Task on Event Extraction. In Proceedings of
BioNLP?09.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Junichi Tsujii. 2011. Extracting bio-molecular
events from literature ? the bionlp?09 shared task. Com-
putational Intelligence, 27(4):513?540.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsujii,
Toshihisa Takagi, and Akinori Yonezawa. 2012. The
genia event and protein coreference tasks of the bionlp
shared task 2011. BMC bioinformatics, 13(Suppl 11):S1.
Hiroaki Kitano. 2002. Systems biology: a brief overview.
Science, 295(5560):1662?1664.
Martin Krallinger, Florian Leitner, and Alfonso Valencia.
2007. Assessment of the Second BioCreative PPI task:
Automatic Extraction of Protein-Protein Interactions. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of BioCreative II, pages 29?39.
Chen Li, Marco Donizelli, Nicolas Rodriguez, Harish
Dharuri, Lukas Endler, Vijayalakshmi Chelliah, Lu Li,
Enuo He, et al 2010. BioModels Database: An enhanced,
curated and annotated resource for published quantitative
kinetic models. BMC Systems Biology, 4:92.
Chen Li, Maria Liakata, and Dietrich Rebholz-Schuhmann.
2013. Biological network extraction from scientific litera-
ture: state of the art and challenges. Briefings in bioinfor-
matics.
Donna Maglott, Jim Ostell, Kim D. Pruitt, and Tatiana
Tatusova. 2005. Entrez gene: gene-centered information
at ncbi. Nucleic Acids Research, 33(suppl 1):D54.
David McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing. Ph.D.
thesis, Brown University.
Huaiyu Mi and Paul Thomas. 2009. PANTHER pathway: an
ontology-based pathway database coupled with data anal-
ysis tools. In Protein Networks and Pathway Analysis,
pages 123?140. Springer.
Huaiyu Mi, Anushya Muruganujan, Emek Demir, Yukiko
Matsuoka, Akira Funahashi, Hiroaki Kitano, and Paul D
Thomas. 2011. Biopax support in celldesigner. Bioinfor-
matics, 27(24):3437?3438.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the liter-
ature using domain adaptation and coreference resolution.
Bioinformatics, 28(13):1759?1765.
Makoto Miwa, Tomoko Ohta, Rafal Rak, Andrew Rowley,
Douglas B. Kell, Sampo Pyysalo, and Sophia Ananiadou.
2013a. A method for integrating and ranking the evidence
for biochemical pathways by mining reactions from text.
Bioinformatics. in press.
Makoto Miwa, Sampo Pyysalo, Tomoko Ohta, and Sophia
Ananiadou. 2013b. Wide coverage biomedical event
extraction using multiple partially overlapping corpora.
BMC bioinformatics, 14(1):175.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest mod-
els for probabilistic HPSG parsing. Computational Lin-
guistics, 34(1):35?80.
Kanae Oda and Hiroaki Kitano. 2006. A comprehensive
map of the toll-like receptor signaling network. Molecular
Systems Biology, 2(1).
Kanae Oda, Yukiko Matsuoka, Akira Funahashi, and Hiroaki
Kitano. 2005. A comprehensive pathway map of epider-
mal growth factor receptor signaling. Molecular systems
biology, 1(1).
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and Jun?ichi
Tsujii. 2008. New challenges for text mining: mapping
between text and manually curated pathways. BMC bioin-
formatics, 9(Suppl 3):S5.
Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Ju-
nichi Tsujii. 2011a. Pathway curation support as an infor-
mation extraction task. Proceedings of LBM?11.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and Jun?ichi
Tsujii. 2011b. Event extraction for dna methylation.
Journal of Biomedical Semantics, 2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c.
From pathways to biomolecular events: opportunities and
challenges. In Proceedings of BioNLP?11, pages 105?
113.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia
Ananiadou. 2012. Open-domain anatomical entity men-
tion detection. In Proceedings of DSSD?12, pages 27?36.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari Bjo?rne,
Filip Ginter, and Tapio Salakoski. 2008. Comparative
analysis of five protein-protein interaction corpora. BMC
Bioinformatics, 9(Suppl 3):S6.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun?ichi
Tsujii. 2011. Towards exhaustive event extraction for pro-
tein modifications. In Proceedings of BioNLP?11, pages
114?123.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sullivan,
Chunhong Mao, Chunxia Wang, Bruno Sobral, Jun?ichi
Tsujii, and Sophia Ananiadou. 2012. Overview of the id,
epi and rel tasks of bionlp shared task 2011. BMC bioin-
formatics, 13(Suppl 11):S2.
Rafal Rak, Andrew Rowley, William Black, and Sophia Ana-
niadou. 2012. Argo: an integrative, interactive, text
mining-based workbench supporting curation. Database:
The Journal of Biological Databases and Curation, 2012.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1044?1050.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. Brat:
a web-based tool for nlp-assisted text annotation. In Pro-
ceedings of EACL?12, pages 102?107.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Junichi
Tsujii. 2005. Syntax annotation for the genia corpus. In
Proceedings of IJCNLP, volume 5, pages 222?227.
Paul Thompson, Raheel Nawaz, John McNaught, and Sophia
Ananiadou. 2011. Enriching a biomedical event corpus
with meta-knowledge annotation. BMC Bioinformatics,
12(1):393.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg Haken-
berg, and Ulf Leser. 2010. A comprehensive benchmark
of kernel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6(7):e1000837, 07.
John Wilbur, Lawrence Smith, and Lorraine Tanabe. 2007.
BioCreative 2. Gene Mention Task. In L. Hirschman,
M. Krallinger, and A. Valencia, editors, Proceedings of
BioCreative II, pages 7?16.
75
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 94?98,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
NaCTeM EventMine for BioNLP 2013 CG and PC tasks
Makoto Miwa and Sophia Ananiadou
National Centre for Text Mining, University of Manchester, United Kingdom
School of Computer Science, University of Manchester, United Kingdom
{makoto.miwa,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper describes NaCTeM entries for
the Cancer Genetics (CG) and Pathway
Curation (PC) tasks in the BioNLP Shared
Task 2013. We have applied a state-of-
the-art event extraction system EventMine
to the tasks in two different settings: a
single-corpus setting for the CG task and
a stacking setting for the PC task. Event-
Mine was applicable to the two tasks with
simple task specific configuration, and it
produced a reasonably high performance,
positioning second in the CG task and first
in the PC task.
1 Introduction
With recent progress in biomedical natural lan-
guage processing (BioNLP), automatic extraction
of biomedical events from texts becomes practi-
cal and the extracted events have been success-
fully employed in several applications, such as
EVEX (Bjo?rne et al, 2012; Van Landeghem et
al., 2013) and PathText (Miwa et al, 2013a).
The practical applications reveal a problem in that
both event types and structures need to be cov-
ered more widely. The BioNLP Shared Task 2013
(BioNLP-ST 2013) offers several tasks addressing
the problem, and especially in the Cancer Genetics
(CG) (Pyysalo et al, 2013) and Pathway Curation
(PC) (Ohta et al, 2013) tasks, new entity/event
types and biomedical problems are focused.
Among dozens of extraction systems proposed
during and after the two previous BioNLP shared
tasks (Kim et al, 2011; Kim et al, 2012; Pyysalo
et al, 2012b), EventMine (Miwa et al, 2012)1
has been applied to several biomedical event ex-
traction corpora, and it achieved the state-of-the-
art performance in several corpora (Miwa et al,
2013b). In these tasks, an event associates with
1http://www.nactem.ac.uk/EventMine/
a trigger expression that denotes its occurrence
in text, has zero or more arguments (entities or
other events) that are identified with their roles
(e.g., Theme, Cause) and may be assigned hedge
attributes (e.g., Negation).
This paper describes how EventMine was ap-
plied to the CG and PC tasks in the BioNLP-ST
2013. We configured EventMine minimally for
the CG task and submit the results using the mod-
els trained on the training and development data
sets with no external resources. We employed a
stacking method for the PC task; the method ba-
sically trained the models on the training and de-
velopment data sets, but it also employed features
representing prediction scores of models on seven
external corpora.
We will first briefly describe EventMine and its
task specific configuration in the next section, then
show and discuss the results, and finally conclude
the paper with future work.
2 EventMine for CG and PC Tasks
This section briefly introduces EventMine and the
PC and CG tasks, and then explains its task spe-
cific configuration.
2.1 EventMine
EventMine (Miwa et al, 2012) is an SVM-based
pipeline event extraction system. For the de-
tails, we refer the readers to Miwa et al (2012;
2013b). EventMine consists of four modules: a
trigger/entity detector, an argument detector, a
multi-argument detector and a hedge detector.
The trigger/entity detector finds words that match
the head words (in their surfaces, base forms
by parsers, or stems by a stemmer) of trig-
gers/entities in the training data, and the detector
classifies each word into specific entity types (e.g.,
DNA domain or region), event types (Regulation)
or a negative type that represents the word does
not participate in any events. The argument
94
detector enumerates all possible pairs among
triggers and arguments that match the semantic
type combinations of the pairs in the training data,
and classifies each pair into specific role types
(e.g., Binding:Theme-Gene or gene product) or
a negative type. Similarly, the multi-argument
detector enumerates all possible combina-
tions of pairs that match the semantic type
structures of the events in the training data,
and classifies each combination into an event
structure type (e.g., Positive regulation:Cause-
Gene or gene product:Theme-Phosphorylation)
or a negative type. The hedge detector attaches
hedges to the detected events by classifying the
events into specific hedge types (Speculation and
Negation) or a negative type.
All the classifications are performed by one-vs-
rest support vector machines (SVMs). The detec-
tors use the types mentioned above as their clas-
sification labels. Labels with scores larger than
the separating hyper-plane of SVM and the label
with the largest value are selected as the predicted
labels; the classification problems are treated as
multi-class multi-label classification problems and
at least one label (including a negative type) needs
to be selected in the prediction.
Features for the classifications include charac-
ter n-grams, word n-grams, shortest paths among
event participants on parse trees, and word n-
grams and shortest paths between event partici-
pants and triggers/entities outside of the events on
parse trees. The last features are employed to cap-
ture the dependencies between the instances. All
gold entity names are replaced with their types,
the feature space is compressed to 220 by hash-
ing to reduce space cost, the positive instances are
weighted to reduce class imbalance problems, the
feature vectors are normalised, and the C parame-
ter for SVM is set to 1.
In the pipeline approach, there is no way to de-
tect instances if the participants are missed by the
preceding modules. EventMine thus aims high
recall in the modules by the multi-label setting
and weighting positive instances. EventMine also
avoids training on instances that cannot be de-
tected by generating the training instances based
on predictions by the preceding modules since the
training and test instances should be similar.
EventMine is flexible and applicable to several
event extraction tasks with task specific configura-
tion on entity, role and event types. This configu-
ration is described in a separate file2.
2.2 CG and PC Tasks
The CG task (Pyysalo et al, 2013) aims to extract
information on the biological processes relating to
the development and progression of cancer. The
annotation is built on the Multi-Level Event Ex-
traction (MLEE) corpus (Pyysalo et al, 2012a),
which EventMine was once applied to. The PC
task (Ohta et al, 2013), on the other hand, aims
to support the curation of bio-molecular pathway
models, and the corpus texts are selected to cover
both signalling and metabolic pathways.
Both CG and PC tasks offer more entity, role
and event types than most previous tasks like GE-
NIA (Kim et al, 2012) does, which may make the
classification problems more difficult.
2.3 Configuration for CG and PC Tasks
We train models for the CG and PC tasks in simi-
lar configuration, except for the incorporation of a
stacking method for the PC task. We first explain
the configuration applied to both tasks and then in-
troduce the stacking method for the PC task.
We employ two kinds of type generalisations
for both tasks: one for the classification labels
and features and the other for the generation of in-
stances. After the disambiguation of trigger/entity
types by the trigger/entity detector, we reduce the
number of event role labels and event structure
labels by the former type generalisations. The
generalisations are required to reduce the com-
putational costs that depend on the number of
the classification labels. Unfortunately, we can-
not evaluate the effect of the generalisations on
the performance since there are too many pos-
sible labels in the tasks. The generalisations
may alleviate the data sparseness problem but
they may also induce over-generalised features
for the problems with enough training instances.
For event roles, we generalise regulation types
(e.g., Positive regulation, Regulation) into a single
REGULATION type and post-transcriptional mod-
ification (PTM) types (e.g., Acetylation, Phos-
phorylation) into a single PTM type for trigger
types, numbered role types into a non-numbered
role type (e.g., Participant2?Participant) for role
2This file is not necessary since the BioNLP ST data for-
mat defines where these semantic types are described, but this
file is separated for the type generalisations explained later
and the specification of gold triggers/entities without repro-
ducing a1/a2 files.
95
types, and event types into a single EVENT type
and entity types into a single ENTITY type for
argument types. For event structures, we apply
the same generalisations except for the general-
isations of numbered role types since the num-
bered role types are important in differentiating
events. Unlike other types, the numbered role
types in events are not disambiguated by any other
modules. The generalisations are also applied to
the features in all the detectors when applicable.
These generalisations are the combination of the
generalisations for the GENIA, Epigenetics and
Post-translational Modifications (EPI), and Infec-
tious Diseases (ID) (Pyysalo et al, 2012b) of the
BioNLP-ST 2011 (Miwa et al, 2012).
The type generalisations on labels and fea-
tures are not directly applicable to generate pos-
sible instances in the detectors since the gen-
eralisations may introduce illegal or unrealis-
tic event structures. Instead, we employ sep-
arate type generalisations to expand the possi-
ble event role pair and event structure types and
cover types, which do not appear in the training
data. For example, if there are Regulation:Theme-
Gene expression instances but there are no Posi-
tive regulation:Theme-Gene expression instances
in the training data, we allow the creation of the
latter instances by generalising the triggers, i.e.,
REGULATION:Theme-Gene expression, and we
used all the created instances for classification.
The type generalisations may incorporate noisy in-
stances but they pose the possibility to find unan-
notated event structures. To avoid introducing un-
expected event structures, we apply the generali-
sations only to the regulation trigger types.
We basically follow the setting for EPI in
Miwa et al (2012). We employ a deep syntactic
parser Enju (Miyao and Tsujii, 2008) and a de-
pendency parser GDep (Sagae and Tsujii, 2007).
We utilise liblinear-java (Fan et al, 2008)3 with
the L2-regularised L2-loss linear SVM setting for
the SVM implementation, and Snowball4 for the
stemmer. We, however, use no external resources
(e.g., dictionaries) or tools (e.g., a coreference
resolver) except for the external corpora in the
stacked models for the PC task.
We train models for the CG task using the con-
figuration described above. For PC, in addition
to the configuration, we incorporated a stacking
3http://liblinear.bwaldvogel.de/
4http://snowball.tartarus.org/
Setting Recall Precision F-score
? 42.87 47.72 45.16
+Exp. 43.37 46.42 44.84
+Exp.+Stack. 43.59 48.77 46.04
Table 1: Effect of the type generalisations for ex-
panding possible instances (+Exp.) and stacking
method (+Stack.) on the PC development data set.
method (Wolpert, 1992) using the models with the
same configuration for seven other available cor-
pora: GENIA, EPI, ID, DNA methylation (Ohta
et al, 2011a), Exhaustive PTM (Pyysalo et al,
2011), mTOR (Ohta et al, 2011b) and CG. The
prediction scores of all the models are used as ad-
ditional features in the detectors. Although some
corpora may not directly relate to the PC task and
models trained on such corpora can produce noisy
features, we use all the corpora without selection
since the stacking often improve the performance,
e.g., (Pyysalo et al, 2012a; Miwa et al, 2013b).
3 Evaluation
We first evaluate the type generalisations for ex-
panding possible event structures and the stack-
ing method in Table 1. The scores were calcu-
lated using the evaluation script provided by the
organisers with the official evaluation metrics (soft
boundary and partial recursive matching). The
generalisations improved recall with the loss of
precision, and they slightly degraded the F-score
in total. The generalisations were applied to the
test set in the submission since this result was ex-
pected as explained in Section 2.3 and the slightly
high recall is favourable for the practical applica-
tions like semantic search engines (Miwa et al,
2013a). Although the improvement by the stack-
ing method (+Exp.+Stack. compared to +Exp.) is
not statistically significant (p=0.14) using the ap-
proximate randomisation method (Noreen, 1989;
Kim et al, 2011), this slight improvement indi-
cates that the corpus in the PC task shares some
information with the other corpora.
Tables 2 and 3 show the official scores of our
entries on the test data sets for the CG and PC
tasks5. EventMine ranked second in the CG task
and first in the PC task. The scores of the best sys-
tem among the other systems (TEES-2.1 (Bjo?rne
and Salakoski, 2013)) are shown for reference.
5We refer to the websites of the tasks for the details of the
event categories.
96
Task System Rec. Prec. F-Score
CG EventMine 48.83 55.82 52.09
TEES-2.1 48.76 64.17 55.41
PC EventMine 52.23 53.48 52.84
TEES-2.1 47.15 55.78 51.10
Table 2: Official best and second best scores on
the CG and PC tasks. Higher scores are shown in
bold.
Task Category EventMine TEES-2.1
CG ANATOMY 71.31 77.20
PATHOL 59.78 67.51
MOLECUL 72.77 72.60
GENERAL 53.08 52.20
REGULAT 39.79 43.08
PLANNED 40.51 39.43
MOD 29.95 34.66
PC SIMPLE 65.60 63.92
NON-REG 65.72 63.37
REGULAT 40.10 39.39
MOD 28.05 28.73
Table 3: F-scores on the CG and PC tasks for event
categories. Higher scores are shown in bold.
EventMine achieved the highest recall for both
tasks, and this is favourable as mentioned above.
This high recall is reasonable since EventMine
solved the problems as multi-label classification
tasks, corrected the class imbalance problem as
explained in Section 2.1 and incorporated the type
generalisations for expanding possible event struc-
tures. The performance (in F-score) on both CG
and PC tasks is slightly lower than the perfor-
mance on the GENIA and ID tasks in the BioNLP-
ST 2011 (Miwa et al, 2012), and close to the per-
formance on the EPI task. This may be partly be-
cause the GENIA and ID tasks deal with a fewer
number of event types than the other tasks.
EventMine performed worse than the best sys-
tem in the CG task, but this result is promis-
ing considering that we did not incorporate any
other resources and tune the parameters (e.g., C
in SVM). The detailed comparison with TEES-
2.1 shows that EventMine performed much worse
than TEES-2.1 in anatomical and pathological
event categories, which contained relatively new
event types. This indicates EventMine missed
some of the new structures in the new event types.
The range of the scores is similar to the
scores on the MLEE corpus (52.34?53.43% in F-
Score (Pyysalo et al, 2012a)) although we can-
not directly compare the results. The ranges of
the scores are around 60% to 70% for non-nested
events (e.g., SIMPLE), 40% for nested events
(e.g., REGULAT) and 30% for modifications (e.g.,
MOD). This large spread of the scores may be
caused by a multiplication of errors in predicting
their participants, since similar spread was seen
in the previous tasks (e.g., (Miwa et al, 2012)).
These results indicate that we may not be able
to improve the performance just by increasing the
training instances.
These results show that EventMine performed
well on the PC task that is a completely novel task
for EventMine, and the stacking would also work
effectively on the test set.
4 Conclusions
This paper explained how EventMine was ap-
plied to the CG and PC tasks in the BioNLP-
ST 2013. EventMine performed well on these
tasks and achieved the second best performance
in the CG task and the best performance in the
PC task. We show the usefulness of incorporat-
ing other existing corpora in the PC task. The
success of this application shows that the Event-
Mine implementation is flexible enough to treat
the new tasks. The performance ranges, however,
shows that we may need to incorporate other novel
techniques/linguistic information to produce the
higher performance.
As future work, we will investigate the cause
of the missed events. We also would like to ex-
tend and apply other functions in EventMine, such
as co-reference resolution, and seek a general ap-
proach that can improve the event extraction per-
formance on all the existing corpora, using the
training data along with external resources.
Acknowledgement
This work is supported by the Biotechnology and
Biological Sciences Research Council (BBSRC)
[BB/G53025X/1] and the Grant-in-Aid for Young
Scientists B [25730129] of the Japan Science and
Technology Agency (JST).
References
Jari Bjo?rne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bioNLP
97
2013 shared task. In Proceedings of BioNLP Shared
Task 2013 Workshop, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Jari Bjo?rne, Sofie Van Landeghem, Sampo Pyysalo,
Tomoko Ohta, Filip Ginter, Yves Van de Peer,
Sophia Ananiadou, and Tapio Salakoski. 2012.
Pubmed-scale event extraction for post-translational
modifications, epigenetics and protein structural re-
lations. In BioNLP: Proceedings of the 2012 Work-
shop on Biomedical Natural Language Processing,
pages 82?90, Montre?al, Canada, June. Association
for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011. Extract-
ing Bio-Molecular Events from Literature ? the
BioNLP?09 Shared Task. Computational Intelli-
gence, 27(4):513?540.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia Event and Protein Coreference tasks of
the BioNLP Shared Task 2011. BMC Bioinformat-
ics, 13(Suppl 11):S1.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Makoto Miwa, Tomoko Ohta, Rafal Rak, Andrew
Rowley, Douglas B. Kell, Sampo Pyysalo, and
Sophia Ananiadou. 2013a. A method for integrat-
ing and ranking the evidence for biochemical path-
ways by mining reactions from text. Bioinformatics.
(In Press).
Makoto Miwa, Sampo Pyysalo, Tomoko Ohta, and
Sophia Ananiadou. 2013b. Wide coverage biomedi-
cal event extraction using multiple partially overlap-
ping corpora. BMC Bioinformatics, 14(1):175.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80, March.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2011a. Event extraction for dna
methylation. Journal of Biomedical Semantics,
2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsu-
jii. 2011b. From pathways to biomolecular
events: Opportunities and challenges. In Proceed-
ings of BioNLP?11, pages 105?113, Portland, Ore-
gon, USA. ACL.
Tomoko Ohta, Sampo Pyysalo, Rafal Rak, Andrew
Rowley, Hong-Woo Chun, Sung-Jae Jung, Sung-Pil
Choi, and Sophia Ananiadou. 2013. Overview of
the pathway curation (PC) task of bioNLP shared
task 2013. In Proceedings of BioNLP Shared Task
2013 Workshop, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and
Jun?ichi Tsujii. 2011. Towards exhaustive event ex-
traction for protein modifications. In Proceedings
of BioNLP?11, pages 114?123, Portland, Oregon,
USA, June. ACL.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-
Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou.
2012a. Event extraction across multiple levels of bi-
ological organization. Bioinformatics, 28(18):i575?
i581.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2012b.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(Suppl
11):S2.
Sampo Pyysalo, Tomoko Ohta, and Sophia Ananiadou.
2013. Overview of the cancer genetics (CG) task
of bioNLP shared task 2013. In Proceedings of
BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. ACL.
S. Van Landeghem, J. Bjorne, C. H. Wei, K. Hakala,
S. Pyysalo, S. Ananiadou, H. Y. Kao, Z. Lu,
T. Salakoski, Y. Van de Peer, and F. Ginter.
2013. Large-scale event extraction from literature
with multi-level gene normalization. PLoS One,
8(4):e55814.
David H Wolpert. 1992. Stacked generalization. Neu-
ral networks, 5(2):241?259.
98
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 89?97,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Making UIMA Truly Interoperable with SPARQL
Rafal Rak and Sophia Ananiadou
National Centre for Text Mining
School of Computer Science, University of Manchester
{rafal.rak,sophia.ananiadou}@manchester.ac.uk
Abstract
Unstructured Information Management
Architecture (UIMA) has been gaining
popularity in annotating text corpora. The
architecture defines common data struc-
tures and interfaces to support interoper-
ability of individual processing compo-
nents working together in a UIMA appli-
cation. The components exchange data by
sharing common type systems?schemata
of data type structures?which extend a
generic, top-level type system built into
UIMA. This flexibility in extending type
systems has resulted in the development of
repositories of components that share one
or several type systems; however, compo-
nents coming from different repositories,
and thus not sharing type systems, remain
incompatible. Commonly, this problem
has been solved programmatically by im-
plementing UIMA components that per-
form the alignment of two type systems,
an arduous task that is impractical with a
growing number of type systems. We al-
leviate this problem by introducing a con-
version mechanism based on SPARQL, a
query language for the data retrieval and
manipulation of RDF graphs. We pro-
vide a UIMA component that serialises
data coming from a source component
into RDF, executes a user-defined, type-
conversion query, and deserialises the up-
dated graph into a target component. The
proposed solution encourages ad hoc con-
versions, enables the usage of heteroge-
neous components, and facilitates highly
customised UIMA applications.
1 Introduction
Unstructured Information Management Architec-
ture (UIMA) (Ferrucci and Lally, 2004) is a frame-
work that supports the interoperability of media-
processing software components by defining com-
mon data structures and interfaces the compo-
nents exchange and implement. The architec-
ture has been gaining interest from academia and
industry alike for the past decade, which re-
sulted in a multitude of UIMA-supporting repos-
itories of analytics. Notable examples include
METANET4U components (Thompson et al,
2011) featured in U-Compare1, DKPro (Gurevych
et al, 2007), cTAKES (Savova et al, 2010),
BioNLP-UIMA Component Repository (Baum-
gartner et al, 2008), and JULIE Lab?s UIMA
Component Repository (JCoRe) (Hahn et al,
2008).
However, despite conforming to the UIMA
standard, each repository of analytics usually
comes with its own set of type systems, i.e., rep-
resentations of data models that are meant to be
shared between analytics and thus ensuring their
interoperability. At present, UIMA does not fa-
cilitate the alignment of (all or selected) types be-
tween type systems, which makes it impossible to
combine analytics coming from different reposito-
ries without an additional programming effort. For
instance, NLP developers may want to use a sen-
tence detector from one repository and a tokeniser
from another repository only to learn that the re-
quired input Sentence type for the tokeniser is
defined in a different type system and namespace
than the output Sentence type of the sentence
detector. Although both Sentence types repre-
sent the same concept and may even have the same
set of features (attributes), they are viewed as two
distinct types by UIMA.
Less trivial incompatibility arises from the same
concept being encoded as structurally different
types in different type systems. Figures 1 and 2
show fragments of some of existing type systems;
1http://nactem.ac.uk/ucompare/
89
(a) DKPro (b) JCoRe (c) ACE
Figure 1: UML diagrams representing fragments of type systems that show differences in encoding
coreferences.
specifically, they show the differences in encod-
ing coreferences and events, respectively. For in-
stance, in comparison to the JCoRe type system in
Figure 1(b), the DKPro type system in Figure 1(a)
has an additional type that points to the beginning
of the linked list of coreferences.
Conceptually similar types in two different type
systems may also be incompatible in terms of the
amount of information they convey. Compare, for
instance, type systems in Figure 2 that encode a
similar concept, event. Not only are they struc-
turally different, but the cTAKES type system in
Figure 2(a) also involves a larger number of fea-
tures than the other two type systems. Although,
in this case, the alignment of any two structures
cannot be carried out without a loss or deficiency
of information, it may still be beneficial to do so
for applications that consist of components that ei-
ther fulfill partially complete information or do not
require it altogether.
The available type systems vary greatly in size,
their modularity, and intended applicability. The
DKPro UIMA software collection, for instance,
includes multiple, small-size type systems organ-
ised around specific syntactic and semantic con-
cepts, such as part of speech, chunks, and named
entities. In contrast, the U-Compare project as
well as cTAKES are oriented towards having a sin-
gle type system. Respectively, the type systems
define nearly 300 and 100 syntactic and seman-
tic types, with U-Compare?s semantic types biased
towards biology and chemistry and cTAKES?s
covering clinical domain. Most of the U-Compare
types extend a fairly expressive higher-level type,
which makes them universally applicable, but at
the same time, breaks their semantic cohesion.
The lack of modularity and the all-embroiling
types suggest that the U-Compare type system is
developed primarily to work with the U-Compare
application.
The Center for Computational Pharmacology
(CCP) type system (Verspoor et al, 2009) is a
radically different approach to the previous sys-
tems. It defines a closed set of top-level types
that facilitate the use of external resources, such
as databases and ontologies. This gives the advan-
tage of having a nonvolatile type system, indiffer-
ent to changes in the external resources, as well as
greater flexibility in handling some semantic mod-
els that would otherwise be impossible to encode
in a UIMA type system. On the other hand, such
an approach shifts the handling of interoperability
from UIMA to applications that must resolve com-
patibility issues at runtime, which also results in
the weakly typed programming of analytics. Addi-
tionally, the UIMA?s native indexing of annotation
types will no longer work with such a type system,
which prompts an additional programming effort
from developers.
The aforementioned examples suggest that es-
tablishing a single type system that could be
shared among all providers is unlikely to ever take
place due to the variability in requirements and
applicability. Instead, we adopt an idea of us-
ing a conversion mechanism that enables align-
ing types across type systems. The conversion
has commonly been solved programmatically by
creating UIMA analytics that map all or (more
likely) selected types between two type systems.
For instance, U-Compare features a component
that translates some of the CPP types into the U-
Compare types. The major drawback of such a
solution is the necessity of having to implement
an analytic which requires programming skills and
becomes an arduous task with an increasing num-
ber of type systems. In contrast, we propose a
conversion based entirely on developers? writing a
query in the well established SPARQL language,
90
(a) cTAKES (b) ACE
(c) Events Type System
Figure 2: UML diagrams representing fragments of type systems that show differences in encoding event
structures.
an official W3C Recommendation2. Our approach
involves 1) the serialisation of UIMA?s internal
data structures to RDF3, 2) the execution of a user-
defined, type-conversion SPARQL query, and 3)
the deserialisation of the results back to the UIMA
structure.
The remainder of this paper is organised as fol-
lows. The next section presents related work. Sec-
tion 3 provides background information on UIMA,
RDF and SPARQL. Section 4 discusses the pro-
posed representation of UIMA structures in RDF,
whereas Section 5 examines the utility of our
method. Section 6 details the available implemen-
tation, and Section 7 concludes the paper.
2 Related Work
In practice, type alignment or conversion is the
creation of new UIMA feature structures based
on the existing ones. Current efforts in this
area mostly involve solutions that are essentially
2http://www.w3.org/TR/2013/REC-sparql11-overview-
20130321
3http://www.w3.org/RDF/
(cascaded) finite state transducers, i.e., an in-
put stream of existing feature structures is being
matched against developers? defined patterns, and
if a match is found, a series of actions follows and
results in one or more output structures.
TextMarker (Kluegl et al, 2009) is currently
one of the most comprehensive tools that de-
fines its own rule-based language. The language
capabilities include the definition of new types,
annotation-based regular expression matching and
a rich set of condition functions and actions. Com-
bined with a built-in lexer that produces basic to-
ken annotations, TextMarker is essentially a self-
contained, UIMA-based annotation tool.
Hernandez (2012) proposed and developed a
suite of tools for tackling the interoperability of
components in UIMA. The suite includes uima-
mapper, a conversion tool designed to work with
a rule-based language for mapping UIMA anno-
tations. The rules are encoded in XML, and?
contrary to the previous language that relies solely
on its own syntax?include XPath expressions for
patterns, constraints, and assigning values to new
91
feature structures. This implies that the input of
the conversion process must be encoded in XML.
PEARL (Pazienza et al, 2012) is a language for
projecting UIMA annotations onto RDF reposito-
ries. Similarly to the previous approaches, the lan-
guage defines a set of rules triggered upon encoun-
tering UIMA annotations. The language is de-
signed primarily to work in CODA, a platform that
facilitates population of ontologies with the output
of NLP analytics. Although it does not directly
facilitate the production or conversion of UIMA
types, the PEARL language shares similarities to
our approach in that it incorporates certain RDF
Turtle, SPARQL-like semantics.
Contrary to the aforementioned solutions, we
do not define any new language or syntax. Instead,
we rely completely on an existing data query and
manipulation language, SPARQL. By doing so,
we shift the problem of conversion from the def-
inition of a new language to representing UIMA
structures in an existing language, such that they
can be conveniently manipulated in that language.
A separate line of research pertains to the for-
malisation of textual annotations with knowledge
representations such as RDF and OWL4. Buyko et
al. (2008) link UIMA annotations to the reference
ontology OLiA (Chiarcos, 2012) that contains a
broad vocabulary of linguistic terminology. The
authors claim that two conceptually similar type
systems can be aligned with the reference ontol-
ogy. The linking involves the use of OLiA?s as-
sociated annotation and linking ontology model
pairs that have been created for a number of an-
notation schemata. Furthermore, a UIMA type
system has to define additional features for each
linked type that tie a given type to an annotation
model. In effect, in order to convert a type from an
arbitrary type system to another similar type sys-
tem, both systems must be modified and an anno-
tation and linking models must be created. Such
an approach generalises poorly and is unsuitable
for impromptu type system conversions.
3 Background
3.1 UIMA Overview
UIMA defines both structures and interfaces to
facilitate interoperability of individual processing
components that share type systems. Type systems
may be defined in or imported by a processing
component that produces or modifies annotations
4http://www.w3.org/TR/owl2-overview/
Figure 3: UML diagram representing relationships
between CASes, views, and feature structures in
UIMA. The shown type system is a fragment of
the built-in UIMA type system.
in a common annotation structure (CAS), i.e., a
CAS is the container of actual data bound by the
type system.
Types may define multiple primitive features as
well as references to feature structures (data in-
stances) of other types. The single-parent inheri-
tance of types is also possible. The resulting struc-
tures resemble those present in modern object-
oriented programming languages.
Feature structures stored in a CAS may be
grouped into several views, each of which hav-
ing its own subject of analysis (Sofa). For in-
stance, one view may store annotations about
a Sofa that stores an English text, whereas an-
other view may store annotations about a dif-
ferent Sofa that stores a French version of the
same text. UIMA defines built-in types including
primitive types (boolean, integer, string, etc.), ar-
rays, lists, as well as several complex types, e.g.,
uima.tcas.Annotation that holds a refer-
ence to a Sofa the annotation is asserted about, and
two features, begin and end, for marking bound-
aries of a span of text. The relationships be-
tween CASes, views, and several prominent built-
in types are shown in Figure 3.
The built-in complex types may further
be extended by developers. Custom types
that mark a fragment of text usually extend
uima.tcas.Annotation, and thus inherit
the reference to the subject of analysis, and the
begin and end features.
92
UIMA element/representation RDF resource
CAS <uima:aux:CAS>
Access to CAS?s views rdfs:member or rdf:_1, rdf:_2, ...
View <uima:aux:View>
View?s name <uima:aux:View:name>
View?s Sofa <uima:aux:View:sofa>
Access to view?s feature structures rdfs:member or rdf:_1, rdf:_2, ...
Access to feature structure?s sequential number <uima:aux:seq>
Type uima.tcas.Annotation <uima:ts:uima.tcas.Annotation>
Feature uima.tcas.Annotation:begin <uima:ts:uima.cas.Annotation:begin>
Access to uima.cas.ArrayBase elements rdfs:member or rdf:_1, rdf:_2, ...
Table 1: UIMA elements and their corresponding RDF resource representations
3.2 RDF and SPARQL
Resource Description Framework (RDF) is a
method for modeling concepts in form of making
statements about resources using triple subject-
predicate-object expressions. The triples are com-
posed of resources and/or literals with the latter
available only as objects. Resources are repre-
sented with valid URIs, whereas literals are val-
ues optionally followed by a datatype. Multiple
interlinked subject and objects ultimately consti-
tute RDF graphs.
SPARQL is a query language for fetching data
from RDF graphs. Search patterns are created
using RDF triples that are written in RDF Turtle
format, a human-readable and easy to manipulate
syntax. A SPARQL triple may contain variables
on any of the three positions, which may (and usu-
ally does) result in returning multiple triples from
a graph for the same pattern. If the same variable
is used more than once in patterns, its values are
bound, which is one of the mechanisms of con-
straining results.
Triple-like patterns with variables are simple,
yet expressive ways of retrieving data from an
RDF graph and constitute the most prominent fea-
ture of SPARQL. In this work, we additionally
utilise features of SPARQL 1.1 Update sublan-
guage that facilitates graph manipulation.
4 Representing UIMA in RDF
We use RDF Schema5 as the primary RDF vocab-
ulary to encode type systems and feature struc-
tures in CASes. The schema defines resources
such as rdfs:Class, rdf:type (to denote a
membership of an instance to a particular class)
5http://www.w3.org/TR/rdf-schema/
and rdfs:subClassOf (as a class inheritance
property)6. It is a popular description language for
expressing a hierarchy of concepts, their instances
and relationships, and forms a base for such se-
mantic languages as OWL.
The UIMA type system structure falls nat-
urally into this schema. Each type is ex-
pressed as rdfs:Class and each feature as
rdfs:Property accompanied by appropriate
rdfs:domain and rdfs:range statements.
Feature structures (instances) are then assigned
memberships of their respective types (classes)
through rdf:type properties.
A special consideration is given to the type
ArrayBase (and its extensions). Since the or-
der of elements in an array may be of impor-
tance, feature structures of the type ArrayBase
are also instances of the class rdf:Seq, a se-
quence container, and the elements of an ar-
ray are accessed through the properties rdf:_1,
rdf:_2, etc., which, in turn, are the subprop-
erties of rdfs:member. This enables query-
ing array structures with preserving the order of
its members. Similar, enumeration-property ap-
proach is used for views that are members of
CASes and feature structures that are members of
views. The order for the latter two is defined in the
internal indices of a CAS and follows the order in
which the views and feature structures were added
to those indices.
We also define several auxiliary RDF resources
to represent relationships between CASes, views
and feature structures (cf. Figure 3). We intro-
duced the scheme name ?uima? for the URIs of
6Following RDF Turtle notation we denote prefixed forms
of RDF resources as prefix:suffix and their full forms
as <fullform>
93
Figure 4: Complete SPARQL query that converts
the sentence type in one type system to a struc-
turally identical type in another type system.
the UIMA-related resources. The fully qualified
names of UIMA types and their features are part
of the URI paths. The paths are additionally pre-
fixed by ?ts:? to avoid a name clash against
the aforementioned auxiliary CAS and view URIs
that, in turn, are prefixed with ?aux:?. Table 1
summarises most of the UIMA elements and their
corresponding representations in RDF.
5 Conversion Capabilities
In this section we examine the utility of the
proposed approach and the expressiveness of
SPARQL by demonstrating several conversion ex-
amples. We focus on technical aspects of conver-
sions and neglect issues related to a loss or defi-
ciency of information that is a result of differences
in type system conceptualisation (as discussed in
Introduction).
5.1 One-to-one Conversion
We begin with a trivial case where two types
from two different type systems have exactly the
same names and features; the only difference
lies in the namespace of the two types. Fig-
ure 4 shows a complete SPARQL query that con-
verts (copies) their.Sentence feature struc-
tures to our.Sentence structures. Both types
extend the uima.tcas.Annotation type and
inherit its begin and end features. The WHERE
clause of the query consists of patterns that match
CASes? views and their feature structures of the
type their.Sentence together with the type?s
begin and end features.
For each solution of the WHERE clause (each
retrieved tuple), the INSERT clause then creates a
new sentence of the target type our.Sentence
(the a property is the shortcut of rdf:type)
Figure 5: SPARQL query that aligns different con-
ceptualisations of event structures between two
type systems. Prefix definitions are not shown.
and rewrites the begin and end values to its fea-
tures. The blank node _:sentence is going to
be automatically re-instantiated with a unique re-
source for each matching tuple making each sen-
tence node distinct. The last line of the INSERT
clause ties the newly created sentence to the view,
which is UIMA?s equivalent of indexing a feature
structure in a CAS.
5.2 One-to-many Conversion
In this use case we examine the conversion of
a container of multiple elements to a set of dis-
connected elements. Let us consider event types
from the ACE and Events type systems as shown
in Figures 2(b) and 2(c), respectively. A single
Event structure in the ACE type system aggre-
gates multiple EventMention structures in an
effort to combine multiple text evidence support-
ing the same event. The NamedEvent type in
the Events type system, on the other hand, makes
no such provision and is agnostic to the fact that
multiple mentions may refer to the same event.
94
To avoid confusion, we will refer to the types
using their RDF prefixed notations, ?ace:? and
?gen:?, to denote the ACE and ?generic? Events
type systems, respectively.
The task is to convert all ace:Events
and their ace:EventMentions into
gen:NamedEvents. There is a cou-
ple of nuances that need to be taken
into consideration. Firstly, although both
ace:EventMention and gen:NamedEvent
extend uima.tcas.Annotation, the be-
gin and end features have different mean-
ings for the two event representations. The
gen:NamedEvent?s begin and end features
represent an anchor/trigger, a word in the text that
initiates the event. The same type of informa-
tion is accessible from ace:EventMention
via its anchor feature instead. Secondly,
although it may be tempting to disregard the
ace:Event structures altogether, they contain
the type feature whose value will be copied to
gen:NamedEvent?s name feature.
The SPARQL query that performs that
conversion is shown in Figure 5. In the
WHERE clause, for each ace:Event,
patterns select ace:EventMentions
and for each ace:EventMention,
ace:EventMentionArguments are
also selected. This behaviour resembles
triply nested for loop in programming lan-
guages. Additionally, ace:Event?s type,
ace:EventMention?s anchor begin and end
values, and ace:EventMentionArgument?s
role and target are selected. In contrast to the
previous example, we cannot use blank nodes for
creating event resources in the INSERT clause,
since the retrieved tuples share event URIs for
each ace:EventMentionArgument. Hence
the last two BIND functions create URIs for
each ace:EventMention and its array of
arguments, both of which are used in the INSERT
clause.
Note that in the INSERT clause, if several
gen:NamedEventParticipants share the
same gen:NamedEvent, the definition of the
latter will be repeated for each such participant.
We take advantage of the fact that adding a triple
to an RDF graph that already exists in the graph
has no effect, i.e., an insertion is simply ignored
and no error is raised. Alternatively, the query
could be rewritten as two queries, one that creates
Figure 6: SPARQL query that converts corefer-
ences expressed as linked lists to an array repre-
sentation. Prefix definitions are not shown.
gen:NamedEvent definitions and another that
creates gen:NamedEventParticipant def-
initions.
To recapitulate, RDF and SPARQL support one-
to-many (and many-to-one) conversions by stor-
ing only unique triple statements and by providing
functions that enable creating arbitrary resource
identifiers (URIs) that can be shared between re-
trieved tuples.
5.3 Linked-list-to-Array Conversion
For this example, let us consider two types
of structures for storing coreferences from the
DKPro and ACE type systems, as depicted in Fig-
ures 1(a) and 1(c), respectively.
The idea is to convert DKPro?s chains of links
into ACE?s entities that aggregate entity mentions,
or?using software developers? vocabulary?to
convert a linked list into an array. The SPARQL
query for this conversion is shown in Figure 6.
The WHERE clause first selects all
dkpro:CoreferenceChain instances from
views. Access to dkpro:CoreferenceLink
instances for each chain is provided by a property
95
path. Property paths are convenient shortcuts
for navigating through nodes of an RDF graph.
In this case, the property path expands to the
chain?s first feature/property followed by any
number (signified by the asterisk) of links? next
feature/property. The pattern with this path will
result in returning all links that are accessible
from the originating chain; however, according
to the SPARQL specification, the order of links
is not guaranteed to be preserved, which in
coreference-supporting applications is usually of
interest. A solution is to make use of the property
<uima:aux:seq> that points to the sequential
number of a feature structure and is unique in the
scope of a single CAS. Since feature structures
are serialised into RDF using deep-first traversal,
the consecutive link structures for each chain will
have their sequence numbers monotonically in-
creasing. These sequence numbers are translated
to form rdf:_nn properties (nn standing for the
number), which facilitates the order of elements in
the ace:Entity array of mentions7. It should
be noted, however, that using the sequence num-
ber property will work only if the links of a chain
are not referred to from another structure. There is
another, robust solution (not shown due to space
limitation and complexity) that involves multiple
INSERT queries and temporary, supporting RDF
nodes. RDF nodes that are not directly relevant
to a CAS and its feature structures are ignored
during the deserialisation process, and thus it is
safe to create any number of such nodes.
6 Tool Support
We have developed a UIMA analysis engine,
SPARQL Annotation Editor, that incorporates the
serialisation of a CAS into RDF (following the
protocol presented in Section 4), the execution of
a user-defined SPARQL query, and the deseriali-
sation of the updated RDF graph back to the CAS.
The RDF graph (de)serialisation and SPARQL
query execution is implemented using Apache
Jena8, an open-source framework for building Se-
mantic Web applications.
To further assist in the development of type-
conversion SPARQL queries, we have provided
two additional UIMA components, RDF Writer
and RDF Reader. RDF Writer serialises CASes to
7The rdf:_nn properties are not required to be consec-
utive in an RDF container
8http://jena.apache.org/
files that can then be used with SPARQL query en-
gines, such as Jena Fuseki (part of the Apache Jena
project), to develop and test conversion queries.
The modified RDF graphs can be imported back
to a UIMA application using RDF Reader, an RDF
deserialisation component.
The three components are featured in Argo (Rak
et al, 2012), a web-based workbench for building
and executing UIMA workflows.
7 Conclusions
The alignment of types between different type sys-
tems using SPARQL is an attractive alternative to
existing solutions. Compared to other solutions,
our approach does not introduce a new language
or syntax; to the contrary, it relies entirely on a
well-defined, standardised language, a character-
istic that immediately broadens the target audi-
ence. Likewise, developers who are unfamiliar
with SPARQL should be more likely to learn this
well-maintained and widely used language than
any other specialised and not standardised syntax.
The expressiveness of SPARQL makes the
method superior to the rule-based techniques,
mainly due to SPARQL?s inherent capability
of random data access and simple, triple-based
querying. At the same time, the semantic cohesion
of data is maintained by a graph representation.
The proposed solution facilitates the rapid
alignment of type systems and increases the flexi-
bility in which developers choose processing com-
ponents to build their UIMA applications. As well
as benefiting the design of applications, the con-
version mechanism may also prove helpful in the
development of components themselves. To en-
sure interoperability, developers usually adopt an
existing type system for a new component. This
essential UIMA-development practice undeniably
increases the applicability of such a component;
however, at times it may also result in having the
ill-defined representation of the data produced by
the component. The availability of an easy-to-
apply conversion tool promotes constructing fine-
tuned type systems that best represent such data.
Acknowledgments
This work was partially funded by the MRC Text
Mining and Screening grant (MR/J005037/1).
96
References
W A Baumgartner, K B Cohen, and L Hunter. 2008.
An open-source framework for large-scale, flexible
evaluation of biomedical text mining systems. Jour-
nal of biomedical discovery and collaboration, 3:1+.
E Buyko, C Chiarcos, and A Pareja-Lora. 2008.
Ontology-based interface specifications for a nlp
pipeline architecture. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC?08), Marrakech, Morocco.
C Chiarcos. 2012. Ontologies of linguistic annota-
tion: Survey and perspectives. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC?12), pages 303?310.
D Ferrucci and A Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
Based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technol-
ogy, Tu?bingen, Germany.
U Hahn, E Buyko, R Landefeld, M Mu?hlhausen,
M Poprat, K Tomanek, and J Wermter. 2008. An
Overview of JCORE, the JULIE Lab UIMA Com-
ponent Repository. In Proceedings of the Language
Resources and Evaluation Workshop, Towards En-
hanc. Interoperability Large HLT Syst.: UIMA NLP,
pages 1?8.
N Hernandez. 2012. Tackling interoperability is-
sues within UIMA workflows. In Proceedings of
the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey. European Language Resources Association
(ELRA).
P Kluegl, M Atzmueller, and F Puppe. 2009.
TextMarker: A Tool for Rule-Based Information Ex-
traction. In Proceedings of the Biennial GSCL Con-
ference 2009, 2nd UIMA@GSCL Workshop, pages
233?240. Gunter Narr Verlag.
M T Pazienza, A Stellato, and A Turbati. 2012.
PEARL: ProjEction of Annotations Rule Language,
a Language for Projecting (UIMA) Annotations over
RDF Knowledge Bases. In Proceedings of the Eight
International Conference on Language Resources
and Evaluation (LREC?12), Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
R Rak, A Rowley, W Black, and S Ananiadou. 2012.
Argo: an integrative, interactive, text mining-based
workbench supporting curation. Database : The
Journal of Biological Databases and Curation, page
bas010.
G K Savova, J J Masanz, P V Ogren, J Zheng, S Sohn,
K C Kipper-Schuler, and C G Chute. 2010. Mayo
clinical Text Analysis and Knowledge Extraction
System (cTAKES): architecture, component evalua-
tion and applications. Journal of the American Med-
ical Informatics Association : JAMIA, 17(5):507?
513.
P Thompson, Y Kano, J McNaught, S Pettifer, T K
Attwood, J Keane, and S Ananiadou. 2011. Promot-
ing Interoperability of Resources in META-SHARE.
In Proceedings of the IJCNLP Workshop on Lan-
guage Resources, Technology and Services in the
Sharing Paradigm (LRTS), pages 50?58.
K Verspoor, W Baumgartner Jr, C Roeder, and
L Hunter. 2009. Abstracting the Types away from a
UIMA Type System. From Form to Meaning: Pro-
cessing Texts Automatically., pages 249?256.
97
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95?104,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using a Random Forest Classifier to recognise translations of biomedical
terms across languages
Georgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun?ichi Tsujii3 Sophia Ananiadou1,2
National Centre for Text Mining, University of Manchester, Manchester, UK1
School of Computer Science, University of Manchester, Manchester, UK2
Microsoft Research Asia, Beijing, China3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We present a novel method to recognise
semantic equivalents of biomedical terms
in language pairs. We hypothesise that
biomedical term are formed by seman-
tically similar textual units across lan-
guages. Based on this hypothesis, we
employ a Random Forest (RF) classifier
that is able to automatically mine higher
order associations between textual units
of the source and target language when
trained on a corpus of both positive and
negative examples. We apply our method
on two language pairs: one that uses the
same character set and another with a dif-
ferent script, English-French and English-
Chinese, respectively. We show that
English-French pairs of terms are highly
transliterated in contrast to the English-
Chinese pairs. Nonetheless, our method
performs robustly on both cases. We eval-
uate RF against a state-of-the-art align-
ment method, GIZA++, and we report a
statistically significant improvement. Fi-
nally, we compare RF against Support
Vector Machines and analyse our results.
1 Introduction
Given a term in a source language and term in a
target language the task of this paper is to classify
this pair as a translation or not. We investigate the
performance of the proposed classifier by apply-
ing it on a balanced classification problem, i.e. our
experimental datasets contain an equal number of
positive and negative examples. The proposed
classification model can be used as a component of
a larger system that automatically compiles bilin-
gual dictionaries of technical terms across lan-
guages. Bilingual dictionaries of terms are impor-
tant resources for many Natural Language Pro-
cessing (NLP) applications including Statistical
Machine Translation (SMT) (Feng et al, 2004;
Huang and Vogel, 2002; Wu et al, 2008), Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997) and Question Answering systems
(Al-Onaizan and Knight, 2002). Especially in the
biomedical domain, manually creating and more
importantly updating such resources is an expen-
sive process, due to the vast amount of neologisms,
i.e. newly introduced terms (Pustejovsky et al,
2001). The UMLS metathesaurus which is one the
most popular hub of multilingual resources in the
biomedical domain, contains technical terms in 21
languages that are linked together using a con-
cept identifier. In Spanish, the second most popu-
lar language in UMLS, only 16.44% of the 7.6M
English terms are covered while other languages
fluctuate between 0.0052% (for Hebrew terms) to
3.26% (for Japanese terms). Hence, these lex-
ica are far for complete and methods that semi-
automatically (i.e., in a post-processing step, cu-
rators can manually remove erroneous dictionary
entries) discover pairs of terms across languages
are needed to enrich such multilingual resources.
Our method can be applied to parallel, aligned cor-
pora, where we expect approximately the same,
balanced classification problem. However, in
comparable corpora the search space of candidate
alignments is of vast size, i.e., quadratic the the
size of the input data. To cope with this heavily
unbalanced classification problem, we would need
to narrow down the number of negative instances
before classification.
We hypothesise that there are language in-
dependent rules that apply to biomedical terms
across many languages. Often the same or simi-
lar textual units (e.g., morphemes and suffixes) are
concatenated to realise the same terms in different
languages. For example, Table 1 illustrates how
a morpheme expressing pain (ache in English) is
used to realise the same terms in English, Chinese
and French. The realisations of the term ?head-
95
English Morpheme: -ache Chinese Morpheme: ? French Morpheme: -mal
head-ache ?-? mal de te?te
back-ache ?-? mal au dos
ear-ache ??-? mal d?oreille
Table 1: An example of English, Chinese and French terms consisting of the same morphemes
ache? is expected to consist of the units for ?head?
and ?ache? regardless of the language of realisa-
tion. Hence, knowing the translations of ?head?
and ?ache? allows the reconstruction ?headache?
in a target language.
In our method, we use a Random Forest (RF) clas-
sifier (Breiman, 2001) to learn the underlying rules
according to which terms are being constructed
across languages. An RF is an ensemble of De-
cision Trees voting for the most popular class. RF
classifiers are popular in the biomedical domain
for various tasks: classification of microarray data
(D??az-Uriarte and De Andres, 2006), compound
classification in cheminformatics (Svetnik et al,
2003), classification of microRNA data (Jiang et
al., 2007) and protein-protein interactions in Sys-
tems Biology (Chen and Liu, 2005). In NLP, RF
classifiers have been used for: Language Mod-
elling (Xu and Jelinek, 2004) and semantic pars-
ing (Nielsen and Pradhan, 2004). To the best of
the authors? knowledge, this is the first attempt to
employ RF for identifying translation equivalents
of biomedical terms.
We prefer RF over other traditional machine learn-
ing approaches such as Support Vector Machines
(SVMs) for a number of reasons. Firstly, RF is
able to automatically construct correlation paths
from the feature space, i.e. decision rules that cor-
respond to the translation rules that we intend
to capture. Secondly, RF is considered one of
the most accurate classifier available (D??az-Uriarte
and De Andres, 2006; Jiang et al, 2007). Finally,
RF is reported to cope well with datasets where the
number of features is larger than the number of ob-
servations (D??az-Uriarte and De Andres, 2006). In
our dataset, the number of features is almost four
times more than that of the observations.
We represent pairs of terms using character gram
features (i.e., first order features). Such shal-
low features have been proven effective in a num-
ber of NLP applications including: Named En-
tity Recognition (Klein et al, 2003), Multilin-
gual Named Entity Transliteration (Klementiev
and Roth, 2006; Freitag and Khadivi, 2007) and
predicting authorship (Stamatatos, 2006). In ad-
dition, by selecting character n-grams instead of
word n-grams, one avoids to segment words in
Chinese which has been proven to be a challenging
topic (Sproat and Emerson, 2003). We evaluate
our proposed method on two datasets of biomed-
ical terms (English-French and English-Chinese)
that contain equal numbers of positive and neg-
ative instances. RF achieves higher classifica-
tion performance than baseline methods. To boost
SVM?s performance further, we used a second or-
der feature space to represent the data. It consists
of pairs of character grams that co-occur in trans-
lation pairs. In the second order feature space, the
performance of SVMs improved significantly.
The rest of the paper is structured as follows. In
Section 2, we present previous approaches in iden-
tifying translation equivalents of terms or named
entities. In Section 3, we define the classifica-
tion problem, we formulate the RF classifier and
we discuss the first and second order feature space
that we use to represent pairs of terms. In Sec-
tion 4, we show that RF achieves superior classi-
fication performance. In Section 5, we overview
our method and we discuss how it can be used to
compile large-scale bilingual dictionaries of terms
from comparable corpora.
2 Related Work
In this section, we review previous approaches
that exploit the internal structure of sequences to
align terms or named entities across languages.
(Klementiev and Roth, 2006; Freitag and Khadivi,
2007) use character gram features, similar to the
feature space that we propose in this paper, to train
discriminative, supervised models. Klementiev
and Roth (2006) introduce a supervised Percep-
tron model for English and Russian named enti-
ties. They construct a character gram feature space
as follows: firstly, they extract all distinct charac-
ter grams from both source and target named en-
tity. Then, they pair character grams of the source
named entity with character grams of the corre-
sponding target named entity into features. In or-
96
der to reduce the number of features, they link
only those character grams whose position offsets
in the source and target sequence differs by -1, 0
or 1. Freitag and Khadivi (2007) employ the same
character gram feature space but they do not con-
straint the included character-grams to their rela-
tive position offsets in the source and target se-
quence. The boolean features are defined for ev-
ery distinct character-grams observed in the data
of length k or shorter. Using this feature space
they train an Averaged Perceptron model, able to
incorporate an arbitrary number of features in the
input vectors, for English and Arabic named en-
tities. The above character gram based methods
mainly focused on aligning named entities of the
general domain, i.e. person names, locations, or-
ganizations, etc., that are transliterated, i.e. present
phonetic similarities, across languages.
SMT-based approaches built on top of existing
SMT frameworks to identify translation pairs of
terms (Tsunakawa et al, 2008; Wu et al, 2008).
Tsunakawa et al (2008), align terms between
a source language Ls and a target language Lt
using a pivot language Lp. They assume that
two bilingual dictionaries exist: from Ls to Lp
and from Lp to Lt. Then, they train GIZA++
(Och and Ney, 2003) on both directions and they
merge the resulting phrase tables into one table
between Ls and Lt, using grow-diag-final heuris-
tics (Koehn et al, 2007). Wu et al (2008), use
morphemes instead of words as translation units
to train a phrase based SMT system for technical
terms in English and Chinese. The use of shorter
lexical fragments, e.g. lemmas, stems and suf-
fixes, as translation units has reportedly reduced
the Out-Of-Vocabulary problem (Virpioja et al,
2007; Popovic and Ney, 2004; Oflazer and El-
Kahlout, 2007).
Hybrid methods exploit that a term or a named en-
tity can be translated in various ways across lan-
guages (Shao and Ng, 2004; Feng et al, 2004; Lu
and Zhao, 2006). For instance, person names are
usually translated by transliteration (i.e., words
exhibiting pronunciation similarities across lan-
guages, are likely to be mutual translations) while
technical terms are likely to be translated by
meaning (i.e., the same semantic units are used to
generate the translation of the term in the target
language). The resulting hybrid systems were re-
ported to perform at least as well as existing SMT
systems (Feng et al, 2004).
Lepage and Denoual (2005) presented an analog-
ical learning machine translation system as part
of the IWSLT task (Eck and Hori, 2005) that re-
quires no training process and it is able to achieve
state-of-the art performance. The core method
of their system models relationships between se-
quences of characters, e.g., sentences, phrases or
words, across languages using proportional analo-
gies, i.e., [a : b = c : d], ?a is to b as c is to d?, and
is able to solve unknown analogical equations,
i.e., [x : y = z :?] (Lepage, 1998). Analogical
learning has been proven effective in translating
unseen words (Langlais and Patry, 2007). Further-
more, analogical learning is reported to achieve a
better precision but a lower recall than a phrase-
based machine translation system when translating
medical terms (Langlais et al, 2009).
3 Methodology
Let em = (e1, ? ? ? , em) be an English term
consisting of m translation units and fn =
(f1, ? ? ? , fn) a French or Chinese term consist-
ing of n units. As translation units, we con-
sider character grams. We define a function f :
(em, fn) ?? {0, 1}:
f(em, fn) =
{
1, if em translates into fn
0, otherwise
The function can be learned by training a Random
Forest (RF) classifier1. Let N be the number of
training instances, |?| the total number of features,
i.e. the number of dimensions of the feature space,
|? | a predefined number of random decision trees
and |?| a predefined number of random features.
An RF classifier is defined as a collection of fully
grown decision tree classifiers, ?i(X) (Breiman,
2001):
RF = {?1(X), ? ? ? , ?? (X)}, X = (e
m, chn)
(1)
A pair of terms is classified as a translation pair
if the majority of the trees is voting for this class
label. Let I(?i(X)) be the vote of the ith tree
in the forest and avj?{0,1} the average number of
votes for class labels 0 (translation) and 1 (non-
translation). The function f of ? decision trees
can be written as the majority function:
f(em, chn) = Maj (I(?1(X)), ? ? ? , I(?? (X)))
=
?
1
2
??
1 I(?i(X)) + 1/2(?1)
r
?
?
(2)
1The WEKA implementation (Hall et al, 2009) of RF was
used for all experiments of this paper.
97
The majority function returns 1 if the majority
of I(?i(X)) is 1, or returns 0 if the majority of
I(?i(X)) is 0. Adding or subtracting 1/2 controls
whether a tie is resolved towards 1 or 0, respec-
tively. In RF ties are resolved randomly. To rep-
resent this, the negative unit (?1) is raised to a
randomly chosen positive integer r ? N+.
We tuned the RF classifier using 140 random
trees and |?| = log2 |?|+ 1 features as suggested
in Breiman (Breiman, 2001).
The RF mechanism that triggers term construction
rules across languages lies in the decision trees.
A RF grows a decision tree by selecting the most
informative feature, i.e. corresponding to the
lowest entropy, out of ? random features. For
each selected feature, a node is created and this
process is repeated for all ? random features of
the unprunned decision trees. In other words, the
process starts with the most informative feature
and builds association rules between all random
features. These are the construction rules that
we are interested in. Figure 1 illustrates a path
in one of the decision trees of an RF classifier
taken from the experiments we conducted on
the English-Chinese dataset. In only one of
thousands of branches of the forest, the classifier
is able to partially trigger the construction rule of
kinase, a type of enzyme, between English and
Chinese. The translation rule correctly associates
the English n-grams kin and as with their Chinese
translation ??. In addition, the translation rule
contains both positive and negative associations
between features. The English n-grams ing and
or are negatively correlated with the term kinase.
3.1 Feature Engineering
Each pair of terms is represented as a feature vec-
tor of character n-grams. We further define two
types of character n-gram features, namely first
order and second order. First order character n-
grams are boolean features that designate the oc-
currence of a corresponding character gram of pre-
defined length in the input term. These features are
monolingual, extracted separately from the source
and target term. The RF classifier is shown to ben-
efit from only monolingual features and achieves
the best observed performance. In contrast, SVMs
were shown not to perform well using the first or-
der feature space because they cannot directly as-
sociate the source with the target character grams.
To enhance the performance of SVMs, we con-
structed a second order feature space that contains
associations between first order features. A sec-
ond order feature is a tuple of a source and a tar-
get character gram that co-occur in one or more
translation pairs. Table 2 illustrates an example.
Second order character n-grams are multilingual
features and are defined over true translation pairs.
For this reason, we extract second order features
from the training data only.
In all experiments, the features were sorted in de-
creasing order of frequency of occurrence. We
trained a RF and two SVM classifiers, namely
linear-SVM and RBF-SVM, using a gradually in-
creasing number of features, always starting from
the top of the list. SMT frameworks cannot be
trained on an increasing number of features be-
cause each training instance needs to correspond
to at least one known translation unit (i.e., first or-
der features). Therefore, GIZA++ is trained on the
complete set of translation units.
4 Experiments
In this section, we discuss the employed datasets
of biomedical terms in English-French and
English-Chinese and three baseline methods. We
compare and discuss RF and SVMs trained on the
first order and second order features. Finally, we
report results of all classification methods evalu-
ated on the same datasets.
4.1 Datasets
For our experiments, we used an online bilin-
gual dictionary2 for English-Chinese terms and the
UMLS metathesaurus3 for English-French terms.
The former contains 31, 700 entries while the lat-
ter is a much larger dictionary containing 84, 000
entries. For training, we used the same number of
instances for both language pairs (i.e., 21, 000 en-
tries) in order not to bias the performance towards
the larger English-French dataset. The remain-
ing instances were used for testing (i.e., 10, 7000
and 63, 000 English-Chinese and English-French
respectively). In the case where a source term cor-
responded to more that one target terms according
to the seed dictionary, we randomly selected only
one translation. Negative instances were created
by randomly matching non-translation pairs of
terms. Since we are dealing with a balanced clas-
2www2.chkd.cnki.net/kns50/
3nlm.nih.gov/research/umls
98
Figure 1: Example of a term construction rule as a branch in a decision tree.
Input pair of English-French terms : (e1, e2, e3, f1, f2, f3)
English first order French first order Second order
?1(e1, e2) ?1(f1, f2) ?1(e1e2, f1f2), ?1(e1e2, f2f3)
?1(e2, e3) ?1(f2, f3) ?1(e2e3, f1f2), ?1(e2e3, f2f3)
Table 2: Example of first and second order features using a predefined n-gram size of 2.
sification problem, we created as many negative
instances as the positive ones in all our datasets.
In all experiments we performed a 3-fold cross-
validation.
4.2 Baselines
We evaluated RF against three classification meth-
ods, namely SVMs, GIZA++ and a Levenshtein
distance-based classifier.
SVMs coordinate a hyperplane in the hyperspace
defined by the features to best separate the posi-
tive and negative instances, i.e. aligned from non-
aligned pairs. In contrast to RF, SVMs do not sup-
port building association rules between features,
i.e., translation units, which in our task seems to be
a deficiency. SVMs produce one final association
rule, i.e. the classification boundary which sepa-
rates positive from negative examples. Its abil-
ity to distinguish aligned from non-aligned pair
of terms depends on how separable the two clus-
ters are. We evaluated several settings for the
SVM classifier. Apart from the default linear ker-
nel function, we applied a radial basis function,
i.e. RBF-SVM. RBF-SVM uses the kernel trick to
project the instances in a higher dimensional space
to better separate the two clusters. While tuning
the SVM?s classification cost C, we observed op-
timal performance for a value of 100. Secondly,
we seeded the association rules of translation units
to the SVM classifier by creating a second or-
der feature space, discussed in detail in section
3.1. We employed the LIBSVM implementation
(Chang and Lin, 2011) of SVMs using both the
linear and RBF kernels.
The second baseline method is GIZA++, an
open source implementation of the 5 IBM-models
(Brown et al, 1993). GIZA++ is traditionally
trained on a bilingual, parallel corpus of aligned
sentences and estimates the probability P (s|t) of a
source translation unit (typically a word), s, given
a target unit t. To apply GIZA++ on our dataset,
we consider the list of terms as parallel sentences.
GIZA++, trained on a list of terms, estimates
the alignment probability of English-Chinese and
English-French textual units, i.e. character n-
grams. Each entry i, j in the translation table
is the probability P (si|tj), where si and tj are
the source and target character n-grams in row i
and column j, respectively. Further details about
training a SMT toolkit for aligning technical terms
can be found in (Tsunakawa et al, 2008; Freitag
and Khadivi, 2007; Wu et al, 2008). After train-
ing GIZA++ we estimate the posterior probabil-
ity P (cfn|em) that a test, Chinese or French term
cfn = {cf1, ? ? ? , cfn} is aligned with a given En-
glish term em = {e1, ? ? ? , em} as follows:
p(cfn|em) = n?m
n?
i=1
m?
j=1
P (cfi|ej) (3)
A threshold ? was defined to classify a pair of
terms into translations or non-translations:
f(em, cfn) =
{
1, if p(cfn|em) ? ?
0, otherwise
(4)
We experimented with different values of ?
(greedy search) and we selected a value that max-
imizes classification performance.
In order to estimate how phonetically similar the
two language pairs are, we employed a third base-
99
(a) English-French dataset (b) English-Chinese dataset
Figure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the first
order dataset
line method that uses the Edit/Levenshtein dis-
tance of pairs of terms to classify instances as
translations or not. The Levenshtein distance is
defined as the minimum edit operations, i.e., inser-
tion, deletions and substitution, required to trans-
form one sequence of characters to another. We
cannot directly calculate the Levenshtein distance
between English-Chinese pairs of terms since the
two languages are using different scripts. There-
fore, before we applied the Levenshtein distance-
based classifier, we converted the Chinese terms
to their pinyin form, i.e., Romanization system of
Chinese characters. As with GIZA++, we selected
a threshold ? that maximizes the performance of
the classifier.
4.3 Results
We hypothesise that a RF classifier is able to form
association paths between first order features. We
also have the theoretical intuition that SVM clas-
sifiers are not able to form such association paths.
As a result, we expect limited performance on the
first order feature set, because it does not contain
any associations among character grams.
Figure 2 shows the F-Score achieved by RF, linear-
SVM, RBF-SVM, GIZA++ and Levenshtein/Edit
distance-based classifier on the English-French
and English-Chinese datasets. RF and SVMs are
trained on an increasing number of features. The
behaviour of the classifiers is approximately the
same in both datasets. Performance is greater on
the English-French dataset since English is more
similar to French than to Chinese.
We also observe that linear-SVM and RBF-SVM
do not behave consistently. RBF-SVM?s perfor-
mance quickly climbs to a maximum and after-
wards it declines while linear-SVM?s performance
is constantly increasing until it balances to a very
high error rate, almost corresponding to random
classification. The linear-SVM classifier performs
poorly using first order features only, indicating
that this feature space is non-linearly separable,
i.e. there exists no hyperplane that separates trans-
lation from non-translation instances. Contrary,
RBF-SVM is able to construct a higher dimen-
sional space by applying the kernel trick so as
to take full advantage of a small number of fre-
quent and informative first order features. In this
higher dimensional space of few but informative
first order features, the RBF-SVM classifier coor-
dinates a hyperplane that effectively separates pos-
itive from negative instances. However, increas-
ing the number of features introduces noise that
affects the performance.
The RF is able to profit from larger sets of first
order features; thus, its performance is continu-
ously increasing until it stabilises at 6, 000 fea-
tures. The branches of the decision trees are shown
to manage features correctly to construct most of
the translation rules. Increasing the size of the fea-
ture space minimises the classification error, be-
cause more translation rules that generalize well
on unseen data are constructed.
The bilingual dictionary that we use for our
experiments contains heterogeneous biomedical
terms of diverse semantic categories. For ex-
ample, our data-set contains common medical
terms such as Intellectual Products (e.g. Pain
Management, prise en charge de la douleur, ?
???) or complex biological concepts such
as Enzymes (e.g. homogentisate 1,2-dioxygenase,
100
(a) English-French dataset (b) English-Chinese dataset
Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second
order dataset
English-French pairs English-Chinese pairs
P R F1 P R F1
GIZA++ 0.901 0.826 0.862 0.907 0.742 0.816
Levenshtein Distance 0.762 0.821 0.791 0.501 0.990 0.668
SVM -RBFsecond-order 0.946 0.884 0.914 0.750 0.899 0.818
Linear-SVMsecond-order 0.866 0.887 0.8763 0.765 0.893 0.824
RFfirst-order 0.962 0.874 0.916 0.779 0.940 0.851
Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance
acide homogentisique-oxydase, ???1,2-??
?). Therefore, we would expect poor perfor-
mance of the supervised methods using only a
small portion of the total set of first order features
due to the high diversity of the terms. For exam-
ple the morpheme ache/ mal/ ? is more frequent
in Disease or Syndrome named entities rather than
Enzyme named entities. However, the results indi-
cate that RF can generalize well on heterogeneous
terms. Figure 2 shows that the RF classifier out-
performs SMT based methods, using only 1000
features.
The Levenshtein distance-based classifier per-
forms considerably better in the English-French
dataset than in English-Chinese. In fact, its best
performance for the English-Chinese dataset is
achieved when classifying every pair of terms as
a translation, i.e. 100% recall but 50% precision.
In a second experiment, we attempted to explore
whether the performance of SVMs can be im-
proved by providing cross-language association
features. We employed the second order feature
set discussed in subsection 3.1. We used a constant
number of 6, 000 first order features, the num-
ber of features that achieved maximum F-Score
for RF in the previous experiment. Besides these
first order features, we added an increasing num-
ber of second order ones. Figure 3 shows the F-
Score curves of the RF, linear-SVM, RBF-SVM,
GIZA++ and Levenshtein distance using this fea-
ture space.
We observe that second order features improved
the performance of both SVMs considerably. In
contrast to the previous experiment, the two SVMs
present consistent bevaviour. Interestingly, the
performance of the RF slightly decreased when
using a small number of second order features.
A possible explanation of this behaviour is that
the second order associative features added noise,
since the RF had already formed the association
rules from first order features. In addition, for m
English and n Chinese or French first order fea-
tures there were m ? n possible combinations of
second order features as explained in Subsection
3.1. Hence, there was a large number of second
order features that we excluded from the train-
ing process. Consequently, decision tree branches
were populated with incomplete association rules
while the RF was able to form these associa-
tions automatically. Nevertheless, as more sec-
ond order features were added, more association
rules were explored and the RF performance in-
101
creased. Table 3 summarises the highest perfor-
mance achieved by the RF, SVMs, GIZA++ and
Levenshtein distance all trained and tested on the
same dataset. The resulting performance of the RF
compared with GIZA++ is statistically significant
(p < 0.0001) in all experiments. Comparing the
RF with the SVMs, we note that in the English-
French dataset, the performance of the SVM-RBF
is approximately the same with the performance
of our proposed method. However, this comes
with a cost. Firstly, SVMs can possibly achieve
a comparable performance to the RF when us-
ing multilingual, second order features. In con-
trast, our experiments show that RF benefit from
monolingual, first order features only. Secondly,
SVMs need a large number of additional multi-
lingual features, (6.000 second order features or
more) to perform similarly to RF. As a conse-
quence, the resulting models of the SVM classi-
fiers are more complex. We measured the aver-
age time needed by the two classifiers to decide
for a single pair of terms. The RF is approx-
imately 30 times faster than SVMs (on average
0.010 and 0.292 seconds, respectively). Finally,
in the English-Chinese dataset the RF performed
significantly better than both SVMs.
5 Discussion And Future Work
In this paper, we presented a novel classification
method that uses Random Forest (RF) to recognise
translations of biomedical terms across languages.
Our approach is based on the hypothesis that in
many languages, there exist some rules for com-
bining textual units, e.g. n-grams, to form biomed-
ical terms. Based on this assumption, we de-
fined a first order feature space of character grams
and demonstrated that an RF classifier is able to
discover such cross language translation rules for
terms. We experimented with two diverse lan-
guage pairs: English-French and English-Chinese.
In the former case, pairs of terms exhibit high pho-
netic similarity while in the latter case they do not.
Our results showed that the proposed method per-
forms robustly in both cases and achieves a signif-
icantly better performance than GIZA++. We also
evaluated Support Vector Machines (SVM) clas-
sifiers on the same first order feature space and
showed that they fail to form translation rules in
both language pairs, possibly because it cannot
associate first order features with each other suc-
cessfully. We attempted to boost the performance
of the SVM classifier by adding association evi-
dence of textual units to the features. We extracted
second order features from the training data and
we defined a new feature set consisting of both first
order and second order features. In this feature
space, the performance of the SVMs improved sig-
nificantly.
In addition to this, we observe from the reported
experiments that RF achieves a better F-Score per-
formance than GIZA++ in all datasets. Nonethe-
less, GIZA++ presents a better precision (but
lower recall) in one dataset, i.e., English/Chinese.
Based on this observation we plan to investigate
the performance of a hybrid system combining RF
with MT approaches.
One trivial approach to apply the proposed method
for compiling large-scale bilingual dictionaries of
terms from comparable corpora would be to di-
rectly classify all possible pairs of terms into
translations or non-translations. However, in
comparable corpora, the size of the search space
is quadratic to the input data. Therefore, the clas-
sification task is much more challenging since the
distribution of positive and negative instances is
highly skewed. To cope with the vast search space
of comparable corpora, we plan to incorporate
context-based approaches with the RF classifica-
tion method. Context-based approaches, such as
distributional vector similarity (Fung and McKe-
own, 1997; Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008), can be used to limit the
number of candidate translations by filtering out
pairs of terms with low contextual similarity.
Finally, the proposed method can be also used to
online augment the phrase table of Statistical Ma-
chine Translation (SMT) in order to better han-
dle the Out-of-Vocabulary problem i.e. inability
to translate textual units that consist of one or
more words and do not occur in the training data
(Habash, 2008).
Acknowledgements
The work described in this paper is partially
funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
102
References
Y. Al-Onaizan and K. Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 400?408. Association for Computational Lin-
guistics.
L. Ballesteros and W.B. Croft. 1997. Phrasal trans-
lation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Fo-
rum, volume 31, pages 84?91. ACM.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3):27.
X.W. Chen and M. Liu. 2005. Prediction of protein?
protein interactions using random decision forest
framework. Bioinformatics, 21(24):4394?4400.
R. D??az-Uriarte and S.A. De Andres. 2006. Gene se-
lection and classification of microarray data using
random forest. BMC bioinformatics, 7(1):3.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?22.
D. Feng, Y. Lv, and M. Zhou. 2004. A new approach
for english-chinese named entity alignment. In Em-
pirical Methods in Natural Language Processing,
pages 372?379.
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247.
P. Fung and K. McKeown. 1997. A technical word-
and term-translation aid using noisy parallel cor-
pora across language groups. Machine Translation,
12(1):53?87.
N. Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons
from monolingual corpora. Proceedings of ACL-08:
HLT, pages 771?779.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
F. Huang and S. Vogel. 2002. Improved named en-
tity translation and bilingual named entity extrac-
tion. In International Conference on Multimodal In-
teraction, pages 253?258. IEEE.
P. Jiang, H. Wu, W. Wang, W. Ma, X. Sun, and Z. Lu.
2007. Mipred: classification of real and pseudo
microrna precursors using random forest prediction
model with combined features. Nucleic acids re-
search, 35(suppl 2):W339?W344.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning.
2003. Named entity recognition with character-level
models. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL, pages
180?183. Association for Computational Linguis-
tics.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 817?
824. Association for Computational Linguistics.
P. Koehn and K. Knight. 2002. Learning a transla-
tion lexicon from monolingual corpora. In Proceed-
ings of the ACL-02 workshop on Unsupervised lex-
ical acquisition-Volume 9, pages 9?16. Association
for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
Proceedings of EMNLP-CoNLL, pages 877?886.
Philippe Langlais, Franc?ois Yvon, and Pierre Zweigen-
baum. 2009. Improvements in analogical learning:
application to translating multi-terms of the medical
domain. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 487?495. Association
for Computational Linguistics.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 728?734. Association for Computational Lin-
guistics.
M. Lu and J. Zhao. 2006. Multi-feature based chinese-
english named entity extraction from comparable
corpora. pages 131?141.
103
R.D. Nielsen and S. Pradhan. 2004. Mixing weak
learners in semantic parsing. In Empirical Methods
in Natural Language Processing.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
K. Oflazer and I.D. El-Kahlout. 2007. Exploring
different representational units in english-to-turkish
statistical machine translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 25?32. Association for Computational
Linguistics.
Maja Popovic and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference
on Language Resources and Evaluation (LREC),
pages 1585?1588, Lisbon,Portugal.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
and M. Morrell. 2001. Automatic extraction
of acronym-meaning pairs from medline databases.
Studies in health technology and informatics,
(1):371?375.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
L. Shao and H.T. Ng. 2004. Mining new word trans-
lations from comparable corpora. In Proceedings
of the 20th international conference on Computa-
tional Linguistics, page 618. Association for Com-
putational Linguistics.
R. Sproat and T. Emerson. 2003. The first international
chinese word segmentation bakeoff. In Proceedings
of the second SIGHAN workshop on Chinese lan-
guage processing-Volume 17, pages 133?143. Asso-
ciation for Computational Linguistics.
Efstathios Stamatatos. 2006. Ensemble-based author
identification using character n-grams. In In Proc.
of the 3rd Int. Workshop on Textbased Information
Retrieval, pages 41?46.
V. Svetnik, A. Liaw, C. Tong, J.C. Culberson, R.P.
Sheridan, and B.P. Feuston. 2003. Random forest:
a classification and regression tool for compound
classification and qsar modeling. Journal of chemi-
cal information and computer sciences, 43(6):1947?
1958.
T. Tsunakawa, N. Okazaki, and J. Tsujii. 2008.
Building bilingual lexicons using lexical translation
probabilities via pivot languages. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, may.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsu-
pervised manner. Machine Translation Summit XI,
2007:491?498.
X. Wu, N. Okazaki, T. Tsunakawa, and J. Tsujii. 2008.
Improving English-to-Chinese Translation for Tech-
nical Terms Using Morphological Information. In
AMTA-2008. MT at work: Proceedings of the Eighth
Conference of the Association for Machine Trans-
lation in the Americas, pages 202?211, Waikiki,
Hawai?i, October.
P. Xu and F. Jelinek. 2004. Random forests in lan-
guage modeling. In Empirical Methods in Natural
Language Processing, pages 325?332. Association
for Computational Linguistics.
104
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, page 1,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Keynote: Supporting evidence-based medicine using text mining
Sophia Ananiadou
School of Computer Science, University of Manchester, UK
sophia.ananiadou@manchester.ac.uk
Evidence-based medicine uses systematic reviews to identify relevant studies to answer specific research
questions. An underlying principle of the approach is the importance of specifying a priori the research
question to drive the review process. Such reviews have a central role in health technology assessments,
development of clinical guidelines and public health guidance, and evidence-informed policy and
practice. However, public health questions are complex and often need to be described using abstract,
fuzzy terminology. Understanding the scope of evidence often emerges during a review and cannot be
defined a priori. Can text mining support a dynamic and multidimensional definition of relevance using
interactive, exploratory searching under uncertainty? Can text mining help reviewers to explore evidence
of interconnections between different factors, diseases and human behaviour?
1
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 69?74,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Building a semantically annotated corpus for congestive heart and 
renal failure from clinical records and the literature 
Noha Alnazzawi, Paul Thompson and Sophia Ananiadou 
School of Computer Science, University of Manchester, UK 
alnazzan@cs.man.ac.uk, {paul.thompson, 
sophia.ananiadou@manchester.ac.uk} 
       
Abstract 
Narrative information in Electronic Health Records 
(EHRs) and literature articles contains a wealth of 
clinical information about treatment, diagnosis, 
medication and family history. This often includes 
detailed phenotype information for specific 
diseases, which in turn can help to identify risk 
factors and thus determine the susceptibility of 
different patients.  Such information can help to 
improve healthcare applications, including Clinical 
Decision Support Systems (CDS). Clinical text 
mining (TM) tools can provide efficient automated 
means to extract and integrate vital information 
hidden within the vast volumes of available text. 
Development or adaptation of TM tools is reliant 
on the availability of annotated training corpora, 
although few such corpora exist for the clinical 
domain.  In response, we have created a new 
annotated corpus (PhenoCHF), focussing on the 
identification of phenotype information for a 
specific clinical sub-domain, i.e., congestive heart 
failure (CHF). The corpus is unique in this domain, 
in its integration of information from both EHRs 
(300 discharge summaries) and literature articles (5 
full-text papers). The annotation scheme, whose 
design was guided by a domain expert, includes 
both entities and relations pertinent to CHF.  Two 
further domain experts performed the annotation, 
resulting in high quality annotation, with 
agreement rates up to 0.92 F-Score.   
1 Introduction 
An ever-increasing number of scientific articles 
is published every year. For example, in 2012, 
more than 500,000 articles were published in 
MEDLINE (U.S. National Library of Medicine , 
2013). A researcher would thus need to review at 
least 20 articles per day in order to keep up to 
date with latest knowledge and evidence in the 
literature (Perez-Rey et al., 2012). 
EHRs constitute a further rich source of 
information about patients? health, representing 
different aspects of care (Jensen et al., 2012). 
However, clinicians at the point of care have 
very limited time to review the potentially large 
amount of data contained within EHRs. This 
presents significant barriers to clinical 
practitioners and computational applications 
(Patrick et al., 2006).  
TM tools can be used to extract phenotype 
information from EHRs and the literature and 
help researchers to identify the characteristics of 
CHF and to better understand the role of the 
deterioration in kidney function in the cycle of 
progression of CHF. 
2 Related work 
There are many well-known publicly available 
corpora of scientific biomedical literature, which 
are annotated for biological entities and/or their 
interactions (often referred to as events) (Roberts 
et al., 2009; Xia  &  Yetisgen-Yildiz, 2012). 
Examples include GENIA (Kim et al., 2008), 
BioInfer (Pyysalo et al., 2007)  GREC 
(Thompson et al., 2009), PennBioIE (Kulick et 
al., 2004), GENETAG (Tanabe et al., 2005) and 
LLL?05 (Hakenberg et al., 2005). However, none 
of these corpora is annotated with the types of 
entities and relationships that are relevant to the 
study of phenotype information.  
On the other hand, corpora of clinical text 
drawn from EHRs are rare, due to privacy and 
confidentiality concerns, but also because of the 
time-consuming, expensive and tedious nature of 
producing high quality annotations, which are 
reliant on the expertise of domain experts 
(Uzuner et al., 2011). A small number of corpora, 
however, have been made available, mainly in 
the context of shared task challenges, which aim 
to encourage the development of information 
extraction (IE) systems. These corpora vary in 
terms of the text type and annotation granularity. 
For example, the corpus presented in (Pestian et 
al., 2007) concerns only structured data from 
radiology reports, while the corpus presented in 
(Meystre  &  Haug, 2006) contains unstructured 
parts of EHRs, but annotated with medical 
problem only at the document level.   
Other corpora are more similar to ours, in that 
that they include text-bound annotations 
69
corresponding to entities or relations.  CLEF 
(Clinical E-Science Framework) (Roberts et al., 
2008) was one of the first such corpora to 
include detailed semantic annotation. It consists 
of a number of different types of clinical records, 
including clinic letters, radiology and 
histopathology reports, which are annotated with 
a variety of clinical entities, relations between 
them and co-reference. However, the corpus has 
not been made publicly available. The more 
recent 2013 CLEF-eHEALTH challenge 
(Suominen et al., 2013) corpus consists of EHRs 
annotated with named entities referring to 
disorders and acronyms/abbreviations, mapped 
to UMLS concept identifiers.  
The Informatics for Integrating Biology at the 
Bedside (i2b2) NLP series of challenges have 
released a corpus of de-identified clinical records 
annotated to support a number of IE challenges 
with multiple levels of annotation, i.e., entities 
and relations (Uzuner et al., 2008; Uzuner, 
2009). The 2010 challenge included the release 
of a corpus of discharge summaries and patient 
reports in which named entities and relations 
concerning medical problems, tests and 
treatments were annotated (Uzuner et al., 2011).  
A corpus of EHRs from Mayo Clinic has been 
annotated with both linguistic information (part-
of?speech tags and shallow parsing results) and 
named entities corresponding to disorders (Ogren 
et al., 2008; Savova et al., 2010).    
3 Description of the corpus 
The discharge summaries in our PhenoCHF 
corpus constitute a subset of the data released for 
the second i2b2 shared task, known as 
?recognising obesity? (Uzuner, 2009). 
PhenoCHF corpus was created by filtering the 
original i2b2 corpus, such that only those 
summaries (a total of 300) for patients with CHF 
and kidney failure were retained.  
The second part of PhenoCHF consists of the 
5 most recent full text articles (at the time of 
query submission) concerning the characteristics 
of CHF and renal failure, retrieved from the 
PubMed Central Open Access database. 
4 Methods and results 
The design of the annotation schema was guided 
by an analysis of the relevant discharge 
summaries, in conjunction with a review of 
comparable domain specific schemata and 
guidelines, i.e., those from the CLEF and i2b2 
shared tasks. The schema is based on a set of 
requirements developed by a cardiologist. Taking 
into account our chosen focus of annotating 
phenotype information relating to the CHF 
disease, the cardiologist was asked firstly to 
determine a set of relevant entity types that relate 
to CHF phenotype information and the role of 
the decline in kidney function in the cycle of 
CHF (exemplified in Table 1), secondly to locate 
words that modify the entity (such as polarity 
clues) and thirdly to identify the types of 
relationships that exist between these entity types 
in the description of phenotype information 
(Table 2) .  
Secondly, medical terms in the records are 
mapped semi-automatically onto clinical 
concepts in UMLS, with the aid of MetaMap 
(Aronson, 2001). 
The same annotation schema and guidelines 
were used for both the discharge summaries and 
the scientific full articles. In the latter, certain 
annotations were omitted, i.e., organ entities, 
polarity clues and relations. This decision was 
taken due to the differing ways in which 
phenotype information is expressed in discharge 
summaries and scientific articles. In discharge 
summaries, phenotype information is explicitly 
described in the patient?s medical history, 
diagnoses and test results. On the other hand, 
scientific articles summarise results and research 
findings. This means that certain types of 
information that occur frequently in discharge 
summaries are extremely rare in scientific 
articles, such that their occurrences are too sparse 
to be useful in training TM systems, and hence 
they were not annotated. 
The annotation was carried out by two medical 
doctors, using the Brat Rapid Annotation Tool 
(brat) (Stenetorp et al., 2012), a highly-
configurable and flexible web-based tool for 
textual annotation.  
Annotations in the corpus should reflect the 
instructions provided in the guidelines as closely 
as possible, in order to ensure that the 
annotations are of ahigh quality. A standard 
means of providing evidence regarding the 
reliability of annotations in a corpus is to 
calculate a statistic known as the inter-annotator 
agreement (IAA). IAA provides assurance that 
different annotators can produce the same 
annotations when working independently and 
separately. There are several different methods of 
calculating IAA, which can be influenced by the 
exact nature of the annotation task. We use the 
measures of precision, recall and F-measure to 
70
indicate the level of inter-annotator reliability 
(Hripcsak  &  Rothschild, 2005). In order to 
carry out such calculations, one set of 
annotations is considered as a gold standard and 
the total number of correct entities is the total 
number of entities annotated by this annotator. 
Precision is the percentage of correct positive 
predictions annotated by the second annotator, 
compared to the first annotator?s assumed gold 
standard. It is calculated as follows: 
 
P = TP / TP + FP 
Recall is the percentage of positive cases 
recognised by the second annotator. It is 
calculated as follows: 
R = TP / TP + FN 
F-score is the harmonic mean between 
precision and recall. 
 F-score =  
2* (Precision * Recall) / Precision + Recall 
 We have calculated separate IAA scores for 
the discharge summaries and the scientific 
articles. Table 3 summarises agreement rates for 
term annotation in the discharge summaries, 
showing results for both individual entity types 
and macro-averaged scores over all entity types. 
Relaxed matching criteria were employed, such 
that annotations added by the two annotators 
were considered as a match if their spans 
overlapped. In comparison to related efforts, the 
IAA rates shown in Table 3 are high.  However, 
it should be noted that the number of targeted 
classes and relations in our corpus is small and 
focused, compared to other related corpora.   
Agreement statistics for scientific articles are 
shown in Table 4. Agreement is somewhat lower 
than for discharge summaries, which this could 
be due to the fact that the annotators (doctors) 
are more used to dealing with discharge 
summaries in their day-to-day work, and so are 
more accustomed to locating information in this 
type of text. Scientific articles are much longer 
and generally include more complex language, 
ideas and analyses, which may require more than 
one reading to fully comprehend the information 
within them. Table 5 shows the agreement rates 
for relation annotation in the discharge 
summaries. The agreement rates for relationships 
are relatively high. This can partly be explained 
by the deep domain knowledge possessed by the 
annotators and partly by the fact that the 
relationships to be identified were relatively 
simple, linking only two pre-annotated entities.
Table 1. Annotated phenotype entity classes 
 
Entity Type Description Example 
Cause any medical problem that 
contributes to the occurrence of 
CHF 
 
 
Risk factors A condition that increases the 
chance of a patient having the 
CHF disease 
 
 
Sign & 
symptom 
any observable manifestation 
of a disease which is 
experienced by a patient and 
reported to the physician 
 
 
Non-
traditional 
risk factor 
Conditions  associated with 
abnormalities in kidney 
functions that put the patient at 
higher risk of developing 
?signs & symptoms? and 
causes of CHF 
 
 
Organ Any body part 
 
71
Relation 
Type 
Description Example 
Causality This relationship links two 
concepts in cases in which 
one concept causes the 
other to occur. 
 
 
 
Finding This relationship links the 
organ to the manifestation 
or abnormal variation that 
is observed during the 
diagnosis process. 
 
 
Negate This is one-way relation to 
relate a negation attribute 
(polarity clue) to the 
condition it negates. 
 
 
Table 2. Description of Annotated Relations 
Table 3. Term annotation agreement statistics for discharge summaries 
Table 4. Overall agreement statistics for terms annotation in scientific articles 
 Causality Finding Negate Macro-
average 
F-score 0.86 0.94 0.95 0.91 
Table 5. Relation annotation and agreement statistics for discharge summaries 
5 Conclusion 
This paper has described the creation of a new 
annotated corpus to facilitate the customisation 
of TM tools for the clinical domain. The corpus1 
consists of 300 discharge summaries and 5 full-
text articles from the literature, annotated for 
CHF phenotype information, including causes, 
risk factors, sign & symptoms and non- 
traditional risk factors. Discharge summaries 
have also been annotated with relationships 
holding between pairs of annotated entities. A 
total 7236 of entities and 1181 relationships have 
been annotated. Extracting phenotype 
                                                          
1 Guidelines and stand-off annotation are publicly available 
at https://code.google.com/p/phenochf-
corpus/source/browse/trunk 
information can have a major impact on our 
deeper understanding of disease ethology, 
treatment and prevention (Xu et al., 2013). 
Currently we are working on confirming the 
utility of the annotated corpus in training and 
customising TM tools, i.e., adapting different 
sequence tagging algorithms (such as 
Conditional Random Fields (CRF) and Hidden 
Markov Model (HMM)) to extract 
comprehensive clinical information from both 
discharge summaries and scientific articles.
 
 Causality Risk 
factor 
Sign & 
Symptom 
Non-
traditional 
risk factor 
Polarity 
clue 
Organ Macro-
average 
F-score 0.95 0.94 0.97 0.83 0.94 0.92 0.92 
 Cause Risk factor Sign & 
Symptoms 
Non-
traditional 
risk factor 
Macro-average 
F-score 0.82 0.84 0.82 .77 0.81 
72
References 
MEDLINE citation counts by year of publication. 
Aronson, A.R. (2001). Effective mapping of 
biomedical text to the UMLS Metathesaurus: the 
MetaMap program. Proceedings of the AMIA 
Symposium, American Medical Informatics 
Association. 
Hakenberg, J., Plake, C., Leser, U., Kirsch, H. and 
Rebholz-Schuhmann, D. (2005). LLL?05 
challenge: Genic interaction extraction-
identification of language patterns based on 
alignment and finite state automata. Proceedings 
of the 4th Learning Language in Logic workshop 
(LLL05). 
Hripcsak, G. and Rothschild, A.S. (2005). Agreement, 
the f-measure, and reliability in information 
retrieval. Journal of the American Medical 
Informatics Association, 12(3): 296-298. 
Jensen, P.B., Jensen, L.J. and Brunak, S. (2012). 
Mining electronic health records: towards better 
research applications and clinical care. Nature 
Reviews Genetics, 13(6): 395-405. 
Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus 
annotation for mining biomedical events from 
literature. BMC Bioinformatics, 9(10). 
Kulick, S., Bies, A., Liberman, M., Mandel, M., 
McDonald, R., Palmer, M., Schein, A., Ungar, L., 
Winters, S. and White, P. (2004). Integrated 
annotation for biomedical information extraction. 
Proc. of the Human Language Technology 
Conference and the Annual Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (HLT/NAACL). 
Meystre, S. and Haug, P.J. (2006). Natural language 
processing to extract medical problems from 
electronic clinical documents: performance 
evaluation. Journal of Biomedical Informatics, 
39(6): 589-599. 
Ogren, P.V., Savova, G.K. and Chute, C.G. (2008). 
Constructing Evaluation Corpora for Automated 
Clinical Named Entity Recognition. LREC. 
Patrick, J., Wang, Y. and Budd, P. (2006). Automatic 
Mapping Clinical Notes to Medical 
Terminologies. Australasian Language 
Technology Workshop. 
Perez-Rey, D., Jimenez-Castellanos, A., Garcia-
Remesal, M., Crespo, J. and Maojo, V. (2012). 
CDAPubMed: a browser extension to retrieve 
EHR-based biomedical literature. BMC Medical 
Informatics and Decision Making, 12(1): 29. 
Pestian, J.P., Brew, C., Matykiewicz, P., Hovermale, 
D., Johnson, N., Cohen, K.B. and Duch, W. 
(2007). A shared task involving multi-label 
classification of clinical free text. Proceedings of 
the Workshop on BioNLP 2007: Biological, 
Translational, and Clinical Language Processing, 
Association for Computational Linguistics. 
Pyysalo, S., Ginter, F., Heimonen, J., Bj?rne, J., 
Boberg, J., J?rvinen, J. and Salakoski, T. (2007). 
BioInfer: a corpus for information extraction in 
the biomedical domain. BMC Bioinformatics, 
8(1): 50. 
Roberts, A., Gaizauskas, R., Hepple, M., Demetriou, 
G., Guo, Y., Setzer, A. and Roberts, I. (2008). 
Semantic annotation of clinical text: The CLEF 
corpus. Proceedings of the LREC 2008 workshop 
on building and evaluating resources for 
biomedical text mining. 
Roberts, A., Gaizauskas, R., Hepple, M., Demetriou, 
G., Guo, Y., Roberts, I. and Setzer, A. (2009). 
Building a semantically annotated corpus of 
clinical texts. Journal of Biomedical Informatics, 
42(5): 950-966. 
Savova, G.K., Masanz, J.J., Ogren, P.V., Zheng, J., 
Sohn, S., Kipper-Schuler, K.C. and Chute, C.G. 
(2010). Mayo clinical Text Analysis and 
Knowledge Extraction System (cTAKES): 
architecture, component evaluation and 
applications. Journal of the American Medical 
Informatics Association, 17(5): 507-513. 
Stenetorp, P., Pyysalo, S., Topi?, G., Ohta, T., 
Ananiadou, S. and Tsujii, J.i. (2012). BRAT: a 
web-based tool for NLP-assisted text annotation. 
Proceedings of the Demonstrations at the 13th 
Conference of the European Chapter of the 
Association for Computational Linguistics, 
Association for Computational Linguistics. 
Suominen, H., Salanter?, S., Velupillai, S., Chapman, 
W.W., Savova, G., Elhadad, N., Pradhan, S., 
South, B.R., Mowery, D.L. and Jones, G.J. (2013). 
Overview of the ShARe/CLEF eHealth Evaluation 
Lab 2013. Information Access Evaluation. 
Multilinguality, Multimodality, and Visualization, 
Springer: 212-231. 
Tanabe, L., Xie, N., Thom, L.H., Matten, W. and 
Wilbur, W.J. (2005). GENETAG: a tagged corpus 
for gene/protein named entity recognition. BMC 
Bioinformatics, 6(Suppl 1): S3. 
Thompson, P., Iqbal, S., McNaught, J. and 
Ananiadou, S. (2009). Construction of an 
annotated corpus to support biomedical 
information extraction. BMC Bioinformatics, 
10(1): 349. 
Uzuner, ?., Goldstein, I., Luo, Y. and Kohane, I. 
(2008). Identifying patient smoking status from 
medical discharge records. Journal of the 
American Medical Informatics Association, 15(1): 
14-24. 
73
Uzuner, ?. (2009). Recognizing obesity and 
comorbidities in sparse data. Journal of the 
American Medical Informatics Association, 16(4): 
561-570. 
Uzuner, ?., South, B.R., Shen, S. and DuVall, S.L. 
(2011). 2010 i2b2/VA challenge on concepts, 
assertions, and relations in clinical text. Journal of 
the American Medical Informatics Association, 
18(5): 552-556. 
Xia, F. and Yetisgen-Yildiz, M. (2012). Clinical 
corpus annotation: challenges and strategies. 
Proceedings of the Third Workshop on Building 
and Evaluating Resources for Biomedical Text 
Mining (BioTxtM'2012) in conjunction with the 
International Conference on Language Resources 
and Evaluation (LREC), Istanbul, Turkey. 
Xu, R., Li, L. and Wang, Q. (2013). Towards 
building a disease-phenotype knowledge base: 
extracting disease-manifestation relationship from 
literature. Bioinformatics, 29(17): 2186-2194. 
 
 
 
 
74
