Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80?86,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The CMU Submission for the Shared Task on Language Identification in
Code-Switched Data
Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{chuchenl,wammar,lsl,cdyer}@cs.cmu.edu
Abstract
We describe the CMU submission for
the 2014 shared task on language iden-
tification in code-switched data. We
participated in all four language pairs:
Spanish?English, Mandarin?English,
Nepali?English, and Modern Standard
Arabic?Arabic dialects. After describing
our CRF-based baseline system, we
discuss three extensions for learning from
unlabeled data: semi-supervised learning,
word embeddings, and word lists.
1 Introduction
Code switching (CS) occurs when a multilingual
speaker uses more than one language in the same
conversation or discourse. Automatic idenefica-
tion of the points at which code switching occurs
is important for two reasons: (1) to help sociolin-
guists analyze the frequency, circumstances and
motivations related to code switching (Gumperz,
1982), and (2) to automatically determine which
language-specific NLP models to use for analyz-
ing segments of text or speech.
CS is pervasive in social media due to its in-
formal nature (Lui and Baldwin, 2014). The first
workshop on computational approaches to code
switching in EMNLP 2014 organized a shared task
(Solorio et al., 2014) on identifying code switch-
ing, providing training data of multilingual tweets
with token-level language-ID annotations. See
?2 for a detailed description of the shared task.
This short paper documents our submission in the
shared task.
We note that constructing a CS data set that is
annotated at the token level requires remarkable
manual effort. However, collecting raw tweets is
easy and fast. We propose leveraging both labeled
and unlabeled data in a unified framework; condi-
tional random field autoencoders (Ammar et al.,
2014). The CRF autoencoder framework consists
of an encoding model and a reconstruction model.
The encoding model is a linear-chain conditional
random field (CRF) (Lafferty et al., 2001) which
generates a sequence of labels, conditional on a
token sequence. Importantly, the parameters of
the encoding model can be interpreted in the same
way a CRF model would. This is in contrary to
generative model parameters which explain both
the observation sequence and the label sequence.
The reconstruction model, on the other hand, inde-
pendently generates the tokens conditional on the
corresponding labels. Both labeled and unlabeled
data can be efficiently used to fit parameters of this
model, minimizing regularized log loss. See ?4.1
for more details.
After modeling unlabeled token sequences, we
explore two other ways of leveraging unlabeled
data: word embeddings and word lists. The word
embeddings we use capture monolingual distribu-
tional similarities and therefore may be indicative
of a language (see ?4.2). A word list, on the other
hand, is a collection of words which have been
manually or automatically constructed and share
some property (see ?4.3). For example, we extract
the set of surface forms in monolingual corpora.
In ?5, we describe the experiments and discuss
results. According to the results, modeling unla-
beled data using CRF autoencoders did not im-
prove prediction accuracy. Nevertheless, more ex-
periments need to be run before we can conclude
this setting. On the positive side, word embed-
dings and word lists have been shown to improve
CS prediction accuracy, provided they have decent
coverage of tokens in the test set.
2 Task Description
The shared task training data consists of code?
switched tweets with token-level annotations.
The data is organized in four language pairs:
English?Spanish (En-Es), English?Nepali (En-
80
Ne), Mandarin?English (Zh-En) and Modern
Standard Arabic?Arabic dialects (MSA-ARZ).
Table 1 shows the size of the data sets provided
for the shared task in each language pair.
For each tweet in the data set, the user ID, tweet
ID, and a list of tokens? start offset and end offset
are provided. Each token is annotated with one
of the following labels: lang1, lang2, ne (i.e.,
named entities), mixed (i.e., mixed parts of lang1
and lang2), ambiguous (i.e., cannot be identified
given context), and other.
Two test sets were used to evaluate each sub-
mission for the shared task in each language pair.
The first test set consists of Tweets, similar to the
training set. The second test set consists of token
sequences from a surprise genre. Since partici-
pants were not given the test sets, we only report
results on a Twitter test set (a subset of the data
provided for shared task participants). Statistics
of our train/test data splits are given in Table 5.
lang. pair split tweets tokens users
En?Ne all 9, 993 146, 053 18
train 7, 504 109, 040 12
test 2, 489 37, 013 6
En?Es all 11, 400 140, 738 9
train 7, 399 101, 451 6
test 4, 001 39, 287 3
Zh?En all 994 17, 408 995
train 662 11, 677 663
test 332 5, 731 332
MSA?ARZ all 5, 862 119, 775 7
train 4, 800 95, 352 6
test 1, 062 24, 423 1
Table 1: Total number of tweets, tokens, and Twit-
ter user IDs for each language pair. For each lan-
guage pair, the first line represents all data pro-
vided to shared task participants. The second and
third lines represent our train/test data split for the
experiments reported in this paper. Since Twit-
ter users are allowed to delete their tweets, the
number of tweets and tokens reported in the third
and fourth columns may be less than the number
of tweets and tokens originally annotated by the
shared task organizers.
3 Baseline System
We model token-level language ID as a sequence
of labels using a linear-chain conditional ran-
dom field (CRF) (Lafferty et al., 2001) described
in ?3.1 with the features in ?3.2.
3.1 Model
A linear-chain CRF models the conditional proba-
bility of a label sequence y given a token sequence
x and given extra context ?, as follows:
p(y | x,?) =
exp?
>
?
|x|
i=1
f(x, y
i
, y
i?1
,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i
, y
?
i?1
,?)
where ? is a vector of feature weights, and f is
a vector of local feature functions. We use ? to
explicitly represent context information necessary
to compute the feature functions described below.
In a linear-chain structure, y
i
only depends on
observed variables x,? and the neighboring labels
y
i?1
and y
i+1
. Therefore, we can use dynamic
programming to do inference in run time that is
quadratic in the number of unique labels and lin-
ear in the sequence length. We use L-BFGS to
learn the feature weights ?, maximizing the L
2
-
regularized log-likelihood of labeled examples L:
``
supervised
(?) =
c
L
2
||?||
2
2
+
?
?x,y??L
log p(y | x,?)
After training the model, we use again use dy-
namic programming to find the most likely label
sequence, for each token sequence in the test set.
3.2 Features
We use the following features in the baseline sys-
tem:
? character n-grams (loweredcased tri- and quad-
grams)
? prefixes and suffixes of lengths 1, 2, 3 and 4
? unicode page of the first character
1
? case (first-character-uppercased vs. all-
characters-uppercased vs. all-characters-
alphanumeric)
? tweet-level language ID predictions from two
off-the-shelf language identifiers: cld2
2
and
ldig
3
1
http://www.unicode.org/charts/
2
https://code.google.com/p/cld2/
3
https://github.com/shuyo/ldig
81
encod
ing 
recon
struc
tion 
x 
yi-1 yi yi+1 
xi-1 xi xi+1 ? ? ? 
? 
Figure 1: A diagram of the CRF autoencoder
4 Using Unlabeled Data
In ?3, we learn the parameters of the CRF model
parameters in a standard fully supervised fashion,
using labeled examples in the training set. Here,
we attempt to use unlabeled examples to improve
our system?s performance in three ways: model-
ing unlabeled token sequences in the CRF autoen-
coder framework, word embeddings, and word
lists.
4.1 CRF Autoencoders
A CRF autoencoder (Ammar et al., 2014) consists
of an input layer, an output layer, and a hidden
layer. Both input and output layer represent the
observed token sequence. The hidden layer rep-
resents the label sequence. Fig. 1 illustrates the
model dependencies for sequence labeling prob-
lems with a first-order Markov assumption. Con-
ditional on an observation sequence x and side in-
formation ?, a traditional linear-chain CRF model
is used to generate the label sequence y. The
model then generates x? which represents a recon-
struction of the original observation sequence. El-
ements of this reconstruction (i.e., x?
i
) are then in-
dependently generated conditional on the corre-
sponding label y
i
using simple categorical distri-
butions.
The parametric form of the model is given by:
p(y, x? | x,?) =
|x|
?
i=1
?
x?
i
|y
i
?
exp?
>
?
|x|
i=1
f(x, y
i?1
, y
i
, i,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i?1
, y
?
i
, i,?)
where ? is a vector of CRF feature weights, f is a
vector of local feature functions (we use the same
features described in ?3.2), and ?
x?
i
|y
i
are categor-
ical distribution parameters of the reconstruction
model representing p(x?
i
| y
i
).
We can think of a label sequence as a low-
cardinality lossy compression of the correspond-
ing token sequence. CRF autoencoders explic-
itly model this intuition by creating an information
bottleneck where label sequences are required to
regenerate the same token sequence despite their
limited capacity. Therefore, when only unlabeled
examples U are available, we train CRF autoen-
coders by maximizing the regularized likelihood
of generating reconstructions x?, conditional on x,
marginalizing values of label sequences y:
``
unsupervised
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)
where R
Dirichlet
is a regularizer based on a vari-
ational approximation of a symmetric Dirichlet
prior with concentration parameter ? for the re-
construction parameters ?.
Having access to labeled examples, it is easy to
modify this objective to learn from both labeled
and unlabeled examples as follows:
``
semi
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
c
unlabeled
?
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)+
c
labeled
?
?
?x,y??L
log p(y | x)
We use block coordinate descent to optimize
this objective. First, we use c
em
iterations of
the expectation maximization algorithm to opti-
mize the ?-block while the ?-block is fixed, then
we optimize the ?-block with c
lbfgs
iterations of
L-BFGS (Liu et al., 1989) while the ?-block is
fixed.
4
4.2 Unsupervised Word Embeddings
For many NLP tasks, using unsupervised
word representations as features improves
accuracy (Turian et al., 2010). We use
word2vec (Mikolov et al., 2013) to train
100?dimensional word embeddings from a
large Twitter corpus of about 20 million tweets
extracted from the live stream, in multiple lan-
guages. We define an additional feature function
4
An open source efficient c++ imple-
mentation of our method can be found at
https://github.com/ldmt-muri/alignment-with-openfst
82
in the CRF autoencoder model ?4.1 for each of
the 100 dimensions, conjoined with the label y
i
.
The feature value is the corresponding dimension
for x
i
. A binary feature indicating the absence of
word embeddings is fired for out-of-vocabulary
words (i.e., words for which we do not have word
embeddings). The token-level coverage of the
word embeddings for each of the languages or
dialects used in the training data is reported in
Table 2.
4.3 Word List Features
While some words are ambiguous, many words
frequently occur in only one of the two lan-
guages being considered. An easy way to iden-
tify the label of such unambiguous words is to
check whether they belong to the vocabulary of
either language. Moreover, named entity recog-
nizers typically rely on gazetteers of named enti-
ties to improve their performance. We generalize
the notion of using monolingual vocabularies and
gazetteers of named entities to general word lists.
Using K word lists {l
1
, . . . , l
K
}, when a token x
i
is labeled with y
i
, we fire a binary feature that con-
joins ?y
i
, ?(x
i
? l
1
), . . . , ?(x
i
? l
K
)?, where ? is
an indicator boolean function. We use the follow-
ing word lists:
? Hindi and Nepali Wikipedia article titles
? multilingual named entities from the JRC
dataset
5
and CoNLL 2003 shared task
? word types in monolingual corpora in MSA,
ARZ, En and Es.
? set difference between the following pairs of
word lists: MSA-ARZ, ARZ-MSA, En-Es, Es-
En.
Transliteration from Devanagari The Nepali?
English tweets in the dataset are romanized. This
renders our Nepali word lists, which are based
on the Devanagari script, useless. Therefore, we
transliterate the Hindi and Nepali named entities
lists using a deterministic phonetic mapping. We
romanize the Devanagari words using the IAST
scheme.
6
We then drop all accent marks on the
characters to make them fit into the 7?bit ASCII
range.
5
http://datahub.io/dataset/jrc-names
6
http://en.wikipedia.org/wiki/
International_Alphabet_of_Sanskrit_
Transliteration
embeddings word lists
language coverage coverage
ARZ 30.7% 68.8%
En 73.5% 55.7%
MSA 26.6% 76.8%
Ne 14.5% 77.0%
Es 62.9% 78.0%
Zh 16.0% 0.7%
Table 2: The type-level coverage of annotated data
according to word embeddings (second column)
and according to word lists (third column), per lan-
guage.
5 Experiments
We compare the performance of five models for
each language pair, which correspond to the five
lines in Table 3. The first model, ?CRF? is the
baseline model described in ?3. The second ?CRF
+ U
test
? and the third ?CRF + U
all
? are CRF au-
toencoder models (see ?4.1) with two sets of un-
labeled data: (1) U
test
which only includes the test
set,
7
and (2) U
all
which includes the test set as well
as all tweets by the set of users who contributed
any tweets in L. The fourth model ?CRF + U
all
+
emb.? is a CRF autoencoder which uses word em-
bedding features (see ?4.2), as well as the features
described in ?3.2. Finally, the fifth model ?CRF +
U
all
+ emb. + lists? further adds word list features
(see ?4.3). In all but the ?CRF? model, we adopt a
transductive learning setup.
Since the CRF baseline is used as the encoding
part of the CRF autoencoder model, we use the
supervisedly-trained CRF parameters to initialize
the CRF autoencoder models. The categorical dis-
tributions of the reconstruction model are initial-
ized with discrete uniforms. We set the weight
of the labeled data log-likelihood c
labeled
= 0.5,
the weight of the unlabeled data log-likelihood
c
unlabeled
= 0.5, the L
2
regularization strength
c
L
2
= 0.3, the concentration parameter of the
Dirichlet prior ? = 0.1, the number of L-BFGS
iterations c
LBFGS
= 4, and the number of EM iter-
ations c
EM
= 4.
8
We stop training after 50 itera-
tions of block coordinate descent.
7
U
test
is potentially useful when the test set belongs to a
different domain than the labeled examples, which is often
referred to as ?domain adaptation?. However we were unable
to test this hypothesis since all the CS annotations we had
access to are from Twitter.
8
Hyper-parameters c
L
2
and ? were tuned using cross-
validation. The remaining hyper-parameters were not tuned.
83
config En?Ne MSA?ARZ En?Es Zh?En
CRF 95.2% 80.5% 94.6% 94.9%
+T
test
95.2% 80.6% 94.6% 94.9%
+T
all
95.2% 80.7% 94.6% 94.9%
+emb. 95.3% 81.3% 95.1% 95.0%
+lists 97.0% 81.2% 96.7% 95.3%
Table 3: Token level accuracy results for each of
the four language pairs.
label predicted predicted
MSA ARZ
true MSA 93.9% 5.3%
true ARZ 32.1% 65.2%
Table 4: Confusion between MSA and ARZ in the
Baseline configuration.
Results. The CRF baseline results are reported
in the first line in Table 3. For three language
pairs, the overall token-level accuracy ranges be-
tween 94.6% and 95.2%. In the fourth language
pair, MSA-ARZ, the baseline accuracy is 80.5%
which indicates the relative difficulty of this task.
The second and third lines in Table 3 show the
results when we use CRF autoencoders with the
unlabeled test set (U
test
), and with all unlabeled
tweets (U
all
), respectively. While semi-supervised
learning did not hurt accuracy on any of the lan-
guages, it only resulted in a tiny increase in accu-
racy for the Arabic dialects task.
The fourth line in Table 3 extends the CRF au-
toencoder model (third line) by adding unsuper-
vised word embedding features. This results in
an improvement of 0.6% for MSA-ARZ, 0.5% for
En-Es, 0.1% for En-Ne and Zh-En.
The fifth line builds on the fourth line by adding
word list features. This results in an improvement
of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,
and degradation of 0.1% in MSA-ARZ.
Analysis and Discussion The baseline perfor-
mance in the MSA-ARZ task is considerably
lower than those of the other tasks. Table 4 illus-
trates how the baseline model confuses lang1 and
lang2 in the MSA-ARZ task. While the baseline
system correctly labels 93.9% of MSA tokens, it
only correctly labels 65.2% of ARZ tokens.
Although the reported semi-supervised results
did not significantly improve on the CRF baseline,
more work needs to be done in order to conclude
these results:
lang. pair |U
test
| |U
all
| |L|
En?Ne 2489 6230 7504
MSA?ARZ 1062 2520 4800
Zh?En 332 332 663
En?Es 4001 7177 7399
Table 5: Number of tweets inL, U
test
andU
all
used
for semi-supervised learning of CRF autoencoders
models.
? Use an out-of-domain test set where some adap-
tation to the test set is more promising.
? Vary the number of labeled examples |L| and
the number of unlabeled examples |U|. Table 5
gives the number of labeled and unlabeled ex-
amples used for training the model. It is pos-
sible that semi-supervised learning would have
been more useful with a smaller |L| and a larger
|U|.
? Tune c
labeled
and c
unlabeled
.
? Split the parameters ? into two subsets: ?
labeled
and ?
unlabeled
; where ?
labeled
are the parameters
which have a non-zero value for any input x in
L and ?
unlabeled
are the remaining parameters in
? which only have non-zero values with unla-
beled examples but not with the labeled exam-
ples.
? Use a richer reconstruction model.
? Reconstruct a transformation of the token se-
quences instead of their surface forms.
? Train a token-level language ID model trained
on a large number of languages, as opposed to
disambiguating only two languages at a time.
Word embeddings improve the results for all
language pairs, but the largest improvement is in
MSA-ARZ and En-Es. Looking into the word em-
beddings coverage of those languages (i.e., MSA,
ARZ, Es, En in Table 2), we find that they are bet-
ter covered than the other languages (Ne, Zh). We
conclude that further improvements on En-Ne and
Zh-En may be expected if they are better repre-
sented in the corpus used to learn word embed-
dings.
As for the word lists, the largest improvement
we get is the romanized word lists of Nepali,
which have a 77.0% coverage and improve the
accuracy by 1.7%. This shows that our translit-
erated word lists not only cover a lot of tokens,
and are also useful for language ID. The Spanish
84
Config lang1 lang2 ne
+lists 84.1% 76.5% 73.7%
-lists 84.2% 77.1% 71.5%
Table 6: F?Measures of two Arabic configura-
tions. lang1 is MSA. lang2 is ARZ.
word lists also have a wide coverage, improving
the overall accuracy by 1.6%. The overall accu-
racy of the Arabic dialects slightly degrades with
the addition of the word lists. Closer inspection
in table 6 reveals that it improves the F?Measure
of the named entities at the expense of both MSA
(lang1) and ARZ (lang2).
6 Related Work
Previous work on identifying languages in a mul-
tilingual document includes (Singh and Gorla,
2007; King and Abney, 2013; Lui et al., 2014).
Their goal is generally more about identifying the
languages that appear in the document than intra?
sentential CS points.
Previous work on computational models of
code?switching include formalism (Joshi, 1982)
and language models that encode syntactic con-
straints from theories of code?switching, such as
(Li and Fung, 2013; Li and Fung, 2014). These
require the existence of a parser for the languages
under consideration. Other work on prediction
of code?switching points, such as (Elfardy et al.,
2013; Nguyen and Dogruoz, 2013) and ours, do
not depend upon such NLP infrastructure. Both of
the aforementioned use basic character?level fea-
tures and dictionaries on sequence models.
7 Conclusion
We have shown that a simple CRF baseline with
a handful of feature templates obtains strong re-
sults for this task. We discussed three methods
to improve over the supervised baseline using un-
labeled data: (1) modeling unlabeleld data using
CRF autoencoders, (2) using pre-trained word em-
beddings, and (3) using word list features.
We show that adding word embedding features
and word lists features is useful when they have
good coverage of words in a data set. While mod-
est improvements are observed due to modeling
unlabeled data with CRF autoenocders, we iden-
tified possible directions to gain further improve-
ments.
While bilingual disambiguation was a good first
step for identifying code switching, we suggest a
reformulation of the task such that each label can
take on one of many languages.
Acknowledgments
We thank Brendan O?Connor who helped assem-
ble the Twitter dataset. We also thank the work-
shop organizers for their hard work, and the re-
viewers for their comments. This work was
sponsored by the U.S. Army Research Labora-
tory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533. The
statements made herein are solely the responsibil-
ity of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
John J. Gumperz. 1982. Discourse Strategies. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th Conference on Computational Linguistics -
Volume 1, COLING ?82, pages 145?150, Czechoslo-
vakia. Academia Praha.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.
Ying Li and Pascale Fung. 2013. Improved mixed lan-
guage speech recognition using asymmetric acous-
tic model and language model with code-switch in-
version constraints. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 7368?7372, May.
Ying Li and Pascale Fung. 2014. Code switch lan-
guage modeling with functional head constraint. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages
4913?4917, May.
85
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation of Computational Linguistics, 2:27?40.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Dong Nguyen and Seza A. Dogruoz. 2013. Word level
language identification in online multilingual com-
munication. Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 857?862. Association for Computational Lin-
guistics.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identi-
fication of languages and encodings in a multilingual
document. In Building and Exploring Web Corpora
(WAC3-2007): Proceedings of the 3rd Web as Cor-
pus Workshop, Incorporating Cleaneval, volume 4,
page 95.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
86
