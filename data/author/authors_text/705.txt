Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 229?232, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Applying spelling error correction techniques
for improving semantic role labelling
Erik Tjong Kim Sang
Informatics Institute
University of Amsterdam, Kruislaan 403
NL-1098 SJ Amsterdam, The Netherlands
erikt@science.uva.nl
Sander Canisius, Antal van den Bosch, Toine Bogers
ILK / Computational Linguistics and AI
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{S.V.M.Canisius,Antal.vdnBosch,
A.M.Bogers}@uvt.nl
1 Introduction
This paper describes our approach to the CoNLL-
2005 shared task: semantic role labelling. We do
many of the obvious things that can be found in the
other submissions as well. We use syntactic trees
for deriving instances, partly at the constituent level
and partly at the word level. On both levels we edit
the data down to only the predicted positive cases
of verb-constituent or verb-word pairs exhibiting a
verb-argument relation, and we train two next-level
classifiers that assign the appropriate labels to the
positively classified cases. Each classifier is trained
on data in which the features have been selected to
optimize generalization performance on the particu-
lar task. We apply different machine learning algo-
rithms and combine their predictions.
As a novel addition, we designed an automatically
trained post-processing module that attempts to cor-
rect some of the errors made by the base system.
To this purpose we borrowed Levenshtein-distance-
based correction, a method from spelling error cor-
rection to repair mistakes in sequences of labels. We
adapted the method to our needs and applied it for
improving semantic role labelling output. This pa-
per presents the results of our approach.
2 Data and features
The CoNLL-2005 shared task data sets provide sen-
tences in which predicate?argument relations have
been annotated, as well as a number of extra anno-
tations like named entities and full syntactic parses
(Carreras and Ma`rquez, 2005). We have used the
parses for generating machine learning instances for
pairs of predicates and syntactic phrases. In princi-
ple each phrase can have a relation with each verb
in the same sentence. However, in order to keep
the number of instances at a reasonable number, we
have only built instances for verb?phrase pairs when
the phrase parent is an ancestor of the verb (400,128
training instances). A reasonable number of ar-
guments are individual words; these do not match
with phrase boundaries. In order to be able to label
these, we have also generated instances for all pairs
of verbs and individual words using the same con-
straint (another 542,217 instances). The parent node
constraint makes certain that embedded arguments,
which do not occur in these data sets, cannot be pre-
dicted by our approach.
Instances which are associated with verb?
argument pairs receive the label of the argument as
class while others in principle receive a NULL class.
In an estimated 10% of the cases, the phrase bound-
aries assigned by the parser are different from those
in the argument annotation. In case of a mismatch,
we have always used the argument label of the first
word of a phrase as the class of the corresponding
instance. By doing this we attempt to keep the posi-
tional information of the lost argument in the train-
ing data. Both the parser phrase boundary errors as
well as the parent node constraint restrict the num-
ber of phrases we can identify. The maximum recall
score attainable with our phrases is 84.64% for the
development data set.
We have experimentally evaluated 30 features
based on the previous work in semantic role la-
belling (Gildea and Jurafsky, 2002; Pradhan et al,
2004; Xue and Palmer, 2004):
? Lexical features (5): predicate (verb), first
phrase word, last phrase word and words im-
mediately before and after the phrase.
? Syntactic features (14): part-of-speech tags
(POS) of: first phrase word, last phrase word,
229
word immediately before phrase and word im-
mediately after phrase; syntactic paths from
word to verb: all paths, only paths for words
before verb and only paths for words after verb;
phrase label, label of phrase parent, subcate-
gorisation of verb parent, predicate frame from
PropBank, voice, head preposition for preposi-
tional phrases and same parents flag.
? Semantic features (2): named entity tag for
first phrase word and last phrase word.
? Positional features (3): position of the phrase
with respect to the verb: left/right, distance in
words and distance in parent nodes.
? Combination features (6): predicate + phrase
label, predicate + first phrase word, predicate
+ last phrase word, predicate + first phrase
POS, predicate + last phrase POS and voice +
left/right.
The output of two parsers was available. We have
briefly experimented with the Collins parses includ-
ing the available punctuation corrections but found
that our approach reached a better performance with
the Charniak parses. We report only on the results
obtained with the Charniak parses.
3 Approach
This section gives a brief overview of the three main
components of our approach: machine learning, au-
tomatic feature selection and post-processing by a
novel procedure designed to clean up the classifier
output by correcting obvious misclassifications.
3.1 Machine learning
The core machine learning technique employed, is
memory-based learning, a supervised inductive al-
gorithm for learning classification tasks based on the
k-nn algorithm. We use the TiMBL system (Daele-
mans et al, 2003), version 5.0.0, patch-2 with uni-
form feature weighting and random tiebreaking (op-
tions: -w 0 -R 911). We have also evaluated two al-
ternative learning techniques. First, Maximum En-
tropy Models, for which we employed Zhang Le?s
Maximum Entropy Toolkit, version 20041229 with
default parameters. Second, Support Vector Ma-
chines for which we used Taku Kudo?s YamCha
(Kudo and Matsumoto, 2003), with one-versus-all
voting and option -V which enabled us to ignore pre-
dicted classes with negative distances.
3.2 Feature selection
In previous research, we have found that memory-
based learning is rather sensitive to the chosen fea-
tures. In particular, irrelevant or redundant fea-
tures may lead to reduced performance. In order
to minimise the effects of this sensitivity, we have
employed bi-directional hill-climbing (Caruana and
Freitag, 1994) for finding the features that were most
suited for this task. This process starts with an empty
feature set, examines the effect of adding or remov-
ing one feature and then starts a new iteration with
the set associated with the best performance.
3.3 Automatic post-processing
Certain misclassifications by the semantic role-
labelling system described so far lead to unlikely and
impossible relation assignments, such as assigning
two indirect objects to a verb where only one is pos-
sible. Our proposed classifier has no mechanism to
detect these errors. One solution is to devise a post-
processing step that transforms the resulting role as-
signments until they meet certain basic constraints,
such as the rule that each verb may have only sin-
gle instances of the different roles assigned in one
sentence (Van den Bosch et al, 2004).
We propose an alternative automatically-trained
post-processing method which corrects unlikely role
assignments either by deleting them or by replacing
them with a more likely one. We do not do this by
knowledge-based constraint satisfaction, but rather
by adopting a method for error correction based on
Levenshtein distance (Levenshtein, 1965), or edit
distance, as used commonly in spelling error correc-
tion. Levenshtein distance is a dynamically com-
puted distance between two strings, accounting for
the number of deletions, insertions, and substitu-
tions needed to transform the one string into the
other. Levenshtein-based error correction typically
matches a new, possibly incorrect, string to a trusted
lexicon of assumedly correct strings, finds the lex-
icon string with the smallest Levenshtein distance
to the new string, and replaces the new string with
the lexicon string as its likely correction. We imple-
mented a roughly similar procedure. First, we gener-
ated a lexicon of semantic role labelling patterns of
A0?A5 arguments of verbs on the basis of the entire
training corpus and the PropBank verb frames. This
230
lexicon contains entries such as abandon A0 V A1,
and categorize A1 V A2 ? a total of 43,033 variable-
length role labelling patterns.
Next, given a new test sentence, we consider all
of its verbs and their respective predicted role la-
bellings, and compare each with the lexicon, search-
ing the role labelling pattern with the same verb at
the smallest Levenshtein distance (in case of an un-
known verb we search in the entire lexicon). For
example, in a test sentence the pattern emphasize A0
V A1 A0 is predicted. One closest lexicon item is
found at Levenshtein distance 1, namely emphasize
A0 V A1, representing a deletion of the final A0. We
then use the nearest-neighbour pattern in the lexicon
to correct the likely error, and apply all deletions
and substitutions needed to correct the current pat-
tern according to the nearest-neighbour pattern from
the trusted lexicon. We do not apply insertions, since
the post-processor module does not have the infor-
mation to decide which constituent or word would
receive the inserted label. In case of multiple possi-
ble deletions (e.g. in deleting one out of two A1s in
emphasize A0 V A1 A1), we always delete the argu-
ment furthest from the verb.
4 Results
In order to perform the optimisation of the seman-
tic role labelling process in a reasonable amount of
time, we have divided it in four separate tasks: prun-
ing the data for individual words and the data for
phrases, and labelling of these two data sets. Prun-
ing amounts to deciding which instances correspond
with verb-argument pairs and which do not. This
resulted in a considerable reduction of the two data
sets: 47% for the phrase data and 80% for the word
data. The remaining instances are assumed to de-
fine verb-argument pairs and the labelling tasks as-
sign labels to them. We have performed a sepa-
rate feature selection process in combination with
the memory-based learner for each of the four tasks.
First we selected the best feature set based on task
accuracy. As soon as a working module for each of
the tasks was available, we performed an extra fea-
ture selection process for each of the modules, opti-
mising overall system F?=1 while keeping the other
three modules fixed.
The effect of the features on the overall perfor-
Words Phrases
Features prune label prune label
predicate -0.04 +0.05 -0.25 -0.52
first word +0.38 +0.16 -0.17 +1.14
last word ? ? -0.01 +1.12
previous word -0.06 +0.02 -0.05 +0.74
next word -0.04 -0.08 +0.44 -0.16
part-of-speech first word -0.01 -0.02 -0.07 -0.11
part-of-speech last word ? ? -0.14 -0.45
previous part-of-speech -0.12 -0.06 +0.22 -1.14
next part-of-speech -0.08 -0.12 -0.01 -0.21
all paths +0.42 +0.10 +0.84 +0.75
path before verb +0.00 -0.02 +0.00 +0.27
path after verb -0.01 -0.01 -0.01 -0.06
phrase label -0.01 -0.02 +0.13 -0.02
parent label +0.03 -0.02 -0.03 +0.00
voice +0.02 -0.04 -0.04 +1.85
subcategorisation -0.01 +0.00 -0.02 +0.03
PropBank frame -0.12 -0.03 -0.16 +1.04
PP head +0.00 +0.00 -0.06 +0.08
same parents -0.02 -0.01 +0.03 -0.05
named entity first word +0.00 +0.00 +0.05 -0.11
named entity last word ? ? -0.04 -0.12
absolute position +0.00 +0.00 +0.00 -0.02
distance in words +0.34 +0.04 +0.16 -0.96
distance in parents -0.02 -0.02 +0.06 -0.04
predicate + label -0.05 -0.07 -0.22 -0.47
predicate + first word -0.05 +0.00 +0.13 +0.97
predicate + last word ? ? -0.03 +0.08
predicate + first POS -0.05 -0.06 -0.20 -0.50
predicate + last POS ? ? -0.13 -0.40
voice + position +0.02 -0.04 -0.05 -0.04
Table 1: Effect of adding a feature to the best feature
sets when memory-based learning is applied to the
development set (overall F?=1). The process con-
sisted of four tasks: pruning data sets for individual
words and phrases, and labelling these two data sets.
Selected features are shown in bold. Unfortunately,
we have not been able to use all promising features.
mance can be found in Table 1. One feature (syntac-
tic path) was selected in all four tasks but in general
different features were required for optimal perfor-
mance in the four tasks. Changing the feature set
had the largest effect when labelling the phrase data.
We have applied the two other learners, Maximum
Entropy Models and Support Vector Machines to the
two labelling tasks, while using the same features as
the memory-based learner. The performance of the
three systems on the development data can be found
in Table 3. Since the systems performed differently
we have also evaluated the performance of a com-
bined system which always chose the majority class
assigned to an instance and the class of the strongest
system (SVM) in case of a three-way tie. The com-
bined system performed slightly better than the best
231
Precision Recall F?=1
Development 76.79% 70.01% 73.24
Test WSJ 79.03% 72.03% 75.37
Test Brown 70.45% 60.13% 64.88
Test WSJ+Brown 77.94% 70.44% 74.00
Test WSJ Precision Recall F?=1
Overall 79.03% 72.03% 75.37
A0 85.65% 81.73% 83.64
A1 76.97% 71.89% 74.34
A2 71.07% 58.20% 63.99
A3 69.29% 50.87% 58.67
A4 75.56% 66.67% 70.83
A5 100.00% 40.00% 57.14
AM-ADV 64.36% 51.38% 57.14
AM-CAU 75.56% 46.58% 57.63
AM-DIR 48.98% 28.24% 35.82
AM-DIS 81.88% 79.06% 80.45
AM-EXT 87.50% 43.75% 58.33
AM-LOC 62.50% 50.96% 56.15
AM-MNR 64.52% 52.33% 57.78
AM-MOD 96.76% 97.64% 97.20
AM-NEG 97.38% 96.96% 97.17
AM-PNC 45.98% 34.78% 39.60
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 80.52% 70.75% 75.32
R-A0 81.47% 84.38% 82.89
R-A1 74.00% 71.15% 72.55
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 100.00% 100.00% 100.00
R-AM-LOC 86.67% 61.90% 72.22
R-AM-MNR 33.33% 33.33% 33.33
R-AM-TMP 64.41% 73.08% 68.47
V 97.36% 97.36% 97.36
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
individual system.
5 Conclusion
We have presented a machine learning approach to
semantic role labelling based on full parses. We
have split the process in four separate tasks: prun-
ing the data bases of word-based and phrase-based
examples down to only the positive verb-argument
cases, and labelling the two positively classified data
sets. A novel automatic post-processing procedure
based on spelling correction, comparing to a trusted
lexicon of verb-argument patterns from the training
material, was able to achieve a performance increase
by correcting unlikely role assignments.
Learning algorithm Precision Recall F?=1
without post-processing:
Maximum Entropy Models 70.78% 70.03% 70.40
Memory-Based Learning 70.70% 69.85% 70.27
Support Vector Machines 75.07% 69.15% 71.98
including post-processing:
Maximum Entropy Models 74.06% 69.84% 71.89
Memory-Based Learning 73.84% 69.88% 71.80
Support Vector Machines 77.75% 69.11% 73.17
Combination 76.79% 70.01% 73.24
Table 3: Effect of the choice of machine learning
algorithm, the application of Levenshtein-distance-
based post-processing and the use of system combi-
nation on the performance obtained for the develop-
ment data set.
Acknowledgements
This research was funded by NWO, the Netherlands
Organisation for Scientific Research, and by Senter-
Novem IOP-MMI.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005. Ann Arbor, MI, USA.
R. Caruana and D. Freitag. 1994. Greedy attribute selection.
In Proceedings of the Eleventh International Conference on
Machine Learning, pages 28?36, New Brunswick, NJ, USA.
Morgan Kaufman.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2003. TiMBL: Tilburg memory based learner, ver-
sion 5.0, reference guide. ILK Technical Report 03-10,
Tilburg University.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
T. Kudo and Y. Matsumoto. 2003. Fast methods for kernel-
based text analysis. In Proceedings of ACL-2003. Sapporo,
Japan.
V. Levenshtein. 1965. Binary codes capable of correcting
deletions, insertions and reversals. Doklady Akademii Nauk
SSSR, 163(4):845?848.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky.
2004. Shallow semantic parsing using support vector ma-
chines. In Proceedings of the HLT/NAACL 2004. Boston,
MA.
A. van den Bosch, S. Canisius, W. Daelemans, I Hendrickx,
and E. Tjong Kim Sang. 2004. Memory-based semantic
role labeling: Optimizing features, algorithm, and output. In
Proceedings of the CoNLL-2004, Boston, MA, USA.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004. Barcelona,
Spain.
232
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 176?180, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing by Inference over High-recall Dependency Predictions
Sander Canisius, Toine Bogers,
Antal van den Bosch, Jeroen Geertzen
ILK / Computational Linguistics and AI
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{S.V.M.Canisius,A.M.Bogers,
Antal.vdnBosch,J.Geertzen}@uvt.nl
Erik Tjong Kim Sang
Informatics Institute
University of Amsterdam, Kruislaan 403
NL-1098 SJ Amsterdam, The Netherlands
erikt@science.uva.nl
1 Introduction
As more and more syntactically-annotated corpora
become available for a wide variety of languages,
machine learning approaches to parsing gain inter-
est as a means of developing parsers without having
to repeat some of the labor-intensive and language-
specific activities required for traditional parser de-
velopment, such as manual grammar engineering,
for each new language. The CoNLL-X shared task
on multi-lingual dependency parsing (Buchholz et
al., 2006) aims to evaluate and advance the state-of-
the-art in machine learning-based dependency pars-
ing by providing a standard benchmark set compris-
ing thirteen languages1. In this paper, we describe
two different machine learning approaches to the
CoNLL-X shared task.
Before introducing the two learning-based ap-
proaches, we first describe a number of baselines,
which provide simple reference scores giving some
sense of the difficulty of each language. Next, we
present two machine learning systems: 1) an ap-
proach that directly predicts all dependency relations
in a single run over the input sentence, and 2) a cas-
cade of phrase recognizers. The first approach has
been found to perform best and was selected for sub-
mission to the competition. We conclude this paper
with a detailed error analysis of its output for two of
the thirteen languages, Dutch and Spanish.
1The data sets were extracted from various existing tree-
banks (Hajic? et al, 2004; Simov et al, 2005; Simov and Osen-
ova, 2003; Chen et al, 2003; Bo?hmova? et al, 2003; Kromann,
2003; van der Beek et al, 2002; Brants et al, 2002; Kawata and
Bartels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006; Civit
Torruella and Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer
et al, 2003; Atalay et al, 2003)
2 Baseline approaches
Given the diverse range of languages involved in
the shared task, each having different characteristics
probably requiring different parsing strategies, we
developed four different baseline approaches for as-
signing labeled dependency structures to sentences.
All of the baselines produce strictly projective struc-
tures. While the simple rules implementing these
baselines are insufficient for achieving state-of-the-
art performance, they do serve a useful role in giving
a sense of the difficulty of each of the thirteen lan-
guages. The heuristics for constructing the trees and
labeling the relations used by each of the four base-
lines are described below.
Binary right-branching trees The first baseline
produces right-branching binary trees. The first to-
ken in the sentence is marked as the top node with
HEAD 0 and DEPREL ROOT. For the rest of the
tree, token n ? 1 serves as the HEAD of token n.
Figure 1 shows an example of the kind of tree this
baseline produces.
Binary left-branching trees The binary left-
branching baseline mirrors the previous baseline.
The penultimate token in the sentence is marked as
the top node with HEAD 0 and DEPREL ROOT
since punctuation tokens can never serve as ROOT2.
For the rest of the tree, the HEAD of token n is token
n+1. Figure 2 shows an example of a tree produced
by this baseline.
2We simply assume the final token in the sentence to be
punctuation.
176
Inward-branching trees In this approach, the
first identified verb3 is marked as the ROOT node.
The part of the sentence to the left of the ROOT is
left-branching, the part to the right of the ROOT is
right-branching. Figure 3 shows an example of a
tree produced by this third baseline.
Nearest neighbor-branching trees In our most
complex baseline, the first verb is marked as the
ROOT node and the other verbs (with DEPREL vc)
point to the closest preceding verb. The other to-
kens point in the direction of their nearest neighbor-
ing verb, i.e. the two tokens at a distance of 1 from
a verb have that verb as their HEAD, the two tokens
at a distance of 2 have the tokens at a distance of 1
as their head, and so on until another verb is a closer
neighbor. In the case of ties, i.e. tokens that are
equally distant from two different verbs, the token is
linked to the preceding token. Figure 4 clarifies this
kind of dependency structure in an example tree.
verb verb punct
ROOT
Figure 1: Binary right-branching tree for an example
sentence with two verbs.
verb verb punct
ROOT
Figure 2: Binary left-branching tree for the example
sentence.
verb verb punct
ROOT
Figure 3: Binary inward-branching tree for the ex-
ample sentence.
3We consider a token a verb if its CPOSTAG starts with a
?V?. This is an obviously imperfect, but language-independent
heuristic choice.
ROOT
verb verb punct
Figure 4: Nearest neighbor-branching tree for the
example sentence.
Labeling of identified relations is done using a
three-fold back-off strategy. From the training set,
we collect the most frequent DEPREL tag for each
head-dependent FORM pair, the most frequent DE-
PREL tag for each FORM, and the most frequent
DEPREL tag in the entire training set. The rela-
tions are labeled in this order: first, we look up if the
FORM pair of a token and its head was present in
the training data. If not, then we assign it the most
frequent DEPREL tag in the training data for that
specific token FORM. If all else fails we label the
token with the most frequent DEPREL tag in the en-
tire training set (excluding punct4 and ROOT).
language baseline unlabeled labeled
Arabic left 58.82 39.72
Bulgarian inward 41.29 29.50
Chinese NN 37.18 25.35
Czech NN 34.70 22.28
Danish inward 50.22 36.83
Dutch NN 34.07 26.87
German NN 33.71 26.42
Japanese right 67.18 64.22
Portuguese right 25.67 22.32
Slovene right 24.12 19.42
Spanish inward 32.98 27.47
Swedish NN 34.30 21.47
Turkish right 49.03 31.85
Table 1: The labeled and unlabeled scores for the
best performing baseline for each language (NN =
nearest neighbor-branching).
The best baseline performance (labeled and un-
labeled scores) for each language is listed in Table
1. There was no single baseline that outperformed
the others on all languages. The nearest neighbor
baseline outperformed the other baselines on five
of the thirteen languages. The right-branching and
4Since the evaluation did not score on punctuation.
177
inward-branching baselines were optimal on four
and three languages respectively. The only language
where the left-branching trees provide the best per-
formance is Arabic.
3 Parsing by inference over high-recall
dependency predictions
In our approach to dependency parsing, a machine
learning classifier is trained to predict (directed) la-
beled dependency relations between a head and a de-
pendent. For each token in a sentence, instances are
generated where this token is a potential dependent
of each of the other tokens in the sentence5. The
label that is predicted for each classification case
serves two different purposes at once: 1) it signals
whether the token is a dependent of the designated
head token, and 2) if the instance does in fact corre-
spond to a dependency relation in the resulting parse
of the input sentence, it specifies the type of this re-
lation, as well.
The features we used for encoding instances for
this classification task correspond to a rather simple
description of the head-dependent pair to be clas-
sified. For both the potential head and dependent,
there are features encoding a 2-1-2 window of words
and part-of-speech tags6; in addition, there are two
spatial features: a relative position feature, encoding
whether the dependent is located to the left or to the
right of its potential head, and a distance feature that
expresses the number of tokens between the depen-
dent and its head.
One issue that may arise when considering each
potential dependency relation as a separate classifi-
cation case is that inconsistent trees are produced.
For example, a token may be predicted to be a de-
pendent of more than one head. To recover a valid
dependency tree from the separate dependency pre-
dictions, a simple inference procedure is performed.
Consider a token for which the dependency relation
is to be predicted. For this token, a number of clas-
sification cases have been processed, each of them
5To prevent explosion of the number of classification cases
to be considered for a sentence, we restrict the maximum dis-
tance between a token and its potential head. For each language,
we selected this distance so that, on the training data, 95% of the
dependency relations is covered.
6More specifically, we used the part-of-speech tags from the
POSTAG column of the shared task data files.
indicating whether and if so how the token is related
to one of the other tokens in the sentence. Some of
these predictions may be negative, i.e. the token is
not a dependent of a certain other token in the sen-
tence, others may be positive, suggesting the token
is a dependent of some other token.
If all classifications are negative, the token is as-
sumed to have no head, and consequently no depen-
dency relation is added to the tree for this token; the
node in the dependency tree corresponding to this
token will then be an isolated one. If one of the clas-
sifications is non-negative, suggesting a dependency
relation between this token as a dependent and some
other token as a head, this dependency relation is
added to the tree. Finally, there is the case in which
more than one prediction is non-negative. By defi-
nition, at most one of these predictions can be cor-
rect; therefore, only one dependency relation should
be added to the tree. To select the most-likely can-
didate from the predicted dependency relations, the
candidates are ranked according to the classification
confidence of the base classifier that predicted them,
and the highest-ranked candidate is selected for in-
sertion into the tree.
For our base classifier we used a memory-based
learner as implemented by TiMBL (Daelemans et
al., 2004). In memory-based learning, a machine
learning method based on the nearest-neighbor rule,
the class for a given test instance is predicted by per-
forming weighted voting over the class labels of a
certain number of most-similar training instances.
As a simple measure of confidence for such a pre-
diction, we divide the weight assigned to the major-
ity class by the total weight assigned to all classes.
Though this confidence measure is a rather ad-hoc
one, which should certainly not be confused with
any kind of probability, it tends to work quite well
in practice, and arguably did so in the context of
this study. The parameters of the memory-based
learner have been optimized for accuracy separately
for each language on training and development data
sampled internally from the training set.
The base classifier in our parser is faced with a
classification task with a highly skewed class dis-
tribution, i.e. instances that correspond to a depen-
dency relation are largely outnumbered by those that
do not. In practice, such a huge number of nega-
tive instances usually results in classifiers that tend
178
to predict fairly conservatively, resulting in high pre-
cision, but low recall. In the approach introduced
above, however, it is better to have high recall, even
at the cost of precision, than to have high precision at
the cost of recall. A missed relation by the base clas-
sifier can never be recovered by the inference proce-
dure; however, due to the constraint that each token
can only be a dependent of one head, excessive pre-
diction of dependency relations can still be corrected
by the inference procedure. An effective method for
increasing the recall of a classifier is down-sampling
of the training data. In down-sampling, instances
belonging to the majority class (in this case the neg-
ative class) are removed from the training data, so
as to obtain a more balanced distribution of negative
and non-negative instances.
Figure 5 shows the effect of systematically re-
moving an increasingly larger part of the negative in-
stances from the training data. First of all, the figure
confirms that down-sampling helps to improve re-
call, though it does so at the cost of precision. More
importantly however, it also illustrates that this im-
proved recall is beneficial for the performance of the
dependency parser. The shape of the performance
curve of the dependency parser closely follows that
of the recall. Remarkably, parsing performance con-
tinues to improve with increasingly stronger down-
sampling, even though precision drops considerably
as a result of this. This shows that the confidence
of the classifier for a certain prediction is a suffi-
ciently reliable indication of the quality of that pre-
diction for fixing the over-prediction of dependency
relations. Only when the number of negative train-
ing instances is reduced to equal the number of pos-
itive instances, the performance of the parser is neg-
atively affected. Based on a quick evaluation of var-
ious down-sampling ratios on a 90%-10% train-test
split of the Dutch training data, we decided to down-
sample the training data for all languages with a ratio
of two negative instances for each positive one.
Table 2 lists the unlabeled and labeled attachment
scores of the resulting system for all thirteen lan-
guages.
4 Cascaded dependency parsing
One of the alternative strategies explored by us was
modeling the parsing process as a cascaded pair of
 0
 20
 40
 60
 80
 100
 2 4 6 8 10
Sampling ratio
PrecisionRecallSystem LAS
Figure 5: The effect of down-sampling on precision
and recall of the base classifier, and on labeled ac-
curacy of the dependency parser. The x-axis refers
to the number of negative instances for each posi-
tive instance in the training data. Training and test-
ing was performed on a 90%-10% split of the Dutch
training data.
basic learners. This approach is similar to Yamada
and Matsumoto (2003) but we only use their Left
and Right reduction operators, not Shift. In the first
phase, each learner predicted dependencies between
neighboring words. Dependent words were removed
and the remaining words were sent to the learners for
further rounds of processing until all words but one
had been assigned a head. Whenever crossing links
prevented further assignments of heads to words, the
learner removed the remaining word requiring the
longest dependency link. When the first phase was
finished another learner assigned labels to pairs of
words present in dependency links.
Unlike in related earlier work (Tjong Kim Sang,
2002), we were unable to compare many different
learner configurations. We used two different train-
ing files for the first phase: one for predicting the
dependency links between adjacent words and one
for predicting all other links. As a learner, we used
TiMBL with its default parameters. We evaluated
different feature sets and ended up with using words,
lemmas, POS tags and an extra pair of features with
the POS tags of the children of the focus word. With
this configuration, this cascaded approach achieved
a labeled score of 62.99 on the Dutch test data com-
pared to 74.59 achieved by our main approach.
179
language unlabeled labeled
Arabic 74.59 57.64
Bulgarian 82.51 78.74
Chinese 82.86 78.37
Czech 72.88 60.92
Danish 82.93 77.90
Dutch 77.79 74.59
German 80.01 77.56
Japanese 89.67 87.41
Portuguese 85.61 77.42
Slovene 74.02 59.19
Spanish 71.33 68.32
Swedish 85.08 79.15
Turkish 64.19 51.07
Table 2: The labeled and unlabeled scores for the
submitted system for each of the thirteen languages.
5 Error analysis
We examined the system output for two languages
in more detail: Dutch and Spanish.
5.1 Dutch
With a labeled attachment score of 74.59 and an
unlabeled attachment score of 77.79, our submitted
Dutch system performs somewhat above the average
over all submitted systems (labeled 70.73, unlabeled
75.07). We review the most notable errors made by
our system.
From a part-of-speech (CPOSTAG) perspective,
a remarkable relative amount of head and depen-
dency errors are made on conjunctions. A likely
explanation is that the tag ?Conj? applies to both co-
ordinating and subordinating conjunctions; we did
not use the FEATS information that made this dis-
tinction, which would have likely solved some of
these errors.
Left- and right-directed attachment to heads is
roughly equally successful. Many errors are made
on relations attaching to ROOT; the system appears
to be overgenerating attachments to ROOT, mostly
in cases when it should have generated rightward
attachments. Unsurprisingly, the more distant the
head is, the less accurate the attachment; especially
recall suffers at distances of three and more tokens.
The most frequent attachment error is generat-
ing a ROOT attachment instead of a ?mod? (mod-
ifier) relation, often occurring at the start of a sen-
tence. Many errors relate to ambiguous adverbs such
as bovendien (moreover), tenslotte (after all), and
zo (thus), which tend to occur rather frequently at
the beginning of sentences in the test set, but less
so in the training set. The test set appears to con-
sist largely of formal journalistic texts which typi-
cally tend to use these marked rhetorical words in
sentence-initial position, while the training set is a
more mixed set of texts from different genres plus
a significant set of individual sentences, often man-
ually constructed to provide particular examples of
syntactic constructions.
5.2 Spanish
The Spanish test data set was the only data set on
which the alternative cascaded approach (72.15) out-
performed our main approach (68.32). A detailed
comparison of the output files of the two systems
has revealed two differences. First, the amount of
circular links, a pair of words which have each other
as head, was larger in the analysis of the submitted
system (7%) than in the cascaded analysis (3%) and
the gold data (also 3%). Second, the number of root
words per sentence (always 1 in the gold data) was
more likely to be correct in the cascaded analysis
(70% correct; other sentences had no root) than in
the submitted approach (40% with 20% of the sen-
tences being assigned no roots and 40% more than
one root). Some of these problems might be solvable
with post-processing
Acknowledgements
This research is funded by NWO, the Netherlands
Organization for Scientific Research under the IMIX
programme, and the Dutch Ministry for Economic
Affairs? IOP-MMI programme.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2004. TiMBL: Tilburg memory based learner, ver-
sion 5.1, reference guide. Technical Report ILK 04-02, ILK
Research Group, Tilburg University.
Erik Tjong Kim Sang. 2002. Memory-based shallow parsing.
Journal of Machine Learning Research, 2(Mar):559?594.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 8th In-
ternational Workshop of Parsing Technologies (IWPT2003).
Nancy, France.
180
