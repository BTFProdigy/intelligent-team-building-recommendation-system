Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 839?849,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distant Supervision for Relation Extraction with Matrix Completion
Miao Fan
?,?,?
, Deli Zhao
?
, Qiang Zhou
?
, Zhiyuan Liu
,?
, Thomas Fang Zheng
?
, Edward Y. Chang
?
?
CSLT, Division of Technical Innovation and Development,
Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, China.

Department of Computer Science and Technology, Tsinghua University, China.
?
HTC Beijing Advanced Technology and Research Center, China.
?
fanmiao.cslt.thu@gmail.com
Abstract
The essence of distantly supervised rela-
tion extraction is that it is an incomplete
multi-label classification problem with s-
parse and noisy features. To tackle the s-
parsity and noise challenges, we propose
solving the classification problem using
matrix completion on factorized matrix of
minimized rank. We formulate relation
classification as completing the unknown
labels of testing items (entity pairs) in a s-
parse matrix that concatenates training and
testing textual features with training label-
s. Our algorithmic framework is based on
the assumption that the rank of item-by-
feature and item-by-label joint matrix is
low. We apply two optimization model-
s to recover the underlying low-rank ma-
trix leveraging the sparsity of feature-label
matrix. The matrix completion problem is
then solved by the fixed point continuation
(FPC) algorithm, which can find the glob-
al optimum. Experiments on two wide-
ly used datasets with different dimension-
s of textual features demonstrate that our
low-rank matrix completion approach sig-
nificantly outperforms the baseline and the
state-of-the-art methods.
1 Introduction
Relation Extraction (RE) is the process of gen-
erating structured relation knowledge from un-
structured natural language texts. Traditional su-
pervised methods (Zhou et al, 2005; Bach and
Badaskar, 2007) on small hand-labeled corpora,
such as MUC
1
and ACE
2
, can achieve high pre-
cision and recall. However, as producing hand-
labeled corpora is laborius and expensive, the su-
pervised approach can not satisfy the increasing
1
http://www.itl.nist.gov/iaui/894.02/related projects/muc/
2
http://www.itl.nist.gov/iad/mig/tests/ace/
Figure 1: Training corpus generated by the basic
alignment assumption of distantly supervised re-
lation extraction. The relation instances are the
triples related to President Barack Obama in the
Freebase, and the relation mentions are some sen-
tences describing him in the Wikipedia.
demand of building large-scale knowledge reposi-
tories with the explosion of Web texts. To address
the lacking training data issue, we consider the dis-
tant (Mintz et al, 2009) or weak (Hoffmann et al,
2011) supervision paradigm attractive, and we im-
prove the effectiveness of the paradigm in this pa-
per.
The intuition of the paradigm is that one
can take advantage of several knowledge bases,
such as WordNet
3
, Freebase
4
and YAGO
5
, to
automatically label free texts, like Wikipedia
6
and New York Times corpora
7
, based on some
heuristic alignment assumptions. An example
accounting for the basic but practical assumption
is illustrated in Figure 1, in which we know
that the two entities (<Barack Obama,
U.S.>) are not only involved in the rela-
tion instances
8
coming from knowledge bases
(President-of(Barack Obama, U.S.)
and Born-in(Barack Obama, U.S.)),
3
http://wordnet.princeton.edu
4
http://www.freebase.com
5
http://www.mpi-inf.mpg.de/yago-naga/yago
6
http://www.wikipedia.org
7
http://catalog.ldc.upenn.edu/LDC2008T19
8
According to convention, we regard a structured triple
r(e
i
, e
j
) as a relation instance which is composed of a pair of
entities <e
i
, e
j
>and a relation name r with respect to them.
839
Error MatrixCompleted Low?rank Matrix
?
Observed Sparse Matrix
TrainingItems
TestingItems
Incomplete LabelsNoisy Features
Figure 2: The procedure of noise-tolerant low-rank matrix completion. In this scenario, distantly super-
vised relation extraction task is transformed into completing the labels for testing items (entity pairs) in
a sparse matrix that concatenates training and testing textual features with training labels. We seek to
recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.
but also co-occur in several relation mentions
9
appearing in free texts (Barack Obama is
the 44th and current President of
the U.S. and Barack Obama was born
in Honolulu, Hawaii, U.S., etc.). We
extract diverse textual features from all those
relation mentions and combine them into a rich
feature vector labeled by the relation names
(President-of and Born-in) to produce a
weak training corpus for relation classification.
This paradigm is promising to generate large-
scale training corpora automatically. However, it
comes up against three technical challeges:
? Sparse features. As we cannot tell what
kinds of features are effective in advance, we
have to use NLP toolkits, such as Stanford
CoreNLP
10
, to extract a variety of textual fea-
tures, e.g., named entity tags, part-of-speech
tags and lexicalized dependency paths. Un-
fortunately, most of them appear only once in
the training corpus, and hence leading to very
sparse features.
? Noisy features. Not all relation mentions
express the corresponding relation instances.
For example, the second relation mention in
Figure 1 does not explicitly describe any rela-
tion instance, so features extracted from this
sentence can be noisy. Such analogous cases
commonly exist in feature extraction.
? Incomplete labels. Similar to noisy fea-
9
The sentences that contain the given entity pair are called
relation mentions.
10
http://nlp.stanford.edu/downloads/corenlp.shtml
tures, the generated labels can be in-
complete. For example, the fourth re-
lation mention in Figure 1 should have
been labeled by the relation Senate-of.
However, the incomplete knowledge base
does not contain the corresponding relation
instance (Senate-of(Barack Obama,
U.S.)). Therefore, the distant supervision
paradigm may generate incomplete labeling
corpora.
In essence, distantly supervised relation extrac-
tion is an incomplete multi-label classification task
with sparse and noisy features.
In this paper, we formulate the relation-
extraction task from a novel perspective of using
matrix completion with low rank criterion. To the
best of our knowledge, we are the first to apply this
technique on relation extraction with distant super-
vision. More specifically, as shown in Figure 2, we
model the task with a sparse matrix whose rows
present items (entity pairs) and columns contain
noisy textual features and incomplete relation la-
bels. In such a way, relation classification is trans-
formed into a problem of completing the unknown
labels for testing items in the sparse matrix that
concatenates training and testing textual features
with training labels, based on the assumption that
the item-by-feature and item-by-label joint matrix
is of low rank. The rationale of this assumption
is that noisy features and incomplete labels are
semantically correlated. The low-rank factoriza-
tion of the sparse feature-label matrix delivers the
low-dimensional representation of de-correlation
for features and labels.
840
We contribute two optimization models, DRM-
C
11
-b and DRMC-1, aiming at exploiting the s-
parsity to recover the underlying low-rank matrix
and to complete the unknown testing labels simul-
taneously. Moreover, the logistic cost function is
integrated in our models to reduce the influence of
noisy features and incomplete labels, due to that
it is suitable for binary variables. We also modify
the fixed point continuation (FPC) algorithm (Ma
et al, 2011) to find the global optimum.
Experiments on two widely used datasets
demonstrate that our noise-tolerant approaches
outperform the baseline and the state-of-the-art
methods. Furthermore, we discuss the influence of
feature sparsity, and our approaches consistently
achieve better performance than compared meth-
ods under different sparsity degrees.
2 Related Work
The idea of distant supervision was firstly pro-
posed in the field of bioinformatics (Craven and
Kumlien, 1999). Snow et al (2004) used Word-
Net as the knowledge base to discover more h-
pyernym/hyponym relations between entities from
news articles. However, either bioinformatic
database or WordNet is maintained by a few ex-
perts, thus hardly kept up-to-date.
As we are stepping into the big data era, the
explosion of unstructured Web texts simulates us
to build more powerful models that can automat-
ically extract relation instances from large-scale
online natural language corpora without hand-
labeled annotation. Mintz et al (2009) adopt-
ed Freebase (Bollacker et al, 2008; Bollacker
et al, 2007), a large-scale crowdsourcing knowl-
edge base online which contains billions of rela-
tion instances and thousands of relation names, to
distantly supervise Wikipedia corpus. The basic
alignment assumption of this work is that if a pair
of entities participate in a relation, all sentences
that mention these entities are labeled by that rela-
tion name. Then we can extract a variety of textu-
al features and learn a multi-class logistic regres-
sion classifier. Inspired by multi-instance learn-
ing (Maron and Lozano-P?erez, 1998), Riedel et al
(2010) relaxed the strong assumption and replaced
all sentences with at least one sentence. Hoff-
mann et al (2011) pointed out that many entity
pairs have more than one relation. They extend-
11
It is the abbreviation for Distant supervision for Relation
extraction with Matrix Completion
ed the multi-instance learning framework (Riedel
et al, 2010) to the multi-label circumstance. Sur-
deanu et al (2012) proposed a novel approach to
multi-instance multi-label learning for relation ex-
traction, which jointly modeled all the sentences in
texts and all labels in knowledge bases for a giv-
en entity pair. Other literatures (Takamatsu et al,
2012; Min et al, 2013; Zhang et al, 2013; Xu
et al, 2013) addressed more specific issues, like
how to construct the negative class in learning or
how to adopt more information, such as name en-
tity tags, to improve the performance.
Our work is more relevant to Riedel et al?s
(2013) which considered the task as a matrix fac-
torization problem. Their approach is composed
of several models, such as PCA (Collins et al,
2001) and collaborative filtering (Koren, 2008).
However, they did not concern about the data noise
brought by the basic assumption of distant super-
vision.
3 Model
We apply a new technique in the field of ap-
plied mathematics, i.e., low-rank matrix comple-
tion with convex optimization. The breakthrough
work on this topic was made by Cand`es and Recht
(2009) who proved that most low-rank matrices
can be perfectly recovered from an incomplete
set of entries. This promising theory has been
successfully applied on many active research ar-
eas, such as computer vision (Cabral et al, 2011),
recommender system (Rennie and Srebro, 2005)
and system controlling (Fazel et al, 2001). Our
models for relation extraction are based on the
theoretic framework proposed by Goldberg et al
(2010), which formulated the multi-label trans-
ductive learning as a matrix completion problem.
The new framework for classification enhances the
robustness to data noise by penalizing differen-
t cost functions for features and labels.
3.1 Formulation
Suppose that we have built a training corpus for
relation classification with n items (entity pairs),
d-dimensional textual features, and t labels (rela-
tions), based on the basic alignment assumption
proposed by Mintz et al (2009). Let X
train
?
R
n?d
and Y
train
? R
n?t
denote the feature matrix
and the label matrix for training, respectively. The
linear classifier we adopt aims to explicitly learn
the weight matrix W ? R
d?t
and the bias column
841
vector b ? R
t?1
with the constraint of minimizing
the loss function l,
arg min
W,b
l(Y
train
,
[
1 X
train
]
[
b
T
W
]
), (1)
where 1 is the all-one column vector. Then we can
predict the label matrix Y
test
? R
m?t
of m testing
items with respect to the feature matrix X
test
?
R
m?d
. Let
Z =
[
X
train
Y
train
X
test
Y
test
]
.
This linear classification problem can be trans-
formed into completing the unobservable entries
in Y
test
by means of the observable entries in
X
train
, Y
train
and X
test
, based on the assumption
that the rank of matrix Z ? R
(n+m)?(d+t)
is low.
The model can be written as,
arg min
Z?R
(n+m)?(d+t)
rank(Z)
s.t. ?(i, j) ? ?
X
, z
ij
= x
ij
,
(1 ? i ? n+m, 1 ? j ? d),
?(i, j) ? ?
Y
, z
i(j+d)
= y
ij
,
(1 ? i ? n, 1 ? j ? t),
(2)
where we use ?
X
to represent the index set of ob-
servable feature entries in X
train
and X
test
, and
?
Y
to denote the index set of observable label en-
tries in Y
train
.
Formula (2) is usually impractical for real prob-
lems as the entries in the matrix Z are corrupted
by noise. We thus define
Z = Z
?
+ E,
where Z
?
as the underlying low-rank matrix
Z
?
=
[
X
?
Y
?
]
=
[
X
?
train
Y
?
train
X
?
test
Y
?
test
]
,
and E is the error matrix
E =
[
E
X
train
E
Y
train
E
X
test
0
]
.
The rank function in Formula (2) is a non-convex
function that is difficult to be optimized. The sur-
rogate of the function can be the convex nucle-
ar norm ||Z||
?
=
?
?
k
(Z) (Cand`es and Recht,
2009), where ?
k
is the k-th largest singular val-
ue of Z. To tolerate the noise entries in the error
matrix E, we minimize the cost functions C
x
and
C
y
for features and labels respectively, rather than
using the hard constraints in Formula (2).
According to Formula (1), Z
?
? R
(n+m)?(d+t)
can be represented as [X
?
,WX
?
] instead of
[X
?
, Y
?
], by explicitly modeling the bias vector
b. Therefore, this convex optimization model is
called DRMC-b,
arg min
Z,b
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
ij
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d)
+ b
j
, y
ij
),
(3)
where ? and ? are the positive trade-off weights.
More specifically, we minimize the nuclear norm
||Z||
?
via employing the regularization terms, i.e.,
the cost functions C
x
and C
y
for features and la-
bels.
If we implicitly model the bias vector b,
Z
?
? R
(n+m)?(1+d+t)
can be denoted by
[1, X
?
,W
?
X
?
] instead of [X
?
, Y
?
], in which W
?
takes the role of [b
T
; W] in DRMC-b. Then we
derive another optimization model called DRMC-
1,
arg min
Z
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
i(j+1)
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d+1)
, y
ij
)
s.t. Z(:, 1) = 1,
(4)
where Z(:, 1) denotes the first column of Z.
For our relation classification task, both features
and labels are binary. We assume that the actual
entry u belonging to the underlying matrix Z
?
is
randomly generated via a sigmoid function (Jor-
dan, 1995): Pr(u|v) = 1/(1 + e
?uv
), given the
observed binary entry v from the observed sparse
matrix Z. Then, we can apply the log-likelihood
cost function to measure the conditional probabil-
ity and derive the logistic cost function for C
x
and
C
y
,
C(u, v) = ? logPr(u|v) = log(1 + e
?uv
),
After completing the entries in Y
test
, we adop-
t the sigmoid function to calculate the conditional
probability of relation r
j
, given entity pair p
i
per-
taining to y
ij
in Y
test
,
Pr(r
j
|p
i
) =
1
1 + e
?y
ij
, y
ij
? Y
test
.
Finally, we can achieve Top-N predicted relation
instances via ranking the values of Pr(r
j
|p
i
).
842
4 Algorithm
The matrix rank minimization problem is NP-
hard. Therefore, Cand?es and Recht (2009) sug-
gested to use a convex relaxation, the nuclear nor-
m minimization instead. Then, Ma et al (2011)
proposed the fixed point continuation (FPC) algo-
rithm which is fast and robust. Moreover, Gold-
frab and Ma (2011) proved the convergence of the
FPC algorithm for solving the nuclear norm mini-
mization problem. We thus adopt and modify the
algorithm aiming to find the optima for our noise-
tolerant models, i.e., Formulae (3) and (4).
4.1 Fixed point continuation for DRMC-b
Algorithm 1 describes the modified FPC algorithm
for solving DRMC-b, which contains two steps for
each iteration,
Gradient step: In this step, we infer the ma-
trix gradient g(Z) and bias vector gradient g(b) as
follows,
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
ij
1+e
x
ij
z
ij
, (i, j) ? ?
X
?
|?
Y
|
?y
i(j?d)
1+e
y
i(j?d)
(z
ij
+b
j
)
, (i, j ? d) ? ?
Y
0, otherwise
and
g(b
j
) =
?
|?
Y
|
?
i:(i,j)??
Y
?y
ij
1 + e
y
ij
(z
i(j+d)
+b
j
)
.
We use the gradient descents A = Z ? ?
z
g(Z)
and b = b ? ?
b
g(b) to gradually find the global
minima of the cost function terms in Formula (3),
where ?
z
and ?
b
are step sizes.
Shrinkage step: The goal of this step is to min-
imize the nuclear norm ||Z||
?
in Formula (3). We
perform the singular value decomposition (SVD)
(Golub and Kahan, 1965) for A at first, and then
cut down each singular value. During the iteration,
any negative value in ?? ?
z
? is assigned by zero,
so that the rank of reconstructed matrix Z will be
reduced, where Z = Umax(?? ?
z
?, 0)V
T
.
To accelerate the convergence, we use a con-
tinuation method to improve the speed. ? is ini-
tialized by a large value ?
1
, thus resulting in the
fast reduction of the rank at first. Then the conver-
gence slows down as ? decreases while obeying
?
k+1
= max(?
k
?
?
, ?
F
). ?
F
is the final value of
?, and ?
?
is the decay parameter.
For the stopping criteria in inner iterations, we
define the relative error to measure the residual of
matrix Z between two successive iterations,
Algorithm 1 FPC algorithm for solving DRMC-b
Input:
Initial matrix Z
0
, bias b
0
; Parameters ?, ?;
Step sizes ?
z
, ?
b
.
Set Z = Z
0
, b = b
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step:
A = Z? ?
z
g(Z),b = b? ?
b
g(b).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
end while
end foreach
Output: Completed Matrix Z, bias b.
||Z
k+1
? Z
k
||
F
max(1, ||Z
k
||
F
)
? ?,
where ? is the convergence threshold.
4.2 Fixed point continuation for DRMC-1
Algorithm 2 is similar to Algorithm 1 except for
two differences. First, there is no bias vector b.
Second, a projection step is added to enforce the
first column of matrix Z to be 1. In addition, The
matrix gradient g(Z) for DRMC-1 is
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
i(j?1)
1+e
x
i(j?1)
z
ij
, (i, j ? 1) ? ?
X
?
|?
Y
|
?y
i(j?d?1)
1+e
y
i(j?d?1)
z
ij
, (i, j ? d? 1) ? ?
Y
0, otherwise
.
Algorithm 2 FPC algorithm for solving DRMC-1
Input:
Initial matrix Z
0
; Parameters ?, ?;
Step sizes ?
z
.
Set Z = Z
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step: A = Z? ?
z
g(Z).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
Projection step: Z(:, 1) = 1.
end while
end foreach
Output: Completed Matrix Z.
843
Dataset # of training
tuples
# of testing
tuples
% with more
than one label
# of features # of relation
labels
NYT?10 4,700 1,950 7.5% 244,903 51
NYT?13 8,077 3,716 0% 1,957 51
Table 1: Statistics about the two widely used datasets.
Model NYT?10 (?=2) NYT?10 (?=3) NYT?10 (?=4) NYT?10 (?=5) NYT?13
DRMC-b 51.4 ? 8.7 (51) 45.6 ? 3.4 (46) 41.6 ? 2.5 (43) 36.2 ? 8.8(37) 84.6 ? 19.0 (85)
DRMC-1 16.0 ? 1.0 (16) 16.4 ? 1.1(17) 16 ? 1.4 (17) 16.8 ? 1.5(17) 15.8 ? 1.6 (16)
Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The
threshold ? means filtering the features that appear less than ? times. The values in brackets pertaining to
DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing
sets.
5 Experiments
In order to conduct reliable experiments, we adjust
and estimate the parameters for our approaches,
DRMC-b and DRMC-1, and compare them with
other four kinds of landmark methods (Mintz et
al., 2009; Hoffmann et al, 2011; Surdeanu et al,
2012; Riedel et al, 2013) on two public datasets.
5.1 Dataset
The two widely used datasets that we adopt are
both automatically generated by aligning Freebase
to New York Times corpora. The first dataset
12
,
NYT?10, was developed by Riedel et al (2010),
and also used by Hoffmann et al (2011) and Sur-
deanu et al (2012). Three kinds of features, name-
ly, lexical, syntactic and named entity tag fea-
tures, were extracted from relation mentions. The
second dataset
13
, NYT?13, was also released by
Riedel et al (2013), in which they only regarded
the lexicalized dependency path between two enti-
ties as features. Table 1 shows that the two datasets
differ in some main attributes. More specifically,
NYT?10 contains much higher dimensional fea-
tures than NYT?13, whereas fewer training and
testing items.
5.2 Parameter setting
In this part, we address the issue of setting param-
eters: the trade-off weights ? and ?, the step sizes
?
z
and ?
b
, and the decay parameter ?
?
.
We set ? = 1 to make the contribution of the
cost function terms for feature and label matrices
equal in Formulae (3) and (4). ? is assigned by a
series of values obeying ?
k+1
= max(?
k
?
?
, ?
F
).
12
http://iesl.cs.umass.edu/riedel/ecml/
13
http://iesl.cs.umass.edu/riedel/data-univSchema/
We follow the suggestion in (Goldberg et al,
2010) that ? starts at ?
1
?
?
, and ?
1
is the largest
singular value of the matrix Z. We set ?
?
= 0.01.
The final value of ?, namely ?
F
, is equal to 0.01.
Ma et al (2011) revealed that as long as the non-
negative step sizes satisfy ?
z
< min(
4|?
Y
|
?
, |?
X
|)
and ?
b
<
4|?
Y
|
?(n+m)
, the FPC algorithm will guaran-
tee to converge to a global optimum. Therefore,
we set ?
z
= ?
b
= 0.5 to satisfy the above con-
straints on both two datasets.
5.3 Rank estimation
Even though the FPC algorithm converges in iter-
ative fashion, the value of ? varying with different
datasets is difficult to be decided. In practice, we
record the rank of matrix Z at each round of iter-
ation until it converges at a rather small threshold
? = 10
?4
. The reason is that we suppose the opti-
mal low-rank representation of the matrix Z con-
veys the truly effective information about underly-
ing semantic correlation between the features and
the corresponding labels.
We use the five-fold cross validation on the val-
idation set and evaluate the performance on each
fold with different ranks. At each round of itera-
tion, we gain a recovered matrix and average the
F1
14
scores from Top-5 to Top-all predicted rela-
tion instances to measure the performance. Figure
3 illustrates the curves of average F1 scores. After
recording the rank associated with the highest F1
score on each fold, we compute the mean and the
standard deviation to estimate the range of optimal
rank for testing. Table 2 lists the range of optimal
ranks for DRMC-b and DRMC-1 on NYT?10 and
NYT?13.
14
F1 =
2?precision?recall
precision+recall
844
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(a) DRMC-b on NYT?10 validation set (? = 5).
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(b) DRMC-1 on NYT?10 validation set (? = 5).
0 100 200 300 400 500
0.104
0.106
0.108
0.11
0.112
0.114
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(c) DRMC-b on NYT?13 validation set.
0 100 200 300 400 5000.106
0.108
0.11
0.112
0.114
0.116
0.118
0.12
0.122
0.124
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(d) DRMC-1 on NYT?13 validation set.
Figure 3: Five-fold cross validation for rank estimation on two datasets.
On both two datasets, we observe an identical
phenomenon that the performance gradually in-
creases as the rank of the matrix declines before
reaching the optimum. However, it sharply de-
creases if we continue reducing the optimal rank.
An intuitive explanation is that the high-rank ma-
trix contains much noise and the model tends to be
overfitting, whereas the matrix of excessively low
rank is more likely to lose principal information
and the model tends to be underfitting.
5.4 Method Comparison
Firstly, we conduct experiments to compare our
approaches with Mintz-09 (Mintz et al, 2009),
MultiR-11 (Hoffmann et al, 2011), MIML-12 and
MIML-at-least-one-12 (Surdeanu et al, 2012) on
NYT?10 dataset. Surdeanu et al (2012) released
the open source code
15
to reproduce the experi-
mental results on those previous methods. More-
over, their programs can control the feature spar-
15
http://nlp.stanford.edu/software/mimlre.shtml
sity degree through a threshold ? which filters the
features that appears less than ? times. They set
? = 5 in the original code by default. Therefore,
we follow their settings and adopt the same way
to filter the features. In this way, we guarantee
the fair comparison for all methods. Figure 4 (a)
shows that our approaches achieve the significant
improvement on performance.
We also perform the experiments to compare
our approaches with the state-of-the-art NFE-13
16
(Riedel et al, 2013) and its sub-methods (N-13,
F-13 and NF-13) on NYT?13 dataset. Figure 4 (b)
illustrates that our approaches still outperform the
state-of-the-art methods. In practical application-
s, we also concern about the precision on Top-N
predicted relation instances. Therefore, We com-
pare the precision of Top-100s, Top-200s and Top-
500s for DRMC-1, DRMC-b and the state-of-the-
16
Readers may refer to the website,
http://www.riedelcastro.org/uschema for the details of
those methods. We bypass the description due to the
limitation of space.
845
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 
Mintz?09
MultiR?11
MIML?12
MIML?at?least?one?12
DRMC?1(Rank=17)
DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 N?13F?13NF?13NFE?13DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 4: Method comparison on two testing sets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1879)DRMC?b(Rank=1993)DRMC?1(Rank=1169)DRMC?b(Rank=1307)DRMC?1(Rank=384)DRMC?b(Rank=464)DRMC?1(Rank=17)DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1378)DRMC?b(Rank=1861)DRMC?1(Rank=719)DRMC?b(Rank=1703)DRMC?1(Rank=139)DRMC?b(Rank=655)DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 5: Precision-Recall curve for DRMC-b and DRMC-1 with different ranks on two testing sets.
Top-N NFE-13 DRMC-b DRMC-1
Top-100 62.9% 82.0% 80.0%
Top-200 57.1% 77.0% 80.0%
Top-500 37.2% 70.2% 77.0%
Average 52.4% 76.4% 79.0%
Table 3: Precision of NFE-13, DRMC-b and
DRMC-1 on Top-100, Top-200 and Top-500 pre-
dicted relation instances.
art method NFE-13 (Riedel et al, 2013). Table 3
shows that DRMC-b and DRMC-1 achieve 24.0%
and 26.6% precision increments on average, re-
spectively.
6 Discussion
We have mentioned that the basic alignment as-
sumption of distant supervision (Mintz et al,
2009) tends to generate noisy (noisy features and
incomplete labels) and sparse (sparse features) da-
ta. In this section, we discuss how our approaches
tackle these natural flaws.
Due to the noisy features and incomplete label-
s, the underlying low-rank data matrix with tru-
ly effective information tends to be corrupted and
the rank of observed data matrix can be extremely
high. Figure 5 demonstrates that the ranks of da-
ta matrices are approximately 2,000 for the initial
optimization of DRMC-b and DRMC-1. Howev-
er, those high ranks result in poor performance.
As the ranks decline before approaching the op-
timum, the performance gradually improves, im-
plying that our approaches filter the noise in data
and keep the principal information for classifica-
tion via recovering the underlying low-rank data
matrix.
Furthermore, we discuss the influence of the
feature sparsity for our approaches and the state-
846
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=43)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2148)DRMC?b(Rank=2291)DRMC?1(Rank=1285)DRMC?b(Rank=1448)DRMC?1(Rank=404)DRMC?b(Rank=489)DRMC?1(Rank=17)DRMC?b(Rank=43)
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=46)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2539)DRMC?b(Rank=2730)DRMC?1(Rank=1447)DRMC?b(Rank=1644)DRMC?1(Rank=433)DRMC?b(Rank=531)DRMC?1(Rank=17)DRMC?b(Rank=46)
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=16)DRMC?b(Rank=51)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=3186)DRMC?b(Rank=3444)DRMC?1(Rank=1728)DRMC?b(Rank=1991)DRMC?1(Rank=489)DRMC?b(Rank=602)DRMC?1(Rank=16)DRMC?b(Rank=51)
Figure 6: Feature sparsity discussion on NYT?10 testing set. Each row (from top to bottom, ? = 4, 3, 2)
illustrates a suite of experimental results. They are, from left to right, five-fold cross validation for
rank estimation on DRMC-b and DRMC-1, method comparison and precision-recall curve with different
ranks, respectively.
of-the-art methods. We relax the feature filtering
threshold (? = 4, 3, 2) in Surdeanu et al?s (2012)
open source program to generate more sparse fea-
tures from NYT?10 dataset. Figure 6 shows that
our approaches consistently outperform the base-
line and the state-of-the-art methods with diverse
feature sparsity degrees. Table 2 also lists the
range of optimal rank for DRMC-b and DRMC-
1 with different ?. We observe that for each ap-
proach, the optimal range is relatively stable. In
other words, for each approach, the amount of tru-
ly effective information about underlying seman-
tic correlation keeps constant for the same dataset,
which, to some extent, explains the reason why our
approaches are robust to sparse features.
7 Conclusion and Future Work
In this paper, we contributed two noise-tolerant
optimization models
17
, DRMC-b and DRMC-1,
for distantly supervised relation extraction task
from a novel perspective. Our models are based on
matrix completion with low-rank criterion. Exper-
17
The source code can be downloaded from https://
github.com/nlpgeek/DRMC/tree/master
iments demonstrated that the low-rank represen-
tation of the feature-label matrix can exploit the
underlying semantic correlated information for re-
lation classification and is effective to overcome
the difficulties incurred by sparse and noisy fea-
tures and incomplete labels, so that we achieved
significant improvements on performance.
Our proposed models also leave open question-
s for distantly supervised relation extraction task.
First, they can not process new coming testing
items efficiently, as we have to reconstruct the data
matrix containing not only the testing items but al-
so all the training items for relation classification,
and compute in iterative fashion again. Second,
the volume of the datasets we adopt are relatively
small. For the future work, we plan to improve our
models so that they will be capable of incremental
learning on large-scale datasets (Chang, 2011).
Acknowledgments
This work is supported by National Program on
Key Basic Research Project (973 Program) under
Grant 2013CB329304, National Science Founda-
tion of China (NSFC) under Grant No.61373075.
847
References
Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature review for Lan-
guage and Statistics II.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general
human knowledge. In Proceedings of the nation-
al conference on Artificial Intelligence, volume 22,
page 1962. AAAI Press.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Ricardo S Cabral, Fernando Torre, Jo?ao P Costeira, and
Alexandre Bernardino. 2011. Matrix completion
for multi-label image classification. In Advances in
Neural Information Processing Systems, pages 190?
198.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717?
772.
Edward Y Chang. 2011. Foundations of Large-Scale
Multimedia Information Management and Retrieval.
Springer.
Michael Collins, Sanjoy Dasgupta, and Robert E
Schapire. 2001. A generalization of principal com-
ponents analysis to the exponential family. In Ad-
vances in neural information processing systems,
pages 617?624.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Maryam Fazel, Haitham Hindi, and Stephen P Boyd.
2001. A rank minimization heuristic with applica-
tion to minimum order system approximation. In
American Control Conference, 2001. Proceedings of
the 2001, volume 6, pages 4734?4739. IEEE.
Andrew Goldberg, Ben Recht, Junming Xu, Robert
Nowak, and Xiaojin Zhu. 2010. Transduction with
matrix completion: Three birds with one stone. In
Advances in neural information processing systems,
pages 757?765.
Donald Goldfarb and Shiqian Ma. 2011. Conver-
gence of fixed-point continuation algorithms for ma-
trix rank minimization. Foundations of Computa-
tional Mathematics, 11(2):183?210.
Gene Golub and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial & Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Michael Jordan. 1995. Why the logistic function? a
tutorial discussion on probabilities and neural net-
works. Computational Cognitive Science Technical
Report.
Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering mod-
el. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 426?434. ACM.
Shiqian Ma, Donald Goldfarb, and Lifeng Chen. 2011.
Fixed point and bregman iterative methods for ma-
trix rank minimization. Mathematical Program-
ming, 128(1-2):321?353.
Oded Maron and Tom?as Lozano-P?erez. 1998. A
framework for multiple-instance learning. Advances
in neural information processing systems, pages
570?576.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777?782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Jasson DM Rennie and Nathan Srebro. 2005. Fast
maximum margin matrix factorization for collabora-
tive prediction. In Proceedings of the 22nd interna-
tional conference on Machine learning, pages 713?
719. ACM.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their mention-
s without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
848
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Process-
ing Systems 17.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
Annual Meeting of the Association for Computation-
al Linguistics: Long Papers-Volume 1, pages 721?
729. Association for Computational Linguistics.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665?670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s (Volume 2: Short Papers), pages 810?815, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
427?434. Association for Computational Linguistic-
s.
849
Proceedings of the TextGraphs-7 Workshop at ACL, pages 44?54,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Bringing the Associative Ability to Social Tag Recommendation 
 
Miao Fan,
?? 
Yingnan Xiao
?
 and Qiang Zhou
?
 
? 
Department of Computer Science and Technology, Tsinghua University 
?
School of Software Engineering, Beijing University of Posts and Telecommunications 
{fanmiao.cslt.thu,lxyxynt}@gmail.com,  
zq-lxd@mail.tsinghua.edu.cn 
 
 
Abstract 
Social tagging systems, which allow users to 
freely annotate online resources with tags, 
become popular in the Web 2.0 era. In order to 
ease the annotation process, research on social 
tag recommendation has drawn much attention 
in recent years. Modeling the social tagging 
behavior could better reflect the nature of this 
issue and improve the result of recommendation. 
In this paper, we proposed a novel approach for 
bringing the associative ability to model the 
social tagging behavior and then to enhance the 
performance of automatic tag recommendation. 
To simulate human tagging process, our 
approach ranks the candidate tags on a 
weighted digraph built by the semantic 
relationships among meaningful words in the 
summary and the corresponding tags for a 
given resource. The semantic relationships are 
learnt via a word alignment model in statistical 
machine translation on large datasets. 
Experiments on real world datasets demonstrate 
that our method is effective, robust and 
language-independent compared with the state-
of-the-art methods. 
1 Introduction 
Social tagging systems, like Flickr 1 , Last.fm 2 , 
Delicious 3  and Douban 4 , have recently become 
major infrastructures on the Web, as they allow 
users to freely annotate online resources with 
personal tags and share them with others. Because 
of the no vocabulary restrictions, there are different 
kinds of tags, such as tags like keywords, category 
names or even named entities. However, we can 
                                                          
1 http://www.flickr.com 
2 http://www.lastfm.com 
3 http://delicious.com 
4 http://www.douban.com 
still find the inner relationship between the tags 
and the resource that they describe. Figure 1 shows 
a snapshot of a social tagging example, where the 
famous artist, Michael Jackson was annotated with 
multiple social tags by users in Last.fm2. Actually, 
Figure 1 can be divided into three parts, which are 
the title, the summary and the tags respectively. 
 
 
Figure 1: A music artist entry from website Last.fm2 
 
We can easily find out that social tags concisely 
indicate the main content of the given online 
resource and some of them even reflect user 
interests. For this reason, social tagging has been 
widely studied and applied in recommender 
systems (Eck et al, 2007; Musto et al, 2009; Zhou 
et al, 2010), advertising (Mirizzi et al, 2010), etc. 
For the sake of easing the process of user 
annotation and providing a better effect of human-
computer interaction, researchers expected to build 
44
automatic social tagging recommender systems, 
which could automatically suggest proper tags for 
a user when he/she wants to annotate an online 
resource. By observing huge amount of online 
resources, researchers found out that most of them 
contain summaries, which could play an important 
role in briefly introducing the corresponding 
resources, such as the artist entry about Michael 
Jackson in Figure 1. Thus some of them proposed 
to automatically suggest tags based on resource 
summaries, which are collectively known as the 
content-based approach (F. Ricci et al, 2011). 
The basic idea of content-based approach in 
recommender systems is to select important words 
from summaries as tags. However, this is far from 
adequate as not all tags are statistically significant 
in the summaries. Some of them even do not 
appear in the corresponding summaries. For 
example, in Figure 1, the popular tag dance does 
not appear in the summary, but why most of users 
choose it as a proper tag to describe Michael 
Jackson.  This ?out-of-summary? phenomenon 
reflects a fact that users usually exploit their own 
knowledge and associative ability to annotate 
online resources. When a summary comes, they 
associate the important words in the summary with 
other semantic-related tags based on their 
knowledge. To improve the automatic tag 
recommendation, a social computing issue (Wang 
et al, 2007), modeling the social tagging behavior 
is the straightforward way. Namely, how to 
analyze the human tagging process and propose a 
suitable approach that can help the computer to 
simulate the process are what we will explore in 
this paper.   
The novel idea of our approach is to rank the 
candidate tags on a weighted digraph built by the 
semantic relationships among meaningful words in 
the summary and the corresponding tags for a 
given resource. The semantic relationships are 
learnt via a word alignment model in statistical 
machine translation. Our approach could bring the 
associative ability to social tag recommendation 
and naturally simulate the whole process of human 
social tagging behavior and then to enhance the 
performance of automatic tag recommendation. So, 
we name this approach for Associative Tag 
Recommendation (ATR). 
The remainder of the paper is organized as 
follows. Section 2 analyzes the process of human 
tagging behavior. Section 3 describes our novel 
approach to simulate the process of human tagging 
behavior for social tag recommendation. Section 4 
compares our approach with the state-of-the-art 
and baseline methods and analyzes the parameter 
influences. Section 5 surveys some related work in 
social tag recommendation. Section 6 concludes 
with our major contributions and proposes some 
open problems for future work. 
 
2 Human Tagging Behavior Analysis 
Here, we will analyze the human tagging process 
to discover the secret why some of the tags are 
widely annotated while are not statistically 
significant or even do not appear in the summaries. 
In most cases, the information in summaries is 
too deficient for users to tag resources or to reflect 
personalities. Users thus exploit their own 
knowledge, which may be partly learnt from other 
resource entries containing both summaries and 
tags in Table 1. Then when they want to tag an 
online resource, they will freely associate 
meaningful words in the summary with other 
semantic related words learnt from former reading 
experiences. However, the result of this association 
behavior will be explosive. Users should judge and 
weigh these candidate tags in brain, usually via 
forming a semantic related word network and 
finally decide the tags that they choose to annotate 
the given resource.  
For example, after browsing plentiful of 
summary-tag pairs, we could naturally acquire the 
semantic relationships between the words, such as 
?singer?, ?pop?, in the summary and the tag, 
?dance?. If we tag the artist entry in Figure 1, the 
tag ?dance? is more likely associated by the words 
like ?pop?, ?artist?, ?Rock & Roll? et al While 
reading the summary of artist Michael Jackson in 
Figure 1, we may construct an abstract tag-network 
in Figure 2 with the important words (king, pop, 
artist et al) in the summary, the associated tags 
(dance, 80s, pop et al and their semantic 
relationships.  
 
Summary: David Lindgren (born April 28, 1982 
in Skelleftea, Sweden) is a Swedish singer and 
musical artist? 
Tags: swedish, pop, dance, musical, david 
lindgren 
Summary: Wanessa God?i Camargo (born on 
45
December 28, 1982), known simply as Wanessa, is 
a Brazilian pop singer? 
Tags: pop, dance, female vocalists, electronic, 
electropop ? 
Table 1: Examples of artist entries from Last.fm2 
 
 
Figure 2: A part of the abstract associative tag-network 
in human brains. 
 
3 Associative Tag Recommendation 
We describe our ATR approach as a three-stage 
procedure by simulating the human annotation 
process analyzed in Section 2. Figure 3 shows the 
overall structure of our approach.  
 
 
Figure 3: The overview of ATR approach. 
 
Stage 1: Summary-tag pairs sampling. Given 
a large collection of tagged resources, we need to 
pre-process the dataset. Generally, the pre-
processing contains tokenizing the summaries, 
extracting the meaningful words and balancing the 
length ratio between the summaries and tags. 
Stage 2: Associative ability acquiring. We 
regard a summary-tag pair as a parallel text. They 
are really suitable to acquire the semantic relation 
knowledge by using word alignment model (In this 
paper, we adopt IBM Model-1) from the large 
amount of summary-tag pairs prepared by Stage 1. 
After gaining the translation probabilities between 
the meaningful words in summaries and tags, our 
social tagging recommender system initially has 
the capability of association, namely from one 
word to many semantic related tags. 
Stage 3: TagRank algorithm for 
recommendation. Stage 2 just helps our 
recommender system acquire the ability of 
associating one word with many semantic related 
tags. However, when the system faces a given 
resource with a long summary, the association 
results may be massive. Thus, we propose a 
TagRank algorithm to order the candidate tags on 
the weighted Tag-digraph, which is built by the 
meaningful words in the summary and their 
semantic related words. 
Before introducing the approach in details, we 
define some general notations, while the other 
specific ones will be introduced in the 
corresponding stage. In our approach, a resource is 
denoted as    , where   is the set of all 
resources. Each resource contains a summary and a 
set of tags. The summary    of resource is simply 
regarded as a bag of meaningful words    
              
  , where     is the count of 
meaningful word    and    is the number of the 
unique meaningful words in  . The tag set 
(annotations)    of resource   is represented as 
                 
  , where     is the count of tag    
and   is the number of the unique tags for  . 
 
3.1 Summary-Tag Pairs Sampling 
We consider that the nouns and tags that appear in 
the corresponding summary are meaningful for our 
tagging recommendation approach.  
46
It is not difficult for language, such as English, 
French et al As for Chinese, Thai and Japanese, 
we still need to do word segmentation (D. D. 
Palmer., 2010). Here, to improve the segmentation 
results of these language texts, we collect all the 
unique tags in resource   as the user dictionary to 
solve the out-of-vocabulary issue. This idea is 
inspired by M. Sun (2011) and we will discuss its 
effort on the performance improvement of our 
system in Section 4.3.  
After the meaningful words have been extracted 
from the summaries, we regard the summary and 
the set of tags as two bags of the sampled words 
without position information for a given resource. 
The IBM Model-1(Brown et al, 1993) was 
adopted for training to gain the translation 
probabilities between the meaningful words in 
summary and the tags. Och and Ney (2003) 
proposed that the performance of word alignment 
models would suffer great loss if the length of 
sentence pairs in the parallel training data set is 
unbalanced. Moreover, some popular online 
resources may be annotated by hundreds of people 
with thousands of tags while the corresponding 
summaries may limit to hundreds of words. So, it 
is necessary to propose a sampling method for 
balanced length of summary-tag pairs.  
One intuitional way is to assign each meaningful 
word in summaries and tags with a term-frequency 
(TF) weight, namely     and    . For each 
extracted meaningful word   in a given summary 
  ,     
   
   
?    
  
   
 and the same tag set 
(annotations)    ,     
   
   
?    
  
   
 . Here, we bring a 
parameter   in this stage, which denotes the length 
ratio between the sampled summary and tag set, 
namely,          
 
3.2 Associative Ability Acquiring 
IBM Model-1 could help our social tagging 
recommender system to learn the lexical 
translation probability between the meaningful 
words in summaries and tags based on the dataset 
provided by stage 1.  We adjust the model to our 
approach, which can be concisely described as, 
 
             ?           
 
                   
 
For each resource  , the relationship between the 
sampled summary   =        
   and the sampled 
tags            
   is connected via a hidden 
variable          
  . For example,      
indicates word    in  at position   is aligned to 
tag    in   at position  . 
For more detail description on mathematics, the 
joint likelihood of   and an alignment   given    
is 
 
             
 
        
 ? (   |     
  
   
       
 
in which                and  (   |      is called 
the translation probability of    given    . The 
alignment is determined by specifying the values 
of    for   from 1 to  , each of which can take any 
value from 0 to  . Therefore, 
 
           
 
        
?  
  
    
 ? ? (   |     
  
   
  
     
 
    
 
The goal is to adjust the translation probabilities 
so as to maximize           subject to the 
constraints that for each  , 
 
?      
 
                                     
 
IBM Model-1 can be trained using Expectation-
Maximization (EM) algorithm (Dempster et al, 
1977) in an unsupervised fashion. At last, we 
obtain the translation probabilities between 
summaries and tags, i.e.,        and        for 
our recommender system acquiring associative 
ability. 
From Eq. (4), we know that IBM Model-1 will 
produce one-to-many alignments from one 
language to another language, and the trained 
model is thus asymmetric. Sometimes, there are a 
few translation pairs appear in both two direction, 
i.e., summary? tag (     ) and tag? summary 
(    ). For this reason, Liu et al (2011) proposed a 
harmonic means to combine the two models.  
 
47
       (
 
         
 
   
         
)
  
              
 
3.3 TagRank Algorithm for Recommendation 
By the time we have generated the ?harmonic? 
translation probability list between meaningful 
words in summaries and tags, our recommender 
system could acquire the capability of association 
like human beings. For instance, it could ?trigger? 
a large amount of semantic related tags from a 
given word: Novel (Figure 4). However, if we 
collected all the ?triggered? tags associated by 
each meaningful word in a given summary, the 
scale would be explosive. Thus we need to explore 
an efficient way that can not only rank these 
candidate tags but also simulate the human tagging 
behavior as much as possible.  
 
 
Figure 4: The association results from the word ?Novel? 
via our social tagging recommender system. 
 
Inspired by the PageRank algorithm (S. Brin and 
L. Page., 1998), we find out that the idea could be 
brought into our approach with a certain degree 
improvement as the human tagging ranking 
process is on a weighted Tag-digraph  . We regard 
the association relationship as one word 
recommending the corresponding candidate tags 
and the degree of preference could be quantified by 
the translation probabilities.  
For a given summary, we firstly sample it via 
the method described in stage 1 to obtain all the 
meaningful words, which are added to the graph as 
a set of seed vertices denoted as   . Then 
according to stage 2, we could obtain a set of 
semantic related vertices associated by these seeds 
denoted as   . We union the    and     to get the 
set of all candidate tags  . For a directed edge     
from    to   , the weight  (   )  equals the 
translation probability from    to   , namely 
 (  |   . So the weighted Tag-digraph could be 
formulized as, 
 
{
 
 
 
 
       
           
   {   }
    {(     )         }
 (   )    (  |   
                       
 
The original TextRank algorithm (Mihalcea et 
al., 2004) just considered the words recommending 
the nearest ones, and assumed that the 
recommending strengths were same. As all the 
words had the equal chance to recommend, it was 
the fact that all the edges in the graph gained no 
direction information. So this method brought little 
improvement on ranking results. In the Eq. (7) they 
used,        represents the set of all the vertices 
that direct to    and         denotes the set of all 
the vertices that direct from   . The factor   is 
usually set to 0.85. 
 
         
         ?
 
|   (  )|         
      (  )             
 
We improve the TextRank model and propose a 
TagRank algorithm (Eq. 8) that is suitable to our 
approach.  For each   , 
 (   )
?  (   )      (  )
 represents 
the proportion of trigger ability from    to   . This 
proportion multiplying the own score of    reflect 
the the degree of recommend contribution to   . 
After we sum up all the vertices willing to 
?recommend?   , namely          , We can 
calculate the score of    in one step. 
Some conceptual words could trigger hundreds 
of tags, so that our recommender system will suffer 
a rather high computation complexity. Thus, we 
add a parameter   which stands for the maximum 
out-degree of the graph  . That means for each 
vertex in the graph  , it can at most trigger top-  
candidate tags with the    highest translation 
probabilities. 
 
48
         
         ?
 (   )
?  (   )      (  )         
      (  ) 
     
 
Starting from vertex initial values assigned to 
the seed nodes (  ) in the graph, the computation 
iterates until convergence below a given threshold 
is achieved. After running the algorithm, a score is 
assigned to each vertex. Finally, our system can 
recommend best   tags with high score for the 
resource. 
 
4 Experiments 
4.1 Datasets and Evaluation Metrics 
Datasets: We prepare two real world datasets with 
diverse properties to test the performance of our 
system in different language environment. Table 2 
lists the statistical information of the English and 
Chinese datasets. 
 
Dataset P Vs Vt Ns Nt 
BOOK 29464 68996 40401 31.5 7.8 
ARTIST 14000 35972 4775 19.0 5.0 
Table 2: Statistical information of two datasets. P , Vs , 
Vt , Ns, and Nt represent the number of parallel texts, the 
vocabulary of summaries, the vocabulary of tags, the 
average number of unique words in each summary and 
the average number of unique tags in each resource 
respectively. 
 
The first dataset, BOOK, was crawled from a 
popular Chinese book review online community 
Douban4, which contains the summaries of books 
and the tags annotated by users. The second dataset, 
ARTIST, was freely obtained via the Last.fm2 API. 
It contains the descriptions of musical artists and 
the tags annotated by users. By comparing the 
characteristics of these two datasets, we find out 
that they differ in language, data size and the 
length ratio (Figure 5). The reason of preparing 
two datasets with diverse characteristics is that we 
would like to demonstrate that our approach is 
effective, robust and language-independent 
compared with others. 
 
Evaluation Metrics: We use precision, recall and 
F-measure to evaluate the performance of our ATR 
approach. Given a resource set  , we regard the set 
of original tags as   , the automatic recommended 
tag set as   . The correctly recommended set of 
tags can be denoted as        . Thus, precision, 
recall and F-measure are defined as5 
 
   
         
    
   
         
    
    
     
     
         
 
The final precision and recall of each method is 
computed by performing 7-fold cross validation on 
both two datasets. 
 
  
 
Figure 5: The length ratio distributions of BOOK and 
ARTIST datasets. 
 
4.2 Methods Comparison  
Baseline Methods: In this section, we compare the 
performance of our associative tagging 
recommendation (ATR) with three other relative 
methods, the state-of-the-art WTM (Liu et al, 
2011), TextRank (Mihalcea et al, 2004) and the 
traditional TFIDF (C. D. Manning et al, 2008; R. 
Baeza-Yates et al, 2011).  
                                                          
5 The reason why we do not calculate the precision, recall and 
F-measure alone is that we cannot guarantee that 
recommending at least one correct tag for each resource. 
49
The reasons we choose those methods to 
compare were as follows. 
?  WTM can reflect the state-of-the-art 
performance on content-based social tag 
recommendation. 
? TextRank can be regarded as a baseline 
method on graph-based social tag 
recommendation. 
? TFIDF, as a traditional method, represents the 
baseline performance and can validate the 
?out-of-summary? phenomenon. 
For the TFIDF value of each word in a given 
summary, it can be calculated by multiplying term 
frequency     
        
   
?    
  
   
 (log 
normalization) by inverted document 
frequency      
          
   
|?           |
  (inverse 
frequency smooth), where |?           | indicates 
the number of resources whose summaries contain 
word  . 
TextRank method regarded the word and its 
forward and backward nearest words as its 
recommendation. Thus, each word in a given 
summary is recommended by its neighborhood 
with no weight. Simply, we use Eq. (7) to calculate 
the final value of each word in a given summary. 
Liu et al (2011) proposed a state of the art 
method which summed up the product the weight 
of a word and its translation probabilities to each 
semantic related tag as the final value of each tag 
in a given resource (Eq. 10). 
 
          ?                
    
                 
 
Experiment Results: Figure 6 illustrates the 
precision-recall curves of ATR, WTM, TextRank 
and TFIDF on two datasets. Each point of a 
precision-recall curve stands for different number 
of recommended tags from     (upper left) to 
     (bottom right). From the Figure 6, we can 
observe that: 
? ATR out-performs WTM, TextRank and 
TFIDF on both datasets. This indicates that 
ATR is a language-independent approach for 
social tag recommendation. 
? ATR shows consistently better performance 
when recommending different number of tags, 
which implies that our approach is efficient 
and robust (Figure 7). 
 
 
 
Figure 6: Performance comparison among ATR, WTM, 
TextRank and TFIDF on BOOK and ARTIST datasets 
when      ,     and vertex initial values are 
assigned to one. 
 
 
 
50
Figure 7: F-measure of ATR, WTM, TextRank and 
TFIDF versus the number of recommended tags ( ) on 
the BOOK and ARTIST datasets when       ,     
and vertex initial values are assigned to one. 
 
4.3 Sampling Methods Discussion 
Section 3.1 proposed an idea on summary-tag pairs 
sampling, which collected all the unique tags as the 
user dictionary to enhance performance of the 
summary segmentation, especially for the Chinese, 
Thai, and Japanese et al Though M. Sun (2011) 
put forward a more general paradigm, few studies 
have verified his proposal. Here, we will discuss 
the efficiency of our sampling method. Figure 8 
shows the comparison of performance between the 
unsampled ATR and (sampled) ATR.  
 
 
Figure 8: Performance comparison between unsampled 
ATR and (sampled) ATR on BOOK datasets when 
     ,     and vertex initial values are assigned to 
one 
 
Experiments on the Chinese dataset BOOK 
demonstrates that our (sampled) ATR approach 
achieves average 19.2% improvement on 
performance compared with the unsampled ATR. 
4.4 Parameter Analysis  
In Section 3, we brought several parameters into 
our approach, namely the harmonic factor  which 
controls the proportion between model      and 
    , the maximum out-degree   which specifies 
the computation complexity of the weighted tag-
digraph and the vertex initial values which may 
affect the final score of some vertices if the 
weighted tag-digraph is not connected. 
We take the BOOK dataset as an example and 
explore their influences to ATR by using 
controlling variables method, which means we 
adjust the focused parameter with the other ones 
stable to observe the results. 
Harmonic factor: In Figure 9, we investigate the 
influence of harmonic factor via the curves of F-
measure of ATR versus the number of 
recommended tags on the BOOK dataset. 
Experiments showed that the performance is 
slightly better when      . As   controls the 
proportion between model      and     ,       
means model      contributes more on 
performance. 
 
 
Figure 9: F-measure of ATR versus the number of 
recommended tags on the BOOK dataset when 
harmonic factor   ranges from 0.0 to 1.0, when     
and vertex initial values are assigned to one. 
 
Maximum out-degree: Actually, during the 
experiments, we have found out that some 
meaningful words could trigger hundreds of 
candidate tags. If we bring all these tags to our 
Tag-Network, the computation complexity will be 
dramatically increased, especially in large datasets. 
To decrease the computation complexity with little 
impact on performance, we need to explore the 
suitable maximum out-degree. Figure 10 illustrates 
how the complexities of tag-digraph will influent 
the performance. We discover that ATR gains 
slight improvement when   is added from 5 to 9 
except the ?leap? from 1 to 5.  It means that     
will be a suitable maximum out-degree, which 
balances the performance and the computation 
complexity. 
51
 
Figure 10: F-measure of ATR versus the number of 
recommended tags on the BOOK dataset, when 
1           and vertex initial values are assigned 
to one. 
 
Vertex initial values: The seeds (meaningful 
words in the summaries) may not be semantic 
related, especially when the maximum out-degree 
is low. As a result, the graph   may be 
disconnected, so that the final score of each vertex 
after iteration may relate to the vertex initial values. 
In Figure 11, we compare three different vertex 
initial values, namely value-one, value of TF (local 
consideration) and value of TFIDF (global 
consideration) to check the influence. However, 
the results show that there is almost no difference 
in F-measure when the maximum out-degree   
ranges from 1 to 9. 
 
Figure 11: F-measure of ATR versus maximum out-
degree on BOOK dataset when the vertex initial values 
equal to Value-One, TF, TFIDF separately with       
and number of recommended tags   = 5.  
5 Related Work  
There are two main stream methods to build a 
social tag recommender system. They are 
collaboration-based method (Herlocker et al, 2004) 
and the content-based approach (Cantador et al, 
2010). 
FolkRank (Jaschke et at., 2008) and Matrix 
Factorization (Rendle et al, 2009) are 
representative collaboration-based methods for 
social tag recommendation. Suggestions of these 
techniques are based on the tagging history of the 
given resource and user, without considering the 
resource summaries. Thus most of these methods 
suffer from the cold-start problem, which means 
they cannot perform effective suggestions for 
resources that no one has annotated. 
To remedy the defect of cold-start problem, 
researchers proposed content-based methods 
exploiting the descriptive information on resources, 
such as summaries. Some of them considered 
social tag recommendation as a classification 
problem by regarding each tag as a category label. 
Various classifiers such as kNN (Fujimura et al, 
2007), SVM (Cao et al, 2009) have been discussed. 
But two issues exposed from these methods. 
? Classification-based methods are highly 
constrained in the quality of annotation, which 
are usually noisy. 
? The training and classification cost are often 
in proportion to the number of classification 
labels, so that these methods may not be 
efficient for real-world social tagging system, 
where thousands of unique tags may belong to 
a resource. 
With the widespread of latent topic models, 
researchers began to pay close attention on 
modeling tags using Latent Dirichlet Allocation 
(LDA) (Blei et al, 2003). Recent studies (Krestel 
et al, 2009; Si and Sun, 2009) assume that both 
tags and words in summary are generated from the 
same set of latent topics. However, most latent 
topic models have to pre-specify the number of 
topic before training. Even though we can use 
cross validation to determine the optimal number 
of topics (Blei et al, 2010), the solution is 
obviously computationally complicated. 
The state of the art research on social tagging 
recommendation (Z. Liu, X. Chen and M. Sun, 
2011) regarded social tagging recommendation 
problem as a task of selecting appropriate tags 
from a controlled tag vocabulary for the given 
resource and bridged the vocabulary gap between 
the summaries and tags using word alignment 
models in statistical machine translation. But they 
simply adopted the weighted sum of the score of 
52
candidate tags, named word trigger method 
(WTM), which cannot reflect the whole process of 
human annotation. 
 
6 Conclusion and Future Work 
In this paper, we propose a new approach for social 
tagging recommendation via analyzing and 
modeling human associative annotation behaviors. 
Experiments demonstrate that our approach is 
effective, robust and language-independent 
compared with the state of the art and baseline 
methods. 
The major contributions of our work are as 
follows. 
? The essential process of human tagging 
process is discovered as the guideline to help 
us build simulating models. 
? A suitable model is proposed to assist our 
social tagging recommender system to learn 
the semantic relationship between the 
meaningful words in summaries and 
corresponding tags.  
? Based on the semantic relationship between 
the meaningful words in the summaries and 
corresponding tags, a weighted Tag-digraph is 
constructed. Then a TagRank algorithm is 
proposed to re-organize and rank the tags. 
Our new approach is also suitable in the tasks 
of keyword extraction, query expansion et al 
where the human associative behavior exists. Thus, 
we list several open problems that we will explore 
in the future: 
? Our approach can be expanded from lexical 
level to sentence level to bring the associative 
ability into semantic-related sentences 
extraction.  
? We will explore the effects on other research 
areas, such as keyword extraction, query 
expansion, where human associative behavior 
exists as well. 
 
Acknowledgements 
The work was supported by the research projects 
of National Science Foundation of China (Grant 
No. 60873173) and National 863 High-Tech research 
projects (Grant No. 2007AA01Z173). The authors 
would like to thank Yi Luo for his insightful 
suggestions. 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 2011. Modern 
information retrieval: the concepts and technology 
behind search, 2nd edition. ACM Press. 
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent 
dirichlet alocation. JMLR, 3:993-1022. 
S. Brin and L. Page. 1998. The anatomy of a large-scale 
hypertextual  web search engine. Computer networks 
and ISDN systems, 30 (1-7): 107-117. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: Parameter estimation. Computational 
linguistics, 19(2):263-311. 
I. Cantador, A. Bellog?n, D. Vallet. 2010. Content-based 
recommendation in social tagging systems. In 
Proceedings of ACM RecSys, pages 237-240. 
H. Cao, M. Xie, L. Xue, C. Liu, F. Teng, and Y. Huang. 
2009. Social tag predication base on supervised 
ranking model. In Proceeding of ECML/PKDD 2009 
Discovery Challenge Workshop, pages 35-48. 
A. P. Dempster, N. M. Laird, D. B. Rubin, et al 1977. 
Maximum likelihood from incomplete data via the 
em algorithm. Journal of the Royal Statistical Society. 
Series B (Methodological), 39 (1): 1-38. 
D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green. 
2007. Automatic generation of social tags for music 
recommendation. In Proceedings of NIPS, pages 
385-392. 
S. Fujimura, KO Fujimura, and H. Okuda. 2007. 
Blogosonomy: Autotagging any text using bloggers? 
knowledge. In Proceedings of WI, pages 205-212. 
J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. 
Riedl. 2004. Evaluating collaborative filtering 
recommender systems. ACM Transactions on 
Information Systems, 22(1):5-53. 
R. Jaschke, L. Marinho, A. hotho, L. Schmidt-Thieme, 
and G. Stumme. 2008. Tag recommendations in 
social bookmarking systems. AI Communications, 
21(4):231-247. 
R. Krestel, P. Fankharser, and W. Nejdl. 2009. Latent 
dirichlet alocation for tag recommendation. In 
Proceedings of ACM RecSys, pages 61-68. 
Z. Liu, X. Chen, M. Sun. 2011. A simple word trigger 
method for social tag suggestion. In Proceedings of 
EMNLP, pages 1577-1588. 
C. D. Manning. P. Raghavan, and H. Schtze. 2008. 
Introduction to information retrieval. Cambridge 
University Press, NY, USA. 
53
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing 
order into texts. In Proceedings of EMNLP, pages 
404-411. Poster.  
R. Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio. 
2010. Semantic tags generation and retrieval for 
online advertising. In Proceedings of CIKM, pages 
1089-1098. 
C. Musto, F. Narducci, M. de Gemmis, P. Lops, and G. 
Semeraro. 2009. STaR: a social tag recommender 
system. In Proceeding of ECML/PKDD 2009 
Discovery Challenge Workshop, pages 215-227. 
F. J. Och and H. Ney. 2003. A systematic comparison of 
various statistical alignment models. Computational 
linguistics, 29(1): 19-51. 
D. D. Palmer. 2010. Text preprocessing. Handbook of 
natural language processing 2nd edition, chapter 2. 
CRC Press. 
S. Rendle, L. Balby Marinho, A. Nanopoulos, and L. 
Schmidt-Thieme. 2009. Learning optimal ranking 
with ensor factorization for tag recommendation. In 
Proceedings of KDD, pages 727-726. 
F. Ricci, L. Rokach, B. Shapira and P. B. Kantor. 2011. 
Recommender Systems Handbook. Springer Press.  
X. Si and M. Sun. 2009. Tag-LDA for scalable real-time 
tag recommendation. Journal of Computational 
Information Systems, 6(1): 23-31. 
M. Sun. 2011.  Natural language processing based on 
naturally annotated web resources. Journal of 
Chinese Information Processing, 25(6): 26-32 
T. C. Zhou, H. Ma, M. R. Lyu, and I. King. 2010. 
UserRec: A user recommendation approach in social 
tagging systems. In Proceedings of AAAI, pages 
1486-1491. 
54
