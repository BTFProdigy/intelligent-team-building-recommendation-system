95
96
97
98
Terminological variants for document selection and
question/answer matching
Olivier Ferret Brigitte Grau Martine Hurault-Plantet
Gabriel Illouz Christian Jacquemin
LIMSI-CNRS
Bat.508 Universit? ParisXI
91403 Orsay, France
{ferret, grau, mhp, gabrieli, jacquemin}@limsi.fr
Abstract
Answering precise questions requires
applying Natural Language techniques
in order to locate the answers inside
retrieved documents. The QALC
system, presented in this paper,
participated to the Question Answering
track of the TREC8 and TREC9
evaluations. QALC exploits an analysis
of documents based on the search for
multi-word terms and their variations.
These indexes are used to select a
minimal number of documents to be
processed and to give indices when
comparing question and sentence
representations. This comparison also
takes advantage of a question analysis
module and recognition of numeric and
named entities in the documents.
1 Introduction
The Question Answering (QA) track at TREC8
and TREC9 is due to the recent need for more
sophisticated paradigms in Information
Retrieval (IR). Question answering generally
refers to encyclopedic or factual questions that
require concise answers. But current IR
techniques do not yet enable a system to give
precise answers to precise questions. Question
answering is thus an area of IR that calls for
Natural Language Processing (NLP) techniques
that can provide rich linguistic features as
output. Such NLP modules should be deeply
integrated in search and matching components
so that answer selection can be performed on
such linguistic features and take advantage of
them. In addition, IR and NLP techniques have
to collaborate in the resulting system in order to
cope with large-scale and broad coverage text
databases while deriving benefit from added
knowledge.
We developed a system for question
answering, QALC, evaluated in the framework
of the QA tracks at TREC8 and TREC9. The
QALC system comprises NLP modules for
multi-word term and named entity extraction
with a specific concern for term conflation
through variant recognition. Since named entity
recognition has already been described
extensively in other publications (Baluja 1999),
we present the contribution of terminological
variants to adding knowledge to our system.
The two main activities involving
terminology in NLP are term acquisition and
term recognition. Basically, terms can be viewed
as a particular type of lexical data. Term
variation may involve structural, morphological,
and semantic transformations of single or multi-
words terms (Fabre and Jacquemin, 2000).
In this paper, we describe how QALC uses
high level indexes, made of terms and variants,
to select among documents the most relevant
ones with regard to a question, and then to
match candidate answers with this question. In
the selection process, the documents first
retrieved by a search engine, are then
postfiltered and ranked through a weighting
scheme based on high level indexes, in order to
retain the top ranked ones. Similarly, all systems
that participated in TREC9 have a search engine
component that firstly selects a subset of the
provided database of about one million
documents. Since a search engine produces a
ranked list of relevant documents, systems then
have to define the highest number of documents
to retain. Indeed, having too many documents
leads to a question processing time that is too
long, but conversely, having too few documents
reduces the possibility of obtaining the correct
answer. For reducing the amount of text to
process, one approach consists of keeping one or
more relevant text paragraphs from each
document retrieved. Kwok et al(2000), for
instance use an IR engine that retrieves the top
300 sub-documents of about 300-550 words and,
on the other hand, the FALCON system
(Harabagiu et al 2000) performs a paragraph
retrieval stage after the application of a boolean
retrieval engine. These systems work on the
whole database and apply a bag-of-words
technique to select passages whereas QALC first
retains a large subset of documents, among
which it then selects relevant documents by
applying richer criteria based on the use of the
linguistic structures of the words.
QALC indexes, used for document selection,
are made of single and multi-word terms
retrieved by a 2-step procedure: (1)?automatic
term extraction from questions through part-of-
speech tagging and pattern matching and
(2)?automatic document indexing through term
recognition and variant conflation. As a result,
linguistic variation is explicitly addressed
through the exploitation of word paradigms,
contrarily to other approaches like the one taken
in COPSY (Schwarz 1988) where an
approximate matching technique between the
query and the documents implicitly takes it into
account. Finally, terms acquired at step?(1) and
indexes from step?(2) are also used by the
matching procedure between a question and the
relevant document sentences.
In the next section, we describe the
architecture of the QALC system. Then, we
present the question processing for term
extraction. We continue with the description of
FASTR, a transformational shallow parser that
recognizes and marks the extracted terms as well
as their linguistic variants within the documents.
The two following sections present the modules
of the QALC system where terms and variants
are used, namely the document selection and
question/answer matching modules. Finally, we
present the results obtained by the QALC
system as well as an evaluation of the
contribution of this NLP technique to the QA
task through the use of the reference collections
for the QA track. In conclusion, suggestions for
more ambitious, but still realistic, developments
using NLP are outlined.
2 System Overview
Natural Language Processing components in the
QALC system (see Figure 1) enrich the selected
documents with terminological indexes in order
to go beyond reasoning about single words. Rich
linguistic features are also used to deduce what a
question is about.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
The analysis of a question relies on a shallow
parser which spots discriminating patterns and
assigns categories to the question. The
categories correspond to the types of entities that
are likely to constitute the answer to the
question.
In order to select the best documents from
the results given by the search engine and to
locate the answers inside them, we work with
terms and their variants, i.e. morphologic,
syntactic and semantic equivalent expressions.
A term extractor has been developed, based on
syntactic patterns which describe complex
nominal phrases and their subparts. These terms
are used by FASTR (Jacquemin 1999), a
shallow transformational natural language
analyzer that recognizes their occurrences and
their variants. Each occurrence or variant
constitutes an index that is subsequently used in
the processes of document ranking and
question/document matching.
Documents are ordered according to a weight
computed thanks to the number and the quality
of the terms and variants they contain. For
example, original terms with proper names are
considered more reliable than semantic variants.
An analysis of the weight graph enables the
system to select a relevant subpart of the
documents, whose size varies along the
questions. This selection takes all its importance
when applying the last processes which consist
of recognizing named-entities and analyzing
each sentence to decide whether it is a possible
answer or not. As such processes are time
consuming we attempt to limit their application
to a minimal number of documents.
Named entities are recognized in the
documents and used to measure the similarity
between the document sentences and a question.
Named entities receive one of the following
types: person, organization, location (city or
place), number (a time expression or a number
expression). They are defined in a way similar to
the MUC task and recognized through a
combination of lexico-syntactic patterns and
significantly large lexical data.
Finally, the question/answer matching
module uses all the data extracted from the
questions and the documents by the preceding
modules. We developed a similarity measure
that attributes weights to each characteristic, i.e.
named entity tags and terms and variants, and
makes a combination of them. The QALC
system proposes long and short answers.
Concerning the short ones, the system focuses
on parts of sentences that contain the expected
named entity tags, when they are known, or on
the largest subpart without any terms of the
question.
3 Terms and Variants
3.1 Term extraction
For automatic acquisition of terms from
questions, we use a simple technique of filtering
through patterns of part-of-speech categories.
No statistical ranking is possible because of the
small size of the questions from which terms are
extracted. First, questions are tagged with the
help of the TreeTagger (Schmid 1999). Patterns
of syntactic categories are then used to extract
terms from the tagged questions. They are very
close to those described by Justeson and
Katz?(1995), but we do not include post-posed
prepositional phrases. The pattern used for
extracting terms is:
(((((JJ | NN | NP | VBG)) ? (JJ | NN | NP | VBG) (NP
| NN))) | (VBD) | (NN) | (NP) | (CD))
where NN are common nouns, NP proper nouns,
JJ adjectives, VBG gerunds, VBD past
participles and CD numeral determiners.
The longest string is acquired first and
substrings can only be acquired if they do not
begin at the same word as the superstring. For
instance, from the sequence nameNN ofIN theDT
USNP helicopterNN pilotNN shotVBD downRP,
the following four terms are acquired: U S
helicopter pilot, helicopter pilot, pilot, and
shoot.
The mode of acquisition chosen for terms
amounts to considering only the substructures
that correspond to an attachment of modifiers to
the leftmost constituents (the closest one). For
instance, the decomposition of US helicopter
pilot into helicopter pilot and pilot is equivalent
to extracting the subconstituents of the structure
[US [helicopter [pilot]]].
3.2 Variant recognition through FASTR
The automatic indexing of documents is
performed by FASTR (Jacquemin 1999), a
transformational shallow parser for the
recognition of term occurrences and variants.
Terms are transformed into grammar rules and
the single words building these terms are
extracted and linked to their morphological and
semantic families.
The morphological family of a single word w
is the set M(w) of terms in the CELEX database
(CELEX 1998) which have the same root
morpheme as w. For instance, the morphological
family of the noun maker is made of the nouns
maker, make and remake, and the verbs to make
and to remake.
The semantic family of a single word w is the
union S (w ) of the synsets  of WordNet1.6
(Fellbaum 1998) to which w belongs. A synset is
a set of words that are synonymous for at least
one of their meanings. Thus, the semantic family
of a word w is the set of the words w' such that
w' is considered as a synonym of one of the
meanings of w. The semantic family of maker,
obtained from WordNet1.6, is composed of
three nouns: maker, manufacturer, shaper and
the semantic family of c a r is car, auto,
automobile, machine, motorcar.
Variant patterns that rely on morphological
and semantic families are generated through
metarules. They are used to extract terms and
variants from the document sentences in the
TREC corpus. For instance, the following
pattern, named NtoSemArg, extracts the
occurrence making many automobiles as a
variant of the term car maker:
VM('maker') RP? PREP? (ART (NN|NP)? PREP)?
ART? (JJ?|?NN?|?NP |?VBD?|?VBG)[0-3] NS('car')
where RP are particles, PREP prepositions, ART
articles, and VBD, VBG verbs. VM('maker') is
any verb in the morphological family of the
noun maker and NS('car') is any noun in the
semantic family of car.
Relying on the above morphological and
semantic families, auto maker, auto parts
maker , car manufacturer, make autos, and
making many automobiles are extracted as
correct variants of the original term car maker
through the set of metarules used for the QA
track experiment. Unfortunately, some incorrect
variants are extracted as well, such as make
those cuts in auto produced by the preceding
metarule.
3.3 Document selection
The output of NLP-based indexing is a list of
term occurrences composed of a document
identifier d, a term identifier?a pair t(q,i)
composed of a question number q and a unique
index i?, a text sequence, and a variation
identifier v (a metarule). For instance, the
following index :
LA092690-0038 t(131,1)
making many automobiles NtoVSemArg
means that the occurrence making many
automobiles from document d=LA092690-0038
is obtained as a variant of term i=1 in question
q=131 (car maker) through the variation
NtoVSemArg given in Section 3.2.
Each document d selected for a question q is
associated with a weight. The weighting scheme
relies on a measure of quality of the different
families of variations described by
Jacquemin?(1999): non-variant occurrences are
weighted 3.0, morphological and morpho-
syntactic variants are weighted 2.0, and
semantic and morpho-syntactico-semantic
variants are weighted 1.0.
Since proper names are more reliable indices
than common names, each term t(q,i) receives a
weight P(t(q , i )) between 0 and 1.0
corresponding to its proportion of proper names.
For instance, President Cleveland's wife is
weighted 2/3=0.66. Since another factor of
reliability is the length of terms, a factor |t(q,i)|
in the weighting formula denotes the number of
words in term t(q,i). The weight Wq(d) of a
query q  in a document d  is given by the
following formula (1). The products of the
weightings of each term extracted by the indexer
are summed over the indices I(d) extracted from
document d and normalized according to the
number of terms |T(q)| in query q.
  
W (d)
( ) ( ( ( , ))) ( , )
( )
q
( ( , ), ) ( )
=
? + ?
?
? w v P t q i t q i
T q
t q i v I d
1 2
         (1)
Mainly two types of weighting curves are
observed for the retrieved documents: curves
with a plateau and a sharp slope at a given
threshold (Figure 2.a) and curves with a slightly
decreasing weight (Figure 2.b).
The edge of a plateau is detected by examining
simultaneously the relative decrease of the slope
with respect to the preceding one, and the
relative decrease of the value with respect to the
preceding one. When a threshold is detected, we
only select documents before this threshold,
otherwise a fixed cutoff threshold is used. In our
experiments, for each query q, the 200 best
ranked documents retrieved by the search
engine1 were subsequently processed by the re-
indexing module. Our studies (Ferret et al 2000)
show that 200 is a minimum number such as
almost all the relevant documents are kept.
When no threshold was detected, we fixed the
value of the threshold to 100.
0
0
10
10
20
20
30
30
40
40
50
50
60
60
70
70
80
80
90
90
100
100
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
8
9
10
rank of the document
w
ei
gh
t
Question #87
rank of the document
Truncation of the ranked list
Question #86
w
ei
gh
t
(a)
(b)
Figure 2. Two types of weighting curve.
Through this method, the cutoff threshold is
8 for question #87 (Who followed Willy Brandt
as chancellor of the Federal Republic of
Germany?, Figure 2(a))2 and 100 for question
#86 (Who won two gold medals in skiing in the
Olympic Games in Calgary?, Figure 2(b)). As
indicated by Figure??2(a), there is an important
difference of weight between documents #8 and
#9. The weight of document #8 is 9.57 while the
                                                           
1 We used in particular Indexal (Loupy et al1998), a search
engine provided by Bertin Technologie.
2 Questions come from the TREC8 data.
weight of document #9 is 7.29 because the term
Federal Republic only exists in document #8.
This term has a high weight because it is
composed of two proper names.
4 Question-Answer Matching
4.1 Question type categorization
Question type categorization is performed in
order to assign features to questions and use
these features for the similarity measurement
between a question and potential answer
sentences. Basically, question categorization
allows the prediction of the kind(s) of answer,
called target (for instance, NUMBER).
Sentences inside the retrieved documents are
labeled with the same tags as questions. During
the similarity measurement, the more the
question and a sentence share the same tags, the
more they are considered as involved in a
question-answer relation. For example:
Question:
How many people live in the Falklands?
?> target = NUMBER
Answer:
F a l k l a n d s  p o p u l a t i o n  o f  <bnumex
TYPE=NUMBER> 2,100 <enumex> is
concentrated.
We established 17 types of answer. Some
systems define more categories. For instance
Prager et al (2000) identify about 50 types of
answer.
4.2 Answer Selection
In the QALC system, we have taken the
sentence as a basic unit because it is large
enough to contain the answer to questions about
simple facts and to give a context that permits
the user to judge if the suggested answer is
actually correct. The module associates each
question with the Na most similar sentences (Na
is equal to 5 for the QA task at TREC).
The overall principle of the selection process
is the following: each sentence from the
documents selected for a question is compared
with this question. To perform this comparison,
sentences and questions are turned into vectors
that contain three kinds of elements: content
words, term identifiers and named entity tags. A
specific weight (between 0 and 1.0) is associated
with each of these elements in order to express
their relative importance.
The content words are the lemmatized forms
of mainly adjectives, verbs and nouns such as
they are given by the TreeTagger. Each content
word in a vector is weighted according to its
degree of specificity in relation to the corpus in
which answers are searched through the tf.idf
weighting scheme. For questions, the term
identifiers refer to the terms extracted by the
term extractor described in Section?3.1 and
receive a fixed weight. In sentence vectors, term
identifiers are associated with the normalized
score from the ranking module (see Section 3.3).
The named entity tags correspond to the possible
types of answers, provided by the question
analysis module. In each sentence these tags
delimit the named entities that were recognized
by the corresponding module of the QALC
system and specify their type. Unlike term
identifiers, named entity tags are given the same
fixed weight in both sentence and question
vectors because the matching module uses the
types of the named entities and not their values.
In our experiments, the linguistic features
(terms and named entities) are used to favor
appropriate sentences when they have not
enough content words in common with the
question or when the question only contains a
few content words. Thus, the weights of term
identifiers or named entity tags are reduced by
applying a coefficient in order to be globally
lower than the weights of the content words.
Finally, the comparison between a sentence
vector Vd and a question vector Vq is achieved
by computing the following similarity measure:
?
?
=
j j
i i
dq
wq
wd
VVsim ),( (2)
where wqj is the weight of an element in the
question vector and wdi is the weight of an
element in a sentence vector that is also in the
question vector. This measure evaluates the
proportion and the importance of the elements in
the question vector that are found in the
sentence vector with regards to all the elements
of the question vector. Moreover, when the
similarity value is nearly the same for two
sentences, we favor the one in which the content
words of the question are the least scattered.
The next part gives an example of the
matching operations for the TREC8 question
Q16 What two US biochemists won the Nobel
Prize in medicine in 1992? This question is
turned into the following vector:
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) <PERSON> (0.5)
16.01 (0.5) 16.04 (0.5)
where <PERSON> is the expected type of the
answer, 16.01 is the identifier of the U S
biochemist term and 16.04 is the identifier of the
Nobel Prize term.
The same kind of vector is built for the
sentence <NUMBER> Two </NUMBER> US
biochemists, <PERSON> Edwin Krebs
</PERSON> and <CITY> Edmond </CITY>
Fischer, jointly won the <NUMBER> 1992
</NUMBER> Nobel Medicine Prize for work
that could advance the search for an anti-cancer
drug, coming from the document FT924-14045
that was selected for the question Q163 :
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) Edwin (0.0)
Krebs (0.0) Edmond (0.0) Fischer (0.0)
work (0.0) advance (0.0) search (0.0)
anti-cancer (0.0) jointly (0.0) drug (0.0)
<PERSON> (0.5) <NUMBER> (0.0) <CITY>(0.0)
16.01 (0.5) 16.04 (0.3)
where the weight 0.0 is given to the elements
that are not part of the question vector. The term
US biochemist is found with no variation and
Nobel Prize appears as a syntactic variant.
Finally, according to (2), the similarity measure
between theses two vectors is equal to 0.974.
5 Results and Evaluation
We sent to TREC9 three runs whose variations
concern the searched engine used and the length
of the answer (250 or 50 characters). Among
those runs, the best one obtained a score of
0.407 with 375 correct answers among 682
questions, for answers of 250 characters length.
The score computed by NIST is the reciprocal
mean of the rank, from 1 to 5, of the correct
                                                           
3 This sentence is taken from the output of the named entity
recognizer.
answer. With this score, the QALC system was
ranked 6th among 25 participants at TREC 9
QA task.
Document selection relies on a quantitative
measure, i.e. the document weight, whose
computation is based on syntactic and semantic
indices, i.e. the terms and the terminological
variants. Those indices allow the system to take
into account words as well as group of words
and their internal relations within the
documents. Following examples, that we have
got from selected documents for TREC9 QA
task, show what kind of indices are added to the
question words.
For the question 252 When was the first flush
toilet invented? , one multi-word extracted term
is flush toilet. This term is marked by FASTR
when recognized in a document, but it is also
marked when a variant is found, as for instance
low-flush toilet in the following document
sentence where low-flush is recognized as
equivalent to flush:
Santa Barbara , Calif. , is giving $ 80 to
anyone who converts to a low-flush toilet.
252.01   flush toilet[JJ][NN]
             low-flush[flush][JJ] toilet[toilet][NN]
             1.00
In the given examples, after the identification
number of the term, appears the reference term,
made of the lemmatized form of the words and
their syntactic category, followed by the variant
found in the sentence, with each word, its
lemmatized form and its category, and finally its
weight.
In the example above, the term found in the
sentence is equivalent to the reference term, and
thus its weight is 1.00.
The second example shows a semantic
variant. Salary and average salary are terms
extracted from the question 337, What's the
average salary of a professional baseball player
?. The semantic variant pay, got from WordNet,
was recognized in the following sentence?:
Did the NBA union opt for the courtroom
because its members, whose average pay tops
$500000 a year, wouldn't stand still for a
strike over free agency ?
337.01    salary[NN] pay[pay][NN] 0.25
337.00    average [JJ]salary[NN]
               average[average][JJ] pay[pay][NN]
               0.40
In order to evaluate the efficiency of the
selection process, we proceeded to several
measures. We apply our system on the material
given for the TREC8 evaluation, one time with
the selection process, and another time without
this process. At each time, 200 documents were
returned by the search engine for each of the 200
questions. When selection was applied, at most
100 documents were selected and subsequently
processed by the matching module. Otherwise,
the 200 documents were processed. The system
was scored by 0.463 in the first case, and by
0.452 in the second case. These results show
that the score increases when processing less
documents above all because it is just the
relevant documents that are selected.
The benefit from performing such a selection
is also illustrated by the results given in Table 1,
computed on the TREC9 results.
Number of documents selected
by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct answers 175
(51%)
200
(59%)
Number of correct answer at
rank 1
88
(50%)
128
(64%)
Table 1. Evaluation of the ranking process
We see that the selection process discards a
lot of documents for 50% of the questions (340
questions are processed from less than 100
documents). The document set retrieved for
those questions had a weighting curve with a
sharp slope and a plateau as in Figure 2(a).
QALC finds more often the correct answer and
in a better position for these 340 questions than
for the 342 remaining ones. The average number
of documents selected, when there are less than
100, is 37. These results are very interesting
when applying such time-consuming processes
as  named ent i ty  recogni t ion and
question/sentence matching. Document selection
will also enable us to apply later on syntactic
and semantic sentence analysis.
6 Conclusion
The goal of a question-answering system is to
find an answer to a precise question, with a
response time short enough to satisfy the user.
As the answer is searched within a great amount
of documents, it seems relevant to apply mainly
numerical methods because they are fast. But, as
we said in the introduction, precise answers
cannot be obtained without adding NLP tools to
IR techniques. In this paper, we proposed a
question answering system which uses
terminological variants first to reduce the
number of documents to process while
increasing the system performance, and then to
improve the matching between a question and its
potential answers. Furthermore, reducing the
amount of text to process will afterwards allow
us to apply more complex methods such as
semantic analysis. Indeed, TREC organizers
foresee a number of possible improvements for
the future?: real-time answering, evaluation and
justification of the answer, completeness of the
answer which could result from answers
distributed along multiple documents, and
finally interactive question answering so that the
user could specify her/his intention. All those
improvements require more data sources as well
as advanced reasoning about pragmatic and
semantic knowledge.
Thus, the improvements that we now want to
bring to our system will essentially pertain to a
semantic and pragmatic approach. For instance,
WordNet that we already use to get the semantic
variants of a word, will be exploited to refine
our set of question types. We also plan to use a
shallow syntactico-semantic parser in order to
construct a semantic representation of both the
potential answer and the question. This
representation will allow QALC to select the
answer not only from the terms and variants but
also from the syntactic and semantic links that
terms share with each other.
References
Baluja, S., Vibhu O. M., Sukthankar, R. 1999
Applying machine learning for high performance
named-entity extraction. P r o c e e d i n g s
PACLING'99 Waterloo, CA. 365-378.
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS, pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of  Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
Justeson J., Katz S. 1995. Technical terminology:
some linguistic properties and an algorithm for
identification in texte. Natural Language
Engineering. 1: 9-27.
Kwok K.L., Grunfeld L., Dinstl N., Chan M. 2000.
TREC9 Cross Language, Web and Question-
Answering Track experiments using PIRCS. Pre-
proceedings of TREC9, Gaithersburg, MD, NIST
Eds. 26-35.
Loupy C. , Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC (1998), 382-389.
Prager J., Brown, E., Radev, D., Czuba, K. (2000),
One Search Engine or two for Question-
Answering, NISTs, Eds., Proceedings of TREC9,
Gaithersburg, MD. 250-254.
Schmid H. 1999. Improvments in Part-of-Speech
Tagging with an Application To German.
Natural?Language Processing Using Very Large
Corpora, Dordrecht, S. Armstrong, K. W. Chuch,
P. Isabelle, E. Tzoukermann,  D. Yarowski, Eds.,
Kluwer Academic Publisher.
Schwarz C. 1988. The TINA Project: text content
analysis at the Corporate Research Laboratories at
Siemens. Proceedings of Intelligent Multimedia
Information Retrieval Systems and Management
(RIAO?88) Cambridge, MA. 361-368.
Proceedings of the ACL 2014 Student Research Workshop, pages 34?40,
Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational Linguistics
A Mapping-Based Approach for General Formal
Human Computer Interaction Using Natural Language
Vincent Letard
LIMSI CNRS
letard@limsi.fr
Sophie Rosset
LIMSI CNRS
rosset@limsi.fr
Gabriel Illouz
LIMSI CNRS
illouz@limsi.fr
Abstract
We consider the problem of mapping nat-
ural language written utterances express-
ing operational instructions1 to formal lan-
guage expressions, applied to French and
the R programming language. Developing
a learning operational assistant requires
the means to train and evaluate it, that is,
a baseline system able to interact with the
user. After presenting the guidelines of
our work, we propose a model to repre-
sent the problem and discuss the fit of di-
rect mapping methods to our task. Finally,
we show that, while not resulting in excel-
lent scores, a simple approach seems to be
sufficient to provide a baseline for an in-
teractive learning system.
1 Introduction
Technical and theoretical advances allow achiev-
ing more and more powerful and efficient opera-
tions with the help of computers. However, this
does not necessarily make it easier to work with
the machine. Recent supervised learning work
(Allen et al, 2007; Volkova et al, 2013) exploited
the richness of human-computer interaction for
improving the efficiency of a human performed
task with the help of the computer.
Contrary to most of what was proposed so far,
our long term goal is to build an assistant system
learning from interaction to construct a correct for-
mal language (FL) command for a given natural
language (NL) utterance, see Table 1. However,
designing such a system requires data collection,
and early attempts highlighted the importance of
usability for the learning process: a system that is
hard to use (eg. having very poor performance)
1We call operational instruction the natural language ex-
pression of a command in any programming language.
would prevent from extracting useful learning ex-
amples from the interaction. We thus need to pro-
vide the system with a basis of abilities and knowl-
edge to allow both incremental design and to keep
the interest of the users, without which data turn
to be way more tedious to collect. We assume that
making the system usable requires the ability to
provide help to the user more often than it needs
help from him/her, that is an accuracy over 50%.
We hypothesize that a parametrized direct
mapping between the NL utterances and the FL
commands can reach that score. A knowledge set
K is built from parametrized versions of the asso-
ciations shown in Table 1. The NL utterance U
best
from K that is the closest to the request-utterance
according to a similarity measure is chosen and its
associated command C(U
best
) is adapted to the
parameters of the request-utterance and returned.
For example, given the request-utterance U
req
:
?Load the file data.csv?, the system should rank
the utterances of K by similarity with U
req
. Con-
sidering the associations represented in Table 1,
the first utterance should be the best ranked, and
the system should return the command:
?var1 <- read.csv("data.csv")?.
Note that several commands can be proposed at
the same time to give the user alternate choices.
We use Jaccard, tf-idf, and BLEU similarity
measures, and consider different selection strate-
gies. We highlight that the examined similarity
measures show enough complementarity to permit
the use of combination methods, like vote or sta-
tistical classification, to improve a posteriori the
efficiency of the retrieval.
2 Related Work
2.1 Mapping Natural Language to Formal
Language
Related problems have been previously processed
using different learning methods. Branavan (2009,
34
NL utterances FL commands (in R)
1 Charge les donne?es depuis ?res.csv? var1=read.csv("res.csv")Load the data from ?res.csv?
2 Trace l?histogramme de la colonne 2 de tab plot(hist(tab[[2]]))Draw a bar chart with column 2 of tab
3 Dessine la re?partition de la colonne 3 de tab plot(hist(tab[[3]]))Draw the distribution of column 3 of tab
4 Somme les colonnes 3 et 4 de tab var2=c(sum(tab[3]),sum(tab[4]))Compute the sum of columns 3 and 4 of tab
5 Somme les colonnes 3 et 4 de tab var3=sum(c(tab[[3]],tab[[4]]))Compute the sum of columns 3 and 4 of tab
Table 1: A sample of NL utterances to FL commands mapping
These examples specify the expected command to be returned for each utterance. The tokens in bold
font are linked with the commands parameters, cf. section 4. Note that the relation between utterances
and commands is a n to n. Several utterances can be associated to the same command and conversely.
2010) uses reinforcement learning to map En-
glish NL instructions to a sequence of FL com-
mands. The mapping takes high-level instructions
and their constitution into account. The scope
of usable commands is yet limited to graphical
interaction possibilities. As a result, the learn-
ing does not produce highly abstract schemes. In
the problematic of interactive continuous learning,
Artzi and Zettlemoyer (2011) build by learning a
semantic NL parser based on combinatory cate-
gorial grammars (CCG). Kushman and Barzilay
(2013) also use CCG in order to generate regu-
lar expressions corresponding to their NL descrip-
tions. This constructive approach by translation
allows to generalize over learning examples, while
the expressive power of regular expressions cor-
respond to the type-3 grammars of the Chomsky
hierarchy. This is not the case for the program-
ming languages since they are at least of type-2.
Yu and Siskind (2013) use hidden Markov mod-
els to learn a mapping between object tracks from
a video sequence and predicates extracted from
a NL description. The goal of their approach is
different from ours but the underlying problem of
finding a map between objects can be compared.
The matched objects constitute here a FL expres-
sion instead of a video sequence track.
2.2 Machine Translation
Machine translation usually refers to transforming
a NL sentence from a source language to another
sentence of the same significance in another natu-
ral language, called target language. This task is
achieved by building an intermediary representa-
tion of the sentence structure at a given level of
abstraction, and then encoding the obtained object
into the target language. While following a dif-
ferent goal, one of the tasks of the XLike project
(Marko Tadic? et al, 2012) was to examine the
possibility of translating statements from NL (En-
glish) to FL (Cycl). Adapting such an approach
to operational formal target language can be inter-
esting to investigate, but we will not focus on that
track for our early goal.
2.3 Information Retrieval
The issue of information retrieval systems can be
compared with the operational assistant?s (OA),
when browsing its knowledge. Question an-
swering systems in particular (Hirschman and
Gaizauskas, 2001), turn out to be similar to OA
since both types of systems have to respond to a
NL utterance of the user by generating an accu-
rate reaction (which is respectively a NL utterance
containing the wanted information, or the execu-
tion of a piece of FL code). However, as in (Toney
et al, 2008), questions answering systems usually
rely on text mining to retrieve the right informa-
tion. Such a method demands large sets of anno-
tated textual data (either by hand or using an au-
tomatic annotator). Yet, tutorials, courses or man-
uals which could be used in order to look for re-
sponses for operational assistant systems are het-
erogeneous and include complex or implicit ref-
erences to operational knowledge. This makes
the annotation of such data difficult. Text min-
ing methods are thus not yet applicable to oper-
ational assistant systems but could be considered
once some annotated data is collected.
35
3 Problem Formulation
As we introduced in the first section, we represent
the knowledge K as a set of examples of a binary
relation R : NL ? FL associating a NL utter-
ance to a FL command. If we consider the simple
case of a functional and injective relation, each
utterance is associated to exactly one command.
This is not realistic since it is possible to reformu-
late nearly any NL sentence. The case of a non in-
jective relation covers better the usual cases: each
command can be associated with one or more ut-
terances, this situation is illustrated by the second
and third examples of Table 1. Yet, the real-life
case should be a non injective nor functional rela-
tion. Not only multiple utterances can refer to a
same command, but one single utterance can also
stand for several distinct commands (see the fourth
and fifth examples2 in Table 1). We must consider
all these associations when matching a request-
utterance U
req
for command retrieval in K .
At this point, several strategies can be used to
determine what to return, with the help of the sim-
ilarity measure ? : NL ? NL ? R between two
NL utterances. Basically, we must determine if
a response should be given, and if so how many
commands to return. To do this, two potential
strategies can be considered for selecting the as-
sociated utterances in K .
The first choice focuses on the number of re-
sponses that are given for each request-utterance.
The n first commands according to the rankings of
their associated utterances in K are returned. The
rank r of a given utterance U is computed with:
r(U |U
req
) =
?
?
?
U
?
? K : ?(U
req
, U
?
) > ?(U
req
, U)
?
?
?
(1)
The second strategy choice can be done by de-
termining an absolute similarity threshold below
which the candidate utterances from K and their
associated sets of commands are considered too
different to match. The resulting set of commands
is given by:
Res = {C ? FL : (U,C) ? K,?(U
req
, U) < t} (2)
with t the selected threshold. Once selected the
set of commands to be given as response, if there
are more than one, the choice of the one to execute
can be done interactively with the help of the user.
2The command 4 returns a vector of the sums of each col-
umn, while the command 5 returns the sum of the columns as
a single integer.
4 Approach
We are given a simple parsing result of both the ut-
terance and the command. The first step to address
is the acquisition of examples and the way to up-
date the knowledge. Then we examine the meth-
ods for retrieving a command from the knowledge
and a given request-utterance.
Correctly mapping utterances to commands re-
quires at least to take their respective parameters
into account (variable names, numeric values, and
quoted strings). We build generic representations
of utterances and commands by identifying the pa-
rameters in the knowledge example pair (see Ta-
ble 1), and use them to reconstruct the command
with the parameters of the request-utterance.
4.1 Retrieving the Commands
We applied three textual similarity measures to
our model in order to compare their strengths and
weaknesses on our task: the Jaccard similarity co-
efficient (Jaccard index), a tf-idf (Term frequency-
inverse document frequency) aggregation, and the
BLEU (Bilingual Evaluation Understudy) mea-
sure.
4.1.1 Jaccard index
The Jaccard index measures a similarity between
two sets valued in the same superset. For the
present case, we compare the set of words of the
input NL instruction and the one of the compared
candidate instruction, valued in the set of possible
tokens. The adapted formula for two sentences S
1
and S
2
results in:
J(s
1
, s
2
) =
|W (s
1
) ?W (s
2
)|
|W (s
1
) ?W (s
2
)|
(3)
where W (S) stands for the set of words of the
sentence S. The Jaccard index is a baseline to
compare co-occurences of unigrams, and should
be efficient mainly with corpora containing few
ambiguous examples.
4.1.2 tf-idf
The tf-idf measure permits, given a word, to clas-
sify documents on its importance in each one, re-
garding its importance in the whole set. This mea-
sure should be helpful to avoid noise bias when it
comes from frequent terms in the corpus. Here,
the documents are the NL utterances from K , and
they are classified regarding the whole request-
utterance, or input sentence s
i
. We then use the
36
following aggregation of the tf-idf values for each
word of s
i
.
tfidf
S
(s
i
, s
c
) =
1
|W (s
i
)|
X
w?W (s
i
)
tfidf(w, s
c
, S) (4)
with S = {s|(s, com) ? K}, where s
i
is the input
sentence, s
c
? S is the compared sentence, and
where the tf-idf is given by:
tfidf(w, s
c
, S) = f(w, s
c
)idf(w, S) (5)
idf(w,S) = log
?
|S|
|{s ? S|w ? s}|
?
(6)
where at last f(w, s) is the frequency of the word
w in the sentence s. As we did for the Jaccard in-
dex, we performed the measures on both raw and
lemmatized words. On the other hand, getting rid
of the function words and closed class words is not
here mandatory since the tf-idf measure already
takes the global word frequency into account.
4.1.3 The BLEU measure
The bilingual evaluation understudy algorithm
(Papineni et al, 2002) focuses on n-grams co-
occurrences. This algorithm can be used to dis-
card examples where the words ordering is too far
from the candidate. It computes a modified pre-
cision based on the ratio of the co-occurring n-
grams within candidate and reference sentences,
on the total size of the candidate normalized by n.
P
BLEU
(s
i
, S) =
X
gr
n
?s
i
max
s
c
?S
occ(gr
n
, s
c
)
grams(s
i
, n)
(7)
where grams(s, n) = |s| ? (n? 1) is the number
of n-grams in the sentence s and occ(gr
n
, s) =
?
gr
n
?
?s
[gr
n
= gr
n
?
] is the number of occur-
rences of the n-gram gr
n
in s. BLEU also uses
a brevity penalty to prevent long sentences from
being too disadvantaged by the n-gram based pre-
cision formula. Yet, the scale of the length of the
instructions in our corpus is sufficiently reduced
not to require its use.
4.2 Optimizing the similarity measure
We applied several combinations of filters to the
utterances compared before evaluating their sim-
ilarity. We can change the set of words taken
into account, discarding or not the non open-class
words3. Identified non-lexical references such as
3Open-class words include nouns, verbs, adjectives, ad-
verbs and interjections.
variable names, quoted character strings and nu-
meric values can also be discarded or transformed
to standard substitutes. Finally, we can apply or
not a lemmatization4 on lexical tokens.By discard-
ing non open-class words, keeping non-lexical ref-
erences and applying the lemmatization, the sec-
ond utterance of Table 1 would then become:
draw bar chart column xxVALxx xxVARxx
5 Experimental Setup
5.1 Parsing
The NL utterances first pass through an arith-
metic expression finder to completely tag them be-
fore the NL analyzer. They are then parsed us-
ing WMATCH, a generic rule-based engine for
language analysis developed by Olivier Galibert
(2009). This system is modular and dispose of
rules sets for both French and English. As an ex-
ample, the simplified parsing result of the first ut-
terance of Table 1 looks like:
<_operation>
<_action> charge|_?V </_action>
<_det> les </_det>
<_subs> donne?es|_?N </_subs>
<_prep> depuis </_prep>
<_unk> "res.csv" </_unk>
</_operation>
Words tagged as unknown are considered as po-
tential variable or function names. We also added
a preliminary rule to identify character strings and
count them among the possibly linked features of
the utterance. The commands are normalized by
inserting spaces between every non semantically
linked character pair and we identify numeric val-
ues, variable/function names and character strings
as features.
Only generative forms of the commands are
associated to utterances in the knowledge. This
form consists in a normalized command with unre-
solved references for every parameter linked with
the learning utterance. These references are re-
solved at the retrieving phase by matching with the
tokens of the request-utterance.
5.2 Corpus Constitution
Our initial corpus consists in 605 associations be-
tween 553 unique NL utterances in French and
240 unique R commands.
4Lemmatization is the process of transforming a word to
its canonical form, or lemma, ignoring the inflections. It can
be performed with a set of rules or with a dictionary. The
developed system uses a dictionary.
37
The low number of documents describing a
majority of R commands and their heterogeneity
make automatic example gathering not yet achiev-
able. These documentations are written for human
readers having global references on the task. Thus,
we added each example pair manually, making
sure that the element render all the example infor-
mation and that the format correspond to the cor-
pus specifications. Those specifications are meant
to be the least restrictive, that is: a NL utterance
must be written as to ask for the execution of the
associated R task. It therefore should be mostly
in the imperative form and reflect, for experienced
people, a usual way they would express the con-
cerned operation for non specialists.
5.3 Evaluation Metrics
The measures that can contribute to a relevant
evaluation of the system depend on its purpose.
Precision and recall values of information retrieval
systems are computed as follows:
P =
# correct responses
# responses given (8)
R =
# correct responses
# responses in K (9)
Note that the recall value is not as important as for
information retrieval: assuming that the situation
showed by the fourth and fifth associations of Ta-
ble 1 are not usual5, there should be few different
valid commands for a given request-utterance, and
most of them should be equivalent. Moreover, the
number of responses given is fixed (so is the num-
ber of responses in K), the recall thus gives the
same information as the precision, with a linear
coefficient variation.
These formulae can be applied to the ?command
level?, that is measuring the accuracy of the sys-
tem in terms of its good command ratio. However,
the user satisfaction can be better measured at the
?utterance level? since it represents the finest gran-
ularity for the user experience. We define the ut-
terance precision uP as:
uP =
# correct utterances
# responses given (10)
where ?# correct utterances? stands for the num-
ber of request-utterances for which the system pro-
vided at least one good command.
5Increasing the tasks covering of the corpus will make
these collisions more frequent, but this hypothesis seems rea-
sonable for a first approach.
6 Results and Discussion
The system was tested on 10% of the corpus (61
associations). The set of known associations K
contains 85% of the corpus (514 associations), in-
stead of 90% in order to allow several distinct
drawings (40 were tested), and thus avoid too
much noise.
6.1 Comparing similarity measures
As shown in Table 2 the tf-idf measure outper-
forms the Jaccard and BLEU measures, whichever
filter combination is applied. The form of the ut-
terances in the corpus causes indeed the repetition
of a small set of words across the associations.
This can explain why the inverse document fre-
quency is that better.
non-lexical included not included
lemmatize yes no yes no
Jaccard 36.5 36.5 21.2 23.0
tf-idf 48.0 51.9 36.5 40.4
BLEU 30.8 32.7 26.9 30.8
chance 1.9
Table 2: Scores of precision by utterance (uP ),
providing 3 responses for each request-utterance.
The lemmatization and the inclusion of non
open-class words (not shown here) does not seem
to have a clear influence on uP , whereas including
the non-lexical tokens allows a real improvement.
This behaviour must result from the low length av-
erage (7.5 words) of the utterances in the corpus.
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9
Number of responses
Ut
te
ra
n
ce
 p
re
cis
io
n 
(uP
)
measure
tfidf
tfidf_inl
Figure 1: Utterance precision (uP ) for a fixed
number of responses by utterance. The tfidf inl
curve includes the non-lexical tokens.
Note that uP is obtained with Equation 10, which
explains the increase of the precision along the
number of responses.
38
Figure 1 shows the precision obtained with tfidf
while increasing the number of commands given
for each request-utterance. It comes out that it
is useful to propose at least 3 commands to the
user. It would not be interesting, though, to offer a
choice of more than 5 items, because the gain on
uP would be offset by the time penalty for retriev-
ing the good command among the proposals.
6.2 Allowing silence
We also tested the strategy of fixing an absolute
threshold to decide between response and silence.
Given a request-utterance and an associated order-
ing of K according to ?, the system will remain
silent if the similarity of the best example in K is
below the defined threshold.
Surprisingly, it turned out that for every mea-
sure, the 6 best similar responses at least were all
wrong. This result seems to be caused by the ex-
istence, in the test set of commands uncovered by
K , of some very short utterances that contain only
one or two lexical tokens.
6.3 Combinations
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8 9
Number of responses
Ut
te
ra
n
ce
 p
re
cis
io
n 
(uP
)
method
vote
tfidf_inl
vote_oracle
learning
Figure 2: Comparison of the combinations with
the tf-idf inl method. Oracle and actual vote are
done using tf-idf, Jaccard, and BLEU, with and
without non-lexical tokens. The training set for
learning is the result of a run on K .
Having tested several methods giving differ-
ent results, combining these methods can be very
interesting depending on their complementarity.
The oracle vote using the best response among
the 6 best methods shows an encouraging progres-
sion margin (cf. Figure 2). The actual vote it-
self outperforms the best method for giving up to
3 responses (reaching 50% for only 2 responses).
However, the curve position is less clear for more
responses, and tests must be performed on other
drawings of K to measure the noise influence.
The complementarity of the methods can also
be exploited by training a classification model to
identify when a method is better than the others.
We used the similarity values as features and the
measure that gave a good response as the refer-
ence class label (best similarity if multiple, and
?none? class if no good response). This setup was
tested with the support vector machines using lib-
svm (Chang and Lin, 2011) and results are shown
in Figure 2. As expected, machine learning per-
forms poorly on our tiny corpus. The accuracy
is under 20% and the system only learned when
to use the best method, and when to give no re-
sponse. Still, it manages to be competitive with
the best method and should be tested again with
more data and multiple drawings of K .
7 Conclusion and Future Work
The simple mapping methods based on similar-
ity ranking showed up to 60% of utterance pre-
cision6 remaining below a reasonable level of user
sollicitation, which validate our prior hypothesis.
A lot of approaches can enhance that score, such
as adding or developing more suitable similarity
measures (Achananuparp et al, 2008), combining
learning and vote or learning to rerank utterances.
However, while usable as a baseline, these
methods only allow poor generalization and really
need more corpus to perform well. As we pointed
out, the non-functionality of the mapping relation
also introduces ambiguities that cannot be solved
using the only knowledge of the system.
Thanks to this baseline method, we are now able
to collect more data by developing an interactive
agent that can be both an intelligent assistant and
a crowdsourcing platform. We are currently de-
veloping a web interface for this purpose. Finally,
situated human computer interaction will allow the
real-time resolving of ambiguities met in the re-
trieval with the help of the user or with the use of
contextual information from the dialogue.
Aknowledgements
The authors are grateful to every internal and ex-
ternal reviewer for their valuable advices. We also
would like to thank Google for the financial sup-
port for the authors participation to the conference.
6The corpus will soon be made available.
39
References
Palakorn Achananuparp, Xiaohua Hu, and Xiajiong
Shen. 2008. The Evaluation of Sentence Similar-
ity Measures. In Data Warehousing and Knowledge
Discovery, Springer.
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Tayson. 2007. PLOW: A Collaborative
Task Learning Agent. In Proceedings of the 22nd
National Conference on Artificial Intelligence.
Yoav Artzi, and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. Pro-
ceedings of the conference on empirical methods in
natural language processing.
S.R.K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading Between the Lines: Learn-
ing to Map High-level Instructions to Commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
Chih-Chung Chang, and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology
Olivier Galibert. 2009. Approches et me?thodologies
pour la re?ponse automatique a` des questions
adapte?es a` un cadre interactif en domaine ouvert.
Doctoral dissertation, Universite? Paris Sud XI.
Lynette Hirschman, and Robert Gaizauskas. 2001.
Natural language question answering: The view
from here. Natural Language Engineering 7. Cam-
bridge University Press.
Nate Kushman, and Regina Barzilay. 2013. Using Se-
mantic Unification to Generate Regular Expressions
from Natural Language. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Marko Tadic?, Boz?o Bekavac, ?Zeljko Agic?, Matea
Srebac?ic?, Das?a Berovic?, and Danijela Merkler.
2012. Early machine translation based semantic an-
notation prototype XLike project www.xlike.org .
Dave Toney, Sophie Rosset, Aure?lien Max, Olivier
Galibert, and e?ric Billinski. 2008. An Evaluation of
Spoken and Textual Interaction on the RITEL Inter-
active Question Answering System In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation.
Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill
Dolan, and Luke Zettlemoyer. 2013. Lightly Su-
pervised Learning of Procedural Dialog System In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics.
Haonan Yu, and Jeffrey Mark Siskind. 2013.
Grounded Language Learning from Video De-
scribed with Sentences. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics.
40
Workshop on Monolingual Text-To-Text Generation, pages 10?19,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Web-based validation for contextual targeted paraphrasing
Houda Bouamor
LIMSI-CNRS
Univ. Paris Sud
hbouamor@limsi.fr
Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Gabriel Illouz
LIMSI-CNRS
Univ. Paris Sud
gabrieli@limsi.fr
Anne Vilnat
LIMSI-CNRS
Univ. Paris Sud
anne@limsi.fr
Abstract
In this work, we present a scenario where con-
textual targeted paraphrasing of sub-sentential
phrases is performed automatically to support
the task of text revision. Candidate para-
phrases are obtained from a preexisting reper-
toire and validated in the context of the orig-
inal sentence using information derived from
the Web. We report on experiments on French,
where the original sentences to be rewrit-
ten are taken from a rewriting memory au-
tomatically extracted from the edit history of
Wikipedia.
1 Introduction
There are many instances where it is reasonable to
expect machines to produce text automatically. Tra-
ditionally, this was tackled as a concept-to-text real-
ization problem. However, such needs apply some-
times to cases where a new text should be derived
from some existing texts, an instance of text-to-text
generation. The general idea is not anymore to pro-
duce a text from data, but to transform a text so as to
ensure that it has desirable properties appropriate for
some intended application (Zhao et al, 2009). For
example, one may want a text to be shorter (Cohn
and Lapata, 2008), tailored to some reader pro-
file (Zhu et al, 2010), compliant with some spe-
cific norms (Max, 2004), or more adapted for sub-
sequent machine processing tasks (Chandrasekar et
al., 1996). The generation process must produce
a text having a meaning which is compatible with
the definition of the task at hand (e.g. strict para-
phrasing for document normalization, relaxed para-
phrasing for text simplification), while ensuring that
it remains grammatically correct. Its complexity,
compared with concept-to-text generation, mostly
stems from the fact that the semantic relationship
between the original text and the new one is more
difficult to control, as the mapping from one text to
another is very dependent on the rewriting context.
The wide variety of techniques for acquiring phrasal
paraphrases, which can subsequently be used by text
paraphrasing techniques (Madnani and Dorr, 2010),
the inherent polysemy of such linguistic units and
the pragmatic constraints on their uses make it im-
possible to ensure that potential paraphrase pairs
will be substitutable in any context, an observation
which was already made at a lexical level (Zhao et
al., 2007). Hence, automatic contextual validation of
candidate rewritings is a fundamental issue for text
paraphrasing with phrasal units.
In this article, we tackle the problem of what we
call targeted paraphrasing, defined as the rewriting
of a subpart of a sentence, as in e.g. (Resnik et al,
2010) where it is applied to making parts of sen-
tences easier to translate automatically. While this
problem is simpler than full sentence rewriting, its
study is justified as it should be handled correctly
for the more complex task to be successful. More-
over, being simpler, it offers evaluation scenarios
which make the performance on the task easier to
assess. Our particular experiments here aim to as-
sist a Wikipedia contributor in revising a text to im-
prove its quality. For this, we use a collection of
phrases that have been rewritten in Wikipedia, and
test the substitutability of paraphrases coming from
a repertoire of sub-sentential paraphrases acquired
10
from different sources. We thus consider that preex-
isting repertoires of sub-sentential paraphrase pairs
are available, and that each potential candidate has to
be tested in the specific context of the desired rewrit-
ing. Due to the large variety of potential phrases
and their associated known paraphrases, we do not
rely on precomputed models of substitutability, but
rather build them on-the-fly using information de-
rived from web queries.1
This article is organized as follows. In section 2,
we first describe the task of text revision, where a
subpart of a sentence is rewritten, as an instance
of targeted paraphrasing. Section 3 presents previ-
ous works on the acquisition of sub-sentential para-
phrases and describes the knowledge sources that we
have used in this work. We then describe in section 4
how we estimate models of phrase substitution in
context by exploiting information coming from the
web. We present our experiments and their results in
section 5, and finally discuss our current results and
future work in section 6.
2 Targeted paraphrasing for text revision
One of the important processes of text revision is
the rewording of parts of sentences. Some reword-
ings are not intended to alter meaning significantly,
but rather to make text more coherent and easier to
comprehend. Those instances which express close
meanings are sub-sentential paraphrases: in their
simpler form, they can involve synonym substitu-
tion, but they can involve more complex deeper
lexical-syntactic transformations.
Such rephrasings are commonly found in record-
ings of text revisions, which now exist in large
quantities in the collaborative editing model of
Wikipedia. In fact, revision histories of the encyclo-
pedia contain a significant amount of sub-sentential
paraphrases, as shown by the study of (Dutrey et al,
2011). This study also reports that there is an impor-
tant variety of rephrasing phenomena, as illustrated
by the difficulty of reaching a good identification
coverage using a rule-based term variant identifica-
tion engine.
1Note that using the web may not always be appropriate, or
that at least it should be used in a different way than what we
propose in this article, in particular in cases where the desired
properties of the rewritten text are better described in controlled
corpora.
The use of automatic targeted paraphrasing as an
authoring aid has been illustrated by the work of
Max and Zock (2008), in which writers are pre-
sented with potential paraphrases of sub-sentential
fragments that they wish to reword. The automatic
paraphrasing technique used is a contextual vari-
ant of bilingual translation pivoting (Bannard and
Callison-Burch, 2005). It has also been proposed
to externalize various text editing tasks, including
proofreading, by having crowdsourcing functions on
text directly from word processors (Bernstein et al,
2010).
Text improvements may also be more specifi-
cally targeted for automatic applications. In the
work by Resnik et al (2010), rephrasings for spe-
cific phrases are acquired through crowdsourcing.
Difficult-to-translate phrases in the source text are
first identified, and monolingual contributors are
asked to provide rephrasings in context. Collected
rephrasings can then be used as input for a Ma-
chine Translation system, which can positively ex-
ploit the increased variety in expression to pro-
duce more confident translations for better estimated
source units (Schroeder et al, 2009).2 For instance,
the phrase in bold in the sentence The number of
people known to have died has now reached 358
can be rewritten as 1) who died, 2) identified to
have died and 3) known to have passed away. All
such rephrasings are grammatically correct, the first
one being significantly shorter, and they all convey
a meaning which is reasonably close to the original
wording.
The task of rewriting complete sentences has also
been addressed in various works (e.g. (Barzilay and
Lee, 2003; Quirk et al, 2004; Zhao et al, 2010)). It
poses, however, numerous other challenges, in par-
ticular regarding how it could be correctly evalu-
ated. Human judgments of whole sentence trans-
formations are complex and intra- and inter-judge
coherence is difficult to attain with hypotheses of
comparable quality. Using sentential paraphrases
to support a given task (e.g. providing alternative
reference translations for optimizing Statistical Ma-
chine Translation systems (Madnani et al, 2008))
2It is to be noted that, in the scenario presented in (Resnik et
al., 2010), monolingual contributors cannot predict how useful
their rewritings will be to the underlying Machine Translation
engine used.
11
can be seen as a proxy for extrinsic evaluation of
the quality of paraphrases, but it is not clear from
published results that improvements on the task are
clearly correlated with the quality of the produced
paraphrases. Lastly, automatic metrics have been
proposed for evaluating the grammaticality of sen-
tences (e.g. (Mutton et al, 2007)). Automatic evalu-
ation of sentential paraphrases has not produced any
consensual results so far, as they do not integrate
task-specific considerations and can be strongly bi-
ased towards some paraphrasing techniques.
In this work, we tackle the comparatively more
modest task of sub-sentential paraphrasing applied
to text revision. In order to use an unbiased
task, we use a corpus of naturally-occurring rewrit-
ings from an authoring memory of Wikipedia ar-
ticles. We use the WICOPACO corpus (Max and
Wisniewski, 2010), a collection of local rephras-
ings from the edit history of Wikipedia which con-
tains instances of lexical, syntactical and semantic
rephrasings (Dutrey et al, 2011), the latter type be-
ing illustrated by the following example:
Ce vers de Nuit rhe?nane d?Apollinaire [qui para??t
presque sans structure rythmique? dont la ce?sure
est comme masque?e]. . . 3
The appropriateness of this corpus for our work
is twofold: first, the fact that it contains naturally-
occurring rewritings provides us with an interest-
ing source of text spans in context which have been
rewritten. Moreover, for those instances where the
meaning after rewriting was not significantly al-
tered, it provides us with at least one candidate
rewriting that should be considered as a correct para-
phrase, which can be useful for training validation
algorithms.
3 Automatic sub-sentential paraphrase
acquisition and generation
The acquisition of paraphrases, and in particular
of sub-sentential paraphrases and paraphrase pat-
terns, has attracted a lot of works with the advent of
data-intensive Natural Language Processing (Mad-
nani and Dorr, 2010). The techniques proposed have
a strong relationship to the type of text corpus used
3This verse from Apollinaire?s Nuit Rhe?nane [which seems
almost without rhythmic structure ? whose cesura is as if
hidden]. . .
for acquisition, mainly:
? pairs of sentential paraphrases (monolingual
parallel corpora) allow for a good precision
but evidently a low recall (e.g. (Barzilay and
McKeown, 2001; Pang et al, 2003; Cohn et
al., 2008; Bouamor et al, 2011))
? pairs of bilingual sentences (bilingual parallel
corpora) allow for a comparatively better re-
call (e.g. (Bannard and Callison-Burch, 2005;
Kok and Brockett, 2010))
? pairs of related sentences (monolingual com-
parable corpora) allow for even higher recall
but possibly lower precision (e.g. (Barzilay
and Lee, 2003; Li et al, 2005; Bhagat and
Ravichandran, 2008; Dele?ger and Zweigen-
baum, 2009)
Although the precision of such techniques can in
some cases be formulated with regards to a prede-
fined reference set (Cohn et al, 2008), it should
more generally be assessed in the specific context
of some use of the paraphrase pair. This refers to
the problem of substituability in context (e.g. (Con-
nor and Roth, 2007; Zhao et al, 2007)), which is a
well studied field at the lexical level and the object of
evaluation campains (McCarthy and Navigli, 2009).
Contextual phrase substitution poses the additional
challenge that phrases are rarer than words, so that
building contextual and grammatical models to en-
sure that the generated rephrasings are both seman-
tically compatible and grammatical is more compli-
cated (e.g. (Callison-Burch, 2008)).
The present work does not aim to present any
original technique for paraphrase acquisition, but
rather focusses on the task of sub-sentential para-
phrase validation in context. We thus resort to some
existing repertoire of phrasal paraphrase pairs. As
explained in section 2, we use the WICOPACO cor-
pus as a source of sub-sentential paraphrases: the
phrase after rewriting can thus be used as a potential
paraphrase in context.4 To obtain other candidates
of various quality, we used two knowledge sources.
The first uses automatic pivot translation (Bannard
and Callison-Burch, 2005), where a state-of-the-art
4Note, however, that in our experiments we will ask our hu-
man judges to assess anew its paraphrasing status in context.
12
general-purpose Statistical Machine Translation sys-
tem is used in a two-way translation. The second
uses manual acquisition of paraphrase candidates.
Web-based acquisition of this type of knowledge has
already been done before (Chklovski, 2005; Espan?a
Bonet et al, 2009), and could be done by crowd-
sourcing, a technique growing in popularity in recent
years. We have instead formulated manual acquisi-
tion as a web-based game. Players can take parts in
two parts of the game, illustrated on Figure 3.
First, players propose sub-sentential paraphrases
in context for selected text spans in web documents
(top of Figure 3), and then raters can take part in as-
sessing paraphrases proposed by other players (bot-
tom of Figure 3). In order to avoid any bias, players
cannot evaluate games in which they played. Eval-
uation is sped up by using a compact word lattice
view for eliciting human judgments, built using the
syntactic fusion algorithm of (Pang et al, 2003).
Data acquisition was done in French to remain co-
herent with our experiments on the French corpus
of WICOPACO, and both players and raters were
native speakers. An important point is that in our
experiments the context of acquisition and of evalu-
ation were different: players were asked to generate
paraphrases in contexts that are different from those
of the WICOPACO corpus used for evaluation. To
this end, web snippets were automatically retrieved
for the various phrases of our dataset without con-
texts, so that sentences from the Web (but not from
Wikipedia) were used for manual paraphrase acqui-
sition. This allows us to simulate the availability of a
preexisting repertoire of (contextless) sub-sentential
paraphrases, and to assess the performance of our
contextual validation techniques on a possibly in-
compatible context.
4 Web-based contextual validation
Given a repertoire of potential phrasal paraphrases
and a context for a naturally-occurring rewriting, our
task consists in deciding automatically which poten-
tial paraphrases can be substituted with good confi-
dence for the original phrase. A concrete instantia-
tion of it could correspond to the proposal of Max
and Zock (2008), where such candidate rephrasings
could be presented in order of decreasing suitability
to a word processor user, possibly during the revi-
sion of a Wikipedia article.
The specific nature of the text units that we are
dealing with calls for a careful treatment: in the
general scenario, it is unlikely that any supervised
corpus would contain enough information for ap-
propriate modeling of the substituability in context
decision. It is therefore tempting to consider using
the Web as the largest available information source,
in spite of several of its known limitations, includ-
ing that data can be of varying quality. It has how-
ever been shown that a large range of NLP applica-
tions can be improved by exploiting n-gram counts
from the Web (using Web document counts as a
proxy) (Lapata and Keller, 2005).
Paraphrase identification has been addressed pre-
viously, both using features computed from an of-
fline corpus (Brockett and Dolan, 2005) and fea-
tures computed from Web queries (Zhao et al,
2007). However, to our knowledge previous work
exploiting information from the Web was limited to
the identification of lexical paraphrases. Although
the probability of finding phrase occurrences sig-
nificantly increases by considering the Web, some
phrases are still very rare or not present in search
engine indexes.
As in (Brockett and Dolan, 2005), we tackle our
paraphrase identification task as one of monolingual
classification. More precisely, considering an orig-
inal phrase p within the context of sentence s, we
seek to determine whether a candidate paraphrase p?
would be a grammatical paraphrase of p within the
context of s. We make use of a Support Vector Ma-
chine (SVM) classifier which exploits the features
described in the remainder of this section.
Edit distance model score Surface similarity on
phrase pairs can be a good indicator that they share
semantic content. In order to account for the cost
of transforming one string into the other, rather
than simply counting common words, we use the
score produced by the Translation Edit Rate met-
ric (Snover et al, 2010). Furthermore, we perform
this computation on strings of lemmas rather than
surface forms:5
5Note that because we computed the TER metric on French
strings, stemming and semantic matching through WordNet
were not activated.
13
Figure 1: Interface of our web-based game for paraphrase acquisition and evaluation. On the top, players reformulate
all text spans highlighted by the game creator on any webpage (a Wikipedia article on the example). On the bottom,
raters evaluate paraphrases proposed by sets of players using a compact word-lattice view. Note that in its standard
definition, the game attributes higher scores to paraphrase candidates that are highly rated and rarer.
hedit = TER(Lemorig, Lempara) (1)
Note that this model is not derived from informa-
tion from the Web, in contrast to all the models de-
scribed next.
Language model score The likelihood of a sen-
tence can be a good indicator of its grammatical-
ity (Mutton, 2006). Language model probabilities
can now be obtained from Web counts. In our ex-
periments, we used the Microsoft Web N-gram Ser-
vice6 for research (Wang et al, 2010) to obtain log
likelihood scores for text units.7 However, this score
is certainly not sufficient as it does not take the orig-
inal wording into account. We therefore used a ratio
of the language model score of the paraphrased sen-
tence with the language model score of the original
6http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
7Note that in order to query on French text, we had to re-
move all diacritics for the service to behave correctly, indepen-
dently of encodings: careful examination of ranked hypotheses
showed that this trick allowed us to obtain results coherent with
expectations.
sentence, after normalization by sentence length of
the language model scores (Onishi et al, 2010):
hLM ratio =
LM(para)
LM(orig)
=
lm(para)1/length(para)
lm(orig)1/length(orig)
(2)
Contextless thematic model scores Cooccurring
words are used in distributional semantics to account
for common meanings of words. We build vector
representations of cooccurrences for both the origi-
nal phrase p and its paraphrase p?. Our contextless
thematic model is built in the following fashion: we
query a search engine to retrieve the top N docu-
ment snippets for phrase p. We then count frequen-
cies for all content words in these snippets, and keep
the set W of words appearing more than a fraction
of N . We then build a vector T (thematic profile)
of dimension |W | where values are computed by the
following formula:
Tnocontorig [w] =
count(p, w)
count(p)
(3)
14
where count(x) correspond to the number of docu-
ments containing a given exact phrase or word ac-
cording to the search engine used and count(x, y)
correspond to the number of documents containing
simultaneously both. We then compute the same
thematic profile for the paraphrase p?, using only the
subset of words W :
Tnocontpara [w] =
count(p?, w)
count(p)
(4)
Finally, we compute a similarity between the two
profiles by taking the cosinus between their two vec-
tors:
hnocontthem =
Tnocontorig ? T
nocont
para
||Tnocontorig || ? ||T
nocont
para ||
(5)
In all our experiments, we used the Yahoo! Search
BOSS8 Web service for obtaining Web counts and
retrieving snippets. Assuming that the distribution
of words in W is not biased by the result ordering
of the search engine, our model measures some sim-
ilarity between the most cooccurring content words
with p and the same words with p?.
Context-aware thematic model scores Our
context-aware thematic model takes into account
the words of sentence s in which the substitution
of p with p? is attempted. We now consider the set
of content words from s (s being the part of the
sentence without phrase p) in lieu of the previous
set of cooccurring words W , and compute the
same profile vectors and similarity between that of
the original sentence and that of the paraphrased
sentence:
hcontthem =
T contorig ? T
cont
para
||T contorig || ? ||T
cont
para||
(6)
However, words from s might not be strongly
cooccurring with p. In order to increase the likeli-
hood of finding thematically related words, we also
build an extended context model, hextcontthem where
content words from s are supplemented with their
most cooccurring words. This is done using the
same procedure as that previously used for finding
content words cooccurring with p.
8http://developer.yahoo.com/search/boss/
5 Experiments
In this section we report on experiments conducted
to assess the performance of our proposed approach
for validating candidate sub-sentential paraphrases
using information from the Web.
5.1 Data used
We randomly extracted 150 original sentences in
French and their rewritings from the WICOPACO
corpus which were marked as paraphrases. Of those,
we kept 100 for our training corpus and the remain-
ing 50 for testing. The number of original phrases of
each length is reported on Figure 2.
phrase length 1 2 3 4 5 6 7 8
original phrases 0 3 29 8 6 2 2 0
paraphrases 39 64 74 36 21 10 5 1
Figure 2: Distribution of number of phrases per phrase
length in tokens for the test corpus
For each original sentence, we collected 5 candi-
date paraphrases to simulate the fact that we had a
repertoire of paraphrases with the required entries:9
? WICOPACO: the original paraphrase from the
WICOPACO corpus;
? GAME: two candidate paraphrases from users
of our Web-based game;
? PIVOTES and PIVOTZH: two candidate para-
phrases obtained by translation by pivot, using
the Google Translate10 online SMT system and
one language close to French as pivot (Span-
ish), and another one more distant (Chinese).
We then presented the original sentence and its 5
paraphrases (in random order) to two judges. Four
native speakers took part in our experiments: they
all took part in the data collection for one half of
the sentences of the training and test corpora and to
the evaluation of paraphrases for the other half. For
the annotation with two classes (paraphrase vs. not
paraphrase), we obtain as inter-judge agreement11 a
9Note that, as a consequence, we did not carry any experi-
ment related to the recall of any technique here.
10http://translate.google.com
11We used R (http://www.r-project.org) to com-
pute this Cohen?s ? value.
15
Figure 3: Example of an original sentence and its 5 associated candidate paraphrases. The phrase in bold from the
original sentence (The brand is at the origin of many concepts that have revolutionized computing.) is paraphrased
as est le promoteur (is the promoter), a popularise? (popularized), origine (origin), est a` la source (is the source), and
l?origine (the origin).
value of ? = 0.65, corresponding to a substantial
agreement according to the literature. An example
of the interface used is provided in Figure 3.
We considered that our technique could not pro-
pose reliable results when web phrase counts were
too low. From the distribution of counts of phrases
and paraphrases from our training set (see Figure 4),
we empirically chose a threshold of 10 for the min-
imum count of any phrase. Our corpus was conse-
quently reduced from 750=150*5 to 434 examples
for the training corpus, and from 250=50*5 to 215
for the test corpus.
  <10 <100 <1000 <10000 <100000 <1000000 >10000000
1020
3040
5060
7080
90100 # of original phrases# of paraphrases
Range of number of counts
Figure 4: Number of phrases and paraphrases per web
count range
Results will be reported for three conditions:
? Possible: the gold standard for instances where
at least one of the judges indicated ?para-
phrases? records the pair as a paraphrase. In
this condition, the test set has 116 instances that
are paraphrases and 99 that are not.
? Sure: the gold standard for instances where not
all judges indicated ?paraphrases? records the
pair as not paraphrase. In this condition, the
test set has 76 instances that are paraphrases
and 139 that are not.
? Surer: only those instances where both judges
agree are recorded. This reduces our training
and test set to respectively 287 and 175 exam-
ples. Thus, results on this subcorpora will not
be directly comparable with the other results.
In this condition, the test set has 76 instances
that are paraphrases and 99 that are not.
5.2 Baseline techniques
Web-count based baselines We used two base-
lines based on simple Web counts. The first one,
WEBLM, considers a candidate sentence a para-
phrase of the original sentence whenever its Web
language model score is higher than that of the orig-
inal phrase. The second one, BOUNDLM, considers
a sentence as a paraphrase whenever the counts for
the bigrams crossing the left and right boundary of
the sub-sentential paraphrase is higher than 10.
Syntactic dependency baseline When rewriting a
subpart of a sentence, the fact that syntactic depen-
dencies between the rewritten phrase and its con-
text are the same than those of the original phrase
and the same context can provide some information
16
about the grammatical and semantic substituability
of the two phrases (Zhao et al, 2007; Max and Zock,
2008). We thus build syntactic dependencies for
both the original and rewritten sentence, using the
French version (Candito et al, 2010) of the Berkeley
probabilistic parser (Petrov and Klein, 2007), and
consider the subset of dependencies for the two sen-
tences that exist between a word inside the phrase
under focus and a word outside it (Deporig and
Deppara). Our CONTDEP baseline considers a sen-
tence as a paraphrase iff Deppara = Deporig.
5.3 Evaluation results
We used the models described in Section 4 to build
a SVM classifier using the LIBSVM package (Chang
and Lin, 2001). Accuracy results are reported on
Figure 5.
WEBLM BOUNDLM CONTDEP CLASSIFIER
POSSIBLE 62.79 54.88 48.53 57.67
SURE 68.37 36.27 51.90 70.69
SURER 56.79 51.41 42.69 62.85
Figure 5: Accuracy results for the three baselines and our
classifier on the test set for the three conditions. Note that
the SURER condition cannot be directly compared with
the other two as the number of training and test examples
are not the same.
The first notable observation is that our task is not
surprisingly a difficult one. The best performance
achieved is an accuracy of 70.69 with our system in
the SURE condition. There are, however, some im-
portant variations across conditions, with a result as
low as 57.67 for our system in the POSSIBLE condi-
tion (recall that in this condition candidates are con-
sidered paraphrases when only one of the two judges
considered it a paraphrase, i.e. when the two judges
disagreed).
Overall, the WEBLM baseline and our system ap-
pear as stronger than the two other baselines. The
two lower baselines, BOUNDLM and CONTDEP, at-
tempt to model local grammatical constraints, which
are not surprisingly not sufficient for paraphrase
identification. WEBLM is comparatively a much
more competitive baseline, but its accuracy in the
SURER condition is not very strong. As this latter
condition considers only consensual judgements for
the two judges, we can hypothesize that the interpre-
tation of its results is more reliable. In this condi-
WICOPACO GAMERS PIVOTES PIVOTZH
POSSIBLE 89.33 67.00 47.33 20.66
SURE 64.00 44.50 31.33 10.66
SURER 86.03 57.34 37.71 12.60
Figure 6: Paraphrase accuracy of our different paraphrase
acquisition methods for the three conditions.
tion, our system obtains the best performance, with
a +6.06 advantage over WEBLM. As found in other
works (e.g. (Bannard and Callison-Burch, 2005)),
using language models for paraphrase validation is
not sufficient as it cannot model meaning preserva-
tion, and our results show that this is also true even
when counts are estimated from the Web. Using a
ratio of normalized LM scores may have improved
the situation a bit.12
Lastly, we report in Figure 6 the paraphrase
accuracy of each individual acquisition technique
(i.e. source of paraphrases from the preexisting
repertoire). The original rewritting from WICO-
PACO obtains not surprisingly a very high para-
phrase accuracy, in particular in the POSSIBLE and
SURER conditions. Paraphrases obtained through
our Web-based game have an acceptable accuracy:
the numbers confirm that paraphrase pairs are highly
context-dependent, because the pairs which were
likely to be paraphrases in the context of the game
are not necessarily so in a different context. This,
of course, may be due to a number of reasons that
we will have to investigate. Lastly, there is a signif-
icant drop in accuracy for the automatic pivot para-
phrasers, but pivoting through Spanish obtained, not
suprisingly again, a much better performance than
pivoting through Chinese.
6 Discussion and future work
We have presented an approach to the task of
targeted paraphrasing in the context of text revi-
sion, a scenario which was supported by naturally-
occurring data from the rephrasing memory of
Wikipedia. Our framework takes a repertoire of ex-
isting sub-sentential paraphrases, coming from pos-
12A possible explanation for the relative good performance of
WEBLM may lie in the fact that our two automatic paraphrasers
using Google Translate as a pivot translation engine tend to pro-
duce strings that are very likely according to the language mod-
els used by the translation system, which we assume to be very
comparable to those that were used in our experiments.
17
sibly any source including manual acquisition, and
validates all candidate paraphrases using informa-
tion from the Web. Our experiments have shown
that the current version of our classifier outperforms
several baselines when considering paraphrases with
consensual judgements in the gold standard refer-
ence.
Although our initial experiments are positive, we
believe that they can be improved in a number of
ways. We intend to broaden our exploration of the
various characteristics at play. We will try more fea-
tures, including e.g. a model of syntactic depen-
dencies derived from the Web, and extend our work
to new languages. We will also attempt to analyze
more precisely our results to identify problematic
cases, some of which could turn to be almost im-
possible to model without resorting to world knowl-
edge, which was beyond our attempted modeling.
Finally, we will also be interested in considering the
applicability of this approach as a framework for the
evaluation of paraphrase acquisition techniques.
Acknowledgments
This work was partly supported by ANR project
Trace (ANR-09-CORD-023). The authors would
like to thank the anonymous reviewers for their help-
ful questions and comments.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, Ann Arbor, USA.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT,
Edmonton, Canada.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, Toulouse, France.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceedings
of the ACM symposium on User interface software and
technology.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT, Columbus,
USA.
Houda Bouamor, Aure?lien Max, and Anne Vilnat. 2011.
Monolingual alignment by edit rate computation on
sentential paraphrase pairs. In Proceedings of ACL,
Short Papers session, Portland, USA.
Chris Brockett and William B. Dolan. 2005. Support
vector machines for paraphrase identification and cor-
pus construction. In Proceedings of The 3rd Inter-
national Workshop on Paraphrasing IWP, Jeju Island,
South Korea.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, Hawai, USA.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
R. Chandrasekar, Christine Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of COLING, Copenhagen, Denmark.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
Timothy Chklovski. 2005. Collecting paraphrase cor-
pora from volunteer contributors. In Proceedings of
KCAP 2005, Banff, Canada.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, Manchester, UK.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Comput. Linguist.,
34(4):597?614.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of ECML, Warsaw, Poland.
Louise Dele?ger and Pierre Zweigenbaum. 2009. Extract-
ing lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Pro-
ceedings of the 2nd Workshop on Building and Using
Comparable Corpora: from Parallel to Non-parallel
Corpora, Singapore.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aure?lien Max. 2011. Local modifications and
paraphrases in wikipedia?s revision history. SEPLN
journal, 46:51?58.
Cristina Espan?a Bonet, Marta Vila, M. Anto`nia Mart??,
and Horacio Rodr??guez. 2009. Coco, a web interface
for corpora compilation. SEPLN journal, 43.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Proceedings of NAACL-
HLT, Los Angeles, USA.
18
Mirella Lapata and Frank Keller. 2005. Web-based Mod-
els for Natural Language Processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Weigang Li, Ting Liu, Yu Zhang, Sheng Li, and Wei
He. 2005. Automated generalization of phrasal para-
phrases from the web. In Proceedings of the IJCNLP
Workshop on Paraphrasing, Jeju Island, South Korea.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are multiple reference
translations necessary? investigating the value of
paraphrased reference translations in parameter opti-
mization. In Proceedings of AMTA, Waikiki, USA.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing Naturally-occurring Corrections and Paraphrases
from Wikipedia?s Revision History. In Proceedings of
LREC 2010, Valletta, Malta.
Aure?lien Max and Michael Zock. 2008. Looking up
phrase rephrasings via a pivot language. In Proceed-
ings of the COLING Workshop on Cognitive Aspects
of the Lexicon, Manchester, United Kingdom.
Aure?lien Max. 2004. From controlled document au-
thoring to interactive document normalization. In Pro-
ceedings of COLING, Geneva, Switzerland.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2).
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proceedings of ACL, Prague, Czech
Republic.
Andrew Mutton. 2006. Evaluation of sentence grammat-
icality using Parsers and a Support Vector Machine.
Ph.D. thesis, Macquarie University.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, Short Papers ses-
sion, Uppsala, Sweden.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignement of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL-HLT, Edmonton, Canada.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT, Rochester, USA.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, Barcelona,
Spain.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin B. Bederson. 2010. Im-
proving translation via targeted paraphrasing. In Pro-
ceedings of EMNLP, Cambridge, MA.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of EACL, Athens, Greece.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2010. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3).
Kuansan Wang, Chris Thrasher, Evelyne Viegas, Xiao-
long Li, and Bo-june (Paul) Hsu. 2010. An Overview
of Microsoft Web N-gram Corpus and Applications.
In Proceedings of the NAACL-HLT Demonstration
Session, Los Angeles, USA.
Shiqi Zhao, Ting Liu, Xincheng Yuan, Sheng Li, and
Yu Zhang. 2007. Automatic acquisition of context-
specific lexical paraphrases. In Proceedings of IJCAI
2007, Hyderabad, India.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint ACL-IJCNLP, Singapore.
Shiqi Zhao, Haifeng Wang, Ting Liu, , and Sheng Li.
2010. Leveraging multiple mt engines for paraphrase
generation. In Proceedings of COLING, Beijing,
China.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model for
sentence simplification. In Proceedings of COLING,
Beijing, China.
19
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 260?265,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
LIMSI?s Participation in the 2013 Shared Task
on Native Language Identification
Thomas Lavergne, Gabriel Illouz, Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Ryo Nagata
LIMSI-CNRS & Konan University
8-9-1 Okamoto
Kobe 658-0072 Japan
rnagata@konan-u.ac.jp
Abstract
This paper describes LIMSI?s participation to
the first shared task on Native Language Iden-
tification. Our submission uses a Maximum
Entropy classifier, using as features character
and chunk n-grams, spelling and grammati-
cal mistakes, and lexical preferences. Perfor-
mance was slightly improved by using a two-
step classifier to better distinguish otherwise
easily confused native languages.
1 Introduction
This paper describes the submission from LIMSI to
the 2013 shared task on Native Language Identifica-
tion (Tetreault et al, 2013). The creation of this new
challenge provided us with a dataset (12,100 TOEFL
essays by learners of English of eleven native lan-
guages (Blanchard et al, 2013)) that was necessary
to us to develop an initial framework for studying
Native Language Identification in text. We expect
that this challenge will draw conclusions that will
provide the community with new insights into the
impact of native language in foreign language writ-
ing. We believe that such a research domain is
crucial, not only for improving our understanding
of language learning and language production pro-
cesses, but also for developing Natural Language
Processing applications to support text improve-
ment.
This article is organized as follows. We first de-
scribe in Section 2 our maximum entropy system
used for the classification of a given text in English
into the native languages of the shared task. We then
introduce the various sets of features that we have in-
cluded in our submission, comprising basic n-gram
features (3.1) and features to capture spelling mis-
takes (3.2), grammatical mistakes (3.3), and lexical
preference (3.4). We next report the performance of
each of our sets of features (4.1) and our attempt to
perform a two-step classification to reduce frequent
misclassifications (4.2). We finally conclude with a
short discussion (section 5).
2 A Maximum Entropy model
Our system is based on a classical maximum entropy
model (Berger et al, 1996):
p?(y|x) =
1
Z?(x)
exp(?>F (x, y))
whereF is a vector of feature functions, ? a vector of
associated parameter values, and Z?(x) the partition
function.
Given N independent samples (xi, yi), the model
is trained by minimizing, with respect to ?, the neg-
ative conditional log-likelihood of the observations:
L(?) = ?
N?
i=1
log p(yi|xi).
This term is complemented with an additional regu-
larization term so as to avoid overfitting. In our case,
an `1 regularization is used, with the additional ef-
fect to produce a sparse model.
The model is trained with a gradient descent algo-
rithm (L-BFGS) using the Wapiti toolkit (Lavergne
et al, 2010). Convergence is determined either by
error rate stability on an held-out dataset or when
limits of numerical precision are reached.
260
3 Features
Our submission makes use of basic features, includ-
ing n-grams of characters and part-of-speech tags.
We further experimented with several sets of fea-
tures that will be described and compared in the fol-
lowing sections.
3.1 Basic features
We used n-grams of characters up to length 4 as fea-
tures. In order to reduce the size of the feature space
and the sparsity of these features, we used a hash
kernel (Shi et al, 2009) of size 216 with a hash fam-
ily of size 4. This allowed us to significantly reduce
the training time with no noticeable impact on the
model?s performance.
Our set of basic features also includes n-grams of
part-of-speech (POS) tags and chunks up to length 3.
Both were computed using an in-house CRF-based
tagger trained on PennTreeBank (Marcus et al,
1993). The POS tags sequences were post-processed
so that word tokens were used in lieu of their cor-
responding POS tags for the following: coordinat-
ing conjunctions, determiners, prepositions, modals,
predeterminers, possessives, pronouns, and question
adverbs (Nagata, 2013).
For instance, from this sentence excerpt:
[NP Some/DT people/NNS] [VP
might/MD think/VB] [SBAR that/IN]
[VP traveling/VBG] [PP in/IN]. . .
we extract n-grams from the pseudo POS-tag se-
quence:
Some NNS MD VB that VBG in. . .
and n-grams from the chunk sequence:
NP VP SBAR VP PP. . .
The length of chunks is encoded as separate fea-
tures that correspond to mean length of each type of
chunks. As shown in (Nagata, 2013), length of noun
sequences is also informative and thus was encoded
as a feature.
3.2 Capturing spelling mistakes
We added a set of features to capture information
about spelling mistakes in the model, following the
intuition that some spelling mistakes may be at-
tributed to the influence of the writer?s native lan-
guage.
To extract these features, each document is pro-
cessed using the ispell1 spell checker. This re-
sults in a list of incorrectly written word forms and
a set of potential corrections. For each word, the
best correction is next selected using a set of rules,
which were built manually after a careful study of
the training dataset.
When a corrected word is found, the incorrect
fragment of the word is isolated by striping from
the original and corrected words common prefix and
suffix, keeping only the inner-most substring differ-
ence. For example, given the following mistake and
correction:
appartment? apartment
this procedure generates the following feature:
pp? p
Such a feature may for instance help to identify na-
tive languages (using latin scripts) where doubling
of letters is frequent.
3.3 Capturing grammatical mistakes
Errors at the grammatical level are captured using
the ?language tool? toolkit (Milkowski, 2010), a
rule-based grammar and style checker. Each rule fir-
ing in a document is mapped to an individual feature.
This triggers features such as
BEEN PART AGREEMENT, corresponding to
cases where the auxiliary be is not followed by a
past participle, or EN A VS AN, corresponding to
confusions between the correct form the articles a
and an.
3.4 Capturing lexical preferences
Learners of a foreign language may have some pref-
erence for lexical choice given some semantic con-
tent that they want to convey2. We made the follow-
ing assumption: the lexical variant chosen for each
word may correspond to the less ambiguous choice
if mapping from the native language to English3.
1http://www.gnu.org/software/ispell/
2We assumed that we should not expect thematic differences
in the contents of the essays across original languages, as the
prompts for the essays were evenly distributed.
3This assumption of course could not hold for advanced
learners of English, who should make their lexical choices in-
dependently of their native language.
261
Thus, for each word in an English essay, if we
knew a corresponding word (or sense) that a writer
may have thought of in her native language, we
would like to consider the most likely translation
into English, according to some reliable probabilis-
tic model of lexical translation into English, as the
lexical choice most likely to be made by a learner of
this native language.
As we obviously do not have access to the word
in the native language of the writer, we approximate
this information by searching for the word that max-
imizes the translation probability of translating back
from the native language after translating from the
original English word. This in fact corresponds to a
widely used way of computing paraphrase probabili-
ties from bilingual translation distributions (Bannard
and Callison-Burch, 2005):
e?l ? argmax
e
?
f
pl(f |e).pl(e|f)
where f ranges over all possible translations of En-
glish word e in a given native language l.
Preferably, we would like to obtain candidate
translations into the native language in context,
that is, by translating complete sentences and us-
ing a posteriori translation probabilities. We could
not do this for a number of reasons, the main one
being that we did not have the possibility of using
or building Statistical Machine Translation systems
for all the language pairs involving English and the
native languages of the shared task. We therefore
resorted to simply finding, for each English word,
the most likely back-translation into English via a
given native language. Using the Google Transla-
tion online Statistical Machine Translation service4,
which proposed translations from and to English and
all the native languages of the shared task, a further
approximation had to be made as, in practice, we
were only able to access the most likely translations
for words in isolation: we considered only the best
translation of the original English word in the native
language, and then kept its best back-translation into
English. We here note some common intuitions with
the use of roundtrip translation as a Machine Trans-
lation evaluation metrics (Rapp, 2009).
4http://translate.google.com
Table 1 provides various examples of back-
translations for English adjectives obtained via each
native language. The samples from the Table show
that our procedure produces a significant number of
non identical back-translations. They also illustrate
some types of undesirable results obtained, which
led us to only consider as features for our classi-
fier the proportion of words in essays for which
the above-defined back-translation yielded the same
word, considering all possible native languages. We
only considered content words, as out-of-context
back-translation for function words would be too un-
reliable. Table 2 shows values for some documents
of the training set. As can be seen, there are impor-
tant differences across languages, some languages
obtaining high scores on average (e.g. French and
Japanese) and others obtaining low scores on aver-
age (e.g. Korean, Turkish). Furthermore, the high-
est score is only rarely obtained for the actual native
language of each document, showing that keeping
the most probable language according to this value
alone would not allow to obtain a good classification
performance.
4 Experiments
4.1 Results per set of features
For all our experiments reported here, we used the
full training data provided using cross-validation to
tune the regularization parameter. Our results are
presented in the top part of Table 3. Using our com-
plete set of features yields our best performance on
accuracy, corresponding to a 0.75% absolute im-
provement over using our basic n-gram features
only. No type of features allows a significant im-
provement over the n-gram features when added in-
dividually.
4.2 Two-step classification
Table 4 contains the confusion matrix for our system
across languages. It clearly stands out that two lan-
guage pairs were harder to distinguish: Hindi (hin)
and Telugu (tel) on the one hand, and Korean (kor)
and Japanese (jpn) on the other.
In order to improve the performance of our model,
we performed a two-step classification focused on
these difficult pairs. For this, we built additional
classifiers for each difficult pairs. Both are built
262
eng abrupt affirmative amazing ambiguous anarchic atrocious attentive awkward
ara sudden positive amazing mysterious messy terrible heedful inappropriate
chi sudden sure amazing ambiguous anarchic atrocious careful awkward
fre sudden affirmative amazing ambiguous anarchic atrocious careful awkward
ger abrupt affirmative incredible ambiguous anarchical gruesome attentively awkward
hin suddenly positive amazing vague chaotic brutal observant clumsy
ita abrupt affirmative amazing ambiguous anarchist atrocious careful uncomfortable
jap sudden positive surprising ambiguous anarchy heinous cautious awkward
kor fortuitous positive amazing ambiguous anarchic severe kind awkward
spa abrupt affirmative surprising ambiguous anarchic atrocious attentive clumsy
tel abrupt affirmative amazing ambiguous anarchic formidable attentive awkward
tur sudden positive amazing uncertain anarchic brutal attentive strange
Table 1: Examples of back translations for English adjectives from the training set via each of the eleven native
languages of the shared task. Back-translations that differ from the original word are indicated using a bold face.
Doc id. Native l. ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
976 ARA 0.80 0.88 0.91 0.95 0.75 0.91 0.87 0.73 0.89 0.79 0.71
29905 CHI 0.84 0.81 0.93 0.87 0.79 0.89 0.89 0.56 0.93 0.62 0.75
61765 FRE 0.73 0.84 0.90 0.71 0.73 0.83 0.86 0.50 0.91 0.58 0.66
100416 GER 0.78 0.80 0.86 0.83 0.72 0.89 0.86 0.70 0.90 0.67 0.67
26649 HIN 0.68 0.75 0.88 0.89 0.67 0.85 0.86 0.69 0.86 0.75 0.77
39189 ITA 0.68 0.85 0.92 0.94 0.74 0.93 0.89 0.69 0.92 0.72 0.72
3044 JPN 0.83 0.81 0.89 0.83 0.68 0.94 0.91 0.71 0.94 0.83 0.70
3150 KOR 0.75 0.86 0.91 0.84 0.76 0.88 0.87 0.55 0.88 0.67 0.73
6614 SPA 0.79 0.90 0.86 0.85 0.78 0.85 0.92 0.67 0.90 0.70 0.68
12600 TEL 0.65 0.74 0.84 0.73 0.71 0.92 0.90 0.76 0.95 0.82 0.58
5565 TUR 0.70 0.77 0.88 0.78 0.70 0.84 0.86 0.72 0.84 0.74 0.71
Table 2: Values corresponding to the proportion of content words in a random essay for each native language for which
back-translation yielded the same word.
FRE GER ITA SPA TUR ARA HIN TEL KOR JPN CHI
FRE 79 4 4 3 2 3 0 0 2 2 1
GER 0 89 2 4 1 0 1 0 2 1 0
ITA 6 1 83 6 1 1 0 0 0 1 1
SPA 4 4 5 72 2 3 3 2 1 1 3
TUR 3 2 1 3 81 1 3 2 0 3 1
ARA 3 0 1 3 3 81 5 2 1 0 1
HIN 1 1 1 3 2 1 64 26 1 0 0
TEL 0 0 1 0 0 1 17 81 0 0 0
KOR 1 1 0 0 3 1 0 0 80 12 2
JPN 1 0 2 2 0 3 0 1 13 73 5
CHI 0 1 0 0 2 2 0 2 3 3 87
Table 4: Confusion matrix on the Test set.
263
Features X-Val Test
ngm 74.83% 75.27%
ngm+ort 74.98% 75.29%
ngm+grm 75.18% 75.63%
ngm+lex 74.85% 75.47%
all 75.57% 75.81%
2-step (a) 75.46% 75.69%
2-step (b) 75.89% 75.98%
Table 3: Accuracy results obtained by cross-validation
and using the provided Test set for various combina-
tions of features and our two 2-step strategies. The fea-
ture sets are: character and part-of-speech n-grams fea-
tures (ngm), spelling features (ort), grammatical features
(grm), and lexical preference features (lex).
from the same feature sets as for the first-step model
but with only three labels: one for each language of
the pair and one for any other language.
The training data used for these new models in-
clude all documents from both languages as well as
document misclassified as one of them by the first-
step classifier (using cross-validation to label the full
training set). The formers keep their original labels
while the later are relabeled as other.
Document classified in one of the difficult pairs
by the first-step classifier were post-processed with
these new models. When the new label predicted is
other, the second best choice of the first step is used.
We investigated two setups for the first classifier:
(a) using the original 11 native languages classi-
fier, and (b) using a new classifier with languages
of the difficult pairs merged, resulting in 9 native
?languages?.
Our results, shown in Figure 3 for easy com-
parison, improve over our system using all fea-
tures only when the first-pass classifier uses the set
of 9 merged pseudo-languages (b). We obtain a
moderate 0.32% absolute improvement in accuracy
over one-step classification on cross-validation, and
0.17% improvement on the Test set.
5 Discussion and conclusion
We have submitted on maximum entropy system to
the shared task on Native Language Identification,
for which our basic set of n-gram features already
obtained a level of performance, around 75% in ac-
curacy, close to the best performance reported in our
submission. The additional feature sets that we have
included in our system, while improving the model,
did not allow us to capture a deeper influence of the
native language.
A first analysis reveals that the model fails to fully
use the additional feature sets due to lack of context.
Future experiments will need to link more closely
these features to the documents for which they pro-
vide useful information.
Due to time constraints and engineering issues,
the two-pass system was not ready by the time of
submission. The results that we have included in
this report show that it is a promising approach that
we should continue to explore. We also plan to con-
duct experiments that exploit the information about
the level of English available in the essays, some-
thing that we did not consider for this submission.
While this information is not directly available, it
may be infered from the data as a first-step classifi-
cation. We believe that studying its influence on the
mistakes make learners of different native language
is a promising direction.
The approach that we have described in this sub-
mission, as most of previously published approaches
for this task, attempts to find mistakes in the text of
the documents. The most typical mistakes are then
used by the classifier to detect the native language.
This does not take into consideration the fact that na-
tive English writers also make errors. It would be in-
teresting to explore the divergence between various
sets of writers/learners, not from the mean of non-
native writers, but from the mean of native writers.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 597?604,
Ann Arbor, Michigan.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1), March.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
264
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Marcin Milkowski. 2010. Developing an open-source,
rule-based proofreading tool. Software - Practice and
Experience, 40(7):543?566.
Ryo Nagata. 2013. Generating a language family tree
from indo-european non-native english texts (to ap-
pear). In Proceedings the 51th Annual Meeting of the
Association for Computational Linguistics (ACL). As-
sociation for Computational Linguistics.
Reinhard Rapp. 2009. The backtranslation score: Auto-
matic mt evalution at the sentence level without refer-
ence translations. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 133?136, Sun-
tec, Singapore.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal of Machine
Learning Research, 10:2615?2637, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
265
