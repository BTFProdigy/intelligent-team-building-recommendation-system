Finding parallel texts on the web using  
cross-language information retrieval 
Achim Ruopp 
University of Washington,  
Seattle, WA 98195, USA 
achimr@u.washington.edu 
Fei Xia 
University of Washington 
 Seattle, WA 98195, USA 
fxia@u.washington.edu 
Abstract 
Discovering parallel corpora on the web is 
a challenging task. In this paper, we use 
cross-language information retrieval tech-
niques in combination with structural fea-
tures to retrieve candidate page pairs from 
a commercial search engine. The candidate 
page pairs are then filtered using tech-
niques described by Resnik and Smith 
(2003) to determine if they are translations. 
The results allow the comparison of effi-
ciency of different parameter settings and 
provide an estimate for the percentage of 
pages that are parallel for a certain lan-
guage pair. 
1 Introduction 
Parallel corpora are invaluable resources in many 
areas of natural language processing (NLP). They 
are used in multilingual NLP as a basis for the cre-
ation of translation models (Brown et. al., 1990), 
lexical acquisition (Gale and Church, 1991) as well 
as for cross-language information retrieval (Chen 
and Nie, 2000). Parallel corpora can also benefit 
monolingual NLP via the induction of monolingual 
analysis tools for new languages or the improve-
ment of tools for languages where tools already 
exist (Hwa et. al., 2005; Pad? and Lapata, 2005; 
Yarowsky and Ngai, 2001). 
For most of the mentioned work, large parallel 
corpora are required. Often these corpora have li-
mited availability due to licensing restrictions 
(Tiedemann and Nygaard, 2004) and/or are domain 
specific (Koehn, 2005). Also parallel corpora are 
only available for a limited set of language pairs. 
As a result, researchers look to the World Wide 
Web as a source for parallel corpora (Resnik and 
Smith, 2003; Ma and Liberman, 1999; Chen and 
Nie, 2000). Because of the web?s world-wide reach 
and audience, many websites are bilingual, if not 
multilingual. The web is therefore a prime candi-
date as a source for such corpora especially for 
language pairs including resource-poor languages. 
 
Resnik and Smith (2003) outlined the following 
three steps for identifying parallel text on the web: 
(1) Locating pages that might have parallel 
translations 
(2) Generating candidate page pairs that might 
be translations 
(3) Structural filtering out of non-translation 
candidate pairs 
 
In most of the previous work, Step (1) is performed 
in an ad-hoc manner using structural features that 
were observed in a limited set of samples of paral-
lel pages. For example a language name in an 
HTML link is considered a strong indication that 
the page is also available translated to the language 
indicated by the link. The reason for this ad-hoc 
approach is that there aren?t any standards as to 
how web developers structure multilingual web 
pages on a server. Often developers use language 
names or identifiers in uniform resource locators 
(URLs) to distinguish different language versions 
of a page on a server. 
When Step (1) is performed using a commercial 
search engine, another obstacle to finding candi-
dates for parallel pages comes into play: the results 
are always relevance-ranked for the end user. In 
this paper, instead of searching exclusively for 
structural features of parallel pages, we are adding 
a dictionary-based sampling technique, based on 
cross-language information retrieval for Step (1). 
We compare the URL results from each of our ex-
periments with three different matching methods 
for Step (2). Finally, for Step (3), we adapted a 
filtering method from Resnik and Smith (2003) to 
determine whether or not a page pair is a true 
translation pair.  
To estimate the percentage of parallel pages that 
are available for a certain language pair in relation 
to the total number of pages available in each of 
the two languages, we modified a technique that 
Bharat and Broder (1998) used to estimate over-
laps of search engine indices. 
We conducted our experiments on the English-
German pair, but the described techniques are 
largely language-independent. The results of the 
experiments in this paper would allow researchers 
to choose the most efficient technique when trying 
to build parallel corpora from the web and guide 
research into further optimizing the retrieval of 
parallel texts from the web.  
2 Methodology 
The first step in finding parallel text on the web 
has two parts. The first part, the sampling proce-
dure, retrieves a set S1 of pages in the source lan-
guage L1 by sending sampling queries to the search 
engine. These sampling queries are structured in 
such a way that they retrieve pages that are likely 
to have translations. The second part, a checking 
procedure,  retrieves a set S2 of pages in target lan-
guage L2 that are likely to contain the translations 
of pages in S1. The two procedures are described in 
Sections 2.1 and 2.2, respectively. 
Step (2) matches up elements of S1 and S2 to 
generate a set of candidates for page pairs that 
could be translations of each other. This is ex-
plained in Section 2.3.  
Step (3), a final filtering step, uses features of 
the pages to eliminate page pairs that are not trans-
lations of each other.  The detail of the step is de-
scribed in Sections 2.4. Figure 1 illustrates the dif-
ferent sets of pages and page pairs created by the 
three steps. 
 
 
2.1 Sampling 
 
2.1 Sampling 
For the baseline case the sampling should select 
pages randomly from the search space. To get a 
random sample of pages from a search engine that 
we can check for translational equivalents in 
another language, we select terms at random from 
a bilingual dictionary. 
Instead of using a manually crafted bilingual 
dictionary, we chose to use a translation lexicon 
automatically created from parallel data, because 
the translation probabilities are useful for our expe-
riments. In this study, the translation lexicon was 
created by aligning part of the German-English 
portion of the Europarl corpus (Koehn, 2005) using 
the Giza++ package (Och and Ney, 2003). 
The drawback of using this translation lexicon is 
that the lexicon is domain-specific to parliamentary 
proceedings. We alleviated this domain-specificity 
by selecting mainly terms with medium frequency 
in the lexicon. 
We sorted the terms by frequency. According to 
Zipf?s law (Zipf, 1949), the frequency of the terms 
is roughly inversely proportional to their rank in 
this list. We choose terms according to a normal 
distribution whose mean is the midpoint of all 
ranks. We tuned the deviation to ? of the mean, so 
as to avoid getting very frequent terms into the 
sample which would just return a large set of unre-
lated pages, as well as very infrequent terms which 
would return few or no results. 
A single word selected with this normal distribu-
tion, together with the lang: parameter set to 
language L1, is submitted to the search engine to 
Sampling 
Language L1 
Checking 
Language L2 
Match 
Filter 
Web 
Figure 1.  Pages and page pairs involved in the 
three steps of the algorithm 
retrieve a sample (in our experiments 100 pages). 
The search engine automatically performs stem-
ming on the term. 
2.1.1 Source Language Expansion 
To obtain a sample yielding more translation can-
didates, it is valuable to use semantically related 
multi-word queries for the sampling procedure.  
To obtain semantically related words, we used 
the standard information retrieval (IR) technique of 
query expansion. Part of the sampling result of 
single-word queries are summaries, delivered back 
by the search engine. To come up with a ranked 
list of terms that are semantically related to the 
original one-word term, we extract and count all 
unigrams from a concatenation of the summaries. 
Stopwords are ignored. After the count, the uni-
grams are ranked by frequency.  
For an n-term query, the original single-word 
query is combined with the first (n-1) terms of this 
ranked list to form an expanded query that is sub-
mitted to the search engine. 
The advantage of this form of expansion is that 
it is largely language independent and often leads 
to highly relevant terms, due to the ranking algo-
rithms employed by the search engines. 
2.1.2 Language Identifiers in URLs 
Once the baseline is established with single and 
multi-word sampling queries, an additional struc-
tural inurl: search parameter, which allows que-
rying for substrings in URLs, can be added to in-
crease the likelihood of finding pages that do have 
translations.  
For this paper we limited our experiments to use 
standard (RFC 3066) two-letter language identifi-
ers for this search parameter: ?en? for English and 
?de? for German. 
2.2 Checking 
The purpose of the checking procedure is to gener-
ate a set of web pages in language L2 that are po-
tentially translations of pages in the sample ob-
tained in the previous section. 
2.2.1 Translating the Sampling Query 
The natural way to do this is to translate the sam-
pling query from language L1 into the target lan-
guage L2. The sampling query does not necessarily 
have a unique one-to-one translation in language 
L2. This is where the translation lexicon created 
from the Europarl corpus comes in. Because the 
lexicon contains probabilities, we can obtain the 
m-best translations for a single term from the sam-
pling query. 
Given a query in L1 with n terms and each term 
has up to m translations, the checking procedure 
will form up to mn queries in L2 and sends each of 
them to the search engine. Because most current 
commercial search engines set a limit on the max-
imum number of queries allowed per day, longer 
sampling queries (i.e., larger n) mean that fewer 
overall samples can be retrieved per day. The ef-
fect of this trade-off on the number of parallel page 
pairs is evaluated in our experiments. 
Source language expansion can lead to sample 
terms that are not part of the translation lexicon. 
These are removed during translation. 
If the inurl: search parameter was used in the 
sampling query, the corresponding inurl: para-
meter for language L2 will be used in the checking 
query. 
2.2.2 Target Language Expansion 
An alternative to translating all terms in an ex-
panded, multi-word sampling query (see Section 
2.1.1) is to translate only the original single sam-
pling word to obtain top m translations in L2, and 
then for each translation do a term expansion on 
the target language side with (n-1) expansion 
terms. The benefit of target language expansion is 
that it only requires m checking queries, where 
source language expansion requires mn checking 
queries. The performance of this different ap-
proach will be evaluated in Section 3.  
2.2.3 Site Parameter 
Another structural search parameter appropriate for 
checking is the site: parameter, which many 
search engines provide. It allows limiting the query 
results to a set of pre-defined sites. In our experi-
ments we use the sites of the top-30 results of the 
sampling set, which is the maximum allowed by 
the Yahoo! search engine. 
 
2.3 Matching Methods 
To obtain page pairs that might be translations of 
each other, pages in sampling set S1 are matched 
up based on URL similarity with pages in corres-
ponding checking set S2. We experimented with 
three methods. 
2.3.1 Fixed Language List 
In the fixed language list matching method, URLs 
differing only in the language names and language 
identifiers (as listed in Table 1) are considered a 
match and added to the set of page pair candidates. 
 
en de 
en-us de-de 
en ge 
enu deu 
enu ger 
english german 
englisch deutsch 
Table 1. Language identifiers and language names 
for Fixed Language List and URL Part Substitution 
 
An example for a match in this category is 
http://ec.europa.eu/education/policies/rec_qual/rec
ognition/diploma_en.html and 
http://ec.europa.eu/education/policies/rec_qual/rec
ognition/diploma_de.html. 
2.3.2 Levenshtein Distance 
In the Levenshtein distance matching method, if 
the Levenshtein distance (also known as edit dis-
tance) between a pair of URLs from S1 and S2 is 
larger than zero1 and is below a threshold, the URL 
pair is considered a match. In our experiments, we 
set the threshold to four, because for most standard 
(RFC 3066) language identifiers the maximum 
Levenshtein distance would be four (e.g. ?en-US? 
vs. ?de-DE? as part of a URL). 
2.3.3 URL Part Substitution 
The third method that we tried does not require 
querying a search engine for a checking set. In-
stead, each URL U1 in the sampling set S1 is parsed 
to determine if it contains a language name or 
identifier at a word boundary. If so, the language 
name or identifier is substituted with the corres-
ponding language name or identifier for the target 
language to form a target language URL U2 ac-
cording to the substitutions listed in Table 1. 
For each resulting U2, an HTTP HEAD request 
is issued to verify whether the page with that URL 
                                                 
1 We don?t want to match identical URLs. 
exists on the server. If the request is successful, the 
pair (U1,U2) is added to the set of page pair candi-
dates. If multiple substitutions are possible for a U1 
all the resulting U2 will be tested. 
 
2.4 Page Filtering 
The goal of this step is to filter out all the page 
pairs that are not true translations.   
2.4.1 Structural Filtering 
One method for filtering is a purely structural, 
language-independent method described in Resnik 
and Smith (2003). In this method, the HTML struc-
ture in each page is linearized and the resulting 
sequences are aligned to determine the structural 
differences between the two files. Their paper dis-
cussed four scalar values that can be calculated 
from the alignment. We used two of the values in 
our experiments, as described below. 
  The first one is called the difference percentage 
(dp), which indicates the percentage of nonshared 
material in the page pair. Given the two linearized 
sequences for a page pair (p1, p2), we used  Eq (1) 
to calculate dp, where length1 is the length of the 
first sequence, and diff 1  is the number of lines in 
the first sequence that do not align to anything in 
the second sequence; length2  and diff 2 are defined 
similarly. 
 
21
21
21 ),( lengthlength
diffdiffppdp ?
??
    (1) 
 
 
 The second value measures the correlation be-
tween the lengths of aligned nonmarkup chunks. 
The idea is that the lengths of corresponding trans-
lated sentences or paragraphs usually correlate. 
The longer a sentence in one language is, the long-
er its translation in another language should be. For 
the sake of simplicity, we assume there is a linear 
correlation between the lengths of the two files, 
and use the Pearson correlation coefficient as a 
length correlation metric. From the two linearized 
sequences, the lengths of nonmarkup chunks are 
recorded into two arrays. The Pearson correlation 
coefficient can be directly calculated on these two 
arrays.  
This metric is denoted as r(p1,p2), and its value  
is in the range of [-1,1]: 1 indicates a perfect posi-
tive linear relationship, 0 indicates there is no li-
near relationship, and -1 indicates a perfect nega-
tive linear relationship between the chunk lengths 
in the two files.  
2.4.2 Content Translation Metric 
As shown in Resnik and Smith (2003), the struc-
tural filtering to judge whether pages are transla-
tions of each other leads to very good precision 
and satisfactory recall.  
However, when using the URL part substitution 
method described in 2.3, many web sites, if they 
receive a request for a URL that does not exist, 
respond by returning the most likely page for 
which there is an existing URL on the server. This 
is often the page content of the original URL be-
fore substitution. Identical pages in the candidate 
page pair2 would be judged as translations by the 
purely structural method and precision would be 
negatively impacted. There are several solutions 
for this, one of them is to use a content-based me-
tric to complement the structural metric. 
Ma and Liberman (1999) define the following 
similarity metric between two pages in a page pair 
(p1, p2): 
1
21 ),( pinTokensOfNum
PairsTokennTranslatioOfNumppc ?
 (2) 
 
To calculate this content-based metric, the trans-
lation lexicon created in Step (1) comes in handy. 
For the first 500 words of each page in the page 
pair candidate, we calculate the similarity metric in 
Eq (2), using the top two translations of the words 
in the translation lexicon.   
 
2.4.3 Linear Combination 
 
We combine the structural metrics (dp and r) and 
the content-based metric c by linear combination:3 
  
3
),(*),(*)),(1(*),( 21212121 ppcapprappdpappt crdpdprc ????
(3) 
                                                 
2 The two identical pages could have different URLs.  
3 We use 1-dp(p1,p2) to turn a dissimilarity measure into 
a similarity measure.  
If tdprc is larger than a predefined threshold, the 
page pair is judged to be a translation. 
2.5 Estimating the Percentage of Parallel 
Pages for a Language Pair 
Statistics on what share of web pages in one lan-
guage have translated equivalents in another lan-
guage are, to our knowledge, not available. Obtain-
ing these statistics is useful from a web metrics 
perspective.  The statistics allow the calculation of 
relative language web page counts and serve as a 
baseline to evaluate methods that try to find paral-
lel pages. 
Fortunately there is a statistical method (Bharat 
and Broder, 1998) that can be adapted to obtain 
these numbers. Bharat and Broder introduce a me-
thod to estimate overlaps in the coverage of a pair 
of search indices and to calculate the relative size 
ratio of search indices. They achieve this by ran-
domly sampling pages in one index and then check 
whether the pages are also present in the other in-
dex. 
Instead of calculating the overlap of pages in 
two search engines, we adapted the method to 
measure the overlap of languages in one search 
engine. Let P(E) represent the probability that a 
web page belongs to a set E. Let P(FE|E) represent 
the conditional probability that there exist transla-
tional equivalents F of E given E. Then 
 
? ? ? ?? ?ESize
FSizeEFP EE ?|
  (4) 
? ? ? ?? ?FSize
ESizeFEP FF ?|
  (5) 
 
Size(FE) and Size(E) can be determined with an 
experiment using one term samples and checking 
with a site: parameter. Size (FE) equals the 
number of page pairs that are determined to be 
translations by the filtering step. Size(E) is the 
number of checked sites per sample (30 in the case 
of Yahoo!) times the number of samples. Size(EF) 
and Size(F) are calculated similarly. 
3 Experiments 
To evaluate the effectiveness of various methods 
described in Section 2, we ran a range of experi-
ments and the results are shown in Table 2.  
  The first column is the experiment ID; the second 
column indicates whether source or target expan-
sion is used in Step (1); the third column shows the 
length of the queries after the expansion (if any). 
For instance, in Experiment #3, source expansion 
is applied, and after the expansion, the new queries 
always have three words: one is the original query 
term, and the other two are terms that are most re-
levant to the original query term according to the 
documents retrieved by the search engine (see Sec-
tion 2.1.2). 
  The fourth and fifth columns indicate whether the 
inurl: and site: parameters are used during 
the search. A blank cell means that the search pa-
rameter is not used in that experiment. For each 
query, the search engine returns the top 100 docu-
ments. 
  The next three columns show the numbers of 
page pairs produced by each of the three matching 
methods describe in Section 2.3. The last three 
columns show the numbers of page pairs after the 
filtering step. Here, we used the linear combination 
(see Section 2.4.3).  All the numbers are the sum of 
the results from 100 sampling queries. Let us ex-
amine the experimental results in detail.  
3.1 Sampling and Checking 
The evaluation of the sampling and checking pro-
cedures are difficult, because the number of trans-
lation page pairs existing on the web is unknown. 
In this study, we evaluated the module indirectly 
by looking at the translation pairs found by the fol-
lowing steps: the matching step and the filtering 
step.  
   A few observations are worth noting. First, query 
expansion increases the number of page pairs 
created in Steps (2) and (3), and source and target 
query expansion lead to similar results. However, 
the difference between n=2 and n=3 is not signifi-
cant. One possible explanation is that the semantic 
divergence between queries on the source side and 
on the target side could become more problematic 
for longer queries.  
Second, using the site: and inurl: search pa-
rameters (described in 2.1.2 and 2.2.3) increases 
the number of discovered page pairs. The potential 
limitation is that inurl: narrows the set of dis-
coverable pages to the ones that contain language 
identifiers in the URL. 
 
 
Expe-
riment 
ID 
Expan-
sion 
type  
Query 
length 
(n) 
inurl: 
Param 
site: 
Param 
Number of page pairs 
(before filtering)  
Number of page pairs  
(after filtering) 
 
List Leven-
shtein 
Sub-
stitution 
List Leven-
shtein 
Sub-
stitution 
1 none 1   5 13 1108 1 1 97 
2 Source 2   8 28 1889 3 4 157 
3 Source 3   10 42 1975 1 10 124 
4 none 1 en/de  58 84 5083 17 22 285 
5 Source 2 en/de  72 132 9279 27 31 433 
6 Source 3 en/de  100 160 9200 25 31 347 
7 none 1   6 18 1099 1 3 92 
8 Target 2   4 24 1771 2 3 143 
9 Target 3   4 12 1761 0 0 149 
10 none 1 en/de  56 93 5041 24 34 281 
11 Target 2 en/de  107 161 9131 27 33 426 
12 Target 3 en/de  45 72 8395 12 15 335 
13 none 1  30 10 258 n/a 6 9 n/a 
14 Source 2  30 22 743 n/a 9 32 n/a 
15 Source 3  30 46 1074 n/a 12 41 n/a 
16 none 1 en/de 30 59 164 n/a 13 15 n/a 
17 Source 2 en/de 30 118 442 n/a 28 50 n/a 
18 Source 3 en/de 30 171 693 n/a 46 49 n/a 
Table 2. Experiment configurations and results
3.2 Matching Methods 
Table 2 shows that among the three matching me-
thods, the URL part substitution method leads to 
many more translation page pairs than the other 
two methods.  
Notice that although the fixed language list me-
thod uses the same language name pair table (i.e., 
Table 1) as the URL part substitution method, it 
works much worse than the latter. This is largely 
due to the different rankings of documents in dif-
ferent languages. For instance, suppose a page p1 in 
L1 is retrieved by a sampling query q1, and p1 has a 
translation page p2 in L2 , it is possible that p2 will 
not be retrieved by the query q2, a query made up 
of the translation of the terms in q1 .
4 
  Another observation is that the Levenshtein dis-
tance matching method outperforms the fixed lan-
guage list method. In addition, it has a unique ad-
vantage: the results allow the automatic learning of 
language identifiers that web developers use in 
URLs to distinguish parallel pages for certain lan-
guage pairs. 
3.3 Parameter Tuning for Linear Combina-
tion of Filtering Metrics 
Before the combined metrics in Eq (3) can be used 
to filter page pairs, the combination parameters 
need to be tuned on a development set. The para-
meters are adp, ar, and ac as well as the threshold 
above which the combined metrics indicate a trans-
lated page pair vs. an unrelated page pair. 
To tune the parameters, we used data from an 
independent test run for the en?de language direc-
tion. We randomly chose 50 candidate pairs from a 
set created with the URL part substitution method 
and manually judged whether or not the pages are 
translations of each other. 
We varied the parameters adp, ar, ac and tdprc over 
a range of empirical values and compared how 
well the combined metrics judgment correlated 
with the human judgment for page translation (we 
calculated the Pearson correlation coefficient). The 
results of tuning are shown in Table 3. 
adp ar ac tdprc 
0.5 1.5 1 > 0.8 
Table 3: Parameter and threshold values  
chosen for linear combination 
                                                 
4 The search engine returns 100 or fewer documents for 
each query. 
3.4 Evaluation of the Filtering Step  
To evaluate the combined filtering method de-
scribed in Section 2.4.3, we chose 110 page pairs 
at random from the 433 candidate page pairs in 
experiment #5 (Language direction en?de, Pairs 
generated with the URL part substitution method 
described in 2.3.3). Each of the page pairs was eva-
luated manually to assess whether it is a true trans-
lation pair.  
On this set, the combined filter had a precision 
of 88.9% and a recall of 36.4%. The high precision 
is encouraging on the noisy test set. The recall is 
low but is acceptable since one can always submit 
more sampling queries to the search engine. Resnik 
and Smith (2003) reported higher precision and 
recall in their experiments. However, their num-
bers and ours are not directly comparable because 
their approach required the existence of parent or 
sibling pages and consequently their test sets were 
less noisy.  
  From the numbers of translation pairs, we can 
make an estimate of available parallel pages for a 
language pair, as explained in Section 2.5. For in-
stance, by using the results of experiment #13, the 
estimate is P(DE|E)=0.03% and P(ED|D)=0.27% (E 
for English, and D for German). This indicates that 
the number of English-German parallel pages is 
small comparing to the total number of English and 
German web pages.  
 
4 Conclusion 
In this paper we show that despite the fact that 
there are no standardized features to identify paral-
lel web pages and despite the relevance ranking of 
commercial search engine results, it is possible to 
come up with reliable methods to gather parallel 
pages using commercial search engines. It is also 
possible to calculate an estimate of how many pag-
es are available parallel in relation to the overall 
number of pages in a certain language. 
  The number of translation pages retrieved by the 
current methods is relatively small. In the future, 
we plan to learn URL patterns from the Levensh-
tein matching method and add them to the patterns 
used in the URL part substitution method. Once 
more translation pages are retrieved, we plan to use 
these pages as parallel data in a statistical machine 
translation (MT) system to evaluate the usefulness 
of this approach to MT.  
Instead of using narrow query interfaces to a 
public search engine interface, it also might be ad-
vantageous to have access to raw indices or crawl 
data of the engines. Such access will enable us to 
take advantage of certain page features that could 
be good indicators of parallel pages. 
References 
Krishna Bharat and Andrei Broder , 1998, A technique 
for measuring the relative size and overlap of public 
Web search engines, Computer Networks and ISDN 
Systems 30(1-7). 
Peter F. Brown, John Cocke, Stephen A. Della Pietra, 
Vincent J. Della Pietra, Fredrick Jelinek, John D. 
Lafferty, Robert L. Mercer, and Paul S. Roossin, 
1990, A statistical approach to machine translation, 
Computational Linguist. 16(2), pp 79-85. 
Jiang Chen and Jian-Yun Nie, 2000, Parallel Web Text 
Mining for Cross-Language IR, in Proceedings of 
RIAO-2000: Content-Based Multimedia Information 
Access. 
William A. Gale and Kenneth W. Church, 1991, Identi-
fying word correspondence in parallel texts, in 'HLT 
'91: Proceedings of the workshop on Speech and 
Natural Language',  NJ, USA, pp. 152-157. 
J.W. Hunt and M.D. McIlroy, M.D., 1976, An Algo-
rithm for Differential File Comparison, Bell Labora-
tories, Computer Science Technical Report 41. 
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara 
Cabezas, and Okan Kolak, 2005, Bootstrapping 
Parsers via Syntactic Projection across Parallel 
Texts, Special Issue of the Journal of Natural Lan-
guage Engineering on Parallel Texts 11(3),  pp 311-
325. 
Philipp Koehn, 2005, Europarl: A Parallel Corpus for 
Statistical Machine Translation, in Proceedings of 
the 2005 MT Summit. 
Xiaoyi Ma and Mark Y. Liberman, 1999, BITS: A Me-
thod for Bilingual Text Search over the Web, in Pro-
ceedings of the 1999 MT Summit, Singapore. 
Franz Josef Och and Hermann Ney, 2003, A Systematic 
Comparison of Various Statistical Alignment Models, 
Computational Linguistics 29(1), pp 19-51. 
Sebastian Pad? and Mirella Lapata, 2005, Cross-
linguistic Projection of Role-Semantic Information, 
in  Proceedings of HLT/EMNLP 2005. 
Philip Resnik and Noah A. Smith, 2003, The Web as a 
Parallel Corpus, Computational Linguistics 29(3), pp 
349-380. 
J?rg Tiedemann and Lars Nygaard, 2004, The OPUS 
corpus - parallel & free, in 'Proceedings of the 
Fourth International Conference on Language Re-
sources and Evaluation (LREC'04)'. 
David Yarowsky and Grace Ngai, 2001, Inducing multi-
lingual POS taggers and NP bracketers via robust 
projection across aligned corpora, in ?Proceedings of 
the Second meeting of the North American Chapter 
of ACL (NAACL 2001)?,  NJ, USA, pp. 1-8. 
George K. Zipf, 1949, Human Behavior and the Prin-
ciple of Least-Effort, Addison-Wesley 
Proceedings of ACL-08: HLT, pages 514?522,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Applying Morphology Generation Models to Machine Translation
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Achim Ruopp
Butler Hill Group
Redmond, WA, USA
v-acruop@microsoft.com
Abstract
We improve the quality of statistical machine
translation (SMT) by applying models that
predict word forms from their stems using
extensive morphological and syntactic infor-
mation from both the source and target lan-
guages. Our inflection generation models are
trained independently of the SMT system. We
investigate different ways of combining the in-
flection prediction component with the SMT
system by training the base MT system on
fully inflected forms or on word stems. We
applied our inflection generation models in
translating English into two morphologically
complex languages, Russian and Arabic, and
show that our model improves the quality of
SMT over both phrasal and syntax-based SMT
systems according to BLEU and human judge-
ments.
1 Introduction
One of the outstanding problems for further improv-
ing machine translation (MT) systems is the diffi-
culty of dividing the MT problem into sub-problems
and tackling each sub-problem in isolation to im-
prove the overall quality of MT. Evidence for this
difficulty is the fact that there has been very little
work investigating the use of such independent sub-
components, though we started to see some success-
ful cases in the literature, for example in word align-
ment (Fraser and Marcu, 2007), target language cap-
italization (Wang et al, 2006) and case marker gen-
eration (Toutanova and Suzuki, 2007).
This paper describes a successful attempt to in-
tegrate a subcomponent for generating word inflec-
tions into a statistical machine translation (SMT)
system. Our research is built on previous work in
the area of using morpho-syntactic information for
improving SMT. Work in this area is motivated by
two advantages offered by morphological analysis:
(1) it provides linguistically motivated clustering of
words and makes the data less sparse; (2) it cap-
tures morphological constraints applicable on the
target side, such as agreement phenomena. This sec-
ond problem is very difficult to address with word-
based translation systems, when the relevant mor-
phological information in the target language is ei-
ther non-existent or implicitly encoded in the source
language. These two aspects of morphological pro-
cessing have often been addressed separately: for
example, morphological pre-processing of the input
data is a common method of addressing the first as-
pect, e.g. (Goldwater and McClosky, 2005), while
the application of a target language model has al-
most solely been responsible for addressing the sec-
ond aspect. Minkov et al (2007) introduced a way
to address these problems by using a rich feature-
based model, but did not apply the model to MT.
In this paper, we integrate a model that predicts
target word inflection in the translations of English
into two morphologically complex languages (Rus-
sian and Arabic) and show improvements in the MT
output. We study several alternative methods for in-
tegration and show that it is best to propagate un-
certainty among the different components as shown
by other research, e.g. (Finkel et al, 2006), and in
some cases, to factor the translation problem so that
the baseline MT system can take advantage of the
reduction in sparsity by being able to work on word
stems. We also demonstrate that our independently
trained models are portable, showing that they can
improve both syntactic and phrasal SMT systems.
514
2 Related work
There has been active research on incorporating
morphological knowledge in SMT. Several ap-
proaches use pre-processing schemes, including
segmentation of clitics (Lee, 2004; Habash and Sa-
dat, 2006), compound splitting (Nie?en and Ney,
2004) and stemming (Goldwater and McClosky,
2005). Of these, the segmentation approach is dif-
ficult to apply when the target language is morpho-
logically rich as the segmented morphemes must be
put together in the output (El-Kahlout and Oflazer,
2006); and in fact, most work using pre-processing
focused on translation into English. In recent
work, Koehn and Hoang (2007) proposed a general
framework for including morphological features in
a phrase-based SMT system by factoring the repre-
sentation of words into a vector of morphological
features and allowing a phrase-based MT system to
work on any of the factored representations, which
is implemented in the Moses system. Though our
motivation is similar to that of Koehn and Hoang
(2007), we chose to build an independent compo-
nent for inflection prediction in isolation rather than
folding morphological information into the main
translation model. While this may lead to search er-
rors due to the fact that the models are not integrated
as tightly as possible, it offers some important ad-
vantages, due to the very decoupling of the compo-
nents. First, our approach is not affected by restric-
tions on the allowable context size or a phrasal seg-
mentation that are imposed by current MT decoders.
This also makes the model portable and applicable
to different types of MT systems. Second, we avoid
the problem of the combinatorial expansion in the
search space which currently arises in the factored
approach of Moses.
Our inflection prediction model is based on
(Minkov et al, 2007), who build models to predict
the inflected forms of words in Russian and Arabic,
but do not apply their work to MT. In contrast, we
focus on methods of integration of an inflection pre-
diction model with an MT system, and on evaluation
of the model?s impact on translation. Other work
closely related to ours is (Toutanova and Suzuki,
2007), which uses an independently trained case
marker prediction model in an English-Japanese
translation system, but it focuses on the problem of
generating a small set of closed class words rather
than generating inflected forms for each word in
translation, and proposes different methods of inte-
gration of the components.
3 Inflection prediction models
This section describes the task and our model for in-
flection prediction, following (Minkov et al, 2007).
We define the task of inflection prediction as the
task of choosing the correct inflections of given tar-
get language stems, given a corresponding source
sentence. The stemming and inflection operations
we use are defined by lexicons.
3.1 Lexicon operations
For each target language we use a lexicon L which
determines the following necessary operations:
Stemming: returns the set of possible morpholog-
ical stems Sw = {s1, ..., sl} for the word w accord-
ing to L. 1
Inflection: returns the set of surface word forms
Iw = {i1, ..., im} for the stems Sw according to L.
Morphological analysis: returns the set of possible
morphological analyses Aw = {a1, ..., av} for w. A
morphological analysis a is a vector of categorical
values, where each dimension and its possible values
are defined by L.
For the morphological analysis operation, we
used the same set of morphological features de-
scribed in (Minkov et al, 2007), that is, seven fea-
tures for Russian (POS, Person, Number, Gender,
Tense, Mood and Case) and 12 for Arabic (POS,
Person, Number, Gender, Tense, Mood, Negation,
Determiner, Conjunction, Preposition, Object and
Possessive pronouns). Each word is factored into
a stem (uninflected form) and a subset of these fea-
tures, where features can have either binary (as in
Determiner in Arabic) or multiple values. Some fea-
tures are relevant only for a particular (set of) part-
of-speech (POS) (e.g., Gender is relevant only in
nouns, pronouns, verbs, and adjectives in Russian),
while others combine with practically all categories
(e.g., Conjunction in Arabic). The number of possi-
ble inflected forms per stem is therefore quite large:
as we see in Table 1 of Section 3, there are on av-
erage 14 word forms per stem in Russian and 24 in
1Alternatively, stemming can return a disambiguated stem
analysis; in which case the set Sw consists of one item. The
same is true with the operation of morphological analysis.
515
Arabic for our dataset. This makes the generation of
correct forms a challenging problem in MT.
The Russian lexicon was obtained by intersecting
a general domain lexicon with our training data (Ta-
ble 2), and the Arabic lexicon was obtained by run-
ning the Buckwalter morphological analyser (Buck-
walter, 2004) on the training data. Contextual dis-
ambiguation of morphology was not performed in
either of these languages. In addition to the forms
supposed by our lexicon, we also treated capitaliza-
tion as an inflectional feature in Russian, and defined
all true-case word variants as possible inflections of
its stem(s). Arabic does not use capitalization.
3.2 Task
More formally, our task is as follows: given a source
sentence e, a sequence of stems in the target lan-
guage S1, . . . St, . . . Sn forming a translation of e,
and additional morpho-syntactic annotations A de-
rived from the input, select an inflection yt from its
inflection set It for every stem set St in the target
sentence.
3.3 Models
We built a Maximum Entropy Markov model for in-
flection prediction following (Minkov et al, 2007).
The model decomposes the probability of an inflec-
tion sequence into a product of local probabilities for
the prediction for each word. The local probabilities
are conditioned on the previous k predictions (k is
set to four in Russian and two in Arabic in our ex-
periments). The probability of a predicted inflection
sequence, therefore, is given by:
p(y | x) =
n?
t=1
p(yt | yt?1...yt?k, xt), yt ? It,
where It is the set of inflections corresponding to St,
and xt refers to the context at position t. The con-
text available to the task includes extensive morpho-
logical and syntactic information obtained from the
aligned source and target sentences. Figure 1 shows
an example of an aligned English-Russian sentence
pair: on the source (English) side, POS tags and
word dependency structure are indicated by solid
arcs. The alignments between English and Russian
words are indicated by the dotted lines. The de-
pendency structure on the Russian side, indicated by
solid arcs, is given by a treelet MT system (see Sec-
tion 4.1), projected from the word dependency struc-
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+pl+gen+masc
????????
VERB+perf+pass+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair with
syntactic and morphological annotation.
ture of English and word alignment information.
The features for our inflection prediction model
are binary and pair up predicates on the context
(x?, yt?1...yt?k) and the target label (yt). The fea-
tures at a certain position t can refer to any word
in the source sentence, any word stem in the tar-
get language, or any morpho-syntactic information
in A. This is the source of the power of a model
used as an independent component ? because it does
not need to be integrated in the main search of an
MT decoder, it is not subject to the decoder?s local-
ity constraints, and can thus make use of more global
information.
3.4 Performance on reference translations
Table 1 summarizes the results of applying the in-
flection prediction model on reference translations,
simulating the ideal case where the translations in-
put to our model contain correct stems in correct
order. We stemmed the reference translations, pre-
dicted the inflection for each stem, and measured the
accuracy of prediction, using a set of sentences that
were not part of the training data (1K sentences were
used for Arabic and 5K for Russian).2 Our model
performs significantly better than both the random
and trigram language model baselines, and achieves
an accuracy of over 91%, which suggests that the
model is effective when its input is clean in its stem
choice and order. Next, we apply our model in the
more noisy but realistic scenario of predicting inflec-
tions of MT output sentences.
2The accuracy is based on the words in our lexicon. We
define the stem of an out-of-vocabulary (OOV) word to be it-
self, so in the MT scenario described below, we will not predict
the word forms for an OOV item, and will simply leave it un-
changed.
516
Russian Arabic
Random 16.4 8.7
LM 81.0 69.4
Model 91.6 91.0
Avg | I | 13.9 24.1
Table 1: Results on reference translations (accuracy, %).
4 Machine translation systems and data
We integrated the inflection prediction model with
two types of machine translation systems: systems
that make use of syntax and surface phrase-based
systems.
4.1 Treelet translation system
This is a syntactically-informed MT system, de-
signed following (Quirk et al, 2005). In this ap-
proach, translation is guided by treelet translation
pairs, where a treelet is a connected subgraph of a
syntactic dependency tree. Translations are scored
according to a linear combination of feature func-
tions. The features are similar to the ones used in
phrasal systems, and their weights are trained us-
ing max-BLEU training (Och, 2003). There are
nine feature functions in the treelet system, includ-
ing log-probabilities according to inverted and direct
channel models estimated by relative frequency, lex-
ical weighting channel models following Vogel et
al. (2003), a trigram target language model, two or-
der models, word count, phrase count, and average
phrase size functions.
The treelet translation model is estimated using
a parallel corpus. First, the corpus is word-aligned
using an implementation of lexicalized-HMMs (He,
2007); then the source sentences are parsed into a
dependency structure, and the dependency is pro-
jected onto the target side following the heuristics
described in (Quirk et al, 2005). These aligned sen-
tence pairs form the training data of the inflection
models as well. An example was given in Figure 1.
4.2 Phrasal translation system
This is a re-implementation of the Pharaoh trans-
lation system (Koehn, 2004). It uses the same
lexicalized-HMM model for word alignment as the
treelet system, and uses the standard extraction
heuristics to extract phrase pairs using forward and
backward alignments. In decoding, the system uses
a linear combination of feature functions whose
weights are trained using max-BLEU training. The
features include log-probabilities according to in-
verted and direct channel models estimated by rel-
ative frequency, lexical weighting channel models,
a trigram target language model, distortion, word
count and phrase count.
4.3 Data sets
For our English-Russian and English-Arabic experi-
ments, we used data from a technical (computer) do-
main. For each language pair, we used a set of paral-
lel sentences (train) for training the MT system sub-
models (e.g., phrase tables, language model), a set
of parallel sentences (lambda) for training the com-
bination weights with max-BLEU training, a set of
parallel sentences (dev) for training a small number
of combination parameters for our integration meth-
ods (see Section 5), and a set of parallel sentences
(test) for final evaluation. The details of these sets
are shown in Table 2. The training data for the in-
flection models is always a subset of the training set
(train). All MT systems for a given language pair
used the same datasets.
Dataset sent pairs word tokens (avg/sent)
English-Russian
English Russian
train 1,642K 24,351K (14.8) 22,002K (13.4)
lambda 2K 30K (15.1) 27K (13.7)
dev 1K 14K (13.9) 13K (13.5)
test 4K 61K (15.3) 60K(14.9)
English-Arabic
English Arabic
train 463K 5,223K (11.3) 4,761K (10.3)
lambda 2K 22K (11.1) 20K (10.0)
dev 1K 11K (11.1) 10K (10.0)
test 4K 44K (11.0) 40K (10.1)
Table 2: Data set sizes, rounded up to the nearest 1000.
5 Integration of inflection models with MT
systems
We describe three main methods of integration we
have considered. The methods differ in the extent to
which the factoring of the problem into two subprob-
lems ? predicting stems and predicting inflections
? is reflected in the base MT systems. In the first
method, the MT system is trained to produce fully
inflected target words and the inflection model can
change the inflections. In the other two methods, the
517
MT system is trained to produce sequences of tar-
get language stems S, which are then inflected by
the inflection component. Before we motivate these
methods, we first describe the general framework for
integrating our inflection model into the MT system.
For each of these methods, we assume that the
output of the base MT system can be viewed as a
ranked list of translation hypotheses for each source
sentence e. More specifically, we assume an out-
put {S1,S2,. . . ,Sm} of m-best translations which
are sequences of target language stems. The transla-
tions further have scores {w1,w2,. . . ,wm} assigned
by the base MT system. We also assume that each
translation hypothesis Si together with source sen-
tence e can be annotated with the annotation A, as
illustrated in Figure 1. We discuss how we convert
the output of the base MT systems to this form in the
subsections below.
Given such a list of candidate stem sequences, the
base MT model together with the inflection model
and a language model choose a translation Y? as
follows:
(1) Yi = argmaxY ?i ?Infl(Si)?1logPIM (Y ?i |Si)+
?2logPLM (Y ?i ), i = 1 . . . n
(2) Y ? = argmaxi=1...n ?1logPIM (Yi|Si) +
?2logPLM (Yi) + ?3wi
In these formulas, the dependency on e and A
is omitted for brevity in the expression for the
probability according to the inflection model PIM .
PLM (Y ?i ) is the joint probability of the sequence
of inflected words according to a trigram language
model (LM). The LM used for the integration is the
same LM used in the base MT system that is trained
on fully inflected word forms (the base MT system
trained on stems uses an LM trained on a stem se-
quence). Equation (1) shows that the model first se-
lects the best sequence of inflected forms for each
MT hypothesis Si according to the LM and the in-
flection model. Equation (2) shows that from these
n fully inflected hypotheses, the model then selects
the one which has the best score, combined with
the base MT score wi for Si. We should note that
this method does not represent standard n-best re-
ranking because the input from the base MT system
contains sequences of stems, and the model is gen-
erating fully inflected translations from them. Thus
the chosen translation may not be in the provided n-
best list. This method is more similar to the one used
in (Wang et al, 2006), with the difference that they
use only 1-best input from a base MT system.
The interpolation weights ? in Equations (1) and
(2) as well as the optimal number of translations n
from the base MT system to consider, given a maxi-
mum of m=100 hypotheses, are trained using a sep-
arate dataset. We performed a grid search on the
values of ? and n, to maximize the BLEU score of
the final system on a development set (dev) of 1000
sentences (Table 2).
The three methods of integration differ in the way
the base MT engine is applied. Since we always dis-
card the choices of specific inflected forms for the
target stems by converting candidate translations to
sequences of stems, it is interesting to know whether
we need a base MT system that produces fully in-
flected translations or whether we can do as well
or better by training the base MT systems to pro-
duce sequences of stems. Stemming the target sen-
tences is expected to be helpful for word alignment,
especially when the stemming operation is defined
so that the word alignment becomes more one-to-
one (Goldwater and McClosky, 2005). In addition,
stemming the target sentences reduces the sparsity
in the translation tables and language model, and is
likely to impact positively the performance of an MT
system in terms of its ability to recover correct se-
quences of stems in the target. Also, machine learn-
ing tells us that solving a more complex problem
than we are evaluated on (in our case for the base
MT, predicting stems together with their inflections
instead of just predicting stems) is theoretically un-
justified (Vapnik, 1995).
However, for some language pairs, stemming one
language can make word alignment worse, if it
leads to more violations in the assumptions of cur-
rent word alignment models, rather than making the
source look more like the target. In addition, using a
trigram LM on stems may lead to larger violations of
the Markov independence assumptions, than using a
trigram LM on fully inflected words. Thus, if we ap-
ply the exact same base MT system to use stemmed
forms in alignment and/or translation, it is not a pri-
ori clear whether we would get a better result than if
we apply the system to use fully inflected forms.
518
5.1 Method 1
In this method, the base MT system is trained in
the usual way, from aligned pairs of source sen-
tences and fully inflected target sentences. The in-
flection model is then applied to re-inflect the 1-best
or m-best translations and to select an output trans-
lation. The hypotheses in the m-best output from the
base MT system are stemmed and the scores of the
stemmed hypotheses are assumed to be equal to the
scores of the original ones.3 Thus we obtain input of
the needed form, consisting of m sequences of target
language stems along with scores.
For this and other methods, if we are working
with an m-best list from the treelet system, every
translation hypothesis contains the annotations A
that our model needs, because the system maintains
the alignment, parse trees, etc., as part of its search
space. Thus we do not need to do anything further
to obtain input of the form necessary for application
of the inflection model.
For the phrase-based system, we generated the
annotations needed by first parsing the source sen-
tence e, aligning the source and candidate transla-
tions with the word-alignment model used in train-
ing, and projected the dependency tree to the target
using the algorithm of (Quirk et al, 2005). Note that
it may be better to use the word alignment main-
tained as part of the translation hypotheses during
search, but our solution is more suitable to situations
where these can not be easily obtained.
For all methods, we study two settings for integra-
tion. In the first, we only consider (n=1) hypotheses
from the base MT system. In the second setting, we
allow the model to use up to 100 translations, and
to automatically select the best number to use. As
seen in Table 3, (n=16) translations were chosen for
Russian and as seen in Table 5, (n=2) were chosen
for Arabic for this method.
5.2 Method 2
In this method, the base MT system is trained to pro-
duce sequences of stems in the target language. The
most straightforward way to achieve this is to stem
the training parallel data and to train the MT sys-
tem using this input. This is our Method 3 described
3It may be better to take the max of the scores for a stem
sequence occurring more than once in the list, or take the log-
sum-exp of the scores.
below. We formulated Method 2 as an intermedi-
ate step, to decouple the impact of stemming at the
alignment and translation stages.
In Method 2, word alignment is performed us-
ing fully inflected target language sentences. After
alignment, the target language is stemmed and the
base MT systems? sub-models are trained using this
stemmed input and alignment. In addition to this
word-aligned corpus the MT systems use another
product of word alignment: the IBM model 1 trans-
lation tables. Because the trained translation tables
of IBM model 1 use fully inflected target words, we
generated stemmed versions of the translation tables
by applying the rules of probability.
5.3 Method 3
In this method the base MT system produces se-
quences of target stems. It is trained in the same way
as the baseline MT system, except its input parallel
training data are preprocessed to stem the target sen-
tences. In this method, stemming can impact word
alignment in addition to the translation models.
6 MT performance results
Before delving into the results for each method, we
discuss our evaluation measures. For automatically
measuring performance, we used 4-gram BLEU
against a single reference translation. We also report
oracle BLEU scores which incorporate two kinds of
oracle knowledge. For the methods using n=1 trans-
lation from a base MT system, the oracle BLEU
score is the BLEU score of the stemmed translation
compared to the stemmed reference, which repre-
sents the upper bound achievable by changing only
the inflected forms (but not stems) of the words in a
translation. For models using n > 1 input hypothe-
ses, the oracle also measures the gain from choos-
ing the best possible stem sequence in the provided
(m=100-best) hypothesis list, in addition to choos-
ing the best possible inflected forms for these stems.
For the models in the tables, even if, say, n=16 was
chosen in parameter fitting, the oracle is measured
on the initially provided list of 100-best.
6.1 English-Russian treelet system
Table 3 shows the results of the baseline and the
model using the different methods for the treelet
MT system on English-Russian. The baseline is the
519
Model BLEU Oracle BLEU
Base MT (n=1) 29.24 -
Method 1 (n=1) 30.44 36.59
Method 1 (n=16) 30.61 45.33
Method 2 (n=1) 30.79 37.38
Method 2 (n=16) 31.24 48.48
Method 3 (n=1) 31.42 38.06
Method 3 (n=32) 31.80 49.19
Table 3: Test set performance for English-to-Russian MT
(BLEU) results by model using a treelet MT system.
treelet system described in Section 4.1 and trained
on the data in Table 2.
We can see that Method 1 results in a good im-
provement of 1.2 BLEU points, even when using
only the best (n = 1) translation from the baseline.
The oracle improvement achievable by predicting
inflections is quite substantial: more than 7 BLEU
points. Propagating the uncertainty of the baseline
system by using more input hypotheses consistently
improves performance across the different methods,
with an additional improvement of between .2 and
.4 BLEU points.
From the results of Method 2 we can see that re-
ducing sparsity at translation modeling is advanta-
geous. Both the oracle BLEU of the first hypothe-
sis and the achieved performance of the model im-
proved; the best performance achieved by Method 2
is .63 points higher than the performance of Method
1. We should note that the oracle performance for
Method 2, n > 1 is measured using 100-best lists of
target stem sequences, whereas the one for Method
1 is measured using 100-best lists of inflected target
words. This can be a disadvantage for Method 1,
because a 100-best list of inflected translations actu-
ally contains about 50 different sequences of stems
(the rest are distinctions in inflections). Neverthe-
less, even if we measure the oracle for Method 2
using 40-best, it is higher than the 100-best oracle
of Method 1. In addition, it appears that using a hy-
pothesis list larger than n > 1=100 is not be helpful
for our method, as the model chose to use only up to
32 hypotheses.
Finally, we can see that using stemming at the
word alignment stage further improved both the or-
acle and the achieved results. The performance of
the best model is 2.56 BLEU points better than the
baseline. Since stemming in Russian for the most
part removes properties of words which are not ex-
pressed in English at the word level, these results
are consistent with previous results using stemming
to improve word alignment. From these results, we
also see that about half of the gain from using stem-
ming in the base MT system came from improving
word alignment, and half came from using transla-
tion models operating at the less sparse stem level.
Overall, the improvement achieved by predicting
morphological properties of Russian words with a
feature-rich component model is substantial, given
the relatively large size of the training data (1.6 mil-
lion sentences), and indicates that these kinds of
methods are effective in addressing the problems
in translating morphology-poor to morphology-rich
languages.
6.2 English-Russian phrasal system
For the phrasal system, we performed integration
only with Method 1, using the top 1 or 100-
best translations. This is the most straightforward
method for combining with any system, and we ap-
plied it as a proof-of-concept experiment.
Model BLEU Oracle BLEU
Base MT (n=1) 36.00 -
Method 1 (n=1) 36.43 42.33
Method 1 (n=100) 36.72 55.00
Table 4: Test set performance for English-to-Russian MT
(BLEU) results by model using a phrasal MT system.
The phrasal MT system is trained on the same
data as the treelet system. The phrase size and dis-
tortion limit were optimized (we used phrase size of
7 and distortion limit of 3). This system achieves a
substantially better BLEU score (by 6.76) than the
treelet system. The oracle BLEU score achievable
by Method 1 using n=1 translation, though, is still
6.3 BLEU point higher than the achieved BLEU.
Our model achieved smaller improvements for the
phrasal system (0.43 improvement for n=1 transla-
tions and 0.72 for the selected n=100 translations).
However, this improvement is encouraging given the
large size of the training data. One direction for
potentially improving these results is to use word
alignments from the MT system, rather than using
an alignment model to predict them.
520
Model BLEU Oracle BLEU
Base MT (n=1) 35.54 -
Method 1 (n=1) 37.24 42.29
Method 1 (n=2) 37.41 52.21
Method 2 (n=1) 36.53 42.46
Method 2 (n=4) 36.72 54.74
Method 3 (n=1) 36.87 42.96
Method 3 (n=2) 36.92 54.90
Table 5: Test set performance for English-to-Arabic MT
(BLEU) results by model using a treelet MT system.
6.3 English-Arabic treelet system
The Arabic system also improves with the use of our
mode: the best system (Method 1, n=2) achieves
the BLEU score of 37.41, a 1.87 point improve-
ment over the baseline. Unlike the case of Rus-
sian, Method 2 and 3 do not achieve better results
than Method 1, though the oracle BLEU score im-
proves in these models (54.74 and 54.90 as opposed
to 52.21 of Method 1). We do notice, however, that
the oracle improvement for the 1-best analysis is
much smaller than what we obtained in Russian.
We have been unable to closely diagnose why per-
formance did not improve using Methods 2 and 3
so far due to the absence of expertise in Arabic, but
one factor we suspect is affecting performance the
most in Arabic is the definition of stemming: the
effect of stemming is most beneficial when it is ap-
plied specifically to normalize the distinctions not
explicitly encoded in the other language; it may hurt
performance otherwise. We believe that in the case
of Arabic, this latter situation is actually happen-
ing: grammatical properties explicitly encoded in
English (e.g., definiteness, conjunction, pronominal
clitics) are lost when the Arabic words are stemmed.
This may be having a detrimental effect on the MT
systems that are based on stemmed input. Further
investigation is necessary to confirm this hypothesis.
6.4 Human evaluation
In this section we briefly report the results of human
evaluation on the output of our inflection prediction
system, as the correlation between BLEU scores and
human evaluation results is not always obvious. We
compared the output of our component against the
best output of the treelet system without our com-
ponent. We evaluated the following three scenarios:
(1) Arabic Method 1 with n=1, which corresponds
to the best performing system in BLEU according to
Table 5; (2) Russian, Method 1 with n=1; (3) Rus-
sian, Method 3 with n=32, which corresponds to the
best performing system in BLEU in Table 3. Note
that in (1) and (2), the only differences in the com-
pared outputs are the changes in word inflections,
while in (3) the outputs may differ in the selection
of the stems.
In all scenarios, two human judges (native speak-
ers of these languages) evaluated 100 sentences that
had different translations by the baseline system and
our model. The judges were given the reference
translations but not the source sentences, and were
asked to classify each sentence pair into three cate-
gories: (1) the baseline system is better (score=-1),
(2) the output of our model is better (score=1), or (3)
they are of the same quality (score=0).
human eval score BLEU diff
Arabic Method 1 0.1 1.9
Russian Method 1 0.255 1.2
Russian Method 3 0.26 2.6
Table 6: Human evaluation results
Table 6 shows the results of the averaged, aggre-
gated score across two judges per evaluation sce-
nario, along with the BLEU score improvements
achieved by applying our model. We see that in all
cases, the human evaluation scores are positive, indi-
cating that our models produce translations that are
better than those produced by the baseline system. 4
We also note that in Russian, the human evaluation
scores are similar for Method 1 and 3 (0.255 and
0.26), though the BLEU score gains are quite differ-
ent (1.2 vs 2.6). This may be attributed to the fact
that human evaluation typically favors the scenario
where only word inflections are different (Toutanova
and Suzuki, 2007).
7 Conclusion and future work
We have shown that an independent model of mor-
phology generation can be successfully integrated
with an SMT system, making improvements in both
phrasal and syntax-based MT. In the future, we
would like to include more sophistication in the de-
sign of a lexicon for a particular language pair based
on error analysis, and extend our pre-processing to
include other operations such as word segmentation.
4However, the improvement in Arabic is not statistically sig-
nificant on this 100 sentence set.
521
References
Tim Buckwalter. 2004. Buckwalter arabic morphological
analyzer version 2.0.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Ini-
tial explorations in English to Turkish statistical ma-
chine translation. In NAACL workshop on statistical
machine translation.
Jenny Finkel, Christopher Manning, and Andrew Ng.
2006. Solving the problem of cascading errors: ap-
proximate Bayesian inference for linguistic annotation
pipelines. In EMNLP.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
EMNLP.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
HLT-NAACL.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL Workshop on Statistical
Machine Translation.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In EMNLP-CoNNL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
AMTA.
Young-Suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In ACL.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency tree translation: Syntactically informed
phrasal SMT. In ACL.
Kristina Toutanova and Hisami Suzuki. 2007. Generating
case markers in machine translation. In NAACL-HLT.
Vladimir Vapnik. 1995. The nature of Statistical Learn-
ing Theory. Springer-Verlag.
Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Cap-
italizing machine translation. In HLT-NAACL.
522
