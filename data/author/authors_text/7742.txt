Taxonomy learning ? factoring the structure of a taxonomy into a 
semantic classification decision 
 
Viktor PEKAR 
Bashkir State University  
Ufa, Russia, 450000 
vpekar@ufanet.ru 
Steffen STAAB 
Institute AIFB, University of Karlsruhe 
http://www.aifb.uni-karlsruhe.de/WBS  
& Learning Lab Lower Saxony 
http://www.learninglab.de 
 
Abstract  
The paper examines different possibilities 
to take advantage of the taxonomic or-
ganization of a thesaurus to improve the 
accuracy of classifying new words into its 
classes. The results of the study demon-
strate that taxonomic similarity between 
nearest neighbors, in addition to their dis-
tributional similarity to the new word, 
may be useful evidence on which classifi-
cation decision can be based. 
1. Introduction 
Machine-readable thesauri are now an indispen-
sable part for a wide range of NLP applications 
such as information extraction or semantics-
sensitive information retrieval. Since their man-
ual construction is very expensive, a lot of recent 
NLP research has been aiming to develop ways 
to automatically acquire lexical knowledge from 
corpus data. 
In this paper we address the problem of large-
scale augmenting a thesaurus with new lexical 
items. The specifics of the task are a big number 
of classes into which new words need to be clas-
sified and hence a lot of poorly predictable se-
mantic distinctions that have to be taken into 
account. For this reason, knowledge-poor ap-
proaches such as the distributional approach are 
particularly suited for this task. Its previous ap-
plications (e.g., Grefenstette 1993, Hearst and 
Schuetze 1993, Takunaga et al1997, Lin 1998, 
Caraballo 1999) demonstrated that cooccurrence 
statistics on a target word is often sufficient for 
its automatical classification into one of numer-
ous classes such as synsets of WordNet. 
Distributional techniques, however, are poorly 
applicable to rare words, i.e., those words for 
which a corpus does not contain enough cooc-
currence data to judge about their meaning. Such 
words are the primary concern of many practical 
NLP applications: as a rule, they are semanti-
cally focused words and carry a lot of important 
information. If one has to do with a specific 
domain of lexicon, sparse data is a problem par-
ticularly difficult to overcome. 
The major challenge for the application of the 
distributional approach in this area is, therefore, 
the development of ways to minimize the 
amount of corpus data required to successfully 
carry out a task. In this study we focus on opti-
mization possibilities of an important phase in 
the process of automatically augmenting a the-
saurus ? the classification algorithm. The main 
hypothesis we test here is that the accuracy of 
semantic classification may be improved by 
taking advantage of information about taxo-
nomic relations between word classes contained 
in a thesaurus. 
On the example of a domain-specific thesaurus 
we compare the performance of three state-of-
the-art classifiers which presume flat organiza-
tion of thesaurus classes and two classification 
algorithms, which make use of taxonomic or-
ganization of the thesaurus: the "tree descend-
ing" and the "tree ascending" algorithms. We 
find that a version of the tree ascending algo-
rithm, though not improving on other methods 
overall, is much better at choosing a supercon-
cept for the correct class of the new word. We 
then propose to use this algorithm to first narrow 
down the search space and then apply the kNN 
method to determine the correct class among 
fewer candidates. 
The paper is organized as follows. Sections 2 
and 3 describe the classification algorithms un-
der study. Section 4 describes the settings and 
data of the experiments. Section 5 details the 
evaluation method. Section 6 presents the results 
of the experiments. Section 7 concludes. 
2. Classification methods 
Classification techniques previously applied to 
distributional data can be summarized according 
to the following methods: the k nearest neighbor 
(kNN) method, the category-based method and 
the centroid-based method. They all operate on 
vector-based semantic representations, which 
describe the meaning of a word of interest (tar-
get word) in terms of counts1 of its coocurrence 
with context words, i.e., words appearing within 
some delineation around the target word. The 
key differences between the methods stem from 
different underlying ideas about how a semantic 
class of words is represented, i.e. how it is de-
rived from the original cooccurrence counts, and, 
correspondingly, what defines membership in a 
class. 
The kNN method is based on the assumption 
that membership in a class is defined by the new 
instance?s similarity to one or more individual 
members of the class. Thereby, similarity is 
defined by a similarity score as, for instance, by 
the cosine between cooccurrence vectors. To 
classify a new instance, one determines the set 
of k training instances that are most similar to 
the new instance. The new instance is assigned 
to the class that has the biggest number of its 
members in the set of nearest neighbors. In addi-
tion, the classification decision can be based on 
the similarity measure between the new instance 
and its neighbors: each neighbor may vote for its 
class with a weight proportional to its closeness 
to the new instance. When the method is applied 
to augment a thesaurus, a class of training in-
stances is typically taken to be constituted by 
words belonging to the same synonym set, i.e. 
lexicalizing the same concept (e.g., Hearst and 
Schuetze 1993). A new word is assigned to that 
synonym set that has the biggest number of its 
members among nearest neighbors. 
                                                     
1 Or, probabilities determined via Maximum Likeli-
hood Estimation. 
The major disadvantage of the kNN method that 
is often pointed out is that it involves significant 
computational expenses to calculate similarity 
between the new instance and every instance of 
the training set. A less expensive alternative is 
the category-based method (e.g., Resnik 1992). 
Here the assumption is that membership in a 
class is defined by the closeness of the new item 
to a generalized representation of the class. The 
generalized representation is built by adding up 
all the vectors constituting a class and normalis-
ing the resulting vector to unit length, thus com-
puting a probabilistic vector representing the 
class. To determine the class of a new word, its 
unit vector is compared to each class vector.  
Thus the number of calculations is reduced to 
the number of classes. Thereby, a class represen-
tation may be derived from a set of vectors cor-
responding to one synonym set (as is done by 
Takunaga et al 1997) or a set of vectors corre-
sponding to a synonym set and some or all sub-
ordinate synonym sets (Resnik 1992). 
Another way to prepare a representation of a 
word class is what may be called the centroid-
based approach (e.g., Pereira et al 1993). It is 
almost exactly like the category-based method, 
the only difference being that a class vector is 
computed slightly differently. All n vectors cor-
responding to class members are added up and 
the resulting vector is divided by n to compute 
the centroid between the n vectors. 
3. Making use of the structure of the the-
saurus 
The classification methods described above pre-
suppose that semantic classes being augmented 
exist independently of each other. For most ex-
isting thesauri this is not the case: they typically 
encode taxonomic relations between word 
classes. It seems worthwhile to employ this in-
formation to enhance the performance of the 
classifiers. 
3.1 Tree descending algorithm 
One way to factor the taxonomic information 
into the classification decision is to employ the 
?tree-descending? classification algorithm, 
which is a familiar technique in text categoriza-
tion. The principle behind this approach is that 
the semantics of every concept in the thesaurus 
tree retains some of the semantics of all its hy-
ponyms in such a way that the upper the concept, 
the more relevant semantic characteristics of its 
hyponyms it reflects. It is thus feasible to deter-
mine the class of a new word by descending the 
tree from the root down to a leaf. The semantics 
of concepts in the thesaurus tree can be repre-
sented by means of one of the three methods to 
represent a class described in Section 2. At every 
tree node, the decision which path to follow is 
made by choosing the child concept that has the 
biggest distributional similarity to the new word. 
After the search has reached a leaf, the new 
word is assigned to that synonym set, which 
lexicalizes the concept that is most similar to the 
new word. This manner of search offers two 
advantages. First, it allows to gradually narrow 
down the search space and thus save on compu-
tational expenses. Second, it ensures that, in a 
classification decision, more relevant semantic 
distinctions of potential classes are given more 
preference than less relevant ones. As in the case 
with the category-based and the centroid-based 
representations, the performance of the method 
may be greatly dependent on the number of sub-
ordinate synonyms sets included to represent a 
concept. 
3.2 Tree ascending algorithm 
Another way to use information about inter-class 
relations contained in a thesaurus is to base the 
classification decision on the combined meas-
ures of distributional similarity and taxonomic 
similarity (i.e., semantic similarity induced from 
the relative position of the words in the thesau-
rus) between nearest neighbors. Suppose words 
in the nearest neighbors set for a given new 
word, e.g., trailer, all belong to different classes 
as in the following classification scenario: box 
(similarity score  to trailer: 0.8), house (0.7), 
barn (0.6), villa (0.5) (Figure 1). In this case, 
kNN will classify trailer into the class 
CONTAINER, since it appears to have biggest 
similarity to box. However, it is obvious that the 
most likely class of trailer is in a different part 
of the thesaurus: in the nearest neighbors set 
there are three words which, though not belong-
ing to one class, are semantically close to each 
other. It would thus be safer to assign the new 
word to a concept that subsumes one or all of the 
three semantically similar neighbors. For exam-
ple, the concepts DWELLING or BUILDING could 
be feasible candidates in this situation. 
Figure 1. A semantic classification scenario. 
The crucial question here is how to calculate the 
total of votes for these two concepts to be able to 
decide which of them to choose or whether to 
prefer CONTAINER. Clearly, one cannot sum or 
average the distributional similarity measures of 
neighbors below a candidate concept. In the first 
case the root will always be the best-scoring 
concept. In the second case the score of the can-
didate concept will always be smaller than the 
score of its biggest-scoring hyponym. 
We propose to estimate the total of votes for 
such candidate concepts based on taxonomic 
similarity between relevant nodes. The taxo-
nomic similarity between two concepts is meas-
ured according to the procedure elaborated in 
(Maedche & Staab, 2000). Assuming that a tax-
onomy is given as a tree with a set of nodes N, a 
set of edges E ? N?N, a unique root ROOT ? N, 
one first determines the least common supercon-
cept of a pair of concepts a,b being compared. It 
is defined by 
)),(),(),((minarg),( crootcbcabalcs
Nc
??? ++=
?
(1) 
where ?(a,b) describes the number of edges on 
the shortest path between a and b. The taxonomic 
similarity between a and b is then given by 
( ) ),(),(),(
),(, crootcbca
crootba ???
?
++
=?        (2) 
where c = lcs(a,b). T is such that 0? T ? 1, with 1 
standing for the maximum taxonomic similarity. 
T is directly proportional to the number of edges 
from the least common superconcept to the root, 
which agrees with the intuition that a given num-
ber of edges between two concrete concepts sig-
nifies greater similarity than the same number of 
edges between two abstract concepts. 
We calculate the total of votes for a candidate 
concept by summing the distributional similarity 
measures of its hyponyms to the target word t 
each weighted by the taxonomic similarity 
measure between the hyponym and the candi-
date node: 
?
?
?=
nIh
hnThtsimnW ),(),()(     (3) 
where In is the set of hyponyms below the can-
didate concept n, sim(t,h) is the distributional 
similarity between a hyponym h and the word to 
be classified t, and T(n,h) is the taxonomic simi-
larity between the candidate concept and the 
hyponym h. 
4. Data and settings of the experiments 
The machine-readable thesaurus we used in this 
study was derived from GETESS2, an ontology 
for the tourism domain. Each concept in the 
ontology is associated with one lexical item, 
which expresses this concept.  From this ontol-
ogy, word classes were derived in the following 
manner. A class was formed by words lexicaliz-
ing all child concepts of a given concept. For 
example, the concept CULTURAL_EVENT in the 
ontology has successor concepts PERFORMANCE, 
OPERA, FESTIVAL, associated with words per-
formance, opera, festival correspondingly. 
Though these words are not synonyms in the 
traditional sense, they are taken to constitute one 
semantic class, since out of all words of the on-
tology?s lexicon their meanings are closest. The 
thesaurus thus derived contained 1052 words 
and phrases (the corpus used in the study had 
data on 756 of them). Out of the 756 concepts, 
182 were non-final; correspondingly, 182 word 
classes were formed. The average depth level of 
the thesaurus is 5.615, the maximum number of 
levels is 9. The corpus from which distributional 
data was obtained was extracted from a web site 
advertising hotels around the world 3 . It con-
tained around 1 million words. 
Collection of distributional data was carried out 
in the following settings. The preprocessing of 
corpus included a very simple stemming (most 
                                                     
2 http://www.daml.org/ontologies/171 
3 http://www.placestostay.com 
common inflections were chopped off; irregular 
forms of verbs, adjectives and nouns were 
changed to their first forms). The context of 
usage was delineated by a window of 3 words on 
either side of the target word, without 
transgressing sentence boundaries. In case a stop 
word other than a proper noun appeared inside 
the window, the window was accordingly ex-
panded. The stoplist included 50 most frequent 
words of the British National Corpus, words 
listed as function words in the BNC, and proper 
nouns not appearing in the sentence-initial posi-
tion. The obtained frequencies of cooccurrence 
were weighted by the 1+log weight function. 
The distributional similarity was measured by 
means of three different similarity measures: the 
Jaccard?s coefficient, L1 distance, and the skew 
divergence. This choice of similarity measures 
was motivated by results of studies by (Levy et 
al 1998) and (Lee 1999) which compared several 
well known measures on similar tasks and found 
these three to be superior to many others. An-
other reason for this choice is that there are dif-
ferent ideas underlying these measures: while 
the Jaccard?s coefficient is a binary measure, L1 
and the skew divergence are probabilistic, the 
former being geometrically motivated and the 
latter being a version of the information theo-
retic Kullback Leibler divergence (cf., Lee 
1999).  
5. Evaluation method 
The performance of the algorithms was assessed 
in the following manner. For each algorithm, we 
held out a single word of the thesaurus as the 
test case, and trained the system on the remain-
ing 755 words. We then tested the algorithm on 
the held-out vector, observing if the assigned 
class for that word coincided with its original 
class in the thesaurus, and counting the number 
of correct classifications (?direct hits?). This was 
repeated for each of the words of the thesaurus. 
However, given the intuition that a semantic 
classification may not be simply either right or 
wrong, but rather of varying degrees of appro-
priateness, we believe that a clearer idea about 
the quality of the classifiers would be given by 
an evaluation method that takes into account 
?near misses? as well. We therefore evaluated 
the performance of the algorithms also in terms 
of Learning Accuracy (Hahn & Schnattinger 
1998), i.e., in terms of how close on average the 
proposed class for a test word was to the correct 
class. For this purpose the taxonomic similarity 
between the assigned and the correct classes is 
measured so that the appropriateness of a par-
ticular classification is estimated on a scale be-
tween 0 and 1, with 1 signifying assignment to 
the correct class. Thus Learning Accuracy is 
compatible with the counting of direct hits, 
which, as will be shown later, may be useful for 
evaluating the methods. 
In the following, the evaluation of the classifica-
tion algorithms is reported both in terms of the 
average of direct hits and Learning Accuracy (?di-
rect+near hits?) over all words in the thesaurus. 
To have a benchmark for evaluation of the algo-
rithms, a baseline was calculated, which was the 
average hit value a given word gets, when its 
class label is chosen at random. The baseline for 
direct hits was estimated at 0.012; for di-
rect+near hits, it was 0.15741. 
6. Results 
We first conducted experiments evaluating per-
formance of the three standard classifiers. To 
determine the best version for each particular 
classifier, only those parameters were varied that, 
as described above, we deemed to be critical in 
the setting of thesaurus augmentation.  
In order to get a view on how the accuracy of the 
algorithms was related to the amount of avail-
able distributional data on the target word, all 
words of the thesaurus were divided into three 
groups depending on the amount corpus data 
available on them (see Table 1). The amount of 
distributional data for a word (the ?frequency? in 
the left column) is the total of frequencies of its 
context words. 
Table 1. Distribution of words of the thesaurus 
into frequency ranges 
Frequency range # words in the range 
0-40 274 
40-500 190 
>500 292 
 
The results of the evaluation of the methods are 
summarized in the tables below. Rows specify 
the measures used to determine distributional 
similarity (JC for Jaccard?s coefficient, L1 for 
the L1 distance and SD for the skew divergence) 
and columns specify frequency ranges. Each cell 
describes the average of direct+near hits / the 
average of direct hits over words of a particular 
frequency range and over all words of the the-
saurus. The statistical significance of the results 
was measured in terms of the one-tailed chi-
square test. 
kNN. Evaluation of the method was conducted 
with k=1, 3, 5, 7, 10, 15, 20, 25, and 30. The 
accuracy of classifications increased with the 
increase of k. However, starting with k=15 the 
increase of k yielded only insignificant im-
provement. Table 2 describes results of evalua-
tion of kNN using 30 nearest neighbors, which 
was found to be the best version of kNN. 
Table 2. kNN, k=30. 
 0-40 40-500 >500 Overall 
JC .33773 
/.17142 
.33924 
/.15384 
.40181 
/.12457 
.37044 
/.15211 
L1  .33503 
/.16428 
.38424 
/.21025 
.38987 
/.14471 
.37636 
/.17195 
SD .31505 
/.14285 
.36316 
/.18461 
.45234 
/.17845 
.38806 
/.17063 
 
Category-based method. To determine the best 
version of this method, we experimented with 
the number of levels of hyponyms below a con-
cept that were used to build a class vector). The 
best results were achieved when a class was 
represented by data from its hyponyms at most 
three levels below it (Table 3).  
Table 3. Category-based method, 3 levels 
 0-40 40-500 >500 Overall 
JC .26918 
/.12142 
.34743 
/.17948 
.47404 
/.28282 
.37554 
/.2023 
L1 .27533 
/.125 
.41736 
/.25128 
.56711 
/.38383 
.43242 
/.26190 
SD .28589 
/.12857 
.34932 
/.18461 
.51306 
/.31649 
.39755 
/.21957 
 
Centroid-based method. As in the case with 
the category-based method, we varied the num-
ber of levels of hyponyms below the candidate 
concept. Table 4 details results of evaluation of 
the best version of this method (a class is repre-
sented by 3 levels of its hyponyms). 
 
Table 4. Centroid-based method, 3 levels. 
 0-40 40-500 >500 Overall 
JC .17362 
/.07831 
.18063 
/.08119 
.30246 
/.14434 
.22973 
/.10714 
L1 .21711 
/.09793 
.30955 
/.13938 
.37411 
/.1687 
.30723 
/.12698 
SD .22108 
/.09972 
.23814 
/.11374 
.36486 
/.16147 
.28665 
/.10714 
 
Comparing the three algorithms we see that 
overall, kNN and the category-based method 
exhibit comparable performance (with the ex-
ception of measuring similarity by L1 distance, 
when the category-based method outperforms 
kNN by a margin of about 5 points; statistical 
significance p<0.001). However, their perform-
ance is different in different frequency ranges: 
for lower frequencies kNN is more accurate (e.g., 
for L1 distance, p<0.001). For higher frequen-
cies, the category-based method improves on 
kNN (L1, p<0.001). The centroid-based method 
exhibited performance, inferior to both those of 
kNN and the category-based method. 
Tree descending algorithm. In experiments 
with the algorithm, candidate classes were repre-
sented in terms of the category-based method, 3 
levels of hyponyms, which proved to be the best 
generalized representation of a class in previous 
experiments. Table 5 specifies the results of its 
evaluation. 
Table 5. Tree descending algorithm. 
 0-40 40-500 >500 Overall 
JC .00726 
/0 
.01213 
/.00512 
.02312 
/.0101 
.014904 
/.005291 
L1 .08221 
/.03214 
.05697 
/.02051 
.21305 
/.11111 
.128844 
/.060846 
SD .08712 
/.03214 
.07739 
/.03589 
.16731 
/.06734 
.011796 
/.047619 
 
Its performance turns out to be much worse than 
that of the standard methods. Both direct+near 
and direct hits scores are surprisingly low, for 0-
40 and 40-500 much lower than chance. This 
can be explained by the fact that some of top 
concepts in the tree are represented by much less 
distributional data than other ones. For example, 
there are less than 10 words that lexicalize the 
top concepts MASS_CONCEPT and 
MATHEMATICAL_CONCEPT and all of their 
hyponyms (compare to more than 150 words 
lexicalizing THING and its hyponyms up to 3 
levels below it). As a result, at the very begin-
ning of the search down the tree, a very large 
portion of test words was found to be similar to 
such concepts. 
Tree ascending algorithm. The experiments 
were conducted with the same number of nearest 
neighbors as with kNN. Table 6 describes the 
results of evaluation of the best version (formula 
3, k=15). 
Table 6. Tree ascending algorithm, total of votes 
according to (3), k=15. 
 0-40 40-500 >500 Overall 
JC .32112 
/.075 
.33553 
/.0923 
.40968 
/.08754 
.36643 
/.08597 
L1 .33369 
/.07142 
.34504 
/.0923 
.42627 
/.09764 
.38005 
/.08862 
SD .31809 
/.06785 
.32489 
/.05128 
.45529 
/.11111 
.38048 
/.08201 
 
There is no statistically significant improvement 
on kNN overall, or in any of the frequency 
ranges. The algorithm favored more upper con-
cepts and thus produced about twice as few di-
rect hits than kNN. At the same time, its di-
rect+near hits score was on par with that of kNN! 
This algorithm thus produced much more near 
hits than kNN, what can be interpreted as its 
better ability to choose a superconcept of the 
correct class. Based on this observation, we 
combined the best version of the tree ascending 
algorithm with kNN in one algorithm in the 
following manner. First the former was used to 
determine a superconcept of the class for the 
new word and thus to narrow down the search 
space. Then the kNN method was applied to 
pick a likely class from the hyponyms of the 
concept determined by the tree ascending 
method. Table 7 specifies the results of evalua-
tion of the proposed algorithm. 
Table 7. Tree ascending algorithm combined with 
kNN, k=30. 
 0-40 40-500 >500 Overall 
JC .34444 
/.16428 
.35858 
/.14358 
.41260 
/.10774 
.38215 
/.14021 
L1 .35147 
/.16428 
.36545 
/.15384 
.41086 
/.11784 
.38584 
/.14682 
SD .32613 
/.13571 
.36485 
/.1641 
.45732 
/.16498 
.39456 
/.1574 
 
The combined algorithm demonstrated impro-
vement both on kNN and the tree ascending 
method of 1 to 3 points in every frequency range 
and overall for direct+near hits (except for the 
40-500 range, L1). The improvement was statis-
tically significant only for L1, ?>500? (p=0.05) 
and for L1, overall (p=0.011). For other similari-
ty measures and frequency ranges it was insigni-
ficant (e.g., for JC, overall, p=0.374; for SD, 
overall, p=0.441). The algorithm did not im-
prove on kNN in terms of direct hits. The hits 
scores set in bold in Table 7 are those which are 
higher than those for kNN in corresponding 
frequency ranges and similarity measures. 
7. Discussion 
In this paper we have examined different possi-
bilities to take advantage of the taxonomic or-
ganization of a thesaurus to improve the accu-
racy of classifying new words into its classes. 
The study demonstrated that taxonomic similar-
ity between nearest neighbors, in addition to 
their distributional similarity to the new word, 
may be a useful evidence on which classification 
decision can be based. We have proposed a ?tree 
ascending? classification algorithm which ex-
tends the kNN method by making use of the 
taxonomic similarity between nearest neighbors. 
This algorithm was found to have a very good 
ability to choose a superconcept of the correct 
class for a new word. On the basis of this finding, 
another algorithm was developed that combines 
the tree ascending algorithm and kNN in order 
to optimize the search for the correct class. Al-
though only limited statistical significance of its 
improvement on kNN was found, the results of 
the study indicate that this algorithm is a promis-
ing possibility to incorporate the structure of a 
thesaurus into the decision as to the class of the 
new word. We conjecture that the tree ascending 
algorithm leaves a lot of room for improvements 
and combinations with other algorithms like 
kNN. 
The tree descending algorithm, a technique 
widely used for text categorization, proved to be 
much less efficient than standard classifiers 
when applied to the task of augmenting a do-
main-specific thesaurus. Its poor performance is 
due to the fact that in such a thesaurus there are 
great differences between top concepts in the 
amount of distributional data used to represent 
them, which very often misleads the top-down 
search. 
We believe that a study of the two algorithms on 
the material of a larger thesaurus, where richer 
taxonomic information is available, can yield a 
further understanding of its role in the perform-
ance of the algorithms. 
References 
Caraballo S. A. (1999) Automatic construction of a 
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 120-126. 
Hahn U. and Schnattinger K. (1998) Towards text 
knowledge engineering. In Proc. of AAAI/IAAI, pp. 
524-531. 
Hearst M. and Schuetze H. (1993) Customizing a 
lexicon to better suit a computational task. In Proc. 
of the SIGLEX Workshop on Acquisition of Lexical 
Knowledge from Text, Columbus Ohio, pp. 55--69. 
Grefenstette G. (1993) Evaluation techniques for 
automatic semantic extraction: comparing syntac-
tic and window based approaches. In Proc. of the 
SIGLEX Workshop on Acquisition of Lexical 
Knowledge from Text, Columbus Ohio. 
Lin D. (1998) Automatic retrieval and clustering of 
similar words. In Proc. of the COLING-ACL?98, 
pp. 768-773. 
Lee L. (1999) Measures of distributional similarity. 
In Proc. of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 25-32. 
Levy J., Bullinaria J., and Patel M. (1998) Explora-
tions in the derivation of word co-occurrence sta-
tistics. South Pacific Journal of Psychology, 10/1, 
pp. 99-111.  
Maedche A. and Staab S. (2000) Discovering concep-
tual relations from text. In Proc. of ECAI-2000, 
IOS Press, pp. 321-324.  
Pereira F., Tishby N., and Lee L. (1993) Distribu-
tional clustering of English words. In Proc. of the 
31st Annual Meeting of the ACL, pp. 183-190. 
Resnik P. (1992) Wordnet and distributional analysis: 
A class-based approach to lexical discovery. AAAI 
Workshop on Statistically-based Natural Language 
Processing Techniques. 
Tokunaga T., Fujii A., Iwayama M., Sakurai N., and 
Tanaka H. (1997) Extending a thesaurus by classi-
fying words. In Proc. of the ACL-EACL Workshop 
on Automatic Information Extraction and Building 
of Lexical Semantic Resources, pp. 16-21.  
Feature Weighting for Co-occurrence-based Classification of Words
Viktor PEKAR
CLG, U. of Wolverhampton
Wolverhampton
UK, WV1 1SB
v.pekar@wlv.ac.uk
Michael KRKOSKA
Mentasys GmbH
Schonfeldstrasse 8
Karlsruhe, Germany, 76131
michael@mentasys.de
Steffen STAAB
Ontoprise GmbH & Institute
AIFB, U. of Karlsruhe
Karlsruhe, Germany, 76128
staab@aifb.uni-karlsruhe.de
Abstract*
The paper comparatively studies methods of
feature weighting in application to the task of
cooccurrence-based classification of words
according to their meaning. We explore parameter
optimization of several weighting methods
frequently used for similar problems such as text
classification. We find that successful application
of all the methods crucially depends on a number
of parameters; only a carefully chosen weighting
procedure allows to obtain consistent improvement
on a classifier learned from non-weighted data.
1 Introduction
Lexical repositories like thesauri and lexicons are
today a key component of many NLP technologies,
where they serve as background knowledge for
processing the semantics of text. But, as is well
known, manual compilation of such resources is a
very costly procedure, and their automated
construction is an important research issue.
One promising possibility to speed up the lexical
acquisition process is to glean the semantics of
words from a corpus by adopting the co-
occurrence model of word meaning. Previous
research has investigated a wide range of its
applications, including automatic construction of
thesauri, their enrichment, acquisition of bilingual
lexicons, learning of information extraction
patterns, named entity classification and others..
The basic idea behind the approach is that the
distribution of a word across lexical contexts (other
words and phrases it co-occurs with) is highly
indicative of its meaning. The method represents
the meaning of a word as a vector where each
feature corresponds to a context and its value to the
frequency of the word?s occurring in that context.
Once such representation is built, machine learning
techniques can be used to perform various lexical
acquisition tasks, e.g. automatically classify or
cluster words according to their meaning.
However, using natural language words as
features inevitably results in very noisy
                                                
.* The study was partially supported by the Russian
Foundation Basic Research grant #03-06-80008.
representations. Because of their inherent
polysemy and synonymy, many context words
become ambiguous or redundant features. It is
therefore desirable to determine a measure of
usefulness of each feature and weight it
accordingly. Still, despite a wide variety of feature
weighting methods existing in machine learning,
these methods are poorly explored in application to
lexical acquisition. There have been a few studies
(e.g., Lin, 1998; Ciaramita, 2002; Alfonseca and
Manandhar, 2002) where word representations are
modified through this or that kind of feature
weighting. But in these studies it is performed only
as a standard pre-processing step on the analogy
with similar tasks like text categorization, and the
choice of a particular weighting procedure is
seldom motivated. To our knowledge, there is no
work yet on evaluation and comparison of different
weighting methods for lexical acquisition.
The goal of this paper is to comparatively study
a number of popular feature weighting methods in
application to the task of word classification.
The structure of the paper is the following.
Section 2 more formally describes the task of
feature weighting. Section 3 describes the
weighting methods under study. Section 4 details
the experimental data, classification algorithms
used, and evaluation methods. Section 5 is
concerned with the results of the experiments and
their discussion. Section 6 presents conclusions
from the study.
2 Two feature weighting strategies
In machine learning, feature weighting before
classification is performed with the purpose to
reflect how much particular features reveal about
class membership of instances. The weights of
features are determined from their distribution
across training classes, which is why the weighting
procedure can be called supervised. In the context
of word classification this procedure can be
formalized as follows.
Let us assume that each word n?N of the
training set is represented as a feature vector,
consisting of features f ? F, and that each n is
assigned a class label c?C, i.e. "n$c?C: n?c. For
each f, from its distribution across C, a certain
function computes its relevance score, specific to
each class. This score can be used directly as its
local weight w(f,c). Alternatively, from class-
specific weights of a feature, one can compute its
single global weight, using some globalization
policy. For example, as a global weight one can
use the maximum local weight of f across all
classes wglob(f)= ),(max cfwCc? . After the weights
have been applied to the training data, a classifier
is learned and evaluated on the test data.
A key decision in the weighting procedure is to
choose a function computing w(f,c). Such functions
typically try to capture the intuition that the best
features for a class are the ones that best
discriminate  the sets of its positive and negative
examples. They determine w(f,c) from the
distribution of f between c and c , attributing
greater weights to those f that correlate with c or c
most. In the present study we include three such
functions widely used in text categorization:
mutual information, information gain ratio and
odds ratio.
There is another view on feature scoring that it is
sometimes adopted in classification tasks.
According to this view, useful are those features
that are shared by the largest number of positive
examples of c. The purpose of emphasizing these
features is to characterize the class without
necessarily discriminating it from other classes.
Functions embodying this view assess w(f,c) from
the distribution of f across n ? c, giving greater
weight to those f that are distributed most uniformly.
Although they do not explicitly aim at underpinning
differences between classes, these functions were
shown to enhance text retrieval (Wilbur and
Sirotkin, 1992) and text categorization (Yang and
Pedersen, 1997). In this paper we experimented with
term strength, a feature scoring function previously
shown to be quite competitive in information
retrieval. Since term strength is an unsupervised
function, we develop two supervised variants of it
tailoring them for the classification task.
3 Feature Weighting Functions
3.1 Mutual Information
Mutual information (MI) is an information-
theoretic measure of association between two
words, widely used in statistical NLP. Pointwise
MI between class c and feature f measures how
much information presence of f contains about c:
)()(
),(log),(
cPfP
cfPcfMI =             (1)
3.2 Gain Ratio
Gain Ratio (GR) is a normalized variant of
Information Gain (IG), introduced into machine
learning from information theory (Quinlan, 1993).
IG measures the number of bits of information
obtained about presence and absence of a class by
knowing the presence or absence of the feature1:
? ?
? ?
=
},{ },{ )()(
),(log),(),(
ccd ffg dPgP
dgPdgPcfIG    (2)
Gain Ratio aims to overcome one disadvantage
of IG which is the fact that IG grows not only with
the increase of dependence between f and c, but
also with the increase of the entropy of f. That is
why features with low entropy receive smaller IG
weights although they may be strongly correlated
with a class. GR removes this factor by
normalizing IG by the entropy of the class:
?
?
-
=
},{
)(log)(
),(),(
ffg
gPgP
cgIGcfGR             (3)
3.3 Odds Ratio
Odds Ratio (OR) is used in information retrieval
to rank documents according to their relevance on
the basis of association of their features with a set
of positive documents. Mladenic (1998) reports
OR to be a particularly successful method of
selecting features for text categorization. The OR
of a feature f, given the set of positive examples
and negative examples for class c, is defined as2:
)|())|(1(
))|(1()|(),(
cfpcfp
cfpcfpcfOR
?-
-?=               (4)
3.4 Term Strength
Term Strength (TS) was introduced by Wilbur
and Sirotkin (1992) for improving efficiency of
document retrieval by feature selection. It was later
studied in a number of works by Yang and her
colleagues (e.g., Yang and Pedersen, 1997), who
found that it performs on par with best
discriminative functions on the document
categorization task. This method is based on the
idea that most valuable features are shared by
related documents. It defines the weight of a
                                                
1 Strictly speaking, the definition does not define IG,
but conditional entropy H(c|f) ; the other ingredient of
the IG function, the entropy of c, being constant and
thus omitted from actual weight calculation.
2 In cases when p(f|c) equals 1 or p(f|c ) equals 0, we
mapped the weight to the maximum OR weight in the class.
feature as the probability of finding it in some
document d given that it has also appeared in the
document d?, similar to d. To calculate TS for
feature f, for each n we first retrieved several
related words n? using a distributional similarity
measure, thus preparing a set of pairs (n, n?). The
TS weight for f was then calculated as the
conditional probability of f appearing in n given
that f appears also in n? (the ordering of words
inside a pair is ignored):
)'|()( nfnfPfTS ??=                          (5)
An important parameter in TS is the threshold on
the similarity measure used to judge two words to
be sufficiently related. Yang and Pedersen
determined this threshold by first deciding how
many documents can be related to a given one and
then finding the average minimum similarity
measure for this number of neighbors over all
documents in the collection. It should be noted that
TS does not make use of the information about
feature-class associations and therefore is
unsupervised and can be used only for global
feature weighting.
We introduce two supervised variants of TS,
which can be applied locally: TSL1 and TSL2. The
first one is different from TS in that, firstly, related
words for n are looked for not in the entire training
set, but within the class of n; secondly, the weight
for a feature is estimated from the distribution of
the feature across pairs of members of only that
class:
c  ,with ),'|(),(1 ???= n'nnfnfPcfTSL       (6)
Thus, by weighting features using TSL1 we aim
to increase similarity between members of a class
and disregard possible similarities across classes.
Both TS and TSL1 require computation of
similarities between a large set of words and thus
incur significant computational costs. We therefore
tried another, much more efficient method to
identify features characteristic of a class, called
TSL2. As TSL1, it looks at how many members of
a class share a feature. But instead of computing a
set of nearest neighbors for each member, it
simply uses all the words in the class as the set
of related words. TSL2 is the proportion of
instances which possess feature f to the total
number of instances in c :
|}{|
|}|{|),(2 cn
nfcncfTSL
?
??=                         (7)
Table 1 illustrates the 10 highest scored features
according to five supervised functions for the class
{ambulance, car, bike, coupe, jeep, motorbike,
taxi, truck} (estimated from the BNC co-
occurrence data described in Section 4).
MI GR OR TSL1 TSL2
see_into
die_after
drive_into
remand_to
run_from
privatise
release_into
switch_to
make_about
entrust_to
knock_by
climb _of
die_after
drive_into
remand_to
privatise
make_about
force_of
plan_with
recover_in
die_after
drive_into
remand_to
privatise
make_about
force_of
plan_with
recover_in
start_up
explode_in
see
drive
take
get
get_into
hear
need
call
send
go_by
see
drive
get
take
get_into
park
hear
wait_for
need
call
Table 1. 10 highest scored features for class
{ambulance, car, bike, coupe, jeep, motorbike,
taxi, truck} according to MI, GR, OR, TSL1, TSL2
The examples vividly demonstrate the basic
differences between the functions emphasizing
discriminative features vs. those emphasizing
characteristic features. The former attribute
greatest weights to very rare context words, some
of which seem rather informative (knock_by,
climb_of, see_into), some also appear to be
occasional collocates (remand_to, recover_in ) or
parsing mistakes (entrust_to, force_of). In contrast,
the latter encourage frequent context words.
Among them are those that are intuitively useful
(drive, park, get_into), but also those that are too
abstract (see, get, take). The inspection of the weights
suggests that both feature scoring strategies are able
to identify different potentially useful features, but at
the same time often attribute great relevance to quite
non-informative features. We next describe an
empirical evaluation of these functions.
4 Experimental Settings
4.1 Data
The evaluation was carried out on the task of
classifying English nouns into predefined semantic
classes. The meaning of each noun n?N was
represented by a vector where features are verbs
v?V with which the nouns are used as either direct
or prepositional objects. The values of the features
were conditional probabilities p(v|n). Two different
datasets were used in the experiments: verb-noun
co-occurrence pairs extracted from the British
National Corpus (BNC)3 and from the Associated
Press 1988 corpus (AP)4. Rare nouns were filtered:
the BNC data contained nouns that appeared with
at least 5 different verbs and the AP data contained
1000 most frequent nouns, each of which appeared
                                                
3 http://www.wlv.ac.uk/~in8113/data/bnc.tar.gz
4 http://www.cs.cornell.edu/home/llee/data/sim.html
with at least 19 different verbs. Co-occurrences
that appeared only once were removed.
To provide the extracted nouns with class labels
needed for training and evaluation, the nouns were
arranged into classes using WordNet in the
following manner. Each class was made up of
those nouns whose most frequent senses are
hyponyms to a node seven edges below the root
level of WordNet. Only those classes were used in
the study that had 5 or more members. Thus, from
the BNC data we formed 60 classes with 514 nouns
and from the AP data 42 classes with 375 nouns.
4.2 Classification algorithms
Two classification algorithms were used in the
study: k  nearest neighbors (kNN) and Na?ve Bayes,
which were previously shown to be quite robust on
highly dimensional representations on tasks
including word classification (e.g., Ciaramita 2002).
The kNN algorithm classifies a test instance by
first identifying its k  nearest neighbors among the
training instances according to some similarity
measure and then assigning it to the class that has
the majority in the set of nearest neighbors. We
used the weighted kNN algorithm: the vote of each
neighbor was weighted by the score of its
similarity to the test instance.
As is well known, kNN?s performance is highly
sensitive to the choice of the similarity metric.
Therefore, we experimented with several similarity
metrics and found that on both datasets Jensen-
Shannon Divergence yields the best classification
results (see Table 1). Incidentally, this is in
accordance with a study by (Dagan et al, 1997)
who found that it consistently performed better
than a number of other popular functions.
Similarity function BNC AP
Jensen-Shannon 41.67 41.33
L1 distance 38.15 39.72
Jaccard 36.82 37.01
Cosine 36.80 34.95
Skew Divergence 35.82 37.34
L2 distance 24.15 26.62
Table 2. Comparison of similarity functions for
the kNN algorithm.
Jensen-Shannon Divergence measures the
(dis)similarity between a train instance n and test
instance m as:
)]||()||([
2
1),( ,, mnmn avgmDavgnDmnJ +=   (8)
where D is the Kullback Leibler divergence
between two probability distributions x and y:
? ?= Vv yvp
xvpxvpyxD
)|(
)|(log)|()||(                (9)
and avgn,m is the average of the distributions of n
and m.
In testing each weighting method, we
experimented with k = 1, 3, 5, 7, 10, 15, 20, 30, 50,
70, and 100 in order to take into account the fact
that feature weighting typically changes the
optimal value of k . The results for kNN reported
below indicate the highest effectiveness measures
obtained among all k  in a particular test.
The Na?ve Bayes algorithm classifies a test
instance m by finding a class c that maximizes
p(c|Vm?m). Assuming independence between
features, the goal of the algorithm can be stated as:
)|()(maxarg)|(maxarg i
Vv
iimii cvpcpVcp
m
?
?
? (10)
where p(ci) and p(v|ci) are estimated during the
training process from the corpus data.
The Na?ve Bayes classifier adopted in the study
was the binary independence model, which
estimates p(v|ci) assuming the binomial distribution
of features across classes. In order to introduce the
information inherent in the frequencies of features
into the model all input probabilities were
calculated from the real values of features, as
suggested in (Lewis, 1998).
4.3 Evaluation method
To evaluate the quality of classifications, we
adopted the ten-fold cross-validation technique. The
same 10 test-train splits were used in all experiments.
Since we found that the difficulty of particular test
sets can vary quite a lot, using the same test-train
splits allowed for estimation of the statistical
significance of differences between the results of
particular methods (one-tailed paired t-test was used
for this purpose). Effectiveness was first measured
in terms of precision and recall, which were then
used to compute the Fb score5. The reported
evaluation measure is microaveraged F scores.
As a baseline, we used the k-nn and the Na?ve
Bayes classifiers trained and tested on non-
weighted instances.
5 Results
5.1 Term Strength
We first describe experiments on finding the
most optimal parameter settings for Term Strength.
As was mentioned in Section 3.4, an important
parameter of term strength that needs to be tuned for
                                                
5 b was set to 1.
a task is the similarity threshold which is used to
judge a pair of words to be semantically related.
Since in both datasets the minimum number of
words in a class was 5, we chose 4 to be the number
of words that can be related to any given word.
Finding the four nearest neighbors for each word in
the collection, we calculated the average minimum
similarity score that a pair of words must have in
order to be considered related. However, since words
vary a lot in terms of the amount of corpus data
available on them, the average similarity threshold
might be inappropriate for many words. Therefore
we tried also another way to select pairs of related
words by simply taking the four most similar words
for each particular word. Table 4 compares the two
methods of locating related words (significant
improvements at a=0.05 are shown in bold).
kNN Na?ve Bayes
TS TSL1 TS TSL1
BNC
Threshold 39.54 36.84 41.67 37.97
Top 4 words 40.90 39.74 41.86 42.64
AP
Threshold 42.12 40.22 37.80 33.82
Top 4 words 42.12 44.45 38.07 36.47
Table 4. Two methods of identifying semantically
related words for TS and TSL1.
We see that using a similarity threshold indeed
produces worse results, significantly so for TSL1. In
the rest of the experiments we used a fixed number
of related words in calculating TS and TSL1.
5.2 Globalization methods
Before comparing global and local variants of
the functions, we studied three ways to derive a
global weight for a feature: (1) using the maximum
local relevance score of a feature across all classes,
(2) its weighted average score (the contribution of
each class-specific score is weighted by the size of
the class), and (3) the sum of all local scores. The
results are shown on Tables 5 and 6 (in bold are
the figures that are significantly different from the
second-best achievement at a=0.05).
MI GR OR TSL1 TSL2
BNC
max 42.83 48.88 46.35 37.9 41.52
wavg 41.29 45.95 42.65 26.47 27.24
sum 41.29 45.95 42.65 26.46 27.24
AP
max 43.17 43.44 44.77 32.48 35.95
wavg 42.93 43.98 41. 37.31 38.12
sum 43.20 43.99 41.61 37.04 37.85
Table 5. Globalization methods on kNN.
MI GR OR TSL1 TSL2
BNC
max 46.52 42.82 43.96 37.17 38.53
wavg 43.4 41.45 43.98 21. 24.5
sum 40.48 43.59 45.15 18.66 22.96
AP
max 39.68 38.6 42.07 38.1 38.64
wavg 39.15 42.1 40.23 33.82 35.15
sum 39.68 42.38 40.76 34.1 35.96
Table 6. Globalization methods on Na?ve Bayes.
As one can see, using a maximum local weight is
usually the best method of globalization. Its
performance is often significantly higher than that
of the other methods. The explanation for this can
be the fact that a feature often has very high scores
relative to specific classes, while in the rest of the
classes its weight is low. Using its weighted
average score or a sum of local scores results in
obscuring its high relevance to some classes. In
contrast, the maximum local score does reflect
high relevance of the feature to these classes. If, in
addition to that, the feature appears in very few
classes, it is unlikely that its being weighted too
highly interferes with the representations of
irrelevant classes. This is confirmed by the fact
that the maximum weight is noticeably better on
the BNC dataset, which contains much more rare
features than the AP one.
5.3 Global vs. Local Weighting
In carrying out either local or global weighting,
there is a choice either to weight only training
instances or also test instances before their
classification. The test instance can be weighted
either by the global weights or by the local weights
of the class it is compared with. Tables 7 and 8
present the results of the evaluation of the
functions along two dimensions: (1) local versus
global weighting and (2) weighted versus un-
weighted test instances. As before, the results for
those methods whose superiority over other ones is
statistically significant appear in bold.
MI GR OR TS TSL1 TSL2
BNC
gl y 42.83 48.88 46.35 34.72 32.48 35.95
loc y 28.43 35.84 34.29 - 15.38 20.75
gl n 40.12 39.74 38.36 40.90 38.35 36.99
loc n 40.32 40.33 39.72 - 39.74 41.29
AP
gl y 43.17 43.44 44.77 38.12 37.9 41.52
loc y 37.31 31.74 37.04 - 33.06 36.68
gl n 41.59 40.78 40.51 42.12 39.25 40.24
loc n 40.74 37.86 41.34 - 44.45 43.70
Table 7. Local vs. global weighting schemas
on kNN.
MI GR OR TS TSL1 TSL2
BNC
gl y 46.52 42.82 43.96 33.87 37.17 38.53
loc y 41.84 43.79 38.32 - 36.01 39.53
gl n 45.54 42.63 40.87 41.86 41.65 45.54
loc n 43.99 38.93 44.38 - 42.64 46.53
AP
gl y 39.68 38.60 42.07 36.22 38.10 38.64
loc y 36.50 31.72 37.04 - 33.56 35.66
gl n 39.16 35.44 38.65 38.07 39.43 39.95
loc n 38.89 27.96 38.15 - 36.47 39.42
Table 8. Local vs. global weighting schemas on
Na?ve Bayes.
The results are largely consistent both across the
datasets and across the classification methods.
Discriminative functions are almost always best in
their global variants; when applying them globally,
it is also advisable to weight test instances. In
contrast, the characteristic functions TSL1 and
TSL2 are usually better when applied locally. It is
also noteworthy that all the variants of TS fare
better when test instances are not weighted.
We believe that the good performance of the
global versions of MI, GR, and OR should be
explained by the fact that features they weight
highest are rare and likely to appear only in one
class so that using the same weight for all classes
does not cause confusion between them. It is also
beneficial to weight test instances globally,
because this guarantees that most features of a test
instance always have a non-zero weight. With
characteristic functions, however, highest weighted
are rather frequent features which are often present
in other classes as well. Using the same weight of
these features for all classes therefore fails to
differentiate classes from each other. Local TSL1
and TSL2 are more advantageous. Although
individual features they weight highest may be
mediocre separators, usually several such features
are given prominence within a class. Taken
collectively they appear to be able to successfully
discriminate a class from other classes.
An interesting observation is that the
combination of a local schema with weighted test
instances is very undesirable with all the functions.
The reason for this is that very often a test instance
has many features different from those in the
training class to which it is being compared.
Because of this, these features receive zero local
weights, which renders the representation of the
test instance extremely sparse.
Table 9 shows how the performance of the most
optimal settings for the six studied function
compares with the baseline (improvements on the
baseline are in bold).
kNN Na?ve Bayes
BNC AP BNC AP
MI 42.83 43.17 46.52 39.68
GR 48.88 43.44 43.79 38.60
OR 46.35 44.77 43.96 42.07
TS 40.90 42.12 41.86 38.07
TSL1 39.74 44.45 42.64 39.43
TSL2 41.29 43.70 46.53 39.95
baseline 41.67 41.33 45.55 39.16
Table 9. The most optimal settings for MI, GR,
OR, TS, TSL1 and TSL2 compared to the baselines.
All the functions often show superiority over the
baseline, except for TS which only once slightly
outperformed it. However, statistical significance
of the improvement was registered only for MI and
OR on the BNC data, using the kNN classifier,
which was 17% and 11% better than the baseline
correspondingly.
Comparing discriminative and characteristic
weighting functions we see that the supervised
variants of TS frequently perform on a par with
MI, GR, and OR. Particularly, TSL2 was the best
performer on Naive Bayes, BNC and the second
best on kNN, AP. We also see that the supervised
variants of TS very often surpass its original
unsupervised variant, but the improvement is
significant only for TSL2, on the BNC dataset
using Naive Bayes (at a=0.001).
5.4 Correlations between the functions
As was mentioned before, an informal inspection
of features emphasized by different functions
suggests that the discriminative functions tend to
give greater weights to rare features, while
characteristic ones to frequent features. In order to
see if this results in disagreement between them as
to the classifications of test instances, we measured
the extent to which classifications resulting from
MI, GR, OR, TSL1, and TSL2 overlap. For this
purpose, we calculated the Kappa coefficient for
all the 10 possible pairs of these functions. The
results are reported in Table 10.
GR OR TSL1 TSL2
MI 0.676
0.762
0.711
0.729
0.584
0.668
0.801
0.788
GR 0.873
0.855
0.483
0.617
0.571
0.703
OR 0.473
0.614
0.588
0.695
TSL1 0.658
0.721
Table 10. The agreement in classifications using
Na?ve Bayes between MI, GR, OR, TSL1, and
TSL2 on the BNC and AP datasets.
On results from both datasets, we see that the
highest agreement is indeed between MI, OR, and
GR and between TSL1 and TSL2. Interestingly,
there is also a relatively strong correlation between
classification resulting from using MI and TSL2.
The lowest agreement is between the
discriminative functions and TSL1.
kNN Na?ve Bayes
BNC AP BNC AP
MI 42.83 43.17 46.52 39.68
GR 48.88 43.44 43.79 38.60
OR 46.35 44.77 43.96 42.07
TSL1 39.74 44.45 42.64 39.43
TSL2 41.29 43.70 46.53 39.95
MI*TSL1 40.51 45.27 44.2 38.1
GR*TSL1 42.47 43.2 43.6 34.37
OR*TSL1 41.49 44.72 46.32 37.3
MI*TSL2 44.81 45.04 46.73 42.36
GR*TSL2 47.53 44.77 44.17 37.55
OR*TSL2 46.35 45.29 46.5 40.47
Table 11. Combinations of TSL1 and TSL2 with
MI, GR, and OR.
5.5 Combination of the functions
An obvious question is whether the effectiveness
of classifications can be increased by combining
the discriminative and the characteristic weights of
a feature given that both provide useful, but
different kinds of evidence about the correct class
label of test instances. To investigate this, we tried
combining each of the discriminative weights of a
feature with each of its supervised characteristic
weights in the following manner. First, both kinds
of weights were estimated from non-weighted
training data. Then they were applied one after the
other to the training data. During the test procedure,
test instances were weighted only with the global
weights. The results of these experiments are
shown in Table 11. Results for those combined
weighting methods which outperformed both of the
component functions are shown in bold.
Certain combined weighting procedures did
improve on both of the component methods.
However, none of them showed an improvement
over 2.5% on the best of the component weighting
methods (no significance for any of the
improvements could be established).
6 Conclusion
In the paper we studied several feature weighting
methods in application to automatic word
classification. Our particular focus was on the
differences between those weighting methods
which encourage features discriminating classes
from each other (odds ratio, gain ratio, mutual
information) and those which favor features that
best characterize classes (term strength).
We find that classification of words into flatly
organized classes is a very challenging task with
quite low upper and lower bounds, which suggests
that a considerable improvement on the baseline is
hard to achieve. We explicitly explored
parameterization of the weighting functions,
finding that the choice of certain parameters,
notably the application of local vs. global weights
and weighted vs. un-weighted test instances, is
critical for the performance of the classifier. We
find that the most optimal weighting procedure
often brings the performance of a classifier
significantly closer to the upper bound, achieving
up to 17% improvement on the baseline.
We find that discriminative and characteristic
weighting procedures are able to identify different
kinds of features useful for learning a classifier,
both consistently enhancing the classification
accuracy. These findings indicate that although
individual characteristic features may be less
powerful class separators, several such features,
taken collectively, are helpful in differentiating
between classes.
References
E. Alfonseca and S. Manandhar. 2002. Extending a
lexical ontology by a combination of
distributional semantics signatures. In
Proceedings of EKAW?02, pp.1-7.
M. Ciaramita. 2002. Boosting automatic lexical
acquisition with morphological information. In
Proceedings of the ACL-02 Workshop on
Unsupervised Lexical Acquisition. pp.17-25.
I. Dagan, L. Lee, and F. C. N. Pereira. 1997.
Similarity-based methods for word sense dis-
ambiguation. In Proceedings of ACL?97, pp. 56-63.
D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information re-
trieval. In Proceedings of ECML?98, pp.4-15.
D. Lin (1998) Automatic retrieval and clustering of
similar words. In Proceedings of COLING-
ACL?98, pp. 768-773.
D. Mladenic. 1998. Feature subset selection in text
learning. In Proceedings of ECML?98, pp.95-100.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
J.W. Wilbur and K. Sirotkin. 1992. The automatic
identification of stopwords. Journal of
Information Science, (18):45-55.
Y. Yang and J.O. Pedersen. 1997. A comparative
study on feature selection in text categorization.
Proceedings of ICML?97, pp. 412-420.
Knowledge Portals
Dr. Steffen Staab
Senior Researcher and Lecturer
Knowledge Management Group
Applied Computer Science Institute (AIFB)
University of Karlsruhe, Germany
sst@aifb.unikarlsruhe.de
www.aifb.unikarlsruhe.de/ sst
Knowledge portals provide views onto domainspecific information on the World
Wide Web, thus facilitating their users to find relevant, domainspecific informa-
tion. The construction of intelligent access and the provisioning of information to
knowledge portals, however, remained an ad hoc task requiring extensive manual
editing and maintenance by the knowledge portal providers. In order to diminish
these efforts we use ontologies as a conceptual backbone for providing, accessing
and structuring information in a comprehensive approach for building and main-
taining knowledge portals. We have built several experimental and one commercial
knowledge portal for knowledge management tasks such as skill management and
corporate history analysis that show how our approach is used in practice. This
practice, however, has exhibited a number bottlenecks, many of which could be
avoided or at least diminished by Human Language Technology. We have used
HLT in order to reduce the costs of ontology engineering and in order to narrow
the gap between finding knowledge in texts and providing it to the portal.
147
148
149
150
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1145?1154,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Generalized Language Model as the Combination of Skipped n-grams
and Modified Kneser-Ney Smoothing
Rene Pickhardt, Thomas Gottron, Martin K
?
orner, Steffen Staab
Institute for Web Science and Technologies,
University of Koblenz-Landau, Germany
{rpickhardt,gottron,mkoerner,staab}@uni-koblenz.de
Paul Georg Wagner and Till Speicher
Typology GbR
mail@typology.de
Abstract
We introduce a novel approach for build-
ing language models based on a system-
atic, recursive exploration of skip n-gram
models which are interpolated using modi-
fied Kneser-Ney smoothing. Our approach
generalizes language models as it contains
the classical interpolation with lower or-
der models as a special case. In this pa-
per we motivate, formalize and present
our approach. In an extensive empirical
experiment over English text corpora we
demonstrate that our generalized language
models lead to a substantial reduction of
perplexity between 3.1% and 12.7% in
comparison to traditional language mod-
els using modified Kneser-Ney smoothing.
Furthermore, we investigate the behaviour
over three other languages and a domain
specific corpus where we observed consis-
tent improvements. Finally, we also show
that the strength of our approach lies in
its ability to cope in particular with sparse
training data. Using a very small train-
ing data set of only 736 KB text we yield
improvements of even 25.7% reduction of
perplexity.
1 Introduction motivation
Language Models are a probabilistic approach for
predicting the occurrence of a sequence of words.
They are used in many applications, e.g. word
prediction (Bickel et al, 2005), speech recogni-
tion (Rabiner and Juang, 1993), machine trans-
lation (Brown et al, 1990), or spelling correc-
tion (Mays et al, 1991). The task language models
attempt to solve is the estimation of a probability
of a given sequence of words w
l
1
= w
1
, . . . , w
l
.
The probability P (w
l
1
) of this sequence can be
broken down into a product of conditional prob-
abilities:
P (w
l
1
) =P (w
1
) ? P (w
2
|w
1
) ? . . . ? P (w
l
|w
1
? ? ?w
l?1
)
=
l
?
i=1
P (w
i
|w
1
? ? ?w
i?1
) (1)
Because of combinatorial explosion and data
sparsity, it is very difficult to reliably estimate the
probabilities that are conditioned on a longer sub-
sequence. Therefore, by making a Markov as-
sumption the true probability of a word sequence
is only approximated by restricting conditional
probabilities to depend only on a local context
w
i?1
i?n+1
of n ? 1 preceding words rather than the
full sequencew
i?1
1
. The challenge in the construc-
tion of language models is to provide reliable esti-
mators for the conditional probabilities. While the
estimators can be learnt?using, e.g., a maximum
likelihood estimator over n-grams obtained from
training data?the obtained values are not very re-
liable for events which may have been observed
only a few times or not at all in the training data.
Smoothing is a standard technique to over-
come this data sparsity problem. Various smooth-
ing approaches have been developed and ap-
plied in the context of language models. Chen
and Goodman (Chen and Goodman, 1999) in-
troduced modified Kneser-Ney Smoothing, which
up to now has been considered the state-of-the-
art method for language modelling over the last
15 years. Modified Kneser-Ney Smoothing is
an interpolating method which combines the es-
timated conditional probabilities P (w
i
|w
i?1
i?n+1
)
recursively with lower order models involving a
shorter local contextw
i?1
i?n+2
and their estimate for
P (w
i
|w
i?1
i?n+2
). The motivation for using lower
order models is that shorter contexts may be ob-
served more often and, thus, suffer less from data
sparsity. However, a single rare word towards the
end of the local context will always cause the con-
text to be observed rarely in the training data and
hence will lead to an unreliable estimation.
1145
Because of Zipfian word distributions, most
words occur very rarely and hence their true prob-
ability of occurrence may be estimated only very
poorly. One word that appears at the end of a local
context w
i?1
i?n+1
and for which only a poor approx-
imation exists may adversely affect the conditional
probabilities in language models of all lengths ?
leading to severe errors even for smoothed lan-
guage models. Thus, the idea motivating our ap-
proach is to involve several lower order models
which systematically leave out one position in the
context (one may think of replacing the affected
word in the context with a wildcard) instead of
shortening the sequence only by one word at the
beginning.
This concept of introducing gaps in n-grams
is referred to as skip n-grams (Ney et al, 1994;
Huang et al, 1993). Among other techniques, skip
n-grams have also been considered as an approach
to overcome problems of data sparsity (Goodman,
2001). However, to best of our knowledge, lan-
guage models making use of skip n-grams mod-
els have never been investigated to their full ex-
tent and over different levels of lower order mod-
els. Our approach differs as we consider all pos-
sible combinations of gaps in a local context and
interpolate the higher order model with all possi-
ble lower order models derived from adding gaps
in all different ways.
In this paper we make the following contribu-
tions:
1. We provide a framework for using modified
Kneser-Ney smoothing in combination with a
systematic exploration of lower order models
based on skip n-grams.
2. We show how our novel approach can indeed
easily be interpreted as a generalized version
of the current state-of-the-art language mod-
els.
3. We present a large scale empirical analysis
of our generalized language models on eight
data sets spanning four different languages,
namely, a wikipedia-based text corpus and
the JRC-Acquis corpus of legislative texts.
4. We empirically observe that introducing skip
n-gram models may reduce perplexity by
12.7% compared to the current state-of-the-
art using modified Kneser-Ney models on
large data sets. Using small training data sets
we observe even higher reductions of per-
plexity of up to 25.6%.
The rest of the paper is organized as follows.
We start with reviewing related work in Section 2.
We will then introduce our generalized language
models in Section 3. After explaining the evalua-
tion methodology and introducing the data sets in
Section 4 we will present the results of our evalu-
ation in Section 5. In Section 6 we discuss why a
generalized language model performs better than
a standard language model. Finally, in Section 7
we summarize our findings and conclude with an
overview of further interesting research challenges
in the field of generalized language models.
2 Related Work
Work related to our generalized language model
approach can be divided in two categories: var-
ious smoothing techniques for language models
and approaches making use of skip n-grams.
Smoothing techniques for language models
have a long history. Their aim is to overcome data
sparsity and provide more reliable estimators?in
particular for rare events. The Good Turing es-
timator (Good, 1953), deleted interpolation (Je-
linek and Mercer, 1980), Katz backoff (Katz,
1987) and Kneser-Ney smoothing (Kneser and
Ney, 1995) are just some of the approaches to
be mentioned. Common strategies of these ap-
proaches are to either backoff to lower order mod-
els when a higher order model lacks sufficient
training data for good estimation, to interpolate
between higher and lower order models or to inter-
polate with a prior distribution. Furthermore, the
estimation of the amount of unseen events from
rare events aims to find the right weights for in-
terpolation as well as for discounting probability
mass from unreliable estimators and to retain it for
unseen events.
The state of the art is a modified version of
Kneser-Ney smoothing introduced in (Chen and
Goodman, 1999). The modified version imple-
ments a recursive interpolation with lower order
models, making use of different discount values
for more or less frequently observed events. This
variation has been compared to other smooth-
ing techniques on various corpora and has shown
to outperform competing approaches. We will
review modified Kneser-Ney smoothing in Sec-
tion 2.1 in more detail as we reuse some ideas to
define our generalized language model.
1146
Smoothing techniques which do not rely on us-
ing lower order models involve clustering (Brown
et al, 1992; Ney et al, 1994), i.e. grouping to-
gether similar words to form classes of words, as
well as skip n-grams (Ney et al, 1994; Huang et
al., 1993). Yet other approaches make use of per-
mutations of the word order in n-grams (Schukat-
Talamazzini et al, 1995; Goodman, 2001).
Skip n-grams are typically used to incorporate
long distance relations between words. Introduc-
ing the possibility of gaps between the words in
an n-gram allows for capturing word relations be-
yond the level of n consecutive words without an
exponential increase in the parameter space. How-
ever, with their restriction on a subsequence of
words, skip n-grams are also used as a technique
to overcome data sparsity (Goodman, 2001). In re-
lated work different terminology and different def-
initions have been used to describe skip n-grams.
Variations modify the number of words which can
be skipped between elements in an n-gram as well
as the manner in which the skipped words are de-
termined (e.g. fixed patterns (Goodman, 2001) or
functional words (Gao and Suzuki, 2005)).
The impact of various extensions and smooth-
ing techniques for language models is investigated
in (Goodman, 2001; Goodman, 2000). In partic-
ular, the authors compared Kneser-Ney smooth-
ing, Katz backoff smoothing, caching, clustering,
inclusion of higher order n-grams, sentence mix-
ture and skip n-grams. They also evaluated com-
binations of techniques, for instance, using skip
n-gram models in combination with Kneser-Ney
smoothing. The experiments in this case followed
two paths: (1) interpolating a 5-gram model with
lower order distribution introducing a single gap
and (2) interpolating higher order models with
skip n-grams which retained only combinations of
two words. Goodman reported on small data sets
and in the best case a moderate improvement of
cross entropy in the range of 0.02 to 0.04.
In (Guthrie et al, 2006), the authors investi-
gated the increase of observed word combinations
when including skips in n-grams. The conclusion
was that using skip n-grams is often more effective
for increasing the number of observations than in-
creasing the corpus size. This observation aligns
well with our experiments.
2.1 Review of Modified Kneser-Ney
Smoothing
We briefly recall modified Kneser-Ney Smoothing
as presented in (Chen and Goodman, 1999). Mod-
ified Kneser-Ney implements smoothing by inter-
polating between higher and lower order n-gram
language models. The highest order distribution
is interpolated with lower order distribution as fol-
lows:
P
MKN
(w
i
|w
i?1
i?n+1
) =
max{c(w
i
i?n+1
) ? D(c(w
i
i?n+1
)), 0}
c(w
i?1
i?n+1
)
+ ?
high
(w
i?1
i?n+1
)
?
P
MKN
(w
i
|w
i?1
i?n+2
) (2)
where c(w
i
i?n+1
) provides the frequency count
that sequence w
i
i?n+1
occurs in training data, D is
a discount value (which depends on the frequency
of the sequence) and ?
high
depends onD and is the
interpolation factor to mix in the lower order dis-
tribution
1
. Essentially, interpolation with a lower
order model corresponds to leaving out the first
word in the considered sequence. The lower order
models are computed differently using the notion
of continuation counts rather than absolute counts:
?
P
MKN
(w
i
|(w
i?1
i?n+1
)) =
max{N
1+
(?w
i
i?n+1
) ? D(c(w
i
i?n+1
)), 0}
N
1+
(?w
i?1
i?n+1
?)
+ ?
mid
(w
i?1
i?n+1
)
?
P
MKN
(w
i
|w
i?1
i?n+2
)) (3)
where the continuation counts are defined as
N
1+
(?w
i
i?n+1
) = |{w
i?n
: c(w
i
i?n
) > 0}|, i.e.
the number of different words which precede the
sequencew
i
i?n+1
. The term ?
mid
is again an inter-
polation factor which depends on the discounted
probability mass D in the first term of the for-
mula.
3 Generalized Language Models
3.1 Notation for Skip n-gram with k Skips
We express skip n-grams using an operator no-
tation. The operator ?
i
applied to an n-gram
removes the word at the i-th position. For in-
stance: ?
3
w
1
w
2
w
3
w
4
= w
1
w
2
w
4
, where is
used as wildcard placeholder to indicate a re-
moved word. The wildcard operator allows for
1
The factors ? and D are quite technical and lengthy. As
they do not play a significant role for understanding our novel
approach we refer to Appendix A for details.
1147
larger number of matches. For instance, when
c(w
1
w
2
w
3a
w
4
) = x and c(w
1
w
2
w
3b
w
4
) = y then
c(w
1
w
2
w4) ? x + y since at least the two se-
quences w
1
w
2
w
3a
w
4
and w
1
w
2
w
3b
w
4
match the
sequence w
1
w
2
w
4
. In order to align with stan-
dard language models the skip operator applied to
the first word of a sequence will remove the word
instead of introducing a wildcard. In particular the
equation ?
1
w
i
i?n+1
= w
i
i?n+2
holds where the
right hand side is the subsequence ofw
i
i?n+1
omit-
ting the first word. We can thus formulate the in-
terpolation step of modified Kneser-Ney smooth-
ing using our notation as
?
P
MKN
(w
i
|w
i?1
i?n+2
) =
?
P
MKN
(w
i
|?
1
w
i?1
i?n+1
).
Thus, our skip n-grams correspond to n-grams
of which we only use k words, after having applied
the skip operators ?
i
1
. . . ?
i
n?k
3.2 Generalized Language Model
Interpolation with lower order models is motivated
by the problem of data sparsity in higher order
models. However, lower order models omit only
the first word in the local context, which might not
necessarily be the cause for the overall n-gram to
be rare. This is the motivation for our general-
ized language models to not only interpolate with
one lower order model, where the first word in a
sequence is omitted, but also with all other skip n-
gram models, where one word is left out. Combin-
ing this idea with modified Kneser-Ney smoothing
leads to a formula similar to (2).
P
GLM
(w
i
|w
i?1
i?n+1
) =
max{c(w
i
i?n+1
) ? D(c(w
i
i?n+1
)), 0}
c(w
i?1
i?n+1
)
+ ?
high
(w
i?1
i?n+1
)
n?1
?
j=1
1
n?1
?
P
GLM
(w
i
|?
j
w
i?1
i?n+1
)
(4)
The difference between formula (2) and formula
(4) is the way in which lower order models are
interpolated.
Note, the sum over all possible positions in
the context w
i?1
i?n+1
for which we can skip a
word and the according lower order models
P
GLM
(w
i
|?
j
(w
i?1
i?n+1
)). We give all lower order
models the same weight
1
n?1
.
The same principle is recursively applied in the
lower order models in which some words of the
full n-gram are already skipped. As in modi-
fied Kneser-Ney smoothing we use continuation
counts for the lower order models, incorporating
the skip operator also for these counts. Incor-
porating this directly into modified Kneser-Ney
smoothing leads in the second highest model to:
?
P
GLM
(w
i
|?
j
(w
i?1
i?n+1
)) = (5)
max{N
1+
(?
j
(w
i
i?n
)) ? D(c(?
j
(w
i
i?n+1
))), 0}
N
1+
(?
j
(w
i?1
i?n+1
)?)
+?
mid
(?
j
(w
i?1
i?n+1
))
n?1
?
k=1
k 6=j
1
n?2
?
P
GLM
(w
i
|?
j
?
k
(w
i?1
i?n+1
))
Given that we skip words at different positions,
we have to extend the notion of the count function
and the continuation counts. The count function
applied to a skip n-gram is given by c(?
j
(w
i
i?n
))=
?
w
j
c(w
i
i?n
), i.e. we aggregate the count informa-
tion over all words which fill the gap in the n-
gram. Regarding the continuation counts we de-
fine:
N
1+
(?
j
(w
i
i?n
)) = |{w
i?n+j?1
:c(w
i
i?n
)>0}| (6)
N
1+
(?
j
(w
i?1
i?n
)?) = |{(w
i?n+j?1
, w
i
) :c(w
i
i?n
)>0}| (7)
As lowest order model we use?just as done for
traditional modified Kneser-Ney (Chen and Good-
man, 1999)?a unigram model interpolated with a
uniform distribution for unseen words.
The overall process is depicted in Figure 1, il-
lustrating how the higher level models are recur-
sively smoothed with several lower order ones.
4 Experimental Setup and Data Sets
To evaluate the quality of our generalized lan-
guage models we empirically compare their abil-
ity to explain sequences of words. To this end we
use text corpora, split them into test and training
data, build language models as well as generalized
language models over the training data and apply
them on the test data. We employ established met-
rics, such as cross entropy and perplexity. In the
following we explain the details of our experimen-
tal setup.
4.1 Data Sets
For evaluation purposes we employed eight differ-
ent data sets. The data sets cover different domains
and languages. As languages we considered En-
glish (en), German (de), French (fr), and Italian
(it). As general domain data set we used the full
collection of articles from Wikipedia (wiki) in the
corresponding languages. The download dates of
the dumps are displayed in Table 1.
1148
Figure 1: Interpolation of models of different or-
der and using skip patterns. The value of n in-
dicates the length of the raw n-grams necessary
for computing the model, the value of k indicates
the number of words actually used in the model.
The wild card symbol marks skipped words in
an n-gram. The arrows indicate how a higher or-
der model is interpolated with lower order mod-
els which skips one word. The bold arrows cor-
respond to interpolation of models in traditional
modified Kneser-Ney smoothing. The lighter ar-
rows illustrate the additional interpolations intro-
duced by our generalized language models.
de en fr it
Nov 22
nd
Nov 04
th
Nov 20
th
Nov 25
th
Table 1: Download dates of Wikipedia snapshots
in November 2013.
Special purpose domain data are provided by
the multi-lingual JRC-Acquis corpus of legislative
texts (JRC) (Steinberger et al, 2006). Table 2
gives an overview of the data sets and provides
some simple statistics of the covered languages
and the size of the collections.
Statistics
Corpus total words unique words
in Mio. in Mio.
wiki-de 579 9.82
JRC-de 30.9 0.66
wiki-en 1689 11.7
JRC-en 39.2 0.46
wiki-fr 339 4.06
JRC-fr 35.8 0.46
wiki-it 193 3.09
JRC-it 34.4 0.47
Table 2: Word statistics and size of of evaluation
corpora
The data sets come in the form of structured text
corpora which we cleaned from markup and tok-
enized to generate word sequences. We filtered the
word tokens by removing all character sequences
which did not contain any letter, digit or common
punctuation marks. Eventually, the word token se-
quences were split into word sequences of length
n which provided the basis for the training and
test sets for all algorithms. Note that we did not
perform case-folding nor did we apply stemming
algorithms to normalize the word forms. Also,
we did our evaluation using case sensitive training
and test data. Additionally, we kept all tokens for
named entities such as names of persons or places.
4.2 Evaluation Methodology
All data sets have been randomly split into a train-
ing and a test set on a sentence level. The train-
ing sets consist of 80% of the sentences, which
have been used to derive n-grams, skip n-grams
and corresponding continuation counts for values
of n between 1 and 5. Note that we have trained
a prediction model for each data set individually.
From the remaining 20% of the sequences we have
randomly sampled a separate set of 100, 000 se-
quences of 5 words each. These test sequences
have also been shortened to sequences of length 3,
and 4 and provide a basis to conduct our final ex-
periments to evaluate the performance of the dif-
ferent algorithms.
We learnt the generalized language models on
the same split of the training corpus as the stan-
dard language model using modified Kneser-Ney
smoothing and we also used the same set of test se-
quences for a direct comparison. To ensure rigour
and openness of research the data set for training
as well as the test sequences and the entire source
code is open source.
2 3 4
We compared the
probabilities of our language model implementa-
tion (which is a subset of the generalized language
model) using KN as well as MKN smoothing with
the Kyoto Language Model Toolkit
5
. Since we
got the same results for small n and small data sets
we believe that our implementation is correct.
In a second experiment we have investigated
the impact of the size of the training data set.
The wikipedia corpus consists of 1.7 bn. words.
2
http://west.uni-koblenz.de/Research
3
https://github.com/renepickhardt/generalized-language-
modeling-toolkit
4
http://glm.rene-pickhardt.de
5
http://www.phontron.com/kylm/
1149
Thus, the 80% split for training consists of 1.3 bn.
words. We have iteratively created smaller train-
ing sets by decreasing the split factor by an order
of magnitude. So we created 8% / 92% and 0.8%
/ 99.2% split, and so on. We have stopped at the
0.008%/99.992% split as the training data set in
this case consisted of less words than our 100k
test sequences which we still randomly sampled
from the test data of each split. Then we trained
a generalized language model as well as a stan-
dard language model with modified Kneser-Ney
smoothing on each of these samples of the train-
ing data. Again we have evaluated these language
models on the same random sample of 100, 000
sequences as mentioned above.
4.3 Evaluation Metrics
As evaluation metric we use perplexity: a standard
measure in the field of language models (Manning
and Sch?utze, 1999). First we calculate the cross
entropy of a trained language model given a test
set using
H(P
alg
) = ?
?
s?T
P
MLE
(s) ? log
2
P
alg
(s) (8)
Where P
alg
will be replaced by the probability
estimates provided by our generalized language
models and the estimates of a language model us-
ing modified Kneser-Ney smoothing. P
MLE
, in-
stead, is a maximum likelihood estimator of the
test sequence to occur in the test corpus. Finally,
T is the set of test sequences. The perplexity is
defined as:
Perplexity(P
alg
) = 2
H(P
alg
)
(9)
Lower perplexity values indicate better results.
5 Results
5.1 Baseline
As a baseline for our generalized language model
(GLM) we have trained standard language models
using modified Kneser-Ney Smoothing (MKN).
These models have been trained for model lengths
3 to 5. For unigram and bigram models MKN and
GLM are identical.
5.2 Evaluation Experiments
The perplexity values for all data sets and various
model orders can be seen in Table 3. In this table
we also present the relative reduction of perplexity
in comparison to the baseline.
model length
Experiments n = 3 n = 4 n = 5
wiki-de MKN 1074.1 778.5 597.1
wiki-de GLM 1031.1 709.4 521.5
rel. change 4.0% 8.9% 12.7%
JRC-de MKN 235.4 138.4 94.7
JRC-de GLM 229.4 131.8 86.0
rel. change 2.5% 4.8% 9.2%
wiki-en MKN 586.9 404 307.3
wiki-en GLM 571.6 378.1 275
rel. change 2.6% 6.1% 10.5%
JRC-en MKN 147.2 82.9 54.6
JRC-en GLM 145.3 80.6 52.5
rel. change 1.3% 2.8% 3.9%
wiki-fr MKN 538.6 385.9 298.9
wiki-fr GLM 526.7 363.8 272.9
rel. change 2.2% 5.7% 8.7%
JRC-fr MKN 155.2 92.5 63.9
JRC-fr GLM 153.5 90.1 61.7
rel. change 1.1% 2.5% 3.5%
wiki-it MKN 738.4 532.9 416.7
wiki-it GLM 718.2 500.7 382.2
rel. change 2.7% 6.0% 8.3%
JRC-it MKN 177.5 104.4 71.8
JRC-it GLM 175.1 101.8 69.6
rel. change 1.3% 2.6% 3.1%
Table 3: Absolute perplexity values and relative
reduction of perplexity from MKN to GLM on all
data sets for models of order 3 to 5
As we can see, the GLM clearly outperforms
the baseline for all model lengths and data sets.
In general we see a larger improvement in perfor-
mance for models of higher orders (n = 5). The
gain for 3-gram models, instead, is negligible. For
German texts the increase in performance is the
highest (12.7%) for a model of order 5. We also
note that GLMs seem to work better on broad do-
main text rather than special purpose text as the
reduction on the wiki corpora is constantly higher
than the reduction of perplexity on the JRC cor-
pora.
We made consistent observations in our second
experiment where we iteratively shrank the size
of the training data set. We calculated the rela-
tive reduction in perplexity from MKN to GLM
1150
for various model lengths and the different sizes
of the training data. The results for the English
Wikipedia data set are illustrated in Figure 2.
We see that the GLM performs particularly well
on small training data. As the size of the training
data set becomes smaller (even smaller than the
evaluation data), the GLM achieves a reduction of
perplexity of up to 25.7% compared to language
models with modified Kneser-Ney smoothing on
the same data set. The absolute perplexity values
for this experiment are presented in Table 4.
model length
Experiments n = 3 n = 4 n = 5
80% MKN 586.9 404 307.3
80% GLM 571.6 378.1 275
rel. change 2.6% 6.5% 10.5%
8% MKN 712.6 539.8 436.5
8% GLM 683.7 492.8 382.5
rel. change 4.1% 8.7% 12.4%
0.8% MKN 894.0 730.0 614.1
0.8% GLM 838.7 650.1 528.7
rel. change 6.2% 10.9% 13.9%
0.08% MKN 1099.5 963.8 845.2
0.08% GLM 996.6 820.7 693.4
rel. change 9.4% 14.9% 18.0%
0.008% MKN 1212.1 1120.5 1009.6
0.008% GLM 1025.6 875.5 750.3
rel. change 15.4% 21.9% 25.7%
Table 4: Absolute perplexity values and relative
reduction of perplexity from MKN to GLM on
shrunk training data sets for the EnglishWikipedia
for models of order 3 to 5
Our theory as well as the results so far suggest
that the GLM performs particularly well on sparse
training data. This conjecture has been investi-
gated in a last experiment. For each model length
we have split the test data of the largest English
Wikipedia corpus into two disjoint evaluation data
sets. The data set unseen consists of all test se-
quences which have never been observed in the
training data. The set observed consists only of
test sequences which have been observed at least
once in the training data. Again we have calcu-
lated the perplexity of each set. For reference, also
the values of the complete test data set are shown
in Table 5.
model length
Experiments n = 3 n = 4 n = 5
MKN
complete
586.9 404 307.3
GLM
complete
571.6 378.1 275
rel. change 2.6% 6.5% 10.5%
MKN
unseen
14696.8 2199.8 846.1
GLM
unseen
13058.7 1902.4 714.4
rel. change 11.2% 13.5% 15.6%
MKN
observed
220.2 88.0 43.4
GLM
observed
220.6 88.3 43.5
rel. change ?0.16% ?0.28% ?0.15%
Table 5: Absolute perplexity values and relative
reduction of perplexity from MKN to GLM for the
complete and split test file into observed and un-
seen sequences for models of order 3 to 5. The
data set is the largest English Wikipedia corpus.
As expected we see the overall perplexity values
rise for the unseen test case and decline for the ob-
served test case. More interestingly we see that the
relative reduction of perplexity of the GLM over
MKN increases from 10.5% to 15.6% on the un-
seen test case. This indicates that the superior per-
formance of the GLM on small training corpora
and for higher order models indeed comes from its
good performance properties with regard to sparse
training data. It also confirms that our motivation
to produce lower order n-grams by omitting not
only the first word of the local context but system-
atically all words has been fruitful. However, we
also see that for the observed sequences the GLM
performs slightly worse than MKN. For the ob-
served cases we find the relative change to be neg-
ligible.
6 Discussion
In our experiments we have observed an im-
provement of our generalized language models
over classical language models using Kneser-Ney
smoothing. The improvements have been ob-
served for different languages, different domains
as well as different sizes of the training data. In
the experiments we have also seen that the GLM
performs well in particular for small training data
sets and sparse data, encouraging our initial mo-
tivation. This feature of the GLM is of partic-
ular value, as data sparsity becomes a more and
more immanent problem for higher values of n.
This known fact is underlined also by the statis-
1151
0%
5%
10%
15%
20%
25%
30%
0.1 1 10 100 1000re
lati
ve
cha
nge
inp
erp
lex
ity
data set size [mio words]
Relative change of perplexity for GLM over MKN
MKN (baseline) for n=3,4, and 5n=5n=4n=3
Figure 2: Variation of the size of the training data on 100k test sequences on the English Wikipedia data
set with different model lengths for GLM.
tics shown in Table 6. The fraction of total n-
grams which appear only once in our Wikipedia
corpus increases for higher values of n. However,
for the same value of n the skip n-grams are less
rare. Our generalized language models leverage
this additional information to obtain more reliable
estimates for the probability of word sequences.
w
n
1
total unique
w
1
0.5% 64.0%
w
1
w
2
5.1% 68.2%
w
1
w
3
8.0% 79.9%
w
1
w
4
9.6% 72.1%
w
1
w
5
10.1% 72.7%
w
1
w
2
w
3
21.1% 77.5%
w
1
w
3
w
4
28.2% 80.4%
w
1
w
2
w
4
28.2% 80.7%
w
1
w
4
w
5
31.7% 81.9%
w
1
w
3
w
5
35.3% 83.0%
w
1
w
2
w
5
31.5% 82.2%
w
1
w
2
w
3
w
4
44.7% 85.4%
w
1
w
3
w
4
w
5
52.7% 87.6%
w
1
w
2
w
4
w
5
52.6% 88.0%
w
1
w
2
w
3
w
5
52.3% 87.7%
w
1
w
2
w
3
w
4
w
5
64.4% 90.7%
Table 6: Percentage of generalized n-grams which
occur only once in the English Wikipedia cor-
pus. Total means a percentage relative to the total
amount of sequences. Unique means a percentage
relative to the amount of unique sequences of this
pattern in the data set.
Beyond the general improvements there is an
additional path for benefitting from generalized
language models. As it is possible to better lever-
age the information in smaller and sparse data sets,
we can build smaller models of competitive per-
formance. For instance, when looking at Table 4
we observe the 3-gram MKN approach on the full
training data set to achieve a perplexity of 586.9.
This model has been trained on 7 GB of text and
the resulting model has a size of 15 GB and 742
Mio. entries for the count and continuation count
values. Looking for a GLM with comparable but
better performance we see that the 5-gram model
trained on 1% of the training data has a perplexity
of 528.7. This GLM model has a size of 9.5 GB
and contains only 427 Mio. entries. So, using a far
smaller set of training data we can build a smaller
model which still demonstrates a competitive per-
formance.
7 Conclusion and Future Work
7.1 Conclusion
We have introduced a novel generalized language
model as the systematic combination of skip n-
grams and modified Kneser-Ney smoothing. The
main strength of our approach is the combination
of a simple and elegant idea with an an empiri-
cally convincing result. Mathematically one can
see that the GLM includes the standard language
model with modified Kneser-Ney smoothing as a
sub model and is consequently a real generaliza-
tion.
In an empirical evaluation, we have demon-
strated that for higher orders the GLM outper-
forms MKN for all test cases. The relative im-
provement in perplexity is up to 12.7% for large
data sets. GLMs also performs particularly well
on small and sparse sets of training data. On a very
1152
small training data set we observed a reduction of
perplexity by 25.7%. Our experiments underline
that the generalized language models overcome in
particular the weaknesses of modified Kneser-Ney
smoothing on sparse training data.
7.2 Future work
A desirable extension of our current definition of
GLMs will be the combination of different lower
lower order models in our generalized language
model using different weights for each model.
Such weights can be used to model the statistical
reliability of the different lower order models. The
value of the weights would have to be chosen ac-
cording to the probability or counts of the respec-
tive skip n-grams.
Another important step that has not been con-
sidered yet is compressing and indexing of gen-
eralized language models to improve the perfor-
mance of the computation and be able to store
them in main memory. Regarding the scalability
of the approach to very large data sets we intend to
apply the Map Reduce techniques from (Heafield
et al, 2013) to our generalized language models in
order to have a more scalable calculation.
This will open the path also to another interest-
ing experiment. Goodman (Goodman, 2001) ob-
served that increasing the length of n-grams in
combination with modified Kneser-Ney smooth-
ing did not lead to improvements for values of
n beyond 7. We believe that our generalized
language models could still benefit from such an
increase. They suffer less from the sparsity of
long n-grams and can overcome this sparsity when
interpolating with the lower order skip n-grams
while benefiting from the larger context.
Finally, it would be interesting to see how ap-
plications of language models?like next word
prediction, machine translation, speech recogni-
tion, text classification, spelling correction, e.g.?
benefit from the better performance of generalized
language models.
Acknowledgements
We would like to thank Heinrich Hartmann for
a fruitful discussion regarding notation of the
skip operator for n-grams. The research lead-
ing to these results has received funding from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013), REVEAL (Grant agree
number 610928).
References
Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Predicting sentences using n-gram language
models. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages
193?200, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18(4):467?479, Decem-
ber.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, TR-10-98, Harvard
University, August.
Stanley Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359?393.
Jianfeng Gao and Hisami Suzuki. 2005. Long dis-
tance dependency in language modeling: An em-
pirical study. In Keh-Yih Su, Junichi Tsujii, Jong-
Hyeok Lee, and OiYee Kwong, editors, Natural
Language Processing IJCNLP 2004, volume 3248
of Lecture Notes in Computer Science, pages 396?
405. Springer Berlin Heidelberg.
Irwin J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3-4):237?264.
Joshua T. Goodman. 2000. Putting it all together:
language model combination. In Acoustics, Speech,
and Signal Processing, 2000. ICASSP ?00. Proceed-
ings. 2000 IEEE International Conference on, vol-
ume 3, pages 1647?1650 vol.3.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling ? extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and York Wilks. 2006. A closer look at skip-
gram modelling. In Proceedings LREC?2006, pages
1222?1225.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
kneser-ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics.
1153
Xuedong Huang, Fileno Alleva, Hsiao-Wuen Hon,
Mei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosen-
feld. 1993. The sphinx-ii speech recognition sys-
tem: an overview. Computer Speech & Language,
7(2):137 ? 148.
F. Jelinek and R.L. Mercer. 1980. Interpolated estima-
tion of markov source parameters from sparse data.
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice, pages 381?397.
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. Acoustics, Speech and Signal Process-
ing, IEEE Transactions on, 35(3):400?401.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Christopher D. Manning and Hinrich Sch?utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517?522.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modelling. Computer Speech & Lan-
guage, 8(1):1 ? 38.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of Speech Recognition. Prentice Hall.
Ernst-G?unter Schukat-Talamazzini, R Hendrych, Ralf
Kompe, and Heinrich Niemann. 1995. Permugram
language models. In Fourth European Conference
on Speech Communication and Technology.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The jrc-acquis: A multi-
lingual aligned parallel corpus with 20+ languages.
In LREC?06: Proceedings of the 5th International
Conference on Language Resources and Evaluation.
A Discount Values and Weights in
Modified Kneser Ney
The discount valueD(c) used in formula (2) is de-
fined as (Chen and Goodman, 1999):
D(c) =
?
?
?
?
?
?
?
0 if c = 0
D
1
if c = 1
D
2
if c = 2
D
3+
if c > 2
(10)
The discounting values D
1
, D
2
, and D
3+
are de-
fined as (Chen and Goodman, 1998)
D
1
= 1 ? 2Y
n
2
n
1
(11a)
D
2
= 2 ? 3Y
n
3
n
2
(11b)
D
3+
= 3 ? 4Y
n
4
n
3
(11c)
with Y =
n
1
n
1
+n
2
and n
i
is the total number of n-
grams which appear exactly i times in the training
data. The weight ?
high
(w
i?1
i?n+1
) is defined as:
?
high
(w
i?1
i?n+1
) = (12)
D
1
N
1
(w
i?1
i?n+1
?)+D
2
N
2
(w
i?1
i?n+1
?)+D
3+
N
3+
(w
i?1
i?n+1
?)
c(w
i?1
i?n+1
)
And the weight ?
mid
(w
i?1
i?n+1
) is defined as:
?
mid
(w
i?1
i?n+1
) = (13)
D
1
N
1
(w
i?1
i?n+1
?)+D
2
N
2
(w
i?1
i?n+1
?)+D
3+
N
3+
(w
i?1
i?n+1
?)
N
1+
(?w
i?1
i?n+1
?)
where N
1
(w
i?1
i?n+1
?), N
2
(w
i?1
i?n+1
?), and
N
3+
(w
i?1
i?n+1
?) are analogously defined to
N
1+
(w
i?1
i?n+1
?).
1154
