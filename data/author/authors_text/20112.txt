Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 160?170,
Dublin, Ireland, August 23-24 2014.
Contrasting Syntagmatic and Paradigmatic Relations:
Insights from Distributional Semantic Models
Gabriella Lapesa
3,1
1
Universit?at Osnabr?uck
Institut f?ur
Kognitionswissenschaft
glapesa@uos.de
Stefan Evert
2
2
FAU Erlangen-N?urnberg
Professur f?ur
Korpuslinguistik
stefan.evert@fau.de
Sabine Schulte im Walde
3
3
Universit?at Stuttgart
Institut f?ur Maschinelle
Sprachverarbeitung
schulte@ims.uni-stuttgart.de
Abstract
This paper presents a large-scale evalua-
tion of bag-of-words distributional models
on two datasets from priming experiments
involving syntagmatic and paradigmatic
relations. We interpret the variation in
performance achieved by different settings
of the model parameters as an indication
of which aspects of distributional patterns
characterize these types of relations. Con-
trary to what has been argued in the litera-
ture (Rapp, 2002; Sahlgren, 2006) ? that
bag-of-words models based on second-
order statistics mainly capture paradig-
matic relations and that syntagmatic rela-
tions need to be gathered from first-order
models ? we show that second-order mod-
els perform well on both paradigmatic and
syntagmatic relations if their parameters
are properly tuned. In particular, our re-
sults show that size of the context window
and dimensionality reduction play a key
role in differentiating DSM performance
on paradigmatic vs. syntagmatic relations.
1 Introduction
Distributional takes on the representation and ac-
quisition of word meaning rely on the assump-
tion that words with similar meaning tend to oc-
cur in similar contexts: this assumption, known as
distributional hypothesis, has been first proposed
by Harris (1954). Distributional Semantic Mod-
els (henceforth, DSMs) are computational mod-
els that operationalize the distributional hypoth-
esis; they produce semantic representations for
words in the form of distributional vectors record-
ing patterns of co-occurrence in large samples of
language data (Sahlgren, 2006; Baroni and Lenci,
2010; Turney and Pantel, 2010). Comparison be-
tween distributional vectors allows the identifica-
tion of shared contexts as an empirical correlate of
the semantic similarity between the target words.
As noted in Sahlgren (2008), the notion of seman-
tic similarity applied in distributional approaches
to meaning is an easy target of criticism, as it is
employed to capture a wide range of semantic re-
lations, such as synonymy, antonymy, hypernymy,
up to topical relatedness.
The study presented in this paper contributes
to the debate concerning the nature of the seman-
tic representations built by DSMs, and it does so
by comparing the performance of several DSMs
in a classification task conducted on priming data
and involving paradigmatic and syntagmatic rela-
tions. Paradigmatic relations hold between words
that occur in similar contexts; they are also called
relations in absentia (Sahlgren, 2006) because
paradigmatically related words do not co-occur.
Examples of paradigmatic relations are synonyms
(e.g., frigid?cold) and antonyms (e.g., cold?hot).
Syntagmatic relations hold between words that co-
occur (relations in praesentia) and therefore ex-
hibit a similar distribution across contexts. Typi-
cal examples of syntagmatic relations are phrasal
associates (e.g., help?wanted) and syntactic collo-
cations (e.g., dog?bark).
Distributional modeling has already tackled the
issue of paradigmatic and syntagmatic relations
(Sahlgren, 2006; Rapp, 2002). Key contributions
of the present work are the scope of its evaluation
(in terms of semantic relations and model parame-
ters) and the new perspective on paradigmatic vs.
syntagmatic models provided by our results.
Concerning the scope of the evaluation, this is
the first study in which the comparison involves
such a wide range of semantic relations (paradig-
matic: synonyms, antonyms and co-hyponyms;
syntagmatic: syntactic collocations, backward and
forward phrasal associates). Moreover, our eval-
uation covers a large number of DSM parame-
ters: source corpus, size and direction of the con-
text window, criteria for feature selection, feature
160
weighting, dimensionality reduction and index of
distributional relatedness. We consider the varia-
tion in performance achieved by different parame-
ter settings as a cue towards characteristic aspects
of specific relations (or groups of relations).
Our work also differs from previous studies
(Sahlgren, 2006; Rapp, 2002) in its focus on
second-order models. We aim to show that they
are able to capture both paradigmatic and syn-
tagmatic relations with appropriate parameter set-
tings. In addition, this focus provides a uniform
experimental design for the evaluation. For ex-
ample, parameters like window size and direction-
ality apply to bag-of-words DSMs and colloca-
tion lists but not to term-context models; dimen-
sionality reduction, whose effect has not yet been
explored systematically in the context of syntag-
matic and paradigmatic relations, is not applicable
to collocation lists.
This paper is structured as follows. Section 2
summarizes previous work. Section 3 describes
the experimental setup, in terms of task, datasets
and evaluated parameters. Section 4 introduces
our model selection methodology. Section 5
presents the results of our evaluation study. Sec-
tion 6 summarizes main findings and sketches on-
going and future work.
2 Previous Work
In this section we discuss previous work relevant
to the distributional modeling of paradigmatic and
syntagmatic relations. For space constraints, we
focus only on two studies (Rapp, 2002; Sahlgren,
2006) in which the two classes of relations are
compared at a global level, and not on studies
that are concerned with specific semantic rela-
tions, e.g., synonymy (Edmonds and Hirst, 2002;
Curran, 2003), hypernymy (Weeds et al., 2004;
Lenci and Benotto, 2012) or syntagmatic predicate
preferences (McCarthy and Carroll, 2003; Erk et
al., 2010), etc.
In previous studies, the comparison of syntag-
matic and paradigmatic relations has been imple-
mented in terms of an opposition between differ-
ent classes of corpus-based models: term-context
models (words as targets, documents or context re-
gions as features) vs. bag-of-words models (words
as targets and features) in Sahlgren (2006); col-
location lists vs. bag-of-words models in Rapp
(2002). Given the high terminological variation
in the literature, in this paper we will adopt the
labels syntagmatic and paradigmatic to character-
ize different types of semantic relations, and we
will use the labels first-order and second-order
to characterize corpus-based models with respect
to the kind of co-occurrence information they en-
code. We will refer to collocation lists and term-
document DSMs as first-order models, and to bag-
of-words DSMs as second-order models
1
.
Rapp (2002) integrates first-order (co-
occurrence lists) and second-order (bag-of-words
DSMs) information to distinguish syntagmatic
and paradigmatic relations. Under the assumption
that paradigmatically related words will be found
among the closest neighbors of a target word in
the DSM space and that paradigmatically and syn-
tagmatically related words will be intermingled
in the list of collocates of the target word, Rapp
proposes to exploit a comparison of the most
salient collocates and the nearest DSM neighbors
to distinguish between the two types of relations.
Sahlgren (2006) compares term-context and
bag-of-words DSMs in a number of tasks involv-
ing syntagmatic and paradigmatic relations. First,
a comparison between the thesaurus entries for tar-
get words (containing both paradigmatically and
syntagmatically related words) and neighbors in
the distributional spaces is conducted. It shows
that, while term-context DSMs produce both syn-
tagmatically and paradigmatically related words,
the nearest neighbors in a bag-of-words DSM
mainly provide paradigmatic information. Bag-
of-words models also performed better than term-
context models in predicting association norms,
in the TOEFL multiple-choice synonymy task and
in the prediction of antonyms (although the dif-
ference in performance was less significant here).
Last, word neighborhoods are analysed in terms of
their part-of-speech distribution. Sahlgren (2006)
observes that bag-of-words spaces contain more
neighbors with the same part of speech as the tar-
get than term-context spaces. He concludes that
bag-of-words spaces privilege paradigmatic rela-
tions, based on the assumption that paradigmati-
cally related word pairs belong to the same part of
speech, while this is not necessarily the case for
syntagmatically related word pairs.
1
Term-document models encode first-order information
because dot products between row vectors are related to co-
occurrence counts of the corresponding words (within docu-
ments). More precisely, for a binary term-document matrix,
cosine similarity is identical to the square root of the MI
2
as-
sociation measure. Please note that our terminology differs
from that of Sch?utze (1998) and Peirsman et al. (2008).
161
Summing up, in both Rapp (2002) and Sahlgren
(2006) it is claimed that second-order models per-
form poorly in predicting syntagmatic relations.
However, neither of those studies involves datasets
containing exclusively syntagmatic relations, as
the evaluation focuses either on paradigmatic rela-
tions (TOEFL multiple choice test, antonymy test)
or on resources containing both types of relations
(thesauri, association norms).
3 Experimental Setting
3.1 Evaluation Task and Data
In this study, bag-of-words DSMs are evaluated on
two datasets containing experimental items from
two priming studies. Each item is a word triple
(target, consistent prime, inconsistent prime) with
a particular semantic relation between target and
consistent prime. Following previous work on
modeling priming effects as a comparison between
prime-target pairs (McDonald and Brew, 2004;
Pad?o and Lapata, 2007; Herda?gdelen et al., 2009),
we evaluate our models in a classification task.
The goal is to identify the consistent prime on the
basis of its distributional relatedness to the tar-
get: if a particular DSM (i.e., a certain parame-
ter combination) is sensitive to a specific relation
(or group of relations), we expect the consistent
primes to be closer to the target in semantic space
than the inconsistent ones.
The first dataset is derived from the Semantic
Priming Project (SPP) (Hutchison et al., 2013).
To the best of our knowledge, our study repre-
sents the first evaluation of bag-of-words DSMs
on items from this dataset. The original data con-
sist of 1661 word triples (target, consistent prime,
inconsistent prime) collected within a large-scale
project aiming at characterizing English words in
terms of a set of lexical and associative/semantic
characteristics, along with behavioral data from
visual lexical decision and naming studies
2
. We
manually discarded all triples containing proper
nouns, adverbs or inflected words. We then
selected five subsets involving different seman-
tic relations, namely: synonyms (SYN), 436
triples (example of a consistent prime and tar-
get: frigid?cold); antonyms (ANT): 135 triples
(e.g., hot?cold); cohyponyms (COH): 159 triples
(e.g., table?chair); forward phrasal associates
(FPA): 144 triples (e.g., help?wanted); back-
2
The dataset is available at http://spp.montana.edu/
ward phrasal associates (BPA): 89 triples (e.g.,
wanted?help).
The second priming dataset is the Generalized
Event Knowledge dataset (henceforth GEK), al-
ready evaluated in Lapesa and Evert (2013): a
collection of 402 triples (target, consistent prime,
inconsistent prime) from three priming studies
conducted to demonstrate that event knowledge
is responsible for facilitation of the processing
of words that denote events and their partici-
pants. The first study was conducted by Fer-
retti et al. (2001), who found that verbs facili-
tate the processing of nouns denoting prototypi-
cal participants in the depicted event and of ad-
jectives denoting features of prototypical partic-
ipants. The study covered five thematic rela-
tions: agent (e.g., pay?customer), patient, fea-
ture of the patient, instrument, location. The sec-
ond study (McRae et al., 2005) focussed on prim-
ing from nouns to verbs. It involved four re-
lations: agent (e.g., reporter?interview), patient,
instrument, location. The third study (Hare et
al., 2009) investigated priming from nouns to
nouns, referring to participants of the same event
or the event itself. The dataset involves seven
relations: event-people (e.g., trial?judge), event-
thing, location-living, location-thing, people-
instrument, instrument-people, instrument-thing.
In the presentation of our results we group syn-
onyms with antonyms and cohyponyms from SPP
as paradigmatic relations, and the entire GEK
dataset with backward and forward phrasal asso-
ciates from SPP as syntagmatic relations.
3.2 Evaluated Parameters
DSMs evaluated in this paper belong to the class of
bag-of-words models. We defined a large vocab-
ulary of target words (27522 lemma types) con-
taining all the items from the evaluated datasets
as well as items from other state-of-the-art evalu-
ation studies (Baroni and Lenci, 2010; Baroni and
Lenci, 2011). Context words were filtered by part-
of-speech (nouns, verbs, adjectives, and adverbs).
Distributional models were built using the UCS
toolkit
3
and the wordspace package for R
4
. The
following parameters have been evaluated:
? Source corpus (abbreviated as corpus in plots
1-4): We compiled DSMs from three corpora
often used in DSM evaluation studies and that
3
http://www.collocations.de/software.html
4
http://r-forge.r-project.org/projects/wordspace/
162
differ in both size and quality: British National
Corpus
5
, ukWaC, and WaCkypedia EN
6
.
? Size of the context window (win.size): As
this parameter quantifies the amount of shared
context involved in the computation of similar-
ity, we expect it to be crucial in determining
whether syntagmatic or paradigmatic relations
are captured. We therefore use a finer granu-
larity for window size than Lapesa and Evert
(2013): 1, 2, 4, 8 and 16 words.
? Directionality of the context window
(win.direction): When collecting co-occurrence
information from the source corpora, we use ei-
ther a directed window (i.e., separate frequency
counts for co-occurrences of a context term
to the left and to the right of the target term)
or an undirected window (i.e., no distinction
between left and right context when collecting
co-occurrence counts).
? Context selection: From the full co-occurrence
matrix collected as described above, we select
dimensions (columns) according to the follow-
ing parameters:
? Criterion for context selection (criterion):
We select the top-ranked dimensions either
according to marginal frequency (i.e., we use
the most frequent words as context terms)
or number of nonzero co-occurrence counts
(i.e., we use the context terms that co-occur
with the highest number of targets).
? Number of context dimensions (con-
text.dim): We select the top-ranked 5000,
10000, 20000, 50000 or 100000 dimensions,
according to the criterion above.
? Feature scoring (score): Co-occurrence counts
are weighted using one of the following associa-
tion measures: frequency, Dice coefficient, sim-
ple log-likelihood, Mutual Information, t-score,
z-score or tf.idf.
7
? Feature transformation (transformation): A
transformation function may be applied to re-
duce the skewness of feature scores. Possible
transformations are: none, square root, logarith-
mic and sigmoid.
5
http://www.natcorp.ox.ac.uk/
6
Both ukWaC and WaCkypedia EN are available at:
wacky.sslmit.unibo.it/doku.php?id=corpora
7
See Evert (2008) for a description of these measures and
details on the calculation of association scores. Note that
we compute ?sparse? versions of the association measures
(where negative values are clamped to zero) in order to pre-
serve the sparseness of the co-occurrence matrix.
? Distance metric (metric): We apply cosine dis-
tance (i.e., angle between vectors) or Manhattan
distance.
? Dimensionality reduction: We apply singular
value decomposition in order to project distri-
butional vectors to a relatively small number of
latent dimensions and compare the results to the
unreduced runs
8
. For the SVD-based models,
there are two additional parameters:
? Number of latent dimensions (red.dim):
Whether to use the first 100, 300, 500, 700
or 900 latent dimensions from the SVD anal-
ysis.
? Number of skipped dimensions (dim.skip):
When selecting latent dimensions, we option-
ally skip the first 50 or 100 SVD compo-
nents. This parameter was inspired by Bul-
linaria and Levy (2012), who found that dis-
carding the initial components of the reduced
matrix, i.e. the SVD components with highest
variance, improves evaluation results.
? Index of distributional relatedness (rel.index):
We propose two alternative ways of quantify-
ing the degree of relatedness between two words
a and b represented in a DSM. The first op-
tion (and standard in distributional modeling)
is to compute the distance (cosine or Manhat-
tan) between the vectors of a and b. The sec-
ond option, proposed in this work, is based on
neighbor rank, i.e. we determine the rank of
the target among the nearest neighbors of each
prime. We expect that the target will occur in a
higher position among the neighbors of the con-
sistent prime than among those of the inconsis-
tent prime. Since this corresponds to a lower
numeric rank value for the consistent prime, we
can treat neighbor rank as a measure of dissim-
ilarity. Neighbor rank is particularly interesting
as an index of relatedness because, unlike a dis-
tance metric, it can capture asymmetry effects
9
.
4 Methodology
In our evaluation study, we tested all the possible
combinations of the parameters listed in section
8
For efficiency reasons, we use randomized SVD (Halko
et al., 2009) with a sufficiently high oversampling factor to
ensure a good approximation.
9
Note that our use of neighbor rank is fully consistent with
the experimental design (primes are shown before targets).
See Lapesa and Evert (2013) for an analysis of the perfor-
mance of neighbor rank as a predictor of priming and discus-
sion of the implications of using rank in cognitive modeling.
163
3.2, resulting in a total of 537600 different model
runs (33600 in the setting without dimensionality
reduction, 504000 in the dimensionality-reduced
setting). The models were generated and evaluated
on a large HPC cluster within approx. 4 weeks.
Our methodology for model selection follows
the proposal of Lapesa and Evert (2013), who con-
sider DSM parameters as predictors of model per-
formance. We analyze the influence of individual
parameters and their interactions using general lin-
ear models with performance (percent accuracy)
as a dependent variable and the model parame-
ters as independent variables, including all two-
way interactions. Analysis of variance ? which
is straightforward for our full factorial design ? is
used to quantify the importance of each parameter
or interaction. Robust optimal parameter settings
are identified with the help of effect displays (Fox,
2003), which marginalize over all the parameters
not shown in a plot and thus allow an intuitive in-
terpretation of the effect sizes of categorical vari-
ables irrespective of the dummy coding scheme.
For each dataset, a separate linear model was
fitted. The results are reported and compared in
section 5. Table 1 lists the global goodness-of-fit
(R
2
) on each dataset, for the reduced and unre-
duced runs. Despite some variability across re-
lations and between unreduced and reduced runs,
the R
2
values are always high (? 75%), showing
that the linear model explains a large part of the
observed performance differences. It is therefore
justified to base our analysis on the linear models.
Relation Dataset Unreduced Reduced
Syntagmatic GEK 93% 87%
Syntagmatic FPA 90% 79%
Syntagmatic BPA 88% 77%
Paradigmatic SYN 92% 85%
Paradigmatic COH 89% 75%
Paradigmatic ANT 89% 76%
Table 1: Evaluation, Global R
2
5 Results
In this section, we present the results of our study.
We begin by looking at the distribution of accu-
racy for different datasets, and by comparing re-
duced and unreduced experimental runs in terms
of minimum, maximum and mean performance.
The results displayed in table 2 show that di-
mensionality reduction with SVD improves the
performance of the models for all datasets but
GEK. We conclude that the information lost by ap-
plying SVD reduction (namely, meaningful distri-
butional features, which are replaced by the gener-
Relation Dataset
Unreduced Reduced
Min Max Mean Min Max Mean
Syntagmatic GEK 54.8 98.4 86.6 48.0 97.0 80.8
Syntagmatic FPA 41.0 98.0 82.3 43.0 98.6 82.1
Syntagmatic BPA 49.4 97.7 83.8 41.6 98.9 83.9
Paradigmatic SYN 54.8 98.4 86.6 57.3 99.0 88.2
Paradigmatic COH 49.0 100.0 92.6 54.3 100.0 94.0
Paradigmatic ANT 69.6 100.0 94.2 57.8 100.0 94.3
Table 2: Distribution of Accuracy
alization encoded in the reduced dimensions) is ir-
relevant to other tasks, but crucial for modeling the
relations in the GEK dataset. This interpretation is
consistent with the detrimental effect of SVD in
tasks involving vector composition reported in the
literature (Baroni and Zamparelli, 2010).
5.1 Importance of Parameters
To obtain further insights into DSM performance
we explore the effect of specific model parameters,
comparing syntagmatic vs. paradigmatic relations
and reduced vs. unreduced runs.
In order to establish a ranking of the parameters
according to their importance wrt. model perfor-
mance, we use a feature ablation approach. The
ablation value for a given parameter is the propor-
tion of variance (R
2
) explained by this parameter
together with all its interactions, corresponding to
the reduction in adjusted R
2
of the linear model fit
if the parameter were left out. In other words, it
allows us to find out whether a certain parameter
has a substantial effect on model performance (on
top of all other parameters). Figures 1 to 4 display
the feature ablation values of all the evaluated pa-
rameters in the unreduced and reduced setting, for
paradigmatic and syntagmatic relations. Parame-
ters are ranked according to their average feature
ablation values in each setting.
Two parameters, namely feature score and fea-
ture transformation, are consistently crucial in
determining DSM performance, both in reduced
and unreduced runs, and for both paradigmatic
and syntagmatic relations. In the next section we
will show that it is possible to identify optimal (or
nearly optimal) values for those parameters that
are constant across relations.
A comparison of figures 1 and 2 with figures 3
and 4 allows us to identify parameters that lose
or gain explanatory power when SVD comes into
play. Feature ablation shows that the effect of the
index of distributional relatedness is substan-
tially smaller in the SVD-reduced runs, but this pa-
rameter still plays an important role. On the other
hand, two parameters gain explanatory power in a
164
ll
l
l
l
l
l
l
lcriterion
win.direction
context.dim
win.size
corpus
metric
transformation
rel.index
score
0 20 40 60Feature Ablation
l SYNANTCOH
Figure 1: Paradigmatic, unreduced
l
l
l
l
l
l
l
l
lcriterion
win.direction
context.dim
win.size
corpus
metric
transformation
rel.index
score
0 20 40 60Feature Ablation
l GEKFPABPA
Figure 2: Syntagmatic, unreduced
l
l
l
l
l
l
l
l
l
l
lcriterionwin.direction
context.dimdim.skip
rel.indexwin.size
red.dimmetric
corpustransformation
score
0 10 20 30Feature Ablation
l SYNANTCOH
Figure 3: Paradigmatic, reduced
l
l
l
l
l
l
l
l
l
l
lcriterionwin.direction
context.dimrel.index
dim.skipred.dim
metriccorpus
win.sizetransformation
score
0 10 20 30Feature Ablation
l GEKFPABPA
Figure 4: Syntagmatic, reduced
SVD-reduced setting: the size of the context win-
dow and the source corpus. Optimal values are
discussed in section 5.2.
Three parameters consistently have little or no
explanatory power: directionality of the con-
text window, criterion for context selection and
number of context dimensions.
We conclude this section by comparing rela-
tions within groups. Within paradigmatic rela-
tions, we note a significant drop in explanatory
power for the relatedness index when it comes to
antonyms. Within syntagmatic relations, the size
of the context window appears to be more crucial
on the GEK dataset than it is for FPA and BPA:
in the next section, the analysis of the best choices
for this parameter will provide a clue for the inter-
pretation of this opposition.
5.2 Best Parameter Values
In this section, we identify the best parameter val-
ues for syntagmatic and paradigmatic relations by
inspecting partial effects plots
10
. Our discussion
starts from the parameters that contribute to the
leading topic of this paper, namely the comparison
between syntagmatic and paradigmatic relations:
10
The partial effect plots in figures 5 to 12 display param-
eter values on the x-axis and their effect size in terms of pre-
dicted accuracy on the y-axis (see section 4 for more details
concerning the calculation of effect size).
window size, parameters related to dimensionality
reduction, and relatedness index.
As already anticipated in the feature ablation
analysis, the size of the context window plays
a crucial role in contrasting syntagmatic and
paradigmatic relations, as well as different rela-
tions within those general groups. The plots in fig-
ures 5 and 6 display its partial effect for paradig-
matic relations in the unreduced and reduced set-
tings, respectively. The plots in figures 7 and 8
display its partial effect for syntagmatic relations.
When no dimensionality reduction is involved, a
very small context window (i.e., one word) is suffi-
cient for all paradigmatic relations, and DSM per-
formance decreases as soon as we enlarge the con-
text window. The picture changes when apply-
ing dimensionality reduction: a 4-word window
is a robust choice for all paradigmatic relations
(although ANT show a further increase in perfor-
mance with an 8-word window), even in the SYN
task that is traditionally associated with very small
windows of 1 or 2 words (cf. Sahlgren (2006)).
A significant interaction between window size
and number of skipped dimensions (not shown for
reasons of space) sheds further light on this matter.
Without skipping SVD dimensions, the reduced
models achieve optimal performance for a 2-word
window and degrade more (COH) or less (ANT)
165
l l
l
l
l
l
l l l
ll l
l
l
l
84
87
90
93
96
1 2 4 8 16
dataset
l
l
l
SYNANTCOH
Figure 5: Window, paradigmatic, unreduced
l
l l l l
l
l
l l l
l
l
l l l
84
87
90
93
96
1 2 4 8 16
dataset
l
l
l
SYNANTCOH
Figure 6: Window, paradigmatic, reduced
l
l
l l l
l
l l l
l
l
l
l
l
l
74
76
78
80
82
84
86
1 2 4 8 16
dataset
l
l
l
GEKFPABPA
Figure 7: Window, syntagmatic, unreduced
l
l
l
l
l
l
l
l
l l
l
l
l l l
74
76
78
80
82
84
86
1 2 4 8 16
dataset
l
l
l
GEKFPABPA
Figure 8: Window, syntagmatic, reduced
quickly for larger windows. With 50 or 100 di-
mensions skipped, performance improves up to a
4- or 8-word window. Our interpretation is that the
first SVD dimensions capture general domain and
topic information dominating the co-occurrence
data; removing these dimensions reveals paradig-
matic semantic relations even for larger windows.
For syntagmatic relations without dimensionality
reduction, a larger context window of 4 words is
needed for FPA and BPA; a further increase of the
window is detrimental. For the GEK dataset, per-
formance peaks at 8 words, and decreases only
minimally for even larger windows. Again, di-
mensionality reduction improves performance for
large co-occurrence windows. For FPA and BPA,
the optimum seems to be achieved with a win-
dow of 4?8 words; performance on GEK contin-
ues to increase up to 16 words, the largest win-
dow size considered in our experiments. Such pat-
terns reflect differences in the nature of the se-
mantic relations involved: smaller windows pro-
vide better contextual representations for paradig-
matic relations while larger windows are needed to
capture syntagmatic relations with bag-of-words
DSMs (because co-occurring words then share a
large portion of their context windows). Interme-
diate window sizes are sufficient for phrasal col-
locates (which are usually adjacent), while event-
based relatedness (GEK) requires larger windows.
Returning briefly to the slight preference shown
by ANT for a larger window, we notice that ANT
seems to be more similar to the syntagmatic rela-
tions than SYN and COH. This is in line with the
observations of Justeson and Katz (1992) concern-
ing the tendency of antonyms to co-occur (e.g., in
coordinations such as short and long). Like syn-
onyms, antonyms are interchangeable in absentia;
but they also enter into syntagmatic patterns that
are uncommon for synonyms.
We now focus on the parameters related to di-
mensionality reduction, namely the number of la-
tent dimensions (figures 9 and 10) and the num-
ber of skipped dimensions (figures 11 and 12).
These parameters represent an extension of the
experiments conducted on the GEK dataset by
Lapesa and Evert (2013). They have already been
applied by Bullinaria and Levy (2012) to a differ-
ent set of tasks, including the TOEFL multiple-
choice synonymy task. In particular, Bullinaria
and Levy found that discarding the initial SVD di-
mensions (with highest variance) leads to substan-
tial improvements, especially in the TOEFL task.
In our experiments, we found no difference be-
tween syntagmatic and paradigmatic relations wrt.
the number of latent dimensions: the more, the
better in both cases (900 dimensions). The number
of skipped dimensions, however, shows some vari-
ability across the different relations. The results
for SYN are in agreement with the findings of Bul-
linaria and Levy (2012) on TOEFL: skipping 50
or 100 initial dimensions improves performance.
Skipping dimensions makes minimal difference
166
ll
l
l l
l
l
l l
l
l
l
l l l
87
90
93
96
100 300 500 700 900
dataset
l
l
l
SYNANTCOH
Figure 9: Latent dimensions, paradigmatic
l
l
l
l l
l
l
l
l l
l
l
l
l
l
76
78
80
82
84
86
100 300 500 700 900
dataset
l
l
l
GEKFPABPA
Figure 10: Latent dimensions, syntagmatic
l
l
l
l
l
ll
l
l
87
89
91
93
95
0 50 100
dataset
l
l
l
SYNANTCOH
Figure 11: Skipped dimensions, paradigmatic
l
l
l
l l
l
l
l
l
80
81
82
83
84
0 50 100
dataset
l
l
l
GEKFPABPA
Figure 12: Skipped dimensions, syntagmatic
for COH (best choice is 50 dimensions), while the
full range of reduced dimensions is necessary for
ANT. Within syntagmatic relations, the full range
of latent dimensions ensures good performance on
phrasal associates (even if skipping 50 dimensions
is not detrimental for BPA). GEK shows a pattern
similar to SYN, with 50 skipped dimensions lead-
ing to a considerable improvement.
We now inspect the best values for the related-
ness index. As shown in figure 13 for the unre-
duced runs and in figure 14 for the reduced runs,
neighbor rank is consistently better than distance
on all datasets. This is not surprising because, as
discussed in section 3.2, our use of neighbor rank
captures asymmetry and mirrors the experimental
setting, in which targets are shown after primes.
A further observation may be made relating to the
degree of asymmetry of different relations. The
unreduced setting in particular shows that syntag-
matic relations are subject to stronger asymme-
try effects than the paradigmatic ones, presumably
due to the directional nature of the relations in-
volved (phrasal associates and syntactic colloca-
tions). Among paradigmatic relations, antonyms
appear to be the least asymmetric ones (because
using neighbor rank instead of distance makes a
comparatively small difference).
We conclude by briefly summarizing the opti-
mal choices for the remaining parameters. The
corresponding partial effects plots are not shown
because of space constraints.
A very strong interaction between score and
transformation characterizes all four settings
(paradigmatic or syntagmatic datasets, reduced or
unreduced experimental runs). Association mea-
sures outperform raw co-occurrence frequency.
Measures based on significance tests (simple-ll,
t-score, z-score) are better than Dice, and to a
lesser extent, MI. Simple-ll is the best choice in
combination with a logarithmic transformation for
paradigmatic relations, z-score appears to be the
best measure for syntagmatic relations in combi-
nation with a square root transformation. The dif-
ference is small, however, and simple-ll with log
transformation works well across all datasets. On-
going experiments with standard tasks show a sim-
ilar pattern, suggesting that this combination of
score and transformation parameters is appropri-
ate for DSMs, regardless of the task involved.
The optimal distance metric is the cosine
distance, consistently outperforming Manhattan.
Concerning source corpus, BNC consistently
yields the worst results, while WaCkypedia and
ukWaC appear to be almost equivalent in the unre-
duced runs. The trade-off between quality and
quantity appears to be strongly biased towards
sheer corpus size in the case of distributional mod-
els. For syntagmatic relations and SVD-reduced
models, ukWaC is clearly the best choice. This
suggests that syntagmatic relations are better cap-
tured by features from a larger lexical inventory,
combined with the abstraction performed by SVD.
167
ll
l
l
l
ll
l
l
l
l
l
75
80
85
90
95
SYN ANT COH GEK FPA BPA
rel.index
l
l
distrank
Figure 13: Relatedness index, unreduced
l
l l
l
l
l
l
l
l
l
l
l
75
80
85
90
95
SYN ANT COH GEK FPA BPA
rel.index
l
l
distrank
Figure 14: Relatedness index, reduced
Concerning minimally explanatory parameters,
inspection of partial effect plots supported the
choice of ?unmarked? default values for direc-
tionality of the context window (i.e., undirected)
and criterion for context selection (i.e., fre-
quency), as well as an intermediate number of
context dimensions (i.e., 50000 dimensions).
5.3 Best Settings
We conclude by comparing the performance
achieved by our robust choice of optimal param-
eter values (?best setting?) from section 5.2 with
the performance of the best model for each dataset.
For space constraints, the analysis of best settings
focuses on the reduced experimental runs. Our
best settings, shown in table 3, perform fairly well
on the respective datasets
11
.
dataset corpus win score transf r.dim d.sk acc best
GEK ukwac 16 s-ll log 900 50 96.0 97.0
FPA ukwac 8 z-sc root 900 0 93.0 98.6
BPA ukwac 8 z-sc root 900 0 95.5 98.9
SYN ukwac 4 s-ll log 900 50 96.3 99.0
COH ukwac 4 s-ll log 900 50 98.7 100
ANT wacky 8 s-ll log 900 0 100 100
Table 3: Best settings: datasets, parameter values,
accuracy (acc), accuracy of the best model (best)
best setting corpus win score transf r.dim d.sk
Syntagmatic ukwac 8 z-sc root 900 0
Paradigmatic ukwac 4 s-ll log 900 50
General ukwac 4 s-ll log 900 0
Table 4: General best settings: parameter values
Dataset Best Synt. Best Para. General
GEK 92.5 94.8 91.3
FPA 93.0 90.2 91.7
BPA 95.5 97.7 95.5
SYN 94.4 96.3 96.3
COH 99.3 98.7 98.7
ANT 99.2 99.2 99.2
Table 5: General best settings: accuracy
11
Abbreviations in tables 3 and 4: win = window size;
transf = transformation; z-sc = z-score; s-ll = simple-ll; r.dim
= number of latent dimensions; d.sk = number of skipped di-
mensions. Parameters with fixed values for all datasets: num-
ber of context dimensions = 50k; direction = undirected; cri-
terion = frequency; metric = cosine; relatedness index = rank.
As a next step, we identified parameter combi-
nations that work well for all types of syntagmatic
and paradigmatic relations, as well as an even
more general setting that is suitable for paradig-
matic and syntagmatic relations alike. Best set-
tings are shown in table 4, their performance on
each dataset is reported in table 5. General models
achieve fairly good performance on all relations.
6 Conclusion
We presented a large-scale evaluation study of
bag-of-words DSMs on a classification task de-
rived from priming experiments. The leading
theme of our study is a comparison between syn-
tagmatic and paradigmatic relations in terms of
the aspects of distributional similarity that char-
acterize them. Our results show that second-order
DSMs are capable of capturing both syntagmatic
and paradigmatic relations, if parameters are prop-
erly tuned. Size of the co-occurrence window as
well as parameters connected to dimensionality re-
duction play a key role in adapting DSMs to par-
ticular relations. Even if we do not address the
more specific task of distinguishing between rela-
tions (e.g., synonyms vs. antonyms; see Scheible
et al. (2013) and references therein), we believe
that such applications may benefit from our de-
tailed analyses on the effects of DSM parameters.
Ongoing and future work is concerned with the
expansion of the evaluation setting to other classes
of models (first-order models, dependency-based
second-order models) and parameters (e.g., di-
mensionality reduction with Random Indexing).
Acknowledgments
We are grateful to Ken MacRae for providing us
the GEK priming data and to the three review-
ers. This research was funded by the DFG Col-
laborative Research Centre SFB 732 (Gabriella
Lapesa) and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde).
168
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting semantic representations from word co-
occurrence statistics: stop-lists, stemming and svd.
Behavior Research Methods, 44:890?907.
James Curran. 2003. From distributional to semantic
similarity. Ph.D. thesis, Institute for Communicat-
ing and Collaborative Systems, School of Informat-
ics, University of Edinburgh.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Lin-
guistics, 28(2):105?144.
Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Stefan Evert. 2008. Corpora and collocations. In Anke
L?udeling and Merja Kyt?o, editors, Corpus Linguis-
tics. An International Handbook, chapter 58. Mou-
ton de Gruyter, Berlin, New York.
Todd Ferretti, Ken McRae, and Ann Hatherell. 2001.
Integrating verbs, situation schemas, and thematic
role concepts. Journal of Memory and Language,
44(4):516?547.
John Fox. 2003. Effect displays in R for gener-
alised linear models. Journal of Statistical Software,
8(15):1?27.
Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical Report 2009-05,
ACM, California Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah
Kelly, and Ken McRae. 2009. Activating event
knowledge. Cognition, 111(2):151?167.
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Amac Herda?gdelen, Marco Baroni, and Katrin Erk.
2009. Measuring semantic relatedness with vector
space models and random walks. In Proceedings
of the 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 50?53.
Keith A. Hutchison, David A. Balota, James H. Neely,
Michael J. Cortese, Emily R. Cohen-Shikora, Chi-
Shing Tse, Melvin J. Yap, Jesse J. Bengson, Dale
Niemeyer, and Erin Buchanan. 2013. The seman-
tic priming project. Behavior Research Methods,
45(4):1099?1114.
John. S. Justeson and Slava M. Katz. 1992. Redefining
antonymy: The textual structure of a semantic rela-
tion. Literary and Linguistic Computing, 7(3):176?
184.
Gabriella Lapesa and Stefan Evert. 2013. Evaluat-
ing neighbor rank and distance measures as predic-
tors of semantic priming. In Proceedings of the
ACL Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2013), pages 66?74.
Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of *SEM 2012: The First Joint Con-
ference on Lexical and Computational Semantics ?
Volume 1, pages 75?79.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Scott McDonald and Chris Brew. 2004. A distri-
butional model of semantic context effects in lexi-
cal processing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 17?24.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd
Ferretti. 2005. A basis for generating expectan-
cies for verbs from nouns. Memory & Cognition,
33(7):1174?1184.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Yves Peirsman, Kris Heylen, and Dirk Speelman.
2008. Putting things in order. First and second order
context models for the calculation of semantic sim-
ilarity. In JADT 2008: 9es Journ?ees internationales
d?Analyse statistique des Donn?ees Textuelles.
Reinhard Rapp. 2002. The computation of word asso-
ciations: Comparing syntagmatic and paradigmatic
approaches. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics -
Volume 1, pages 1?7.
Magnus Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, University
of Stockolm.
169
Magnus Sahlgren. 2008. The distributional hypothe-
sis. Rivista di Linguistica (Italian Journal of Lin-
guistics), 20(1):33?53.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering Distributional Dif-
ferences between Synonyms and Antonyms in a
Word Space Model. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, pages 489?497.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 27(1):97?
123.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference of Computational Linguistics, pages
1015?1021, Geneva, Switzerland.
170
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 66?74,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Evaluating Neighbor Rank and Distance Measures
as Predictors of Semantic Priming
Gabriella Lapesa
Universita?t Osnabru?ck
Institut fu?r Kognitionswissenschaft
Albrechtstr. 28, 49069 Osnabru?ck
glapesa@uos.de
Stefan Evert
FAU Erlangen-Nu?rnberg
Professur fu?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen
severt@fau.de
Abstract
This paper summarizes the results of a
large-scale evaluation study of bag-of-
words distributional models on behavioral
data from three semantic priming experi-
ments. The tasks at issue are (i) identifi-
cation of consistent primes based on their
semantic relatedness to the target and (ii)
correlation of semantic relatedness with
latency times. We also provide an evalu-
ation of the impact of specific model pa-
rameters on the prediction of priming. To
the best of our knowledge, this is the first
systematic evaluation of a wide range of
DSM parameters in all possible combina-
tions. An important result of the study
is that neighbor rank performs better than
distance measures in predicting semantic
priming.
1 Introduction
Language production and understanding make ex-
tensive and immediate use of world knowledge
information that concerns prototypical events.
Plenty of experimental evidence has been gathered
to support this claim (see McRae and Matzuki,
2009, for an overview). Specifically, a number of
priming studies have been conducted to demon-
strate that event knowledge is responsible for fa-
cilitation of processing of words that denote events
and their participants (Ferretti et al, 2001; McRae
et al, 2005; Hare et al, 2009). The aim of our re-
search is to investigate to which extent such event
knowledge surfaces in linguistic distribution and
can thus be captured by Distributional Semantic
Models (henceforth, DSMs). In particular, we test
the capabilities of bag-of-words DSMs in simu-
lating priming data from the three aforementioned
studies.
DSMs have already proven successful in sim-
ulating priming effects (Pado? and Lapata, 2007;
Herdag?delen et al, 2009; McDonald and Brew,
2004). Therefore, in this work, we aim at a more
specific contribution to the study of distributional
modeling of priming: to identify the indexes of
distributional relatedness that produce the best
performance in simulating priming data and to as-
sess the impact of specific model parameters on
such performance. In addition to distance in the
semantic space, traditionally used as an index of
distributional relatedness in DSMs, we also intro-
duce neighbor rank as a predictor of priming ef-
fects. Distance and a number of rank-based mea-
sures are compared with respect to their perfor-
mance in two tasks: the identification of congruent
primes on the basis of distributional relatedness
to the targets (we measure accuracy in picking up
the congruent prime) and the prediction of latency
times (we measure correlation between distribu-
tional relatedness and reaction times). The results
of our experiments show that neighbor rank is a
better predictor than distance for priming data.
Our approach to DSM evaluation constitutes
a methodological contribution of this study: we
use linear models with performance (accuracy or
correlation) as a dependent variable and various
model parameters as independent variables, in-
stead of looking for optimal parameter combina-
tions. This approach is robust to overfitting and
allows to analyze the influence of individual pa-
rameters as well as their interactions.
The paper is structured as follows. Section
2 provides an overview of the modeled datasets.
Section 3 introduces model parameters and in-
dexes of distributional relatedness evaluated in this
paper, describes the experimental tasks and out-
lines our statistical approach to DSM evaluation.
Section 4 presents results for the accuracy and cor-
relation tasks and evaluates the impact of model
parameters on performance. We conclude in sec-
tion 5 by sketching ongoing work and future de-
velopments of our research.
66
Dataset Relation N Primec Primei Target Fac
V-N
AGENT 28 Pay Govern Customer 27*
PATIENT 18 Invite Arrest Guest 32*
PATIENT FEATURE 20 Comfort Hire Upset 33*
INSTRUMENT 26 Cut Dust Rag 32*
LOCATION 24 Confess Dance Court - 5
N-V
AGENT 30 Reporter Carpenter Interview 18*
PATIENT 30 Bottle Ball Recycle 22*
INSTRUMENT 32 Chainsaw Detergent Cut 16*
LOCATION 24 Beach Pub Tan 18*
N-N
EVENT-PEOPLE 18 Trial War Judge 32*
EVENT-THING 26 War Gun Banquet 33*
LOCATION-LIVING 24 Church Gym Athlete 37*
LOCATION-THING 30 Pool Garage Car 29*
PEOPLE-INSTRUMENT 24 Hiker Barber Compass 45*
INSTRUMENT-PEOPLE 24 Razor Compass Barber -10
INSTRUMENT-THING 24 Hair Scissors Oven 58*
Table 1: Overview of datasets: thematic relations, number of triples, example stimuli, facilitation effects
2 Data
This section introduces the priming datasets which
are the object of the present study. All the experi-
ments we aim to model were conducted to provide
evidence for the immediate effect of event knowl-
edge in language processing.
The first dataset comes from Ferretti et al
(2001), who found that verbs facilitate the process-
ing of nouns denoting prototypical participants in
the depicted event and of adjectives denoting fea-
tures of prototypical participants. In what follows,
the dataset from this study will be referred to as
V-N dataset.
The second dataset comes from McRae et al
(2005). In this experiment, nouns were found to
facilitate the processing of verbs denoting events
in which they are prototypical participants. In this
paper, this dataset is referred to as N-V dataset.
The third dataset comes from Hare et al (2009),
who found a facilitation effect from nouns to
nouns denoting events or their participants. We
will refer to this dataset as N-N dataset.
Experimental items and behavioral data from
these three experiments have been pooled together
in a global dataset that contains 404 word triples
(Target, Congruent Prime, Incongruent Prime).
For every triple, the dataset contains mean reac-
tion times for the congruent and incongruent con-
ditions, and a label for the thematic relation in-
volved. Table 1 provides a summary of the exper-
imental data. It specifies the number of triples for
every relation in the datasets (N) and gives an ex-
ample triple (Primecongruent , Primeincongruent , Tar-
get). Facilitation effects and stars marking signif-
icance by participants and items reported in the
original studies are also specified for every rela-
tion (Fac). Relations for which the experiments
showed no priming effect are highlighted in bold.
3 Method
3.1 Models
Building on the Distributional Hypothesis (Har-
ris, 1954), DSMs are employed to produce seman-
tic representations of words from patterns of co-
occurrence in texts or documents (Sahlgren, 2006;
Turney and Pantel, 2010). Semantic representa-
tions in the form of distributional vectors are com-
pared to quantify the amount of shared contexts as
an empirical correlate of semantic similarity. For
the purposes of this study, similarity is understood
in terms of topical relatedness (words connected
to a particular situation) rather than attributional
similarity (synonyms and near-synonyms).
DSMs evaluated in this study belong to the class
of bag-of-words models: the distributional vector
of a target word consists of co-occurrence counts
with other words, resulting in a word-word co-
occurrence matrix. The models cover a large vo-
cabulary of target words (27668 words in the un-
tagged version; 31713 words in the part-of-speech
tagged version). It contains the stimuli from the
datasets described in section 2 and further target
words from state-of-the-art evaluation studies (Ba-
roni and Lenci, 2010; Baroni and Lenci, 2011;
Mitchell and Lapata, 2008). Contexts are fil-
tered by part-of-speech (nouns, verbs, adjectives,
and adverbs) and by frequency thresholds. Nei-
ther syntax nor word order were taken into ac-
count when gathering co-occurrence information.
Distributional models were built using the UCS
67
toolkit1 and the wordspace package for R2. The
evaluated parameters are:
? Corpus: British National Corpus3; ukWaC4;
WaCkypedia EN5; WP5006; and a concate-
nation of BNC, ukWaC, and WaCkype-
dia EN (called the joint corpus);
? Window size: 2, 5, or 15 words to the left
and to the right of the target;
? Part of speech: no part of speech tags; part
of speech tags for targets; part of speech tags
for targets and contexts;
? Scoring measure: frequency; Dice coeffi-
cient; simple log-likelihood; Mutual Infor-
mation; t-score; z-score;7
? Vector transformation: no transformation;
square root, sigmoid or logarithmic transfor-
mation;
? Dimensionality reduction: no dimension-
ality reduction; Singular Value Decompo-
sition to 300 dimensions using randomized
SVD (Halko et al, 2009); Random Indexing
(Sahlgren, 2005) to 1000 dimensions;
? Distance measure: cosine, euclidean or
manhattan distance.
3.2 Indexes of Distributional Relatedess
3.2.1 Distance and Rank
The indexes of distributional relatedness described
in this section represent alternative perspectives
on the semantic representation inferred by DSMs
from co-occurrence data.
Given a target, a prime, and a matrix of dis-
tances produced by a distributional model, we test
the following indexes of relatedness between tar-
get and prime:
? Distance: distance between the vectors of
target and prime in the semantic space;
? Backward association: rank of prime
among the neighbors of target, as in Hare et
al. (2009);8
? Forward association: rank of target in the
neighbors of prime;
1http://www.collocations.de/software.html
2http://r-forge.r-project.org/projects/wordspace/
3http://www.natcorp.ox.ac.uk/
4http://wacky.sslmit.unibo.it/doku.php?id=corpora
5http://wacky.sslmit.unibo.it/doku.php?id=corpora
6A subset of WaCkypedia EN containing the initial 500
words of each article, which amounts to 230 million tokens.
7See Evert (2004) for a description of these measures and
details on the calculation of association scores.
8This type of association is labeled as ?backward? be-
cause it goes from targets to primes, while in the experimental
setting targets are shown after primes.
? Average rank: average of backward and for-
ward association.
Indexes of distributional relatedness were consid-
ered as an additional parameter in the evaluation,
labeled relatedness index below. Every combi-
nation of the parameters described in section 3.1
with each value of the relatedness index param-
eter defines a DSM. The total number of models
evaluated in our study amounts to 38880.
3.2.2 Motivation for Rank
This section provides some motivation for the use
of neighbor rank as a predictor of priming effects
in DSMs, on the basis of general cognitive princi-
ples and of previous modeling experiments.
In distributional semantic modeling, similar-
ity between words is calculated according to Eu-
clidean geometry: the more similar two words are,
the closer they are in the semantic space. One of
the axioms of spatial models is symmetry (Tver-
sky, 1977): the distance between point a and point
b is equal to the distance between point b and point
a. Cognitive processes, however, often violate the
symmetry axiom. For example, asymmetric asso-
ciations are often found in word association norms
(Griffiths et al, 2007).
Our study also contains a case of asymmetry.
In particular, the results from Hare et al (2009),
which constitute our N-N dataset, show priming
from PEOPLE to INSTRUMENTs, but not from IN-
STRUMENTs to PEOPLE. This asymmetry can-
not be captured by distance measures for reasons
stated above. However, the use of rank-based in-
dexes allows to overcome the limitation of sym-
metrical distance measures by introducing direc-
tionality (in our case, target? prime vs. prime?
target), and this without discarding the established
and proven measures.
Rank has already proven successful in model-
ing priming effects with DSMs. Hare et al (2009)
conducted a simulation on the N-N dataset using
LSA (Landauer and Dumais, 1997) and BEAGLE
(Jones and Mewhort, 2007) trained on the TASA
corpus. Asymmetric priming was correctly pre-
dicted by the context-only version of BEAGLE us-
ing rank (namely, rank of prime among neighbors
of target, cf. backward rank in section 3.2.1).
Our study extends the approach of Hare et al
(2009) in a number of directions. First, we in-
troduce and evaluate several different rank-based
measures (section 3.2.1). Second, we evaluate
rank in connection with specific parameters and on
68
larger corpora. Third, we extend the use of rank-
based measures to the distributional simulation of
two other experiments on event knowledge (Fer-
retti et al, 2001; McRae et al, 2005). Note that
our simulation differs from the one by Hare et al
(2009) with respect to tasks (they test for a sig-
nificant difference of mean distances between tar-
get and related vs. unrelated prime) and the class
of DSMs (we use term-term models, rather than
LSA; our models are not sensitive to word order,
unlike BEAGLE).
3.3 Tasks and Analysis of Results
The aim of this section is to introduce the exper-
imental tasks whose results will be discussed in
section 4 and to describe the main features of the
analysis we applied to interpret these results.
Two experiments have been carried out:
? A classification experiment: given a target
and two primes, distributional information is
used to identify the congruent prime. Perfor-
mance in this task is measured by classifica-
tion accuracy (section 4.1).
? A prediction experiment: the informa-
tion concerning distributional relatedness be-
tween targets and congruent primes is tested
as a predictor for latency times. Performance
in this task is quantified by Pearson correla-
tion (section 4.2).
Concerning the interpretation of the evaluation re-
sults, it would hardly be meaningful to look at the
best parameter combination or the average across
all models. The best model is likely to be over-
fitted tremendously (after testing 38880 param-
eter settings over a dataset of 404 data points).
Mean performance is largely determined by the
proportions of ?good? and ?bad? parameter set-
tings among the evaluation runs, which include
many non-optimal parameter values that were only
included for completeness.
Instead, we analyze the influence of individ-
ual DSM parameters and their interactions using
linear models with performance (accuracy or cor-
relation) as a dependent variable and the various
model parameters as independent variables. This
approach allows us to identify parameters that
have a significant effect on model performance
and to test for interactions between the parameters.
Based on the partial effects of each parameter (and
significant interactions) we can select a best model
in a robust way.
This statistical analysis contains some elements
of novelty with respect to the state-of-the-art DSM
evaluation. Broadly speaking, approaches to DSM
evaluation described in the literature fall into two
classes. The first one can be labeled as best model
first, as it implies the identification of the opti-
mal configuration of parameters on an initial task,
considered more basic; the best performing model
on the general task is therefore evaluated on other
tasks of interest. This is the approach adopted, for
example, by Pado? and Lapata (2007). In the sec-
ond approach, described in Bullinaria and Levy
(2007; 2012), evaluation is conducted via incre-
mental tuning of parameters: parameters are eval-
uated sequentially to identify the best performing
value on a number of tasks. Such approaches to
DSM evaluation have specific limitations. The
former approach does not assess which parame-
ters are crucial in determining model performance,
since its goal is the evaluation of performance of
the same model on different tasks. The latter ap-
proach does not allow for parameter interactions,
considering parameters individually. Both limita-
tions are avoided in the analysis used here.
4 Results
4.1 Identification of Congruent Prime
This section presents the results from the first task
evaluated in our study. We used the DSMs to iden-
tify which of the two primes is the congruent one
based on their distributional relatedness to the tar-
get. For every triple in the dataset, the different in-
dexes of distributional relatedness (parameter re-
latedness index) were used to compare the associ-
ation between the target and the congruent prime
with the association between the target and the in-
congruent prime. Accuracy of DSMs in picking up
the congruent prime was calculated on the global
dataset and separately for each subset.9
Figure 1 displays the distribution of the accu-
racy scores of all tested models in the task, on the
global dataset. All accuracy values are specified
as percentages. Minimum, maximum, mean and
standard deviation of the accuracy values for the
global dataset and for the three subsets are dis-
played in table 2.
The mean performance on N-N is lower than on
9The small number of triples for which no prediction
could be made because of missing words in the DSMs were
considered mistakes. The coverage of the models ranges from
97.8% to 100% of the triples, with a mean of 99%.
69
0500
1000
1500
2000
50 60 70 80 90 100
Figure 1: Identification of congruent prime: distri-
bution of accuracy (%) for global dataset
Dataset Min Max Mean ?
Global 50.2 96.5 80.2 9.2
V-N 45.8 95.8 80.0 8.4
N-V 49.1 99.1 82.7 9.7
N-N 47.6 97.6 78.7 10.0
Table 2: Identification of congruent prime: mean
and range for global dataset and subsets
N-V and slightly lower than on V-N. This effect
may be interpreted as being due to mediated prim-
ing, as no verb is explicitly involved in the N-N
relationship. Yet, the relatively high accuracy on
N-N and its relatively small difference from N-V
and V-N does not speak in favor of a different un-
derlying mechanism responsible for this effect. In-
deed, McKoon and Ratcliff (1992) suggested that
effects traditionally considered as instances of me-
diated priming are not due to activation spreading
through a mediating node, but result from a direct
but weaker relatedness between prime and target
words. This hypothesis found computational sup-
port in McDonald and Lowe (2000).10
4.1.1 Model Parameters and Accuracy
The aim of this section is to assess which param-
eters have the most significant impact on the per-
formance of DSMs in the task of identification of
the congruent prime.
We trained a linear model with the eight DSM
parameters as independent variables (R2 = 0.70)
and a second model that also includes all two-way
interactions (R2 = 0.89). Given the improvement
in R2 as a consequence of the inclusion of two-way
interactions in the linear model, we will focus on
the results from the model with interactions. Table
3 shows results from the analysis of variance for
10The interpretation of the N-N results in terms of spread-
ing activation is also rejected by Hare et al (2009, 163).
the model with interactions. For every parameter
(and interaction of parameters) we report degrees
of freedom (df ), percentage of explained variance
(R2), and a significance code (signif ). We only
list significant interactions that explain at least 1%
of the variance. Even though all parameters and
many interactions are highly significant due to the
large number of DSMs that were tested, an analy-
sis of their predictive power in terms of explained
variance allows us to make distinctions between
parameters.
Parameter df R2 signif
corpus 4 7.44 ***
window 2 4.39 ***
pos 2 0.92 ***
score 5 7.39 ***
transformation 3 3.79 ***
distance 2 22.20 ***
dimensionality reduction 2 10.52 ***
relatedness index 3 13.67 ***
score:transformation 15 4.53 ***
distance:relatedness index 12 2.24 ***
distance:dim.reduction 4 2.16 ***
window:dim.reduction 4 1.73 ***
Table 3: Accuracy: Parameters and interactions
Results in table 3 indicate that distance, dimen-
sionality reduction and relatedness index are the
parameters with the strongest explanatory power,
followed by corpus and score. Window and trans-
formation have a weaker explanatory power, while
pos falls below the 1% threshold. There is a
strong interaction between score and transforma-
tion, which has more influence than one of the in-
dividual parameters, namely transformation.
Figures 2 to 7 display the partial effects of dif-
ferent model parameters (pos was excluded be-
cause of its low explanatory power). One of the
main research questions behind this work was
whether neighbor rank performs better than dis-
tance in predicting priming data. The partial ef-
fect of relatedness index in Figure 6 confirms our
hypothesis: forward rank achieves the best perfor-
mance, distance the worst.11
Accuracy improves for models trained on big-
ger corpora (parameter corpus, figure 2; corpora
are ordered by size) and larger context windows
(parameter window, figure 3). Cosine is the best
performing distance measure (figure 4). Interest-
ingly, dimensionality reduction is found to neg-
atively affect model performance: as shown in
figure 7, both random indexing (ri) and singular
11Backward rank is equivalent to distance in this task.
70
74
76
78
80
82
84
86
bnc wp500 wacky ukwac joint
l
l
l
l
l
Figure 2: Corpus
74
76
78
80
82
84
86
2 5 15
l
l
l
Figure 3: Window
74
76
78
80
82
84
86
cos eucl man
l
l
l
Figure 4: Distance
l l
l
l
l
l
no
ne
freq Dice MI s?ll t?sc z?sc
74
76
78
80
82
84
86
l none
log
root
sigmoid
Figure 5: Score + Transformation
74
76
78
80
82
84
86
dist back_rank forw_rank avg_rank
l l
l
l
Figure 6: Rel. Index
74
76
78
80
82
84
86
none ri rsvd
l
l
l
Figure 7: Dim. Reduction
value decomposition (rsvd) cause a decrease in
predicted accuracy.
Because of the strong interaction between score
and transformation, only their combined effect
is shown (figure 5). Among the scoring mea-
sures, stochastic association measures perform
better than frequency: in particular log-likelihood
(simple-ll), z-score and t-score are the best mea-
sures. We can identify a general tendency of trans-
formation to lower accuracy. This is true for all
scores except log-likelihood: square root and (to a
lesser extent) logarithmic transformation result in
an improvement for this measure.
Figure 8 displays the interaction between the
parameters distance and dimensionality reduction.
Despite a general tendency for dimensionality re-
duction to lower accuracy, we found an interac-
tion between cosine distance and singular value
decomposition: in this combination, accuracy re-
mains stable and is even minimally higher com-
pared to no dimensionality reduction.
l l
l
cos eucl man
68
70
72
74
76
78
80
82
84
86
88
l none
ri
rsvd
Figure 8: Distance + Dimensionality Reduction
4.2 Correlation to Reaction Times
The results reported in section 4.1 demonstrate
that forward rank is the best index for identifying
which of the two primes is the congruent one. The
aim of this section is to find out whether rank is
also a good predictor of latency times. We check
correlation between distributional relatedness and
reaction times and evaluate the impact of model
parameters on this task.
Figure 9 displays the distribution of Pearson
correlation coefficient achieved by the different
DSMs on the global dataset.
0
500
1000
1500
0.0 0.1 0.2 0.3 0.4 0.5
Figure 9: Distribution of Pearson correlation be-
tween relatedness and RT in the global dataset
Figure 9 shows that the majority of the models
perform rather poorly, and that only few models
achieve moderate correlation with RT. DSM per-
71
formance in the correlation task appears to be less
robust to non-optimal parameter settings than in
the accuracy task (cf. figure 1).
Minimum, maximum, mean and standard devi-
ation correlation for the global dataset and for the
three evaluation subsets are shown in table 4. In all
the cases, absolute correlation values are used so
as not to distinguish between positive and negative
correlation.
Dataset Min Max Mean ?
Global -0.26 0.47 0.19 0.10
V-N -0.34 0.57 0.2 0.12
N-V -0.35 0.41 0.11 0.06
N-N -0.29 0.42 0.16 0.09
Table 4: Mean and range of Pearson correlation
coefficients on global dataset and subsets
4.2.1 Model Parameters and Correlation
In this section we discuss the impact of differ-
ent model parameters on correlation with reaction
times.
We trained a linear model with absolute Pearson
correlation on the global dataset as dependent vari-
able and the eight DSM parameters as independent
variables (R2 = 0.53), and a second model that in-
cludes two-way interactions (R2 = 0.77). Table
5 is based on the model with interactions; it re-
ports the degrees of freedom (df ), proportion of
explained variance (R2) and a significance code
(signif ) for every parameter and every interaction
of parameters (above 1% of explained variance).
Parameter df R2 signif
corpus 4 7.45 ***
window 2 0.47 ***
pos 2 0.20 ***
score 5 3.03 ***
transformation 3 3.52 ***
distance 2 4.27 ***
dimensionality reduction 2 10.57 ***
relatedness index 3 23.40 ***
dim.reduction:relatedness index 6 5.21 ***
distance:dim.reduction 4 4.11 ***
distance:relatedness index 6 3.77 ***
score:transformation 15 3.22 ***
score:relatedness index 15 1.37 ***
Table 5: Correlation: Parameters and interactions
Relatedness index is the most important param-
eter, followed by dimensionality reduction and
corpus. The explanatory power of the other pa-
rameters (score, transformation, distance) is lower
than for the accuracy task, and two parameters
(window and pos) explain less than 1% of the vari-
ance each. By contrast, the explanatory power of
interactions is higher in this task. Table 5 shows
the five relevant interactions with an overall higher
R2 compared to the accuracy task (cf. table 3).
The partial effect plot for relatedness index (fig-
ure 14) confirms the findings of the accuracy task:
forward rank is the best value for this parameter.
The best values for the other parameters, however,
show opposite tendencies with respect to the accu-
racy task. Models trained on smaller corpora (fig-
ure 10) perform better than those trained on big-
ger ones. Cosine is still the best distance measure,
but manhattan distance performs equally well in
this task (parameter distance, figure 12). Singu-
lar value decomposition (parameter dimensional-
ity reduction, figure 15) weakens the correlation
values achieved by the models, but no significant
difference is found between random indexing and
the unreduced data.
Co-occurrence frequency performs better than
statistical association measures and transforma-
tion improves correlation: figure 13 displays the
interaction between these two parameters. Trans-
formation has a positive effect for every score, but
the optimal transformation differs. Its impact is
particularly strong for the Dice coefficient, which
reaches the same performance as frequency when
combined with a square root transformation.
Let us conclude by discussing the interaction
between distance and dimensionality reduction
(figure 16). Based on the partial effects of the indi-
vidual parameters, any combination of manhattan
or cosine distance with random indexing or no di-
mensionality reduction should be close to optimal.
However, the interaction plot reveals that manhat-
tan distance with random indexing is the best com-
bination, outperforming the second best (cosine
without dimensionality reduction) by a consider-
able margin. The positive effect of random index-
ing is quite surprising and will require further in-
vestigation.
l l l
cos eucl man
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
l none
ri
rsvd
Figure 16: Distance + Dimensionality Reduction
72
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
bnc wp500 wacky ukwac joint
l
l l l
l
Figure 10: Corpus
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
2 5 15
l l
l
Figure 11: Window
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
cos eucl man
l
l
l
Figure 12: Distance
l l
l
l
l
l
freq Dice MI s?ll t?sc z?sc
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
l none
log
root
sigmoid
Figure 13: Score + Transformation
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
dist back_rank forw_rank avg_rank
l
l
l
l
Figure 14: Rel. Index
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
none ri rsvd
l l
l
Figure 15: Dim. Reduction
5 Conclusion
In this paper, we presented the results of a large-
scale evaluation of distributional models and their
parameters on behavioral data from priming ex-
periments. Our study is, to the best of our knowl-
edge, the first systematic evaluation of such a wide
range of DSM parameters in all possible combina-
tions. Our study also provides a methodological
contribution to the problem of DSM evaluation.
We propose to apply linear modeling to determine
the impact of different model parameters and their
interactions on the performance of the models. We
believe that this type of analysis is robust against
overfitting. Moreover, effects can be tested for
significance and various forms of interactions be-
tween model parameters can be captured.
The main findings of our evaluation can be sum-
marized as follows. Forward association (rank of
target among the nearest neighbors of the prime)
performs better than distance in both tasks at is-
sue: identification of congruent prime and correla-
tion with latency times. This finding confirms and
extends the results of previous studies (Hare et al,
2009). The relevance of rank-based measures for
cognitive modeling is discussed in section 3.2.2.
Identification of congruent primes on the ba-
sis of distributional relatedness between prime and
target is improved by employing bigger corpora
and by using statistical association measures as
scoring functions, while correlation to reaction
times is strengthened by smaller corpora and co-
occurrence frequency or Dice coefficient. A sig-
nificant interaction between transformation and
scoring function is found in both tasks: consider-
ing the interaction between these two parameters
turned out to be vital for the identification of opti-
mal parameter values.
Some preliminary analyses of individual the-
matic relations showed substantial improvements
of correlations. Therefore, future work will focus
on finer-grained linear models for single relations
and on further modeling of reaction times, extend-
ing the study by Hutchinson et al (2008).
Further research steps also include an evalua-
tion of syntax-based models (Baroni and Lenci,
2010; Pado? and Lapata, 2007) and term-document
models on the tasks tackled in this paper, as well
as an evaluation of all models on standard tasks.
Acknowledgments
We are grateful to Ken MacRae for providing us
the priming data modeled here and to Alessandro
Lenci for his contribution to the development of
this study. We would also like to thank the Com-
putational Linguistics group at the University of
Osnabru?ck and the Corpus Linguistics group at the
University Erlangen for feedback. Thanks also go
to three anonymous reviewers, whose comments
helped improve our analysis, and to Sascha Alex-
eyenko for helpful advice. The first author?s PhD
project is funded by a Lichtenberg grant from the
Ministry of Science and Culture of Lower Saxony.
73
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How
we blessed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
GEMS ?11, pages 1?10. Association for Computa-
tional Linguistics.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting semantic representations from word co-
occurrence statistics: stop-lists, stemming and svd.
Behavior Research Methods, 44:890?907.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
IMS, University of Stuttgart.
Todd Ferretti, Ken McRae, and Ann Hatherell. 2001.
Integrating verbs, situation schemas, and thematic
role concepts. Journal of Memory and Language,
44(4):516?547.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:211?244.
Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical Report 2009-05,
ACM, California Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah
Kelly, and Ken McRae. 2009. Activating event
knowledge. Cognition, 111(2):151?167.
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Amac Herdag?delen, Marco Baroni, and Katrin Erk.
2009. Measuring semantic relatedness with vector
space models and random walks. In Proceedings
of the 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 50?53.
Keith A. Hutchinson, David A. Balota, Michael J.
Cortese, and Jason M. Watson. 2008. Predicting
semantic priming at the item level. The Quarterly
Journal of Experimental Psychology, 61(7):1036?
1066.
Michael Jones and Douglas Mewhort. 2007. Repre-
senting word meaning and order information in a
composite holographic lexicon. Psychological Re-
view, 114:1?37.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
Will Lowe and Scott McDonald. 2000. The direct
route: mediated priming in semantic space. Tech-
nical report, Division of Informatics, University of
Edinburgh.
Scott McDonald and Chris Brew. 2004. A distribu-
tional model of semantic context effects in lexical
processing. In Proceedings of ACL-04, pages 17?
24.
Gain McKoon and Roger Ratcliff. 1992. Spreading ac-
tivation versus compound cue accounts of priming:
Mediated priming revisited. Journal of Experimen-
tal Psychology: Learning, Memory and Cognition,
18:1155?1172.
Ken McRae and Kazunaga Matzuki. 2009. People use
their knowledge of common events to understand
language, and do so as quickly as possible. Lan-
guage and Linguistics Compass, 3(6):1417?1429.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd
Ferretti. 2005. A basis for generating expectan-
cies for verbs from nouns. Memory & Cognition,
33(7):1174?1184.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Proceedings of the Methods and Appli-
cations of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering, TKE 2005.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, University
of Stockolm.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84:327?352.
74
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 50?59,
Dublin, Ireland, August 23, 2014.
NaDiR: Naive Distributional Response Generation
Institut f?ur
Maschinelle Sprachverarbeitung
Universit?at Stuttgart
Gabriella Lapesa
Institut f?ur
Kognitionswissenschaft
Universit?at Osnabr?uck
glapesa@uos.de
Stefan Evert
Professur f?ur
Korpuslinguistik
FAU Erlangen-N?urnberg
stefan.evert@fau.de
Abstract
This paper describes NaDiR (Naive DIstributional Response generation), a corpus-based system
that, from a set of word stimuli as an input, generates a response word relying on association
strength and distributional similarity. NaDiR participated in the CogALex 2014 shared task on
multiword associations (restricted systems track), operationalizing the task as a ranking problem:
candidate words from a large vocabulary are ranked by their average association or similarity to
a given set of stimuli. We also report on a number of experiments conducted on the shared
task data, comparing first-order models (based on co-occurrence and statistical association) to
second-order models (based on distributional similarity).
1 Introduction
This paper describes NaDiR, a corpus-based system designed for the reverse association task. NaDiR
is an acronym for Naive Distributional Response generation. NaDiR is naive because it is based on a
very simple algorithm that operationalizes the multiword association task as a ranking problem: candi-
date words from a large vocabulary are ranked by their average statistical association or distributional
similarity to a given set of stimuli, then the highest-ranked candidate is selected as NaDiR?s response.
We compare models based on collocations (first-order models, see Evert (2008) for an overview) to
models based on distributional similarity (second-order models; see Sahlgren (2006), Turney and Pan-
tel (2010), and reference therein for a review). Previous work on this task showed that co-occurrence
models outperform distributional semantic models (henceforth, DSMs), and that using rank measures
improves performance because it accounts for directionality of the association/similarity (e.g., the asso-
ciation from stimulus to response may be larger than the association from response to stimulus). Our
results corroborate both claims.
The paper is structured as follows: section 2 provides an overview of the task and of the problems
we encountered in its implementation; section 3 summarizes related work; section 4 describes NaDiR in
detail; section 5 reports the results of our experiments on the shared task training and test data; section 6
describes ongoing and future work on NaDiR.
2 The Task and its Problems
The shared task datasets are derived from the Edinburgh Associative Thesaurus (Kiss et al., 1973)
1
. The
Edinburgh Associative Thesaurus (henceforth, EAT) contains free associations to approximately 8000
English cue words. For each cue (e.g., visual) EAT lists all associations collected in the survey (e.g., aid,
eyes, aids, see, eye, seen, sight, etc.) sorted according to the number of subjects who responded with the
respective word. The CogALex shared task on multiword association is based on the EAT dataset, and
is in fact a reverse association task (Rapp, 2014). The top five responses for a target word are provided
as stimuli (e.g., aid, eyes, aids, see, eye), and the participating systems are required to generate the
original cue as a response (e.g., visual). The training and the test sets are random extracts of 2000 EAT
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.eat.rl.ac.uk/
50
items each, with minimal pre-processing (only items containing multiword units and non-alphabetical
characters were discarded).
A key problem we had to tackle while developing our system was the unrestricted set of possible re-
sponses in combination with a discrete association task, which requires the algorithm to pick exactly the
right answer out of tens of thousands of possible responses. This feature makes this task much more dif-
ficult than the multiple-choice tasks often used to evaluate distributional semantic models. The problem
is further complicated by the fact that the response may be an inflected form and only a prediction of the
exact form was accepted as a correct answer. The need for a solution to these issues motivates various
aspects of the NaDiR algorithm, described in section 4.
3 Related Work
Previous studies based on free association norms differ considerably in terms of the type of task (regular
free association task ? one stimulus, one response vs. multiword association task ? many stimuli, one
response), gold standards, and key features of the evaluated models (e.g., source corpora used and choice
of a candidate vocabulary from which responses are selected).
In regular free association tasks (one stimulus, one response), responses are known to contain both
paradigmatically and syntagmatically related words. Rapp (2002) proposes to integrate first-order (co-
occurrence lists) and second-order (bag-of-words DSMs) information to distinguish syntagmatic from
paradigmatic relations by exploiting the comparison of most salient collocates and nearest neighbors.
A task derived from the EAT norms was used in the ESSLLI 2008 shared task
2
. Results from first-
order co-occurrence data turned out to be much better than those from second-order DSMs, in line with
the findings made by Rapp (2002) and Wettler et al. (2005).
A similar picture emerges from studies on the multiword association task. Models based on first-order
co-occurrence (collocations) outperform models based on vector similarity. This superiority, however, is
not validated via a direct comparison: results were obtained by studies with different features and goals
(see Rapp (2014) for a review; see Griffiths et al. (2007) and Smith et al. (2013) for evaluations of
models based on Latent Semantic Analysis). A specific feature of successful studies on the multiword
association task is that they introduce an element of directionality (Rapp, 2013; Rapp, 2014), which
allows a correct implementation of the directionality of the modeled effects (from stimulus to response).
Our survey of related studies motivated the choice to base NaDiR on first-order or second-order co-
occurrence statistics, and to use collocate or neighbor rank to account for directionality. Our main contri-
bution to research on the reverse association task is a systematic experimental comparison of first-order
and second-order models (using the same gold standard, same source corpus, and same candidate vocab-
ulary), which enables us to give a sound answer to the question whether first-order models are indeed
superior for multiword association tasks.
4 NaDiR
NaDiR operationalizes the multiword association task as a ranking problem. For each set of stimuli,
the possible response words (?candidates?) are ranked according to their average association strength or
distributional similarity to the stimulus words. The top-ranked candidate is selected as NaDiR?s response.
One advantage of the ranking approach is that it provides additional insights into the experimental results:
if the model prediction is not correct, the rank of the correct answer can be used as a measure how ?close?
the model came to the human associations.
Since neither a fixed set of response candidates nor an indication of the source of the training and
test data were available (and we did not google for the training sets), we compiled a large vocabulary of
possible responses. We believe that restricting the vocabulary to the 8,033 cue words in the EAT would
have improved our results considerably. More details concerning the choice of the candidate vocabulary
are reported in section 4.1.
2
http://wordspace.collocations.de/doku.php/data:esslli2008:correlation with free
association norms
51
NaDiR uses either first-order or second-order co-occurrence statistics to predict the association
strength between stimuli and responses. In the first case (?collocations?), we apply one of several stan-
dard statistical association measures to co-occurrence counts obtained from a large corpus. In the second
case, association is quantified by cosine similarity in a distributional semantic model built from the same
corpus. Both first-order and second-order statistics were collected from UKWaC in order to compete in
the constrained track of the shared task.
Recent experiments (Hare et al., 2009; Lapesa and Evert, 2013; Lapesa et al., to appear) suggest
that semantic relations are often better captured by neighbour ranks rather than direct use of statistical
association measures or cosine similarity values. Therefore, NaDiR can alternatively quantify association
strength by collocate rank and similarity by neighbour rank. In our experiments (section 5), we compare
the different approaches.
NaDiR is designed for the multiword association task, and it contains additional features related to the
particular design of the CogALex shared task:
? We reduce the number of candidates by selecting the most likely response POS with a machine-
learning algorithm (section 4.1);
? NaDiR operates on lemmatized data in order to reduce sparseness. We lemmatize stimuli using a
heuristic method (section 4.1), predict a response lemma, and then use machine-learning techniques
to generate a plausible word form (section 4.3).
4.1 Pre-processing and Vocabulary
Our experiments were conducted on the UKWaC
3
corpus. UKWaC contains 2 billion words, web-
crawled from the .uk domain between 2005 and 2007. The release of UKWaC also contains linguistic
annotation (pos-tagging and lemmatization) performed with Tree Tagger
4
.
To assign a part-of-speech tag and a lemma to every word in the dataset without relying on external
tools, we adopted the following mapping strategy based on the linguistic annotation already available in
UKWaC:
1. We extracted all attested wordform/part of speech/lemma combinations from UKWaC, together
with their frequency;
2. Every word form in the training set was assigned to the most frequent part of speech/lemma combi-
nation attested in UKWaC.
We believe that the advantages of constructing distributional models based on lemmatized words over-
come the drawbacks of this type of out-of-context lemmatization and part-of-speech assignment.
The part-of-speech information added to every word in the dataset by the mapping procedure was
used to train a classifier that, given the parts of speech of the stimuli, predicts the part of speech of the
response. We trained a support-vector machine, using the svm function from the R package e1071
5
,
with standard settings.
The part-of-speech classifier is based on a coarse part-of-speech tagset with only five tags: N (noun),
J (adjective), V (verb), R (adverb), other (closed-class words). We considered each row of the dataset
as an observation, with the part of speech of the response as predicted value, and the part of speech of
the stimulus words as predictors. Every observation is represented as a bag of tags, i.e., a vector listing
for each of the five tags how often it occurs among the stimuli. For example, if a set of stimuli contains
3 nouns, one verb and one adjective, the corresponding bag-of-tags vector looks as follows: {N = 3; V =
1; J = 1; R = 0; other = 0}. On the training set, the part-of-speech classifier achieves an accuracy of
72%.
The vocabulary of our models only contains lemmatized open-class words (this information is avail-
able in the annotation of the corpus). By inspecting the frequencies of stimuli and response words in the
training dataset, we established a reasonable minimum frequency threshold for candidate words of 100
occurrences in UKWaC. With this threshold, only 10 response words and 16 stimulus words from the
3
wacky.sslmit.unibo.it/doku.php?id=corpora
4
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
5
http://cran.r-project.org/web/packages/e1071/index.html
52
training dataset are excluded from the vocabulary. Given the large size of the dataset, we decided that
a minimal loss in coverage would be justified by the reduced computational complexity. The resulting
candidate vocabulary contains 155,811 words.
4.2 First- and Second-order Statistics
The aim of this section is to describe the parameters involved in the collection of first-order and second-
order statistics from UKWaC. All models have been built and evaluated using the UCS toolkit
6
and the
wordspace package for R (Evert, to appear)
7
.
First-order Models
Collocation data are compiled from UKWaC based on the vocabulary described in section 4.1. Both
nodes (rows of the co-occurrence matrix) and collocates (columns of the co-occurrence matrix) are cho-
sen from this vocabulary. Collection of first-order models involved the manipulation of a number of
parameters, briefly summarized below.
We adopted three different window sizes:
? symmetric window, 2 words to the left and to the right of the node;
? asymmetric window, 3 words to the left of the node;
? asymmetric window, 3 words to the right of the node.
We tested the following association scores (Evert, 2008):
? co-occurrence frequency;
? simple log-likelihood (similar to local MI used by Baroni and Lenci (2010));
? conditional probability.
Our experiments involved a third parameter, the index of association strength, which determines al-
ternative ways of quantifying the degree of association between targets and contexts in the first-order
model. Given two words a and b represented in a first-order model, we propose two alternative ways of
quantifying the degree of association between a and b. The first option (and standard in corpus-based
modeling) is to compute the association score between a and b. The alternative choice is based on rank
among collocates. Given two words a and b, in our task stimulus and potential response, we consider:
? forward rank: the rank of the potential response among the collocates of the stimulus;
? backward rank: the rank of the stimulus among the collocates of the potential response;
? average rank: the average of forward and backward rank.
Second-order Models
Based on the results of a large-scale evaluation of DSM parameters (Lapesa and Evert, under review)
and the modeling of semantic priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear), we
identified a robust configuration of parameters for second-order models that we decided to adopt in this
study. Second-order models involved in our experiments share the following parameter settings:
? The target words (rows) are defined by the vocabulary described in section 4.1.
? The context words (columns) are the 50,000 most frequent context words in the respective co-
occurrence matrices. The 50 most frequent words in UKWaC are discarded.
? Co-occurrence vectors are scored with a sparse version of simple-log likelihood, in which negative
values clamped to zero in order to preserve the sparseness of the co-occurrence matrix. Scored
vectors are rescaled by applying a logarithmic transformation.
? We reduce the scored co-occurrence matrix to 1000 latent dimensions using randomized SVD
(Halko et al., 2009).
? We adopt cosine distance (i.e. the angle between vectors) as a distance metric for the computation
of vector similarity.
6
http://www.collocations.de/software.html
7
http://r-forge.r-project.org/projects/wordspace/
53
Our experiments on second-order models involved the manipulation of two parameters: window size
and index of association strength.
The size of the context window quantifies the amount of shared context involved in the computation of
similarity. We expect the manipulation of window size to be crucial in determining model performance,
as different context windows will enable the model to capture different types of relations between re-
sponse and stimulus words (Sahlgren, 2006; Lapesa et al., to appear). In our experiments with NaDiR,
we adopted three different window sizes:
? symmetric window, 2 words to the left and to the right of the target;
? symmetric window, 4 words to the left and to the right of the target;
? symmetric window, 16 words to the left and to the right of the target.
The values for index of association strength are the same as for the first-order models, computing ranks
among the nearest neighbors of the stimulus or response word. The use of rank-based measures is of
particular interest, because: (i) it allows us to model directionality (while, for example, cosine distance is
symmetric); (ii) it already proved successful in modeling behavioral data (Hare et al., 2009; Lapesa and
Evert, 2013); (iii) since the vocabulary of first-order and second-order models are identical, rank-based
measures allow a direct comparison between the two classes of models, as well as experiments based on
their combination.
4.3 Response Generation
To generate a response for a set of stimuli in the training/test dataset, we apply the following procedure:
1. For each set of stimuli, we compute association strengths or similarities between each stimulus and
each response candidate, adopting one of the measures described in section 4.2.
2. From the set of potential responses, we select the words whose POS agrees with the predictions of
the classifier described in section 4.1. Stimulus words are discarded from the potential answers.
3. We compute the average association strength or similarity across all five stimuli; if a stimulus does
not appear in the model, it is simply omitted from the average.
4. The top-ranked candidate is the POS-disambiguated lemma suggested as a response by NaDiR.
5. We generate a suitable word form by inverting the heuristic lemmatization; if the full Penn tag (e.g.,
NNS: noun, common, plural; NN: noun, common, singular or mass, etc.) of the response is known,
this step can be implemented as a deterministic lookup (since a word form is usually determined
uniquely by lemma and Penn tag). We therefore trained a second SVM classifier that predicts the
full Penn tag of the response based on the full tags of the stimuli. On the training set, this part-of-
speech classifier reaches an accuracy of 68%.
5 Experiments
In our experiments, we compared first-order (collocations) and second-order (DSM) models; for each
class of models, we evaluated the different parameter values described in section 4.2. Table 1 summarizes
the evaluated parameters for first-order and second-order models.
Model Window Score Relatedness Index
first-order symmetric, 2 frequency association score
left 3, right 0 simple log-likelihood forward rank
left 0, right 3 conditional probability backward rank
average rank
second-order symmetric, 2 simple log-likelihood distance
symmetric, 4 forward rank
symmetric, 16 backward rank
average rank
Table 1: Evaluated Parameters for First- and Second-order Models
54
Tables 2 to 5 display the results of our experiments on the training data, separately for first-order (tables
2-4) and second-order models (table 5). Parameter configurations are reported in the Parameter column
8
.
The number of correct responses in the lemmatized version is reported in the column Lemma (showing
how often our system predicted the correct lemma). The column Wordform reports the number of correct
responses for which, before inverting the lemmatization, the inflected form was already identical to the
lemma. As the task of predicting exactly one word is particularly difficult, we further characterize the
performance of our evaluated models by reporting the number of cases in which the correct answer from
the training set was among the first 10 (< 10), 50 (< 50), or 100 (< 100) ranked candidates. In the last
column, we report the average rank of the correct responses (Avg correct).
The results reported in tables 2 to 5 allowed us to identify best parameter configurations for the first-
order (symmetric 2 words window, frequency, backward rank) and second-order models (2 words win-
dow, distance). We evaluated these configurations on the test data (table 6). Table 7 compares the
performance of the best first-order and the best second-order model on the training and test datasets,
both for lemmatized response (Training-Lemma, Test-Lemma) and generation of the correct word form
(Training-Inflected, Test-Inflected).
A considerable portion of the experiments reported in this paper were conducted after the submission
deadline of the CogALex shared task. As a consequence, our submitted results do not correspond to the
best overall configuration found in the evaluation study. The submission was based on a second order
model, a 4-word window, and cosine distance as index of distributional similarity. In this configuration,
NaDiR generated 262 correct responses, corresponding to an accuracy of 13%.
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
2 2 85 372 561 1400
Freq
fwd
0 0 77 359 550 6258
Freq
bwd
555 464 973 1269 1369 1546
Freq
avg
424 322 677 848 934 5969
Simple-ll
ass
33 28 237 721 985 933
Simple-ll
fwd
405 319 760 916 947 12031
Simple-ll
bwd
531 444 914 1141 1253 1971
Simple-ll
avg
490 388 785 918 950 11645
Cond.prob
ass
18 16 329 746 970 978
Cond.prob
fwd
0 0 77 359 550 6258
Cond.prob
bwd
422 359 856 1129 1255 1719
Cond.prob
avg
343 256 611 860 971 5948
Table 2: First Order Models - Symmetric Window: 2 words to the left/right of the node - Training Data
5.1 Discussion
The results of our experiments are in line with the tendencies identified in the literature (see section
3). First-order models based on direct co-occurrence (high scores are assigned to words that co-occur),
outperform second-order models based on distributional similarity (smaller distances between words that
occur in similar contexts).
For the first-order models, the best index of association strength is backward rank (the rank of the
stimulus among the collocates of the potential response), fully congruent with the experimental setting
(in the EAT norm, subjects produced the stimuli as free associations of the expected response). Surpris-
ingly, frequency outperforms simple-log likelihood (which is usually considered to be among the best
association measures for the identification of collocations). In line with the results achieved by Rapp
(2014), a symmetric window of 2 words to the left and to the right of the target achieves best results.
For the second-order models, the smallest context window (2 words) achieves the best performance.
8
Abbreviations used in the tables: ass = association score; dist = distance; fwd = forward rank; bwd = backward rank; avg
= average rank.
55
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
1 1 63 279 450 1733
Freq
fwd
0 0 32 219 395 7575
Freq
bwd
358 292 789 1124 1247 1974
Freq
avg
277 191 515 690 793 7251
Simple-ll
ass
23 18 196 618 878 1259
Simple-ll
fwd
271 196 605 789 842 14177
Simple-ll
bwd
369 296 737 1002 1135 2848
Simple-ll
avg
346 251 636 798 845 13760
Cond.prob
ass
7 6 209 588 806 1234
Cond.prob
fwd
0 0 32 219 395 7575
Cond.prob
bwd
284 230 659 974 1109 2318
Cond.prob
avg
201 137 462 711 851 7230
Table 3: First Order Models ? Asymmetric Window: 3 words to the left of the node ? Training Data
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
1 1 63 279 450 1733
Freq
fwd
0 0 32 219 395 7575
Freq
bwd
358 292 789 1124 1247 1974
Freq
avg
277 191 515 690 793 7251
Simple-ll
ass
25 22 220 643 891 1168
Simple-ll
fwd
321 250 708 895 936 12244
Simple-ll
bwd
507 424 884 1142 1246 2223
Simple-ll
avg
402 314 740 901 939 11868
Cond.prob
ass
26 20 279 665 864 1282
Cond.prob
fwd
0 0 59 298 498 7543
Cond.prob
bwd
381 319 791 1094 1201 1981
Cond.prob
avg
278 209 535 800 922 7214
Table 4: First Order Models ? Asymmetric Window: 3 words to the right of the node ? Training Data
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
2
dist
264 208 686 1077 1224 936
2
fwd
127 83 380 703 849 1560
2
bwd
73 56 275 584 720 3524
2
avg
157 106 436 750 911 1507
4
dist
255 200 665 1037 1195 997
4
fwd
108 73 338 651 824 1750
4
bwd
77 57 254 545 694 3843
4
avg
129 87 397 710 862 1694
16
dist
206 158 546 910 1062 1433
16
fwd
63 40 252 512 667 2481
16
bwd
49 37 188 449 581 4949
16
avg
79 56 282 560 713 2416
Table 5: Second order models ? Training data
Considering the good results from collocation-based models, we would have expected a better perfor-
mance from larger windows, traditionally considered to be more sensitive to syntagmatic relations. A
significant difference between first-order and second-order models is the fact that neighbor rank works
less well than the distance between vectors, while collocate rank outperformed the association scores.
56
Model Lemma Wordform < 10 < 50 < 100 Avg correct
first-order 572 490 1010 1303 1408 1366
second-order 304 246 734 1119 1256 569
Table 6: Best models (first order and second order) ? Performance on test data
Model Training-Lemma Training-Inflected Test-Lemma Test-Inflected
first-order 27.7% (555) 26.9% (538) 28.6% (572) 27.7% (554)
second-order 13.2% (264) 12.0% (241) 15.0% (304) 14.0% (279)
Table 7: Performance (% accuracy and number of correct responses) of the best first-order and second-
order model on training vs. test dataset (lemmatized response vs. response with restored inflection)
The observation for second-order models contrasts with previous work showing that rank consistently
outperforms distance in modeling priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear) and
also in standard tasks such as prediction of similarity ratings and noun clustering (Lapesa and Evert, un-
der review). Among the standard tasks, the only case in which the use of neighbor rank did not produce
significant improvements with respect to vector distance was the TOEFL multiple-choice synonymy task.
Despite clear differences, the TOEFL task and the reverse association task share the property that they
involve multiple stimuli. The results presented in this paper, together with those achieved on the TOEFL
task, seem to suggest that a better strategy for the use of neighbor rank needs to be developed when
multiple stimuli are involved.
6 Conclusions and Future Work
The results of the evaluation reported in this paper confirmed the tendencies identified in previous studies:
first-order models, based on direct co-occurrence, outperform second-order models, based on distribu-
tional similarity. We consider the experimental results described in this paper as a first exploration into
the dynamics of the reverse association task, and we believe that our systematic evaluation of first- and
second-order models represents a good starting point for future work, which targets improvements of
NaDiR at many levels.
The first point of improvement concerns the size of the vocabulary. We aim at finding a more op-
timal cutoff on the training data, for example by implementing a frequency bias similar to Wettler et
al. (2005). We are confident that NaDiR will significantly benefit from a smaller range of potential
responses (compared to the 155,811 lemmatized candidate words in the current version).
We are also conducting experiments using log ranks instead of plain ranks: since we compute an arith-
metic mean of the rank values, a single very high rank (from a poorly matched stimulus) will dominate
the average. We therefore assume that log ranks will improve results and make NaDiR?s responses more
robust.
An interesting research direction targets the integration of first- and second-order statistics in the pro-
cess of response generation. The evaluation results reported in this paper revealed that a very small
context window achieves the best performance for second-order models: as widely acknowledged in the
literature (Sahlgren, 2006; Lapesa et al., to appear), smaller context windows highlight paradigmatic
relations. First-order models, on the other hand, highlight syntagmatic relations (Rapp, 2002). The best
second-order and first-order models from the evaluation reported in this paper are likely to focus on dif-
ferent types of relations between response and stimulus words: this leads us to believe that an integration
of the two sources may produce improvements in NaDiR?s performance.
At a general level, we plan to make more elaborate use of the training data. In the experiments
presented in this paper, training data were used to set a frequency threshold for potential responses, train
the part-of-speech classifiers, and find the best configuration for first- and second-order models.
A possible new application of NaDiR is the modeling of datasets containing semantic norms or concept
properties, such as the McRae norms (McRae et al., 2005) or BLESS (Baroni and Lenci, 2011). Those
datasets are standard in DSM evaluation, and their modeling can be implemented in terms of a reverse
57
association task, with the additional advantage that the relations between concepts and properties in those
datasets are labelled with property types for the McRae norms (e.g., encyclopedic, taxonomic, situated)
or semantic relations (e.g., hypernymy, meronymy, event-related) for BLESS. This allows a specific
evaluation for each property type or semantic relation, which will in turn give new insights into the
semantic knowledge encoded in the different corpus-based representations (first order vs. second order
vs. hybrid) and how model parameters affect these representations (e.g., window size in the comparison
of syntagmatic vs. paradigmatic relations).
Acknowledgments
Gabriella Lapesa?s research is funded by the DFG Collaborative Research Centre SFB 732 (University
of Stuttgart).
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS ?11, pages 1?10.
Association for Computational Linguistics.
Stefan Evert. 2008. Corpora and collocations. In Anke L?udeling and Merja Kyt?o, editors, Corpus Linguistics. An
International Handbook, chapter 58. Mouton de Gruyter, Berlin, New York.
Stefan Evert. to appear. Distributional semantics in R with the wordspace package. In Proceedings of COLING
2014: System Demonstrations.
Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. 2009. Finding structure with randomness: Stochastic
algorithms for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California
Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae. 2009. Activating event knowledge.
Cognition, 111(2):151?167.
G. R. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973. An associative thesaurus of English and its computer
analysis. In The Computer and Literary Studies. Edinburgh University Press.
Gabriella Lapesa and Stefan Evert. 2013. Evaluating neighbor rank and distance measures as predictors of se-
mantic priming. In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics
(CMCL 2013), pages 66?74.
Gabriella Lapesa, Stefan Evert, and Sabine Schulte im Walde. to appear. Contrasting syntagmatic and paradig-
matic relations: Insights from distributional semantic models. In Proceedings of the 3rd Joint Conference on
Lexical and Computational Semantics (*SEM). Dublin, Ireland, August 2014.
Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for
a large set of living and nonliving things. Behavior Research Methods, 4(37):547?559.
Reinhard Rapp. 2002. The computation of word associations: Comparing syntagmatic and paradigmatic ap-
proaches. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, pages
1?7.
Reinhard Rapp. 2013. From stimulus to associations and back. In Proceedings of the 10th Workshop on Natural
Language Processing and Cognitive Science.
Reinhard Rapp. 2014. Corpus-based computation of reverse associations. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Evaluation (LREC?14).
Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and
paradigmatic relations between words in high-dimensional vector spaces. Ph.D. thesis, University of Stockolm.
58
Kevin A. Smith, David E. Huber, and Edward Vul. 2013. Multiply-constrained semantic search in the remote
associates test. Cognition, 128(1):64?75.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier. 2005. Free word associations correspond to contiguities
between words in texts. Journal of Quantitative Linguistics, 1(12):111?122.
59
