Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017?1024
Manchester, August 2008
Bayesian Semi-Supervised Chinese Word Segmentation
for Statistical Machine Translation
Jia Xu
?
, Jianfeng Gao
?
, Kristina Toutanova
?
, Hermann Ney
?
Computer Science 6
?
Microsoft Corporation
?
RWTH Aachen University One Microsoft Way
D-52056 Aachen, Germany Redmond, WA 98052, USA
{xujia,ney}@cs.rwth-aachen.de {jfgao,kristout}@microsoft.com
Abstract
Words in Chinese text are not naturally
separated by delimiters, which poses a
challenge to standard machine translation
(MT) systems. In MT, the widely used
approach is to apply a Chinese word seg-
menter trained from manually annotated
data, using a fixed lexicon. Such word
segmentation is not necessarily optimal
for translation. We propose a Bayesian
semi-supervised Chinese word segmenta-
tion model which uses both monolingual
and bilingual information to derive a seg-
mentation suitable for MT. Experiments
show that our method improves a state-of-
the-art MT system in a small and a large
data environment.
1 Introduction
Chinese sentences are written in the form of a se-
quence of Chinese characters, and words are not
separated by white spaces. This is different from
most European languages and poses difficulty in
many natural language processing tasks, such as
machine translation.
It is difficult to define ?correct? Chinese word
segmentation (CWS) and various definitions have
been proposed. In this work, we explore the idea
that the best segmentation depends on the task, and
concentrate on developing a CWS method for MT,
which leads to better translation performance.
The common solution in Chinese-to-English
translation has been to segment the Chinese text
using an off-the-shelf CWS method, and to apply
a standard translation model given the fixed seg-
mentation. The most widely applied method for
MT is unigram segmentation, such as segmenta-
tion using the LDC (LDC, 2003) tool, which re-
quires a manual lexicon containing a list of Chi-
nese words and their frequencies. The lexicon and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
frequencies are obtained using manually annotated
data. This method is sub-optimal for MT. For ex-
ample, ?(paper) and ](card) can be two words
or composed into one word ?](cards). Since ?
]does not exist in the manual lexicon, it cannot
be generated by this method.
In addition to unigram segmentation, other
methods have been proposed. For example, (Gao
et al, 2005) described an adaptive CWS system,
and (Andrew, 2006) employed a conditional ran-
dom field model for sequence segmentation. How-
ever, these methods are not specifically devel-
oped for the MT application, and significant im-
provements in translation performance need to be
shown.
In (Xu et al, 2004) and (Xu et al, 2005),
word segmentations are integrated into MT sys-
tems during model training and translation. We re-
fine the method in training using a Bayesian semi-
supervised CWS approach motivated by (Goldwa-
ter et al, 2006). We describe a generative model
which consists of a word model and two alignment
models, representing the monolingual and bilin-
gual information, respectively. In our methods, we
first segment Chinese text using a unigram seg-
menter, and then learn new word types and word
distributions, which are suitable for MT.
Our experiments on both large (NIST) and small
(IWSLT) data tracks of Chinese-to-English trans-
lation show that our method improves the per-
formance of a state-of-the-art machine translation
system.
2 Review of the Baseline System
2.1 Word segmentation
In statistical machine translation, we are given a
Chinese sentence in characters c
K
1
= c
1
. . . c
K
which is to be translated into an English sentence
e
I
1
= e
1
. . . e
I
. In order to obtain a more adequate
mapping between Chinese and English words, c
K
1
is usually segmented into words f
J
1
= f
1
. . . f
J
in
preprocessing.
In our baseline system, we apply the commonly
1017
used unigram model to generate the segmenta-
tion. Given a manually compiled lexicon contain-
ing words and their relative frequencies P
s
(f
?
j
),
the best segmentation f
J
1
is the one that maximizes
the joint probability of all words in the sentence,
with the assumption that words are independent of
each other
1
:
f
J
1
= argmax
f
?
J
?
1
Pr(f
?
J
?
1
|c
K
1
)
? argmax
f
?
J
?
1
J
?
?
j=1
P
s
(f
?
j
),
where the maximization is taken over Chinese
word sequences whose character sequence is c
K
1
.
2.2 Translation system
Once we have segmented the Chinese sentences
into words, we train standard alignment models
in both directions with GIZA++ (Och and Ney,
2002) using models of IBM-1 (Brown et al, 1993),
HMM (Vogel et al, 1996) and IBM-4 (Brown et
al., 1993).
Our MT system uses a phrase-based decoder
and the log-linear model described in (Zens and
Ney, 2004). Features in the log-linear model in-
clude translation models in two directions, a lan-
guage model, a distortion model and a sentence
length penalty. The feature weights are tuned on
the development set using a downhill simplex al-
gorithm (Press et al, 2002). The language model
is a statistical ngram model estimated using modi-
fied Kneser-Ney smoothing.
3 Unigram Dirichlet Process Model for
CWS
The simplest version of our model is based on a
unigram Dirichlet Process (DP) model, using only
monolingual information. Different from a stan-
dard unigram model for CWS, our model can in-
troduce new Chinese word types and learn word
distributions automatically from unlabeled data.
According to this model, a corpus of Chinese
words f
1
, . . . f
m
, . . . , f
M
is generated via:
G|?, P
0
? DP (?, P
0
)
f
m
|G ? G
where G is a distribution over words drawn from a
Dirichlet Process prior with base measure P
0
and
concentration parameter ?.
We never explicitly estimate G but instead
integrate over its possible values and perform
Bayesian inference. It is easy to compute the
1
The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol P (?).
probability of a Chinese word given a set of al-
ready generated words, while integrating over G.
This is done by casting Chinese word generation
as a Chinese restaurant process (CRP) (Aldous,
1985), i.e. a restaurant with an infinite num-
ber of tables (approximately corresponding to Chi-
nese word types), each table with infinite number
of seats (approximately corresponding to Chinese
word frequencies).
The Dirichlet Process model can be viewed in-
tuitively as a cache model (Goldwater et al, 2006).
Each word f
j
in the corpus is either retrieved from
a cache or generated anew given the previously ob-
served words f
?j
:
P (f
j
|f
?j
) =
N(f
j
)
+
?P
0
(f
j
)
N + ?
, (1)
whereN(f
j
) is the number of Chinese words f
j
in
the previous context. N is the total number of Chi-
nese words, P
0
is the base probability over words,
and ? influences the probability of introducing a
new word at each step and controls the size of the
lexicon. The probability of generating a word from
the cache increases as more instances of that word
are seen.
For the base distribution P
0
, which governs the
generation of new words, we use the following dis-
tribution (called the spelling model):
P
0
(f) = P (L)P
0
(f |L)
=
?
L
L!
e
??
u
L
(2)
where
1
u
is the number of characters in the docu-
ment, i.e. character vocabulary size, and L is the
number of Chinese characters of word f . We note
that this is a Poisson distribution on word length
and a unigram distribution on characters given the
length. We used ? = 2 and ? = 0.3 in our experi-
ments.
4 CWS Model for MT
As a solution to the problems with the conventional
approach to CWS mentioned in Section 1, we pro-
pose a generative model for CWS in Section 4.1,
and then extend the model to a more general but
deficient model, similar to a maximum entropy
model in which most features are derived from the
submodels of the generative model.
4.1 Generative Model
The generative model assume that a corpus of par-
allel sentences (c
1
K
,e
1
I
) is generated along with a
hidden sequence of Chinese words f
1
J
and a hid-
den word alignment b
1
I
for every sentence. The
alignment indicates the aligned Chinese word f
b
i
for each English word e
i
, where f
0
indicates a spe-
cial null word as in the IBM models.
1018
Without assuming any special form for the prob-
ability of a sentence pair along with hidden vari-
ables, we can factor it into a monolingual Chi-
nese sentence probability and a bilingual transla-
tion probability as follows:
Pr(c
1
K
, e
1
I
, f
1
J
, b
1
I
)
=Pr(c
K
1
, f
J
1
)Pr(e
I
1
, b
I
1
|f
J
1
)
=Pr(f
J
1
)?(f
1
J
, c
1
K
)Pr(e
I
1
, b
I
1
|f
J
1
),
where ?(f
J
1
, c
K
1
) is 1 if the characters of the se-
quence of words f
1
J
are c
1
K
, and to 0 other-
wise. We can drop the conditioning on c
1
K
in
Pr(e
I
1
, b
I
1
|f
J
1
), because the characters are deter-
ministic given the words.
The joint probability of the observations
(c
1
K
, e
I
1
) can be obtained by summing over all
possible values of the hidden variables f
J
1
and b
I
1
.
In Sections 4.1.1 and 4.1.2, we will describe
the modeling assumptions behind the monolingual
Chinese sentence model and the translation model,
respectively.
4.1.1 Monolingual Chinese sentence model
We use the Dirichlet Process unigram word
model introduced in section 3. In this model, the
parameters of a distribution over words G are first
drawn from the Dirichlet prior DP (?, P
0
). Words
are then independently generated according to G.
The probability of a sequence of Chinese words in
a sentence is thus:
Pr(f
J
1
) ?
J
?
j=1
P (f
j
|G) (3)
4.1.2 Translation model
We employ the Dirichlet Process inverse IBM
model 1 to generate English words and alignment
given the Chinese words. In this model, for every
Chinese word f (including the null word), a distri-
bution over English words G
f
is first drawn from
a Dirichlet Process prior DP (?, P
0
(e)), where
P
0
(e) we used the empirical distribution over En-
glish words in the parallel data. Then, given these
parameters, the probability of an English sentence
and alignment given a Chinese sentence (sequence
of words) is given by:
P (e
I
1
, b
I
1
|f
J
1
, G
f
) =
I
?
i=1
1
J + 1
P (e
i
|G
f
b
i
)
This is the same model form as inverse IBM
model 1, except we have placed Dirichlet Process
priors on the Chinese-word specific distributions
over English words.
2
2
f
b
i
is the Chinese word aligned to e
i
and G
f
b
i
is the
distribution over English words conditioned on the word f
b
i
.
Similarly, e
a
j
is the English word aligned to f
j
in the other di-
rection and G
e
a
j
is the distribution over Chinese words con-
ditioned on e
a
j
.
In practice, we observed that using a word-
alignment model in one direction is not sufficient.
We then added a factor to our model which in-
cludes word alignment in the other direction , i.e. a
Dirichlet Process IBM model 1. We ignore the de-
tailed description here, because the calculation is
the same as that of the inverse IBM model 1. Ac-
cording to this model, for every English word e (in-
cluding the null word), a distribution over Chinese
words G
e
is first drawn from a Dirichlet Process
prior DP (?, P
0
(f)). Here, for the base distribu-
tion P
0
(f) we used the same spelling model as for
the monolingual unigram Dirichlet Process prior.
The probability of a sequence of Chinese words
f
J
1
and a word alignment a
J
1
given a sequence of
English words e
I
1
is then:
P (f
J
1
, a
J
1
|e
I
1
, G
e
) =
J
?
j=1
1
I + 1
P (f
j
|G
e
a
j
)
4.2 Final Model
We put the monolingual model and the transla-
tion models in both directions together into a sin-
gle model, where each of the component models
is weighted by a scaling factor. This is similar to
a maximum entropy model. We fit the weights of
the sub-models on a development set by maximiz-
ing the BLEU score of the final translation.
P (c
K
1
, e
I
1
, f
J
1
, a
J
1
, b
I
1
) (4)
?
1
Z
P (f
J
1
)
?
1
? P (e
I
1
, b
I
1
|f
J
1
)
?
2
?P (f
J
1
, a
J
1
|e
I
1
)
?
3
,
where Z is the normalization factor.
In practice we do not re-normalize the proba-
bilities and our model is thus deficient because it
does not sum to 1 over valid observations. How-
ever, we found the model work very good in our
experiments. Similar deficient models have been
used very successfully before, for example, in the
IBM models 3?6 and in the unsupervised grammar
induction model of (Klein and Manning, 2002).
5 Gibbs Sampling Training
It is generally impossible to find the most likely
segmentation according to our Bayesian model us-
ing exact inference, because the hidden variables
do not allow exact computation of the integrals.
Nonetheless, it is possible to define algorithms us-
ing Markov chain Monte Carlo (MCMC) that pro-
duce a stream of samples from the posterior dis-
tribution of the hidden variables given the obser-
vations. We applied the Gibbs sampler (Geman
and Geman, 1984) ? one of the simplest MCMC
methods, in which transitions between states of the
1019
Figure 1: Case I, transition from a no-boundary to
a boundary state, f to f
?
f
??
.
Figure 2: Case II, transition from a boundary to a
no-boundary state, f
?
f
??
to f .
Markov chain result from sampling each compo-
nent of the state conditioned on the current value
of all other variables.
In our problem, the observations are D =
(d
1
, ..d
n
, .., d
N
), where d
n
=(c
K
1
, e
I
1
) indicates a
bilingual sentence pair, the hidden variables are the
word segmentations f
J
1
and the alignments in two
directions a
J
1
and b
I
1
.
To perform Gibbs sampling, we start with
an initial word segmentation and initial word
alignments, and iteratively re-sample the word-
segmentation and alignments according to our
model of Equation 4.
Note that for efficiency, we only allow limited
modifications to the initial word alignments. Thus
we only use models derived from IBM-1 (instead
of IBM-4) for comparing different word segmenta-
tions. On the other hand, re-sampling the segmen-
tation causes re-linking alignment points to parts
or groups of the original words.
Hence, we organize our sampling process
around possible word boundaries. For each char-
acter c
k
in each sentence, we consider two alterna-
tive segmentations: c
k
+
indicates the segmentation
where there is a boundary after c
k
and c
k
?
indi-
cates the segmentation where there is no boundary
after c
k
, keeping all other boundaries fixed. Let f
denote the single word spanning character c
k
when
there is no boundary after it, and f
?
,f
??
denote the
two adjacent words resulting if there is a bound-
ary: f
?
includes c
k
and f
??
starts just to the right,
with character c
k+1
. The introduction of f
?
and
f
??
leads to M new possible alignments in the E-
to-C direction b
+
k1
, . . . , b
+
kM
, such as in Figure 1.
Together with the boundary vs no-boundary state
at each character position, we re-sample a set of
alignment links between English words and any of
the Chinese words f ,f
?
, and f
??
, keeping all other
word alignments in the sentence pair fixed. (See
Figures 1 and 2.)
Table 1: General Algorithm of GS for CWS.
Input: D with an initial segmentation and alignments
Output: D with sampled segmentation and alignments
for n = 1 to
?
N
for k = 1 to K that c
k
? d
n
Create M+1 candidates, cba
+
k,m
and cba
?
k
, where
cba
+
k,m
: there is a word boundary after c
k
cba
?
k
: there is no word boundary after c
k
Compute probabilities
P (cba
+
k,m
|dh
nk
?
)
P (cba
?
k
|dh
nk
?
)
Sample boundary and relevant alignments
Update counts
Thus at each step in the Gibbs sampler, we con-
sider a set of alternatives for the boundary after
c
k
and relevant alignment links, keeping all other
hidden variables fixed. At each step, we need to
compute the probability of each of the alternatives,
given the fixed values of the other hidden variables.
We introduce some notation to make the presen-
tation easier. For every position k in sentence pair
n, we denote by dh
nk
?
the observations and hid-
den variables for all sentences other than sentence
n, and the observations and hidden variables in-
side sentence n, not involving character position
c
k
. The fixed variables inside the sentence are the
words not neighboring position k, and the align-
ments in both directions to these words.
In the process of sampling, we consider a set
of alternatives: segmentation c
k
+
along with the
product space of relevant alignments in both direc-
tions b
+
k1
, . . . , b
+
kM
, and a
+
k
, and segmentation c
?
k
along with relevant alignments b
k
?
and a
?
k
. For
brevity, we denote these alternatives by cba
k,m
+
and cba
k
?
.
We describe how we derive the set of alterna-
tives in section 5.2 and how we compute their
probabilities in section 5.1.
Table 1 shows schematically one iteration of
Gibbs sampling through the whole training corpus
of parallel sentences, where
?
N is the number of
parallel sentences.
5.1 Computing probabilities of alternatives
For the Gibbs sampling algorithm in Table 1, we
need to compute the probability of each alternative
segmentation/alignments, given the fixed values of
the rest of the data dh
nk
?
. The probability of the
hidden variables in the alternatives is proportional
to the joint probability of the hidden variables and
observations, and thus it is sufficient to compute
the probability of the latter. We compute these
probabilities using the Chinese restaurant process
sampling scheme for the Dirichlet Process, thus in-
1020
tegrating over all of the possible values of the dis-
tributions G, G
f
and G
e
.
Let cba
k
denote an alternative hypothesis in-
cluding boundary or no boundary at position k,
and relevant alignments to English words in both
directions of the one or two Chinese words result-
ing from the segmentation at k. The probability of
this configuration given by our model is:
P (cba
k
|dh
nk
?
) ? P
m
(cba
k
|dh
nk
?
)
?
1
(5)
?P
ef
(cba
k
|dh
nk
?
)
?
2
? P
fe
(cba
k
|dh
nk
?
)
?
3
,
where P
m
(cba
k
|dh
nk
?
) is the monolingual
word probability, and P
fe
(cba
k
|dh
nk
?
) and
P
ef
(cba
k
|dh
nk
?
) are the translation probabilities
in the two directions.
We now describe the computations of each of
the component probabilities.
5.1.1 Word model probability
The word model probability P
m
(cab
k
|dh
nk
?
)
in Equation 5 is derived from Equations 3 and 1:
There are two cases, depending on whether the
hypothesis specifies that there is a boundary after
character c
k
, in which case we need the probabili-
ties of the two resulting words f
?
, and f
??
, or there
is no boundary, in which case we need the proba-
bility of the single word f . (See the initial states in
Figures 1 and 2, respectively.)
Let N denote the total number of word tokens
in the rest of the corpus dh
nk
?
, and N(f) denote
the number of instances of word f in dh
nk
?
. The
probabilities in the two cases are
P
m
(c
+
k
|dh
nk
?
) ?
N(f
?
) + ?P
0
(f
?
)
N + ?
?
N(f
??
) + ?P
0
(f
??
)
N + ?
P
m
(c
?
k
|dh
nk
?
) ?
N(f) + ?P
0
(f)
N + ?
Here P
0
(f) is computed using Equation 2.
5.1.2 Translation model probability
The translation model probabilities depend on
whether or not there is a segmentation boundary
at c
k
and which English words are aligned to the
relevant Chinese words.
In the first case, assume that there is a word
boundary in cab
k
, and that English words {e
?
} are
aligned to f
?
and words {e
??
} are aligned to f
??
in
the E-to-C direction according to the alignment b
k
,
and that f
?
is aligned to e
?
?
and f
??
is aligned to e
?
??
in the C-to-E direction according to the alignment
a
k
(see the initial state in Figure 1). Here we over-
loaded notations and use b
k
and a
k
to indicate the
alignments of the relevant Chinese words at posi-
tion k to any English words. Let I denote the total
number of English words in the sentence, and J+1
denote the number of Chinese words according to
this segmentation. We also denote the total num-
ber of English words aligned to either f
?
or f
??
in
the E-to-C direction by P .
The translation model probability in the E-to-C
direction is thus:
P
ef
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
1
(J + 2
)
P
?
e
?
P (e
?
|f
?
, dh
nk
?
)
?
e
??
P (e
??
|f
??
, dh
nk
?
)
Here we compute P (e|f, dh
nk
?
) as:
P (e|f, dh
nk
?
) =
N(e, f) + ?P
0
(e)
N(f) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
The translation probability in the other direction
is similarly computed as:
P
fe
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
(
1
I + 1
)
2
P (f
?
|e
?
, dh
nk
?
)P (f
??
|e
?
, dh
nk
?
)
And P (f |e, dh
nk
?
) is computed as:
P (f |e, dh
nk
?
) =
N(f, e) + ?P
0
(f)
N(e) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
In the second case, if the hypothesis in evalua-
tion does not have a word boundary at position k,
the total number of Chinese words would be one
less, i.e. J instead of J +1 in the equations above,
and there would be a single set of English words
aligned to the word f in the E-to-C direction, and a
single word e
?
aligned to f in the C-to-E direction
(see the initial state in Figure 2. The probability of
this hypothesis is computed analogously.
5.2 Determining the set of alternative
hypotheses
As mentioned earlier, we consider alternative
alignments which deviate minimally from the cur-
rent alignments, and which satisfy the constraints
of the IBM model 1 in both directions. In order
to describe the set of alternatives, we consider two
cases, depending on whether there is a boundary at
the current character before sampling at position k.
Case 1. There was no boundary at c
k
in the previ-
ous state (see Figure 1).
1021
If there is no boundary at c
k
, there is a sin-
gle word f spanning that position. We denote by
{e} the set of English words aligned to f at that
state in the E-to-C direction and by e
?
the En-
glish word aligned to f in the C-to-E direction.
Since every state we consider satisfies the IBM
one-to-many constraints, there is exactly one En-
glish word aligned to f in the C-to-E direction and
the words {e} have no other words aligned to them
in the E-to-C direction.
In this case, we consider as hypothesis cba
k
?
the same segmentation and alignment as in the pre-
vious state. (see Table 1 for an overview of the
alternative hypotheses.)
We consider M different hypotheses which in-
clude a boundary at k in this case, where M de-
pends on the number of words {e} aligned to f
in the previous state. Because we are breaking
the word f into two words f
?
and f
??
by placing
a boundary at c
k
, we need to re-align the words
{e} to either f
?
or f
??
. Additionally we need to
align f
?
and f
??
to English words in the C-to-E
direction. The number of different hypotheses is
equal to 2
P
where P = |{e}|. These alternatives
arise by considering that each of the words in {e}
needs to align to either f
?
or f
??
, and there are 2
P
combinations of these alignments. For example, if
{e} = {e
1
, e
2
}, after splitting the word f there are
four possible alignments, illustrated in Figure 1:
I. (f
?
, e
1
) and (f
??
, e
2
), II. (f
?
, e
2
) and (f
??
, e
1
),
III. (f
?
, e
1
) and (f
?
, e
2
), IV. (f
??
, e
1
) and (f
??
, e
2
).
For the alignment a
k
in the C-to-E direction, we
consider only one option, in which both resulting
words f
?
and f
??
align to e
?
. These alternatives
form cba
k,m
+
in Table 1.
Case 2. There was a boundary at c
k
in the previous
state (see Figure 2).
In this case, for the hypotheses c
+
k
we consider
only one alternative, which is exactly the same as
the assignment of segmentation and alignments in
the previous state. Thus we have M = 1 in Table
1.
Let f
?
and f
??
denote the two words at position
k in the previous state, {e
?
} and {e
??
} denote the
sets of English words aligned to them in the E-to-C
direction, respectively, and e
?
?
and e
?
??
denote the
English words aligned to f
?
and f
??
in the C-to-E
direction.
We consider only one hypothesis cba
k
?
where
there is no boundary at c
k
. In this hypothesis, there
is a single word f = f
?
f
??
spanning position k,
and all words {e
?
} ? {e
??
} align to f in the E-to-
C direction. For the C-to-E direction we consider
the ?better? of the alignments (f, e
?
?
) and (f, e
??
?
)
where the better alignment is defined as the one
having higher probability according to the C-to-E
word translation probabilities.
Table 2: Complete Algorithm of Gibbs Sampler
for CWS including Alignment Models.
Input: D, F
0
Output: A
T
, F
T
for t = 1 to T
Run GIZA++ on (D,F
t?1
) to obtain A
t
Run GS on (D,F
t?1
, A
t
) to obtain F
t
5.3 Complete segmentation algorithm
So far, we have described how we re-sample word
segmentation and alignments according to our
model, starting from an initial segmentation and
alignments from GIZA++. Putting these pieces to-
gether, the algorithm is summarized in Table 1.
We found that we can further improve perfor-
mance by repeatedly aligning the corpus using
GIZA++, after deriving a new segmentation us-
ing our model. The complete algorithm which in-
cludes this step is shown in Table 2, where F
t
in-
dicates the word segmentation at iteration t and A
t
denotes the GIZA++ corpus alignment in both di-
rections. The GS re-segmentation step is done ac-
cording to the algorithm in Table 1.
Using this algorithm, we obtain a new segmen-
tation of the Chinese data and train the translation
models using this segmentation as in the baseline
MT system. To segment the test data for transla-
tion, we use a unigram model, trained with maxi-
mum likelihood estimation off of the final segmen-
tation of the training corpus F
T
.
6 Translation Experiments
We performed experiments using our models for
CWS on a large and a small data track. We evalu-
ated performance by measuring WER (word error
rate), PER (position-independent word error rate),
BLEU (Papineni et al, 2002) and TER (translation
error rate) (Snover et al, 2006) using multiple ref-
erences.
6.1 Translation Task: Large Track NIST
We first report the experiments using our mono-
lingual unigram Dirichlet Process model for word
segmentation on the NIST machine translation task
(NIST, 2005). Because of the computational re-
quirements, we only employed the monolingual
word model for this large data track, i.e. the fea-
ture weights were ?
1
= 1, ?
2
= 0, ?
3
= 0. There-
fore, no alignment information needs to be main-
tained in this case.
The bilingual training corpus is a superset of
corpora in the news domain collected from differ-
ent sources.
We took LDC (LDC, 2003) as a baseline CWS
method (Base). As shown in Table 3, the training
corpus in each language contains more than two
million sentences. There are 56 million Chinese
1022
Table 3: Statistics of corpora in task NIST.
Data Sents. Words[K] Voc.[K]
Cn. En. Cn. En.
Chars 2M 56M 49.5M 65.4 211
Base 39.2M 95.7
GS 40.5M 95.4
02 878 23.1 28.0 2.04 4.34
03 919 24.6 29.2 2.21 4.91
04 1788 49.8 60.7 2.61 6.71
05 1082 30.8 34.2 2.30 5.39
Table 4: Translation performance [% BLEU] with
the baseline(LDC) and GS method on NIST.
MT-eval LDC(Base) GS
2005 32.85 33.26
2002 34.32 34.36
2003 33.41 33.75
2004 33.74 34.06
characters. The LDC and GS word segmentation
methods generated 39.2 and 40.5 million running
words, respectively.
The scaling factors of the translation models de-
scribed in Section 2.2 were optimized on the devel-
opment corpus, MT-eval 05 with 1082 sentences.
The resulting systems were evaluated on the test
corpora MT-eval 02-04. For convenience, we only
list the statistics of the first English reference.
Starting from the baseline LDC output as ini-
tial word segmentation, we performed Gibbs sam-
pling (GS) of word segmentations using 30 itera-
tions over the Chinese training corpus.
Since BLEU is the official NIST measure of
translation performance, we show the translation
results measured in BLEU score only. As shown
in Table 4, on the development data MT-eval 05,
the BLEU score was improved by 0.4% absolute or
more than 1% relative using GS. Similarly, the ab-
solute BLEU scores are also improved on all other
test sets, in the range of 0.04% to 0.4%.
We can see that even a monolingual semi-
supervised word segmentation method can outper-
form a supervised one in MT, probably because the
training/test corpora contain many unknown words
and words have different frequencies in our MT
data from they do in the manually labeled CWS
data.
6.2 Translation Task: Small Track IWSLT
We evaluate our full model, using both monolin-
gual and bilingual information, on the IWSLT data.
As shown in Table 5, the Chinese training
corpus was segmented using the unigram seg-
menter (Base) described in Section 2.1 and our GS
method. Since the unigram segmenter performs
better in our experiments, we took it as the base-
line and the method for initialization in later ex-
periments. We see that the vocabulary size of the
Chinese training corpus was reduced more signif-
icantly by GS than by the baseline method, even
Table 5: Statistics of corpora in task IWSLT.
Test Sents. Words[K] Voc.
Cn. En. Cn. En.
Chars 42.9K 520 420 2780 9930
Base 394 8800
GS 398 6230
Dev2 500 3.74 3.82 1004 821
Dev3 506 4.01 3.90 980 820
Eval 489 3.39 3.72 904 810
Table 6: Translation performance with different
CWS methods on IWSLT[%].
Test Method WER PER BLEU TER
Dev2 Unigram (Base) 38.2 31.2 55.4 37.0
GS 36.8 30.0 56.6 35.5
Dev3 Unigram (Base) 33.5 27.5 60.4 32.1
GS 32.3 26.6 61.0 31.4
Eval Characters 49.3 41.8 35.4 47.5
LDC 46.2 40.0 39.2 45.0
ICT 45.9 40.4 40.1 44.9
Unigram (Base) 46.8 40.2 41.6 45.6
9-gram 46.9 40.4 40.1 45.4
GS 45.9 40.0 41.6 44.8
though they resulted in a similar number of run-
ning words. This shows that the distribution of
Chinese words is more concentrated when using
GS.
The parameter optimizations were performed on
the Dev2 data with 500 sentences, and evaluations
were done both on Dev3 and on Eval data, i.e. the
evaluation corpus of (IWSLT, 2007).
The model weights ? of GS from Section 5.1.2
were optimized using the Powell (Press et al,
2002) algorithm with respect to the BLEU score.
We obtained ?
1
= 1.4, ?
2
= 1 and ?
3
= 0.8 as
optimal values and T = 4 as the optimal number
of iterations of re-alignment with GIZA++.
For a fair comparison, we evaluated on various
CWS methods including translation on characters
, LDC (LDC, 2003), ICT (Zhang et al, 2003), uni-
gram, 9-gram and GS. Improvements using GS can
be seen in Table 6. Under all test sets and evalua-
tion criteria, GS outperforms the baseline method.
The absolute WER decreases with 1.2% on Dev3
and with 1.1% on Eval data over baseline.
We compared the translation outputs using GS
with the baseline method. On the Eval data, 196
sentences are different out of 489 lines, where 64
sentences from GS are better, 33 sentences are
worse, and the rests have similar translation qual-
ities. Table 7 shows two examples from the Eval
corpus. We list segmentations produced by the
baseline and GS methods, as well as the transla-
tions corresponding to these segmentations. The
GS method generates better translation results than
the baseline method in these cases.
1023
Table 7: Segmentation and translation outputs with
baseline and GS methods.
a) Baseline ? ?4 m?
do you have a ?
GS ??4m?
do you have a shorter way ?
REF is there a shorter route ?
b) Baseline >????
please show me the in .
GS >????
please show me the total price .
REF can you tell me the total amount ?
7 Conclusion and future work
We showed that it is possible to learn Chinese word
boundaries such that the translation performance
of Chinese-to-English MT systems is improved.
We presented a Bayesian generative model for
parallel Chinese-English sentences which uses
word segmentation and alignment as hidden vari-
ables, and incorporates both monolingual and
bilingual information to derive a segmentation
suitable for MT.
Starting with an initial word segmentation, our
method learns both new Chinese words and dis-
tributions for these words. In a large and a small
data environment, our method outperformed the
standard Chinese word segmentation approach in
terms of the Chinese to English translation quality.
In future work, we plan to enrich our monolingual
and bilingual models to better represent the true
distribution of the data.
8 Acknowledgments
Jia Xu conducted this research during her intern-
ship at Microsoft Research. This material is also
partly based upon work supported by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
Aldous, D. 1985. Exchangeability and related topics.
In
?
Ecole d??et?e de probabilit?es de Saint-Flour, XIII-
1983, pages 1?198, Springer, Berlin.
Andrew, G. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In
Proceedings of EMNLP, Sydney, July.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
Gao, J., M. Li, A. Wu, and C. Huang. 2005. Chi-
nese word segmentation and named entity recogni-
tion: A pragmatic approach. Computational Lin-
guistics, 31(4).
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of Coling/ACL, Sydney,
July.
IWSLT. 2007. International workshop on
spoken language translation home page.
http://www.slt.atr.jp/IWSLT2007.
Klein, D. and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL, pages 128?135.
LDC. 2003. Linguistic data consor-
tium Chinese resource home page.
http://www.ldc.upenn.edu/Projects/Chinese.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
Och, F. J. and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302,
Philadelphia, PA, July.
Papineni, K. A., S. Roukos, T. W., and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, July.
Press, W. H., S. A. Teukolsky, W. T. Vetterling, and
B. P. Flannery. 2002. Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA, pages 223?231, Cambridge, MA, August.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of COLING.
Xu, J., R. Zens, and H. Ney. 2004. Do we need
Chinese word segmentation for statistical machine
translation? In Proceedings of the SIGHAN Work-
shop on Chinese Language Learning, pages 122?
128, Barcelona, Spain, July.
Xu, J., E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proceedings of IWSLT, pages
141?147, Pittsburgh, PA, October.
Zens, R. and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceedings
of HLT/NAACL, Boston, MA, May.
Zhang, H., H. Yu, D. Xiong, and Q. Liu. 2003.
HHMM-based Chinese lexical analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Learning, pages 184?187, July.
1024
Proceedings of ACL-08: HLT, pages 81?88,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Phrase Table Training For Precision and Recall:
What Makes a Good Phrase and a Good Phrase Pair?
Yonggang Deng? , Jia Xu+ and Yuqing Gao?
?IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA
{ydeng,yuqing}@us.ibm.com
+Chair of Computer Science VI, RWTH Aachen University, D-52056 Aachen, Germany
xujia@cs.rwth-aachen.de
Abstract
In this work, the problem of extracting phrase
translation is formulated as an information re-
trieval process implemented with a log-linear
model aiming for a balanced precision and re-
call. We present a generic phrase training al-
gorithm which is parameterized with feature
functions and can be optimized jointly with
the translation engine to directly maximize
the end-to-end system performance. Multiple
data-driven feature functions are proposed to
capture the quality and confidence of phrases
and phrase pairs. Experimental results demon-
strate consistent and significant improvement
over the widely used method that is based on
word alignment matrix only.
1 Introduction
Phrase has become the standard basic translation
unit in Statistical Machine Translation (SMT) since
it naturally captures context dependency and models
internal word reordering. In a phrase-based SMT
system, the phrase translation table is the defining
component which specifies alternative translations
and their probabilities for a given source phrase. In
learning such a table from parallel corpus, two re-
lated issues need to be addressed (either separately
or jointly): which pairs are considered valid trans-
lations and how to assign weights, such as proba-
bilities, to them. The first problem is referred to as
phrase pair extraction, which identifies phrase pairs
that are supposed to be translations of each other.
Methods have been proposed, based on syntax, that
take advantage of linguistic constraints and align-
ment of grammatical structure, such as in Yamada
and Knight (2001) and Wu (1995). The most widely
used approach derives phrase pairs from word align-
ment matrix (Och and Ney, 2003; Koehn et al,
2003). Other methods do not depend on word align-
ments only, such as directly modeling phrase align-
ment in a joint generative way (Marcu and Wong,
2002), pursuing information extraction perspective
(Venugopal et al, 2003), or augmenting with model-
based phrase pair posterior (Deng and Byrne, 2005).
Using relative frequency as translation probabil-
ity is a common practice to measure goodness of
a phrase pair. Since most phrases appear only a
few times in training data, a phrase pair translation
is also evaluated by lexical weights (Koehn et al,
2003) or term weighting (Zhao et al, 2004) as addi-
tional features to avoid overestimation. The transla-
tion probability can also be discriminatively trained
such as in Tillmann and Zhang (2006).
The focus of this paper is the phrase pair extrac-
tion problem. As in information retrieval, precision
and recall issues need to be addressed with a right
balance for building a phrase translation table. High
precision requires that identified translation candi-
dates are accurate, while high recall wants as much
valid phrase pairs as possible to be extracted, which
is important and necessary for online translation that
requires coverage. In the word-alignment derived
phrase extraction approach, precision can be im-
proved by filtering out most of the entries by using
a statistical significance test (Johnson et al, 2007).
On the other hand, there are valid translation pairs
in the training corpus that are not learned due to
word alignment errors as shown in Deng and Byrne
(2005).
81
We would like to improve phrase translation ac-
curacy and at the same time extract as many as pos-
sible valid phrase pairs that are missed due to in-
correct word alignments. One approach is to lever-
age underlying word alignment quality such as in
Ayan and Dorr (2006). In this work, we present a
generic discriminative phrase pair extraction frame-
work that can integrate multiple features aiming to
identify correct phrase translation candidates. A sig-
nificant deviation from most other approaches is that
the framework is parameterized and can be opti-
mized jointly with the decoder to maximize transla-
tion performance on a development set. Within the
general framework, the main work is on investigat-
ing useful metrics. We employ features based on
word alignment models and alignment matrix. We
also propose information metrics that are derived
from both bilingual and monolingual perspectives.
All these features are data-driven and independent of
languages. The proposed phrase extraction frame-
work is general to apply linguistic features such as
semantic, POS tags and syntactic dependency.
2 A Generic Phrase Training Procedure
Let e = eI1 denote an English sentence and let
f = fJ1 denote its translation in a foreign lan-
guage, say Chinese. Phrase extraction begins with
sentence-aligned parallel corpora {(ei, fi)}. We use
E = eieib and F = f
je
jb
to denote an English and
foreign phrases respectively, where ib(jb) is the po-
sition in the sentence of the beginning word of the
English(foreign) phrase and ie(je) is the position of
the ending word of the phrase.
We first train word alignment models and will use
them to evaluate the goodness of a phrase and a
phrase pair. Let fk(E,F ), k = 1, 2, ? ? ? ,K be K
feature functions to be used to measure the quality
of a given phrase pair (E,F ). The generic phrase
extraction procedure is an evaluation, ranking, fil-
tering, estimation and tuning process, presented in
Algorithm 1.
Step 1 (line 1) is the preparation stage. Begin-
ning with a flat lexicon, we train IBM Model-1 word
alignment model with 10 iterations for each trans-
lation direction. We then train HMM word align-
ment models (Vogel et al, 1996) in two directions
simultaneously by merging statistics collected in the
Algorithm 1 A Generic Phrase Training Procedure
1: Train Model-1 and HMM word alignment models
2: for all sentence pair (e, f) do
3: Identify candidate phrases on each side
4: for all candidate phrase pair (E,F ) do
5: Calculate its feature function values fk
6: Obtain the score q(E,F ) =
?K
k=1 ?kfk(E,F )
7: end for
8: Sort candidate phrase pairs by their final scores q
9: Find the maximum score qm = max q(E,F )
10: for all candidate phrase pair (E,F ) do
11: If q(E,F ) ? qm? ? , dump the pair into the pool
12: end for
13: end for
14: Built a phrase translation table from the phrase pair pool
15: Discriminatively train feature weights ?k and threshold ?
E-step from two directions motivated by Zens et al
(2004) with 5 iterations. We use these models to de-
fine the feature functions of candidate phrase pairs
such as phrase pair posterior distribution. More de-
tails will be given in Section 3.
Step 2 (line 2) consists of phrase pair evalua-
tion, ranking and filtering. Usually all n-grams up
to a pre-defined length limit are considered as can-
didate phrases. This is also the place where lin-
guistic constraints can be applied, say to avoid non-
compositional phrases (Lin, 1999). Each normalized
feature score derived from word alignment models
or language models will be log-linearly combined
to generate the final score. Phrase pair filtering is
simply thresholding on the final score by comparing
to the maximum within the sentence pair. Note that
under the log-linear model, applying threshold for
filtering is equivalent to comparing the ?likelihood?
ratio.
Step 3 (line 14) pools all candidate phrase pairs
that pass the threshold testing and estimates the fi-
nal phrase translation table by maximum likelihood
criterion. For each candidate phrase pair which is
above the threshold, we assign HMM-based phrase
pair posterior as its soft count when dumping them
into the global phrase pair pool. Other possibilities
for the weighting include assigning constant one or
the exponential of the final score etc.
One of the advantages of the proposed phrase
training algorithm is that it is a parameterized pro-
cedure that can be optimized jointly with the trans-
82
lation engine to minimize the final translation errors
measured by automatic metrics such as BLEU (Pa-
pineni et al, 2002). In the final step 4 (line 15), pa-
rameters {?k, ?} are discriminatively trained on a
development set using the downhill simplex method
(Nelder and Mead, 1965).
This phrase training procedure is general in the
sense that it is configurable and trainable with dif-
ferent feature functions and their parameters. The
commonly used phrase extraction approach based
on word alignment heuristics (referred as ViterbiEx-
tract algorithm for comparison in this paper) as de-
scribed in (Och, 2002; Koehn et al, 2003) is a spe-
cial case of the algorithm, where candidate phrase
pairs are restricted to those that respect word align-
ment boundaries.
We rely on multiple feature functions that aim to
describe the quality of candidate phrase translations
and the generic procedure to figure out the best way
of combining these features. A good feature func-
tion pops up valid translation pairs and pushes down
incorrect ones.
3 Features
Now we present several feature functions that we in-
vestigated to help extracting correct phrase transla-
tions. All these features are data-driven and defined
based on models, such as statistical word alignment
model or language model.
3.1 Model-based Phrase Pair Posterior
In a statistical generative word alignment model
(Brown et al, 1993), it is assumed that (i) a random
variable a specifies how each target word fj is gen-
erated by (therefore aligned to) a source 1 word eaj ;
and (ii) the likelihood function f(f ,a|e) specifies a
generative procedure from the source sentence to the
target sentence. Given a phrase pair in a sentence
pair, there will be many generative paths that align
the source phrase to the target phrase. The likelihood
of those generative procedures can be accumulated
to get the likelihood of the phrase pair (Deng and
Byrne, 2005). This is implemented as the summa-
tion of the likelihood function over all valid hidden
word alignments.
1The word source and target are in the sense of word align-
ment direction, not as in the source-channel formulation.
More specifically, let A(j1,j2)(i1,i2) be the set of word
alignment a that aligns the source phrase ej1i1 to the
target phrase f j2j1 (links to NULL word are ignored
for simplicity):
A(j1,j2)(i1,i2) = {a : aj ? [i1, i2] iff j ? [j1, j2]}
The alignment set given a phrase pair ignores those
pairs with word links across the phrase boundary.
Consequently, the phrase-pair posterior distribution
is defined as
P?(e
i2
i1 ? f
j2
j1 |e, f) =
?
a?A
(j1,j2)
(i1,i2)
f(a, f |e; ?)
?
a f(a, f |e; ?)
.(1)
Switching the source and the target, we can obtain
the posterior distribution in another translation di-
rection. This distribution is applicable to all word
alignment models that follow assumptions (i) and
(ii). However, the complexity of the likelihood func-
tion could make it impractical to calculate the sum-
mations in Equation 1 unless an approximation is
applied.
Several feature functions will be defined on top of
the posterior distribution. One of them is based on
HMM word alignment model. We use the geometric
mean of posteriors in two translation directions as
a symmetric metric for phrase pair quality evalua-
tion function under HMM alignment models. Table
1 shows the phrase pair posterior matrix of the ex-
ample.
Replacing the word alignment model with IBM
Model-1 is another feature function that we added.
IBM Model-1 is simple yet has been shown to be
effective in many applications (Och et al, 2004).
There is a close form solution to calculate the phrase
pair posterior under Model-1. Moreover, word to
word translation table under HMM is more concen-
trated than that under Model-1. Therefore, the pos-
terior distribution evaluated by Model-1 is smoother
and potentially it can alleviate the overestimation
problem in HMM especially when training data size
is small.
3.2 Bilingual Information Metric
Trying to find phrase translations for any possible n-
gram is not a good idea for two reasons. First, due
to data sparsity and/or alignment model?s capabil-
ity, there would exist n-grams that cannot be aligned
83
    f1                  f2                 f3 
(that)   (is)   (what) 
 
 
what?s   that 
  e1                e2 
e11 e
2
1 e
2
2 HBL(f
j2
j1
)
f11 0.0006 0.012 0.89 0.08
f21 0.0017 0.035 0.343 0.34
f31 0.07 0.999 0.0004 0.24
f22 0.03 0.0001 0.029 0.7
f32 0.89 0.006 0.006 0.05
f33 0.343 0.002 0.002 0.06
HBL(e
i2
i1
) 0.869 0.26 0.70
Table 1: Phrase pair posterior distribution for the example
well, for instance, n-grams that are part of a para-
phrase translation or metaphorical expression. To
give an example, the unigram ?tomorrow? in ?the day
after tomorrow? whose Chinese translation is a sin-
gle word ???. Extracting candidate translations
for such kind of n-grams for the sake of improving
coverage (recall) might hurt translation quality (pre-
cision). We will define a confidence metric to esti-
mate how reliably the model can align an n-gram in
one side to a phrase on the other side given a par-
allel sentence. Second, some n-grams themselves
carry no linguistic meaning; their phrase translations
can be misleading, for example non-compositional
phrases (Lin, 1999). We will address this in section
3.3.
Given a sentence pair, the basic assumption is that
if the HMM word alignment model can align an En-
glish phrase well to a foreign phrase, the posterior
distribution of the English phrase generating all for-
eign phrases on the other side is significantly biased.
For instance, the posterior of one foreign phrase is
far larger than that of the others. We use the entropy
of the posterior distribution as the confidence metric:
HBL(e
i2
i1 |e, f) = H(P??HMM (e
i2
i1 ? ?)) (2)
where H(P ) = ?
?
x P (x) logP (x) is the entropy
of a distribution P (x), P??HMM (e
i2
i1 ? ?) is the
normalized probability (sum up to 1) of the pos-
terior P?HMM (e
i2
i1 ? ?) as defined in Equation 1.
Low entropy signals a high confidence that the En-
glish phrase can be aligned correctly. On the other
hand, high entropy implies ambiguity presented in
discriminating the correct foreign phrase from the
others from the viewpoint of the model.
Similarly we calculate the confidence metric of
aligning a foreign phrase correctly with the word
alignment model in foreign to English direction. Ta-
ble 1 shows the entropy of phrases. The unigram
of foreign side f22 is unlikely to survive with such
high ambiguity. Adding the entropy in two direc-
tions defines the bilingual information metric as an-
other feature function, which describes the reliabil-
ity of aligning each phrase correctly by the model.
Note that we used HMM word alignment model to
find the posterior distribution. Other models such as
Model-1 can be applied in the same way. This fea-
ture function quantitatively captures the goodness of
phrases. During phrase pair ranking, it can help
to move upward phrases that can be aligned well
and push downward phrases that are difficult for the
model to find correct translations.
3.3 Monolingual Information Metric
Now we turn to monolingual resources to evaluate
the quality of an n-gram being a good phrase. A
phrase in a sentence is specified by its boundaries.
We assume that the boundaries of a good phrase
should be the ?right? place to break. More generally,
we want to quantify how effective a word bound-
ary is as a phrase boundary. One would perform say
NP-chunking or parsing to avoid splitting a linguis-
tic constituent. We apply a language model (LM)
to describe the predictive uncertainty (PU ) between
words in two directions.
Given a history wn?11 , a language model specifies
a conditional distribution of the future word being
predicted to follow the history. We can find the en-
tropy of such pdf: HLM (w
n?1
1 ) = H(P (?|w
n?1
1 )).
So given a sentencewN1 , the PU of the boundary be-
tween word wi and wi+1 is established by two-way
entropy sum using a forward and backward language
model: PU(wN1 , i) = HLMF (w
i
1) + HLMB(w
i+1
N )
We assume that the higher the predictive uncer-
tainty is, the more likely the left or right part of the
word boundary can be ?cut-and-pasted? to form an-
other reasonable sentence. So a good phrase is char-
acterized with high PU values on the boundaries.
For example, in ?we want to have a table near the
window?, the PU value of the point after ?table? is
0.61, higher than that between ?near? and ?the? 0.3,
using trigram LMs.
With this, the feature function derived from
84
monolingual clue for a phrase pair can be defined
as the product of PUs of the four word boundaries.
3.4 Word Alignments Induced Metric
The widely used ViterbiExtract algorithm relies
on word alignment matrix and no-crossing-link as-
sumption to extract phrase translation candidates.
Practically it has been proved to work well. How-
ever, discarding correct phrase pairs due to incorrect
word links leaves room for improving recall. This
is especially true for not significantly large training
corpora. Provided with a word alignment matrix,
we define within phrase pair consistency ratio (WP-
PCR) as another feature function. WPPCR was used
as one of the scores in (Venugopal et al, 2003) for
phrase extraction. It is defined as the number of con-
sistent word links associated with any words within
the phrase pair divided by the number of all word
links associated with any words within the phrase
pair. An inconsistent link connects a word within
the phrase pair to a word outside the phrase pair. For
example, the WPPCR for (e21, f
2
1 ) in Table 1 is 2/3.
As a special case, the ViterbiExtract algorithm ex-
tracts only phrase pairs with WPPCR is 1.
To further discriminate the pairs with higher WP-
PCR from those with lower ratio, we apply a Bi-
Linear Transform (BLT) (Oppenheim and Schafer,
1989) mapping. BLT is commonly used in sig-
nal processing to attenuate the low frequency parts.
When used to map WPPCR, it exaggerates the dif-
ference between phrase pairs with high WPPCR and
those with low WPPCR, making the pairs with low
ratio more unlikely to be selected as translation can-
didates. One of the nice properties of BLT is that
there is a parameter that can be changed to adjust
the degree of attenuation, which provides another di-
mension for system optimization.
4 Experimental Results
We evaluate the effect of the proposed phrase extrac-
tion algorithm with translation performance. We do
experiments on IWSLT (Paul, 2006) 2006 Chinese-
English corpus. The task is to translate Chinese ut-
terances in travel domain into English. We report
only text (speech transcription) translation results.
The training corpus consists of 40K Chinese-
English parallel sentences in travel domain with to-
Eval Set 04dev 04test 05test 06dev 06test
# of sentences 506 500 506 489 500
# of words 2808 2906 3209 5214 5550
# of refs 16 16 16 7 7
Table 2: Dev/test set statistics
tal 306K English words and 295K Chinese words.
In the data processing step, Chinese characters are
segmented into words. English text are normalized
and lowercased. All punctuation is removed.
There are five sets of evaluation sentences in
tourism domain for development and test. Their
statistics are shown in Table 2. We will tune training
and decoding parameters on 06dev and report results
on other sets.
4.1 Training and Translation Setup
Our decoder is a phrase-based multi-stack imple-
mentation of the log-linear model similar to Pharaoh
(Koehn et al, 2003). Like other log-linear model
based decoders, active features in our transla-
tion engine include translation models in two di-
rections, lexicon weights in two directions, lan-
guage model, lexicalized distortion models, sen-
tence length penalty and other heuristics. These fea-
ture weights are tuned on the dev set to achieve op-
timal translation performance using downhill sim-
plex method. The language model is a statistical
trigram model estimated with Modified Kneser-Ney
smoothing (Chen and Goodman, 1996) using only
English sentences in the parallel training data.
Starting from the collection of parallel training
sentences, we build word alignment models in two
translation directions, from English to Chinese and
from Chinese to English, and derive two sets of
Viterbi alignments. By combining word alignments
in two directions using heuristics (Och and Ney,
2003), a single set of static word alignments is then
formed. Based on alignment models and word align-
ment matrices, we compare different approaches of
building a phrase translation table and show the fi-
nal translation results. We measure translation per-
formance by the BLEU (Papineni et al, 2002) and
METEOR (Banerjee and Lavie, 2005) scores with
multiple translation references.
85
BLEU Scores
Table 04dev 04test 05test 06dev 06test
HMM 0.367 0.407 0.473 0.200 0.190
Model-4 0.380 0.403 0.485 0.210 0.204
New 0.411 0.427 0.500 0.216 0.208
METEOR Scores
Table 04dev 04test 05test 06dev 06test
HMM 0.532 0.586 0.675 0.482 0.471
Model-4 0.540 0.593 0.682 0.492 0.480
New 0.568 0.614 0.691 0.505 0.487
Table 3: Translation Results
4.2 Translation Results
Our baseline phrase table training method is the
ViterbiExtract algorithm. All phrase pairs with re-
spect to the word alignment boundary constraint are
identified and pooled to build phrase translation ta-
bles with the Maximum Likelihood criterion. We
prune phrase translation entries by their probabili-
ties. The maximum number of words in Chinese and
English phrases is set to 8 and 25 respectively for all
conditions2. We perform online style phrase train-
ing, i.e., phrase extraction is not particular for any
evaluation set.
Two different word alignment models are trained
as the baseline, one is symmetric HMM word align-
ment model, the other is IBM Model-4 as imple-
mented in the GIZA++ toolkit (Och and Ney, 2003).
The translation results as measured by BLEU and
METEOR scores are presented in Table 3. We notice
that Model-4 based phrase table performs roughly
1% better in terms of both BLEU and METEOR
scores than that based on HMM.
We follow the generic phrase training procedure
as described in section 2. The most time consuming
part is calculating posteriors, which is carried out in
parallel with 30 jobs in less than 1.5 hours.
We use the Viterbi word alignments from HMM
to define within phrase pair consistency ratio as dis-
cussed in section 3.4. Although Table 3 implies that
Model-4 word alignment quality is better than that
of HMM, we did not get benefits by switching to
Model-4 to compute word alignments based feature
values.
In estimating phrase translation probability, we
use accumulated HMM-based phrase pair posteriors
2We chose large numbers for phrase length limit to build a
strong baseline and to avoid impact of longer phase length.
as their ?soft? frequencies and then the final trans-
lation probability is the relative frequency. HMM-
based posterior was shown to be better than treating
each occurrence as count one.
Once we have computed all feature values for all
phrase pairs in the training corpus, we discrimina-
tively train feature weights ?ks and the threshold
? using the downhill simplex method to maximize
the BLEU score on 06dev set. Since the translation
engine implements a log-linear model, the discrim-
inative training of feature weights in the decoder
should be embedded in the whole end-to-end system
jointly with the discriminative phrase table training
process. This is globally optimal but computation-
ally demanding. As a compromise, we fix the de-
coder feature weights and put all efforts on optimiz-
ing phrase training parameters to find out the best
phrase table.
The translation results with the discriminatively
trained phrase table are shown as the row of ?New?
in Table 3. We observe that the new approach is con-
sistently better than the baseline ViterbiExtract algo-
rithmwith either Model-4 or HMMword alignments
on all sets. Roughly, it has 0.5% higher BLEU score
on 2006 sets and 1.5% to 3% higher on other sets
than Model-4 based ViterbiExtract method. Similar
superior results are observed when measured with
METEOR score.
5 Discussions
The generic phrase training algorithm follows an in-
formation retrieval perspective as in (Venugopal et
al., 2003) but aims to improve both precision and
recall with the trainable log-linear model. A clear
advantage of the proposed approach over the widely
used ViterbiExtract method is trainability. Under the
general framework, one can put as many features as
possible together under the log-linear model to eval-
uate the quality of a phrase and a phase pair. The
phrase table extracting procedure is trainable and
can be optimized jointly with the translation engine.
Another advantage is flexibility, which is pro-
vided partially by the threshold ? . As the figure
1 shows, when we increase the threshold by al-
lowing more candidate phrase pair hypothesized as
valid translation, we observe the phrase table size in-
creases monotonically. On the other hand, we notice
86
1
2
3
4
5
6
7
8
91
01
11
21
31
41
5
0.170.180.190.
2
Thr
esh
old
Thr
esh
oldi
ng E
ffec
ts
Translation Performance
0
5
10
1555.
566.5
Log10 of the number of Entries in the PhraseTable
BLE
U
Phr
ase
tabl
e S
ize
Figure 1: Thresholding effects on translation perfor-
mance and phrase table size
that the translation performance improves gradually.
After reaching its peak, the BLEU score drops as the
threshold ? increases. When ? is large enough, the
translation performance is not changing much but
still worse than the peak value. It implies a balanc-
ing process between precision and recall. The final
optimal threshold ? is around 5.
The flexibility is also enabled by multiple con-
figurable features used to evaluate the quality of a
phrase and a phrase pair. Ideally, a perfect combina-
tion of feature functions divides the correct and in-
correct candidate phrase pairs within a parallel sen-
tence into two ordered separate sets. We use feature
functions to decide the order and the threshold ? to
locate the boundary guided with a development set.
So the main issue to investigate now is which
features are important and valuable in ranking can-
didate phrase pairs. We propose several informa-
tion metrics derived from posterior distribution, lan-
guage model and word alignments as feature func-
tions. The ViterbiExtract is a special case where
a single binary feature function defined from word
alignments is used. Its good performance (as shown
in Table 3) suggests that word alignments are very
indicative of phrase pair quality. So we design com-
parative experiments to capture word alignment im-
pact only. We start with basic features that in-
clude model-based posterior, bilingual and mono-
lingual information metrics. Its results on different
test sets are presented in the ?basic? row of Table 4.
We add word alignment feature (?+align? row), and
Features 04dev 04test 05test 06dev 06test
basic 0.393 0.406 0.496 0.205 0.199
+align 0.401 0.429 0.502 0.208 0.196
+align BLT 0.411 0.427 0.500 0.216 0.208
Table 4: Translation Results (BLEU) of discriminative
phrase training approach using different features
75K
250K 1
32K
PP1
PP3
PP2
Model?4
New
Features 04dev 04test 05test 06dev 06test
PP2 0.380 0.395 0.480 0.207 0.202
PP1+PP2 0.380 0.403 0.485 0.210 0.204
PP2+PP3 0.411 0.427 0.500 0.216 0.208
PP1+PP2+PP3 0.412 0.432 0.500 0.217 0.214
Table 5: Translation Results (BLEU) of Different Phrase
Pair Combination
then apply bilinear transform to the consistency ratio
WPPCR as described in section 3.4 (?+align BLT?
row). The parameter controlling the degree of atten-
uation in BLT is also optimized together with other
feature weights.
With the basic features, the new phrase extraction
approach performs better than the baseline method
with HMM word alignment models but similar to
the baseline method with Model-4. With the word
alignment based feature WPPCR, we obtain a 2%
improvement on 04test set but not much on other
sets except slight degradation on 06test. Finally, ap-
plying BLT transform to WPPCR leads to additional
0.8 BLEU point on 06dev set and 1.2 point on 06test
set. This confirms the effectiveness of word align-
ment based features.
Now we compare the phrase table using the pro-
posed method to that extracted using the baseline
ViterbiExtract method with Model-4 word align-
ments. The Venn diagram in Table 5 shows how the
two phrase tables overlap with each other and size
of each part. As expected, they have a large num-
ber of common phrase pairs (PP2). The new method
is able to extract more phrase pairs than the base-
line with Model-4. PP1 is the set of phrase pairs
found by Model-4 alignments. Removing PP1 from
the baseline phrase table (comparing the first group
of scores) or adding PP1 to the new phrase table
87
(the second group of scores) overall results in no or
marginal performance change. On the other hand,
adding phrase pairs extracted by the new method
only (PP3) can lead to significant BLEU score in-
creases (comparing row 1 vs. 3, and row 2 vs. 4).
6 Conclusions
In this paper, the problem of extracting phrase trans-
lation is formulated as an information retrieval pro-
cess implemented with a log-linear model aiming for
a balanced precision and recall. We have presented
a generic phrase translation extraction procedure
which is parameterized with feature functions. It
can be optimized jointly with the translation engine
to directly maximize the end-to-end translation per-
formance. Multiple feature functions were investi-
gated. Our experimental results on IWSLT Chinese-
English corpus have demonstrated consistent and
significant improvement over the widely used word
alignment matrix based extraction method. 3
Acknowledgement We would like to thank Xi-
aodong Cui, Radu Florian and other IBM colleagues
for useful discussions and the anonymous reviewers
for their constructive suggestions.
References
N. Ayan and B. Dorr. 2006. Going beyond AER: An
extensive analysis of word alignments and their impact
on MT. In Proc. of ACL, pages 9?16.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proc. of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
pages 65?72.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of machine transla-
tion: Parameter estimation. Computational Linguis-
tics, 19:263?312.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310?318.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In Proc.
of HLT-EMNLP, pages 169?176.
3By parallelism, we have shown the feasibility and effec-
tiveness (results not presented here) of the proposed method in
handling millions of sentence pairs.
H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Im-
proving translation quality by discarding most of the
phrasetable. In Proc. of EMNLP-CoNLL, pages 967?
975.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of ACL, pages 317?
324.
D. Marcu and D. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. Computer Journal, 7:308?
313.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och, D. Gildea, and et al 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
HLT-NAACL, pages 161?168.
F. Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen, Germany.
A. V. Oppenheim and R. W. Schafer. 1989. Discrete-
Time Signal Processing. Prentice-Hall.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Paul. 2006. Overview of the IWSLT 2006 evaluation
campaign. In Proc. of IWSLT, pages 1?15.
C. Tillmann and T. Zhang. 2006. A discriminative global
training algorithm for statistical MT. In Proc. of ACL,
pages 721?728.
A. Venugopal, S. Vogel, and A. Waibel. 2003. Effective
phrase translation extraction from alignment models.
In Proc. of ACL, pages 319?326.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In Proc. of
the COLING.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proc. of ACL,
pages 244?251.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. of ACL, pages 523?
530.
R. Zens, E. Matusov, and H. Ney. 2004. Improved word
alignment using a symmetric lexicon model. In Proc.
of COLING, pages 36?42.
B. Zhao, S. Vogel, M. Eck, and A. Waibel. 2004. Phrase
pair rescoring with term weighting for statistical ma-
chine translation. In Proc. of EMNLP, pages 206?213.
88
Do We Need Chinese Word Segmentation
for Statistical Machine Translation?
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science VI
Computer Science Department
RWTH Aachen University, Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In Chinese texts, words are not separated by
white spaces. This is problematic for many nat-
ural language processing tasks. The standard
approach is to segment the Chinese character
sequence into words. Here, we investigate Chi-
nese word segmentation for statistical machine
translation. We pursue two goals: the first one
is the maximization of the final translation qual-
ity; the second is the minimization of the man-
ual effort for building a translation system.
The commonly used method for getting the
word boundaries is based on a word segmenta-
tion tool and a predefined monolingual dictio-
nary. To avoid the dependence of the trans-
lation system on an external dictionary, we
have developed a system that learns a domain-
specific dictionary from the parallel training
corpus. This method produces results that are
comparable with the predefined dictionary.
Further more, our translation system is able
to work without word segmentation with only a
minor loss in translation quality.
1 Introduction
In Chinese texts, words composed of single or
multiple characters, are not separated by white
spaces, which is different from most of the west-
ern languages. This is problematic for many
natural language processing tasks. Therefore,
the usual method is to segment a Chinese char-
acter sequence into Chinese ?words?.
Many investigations have been performed
concerning Chinese word segmentation. For
example, (Palmer, 1997) developed a Chinese
word segmenter using a manually segmented
corpus. The segmentation rules were learned
automatically from this corpus. (Sproat and
Shih, 1990) and (Sun et al, 1998) used a
method that does not rely on a dictionary or a
manually segmented corpus. The characters of
the unsegmented Chinese text are grouped into
pairs with the highest value of mutual informa-
tion. This mutual information can be learned
from an unsegmented Chinese corpus.
We will present a new method for segment-
ing the Chinese text without using a manually
segmented corpus or a predefined dictionary. In
statistical machine translation, we have a bilin-
gual corpus available, which is used to obtain
a segmentation of the Chinese text in the fol-
lowing way. First, we train the statistical trans-
lation models with the unsegmented bilingual
corpus. As a result, we obtain a mapping of
Chinese characters to the corresponding English
words for each sentence pair. By using this map-
ping, we can extract a dictionary automatically.
With this self-learned dictionary, we use a seg-
mentation tool to obtain a segmented Chinese
text. Finally, we retrain our translation system
with the segmented corpus.
Additionally, we have performed experiments
without explicit word segmentation. In this
case, each Chinese character is interpreted as
one ?word?. Based on word groups, our ma-
chine translation system is able to work without
a word segmentation, while having only a minor
translation quality relative loss of less than 5%.
2 Review of the Baseline System for
Statistical Machine Translation
2.1 Principle
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =
f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =
e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
The decomposition into two knowledge sources
in Equation 2 is known as the source-channel
approach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1)1. The target lan-
guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to the
target language sentence. The argmax opera-
tion denotes the search problem, i.e. the gener-
ation of the output sentence in the target lan-
guage. We have to maximize over all possible
target language sentences.
The resulting architecture for the statistical
machine translation approach is shown in Fig-
ure 1 with the translation model further decom-
posed into lexicon and alignment model.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation ap-
proach based on Bayes decision rule.
2.2 Alignment Models
The alignment model Pr(fJ1 , aJ1 |eI1) introduces
a ?hidden? alignment a = aJ1 , which describes
1The notational convention will be as follows: we use
the symbol Pr(?) to denote general probability distri-
butions with (nearly) no specific assumptions. In con-
trast, for model-based probability distributions, we use
the generic symbol p(?).
a mapping from a source position j to a target
position aj . The relationship between the trans-
lation model and the alignment model is given
by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (3)
In this paper, we use the models IBM-1, IBM-
4 from (Brown et al, 1993) and the Hidden-
Markov alignment model (HMM) from (Vogel et
al., 1996). All these models provide different de-
compositions of the probability Pr(fJ1 , aJ1 |eI1).
A detailed description of these models can be
found in (Och and Ney, 2003).
A Viterbi alignment a?J1 of a specific model is
an alignment for which the following equation
holds:
a?J1 = argmax
aJ1
Pr(fJ1 , aJ1 |eI1). (4)
The alignment models are trained on a bilin-
gual corpus using GIZA++(Och et al, 1999;
Och and Ney, 2003). The training is done it-
eratively in succession on the same data, where
the final parameter estimates of a simpler model
serve as starting point for a more complex
model. The result of the training procedure is
the Viterbi alignment of the final training iter-
ation for the whole training corpus.
2.3 Alignment Template Approach
In the translation approach from Section 2.1,
one disadvantage is that the contextual informa-
tion is only taken into account by the language
model. The single-word based lexicon model
does not consider the surrounding words. One
way to incorporate the context into the trans-
lation model is to learn translations for whole
word groups instead of single words. The key
elements of this translation approach (Och et
al., 1999) are the alignment templates. These
are pairs of source and target language phrases
with an alignment within the phrases.
The alignment templates are extracted from
the bilingual training corpus. The extraction al-
gorithm (Och et al, 1999) uses the word align-
ment information obtained from the models in
Section 2.2. Figure 2 shows an example of a
word aligned sentence pair. The word align-
ment is represented with the black boxes. The
figure also includes some of the possible align-
ment templates, represented as the larger, un-
filled rectangles. Note that the extraction algo-
rithm would extract many more alignment tem-
plates from this sentence pair. In this example,
the system input was the sequence of Chinese
characters without any word segmentation. As
can be seen, a translation approach that is based
on phrases circumvents the problem of word seg-
mentation to a certain degree. This method will
be referred to as ?translation with no segmen-
tation? (see Section 5.2).
they
will
also
go
to
hangzhou
for
a
visit
Figure 2: Example of a word aligned sentence
pair and some possible alignment templates.
In the Chinese?English DARPA TIDES eval-
uations in June 2002 and May 2003, carried out
by NIST (NIST, 2003), the alignment template
approach performed very well and was ranked
among the best translation systems.
Further details on the alignment template ap-
proach are described in (Och et al, 1999; Och
and Ney, 2002).
3 Task and Corpus Statistics
In Section 5.3, we will present results for a
Chinese?English translation task. The domain
of this task is news articles. As bilingual train-
ing data, we use a corpus composed of the En-
glish translations of a Chinese Treebank. This
corpus is provided by the Linguistic Data Con-
sortium (LDC), catalog number LDC2002E17.
In addition, we use a bilingual dictionary with
10K Chinese word entries provided by Stephan
Vogel (LDC, 2003b).
Table 1 shows the corpus statistics of this
task. We have calculated both the number of
words and the number of characters in the cor-
pus. In average, a Chinese word is composed
of 1.49 characters. For each of the two lan-
guages, there is a set of 20 special characters,
such as digits, punctuation marks and symbols
like ?()%$...?
The training corpus will be used to train a
word alignment and then extract the alignment
templates and the word-based lexicon. The re-
sulting translation system will be evaluated on
the test corpus.
Table 1: Statistics of training and test corpus.
For each of the two languages, there is a set of 20
special characters, such as digits, punctuation
marks and symbols like ?()%$...?
Chinese English
Train Sentences 4 172
Characters 172 874 832 760
Words 116 090 145 422
Char. Vocab. 3 419 + 20 26 + 20
Word Vocab. 9 391 9 505
Test Sentences 993
Characters 42 100 167 101
Words 28 247 26 225
4 Segmentation Methods
4.1 Conventional Method
The commonly used segmentation method is
based on a segmentation tool and a monolingual
Chinese dictionary. Typically, this dictionary
has been produced beforehand and is indepen-
dent of the Chinese text to be segmented. The
dictionary contains Chinese words and their fre-
quencies. This information is used by the seg-
mentation tool to find the word boundaries. In
the LDC method (see Section 5.2) we have used
the dictionary and segmenter provided by the
LDC. More details can be found on the LDC
web pages (LDC, 2003a). This segmenter is
based on two ideas: it prefers long words over
short words and it prefers high frequency words
over low frequency words.
4.2 Dictionary Learning from Alignments
In this section, we will describe our method of
learning a dictionary from a bilingual corpus.
As mentioned before, the bilingual training
corpus listed in Section 3 is the only input to the
system. We firstly divide every Chinese charac-
ters in the corpus by white spaces, then train
the statistical translation models with this un-
segmented Chinese text and its English trans-
lation, details of the training method are de-
scribed in Section 2.2.
To extract Chinese words instead of phrases
as in Figure 2, we configure the training pa-
rameters in GIZA++, the alignment is then re-
stricted to a multi-source-single-target relation-
ship, i.e. one or more Chinese characters are
translated to one English word.
The result of this training procedure is an
alignment for each sentence pair. Such an align-
ment is represented as a binary matrix with J ?I
elements.
An example is shown in Figure 3. The un-
segmented Chinese training sentence is plotted
along the horizontal axes and the corresponding
English sentence along the vertical axes. The
black boxes show the Viterbi alignment for this
sentence pair. Here, for example the first two
Chinese characters are aligned to ?industry?,
the next four characters are aligned to ?restruc-
turing?.
industry
restructuring
made
vigorous
progress
Figure 3: Example of an alignment without
word segmentation.
The central idea of our dictionary learning
method is: a contiguous sequence of Chinese
characters constitute a Chinese word, if they
are aligned to the same English word. Using
this idea and the bilingual corpus, we can au-
tomatically generate a Chinese word dictionary.
Table 2 shows the Chinese words that are ex-
tracted from the alignment in Figure 3.
Table 2: Word entries in Chinese dictionary
learned from the alignment in Figure 3.
We extract Chinese words from all sentence
pairs in the training corpus. Therefore, it is
straightforward to collect word frequency statis-
tics that are needed for the segmentation tool.
Once, we have generated the dictionary, we can
produce a segmented Chinese corpus using the
method described in Section 4.1. Then, we
retrain the translation system using the seg-
mented Chinese text.
4.3 Word Length Statistics
In this section, we present statistics of the word
lengths in the LDC dictionary as well as in the
self-learned dictionary extracted from the align-
ment.
Table 3 shows the statistics of the word
lengths in the LDC dictionary as well as in
the learned dictionary. For example, there are
2 368 words consisting of a single character in
learned dictionary and 2 511 words in the LDC
dictionary. These single character words rep-
resent 16.9% of the total number of entries in
the learned dictionary and 18.6% in the LDC
dictionary.
We see that in the LDC dictionary more than
65% of the words consist of two characters and
about 30% of the words consist of a single char-
acter or three or four characters. Longer words
with more than four characters constitute less
than 1% of the dictionary. In the learned dic-
tionary, there are many more long words, about
15%. A subjective analysis showed that many
of these entries are either named entities or
idiomatic expressions. Often, these idiomatic
expressions should be segmented into shorter
words. Therefore, we will investigate methods
to overcome this problem in the future. Some
suggestions will be discussed in Section 6.
Table 3: Statistics of word lengths in the LDC
dictionary and in the learned dictionary.
word LDC dictionary learned dictionary
length frequency [%] frequency [%]
1 2 334 18.6 2 368 16.9
2 8 149 65.1 5 486 39.2
3 1 188 9.5 1 899 13.6
4 759 6.1 2 084 14.9
5 70 0.6 791 5.7
6 20 0.2 617 4.4
7 6 0.0 327 2.3
?8 11 0.0 424 3.0
total 12 527 100 13 996 100
5 Translation Experiments
5.1 Evaluation Criteria
So far, in machine translation research, a sin-
gle generally accepted criterion for the evalu-
ation of the experimental results does not ex-
ist. We have used three automatic criteria. For
the test corpus, we have four references avail-
able. Hence, we compute all the following cri-
teria with respect to multiple references.
? WER (word error rate):
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed
to convert the generated sentence into the
reference sentence.
? PER (position-independent word error
rate):
A shortcoming of the WER is that it re-
quires a perfect word order. The word or-
der of an acceptable sentence can be dif-
ferent from that of the target sentence, so
that the WER measure alone could be mis-
leading. The PER compares the words in
the two sentences ignoring the word order.
? BLEU score:
This score measures the precision of un-
igrams, bigrams, trigrams and fourgrams
with respect to a reference translation with
a penalty for too short sentences (Papineni
et al, 2001). The BLEU score measures
accuracy, i.e. large BLEU scores are bet-
ter.
5.2 Summary: Three Translation
Methods
In the experiments, we compare the following
three translation methods:
? Translation with no segmentation: Each
Chinese character is interpreted as a single
word.
? Translation with learned segmentation:
It uses the self-learned dictionary.
? Translation with LDC segmentation:
The predefined LDC dictionary is used.
The core contribution of this paper is the
method we called ?translation with learned seg-
mentation?, which consists of three steps:
? The input is a sequence of Chinese charac-
ters without segmentation. After the train-
ing using GIZA++, we extract a mono-
lingual Chinese dictionary from the align-
ment. This is discussed in Section 4.2, and
an example is given in Figure 3 and Table 2.
? Using this learned dictionary, we segment
the sequence of Chinese characters into
words. In other words, the LDC method
is used, but the LDC dictionary is replaced
by the learned dictionary (see Section 4.1).
? Based on this word segmentation, we
perform another training using GIZA++.
Then, after training the models IBM1,
HMM and IBM4, we extract bilingual word
groups, which are referred as alignment
templates.
5.3 Evaluation Results
The evaluation is performed on the LDC corpus
described in Section 3. The translation perfor-
mance of the three systems is summarized in
Table 4 for the three evaluation criteria WER,
PER and BLEU. We observe that the trans-
lation quality with the learned segmentation is
similar to that with the LDC segmentation. The
WER of the system with the learned segmenta-
tion is somewhat better, but PER and BLEU
are slightly worse. We conclude that it is possi-
ble to learn a domain-specific dictionary for Chi-
nese word segmentation from a bilingual corpus.
Therefore the translation system is independent
of a predefined dictionary, which may be unsuit-
able for a certain task.
The translation system using no segmenta-
tion performs slightly worse. For example, for
the WER there is a loss of about 2% relative
compared to the system with the LDC segmen-
tation.
Table 4: Translation performance of different
segmentation methods (all numbers in percent).
method error rates accuracy
WER PER BLEU
no segment. 73.3 56.5 27.6
learned segment. 70.4 54.6 29.1
LDC segment. 71.9 54.4 29.2
5.4 Effect of Segmentation on
Translation Results
In this section, we present three examples of the
effect that segmentation may have on transla-
tion quality. For each of the three examples in
Figure 4, we show the segmented Chinese source
sentence using either the LDC dictionary or the
self-learned dictionary, the corresponding trans-
lation and the human reference translation.
In the first example, the LDC dictionary
leads to a correct segmentation, whereas with
the learned dictionary the segmentation is erro-
neous. The second and third token should be
combined (?Hong Kong?), whereas the fifth to-
ken should be separated (?stabilize in the long
term?). In this case, the wrong segmentation of
the Chinese source sentence does not result in a
wrong translation. A possible reason is that the
translation system is based on word groups and
can recover from these segmentation errors.
In the second example, the segmentation with
the LDC dictionary produces at least one error.
The second and third token should be combined
(?this?). It is possible to combine the seventh
and eighth token to a single word because the
eighth token shows only the tense. The segmen-
tation with the learned dictionary is correct.
Here, the two segmentations result in different
translations.
In the third example, both segmentations are
incorrect and these segmentation errors affect
the translation results. In the segmentation
with the LDC dictionary, the first Chinese char-
acters should be segmented as a separate word.
The second and third character and maybe even
the fourth character should be combined to one
word.2 The fifth and sixth character should be
combined to a single word. In the segmentation
with the learned dictionary, the fifth and sixth
token (seventh and eighth character) should be
combined (?isolated?). We see that this term is
missing in the translation. Here, the segmenta-
tion errors result in translation errors.
6 Discussion and Future Work
We have presented a new method for Chinese
word segmentation. It avoids the use of a pre-
defined dictionary and instead learns a corpus-
specific dictionary from the bilingual training
corpus.
The idea is extracting a self-learned dictio-
nary from the trained alignment models. This
method has the advantage that the word entries
in the dictionary all occur in the training data,
and its content is much closer to the training
text as a predefined dictionary, which can never
cover all possible word occurrences. Here, if the
content of the test corpus is closer to that of the
2This is an example of an ambiguous segmentation.
Example 1
LDC dictionary:
                              
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Learned dictionary:
                          
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Reference:
It will be benificial for the stability and
prosperity of Hong Kong in the long run.
Example 2
LDC dictionary:
                               
         
but this meeting down or achieved certain
progress.
Learned dictionary:
                             
    
however, this meeting straight down still
achieved certain progress.
Reference:
Neverless, this meeting has achieved some
progress.
Example 3
LDC dictionary:
...                                 
 ...
... the unification of the world carried adjacent
isolate of proof, ...
Learned dictionary:
...                                
 ...
... in the world faced with a became another
proof, ...
Reference:
... another proof that ... is facing isolation in the
world ...
Figure 4: Translation examples using the
learned dictionary and the LDC dictionary.
training corpus, the quality of the dictionary is
higher and the translation performance would
be better.
The experiments showed that the transla-
tion quality with the learned segmentation is
competitive with the LDC segmentation. Ad-
ditionally, we have shown the feasibility of a
Chinese?English statistical machine translation
system that works without any word segmenta-
tion. There is only a minor loss in translation
performance. Further improvements could be
possible by tuning the system toward this spe-
cific task.
We expect that our method could be im-
proved by considering the word length as dis-
cussed in Section 4.3. As shown in the word
length statistics, long words with more than
four characters occur only occasionally. Most of
them are named entity words, which are writ-
ten in English in upper case. Therefore, we can
apply a simple rule: we accept a long Chinese
word only if the corresponding English word is
in upper case. This should result in an improved
dictionary. An alternative way is to use the
word length statistics in Table 3 as a prior dis-
tribution. In this case, long words would get a
penalty, because their prior probability is low.
Because the extraction of our dictionary is
based on bilingual information, it might be in-
teresting to combine it with methods that use
monolingual information only.
For Chinese?English, there is a large num-
ber of bilingual corpora available at the LDC.
Therefore using additional corpora, we can ex-
pect to get an improved dictionary.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Pa-
rameter estimation. Computational Linguis-
tics, 19(2):263?311, June.
LDC. 2003a. LDC Chinese resources home
page. http://www.ldc.upenn.edu/Projects/
Chinese/LDC ch.htm.
LDC. 2003b. LDC resources home page.
http://www.ldc.upenn.edu/Projects/TIDES/
mt2004cn.htm.
NIST. 2003. Machine translation home page.
http://www.nist.gov/speech/tests/mt/
index.htm.
F. J. Och and H. Ney. 2002. Discriminative
training and maximum entropy models for
statistical machine translation. In Proc. of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51,
March.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical ma-
chine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natu-
ral Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland,
College Park, MD, June.
D. D. Palmer. 1997. A trainable rule-based
algorithm for word segmentation. In Proc.
of the 35th Annual Meeting of ACL and 8th
Conference of the European Chapter of ACL,
pages 321?328, Madrid, Spain, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J.
Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Techni-
cal Report RC22176 (W0109-022), IBM Re-
search Division, Thomas J. Watson Research
Center, September.
R. W. Sproat and C. Shih. 1990. A statistical
method for finding word boundaries in Chi-
nese text. Computer Processing of Chinese
and Oriental Languages, 4:336?351.
M. Sun, D. Shen, and B. K. Tsou. 1998. Chi-
nese word segmentation without using lexi-
con and hand-crafted training data. In Proc.
of the 36th Annual Meeting of ACL and
17th Int. Conf. on Computational Linguistics
(COLING-ACL 98), pages 1265?1271, Mon-
treal, Quebec, Canada, August.
S. Vogel, H. Ney, and C. Tillmann. 1996.
HMM-based word alignment in statistical
translation. In COLING ?96: The 16th Int.
Conf. on Computational Linguistics, pages
836?841, Copenhagen, Denmark, August.
Proceedings of the Workshop on Statistical Machine Translation, pages 78?85,
New York City, June 2006. c?2006 Association for Computational Linguistics
Partitioning Parallel Documents Using Binary Segmentation
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science 6
Computer Science Department
RWTH Aachen University
D-52056 Aachen Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, large
numbers of parallel sentences are required
to train the model parameters. However,
plenty of the bilingual language resources
available on web are aligned only at the
document level. To exploit this data,
we have to extract the bilingual sentences
from these documents.
The common method is to break the doc-
uments into segments using predefined
anchor words, then these segments are
aligned. This approach is not error free,
incorrect alignments may decrease the
translation quality.
We present an alternative approach to ex-
tract the parallel sentences by partitioning
a bilingual document into two pairs. This
process is performed recursively until all
the sub-pairs are short enough.
In experiments on the Chinese-English
FBIS data, our method was capable of
producing translation results comparable
to those of a state-of-the-art sentence
aligner. Using a combination of the two
approaches leads to better translation per-
formance.
1 Introduction
Current statistical machine translation systems use
bilingual sentences to train the parameters of the
translation models. The exploitation of more bilin-
gual sentences automatically and accurately as well
as the use of these data with the limited computa-
tional requirements become crucial problems.
The conventional method for producing parallel
sentences is to break the documents into sentences
and to align these sentences using dynamic program-
ming. Previous investigations can be found in works
such as (Gale and Church, 1993) and (Ma, 2006).
A disadvantage is that only the monotone sentence
alignments are allowed.
Another approach is the binary segmentation
method described in (Simard and Langlais, 2003),
(Xu et al, 2005) and (Deng et al, 2006), which
separates a long sentence pair into two sub-pairs re-
cursively. The binary reordering in alignment is al-
lowed but the segmentation decision is only opti-
mum in each recursion step.
Hence, a combination of both methods is ex-
pected to produce a more satisfying result. (Deng
et al, 2006) performs a two-stage procedure. The
documents are first aligned at level using dynamic
programming, the initial alignments are then refined
to produce shorter segments using binary segmen-
tation. But on the Chinese-English FBIS training
corpus, the alignment accuracy and recall are lower
than with Champollion (Ma, 2006).
We refine the model in (Xu et al, 2005) using
a log-linar combination of different feature func-
tions and combine it with the approach of (Ma,
2006). Here the corpora produced using both ap-
proaches are concatenated, and each corpus is as-
signed a weight. During the training of the word
alignment models, the counts of the lexicon entries
78
are linear interpolated using the corpus weights. In
the experiments on the Chinese-English FBIS cor-
pus the translation performance is improved by 0.4%
of the BLEU score compared to the performance
only with Champollion.
The remainder of this paper is structured as fol-
lows: First we will briefly review the baseline statis-
tical machine translation system in Section 2. Then,
in Section 3, we will describe the refined binary seg-
mentation method. In Section 4.1, we will introduce
the methods to extract bilingual sentences from doc-
ument aligned texts. The experimental results will
be presented in Section 4.
2 Review of the Baseline Statistical
Machine Translation System
In this section, we briefly review our translation sys-
tem and introduce the word alignment models.
In statistical machine translation, we are given
a source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )
}
= argmax
I,eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (1)
The decomposition into two knowledge sources in
Equation 1 allows independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1)1. The translation model can be further
extended to a statistical alignment model with the
following equation:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
The alignment model Pr(fJ1 , aJ1 |eI1) introduces a
?hidden? word alignment a = aJ1 , which describes a
mapping from a source position j to a target position
aj .
1The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
Monotone Non-
monotone
Target B A
Positions C D
Source Positions
Figure 1: Two Types of Alignment
The IBM model 1 (IBM-1) (Brown et al, 1993)
assumes that all alignments have the same probabil-
ity by using a uniform distribution:
p(fJ1 |eI1) =
1
IJ ?
J?
j=1
I?
i=1
p(fj |ei) (2)
We use the IBM-1 to train the lexicon parameters
p(f |e), the training software is GIZA++ (Och and
Ney, 2003).
To incorporate the context into the translation
model, the phrase-based translation approach (Zens
et al, 2005) is applied. Pairs of source and tar-
get language phrases are extracted from the bilin-
gual training corpus and a beam search algorithm is
implemented to generate the translation hypothesis
with maximum probability.
3 Binary Segmentation Method
3.1 Approach
Here a document or sentence pair (fJ1 , eI1) 2 is repre-
sented as a matrix. Every element in the matrix con-
tains a lexicon probability p(fj |ei), which is trained
on the original parallel corpora. Each position di-
vides a matrix into four parts as shown in Figure 1:
the bottom left (C), the upper left (A), the bottom
right (D) and the upper right (B). We use m to de-
note the alignment direction, m = 1 means that the
alignment is monotone, i.e. the bottom left part is
connected with the upper right part, and m = 0
means the alignment is non-monotone, i.e. the upper
left part is connected with the bottom right part, as
shown in Figure 1.
3.2 Log-Linear Model
We use a log-linear interpolation to combine differ-
ent models: the IBM-1, the inverse IBM-1, the an-
2Sentences are equivalent to segments in this paper.
79
chor words model as well as the IBM-4. K denotes
the total number of models.
We go through all positions in the bilingual sen-
tences and find the best position for segmenting the
sentence:
(?i, j?, m?) = argmax
i,j,m
{ K?
k=1
?khk(j, i,m|fJ1 , eI1)
}
,
where i ? [1, I ? 1] and j ? [1, J ? 1] are posi-
tions in the source and target sentences respectively.
The feature functions are described in the follow-
ing sections. In most cases, the sentence pairs are
quite long and even after one segmentation we may
still have long sub-segments. Therefore, we separate
the sub-segment pairs recursively until the length of
each new segment is less than a defined value.
3.3 Normalized IBM-1
The function in Equation 2 can be normalized by
the source sentence length with a weighting ? as de-
scribed in (Xu et al, 2005):
The monotone alignment is calculated as
h1(j, i, 1|fJ1 , eI1) = log(p(f j1 |ei1)??
1
j+(1??) (3)
?p(fJj+1|eIi+1)??
1
J?j+(1??)),
and the non-monotone alignment is formulated in
the same way.
We also use the inverse IBM-1 as a feature, by ex-
changing the place of ei1 and f j1 its monotone align-
ment is calculated as:
h2(j, i, 1|fJ1 , eI1) = log(p(ei1|f j1 )??
1
i+(1??) (4)
?p(eIi+1|fJj+1)??
1
I?i+(1??))
3.4 Anchor Words
In the task of extracting parallel sentences from
the paragraph-aligned corpus, selecting some anchor
words as preferred segmentation positions can ef-
fectively avoid the extraction of incomplete segment
pairs. Therefore we use an anchor words model to
prefer the segmentation at the punctuation marks,
where the source and target words are identical:
h3(j, i,m|fJ1 , eI1) =
{ 1 : fj = ei ? ei ? A
0 : otherwise
A is a user defined anchor word list, here we use
A={.,??;}. If the corresponding model scaling factor
?3 is assigned a high value, the segmentation posi-
tions are mostly after anchor words.
3.5 IBM-4 Word Alignment
If we already have the IBM-4 Viterbi word align-
ments for the parallel sentences and need to retrain
the system, for example to optimize the training pa-
rameters, we can include the Viterbi word align-
ments trained on the original corpora into the binary
segmentation. In the monotone case, the model is
represented as
h4(j, i, 1|fJ1 , eI1) =
log
(
N(f j1 , ei1) +N(fJj+1, eIi+1)
N(fJ1 , eI1)
)
,
where N(f j1 , ei1) denotes the number of the align-
ment links inside the matrix (1, 1) and (j, i). In the
non-monotone case the model is formulated in the
same way.
3.6 Word Alignment Concatenation
As described in Section 2, our translation is based on
phrases, that means for an input sentence we extract
all phrases matched in the training corpus and trans-
late with these phrase pairs. Although the aim of
segmentation is to split parallel text into translated
segment pairs, but the segmentation is still not per-
fect. During sentence segmentation we might sep-
arate a phrase into two segments, so that the whole
phrase pair can not be extracted.
To avoid this, we concatenate the word align-
ments trained with the segmentations of one sen-
tence pair. During the segmentation, the position of
each segmentation point in the sentence is memo-
rized. After training the word alignment model with
the segmented sentence pairs, the word alignments
are concatenated again according to the positions of
their segments in the sentences. The original sen-
tence pairs and the concatenated alignments are then
used for the phrase extraction.
80
Table 1: Corpus Statistics: NIST
Chinese English
Train Sentences 8.64 M
Running Words 210 M 226 M
Average Sentence Length 24.4 26.3
Vocabulary 224 268 359 623
Singletons 98 842 156 493
Segmentation Sentences 17.9 M
Running Words 210 M 226 M
Average Sentence Length 11.7 12.6
Vocabulary 221 517 353 148
Singletons 97 062 152 965
Segmentation with Additional Data Sentences 19.5 M
Running Words 230 M 248 M
Added Running Words 8.0% 8.2%
Evaluation Sentences 878 3 512
Running Words 24 111 105 516
Vocabulary 4 095 6 802
OOVs (Running Words) 8 658
4 Translation Experiments
4.1 Bilingual Sentences Extraction Methods
In this section, we describe the different methods to
extract the bilingual sentence pairs from the docu-
ment aligned corpus.
Given each document pair, we assume that the
paragraphs are aligned one to one monotone if both
the source and target language documents contain
the same number of paragraphs; otherwise the para-
graphs are aligned with the Champollion tool.
Starting from the parallel paragraphs we extract
the sentences using three methods:
1. Binary segmentation
The segmentation method described in Sec-
tion 3 is applied by treating the paragraph pairs
as long sentence pairs. We can use the anchor
words model described in Section 3.4 to prefer
splitting at punctuation marks.
The lexicon parameters p(f |e) in Equation 2
are estimated as follows: First the sentences are
aligned roughly using the dynamic program-
ming algorithm. Training on these aligned sen-
tences, we get the initial lexicon parameters.
Then the binary segmentation algorithm is ap-
plied to extract the sentences again.
2. Champollion
After a paragraph is divided into sentences at
punctuation marks, the Champollion tool (Ma,
2006) is used, which applies dynamic program-
ming for the sentence alignment.
3. Combination
The bilingual corpora produced by the binary
segmentation and Champollion methods are
concatenated and are used in the training of the
translation model. Each corpus is assigned a
weight. During the training of the word align-
ment models, the counts of the lexicon en-
tries are linearly interpolated using the corpus
weights.
4.2 Translation Tasks
We will present the translation results on two
Chinese-English tasks.
1. On the large data track NIST task (NIST,
2005), we will show improvements using the
refined binary segmentation method.
81
Table 2: Corpus Statistics: FBIS
Segmentation Champollion
Chinese English Chinese English
Train Sentences 739 899 177 798
Running Words 8 588 477 10 111 752 7 659 776 9 801 257
Average Sentence Length 11.6 13.7 43.1 55.1
Vocabulary 34 896 56 573 34 377 55 775
Singletons 4 775 19 283 4 588 19 004
Evaluation Sentences 878 3 513 878 3 513
Running Words 24 111 105 516 24 111 105 516
Vocabulary 4 095 6 802 4 095 6 802
OOVs (Running Words) 109 2 257 119 2 309
2. On the FBIS corpus, we will compare the dif-
ferent sentence extraction methods described in
Section 4.1 with respect to translation perfor-
mance. We do not apply the extraction meth-
ods on the whole NIST corpora, because some
corpora provided by the LDC (LDC, 2005) are
sentence aligned but not document aligned.
4.3 Corpus Statistics
The training corpora used in NIST task are a set of
individual corpora including the FBIS corpus. These
corpora are provided by the Linguistic Data Consor-
tium (LDC, 2005), the domains are news articles.
The translation experiments are carried out on the
NIST 2002 evaluation set.
As shown in Table 1, there are 8.6 million sen-
tence pairs in the original corpora of the NIST task.
The average sentence length is about 25. After seg-
mentation, there are twice as many sentence pairs,
i.e. 17.9 million, and the average sentence length
is around 12. Due to a limitation of GIZA++, sen-
tences consisting of more than one hundred words
are filtered out. Segmentation of long sentences cir-
cumvents this restriction and allows us include more
data. Here we were able to add 8% more Chinese
and 8.2% more English running words to the train-
ing data. The training time is also reduced.
Table 2 presents statistics of the FBIS data. Af-
ter the paragraph alignment described in Section 4.1
we have nearly 81 thousand paragraphs, 8.6 million
Chinese and 10.1 million English running words.
One of the advantages of the binary segmentation is
that we do not loose words during the bilingual sen-
tences extraction. However, we produce sentence
pairs with very different lengths. Using Champol-
lion we loose 10.8% of the Chinese and 3.1% of the
English words.
4.4 Segmentation Parameters
We did not optimize the log-linear model scaling
factors for the binary segmentation but used the fol-
lowing fixed values: ?1 = ?2 = 0.5 for the IBM-1
models in both directions; ?3 = 108, if the anchor
words model is is used; ?4 = 30, if the IBM-4 model
is used. The maximum sentence length is 25.
4.5 Evaluation Criteria
We use four different criteria to evaluate the transla-
tion results automatically:
? WER (word error rate):
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the reference sentence,
divided by the reference sentence length.
? PER (position-independent word error rate):
A shortcoming of the WER is that it requires a
perfect word order. The word order of an ac-
ceptable sentence can be differ from that of the
target sentence, so that the WER measure alone
could be misleading. The PER compares the
words in the two sentences ignoring the word
order.
? BLEU score:
This score measures the precision of unigrams,
82
0 0.2 0.4 0.6 0.8 131.8
31.9
32
32.1
32.2
Weight for the Binary Segmentation
BLE
U[%
]
Figure 2: Translation performance as a function of
the weight for the binary segmentation ? ( weight
for Champollion: 1? ? )
bigrams, trigrams and fourgrams with a penalty
for too short sentences. (Papineni et al, 2002).
? NIST score:
This score is similar to BLEU, but it uses
an arithmetic average of N-gram counts rather
than a geometric average, and it weights more
heavily those N-grams that are more informa-
tive. (Doddington, 2002).
The BLEU and NIST scores measure accuracy,
i.e. larger scores are better. In our evaluation the
scores are measured as case insensitive and with re-
spect to multiple references.
4.6 Translation Results
For the segmentation of long sentences into short
segments, we performed the experiments on the
NIST task. Both in the baseline and the segmenta-
tion systems we obtain 4.7 million bilingual phrases
during the translation. The method of alignment
concatenation increases the number of the extracted
bilingual phrase pairs from 4.7 million to 4.9 mil-
lion, the BLEU score is improved by 0.1%. By
including the IBM-4 Viterbi word alignment, the
NIST score is improved. The training of the base-
line system requires 5.9 days, after the sentence seg-
mentation it requires only 1.5 days. Moreover, the
segmentation allows the inclusion of long sentences
that are filtered out in the baseline system. Using
the added data, the translation performance is en-
hanced by 0.3% in the BLEU score. Because of
the long translation period, the translation parame-
ters are only optimized on the baseline system with
respect to the BLEU score, we could expect a further
improvement if the parameters were also optimized
on the segmentation system.
Our major objective here is to introduce another
approach to parallel sentence extraction: binary seg-
mentation of the bilingual texts recursively. We use
the paragraph-aligned corpus as a starting point. Ta-
ble 4 presents the translation results on the train-
ing corpora generated by the different methods de-
scribed in Section 4.1. The translation parameters
are optimized with the respect to the BLEU score.
We observe that the binary segmentation methods
are comparable to Champollion and the segmenta-
tion with anchors outperforms the one without an-
chors. By combining the methods of Champol-
lion and the binary segmentation with anchors, the
BLEU score is improved by 0.4% absolutely.
We optimized the weightings for the binary seg-
mentation method, the sum of the weightings for
both methods is one. As shown in Figure 2, using
one of the methods alone does not produce the best
result. The maximum BLEU score is attained when
both methods are combined with equal weightings.
5 Discussion and Future Work
We successfully applied the binary sentence seg-
mentation method to extract bilingual sentence pairs
from the document aligned texts. The experiments
on the FBIS data show an enhancement of 0.4% of
the BLEU score compared to the score obtained us-
ing a state-of-art sentence aligner. In addition to the
encouraging results obtained, further improvements
could be achieved in the following ways:
1. By extracting bilingual paragraphs from the
documents, we lost running words using Cham-
pollion. Applying the segmentation approach
to paragraph alignment might avoid the loss of
this data.
2. We combined a number of different models in
the binary segmentation, such as IBM-1, and
anchor words. The model weightings could be
optimized with respect to translation quality.
83
Table 3: Translation Results using Refined Segmentation Methods on NIST task
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Baseline 62.7 42.1 8.95 33.5
Segmentation 62.6 42.4 8.80 33.5
Segmentation + concatenation 62.4 42.3 8.84 33.6
Segmentation + concatenation + IBM-4 62.8 42.4 8.91 33.6
Segmentation + added data 62.9 42.5 9.00 33.9
Table 4: Translation Results on Sentence Alignment Task with FBIS Training Corpus
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Champollion 64.2 43.7 8.61 31.8
Segmentation without Anchors 64.3 44.4 8.57 31.8
Segmentation with Anchors 64.0 43.9 8.58 31.9
Champollion + Segmentation with Anchors 64.3 44.2 8.57 32.2
3. In the binary segmentation method, an incor-
rect segmentation results in further mistakes
in the segmentation decisions of all its sub-
segments. An alternative method (Wu, 1997)
makes decisions at the end but has a high com-
putational requirement. A restricted expansion
of the search space might better balance seg-
mentation accuracy and the efficiency.
6 Acknowledgments
This work was supported by the European Union
under the integrated project TC-Star (Technology
and Corpora for Speech to Speech Translation,
IST-2002-FP6-506738, http://www.tc-star.org) and
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Y. Deng, S. Kumar, and W. Byrne. 2006. Segmenta-
tion and alignment of parallel text for statistical ma-
chine translation. Natural Language Engineering, Ac-
cepted. To appear.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology,
pages 128?132, San Diego, California, March.
W. A. Gale and K. W. Church. 1993. A program for
aligning sentences in bilingual corpora. Computa-
tional Linguistics, 19(1):75?90.
LDC. 2005. Linguistic data consortium resource home
page. http://www.ldc.upenn.edu/Projects/TIDES.
X. Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of the fifth interna-
tional conference on Language Resources and Evalu-
ation (LREC), Genoa, Italy, Accepted. To appear.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, July.
M. Simard and P. Langlais. 2003. Statistical transla-
tion alignment with compositionality constraints. In
NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Be-
yond, Edmonton, Canada, May.
84
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
J. Xu, R. Zens, and H. Ney. 2005. Sentence segmentation
using IBM word alignment model 1. In Proceedings of
EAMT 2005 (10th Annual Conference of the European
Association for Machine Translation), pages 280?287,
Budapest, Hungary, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
85
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2031?2041, Dublin, Ireland, August 23-29 2014.
Query Lattice for Translation Retrieval
Meiping Dong
?
, Yong Cheng
?
, Yang Liu
?
, Jia Xu
?
, Maosong Sun
?
,
Tatsuya Izuha

, Jie Hao
#
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
hellodmp@163.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
chengyong3001@gmail.com, xu@tsinghua.edu.cn

Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
#
Toshiba (China) R&D Center
haojie@toshiba.com.cn
Abstract
Translation retrieval aims to find the most likely translation among a set of target-language strings
for a given source-language string. Previous studies consider the single-best translation as a query
for information retrieval, which may result in translation error propagation. To alleviate this
problem, we propose to use the query lattice, which is a compact representation of exponentially
many queries containing translation alternatives. We verified the effectiveness of query lattice
through experiments, where our method explores a much larger search space (from 1 query to
1.24 ? 10
62
queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves
more accurately (from 83.76% to 93.16% in precision) than the standard method based on the
query single-best. In addition, we show that query lattice significantly outperforms the method
of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.
1 Introduction
Translation retrieval aims to search for the most probable translation candidate from a set of target-
language strings for a given source-language string. Early translation retrieval methods were widely
used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al.,
1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records
that are pairs of source-language and target-language strings. Given an input source string, the retrieval
system returns a translation record of maximum similarity to the input on the source side. Although these
methods prove to be effective in example-based and memory-based translation systems, they heavily rely
on parallel corpora that are limited both in size and domain.
More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends
only on monolingual corpora. Given an input source string, their system retrieves translation candidates
from a set of target-language sentences. This can be done by combining machine translation (MT) and
information retrieval (IR): machine translation is used to transform the input source string to a coarse
translation, which serves as a query to retrieve the most probable translation in the monolingual corpus.
Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora
that are readily available on the Web.
However, the MT + IR pipeline suffers from the translation error propagation problem. Liu et al.
(2012) use 1-best translations, which are inevitably erroneous due to the ambiguity and structural di-
vergence of natural languages, as queries to the IR module. As a result, translation mistakes will be
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Corresponding author: Jia Xu. Tel: +86-10-62781693 Ext 1683. Homepage: iiis.tsinghua.edu.cn/?xu
2031
propagated to the retrieval process. This situation aggravates when high-accuracy MT systems are not
available for resource-scarce languages.
In this work, we propose to use query lattice in translation retrieval to alleviate the translation error
propagation problem. A query lattice is a compact representation of exponentially many queries. We
design a retrieval algorithm that takes the query lattice as input to search for the most probable translation
candidate from a set of target-language sentences. As compared with Liu et al. (2012), our approach
explores a much larger search space (from 1 query to 1.24? 10
62
queries), runs much faster (from 0.75
second per sentence to 0.13), and retrieves more accurately (from 83.76% to 93.16%). We also evaluate
our approach on extracting parallel sentences from comparable corpora. Experiments show that our
translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system.
2 Related Work
Our work is inspired by three research topics: retrieving translation candidates from parallel corpus,
using lattice to compactly represent exponentially many alternatives, and using lattice as query in infor-
mation retrieval.
1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from
existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990;
Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use
a parallel corpus (e.g., translation records that are pairs of source-language and target-language
strings), they focus on calculating the similarity between two source-language strings. In contrast,
we evaluate the translational equivalence of a given source string and a target string in a large
monolingual corpus.
2. Lattice in Machine Translation. Lattices have been widely used in machine translation: consider-
ing Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Mat-
soukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes
risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system
combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a
lattice that encodes exponentially many translation candidates as a single query to retrieve similar
target sentences via an information retrieval system.
3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to
Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies
or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document
retrieval, however, lattices are used as a compact representation of multiple speech recognition
transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004;
Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that
uses the bag-of-words model because translation retrieval must take structure and dependencies in
text into account to ensure translational equivalence.
3 Query Lattice for Translation Retrieval
3.1 Translation Retrieval
Let f be a source-language string, E be a set of target-language strings, the problem is how to find the
most probable translation
?
e from E. Note that E is a monolingual corpus rather than a parallel corpus.
Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin
and Tanaka, 2000; Baldwin, 2001) does not apply here.
We use P (e|f) to denote the probability that a target-language sentence e is the translation of a source-
language sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by
2032
introducing a coarse translation q as a hidden variable:
P (e|f) =
?
q?Q(f)
P (q, e|f) (1)
=
?
q?Q(f)
P (q|f)? P (e|q, f) (2)
where P (q|f) is a translation sub-model, P (e|q, f) is a retrieval sub-model, and Q(f) is the set of all
possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model.
To take advantage of various translation and retrieval information sources, we use a log-linear model
(Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned
on a source sentence f parameterized by a real-valued vector ?:
P (q, e|f ;?) =
exp(? ? h(q, e, f))
?
q
?
?Q(f)
?
e
?
?E
exp(? ? h(q
?
, e
?
, f))
(3)
where h(?) is a vector of feature functions and ? is the corresponding feature weight vector.
Accordingly, the decision rule for the latent variable model is given by
?
e = argmax
e?E
{
?
q?Q(f)
exp(? ? h(q, e, f))
}
(4)
As there are exponentially many queries, it is efficient to approximate the summation over all possible
queries by using maximization instead:
?
e ? argmax
e?E
{
max
q?Q(f)
{
? ? h(q, e, f)
}
}
(5)
Unfortunately, the search space is still prohibitively large since we need to enumerate all possible
queries. Liu et al. (2012) split Eq. (5) into two steps. In the first step, a translation module runs to
produce the 1-best translation
?
q of the input string f as a query:
?
q ? argmax
q?Q(f)
{
?
t
? h
t
(q, e, f)
}
(6)
where h
t
(?) is a vector of translation features and ?
t
is the corresponding feature weight vector. In the
second step, a monolingual retrieval module takes the 1-best translation
?
q as a query to search for the
target string
?
e with the highest score:
?
e ? argmax
e?E
{
?
r
? h
r
(
?
q, e, f)
}
(7)
where h
r
(?) is a vector of retrieval features and ?
r
is the corresponding feature weight vector.
Due to the ambiguity of translation, however, state-of-the-art MT systems are still far from producing
high-quality translations, especially for distantly-related languages. As a result, the 1-best translations
are usually erroneous and potentially introduce retrieval mistakes.
A natural solution is to use n-best lists as queries:
?
e ? argmax
e?E
{
max
q?N(f)
{
? ? h(q, e, f)
}
}
(8)
where N(f) ? T(f) is the n-best translations of the input source sentence f .
2033
Figure 1: Two kinds of query lattices: (a) search graph that is generated after phrase-based decoding and
(b) translation option graph that is generated before decoding. Translation option graph is more compact
and encodes more translation candidates.
Although using n-best lists apparently improves the retrieval accuracy over using 1-best lists, there
are two disadvantages. First, the decision rule in Eq. (8) requires to enumerate all the n translations and
retrieve for n times. In other words, the time complexity increases linearly. Second, an n-best list only
accounts for a tiny fraction of the exponential search space of translation. To make things worse, there
are usually very few variations in n-best translations because of spurious ambiguity - a situation where
multiple derivations give similar or even identical translations.
Therefore, we need to find a more elegant way to enable the retrieval module to explore exponentially
many queries without sacrificing efficiency.
3.2 Query Lattice
We propose to use query lattice to compactly represent exponentially many queries. For example, given
a source sentence ?bushi yu shalong juxing huitan?, we can use the search graph produced by a phrase-
based translation system (Koehn et al., 2007) as a lattice to encode exponentially many derivations.
Figure 1(a) shows a search graph for the example source sentence. Each edge is labeled with an
English phrase as well as the corresponding translation feature value vector. Node 0 denotes the starting
node. Node 7 and node 8 are two ending nodes. Each path from the starting node to an ending node
denotes a query. Paths that reach the same node in the lattice correspond to recombined hypotheses
that have equivalent feature histories (e.g., coverage, last generated target words, the end of last covered
source phrase, etc) in phrase-based decoding.
However, there are two problems with using search graph as query lattice. First, it is computationally
expensive to run a phrase-based system to generate search graphs. The time complexity for phrase-based
decoding with beam search is O(n
2
b) (Koehn et al., 2007), where n is the length of source string and b is
the beam width. Moreover, the memory requirement is usually very high due to language models. As a
result, translation is often two orders of magnitude slower than retrieval. Second, a search graph has too
many ?duplicate? edges due to different reordering, which increase the time complexity of retrieval (see
Section 3.3). For example, in Figure 1(a), the English phrase ?Sharon? occurs two times due to different
reordering.
Alternatively, we propose to use translation option graph as query lattice. In a phrase-based trans-
lation system, translation options that are phrase pairs matching a substring in the input source string
are collected before decoding. These translation options form a query lattice with monotonic reorder-
ing. Figure 1(b) shows an example translation option graph, in which nodes are sorted according to the
positions of source words. Each edge is labeled with an English phrase as well as the corresponding
translation feature value vector.
We believe that translation option graph has three advantages over search graph:
1. Improved efficiency in translation. Translation option graph requires no decoding.
2. Improved efficiency in retrieval. Translation option graph has no duplicate edges.
2034
Algorithm 1 Retrieval with lattice as query.
1: procedure LATTICERETRIEVE(L(f),E, k)
2: Q? GETWORDS(L(f)) . Get distinct words in the lattice to form a coarse query
3: E
k
? RETRIEVE(E, Q, k) . Retrieve top-k target sentences using the coarse query
4: for all e ? E
k
do
5: FINDPATH(L(f), e) . Find a path with the highest score
6: end for
7: SORT(E
k
) . Sort retrieved sentences according the scores
8: return E
k
9: end procedure
Algorithm 2 Find a path with the highest score.
1: procedure FINDPATH(L(f), e)
2: for v ? L(f) in topological order do
3: path(v)? ? . Initialize the Viterbi path at node v
4: score(v)? 0 . Initialize the Viterbi score at node v
5: for u ? IN(v) do . Enumerate all antecedents
6: p? path(u) ? {e
u?v
} . Generate a new path
7: s? score(u) + COMPUTESCORE(e
u?v
) . Compute the path score
8: if s > score(v) then
9: path(v)? p . Update the Viterbi path
10: score(v)? s . Update the Viterbi score
11: end if
12: end for
13: end for
14: end procedure
3. Enlarged search space. Translation option graph represents the entire search space of monotonic
decoding while search graph prunes many translation candidates.
In Figure 1, the search graph has 9 nodes, 10 edges, 4 paths, and 3 distinct translations. In contrast,
the translation option graph has 6 nodes, 9 edges, 10 paths, and 10 distinct translations. Therefore,
translation option graph is more compact and encodes more translation candidates.
Although translation option graph ignores language model and lexcialized reordering models, which
prove to be critical information sources in machine translation, we find that it achieves comparable or
even better retrieval accuracy than search graph (Section 4). This confirms the finding of Liu et al. (2012)
that language model and lexicalized reordering models only have modest effects on translation retrieval.
3.3 Retrieval with Query Lattice
Given a target corpus E and a query lattice L(f) ? Q(f), our goal is to find the target sentence
?
e with
the highest score ? ? h(q, e, f):
?
e ? argmax
e?E
{
max
q?L(f)
{
? ? h(q, e, f)
}
}
(9)
Due to the exponentially large search space, we use a coarse-to-fine algorithm to search for the target
sentence with the highest score, as shown in Algorithm 1. We use an example to illustrate the basic idea.
Given an input source sentence ?bushi yu shalong juxing le huitan?, our system first generates a query
lattice like Figure 1(a). It is non-trivial to directly feed the query lattice to a retrieval system. Instead, we
would like to first collect all distinct words in the lattice: {?Bush?, ?and? , ?Sharon?, ?held?, ?a?, ?talk?,
?talks?, ?with?}. This set serves as a coarse single query and the retrieval system returns a list of target
sentences that contain these words:
2035
Chinese English
Training 1.21M 1.21M
Dev in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Test in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based
translation model and language model for Moses (Koehn et al., 2007). The development set is used
to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set
consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To
examine the effect of domains on retrieval performance, we used two development and test sets: in-
domain and out-domain.
President Bush gave a talk at a meeting
Bush held a meeting with Sharon
Sharon and Bush attended a meeting held at London
Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences
(scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we
can match each retrieved sentence against the query lattice to find a path with the highest score using
additional translation features. For example, the Viterbi path for ?Bush held a meeting with Sharon? in
Figure 1(a) is ?Bush held talks with Sharon?. The translation features of matched arcs in the path are
collected to compute the overall score according to Eq. (9). Finally, the algorithm returns a sorted list:
Bush held a meeting with Sharon
President Bush gave a talk at a meeting
Sharon and Bush attended a meeting held at London
More formally, the input of Algorithm 1 are a query lattice L(f), a target corpus E, and a parameter
k (line 1). The function GETWORDS simply collects all the distinct words appearing in the lattice (line
2), which are used for constructing a coarse boolean query Q. Then, the function RETRIEVE runs to
retrieve the top-k target sentences E
k
in the target corpus E only using standard IR features according
to the query Q (line 3). These first two steps eliminate most unlikely candidates and return a coarse set
of target sentence candidates efficiently.
1
Then, a procedure FINDPATH(L(f), e) runs to search for the
translation with the highest score for each candidate (lines 4-6). Finally, the algorithm returns the sorted
list of target sentences (lines 7-9).
Algorithm 2 shows the procedure FINDPATH(L(f), e), which searches for the path with higher score
using a Viterbi-style algorithm. The function COMPUTESCORE scores an edge according to the Eq. (9)
which linearly combines the translation and retrieval features.
Generally, the lattice-based retrieval algorithm has a time complexity of O(k|E|), where |E| is the
number of edges in the lattice.
4 Experiments
In this section, we try to answer two questions:
1. Does using query lattices improve translation retrieval accuracy over using n-best lists?
2. How does translation retrieval benefit other end-to-end NLP tasks such as machine translation?
1
In our experiments, we set the parameter k to 500 as a larger value of k does not give significant improvements but introduce
more noises.
2036
Accordingly, we evaluated our system in two tasks: translation retrieval (Section 4.1) and parallel
corpus mining (Section 4.2).
4.1 Evaluation on Translation Retrieval
4.1.1 Experimental Setup
In this section, we evaluate the accuracy of translation retrieval: given a query set (i.e., source sentences),
our system returns a sorted list of target sentences. The evaluation metrics include precision@n and
recall.
The datasets for the retrieval evaluation are summarized in Table 1. The training set, which is used to
train the phrase-based translation model and language model for the-state-of-the-art phrase-based system
Moses (Koehn et al., 2007), contains 1.21M Chinese-English sentences with 32.0M Chinese words and
35.2M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on
the English side of the training corpus. The development set, which is used to optimize feature weights
using the minimum-error-rate algorithm (Och, 2003), consists of query set and a document set. We
sampled 5K parallel sentences randomly, in which 5K Chinese sentences are used as queries and half
of their parallelled English sentences(2.5K) mixed with other English sentences(2.3M) as the retrieval
document set. As a result, we can compute precision and recall in a noisy setting. The test set is used
to compute retrieval evaluation metrics. To examine the effect of domains on retrieval performance, we
used two data sets: in-domain and out-domain. The in-domain development and test sets are close to
the training set while the out-domain data sets are not.
We compare three variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice,
we further distinguish between search graph and translation option graph. They are generated by Moses
with the default setting.
We use both translation and retrieval features in the experiments. The translation features include
phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, lan-
guage models, and word penalty. Besides the conventional IR features such as term frequency and
inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002):
the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity
penalty. These features impose structural constraints on retrieval and ensure translation closeness of re-
trieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss
function we used in our experiment is 1?P@n. Note that using translation option graph as query lattice
does not include language models and distance-based lexicalized reordering models as features.
4.1.2 Evaluation Results
Table 2 shows the results on the in-domain test set. The ?# candidates? column gives the number of
translation candidates explored by the retrieval module for each source sentence on average. The lattices,
either generated by search graph or by translation options, contain exponentially many candidates. We
find that using lattices dramatically improves the precisions over using 1-best and n-best lists. All the
improvements over 1-best and n-best lists are significant statistically. The 1-best, n-best, and the search
graph lattice share with the same translation time: 5,640 seconds for translating 5,000 queries. Note
that the translation time is zero for the translation option graph because it does not need phrase-based
decoding. For retrieval, the time cost for the n-best list method generally increases linearly. As the search
graph lattice contains many edges, the retrieval time increases by an order of magnitude as compared
with 100-best list. An interesting finding is that using translation options as a lattice contains more
candidates and consumes much less time for retrieval than using search graph as a lattice. One possible
reason is that a search graph generated by Moses usually contains many redundant edges. For example,
Figure 1 is actually a search graph and many phrases occurs multiple times in the lattice (e.g., ?and?
and ?Sharon?). In contrast, a lattice built by translation options hardly has any redundant edges but
still represents exponentially many possible translations. We can also see that the lattice constructed by
search graph considering language model can benefit the precision much, especially when n is little. But
this advantage decreases with n increasing and the time consumed by translation options as lattice is
much less than the search graph as lattice. Besides, the margin between them is not too large so we can
2037
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 87.40 91.40 92.24 92.88 93.64 5,640 82
10-best 10 89.84 93.20 93.96 94.36 95.56 5,640 757
100-best 100 90.76 94.32 95.00 95.76 96.76 5,640 7,421
lattice (graph) 1.20? 10
54
93.60 96.08 96.28 96.52 96.80 5,640 89,795
lattice (options) 4.14? 10
62
93.28 95.84 95.96 96.16 96.84 0 307
Table 2: Results on the in-domain test set. We use the minimum-error-rate training algorithm (Och,
2003) to optimize the feature with the respect to 1?P@n.
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 67.32 76.60 79.40 81.80 83.76 3,660 92
10-best 10 72.68 80.96 83.36 85.84 88.76 3,660 863
100-best 100 78.60 85.76 87.76 89.64 92.16 3,660 8,418
lattice (graph) 1.51? 10
61
84.32 89.40 90.68 91.56 92.44 3,660 67,205
lattice (options) 1.24? 10
65
81.92 88.00 89.80 91.24 93.16 0 645
Table 3: Results on the out-of-domain test set.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 2: In-domain Precision-Recall curves.
0.3 0.4 0.5 0.6 0.7 0.8 0.90.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 3: Out-domain Precision-Recall curves.
abandon some little precision for obtain the large time reducing. Therefore, using translation options as
lattices seems to be both effective and efficient.
Table 3 shows the results on the out-of-domain test set. While the precisions for all methods drop, the
margins between lattice-based retrieval and n-best list retrieval increase, suggesting that lattice-based
methods are more robust when dealing with noisy datasets.
Figures 2 and 3 show the Precision-Recall curves on the in-domain and out-of-domain test sets. As
the query set is derived from parallel sentences, recall can be computed in our experiments. The curves
show that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the
out-of-domain test set.
4.2 Evaluation on Parallel Corpus Mining
In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel
corpus from a comparable corpus.
4.2.1 Experimental Setup
The comparable corpus for extracting parallel sentences contains news articles published by Xinhua
News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and
1.7M English articles.
We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system.
2038
language articles sentences words vocabulary
Chinese 1.2M 18.5M 441.2M 2.1M
English 1.7M 17.8M 440.2M 3.4M
Table 4: The Xinhua News Comparable Corpus from 1995 to 2010
Munteanu and Marcu (2005) this work
English words Chinese words BLEU English words Chinese Words BLEU
5.00M 4.12M 22.84 5.00M 3.98M 25.44
10.00M 8.20M 25.10 10.00M 8.17M 26.62
15.00M 12.26M 25.41 15.00M 12.49M 26.49
20.00M 16.30M 25.56 20.00M 16.90M 26.87
Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system.
Given a comparable corpus (see Table 4), both systems extract parallel corpora that are used for training
phrase-base models (Koehn et al., 2007). The baseline system is a re-implementation of the method
described in (Munteanu and Marcu, 2005). Our system uses translation option graph as query lattice.
Our system significantly outperforms the baseline for various sizes.
It assigned a score to each sentence pair using a classifier. Our system used translation option graph as
query lattices due to its simplicity and effectiveness. For each source sentence in the comparable corpus,
our system retrieved the top target sentence together with a score.
To evaluate the quality of extracted parallel corpus, we trained phrase-based models on it and ran
Moses on NIST datasets. The development set is the NIST 2005 test set and the test set is the NIST 2006
test set. The final evaluation metric is case-insensitive BLEU-4.
4.2.2 Evaluation Results
Table 5 shows the comparison of BLEU scores using parallel corpora extracted by the baseline and our
system. We find that our system significantly outperforms the baseline for various parallel corpus sizes.
This finding suggests that using lattice to compactly represent exponentially many alternatives does help
to alleviate the translation error propagation problem and identify parallel sentences of high translational
equivalence.
5 Conclusion
In this work, we propose to use query lattice to address the translation error propagation problem in
translation retrieval. Two kinds of query lattices are used in our experiments: search graph and translation
option graph. We show that translation option graph is more compact and represents a much larger
search space. Our experiments on Chinese-English datasets show that using query lattices significantly
outperforms using n-best lists in the retrieval task. Moreover, we show that translation retrieval is capable
of extracting high-quality parallel corpora from a comparable corpus. In the future, we plan to apply
our approach to retrieving translation candidates directly from the Web, which can be seen as a huge
monolingual corpus.
Acknowledgments
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013 and No. 61033001), the 863 Program (No. 2012AA011102),
Toshiba Corporation Corporate Research & Development Center, and the Singapore National Research
Foundation under its International Research Centre @ Singapore Funding Initiative and administered by
the IDM Programme.
2039
References
T. Baldwin and H. Tanaka. 2000. The effects of word order and segmentation on translation retrieval performance.
In Proceedings of COLING.
Timothy Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proceedings of
ACL, pages 18?25, Toulouse, France, July. Association for Computational Linguistics.
Karen Cheung and Douglas Vogel. 2005. Complexity reduction in lattice-based information retrieval. Information
Retrieval, pages 285?299.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou Ng. 2010. Statistical lattice-based spoken document
retrieval. ACM Transactions on Information Systems, 28(1).
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020, Columbus, Ohio, June. Association for Computational Linguistics.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan L?u. 2009. Lattice-based system combination for statis-
tical machine translation. In Proceedings of EMNLP, pages 1105?1113, Singapore, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic, June. Association for Computational Linguistics.
Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun. 2012. THUTR: A translation retrieval system. In Proceed-
ings of COLING - Demo and Poster Sessions, pages 321?328, Mumbai, India, December. The COLING 2012
Organizing Committee.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate
training for statistical machine translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Spyros Matsoukas, Ivan Bulyko, Bing Xiang, Kham Nguyen, Richard Schwartz, and John Makhoul. 2007. In-
tegrating speech recognition and machine translation. In Proceedings of ICASSP, volume 4, pages IV?1281.
IEEE.
C.N. Moore. 1958. A mathematical theory of the use of language symbols in retrieval. In ICSI 1958.
Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31(4):477?1504.
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993. Two approaches to matching in example-based machine
translation. In TMI 1993.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302, Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Uta Priss. 2000. Lattice-based information retrieval. Knwoledge Organization, 27(3):132?142.
Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-NAACL, pages 129?136, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
S. Sato and M. Nagao. 1990. Toward memory-based translation. In Proceedings of COLING.
Andreas Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of ICSLP.
2040
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding
for statistical machine translation. In Proceedings of EMNLP, pages 620?629, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In Proceedings of IWSLT 2005, pages 141?147, Pittsburgh, PA, October.
Zheng-Yu Zhou, Peng Yu, Ciprian Chelba, and Frank Seide. 2006. Towards spoken-document retrieval for the
internet: Lattice indexing for large-scale web-search architectures. In Proceedings of HLT-NAACL, pages 415?
422, New York City, USA, June. Association for Computational Linguistics.
2041
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970?979,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Enhancing Chinese Word Segmentation Using Unlabeled Data
Weiwei Sun?? and Jia Xu?
?Department of Computational Linguistics, Saarland University
?German Research Center for Artificial Intelligence (DFKI)
D-66123, Saarbru?cken, Germany
wsun@coli.uni-saarland.de, Jia.Xu@dfki.de
Abstract
This paper investigates improving supervised
word segmentation accuracy with unlabeled
data. Both large-scale in-domain data and
small-scale document text are considered. We
present a unified solution to include features
derived from unlabeled data to a discrimina-
tive learning model. For the large-scale data,
we derive string statistics from Gigaword to
assist a character-based segmenter. In addi-
tion, we introduce the idea about transductive,
document-level segmentation, which is de-
signed to improve the system recall for out-of-
vocabulary (OOV) words which appear more
than once inside a document. Novel features1
result in relative error reductions of 13.8% and
15.4% in terms of F-score and the recall of
OOV words respectively.
1 Introduction
Chinese sentences are written in continuous se-
quence of characters without explicit delimiters such
as space characters. To find the basic language units,
i.e. words, segmentation is a necessary initial step
for Chinese language processing. Previous research
shows that word segmentation models trained on la-
beled data are reasonably accurate. In this paper,
we investigate improving supervised word segmen-
tation with unlabeled data.
We distinguish three types of unlabeled data,
namely large-scale in-domain data, out-of-domain
data and small-scale document text. Both large-scale
1You can download our derived features at
http://www.coli.uni-saarland.de/?wsun/
semi-cws-feats-emnlp11.tgz.
in-domain and out-of-domain data is popular for en-
hancing NLP tasks. Learning from these two types
of unlabeled data normally involves semi-supervised
learning. The difference between them is that out-
of-domain data is usually used for domain adapta-
tion. For a number of NLP tasks, there are relatively
large amounts of labeled training data. In this sit-
uation, supervised learning can provide competitive
results, and it is difficult to improve them any further
by using extra unlabeled data. Chinese word seg-
mentation is one of this kind of tasks, since several
large-scale manually annotated corpora are publicly
available. In this work, we first exploit unlabeled in-
domain data to improve strong supervised models.
We leave domain adaptation for our future work.
We introduce the third type of unlabeled data with
a transductive learning, document-level view. Many
applications of word segmentation involve process-
ing a whole document, such as information retrieval.
In this situation, the text of the current document
can provide additional useful information to seg-
ment a sentence. Take the word ????/elastane?
for example2. As a translated terminology word, it
lacks compositionality. Moreover, this word appears
rarely in general texts. As a result, if it does not ap-
pear in the training data, it is very hard for statis-
tical models to recognize this word. Nevertheless,
when we deal with an article discussing an elastane
company, this word may appear more than once in
this article, and the document information can help
recognize this word. This idea is closely related to
transductive learning in the sense that the segmen-
tation model knows something about the problem it
2This example is from an article indexed as chtb 0041 in the
Penn Chinese Treebank corpus.
970
is going to resolve. In this work, we are also con-
cerned with enhancing word segmentation with the
document information.
We present a unified ?feature engineering? ap-
proach for learning segmentation models from both
labeled and unlabeled data. Our method is a simple
two-stage process. First, we use unannotated corpus
to extract string and document information, and then
we use these information to construct new statistics-
based and document-based feature mapping for a
discriminative word segmenter. We are relying on
the ability of discriminative learning method to iden-
tify and explore informative features, which play
central role to boost the segmentation performance.
This simple solution has been shown effective for
named entity recognition (Miller et al, 2004) and
dependency parsing (Koo et al, 2008). In their im-
plementations, word clusters derived from unlabeled
data are imported as features to discriminative learn-
ing approaches.
To demonstrate the effectiveness of our approach,
we conduct experiments on the Penn Chinese Tree-
bank (CTB) data. CTB is a collection of docu-
ments which are separately annotated. This anno-
tation style allows us to evaluate our transductive
segmentation method. Our experiments show that
both statistics-based and document-based features
are effective in the word segmentation application.
In general, the use of unlabeled data can be moti-
vated by two concerns: First, given a fixed amount
of labeled data, we might wish to leverage unla-
beled data to improve the performance of a super-
vised model. Second, given a fixed target perfor-
mance level, we might wish to use unlabeled data
to reduce the amount of annotated data necessary
to reach this target. We show that our approach
yields improvements for fixed data sets, even when
large-scale labeled data is available. The new fea-
tures result in relative error reductions of 13.8% and
15.4% in terms of the balanced F-score and the re-
call of out-of-vocabulary (OOV) words respectively.
By conducting experiments on data sets of varying
sizes, we demonstrate that for fixed levels of perfor-
mance, the new features derived from unlabeled data
can significantly reduce the need of labeled data.
The remaining part of the paper is organized as
follows. Section 2 describes the details of our sys-
tem, especially the design of the derived features.
B Current character is the start of a word con-
sisting of more than one character.
E Current character is the end of a word con-
sisting of more than one character.
I Current character is a middle of a word con-
sisting of more than two characters.
S Current character is a word consisting of
only one character.
Table 1: The start/end representation.
Section 3 presents experimental results and empir-
ical analysis. Section 4 reviews the related work.
Section 5 concludes the paper.
2 Method
2.1 Discriminative Character-based Word
Segmentation
The Character-based approach is a dominant word
segmentation solution for Chinese text process-
ing. This approach treats word segmentation as a
sequence tagging problem, assigning labels to the
characters indicating whether a character locates at
the beginning of, inside or at the end of a word. This
character-by-character method was first proposed
by (Xue, 2003), and a number of discriminative
sequential learning algorithms have been exploited,
including structured perceptron (Jiang et al, 2009),
the Passive-Aggressive algorithm (Sun, 2010),
conditional random fields (CRFs) (Tseng et al,
2005), and latent variable CRFs (Sun et al, 2009).
In this work, we use the Start/End representation to
express the position information of every character.
Table 2.1 shows the meaning of each character
label. For example, the target label representation
of the book title ???????????/The Se-
cret Journal of Premier Zhao Ziyang? is as follows.
? ? ? ? ? ? ? ? ? ?
B I E B E S B E B E
Key to our approach is to allow informative fea-
tures derived from unlabeled data to assist the seg-
menter. In our experiments, we employed three
different feature sets: a baseline feature set which
draws upon ?normal? information from labeled
training data, a statistics-based feature set that uses
statistical information derived from a large-scale in-
domain corpus, and a document-based feature set
971
that uses information encoded in the surrounding
text.
2.2 Baseline Features
In this work, to train a good traditional supervised
segmenter, our baseline feature templates includes
the ones described in (Sun et al, 2009; Sun, 2010).
These features are divided into two types: char-
acter features and word type features. Note that
the word type features are indicator functions that
fire when the local character sequence matches a
word uni-gram or bi-gram. Dictionary containing
word uni-grams and bi-grams is collected from the
training data. To conveniently illustrate, we de-
note a candidate character token ci with a context
...ci?1cici+1.... We use c[s:e] to express a string that
starts at the position s and ends at the position e.
For example, c[i:i+1] expresses a character bi-gram
cici+1. The character features are listed below.
? Character uni-grams: cs (i? 3 < s < i+ 3)
? Character bi-grams: cscs+1 (i?3 < s < i+3)
? Whether cs and cs+1 are identical, for i ? 2 <
s < i+ 2.
? Whether cs and cs+2 are identical, for i ? 4 <
s < i+ 2.
The word type features are listed as follows.
? The identity of the string c[s:i] (i? 6 < s < i),
if it matches a word from the list of uni-gram
words;
? The identity of the string c[i:e] (i < e < i + 6),
if it matches a word; multiple features could be
generated.
? The identity of the bi-gram c[s:i?1]c[i:e] (i?6 <
s, e < i+6), if it matches a word bi-gram from
the list of uni-gram words.
? The identity of the bi-gram c[s:i]c[i+1:e] (i?6 <
s, e < i + 6), if it matches a word bi-gram;
multiple features could be generated.
Idiom In linguistics, idioms are usually presumed
to be figures of speech contradicting the principle of
compositionality. As a result, it is very hard to rec-
ognize out-of-vocabulary idioms for word segmen-
tation. Nonetheless, the lexicon of idioms can be
taken as a close set, which helps resolve the problem
well. In our previous work (Sun, 2011), we collect
12992 idioms from several free online Chinese dic-
tionaries. This linguistic resource is publicly avail-
able3. In this paper, we use this idiom dictionary to
derive the following feature.
? Does ci locate at the beginning of, inside or
at the end of an idiom? If the string c[s:i]
(s < i) matches an item from the idiom lexi-
con, the feature template receives a string value
?E-IDIOM?. Similarly, we can define when this
feature ought to be set to ?B-IDIOM? or ?I-
IDIOM?. Note that all idioms are larger than
one character, so there is no ?S-IDIOM? fea-
ture here.
2.3 Statistics-based Features
In order to distill information from unlabeled data,
we borrow ideas from some previous research on
unsupervised word segmentation. The statistical in-
formation acquired from a relatively large amount
of unlabeled data are designed as features correlated
with the position where a character locates in a word
token. These features are based on three widely used
criteria.
2.3.1 Mutual Information
Empirical mutual information is widely used in
NLP. Informally, mutual information compares the
probability of observing x and y together with the
probabilities of observing x and y independently. If
there is a genuine association between x and y, the
I(x, y) = log p(x,y)p(x)p(y) should be greater than 0.Some previous work claimed that the larger
the mutual information between two consecutive
strings, the higher the possibility of the two strings
being combined together. We adopt this idea in our
character-based segmentation model. The empiri-
cal mutual information between two character bi-
grams is computed by counting how often they ap-
pear in the large-scale unlabeled corpus. Given a
3http://www.coli.uni-saarland.de/?wsun/
idiom.txt.
972
Chinese character string c[i?2:i+1], the mutual infor-
mation between substrings c[i?2:i?1] and c[i:i+1] is
computed as:
MI(c[i?2:i?1], c[i:i+1]) = log
p(c[i?2:i+1])
p(c[i?2:i?1])p(c[i:i+1])
For each character ci, we incorporate the MI of the
character bi-grams into our model. They include,
? MI(c[i?2:i?1], c[i:i+1]),
? MI(c[i?1:i], c[i+1:i+2]).
2.3.2 Accessor Variety
When a string appears under different linguistic
environments, it may carry a meaning. This prin-
ciple is introduced as the accessor variety criterion
for identifying meaningful Chinese words in (Feng
et al, 2004). This criterion evaluates how indepen-
dently a string is used, and thus how likely it is that
the string can be a word. Given a string s, which
consists of l (l ? 2) characters, we define the left
accessor variety of Llav(s) as the number of distinct
characters that precede s in a corpus. Similarly, the
right accessor variety Rlav(s) is defined as the num-
ber of distinct characters that succeed s.
We first extract all strings whose length are be-
tween 2 and 4 from the unlabeled data, and calculate
their accessor variety values. For each character ci,
we then incorporate the following information into
our model,
? Accessor variety of strings with length 4:
L4av(c[i:i+3]), L4av(c[i+1:i+4]), R4av(c[i?3:i]),
R4av(c[i?4:i?1]);
? Accessor variety of strings with length 3:
L3av(c[i:i+2]), L3av(c[i+1:i+3]), R3av(c[i?2:i]),
R3av(c[i?3:i?1]);
? Accessor variety of strings with length 2:
L2av(c[i:i+1]), L2av(c[i+1:i+2]), R2av(c[i?1:i]),
R2av(c[i?2:i?1]).
2.3.3 Punctuation as Anchor Words
Punctuation marks are symbols that indicate the
structure and organization of written language, as
well as intonation and pauses to be observed when
reading aloud. Punctuation marks can be taken as
perfect word delimiters, and can be used as anchor
words to harvest lexical knowledge. The preced-
ing and succeeding strings of punctuations carry ad-
ditional wordbreak information, since punctuations
should be segmented as a word. Note that such in-
formation is biased because not all words can appear
before or after punctuations. For example, punctua-
tions can not be followed by particles, such as ???,
??? and ??? which are indicators of aspects. Nev-
ertheless, our experiments will show this kind of in-
formation is still useful for word segmentation.
When a string appears many times preceding or
succeeding punctuations, there tends to be word-
breaks succeeding or preceding that string. To uti-
lize the wordbreak information provided by punctu-
ations, we extract all strings with length l(2 ? l ?
4) which precede or succeed punctuations in the un-
labeled data. We define the left punctuation variety
of Llpv(s) as the number of times a punctuation pre-
cedes s in a corpus. Similarly, the right punctua-
tion variety Rlpv(s) is defined as the number of how
many times a punctuation succeeds s. These two
variables evaluate how likely a string can be sepa-
rated at its start or end positions.
We first gather all strings surrounding punctua-
tions in the unlabeled data, and calculate their punc-
tuation variety values. The length of each string is
also restricted between 2 and 4. For each charac-
ter ci, we import the following information into our
model,
? Punctuation variety of strings with length 4:
L4pv(c[i:i+3]), R4pv(c[i?3:i]);
? Punctuation variety of strings with length 3:
L3pv(c[i:i+2]), R3pv(c[i?2:i]);
? Punctuation variety of strings with length 2:
L2pv(c[i:i+1]), R2pv(c[i?1:i]).
Punctuations can be viewed as mark-up?s of Chi-
nese text. Our motivation to use the punctuation in-
formation to assist a word segmenter is similiar to
(Spitkovsky et al, 2010) in a way to explore ?artifi-
cial? word (or phrase) break symbols. In their work,
four common HTML tags are successfully used as
raw phrase bracketings to improve unsupervised de-
pendency parsing.
973
2.3.4 Binary or Numeric Features
The derived information introduced above is all
expressed as real values. The natural way to in-
corporate these statistics into a discriminative learn-
ing model is to directly use them as numeric fea-
tures. However, our experiments show that this sim-
ple choice does not work well. The reason is that
these statistics actually behave non-linearly to pre-
dict character labels. For each type of statistics, one
weight alone cannot capture the relation between its
value and the possibility that a string forms a word.
Instead, we represent these statistics as discrete fea-
tures.
For the mutual information, this is done by round-
ing down decimal number. The integer part of each
MI value is used as a string feature. For the ac-
cessor variety and punctuation variety information,
since their values are integer, we can directly use
them as string features. The accessor variety and
punctuation variety could be very large, so we set
thresholds to cut off large values to deal with the
data sparse problem. Specially, if an accessor va-
riety value is greater than 50, it is incorporated as
a feature ?> 50?; if the value is greater than 30
but not greater than 50, it is incorporated as a fea-
ture ?30 ? 50?; else the value is individually in-
corporated as a string feature. For example, if the
left accessory variety of a character bi-gram c[i:i+1]
is 29, the binary feature ?L2av(c[i:i+1])=29? will be
set to 1, while other related binary features such as
?L2av(c[i:i+1]) = 15? or ?L2av(c[i:i+1]) > 50? will
be set to 0. Similarly, we can discretize the punc-
tuation variety features. However, we only set one
threshold, 30, for this value. These thresholds can
be tuned by using held-out data.
2.4 Document-based Features
It is meaningless to derive statistics of a document
and use it for word segmentation, since most doc-
uments are relatively short, and values are statisti-
cally unreliable. Our experiments confirm this idea.
Instead, we propose the following binary features
which are based on the string count in the given doc-
ument that is simply the number of times a given
string appears in that document. For each character
ci, our document-based features include,
? Whether the string count of c[s:i] is equal to that
of c[s:i+1] (i ? 3 ? s ? i). Multiple features
are generated for different string length.
? Whether the string count of c[i:e] is equal to that
of c[i?1:e] (i ? e ? i + 3). Multiple features
are generated for different string length.
The intuition is as follows. The string counts of
c[s:i] and c[s:i+1] being equal means that when c[s:i]
appears, it appears inside c[s:i+1]. In this case, c[s:i]
is not independently used in this document, and this
feature suggests the segmenter not assign a ?S? or
?E? label to the character ci. Similarly, the string
counts of c[i:e] and c[i?1:e] being equal means c[i:e]
is not independently used in this document, and this
feature suggests segmenter not assign a ?S? or ?B?
label to ci. We do not directly use the string counts
to prevent a bias towards longer documents.
3 Experiments
3.1 Setting
The SIGHAN Bakeoffs provide several large-scale
labeled data for the research on Chinese word seg-
mentation. Although these data sets are labeled on
continuous run texts, they do not contain the docu-
ment boundary information. CTB is a segmented,
part-of-speech tagged, and fully bracketed corpus
in the constituency formalism. It is also an popu-
lar data set to evaluate word segmentation methods,
such as (Jiang et al, 2009; Sun, 2011). CTB is a
collection of documents which are separately anno-
tated. This annotation style allows us to calculate
the so-called document-based features and to further
evaluate our approach. In this paper, we use CTB 6.0
as our main corpus and define the training, develop-
ment and test sets according to the Chinese sub-task
of the CoNLL 2009 shared task4. Table 2 shows the
statistics of our experimental settings.
Data set # of sent. # of words # of char.
Training 22277 609060 1004266
Devel. 1763 49646 83710
Test 2557 73152 121008
Table 2: Training, development and test data on CTB 6.0
4We would like to thank Prof. Nianwen Xue for the help
with the division of the data
974
Chinese Gigaword is a comprehensive archive
of newswire text data that has been acquired over
several years by the Linguistic Data Consortium
(LDC). The large-scale unlabeled data we use in
our experiments comes from the Chinese Gigaword
(LDC2005T14). We choose the Mandarin news text,
i.e. Xinhua newswire. This data covers all news
published by Xinhua News Agency (the largest news
agency in China) from 1991 to 2004, which contains
over 473 million characters.
F-score is used as the accuracy measure. Define
precision p as the percentage of words in the decoder
output that are segmented correctly, and recall r as
the percentage of gold standard output words that are
correctly segmented by the decoder. The (balanced)
F-score is 2pr/(p + r). We also report the recall of
OOV words. Note that, all idioms in our extra idiom
lexicon are added into the in-vocabulary word list.
CRFsuite (Okazaki, 2007) is an implementation
of Conditional Random Fields (CRFs) (Lafferty
et al, 2001) for labeling sequential data. It is a
speed-oriented implementation, which is written in
pure C. In our experiments, we use this toolkit to
learn global linear models for segmentation. We use
the stochastic gradient descent algorithm to resolve
the optimization problem, and set default values for
other learning parameters.
3.2 Main Results
Table 3 summarizes the segmentation results on the
development data with different configurations, rep-
resenting a few choices between baseline, statistics-
based and document-based feature sets. In this table,
the symbol ?+? means features of current configura-
tion contains both the baseline features and new fea-
tures for semi-supervised or transductive learning.
From this table, we can clearly see the impact of fea-
tures derived from the large-scale unlabeled data and
the current document. Comparison between the per-
formance of the baseline and ?+MI? shows that the
widely used mutual information is not helpful. Both
good segmentation techniques and valuable labeled
corpora have been developed, and pure supervised
systems can provide strong performance. It is not
a trial to design new features to enhance supervised
models.
There are significant increases when accessor va-
riety features and punctuation variety features are
Devel. P R F?=1 Roov
Baseline 95.41 95.52 95.46 77.68
+MI 95.50 95.48 95.49 77.98
+AV(2) 95.85 96.04 95.94 79.31
+AV(2,3) 95.95 96.19 96.07 80.61
+AV(2,3,4) 96.14 95.99 96.07 81.83
+PU(2) 95.86 96.07 95.97 79.70
+PU(2,3) 95.98 96.25 96.11 80.42
+PU(2,3,4) 96.00 96.19 96.10 80.53
+MI+AV(2,3,4)+PU(2,3,4)
96.17 96.22 96.19 80.42
+DOC 95.69 95.64 95.66 79.89
+MI+AV(2,3,4)+PU(2,3,4)+DOC
96.21 96.23 96.22 81.75
Table 3: Segmentation performance with different feature
sets on the development data. Abbreviations: MI=mutual
information; AV=accessor variety; PU=punctuation va-
riety; DOC=document features. The numbers in each
bracket pair are the lengths of strings. For example,
PU(2,3) means punctuation variety features of character
bi-grams and tri-grams are added.
separately added. Extending the length of neigh-
boring string helps a little from 2 to 3. Al-
though the OOV recall increases when the length
is extended from 3 to 4, there is no improve-
ment of the overall balanced F-score. The
line ?+MI+AV(2,3,4)+PU(2,3,4)? shows the perfor-
mance when all statistics-based features are added.
The combination of the ?AV? and ?PU? features
gives further helps. This system can be seen as a
pure semi-supervised system. The line ?+DOC? is
the result when document-based features are added.
In spite of its simplicity, the document-based fea-
tures can help the task. However, when we combine
statistics-based features with document-based fea-
tures, we cannot get further improvement in terms
of F-score.
Table 4 shows the segmentation perfor-
mance on the test data set. The final re-
sults of our system are achieved with the
?+MI+AV(2,3,4)+PU(2,3,4)+DOC? feature config-
uration. The new features result in relative error
reductions of 13.8% and 15.4% in terms of the
balanced F-score and the recall of OOV words
respectively.
975
 87 88
 89 90
 91 92
 93 94
 95 96
 97
 100  200  300  400  500  600  700  800  900  1000
F
-
s
c
o
r
e
Training data size (thousands of characters)
Baseline features+Statistics-based features+Document-based featuresAll features
 68
 70
 72
 74
 76
 78
 80
 82
 84
 100  200  300  400  500  600  700  800  900  1000
O
O
V
 
R
e
c
a
l
l
 
(
%
)
Training data size (thousands of characters)
Baseline features+Statistics-based features+Document-based featuresAll features
Figure 1: The learning curves of different models.
-15-10
-5 0
 5 10
 15 20
 25
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-20-15
-10-5
 0 5
 10 15
 20
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-30-25
-20-15
-10-5
 0 5
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-15-10
-5 0
 5 10
 15
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
-14-12
-10-8
-6-4
-2 0
 2 4
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-10-5
 0 5
 10 15
 20 25
 30
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-5 0
 5 10
 15 20
 25
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-25-20
-15-10
-5 0
 5 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
Figure 2: Scatter plot of feature score against feature value. The left side shows is L2pv(c[i:i+1] feature while the right
side is the R2pv(c[i:i+1] feature.
Test P R F?=1 Roov
Baseline 95.21 94.90 95.06 75.52
Final 95.86 95.62 95.74 79.28
Table 4: Segmentation performance on the test data.
3.3 Learning Curves
We performed additional experiments to evaluate the
effect of the derived features as the amount of train-
ing data is varied. Figure 1 displays the F-score
and the OOV recall of systems with different feature
sets when trained on smaller portions of the labeled
data. Note that there is no change in the configura-
tion of the unlabeled data. We can clearly see that
the derived features obtain consistent gains regard-
less of the size of the training set. The improvement
is more significant when little labeled data is ap-
plied. Both statistics-based features and document-
based features can help improve the overall perfor-
mance. Especially, they can help to recognize more
unknown words, which is important for many appli-
cations. The F-score of semi-supervised models, i.e.
models trained with statistics-based features, does
not achieve further improvement when document-
based features are added. Nonetheless, the OOV re-
call obtains slightly improvements.
It is interesting to consider the amount by which
derived features reduce the need for supervised data,
given a desired level of accuracy. The change of
the F-score in Figure 1 suggests that derived fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
model with extra features trained on 500k characters
976
is slightly higher than the performance of the model
with only baseline features trained on the whole la-
beled data.
3.4 Feature Analysis
We discussed the choice of using binary or numeric
features in Section 2.3.4. In our experiment, when
the accessor variety and punctuation variety infor-
mation are integrated as numeric features, they do
not contribute. To show the non-linear way that
these features contribute to the prediction problem,
we present the scatter plots of the score of each
feature (i.e. the weight multiply the feature value)
against the value of the feature. Figure 2 shows
the relation between the score and the value of
the punctuation variety features. For example, the
weight of the binary feature ?L2pu(c[i:i+1]) = 26
combined with the label ?B? learned by the final
model is 0.815141, so the score of this combina-
tion is 0.815141 ? 26 = 21.193666 and a point
(26, 21.193666) is drawn. These plots indicate the
punctuation variety features contribute to the final
model in a very complicated way. It is impossible
to use one weight to capture it. The accessor va-
riety features affect the model in the same way, so
we do not give detailed discussions. We only show
the same scatter plot of the L2av(c[i:i+1]) feature tem-
plate in Figure 3.
-20-15
-10-5
 0 5
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?B?
-10-8
-6-4
-2 0
 2 4
 6 8
 10
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?I?
-10-8
-6-4
-2 0
 2 4
 6 8
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?E?
-5
 0
 5
 10
 15
 20
 5  10  15  20  25  30
S
c
o
r
e
Feature value
Label ?S?
Figure 3: Scatter plot of feature score against feature
value for L2av(c[i:i+1]).
4 Related Work
Xu et al (2008) presented a Bayesian semi-
supervised approach to derive task-oriented word
segmentation for machine translation (MT). This
method learns new word types and word distribu-
tions on unlabeled data by considering segmentation
as a hidden variable in MT. Different from their con-
cern, our focus is general word segmentation.
The ?feature-engineering? semi-supervised ap-
proach has been successfully applied to named en-
tity recognition (Miller et al, 2004) and depen-
dency parsing (Koo et al, 2008). These two papers
demonstrated the effectiveness of using word clus-
ters as features in discriminative learning. More-
over, Turian et al (2010) compared different word
clustering algorithms and evaluated their effect on
both named entity recognition and text chunking.
As mentioned earlier, the feature design is in-
spired by some previous research on word segmen-
tation. The accessor variety criterion is proposed to
extract word types, i.e. the list of possible words,
in (Feng et al, 2004). Different from their work,
our method resolves the segmentation problem of
running texts, in which this criterion is used to de-
fine features correlated with the character position
labels. Li and Sun (2009) observed that punctuations
are perfect delimiters which provide useful informa-
tion for segmentation. Their method can be viewed
as a self-training procedure, in which extra punctu-
ation information is incorporated to filter out auto-
matically predicted samples. We use the punctua-
tion information in a different way. In our method,
the counts of the preceding and succeeding strings
of punctuations are incorporated directly as features
into a supervised model.
In machine learning, transductive learning is a
learning framework that typically makes use of un-
labeled data. The goal of transductive learning is
to only infer labels for the unlabeled data points in
the test set rather than to learn a general classifica-
tion function that can be applied to any future data
sets. This means that the test data is known as a
priori knowledge and can be used to construct bet-
ter hypotheses. Although the idea to explore the
document-level information in our work is similar
to transductive learning, we do not use state-of-the-
art transductive learning algorithms which involve
learning when they meet the test data. For real-world
applications, our approach is efficient by avoiding
re-training.
977
5 Conclusion and Future Work
In this paper, we have presented a simple yet effec-
tive approach to explore unlabeled data for Chinese
word segmentation. We are concerned with large-
scale in-domain data and the document text. Ex-
periments show that our approach achieves substan-
tial improvement over a competitive baseline. Es-
pecially, the informative features derived from un-
labeled data lead to significant improvement of the
recall of unknown words. Our immediate concern
for future work is to exploit the out-of-domain data
to improve the robustness of current word segmen-
tation systems. The idea would be to extract do-
main information from unlabeled data and define
them as features in our unified approach. The word-
based approach is an alternative for word segmenta-
tion. This kind of segmenters sequentially predicts
whether the local sequence of characters make up a
word. A natural avenue for future work is the exten-
sion of our method to the word-based approach. The
word segmentation task is similar to constituency
parsing, in the sense of finding boundaries of lan-
guage units. Another interesting question is whether
our method can be adapted to resolve constituency
parsing.
Acknowledgments
The work is supported by the project TAKE (Tech-
nologies for Advanced Knowledge Extraction),
funded under contract 01IW08003 by the German
Federal Ministry of Education and Research. The
author is also funded by German Academic Ex-
change Service (DAAD).
References
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Comput. Linguist., 30:75?
93.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic adaptation of annotation standards:
Chinese word segmentation and pos tagging ? a
case study. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 522?
530. Association for Computational Linguistics,
Suntec, Singapore.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Proceedings of ACL-08: HLT, pages 595?
603. Association for Computational Linguistics,
Columbus, Ohio.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of
the Eighteenth International Conference on Ma-
chine Learning, pages 282?289. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Zhongguo Li and Maosong Sun. 2009. Punctuation
as implicit annotations for Chinese word segmen-
tation. Comput. Linguist., 35:505?512.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 337?342. As-
sociation for Computational Linguistics, Boston,
Massachusetts, USA.
Naoaki Okazaki. 2007. Crfsuite: a fast implementa-
tion of conditional random fields (crfs).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan
Alshawi. 2010. Profiting from mark-up: Hyper-
text annotations for guided parsing. In Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1278?
1287. Association for Computational Linguistics,
Uppsala, Sweden.
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison
and combination. In Coling 2010: Posters, pages
1211?1219. Coling 2010 Organizing Committee,
Beijing, China.
Weiwei Sun. 2011. A stacked sub-word model
for joint Chinese word segmentation and part-of-
speech tagging. In Proceedings of the ACL 2011
Conference. Association for Computational Lin-
guistics, Portland, Oregon, United States.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii. 2009. A
978
discriminative latent variable Chinese segmenter
with hybrid word/character information. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 56?64. Association for Com-
putational Linguistics, Boulder, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In In
Fourth SIGHAN Workshop on Chinese Language
Processing.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and
general method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
384?394. Association for Computational Linguis-
tics, Uppsala, Sweden.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised Chi-
nese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics
(Coling 2008), pages 1017?1024. Coling 2008
Organizing Committee, Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. In International Journal
of Computational Linguistics and Chinese Lan-
guage Processing.
979
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77?81,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Further Experiments with Shallow Hybrid MT Systems
Christian Federmann1, Andreas Eisele1, Hans Uszkoreit1,2,
Yu Chen1, Sabine Hunsicker1, Jia Xu1
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit,yuchen,sabine.hunsicker,jia.xu}@dfki.de
Abstract
We describe our hybrid machine trans-
lation system which has been developed
for and used in the WMT10 shared task.
We compute translations from a rule-
based MT system and combine the re-
sulting translation ?templates? with par-
tial phrases from a state-of-the-art phrase-
based, statistical MT engine. Phrase sub-
stitution is guided by several decision
factors, a continuation of previous work
within our group. For the shared task,
we have computed translations for six lan-
guage pairs including English, German,
French and Spanish. Our experiments
have shown that our shallow substitu-
tion approach can effectively improve the
translation result from the RBMT system;
however it has also become clear that a
deeper integration is needed to further im-
prove translation quality.
1 Introduction
In recent years the quality of machine translation
(MT) output has improved greatly, although each
paradigm suffers from its own particular kind of
errors: statistical machine translation (SMT) of-
ten shows poor syntax, while rule-based engines
(RBMT) experience a lack in vocabulary. Hybrid
systems try to avoid these typical errors by com-
bining techniques from both paradigms in a most
useful manner.
In this paper we present the improved version of
the hybrid system we developed last year?s shared
task (Federmann et al, 2009). We take the out-
put from an RBMT engine as basis for our hybrid
translations and substitute noun phrases by trans-
lations from an SMT engine. Even though a gen-
eral increase in quality could be observed, our sys-
tem introduced errors of its own during the substi-
tution process. In an internal error analysis, these
degradations were classified as follows:
- the translation by the SMT engine is incorrect
- the structure degrades through substitution
(because of e.g. capitalization errors, double
prepositions, etc.)
- the phrase substitution goes astray (caused by
alignment problems, etc.)
Errors of the first class cannot be corrected, as
we have no way of knowing when the translation
by the SMT engine is incorrect. The other two
classes could be eliminated, however, by introduc-
ing additional steps for pre- and post-processing
as well as improving the hybrid algorithm itself.
Our current error analysis based on the results of
this year?s shared task does not show these types
of errors anymore.
Additionally, we extended our coverage to also
include the language pairs English?French and
English?Spanish in both directions as well as
English?German, compared to last year?s initial
experiments for German?English only. We were
able to achieve an increase in translation quality
for this language set, which shows that the substi-
tution method works for different language config-
urations.
2 Architecture
Our hybrid translation system takes translation
output from a) the Lucy RBMT system (Alonso
and Thurmair, 2003) and b) a Moses-based SMT
system (Koehn et al, 2007). We then identify
noun phrases inside the rule-based translation and
compute the most likely correspondences in the
statistical translation output. For these, we apply a
factored substitution method that decides whether
the original RBMT phrase should be kept or rather
be replaced by the Moses phrase. As this shallow
substitution process may introduce problems at
77
phrase boundaries, we afterwards perform several
post-processing steps to cleanup and finalize the
hybrid translation result. A schematic overview
of our hybrid system and its main components is
given in figure 1.
Figure 1: Schematic overview of the hybrid MT
system architecture.
2.1 Input to the Hybrid System
Lucy RBMT System We obtain the translation
as well as linguistic structures from the RBMT
system. An internal evaluation has shown that
these structures are usually of a high quality which
supports our initial decision to consider the RBMT
output as an appropriate ?template? for our hybrid
translation approach. The Lucy translation output
can include additional markup that allows to iden-
tify unknown words or other, local phenomena.
The Lucy system is a transfer-based MT system
that performs translation in three phases, namely
analysis, transfer, and generation. Intermediate
tree structures for each of the translation phases
can be extracted from the Lucy system to guide
the hybrid system. Sadly, only the 1-best path
through these three phases is given, so no alterna-
tive translation possibilities can be extracted from
the given data; a fact that clearly limits the poten-
tial for more deeply integrated hybrid translation
approaches. Nevertheless, the availability of the
1-best trees already allows to improve the transla-
tion quality of the RBMT system as we will show
in this paper.
Moses SMT System We used a state-of-the-art
Moses SMT system to create statistical phrase-
based translations of our input text. Moses has
been modified so that it returns the translation re-
sults together with the bidirectional word align-
ments between the source texts and the transla-
tions. Again, we make use of markup which helps
to identify unknown words as these will later guide
the factored substitution method. Both of the
translation models and the language models within
our SMT systems were only trained with lower-
cased and tokenized Europarl training data. The
system used sets of feature weights determined us-
ing data sets also from Europarl (test2008). In
addition, we used LDC gigaword corpus to train
large scale n-gram language models to be used in
our hybrid system. We tokenized the source texts
using the standard tokenizers available from the
shared task website. The SMT translations are re-
cased before being fed into the hybrid system to-
gether with the word alignment information.The
hybrid system can easily be adapted to support
other statistical translation engines. If the align-
ment information is not available, a suitable align-
ment tool would be necessary to compute it as the
alignment is a key requirement for the hybrid sys-
tem.
2.2 Aligning RBMT and SMT Output
We compute alignment in several components of
the hybrid system, namely:
source-text-to-tree: we first find an alignment
between the source text and the correspond-
ing analysis tree(s). As Lucy tends to sub-
divide large sentences into several smaller
units, it sometimes becomes necessary to
align more than one tree structure to a given
source sentence.
analysis-transfer-generation: for each of the
analysis trees, we re-construct the path from
its tree nodes, via the transfer tree, and their
corresponding generation tree nodes.
tree-to-target-text: similarly to the first align-
ment process, we find a mapping between
generation tree nodes and the actual transla-
tion output of the RBMT system.
source-text-to-tokenized: as the Lucy RBMT
system works on non-tokenized input text
and our Moses system takes tokenized input,
78
we need to align the source text to its tok-
enized form.
Given the aforementioned alignments, we can then
correlate phrases from the rule-based translation
with their counterparts from the statistical trans-
lation, both on source or target side. As our
hybrid approach relies on the identification of
such phrase pairs, the computation of the different
alignments is critical to obtain good combination
performance.
Please note that all these tree-based alignments
can be computed with a very high accuracy. How-
ever, due to the nature of statistical word align-
ment, the same does not hold for the alignment
obtained from the Moses system. If the alignment
process has produced erroneous phrase tables, it is
very likely that Lucy phrases and their ?aligned?
SMT matches simply will not fit. Or put the other
way round: the better the underlying SMT word
alignment, the greater the potential of the hybrid
substitution approach.
2.3 Factored Substitution
Given the results of the alignment process, we can
then identify ?interesting? phrases for substitution.
Following our experimental setup from last year?s
shared task, we again decided to focus on noun
phrases as these seem to be best-suited for in-place
swapping of phrases. Our initial assumption is that
SMT phrases are better on a lexical level, hence
we aim to replace Lucy?s noun phrases by their
Moses counterparts.
Still, we want to perform the substitution in a
controlled manner in order to avoid problems or
non-matching insertions. For this, we have (man-
ually) derived a set of factors that are checked for
each of the phrase pairs that are processed. The
factors are described briefly below:
identical? simply checks whether two candidate
phrases are identical.
too complex? a Lucy phrase is ?too complex?
to substitute if it contains more than 2
embedded noun phrases.
many-to-one? this factor checks if a Lucy phrase
containing more than one word is mapped to
a Moses phrase with only one token.
contains pronoun? checks if the Lucy phrase
contains a pronoun.
contains verb? checks if the Lucy phrase con-
tains a verb.
unknown? checks whether one of the phrases is
marked as ?unknown?.
length mismatch computes the number of words
for both phrases and checks if the absolute
difference is too large.
language model computes language model
scores for both phrases and checks which is
more likely according to the LM.
All of these factors have been designed and ad-
justed during an internal development phase using
data from previous shared tasks.
2.4 Post-processing Steps
After the hybrid translation has been computed,
we perform several post-processing steps to clean
up and finalize the result:
cleanup first, we perform basic cleanup opera-
tions such as whitespace normalization, cap-
italizing the first word in each sentence, etc.
multi-words then, we take care of proper han-
dling of multi-word expressions. Using the
tree structures from the RBMT system we
eliminate superfluous whitespace and join
multi-words, even if they were separated in
the SMT phrase.
prepositions finally, we give prepositions a spe-
cial treatment. Experience from last year?s
shared task had shown that things like double
prepositions contributed to a large extent to
the amount of avoidable errors. We tried to
circumvent this class of error by identifying
the correct prepositions; erroneous preposi-
tions are removed.
3 Hybrid Translation Analysis
We evaluated the intermediate outputs using
BLEU (Papineni et al, 2001) against human refer-
ences as in table 3. The BLEU score is calculated
in lower case after the text tokenization. The trans-
lation systems compared are Moses, Lucy, Google
and our hybrid system with different configura-
tions:
Hybrid: we use the language model with case
information and substitute some NPs in Lucy
outputs by Moses outputs.
Hybrid LLM: same as Hybrid but we use a
larger language model.
79
Table 1: Intermediate results of BLEU[%] scores for WMT10 shared task.
System de?en en?de fr?en en?fr es?en en?es
Moses 18.32 12.66 22.26 20.06 24.28 24.72
Lucy 16.85 12.38 18.49 17.61 21.09 20.85
Google 25.64 18.51 28.53 28.70 32.77 32.20
Hybrid 17.29 13.05 18.92 19.58 22.53 23.55
Hybrid LLM 17.37 13.73 18.93 19.76 22.61 23.66
Hybrid SG 17.43 14.40 19.67 20.55 24.37 24.99
Hybrid NCLM 17.38 14.42 19.56 20.55 24.41 24.92
Hybrid SG: same as Hybrid but the NP substitu-
tions are based on Google output instead of
Moses translations.
Hybrid NCLM: same as Hybrid but we use the
language model without case information.
We participated in the translation evaluation in
six language pairs: German to English (de?en),
English to German (en?de), French to English
(fr?en), English to French (en?fr), Spanish to
English (es?en) and English to Spanish (en?es).
As shown in table 3, the Moses translation sys-
tem achieves better results overall than the Lucy
system does. Google?s system outperforms other
systems in all language pairs. The hybrid transla-
tion as described in section 2 improves the Lucy
translation quality with a BLEU score up to 2.7%
absolutely.
As we apply a larger language model or a lan-
guage model without case information, the trans-
lation performance can be improved further. One
major problem in the hybrid translation is that the
Moses outputs are still not good enough to replace
the Lucy outputs, therefore we experimented on
a hybrid translation of Google and Lucy systems
and substitute some unrelaible NP translations by
the Google?s translations. The results in the line
of ?Hybrid SG? shows that the hybrid translation
quality can be enhanced if the translation system
where we select substitutions is better.
4 Internal Evaluation of Results
In the analysis of the remaining issues, the fol-
lowing main sources of problems can be distin-
guished:
- Lucy?s output contains structural errors that
cannot be fixed by the chosen approach.
- Lucy results contain errors that could have
been corrected by alternative expressions
from SMT, but the constraints in our system
were too restrictive to let that happen.
- The SMT engine we use generates subopti-
mal results that find their way into the hybrid
result.
- SMT results that are good are incorporated
into the hybrid results in a wrong way.
We have inspected a part of the results and classi-
fied the problems according to these criteria. As
this work is still ongoing, it is too early to report
numerical results for the relative frequencies of the
different causes of the error. However, we can
already see that three of these four cases appear
frequently enough to justify further attention. We
observed several cases in which the parser in the
Lucy system was confused by unknown expres-
sions and delivered results that could have been
significantly improved by a more robust parsing
approach. We also encountered several cases in
which an expression from SMT was used although
the original Lucy output would have been better.
Also we still observe problems finding to correct
correspondences between Lucy output and SMT
output, which leads to situations where material is
inserted in the wrong place, which can lead to the
loss of content words in the output.
5 Conclusion and Outlook
In our contribution to the shared task we have ap-
plied the hybrid architecture from (Federmann et
al., 2009) to six language pairs. We have identi-
fied and fixed many of the problems we had ob-
served last year, and we think that, in addition to
the increased coverage in laguage pairs, the overall
quality has been significantly increased.
However, in the last section we characterized
three main sources of problems that will require
further attention. We will address these problems
in the near future in the following way:
80
1. We will investigate in more detail the align-
ment issue that leads to occasional loss of
content words, and we expect that a careful
inspection and correction of the code will in
all likelihood give us a good remedy.
2. The problem of picking expressions from the
SMT output that appear more probable to the
language model although they are inferior to
the original expression from the RBMT sys-
tem is more difficult to fix. We will try to find
better thresholds and biases that can at least
reduce the number of cases in which this type
of degradation happen.
3. Finally, we will also address the robustness
issue that leads to suboptimal structures from
the RBMT engine caused by parsing failures.
Our close collaboration with Lucy enables us to
address these issues in a very effective way via the
inspection and classification of intermediate struc-
tures and, if these structures indicate parsing prob-
lems, the generation of variants of the input sen-
tence that facilitate correct parsing.
Acknowledgments
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme. The authors want to thank Michael
Jellinghaus and Bastian Simon for help with the
inspection of intermediate results and classifica-
tion of errors.
References
Juan A. Alonso and Gregor Thurmair. 2003. The Com-
prendium Translator system. In Proc. of the Ninth
MT Summit.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combination
using factored word substitution. In Proceedings of
the Fourth Workshop on Statistical Machine Transla
tion, pages 70?74, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
81
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485?489,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
DFKI Hybrid Machine Translation System for WMT 2011
- On the Integration of SMT and RBMT
Jia Xu and Hans Uszkoreit and Casey Kennington and David Vilar and Xiaojun Zhang
DFKI GmbH, Language Technology Lab
Stuhlsatzenhausweg 3
D-66123 Saarbru?cken Germany
{Jia.Xu,uszkoreit,David.Vilar}@dfki.de, {bakuzen,xiaojun.zhang.iiken}@gmail.com
Abstract
We present the DFKI hybrid translation sys-
tem at the WMT workshop 2011. Three SMT
and two RBMT systems are combined at the
level of the final translation output. The trans-
lation results show that our hybrid system sig-
nificantly outperformed individual systems by
exploring strengths of both rule-based and sta-
tistical translations.
1 Introduction
Machine translation (MT), in particular the statisti-
cal approach to it, has undergone incremental im-
provements in recent years. While rule-based ma-
chine translation (RBMT) maintains competitive-
ness in human evaluations. Combining the advan-
tages of both approaches have been investigated by
many researchers such as (Eisele et al, 2008).
Nonetheless, significant improvements over statis-
tical approaches still remain to be shown. In this
paper, we present the DFKI hybrid system in the
WMT workshop 2011. Our system is different from
the system of the last year (Federmann et al, 2010),
which is based on the shallow phrase substitution.
In this work, two rule-based translation systems are
applied. In addition, three statistical machine trans-
lation systems are built, including a phrase-based,
a hierarchical phrase-based and a syntax-based sys-
tem. Instead of combining with rules or post-editing,
we perform system combination on the final transla-
tion hypotheses. We applied the CMU open toolkit
(Heafield and Lavie, 2010) among numerous com-
bination methods such as (Matusov, 2009), (Sim et
al., 2007) and (He et al, 2008). The final transla-
tion output outperforms each individual output sig-
nificantly.
2 Individual translation systems
2.1 Phrase-based system
We use the IBM model 1 and 4 (Brown et al, 1993)
and Hidden-Markov model (HMM) (Vogel et al,
1996) to train the word alignment using the mgiza
toolkit1. We applied the EMS in Moses (Koehn et
al., 2007) to build up the phrase-based translation
system. Features in the log-linear model include
translation models in two directions, a language
model, a distortion model and a sentence length
penalty. A dynamic programming beam search al-
gorithm is used to generate the translation hypoth-
esis with maximum probability. We applied a 5-
gram mixture language model with each sub-model
trained on one fifth of the monolingual corpus with
Kneser-Ney smoothing using SRILM toolkit (Stol-
cke, 2002). We did not perform any tuning, because
it hurts the evaluation performance in our experi-
ments.
2.2 Syntax-based system
To capture the syntactic structure, we also built a
tree-based system using the same configuration of
EMS in Moses (Koehn et al, 2007). Tree-based
models operate on so-called grammar rules, which
include variables in the mapping rules. To increase
the diversity of models in combination, the lan-
guage model in each individual translation system
is trained differently. For the tree-based system,
we applied a 4-gram language model with Kneser-
Ney smoothing using SRILM toolkit (Stolcke, 2002)
trained on the whole monolingual corpus. The
test2007 news part is applied to tune the feature
weights using mert, because the tuning on test2007
1http://geek.kyloo.net/software/doku.php/mgiza:overview
485
improves the translation performance more than the
tuning on test2008 in a small-scale experiment for
the tree-based system.
2.3 Hierarchical phrase-based system
For the hierarchical system, we used the open source
hierarchical phrased-based system Jane, developed
at RWTH and free for non-commercial use (Vi-
lar et al, 2010). This approach is an extension
of the phrase-based approach, where the phrases
are allowed to have gaps (Chiang, 2007). In this
way long-range dependencies and reorderings can
be modeled in a consistent statistical framework.
The system uses a fairly standard setup, trained
using the bilingual data provided by the organizers,
word aligned using the mgiza. Two 5-gram language
models were used during decoding: one trained on
the monolingual part of the bilingual training data,
and a larger one trained on the additional news data.
Decoding was carried out using the cube pruning al-
gorithm. The tuning is performed on test2008 with-
out further experiments.
2.4 Rule-based systems
We applied two rule-based translation systems, the
Lucy system (Lucy, 2011) and the Linguatec sys-
tem (Aleksic? and Thurmair, 2011). The Lucy sys-
tem is a recent offspring of METAL. The Linguatec
system is a modular system consisting of grammar,
lexicon and morphological analyzers based on logic
programming using slot grammar.
3 Hybrid translation
A hybrid approach combining rule-based and sta-
tistical machine translation is usually investigated
with an in-box integration, such as multi-way trans-
lation (Eisele et al, 2008), post-editing (Ueffing et
al., 2008) or noun phrase substitution (Federmann
et al, 2010). However, significant improvements
over state-of-the-art statistical machine translation
are still expected. In the meanwhile system combi-
nation methods for instance described in (Matusov,
2009), (Sim et al, 2007) and (He et al, 2008) are
mostly evaluated to combine statistical translation
systems, rule-based systems are not considered. In
this work, we integrate the rule-based and statistical
machine translation system on the level of the final
PBT Syntax
PBT-2010 18.32
Max80words 20.65 21.10
Max100words 20.78
+Compound 21.52 22.13
+Newparallel 21.77
Table 1: Translation performance BLEU[%] on
phrase/syntax-based system using various settings eval-
uated on test10.
translation hypothesis and treat the rule-based sys-
tem anonymously as an individual system. In this
way an black-box integration is allowed using the
current system combination techniques.
We applied the CMU open toolkit (Heafield
and Lavie, 2010) MEMT, a package by Kenneth
Heafield to combine the translation hypotheses. The
language model is trained on the target side of the
parallel training corpus using SRILM (Stolcke,
2002). We used only the Europarl part to train lan-
guage models for tuning and all target side of paral-
lel data to train language models for decoding. The
beam size is set to 80, and 300 nbest is considered.
4 Translation experiments
4.1 MT Setup
The parallel training corpus consists of 1.8
million German-English parallel sentences from
Europarl-v6 (Koehn, MT Summit 2005) and news-
commentary with 48 million tokenized German
words and 54 million tokenized English words re-
spectively. The monolingual training corpus con-
tains the target side of the parallel training cor-
pus and the additional monolingual language model
training data downloaded from (SMT, 2011). We
did not apply the large-scale Gigaword corpus, be-
cause it does not significantly reduce the perplexity
of our language model but raises the computational
requirement heavily.
4.2 Single systems
For each individual translation system, different
configurations are experimented to achieve a higher
translation quality. We take phrase- and syntax-
based translation system as examples. Table 1
presents official submission result on DE-EN by
486
PBT+Syntax 20.37
PBT+Syntax+HPBT 20.78
PBT+HPBT+Linguatec+Lucy 20.27
PBT+Syntax+HPBT+Linguatec+Lucy 20.81
Table 2: Translation performance BLEU[%] on test2011
using hybrid system tuned on test10 with various settings
(DE-EN).
DFKI in 2010. In 2010?s translation system only
Europarl parallel corpus was applied, and the trans-
lation output was evaluated as 18.32% in the BLEU
score. In 2011, we added the News Commentary
parallel corpus and trained the language model on all
monolingual data provided by (SMT, 2011) except
for Gigaword. As shown in Table 1, if we increase
the maximum sentence length of the training cor-
pus from 80 to 100, the BLEU score increases from
20.65% to 20.78%. In the error analysis, we found
that many OOVs come from the compound words
in German. Therefore, we applied the compound
splitting for both German and English by activating
the corrensponding settings in the EMS in Moses.
This leads to a further improvement of nearly 1%
in the BLEU score. As we add the new parallel
corpus provided on the homepage of SMT work-
shop in 2011 (SMT, 2011) to the corpus in 2010,
a slight improvement can be achieved. Within one
year, the score for the DFKI PBT system DE-EN has
improved by nearly 3.5% absolute and 20% relative
BLEU score points, as shown in Table 1.
In the phrase-based translation, the tuning was not
applied, because it improves the results on the held-
out data but hurts the results on the evaluation set.
In our observation, the decrease is in the range of
0.01% to 1% in the BLEU score. However tun-
ing does help for the Tree-based system. Therefore
we applied the test2007 to optimize the parameters,
which enhanced the BLEU score from 17.52% to
21.10%. The compound splitting also improves the
syntax system, with about 1% in the BLEU score.
We did not add the new parallel corpus into the train-
ing for syntax system due to its larger computational
requirement than that of the phrase-based system.
Test10 Test08 Test11
Hybrid-2010 17.43
PBT 21.77 20.70 20.40
Syntax 22.13 20.50 20.49
HPBT 19.21 18.26 17.06
Linguatec 16.59 16.07 15.97
Lucy 16.57 16.66 16.68
Hybrid-2011 23.88 21.13 21.25
Table 3: Translation performance BLEU[%] on three test
sets using different translation systems in 2011 submis-
sion (DE-EN).
Test10 Test11
Hybrid-2010 14.42
PBT 15.46 14.05
Linguatec 14.92 12.92
Lucy 13.77 13.0
Hybrid-2011 15.55 15.83
Table 4: Translation performance BLEU[%] on two test
sets using different translation systems in 2011 submis-
sion (EN-DE).
4.3 Hybrid system
We applied test10 as the held-out data to tune
the German-English and English-German transla-
tion systems. For experiments, we applied a small-
scaled 4-gram language model trained only on the
target side of the Europarl parallel training data. As
shown in Table 2, different combinations are per-
formed on the hypotheses generated from single sys-
tems. We first combined the PBT with syntax sys-
tem, then together with the HPBT system. The
translation result in the BLEU score performs best
when we combine all three statistical machine trans-
lation systems and two rule-based systems together.
4.4 Evaluation results
For the decoding during the WMT evaluation, we
applied a larger 4-gram language model trained on
the target side of all parallel training corpus. As
shown in Table 3, in last year?s evaluation the DFKI
hybrid translation result was evaluated as 17.34% in
the BLEU score. In 2011, among all the transla-
tion systems, the syntax system performs the best
on test10 and test11, while the PBT performs the
487
SRC Diese Verordnung wurde vom Gesundheitsministerium in diesem Jahr einigermassen gemildert - die Ku?hlschrankpflicht fiel weg.
REF It was mitigated by the Ministry of Health this year - the obligation to have a refrigerator has been removed.
PBT This regulation by the Ministry of Health in this year - somewhat mitigated the fridge duty fell away.
Syntax This regulation was somewhat mitigated by the Ministry of Health this year - the refrigerator duty fell away.
HPBT This regulation was by the Ministry of Health in reasonably Dokvadze this year - the Ku?hlschrankpflicht fell away.
Linguatec This ordinance was soothed to some extent by the brazilian ministry of health this year, the refrigerator duty was discontinued.
Lucy This regulation was quite moderated by the Department of Health, Education and Welfare this year - the refrigerator duty was omitted.
Hybrid This regulation was somewhat mitigated by the Ministry of Health this year - the fridge duty fell away.
SRC Die Deregulierung und Bakalas ehemalige Bergarbeiterwohnungen sind ein brisantes Thema.
REF Deregulation and Bakala ?s former mining flats are local hot topic.
PBT The deregulation and Bakalas former miners? homes are a sensitive issue.
Syntax The deregulation and Bakalas former miners? homes are a sensitive issue.
HPBT The deregulation and Bakalas former Bergarbeiterwohnungen are a hot topic.
Linguatec Former miner flats are an explosive topic the deregulation and Bakalas.
HPBT The deregulation and Bakalas former miner apartments are an explosive topic.
Hybrid The deregulation and Bakalas former miners? apartments are a sensitive issue.
Table 5: Examples of translation output by the different systems.
best on test08. The rule-based sytems, Linguatec
and Lucy are expected to have a higher score in the
human evaluation than in the automatic evaluation.
Furthermore, as we can see from Table 3, there is
still room to improve the Jane system, with better
modeling, configurations or even higher-order lan-
guage model. Using the hybrid system we success-
fully improved the translation result to 23.88% on
test10. The hybrid system outperforms the best sin-
gle system by 0.43% and 0.76% in the BLEU score
on the test08 and test11, respectively.
For the translation from English to German, the
translation result of last year?s submission was eval-
uated as 14.42% in the BLEU score, as shown in Ta-
ble 4. In this year, the phrase-based translation result
is 15.46% in the BLEU score. We only set up one
statistical translation system due to time limitation.
With the respect of the BLEU score, phrase-based
translation outperforms rule-based translations. Be-
tween rule-based translation systems, Linguatec per-
forms better on the test10 (14.92%) and Lucy per-
forms better on the test11 (13.0%). Combining three
translation hypotheses leads to a smaller improve-
ment (from 15.46% to 15.55%) on the test10 and a
greater improvement (from 14.05% to 15.83%) on
the test11 in the BLEU score over the single best
translation system. Comparing to last year?s trans-
lation output, the improvement is over one percent
absolutely (from 14.42% to 15.55%) in the BLEU
score on the test10.
4.5 Output examples
Table 5 shows two translation examples from the
MT output of the test2011. We list the source sen-
tence in German and its reference translation as
well as the translation results generated by different
translation systems. As can be seen from Table 5,
the translation quality of source sentences is greatly
improved using the hybrid system over the single in-
dividual systems. Translations of words and word
orderings are more appropriate by the hybrid sys-
tem.
5 Conclusion and future work
We presented the DFKI hybrid translation system
submitted in the WMT workshop 2011. The hy-
brid translation is performed on the final translation
output by individual systems, including a phrase-
based system, a syntax-based system, a hierarchical
phrase-based system and two rule-based systems.
Combining the results from statistical and rule-
based systems significantly improved the translation
performance over the single-best system, which is
shown by the automatic evaluation scores and the
output examples. Despite of the encouraging results,
there is still room to improve our system, such as the
tuning in the phrase-based translation and a better
language model in the combination.
488
References
Vera Aleksic? and Gregor Thurmair. 2011. Personal
translator at wmt2011 - a rule-based mt system with
hybrid components. In Proceedings of WMT work-
shop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Andreas Eisele, Christian Federmann, Hans Uszkoreit,
Herve? Saint-Amand, Martin Kay, Michael Jellinghaus,
Sabine Hunsicker, Teresa Herrmann, and Yu Chen.
2008. Hybrid architectures for multi-engine machine
translation. In Proceedings of Translating and the
Computer 30, pages ASLIB, ASLIB/IMI, London,
United Kingdom, November.
Christian Federmann, Andreas Eisele, Hans Uszkoreit,
Yu Chen, Sabine Hunsicker, and Jia Xu. 2010. Fur-
ther experiments with shallow hybrid mt systems. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 237?
248, Uppsala, Sweden. John Benjamins.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
October.
Kenneth Heafield and Alon Lavie. 2010. Voting on n-
grams for machine translation system combination. In
Proc. Ninth Conference of the Association for Machine
Translation in the Americas, Denver, Colorado, Octo-
ber.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL.
Philipp Koehn. MT Summit 2005. Europarl: A parallel
corpus for statistical machine translation.
Lucy. 2011. Home page of software lucy and services.
http://www.lucysoftware.com.
Evgeny Matusov. 2009. Combining Natural Language
Processing Systems to Improve Machine Translation
of Speech. Ph.D. thesis, Department of Electrical
and Computer Engineering, Johns Hopkins University,
Baltimore, MD.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodland. 2007. Consensus network decoding
for statistical machine translation system combination.
In IN IEEE INT. CONF. ON ACOUSTICS, SPEECH,
AND SIGNAL PROCESSING.
SMT. 2011. Sixth workshop on statistical machine trans-
lation home page. http://www.statmt.org/wmt11/.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference On Spoken Language Processing, pages
901?904, Denver, Colorado, September.
Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo ic
Dugast, George F. Foster, Roland Kuhn, Jean Senel-
lart, and Jin Yang. 2008. Tighter integration of rule-
based and statistical mt in serial system combination.
In Proceedings of COLING 2008, pages 913?920.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Trans-
lation, Extended with Reordering and Lexicon Mod-
els. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
262?270, Uppsala, Sweden, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copenhagen,
Denmark, August.
489
