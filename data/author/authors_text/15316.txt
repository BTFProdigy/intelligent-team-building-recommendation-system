Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 57?60,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Framework for Entailed Relation Recognition
Dan Roth Mark Sammons V.G.Vinod Vydiswaran
University of Illinois at Urbana-Champaign
{danr|mssammon|vgvinodv}@illinois.edu
Abstract
We define the problem of recognizing entailed re-
lations ? given an open set of relations, find all oc-
currences of the relations of interest in a given doc-
ument set ? and pose it as a challenge to scalable
information extraction and retrieval. Existing ap-
proaches to relation recognition do not address well
problems with an open set of relations and a need
for high recall: supervised methods are not eas-
ily scaled, while unsupervised and semi-supervised
methods address a limited aspect of the problem, as
they are restricted to frequent, explicit, highly lo-
calized patterns. We argue that textual entailment
(TE) is necessary to solve such problems, propose
a scalable TE architecture, and provide preliminary
results on an Entailed Relation Recognition task.
1 Introduction
In many information foraging tasks, there is a need
to find all text snippets relevant to a target concept.
Patent search services spend significant resources
looking for prior art relevant to a specified patent
claim. Before subpoenaed documents are used in
a court case or intelligence data is declassified, all
sensitive sections need to be redacted. While there
may be a specific domain for a given application,
the set of target concepts is broad and may change
over time. For these knowledge-intensive tasks,
we contend that feasible automated solutions re-
quire techniques which approximate an appropri-
ate level of natural language understanding.
Such problems can be formulated as a relation
recognition task, where the information need is ex-
pressed as tuples of arguments and relations. This
structure provides additional information which
can be exploited to precisely fulfill the informa-
tion need. Our work introduces the Entailed Rela-
tion Recognition paradigm, which leverages a tex-
tual entailment system to try to extract all relevant
passages for a given structured query without re-
quiring relation-specific training data. This con-
trasts with Open Information Extraction (Banko
and Etzioni, 2008) and On-Demand Information
Extraction (Sekine, 2006), which aim to extract
large databases of open-ended facts, and with su-
pervised relation extraction, which requires addi-
tional supervised data to learn new relations.
Specifically, the contributions of this paper are:
1. Introduction of the entailed relation recognition
framework; 2. Description of an architecture and a
system which uses structured queries and an exist-
ing entailment engine to perform relation extrac-
tion; 3. Empirical assessment of the system on a
corpus of entailed relations.
2 Entailed Relation Recognition (ERR)
In the task of Entailed Relation Recognition, a cor-
pus and an information need are specified. The
corpus comprises all text spans (e.g. paragraphs)
contained in a set of documents. The information
need is expressed as a set of tuples encoding rela-
tions and entities of interest, where entities can be
of arbitrary type. The objective is to retrieve all
relevant text spans that a human would recognize
as containing a relation of interest. For example:
Information Need: An organization acquires weapons.
Text 1: ...the recent theft of 500 assault rifles by FARC...
Text 2: ...the report on FARC activities made three main ob-
servations. First, their allies supplied them with the 3? mor-
tars used in recent operations. Second, ...
Text 3: Amnesty International objected to the use of artillery
to drive FARC militants from heavily populated areas.
An automated system should identify Texts 1 and
2 as containing the relation of interest, and Text 3
as irrelevant. The system must therefore detect
relation instances that cross sentence boundaries
(?them? maps to ?FARC?, Text 2), and that re-
quire inference (?theft? implies ?acquire?, Text 1).
It must also discern when sentence structure pre-
cludes a match (?Amnesty International... use...
artillery? does not imply ?Amnesty International
57
acquires artillery?, Text 3).
The problems posed by instances like Text 2
are beyond the scope of traditional unsuper-
vised and semi-supervised relation-extraction ap-
proaches such as those used by Open IE and On-
Demand IE, which are constrained by their de-
pendency on limited, sentence-level structure and
high-frequency, highly local patterns, in which
relations are explicitly expressed as verbs and
nouns. Supervised methods such as (Culotta and
Sorensen, 2004) and (Roth and Yih, 2004) pro-
vide only a partial solution, as there are many pos-
sible relations and entities of interest for a given
domain, and such approaches require new anno-
tated data each time a new relation or entity type is
needed. Information Retrieval approaches are op-
timized for document-level performance, and en-
hancements like pseudo-feedback (Rocchio, 1971)
are less applicable to the localized text spans
needed in the tasks of interest; as such, it is un-
likely that they will reliably retrieve all correct in-
stances, and not return superficially similar but in-
correct instances (such as Text 3) with high rank.
Attempts have been made to apply Textual En-
tailment in larger scale applications. For the task
of Question Answering, (Harabagiu and Hickl,
2006) applied a TE component to rerank candidate
answers returned by a retrieval step. However, QA
systems rely on redundancy in the same way Open
IE does: a large document set has so many in-
stances of a given relation that at least some will
be sufficiently explicit and simple that standard IR
approaches will retrieve them. A single correct in-
stance suffices to complete the QA task, but does
not meet the needs of the task outlined here.
Recognizing relation instances requiring infer-
ence steps, in the absence of labeled training data,
requires a level of text understanding. A suit-
able proxy for this would be a successful Textual
Entailment Recognition (TE) system. (Dagan et
al., 2006) define the task of Recognizing Textual
Entailment (RTE) as: ...a directional relation be-
tween two text fragments, termed T ? the entailing
text, and H ? the entailed text. T entails H if, typ-
ically, a human reading T would infer that H is
most likely true. For relation recognition, the rela-
tion triple (e.g. ?Organization acquires weapon?)
is the hypothesis, and a candidate text span that
might contain the relation is the text. The def-
inition of RTE clearly accommodates the range
of phenomena described for the examples above.
However, the more successful TE systems (e.g.
(Hickl and Bensley, 2007)) are typically resource
intensive, and cannot scale to large retrieval tasks
if a brute force approach is used.
We define the task of Entailed Relation Recog-
nition thus: Given a text collection D, and an in-
formation need specified in a set of [argument, re-
lation, argument] triples S: for each triple s ? S,
identify all texts d ? D such that d entails s.
The information need triples, or queries, encode
relations between arbitrary entities (specifically,
these are not constrained to be Named Entities).
This problem is distinct from recent work in
Textual Entailment as we constrain the structure
of the Hypothesis to be very simple, and we re-
quire that the task be of a significantly larger scale
than the RTE tasks to date (which are typically of
the order of 800 Text-Hypothesis pairs).
3 Scalable ERR Algorithm
Our scalable ERR approach, SERR, consists of
two stages: expanded lexical retrieval, and entail-
ment recognition. The SERR algorithm is pre-
sented in Fig. 1. The goal is to scale Textual
Entailment up to a task involving large corpora,
where hypotheses (queries) may be entailed by
multiple texts. The task is kept tractable by de-
composing TE capabilities into two steps.
The first step, Expanded Lexical Retrieval
(ELR), uses shallow semantic resources and simi-
larity measures, thereby incorporating some of the
semantic processing used in typical TE systems.
This is required to retrieve, with high recall, se-
mantically similar content that may not be lexi-
cally similar to query terms, to ensure return of
a set of texts that are highly likely to contain the
concept of interest.
The second step applies a textual entailment
system to this text set and the query in order to
label the texts as ?relevant? or ?irrelevant?, and re-
quires deeper semantic resources in order to dis-
cern texts containing the concept of interest from
those that do not. This step emphasizes higher pre-
cision, as it filters irrelevant texts.
3.1 Implementation of SERR
In the ELR stage, we use a structured query that
allows more precise search and differential query
expansion for each query element. Semantic units
in the texts (e.g. Named Entities, phrasal verbs)
are indexed separately from words; each index is
58
SERR Algorithm
SETUP:
Input: Text set D
Output: Indices {I} over D
for all texts d ? D
Annotate d with local semantic content
Build Search Indices {I} over D
APPLICATION:
Input: Information need S
EXPANDED LEXICAL RETRIEVAL (ELR)(s):
R? ?
Expand s with semantically similar words
Build search query q
s
from s
R? k top-ranked texts for q
s
using indices {I}
return R
SERR:
Answer set A? ?
for all queries s ? S
R? ELR(s)
Answer set A
s
? ?
for all results r ? R
Annotate s, r with NLP resources
if r entails s
A
s
? A
s
? r
A? A ? {A
s
}
return A
Figure 1. SERR algorithm
a hierarchical similarity structure based on a type-
specific metric (e.g. WordNet-based for phrasal
verbs). Query structure is also used to selectively
expand query terms using similarity measures re-
lated to types of semantic units, including distribu-
tional similarity (Lin and Pantel, 2001), and mea-
sures based on WordNet (Fellbaum, 1998).
We assess three different Textual Entailment
components: LexPlus, a lexical-level system
that achieves relatively good performance on the
RTE challenges, and two variants of Predicate-
based Textual Entailment, PTE-strict and PTE-
relaxed, which use a predicate-argument repre-
sentation. The former is constrained to select a
single predicate-argument structure from each re-
sult, which is compared to the query component-
by-component using similarity measures similar to
the LexPlus system. PTE-relaxed drops the single-
predicate constraint, and can be thought of as a
?bag-of-constituents? model. In both, features are
extracted based on the predicate-argument compo-
nents? match scores and their connecting structure,
and the rank assigned by ELR. These features are
used by a classifier that labels each result as ?rel-
evant? or ?irrelevant?. Training examples are se-
lected from the top 7 results returned by ELR for
queries corresponding to entailment pair hypothe-
ses from the RTE development corpora; test exam-
ples are similarly selected from results for queries
from the RTE test corpora (see section 3.2).
3.2 Entailed Relation Recognition Corpus
To assess performance on the ERR task, we de-
rive a corpus from the publicly available RTE
data. The corpus consists of a set S of informa-
tion needs in the form of [argument, relation, argu-
ment] triples, and a set D of text spans (short para-
graphs), half of which entail one or more s ? S
while the other half are unrelated to S. D com-
prises all 1, 950 Texts from the IE and IR sub-
tasks of the RTE Challenge 1?3 datasets. The
shorter hypotheses in these examples allow us to
automatically induce their structured query form
from their shallow semantic structure. S was au-
tomatically generated from the positive entailment
pairs in D, by annotating their hypotheses with a
publicly available SRL tagger (Punyakanok et al,
2008) and inferring the relation and two main ar-
guments to form the equivalent queries.
Since some Hypotheses and Texts appear mul-
tiple times in the RTE corpora, we automatically
extract mappings from positive Hypotheses to one
or more Texts by comparing hypotheses and texts
from different examples. This provides the label-
ing needed for evaluation. In the resulting corpus,
a wide range of relations are sparsely represented;
they exemplify many linguistic and semantic char-
acteristics required to infer the presence of non-
explicit relations.
4 Results and Discussion
Top # Basic ELR Rel.Impr. Err.Redu.
1 48.1% 55.2% +14.8% 13.7%
2 68.1% 72.8% +6.9% 14.7%
3 75.2% 78.5% +4.4% 17.7%
Table 1. Change in relevant results retrieved in top 3
positions for basic and expanded lexical retrieval
System Acc. Prec. Rec. F
1
Baseline 18.1 18.1 100.0 30.7
LexPlus 81.6 44.9 62.5 55.5
PTE-relax. 71.9 37.7 72.0 49.0
(0.1) (5.5) (6.2) (4.1)
PTE-strict 83.6 55.4 61.5 57.9
(1.3) (3.4) (7.9) (2.1)
Table 2. Comparison of performance of SERR with
different TE algorithms. Numbers in parentheses are
standard deviations.
Table 1 compares the results of SERR with and
59
# System RTE 1 RTE 2 RTE 3 Avg. Acc.
LexPlus 49.0 65.2 [3] 76.5 [2] 66.3
PTE-relaxed 54.5 (1.0) 68.7 (1.5) [3] 82.3 (2.0) [1] 71.2 (1.2)
PTE-strict 64.8 (2.3) [1] 71.2 (2.6) [3] 76.0 (3.2) [2] 71.8 (2.6)
Table 3. Performance (accuracy) of SERR system variants on RTE challenge
examples; numbers in parentheses are standard deviations, while numbers in
brackets indicate where systems would have ranked in the RTE evaluations.
Comparisons
Standard TE 3,802,500
SERR 13,650
Table 4. Entailment compar-
isons needed for standard TE
vs. SERR
without the ELR?s semantic enhancements. For
each rank k, the entries represent the proportion of
queries for which the correct answer was returned
in the top k positions. The semantic enhancements
improve the number of matched results at each of
the top 3 positions.
Table 2 compares variants of the SERR imple-
mentation. The baseline labels every result re-
turned by ELR as ?relevant?, giving high recall
but low precision. PTE-relaxed performs better
than baseline, but poorly compared to PTE-strict
and LexPlus. Our analysis shows that LexPlus
has a relatively high threshold, and correctly labels
as negative some examples mislabeled by PTE-
relaxed, which may match two of the three con-
stituents in a hypothesis and label that result as
positive. PTE-strict will correctly identify some
such examples as it will force some match edges to
be ignored, and will correctly identify some neg-
ative examples due to structural constraints even
when LexPlus finds matches for all query terms.
PTE-strict strikes the best balance between preci-
sion and recall on positive examples.
Table 3 shows the accuracy of SERR?s clas-
sification of the examples from each RTE chal-
lenge; results not returned in the top 7 ranks by
ELR are labeled ?irrelevant?. PTE-strict and PTE-
relaxed perform comparably overall, though PTE-
strict has more uniform results over the different
challenges. Both outperform the LexPlus system
overall, and perform well compared to the best re-
sults published for the RTE challenges.
The significant computational gain of SERR is
shown in Table 4, exhibiting the much greater
number of comparisons required by a brute force
TE approach compared to SERR: SERR performs
well compared to published results for RTE chal-
lenges 1-3, but makes only 0.36% of the TE com-
parisons needed by standard approaches on our
ERR task.
5 Conclusion
We have proposed an approach to solving the En-
tailed Relation Recognition task, based on Tex-
tual Entailment, and implemented a solution that
shows that a Textual Entailment Recognition sys-
tem can be scaled to a much larger IE problem
than that represented by the RTE challenges. Our
preliminary results demonstrate the utility of the
proposed architecture, which allows strong perfor-
mance in the RTE task and efficient application to
a large corpus (table 4).
Acknowledgments
We thank Quang Do, Yuancheng Tu, and Kevin
Small. This work is funded by a grant from Boeing
and by MIAS, a DHS-IDS Center for Multimodal
Information Access and Synthesis at UIUC.
References
[Banko and Etzioni2008] M. Banko and O. Etzioni. 2008.
The Tradeoffs Between Open and Traditional Relation Ex-
traction. In ACL-HLT, pages 28?36.
[Culotta and Sorensen2004] A. Culotta and J. Sorensen.
2004. Dependency Tree Kernels for Relation Extraction.
In ACL, pages 423?429.
[Dagan et al2006] I. Dagan, O. Glickman, and B. Magnini,
editors. 2006. The PASCAL Recognising Textual Entail-
ment Challenge., volume 3944. Springer-Verlag, Berlin.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
[Harabagiu and Hickl2006] S. Harabagiu and A. Hickl. 2006.
Methods for Using Textual Entailment in Open-Domain
Question Answering. In ACL, pages 905?912.
[Hickl and Bensley2007] A. Hickl and J. Bensley. 2007. A
Discourse Commitment-Based Framework for Recogniz-
ing Textual Entailment. In ACL, pages 171?176.
[Lin and Pantel2001] D. Lin and P. Pantel. 2001. Induction of
semantic classes from natural language text. In SIGKDD,
pages 317?322.
[Punyakanok et al2008] V. Punyakanok, D. Roth, and
W. Yih. 2008. The Importance of Syntactic Parsing and
Inference in Semantic Role Labeling. CL, 34(2).
[Rocchio1971] J. Rocchio, 1971. Relevance feedback in In-
formation Retrieval, pages 313?323. Prentice Hall.
[Roth and Yih2004] D. Roth and W. Yih. 2004. A linear pro-
gramming formulation for global inference in natural lan-
guage tasks. In CoNLL, pages 1?8.
[Sekine2006] S. Sekine. 2006. On-Demand Information Ex-
traction. In COLING/ACL, pages 731?738.
60
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 21?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Textual Entailment 
Mark Sammons, University of Illinois
Idan Szpektor, Yahoo!
V.G. Vinod Vydiswaran, University of Illinois
The NLP and ML communities are rising to grander, larger-scale
challenges such as Machine Reading, Learning by Reading, and Learning
to Read, challenges requiring deeper and more integrated natural
language understanding capabilities.
The task of Recognizing Textual Entailment (RTE) requires automated
systems to identify when two spans of text share a common meaning --
for example, that ``Alphaville Inc.'s attempted acquisition of Bauhaus
led to a jump in both companies' stock prices'' entails ``Bauahaus'
stock rose'', but not ``Alphaville acquired Bauhaus''.  This general
capability would be a solid proxy for Natural Language Understanding,
and has direct relevance to the grand challenges named above.
Moreover, it could be used to improve performance in a large range of
Natural Language Processing tasks such as Information Extraction,
Question Answering, Exhaustive Search, Machine Translation and many
others.  The operational definition of Textual Entailment used by
researchers in the field avoids commitment to any specific knowledge
representation, inference method, or learning approach, thus
encouraging application of a wide range of techniques to the problem.
Techniques developed for RTE have now been successfully applied in the
domains of Question Answering, Relation Extraction, and Machine
translation, and RTE systems continue to improve their performance
even as the corpora on which they are evaluated (provided first by
PASCAL, and now by NIST TAC) have become progressively more
challenging.  Over the sequence of RTE challenges from PASCAL and NIST
TAC, the more successful systems seem to have converged in their
overall approach.
The goal of this tutorial is to introduce the task of Recognizing
Textual Entailment to researchers from other areas of NLP.  We will
identify and analyze common inference and learning approaches from a
range of the more successful RTE systems, and investigate the role of
knowledge resources.  We will examine successful applications of RTE
techniques to Question Answering and Machine Translation, and identify
key research challenges that must be overcome to continue improving
RTE systems.
21
Tutorial Outline
1. Introduction (35 minutes)
Define and motivate the Recognizing Textual Entailment (RTE)
task. Introduce the RTE evaluation framework. Define the relationship
between RTE and other major NLP tasks.  Identify (some of) the
semantic challenges inherent in the RTE task, including the
introduction of 'contradiction' as an entailment category.  Describe
the use of RTE components/techniques in Question Answering, Machine
Translation, and Relation Extraction.
2. The State of the Art (35 minutes)
Outline the basic structure underlying RTE systems.  With reference to
recent publications on RTE: cover the range of preprocessing/analysis
that may be used; define representations/data structures typically
used; outline inference procedures and machine learning techniques.
Identify challenging aspects of the RTE problem in the context
of system successes and failures. 
3. Machine Learning for Recognizing Textual Entailment (35 minutes)
Describe the challenges involved in applying machine learning techniques
to the Textual Entailment problem.  Describe in more detail the main
approaches to inference, which explicitly or implicitly use the concept
of alignment.  Show how alignment fits into assumptions of semantic
compositionality, how it facilitates machine learning approaches, and
how it can accommodate phenomena-specific resources.  Show how it
can be used for contradiction detection.  
4. Knowledge Acquisition and Application in Textual Entailment (35 minutes)
Establish the role of knowledge resources in Textual Entailment,
and the consequent importance of Knowledge Acquisition.
Identify knowledge resources currently used in RTE systems, and their
limitations.  Describe existing knowledge acquisition approaches,
emphasizing the need for learning directional semantic relations.
Define suitable representations and algorithms for using knowledge,
including context-sensitive knowledge application.  Discuss the
problem of noisy data, and the prospects for new knowledge
resources/new acquisition approaches.
22
5. Key Challenges for Recognizing Textual Entailment (15 minutes)
Identify the key challenges in improving textual entailment systems:
more reliable inputs (when is a solved problem not solved), domain
adaptation, missing knowledge, scaling up.  The need for a common
entailment infrastructure to promote resource sharing and development.
Biographical Information of the Presenters
Mark Sammons
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
Phone: 1-217-265-6759
Email: mssammon@illinois.edu
Mark Sammons is a Principal Research Scientist working with the Cognitive 
Computation Group at the University of Illinois.  His primary interests are in Natural 
Language Processing and Machine Learning, with a focus on integrating diverse 
information sources in the context of Textual Entailment. His work has focused on 
developing a Textual Entailment framework that can easily incorporate new resources; 
designing appropriate inference procedures for recognizing entailment; and identifying 
and developing automated approaches to recognize and represent implicit content in 
natural language text. Mark received his MSC in Computer Science from the University 
of Illinois in 2004, and his PhD in Mechanical Engineering from the University of Leeds, 
England, in 2000. 
Idan Szpektor
Yahoo! Research, Building 30 Matam Park, Haifa 31905, ISRAEL.
Phone: + 972-74-7924666; Email: idan@yahoo-inc.com 
Idan Szpektor is a Research Scientist at Yahoo! Research. His primary research 
interests are  in natural language processing, machine learning and information 
retrieval. Idan recently submitted his PhD thesis at Bar-Ilan University where he worked 
on unsupervised acquisition and application of broad-coverage knowledge-bases for 
Textual Entailment. He has been a main organizer of the second PASCAL Recognizing 
Textual Entailment Challenge and an advisor for the third RTE Challenge. He served on 
the program committees of EMNLP and TextInfer and reviewed papers for ACL, 
COLING and EMNLP. Idan Szpektor received his M.Sc. from Tel-Aviv University in 
2005, where he worked on unsupervised knowledge acquisition for Textual Entailment. 
V.G.Vinod Vydiswaran
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
23
Phone: 1-217-333-2584
Email: vgvinodv@illinois.edu
V.G.Vinod Vydiswaran is a 3rd year Ph.D. student in the Department of Computer 
Science at the University of Illinois at Urbana-Champaign. His research interests include 
text informatics, natural language processing, machine learning, and information 
extraction. His work has included developing a Textual Entailment system, and applying 
Textual Entailment to relation extraction and information retrieval. He received his 
Masters degree from Indian Institute of Technology Bombay, India in 2004, where he 
worked on Conditional models for Information Extraction. Later, he worked at Yahoo! 
Research & Development Center at Bangalore, India, on scaling Information Extraction 
technologies over the Web.
24
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1199?1208,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
?Ask not what Textual Entailment can do for You...?
Mark Sammons V.G.Vinod Vydiswaran Dan Roth
University of Illinois at Urbana-Champaign
{mssammon|vgvinodv|danr}@illinois.edu
Abstract
We challenge the NLP community to par-
ticipate in a large-scale, distributed effort
to design and build resources for devel-
oping and evaluating solutions to new and
existing NLP tasks in the context of Rec-
ognizing Textual Entailment. We argue
that the single global label with which
RTE examples are annotated is insufficient
to effectively evaluate RTE system perfor-
mance; to promote research on smaller, re-
lated NLP tasks, we believe more detailed
annotation and evaluation are needed, and
that this effort will benefit not just RTE
researchers, but the NLP community as
a whole. We use insights from success-
ful RTE systems to propose a model for
identifying and annotating textual infer-
ence phenomena in textual entailment ex-
amples, and we present the results of a pi-
lot annotation study that show this model
is feasible and the results immediately use-
ful.
1 Introduction
Much of the work in the field of Natural Lan-
guage Processing is founded on an assumption
of semantic compositionality: that there are iden-
tifiable, separable components of an unspecified
inference process that will develop as research
in NLP progresses. Tasks such as Named En-
tity and coreference resolution, syntactic and shal-
low semantic parsing, and information and rela-
tion extraction have been identified as worthwhile
tasks and pursued by numerous researchers. While
many have (nearly) immediate application to real
world tasks like search, many are also motivated
by their potential contribution to more ambitious
Natural Language tasks. It is clear that the compo-
nents/tasks identified so far do not suffice in them-
selves to solve tasks requiring more complex rea-
soning and synthesis of information; many other
tasks must be solved to achieve human-like perfor-
mance on tasks such as Question Answering. But
there is no clear process for identifying potential
tasks (other than consensus by a sufficient num-
ber of researchers), nor for quantifying their po-
tential contribution to existing NLP tasks, let alne
to Natural Language Understanding.
Recent ?grand challenges? such as Learning by
Reading, Learning To Read, andMachine Reading
are prompting more careful thought about the way
these tasks relate, and what tasks must be solved
in order to understand text sufficiently well to re-
liably reason with it. This is an appropriate time
to consider a systematic process for identifying
semantic analysis tasks relevant to natural lan-
guage understanding, and for assessing their
potential impact on NLU system performance.
Research on Recognizing Textual Entailment
(RTE), largely motivated by a ?grand challenge?
now in its sixth year, has already begun to address
some of the problems identified above. Tech-
niques developed for RTE have now been suc-
cessfully applied in the domains of Question An-
swering (Harabagiu and Hickl, 2006) and Ma-
chine Translation (Pado et al, 2009), (Mirkin
et al, 2009). The RTE challenge examples are
drawn from multiple domains, providing a rel-
atively task-neutral setting in which to evaluate
contributions of different component solutions,
and RTE researchers have already made incremen-
tal progress by identifying sub-problems of entail-
ment, and developing ad-hoc solutions for them.
In this paper we challenge the NLP community
to contribute to a joint, long-term effort to iden-
tify, formalize, and solve textual inference prob-
lems motivated by the Recognizing Textual Entail-
ment setting, in the following ways:
(a) Making the Recognizing Textual Entailment
setting a central component of evaluation for
1199
relevant NLP tasks such as NER, Coreference,
parsing, data acquisition and application, and oth-
ers. While many ?component? tasks are consid-
ered (almost) solved in terms of expected improve-
ments in performance on task-specific corpora, it
is not clear that this translates to strong perfor-
mance in the RTE domain, due either to prob-
lems arising from unrelated, unsolved entailment
phenomena that co-occur in the same examples,
or to domain change effects. The RTE task of-
fers an application-driven setting for evaluating a
broad range of NLP solutions, and will reinforce
good practices by NLP researchers. The RTE
task has been designed specifically to exercise tex-
tual inference capabilities, in a format that would
make RTE systems potentially useful components
in other ?deep? NLP tasks such as Question An-
swering and Machine Translation. 1
(b) Identifying relevant linguistic phenomena,
interactions between phenomena, and their
likely impact on RTE/textual inference. Deter-
mining the correct label for a single textual en-
tailment example requires human analysts to make
many smaller, localized decisions which may de-
pend on each other. A broad, carefully conducted
effort to identify and annotate such local phenom-
ena in RTE corpora would allow their distributions
in RTE examples to be quantified, and allow eval-
uation of NLP solutions in the context of RTE. It
would also allow assessment of the potential im-
pact of a solution to a specific sub-problem on the
RTE task, and of interactions between phenomena.
Such phenomena will almost certainly correspond
to elements of linguistic theory; but this approach
brings a data-driven approach to focus attention on
those phenomena that are well-represented in the
RTE corpora, and which can be identified with suf-
ficiently close agreement.
(c) Developing resources and approaches that
allow more detailed assessment of RTE sys-
tems. At present, it is hard to know what spe-
cific capabilities different RTE systems have, and
hence, which aspects of successful systems are
worth emulating or reusing. An evaluation frame-
work that could offer insights into the kinds of
sub-problems a given system can reliably solve
would make it easier to identify significant ad-
vances, and thereby promote more rapid advances
1The Parser Training and Evaluation using Textual En-
tailment track of SemEval 2 takes this idea one step further,
by evaluating performance of an isolated NLP task using the
RTE methodology.
through reuse of successful solutions and focus on
unresolved problems.
In this paper we demonstrate that Textual En-
tailment systems are already ?interesting?, in that
they have made significant progress beyond a
?smart? lexical baseline that is surprisingly hard
to beat (section 2). We argue that Textual Entail-
ment, as an application that clearly requires so-
phisticated textual inference to perform well, re-
quires the solution of a range of sub-problems,
some familiar and some not yet known. We there-
fore propose RTE as a promising and worthwhile
task for large-scale community involvement, as it
motivates the study of many other NLP problems
in the context of general textual inference.
We outline the limitations of the present model
of evaluation of RTE performance, and identify
kinds of evaluation that would promote under-
standing of the way individual components can
impact Textual Entailment system performance,
and allow better objective evaluation of RTE sys-
tem behavior without imposing additional burdens
on RTE participants. We use this to motivate a
large-scale annotation effort to provide data with
the mark-up sufficient to support these goals.
To stimulate discussion of suitable annotation
and evaluation models, we propose a candidate
model, and provide results from a pilot annota-
tion effort (section 3). This pilot study establishes
the feasibility of an inference-motivated annota-
tion effort, and its results offer a quantitative in-
sight into the difficulty of the TE task, and the dis-
tribution of a number of entailment-relevant lin-
guistic phenomena over a representative sample
from the NIST TAC RTE 5 challenge corpus. We
argue that such an evaluation and annotation ef-
fort can identify relevant subproblems whose so-
lution will benefit not only Textual Entailment but
a range of other long-standing NLP tasks, and can
stimulate development of new ones. We also show
how this data can be used to investigate the behav-
ior of some of the highest-scoring RTE systems
from the most recent challenge (section 4).
2 NLP Insights from Textual Entailment
The task of Recognizing Textual Entailment
(RTE), as formulated by (Dagan et al, 2006), re-
quires automated systems to identify when a hu-
man reader would judge that given one span of text
(the Text) and some unspecified (but restricted)
world knowledge, a second span of text (the Hy-
1200
Text: The purchase of LexCorp by BMI for $2Bn
prompted widespread sell-offs by traders as they
sought to minimize exposure.
Hyp 1: BMI acquired another company.
Hyp 2: BMI bought LexCorp for $3.4Bn.
Figure 1: Some representative RTE examples.
pothesis) is true. The task was extended in (Gi-
ampiccolo et al, 2007) to include the additional
requirement that systems identify when the Hy-
pothesis contradicts the Text. In the example
shown in figure 1, this means recognizing that the
Text entails Hypothesis 1, while Hypothesis 2 con-
tradicts the Text. This operational definition of
Textual Entailment avoids commitment to any spe-
cific knowledge representation, inference method,
or learning approach, thus encouraging applica-
tion of a wide range of techniques to the problem.
2.1 An Illustrative Example
The simple RTE examples in figure 1 (most RTE
examples have much longer Texts) illustrate some
typical inference capabilities demonstrated by hu-
man readers in determining whether one span of
text contains the meaning of another.
To recognize that Hypothesis 1 is entailed by the
text, a human reader must recognize that ?another
company? in the Hypothesis can match ?Lex-
Corp?. She must also identify the nominalized
relation ?purchase?, and determine that ?A pur-
chased by B? implies ?B acquires A?.
To recognize that Hypothesis 2 contradicts the
Text, similar steps are required, together with the
inference that because the stated purchase price is
different in the Text and Hypothesis, but with high
probability refers to the same transaction, Hypoth-
esis 2 contradicts the Text.
It could be argued that this particular example
might be resolved by simple lexical matching; but
it should be evident that the Text can be made
lexically very dissimilar to Hypothesis 1 while
maintaining the Entailment relation, and that con-
versely, the lexical overlap between the Text and
Hypothesis 2 can be made very high, while main-
taining the Contradiction relation. This intuition
is borne out by the results of the RTE challenges,
which show that lexical similarity-based systems
are outperformed by systems that use other, more
structured analysis, as shown in the next section.
Rank System id Accuracy
1 I 0.735
2 E 0.685
3 H 0.670
4 J 0.667
5 G 0.662
6 B 0.638
7 D 0.633
8 F 0.632
9 A 0.615
9 C 0.615
9 K 0.615
- Lex 0.612
Table 1: Top performing systems in the RTE 5 2-
way task.
Lex E G H I J
Lex 1.000 0.667 0.693 0.678 0.660 0.778
(184,183) (157,132) (168,122) (152,136) (165,137) (165,135)
E 1.000 0.667 0.675 0.673 0.702
(224,187) (192,112) (178,131) (201,127) (186,131)
G 1.000 0.688 0.713 0.745
(247,150) (186,120) (218,115) (198,125)
H 1.000 0.705 0.707
(219,183) (194,139) (178,136)
I 1.000 0.705
(260,181) (198,135)
J 1.000
(224,178)
Table 2: In each cell, top row shows observed
agreement and bottom row shows the number of
correct (positive, negative) examples on which the
pair of systems agree.
2.2 The State of the Art in RTE 5
The outputs for all systems that participated in the
RTE 5 challenge were made available to partici-
pants. We compared these to each other and to
a smart lexical baseline (Do et al, 2010) (lexical
match augmented with a WordNet similarity mea-
sure, stemming, and a large set of low-semantic-
content stopwords) to assess the diversity of the
approaches of different research groups. To get
the fullest range of participants, we used results
from the two-way RTE task. We have anonymized
the system names.
Table 1 shows that many participating systems
significantly outperform our smart lexical base-
line. Table 2 reports the observed agreement be-
tween systems and the lexical baseline in terms of
the percentage of examples on which a pair of sys-
tems gave the same label. The agreement between
most systems and the baseline is about 67%, which
suggests that systems are not simply augmented
versions of the lexical baseline, and are also dis-
tinct from each other in their behaviors.2
Common characteristics of RTE systems re-
2Note that the expected agreement between two random
RTE decision-makers is 0.5, so the agreement scores accord-
ing to Cohen?s Kappa measure (Cohen, 1960) are between
0.3 and 0.4.
1201
ported by their designers were the use of struc-
tured representations of shallow semantic content
(such as augmented dependency parse trees and
semantic role labels); the application of NLP re-
sources such as Named Entity recognizers, syn-
tactic and dependency parsers, and coreference
resolvers; and the use of special-purpose ad-hoc
modules designed to address specific entailment
phenomena the researchers had identified, such as
the need for numeric reasoning. However, it is
not possible to objectively assess the role these ca-
pabilities play in each system?s performance from
the system outputs alone.
2.3 The Need for Detailed Evaluation
An ablation study that formed part of the of-
ficial RTE 5 evaluation attempted to evaluate
the contribution of publicly available knowledge
resources such as WordNet (Fellbaum, 1998),
VerbOcean (Chklovski and Pantel, 2004), and
DIRT (Lin and Pantel, 2001) used by many of
the systems. The observed contribution was in
most cases limited or non-existent. It is premature,
however, to conclude that these resources have lit-
tle potential impact on RTE system performance:
most RTE researchers agree that the real contribu-
tion of individual resources is difficult to assess.
As the example in figure 1 illustrates, most RTE
examples require a number of phenomena to be
correctly resolved in order to reliably determine
the correct label (the Interaction problem); a per-
fect coreference resolver might as a result yield lit-
tle improvement on the standard RTE evaluation,
even though coreference resolution is clearly re-
quired by human readers in a significant percent-
age of RTE examples.
Various efforts have been made by individ-
ual research teams to address specific capabili-
ties that are intuitively required for good RTE
performance, such as (de Marneffe et al, 2008),
and the formal treatment of entailment phenomena
in (MacCartney and Manning, 2009) depends on
and formalizes a divide-and-conquer approach to
entailment resolution. But the phenomena-specific
capabilities described in these approaches are far
from complete, and many are not yet invented. To
devote real effort to identify and develop such ca-
pabilities, researchers must be confident that the
resources (and the will!) exist to create and eval-
uate their solutions, and that the resource can be
shown to be relevant to a sufficiently large subset
of the NLP community. While there is widespread
belief that there are many relevant entailment phe-
nomena, though each individually may be rele-
vant to relatively few RTE examples (the Sparse-
ness problem), we know of no systematic analysis
to determine what those phenomena are, and how
sparsely represented they are in existing RTE data.
If it were even known what phenomena were
relevant to specific entailment examples, it might
be possible to more accurately distinguish system
capabilities, and promote adoption of successful
solutions to sub-problems. An annotation-side
solution also maintains the desirable agnosticism
of the RTE problem formulation, by not imposing
the requirement on system developers of generat-
ing an explanation for each answer. Of course, if
examples were also annotated with explanations
in a consistent format, this could form the basis of
a new evaluation of the kind essayed in the pilot
study in (Giampiccolo et al, 2007).
3 Annotation Proposal and Pilot Study
As part of our challenge to the NLP commu-
nity, we propose a distributed OntoNotes-style ap-
proach (Hovy et al, 2006) to this annotation ef-
fort: distributed, because it should be undertaken
by a diverse range of researchers with interests
in different semantic phenomena; and similar to
the OntoNotes annotation effort because it should
not presuppose a fixed, closed ontology of entail-
ment phenomena, but rather, iteratively hypoth-
esize and refine such an ontology using inter-
annotator agreement as a guiding principle. Such
an effort would require a steady output of RTE ex-
amples to form the underpinning of these annota-
tions; and in order to get sufficient data to repre-
sent less common, but nonetheless important, phe-
nomena, a large body of data is ultimately needed.
A research team interested in annotating a new
phenomenon should use examples drawn from the
common corpus. Aside from any task-specific
gold standard annotation they add to the entail-
ment pairs, they should augment existing explana-
tions by indicating in which examples their phe-
nomenon occurs, and at which point in the exist-
ing explanation for each example. In fact, this
latter effort ? identifying phenomena relevant to
textual inference, marking relevant RTE examples,
and generating explanations ? itself enables other
researchers to select from known problems, assess
their likely impact, and automatically generate rel-
1202
evant corpora.
To assess the feasibility of annotating RTE-
oriented local entailment phenomena, we devel-
oped an inference model that could be followed by
annotators, and conducted a pilot annotation study.
We based our initial effort on observations about
RTE data we made while participating in RTE
challenges, together with intuitive conceptions of
the kinds of knowledge that might be available in
semi-structured or structured form. In this sec-
tion, we present our annotation inference model,
and the results of our pilot annotation effort.
3.1 Inference Process
To identify and annotate RTE sub-phenomena in
RTE examples, we need a defensible model for the
entailment process that will lead to consistent an-
notation by different researchers, and to an exten-
sible framework that can accommodate new phe-
nomena as they are identified.
We modeled the entailment process as one of
manipulating the text and hypothesis to be as sim-
ilar as possible, by first identifying parts of the
text that matched parts of the hypothesis, and then
identifying connecting structure. Our inherent as-
sumption was that the meanings of the Text and
Hypothesis could be represented as sets of n-ary
relations, where relations could be connected to
other relations (i.e., could take other relations as
arguments). As we followed this procedure for a
given example, we marked which entailment phe-
nomena were required for the inference. We illus-
trate the process using the example in figure 1.
First, we would identify the arguments ?BMI?
and ?another company? in the Hypothesis as
matching ?BMI? and ?LexCorp? respectively, re-
quiring 1) Parent-Sibling to recognize that ?Lex-
Corp? can match ?company?. We would tag the
example as requiring 2) Nominalization Resolu-
tion to make ?purchase? the active relation and
3) Passivization to move ?BMI? to the subject po-
sition. We would then tag it with 4) Simple Verb
Rule to map ?A purchase B? to ?A acquire B?.
These operations make the relevant portion of the
Text identical to the Hypothesis, so we are done.
For the same Text, but with Hypothesis 2 (a neg-
ative example), we follow the same steps 1-3. We
would then use 4) Lexical Relation to map ?pur-
chase? to ?buy?. We would then observe that the
only possible match for the hypothesis argument
?for $3.4Bn? is the text argument ?for $2Bn?. We
would label this as a 5) Numerical Quantity Mis-
match and 6) Excluding Argument (it can?t be the
case that in the same transaction, the same com-
pany was sold for two different prices).
Note that neither explanation mentions
the anaphora resolution connecting ?they? to
?traders?, because it is not strictly required to
determine the entailment label.
As our example illustrates, this process makes
sense for both positive and negative examples. It
also reflects common approaches in RTE systems,
many of which have explicit alignment compo-
nents that map parts of the Hypothesis to parts of
the Text prior to a final decision stage.
3.2 Annotation Labels
We sought to identify roles for background knowl-
edge in terms of domains and general inference
steps, and the types of linguistic phenomena that
are involved in representing the same information
in different ways, or in detecting key differences
in two similar spans of text that indicate a differ-
ence in meaning. We annotated examples with do-
mains (such as ?Work?) for two reasons: to estab-
lish whether some phenomena are correlated with
particular domains; and to identify domains that
are sufficiently well-represented that a knowledge
engineering study might be possible.
While we did not generate an explicit repre-
sentation of our entailment process, i.e. explana-
tions, we tracked which phenomena were strictly
required for inference. The annotated corpora and
simple CGI scripts for annotation are available at
http://cogcomp.cs.illinois.edu/Data/ACL2010 RTE.php.
The phenomena that we considered during an-
notation are presented in Tables 3, 4, 5, and 6. We
tried to define each phenomenon so that it would
apply to both positive and negative examples, but
ran into a problem: often, negative examples can
be identified principally by structural differences:
the components of the Hypothesis all match com-
ponents in the Text, but they are not connected
by the appropriate structure in the Text. In the
case of contradictions, it is often the case that a
key relation in the Hypothesis must be matched to
an incompatible relation in the Text. We selected
names for these structural behaviors, and tagged
them when we observed them, but the counterpart
for positive examples must always hold: it must
necessarily be the case that the structure in the
Text linking the arguments that match those in the
1203
Hypothesis must be comparable to the Hypothesis
structure. We therefore did not tag this for positive
examples.
We selected a subset of 210 examples from the
NIST TAC RTE 5 (Bentivogli et al, 2009) Test
set drawn equally from the three sub-tasks (IE, IR
and QA). Each example was tagged by both an-
notators. Two passes were made over the data: the
first covered 50 examples from each RTE sub-task,
while the second covered an additional 20 exam-
ples from each sub-task. Between the two passes,
concepts the annotators identified as difficult to
annotate were discussed and more carefully spec-
ified, and several new concepts were introduced
based on annotator observations.
Tables 3, 4, 5, and 6 present information
about the distribution of the phenomena we
tagged, and the inter-annotator agreement (Co-
hen?s Kappa (Cohen, 1960)) for each. ?Occur-
rence? lists the average percentage of examples la-
beled with a phenomenon by the two annotators.
Domain Occurrence Agreement
work 16.90% 0.918
name 12.38% 0.833
die kill injure 12.14% 0.979
group 9.52% 0.794
be in 8.57% 0.888
kinship 7.14% 1.000
create 6.19% 1.000
cause 6.19% 0.854
come from 5.48% 0.879
win compete 3.10% 0.813
Others 29.52% 0.864
Table 3: Occurrence statistics for domains in the
annotated data.
Phenomenon Occurrence Agreement
Named Entity 91.67% 0.856
locative 17.62% 0.623
Numerical Quantity 14.05% 0.905
temporal 5.48% 0.960
nominalization 4.05% 0.245
implicit relation 1.90% 0.651
Table 4: Occurrence statistics for hypothesis struc-
ture features.
From the tables it is apparent that good perfor-
mance on a range of phenomena in our inference
model are likely to have a significant effect on
RTE results, with coreference being deemed es-
sential to the inference process for 35% of exam-
ples, and a number of other phenomena are suffi-
ciently well represented to merit near-future atten-
tion (assuming that RTE systems do not already
handle these phenomena, a question we address in
section 4). It is also clear from the predominance
of Simple Rewrite Rule instances, together with
Phenomenon Occurrence Agreement
coreference 35.00% 0.698
simple rewrite rule 32.62% 0.580
lexical relation 25.00% 0.738
implicit relation 23.33% 0.633
factoid 15.00% 0.412
parent-sibling 11.67% 0.500
genetive relation 9.29% 0.608
nominalization 8.33% 0.514
event chain 6.67% 0.589
coerced relation 6.43% 0.540
passive-active 5.24% 0.583
numeric reasoning 4.05% 0.847
spatial reasoning 3.57% 0.720
Table 5: Occurrence statistics for entailment phe-
nomena and knowledge resources
Phenomenon Occurrence Agreement
missing argument 16.19% 0.763
missing relation 14.76% 0.708
excluding argument 10.48% 0.952
Named Entity mismatch 9.29% 0.921
excluding relation 5.00% 0.870
disconnected relation 4.52% 0.580
missing modifier 3.81% 0.465
disconnected argument 3.33% 0.764
Numeric Quant. mismatch 3.33% 0.882
Table 6: Occurrences of negative-only phenomena
the frequency of most of the domains we selected,
that knowledge engineering efforts also have a key
role in improving RTE performance.
3.3 Discussion
Perhaps surprisingly, given the difficulty of the
task, inter-annotator agreement was consistently
good to excellent (above 0.6 and 0.8, respec-
tively), with few exceptions, indicating that for
most targeted phenomena, the concepts were well-
specified. The results confirmed our initial intu-
ition about some phenomena: for example, that
coreference resolution is central to RTE, and that
detecting the connecting structure is crucial in dis-
cerning negative from positive examples. We also
found strong evidence that the difference between
contradiction and unknown entailment examples
is often due to the behavior of certain relations that
either preclude certain other relations holding be-
tween the same arguments (for example, winning
a contest vs. losing a contest), or which can only
hold for a single referent in one argument position
(for example, ?work? relations such as job title are
typically constrained so that a single person holds
one position).
We found that for some examples, there was
more than one way to infer the hypothesis from the
text. Typically, for positive examples this involved
overlap between phenomena; for example, Coref-
erence might be expected to resolve implicit rela-
1204
tions induced from appositive structures. In such
cases we annotated every way we could find.
In future efforts, annotators should record the
entailment steps they used to reach their decision.
This will make disagreement resolution simpler,
and could also form a possible basis for generating
gold standard explanations. At a minimum, each
inference step must identify the spans of the Text
and Hypothesis that are involved and the name of
the entailment phenomenon represented; in addi-
tion, a partial order over steps must be specified
when one inference step requires that another has
been completed.
Future annotation efforts should also add a
category ?Other?, to indicate for each example
whether the annotator considers the listed entail-
ment phenomena sufficient to identify the label. It
might also be useful to assess the difficulty of each
example based on the time required by the anno-
tator to determine an explanation, for comparison
with RTE system errors.
These, together with specifications that mini-
mize the likely disagreements between different
groups of annotators, are processes that must be
refined as part of the broad community effort we
seek to stimulate.
4 Pilot RTE System Analysis
In this section, we sketch out ways in which
the proposed analysis can be applied to learn
something about RTE system behavior, even
when those systems do not provide anything
beyond the output label. We present the analysis
in terms of sample questions we hope to answer
with such an analysis.
1. If a system needs to improve its performance,
which features should it concentrate on? To an-
swer this question, we looked at the top-5 systems
and tried to find which phenomena are active in
the mistakes they make.
(a) Most systems seem to fail on examples that
need numeric reasoning to get the entailment de-
cision right. For example, system H got all 10 ex-
amples with numeric reasoning wrong.
(b) All top-5 systems make consistent errors in
cases where identifying a mismatch in named en-
tities (NE) or numerical quantities (NQ) is impor-
tant to make the right decision. System G got 69%
of cases with NE/NQ mismatches wrong.
(c) Most systems make errors in examples that
have a disconnected or exclusion component (ar-
gument/relation). System J got 81% of cases with
a disconnected component wrong.
(d) Some phenomena are handled well by certain
systems, but not by others. For example, failing
to recognize a parent-sibling relation between
entities/concepts seems to be one of the top-5
phenomena active in systems E and H. System
H also fails to correctly label over 53% of the
examples having kinship relation.
2. Which phenomena have strong correlations
to the entailment labels among hard examples?
We called an example hard if at least 4 of the top 5
systems got the example wrong. In our annotation
dataset, there were 41 hard examples. Some of
the phenomena that strongly correlate with the
TE labels on hard examples are: deeper lexical
relation between words (? = 0.542), and need
for external knowledge (? = 0.345). Further, we
find that the top-5 systems tend to make mistakes
in cases where the lexical approach also makes
mistakes (? = 0.355).
3. What more can be said about individual
systems? In order to better understand the system
behavior, we wanted to check if we could predict
the system behavior based on the phenomena
we identified as important in the examples.
We learned SVM classifiers over the identified
phenomena and the lexical similarity score to
predict both the labels and errors systems make
for each of the top-5 systems. We could predict all
10 system behaviors with over 70% accuracy, and
could predict labels and mistakes made by two of
the top-5 systems with over 77% accuracy. This
indicates that although the identified phenomena
are indicative of the system performance, it is
probably too simplistic to assume that system
behavior can be easily reproduced solely as a
disjunction of phenomena present in the examples.
4. Does identifying the phenomena correctly
help learn a better TE system? We tried to
learn an entailment classifier over the phenomenon
identified and the top 5 system outputs. The results
are summarized in Table 7. All reported num-
bers are 20-fold cross-validation accuracy from
an SVM classifier learned over the features men-
tioned. The results show that correctly identify-
ing the named-entity and numeric quantity mis-
1205
No. Feature description No. of Accuracy over which features
feats phenomena pheno. + sys. labels
(0) Only system labels 5 ? 0.714
(1) Domain and hypothesis features (Tables 3, 4) 16 0.510 0.705
(2) (1) + NE + NQ 18 0.619 0.762
(3) (1) + Knowledge resources (subset of Table 5) 22 0.662 0.762
(4) (3) + NE + NQ 24 0.738 0.805
(5) (1) + Entailment and Knowledge resources (Table 5) 29 0.748 0.791
(6) (5) + negative-only phenomena (Table 6) 38 0.971 0.943
Table 7: Accuracy in predicting the label based on the phenomena and top-5 system labels.
matches improves the overall accuracy signifi-
cantly. If we further recognize the need for knowl-
edge resources correctly, we can correctly explain
the label for 80% of the examples. Adding the
entailment and negation features helps us explain
the label for 97% of the examples in the annotated
corpus.
It must be clarified that the results do not show
the textual entailment problem itself is solved with
97% accuracy. However, we believe that if a
system could recognize key negation phenomena
such as Named Entity mismatch, presence of Ex-
cluding arguments, etc. correctly and consistently,
it could model them as a Contradiction features
in the final inference process to significantly im-
prove its overall accuracy. Similarly, identifying
and resolving the key entailment phenomena in
the examples, would boost the inference process
in positive examples. However, significant effort
is still required to obtain near-accurate knowledge
and linguistic resources.
5 Discussion
NLP researchers in the broader community contin-
ually seek new problems to solve, and pose more
ambitious tasks to develop NLP and NLU capabil-
ities, yet recognize that even solutions to problems
which are considered ?solved? may not perform as
well on domains different from the resources used
to train and develop them. Solutions to such NLP
tasks could benefit from evaluation and further de-
velopment on corpora drawn from a range of do-
mains, like those used in RTE evaluations.
It is also worthwhile to consider each task as
part of a larger inference process, and therefore
motivated not just by performance statistics on
special-purpose corpora, but as part of an inter-
connected web of resources; and the task of Rec-
ognizing Textual Entailment has been designed to
exercise a wide range of linguistic and reasoning
capabilities.
The entailment setting introduces a potentially
broader context to resource development and as-
sessment, as the hypothesis and text provide con-
text for each other in a way different than local
context from, say, the same paragraph in a docu-
ment: in RTE?s positive examples, the Hypothe-
sis either restates some part of the Text, or makes
statements inferable from the statements in the
Text. This is not generally true of neighboring sen-
tences in a document. This distinction opens the
door to ?purposeful?, or goal-directed, inference
in a way that may not be relevant to a task studied
in isolation.
The RTE community seems mainly convinced
that incremental advances in local entailment phe-
nomena (including application of world knowl-
edge) are needed to make significant progress.
They need ways to identify sub-problems of tex-
tual inference, and to evaluate those solutions both
in isolation and in the context of RTE. RTE system
developers are likely to reward well-engineered
solutions by adopting them and citing their au-
thors, because such solutions are easier to incor-
porate into RTE systems. They are also more
likely to adopt solutions with established perfor-
mance levels. These characteristics promote pub-
lication of software developed to solve NLP tasks,
attention to its usability, and publication of mate-
rials supporting reproduction of results presented
in technical papers.
For these reasons, we assert that RTE is a nat-
ural motivator of new NLP tasks, as researchers
look for components capable of improving perfor-
mance; and that RTE is a natural setting for evalu-
ating solutions to a broad range of NLP problems,
though not in its present formulation: we must
solve the problem of credit assignment, to recog-
nize component contributions. We have therefore
proposed a suitable annotation effort, to provide
the resources necessary for more detailed evalua-
tion of RTE systems.
We have presented a linguistically-motivated
1206
analysis of entailment data based on a step-wise
procedure to resolve entailment decisions, in-
tended to allow independent annotators to reach
consistent decisions, and conducted a pilot anno-
tation effort to assess the feasibility of such a task.
We do not claim that our set of domains or phe-
nomena are complete: for example, our illustra-
tive example could be tagged with a domain Merg-
ers and Acquisitions, and a different team of re-
searchers might consider Nominalization Resolu-
tion to be a subset of Simple Verb Rules. This kind
of disagreement in coverage is inevitable, but we
believe that in many cases it suffices to introduce
a new domain or phenomenon, and indicate its re-
lation (if any) to existing domains or phenomena.
In the case of introducing a non-overlapping cate-
gory, no additional information is needed. In other
cases, the annotators can simply indicate the phe-
nomena being merged or split (or even replaced).
This information will allow other researchers to
integrate different annotation sources and main-
tain a consistent set of annotations.
6 Conclusions
In this paper, we have presented a case for a broad,
long-term effort by the NLP community to coordi-
nate annotation efforts around RTE corpora, and to
evaluate solutions to NLP tasks relating to textual
inference in the context of RTE. We have iden-
tified limitations in the existing RTE evaluation
scheme, proposed a more detailed evaluation to
address these limitations, and sketched a process
for generating this annotation. We have proposed
an initial annotation scheme to prompt discussion,
and through a pilot study, demonstrated that such
annotation is both feasible and useful.
We ask that researchers not only contribute
task specific annotation to the general pool, and
indicate how their task relates to those already
added to the annotated RTE corpora, but also in-
vest the additional effort required to augment the
cross-domain annotation: marking the examples
in which their phenomenon occurs, and augment-
ing the annotator-generated explanations with the
relevant inference steps.
These efforts will allow a more meaningful
evaluation of RTE systems, and of the compo-
nent NLP technologies they depend on. We see
the potential for great synergy between different
NLP subfields, and believe that all parties stand to
gain from this collaborative effort. We therefore
respectfully suggest that you ?ask not what RTE
can do for you, but what you can do for RTE...?
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This research was
partly sponsored by Air Force Research Labora-
tory (AFRL) under prime contract no. FA8750-
09-C-0181, by a grant from Boeing and by MIAS,
the Multimodal Information Access and Synthesis
center at UIUC, part of CCICADA, a DHS Center
of Excellence. Any opinions, findings, and con-
clusion or recommendations expressed in this ma-
terial are those of the author(s) and do not neces-
sarily reflect the view of the sponsors.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The
fifth pascal recognizing textual entailment chal-
lenge. In Notebook papers and Results, Text Analy-
sis Conference (TAC), pages 14?24.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04), pages 33?40.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL-08: HLT, pages
1039?1047, Columbus, Ohio, June. Association for
Computational Linguistics.
Quang Do, Dan Roth, Mark Sammons, Yuancheng
Tu, and V.G.Vinod Vydiswaran. 2010. Robust,
Light-weight Approaches to compute Lexi-
cal Similarity. Computer Science Research
and Technical Reports, University of Illinois.
http://L2R.cs.uiuc.edu/?danr/Papers/DRSTV10.pdf.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 1?9, Prague, June. Association
for Computational Linguistics.
1207
Sanda Harabagiu and Andrew Hickl. 2006. Meth-
ods for Using Textual Entailment in Open-Domain
Question Answering. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 905?912, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT/NAACL,
New York.
D. Lin and P. Pantel. 2001. DIRT: discovery of in-
ference rules from text. In Proc. of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2001, pages 323?328.
Bill MacCartney and Christopher D. Manning. 2009.
An extended model of natural logic. In The Eighth
International Conference on Computational Seman-
tics (IWCS-8), Tilburg, Netherlands.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In ACL/AFNLP, pages 791?
799, Suntec, Singapore, August. Association for
Computational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 297?305, Suntec, Singapore,
August. Association for Computational Linguistics.
1208
