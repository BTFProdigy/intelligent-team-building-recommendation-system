Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1146?1154,
Beijing, August 2010
?Got You!?: Automatic Vandalism Detection in Wikipedia                                                                                         
with Web-based Shallow Syntactic-Semantic Modeling 
                                William Yang Wang and Kathleen R. McKeown 
Department of Computer Science  
Columbia University 
yw2347@columbia.edu kathy@cs.columbia.edu 
Abstract 
Discriminating vandalism edits from 
non-vandalism edits in Wikipedia is a 
challenging task, as ill-intentioned edits 
can include a variety of content and be 
expressed in many different forms and 
styles. Previous studies are limited to 
rule-based methods and learning based 
on lexical features, lacking in linguistic 
analysis. In this paper, we propose a 
novel Web-based shallow syntactic-
semantic modeling method, which utiliz-
es Web search results as resource and 
trains topic-specific n-tag and syntactic 
n-gram language models to detect van-
dalism. By combining basic task-specific 
and lexical features, we have achieved 
high F-measures using logistic boosting 
and logistic model trees classifiers, sur-
passing the results reported by major 
Wikipedia vandalism detection systems.  
1 Introduction 
Online open collaboration systems are becoming 
a major means of information sharing on the 
Web. With millions of articles from millions of 
resources edited by millions of people, Wikipe-
dia is a pioneer in the fast growing, online know-
ledge collaboration era. Anyone who has Inter-
net access can visit, edit and delete Wikipedia 
articles without authentication. 
A primary threat to this convenience, however, 
is vandalism, which has become one of Wikipe-
dia?s biggest concerns (Geiger, 2010). To date, 
automatic countermeasures mainly involve rule-
based approaches and these are not very effec-
tive. Therefore, Wikipedia volunteers have to 
spend a large amount of time identifying vanda-
lized articles manually, rather than spending 
time contributing content to the articles. Hence, 
there is a need for more effective approaches to 
automatic vandalism detection. 
In contrast to spam detection tasks, where a 
full spam message, which is typically 4K Bytes 
(Rigoutsos and Huynh, 2004), can be sampled 
and analyzed (Itakura and Clarke, 2009), Wiki-
pedia vandals typically change only a small 
number of words or sentences in the targeted 
article. In our preliminary corpus (Potthast et al, 
2007), we find the average size of 201 vanda-
lized texts to be only 1K Byte. This leaves very 
few clues for vandalism modeling. The question 
we address in this paper is: given such limited 
information, how can we better understand and 
model Wikipedia vandalism? 
Our proposed approach establishes a novel 
classification framework, aiming at capturing 
vandalism through an emphasis on shallow syn-
tactic and semantic modeling. In contrast to pre-
vious work, we recognize the significance of 
natural language modeling techniques for Wiki-
pedia vandalism detection and utilize Web 
search results to construct our shallow syntactic 
and semantic models. We first construct a base-
line model that captures task-specific clues and 
lexical features that have been used in earlier 
work (Potthast et al, 2008; Smets et al, 2008) 
augmenting these with shallow syntactic and 
semantic features.  Our main contributions are: 
? Improvement over previous modeling me-
thods with three novel lexical features 
? Using Web search results as training data 
for syntactic and semantic modeling 
? Building topic-specific n-tag syntax models 
and syntactic n-gram models for shallow 
syntactic and semantic modeling 
1146
2 Related Work 
So far, the primary method for automatic 
vandalism detection in Wikipedia relies on rule-
based bots. In recent years, however, with the 
rise of statistical machine learning, researchers 
have begun to treat Wikipedia vandalism 
detection task as a classification task. To the best 
of our knowledge, we are among the first to 
consider the shallow syntactic and semantic 
modeling using Natural Language Processing 
(NLP) techniques, utilizing the Web as corpus to 
detect vandalism. 
ClueBot (Carter, 2007) is one of the most ac-
tive bots fighting vandalism in Wikipedia. It 
keeps track of the IP of blocked users and uses 
simple regular expressions to keep Wikipedia 
vandalism free. A distinct advantage of rule-
based bots is that they have very high precision. 
However they suffer from fixed-size knowledge 
bases and use only rigid rules. Therefore, their 
average recall is not very high and they can be 
easily fooled by unseen vandalism patterns. Ac-
cording to Smets et al, (2008) and Potthast et al, 
(2008), rule-based bots have a perfect precision 
of 1 and a recall of around 0.3. 
The Wikipedia vandalism detection research 
community began to concentrate on the machine 
learning approaches in the past two years. Smets 
et al (2008) wrapped all the content in diff text 
into a bag of words, disregarding grammar and 
word order. They used Na?ve Bayes as the 
classification algorithm. Compared to rule-based 
methods, they show an average precision of 0.59 
but are able to reach a recall of 0.37. Though 
they are among the first to try machine learning 
approaches, the features in their study are the 
most straightforward set of features. Clearly, 
there is still room for improvement. 
More recently, Itakura and Clarke (2009) have 
proposed a novel method using Dynamic Mar-
kov Compression (DMC). They model their ap-
proach after the successful use of DMC in Web 
and Mail Spam detection (Bratko et al, 2006). 
The reported average precision is 0.75 and ave- 
rage recall is 0.73.  
To the best of our knowledge, Potthast et al, 
(2008) report the best result so far for Wikipedia 
vandalism detection. They craft a feature set that 
consists of interesting task-specific features. For 
example, they monitor the number of previously 
submitted edits from the same author or IP, 
which is a good feature to model author contri-
bution. Their other contributions are the use of a 
logistic regression classifier, as well as the use 
of lexical features. They successfully demon-
strate the use of lexical features like vulgarism 
frequency.  Using all features, they reach an av-
erage precision of 0.83 and recall of 0.77.  
In addition to previous work on vandalism de-
tection, there is also earlier work using the web 
for modeling. Biadsy et al (2008) extract pat-
terns in Wikipedia to generate biographies au-
tomatically. In their experiment, they show that 
when using Wikipedia as the only resource for 
extracting named entities and corresponding col-
locational patterns, although the precision is typ-
ically high, recall can be very low. For that rea-
son, they choose to use Google to retrieve train-
ing data from the Web. In our approach, instead 
of using Wikipedia edits and historical revisions, 
we also select the Web as a resource to train our 
shallow syntactic and semantic models. 
3 Analysis of  Types of Vandalism 
In order to better understand the characteristics 
of vandalism cases in Wikipedia, we manually 
analyzed 201 vandalism edits in the training set 
of our preliminary corpus.  In order to concen-
trate on textual vandalism detection, we did not 
take into account the cases where vandals hack 
the image, audio or other multimedia resources 
contained in the Wikipedia edit. 
We found three main types of vandalism, 
which are shown in Table 1 along with corres-
ponding examples. These examples contain both 
the title of the edit and a snippet of the diff-ed 
content of vandalism, which is the textual differ-
ence between the old revision and the new revi-
sion, derived through the standard diff algorithm 
(Heckel, 1978). 
? Lexically ill-formed 
This is the most common type of vandal-
ism in Wikipedia. Like other online van-
dalism acts, many vandalism cases in 
Wikipedia involve ill-intentioned or ill-
formed words such as vulgarisms, invalid 
letter sequences, punctuation misuse and 
Web slang. An interesting observation is 
that vandals almost never add emoticons 
in Wikipedia. For the first example in  
1147
Table 1: Vandalism Types and Examples 
Table 1, vulgarism and punctuation mi-
suse are observed. 
? Syntactically ill-formed 
Most vandalism cases that are lexically 
ill-intentioned tend to be syntactically ill-
formed as well. It is not easy to capture 
these cases by solely relying on lexical 
knowledge or rule-based dictionaries and 
it is also very expensive to update dictio-
naries and rules manually. Therefore, we 
think that is crucial to incorporate more 
syntactic cues in the feature set in order to 
improve performance. Moreover, there are 
also some cases where an edit could be 
lexically well-intentioned, yet syntactical-
ly ill-formed. The first example of syntac-
tic ill-formed in Table 1 is of this kind. 
     Table 2: Feature Sets and Corresponding          
Features of Our Vandalism Detection System 
? Lexically and syntactically well 
formed, but semantically ill-
intentioned 
This is the trickiest type of vandalism to 
identify. Vandals of this kind might have 
good knowledge of the rule-based vandal-
ism detecting bots. Usually, this type of 
vandalism involves off-topic comments, 
inserted biased opinions, unconfirmed in-
formation and lobbying using very subjec-
tive comments. However, a common cha-
racteristic of all vandalism in this category 
is that it is free of both lexical and syntac-
tic errors. Consider the first example of 
semantic vandalism in Table 1 with edit 
title ?Global Warming?: while the first 
sentence for that edit seems to be fairly 
normal (the author tries to claim another 
explanation of the global warming effect), 
the second sentence makes a sudden tran-
sition from the previous topic to mention 
a basketball star and makes a ridiculous 
conclusion in the last sentence.  
In this work, we realize the importance of in-
corporating NLP techniques to tackle all the 
above types of vandalism, and our focus is on 
the syntactically ill-formed and semantically ill-
intentioned types that could not be detected by 
rule-based systems and straightforward lexical 
features.  
Vandalism 
Types 
Examples 
Lexically 
ill-formed 
Edit Title:  IPod 
shit!!!!!!!!!!!!!!!!!!!!!! 
 
 
Syntactically 
ill-formed 
Edit Title: Rock music 
DOWN WITH SOCIETY 
MADDISON STREET RIOT 
FOREVER. 
Edit Title: Vietnam War 
Crabinarah sucks dont buy it 
 
 
 
 
 
 
Lexically + 
syntactically  
well-formed, 
semantically  
ill-intentioned 
Edit Title: Global Warming 
Another popular theory in-
volving global warming is 
the concept that global 
warming is not caused by 
greenhouse gases. The theory 
is that Carlos Boozer is the 
one preventing the infrared 
heat from escaping the at-
mosphere. Therefore, the 
Golden State Warriors will 
win next season. 
Edit Title: Harry Potter 
Harry Potter is a teenage 
boy who likes to smoke 
crack with his buds. They 
also run an illegal smuggling 
business to their headmaster 
dumbledore. He is dumb! 
 
Feature 
Sets 
Features 
Task-
specific 
Number of Revisions; 
Revisions Size Ratio; 
Lexical Vulgarism; Web Slang;  
Punctuation Misuse; 
Comment Cue Words; 
Syntactic Normalized Topic-specific N-tag 
Log Likelihood and Perplexity  
Semantic Normalized Topic-specific  
Syntactic N-gram Log  
Likelihood and Perplexity 
 
1148
4 Our System 
We propose a shallow syntactic-semantic fo-
cused classification approach for vandalism de-
tection (Table 2). In contrast to previous work, 
our approach concentrates on the aspect of using 
natural language techniques to model vandalism. 
Our shallow syntactic and semantic modeling 
approaches extend the traditional n-gram lan-
guage modeling method with topic-specific n-
tag (Collins et al, 2005) syntax models and top-
ic-specific syntactic n-gram semantic models. 
Moreover, in the Wikipedia vandalism detection 
task, since we do not have a sufficient amount of 
training data to model the topic of each edit, we 
propose the idea of using the Web as corpus by 
retrieving search engine results to learn our top-
ic-specific n-tag syntax and syntactic n-gram 
semantic models. The difference between our 
syntactic and semantic modeling is that n-tag 
syntax models only model the order of sentence 
constituents, disregarding the corresponding 
words. Conversely, for our syntactic n-gram 
models, we do keep track of words together with 
their POS tags and model both the word and 
syntactic compositions as a sequence. The detail 
of our shallow syntactic-semantic modeling me-
thod will be described in subsection 4.4. 
We use our shallow syntactic-semantic model 
to augment our base model, which builds on ear-
ly work. For example, when building one of our 
task-specific features, we extract the name of the 
author of this revision to query Wikipedia about 
the historical behavior of this author. This kind 
of task-specific global feature tends to be very 
informative and thus forms the basis of our sys-
tem. For lexical level features, we count vulgar-
ism frequencies and also introduce three new 
lexical features: Web slang, punctuation misuse 
and comment cue words, all of which will be 
described in detail in 4.2 and 4.3.  
4.1 Problem Representation 
The vandalism detection task can be formu-
lated as the following problem. Let?s assume we 
have a vandalism corpus C, which contains a set 
of Wikipedia edits S. A Wikipedia edit is de-
noted as ei. In our case, we have S = {e1, e2?,en}. 
Each edit e has two consecutive revisions (an old 
revision Rold and a new revision Rnew) that are 
unique in the entire data set. We write that e = 
{Rold, Rnew}. With the use of the standard diff 
algorithm, we can produce a text Rdiff, showing 
the difference between these two revisions, so 
that e = {Rold, Rnew, Rdiff }.  Our task is: given S, 
to extract features from edit e ?S and train a 
logistic boosting classifier. On receiving an edit 
e from the test set, the classifier needs to decide 
whether this e is a vandalism edit or a non-
vandalism edit. e?{1,0}.  
4.2 Basic Task-specific and Lexical Fea-
tures  
Task-specific features are domain-dependent and 
are therefore unique in this Wikipedia vandalism 
detection task. In this work, we pick two task-
specific features and one lexical feature that 
proved effective in previous studies. 
? Number of Revisions 
This is a very simple but effective feature 
that is used by many studies (Wilkinson 
and Huberman, 2007; Adler et al, 2008; 
Stein and Hess, 2007). By extracting the 
author name for the new revision Rnew, we 
can easily query Wikipedia and count how 
many revisions the author has modified in 
the history. 
? Revision Size Ratio 
Revision size ratio measures the size of 
the new revision versus the size of the old 
revision in an edit. This measure is an in-
dication of how much information is 
gained or lost in the new revision Rnew, 
compared to the old revision Rold, and can 
be expressed as: 
   RevRatio(?)  =  
 Count (w)w  ? R  new
  Count (w)w  ? R  old
 
 
where W represents any word token of a 
revision. 
? Vulgarism Frequency 
Revision size ratio measures the size of 
the new revision versus the Vulgarism 
frequency was first introduced by Potthast 
et al (2008). However, note that not all 
vulgarism words should be considered as 
vandalism and sometime even the Wiki-
pedia edit?s title and content themselves 
contain vulgarism words.  
1149
 
For each diff text in an edit e, we count 
the total number of appearances of vulgar-
ism words v where v is in our vulgarism 
dictionary1. 
VulFreq ? =  Count(?)
??Rdiff
 
4.3 Novel Lexical Features 
In addition to previous lexical features, we pro-
pose three novel lexical features in this paper: 
Web slang frequency, punctuation misuse, and 
comment cue words frequency.  
? Web Slang and Punctuation Misuse  
Since Wikipedia is an open Web applica-
tion, vandalism also contains a fair 
amount of Web slang, such as, ?haha?, 
?LOL? and ?OMG?. We use the same me-
thod as above to calculate Web slang fre-
quency, using a Web slang dictionary2. In 
vandalism edits, many vandalism edits al- 
                                                 
1 http://www.noswearing.com/dictionary 
2 http://www.noslang.com/dictionary/full 
so contain punctuation misuse, for exam-
ple, ?!!!? and ?????. However, we have 
not observed a significant amount of emo-
ticons in the vandalism edits. Based on 
this, we only keep track of Web slang fre-
quency and the occurrence of punctuation 
misuse. 
? Comment Cue Words 
Upon committing each new revision in 
Wikipedia, the author is required to enter 
some comments describing the change. 
Well-intentioned Wikipedia contributors 
consistently use these comments to ex-
plain the motivation for their changes. For 
example, common non-vandalism edits 
may contain cue words and phrases like 
?edit revised, page changed, item cleaned 
up, link repaired or delinked?. In contrast, 
vandals almost never take their time to 
add these kinds of comments. We can 
measure this phenomenon by counting the 
frequency of comment cue words.  
1150
4.4 Topic-specific N-tag Syntax Models and 
Syntactic N-grams for Shallow Syntac-
tic and Semantic Modeling 
In Figure 1, we present the overview of our ap-
proach, which uses Web-trained topic-specific 
training for both: (1) n-tag syntax models for 
shallow syntactic modeling and (2) syntactic n-
gram models for shallow semantic modeling.  
For each Wikipedia edit, we consider its title 
as an approximate semantic representation, using 
it as a query to build topic-specific models.  In 
addition, we also use the title information to 
model the syntax of this topic.  
Given Rdiff, we produce the syntactic version 
of the diff-ed text using a probabilistic POS tag-
ger (Toutanova and Manning, 2000; Toutanova 
et al, 2003). The edit title is extracted from the 
corpus (either Rnew or Rold) and is used to query 
multiple Web search engines in order to collect 
the n-tag and n-gram training data from the top-k 
results. Before we start training language models, 
we tag the top-k results using the POS tagger. 
Note that when modeling n-tag syntax models, it 
is necessary to remove all the words. With the 
POS-only sequences, we train topic-specific n-
tag models to describe the syntax of normal text 
on the same topic associated with this edit. With 
the original tagged sequences, we train syntactic 
n-gram models to represent the semantics of the 
normal text of this edit. 
After completing the training stage, we send 
the test segment (i.e. the diff-ed text sequence) to 
both the learned n-tag syntax models and the 
learned syntactic n-gram models. For the n-tag 
syntax model, we submit the POS tag-only ver-
sion of the segment. For the syntactic n-gram 
model, we submit a version of the segment 
where each original word is associated with its 
POS-tag. In both cases we compute the log-
likelihood and the perplexity of the segment.  
Finally, we normalize the log likelihood and 
perplexity scores by dividing them by the length 
of Rdiff, as this length varies substantially from 
one edit to another. 3 We expect an edit that has 
low log likelihood probability and perplexity to 
be vandalism, and it is very likely to be unre-
lated to the syntax and semantic of the normal 
text of this Wikipedia edit. In the end, the nor-
malized log probability and perplexity scores 
will be incorporated into our back-end classifier 
with all task-specific and lexical features. 
Web as Corpus: In this work, we leverage 
Web search results to train the syntax and se-
mantic models. This is based on the assumption 
that the Web itself is a large corpus and Web 
search results can be a good training set to ap-
proximate the semantics and syntax of the query.    
Topic-specific Modeling: We introduce a 
topic-specific modeling method that treats every 
edit in Wikipedia as a unique topic. We think 
that the title of each Wikipedia edit is an approx-
imation of the topic of the edit, so we extract the 
title of each edit and use it as keywords to re-
trieve training data for our shallow syntactic and 
semantic modeling. 
Topic-specific N-tag and Syntactic N-gram: 
In our novel approach, we tag all the top-k query 
results and diff text with a probabilistic POS tag-
ger in both the training and test set of the vandal-
ism corpus. Figure 2(a) is an example of a POS-
tagged sequence in a top-k query result.  
For shallow syntactic modeling, we use an n-
tag modeling method (Collins et al, 2005). Giv-
en a tagged sequence, we remove all the words 
and only keep track of its POS tags: tagi-2 tagi-1 
                                                 
3 Although we have experimented with using the 
length of Rdiff as a potential feature, it does not appear 
to be a good indicator of vandalism. 
(a) 
Rock/NNP and/CC roll/NN -LRB-/-LRB- 
also/RB spelled/VBD Rock/NNP 'n'/CC 
Roll/NNP 
(b) 
NNP CC NN -LRB- RB VBD NNP CC 
NNP 
(c) 
Rock/NNP !/. !/. !/. and/CC roll/VB 
you/PRP !/. !/. !/. 
(d) 
NNP . . . CC VB PRP . . . 
 
Figure 2. Topic-specific N-tag and Syntactic 
N-gram modeling for the edit ?Rock and 
Roll? in Wikipedia (a) The Web-derived 
POS tagged sequence (b) The Web-derived 
POS tag-only sequence (c) A POS tagged 
vandalism diff text Rdiff (d) A POS tag-only 
vandalism Rdiff 
 
1151
tagi. This is similar to n-gram language modeling, 
but instead, we model the syntax using POS tags, 
rather than its words. In this example, we can 
use the system in Figure 2 (b) to train an n-tag 
syntactic model and use the one in Figure 2 (d) 
to test. As we see, for this test segment, it be-
longs to the vandalism class and has very differ-
ent syntax from the n-tag model. Therefore, the 
normalized log likelihood outcome from the n-
tag model is very low. 
In order to model semantics, we use an im-
proved version of the n-gram language modeling 
method. Instead of only counting wordi-2 wordi-1 
wordi, we model composite tag/word feature, e.g. 
tagi-2wordi-2 tagi-1wordi-1 tagiwordi. This syntactic 
n-gram modeling method has been successfully 
applied to the task of automatic speech recogni-
tion (Collins et al, 2005). In the example in Fig-
ure 2, the vandalism diff text will probably score 
low, because although it shares an overlap bi-
gram ?and roll? with the phrase ?rock and roll? 
in training text, once we apply the shallow syn-
tactic n-gram modeling method, the POS tag 
bigram ?and/CC roll/VB? in diff text will be dis-
tinguished from the ?and/CC roll/NN? or 
?and/CC roll/NNP? in the training data. 
5 Experiments 
To evaluate the effectiveness of our approach, 
we first run experiments on a preliminary corpus 
that is also used by previous studies and com-
pare the results. Then, we conduct a second ex-
periment on a larger corpus and analyze in detail 
the features of our system. 
5.1 Experiment Setup 
In our experiments, we use a Wikipedia vandal-
ism detection corpus (Potthast et al, 2007) as a 
preliminary corpus. The preliminary corpus con-
tains 940 human-assessed edits from which 301 
edits are classified as vandalism. We split the 
corpus and keep a held-out 100 edits for each 
class in testing and use the rest for training. In 
the second experiment, we adopt a larger corpus 
(Potthast et al, 2010) that contains 15,000 edits 
with 944 marked as vandalism. The split is 300 
edits for each class in held-out testing and the 
rest used for training. In the description of the 
second corpus, each edit has been reviewed by at 
least 3 and up to 15 annotators. If more than 2/3 
of the annotators agree on a given edit, then the 
edit is tagged as one of our target classes. Only 
11 cases are reported where annotators fail to 
form a majority inter-labeler agreement and in 
those cases, the class is decided by corpus au-
thors arbitrarily.    
In our implementation, the Yahoo! 4  search 
engine and Bing5 search engine are the source 
for collecting top-k results for topic-specific n-
gram training data, because Google has a daily 
query limit. We retrieve top-100 results from 
Yahoo!, and combine them with the top-50 re-
sults from Bing.   
For POS tagging, we use the Stanford POS 
Tagger (Toutanova and Manning, 2000; Touta-
nova et al, 2003) with its attached wsj3t0-18- 
bidirectional model trained from the Wall Street 
Journal corpus. For both shallow syntactic and 
semantic modeling, we train topic-specific tri-
gram language models on each edit using the 
SRILM toolkit (Stolcke, 2002). 
In this classification task, we used two logistic 
classification methods that haven?t been used 
before in vandalism detection. Logistic model 
trees (Landwehr et al, 2005) combine tree in-
duction with linear modeling. The idea is to use 
the logistic regression to select attributes and 
build logistic regression at the leaves by incre-
mentally refining those constructed at higher 
levels in the tree. The second method we used, 
logistic boosting (Friedman et al, 2000), im-
proves logistic regression with boosting. It 
works by applying the classification algorithm to 
reweighted versions of the data and then taking a 
weighted majority vote of the sequence of clas-
sifiers thus produced.    
5.2 Preliminary Experiment 
In the preliminary experiment, we tried logistic 
boosting classifiers and logistic model trees as 
classifiers with 10-fold cross validation. The 
rule-based method, ClueBot, is our baseline.  
We also implemented another baseline system, 
using the bag of words (BoW) and Naive Bayes 
method (Smets et al, 2008) and the same toolkit 
(McCallum, 1996) that Smets et al used. Then, 
we compare our result with Potthast et al (2008), 
who used the same corpus as us. 
 
                                                 
4 http://www.yahoo.com 
5 http://www.bing.com 
1152
 
Table 3: Preliminary Experiment Results; The 
acronyms: BoW: Bag of Words, LMT: Logistic 
Model Trees, LB: Logistic Boosting, Task-
specific + Lexical: features in section 4.1 and 4.2 
 
As we can see in Table 3, the ClueBot has a 
F-score (F1) of 0.43. The BoW + Na?ve Bayes 
approach improved the result and reached an F1 
of 0.75. Compared to these results, the system of 
Potthast et al (2008) is still better and has a F1 
of 0.80. 
For the results of our system, LMT gives us a 
0.89 F1 and LogitBoost (LB) gives a 0.95 F1. A 
significant F1 improvement of 15% was 
achieved in comparison to the previous study 
(Potthast et al, 2008). Another finding is that we 
find our shallow syntactic-semantic modeling 
method improves 2-4% over our task-specific 
and lexical features.  
5.3 Results and Analysis 
In the second experiment, a notable difference 
from the preliminary evaluation is that we have 
an unbalanced data problem. So, we use random 
down-sampling method to resample the majority 
class into balanced classes in the training stage. 
Then, we also use the two classifiers with 10-
fold cross validation. 
The F1 result reported by our BoW + Na?ve 
Bayes baseline is 0.68. Next, we test our task-
specific and lexical features that specified in sec-
tion 4.1 and 4.2. The best result is a F1 of 0.82, 
using logistic boosting. Finally, with our topic-
specific shallow syntactic and semantic model- 
 
Table 4: Second Experiment Results 
 
ing features, we have a precision of 0.86, a recall 
of 0.85 and F1 of 0.85. 
Though we are surprised to see the overall F1 
for the second experiment are not as high as the 
first one, we do see that the topic-specific shal-
low syntactic and semantic modeling methods 
play an important role in improving the result.  
Looking back at the related work we men-
tioned in section 2, though we use newer data 
sets, our overall results still seem to surpass ma-
jor vandalism detection systems. 
6 Conclusion and Future Works 
We have described a practical classification 
framework for detecting Wikipedia vandalism 
using NLP techniques and shown that it outper-
forms rule-based methods and other major ma-
chine learning approaches that are previously 
applied in the task.  
In future work, we would like to investigate 
deeper syntactic and semantic cues to vandalism. 
We hope to improve our models using shallow 
parsing and full parse trees. We may also try 
lexical chaining to model the internal semantic 
links within each edit. 
Acknowledgements 
The authors are grateful to Julia Hirschberg, 
Yves Petinot, Fadi Biadsy, Mukund Jha, Wei-
yun Ma, and the anonymous reviewers for useful 
feedback. We thank Potthast et al for the Wiki-
pedia vandalism detection corpora. 
Systems Recall Precision F1 
ClueBot 0.27 1 0.43 
BoW + 
Na?ve Bayes 
0.75 0.74 0.75 
Potthast 
et. al., 2008 
0.77 0.83 0.80 
Task-specific 
+Lexical 
(LMT) 
0.87 0.87 0.87 
Task-specific 
+Lexical (LB) 
0.92 0.91 0.91 
Our System 
 (LMT) 
0.89 0.89 0.89 
Our System 
(LB) 
0.95 0.95 0.95 
 
Features Recall Precision F1 
BoW +  
Na?ve Bayes 
0.68 0.68 0.68 
Task-specific 
(LMT) 
0.81 0.80 0.80 
Task-specific 
+Lexical(LMT) 
0.81 0.81 0.81 
Our System 
(LMT) 
0.84 0.83 0.83 
Task-specific 
(LB) 
0.81 0.80 0.80 
Task-specific + 
Lexical (LB) 
0.83 0.82 0.82 
Our System 
(LB) 
0.86 0.85 0.85 
 
1153
References 
Adler, B. Thomas, Luca de Alfaro, Ian Pye and 
Vishwanath Raman. 2008. Measuring Author Con-
tributions to the Wikipedia. In Proc. of the ACM 
2008 International Symposium on Wikis. 
Biadsy, Fadi, Julia Hirschberg, and Elena Filatova. 
2008. An Unsupervised Approach to Biography 
Production using Wikipedia. In Proc. of the 46th 
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 807?815. 
Bratko, Andrej,  Gordon V. Cormack, Bogdan Filip-
ic,Thomas R. Lynam and Blaz Zupan. 2006. Spam 
Filtering Using Statistical Data Compression 
Models. Journal of Machine Learning Research, 
pages 7:2673-2698. 
Collins, Michael, Brian Roark and Murat Saraclar. 
2005. Discriminative Syntactic Language Model-
ing for Speech Recognition. In Proc. of the 43rd 
Annual Meeting of the Association for Computa-
tional Linguistics. pages 507?514. 
Friedman, Jerome, Trevor Hastie and Robert Tibshi-
rani. 2000. Additive Logistic Regression: a Statis-
tical View of Boosting. Annals of Statistics 28(2), 
pages 337-407. 
Geiger, R. Stuart. 2010. The Work of Sustaining Or-
der in Wikipedia: The Banning of a Vandal. In 
Proc. of the 2010 ACM Conference on Computer 
Supported Cooperative Work, pages 117-126. 
Heckel, Paul. 1978. A Technique for Isolating Differ-
ences Between Files. Communications of the ACM, 
pages 264?268 
Itakura, Kelly Y. and Charles L. A. Clarke. 2009.  
Using Dynamic Markov Compression to Detect 
Vandalism in the Wikipedia. In Proc. of the 32nd 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, pages 
822-823. 
Landwehr, Niels, Mark Hall and Eibe Frank. 2005. 
Logistic Model Trees. Machine Learning, 59(1-2), 
pages 161?205. 
McCallum, Andrew. 1996. Bow: a Toolkit for Statis-
tical Language Modeling, Text Retrieval, Classifi-
cation and Clustering.  
Potthast, Martin, Benno Stein, and Robert Gerling. 
2008. Automatic Vandalism Detection in Wikipe-
dia. In Proc. of the 30th European Conference on 
Information Retrieval, Lecture Notes in Computer 
Science, pages 663-668. 
Potthast, Martin and Robert Gerling. 2007. Wikipedia 
Vandalism Corpus WEBIS-VC07-11. Web Tech-
nology & Information Systems Group, Bauhaus 
University Weimar. 
Potthast, Martin, Benno Stein and Teresa Holfeld. 
2010. PAN Wikipedia Vandalism Training Corpus 
PAN-WVC-10. Web Technology & Information 
Systems Group, Bauhaus University Weimar. 
Rigoutsos, Isidore and Tien Huynh. 2004. Chung-
Kwei: a pattern-discovery-based system for the au-
tomatic identification of unsolicited e-mail mes-
sages (SPAM). In Proc. of the First Conference on 
E-mail and Anti-Spam. 
Smets, Koen, Bart Goethals and Brigitte Verdonk. 
2008. Automatic Vandalism Detection in Wikipe-
dia: Towards a Machine Learning Approach In 
Proc. of AAAI '08, Workshop on Wikipedia and 
Artificial Intelligence, pages 43-48. 
Stein, Klaus and Claudia Hess. 2007. Does It Matter 
Who Contributes: a Study on Featured Articles in 
the German Wikipedia. In Proc. of the ACM 18th 
Conference on Hypertext and Hypermedia, pages 
171?174. 
Stolcke, Andreas. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Interna-
tional Conference on Spoken Language 
Processing, volume 2, pages 901?904. 
Toutanova, Kristina and Christopher D. Manning. 
2000. Enriching the Knowledge Sources Used in a 
Maximum Entropy Part-of-Speech Tagger. In Proc. 
of the Joint SIGDAT Conference on Empirical Me-
thods in Natural Language Processing and Very 
Large Corpora, pages 63-70. 
Toutanova, Kristina, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency 
Network. In Proceedings of Human Language 
Technology Conference and the North American 
Chapter of the Association of Computational Lin-
guistics Series, pages 252-259. 
Wilkinson,Dennis and Bernardo Huberman. 2007. 
Cooperation and Quality in Wikipedia. In Proc. of 
the ACM 2007 International Symposium on Wikis, 
pages 157?164. 
1154
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 869?873,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Domain Partitioning for Multi-Domain Learning
Di Wang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
diwang@cs.cmu.edu
Chenyan Xiong
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
cx@cs.cmu.edu
William Yang Wang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
ww@cmu.edu
Abstract
Multi-Domain learning (MDL) assumes that
the domain labels in the dataset are known.
However, when there are multiple metadata at-
tributes available, it is not always straightfor-
ward to select a single best attribute for do-
main partition, and it is possible that combin-
ing more than one metadata attributes (includ-
ing continuous attributes) can lead to better
MDL performance. In this work, we propose
an automatic domain partitioning approach
that aims at providing better domain identi-
ties for MDL. We use a supervised clustering
approach that learns the domain distance be-
tween data instances , and then cluster the data
into better domains for MDL. Our experiment
on real multi-domain datasets shows that us-
ing our automatically generated domain parti-
tion improves over popular MDL methods.
1 Introduction
Instead of assuming data are i.i.d, Multi-domain
learning (MDL) methods assumes that data come
from several domains and make use of domain la-
bels to improve modeling performance (Daume? III,
2007). The motivation of using MDL is that datasets
from different domains could be different, in two
ways. First, the feature distribution p(x) could be
domain specific, meaning that the importance of
each feature is different across domains. Second,
the distribution of label Y given X , p(y|x), of dif-
ferent domains could be different. These differ-
ences could create problems for traditional machine
learning methods: models learned from one domain
might not be generalizable to other domains (Ben-
David et al, 2006; Ben-David et al, 2010).
One common assumption of MDL methods is that
the domain identities are pre-defined. For example,
in the multi-domain Amazon product review dataset
(Finkel and Manning, 2009), the product categories
are typically used as the domain identities. How-
ever, a question raised by Joshi et al (2012) is that,
in real-world data sets, there could be many ways to
split data into domains, and it is hard to decide which
one to use. Consider the Amazon product reviews,
where we have multiple attributes attached to each
review: for example, product category, reviewer lo-
cation, price, and number of feedback. Which at-
tribute is the most informative domain label? Or we
should use all of these meta-data and partition the
data into many small domains?
In this paper, we investigate the problem of au-
tomatic domain partitioning. We propose an em-
pirical domain difference testing method to exam-
ine whether two groups of data are i.i.d, or gener-
ated from different distributions, and how different
they are. Using this approach, we generate data pairs
that belong to the same distribution, and data pairs
that should be partitioned into different domains.
These pairs are then used as training data for a super-
vised clustering algorithm, which automatically par-
titions the dataset into several domains. In the eval-
uation, we show that our automatically-partitioned
domains improve the performances of two popular
MDL methods on real sentiment analysis data sets.
Note that Joshi et al (2013) proposed a Multi-
Attribute Multi-Domain learning (MAMD) method,
which also exploited multiple dimensions of meta-
869
data and provided extensions to two traditional MDL
methods. However, extensions to the MAMD set-
ting may not be trivial for every MDL algorithm,
while our method serves as a pre-processing step and
can be easily used for all MDL approaches. In ad-
dition to this, MAMD only works with categorical
metadata, and can not fully utilize information in the
form of continuous metadata values.
2 Automatic Domain Partitioning
In this section, we introduce the Automatic Domain
Partitioning (ADP) problem: given data X , meta-
data M and label Y , find a function g : M 7? I such
that the common MDL methods perform better with
data X and domain identity I in the prediction of Y .
For example, on Amazon sentiment analysis data, X
is the feature matrix extracted from reviews, Y is the
positive or negative label vector, and M is the meta-
data matrix associated with reviews (e.g. product
price and category).
Our approach works as follows: in training, we
first use an empirical domain difference testing
method to detect whether two groups of data should
be considered as different domains; after that we ap-
ply supervised clustering to learn the distance met-
ric between two data points, i.e. how different thay
are in MDL view, from training data generated by
our domain difference test method; finally, based on
the distance metric learned, we cluster our data into
several clusters, and train MDL models with those
clusters as domain labels; in testing, we assign data
instance to its nearest cluster and use that cluster as
its domain identity, and then apply the trained MDL
models for prediction.
2.1 Empirical Domain Difference Test
The key motivation of MDL is that a model fits for
one domain may not fit well for other domains. Fol-
lowing the same motivation, we propose an empiri-
cal method for domain difference test called Domain
Model Loss (DML) that provides us the domain dif-
ference score d(G1, G2) between two groups of data
G1 = {X1, Y1} and G2 = {X2, Y2}.
DomainModel Loss If the mapping functions f1 :
X1 7? Y1 and f2 : X2 7? Y2 are different for
two data groups, we could directly use the disagree-
ment of f1 and f2 as domain difference score. More
specifically, if we train two classifiers f?1 : X1 7?
Y1, f?2 : X2 7? Y2 individually on G1 and G2, we
could have the K-fold empirical loss:
l?(f1, G1) =
1
K
?
i
Error of f1 on i-th fold of G1,
l?(f2, G2) =
1
K
?
i
Error of f2 on i-th fold of G2.
And we could also apply the trained model f1 on
G2, and f2 on G1 to get:
l?(f1, G2) = Error of f1 on G2,
l?(f2, G1) = Error of f2 on G1.
Then, if G1 and G2 are actually the same with each
other, then both models will have same empirical
loss on either data set, but if they are not, we will
have a positive DML score:
DML(G1, G2) =
1
2
(L?(f1, G2) + L?(f2, G1)),
where:
L?(f1, G2) =
l?(f1, G2)? l?(f1, G1)
l?(f1, G1)
,
L?(f2, G1) =
l?(f2, G1)? l?(f2, G2)
l?(f2, G2)
.
2.2 Supervised Clustering for Domain
Partitioning
Our domain difference test method calculates the
distance between two partitioned data groups. How-
ever, to directly use it for domain partitioning, we
must go through all possible combinations of do-
main assignments in exponential time, which is in-
feasible. Our solution is to use a polynomial-time
supervised clustering method developed by Xing et
al. (2002) to learn a distance function that calculates
the distance between any two data points. Formally,
given a set of data pairs D, which belong to different
domains, and a set of data pairs S, which belong to
the same domain, it learns a distance metric A by:
max
A
g(A) =
?
(i,j)?D
?
(mi ?mj)TA(mi ?mj)
s.t.f(A) =
?
(i,j)?S
(mi ?mj)
TA(mi ?mj) ? 1
A  0,
870
where mi,mj are meta data of i and j.
The metadata M are preprocessed as follows: 1)
Each categorical attribute was converted to several
binary questions, one per category, and each bi-
nary question was considered as one metadata di-
mension in ADP method. For example, if categor-
ical attribute ?Product Type? has two values ?Mu-
sic? and ?Electronics?, then there will be two dimen-
sions of metadata corresponding to ?Product Type?
in ADP. Two metadata dimensions correspond to bi-
nary questions: ?Is Product Type Music? and ?Is
Product Type Electronics?. 2) Each continuous at-
tribute was normalized by scaling between 0 and 1.
The training data S,D for metric learning are gen-
erated as follows:
1. For each dimension Mk of M , split data at
value 0.5, sample two equally sized groups, ap-
ply our domain difference testing method and
find the difference between these data groups.
2. Assign distance to each pair of instances by the
average distance of all partitions that partitions
the pair into different groups.
3. Select top n similar pairs as S and top n differ-
ent pairs as D.
The learned distance metric A now conveys the
domain difference information obtained from our
domain distance test results: which meta attributes
are important for domain partitioning and which are
not as important. Following Xing et al (2002), we
transfer the instance?s metadata feature M by MBT ,
where BTB = A. Then we use a clustering method
on MBT , and the output is our domain partitioning
result.
3 Experiment Methodology
Datasets To evaluate our methods, we used two
subsets of Amazon review corpus (Jindal and Liu,
2008), which originally contain 5.8 million reviews
with a variety of metadata about products and users.
The first subset (BOOK) contains 20,000 reviews on
books published by eleven most popular publishers,
while the second (PROD) is reviews about products
within seven most common product categories. We
randomly split each dataset into training and testing
sets with equal size. The task is to predict a positive
or negative label for each review. Case insensitive
unigrams excluding stop words are used as features,
and all features appear less than 500 times are re-
moved for efficient experiment processing. Reviews
of 4 or 5 stars are considered positive and 1 or 2 stars
are considered negative, while 3 stars reviews are ex-
cluded. Each review has multiple metadata such as
book?s publisher, product?s type, user?s state loca-
tion, product price, review year, and number of other
user feedback. Reviews with missing metadata are
filtered out.
MDL Methods Our first MDL algorithm is the
Frustratingly Easy Domain Adaptation (FEDA)
(Daume? III, 2007) which is easy to implement and
achieved competitive performance on many applica-
tions. It creates an augmented feature space as the
Cartesian product of the input features and the orig-
inal domains plus a shared domain. Then it uses a
SVM classifier over the augmented feature space to
obtain classification result. Specifically, our FEDA
methods use L2-regularized SVM with linear ker-
nel by LIBLINEAR package1. The parameters C =
0.01 was selected using five-fold cross-validation on
training set.
Our second MDL algorithm is Multi-Domain
Regularization (MDR) (Dredze and Crammer,
2008), which is a classifier combination ap-
proach based on Confidence-Weighted (CW) learn-
ing (Dredze et al, 2008). The CW learning is an on-
line update method that maintains probabilistic con-
fidence for each parameter by keeping track of its
variance. In our experiments, we use the CW im-
plementation provided by its authors and choose the
best performing configurations described in (Dredze
and Crammer, 2008).
Domain Partition Methods We evaluated the do-
main partition results provided by our ADP on the
two MDL methods (FEDA & MDR). For simplic-
ity and efficiency, we use Naive Bayes as our base
prediction model f1 and f2 to generate the domain
model loss score, described in section 2.1. In train-
ing data generation, we choose top 10% similar pairs
as S and top 10% different pairs as D. And given
the learnt distance metric A, we use K-means to do
the clustering. The number of clusters is selected by
five-fold cross-validation on training set.
1http://www.csie.ntu.edu.tw/?cjlin/liblinear
871
We compare our domain partition quality with
three other methods: 1) 1-Best chooses best per-
forming categorical metadata on a validation set as
domain indicators, where the original training set
was splitted equally to train and validate the per-
formance of each categorical attribute; 2) Random
partition that assigns domain identities to instances
randomly with same number of domains as 1-Best.
We run each random partition ten times and took the
average; 3) MAMD proposed by Joshi et al (2013).
However, the original version of MAMD does not
support continuous attribute such as price. So we
made an extension that sorts these values to ten bins
and then treats them as categorical values.
4 Results and Discussions
Partition + MDL PROD BOOK
ADP + FEDA 82.02 ? 86.22 ??
MAMD + FEDA 81.04 86.08
1-Best + FEDA 82.00 85.85
Random + FEDA 79.36 84.72
ADP + MDR 82.10 ] ? ? 86.62 ] ? ?
MAMD + MDR 80.17 84.37
1-Best + MDR 79.79 83.68
Random + MDR 74.65 81.16
Table 1: Overall accuracies on PROD and BOOK
datasets. ADP results that are statistically significantly
better than MAMD are marked with ], and better than 1-
Best and Random are indicated by ? and ? respectively,
using a paired t-test, with p < 0.05.
Table 1 shows the overall experimental results
of four domain partition methods with two MDL
methods on PROD and BOOK datasets. One could
see that when using MDR method, ADP could
significantly outperform all baselines on both data
sets, with relatively more than 2% gains. For
FEDA, on PROD data, ADP performs the same with
MAMD and 1-Best; on BOOK data, ADP outper-
forms 1-Best significantly, but is just slightly better
than MAMD. One possible reason is that the best
numbers of cluster selected by cross-validation are
around 150. With such large number of none-perfect
domains, FEDA will generate huge dimension of
features and perhaps require more training data to
provide better performances. Another possible rea-
son is that FEDA and the SVM underlying FEDA
are very robust against bad domain partition results.
This might be the reason of high FEDA baselines.
In general, our ADP method helps existing MDL
approaches achieve better performance, while bad
(Random) partitioning does hurt.
Figure 1(a) and 1(b) shows the performances of
applying FEDA on different domain partitioning
methods on PROD and BOOK, while Figure 1(c)
and 1(d) shows experiment results with MDR. The
x-axis is the size of the output domains (the K in
our K-means clustering), and y-axis is the accuracy
of models. With our domain partitioning approach,
MDR can perform consistently higher than all the
three baselines on both dataset when k > 50. As
we discussed for Table 1, FEDA might be less sen-
sitive to domain partition results, which causes high
baseline performance and high ADP+FEDA perfor-
mance with small K. Since the performance trends
to increase along with k until 50 in three figures
(1(b), 1(c) and 1(d)), we believe that the ground-
truth domain size is likely larger than 50. These
results clearly indicate ADP does provide more de-
sirable domain assignments for MDL. The domain
selected by 1-Best such as publishers has only 11
domains, which limits the ability of 1-Best to com-
pletely express domain information. And our gener-
ated domains integrate multiple metadata attributes,
lead to more detailed domain partitions, and enhance
the ability of MDL methods to capture the difference
between different groups of data. Although accu-
racies are growing with k in general, we also see
that there are fluctuations on curves especially when
curves are zoomed to a small range. To get smoother
results, we can sample more data to calculate do-
main similarity and repeat the K-means clustering
with more different initializations.
5 Conclusions
In this paper, we propose an Automatic Domain Par-
tition (ADP) method that provides better domain
identities for multi-domain learning methods. We
first propose a new approach to identify whether two
data groups should be considered as different do-
mains, by comparing the differences using Domain
Model Loss. We use a supervised clustering ap-
proach to train our model with labels generated by
domain difference tests, and cluster the re-weighted
872
metadata as our domain partition by K-means. Ex-
periments on real world multi-domain data show
that the domain identities generated by our method
can improve the performance of MDL models.
0 50 100 150 20079
79.580
80.581
81.582
82.5
Number of domains (K)
Accura
cy %
 
 
ADP1?BestRandomMAMD
(a) FEDA results on PROD
0 50 100 150 20084.5
85
85.5
86
86.5
87
Number of domains (K)
Accura
cy %
 
 
ADP1?BestRandomMAMD
(b) FEDA results on BOOK
0 50 100 150 20072
74
76
78
80
82
84
Number of domains (K)
Accur
acy %
 
 
ADP1?BestRandomMAMD
(c) MDR results on PROD
0 50 100 150 20081
82
83
84
85
86
87
Number of domains (K)
Accur
acy %
 
 
ADP1?BestRandomMAMD
(d) MDR results on BOOK
Figure 1: Accuracies over different size of the output do-
mains (K)
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 137?144.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151?175.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 689?697.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
Machine Learning, Proceedings of the Twenty-Fifth
International Conference (ICML), pages 264?271.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of the 2009 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 602?610.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL).
Nitin Jindal and Bing Liu. 2008. Opinion spam and anal-
ysis. In Proceedings of the International Conference
on Web Search and Web Data Mining (WSDM), pages
219?230.
Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn Penstein Rose?. 2012. Multi-domain learn-
ing: When do domains matter? In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, (EMNLP-CoNLL), pages 1302?
1312.
Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn P. Rose?. 2013. Whats in a domain? multi-
domain learning for multi-attribute data. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 685?690, Atlanta, Georgia, June. Association
for Computational Linguistics.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stu-
art J. Russell. 2002. Distance metric learning with
application to clustering with side-information. In
Advances in Neural Information Processing Systems
(NIPS), pages 505?512.
873
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1325?1336,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
This Text Has the Scent of Starbucks:
A Laplacian Structured Sparsity Model for
Computational Branding Analytics
William Yang Wang
School of Computer Science
Carnegie Mellon University
ww@cmu.edu
Edward Lin and John Kominek
Voci Technologies, Inc.
Pittsburgh, PA 15217
{ed.lin,john.kominek}@vocitec.com
Abstract
We propose a Laplacian structured sparsity
model to study computational branding ana-
lytics. To do this, we collected customer re-
views from Starbucks, Dunkin? Donuts, and
other coffee shops across 38 major cities
in the Midwest and Northeastern regions of
USA. We study the brand related language
use through these reviews, with focuses on
the brand satisfaction and gender factors. In
particular, we perform three tasks: auto-
matic brand identification from raw text, joint
brand-satisfaction prediction, and joint brand-
gender-satisfaction prediction. This work ex-
tends previous studies in text classification by
incorporating the dependency and interaction
among local features in the form of structured
sparsity in a log-linear model. Our quantita-
tive evaluation shows that our approach which
combines the advantages of graphical model-
ing and sparsity modeling techniques signifi-
cantly outperforms various standard and state-
of-the-art text classification algorithms. In ad-
dition, qualitative analysis of our model re-
veals important features of the language uses
associated with the specific brands.
1 Introduction
In marketing science, branding is a modern market-
ing strategy of creating a unique image for a prod-
uct in the customers? mind. Establishing the brand
in the broad social context is just as important as
building a good product (Makens, 1965; Lederer
and Hill, 2001; Kim et al, 2013). In fact, blind
taste test experiments have frequently shown how
branding directly leads to the success of products
and companies. Most notably is a continued study
sponsored by Pepsi, known as the Pepsi Challenge1,
where Pepsi demonstrates how even though people
preferred the taste of Pepsi, Coca-Cola?s branding
has made it more popular. Even now, Microsoft
uses similar blind taste tests2 to compare search en-
gines, Bing and Google, showing that although par-
ticipants prefer Bing?s results, Google?s brand might
have strengthened over the years. These studies all
suggest that brand and its associations play impor-
tant roles in the customers? perceptions and deci-
sions.
To accommodate the market change, companies
frequently adjust branding strategies by analyzing
how their customers receive and respond to brand-
ing messages. So far, such analysis is often done
by using surveys and focus groups (Moon and
Quelch, 2006), which is expensive and not time-
efficient. Recently, with the advance of machine
learning techniques, researchers from the chemistry
and vision communities started to pay attention to
the problem of automatic brand identification from
smell (Luo et al, 2004) and images (Pelisson et al,
2003). In contrast, even though textual data that
contains hidden branding information is abundantly
available in many forms over the Web, automatic
discovery and computational analysis on such data
are not well studied in the past.
Computational branding analytics (CBA) seeks to
extract information, trends, and demographics about
a brand on the basis of free-form text, e.g. from
blogs, Twitter comments, reviews, or forum posts.
As described in Section 3, in this study we use a sub-
1http://en.wikipedia.org/wiki/Pepsi Challenge
2http://www.bingiton.com/
1325
set of online Yelp reviews that discuss coffee shops.
The main reason is that this source has the advan-
tage of providing ground truth of multi-labeled data:
each review has meta-information defining a 5-star
rating, the object of the review, and the reviewer?s
name (from which we infer gender). For the pur-
pose of this paper we decompose CBA into three
sub-problems.
? How well can the brand being discussed be
identified by the raw text?
? How well can the joint value of brand and rat-
ing be predicted?
? How well can the joint value of brand, rating,
and gender be predicted?
There are two reasons why one may want to con-
struct text-based classifiers of brand, rating, and gen-
der, when such information is present in the review
header. The first is that trained classifiers can then be
applied to other data sources, such as blogs, where
what is available is only the review itself. The sec-
ond is that by ?opening the hood? to the classifier
one can examine which words exhibit high affilia-
tion with the predicted variables. This can be done,
for example, to contrast the preferences of males and
females with respect to evaluating the qualities of a
coffee shop. Examples of such insights are provided
in Section 5.5.
In this paper, we propose a Laplacian structured
sparsity model for computational branding analyt-
ics. Our main contributions are two-fold: first, in
the novel task of automatic brand identification from
text, we show that by incorporating the dependency
structure and graphical interactions among local
features, our model significantly outperforms vari-
ous text classification algorithms such as the stan-
dard logistic regression, principle component anal-
ysis (PCA), linear kernel support vector machine
(SVM), sparse, non-sparse, and mixed-penalty log-
linear models. These improvements could also be
seen from a joint brand-satisfaction prediction task
and a gender-specific joint brand-satisfaction predic-
tion task. In addition, our Laplacian augmented L1-
ball projection experiment shows that the advantage
of Laplacian structured sparsity is robust across dif-
ferent parameter settings in a L1-constrained prob-
lem. Secondly, the qualitative analysis of our ma-
chine learning model shows the interesting features
and language use that relate to brand and its associ-
ated pragmatics.
In the next section, we outline related work in
CBA, sparsity, and spectral graph learning. In Sec-
tion 3, we describe the corpus in this study. The
Laplacian structured sparsity model is introduced in
Section 4. The experimental setup and results are
presented in Section 5. A short discussion is fol-
lowed in Section 6 and we conclude in Section 7.
2 Related Work
Early work on statistical brand analysis in the
marketing community dates back to the work of
Kuehn (1962), where he first hypothesizes that
brand choice could be described as a learning pro-
cess. Guadagni and Little (1983) further empiri-
cally tested the hypothesis by building a calibrated
multinomial logistic regression model to predict the
purchase of ground coffee, using the data from the
optical scanning of product code in supermarkets.
Outside the marketing community, statistical brand
analysis is rarely seen. More recently, a study (Luo
et al, 2004) applies neural networks to identify
cigarette brands, with the hope of detecting illegal
cigarettes from smell features. In image process-
ing, researchers have studied the problem of brand
identification from image using histogram compar-
ison (Pelisson et al, 2003). However, to the best
of our knowledge, even though textual data is vastly
available, the problems of automatic brand identi-
fication from raw text and computational branding
analytics, are new.
Although the domain of our data is on branding,
our work also aligns with previous work in text and
language classification. Over the years, logistic re-
gression and linear kernel SVM have shown to be
very successful in various regression and classifi-
cation tasks in NLP (Chahuneau et al, 2012; Bi-
adsy et al, 2011). Recently, sparse discriminative
methods that model the sparse nature of text be-
come attractive, because unlike dense models, they
are less likely to overfit to the training data, easier
to interpret, and often lead to state-of-the-art results.
For example, Eisenstein et al (2011b) use the L1,?
sparsity model to discover sociolinguistic patterns.
Wang et al (2012a) compare lasso, ridge, and elas-
tic net models to predict impoliteness behaviors in
teenager conversations. Martins et al (2011) inves-
tigate the tree-structured overlapping group lasso for
1326
structured prediction problems. Chen et al (2013)
study the use of element-wise, group-wise, and hi-
erarchical sparsity models for dialogue act classif-
cation. Sparse inducing priors are also investigated
and shown to be effective in generative models for
topic modeling (Eisenstein et al, 2011a; Wang et
al., 2012b; Paul and Dredze, 2012).
Besides lacking sparsity, since the traditional dis-
criminative methods in NLP often use interdepen-
dent features such as n-grams tokens, and part-of-
speech tags, they also suffer from the problem of not
explicitly modeling the complex dependency struc-
ture and interaction of local features from a global
perspective. To solve this problem, graph meth-
ods seem to be a good solution, because they are
simple, generalizable, and are often used to model
such complex dependency structures (Cohen, 2012).
However, combining the sparse modeling and spec-
tral graphical modeling approaches in a principled
way is challenging. Belkin et al (2006) and Wein-
berger et al (2007) are among the first to investi-
gate graph Laplacians as a manifold regularization
method for statistical learning. Recently, Gao et
al. (2012) propose a histogram intersection based
kNN method to construct a Laplacian matrix for a
least-square sparse coding problem in image pro-
cessing. Unfortunately, this method might be too
specific to the SIFT-based image coding tasks, thus
might not be applicable to the text classification
problem that utilizes n-gram lexical features.
3 Datasets
We collected Yelp reviews from 1,860 Starbucks,
Dunkin? Donuts3, and other coffee shops all over
the Midwest and Northeast regions in the period of
2009. A detail statistics of our data can be found
in Table 1. The Midwest region includes 12 states4
and 19 major cities, and the Northeast region in-
cludes 9 states5 and 19 major cities. For each region,
we divide the coffee shops into 60% training, 20%
development, and 20% test, and there are no over-
laps of coffee shops among these subsets. There are
three values for the brand label: Starbucks, Dunkin?
Donuts, and all other coffee shop brands. The ma-
3We chose these two brands because they are reported as
the leading coffee shops by WSJ (Ovide, 2011) and Forbes (Di-
Carlo, 2004).
4IL, WI, SD, ND, MN, MO, OH, NE, KS, IA, IN, and MI.
5CT, ME, MA, NH, RI, VT, NJ, NY and PA.
Coffee Shops Reviews
Train Dev. Test Train Dev. Test
1 451 150 150 3,513 1,087 1,424
2 665 222 222 6,982 2,530 2,358
T. 1,116 372 372 10,495 3,617 3,782
Table 1: Dataset statistics. 1: midwest region. 2: north-
east region. T.: total.
jority class is ?all other coffee shop brands?, and
the majority baseline is shown in Table 2. In the
task of joint brand-satisfaction prediction, we utilize
the review scores to approximate user satisfaction:
scores 1-2 as the unsatisfactory label, 3 as moder-
ate, and 4-5 as satisfactory. Since the Yelp reviews
do not reveal the reviewer?s gender, we use a similar
method that U.S. Census Bureau used (OConnell
and Gooding, 2006): we first automatically match
the first name of the reviewer with the prior name-
gender distributions in the census records, then man-
ually examine the no-match cases and a subsample
of the matched cases. For those who we cannot
determine the gender, the review will be dropped
from the gender-specific brand-satisfaction predic-
tion task. After filtering, there are 8,528 documents
for training, 2,928 for development, and, 3,046 for
testing. Since the focus of this paper is not on fea-
ture engineering, we use unigram features to repre-
sent each review. Below is an example of positive
review from a male Starbucks customer from Mid-
west.
My favorite place for my iced vanilla lattes.
They have screwed up my order before: instead
of a grande, I got a venti. Not a fan of their
pastries though. I got a donut once, and ended
up feeding it to a pigeon in city garden. Friendly
and fast service. Not open Sundays.
The coffee shop dataset is freely available6 for re-
search purposes.
4 Our Approach
4.1 Problem Formulation and Predictive Tasks
The automatic brand identification problem could
be considered as a traditional multiclass classifica-
6http://www.cs.cmu.edu/?yww/data/emnlp2013.zip
1327
tion problem where the estimated label Y? could
be drawn from Mult(?), where ? is the parame-
ter for the multinomial distribution. To solve this,
a simple but accurate solution is to decompose the
multiclass problem into multiple binary classifica-
tion problems (Rifkin and Klautau, 2004) by train-
ing k one-vs-all binary classifiers, and then use the
argmax criteria to select the best hypothesis from the
k posteriors. As for a binary classifier, we need to
infer the posterior from a Bernoulli distribution that
is parametrized by ??y. Similarly, we can derive k
binary classifiers:
??(1)y , ??
(2)
y , ..., ??
(k)
y . (1)
So, instead of drawing Y? from a multinomial distri-
bution Mult(?), we can draw the final label Y? that
has the largest posterior across all k classifiers:
Y? = argmax
Y,i=1,2,...,k
Pr(Y |??(i)y , ~Xt) (2)
where ~Xt is the testing vector, and Pr(Y |??
(i)
y , ~Xt) is
the posterior probability given the learned classifiers
and the testing vector.
In this paper, we investigate three multiclass clas-
sification tasks: first, we perform a 3-way classi-
fication task for automatic brand identification. In
the task of brand-satisfaction prediction, we model
the brand and the satisfaction label at the same
time (Chahuneau et al, 2012): we perform the task
of jointly predicting aggregate brand-satisfaction
score for a review using 9-way classification. Sim-
ilarly, we perform 18-way classification for the
gender-specific joint brand-satisfaction prediction
task.
4.2 The Log-Linear Framework and Its
Regularized Variants
If we consider the standard logistic regression model
as the binary classifier in this log-linear framework7,
then each classifier can be written as:
??y =
exp
(
~W> ~Xj
)
1 + exp
(
~W> ~Xj
) (3)
here, ~Xj is the j-th observed feature vector, label
y ? {0, 1}, and ~W is a vector of the coefficients. To
7We thank Jacob Eisenstein for the initial derivation of the
logistic regression model.
estimate the model parameters in equation (3), we
only need to set the weights ~W . We can obtain the
following log likelihood, and its gradient function
by taking the first-order partial derivative of ~W :
` =
?
j
yj log ??yj + (1? yj) log(1? ??yj ) (4)
?`
? ~W
=
?
j
(
???yj
? ~W
)(
yj
??yj
?
1? yj
1? ??yj
)
(5)
???yj
? ~W
=
(
??yj ? (??yj )
2
)
~X, (6)
since the log likelihood objective function (4) is con-
cave, using standard gradient ascent with maximum
likelihood estimation can solve the problem. How-
ever, this model does not penalize the noisy features
and unreliable features that might overfit to the train-
ing data. To address this issue, we introduce the L1
norm from lasso technique (Tibshirani, 1996) to reg-
ularize the above likelihood function. Thus, instead
of maximizing the likelihood, we can minimize the
loss function of the negative log-likelihood with a
linear penalty:
min
(
? `+ ?1|| ~W ||
)
(7)
where ?1 is the regularization coefficient. The bene-
fit of L1 penalty in a discriminative model is sim-
ilar to the double exponential distribution of the
sparse priors in generative models (Eisenstein et al,
2011a): they both push the weights of many noisy
features into zeros, revealing only the important fea-
tures. However, since the L1 penalty can intro-
duce discontinuities to the original convex function,
we can also consider an alternative non-sparse ridge
estimator (Le Cessie and Van Houwelingen, 1992)
with log loss and L2 norm, and has the convex prop-
erty:
min
(
? `+ ?2|| ~W ||
2
)
(8)
Another option that balances the sparsity and
smoothness would be the elastic net model (Zou and
Hastie, 2005) that uses the composite penalty:
min
(
? `+ ?1|| ~W ||+ ?2|| ~W ||
2
)
(9)
4.3 The Laplacian Structured Sparsity Model
So far, none of the above element-wise penalty mod-
els in the previous subsection takes into account the
1328
dependency structure of the local features. Inspired
by Gao et al(2012), we group the local features that
have similar distributions together. The intuition is
that, for features that have very similar empirical dis-
tributions in the training set, their weights should not
be drastically different after the learning process in
the same task. In our new objective function, it is
desirable to introduce a new component that struc-
turally penalize these cases where features that are
very similar to each other, but have learned com-
pletely different weights, probably due to the noise
or the data sparsity issue in the training data.
The Objective Function: To do this, we first define
an inter-feature affinity matrix A, where A(p,q) mea-
sures the similarity between a pair of features p and
q. In the spectral graph theory, this affinity matrix
can be viewed as a weighted undirected graph G =
(V,E), where each node Vp denotes a feature p, and
each edge E(p,q) indicates the closeness among the
features p and q. We also introduce a weighted di-
agonal degree matrix D, of which each element in
the diagonal D(p,p) is the sum of all weighted con-
nections of node Vp: D(p,p) =
?Q
q=1A(p,q). We
propose the following objective function:
min
(
? `+ ?1|| ~W ||+ ?2|| ~W ||
2 (10)
+ ?
?
(p,q)
|| ~Wp ? ~Wq||
2A(p,q)
)
(11)
We then denote a graph Laplacian matrix L = D ?
A (Belkin and Niyogi, 2001), and rewrite the objec-
tive function as:
min
(
? `+ ?1|| ~W ||+ ?2|| ~W ||
2 (12)
+ ?( ~W>L ~W )
)
(13)
where ? is the regularization parameter for the
Laplacian structured sparsity term. Intuitively, the
objective function can be interpreted as the sum of
a negative log loss function, the sparsity-inducing
penalty, the quadratic penalty, and the Laplacian
structured penalty. Or, another view of this new
model could be seen as a Laplacian augmented elas-
tic net model where structured sparsity and feature
interaction are considered.
The Laplacian Matrix: In this model, a key aspect
is to derive the Laplacian matrix L. We propose the
following three steps to learn the Laplacian matrix:
Figure 1: An example of the graph G, the corresponding
affinity matrixA, and the corresponding Laplacian matrix
L.
1. Construct the distance matrix Dist. To con-
struct the distance matrix between each fea-
ture, we first transpose the instance-feature ma-
trix, I =
?
j
~Xj , and assume that each feature
(e.g. unigram in our task) is a random variable
that has a multinomial distribution over the in-
stances in the training set. Then, we compare
each pair of features, and calculate the inter-
feature distance matrix Dist with Euclidean
distance as a measure, and use the k-nearest
neighbors (kNN) method (Beyer et al, 1999)
to select the top neighbors of each feature.
2. Derive the affinity matrix A. To assign the
weight on the edge E(p,q) for each connected
nodes (the kNN of V in Dist), we use the
cosine similarity cosine(Vp, Vq) metric (Wang
and Hirschberg, 2011).
3. Generate the degree matrix D and Lapla-
cian matrix L. As discussed earlier, we sum
up the symmetric affinity matrix by row, and
obtain a diagonal degree matrix D, and we fur-
ther define a Laplacian matrix L = D ?A.
To calculate the above matrices in an efficient man-
ner, we partition the covariate into blocks, and pro-
cess each block in parallel (Chen et al, 2011). An
intuitive example of the graphG, its associated affin-
ity matrix A, and Laplacian matrix L, is shown in
Figure 1.
Parameter Estimation: Regarding the optimiza-
tion of objective function in (12-13), a notable prob-
lem is that the sparsity inducing L1 term is non-
differentiable, whereas this is not the case for the L2
norm and the Laplacian structured sparsity term. If
we first take the derivative of the latter two terms,
1329
and we can derive the following gradient compo-
nents:
?(?2|| ~W ||2 + ?( ~W>L ~W ))
? ~W
(14)
= 2?2 ~W + ?( ~W
>L> + ~W>L) (15)
= 2?2 ~W + ?(L
> + L) ~W (16)
since our Laplacian matrix is symmetric, we can
rewrite (16) as
2(?2 ~W + ?L ~W ) (17)
Then, we combine the gradient of the log loss func-
tion in (5) with (17), and apply a bound-constrained
re-formulation (Schmidt et al, 2007) and the lim-
ited memory BFGS (L-BFGS) method (Liu and No-
cedal, 1989) to solve the L1 regularized problem.
The L-BFGS method has relatively low space com-
plexity, and does not require the calculation of full
Hessian matrix, thus it is often used for L1 optimiza-
tion problems.
Augmented Laplacian for an L1-Constrained
Problem: Instead of formulating the L1-regularized
problem by adding the L1 norm, an alternative so-
lution is to formulate a L1-constrained problem by
fixing the sum of all weights ? in the weight vector
~W . The reason is because adding the L1 norm will
make the objective function not continuously differ-
entiable, where as the L1 constraint could be just a
simple linear constraint (Lee et al, 2006). Thus, the
alternative L1-constrained problem could be defined
as:
min(?`), s.t.
?
p
~Wp ? ? (18)
To test the robustness of Laplacian structured spar-
sity term in the setup of a L1-constrained problem,
we can incorporate the Laplacian penalty term into
the above formula, and derive:
min
(
? `+ ?( ~W>L ~W )
)
, s.t.
?
p
~Wp ? ? (19)
Note that the Laplacian matrix is positive-
semidefinite,
~W>L ~W = ~W>
?
(p,q)
L(p,q) ~W (20)
=
?
(p,q)
~W>L(p,q) ~W (21)
=
?
(p,q)
|| ~Wp ? ~Wq||
2A(p,q) (22)
because this graph Laplacian penalty can be viewed
as a quadratic term, and the objective function
in equation 19 is now convex differentiable and
will produce sparse estimates, so that we are able
to use a limited-memory projected quasi-Newton
method (Schmidt et al, 2009) to solve the dual form
of this problem. The Lagrangian dual form of the
problem in equation 19 can be written as:
L( ~W, ?) = ?`+ ?( ~W>L ~W ) (23)
+ ?
(
?
p
~Wp ? ?
)
? ? ~W (24)
where ? ? R is a Lagrange multiplier, and ? ? Rp+
is a p-dimensional vector of non-negative Lagrange
multipliers. And then we can take first-order partial
derivative with regard to ~W , and set it to zero to de-
rive the optimality:
?L
? ~W
= ?
?
j
(
??yj ? (??yj )
2
)
~X
(
yj
??yj
?
1? yj
1? ??yj
)
(25)
+ 2?L ~W + ? ? ? = 0 (26)
To speed up the training, we use the linear-time L1-
ball projection method from Duchi et al (2008) in
our implementation.
5 Experiments
We first compare our model to various baselines in
the 3-way automatic brand identification task. Be-
sides the logistic regression, lasso, ridge and elas-
tic net model that we introduced in Section 4.2, we
also compare with a PCA-based logistic regression
model where the dimensions of the feature space is
reduced in half before the classification. A state-of-
the-art linear kernel SVM model (Chang and Lin,
2011) is also taken into the comparison. In the
second part, we perform 9-way joint classification
of the brand-satisfaction labels. Similarly, we also
perform a 18-way joint classification of the brand-
gender-satisfaction labels. To test the robustness
of our model, we vary the levels of sparsity of our
Laplacian augmented method in a L1-constrained
problem. Finally, we analyze the identified features
for CBA. Throughout this section, we use classifi-
cation accuracy to report the results. We tune the
regularization parameters of log-linear models and
1330
Method Dev. Test
Majority class 75.67 78.08
Logistic regression 91.98 91.06
Linear SVM 92.45 91.75
PCA 91.67 91.20
Lasso 92.81 91.96
Ridge 92.56 91.67
Elastic net 92.81 91.83
Laplacian structured sparsity 93.17* 92.44*
Table 2: The automatic brand identification (3-way) per-
formances. The best result is highlighted in bold. * indi-
cates p < .001 comparing to the second best result.
the cost parameter of the SVM on the development
set, and report results on both the development set
and the held-out test set. The parameter for kNN
was set to 5 according to previous literature (Gao et
al., 2012). A paired two-tailed t-test is used to test
the statistical differences among various models.
5.1 Automatic Brand Identification from Text
Given any piece of raw text from the Web (e.g.
blogs, tweets, news, or forum posts), the first task
for CBA is to identify which brand this text is re-
lated to. Our customer review data set is useful for
this task, because the ground truth of the brand label
is attached to each review. Table 2 shows the re-
sult of our model in this automatic brand identifica-
tion task. In this 3-way classification task, the over-
all results indicate that it is relatively easy to iden-
tify the related brand from customer reviews. When
evaluating our Laplacian structured sparsity model,
our proposed model obtains the best performances
of 93.17% and 92.44%, which are statistically bet-
ter than the second best results (p < .001) in both
datasets.
5.2 Joint Brand-Satisfaction Prediction
In our training data set, we observe a subtle correla-
tion between the brand and satisfaction labels (r =
0.09, p < .001), which suggests us that it might be
interesting to perform a joint prediction task for the
brand-satisfaction labels. This task is also attractive
from the business perspective, because it would be
very useful for the companies to directly identify
user?s level of satisfaction about their brands. Ta-
ble 3 shows that we achieve 69.56% accuracy on the
Method Dev. Test
Majority class 55.43 55.18
Logistic regression 65.80 65.80
Linear SVM 67.67 65.44
PCA 63.92 62.53
Lasso 68.37 66.84
Ridge 67.79 65.55
Elastic net 68.79 66.82
Laplacian structured sparsity 69.56* 67.32*
Table 3: The joint brand-satisfaction prediction (9-way)
performances. The best result is highlighted in bold. *
indicates p < .001 comparing to the second best result.
Method Dev. Test
Majority class 28.24 27.68
Logistic regression 36.03 35.16
Linear SVM 41.05 39.49
PCA 35.35 34.44
Lasso 40.74 39.53
Ridge 40.98 38.94
Elastic net 41.15 38.96
Laplacian structured sparsity 41.22* 40.22*
Table 4: The joint brand-gender-satisfaction prediction
(18-way) performances. The best result is highlighted in
bold. * indicates p < .001 comparing to the second best
result.
development set, and 67.32% accuracy on the test
set using our proposed Laplacian structured model
(p < .001 comparing to the second best results).
5.3 Joint Brand-Gender-Satisfaction
Prediction
Another big interest in the marketing community is
to predict subgroup preferences of specific brands.
In this direction, we perform a 18-way joint brand-
gender-satisfaction prediction using the gender la-
bels that we described in Section 3. Table 4
shows that our proposed Laplacian structured spar-
sity model obtains a test accuracy of 40.22%, signif-
icantly better than the second best result (p < .001).
1331
Figure 2: Automatic brand identification test perfor-
mance varying the level of sparsity ? in a L1 constrained
problem.
Figure 3: Joint brand-satisfaction prediction test perfor-
mance varying the level of sparsity ? in a L1 constrained
problem.
5.4 Varying the Level of Sparsity in a
L1-Constrained Problem
To test the robustness of the Laplacian structured
sparsity component, we exponentially increase the
sum of weights ? to vary the level of sparsity in a
L1-constrained setup. When ? increases, the non-
zero weights in the model also increases. Figures 2
and 3 show that the Laplacian augmented L1-ball
projection statistically outperform the L1-ball pro-
jection baseline in all levels of sparsity (p < .001).
In Figure 4, Laplacian augmented L1-ball projection
is also statistically better than the L1-ball projection
(p < .001), except when ? = 32 and ? = 64.
5.5 Exploratory Data Analysis
We outline the top 15 keywords from the Laplacian
structured sparsity model that are associated with the
Starbucks and Dunkin? Donuts brands in the auto-
matic brand identification task in the Table 5. First
of all, it is observed that our model has discovered
synonyms for both brands: ?sbux?, ?dd?, ?dds?.
Figure 4: Joint brand-gender-satisfaction prediction test
performance varying the level of sparsity ? in a L1 con-
strained problem.
Also, the results imply that Starbucks? unique cup
size branding strategy, ?venti?, ?grande?, ?tall?, has
resonated with their customers as the words promi-
nently show up as top features in reviews. Aligned
with previous study in marketing science (Moon and
Quelch, 2006), an informative set of features re-
lated to Starbucks store decorations showed up in
our model: ?store?, ?restroom?, ?public?, ?bath-
room?, and ?spacious?. In contrast, these features
stopped to show up on the list of Dunkin? Donuts.
Instead, TV and game (sports), which are indeed
important features of dining at Dunkin? Donut, ap-
peared. Note that Baskin-Robbins, which is a sub-
brand of Dunkin? Brands Group, Inc., also appeared
as informative features to predict Dunkin? Donuts.
To understand the preferences of different gen-
der subgroups towards the two brands, we contrast
in Table 6 and Table 7 the top features that identify
the satisfied female and male customers in the joint
brand-gender-satisfaction prediction task.
Interestingly, it seems that the female customers
identify Starbucks as a place for ?studying?, with
?fireplace? as the top preference of the spots in the
store, and ?winter? is also a high-ranked feature.
Also, the adjective ?super? was frequently men-
tioned by the female Starbucks customers (but not
the males). As for Dunkin? Donuts, the top-ranked
keywords are still mainly associated with its names,
but it seems the snack ?Munchkins? is highly pre-
ferred by the female customers. Not surprisingly,
the cue words that the male customers identify the
Starbucks brand do not always agree with those of
the females. For example, instead of ?fireplace?,
they prefer staying at the ?patio?, and drink the cof-
fee from the ?clover? brewing system. Interestingly,
1332
Starbucks weight Dunkin? weight
starbucks 1.9365 dd 2.4224
sbux 1.0152 dunkin 1.7781
venti 0.8216 donuts 1.6989
corporate 0.7032 dunks 1.6455
store 0.6580 dds 1.4936
particular 0.6512 donut 1.3979
tall 0.5496 dunkins 1.3729
restroom 0.5447 glazed 0.9975
tourists 0.5431 robbins 0.9402
public 0.5260 baskin 0.8578
lines 0.4956 sugar 0.6475
drink 0.4787 d 0.6327
bathroom 0.4721 ice 0.5835
spacious 0.4629 stale 0.5404
location 0.4611 game 0.5049
grande 0.4563 tv 0.5010
Table 5: Top features that identify the Starbucks and
Dunkin? Donuts brands from the best model.
.
on the Dunkin?s side, ?munchkins? also disappeared
and replaced by ?glazed? (donuts). However, both
males and females agreed that ?fast? or ?quick? ser-
vice was an important feature of creating satisfac-
tion, which echoes with the result from self-reported
customer surveys (Moon and Quelch, 2006).
The word ?name? is a prominent indicator for the
female customers of Starbucks: at first we were puz-
zled, but after we digged into the database, we found
reviews such as:
? ?... and the baristas are one of the nicest they
always ask for your name, so you never end up
with coffee meant for the guy behind you.?
? ?... she asked me my name and i told her and
she excidetly proclaimed melissa and wrote my
name on the cup. This place was probably one
of the better starbucks ive been to.?
? ?... all of their employees are really friendly,
and embarrassingly enough most know me by
name and know my typical drink order grande
nonfat misto with a flavor shot of white mocha.
This is actually very helpful.?
The above examples show how our system effec-
tively serves as a salient keyword spotter. And that
as a keyword spotter one can use it to extract sur-
rounding context and feed that through to the next
Starbucks weight Dunkin? weight
starbucks 0.5013 dd 0.6931
chain 0.4268 dds 0.5620
winter 0.3382 dunkin 0.5344
fireplace 0.3089 donuts 0.4270
studying 0.2972 donut 0.3732
particular 0.2967 dunks 0.3687
super 0.2786 morning 0.3077
name 0.2543 quick 0.3012
know 0.2443 how 0.2940
because 0.2263 munchkins 0.2758
Table 6: Top features that jointly identify the satisfied
female customers and the Starbucks and Dunkin? Donuts
brands from the best model.
Starbucks weight Dunkin? weight
starbucks 0.6632 dd 0.7491
throw 0.3514 dunkin 0.6075
know 0.2959 dds 0.5333
store 0.2885 donuts 0.5326
fix 0.2498 donut 0.3215
particular 0.2487 dunks 0.3158
sbux 0.2462 morning 0.3095
patio 0.2349 rush 0.3030
prefer 0.2324 fast 0.2979
clover 0.2215 moving 0.2520
corporate 0.2153 glazed 0.2326
Table 7: Top features that jointly identify the satisfied
male customers and the Starbucks and Dunkin? Donuts
brands from the best model.
stage of analysis, including examination by humans.
This is extremely practical and useful, because it
provides actionable items. For example, analysts
can advise managers to revise their training manual
and tell store employees to remember the names of
your frequent female customers.
6 Discussions
In our preliminary experiments, we have also ex-
perimented with the setup where the two keywords
?starbucks? and ?dunkin? were removed from the
list of features. This change resulted in a uniformed
2% decrease in performances across all the models
in Table 2, which did not affect the comparisons.
1333
However, we kept these two keywords in our final
experiments, because the reviewers sometimes men-
tion ?Dunkin? in Starbucks reviews, and vice versa.
Removing the two keywords could be problematic,
since it changes the natural distribution of the data.
Regarding the alternative problem setups, our pre-
liminary experiments showed that instead of using
one-vs-all binary classifiers, a direct 9-way multi-
class classification of joint brand-satisfaction labels
using logistic regression only resulted an accuracy
of 62%. We also did not adopt the hierarchical clas-
sification pipeline, where instead of performing joint
classification, multiple layers of classifiers could be
trained to classify brand, gender, and satisfaction la-
bels incrementally. This is because the hierarchical
classifiers suffered from the error propagation prob-
lem, and the second/third layer classifier could not
correct the errors from the previous layers (Bennett
and Nguyen, 2009).
Our proposed method to generate inter-feature
affinity matrix captures interesting dependency of
features in this dataset. For example, although the
words ?frappuccino? and ?slurping?, ?furniture? and
?mismatched? are semantically very different, our
method actually group them together due to the sub-
tle interactions of these word pairs in our tasks. The
example in Figure 1 is also very specific to our
dataset. This is very useful, because the word se-
mantic similarity might be context-dependent, and
our method learns and adapts the semantic similar-
ity on the fly, hinges on the particular training set.
On the other end of the spectrum, even though our
method is desirable in our task, one might need to be
cautious when working on very small data sets with
only a handful of samples. This is because small
samples typically have large variances in feature dis-
tributions, and that the generated Laplacian matrix
might not be as reliable as in our study. To alleviate
this potential problem, one might consider building
the Laplacian matrix using external resources such
as WordNet or FrameNet, even though this approach
could also introduce biases due to the mismatched
task domains.
We also observed that the accuracy of the auto-
matic brand identification task was high, indicating
the promising future of CBA for hidden brand infor-
mation from other genres of text over the Web. Al-
though the performances of joint brand-satisfaction
and joint brand-gender-satisfaction predictions are
relatively lower, there is still much room for im-
provements: for example, using the syntactic, se-
mantic, and meta-data features could potentially en-
rich the proposed model. Also, it is possible to con-
sider the higher order n-gram features for better ex-
ploratory data analysis. However, since the focus of
this paper is a proof of concept for Laplacian struc-
tured sparsity models and computational branding
analytics, we have not yet explored various multi-
view representations to augment our model.
Why does Laplacian structured sparsity model
work better in these classication tasks? Similar to
the application in image classifcation (Gao et al,
2010), one advantage of Laplacian regularization in
text classification is that our model can explicitly
model the dependency of local features. Another
reason is the expressiveness of our model: our model
allows one to express the feature interactions in a
structured manner. Thirdly, by embedding the struc-
ture in the regularization term, our model is more
flexible: one can now control the structured penalty
by tuning the regularization parameter on the devel-
opment set.
7 Conclusions
We introduce a Laplacian structured sparsity model
for computational branding analytics (CBA). In the
automatic brand identification, our model achieves
the best result, dominating many competitive base-
lines. We also introduce the tasks of joint brand-
satisfaction and brand-gender-satisfaction predic-
tions, and show that the Laplacian structured spar-
sity do well in these tasks. A closer evaluation that
varying the levels of sparsity in a L1 constrained
problem also indicates that the Laplacian augmented
L1-ball projection model can provide state-of-the-
art results. By examining the weights of the de-
rived Laplacian structured sparsity model, interest-
ing indicators of brands and theirs gender-specific
customer satisfaction associations are also discov-
ered. In the future, we would like to investigate other
methods for generating robust inter-feature Lapla-
cians that include deep syntactic and semantic fea-
tures.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for valuable comments.
1334
References
M. Belkin and P. Niyogi. 2001. Laplacian eigenmaps
and spectral techniques for embedding and cluster-
ing. Advances in neural information processing sys-
tems (NIPS).
M. Belkin, P. Niyogi, and V. Sindhwani. 2006. Mani-
fold regularization: A geometric framework for learn-
ing from labeled and unlabeled examples. The Journal
of Machine Learning Research (JMLR).
P. N. Bennett and N. Nguyen. 2009. Refined experts:
improving classification in large taxonomies. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval.
K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft.
1999. When is nearest neighbor meaningful? Pro-
ceedings of the International Conference on Database
Theory (ICDT).
F. Biadsy, W.Y. Wang, A. Rosenberg, and J. Hirschberg.
2011. Intoxication detection using phonetic, phono-
tactic and prosodic cues. In Proceedings of the 12th
Annual Conference of the International Speech Com-
munication Association (Interspeech 2011).
V. Chahuneau, K. Gimpel, B.R. Routledge, L. Scherlis,
and N.A. Smith. 2012. Word salad: Relating food
prices and descriptions.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library for
support vector machines. ACM Transactions on Intel-
ligent Systems and Technology (TIST).
W. Y. Chen, Y. Song, H. Bai, C. J. Lin, and E. Y. Chang.
2011. Parallel spectral clustering in distributed sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI).
Y. N. Chen, W. Y. Wang, and A. I. Rudnicky. 2013.
An empirical investigation of sparse log-linear models
for improved dialogue act classification. In Proceed-
ings of the 38th International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2013).
W. W. Cohen. 2012. Learning similarity measures based
on random walks. In Procceedings of the 21nd ACM
International Conference on Information and Knowl-
edge Management (CIKM).
L. DiCarlo. 2004. Dunkin? donuts vs. starbucks. In
Forbes.com - Monday Matchup.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the l1-ball for learn-
ing in high dimensions. In Proceedings of the 25th in-
ternational conference on Machine learning (ICML).
J. Eisenstein, A. Ahmed, and E. Xing. 2011a. Sparse
additive generative models of text. Proceedings of the
28th International Conference on Machine Learning
(ICML 2011).
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011b. Dis-
covering sociolinguistic associations with structured
sparsity. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT).
S. Gao, I. W. Tsang, L. T. Chia, and P. Zhao. 2010. Local
features are not lonely?laplacian sparse coding for im-
age classification. In 2010 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR).
S. Gao, I. Tsang, and L. Chia. 2012. Laplacian sparse
coding, hypergraph laplacian sparse coding, and ap-
plications. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI).
P. M. Guadagni and J. D. C. Little. 1983. A logit model
of brand choice calibrated on scanner data. Marketing
science.
M. K. Kim, K. Lopetcharat, and M. A. Drake. 2013. In-
fluence of packaging information on consumer liking
of chocolate milk. Journal of dairy science.
A.A. Kuehn. 1962. Consumer brand choice as a learning
process.
S. Le Cessie and JC Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied statistics.
Chris Lederer and Sam Hill. 2001. See your brands
through your customers eyes. Harvard Business Re-
view, 79(6):125?133.
S.I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. 2006. Effi-
cient l1 regularized logistic regression. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI).
D.C. Liu and J. Nocedal. 1989. On the limited memory
bfgs method for large scale optimization. Mathemati-
cal programming.
D. Luo, H.G. Hosseini, and J.R. Stewart. 2004. Appli-
cation of ann with extracted parameters from an elec-
tronic nose in cigarette brand identification. Sensors
and Actuators B: Chemical.
J. C. Makens. 1965. Effect of brand preference upon
consumers perceived taste of turkey meat. Journal of
Applied Psychology.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Structured sparsity in
structured prediction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Y. Moon and J. Quelch. 2006. Starbucks: delivering
customer service. Harvard Business School.
M. OConnell and G. Gooding. 2006. The use of first
names to evaluate reports of gender and its effect
on the distribution of married and unmarried couple
households. In Proceedings of the Annual Meetings of
the Population Association of America.
S. Ovide. 2011. Face off! dunkin? donuts vs. starbucks.
In Deal Journal - Wall Street Journal Blogs.
1335
M. Paul and M. Dredze. 2012. Factorial lda: Sparse
multi-dimensional text models. In Advances in Neural
Information Processing Systems (NIPS).
F. Pelisson, D. Hall, O. Riff, and J. Crowley. 2003. Brand
identification using gaussian derivative histograms.
Computer Vision Systems.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-
all classification. The Journal of Machine Learning
Research (JMLR).
M. Schmidt, G. Fung, and R. Rosales. 2007. Fast opti-
mization methods for l1 regularization: A comparative
study and two new approaches. Machine Learning.
M. Schmidt, E. Van Den Berg, M. Friedlander, and
K. Murphy. 2009. Optimizing costly functions
with simple constraints: A limited-memory projected
quasi-newton algorithm. In Proceedings of Confer-
ence on Artificial Intelligence and Statistics (AIStats).
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological).
W. Y. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
learning. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue (SIGDIAL 2011).
W. Y. Wang, S. Finkelstein, A. Ogan, A. W. Black, and
J. Cassell. 2012a. ?love ya, jerkface?: using sparse
log-linear models to build positive (and impolite) re-
lationships with teens. In Proceedings of the 13th
annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL 2012).
W. Y. Wang, E. Mayfield, S. Naidu, and J. Dittmar.
2012b. Historical analysis of legal opinions with a
sparse mixed-effects latent variable model. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
K. Q. Weinberger, F. Sha, Q. Zhu, and L. K. Saul. 2007.
Graph laplacian regularization for large-scale semidef-
inite programming. Advances in neural information
processing systems (NIPS).
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology).
1336
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1152?1158,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency Parsing for Weibo:
An Efficient Probabilistic Logic Programming Approach
William Yang Wang, Lingpeng Kong, Kathryn Mazaitis, William W. Cohen
Language Technologies Institute & Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA.
{yww,lingpenk,krivard,wcohen}@cs.cmu.edu
Abstract
Dependency parsing is a core task in NLP,
and it is widely used by many applica-
tions such as information extraction, ques-
tion answering, and machine translation.
In the era of social media, a big chal-
lenge is that parsers trained on traditional
newswire corpora typically suffer from the
domain mismatch issue, and thus perform
poorly on social media data. We present a
new GFL/FUDG-annotated Chinese tree-
bank with more than 18K tokens from Sina
Weibo (the Chinese equivalent of Twit-
ter). We formulate the dependency pars-
ing problem as many small and paralleliz-
able arc prediction tasks: for each task,
we use a programmable probabilistic first-
order logic to infer the dependency arc of a
token in the sentence. In experiments, we
show that the proposed model outperforms
an off-the-shelf Stanford Chinese parser,
as well as a strong MaltParser baseline that
is trained on the same in-domain data.
1 Introduction
Weibo, in particular Sina Weibo
1
, has attracted
more than 30% of Internet users (Yang et al.,
2012), making it one of the most popular social
media services in the world. While Weibo posts
are abundantly available, NLP techniques for ana-
lyzing Weibo posts have not been well-studied in
the past.
Syntactic analysis of Weibo is made difficult
for three reasons: first, in the last few decades,
Computational Linguistics researchers have pri-
marily focused on building resources and tools us-
ing standard English newswire corpora
2
, and thus,
1
http://en.wikipedia.org/wiki/Sina Weibo
2
For example, Wall Street Journal articles are used for
building the Penn Treebank (Marcus et al., 1993).
there are fewer resources in other languages in
general. Second, microblog posts are typically
short, noisy (Gimpel et al., 2011), and can be
considered as a ?dialect?, which is very differ-
ent from news data. Due to the differences in
genre, part-of-speech taggers and parsers trained
on newswire corpora typically fail on social media
texts. Third, most existing parsers use language-
independent standard features (McDonald et al.,
2005), and these features may not be optimal for
Chinese (Martins, 2012). To most of the applica-
tion developers, the parser is more like a blackbox,
which is not directly programmable. Therefore,
it is non-trivial to adapt these generic parsers to
language-specific social media text.
In this paper, we present a new probabilistic de-
pendency parsing approach for Weibo, with the
following contributions:
? We present a freely available Chinese Weibo
dependency treebank
3
, manually annotated
with more than 18,000 tokens;
? We introduce a novel probabilistic logic
programming approach for dependency arc
prediction, making the parser directly pro-
grammable for theory engineering;
? We show that the proposed approach outper-
forms an off-the-shelf dependency parser, as
well as a strong baseline trained on the same
in-domain data.
In the next section, we describe existing work
on dependency parsing for Chinese. In Section 3,
we present the new Chinese Weibo Treebank to
the research community. In Section 4, we intro-
duce the proposed efficient probabilistic program-
ming approach for parsing Weibo. We show the
experimental results in Section 5, and conclude in
Section 6.
3
http://www.cs.cmu.edu/?yww/data/WeiboTreebank.zip
1152
2 Related Work
Chinese dependency parsing has attracted many
interests in the last fifteen years. Bikel and Chi-
ang (2000; 2002) are among the first to use Penn
Chinese Tree Bank for dependency parsing, where
they adapted Xia?s head rules (Xia, 1999). An im-
portant milestone for Chinese dependency pars-
ing is that, a few years later, the CoNLL shared
task launched a track for multilingual dependency
parsing, which also included Chinese (Buchholz
and Marsi, 2006; Nilsson et al., 2007). These
shared tasks soon popularized Chinese depen-
dency parsing by making datasets available, and
there has been growing amount of literature since
then (Zhang and Clark, 2008; Nivre et al., 2007;
Sagae and Tsujii, 2007; Che et al., 2010; Carreras,
2007; Duan et al., 2007).
Besides the CoNLL shared tasks, there are also
many interesting studies on Chinese dependency
parsing. For example, researchers have studied
case (Yu et al., 2008) and morphological (Li and
Zhou, 2012) structures for learning a Chinese de-
pendency parser. Another direction is to perform
joint learning and inference for POS tagging and
dependency parsing (Li et al., 2011; Hatori et al.,
2011; Li et al., 2011; Ma et al., 2012). In recent
years, there has been growing interests in depen-
dency arc prediction in Chinese (Che et al., 2014),
and researchers have also investigated character-
level Chinese dependency parsing (Zhang et al.,
2014). However, even though the above methods
all have merits, the results are reported only on
standard newswire based Chinese Treebank (e.g.
from People?s Daily (Liu et al., 2006)), and it is
unclear how they would perform on Weibo data.
To the best of our knowledge, together with the
recent study on parsing tweets (Kong et al., 2014),
we are among the first to study the problem of de-
pendency parsing for social media text.
3 The Chinese Weibo Treebank
We use the publicly available ?topia dataset (Ling
et al., 2013) for dependency annotation. An in-
teresting aspect of this Weibo dataset is that, be-
sides the Chinese posts, it also includes a copy of
the English translations. This allows us to observe
some interesting phenomena that mark the differ-
ences of the two languages. For example:
? Function words are more frequently used in
English than in Chinese. When examin-
Figure 1: An example of pro-drop phenomenon
from the Weibo data.
ing this English version of the Weibo cor-
pus for the total counts of the word ?the?,
there are 2,084 occurrences in 2,003 sen-
tences. Whereas in Chinese, there are only
52 occurrences of the word ?the? out of the
2,003 sentences.
? The other interesting thing is the position of
the head. In English, the head of the tree
occurs more frequent on the left-to-middle
of the sentence, while the distribution of the
head is more complicated in Chinese. This is
also verified from the parallel Weibo data.
? Another well-known issue in Chinese is that
Chinese is a pro-drop topical language. This
is extremely prominent in the short text,
which clearly creates a problem for parsing.
For example, in the Chinese Weibo data, we
have observed the sentence in Figure 1.
To facilitate the annotation process, we first
preprocess the Weibo posts using the Stanford
NLP pipeline, including a Chinese Word Seg-
menter (Tseng et al., 2005) and a Chinese Part-
of-Speech tagger (Toutanova and Manning, 2000).
Two native speakers of Chinese with strong lin-
guistic backgrounds have annotated the depen-
dency relations from 1,000 posts of the ?topia
dataset, using the FUDG (Schneider et al., 2013)
and GFL annotation tool (Mordowanec et al.,
2014). The annotators communicate regularly dur-
ing the annotation process, and a coding man-
ual that relies majorly on the Stanford Dependen-
cies (Chang et al., 2009) is designed. The anno-
tation process has two stages: in the first stage,
we rely on the word segmentation produced by
the segmenter, and produce a draft version of the
treebank; in the second stage, the annotators ac-
tively discuss the difficult cases to reach agree-
ments, manually correct the mis-segmented word
tokens, and revise the annotations of the tricky
cases. The final inter-annotator agreement rate on
a randomly-selected subset of 373 tokens in this
1153
treebank is 82.31%.
Fragmentary Unlabeled Dependency Grammar
(FUDG) is a newly proposed flexible framework
that offers a relative easy way to annotate the syn-
tactic structure of text. Beyond the traditional tree
view of dependency syntax in which the tokens
of a sentence form nodes in a tree, FUDG also
allows the annotation of additional lexical items
such as multiword expressions. It provides special
devices for coordination and coreference; and fa-
cilitates underspecified (partial) annotations where
producing a complete parse would be difficult.
Graph Fragment Language (GFL) is an implemen-
tation of unlabeled dependency annotations in the
FUDG framework, which fully supports Chinese,
English and other languages. The training set of
our Chinese Weibo Treebank
4
includes 14,774 to-
kens, while the development and test sets include
1,846 and 1,857 tokens respectively.
4 A Programmable Parser with
Personalized PageRank Inference
A key problem in multilingual dependency parsing
is that generic feature templates may not work well
for every language. For example, Martins (2012)
shows that for Chinese dependency parsing, when
adding the generic grandparents and siblings fea-
tures, the performance was worse than using the
standard bilexical, unilexical, and part-of-speech
features. Unfortunately, for many parsers such
as Stanford Chinese Parser (Levy and Manning,
2003) and MaltParser (Nivre et al., 2007), it is
very difficult for programmers to specify the fea-
ture templates and inference rules for dependency
arc prediction.
In this work, we present a Chinese dependency
parsing method for Weibo, based on efficient prob-
abilistic first-order logic programming (Wang et
al., 2013). The advantage of probabilistic pro-
gramming for parsing is that, software engineers
can simply conduct theory engineering, and op-
timize the performance of the parser for a spe-
cific genre of the target language. Recently, proba-
bilistic programming approaches (Goodman et al.,
2012; Wang et al., 2013; Lloyd et al., 2014) have
demonstrated its efficiency and effectiveness in
many areas such as information extraction (Wang
et al., 2014), entity linking, and text classifica-
tion (Wang et al., 2013).
4
The corpus is freely available for download at the URL
specified in Section 1.
Algorithm 1 A Dependency Arc Inference Algo-
rithm for Parsing Weibo
Given:
(1) a sentence with tokens T
i
, where i is the in-
dex, and L is the length;
(2) a databaseD of token relations from the cor-
pus;
(3) first-order logic inference rule set R.
for i = 1? L tokens do
S? ConstructSearchSpace(T
i
, R,D);
~
P
i
? InferParentUsingProPPR(T
i
,S);
end for
Greedy Global Inference
for i = 1? L tokens do
Y
i
= arg max
~
P
i
;
end for
4.1 Problem Formulation
We formulate the dependency parsing prob-
lem as many small dependency arc prediction
problems. For each token, we form the par-
ent inference problem of a token T
i
as solving a
query edge(T
i
, ?) using stochastic theorem prov-
ing on a search graph. Our approach relies on a
database D of inter-token relations. To construct
the database, we automatically extract the token
relations from the text data. For example, to de-
note the adjacency of two tokens T
1
and T
2
, we
store the entry adjacent(T
1
, T
2
) in D. One can
also store the part-of-speech tag of a token in the
form haspos(T
1
, DT ). There is no limitations
on the arity and the types of the predicates in the
database.
Given the database of token relations, one then
needs to construct the first-order logic inference
theory R for predicting dependency arcs. For ex-
ample, to construct simple bilexical and bi-POS
inference rules to model the dependency of an ad-
jacent head and a modifier, one can write first-
order clauses such as:
edge(V1,V2) :-
adjacent(V1,V2),hasword(V1,W1),
hasword(V2,W2),keyword(W1,W2) #adjWord.
edge(V1,V2) :-
adjacent(V1,V2),haspos(V1,W1),
haspos(V2,W2),keypos(W1,W2) #adjPos.
keyword(W1,W2) :- # kw(W1,W2).
keypos(W1,W2) :- # kp(W1,W2).
1154
Figure 2: After mapping the database D to theory R, here is an example of search space for dependency
arc inference. The query is edge(S
1
T
5
, X), and there exists one correct and multiple incorrect solutions
(highlighted in bold).
Here, we associate a feature vector ?
c
with each
clause, which is annotated using the # symbol af-
ter each clause in the theory set. Note that the last
two (keyword and keypos) clauses are feature tem-
plates that allow us to learn the specific bi-POS
tags and bilexical words from the data. In order
for one to solve the query edge(T
i
, ?), we first
need to map the entities from D to R to construct
the search space. The details for constructing and
searching in the graph can be found in previous
studies on probabilistic first-order logic (Wang et
al., 2013) and stochastic logic programs (Cussens,
2001). An example search space is illustrated in
Figure 2. Note that now the edges in the search
graph correspond to the feature vector ?
c
in R.
The overall dependency arc inference algorithm
can be found in Algorithm 1. For each of the par-
ent inference subtask, we use ProPPR (Wang et al.,
2013) to perform efficient personalized PageRank
inference. Note that to ensure the validity of the
dependency tree, we break the loops in the final
parse graph into a parse tree using the maximum
personalized PageRank score criteria. When mul-
tiple roots are predicted, we also select the most
likely root by comparing the personalized PageR-
ank solution scores.
To learn the more plausible theories, one needs
to upweight weights for relevant features, so
that they have higher transition probabilities on
the corresponding edges. To do this, we use
stochastic gradient descent to learn from training
queries, where the correct and incorrect solutions
are known. The details of the learning algorithm
are described in the last part of this section.
4.2 Personalized PageRank Inference
For the inference of the parent of each token, we
utilize ProPPR (Wang et al., 2013). ProPPR al-
lows a fast approximate proof procedure, in which
only a small subset of the full proof graph is
generated. In particular, if ? upper-bounds the
reset probability, and d upperbounds the degree
of nodes in the graph, then one can efficiently
find a subgraph with O(
1
?
) nodes which approx-
imates the weight for every node within an er-
ror of d (Wang et al., 2013), using a variant of
the PageRank-Nibble algorithm of Andersen et al
(2008).
4.3 Parameter Estimation
Our parameter learning algorithm is implemented
using a parallel stochastic gradient descent vari-
ant to optimize the log loss using the supervised
personalized PageRank algorithm (Backstrom and
1155
Method Dev. Test
Stanford Parser (Xinhua) 0.507 0.489
Stanford Parser (Chinese) 0.597 0.581
MaltParser (Full) 0.669 0.654
Our methods ? ProPPR
ReLU (Bi-POS) 0.506 0.517
ReLU (Bilexical) 0.635 0.616
ReLU (Full) 0.668 0.666
Truncated tanh (Bi-POS) 0.601 0.594
Truncated tanh (Bilexical) 0.650 0.634
Truncated tanh (Full) 0.667 0.675*
Table 1: Comparing our Weibo parser to other
baselines (UAS). The off-the-shelf Stanford parser
uses its attached Xinhua and Chinese factored
models, which are trained on external Chinese
treebank of newswire data. MaltParser was trained
on the same in-domain data as our proposed ap-
proach. * indicates p < .001 comparing to the
MaltParser.
Leskovec, 2011). The idea is that, given the
training queries, we perform a random walk with
restart process, and upweight the edges that are
more likely to end up with a known correct parent.
We learn the transition probability from two nodes
(u, v) in the search graph using: Pr
w
(v|u) =
1
Z
f(w,?
c
restart
), where we use two popular non-
linear parameter learning functions from the deep
learning community:
? Rectified Linear Unit (ReLU) (Nair and Hin-
ton, 2010): max(0, x);
? The Hyperbolic Function (Glorot and Ben-
gio, 2010): tanh(x).
as the f in this study. ReLU is a desirable
non-linear function, because it does not have the
vanishing gradient problem, and produces sparse
weights. For the weights learned from tanh(x),
we truncate the negative weights on the edges,
since the default weight on the feature edges is
w = 1.0 (existence), and w = 0.0 means that the
edge does not exist in the inference stage.
5 Experiments
In this experiment, we compare the proposed
parser with two well-known baselines. First,
we compare with an off-the-shelf Stanford Chi-
nese Parser (Levy and Manning, 2003). Second,
we compare with the MaltParser (Nivre et al.,
2007) that is trained on the same in-domain Weibo
dataset. The train, development, and test splits are
described in Section 3. We tune the regulariza-
tion hyperparameters of the models on the dev. set,
and report Unlabeled Attachment Score (UAS) re-
sults for both the dev. set and the hold-out test set.
We experiment with the bilexical and bi-POS first-
order logic theory separately, as well as a com-
bined full model with directional and distance fea-
tures.
The results are shown in Table 1. We see that
both of the two attached pre-trained models from
the Stanford parser do not perform very well on
this Weibo dataset, probably because of the mis-
matched training and test data. MaltParser is
widely considered as one of the most popular de-
pendency parsers, not only because of its speed,
but also the acclaimed accuracy. We see that when
using the full model, the UAS results between our
methods and MaltParser are very similar on the de-
velopment set, but both of our approaches outper-
form the Maltparser in the holdout test set. The
truncated tanh variant of ProPPR obtains the best
UAS score of 0.675.
6 Conclusion
In this paper, we present a novel Chinese de-
pendency treebank, annotated using Weibo data.
We introduce a probabilistic programming depen-
dency arc prediction approach, where theory en-
gineering is made easy. In experiments, we show
that our methods outperform an off-the-shelf Stan-
ford Chinese Parser, as well a strong MaltParser
that is trained on the same in-domain data. The
Chinese Weibo Treebank is made freely available
to the research community. In the future, we plan
to apply the proposed approaches to dependency
and semantic parsing of other languages.
Acknowledgements
We are grateful to anonymous reviewers for useful
comments. This research was supported in part
by DARPA grant FA8750-12-2-0342 funded un-
der the DEFT program, and a Google Research
Award. The authors are solely responsible for the
contents of the paper, and the opinions expressed
in this publication do not reflect those of the fund-
ing agencies.
1156
References
Reid Andersen, Fan R. K. Chung, and Kevin J. Lang.
2008. Local partitioning for directed graphs using
pagerank. Internet Mathematics, 5(1):3?22.
Lars Backstrom and Jure Leskovec. 2011. Supervised
random walks: predicting and recommending links
in social networks. In Proceedings of the fourth
ACM international conference on Web search and
data mining, pages 635?644. ACM.
Daniel M Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 12, pages 1?6. Associa-
tion for Computational Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL, pages 957?961.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59. Association for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
13?16. Association for Computational Linguistics.
Wanxiang Che, Jiang Guo, and Ting Liu. 2014. Re-
liable dependency arc recognition. Expert Systems
with Applications, 41(4):1716?1722.
David Chiang and Daniel M. Bikel. 2002. Recover-
ing latent information in treebanks. In Proceedings
of the 19th International Conference on Computa-
tional Linguistics - Volume 1, COLING ?02, pages
1?7, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
James Cussens. 2001. Parameter estimation in
stochastic logic programs. Machine Learning,
44(3):245?271.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In EMNLP-CoNLL, pages 940?
946.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42?47. Association for Computa-
tional Linguistics.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International Conference on Artificial
Intelligence and Statistics, pages 249?256.
Noah Goodman, Vikash Mansinghka, Daniel Roy,
Keith Bonawitz, and Daniel Tarlow. 2012. Church:
a language for generative models. arXiv preprint
arXiv:1206.3255.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tag-
ging and dependency parsing in chinese. In IJC-
NLP, pages 1216?1224.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2014), Doha, Qatar, October. ACL.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439?446. Association for Computational Lin-
guistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454. Association for
Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,
Wenliang Chen, and Haizhou Li. 2011. Joint mod-
els for chinese pos tagging and dependency pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1180?1191. Association for Computational Linguis-
tics.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting
on Association for Computational Linguistics, ACL
?13. Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Build-
ing a dependency treebank for improving chinese
parser. Journal of Chinese Language and Comput-
ing, 16(4):207?224.
1157
James Robert Lloyd, David Duvenaud, Roger Grosse,
Joshua B Tenenbaum, and Zoubin Ghahramani.
2014. Automatic construction and natural-language
description of nonparametric regression models.
arXiv preprint arXiv:1402.4304.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren. 2012.
Easy-first chinese pos tagging and dependency pars-
ing. In COLING, pages 1731?1746.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313?330.
Andr?e Filipe Torres Martins. 2012. The Geometry of
Constrained Structured Prediction: Applications to
Inference and Learning of Natural Language Syn-
tax. Ph.D. thesis, Columbia University.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Michael T. Mordowanec, Nathan Schneider, Chris
Dyer, and Noah A. Smith. 2014. Simplified de-
pendency annotations with gfl-web. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations.
ACL.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807?814.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The conll 2007 shared task on dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL, pages 915?932. sn.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kenji Sagae and Jun?ichi Tsujii. 2007. Depen-
dency parsing and domain adaptation with lr models
and parser ensembles. In EMNLP-CoNLL, volume
2007, pages 1044?1050.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63?70. Association for
Computational Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2013. Programming with personalized
pagerank: a locally groundable first-order proba-
bilistic logic. In Proceedings of the 22nd ACM in-
ternational conference on Conference on informa-
tion & knowledge management, pages 2129?2138.
ACM.
William Yang Wang, Kathryn Mazaitis, Ni Lao, Tom
Mitchell, and William W Cohen. 2014. Effi-
cient inference and learning in a large knowledge
base: Reasoning with extracted information using
a locally groundable first-order probabilistic logic.
arXiv preprint arXiv:1404.3301.
Fei Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proceedings of the 5th
Natural Language Processing Pacific Rim Sympo-
sium (NLPRS-99), pages 398?403.
Fan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.
Automatic detection of rumor on sina weibo. In Pro-
ceedings of the ACM SIGKDD Workshop on Mining
Data Semantics, page 13. ACM.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large scale
automatically constructed case structures. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 1049?
1056. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571. Association for Computa-
tional Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Character-level chinese dependency
parsing. In Proceedings of the 52th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2014), Baltimore, MD, USA, June. ACL.
1158
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 740?749,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Historical Analysis of Legal Opinions
with a Sparse Mixed-Effects Latent Variable Model
William Yang Wang1 and Elijah Mayfield1 and Suresh Naidu2 and Jeremiah Dittmar3
1School of Computer Science, Carnegie Mellon University
2Department of Economics and SIPA, Columbia University
3American University and School of Social Science, Institute for Advanced Study
{ww,elijah}@cmu.edu sn2430@columbia.edu dittmar@american.edu
Abstract
We propose a latent variable model to enhance
historical analysis of large corpora. This work
extends prior work in topic modelling by in-
corporating metadata, and the interactions be-
tween the components in metadata, in a gen-
eral way. To test this, we collect a corpus
of slavery-related United States property law
judgements sampled from the years 1730 to
1866. We study the language use in these
legal cases, with a special focus on shifts in
opinions on controversial topics across differ-
ent regions. Because this is a longitudinal
data set, we are also interested in understand-
ing how these opinions change over the course
of decades. We show that the joint learning
scheme of our sparse mixed-effects model im-
proves on other state-of-the-art generative and
discriminative models on the region and time
period identification tasks. Experiments show
that our sparse mixed-effects model is more
accurate quantitatively and qualitatively inter-
esting, and that these improvements are robust
across different parameter settings.
1 Introduction
Many scientific subjects, such as psychology, learn-
ing sciences, and biology, have adopted computa-
tional approaches to discover latent patterns in large
scale datasets (Chen and Lombardi, 2010; Baker and
Yacef, 2009). In contrast, the primary methods for
historical research still rely on individual judgement
and reading primary and secondary sources, which
are time consuming and expensive. Furthermore,
traditional human-based methods might have good
precision when searching for relevant information,
but suffer from low recall. Even when language
technologies have been applied to historical prob-
lems, their focus has often been on information re-
trieval (Gotscharek et al, 2009), to improve acces-
sibility of texts. Empirical methods for analysis and
interpretation of these texts is therefore a burgeoning
new field.
Court opinions form one of the most important
parts of the legal domain, and can serve as an excel-
lent resource to understand both legal and political
history (Popkin, 2007). Historians often use court
opinions as a primary source for constructing in-
terpretations of the past. They not only report the
proceedings of a court, but also express a judges?
views toward the issues at hand in a case, and reflect
the legal and political environment of the region and
period. Since there exists many thousands of early
court opinions, however, it is difficult for legal his-
torians to manually analyze the documents case by
case. Instead, historians often restrict themselves to
discussing a relatively small subset of legal opinions
that are considered decisive. While this approach
has merit, new technologies should allow extraction
of patterns from large samples of opinions.
Latent variable models, such as latent Dirichlet al
location (LDA) (Blei et al, 2003) and probabilistic
latent semantic analysis (PLSA) (Hofmann, 1999),
have been used in the past to facilitate social science
research. However, they have numerous drawbacks,
as many topics are uninterpretable, overwhelmed by
uninformative words, or represent background lan-
guage use that is unrelated to the dimensions of anal-
ysis that qualitative researchers are interested in.
SAGE (Eisenstein et al, 2011a), a recently pro-
posed sparse additive generative model of language,
addresses many of the drawbacks of LDA. SAGE
assumes a background distribution of language use,
and enforces sparsity in individual topics. Another
advantage, from a social science perspective, is that
SAGE can be derived from a standard logit random-
utility model of judicial opinion writing, in contrast
to LDA. In this work we extend SAGE to the su-
pervised case of joint region and time period pre-
diction. We formulate the resulting sparse mixed-
effects (SME) model as being made up of mixed
effects that not only contain random effects from
sparse topics, but also mixed effects from available
metadata. To do this we augment SAGE with two
sparse latent variables that model the region and
time of a document, as well as a third sparse latent
740
variable that captures the interactions among the re-
gion, time and topic latent variables. We also intro-
duce a multiclass perceptron-style weight estimation
method to model the contributions from different
sparse latent variables to the word posterior prob-
abilities in this predictive task. Importantly, the re-
sulting distributions are still sparse and can therefore
be qualitatively analyzed by experts with relatively
little noise.
In the next two sections, we overview work re-
lated to qualitative social science analysis using la-
tent variable models, and introduce our slavery-
related early United States court opinion data. We
describe our sparse mixed-effects model for joint
modeling of region, time, and topic in section 4.
Experiments are presented in section 5, with a ro-
bust analysis from qualitative and quantitative stand-
points in section 5.2, and we discuss the conclusions
of this work in section 6.
2 Related Work
Natural Language Processing (NLP) methods for
automatically understanding and identifying key
information in historical data have not yet been
explored until recently. Related research efforts
include using the LDA model for topic model-
ing in historical newspapers (Yang et al, 2011),
a rule-based approach to extract verbs in histor-
ical Swedish texts (Pettersson and Nivre, 2011),
a system for semantic tagging of historical Dutch
archives (Cybulska and Vossen, 2011).
Despite our historical data domain, our approach
is more relevant to text classification and topic mod-
elling. Traditional discriminative methods, such as
support vector machine (SVM) and logistic regres-
sion, have been very popular in various text cate-
gorization tasks (Joachims, 1998; Wang and McKe-
own, 2010) in the past decades. However, the main
problem with these methods is that although they are
accurate in classifying documents, they do not aim
at helping us to understand the documents.
Another problem is lack of expressiveness. For
example, SVM does not have latent variables to
model the subtle differences and interactions of fea-
tures from different domains (e.g. text, links, and
date), but rather treats them as a ?bag-of-features?.
Generative methods, by contrast, can show the
causes to effects, have attracted attentions in re-
cent years due to the rich expressiveness of the
models and competitive performances in predictive
tasks (Wang et al, 2011). For example, Nguyen et
al. (2010) study the effect of the context of inter-
action in blogs using a standard LDA model. Guo
and Diab (2011) show the effectiveness of using se-
mantic information in multifaceted topic models for
text categorization. Eisenstein et al (2010) use a
latent variable model to predict geolocation infor-
mation of Twitter users, and investigate geographic
variations of language use. Temporally, topic mod-
els have been used to show the shift in language use
over time in online communities (Nguyen and Rose?,
2011) and the evolution of topics over time (Shub-
hankar et al, 2011).
When evaluating understandability, however,
dense word distributions are a serious issue in many
topic models as well as other predictive tasks. Such
topic models are often dominated by function words
and do not always effectively separate topics. Re-
cent work have shown significant gains in both pre-
dictiveness and interpretatibility by enforcing spar-
sity, such as in the task of discovering sociolinguistic
patterns of language use (Eisenstein et al, 2011b).
Our proposed sparse mixed-effects model bal-
ances the pros and cons the above methods, aim-
ing at higher classification accuracies using the SME
model for joint geographic and temporal aspects pre-
diction, as well as richer interaction of components
from metadata to enhance historical analysis in legal
opinions. To the best of our knowledge, this study is
the first of its kind to discover region and time spe-
cific topical patterns jointly in historical texts.
3 Data
We have collected a corpus of slavery-related United
States supreme court legal opinions from Lexis
Nexis. The dataset includes 5,240 slavery-related
state supreme court cases from 24 states, during the
period of 1730 - 1866. Optical character recognition
(OCR) software was used by Lexis Nexis to digitize
the original documents. In our region identification
task, we wish to identify whether an opinion was
written in a free state1 (R1) or a slave state (R2)2.
In our time identification experiment, we approx-
imately divide the legal documents into four time
quartiles (Q1, Q2, Q3, and Q4), and predict which
quartile the testing document belongs to. Q1 con-
tains cases from 1837 or earlier, where as Q2 is for
1838-1848, Q3 is for 1849-1855, and Q4 is for 1856
and later.
4 The Sparse Mixed-Effects Model
To address the over-parameterization, lack of ex-
pressiveness and robustness issues in LDA, the
SAGE (Eisenstein et al, 2011a) framework draws a
1Including border states, this set includes CT, DE, IL, KY,
MA, MD, ME, MI, NH, NJ, NY, OH, PA, and RI.
2These states include AR, AL, FL, GA, MS, NC, TN, TX,
and VA.
741
Figure 1: Plate diagram representation of the proposed
Sparse Mixed-Effects model with K topics, Q time peri-
ods, and R regions.
constant background distribution m, and additively
models the sparse deviation ? from the background
in log-frequency space. It also incorporates latent
variables ? to model the variance for each sparse de-
viation ?. By enforcing sparsity, the model might be
less likely to overfit the training data, and requires
estimation of fewer parameters.
This paper further extends SAGE to analyze mul-
tiple facets of a document collection, such as the
regional and temporal differences. Figure 1 shows
the graphical model of our proposed sparse mixed-
effects (SME) model. In this SME model, we still
have the same Dirichlet ?, the latent topic proportion
?, and the latent topic variable z as the original LDA
model. For each document d, we are able to ob-
serve two labels: the region label y(R)d and the time
quartile label y(Q)d . We also have a background dis-
tributionm that is drawn from a uninformative prior.
The three major sparse deviation latent variables are
?(T )k for topics, ?
(R)
j for regions, and ?
(Q)
q for time
periods. All of the three latent variables are condi-
tioned on another three latent variables, which are
their corresponding variances ? (T )k , ?
(R)
j and ?
(Q)
q .
In the intersection of the plates for topics, regions,
and time quartiles, we include another sparse latent
variable ?(I)qjk, which is conditioned on a variance
? (I)qjk, to model the interactions among topic, region
and time. ?(I)qjk is the linear combination of time pe-
riod, region and topic sparse latent variables, which
absorbs the residual variation that is not captured in
the individual effects.
In contrast to traditional multinomial distribution
of words in LDA models, we approximate the con-
ditional word distribution in the document d as the
exponentiated sum ? of all latent sparse deviations
?(T )k , ?
(R)
j , ?
(Q)
q , and ?
(I)
qjk, as well as the background
m:
P (w(d)n |z
(d)
n , ?,m, y
(R)
d , y
(Q)
d ) ? ?
=exp
(
m+ ?(T )
z(d)n
+ ?(R)?(R)y(r)
+ ?(Q)?(Q)y(q) + ?
(I)
y(r),y(q),z(d)n
)
Despite SME learns in a Bayesian framework, the
above ?(R) and ?(Q) are dynamic parameters that
weight the contributions of ?(R)
y(r)
and ?(Q)
y(q)
to the
approximated word posterior probability. A zero-
mean Laplace prior ? , which is conditioned on pa-
rameter ?, is introduced to induce sparsity, where
its distribution is equivalent to the joint distribution,?
N (?;m, ?)?(? ;?)d? , and ?(? ;?)d? is the Expo-
nential distribution (Lange and Sinsheimer, 1993).
We first describe a generative story for this SME
model:
? Draw a background m from corpus mean and ini-
tialize ?(T ), ?(R), ?(Q) and ?(I) sparse deviations
from corpus
? For each topic k
? For each word i
? Draw ? (T )k,i ? ?(?)
? Draw ?(T )k,i ? N (0, ?
(T )
k,i )
? Set ?k ? exp(m+?k+?(R)?(R)+?(Q)?(Q)+
?(I))
? For each region j
? For each word i
? Draw ? (R)j,i ? ?(?)
? Draw ?(R)j,i ? N (0, ?
(R)
j,i )
? Update ?j ? exp(m + ?(R)?j + ?(T ) +
?(Q)?(Q) + ?(I))
? For each time quartile q
? For each word i
? Draw ? (Q)q,i ? ?(?)
? Draw ?(Q)q,i ? N (0, ?
(Q)
q,i )
? Update ?q ? exp(m + ?(Q)?q + ?(T ) +
?(R)?(R) + ?(I))
? For each time quartile q, for each region j, for each
topic k
? For each word i
? Draw ? (I)q,j,k,i ? ?(?)
? Draw ?(I)q,j,k,i ? N (0, ?
(I)
q,j,k,i)
? Update ?q,j,k ? exp(m + ?q,j,k + ?(T ) +
?(R)?(R) + ?(Q)?(Q))
742
? For each document d
? Draw the region label y(R)d
? Draw the time quartile label y(Q)d
? For each word n, draw w(d)n ? ?yd
4.1 Parameter Estimation
We follow the MAP estimation method that Eisen-
stein et al (2011a) used to train all sparse latent vari-
ables ?, and perform Bayesian inference on other la-
tent variables. The estimation of all variance vari-
ables ? remains as plugging the compound distri-
bution of Normal-Jeffrey?s prior, where the latter is
a replacement of the Exponential prior. When per-
forming Expectation-Maximization (EM) algorithm
to infer the latent variables in SME, we derive the
following likelihood function:
L =
?
d
?logP (?d|?)?+
?
logP (Z(d)n |?d)
?
+
Nd?
n
?
logP (w(d)n |z
(d)
n , ?,m, y
(R)
d , y
(Q)
d )
?
+
?
k
?logP (?(T )k |0, ?
(T )
k )?+
?
k
?logP (? (T )k |?)?
+
?
j
?logP (?(R)j |0, ?
(R)
j )?+
?
j
?logP (? (R)j |?)?
+
?
q
?logP (?(Q)q |0, ?
Q)
q )?+
?
q
?logP (? (Q)q |?)?
+
?
q
?
j
?
k
?logP (?(I)q,j,k|0, ?
(I)
q,j,k)?
+
?
q
?
j
?
k
?logP (? (I)q,j,k|?)?
?
?
logQ(?, z, ?)
?
The above E step likelihood score can be intuitively
interpreted as the sum of topic proportion scores, la-
tent topic scores, the word scores, the ? scores with
their priors, and minus the joint variance. In the M
step, when we use Newton?s method to optimize the
sparse deviation ?k parameter, we need to modify
the original likelihood function in SAGE and its cor-
responding first and second order derivatives when
deriving the gradient and Hessian matrix. The like-
lihood function for sparse topic deviation ?k is:
L(?k) = ?c
(T )
k ?T?k
? Cd log
?
q
?
j
?
i
exp(?(Q)?qi + ?
(R)?ji
+ ?ki + ?qjki +mi)? ?kTdiag(?(?
(T )
k )
?1?)?(T )k /2
and we can derive the gradient when taking the first
order partial derivative:
?L
??(T )k
=?c(T )k ? ?
?
q
?
j
?Cqjk??qjk
? diag(?(? (T )k )
?1?)?(T )k
where c(T )k is the true count, and ?qjk is the log
word likelihood in the original likelihood function.
Cqjk is the expected count from combinations of
time, region and topic.
?
q
?
j?Cqjk??qjk will then
be taken the second order derivative to form the Hes-
sian matrix, instead of ?Ck??k in the previous SAGE
setting.
To learn the weight parameters ?(R) and ?(Q),
we can approximate the weights using a multiclass
perceptron-style (Collins, 2002) learning method. If
we say that the notation of
?
V (R?) is to marginalize
out all other variables in ? except ?(R), and P (y(R)d )
is the prior for the region prediction task, we can pre-
dict the expected region value y?(R)d of a document d:
y?(R)d ? argmax
y?(R)d
exp
(?
V (R?) log ? + logP (y(R)d )
)
=argmax
y?(R)d
(
exp
(?
V (R?)
(
m+ ?(T )
z(d)n
+ ?(R)?(R)
y(R)d
+ ?(Q)?(Q)
y(Q)d
+ ?(I)
y(R)d ,y
(Q)
d ,z
(d)
n
))
P (y(R)d )
)
If the symbol ? is the hyperprior for the learning
rate and y?(R)d is the true label, the update procedure
for the weights becomes:
?(R
?)
d = ?
(R)
d + ?(y?
(R)
d ? y?
(R)
d )
Similarly, we derive the ?(Q) parameter using the
above formula. It is necessary to normalize the
weights in each EM loop to preserve the sparsity
property of latent variables. The weight update of
?(R) and ?(Q) is bound by the averaged accuracy
of the two classification tasks in the training data,
which is similar to the notion of minimizing empiri-
cal risk (Bahl et al, 1988). Our goal is to choose the
two weight parameters that minimize the empirical
classification error rate on training data when learn-
ing the word posterior probability.
5 Prediction Experiments
We perform three quantitative experiments to evalu-
ate the predictive power of the sparse mixed-effects
model. In these experiments, to predict the region
and time period labels of a given document, we
743
jointly learn the two labels in the SME model, and
choose the pair which maximizes the probability of
the document.
In the first experiment, we compare the prediction
accuracy of our SME model to a widely used dis-
criminative learner in NLP ? the linear kernel sup-
port vector machine (SVM)3. In the second experi-
ment, in addition to the linear kernel SVM, we also
compare our SME model to a state-of-the-art sparse
generative model of text (Eisenstein et al, 2011a),
and vary the size of input vocabulary W exponen-
tially from 29 to the full size of our training vocab-
ulary4. In the third experiment, we examine the ro-
bustness of our model by examining how the number
of topics influences the prediction accuracy when
varying the K from 10 to 50.
Our data consists of 4615 training documents and
625 held-out documents for testing. While individ-
ual judges wrote multiple opinions in our corpus,
no judges overlapped between training and test sets.
When measuring by the majority class in the testing
condition, the chance baseline for the region iden-
tification task is 57.1% and the time identification
task is 32.3%. We use three-fold cross-validation to
infer the learning rate ? and cost C hyperpriors in
the SME and SVM model respectively. We use the
paired student t-test to measure the statistical signif-
icance.
5.1 Quantitative Results
5.1.1 Comparing SME to SVM
We show in this section the predictive power of
our sparse mixed-effects model, comparing to a lin-
ear kernel SVM learner. To compare the two mod-
els in different settings, we first empirically set the
number of topics K in our SME model to be 25, as
this setting was shown to yield a promising result in
a previous study (Eisenstein et al, 2011a) on sparse
topic models. In terms of the size of vocabulary W
for both the SME and SVM learner, we select three
values to represent dense, medium or sparse feature
spaces: W1 = 29, W2 = 212, and the full vocabu-
lary size of W3 = 213.8. Table 1 shows the accuracy
of both models, as well as the relative improvement
(gain) of SME over SVM.
When looking at the experiment results under dif-
ferent settings, we see that the SME model always
outperforms the SVM learner. In the time quar-
tile prediction task, the advantage of SME model
3In our implementation, we use LibSVM (Chang and Lin,
2011).
4To select the vocabulary size W , we rank the vocabulary
by word frequencies in a descending order, and pick the top-W
words.
Method Time Gain Region Gain
SVM (W1) 33.2% ? 69.7% ?
SME (W1) 36.4% 9.6% 71.4% 2.4%
SVM (W2) 35.8% ? 72.3% ?
SME (W2) 40.9% 14.2% 74.0% 2.4%
SVM (W3) 36.1% ? 73.5% ?
SME (W3) 41.9% 16.1% 74.8% 1.8%
Table 1: Compare the accuracy of the linear kernel sup-
port vector machine to our sparse mixed-effects model in
the region and time identification tasks (K = 25). Gain:
the relative improvement of SME over SVM.
is more salient. For example, with a medium den-
sity feature space of 212, SVM obtained an accuracy
of 35.8%, but SME achieved an accuracy of 40.9%,
which is a 14.2% relative improvement (p < 0.001)
over SVM. When the feature space becomes sparser,
the SME obtains an increased relative improvement
(p < 0.001) of 16.1%, using full size of vocabu-
lary. The performance of SVM in the binary region
classification is stronger than in the previous task,
but SME is able to outperform SVM in all three set-
tings, with tightened advantages (p < 0.05 in W2
and p < 0.001 in W3). We hypothesize that it might
because that SVM, as a strong large margin learner,
is a more natural approach in a binary classification
setting, but might not be the best choice in a four-
way or multiclass classification task.
5.1.2 Comparing SME to SAGE
In this experiment, we compare SME with a state-
of-the-art sparse generative model: SAGE (Eisen-
stein et al, 2011a).
Most studies on topic modelling have not been
able to report results when using different sizes of
vocabulary for training. Because of the importance
of interpretability for social science research, the
choice of vocabulary size is critical to ensure un-
derstandable topics. Thus we report our results at
various vocabulary sizes W on SME and SAGE. To
better validate the performance of SME, we also in-
clude the performance of SVM in this experiment,
and fix the number of topics K = 10 for the SME
and SAGE models, which is a different value for the
number of topicsK than the empiricalK we used in
the experiment of Section 5.1.1. Figure 2 and Fig-
ure 3 show the experiment results in both time and
region classification task.
In Figure 2, we evaluate the impacts of W on our
time quartile prediction task. The advantage of the
SME model is very obvious throughout the experi-
ments. Interestingly, when we continue to increase
744
Figure 2: Accuracy on predicting the time quartile vary-
ing the vocabulary size W , while K is fixed to 10.
Figure 3: Accuracy on predicting the region varying the
vocabulary size W , while K is fixed to 10.
the vocabulary size W exponentially and make the
feature space more sparse, SME obtains its best re-
sult at W = 213, where the relative improvement
over SAGE and SVM is 16.8% and 22.9% respec-
tively (p < 0.001 under all comparisons).
Figure 3 shows the impacts of W on the accu-
racy of SAGE and SME in the region identification
task. In this experiment, the results of SME model
are in line with SAGE and SVM when the feature
space is dense. However, when W reaches the full
vocabulary size, we have observed significantly bet-
ter results (p < 0.001 in the comparison to SAGE
and p < 0.05 with SVM). We hypothesize that there
might be two reasons: first, the K parameter is set
to 10 in this experiment, which is much denser than
the experiment setting in Section 5.1.1. Under this
condition, the sparse topic advantage of SME might
be less salient. Secondly, in the two tasks, it is ob-
served that the accuracy of the binary region classi-
fication task is much higher than the four-way task,
thus while the latter benefits significantly from this
joint learning scheme of the SME model, but the for-
mer might not have the equivalent gain5.
5We hypothesize that this problem might be eliminated if
5.1.3 Influence of the number of topics K
Figure 4: Accuracy on predicting the time quartile vary-
ing the number of topics K, while W is fixed to 29.
Figure 5: Accuracy on predicting the region varying the
number of topics K, while W is fixed to 29.
Unlike hierarchical Dirichlet processes (Teh et al,
2006), in parametric Bayesian generative models,
the number of topics K is often set manually, and
can influence the model?s accuracy significantly. In
this experiment, we fix the input vocabulary W to
29, and compare the mixed-effect model with SAGE
in both region and time identification tasks.
Figure 4 shows how the variations of K can in-
fluence the system performance in the time quartile
prediction task. We can see that the sparse mixed-
effects model (SME) reaches its best performance
when the K is 40. After increasing the number of
topics K, we can see SAGE consistently increase
its accuracy, obtaining its best result when K = 30.
When comparing these two models, SME?s best per-
formance outperforms SAGE?s with an absolute im-
provement of 3%, which equals to a relative im-
provement (p < 0.001) of 8.4%. Figure 5 demon-
strates the impacts of K on the predictive power of
SME and SAGE in the region identification task.
the two tasks in SME have similar difficulties and accuracies,
but this needs to be verified in future work.
745
Keywords discovered by the SME model
Prior to 1837 (Q1) pauperis, footprints, American Colonization Society, manumissions, 1797
1838 - 1848 (Q2) indentured, borrowers, orphan?s, 1841, vendee?s, drawer?s, copartners
1849 - 1855 (Q3) Frankfort, negrotrader, 1851, Kentucky Assembly, marshaled, classed
After 1856 (Q4) railroadco, statute, Alabama, steamboats, Waterman?s, mulattoes, man-trap
Free Region (R1) apprenticed, overseer?s, Federal Army, manumitting, Illinois constitution
Slave Region (R2) Alabama, Clay?s Digest, oldest, cotton, reinstatement, sanction, plantation?s
Topic 1 in Q1 R1 imported, comaker, runs, writ?s, remainderman?s, converters, runaway
Topic 1 in Q1 R2 comaker, imported, deceitful, huston, send, bright, remainderman?s
Topic 2 in Q1 R1 descendent, younger, administrator?s, documentary, agreeable, emancipated
Topic 2 in Q1 R2 younger, administrator?s, grandmother?s, plaintiffs, emancipated, learnedly
Topic 3 in Q2 R1 heir-at-law, reconsidered, manumissions, birthplace, mon, mother-in-law
Topic 3 in Q2 R2 heir-at-law, reconsideration, mon, confessions, birthplace, father-in-law?s
Topic 4 in Q2 R1 indentured, apprenticed, deputy collector, stepfather?s, traded, seizes
Topic 4 in Q2 R2 deputy collector, seizes, traded, hiring, stepfather?s, indentured, teaching
Topic 5 in Q4 R1 constitutionality, constitutional, unconstitutionally, Federal Army, violated
Topic 5 in Q4 R2 petition, convictions, criminal court, murdered, constitutionality, man-trap
Table 2: A partial listing of an example for early United States state supreme court opinion keywords generated from
the time quartile ?(Q) , region ?(R) and topic-region-time ?(I) interactive variables in the sparse mixed-effects model.
Except that the two models tie up when K = 10,
SME outperforms SAGE for all subsequent varia-
tions ofK. Similar to the region task, SME achieves
the best result when K is sparser (p < 0.01 when
K = 40 and K = 50).
5.2 Qualitative Analysis
In this section, we qualitatively evaluate the topics
generated vis-a-vis the secondary literature on the
legal and political history of slavery in the United
States. The effectiveness of SME could depend not
just on its predictive power, but also in its ability
to generate topics that will be useful to historians
of the period. Supreme court opinions on slavery
are of significant interest for American political his-
tory. The conflict over slave property rights was at
the heart of the ?cold war? (Wright, 2006) between
North and South leading up to the U.S. Civil War.
The historical importance of this conflict between
Northern and Southern legal institutions is one of the
motivations for choosing our data domain.
We conduct qualitative analyses on the top-ranked
keywords6 that are associated with different geo-
graphical locations and different temporal frames,
generated by our SME model. In our analysis, for
6Keywords were ranked by word posterior probabilities.
each interaction of topic, region, and time period, a
list of the most salient vocabulary words was gener-
ated. These words were then analyzed in the context
of existing historical literature on the shift in atti-
tudes and views over time and across regions. Table
2 shows an example of relevant keywords and topics.
This difference between Northern and Southern
opinion can be seen in some of the topics generated
by the SME. Topic 1 deals with transfers of human
beings as slave property. The keyword ?remainder-
man? designates a person who inherits or is entitled
to inherit property upon the termination of an es-
tate, typically after the death of a property owner,
and appears in Northern and Southern cases. How-
ever, in Topic 1 ?runaway? appears as a keyword in
decisions from free states but not in decisions from
slave states. The fact that ?runaway? is not a top
word in the same topic in the Southern legal opin-
ions is consistent with a spatial (geolocational) di-
vision in which the property claims of slave owners
over runaways were not heavily contested in South-
ern courts.
Topic 3 concerns bequests, as indicated by the
term ?heir-at-law?, but again the term ?manumis-
sions?, ceases to show up in the slave states after the
first time quartile, perhaps reflecting the hostility to
746
manumissions that southern courts exhibited as the
conflict over slavery deepened.
Topic 4 concerns indentures and apprentices. In-
terestingly, the terms indentures and apprenticeships
are more prominent in the non-slave states, reflect-
ing the fact that apprenticeships and indentures were
used in many border states as a substitute for slavery,
and these were often governed by continued usage of
Master and Servant law (Orren, 1992).
Topic 5 shows the constitutional crisis in the
states. In particular, the anti-slavery state courts are
prone to use the term ?unconstitutional? much more
often than the slave states. The word ?man-trap?, a
term used to refer to states where free blacks could
be kidnapped purpose of enslaving them. The fugi-
tive slave conflicts of the mid-19th century that led
to the civil war were precisely about this aversion
of the northern states to having to return runaway
slaves to the Southern states.
Besides these subjective observations about the
historical significance of the SME topics, we also
conduct a more formal analysis comparing the SME
classification to that conducted by a legal histo-
rian. Wahl (2002) analyses and classifies by hand
10989 slave cases in the US South into 6 categories:
?Hires?, ?Sales?, ?Transfers?, ?Common Carrier?,
?Black Rights? and ?Other?. An example of ?Hires?
is Topic 4. Topics 1, 2, and 3 concern ?Transfers? of
slave property between inheritors, descendants and
heirs-at-law. Topic 5 would be classified as ?Other?.
We take each of our 25 modelled topics and clas-
sify them along Wahl?s categories, using ?Other?
when a classification could not be obtained. The
classifications are quite transparent in virtually all
cases, as certain words (such as ?employer? or ?be-
quest?) clearly designate certain categories (respec-
tively, such as ?Hires? or ?Transfers?). We then cal-
culate the probability of each of Wahl?s categories in
Region 2. We then compare these to the relative fre-
quencies of Wahl?s categorization in the states that
overlap with our Region 2 in Figure 6 and do a ?2
test for goodness of fit, which allows us to reject dif-
ference at 0.1% confidence.
The SME model thus delivers topics that, at a first
pass, are consistent with the history of the period
as well as previous work by historians, showing the
qualitative benefits of the model. We plan to conduct
more vertical and temporal analyses using SME in
the future.
6 Conclusion and Future Work
In this work, we propose a sparse mixed-effects
model for historical analysis of text. This model is
built on the state-of-the-art in latent variable mod-
Figure 6: Comparison with Wahl (2002) classification.
elling and extends that model to a setting where
metadata is available for analysis. We jointly model
those observed labels as well as unsupervised topic
modelling. In our experiments, we have shown that
the resulting model jointly predicts the region and
the time of a given court document. Across vocab-
ulary sizes and number of topics, we have achieved
better system accuracy than state-of-the-art genera-
tive and discriminative models of text. Our quantita-
tive analysis shows that early US state supreme court
opinions are predictable, and contains distinct views
towards slave-related topics, and the shifts among
opinions depending on different periods of time. In
addition, our model has been shown to be effective
for qualitative analysis of historical data, revealing
patterns that are consistent with the history of the
period.
This approach to modelling text is not limited
to the legal domain. A key aspect of future work
will be to extend the Sparse Mixed-Effects paradigm
to other problems within the social sciences where
metadata is available but qualitative analysis at a
large scale is difficult or impossible. In addition
to historical documents, this can include humani-
ties texts, which are often sorely lacking in empir-
ical justifications, and analysis of online communi-
ties, which are often rife with available metadata but
produce content far faster than it can be analyzed by
experts.
Acknowledgments
We thank Jacob Eisenstein, Noah Smith, and anony-
mous reviewers for valuable suggestions. William
Yang Wang is supported by the R. K. Mellon Presi-
dential Fellowship.
747
References
Lalit R. Bahl, Peter F. Brown., Peter V. de Souza, and
Robert L. Mercer. 1988. A new algorithm for the
estimation of hidden Markov model parameters. In
IEEE Inernational Conference on Acoustics, Speech
and Signal Processing, ICASSP, pages 493?496.
Ryan S.J.D. Baker and Kalina Yacef. 2009. The state of
educational data mining in 2009: a review and future
visions. In Journal of Educational Data Mining, pages
3?17.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learn-
ing Research (JMLR), pages 993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Transac-
tions on Intelligent System Technologies, pages 1?27.
Jake Chen and Stefano Lombardi. 2010. Biological data
mining. Chapman and Hall/CRC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1?8.
Agata Katarzyna Cybulska and Piek Vossen. 2011. His-
torical event extraction from text. In Proceedings of
the 5th ACL-HLT Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Humani-
ties, pages 39?43.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 1277?
1287.
Jacob Eisenstein, Amr Ahmed, and Eric. Xing. 2011a.
Sparse additive generative models of text. Proceed-
ings of the 28th International Conference on Machine
Learning (ICML 2011), pages 1041?1048.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011b. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL HLT 2011),
pages 1365?1374.
Annette Gotscharek, Andreas Neumann, Ulrich Reffle,
Christoph Ringlstetter, and Klaus U. Schulz. 2009.
Enabling information retrieval on historical document
collections: the role of matching procedures and spe-
cial lexica. In Proceedings of The Third Workshop
on Analytics for Noisy Unstructured Text Data (AND
2009), pages 69?76.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 552?561.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI 1999), pages 289?296.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures.
Kenneth Lange and Janet S. Sinsheimer. 1993. Nor-
mal/independent distributions and their applications in
robust regression.
Dong Nguyen and Carolyn Penstein Rose?. 2011. Lan-
guage use as a reflection of socialization in online
communities. In Workshop on Language in Social Me-
dia at ACL.
Dong Nguyen, Elijah Mayfield, and Carolyn P. Rose?.
2010. An analysis of perspectives in interactive set-
tings. In Proceedings of the First Workshop on Social
Media Analytics (SOMA 2010), pages 44?52.
Karen Orren. 1992. Belated feudalism: labor, the law,
and liberal development in the united states.
Eva Pettersson and Joakim Nivre. 2011. Automatic verb
extraction from historical swedish texts. In Proceed-
ings of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and Hu-
manities, pages 87?95.
William D. Popkin. 2007. Evolution of the judicial opin-
ion: institutional and individual styles. NYU Press.
Kumar Shubhankar, Aditya Pratap Singh, and Vikram
Pudi. 2011. An efficient algorithm for topic ranking
and modeling topic evolution. In Proceedings of Inter-
national Conference on Database and Expert Systems
Applications.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, pages 1566?1581.
Jenny Bourne Wahl. 2002. The Bondsman?s Burden: An
Economic Analysis of the Common Law of Southern
Slavery. Cambridge University Press.
William Yang Wang and Kathleen McKeown. 2010. ?got
you!?: automatic vandalism detection in wikipedia
with web-based shallow syntactic-semantic modeling.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1146?1154.
William Yang Wang, Kapil Thadani, and Kathleen McK-
eown. 2011. Identifyinge event descriptions using co-
training with online news summaries. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 281?291.
748
Gavin Wright. 2006. Slavery and american economic
development. Walter Lynwood Fleming Lectures in
Southern History.
Tze-I Yang, Andrew Torget, and Rada Mihalcea. 2011.
Topic modeling on historical newspapers. In Proceed-
ings of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and Hu-
manities, pages 96?104.
749
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1155?1165,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Semiparametric Gaussian Copula Regression Model
for Predicting Financial Risks from Earnings Calls
William Yang Wang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
yww@cs.cmu.edu
Zhenhao Hua
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
zhua@cs.cmu.edu
Abstract
Earnings call summarizes the financial
performance of a company, and it is an
important indicator of the future financial
risks of the company. We quantitatively
study how earnings calls are correlated
with the financial risks, with a special fo-
cus on the financial crisis of 2009. In par-
ticular, we perform a text regression task:
given the transcript of an earnings call, we
predict the volatility of stock prices from
the week after the call is made. We pro-
pose the use of copula: a powerful statis-
tical framework that separately models the
uniform marginals and their complex mul-
tivariate stochastic dependencies, while
not requiring any prior assumptions on the
distributions of the covariate and the de-
pendent variable. By performing probabil-
ity integral transform, our approach moves
beyond the standard count-based bag-of-
words models in NLP, and improves pre-
vious work on text regression by incor-
porating the correlation among local fea-
tures in the form of semiparametric Gaus-
sian copula. In experiments, we show
that our model significantly outperforms
strong linear and non-linear discriminative
baselines on three datasets under various
settings.
1 Introduction
Predicting the risks of publicly listed companies is
of great interests not only to the traders and ana-
lysts on the Wall Street, but also virtually anyone
who has investments in the market (Kogan et al,
2009). Traditionally, analysts focus on quantita-
tive modeling of historical trading data. Today,
even though earnings calls transcripts are abun-
dantly available, their distinctive communicative
practices (Camiciottoli, 2010), and correlations
with the financial risks, in particular, future stock
performances (Price et al, 2012), are not well
studied in the past.
Earnings calls are conference calls where a
listed company discusses the financial perfor-
mance. Typically, a earnings call contains two
parts: the senior executives first report the oper-
ational outcomes, as well as the current financial
performance, and then discuss their perspectives
on the future of the company. The second part of
the teleconference includes a question answering
session where the floor will be open to investors,
analysts, and other parties for inquiries. The ques-
tion we ask is that, even though each earnings call
has distinct styles, as well as different speakers
and mixed formats, can we use earnings calls to
predict the financial risks of the company in the
limited future?
Given a piece of earnings call transcript, we
investigate a semiparametric approach for auto-
matic prediction of future financial risk
1
. To do
this, we formulate the problem as a text regres-
sion task, and use a Gaussian copula with prob-
ability integral transform to model the uniform
marginals and their dependencies. Copula mod-
els (Schweizer and Sklar, 1983; Nelsen, 1999)
are often used by statisticians (Genest and Favre,
2007; Liu et al, 2012; Masarotto and Varin, 2012)
and economists (Chen and Fan, 2006) to study the
bivariate and multivariate stochastic dependency
among random variables, but they are very new
to the machine learning (Ghahramani et al, 2012;
Han et al, 2012; Xiang and Neville, 2013; Lopez-
paz et al, 2013) and related communities (Eick-
hoff et al, 2013). To the best of our knowledge,
even though the term ?copula? is named for the
resemblance to grammatical copulas in linguistics,
copula models have not been explored in the NLP
community. To evaluate the performance of our
approach, we compare with a standard squared
loss linear regression baseline, as well as strong
baselines such as linear and non-linear support
1
In this work, the risk is defined as the measured volatil-
ity of stock prices from the week following the earnings call
teleconference. See details in Section 5.
1155
vector machines (SVMs) that are widely used in
text regression tasks. By varying different exper-
imental settings on three datasets concerning dif-
ferent periods of the Great Recession from 2006-
2013, we empirically show that our approach sig-
nificantly outperforms the baselines by a wide
margin. Our main contributions are:
? We are among the first to formally study tran-
scripts of earnings calls to predict financial
risks.
? We propose a novel semiparametric Gaussian
copula model for text regression.
? Our results significantly outperform standard
linear regression and strong SVM baselines.
? By varying the number of dimensions of the
covariates and the size of the training data,
we show that the improvements over the
baselines are robust across different param-
eter settings on three datasets.
In the next section, we outline related work in
modeling financial reports and text regression. In
Section 3, the details of the semiparametric cop-
ula model are introduced. We then describe the
dataset and dependent variable in this study, and
the experiments are shown in Section 6. We dis-
cuss the results and findings in Section 7 and then
conclude in Section 8.
2 Related Work
Fung et al (2003) are among the first to study
SVM and text mining methods in the market pre-
diction domain, where they align financial news
articles with multiple time series to simulate the
33 stocks in the Hong Kong Hang Seng Index.
However, text regression in the financial domain
have not been explored until recently. Kogan et
al. (2009) model the SEC-mandated annual re-
ports, and performs linear SVM regression with
-insensitive loss function to predict the mea-
sured volatility. Another recent study (Wang et
al., 2013) uses exactly the same max-margin re-
gression technique, but with a different focus on
the financial sentiment. Using the same dataset,
Tsai and Wang (2013) reformulate the regression
problem as a text ranking problem. Note that
all these regression studies above investigate the
SEC-mandated annual reports, which are very dif-
ferent from the earnings calls in many aspects such
as length, format, vocabulary, and genre. Most
recently, Xie et al (2013) have proposed the use
of frame-level semantic features to understand fi-
nancial news, but they treat the stock movement
prediction problem as a binary classification task.
Broadly speaking, our work is also aligned to re-
cent studies that make use of social media data
to predict the stock market (Bollen et al, 2011;
Zhang et al, 2011).
Despite our financial domain, our approach is
more relevant to text regression. Traditional dis-
criminative models, such as linear regression and
linear SVM, have been very popular in various
text regression tasks, such as predicting movie rev-
enues from reviews (Joshi et al, 2010), under-
standing the geographic lexical variation (Eisen-
stein et al, 2010), and predicting food prices from
menus (Chahuneau et al, 2012). The advantage of
these models is that the estimation of the parame-
ters is often simple, the results are easy to inter-
pret, and the approach often yields strong perfor-
mances. While these approaches have merits, they
suffer from the problem of not explicitly model-
ing the correlations and interactions among ran-
dom variables, which in some sense, correspond-
ing to the impractical assumption of independent
and identically distributed (i.i.d) of the data. For
example, when bag-of-word-unigrams are present
in the feature space, it is easier if one does not ex-
plicitly model the stochastic dependencies among
the words, even though doing so might hurt the
predictive power, while the variance from the cor-
relations among the random variables is not ex-
plained.
3 Copula Models for Text Regression
In NLP, many statistical machine learning meth-
ods that capture the dependencies among ran-
dom variables, including topic models (Blei et al,
2003; Lafferty and Blei, 2005; Wang et al, 2012),
always have to make assumptions with the under-
lying distributions of the random variables, and
make use of informative priors. This might be
rather restricting the expressiveness of the model
in some sense (Reisinger et al, 2010). On the
other hand, once such assumptions are removed,
another problem arises ? they might be prone to
errors, and suffer from the overfitting issue. There-
fore, coping with the tradeoff between expressive-
ness and overfitting, seems to be rather important
in statistical approaches that capture stochastic de-
pendency.
Our proposed semiparametric copula regression
model takes a different perspective. On one hand,
copula models (Nelsen, 1999) seek to explicitly
model the dependency of random variables by sep-
arating the marginals and their correlations. On
the other hand, it does not make use of any as-
1156
sumptions on the distributions of the random vari-
ables, yet, the copula model is still expressive.
This nice property essentially allows us to fuse
distinctive lexical, syntactic, and semantic feature
sets naturally into a single compact model.
From an information-theoretic point of
view (Shannon, 1948), various problems in text
analytics can be formulated as estimating the
probability mass/density functions of tokens
in text. In NLP, many of the probabilistic text
models work in the discrete space (Church and
Gale, 1995; Blei et al, 2003), but our model is
different: since the text features are sparse, we
first perform kernel density estimates to smooth
out the zeroing items, and then calculate the
empirical cumulative distribution function (CDF)
of the random variables. By doing this, we
are essentially performing probability integral
transform? an important statistical technique
that moves beyond the count-based bag-of-words
feature space to marginal cumulative density
functions space. Last but not least, by using
a parametric copula, in our case, the Gaussian
copula, we reduce the computational cost from
fully nonparametric methods, and explicitly
model the correlations among the covariate and
the dependent variable.
In this section, we first briefly look at the
theoretical foundations of copulas, including the
Sklar?s theorem. Then we describe the proposed
semiparametric Gaussian copula text regression
model. The algorithmic implementation of our ap-
proach is introduced at the end of this section.
3.1 The Theory of Copula
In the statistics literature, copula is widely known
as a family of distribution function. The idea be-
hind copula theory is that the cumulative distri-
bution function (CDF) of a random vector can be
represented in the form of uniform marginal cu-
mulative distribution functions, and a copula that
connects these marginal CDFs, which describes
the correlations among the input random variables.
However, in order to have a valid multivariate dis-
tribution function regardless of n-dimensional co-
variates, not every function can be used as a copula
function. The central idea behind copula, there-
fore, can be summarize by the Sklar?s theorem and
the corollary.
Theorem 1 (Sklar?s Theorem (1959)) Let F
be the joint cumulative distribution function
of n random variables X
1
, X
2
, ..., X
n
. Let
the corresponding marginal cumulative dis-
tribution functions of the random variable be
F
1
(x
1
), F
2
(x
2
), ..., F
n
(x
n
). Then, if the marginal
functions are continuous, there exists a unique
copula C, such that
F (x
1
, ..., x
n
) = C[F
1
(x
1
), ..., F
n
(x
n
)]. (1)
Furthermore, if the distributions are continuous,
the multivariate dependency structure and the
marginals might be separated, and the copula can
be considered independent of the marginals (Joe,
1997; Parsa and Klugman, 2011). Therefore, the
copula does not have requirements on the marginal
distributions, and any arbitrary marginals can be
combined and their dependency structure can be
modeled using the copula. The inverse of Sklar?s
Theorem is also true in the following:
Corollary 1 If there exists a copula C : (0, 1)
n
and marginal cumulative distribution func-
tions F
1
(x
1
), F
2
(x
2
), ..., F
n
(x
n
), then
C[F
1
(x
1
), ..., F
n
(x
n
)] defines a multivariate
cumulative distribution function.
3.2 Semiparametric Gaussian Copula Models
The Non-Parametric Estimation
We formulate the copula regression model as fol-
lows. Assume we have n random variables of text
features X
1
, X
2
, ..., X
n
. The problem is that text
features are sparse, so we need to perform non-
parametric kernel density estimation to smooth out
the distribution of each variable. Let f
1
, f
2
, ..., f
n
be the unknown density, we are interested in de-
riving the shape of these functions. Assume we
have m samples, the kernel density estimator can
be defined as:
?
f
h
(x) =
1
m
m
?
i=1
K
h
(x? x
i
) (2)
=
1
mh
m
?
i=1
K
(
x? x
i
h
)
(3)
Here, K(?) is the kernel function, where in our
case, we use the Box kernel
2
K(z):
K(z) =
1
2
, |z| ? 1, (4)
= 0, |z| > 1. (5)
Comparing to the Gaussian kernel and other ker-
nels, the Box kernel is simple, and computation-
ally inexpensive. The parameter h is the band-
width for smoothing
3
.
2
It is also known as the original Parzen windows (Parzen,
1962).
3
In our implementation, we use the default h of the Box
kernel in the ksdensity function in Matlab.
1157
Now, we can derive the empiri-
cal cumulative distribution functions
?
F
X
1
(
?
f
1
(X
1
)),
?
F
X
2
(
?
f
2
(X
2
)), ...,
?
F
X
n
(
?
f
n
(X
n
)) of
the smoothed covariates, as well as the dependent
variable y and its CDF
?
F
y
(
?
f(y)). The empirical
cumulative distribution functions are defined as:
?
F (?) =
1
m
m
?
i=1
I{x
i
? ?} (6)
where I{?} is the indicator function, and ? in-
dicates the current value that we are evaluating.
Note that the above step is also known as prob-
ability integral transform (Diebold et al, 1997),
which allows us to convert any given continuous
distribution to random variables having a uniform
distribution. This is of crucial importance to mod-
eling text data: instead of using the classic bag-of-
words representation that uses raw counts, we are
now working with uniform marginal CDFs, which
helps coping with the overfitting issue due to noise
and data sparsity.
The Parametric Copula Estimation
Now that we have obtained the marginals, and then
the joint distribution can be constructed by apply-
ing the copula function that models the stochastic
dependencies among marginal CDFs:
?
F (
?
f
1
(X
1
), ...,
?
f
1
(X
n
),
?
f(y)) (7)
= C[
?
F
X
1
(
?
f
1
(X
1
)
)
, ...,
?
F
X
n
(
?
f
n
(X
n
)
)
,
?
F
y
(
?
f
y
(y)
)
] (8)
In this work, we apply the parametric Gaussian
copula to model the correlations among the text
features and the label. Assume x
i
is the smoothed
version of random variable X
i
, and y is the
smoothed label, we have:
F (x
1
, ..., x
n
, y) (9)
= ?
?
(
?
?1
[F
x
1
(x
1
)], ..., ,?
?1
[F
x
n
(x
n
)],?
?1
[F
y
(y)]
)
(10)
where ?
?
is the joint cumulative distribution func-
tion of a multivariate Gaussian with zero mean and
? variance. ?
?1
is the inverse CDF of a standard
Gaussian. In this parametric part of the model, the
parameter estimation boils down to the problem of
learning the covariance matrix ? of this Gaussian
copula. In this work, we perform standard maxi-
mum likelihood estimation for the ? matrix.
To calibrate the ? matrix, we make use of
the power of randomness: using the initial ?
from MLE, we generate random samples from
the Gaussian copula, and then concatenate previ-
ously generated joint of Gaussian inverse marginal
CDFs with the newly generated random copula
numbers, and re-estimate using MLE to derive the
final adjusted ?. Note that the final ? matrix has
to be symmetric and positive definite.
Computational Complexity
One important question regarding the proposed
semiparametric Gaussian copula model is the cor-
responding computational complexity. This boils
down to the estimation of the
?
? matrix (Liu et al,
2012): one only needs to calculate the correla-
tion coefficients of n(n ? 1)/2 pairs of random
variables. Christensen (2005) shows that sort-
ing and balanced binary trees can be used to cal-
culate the correlation coefficients with complex-
ity of O(n log n). Therefore, the computational
complexity of MLE for the proposed model is
O(n log n).
Efficient Approximate Inference
In this regression task, in order to perform
exact inference of the conditional probability
distribution p(F
y
(y)|F
x
1
(x
1
), ..., F
x
n
(x
n
)),
one needs to solve the mean response
?
E(F
y
(y)|F
x
1
(x
1
), ..., F
x
1
(x
1
)) from a joint
distribution of high-dimensional Gaussian copula.
Assume in the simple bivariate case of Gaussian
copula regression, the covariance matrix ? is:
? =
[
?
11
?
12
?
22
]
We can easily derive the conditional density that
can be used to calculate the expected value of the
CDF of the label:
C(F
y
(y)|F
x
1
(x
1
); ?) =
1
|?
22
? ?
T
12
?
?1
11
?
12
|
1
2
exp
(
?
1
2
?
T
(
[?
22
? ?
T
12
?
?1
11
?
12
]
?1
? I
)
?
)
(11)
where ? = ?
?1
[F
y
(y)]? ?
T
12
?
?1
11
?
?1
[F
x
1
(x
1
)].
Unfortunately, the exact inference can be in-
tractable in the multivariate case, and approximate
inference, such as Markov Chain Monte Carlo
sampling (Gelfand and Smith, 1990; Pitt et al,
2006) is often used for posterior inference. In this
work, we propose an efficient sampling method
to derive y given the text features ? we sample
F
y
(y) s.t. it maximizes the joint high-dimensional
Gaussian copula density:
?
F
y
(y) ? arg max
F
y
(y)?(0,1)
1
?
det ?
exp
(
?
1
2
?
T
?
(
?
?1
? I
)
??
)
(12)
1158
where
? =
?
?
?
?
?
?
?1
(F
x
1
(x
1
))
.
.
.
?
?1
(F
x
n
(x
n
))
?
?1
(F
y
(y))
?
?
?
?
?
Again, the reason why we perform approxi-
mated inference is that: exact inference in the
high-dimensional Gaussian copula density is non-
trivial, and might not have analytical solutions,
but approximate inference using maximum den-
sity sampling from the Gaussian copula signifi-
cantly relaxes the complexity of inference. Fi-
nally, to derive y?, the last step is to compute the
inverse CDF of
?
F
y
(y).
3.3 Algorithmic Implementation
The algorithmic implementation of our semipara-
metric Gaussian copula text regression model is
shown in Algorithm 1. Basically, the algorithm
can be decomposed into four parts:
? Perform nonparametric Box kernel density
estimates of the covariates and the dependent
variable for smoothing.
? Calculate the empirical cumulative distribu-
tion functions of the smoothed random vari-
ables.
? Estimate the parameters (covariance ?) of the
Gaussian copula.
? Infer the predicted value of the dependent
variable by sampling the Gaussian copula
probability density function.
4 Datasets
We use three datasets
4
of transcribed quarterly
earnings calls from the U.S. stock market, focus-
ing on the period of the Great Recession.
The pre-2009 dataset consists of earnings calls
from the period of 2006-2008, which includes
calls from the beginning of economic downturn,
the outbreak of the subprime mortgage crisis, and
the epidemic of collapses of large financial insti-
tutions. The 2009 dataset contains earnings calls
from the year of 2009, which is a period where the
credit crisis spreads globally, and the Dow Jones
Industrial Average hit the lowest since the begin-
ning of the millennium. The post-2009 dataset in-
cludes earnings calls from the period of 2010 to
2013, which concerns the recovery of global econ-
omy. The detailed statistics is shown in Table 1.
4
http://www.cs.cmu.edu/?yww/data/earningscalls.zip
Algorithm 1 A Semi-parametric Gaussian Copula
Model Based Text Regression Algorithm
Given:
(1) training data (X
(tr)
, ~y
(tr)
);
(2) testing data (X
(te)
, ~y
(te)
);
Learning:
for i = 1? n dimensions do
X
(tr)
?
i
? BoxKDE(X
(tr)
i
, X
(tr)
i
);
U
(tr)
i
? EmpiricalCDF (X
(tr)
?
i
);
X
(te)
?
i
? BoxKDE(X
(tr)
i
, X
(te)
i
);
U
(te)
i
? EmpiricalCDF (X
(te)
?
i
);
end for
y
(tr)
?
? BoxKDE(y
(tr)
, y
(tr)
);
v
(tr)
? EmpiricalCDF (y
(tr)
?
);
Z
(tr)
? GaussianInverseCDF ([U
(tr)
v
(tr)
]);
?
?? CorrelationCoefficients(Z
(tr)
);
r ?MultiV ariateGaussianRandNum(0,
?
?, n);
Z
(tr)
?
= GaussianCDF (r);
?
?? CorrelationCoefficients([Z
(tr)
Z
(tr)
?
]);
Inference:
for j = 1? m instances do
max
j
? 0;
?
Y
?
= 0;
for k = 0.01? 1 do
Z
(te)
? GaussianInverseCDF ([U
(te)
k]);
p
j
=
MultiV ariateGaussianPDF (Z
(te)
,
?
?)
?
n
GaussianPDF (Z
(te)
)
;
if p
j
? max
j
then
max
j
= p
j
;
?
Y
?
= k;
end if
end for
end for
y? ? InverseCDF (~y
(tr)
,
?
Y
?
);
Dataset #Calls #Companies #Types #Tokens
Pre-2009 3694 2746 371.5K 28.7M
2009 3474 2178 346.2K 26.4M
Post-2009 3726 2107 377.4K 28.6M
Table 1: Statistics of three datasets. Types: unique
words. Tokens: word tokens.
Note that unlike the standard news corpora in
NLP or the SEC-mandated financial report, Tran-
scripts of earnings call is a very special genre
of text. For example, the length of WSJ docu-
ments is typically one to three hundreds (Harman,
1995), but the averaged document length of our
three earnings calls datasets is 7677. Depending
on the amount of interactions in the question an-
swering session, the complexities of the calls vary.
This mixed form of formal statement and informal
speech brought difficulties to machine learning al-
gorithms.
5 Measuring Financial Risks
Volatility is an important measure of the financial
risk, and in this work, we focus on predicting the
future volatility following the earnings teleconfer-
1159
ence call. For each earning call, we have a week of
stock prices of the company after the day on which
the earnings call is made. The Return of Day t is:
r
t
=
x
t
x
t?1
? 1 (13)
where x
t
represents the share price of Day t, and
the Measured Stock Volatility from Day t to t+ ? :
y
(t,t+?)
=
?
?
?
i=0
(r
t+i
? r?)
2
?
(14)
Using the stock prices, we can use the equations
above to calculate the measured stock volatility af-
ter the earnings call, which is the standard measure
of risks in finance, and the dependent variable y of
our predictive task.
6 Experiments
6.1 Experimental Setup
In all experiments throughout this section, we use
80-20 train/test splits on all three datasets.
Feature sets:
We have extracted lexical, named entity, syntactic,
and frame-semantics features, most of which have
been shown to perform well in previous work (Xie
et al, 2013). We use the unigrams and bigrams
to represent lexical features, and the Stanford part-
of-speech tagger (Toutanova et al, 2003) to extract
the lexicalized named entity and part-of-speech
features. A probabilistic frame-semantics parser,
SEMAFOR (Das et al, 2010), is used to provide
the FrameNet-style frame-level semantic annota-
tions. For each of the five sets, we collect the top-
100 most frequent features, and end up with a total
of 500 features.
Baselines:
The baselines are standard squared-loss linear
regression, linear kernel SVM, and non-linear
(Gaussian) kernel SVM. They are all standard
algorithms in regression problems, and have
been shown to have outstanding performances in
many recent text regression (Kogan et al, 2009;
Chahuneau et al, 2012; Xie et al, 2013; Wang
et al, 2013; Tsai and Wang, 2013). We use
the Statistical Toolbox?s linear regression imple-
mentation in Matlab, and LibSVM (Chang and
Lin, 2011) for training and testing the SVM mod-
els. The hyperparameter C in linear SVM, and
the ? and C hyperparameters in Gaussian SVM
are tuned on the training set using 10-fold cross-
validation. Note that since the kernel density esti-
mation in the proposed copula model is nonpara-
metric, and we only need to learn the ? in the
Gaussian copula, there is no hyperparameters that
need to be tuned.
Evaluation Metrics:
Spearman?s correlation (Hogg and Craig, 1994)
and Kendall?s tau (Kendall, 1938) have been
widely used in many regression problems in NLP
(Albrecht and Hwa, 2007; Yogatama et al, 2011;
Wang et al, 2013; Tsai and Wang, 2013), and here
we use them to measure the quality of predicted
values
?
y by comparing to the vector of ground
truth y. In contrast to Pearson?s correlation, Spear-
man?s correlation has no assumptions on the rela-
tionship of the two measured variables. Kendall?s
tau is a nonparametric statistical metric that have
shown to be inexpensive, robust, and represen-
tation independent (Lapata, 2006). We also use
paired two-tailed t-test to measure the statistical
significance between the best and the second best
approaches.
6.2 Comparing to Various Baselines
In the first experiment, we compare the proposed
semiparametric Gaussian copula regression model
to three baselines on three datasets with all fea-
tures. The detailed results are shown in Table 2.
On the pre-2009 dataset, we see that the linear re-
gression and linear SVM perform reasonably well,
but the Gaussian kernel SVM performs less well,
probably due to overfitting. The copula model
outperformed all three baselines by a wide mar-
gin on this dataset with both metrics. Similar per-
formances are also obtained in the 2009 dataset,
where the result of linear SVM baseline falls be-
hind. On the post-2009 dataset, none of results
from the linear and non-linear SVM models can
match up with the linear regression model, but
our proposed copula model still improves over all
baselines by a large margin. Comparing to second-
best approaches, all improvements obtained by the
copula model are statistically significant.
6.3 Varying the Amount of Training Data
To understand the learning curve of our proposed
copula regression model, we use the 25%, 50%,
75% subsets from the training data, and evaluate
all four models. Figure 1 shows the evaluation
results. From the experiments on the pre-2009
dataset, we see that when the amount of training
data is small (25%), both SVM models have ob-
tained very impressive results. This is not surpris-
ing at all, because as max-margin models, soft-
margin SVM only needs a handful of examples
that come with nonvanishing coefficients (support
vectors) to find a reasonable margin. When in-
1160
Method Pre-2009 2009 Post-2009
Spearman Kendall Spearman Kendall Spearman Kendall
linear regression: 0.377 0.259 0.367 0.252 0.314 0.216
linear SVM: 0.364 0.249 0.242 0.167 0.132 0.091
Gaussian SVM: 0.305 0.207 0.280 0.192 0.152 0.104
Gaussian copula: 0.425* 0.315* 0.422* 0.310* 0.375* 0.282*
Table 2: Comparing the learning algorithms on three datasets with all features. The best result is high-
lighted in bold. * indicates p < .001 comparing to the second best result.
Figure 1: Varying the amount of training data. Left column: pre-2009 dataset. Middle column: 2009
dataset. Right column: post-2009 dataset. Top row: Spearman?s correlation. Bottom row: Kendall?s tau.
creasing the amount of training data to 50%, we do
see the proposed copula model catches up quickly,
and lead all baseline methods undoubtably at 75%
training data. On the 2009 dataset, we observe
very similar patterns. Interestingly, the proposed
copula regression model has dominated all meth-
ods for both metrics throughout all proportions of
the ?post-2009? earnings calls dataset, where in-
stead of financial crisis, the economic recovery is
the main theme. In contrast to the previous two
datasets, both linear and non-linear SVMs fail to
reach reasonable performances on this dataset.
6.4 Varying the Amount of Features
Finally, we investigate the robustness of the pro-
posed semiparametric Gaussian copula regression
model by varying the amount of features in the co-
variate space. To do this, we sample equal amount
of features from each feature set, and concatenate
them into a feature vector. When increasing the
amount of total features from 100 to 400, the re-
sults are shown in Figure 2. On the pre-2009
dataset, we see that the gaps between the best-
perform copula model and the second-best linear
regression model are consistent throughout all fea-
ture sizes. On the 2009 dataset, we see that the
performance of Gaussian copula is aligned with
the linear regression model in terms of Spearman?s
correlation, where the former seems to perform
better in terms of Kendall?s tau. Both linear and
non-linear SVM models do not have any advan-
tages over the proposed approach. On the post-
2009 dataset that concerns economic growth and
recovery, the boundaries among all methods are
very clear. The Spearman?s correlation for both
SVM baselines is less than 0.15 throughout all set-
tings, but copula model is able to achieve 0.4 when
using 400 features. The improvements of copula
1161
Figure 2: Varying the amount of features. Left column: pre-2009 dataset. Middle column: 2009 dataset.
Right column: post-2009 dataset. Top row: Spearman?s correlation. Bottom row: Kendall?s tau.
Pre-2009 2009 Post-2009
2008/CD 2008 first quarter
2008 million/CD revenue/NN
third quarter 2008/CD revenue
third million quarter of
third/JJ million in compared to
the third the fourth million in
million/CD fourth quarter Peter/PERSON
capital fourth call
million fourth/JJ first/JJ
FE Trajector entity $/$ million/CD
Table 3: Top-10 features that have positive corre-
lations with stock volatility in three datasets.
model over squared loss linear regression model
are increasing, when working with larger feature
spaces.
6.5 Qualitative Analysis
Like linear classifiers, by ?opening the hood? to
the Gaussian copula regression model, one can ex-
amine features that exhibit high correlations with
the dependent variable. Table 3 shows the top fea-
tures that are positively correlated with the future
stock volatility in the three datasets. On the top
features from the ?pre-2009? dataset, which pri-
marily (82%) includes calls from 2008, we can
clearly observe that the word ?2008? has strong
correlation with the financial risks. Interestingly,
the phrase ?third quarter? and its variations, not
only play an important role in the model, but also
highly correlated to the timeline of the financial
crisis: the Q3 of 2008 is a critical period in the
recession, where Lehman Brothers falls on the
Sept. 15 of 2008, filing $613 billion of debt ?
the biggest bankruptcy in U.S. history (Mamudi,
2008). This huge panic soon broke out in vari-
ous financial institutions in the Wall Street. On
the top features from ?2009? dataset, again, we see
the word ?2008? is still prominent in predicting fi-
nancial risks, indicating the hardship and extended
impacts from the center of the economic crisis.
After examining the transcripts, we found sen-
tences like: ?...our specialty lighting business that
we discontinued in the fourth quarter of 2008...?,
?...the exception of fourth quarter revenue which
was $100,000 below our guidance target...?, and
?...to address changing economic conditions and
their impact on our operations, in the fourth quar-
ter we took the painful but prudent step of de-
creasing our headcount by about 5%...?, show-
ing the crucial role that Q4 of 2008 plays in 2009
earnings calls. Interestingly, after the 2008-2009
crisis, in the recovery period, we have observed
new words like ?revenue?, indicating the ?back-to-
normal? trend of financial environment, and new
features that predict financial volatility.
7 Discussions
In the experimental section, we notice that the
proposed semiparametric Gaussian copula model
has obtained promising results in various setups
on three datasets in this text regression task. The
1162
main questions we ask are: how is the pro-
posed model different from standard text regres-
sion/classification models? What are the advan-
tages of copula-based models, and what makes it
perform so well?
One advantage we see from the copula model
is that it does not require any assumptions on
the marginal distributions. For example, in latent
Dirichlet alocation (Blei et al, 2003), the topic
proportion of a document is always drawn from
a Dirichlet(?) distribution. This is rather re-
stricted, because the possible shapes from a K?1
simplex of Dirichlet is always limited in some
sense. In our copula model, instead of using some
priors, we just calculate the empirical cumulative
distribution function of the random variables, and
model the correlation among them. This is ex-
tremely practical, because in many natural lan-
guage processing tasks, we often have to deal with
features that are extracted from many different do-
mains and signals. By applying the Probability
Integral Transform to raw features in the copula
model, we essentially avoid comparing apples and
oranges in the feature space, which is a common
problem in bag-of-features models in NLP.
The second hypothesis is about the semiparam-
etirc parameterization, which contains the non-
parametric kernel density estimation and the para-
metric Gaussian copula regression components.
The benefit of a semiparametric model is that here
we are not interested in performing completely
nonparametric estimations, where the infinite di-
mensional parameters might bring intractability.
In contrast, by considering the semiparametric
case, we not only obtain some expressiveness from
the nonparametric models, but also reduce the
complexity of the task: we are only interested in
the finite-dimensional components ? in the Gaus-
sian copula with O(n log n) complexity, which
is not as computationally difficult as the com-
pletely nonparametric cases. Also, by modeling
the marginals and their correlations seperately, our
approach is cleaner, easy-to-understand, and al-
lows us to have more flexibility to model the un-
certainty of data. Our pilot experiment also aligns
with our hypothesis: when not performing the ker-
nel density estimation part for smoothing out the
marginal distributions, the performances dropped
significantly when sparser features are included.
The third advantage we observe is the power of
modeling the covariance of the random variables.
Traditionally, in statistics, independent and identi-
cally distributed (i.i.d) assumptions among the in-
stances and the random variables are often used in
various models, such that the correlations among
the instances or the variables are often ignored.
However, this might not be practical at all: in im-
age processing, the ?cloud? pixel of a pixel show-
ing the blue sky of a picture are more likelihood to
co-occur in the same picture; in natural language
processing, the word ?mythical? is more likely to
co-occur with the word ?unicorn?, rather than the
word ?popcorn?. Therefore, by modeling the cor-
relations among marginal CDFs, the copula model
has gained the insights on the dependency struc-
tures of the random variables, and thus, the perfor-
mance of the regression task is boosted.
In the future, we plan to apply the proposed
approach to large datasets where millions of fea-
tures and millions of instances are involved. Cur-
rently we have not experienced the difficulty when
estimating the Gaussian copula model, but paral-
lel methods might be needed to speedup learning
when significantly more marginal CDFs are in-
volved. The second issue is about overfitting. We
see that when features are rather noisy, we might
need to investigate regularized copula models to
avoid this. Finally, we plan to extend the proposed
approach to text classification and structured pre-
diction problems in NLP.
8 Conclusion
In this work, we have demonstrated that the more
complex quarterly earnings calls can also be used
to predict the measured volatility of the stocks in
the limited future. We propose a novel semipara-
metric Gausian copula regression approach that
models the dependency structure of the language
in the earnings calls. Unlike traditional bag-of-
features models that work discrete features from
various signals, we perform kernel density esti-
mation to smooth out the distribution, and use
probability integral transform to work with CDFs
that are uniform. The copula model deals with
marginal CDFs and the correlation among them
separately, in a cleaner manner that is also flexible
to parameterize. Focusing on the three financial
crisis related datasets, the proposed model signif-
icantly outperform the standard linear regression
method in statistics and strong discriminative sup-
port vector regression baselines. By varying the
size of the training data and the dimensionality of
the covariates, we have demonstrated that our pro-
posed model is relatively robust across different
parameter settings.
Acknowledgement
We thank Alex Smola, Barnab?as P?oczos, Sam
Thomson, Shoou-I Yu, Zi Yang, and anonymous
reviewers for their useful comments.
1163
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level mt evaluation with pseudo refer-
ences. In Proceedings of Annual Meeting of the As-
sociation for Computational Linguistics.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of machine
Learning research.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science.
Belinda Camiciottoli. 2010. Earnings calls: Exploring
an emerging financial reporting genre. Discourse &
Communication.
Victor Chahuneau, Kevin Gimpel, Bryan R Routledge,
Lily Scherlis, and Noah A Smith. 2012. Word
salad: Relating food prices and descriptions. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology.
Xiaohong Chen and Yanqin Fan. 2006. Estimation
of copula-based semiparametric time series models.
Journal of Econometrics.
David Christensen. 2005. Fast algorithms for the cal-
culation of kendalls ? . Computational Statistics.
Kenneth Church and William Gale. 1995. Poisson
mixtures. Natural Language Engineering.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Human language technologies: The
2010 annual conference of the North American
chapter of the association for computational linguis-
tics.
Francis X Diebold, Todd A Gunther, and Anthony S
Tay. 1997. Evaluating density forecasts.
Carsten Eickhoff, Arjen P. de Vries, and Kevyn
Collins-Thompson. 2013. Copulas for information
retrieval. In Proceedings of the 36th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing.
Pui Cheong Fung, Xu Yu, and Wai Lam. 2003. Stock
prediction: Integrating text mining approach using
real-time news. In Proceedings of IEEE Interna-
tional Conference on Computational Intelligence for
Financial Engineering.
Alan Gelfand and Adrian Smith. 1990. Sampling-
based approaches to calculating marginal densities.
Journal of the American statistical association.
Christian Genest and Anne-Catherine Favre. 2007.
Everything you always wanted to know about copula
modeling but were afraid to ask. Journal of Hydro-
logic Engineering.
Zoubin Ghahramani, Barnab?as P?oczos, and Jeff
Schneider. 2012. Copula-based kernel dependency
measures. In Proceedings of the 29th International
Conference on Machine Learning.
Fang Han, Tuo Zhao, and Han Liu. 2012. Coda: High
dimensional copula discriminant analysis. Journal
of Machine Learning Research.
Donna Harman. 1995. Overview of the second text re-
trieval conference (trec-2). Information Processing
& Management.
Robert V Hogg and Allen Craig. 1994. Introduction to
mathematical statistics.
Harry Joe. 1997. Multivariate models and dependence
concepts.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A Smith. 2010. Movie reviews and revenues:
An experiment in text regression. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika.
Shimon Kogan, Dimitry Levin, Bryan Routledge, Ja-
cob Sagi, and Noah Smith. 2009. Predicting risk
from financial reports with regression. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
John Lafferty and David Blei. 2005. Correlated topic
models. In Advances in neural information process-
ing systems.
Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics.
Han Liu, Fang Han, Ming Yuan, John Lafferty, and
Larry Wasserman. 2012. High-dimensional semi-
parametric gaussian copula graphical models. The
Annals of Statistics.
David Lopez-paz, Jose M Hern?andez-lobato, and
Ghahramani Zoubin. 2013. Gaussian process vine
copulas for multivariate dependence. In Proceed-
ings of the 30th International Conference on Ma-
chine Learning.
Sam Mamudi. 2008. Lehman folds with record $613
billion debt. MarketWatch.com.
1164
Guido Masarotto and Cristiano Varin. 2012. Gaussian
copula marginal regression. Electronic Journal of
Statistics.
Roger B Nelsen. 1999. An introduction to copulas.
Springer Verlag.
Rahul A Parsa and Stuart A Klugman. 2011. Copula
regression. Variance Advancing and Science of Risk.
Emanuel Parzen. 1962. On estimation of a probability
density function and mode. The annals of mathe-
matical statistics.
Michael Pitt, David Chan, and Robert Kohn. 2006.
Efficient bayesian inference for gaussian copula re-
gression models. Biometrika.
McKay Price, James Doran, David Peterson, and Bar-
bara Bliss. 2012. Earnings conference calls and
stock returns: The incremental informativeness of
textual tone. Journal of Banking & Finance.
Joseph Reisinger, Austin Waters, Bryan Silverthorn,
and Raymond J Mooney. 2010. Spherical topic
models. In Proceedings of the 27th International
Conference on Machine Learning.
Berthold Schweizer and Abe Sklar. 1983. Probabilis-
tic metric spaces.
Claude Shannon. 1948. A mathematical theory of
communication. In The Bell System Technical Jour-
nal.
Abe Sklar. 1959. Fonctions de r?epartition `a n dimen-
sions et leurs marges. Universit?e Paris 8.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology.
Ming-Feng Tsai and Chuan-Ju Wang. 2013. Risk
ranking from financial reports. In Advances in In-
formation Retrieval.
William Yang Wang, Elijah Mayfield, Suresh Naidu,
and Jeremiah Dittmar. 2012. Historical analysis
of legal opinions with a sparse mixed-effects latent
variable model. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-
Ting Chang. 2013. Financial sentiment analysis for
risk prediction. In Proceedings of the Sixth Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Rongjing Xiang and Jennifer Neville. 2013. Collec-
tive inference for network data with copula latent
markov networks. In Proceedings of the sixth ACM
international conference on Web search and data
mining.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ?an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R Routledge, and Noah A Smith.
2011. Predicting a scientific community?s response
to an article. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Xue Zhang, Hauke Fuehres, and Peter A Gloor. 2011.
Predicting stock market indicators through twitter ?i
hope it is not as bad as i fear?. Procedia-Social and
Behavioral Sciences.
1165
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 152?161,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Detecting Levels of Interest from Spoken Dialog with Multistream
Prediction Feedback and Similarity Based Hierarchical Fusion Learning
William Yang Wang
Department of Computer Science
Columbia University
New York, NY 10027
yw2347@columbia.edu
Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
Abstract
Detecting levels of interest from speakers
is a new problem in Spoken Dialog Under-
standing with significant impact on real world
business applications. Previous work has fo-
cused on the analysis of traditional acous-
tic signals and shallow lexical features. In
this paper, we present a novel hierarchical fu-
sion learning model that takes feedback from
previous multistream predictions of promi-
nent seed samples into account and uses a
mean cosine similarity measure to learn rules
that improve reclassification. Our method is
domain-independent and can be adapted to
other speech and language processing areas
where domain adaptation is expensive to per-
form. Incorporating Discriminative Term Fre-
quency and Inverse Document Frequency (D-
TFIDF), lexical affect scoring, and low and
high level prosodic and acoustic features, our
experiments outperform the published results
of all systems participating in the 2010 Inter-
speech Paralinguistic Affect Subchallenge.
1 Introduction
In recent years, there has been growing interest in
identifying speakers? emotional state from speech
(Devillers and Vidrascu, 2006; Ai et al, 2006; Lis-
combe et al, 2005). For Spoken Dialog Systems
(SDS), the motivation has been to provide users with
improved over-the-phone services by recognizing
emotions such as anger and frustration and direct-
ing users to a human attendant. Other forms of par-
alinguistic information which researchers have at-
tempted to detect automatically include other classic
emotions, charismatic speech (Biadsy et al, 2008),
and deceptive speech (Hirschberg et al, 2005).
More recently, the 2010 Interspeech Paralinguisic
Affect Subchallenge sparked interest in detecting a
speaker?s level of interest (LOI), including both the
speaker?s interest in the topic and his/her willingness
to participating in the dialog (Schuller et al, 2010).
Sensing users? LOI in SDS should be useful in sales
domains, political polling, or service subscription.
In this paper, we present a similarity-based hi-
erarchical regression approach to predicting speak-
ers? LOI. The system has been developed based on
the hierarchical fusion learning of lexical and acous-
tic cues from speech. We investigate the contri-
bution of a novel source of information, Discrimi-
native TFIDF; lexical affect scoring; and prosodic
event features. Inspired by the successful use of
Pseudo Relevance Feedback (Tao and Zhai, 2006)
techniques in Information Retrieval and the cosine
similarity measure (Salton, 1989) in Data Mining,
we design a novel learning model which takes the
multistream prediction feedback that is initially re-
turned from seed samples 1 and uses a mean cosine
similarity measure to calculate the distance between
the new instance and prominent seed data points in
the Euclidean Space. We then add this similarity
measure as a new feature to perform a reclassifi-
cation. Our main contributions in this paper are:
(1) the novel Discriminative TFIDF approach for
lexical modeling and keywords spotting; (2) using
lexical affect scoring and language modeling tech-
niques to augment lexical modeling; (3) combin-
1Seed samples are from a random small subset in the test
set.
152
ing (1) and (2) with additional low-level prosodic
features together with voice quality and high-level
prosodic event features; and (4) introducing a mul-
tistream prediction feedback and mean cosine simi-
larity based fusion learning approach.
We outline related work in Section 2. The corpus,
system features, and machine learning approaches
are described in Section 3. We describe our experi-
mental results in Section 4 and conclude in Section
5.
2 Related Work
Schuller et al (2006) were among the first to study
LOI from conversational speech. They framed this
task as either a three-way or binary classification,
extracting standard acoustic features and building a
bag-of-words vector space model for lexical anal-
ysis. By linearly combining lexical features with
acoustic features, they achieved high F-measures
when using Support Vector Machine (SVM). Since
a bag-of-words model is a naive model, there may
be more valuable lexical information that it cannot
capture. Moreover, as lexical and acoustic features
are extracted from different domains, a single layer
linear combination may not yield the optimal results.
In 2010, Interspeech launched a Paralinguistic
Challenge (Schuller et al, 2010) that included the
task of detecting LOI from speech as a subchallenge.
Competitors were given conversational speech cor-
pora with annotated LOI, baseline acoustic features,
and two baseline results. The evaluation metric used
for the challenge was primarily the cross correlation
2 (CC) measure (Grimm et al, 2008), with mean
linear error 3 (MLE) also taken into consideration.
The baseline was built only on acoustic features, and
the CC and MLE for Training vs. Development sets
were 0.604 and 0.118. For the test data, CC and
MLE scores of 0.421 and 0.146 were observed.
Gajsek et al (2010) participated in this challenge
and proposed the use of Gaussian Mixture Models
as Universal Background Model (GMM-UBM) with
relevance MAP estimation for the acoustic data.
This is based on the success of GMM-UBM mod-
2Pearson product-moment correlation coefficient is a mea-
sure of the linear dependence that is widely used in regression
settings.
3MLE is a regression performance measure for the mean ab-
solute error between an estimator and the true value.
eling in the speaker identification tasks (Reynolds et
al., 2000). They achieved CC and MLE of 0.630 and
0.123 in the training vs. development condition, but
CC and MLE of only 0.390 and 0.143 in the testing
condition. This performance may have been due to
the fact that different subsets of the corpus include
different speakers: acoustic features alone may not
be robust enough to capture the speaker variation.
Jeon et al (2010) approach won the 2010 Sub-
challenge for this task. In addition to the baseline
acoustic features provided, they used term frequency
and a subjectivity dictionary to mine the lexical in-
formation. In addition to a linear combination of
all lexical and acoustic features, they designed a hi-
erarchical regression framework with multiple level
of combinations. Its first two combiners tackle the
prediction problems from different acoustic classi-
fiers and then uses a final stage SVM classifier to
combine the overall acoustic predictions with lexi-
cal predictions to form the final output. They report
a result of 0.622 for CC and 0.115 for MLE. On the
test set, they report CC and MLE of 0.428 and 0.146
respectively.
3 Our System
Unlike previous approaches, we emphasize lexical
modeling, to counter problems of speaker variation
in acoustic features (Jeon et al, 2010). We propose
an improved version of standard TFIDF (Spa?rck
Jones, 1972) ? Discriminative TFIDF ? which
computes the IDF score of the target word by dis-
criminating its different mean LOI score tags during
training to produce more informative keyword spot-
ting in testing.
In addition to Discriminative TFIDF, we uti-
lize the Dictionary of Affect in Language (DAL)
(Whissell, 1989) to detect lexical affect and com-
pute an utterance-level affect score. To maximize
the coverage of lexical cues, we also train trigram
language models on the training data to capture con-
textual information and use the test output log like-
lihoods and perplexities as features. Besides these
lexical features and the 1582 baseline acoustic fea-
tures from the Interspeech Paralinguistic Challenge,
we extract 32 additional prosodic and voice quality
features using Praat (Boersma, 2001). In order to
model sentence-level prosodic events, we use Au-
153
ToBI (Rosenberg, 2010) to extract pitch accent and
phrase-based features. These features are described
in detail in Section 3.2.
The simplest approach to classification is to in-
clude all features in a single classifier. However,
different features streams include different number
of features, extracted and represented in different
domains. The Sum Rule approach (Kittler et al,
1998) is an early solution to this classifier combi-
nation problem. Instead, we train 1st-tier classi-
fiers for each of the feature streams and then train
a 2nd-tier classifier to weight the posterior predic-
tions of the 1st-tier classifiers. We further improve
this method by integrating a novel model which con-
siders the 1st-tier multistream prediction feedback
from the seed samples and uses a mean cosine simi-
larity method to measure the distance between a new
instance and prominent seed samples. We use this
similarity measure to improve classification.
3.1 Corpus
The corpus we use in our experiments is the 2010
Paralinguistic Challenge Affect Subchallenge cor-
pus Technische Universta?t Munche?n Audiovisual In-
terest Corpus (TUM AVIC), provided by Schuller
(2010). The corpus consists of 10 hours of audio-
visual recordings of interviews in which an inter-
viewer provides commercial presentations of vari-
ous products to a subject. The subject and inter-
viewer discuss the product, and the subject com-
ments on his/her interest in it. Subjects were in-
structed to relax and not to worry about politeness
in the conversation. 21 subjects participated (11
male, 10 female), including three Asians and the rest
of European background. All interviews were con-
ducted in English; while none of the subjects were
native speakers, all were fluent. 11 subjects were
younger than 30; 7 were between 30-40; and 3 were
over 40. The subject portions of the recordings were
segmented into speaker turns (continuous speech by
one speaker with backchannels by the interviewer
ignored). These were further segmented into sub-
speaker turns at grammatical phrase boundaries such
that each segment is shorter than 2sec.
These smaller segments were annotated by four
male undergraduate psychology students for subject
LOI, using a 5-point scale as follows: (-2) Disin-
terest (subject is totally tired of discussing this topic
and totally passive); (-1) Indifference (subject is pas-
sive and does not want to give feedback); (0) Neu-
trality (subject follows and participates in the dialog,
but it is not recognized if she/he is interested in the
topic); (1) Interest (subject wants to talk about the
topic, follows the interviewer and asks questions);
(2) Curiosity (subject is strongly interest in the topic
and wants to learn more.) A normalized mean LOI
is then derived from mean LOI/2, to map the scores
into [-1, +1]. (Note that no negative scores occur
for this corpus.) In our experiments, we consider
the normalized mean LOI score as the label for each
sub-speaker turn segment; we refer to this as ?mean
LOI? below. The corpus was divided for the Sub-
challenge into training, development, and test cor-
pora; we use these divisions in our experimens.
3.2 Feature Sets
Table 1 provides an overview of the feature sets in
our system.
Discriminative TFIDF
In the standard vector space model, each word
is associated with its Term Frequency (TF) in the
utterance. The Inverse Document Frequency (IDF)
provides information on how rare the word is over
all utterances. The standard TFIDF vector of a term
t in an utterance u is represented as V(t,u):
V (t, u) = TF ? IDF =
C(t, u)
C(v, u)
? log
|U |
?
u(t)
TF is calculated by dividing the number of occur-
rences of term t in the utterance u by the total num-
ber of tokens v in the utterance u. IDF is the log of
the total number of utterances U in the training set,
divided by the number of utterances in the training
set in which the term t appears. u(t) can be viewed
as a simple function: if t appears in utterance u, then
it returns 1, otherwise 0.
In Discriminative TFIDF we add additional infor-
mation to the TFIDF metrics. When calculating IDF,
we weight each word by the distribution of its labels
in the training set. This helps us to weight words by
the LOI of the utterances they are uttered in. An in-
tuitive example is this: Although the words ?chaos?
and ?Audi? both appear once in the corpus, the oc-
currence of ?Audi? is in an utterance with a Mean
LOI score of 0.9, while ?chaos? appears in an utter-
ance with a label of 0.1. A standard TFIDF approach
154
Feature Sets Features
Discriminative TFIDF Sum of word-level Discriminative TFIDF scores
Lexical Affect Scoring Sum of word-level lexical affect scores
Language Modeling Trigram language model log-likelihood and perplexity
Acoustic Features 1582 acoustic features. Detail see Schuller et. al, (2010)
Pulses # Pulses, # Periods, Mean Periods, SDev Period
Voicing Fraction, # Voice Breaks, Degree, Voiced2total Frames
Jitter Local, Local (absolute), RAP, PPQ5
Shimmer Local, Local (dB), APQ3, APQ5, APQ11
Harmonicity Mean Autocorrelation, Mean NHR, Mean NHR (dB)
Duration Seconds
Fundamental Frequency Min, Max, Mean, Median, SDev, MAS
Energy Min, Max, Mean, SDev
Prosodic Events Pitch accents, intermediate phrase, and intonational boundaries.
Table 1: Feature Sets. RAP: Relative Average Perturbation. PPQ5: five-point Period Perturbation Quotient. APQn:
n-point Amplitude Perturbation Quotient. NHR: Noise-to-Harmonics Ratio. MAS: Mean Absolute Slope.
will give these two terms the same score. To differ-
entiate the importance of these two words, we define
our Discriminative TFIDF measure as follow:
V (t, u) =
C(t, u)
C(v, u)
?log
|U |
?
u(t) ? (1? |MeanLOI|)
Here, the Mean LOI score ranging from (0,1) is
the label of each utterance. Instead of summing
the u(t) scores directly, we now assign a weight to
each utterance. The weight is (1? |MeanLOI|) in
our task. The overall IDF score of words important
to identifying the LOI of an utterance will thus be
boosted, as the denominator of the IDF metric de-
creases compared to the standard TFIDF. Discrimi-
native TFIDF can be viewed as a generalized version
of Delta TFIDF (Martineau and Finin, 2009) that can
be used in various regression settings.
Wang and McKeown (2010) show that adding
Part-of-Speech (POS) information to text can be
helpful in similar classification tasks. So we have
also used the Stanford POS tagger (Toutanova and
Manning, 2000) to tag these transcripts before cal-
culating the Discriminative TFIDF score.
Lexical Affect Scoring
Whissell?s Dictionary of Affect in Language
(DAL) (Whissell, 1989) attempts to quantify emo-
tional language by asking raters to judge 8742 words
collected from various sources including college es-
says, interviews, and teenagers descriptions of their
own emotional state. Its pleasantness (EE) score in-
dicates the negative or positive valence of a word,
rated on a scale from 1 to 3. For example, ?aban-
don? scores 1.0, implying a fairly low level of pleas-
antness. A previous study (Agarwal et al, 2009)
notes that one of the advantages of this dictionary
is that it has different scores for various forms of a
root word. For example, the words ?affect? and ?af-
fection? have very different meanings; if they were
given the same score, the lexical affect quantifica-
tion might not be discriminative. To calculate an
utterance?s lexical affect score, we first remove the
stopwords and then sum up 4 the EE score of each
word in the utterance.
Statistical Language Modeling
In order to capture the contextual information and
maximize the use of lexical information, we also
train a statistical language model to augment the
Discriminative TFIDF and lexical affect scores. We
train trigram language models on the training set
using the SRI Language Modeling Tookit (Stolcke,
2002). In the testing stage, the log likelihood and
perplexity scores are used as language modeling fea-
tures. Due to the data sparsity issue, we are not able
to train language models on subsets of training data
that correspond to different LOI scores.
4We have experimented with Min, Max and Mean scores,
but the results were poor.
155
Acoustic, Prosodic and Voice Quality Features
As noted above, the TUM AVIC corpus includes
acoustic features (Schuller et al, 2010) for all of the
data sets. These include: PCM loudness, MFCC[0-
14], log Mel Frequency Band[0-7], Line Spectral
Pairs Frequency [0-7], F0 by Sub-Harmonic Sum.,
F0 Envelope, Voicing Probability, Jitter Local, Jit-
ter Difference of Difference of Periods, and Shim-
mer local. We have extracted an additional 32 stan-
dard prosodic and voice quality features to aug-
ment these, including Glottal Pulses, Voicing, Jitter,
Shimmer, Harmonicity, Duration, Fundamental Fre-
quency, and Energy (See Table 1).
Prosodic Event Features
To examine the contribution of higher-level
prosodic events, we have also experimented with
AuToBI (Rosenberg, 2010) to automatically de-
tect pitch accents, word boundaries, intermedi-
ate phrase boundaries, and intonational bound-
aries in utterances. AuToBI requires annotated
word boundary information; since we do not have
hand-annotated boundaries, we use the Penn Pho-
netics Lab Forced Aligner (Yuan and Liberman,
2008) to align each utterance with its transcription.
We use AuToBI?s models, which were trained on
the spontaneous speech Boston Directions Corpus
(BDC) (Hirschberg and Nakatani, 1996), to identify
prosodic events in our corpus.
3.3 Fusion Learning Approaches
Assuming that our various lexical, acoustic and
prosodic feature streams are informative to some ex-
tent when tested separately, we want to combine in-
formation from the streams in different domains to
improve prediction. We experimented with several
approaches, including Bag-of-Features, Sum Rule
combination, Hierarchical Fusion, and a new ap-
proach. We present here results of each on our LOI
prediction task. In the Bag-of-Features approach,
a simple classification method includes all features
in a single classifier. A potential problem with this
method is that, when combining 1582 acoustic fea-
tures with 10 lexical features, the classifier will treat
them equally, so potentially more useful lexical fea-
tures will not be evaluated properly. A second prob-
lem is that our features are extracted from differ-
ent domains using different methods, and normal-
ization across domains is not possible in a bag-of-
features classification/regression approach. Another
possible approach is the Sum Rule Combiner, which
uses product or sum rules to combine the predictions
from 1st-tier classifiers. Kittler et al (1998) show
that the Sum Rule approach outperforms the product
rule, max rule and mean rule approaches when com-
bining classifiers. Their sensitivity analysis shows
that this approach is most resilient to estimation er-
rors.
A third method of combining features is the Hier-
archical Fusion approach of fusing multistream in-
formation, which involves multiple classifiers and
performs classification/regression in multiple stages.
This can be implemented by first training 1st-tier
classifiers for each single stream of features, collect-
ing the predictions from these classifiers, and train-
ing a 2nd-tier supervector classifier to weight the im-
portance of predictions from the different streams
and make a final prediction. The rationale behind
this approach is to solve the cross-domain issue by
letting the 2nd-tier classifier weight the streams, as
the predictions from 1st-tier classifiers will be in a
unified/normalized form (e.g. 0 to 1 in this task).
The Multistream Prediction Feedback and Mean
Cosine Similarity based Hierarchical Fusion
Our Multistream Prediction Feedback and Mean
Cosine Similarity based Hierarchical Fusion ap-
proach combines a similarity based two-stage ap-
proach with a multistream feedback approach. Fig-
ure 1 shows the architecture of this system. It is
based on the intuition that, if we can identify the
prominent samples (e.g. the samples that all 1st-tier
classifiers assign high average prediction scores),
then we can measure the average distance between
a new sample and all these prominent samples in the
Euclidean Space. Furthermore, we can use this av-
erage distance (average similarity) as a new feature
to improve the 2nd-tier classifier?s final prediction.
To implement this process, we first train five
1st-tier Additive Logistic Regression (Friedman et
al., 2000) classifiers and a Random Subspace meta
learning (Ho, 1998) 1st-tier classifier (for the acous-
tic stream), using six different feature streams in our
training data. In the testing stage, we use a random
subset of the test set as seed samples. Next, we run
the seed samples for each of these 1st-tier classifiers
156
Seed 
Samples
Discriminative 
TFIDF
2nd-Tier 
Classifier:
RBF Kernel SVM
1st-Tier 
Addictive Regression and 
Random Subspace  
Classifiers:
New 
Samples
Lexical Affect 
Scoring
Language 
Modeling
Prosodic and 
Voice Quality
Acoustic
Prosodic Events
1st-Tier Predictions (seed)
S1: 0.8, 0.9, 0.6, 0.5, 0.7, 0.8S2: 0.3, 0.5, 0.4, 0.3, 0.2, 0.4S3: 0.4, 0.1, 0.3, 0.3, 0.1, 0.5?????
Maxn (Mean(Si))
Top-N
Prominent Samples
Avg. Cosine Similarly
1st-Tier Predictions (new)
S4: 0.7, 0.8, 0.6, 0.5, 0.8, 0.8, 99%S5: 0.6, 0.5, 0.4, 0.3, 0.7, 0.4, 72%?????
Final Prediction
Figure 1: The Overview of Multistream Prediction Feedback and Mean Cosine Similarity based Hierarchical
Fusion Learning
to obtain prediction scores ranging from 0 to 1. Now,
we take the mean of these predicted scores for each
sample, and use the following method to select the
top n samples from the seed samples S as ?promi-
nent samples?:
Prominent(S, n) = Maxn(Mean(S))
Recall that the cosine similarity (Salton, 1989) of
two utterances Ui, Uj in the vector-space model is:
cos(Ui, Uj) =
Ui ? Uj
||Ui||2 ? ||Uj ||2
where ??? indicates ?dot product?. Now, given our
hypothesized prominent samples, for each of these
samples and new samples, we choose the original
Discriminative TFIDF, Lexical Affect Scoring, Lan-
guage Modeling, Prosodic and Voice Quality, and
Prosodic Event features as k vectors to represent all
the samples in Euclidean Space. The reason we drop
the acoustic features from the vector space model is
because of the dimensionality issue ? 1582 acous-
tic features. We substitute our 32 standard prosodic
features instead. Now we use the mean cosine simi-
larity score to represent how far a new sample Un is
from the prominent samples US in the space:
Sim(Un, US) = Mean
?
?
?k
i=1 Vn ? Vs?
?k
i=1 V
2
n ?
?
?k
i=1 V
2
s
?
?
In the next step, we add this mean cosine sim-
ilarity measure as a new feature and include it in
the 2nd-tier classifier for reclassification. Now, in
the reclassification stage, all 1st-tier feature stream
predictions will be re-weighted by the new 2nd-tier
classifier that incorporated with Multistream Feed-
back information.
The reason why the Multistream Prediction Feed-
back is useful in this task is that, like many spoken
language understanding tasks, in LOI detection, if
we have a different set of speakers with different
genders, ages, and speaker styles, the overall feature
distribution for lexical, prosodic, and acoustic cues
in the test set can be very different from the training
set. Traditional speaker adaptation techniques typi-
157
cally focus only on the acoustic stream and may be
very expensive to perform. So, by extracting more
knowledge about the lexical, prosodic, and acoustic
features distributions in test set using our novel ap-
proach, we will have a better understanding about
the skewed distributions in the test set. In addition,
our approach is inexpensive and does not require ex-
tra unlabeled data.
4 Experiments and Results
We conduct our experiments in three parts. First, we
examine how well the Discriminative TFIDF feature
performs, compared with standard TFIDF feature.
Secondly, we look at how different feature sets influ-
ence our results. For the first two parts, we evaluate
our features using the Subchallenge training vs. de-
velopment sets only. Finally, we compare our sim-
ilarity based multistream fusion feedback approach
to other feature-combining approaches. We exam-
ine our final system first comparing training vs. de-
velopment performance, and then combined training
and development sets vs. the test set. WEKA (Wit-
ten and Frank, 2005) and LIBSVM (Chang and Lin,
2001) are used for regression.
4.1 TFIDF v.s. Discriminative TFIDF
Method CC MLE
TFIDF 0.296 0.142
D-TFIDF 0.368 0.140
S-D-TFIDF 0.381 0.136
Table 2: Single TFIDF Feature Stream Single Re-
gression Results (Train vs. Develop, Additive Logis-
tic Regression). D-TFIDF: Discriminative TFIDF. S-D-
TFIDF: the POS tagged version of D-TFIDF. CC: Cross
Correlation. MLE: Mean Linear Error.
When working with the training and develop-
ment sets, we are able to access the label and tran-
scriptions of each set to calculate the Discrimina-
tive TFIDF scores. For the testing scenario dis-
cussed in in Section 4.3, we do not have these anno-
tations. So, we redefine the task as a keyword spot-
ting task, where we can use the identified keywords
in the training and development sets as keyword fea-
tures in testing. We also sum up the word-level
TFIDF scores and use the sentence-level TFIDF as
a single feature for the classification experiment.
The regression algorithm we use is Additive Logis-
tic Regression with 50 iterations. Table 2 shows
how different approaches perform in the experiment.
We see that the Syntactic Discriminative TFIDF ap-
proach is much more informative than the standard
TFIDF approach. Note that, after calculating the
global IDF score, the standard TFIDF approach se-
lects 732 terms as top-1 level keywords. In contrast,
our Discriminative TFIDF has stronger discrimina-
tive power and picks a total number of 59 truly rare
terms as top-1 level keywords.
4.2 Regression with Different Feature Streams
Table 3 shows performance using different feature
streams in our system. We see that the acoustic
Feature Streams CC MLE
S-D-TFIDF 0.394 0.132
Language Modeling 0.404 0.141
Prosodic Events 0.458 0.133
Lexical Affect Scoring 0.459 0.132
Standard Prosody + VQ 0.591 0.122
Acoustic 0.607 0.118
Multistream Feedback (n=3) 0.234 0.150
Multistream Feedback (n=10) 0.262 0.149
Multistream Feedback (n=20) 0.290 0.146
Table 3: Comparing Contributions of Different Fea-
ture Streams in the 2nd-tier Classifier (Training vs. De-
velopmen, Random Subspace for the 1st-tier classifier of
Acoustic Stream, and Additive Logistic Regression for
other 1st-tier classifiers. Radial Basis Function (RBF)
Kernel SVM as 2nd-tier Classifier.) S-D-TFIDF: the POS
tagged version of D-TFIDF. VQ: Voice Quality. n: Top-n
Feedback. CC: Cross Correlation. MLE: Mean Linear
Error.
and prosodic features are the dominating features in
this task. The Prosodic Events feature stream also
emerges as a new informative high-level prosodic
feature in this task.
When testing the multistream feedback informa-
tion as a single feature stream, we see in the bottom
half of Table 3 that CC and MLE are improved when
we increase the number of prominent samples. Dis-
criminative TFIDF and Language Modeling are also
158
important, as seen from these results, but the Lexi-
cal Affect Scoring feature performs best among the
lexical features in this task. We suspect that the rea-
son may be a data sparsity issue, as we do not have a
large amount of data for training robust global Dis-
criminative IDF scores, language models, and the
feedback stream. In contrast, the DAL is trained on
much larger amounts of data.
4.3 Comparing with State-of-the-Art Systems
Table 4 compares our approach to alternative learn-
ing approaches. The first half of this table reports
results on training vs. development sets, and the sec-
ond half compares combined training and develope-
men vs. test set result.
Method CC MLE
Shuller et al,(2010) 0.604 0.118
Jeon et al, (2010) 0.622 0.115
Gajsek et al (2010) 0.630 0.123
Bag-of-features Fusion 0.602 0.118
Sum Rule Combination 0.617 0.117
SVM Hierarchical Fusion 0.628 0.115
Feedback + Hierarchical Fusion 0.640 0.113
Gajsek et al (2010) 0.390 0.143
Shuller et al,(2010) 0.421 0.146
Jeon et al, (2010) 0.428 0.146
Bag-of-features Fusion 0.420 0.145
Sum Rule Combination 0.422 0.138
SVM Hierarchical Fusion 0.450 0.131
Feedback + Hierarchical Fusion 0.480 0.131
Table 4: Comparing Different Systems. Above: Train-
ing vs. Development. Bottom: Combined Training+ De-
velopment vs. Test. CC: Cross Correlation. MLE: Mean
Linear Error.
Note that, in order to transcribe the test data, we
have trained a 20 Gaussian per state 39 MFCC Hid-
den Markov Model speech recognizer with HTK, us-
ing the training and development sets together with
TIMIT (Fisher et al, 1986), the Boston Directions
Corpus (BDC) (Hirschberg and Nakatani, 1996),
and the Columbia Game Corpus (Hirschberg et al,
2005). The word error rate (WER) is 29% on the
development set.
Note that a Bag-of-Features approach combin-
ing all features results in poorer performance than
the use of acoustic features alone. The Sum Rule
approach improves over this method by achieving
CC score of 0.422. Although the improvement of
CC seems small, it is extremely statistically signifi-
cant (Paired t-test with two-tailed P-value less than
0.0001), comparing to the Bag-of-features model.
However, when using the SVM as the 2nd-tier su-
pervector classifier to weight different prediction
streams, we achieve 0.628 CC and 0.115 MLE in
training vs. development data, and 0.450 CC and
0.131 MLE on the test set; this result is significantly
different from the Bag-of-features baseline (paired
t-test, p < 0.0001), but it is not significantly differ-
ent from the Sum Rule Combination approach.
Augmenting the SVM hierarchical fusion learn-
ing approach with multistream feedback, we observe
a significant improvement over all other systems and
methods. We obtain a final CC of 0.480 and MLE of
0.131 in the test mode, which is sigificantly differ-
ent from the Bag-of-features approach (paired t-test
p < 0.0001), but does not differ significantly from
the SVM hierarchical fusion approach.
5 Conclusion
Detecting levels of interest from speakers is an im-
portant problem for Spoken Dialog Understanding.
While earlier work, done in the 2010 Interspeech
Paralinguistic Affect Subchallenge, employing tra-
ditional acoustic features and shallow lexical fea-
tures, achieved good results, our new features ?
Discriminative TFIDF, lexical affect scoring, lan-
guage modeling, prosodic event ? when used with
standard prosodic features and our new Multistream
Prediction Feedback and Mean Cosine Similarity
heuristic-based Hierarchical Learning method im-
proves over all published results on the LOI cor-
pus. Our method is domain-independent and can
be adapted to other speech and language process-
ing areas where domain adaptation is expensive to
perform. In the future, we would like to experiment
with different distributional similarity measures and
bootstrapping strategies.
159
Acknowledgments
The first author was funded by Kathleen McKeown
while conducting the research. We would also like
to thank Andrew Rosenberg and three anonymous
reviewers for their useful comments.
References
Agarwal, Apoorv and Biadsy, Fadi and Mckeown, Kath-
leen R. 2009. Contextual Phrase-Level Polarity Anal-
ysis Using Lexical Affect Scoring And Syntactic N-
Grams. in EACL 2009.
Ai, Hua and Litman, Diane J. and Forbes-Riley, Kate and
Rotaru, Mihai and Tetreault, Joel and Purandare, Am-
ruta 2006. Using System and User Performance Fea-
tures to Improve Emotion Detection in Spoken Tutor-
ing Dialogs. in INTERSPEECH 2006.
Biadsy, Fadi and Rosenberg, Andrew and Carlson, Rolf
and Hirschberg, Julia and Strangert, Eva. 2008. A
Cross-Cultural Comparison of American, Palestinian,
and Swedish Perception of Charismatic Speech. in
Proceedings of the Speech Prosody 2008.
Boersma, Paul. 2001. Praat, a system for doing phonet-
ics by computer. in Glot International.
Chang,Chih-Chung and Lin, Chih-Jen. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at www.csie.ntu.edu.tw/? cjlin/libsvm.
Devillers, Laurence and Vidrascu, Laurence 2006. Real-
life Emotions Detection with Lexical and Paralinguis-
tic Cues on Human-Human Call Center Dialogs. in
INTERSPEECH 2006.
Fisher, William M. and Doddington, George R. and
Goudie-Marshall, Kathleen M. 1986. The DARPA
Speech Recognition Research Database: Specifica-
tions and Status. in DARPA Workshop on Speech
Recognition.
Friedman, Jerome and Hastie, Trevor and Tibshirani,
Robert. 2000. Additive logistic regression: a statis-
tical view of boosting. in Ann. Statist..
Gajs?ek, Rok and Z?ibert, Janez and Justin, Tadej and
S?truc, Vitomir and Vesnicer, Bos?tjan and Mihelic?,
France. 2010. Gender and Affect Recognition Based
on GMM and GMMUBM modeling with relevance
MAP estimation. in INTERSPEECH 2010.
Grimm, Michael and Kroschel, Kristian and Narayana,
Shrikanth. 2008. The Vera am Mittag German Audio-
Visual Emotional Speech Database. in IEEE ICME.
Hirschberg, Julia and Nakatani, Christine H. 1996. A
prosodic analysis of discourse segments in direction-
giving monologues. in ACL 1996.
Hirschberg, Julia and Benus, Stefan and Brenier, Jason
M. and Enos, Frank and Friedman, Sarah and Gilman,
Sarah and Gir, Cynthia and Graciarena, Martin and
Kathol, Andreas and Michaelis, Laura. 2005. Distin-
guishing Deceptive from Non-Deceptive Speech. in
INTERSPEECH 2005.
Ho, Tin Kam. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
PAMI.
Jeon, Je Hun and Xia, Rui and Liu, Yang. 2010. Level of
Interest Sensing in Spoken Dialog Using Multi-level
Fusion of acoustic and Lexical Evidence. in INTER-
SPEECH 2010.
Kittler, Josef and Hatef, Mohamad and Duin, Robert P.
W. and Matas, Jiri. 1998. On combining classifiers.
IEEE Transactions on PAMI.
Laskowski, Kornel and Burger, Susanne. 2007. Analysis
of the Occurrence of Laughter in Meetings. in INTER-
SPEECH 2007.
Liscombe, Jackson and Hirschberg, Julia and Venditti,
Jennifer J.. 2005. Detecting Certainness in Spoken
Tutorial Dialogues. in Eurospeech.
Martineau, Justin and Finin, Tim. 2009. Delta TFIDF:
An Improved Feature Space for Sentiment Analysis.
in ICWSM.
Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn,
Robert B. 2000. Speaker verication using adapted
gaussian mixture models. in Digital Signal Process-
ing.
Rosenberg, Andrew. 2010. AuToBI - A Tool for Auto-
matic ToBI Annotation. in INTERSPEECH 2010.
Salto, Gerard 1989. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of Informa-
tion by Computer.
Schuller, Bjo?ern, and Ko?hler, Niels and Mu?eller, Ronald
and Rigoll, Gerhard. 2006. Recognition of Interest
in Human Conversational Speech. in INTERSPEECH
2006.
Schuller, Bjo?ern, and Steidl, Stefan and Batliner, An-
ton and Burkhardt, Felix and Devillers, Laurence and
Mu?eller, Christian and Narayanan, Shrikanth. 2010.
The INTERSPEECH 2010 Paralinguistic Challenge.
in INTERSPEECH 2010.
Spa?rck Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval. in
Journal of Documentation.
Stolcke, Andreas. 2002. SRILM-an extensible language
modeling toolkit. in ICSLP 2002.
Toutanova, Kristina and Manning, Christopher D..
2000. Enriching the Knowledge Sources Used in
a Maximum Entropy Part-of-Speech Tagger. in
EMNLP/VLC-2000.
Tao, Tao and Zhai, ChengXiang 2006. Regularized esti-
mation of mixture models for robust pseudo-relevance
feedback. in SIGIR 2006.
160
Wang, William Yang and McKeown, Kathleen. 2010.
?Got You!?: Automatic Vandalism Detection in
Wikipedia with Web-based Shallow Syntactic-
Semantic Modeling. in COLING 2010.
Wang, Chingning and Zhang, Ping and Choi, Risook and
DEredita, Michael. 2002. Understanding consumers
attitude toward advertising. in Eighth Americas conf.
on Information System.
Witten, Ian H. and Frank, Eibe 2005. Data mining:
Practical machine learning tools and techniques, 2nd
Edition. San Francisco: Morgan Kaufmann.
Whissell, Cynthia. 1989. The Dictionary of Affect in
Language. in R. Plutchik and H. Kellerman, Editors,
Emotion: Theory Research and Experience.
Yuan, Jiahong and Liberman, Mark. 2008. Speaker iden-
tification on the SCOTUS corpus. in Proceedings of
acoustics ?08.
161
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 20?29,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
?Love ya, jerkface?: using Sparse Log-Linear Models to Build
Positive (and Impolite) Relationships with Teens
William Yang Wang, Samantha Finkelstein, Amy Ogan, Alan W Black, Justine Cassell
School of Computer Science, Carnegie Mellon University
{yww, slfink, aeo, awb, justine}@cs.cmu.edu
Abstract
One challenge of implementing spoken di-
alogue systems for long-term interaction is
how to adapt the dialogue as user and sys-
tem become more familiar. We believe this
challenge includes evoking and signaling as-
pects of long-term relationships such as rap-
port. For tutoring systems, this may addi-
tionally require knowing how relationships are
signaled among non-adult users. We therefore
investigate conversational strategies used by
teenagers in peer tutoring dialogues, and how
these strategies function differently among
friends or strangers. In particular, we use an-
notated and automatically extracted linguis-
tic devices to predict impoliteness and posi-
tivity in the next turn. To take into account
the sparse nature of these features in real data
we use models including Lasso, ridge estima-
tor, and elastic net. We evaluate the predictive
power of our models under various settings,
and compare our sparse models with stan-
dard non-sparse solutions. Our experiments
demonstrate that our models are more ac-
curate than non-sparse models quantitatively,
and that teens use unexpected kinds of lan-
guage to do relationship work such as signal-
ing rapport, but friends and strangers, tutors
and tutees, carry out this work in quite differ-
ent ways from one another.
1 Introduction and Related Work
Rapport, the harmonious synchrony between in-
terlocutors, has numerous benefits for a range of
dialogue types, including direction giving (Cas-
sell et al, 2007) or contributing to patient recov-
ery (Vowles and Thompson, 2012). In peer tutor-
ing, an educational paradigm in which students of
similar ability tutor one another, friendship among
tutors and tutees leads to better learning (Gartner et
al., 1971). With the burgeoning use of spoken dia-
logue systems in education, understanding the pro-
cess by which two humans build and signal rapport
during learning becomes a vital step for implement-
ing spoken dialogue systems (SDSs) that can initi-
ate (and, as importantly, maintain) a successful re-
lationship with students over time. However, im-
plementing a tutorial dialogue system that appropri-
ately challenges students in the way that peers do
so well (Sharpley et al, 1983), while still demon-
strating the rapport that peers can also provide, calls
for understanding the differences in communication
between peer tutors just meeting and those who are
already friends.
The Tickle-Degnen and Rosenthal (1990) model
provides a starting point by outlining the compo-
nents of rapport, including the finding that positiv-
ity decreases over the course of a relationship. The
popularity of this model, however, has not dimin-
ished the disproportionate attention that positivity
and politeness receive in analyses of rapport (Brown
and Levinson, 1978), including in the vast majority
of computational approaches to rapport-building in
dialogue (Stronks et al, 2002; Johnson and Rizzo,
2004; Bickmore and Picard, 2005; Gratch et al,
2006; McLaren et al, 2007; Cassell et al, 2007;
Baker et al, 2008; Bickmore et al, 2011). The
creation and expression of rapport is complex, and
can also be signaled through negative, or impolite,
exchanges (Straehle, 1993; Watts, 2003; Spencer-
Oatey, 2008) that communicate affection and re-
lationship security among intimates who can flout
common social norms (Culpeper, 2011; Kienpoint-
ner, 1997).
However, it is an open question as to whether such
rudeness is likely to impress a new student on the
first day of class. We must better understand how
and when impoliteness and other negative dialogue
moves can contribute to the development and ex-
pression of the rapport that is so important in educa-
tional relationships. In this analysis, then, we begin
with a corpus of tutoring chat data annotated with
a set of affectively-charged linguistic devices (e.g.
complaining, emoticons), and then differentiate be-
tween the linguistic devices that friend and stranger
interlocutors employ (with friendship standing as a
proxy for pre-existent rapport) and the resulting so-
cial effects or functions of those devices on the part-
ners.
Since our ultimate goal is to build an SDS that
can adapt to the user?s language in real time, we
also automatically extract lexical and syntactic fea-
tures from the conversations. And, in order to deter-
mine what the system should say to evoke particular
20
responses, we predict social effects in partner two
from the use of the linguistic devices in partner one.
Since we want to understand how the system can
deal with newly met peers as well as peers who
have become friends, we develop and evaluate our
model on dyads of friends and then evaluate the
same model with dyads of strangers, to examine
whether dyads with less a priori rapport react dif-
ferently to the same linguistic devices.
Of course, in addition to understanding the phe-
nomenon of rapport in all of its complexity, a major
challenge for building rapport-signaling SDS is to
construct a compact feature space that capture only
reliable rapport signals and generalizes well across
different speakers. Of course phenomena such as in-
sults, complaints and pet names, no matter how im-
portant, appear relatively rarely in data of this sort.
Training discriminative models with maximum like-
lihood estimators (MLE) on such datasets usually re-
sults in assigning too much weight on less frequent
signals. This standard MLE training method not
only produces dense models, but may also overes-
timates lower frequency features that might be unre-
liable signals and overfit to a particular set of speak-
ers. In recent studies on speaker state prediction that
use lexical features, it has been shown that MLE
estimators demonstrate large performance gaps be-
tween non-overlapping speaker datasets (Jeon et al,
2010; Wang et al, 2012a).
On the other hand, recent studies on `1/`2
based group penalty for evaluating dialogue systems
(Gonza?lez-Brenes and Mostow, 2011), structured
sparsity for linguistic structure prediction (Mar-
tins et al, 2011), and discovering historical legal
opinions with a sparse mixed-effects latent vari-
able model (Wang et al, 2012b) have all shown
concrete benefits of modeling sparsity in language-
related predictive tasks. We therefore apply sparsity-
sensitive models that can prevent less frequent
features from overfitting. We start with the `1-
regularized Lasso (Tibshirani, 1994) model, since,
compared to other covariance matrix based sparse
models, such as sparse Principal Component Anal-
ysis (PCA) and sparse Canonical Correlation Anal-
ysis (CCA), the Lasso model is straightforward and
requires fewer computing resources when the fea-
ture dimension is high. Hence, we compare the con-
tributions of both automated features and annotated
features using the proposed Lasso model to predict
impoliteness and positivity.
In addition to Lasso and a logistic regression base-
line, we introduce two alternative penalty models:
the non-sparse ridge (le Cessie and van Houwelin-
gen, 1992) estimator, and an elastic net model (Zou
and Hastie, 2005). The ridge estimator applies a
quadratic penalty for feature selection, resulting in
a smooth objective function and a non-sparse fea-
ture space, which can be seen as a strong non-sparse
penalty model. We investigate the elastic net model,
because it balances the pros and cons of Lasso and
ridge estimators, and enforces composite penalty. In
addition to the model comparisons, by varying the
different sizes of feature windows (number of turns
in the dialogue history), we empirically show that
our proposed sparse log-linear model is flexible, en-
abling the model to capture long-range dependency.
This approach also allows us to extend previous
work on speaker state prediction. Although speaker
state prediction has attracted much attention in the
dialogue research community, most studies have fo-
cused on the analysis of anger, frustration, and other
classic emotions (Litman and Forbes-Riley, 2004;
Liscombe et al, 2005; Devillers and Vidrascu, 2006;
Ai et al, 2006; Grimm et al, 2007; Gupta and Ni-
tendra., 2007; Metallinou et al, 2011). Recently,
Wang and Hirschberg (2011) proposed a hierarchi-
cal model that detects level of interest of speakers
in dialogue, using a multistream prediction feedback
technique. However, to the best of our knowledge,
we are among the first to study the problem of auto-
matic impoliteness and positivity prediction in dia-
logue. Because our ultimate goal is to build an SDS
that responds to users? language use over time, the
features from the user?s target turn that the model is
aiming to predict are not observable, which renders
the task more difficult than previous speaker state
detection tasks.
Our main contributions are three-fold: (1) analy-
sis of linguistic devices that function to signal rap-
port among friends - and their effects on non-friend
dyads; (2) detailed analyses of language behavior
features that predict these rapport behaviors - both
impoliteness and positivity - in the next turn of
teenagers? peer tutoring sessions; (3) an evaluation
of non-sparse and sparse log-linear models for pre-
dicting impoliteness and positivity.
By understanding the signals of rapport that a per-
son is likely to display in response to various lin-
guistic devices, we can begin to build an SDS that
can anticipate the social response and adapt to the
rapport-signaling efforts of its partner, both as a
newly introduced technology, and, over time, as a
system with whom the user has a rapport.
2 The Corpus
We use the data from a previous study evaluating the
impact of a peer tutoring intervention that monitored
students? collaboration and in some cases provided
adaptive support (Walker et al, 2011). In the inter-
vention, peer tutors observed the work of their tutee
21
and supported them through a chat interface as they
completed algebra problems. The system logged all
chat and other information about the problem steps.
Participants were 130 high school students (81 fe-
male) in grades 7-12 from one American high school
with some prior knowledge of the algebra material.
Participants were asked to sign up for the study with
a friend. Those who were interested but were un-
able to participate with a friend, were matched with
another unmatched participant. In an after-school
session, participants first took a 20-minute pre-test
on the math concepts, and then spent 20 minutes
working alone with the computer to prepare for tu-
toring. One student in each dyad was then randomly
assigned the role of tutor, while the other was given
the role of tutee, regardless of relative ability. They
spent the next 60 minutes engaging in tutoring. Fi-
nally, students were given a domain posttest isomor-
phic to the pretest.
54 dyads signed up as friends and 6 were un-
matched strangers. To compare behavior between
friends and strangers in the face of very different
data set sizes we use 48 friend dyads for training,
and select 6 friend and 6 stranger dyads as two sep-
arate test sets. The total number of utterances in the
friend training set, friend test set, and stranger test
set are 4538, 468 and 402. To perform turn-based
prediction experiments, we concatenate the text in
the utterances by the same speaker into a single turn,
and perform an ?OR? operation1 on features (See
Section 3 for details) in multiple utterances of the
same speaker to generate the turn-based binary fea-
tures.
3 Feature Engineering
In this section, we describe both the annotated and
automatically extracted features analyzed.
3.1 Annotated Features and Labels2
To understand what linguistic devices participated in
positivity and impoliteness during tutoring, we an-
notated all 60 dyads for surface-level language be-
haviors such as complaints, challenges (Culpeper,
1996) and praise. We also automatically identi-
fied chat features that socially color the communi-
cation, such as excessive punctuation[P] or capital-
ization[Ca]. Utterances could receive more than one
code, and inter-rater reliability ranged from K=.71
to K=1.
Because these linguistic behaviors may serve a
range of different functions in context, such as rude
1If any of the utterances within one turn has this feature
turned on, then we say that we have observed this feature in
this turn.
2We thank Erin Walker for data collection and annotation.
language serving to cement a relationship (Arding-
ton, 2006), or teasing to increase rapport (Straehle,
1993), we also annotate the social functionality
of each utterance in context, in terms of positivity
(K=.79)3 and impoliteness (K=.76), which are seen
as holding down opposite kinds of social functional-
ity (Terkourafi, 2008). Details of annotation can be
found in our recent work (Ogan et al, 2012).
Language Behavior Features
Language behavior features were annotated by
two raters, based on previous work on impo-
liteness (Culpeper, 1996), positivity (Boyer et
al., 2008), and computer-mediated communica-
tion (Herring and Zelenkauskaite, 2009), as fol-
lows:.
? Insults[Di] (?=1): Personalized negative voca-
tives or references. eg. ?you are so weird.?
? Challenges[Ch] (? =.91): Directly questioning
partner?s decision or ability. eg. Partner 1:
?see I am helping?, Partner 2: ?barely.?
? Condescensions / brags[C] (?=1): Asserting
authority or partner?s inferiority. eg. Tutee:
?nothing you have done has affected me what
so ever.?
? Message enforcer[Ef] (?=.85): Emphasizing
text or attracting partner?s attention. eg. ?Earth
to Erin.?
? Dismissal / Silencer / Curse[Cu] (? =.76): As-
serting unimportance of contribution/partner.
eg. ?shuttttt up computer.?
? Pet name[Pe] (? = .9): Vocatives that may or
may not be insulting. eg. ?whats up homie??
? Criticisms / exclusive complaints[EC] (?=.8):
Negative evaluation of partner. eg. ?You are so
bad at this dude.?
? Inclusive complaints[I] (?=.78): Complaints
directed outside the partner, such as at the task,
computers, or study. eg. ?This is really dumb,
ya think??
? Laughter[L] (?=1): eg. ?haha?, ?lol?
? Off-task[O] (?=.71): Doesn?t pertain to or ad-
vance tutorial dialogue. eg. ?Coming over after
this??
Impoliteness and Positivity Labels
While the surface-level features were coded based
on a single utterance, context determined the labels
for impoliteness and positivity, including the recent
tone of the dialogue and the partner?s response to
the utterance. Utterances were coded as positivity
(?=.79) when they included goals that directly added
positive affect into the exchange through praise, em-
pathy, reassurance, cooperative talk (McLaren et al,
3We use Cohen?s kappa in this study.
22
2011), task enthusiasm, and making or responding
to jokes. Impoliteness (?=.76) included both coop-
eratively rude utterances such as teasing (typical eg.
?hahah you?re the worst tutor ever?) and uncooper-
atively rude utterances that may cause offense (typ-
ical eg. ?um why don?t you try actually explainin
urself..?) (Kienpointner, 1997).
3.2 Automated Features
To compare the performance between what could be
automatically extracted from dialogue and hand an-
notation, we extracted 2,872 unigram and 12,016 bi-
gram features from the text corpus. Using the Stan-
ford PoS tagger4 with its attached model, we also
extracted 46 common part-of-speech tags from the
text. In addition to the above lexical and syntac-
tic features, we automatically extracted the capital-
ization features[Ca] that have at least one full word
(eg. ?CALM DOWN?) (Chovanec, 2009). Since
a recent text prediction task (Wang and McKeown,
2010) observed benefits from modeling punctua-
tion features[P], we extracted the expressive punc-
tuation that included at least one exclamation point
or more than one question-mark (eg. ?I don?t get
it?!??!?) (Crystal, 2001). We used a smiley dictio-
nary5 to extract the emoticons[E] that convey emo-
tional states (Sa?nchez et al, 2006) from text.
4 Sparse Log-Linear Models
We formulate our impoliteness and positivity predic-
tion problems as binary classifications. To do this,
we estimate the label y?t ? Bernoulli(??). First, we
introduce a standard log-linear parametrization6 to
our predictive tasks:
??~yt =
exp
?
i ~wi ~fi(~yt)
1 + exp
?
i ~wi ~fi(~yt)
, (1)
where ~f(~yt) is a set of feature functions computed
on the observation vector ~yt. The term ~wi puts a
weight on feature i for predicting impoliteness, and
our estimation problem is now to set these weights.
The log-likelihood and the gradient are:
` =
?
t
yt log ??~yt + (1? yt) log(1? ??~yt) (2)
?`
? ~w =
?
t
(
???~yt
? ~w
)(
yt
??~yt
? 1? yt
1? ??~yt
)
(3)
???~yt
? ~w =
(
??~yt ? (??~yt)2
)
~f(~yt), (4)
4http://nlp.stanford.edu/software/tagger.shtml
5http://www.techdictionary.com/emoticon.html
6We thank Jacob Eisenstein for the formulation of logistic
regression model.
so the parameters can be set using gradient as-
cent. To control the overall complexity, we can ap-
ply regularized models on the elements of ~w. A
sparsity-inducing model, such as the Lasso (Tibshi-
rani, 1994) or elastic net (Zou and Hastie, 2005)
model, will drive many of these weights to zero, re-
vealing important interactions between the impolite-
ness/positivity label and other features. Instead of
maximizing the log-likelihood, we can minimize the
following Lasso model that consists of the negative
log-likelihood loss function:
min
(
? `+
?
i
?1||~wi||
)
(5)
Since the Lasso penalty can introduce discontinu-
ities to the original convex function, we can also
consider an alternative non-sparse ridge estima-
tor (le Cessie and van Houwelingen, 1992) that has
the convex property:
min
(
? `+
?
i
?2||~wi||2
)
(6)
In addition to the Lasso and ridge estimators, the
composite penalty based elastic net model balances
the sparsity and smoothness properties of both Lasso
and ridge estimators:
min
(
? `+
?
i
?1||wi||+
?
i
?2||wi||2
)
(7)
Our log-linear model is quite flexible; by compar-
ing various restrictions, we can test different features
when modeling impoliteness and positivity. In addi-
tion, the model can incorporate features from previ-
ous time windows, which requires much less compu-
tational complexity compared to standard high order
Markov models. We use the L-BFGS method (Liu
and Nocedal, 1989) for the numerical optimization.
5 Empirical Experiments
We predict impoliteness vs. non-impoliteness and
positivity vs. non-positivity of an interlocutor in the
immediate future turn, given only information from
current/previous turns. Because accuracy, precision,
recall and F-measure are threshold-based point esti-
mation metrics that might prevent one from observ-
ing the big picture of system performance, we con-
sider the Receiver Operating Characteristic (ROC)
metric to evaluate the dynamics of the true posi-
tive rate vs. the false positive rate (Hanley and Mc-
Neil, 1982) in our system. We mainly use Area Un-
der Curve (AUC) as a metric to compare classifiers,
since it maps the ROC metric to a single scalar value
representing expected performance. A random clas-
sifier will have an AUC of 0.5 (Fawcett, 2006).
23
Models P Ca E L O Ef Pe Di C EC Ch Cu I
Impoliteness Prediction
Tr-Te .44 -1.10 .62 .72 .09 .64 .09 1.29 .96 .89 .69 .77 -0.19
Te-Tr -2.48 .54 -0.26 0.15 .59 1.62 .24 .22 .89 .72 .75 .04 -0.18
Positivity Prediction
Tr-Te -0.87 .19 .36 .55 1.06 -0.62 .69 -1.63 -1.57 .16 -0.41 1.22 .86
Te-Tr -1.39 -0.46 .70 .48 .46 .33 .62 -0.71 .70 -0.65 -0.47 -0.54 .78
Table 1: Comparing the Learned Weights of Different Features when Predicting the Partner?s Impoliteness in a Non-
Sparse Log-Linear Model. Tr-Te: predict tutee turn with tutor turn. Te-Tr: predict tutor turn with tutee turn. For full
name of features, see Section 3.
5.1 Comparing the Learned Weights of
Different Features
In our previous analysis of these data (Ogan et al,
2012), a PCA method allowed us to group linguistic
behaviors in order to address the issue of data spar-
sity. With the use of log-linear models, we are able
to investigate the contributions of individual lan-
guage behaviors in one student?s turn to the predic-
tion of social functions in their partner?s next turn. In
this experiment, we evaluate the weights of various
linguistic devices in a standard logistic regression
model. We found that behaviors commonly asso-
ciated with impoliteness were predictors of partner
impoliteness in the next turn, while positive behav-
iors such as laughter were predictors of upcoming
positivity. SDSs can leverage this knowledge to take
the partners lead during a tutoring session, using the
partners positivity or impoliteness to determine the
affect of the systems upcoming move. As we intend
to develop a system that acts as a tutee, however, we
further divided the analysis by tutoring role, inves-
tigating how partners in different roles employ lan-
guage features differently, such that the system can
act in accordance with its given role. Table 1 shows
the results.
Similarly to the collapsed factors in our previous
work, we found here that tutors and tutees do in
fact use language behaviors differently, and to ac-
complish different social functions. Effectively, this
means that certain language behaviors may instigate
impoliteness when said by one partner, but lead to
positivity when expressed by the other. For exam-
ple, tutee bragging predicts a response of positiv-
ity on behalf of the tutor (~w(TE)C = .7), perhaps be-cause the tutor wants to be supportive of a prote?ge??s
self-efficacy and success. Conversely, when the tu-
tor brags during a peer tutoring dialogue, the tu-
tee, who may feel threatened by the tutors bravado,
is extremely likely to respond with impoliteness (
~w(TR)C = .96). In a peer tutoring paradigm, whenthe more powerful partner (the tutor) expresses dom-
inance through self-inflation, the subordinate part-
ner may use impoliteness to regain some social con-
trol. On the other hand, some language behaviors
actively work to tear down this power imbalance,
such as inclusive complaining, where the partners
take an us against the task approach, building sol-
idarity through complaining about the experiment.
These utterances predict positivity whether used by
the tutor ( ~w(TR)I = .86) or tutee ( ~w(TE)I = .78).Other comparisons between weighted features by
role demonstrate similarly theoretically-motivated
findings that shed light on how language is used to
achieve social functions.
5.2 Comparing the Contributions of Different
Features on Friend and Stranger Datasets
A previous study (Ogan et al, 2012) on these same
data seemed to indicate that negative conversational
strategies composed of linguistic devices such as
complaining and insults were correlated with learn-
ing in the friend dyads and negatively correlated
with learning in strangers. However the small num-
ber of stranger dyads prevented them from draw-
ing conclusions about particular linguistic devices
from the data. Here, we empirically show the pre-
dictive performance of different feature sets on both
friend and stranger test sets in Table 2 , using a
sparse Lasso model with features from only the
current turn. In the impoliteness prediction task,
when predicting on the test set that consists of only
friends, we observe statistically significant improve-
ment over a random baseline, using surface-level
language behavior features, lexical, lexical + syn-
tactic, all automatic, and all features. When com-
bining all features, the best AUC is .621. The auto-
matic features, mainly including n-grams and part-
of-speech tags, have emerged as a useful automated
feature space. On the other hand, we do not observe
any significant results on the stranger datasets, sug-
gesting that strangers do not respond with impolite-
ness in the same way that friends do. When pre-
dicting positivity on the friend dataset, we see that
24
the performance of surface-level language behavior
features has dropped from the first task, and the sta-
tistical t-test is non-significant when comparing to
a random baseline. This is not surprising, because
we have shown in the previous section that surface-
level language behavior features are strong indica-
tors of impoliteness, but might not have advantages
in predicting positivity for friends. Interestingly, the
automated features outperform the combination of
all features, indicating a promising future for the ac-
tual deployment of an SDS that can interact using
appropriate positivity and impoliteness.
When predicting positivity in the stranger dataset,
we find the opposite trend. In contrast to the impo-
liteness prediction task, the overall performance on
the stranger dataset improved, and the lexical, lexi-
cal+syntactic, and all feature combination have sig-
nificantly outperformed the chance baseline. These
results suggest that positivity is a predictable behav-
ior among strangers, who may all express uniform
positivity across all dyads, while it is the impolite-
ness that is predictable among friends. Perhaps it
is that through the development of a rapport with a
partner, the particular ways in which positivity is ex-
pressed becomes personalized to the dyad, and can
no longer be applied to other groups who have their
own expressions of positivity. In other words, un-
like in Tolstoy?s world, here unhappy families are all
alike; every happy family is happy in its own way.
We must look to the easily-predictable impoliteness
among friends instead, arguing strongly for the in-
clusion of impoliteness in a model of rapport.
5.3 Comparing Logistic Regression, Lasso,
Ridge, and Elastic Net
While our previous work (Ogan et al, 2012) demon-
strated that PCA is a useful feature selection method
when there are only a dozen features, in this experi-
ment, the dimension of our feature space is substan-
tially higher, which aligns to the size of vocabulary.
Thus, covariance-based feature selection methods,
such as PCA, might be too slow. Here we compare
the performances of standard MLE trained logistic
regression, Lasso, non-sparse ridge, and elastic net
models. In particular, we demonstrate the predic-
tive power of Lasso and elastic net models, varying
distinct levels of sparsity. In the Figure 1, we show
the comparison of three different models in the im-
politeness prediction task. The horizontal axis rep-
resents different values of regularization coefficient
?. For the Lasso model and the elastic net model,
increasing the value ? will result in a sparser feature
space, and we set the ? = ?1 = ?2 in the elastic net
model to promote same level of sparsity and smooth-
ness. The result at ? = 0 represents the standard
Feature Sets F-AUC p S-AUC p
Impoliteness Prediction
Random .500 - .500 -
Behavior .596 .017 .505 .473
Lex .599 .014 .435 .819
Lex + POS .605 .009 .425 .857
All Auto .591 .022 .451 .751
All Features .621 .003 .427 .850
Positivity Prediction
Random .500 - .500 -
Behavior .549 .141 .527 .302
Lex .623 .003 .601 .025
Lex + POS .646 .001 .587 .047
All Auto .651 .001 .577 .070
All Features .641 .001 .608 .019
Table 2: Comparing contributions of different feature
streams on both friend and stranger testsets with Lasso
model when predicting impoliteness and positivity of the
next turn using only features from the current turn. ( F-:
the friend test set. S: the stranger test set. p: one-tailed
p-value by comparing to a random classifier. Behavior:
detailed surface-level language behavior features defined
in Section 3. Lex: unigram and bigram. POS: part-of-
speech features. All Auto: all automatically extracted
features (Lex + POS + punctuation + caps + emoti-
cons).)
non-sparse logistic regression model, which obtains
an AUC of .563. When introducing penalty for large
weights in this standard model, .4 to .5 significant
improvements (p = .003 for Lasso, p = .007 for
ridge, and p = .004 for elastic net) of AUC are
observed from Lasso, ridge and elastic net models
when ? = 1. The elastic net model that balances
sparsity and smoothness, has obtain the best result
in this experiment. The best result of elastic net
model is .63 when ? = 7. This experiment shows
that all three penalty models have outperformed the
non-sparse logistic regression model. The elastic net
model, which balances sparisty and smoothness, ob-
tains the best results when predicting impoliteness.
Figure 2 shows the comparison of three models on
the friend dataset in the positivity prediction task.
When ? = 0, the standard logistic regression model
has an AUC of .638. When increasing the ? to 1,
both Lasso and elastic net models have shown sig-
nificant improvements (both p < .001) in AUC, but
not the non-sparse ridge estimator. The Lasso model
is found to be the best model in this task: we obtain
better results when the model gets sparser until the
model is too sparse when ? = 6. In contrast to the
experiment in Figure 1, we see that both the ridge
and elastic net models do not very strong advantages
25
in this positivity prediction task. We hypothesize
that the reason why Lasso works better in the pos-
itivity task is that the frequency of positivity labels
is substantially higher than the impoliteness labels in
our corpus, so that a Lasso model that enforces full
`1 penalty fits better in this task. In contrast, since
the impoliteness label is less frequent, a denser elas-
tic net composite penalty model that preserve critical
features, works the best in the impoliteness predic-
tion task. In general, we can see that sparse log-
linear models outperform standard log-linear mod-
els as well as non-sparse ridge estimators in the two
tasks.
Figure 1: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Impoliteness
with Lasso, Ridge, and Elastic Net Models
Figure 2: Comparing Impacts of Different Levels of Spar-
sity on the Friend Dataset When Predicting Positivity
with Lasso, Ridge, and Elastic Net Models
5.4 Comparing Impacts of Different Feature
Window Sizes
A practical problem for parameter estimation in both
generative and discriminative models for dialogue
processing is to evaluate how much history the sys-
tem should take into account, so that it can have
enough information to make correct predictions. In
this experiment, we investigate the impact of using
different feature window sizes using the elastic net
model. We compare the two-tailed student t-test be-
tween the baseline that only uses features from the
current turn and models that use current + previous
n turn(s). For the friend dataset, when only using
the features from the current turn to predict the im-
politeness in the immediate next turn, we observe
an AUC of .619. The best result is obtained when
we combine the previous two turns together with the
current feature turn: an AUC of .635, significantly
better (p = .03) than only using the current turn win-
dow. The patterns on the non-friend dataset are less
clear, while the model obtains the best result when
window size is +3 previous turns, the improvement
is not significant (p = .962). In the positivity task,
we also observe benefits to incorporating larger fea-
ture windows. The AUC on the friend test set starts
at .638, when only using the current feature window
in the elastic net model. After incorporating larger
feature windows, we obtain the best result of .675 at
the +4 window (p = .04). Similarly, the AUC on
non-friend test set initializes at .618, but climbs to
.632 at the +4 window.
6 Error Analysis and Discussion
We performed an error analysis to understand the
contexts under which our model failed to accurately
predict a students? social response, and discuss the
implications of these examples based on a theoret-
ical understanding of the roles of tutors and tutees
as well as friends and strangers. The following is
an example error produced when looking only at the
previous turn to predict the current turn:
? Tutee (impolite): ?dude thats def wrong i gotta
subract 16m not just 16? (the current turn)
? Tutor (non-impolite): ?16m is what has to be
subtracted from both sides? (the next turn, pre-
dicted incorrectly)
In the segment above the tutee challenges the tutor
by pointing out a ?def? mistake; the tutor responds
with a task-oriented contribution that moves the di-
alogue forward, but does not escalate the face threat
(Ogan et al, 2012). And, in fact, if we look one
more turn back in the history, the tutor once again
uses calm language: ?wait it says youre wrong i dont
know why ust wait?. The increased window size
is implicitly evoking the differential conversational
strategies of tutors vs. tutees. And while the current
data set is too small to build separate models for tu-
tors and tutees, in this case (and based on the prior
work in Ogan et al, 2012), accounting for role dis-
tinctions that differentiate strategies taken by tutors
and tutees is the likely reason behind the improve-
ment due to window size.
Conversely to the friend data set, the false nega-
tives that occur when predicting impoliteness in the
stranger data set are not improved by increasing the
26
window size, as is demonstrated in the following ex-
change:
? Tutor (non-impolite): ?subtract ym from both
sides.?
? Tutee (non-impolite): ?first step? first Step??
? Tutor (non-impolite): ?subtract hb from both
sides? (the current turn)
? Tutee (impolite): ?first step? FIRST
STEP??????????? (the next turn, predicted in-
correctly)
The impolite tutee utterance at turn 4 is predicted
to be non-impolite when analysis is limited to the
previous turn, as is also shown in the first example
in this section. However, unlike the previous ex-
ample which improved with an expanding window
size, looking back to turns 1 and 2 does not improve
the model. While we do not have enough stranger
dyads to completely explore this phenomenon, it
seems clear that strangers? responses do not follow
the same patterns as friends. The current unpre-
dictability of strangers can be due to a number of
social phenomena, such as less affect (both posi-
tive and negative) overall, which results in a differ-
ent conversational flow. Less overall affect means
that there is less likely to be useful information in
the previous utterances. This is an important dis-
tinction between designing models for dyads with
rapport and those without, which is a primary con-
cern in the development of social SDSs. Among
strangers, other techniques may need to be used to
increase model accuracy, such as looking at the con-
tent of the utterances to determine whether or not a
speaker had been repeating themselves, as is shown
in this example, which could likely be an indicator
of rudeness.
As a final example of how the error analysis
can reveal important phenomena for future study,
when examining the prediction of positivity on the
stranger test set, we first observe that emoticons
are useful indicators of positivity. However, some-
times emoticons serve quite different social func-
tions, which leads to false positives:
? Tutor (non-positivity): ?Simplify ! :)? (the cur-
rent turn)
? Tutee (non-positivity): ?y didnt it chang? (the
next turn, predicted incorrectly)
Here, the smiley face is used by the tutor primarily
to mitigate the face threat of an impolite command.
However, since the experiment reported in Section
6.1 shows that our model attributes more weight to
emoticons when predicting positivity, the model errs
on this utterance. Here the error analysis suggests
that in fact we might need to investigate more com-
plicated latent variable models to capture the subtle
social functionality of some language use in context.
7 Conclusion
Long-term relationships involve the expression of
both positive and negative sentiments and, paradox-
ically, both can serve to increase closeness. In this
paper, we have addressed the novel task of predict-
ing impoliteness and positivity in teenagers? peer tu-
toring conversations, and our results shed light on
what kinds of behaviors evoke these social functions
for friends and for strangers, and for tutors and tu-
tees. Our investigation has successfully predicted
impoliteness and positivity on the basis of both an-
notated and automatically extracted features, sug-
gesting that a dialogue system will one day be able to
employ analyses such as these to signal relationships
with users. And while social features such as those
we annotated are naturally quite rare in dialogue, our
quantitative experiments have demonstrated the ca-
pabilities of modeling sparsity in log-linear models:
elastic net and Lasso models outperformed standard
logistic regression model and the non-sparse ridge
penalty model.
We found that positivity is much more predictable
for strangers than is impoliteness, while the oppo-
site was true for friends. This could lend support for
the importance of positivity as a rapport-signaling
function in the early stages of a relationship (as
in (Tickle-Degnen and Rosenthal, 1990)), and indi-
cating the need for further research on the increasing
importance of impoliteness as a rapport signal over
the course of relationship development.
We also found that performance on the prediction
tasks increased with larger feature window sizes,
particularly for impoliteness among friends and pos-
itivity among strangers. From our error analysis,
we see that this improvement may arise because dif-
ferent behaviors predict impoliteness and positivity
based on the social role of the speaker. Thus tu-
tee bragging predicts positivity in tutors, while tu-
tor bragging negatively predicts positivity among tu-
tees. The power differential between the two may
lead tutees to want to take tutors ?down a peg? while
tutors struggle to maintain the position of power in
the dyad.
While results such as these may seem specific to
teenage peer tutors, the general conclusion remains,
that linguistic devices have different social functions
in different contexts, and dialogue systems that in-
tend to spend a lifetime on the job will do well to
adapt their language to the stage of relationship with
a user, and the social role they play.
27
References
Hua Ai, Diane J. Litman, Kate Forbes-Riley, Mihai Ro-
taru, Joel Tetreault, and Amruta Purandare. 2006.
Using system and user performance features to im-
prove emotion detection in spoken tutoring dialogs. In
Proceedings of the Ninth International Conference on
Spoken Language Processing (Interspeech 2006).
Angela M. Ardington. 2006. Playfully negotiated activ-
ity in girls talk. Journal of Pragmatics, 38(1):73 ? 95.
Rachel E. Baker, Alastair J. Gill, and Justine Cassell.
2008. Reactive redundancy and listener comprehen-
sion in direction-giving. In Proceedings of the 9th
SIGdial Workshop on Discourse and Dialogue.
Timothy W. Bickmore and Rosalind W. Picard. 2005.
Establishing and maintaining long-term human-
computer relationships. ACM Transactions on
Computer-Human Interaction.
Timothy Bickmore, Laura Pfeifer, and Daniel Schulman.
2011. Relational agents improve engagement and
learning in science museum visitors. In Proceedings
of the 10th international conference on Intelligent vir-
tual agents, IVA?11.
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis,
Mladen Vouk, and James Lester. 2008. Balancing
cognitive and motivational scaffolding in tutorial di-
alogue. In Proceedings of the 9th international con-
ference on Intelligent Tutoring Systems, ITS ?08.
Penelope Brown and Stephen Levinson. 1978. Uni-
versals in language usage: Politeness phenomena. In
Questions and politeness: Strategies in social interac-
tion.
Justine Cassell, Alastair J. Gill, and Paul A. Tepper.
2007. Coordination in conversation and rapport. In
Proceedings of the Workshop on Embodied Language
Processing, EmbodiedNLP ?07, pages 41?50, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jan Chovanec. 2009. Simulation of spoken interaction in
written online media texts. Brno Studies in English.
David Crystal. 2001. Language and the internet. Cam-
bridge University Press.
Jonathan Culpeper. 1996. Towards an anatomy of impo-
liteness. In Journal of Pragmatics.
Jonathan Culpeper. 2011. Impoliteness: Using language
to cause offence.
Laurence Devillers and Laurence Vidrascu. 2006. Real-
life emotions detection with lexical and paralinguistic
cues on human-human call center dialogs. In Proceed-
ings of the Ninth International Conference on Spoken
Language Processing (Interspeech 2006).
A Gartner, M Kohler, and F Riessman. 1971. Children
teach children: Learning by teaching. In New York and
London: Harper and Row.
Jose? Gonza?lez-Brenes and Jack Mostow. 2011. Which
system differences matter? using l1/l2 regulariza-
tion to compare dialogue systems. In Proceedings of
the SIGDIAL 2011 Conference, pages 8?17, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Jonathan Gratch, Anna Okhmatovskaia, Francois
Lamothe, Stacy Marsella, Mathieu Morales, Rick J.
van der Werf, and Louis-Philippe Morency. 2006.
Virtual rapport. In Proceedings of the International
Conference on Intelligent Virtual Agents (IVA 2006).
M. Grimm, E. Mower K. Kroschel, and S. Narayanan.
2007. Primitives-based evaluation and estimation of
emotions in speech. In Speech Communication.
P. Gupta and R. Nitendra. 2007. Two-stream emo-
tion recognition for call center monitoring. In Pro-
ceedings of the 8th Annual Conference of the Inter-
national Speech Communication Association (Inter-
speech 2007).
Susan C. Herring and Asta Zelenkauskaite. 2009. Sym-
bolic capital in a virtual heterosexual market. In Writ-
ten Communication.
Je Hun Jeon, Rui Xia, and Yang Liu. 2010. Level of in-
terest sensing in spoken dialog using multi-level fusion
of acoustic and lexical evidence. In Proceedings of the
11th Annual Conference of the International Speech
Communication Association (Interspeech 2010), IN-
TERSPEECH 2010.
W. Lewis Johnson and Paola Rizzo. 2004. Politeness in
tutoring dialogs: run the factory, thats what id do. In
Intelligent Tutoring Systems, Lecture Notes in Com-
puter Science.
Manfred Kienpointner. 1997. Varieties of rudeness:
types and functions of impolite utterances. In Func-
tions of Language.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191?201.
Jackson Liscombe, Julia Hirschberg, and Jennifer J. Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. In Proceedings of the 6th Annual Confer-
ence of the International Speech Communication As-
sociation (Interspeech 2005).
D. Litman and K. Forbes-Riley. 2004. Predicting stu-
dent emotions in computer-human tutoring dialogues.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2004).
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Structured sparsity in structured
prediction. In Proceedings of the 2011 Conference on
28
Empirical Methods in Natural Language Processing,
pages 1500?1511, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Bruce M. McLaren, Sung-Joo Lim, David Yaron, and
Ken Koedinger. 2007. Can a polite intelligent tutor-
ing system lead to improved learning outside of the
lab? In Proceedings of the 2007 conference on Arti-
ficial Intelligence in Education: Building Technology
Rich Learning Contexts That Work.
Bruce McLaren, DeLeeuwm Krista E., and Richard E.
Mayer. 2011. Polite web-based intelligent tutors: Can
they im-prove learning in classrooms? In Computers
and Education.
Angeliki Metallinou, Martin Wollmer, Athanasios
Katsamanis, Florian Eyben, Bjorn Schuller, and
Shrikanth S. Narayanan. 2011. Context-sensitive
learning for enhanced audiovisual emotion classifica-
tion. IEEE Transactions on Affective Computing.
Amy Ogan, Samantha Finkelstein, Erin Walker, Ryan
Carlson, and Justine Cassell. 2012. Rudeness and
rapport: Insults and learning gains in peer tutoring. In
Proceedings of the 11 International Conference on In-
telligence Tutoring Systems (ITS 2012).
J. Alfredo Sa?nchez, Norma P. Herna?ndez, Julio C. Pena-
gos, and Yulia Ostro?vskaya. 2006. Conveying mood
and emotion in instant messaging by using a two-
dimensional model for affective states. In Proceedings
of VII Brazilian symposium on Human factors in com-
puting systems, IHC ?06, pages 66?72, New York, NY,
USA. ACM.
A. Sharpley, J. Irvine, and C. Sharpley. 1983. An exami-
nation of the effectiveness of a cross-age tutoring pro-
gram in mathematics for elementary school children.
In American Educational Research Journal.
Helen Spencer-Oatey. 2008. Face (im)politeness and
rapport. In Culturally Speaking: Culture, Communi-
cation and Politeness Theory.
Carolyn A. Straehle. 1993. ?samuel?? ?yes dear?? teas-
ing and conversatrion rapport. In Framing in Dis-
course.
Bas Stronks, Anton Nijholt, Paul van Der Vet, Dirk
Heylen, and Aaron Machado. 2002. Designing for
friendship: Becoming friends with your eca. In Pro-
ceedings of Embodied conversational agents - let?s
specify and evaluate (AAMAS).
Marina Terkourafi. 2008. Toward a unified theory of po-
liteness, impoliteness, and rudeness. Impoliteness in
language: studies on its interplay with power in the-
ory and practice.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
nature of rapport and its nonverbal correlates. In Psy-
chological Inquiry.
Kevin E. Vowles and Miles Thompson. 2012. The
patient-provider relationship in chronic pain. In Psy-
chiatric Management of Pain.
Erin Walker, Nikol Rummel, and Kenneth R. Koedinger.
2011. Is it feedback relevance or increased account-
ability that matters? In Proceedings of the 10th Inter-
national Conference on Computer-Supported Collab-
orative Learning (CSCL 2011).
William Yang Wang and Julia Hirschberg. 2011. Detect-
ing levels of interest from spoken dialog with multi-
stream prediction feedback and similarity based hier-
archical fusion learning. In Proceedings of the 12th
annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL 2011), Portland, OR., USA, June. ACL.
William Yang Wang and Kathleen McKeown. 2010. ?got
you!?: Automatic vandalism detection in wikipedia
with web-based shallow syntactic-semantic modeling.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1146?1154, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
William Yang Wang, Fadi Biadsy, Andrew Rosenberg,
and Julia Hirschberg. 2012a. Automatic detection
of speaker state: Lexical, prosodic, and phonetic ap-
proaches to level-of-interest and intoxication classifi-
cation. Computer Speech & Language.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012b. Historical analysis of le-
gal opinions with a sparse mixed-effects latent vari-
able model. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012).
Richard J. Watts. 2003. Politeness. Cambridge Univer-
sity Press.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
29
