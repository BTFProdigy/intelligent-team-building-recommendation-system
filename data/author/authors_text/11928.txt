Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Building a Competitive Opinion Summarization System: 
Challenges and Keys 
 
 
Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andr?s Montoyo 
Department of Software and Computing Systems 
University of Alicante 
Apartado de Correos 99, E-03080, Alicante, Spain 
{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es 
 
 
 
Abstract 
This paper presents an overview of our participation in 
the TAC 2008 Opinion Pilot Summarization task, as 
well as the proposed and evaluated post-competition 
improvements. We first describe our opinion 
summarization system and the results obtained. Further 
on, we identify the system?s weak points and suggest 
several improvements, focused both on information 
content, as well as linguistic and readability aspects. We 
obtain encouraging results, especially as far as F-
measure is concerned, outperforming the competition 
results by approximately 80%. 
1 Introduction 
The Opinion Summarization Pilot (OSP) task 
within the TAC 2008 competition consisted in 
generating summaries from answers to opinion 
questions retrieved from blogs (the Blog061 
collection). The questions were organized around 
25 targets ? persons, events, organizations etc.  
Additionally, a set of text snippets that contained 
the answers to the questions were provided by the 
organizers, their use being optional. An example of 
target, question and provided snippet is given in 
Figure 1. 
 
 
 
 
 
Figure 1. Examples of target, question and snippet 
 
                                                           
*Elena Lloret is funded by the FPI program (BES-2007-
16268) from the Spanish Ministry of Science and Innovation, 
under the project TEXT-MESS (TIN-2006-15265)  
1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html 
The techniques employed by the participants were 
mainly based on the already existing 
summarization systems. While most participants 
added new features (sentiment, pos/neg sentiment, 
pos/neg opinion) to account for the presence of 
positive opinions or negative ones - CLASSY 
(Conroy and Schlessinger, 2008); CCNU (He et 
al.,2008);  LIPN (Bossard et al, 2008);  IIITSum08 
(Varma et al, 2008) -, efficient methods were 
proposed focusing on the retrieval and filtering 
stage, based on polarity ? DLSIUAES (Balahur et 
al., 2008) - or on separating information rich 
clauses - italica (Cruz et al, 2008). In general, 
previous work in opinion mining includes 
document level sentiment classification using 
supervised (Chaovalit and Zhou, 2005) and 
unsupervised methods (Turney, 2002), machine 
learning techniques and sentiment classification 
considering rating scales (Pang, Lee and 
Vaithyanathan, 2002), and scoring of features 
(Dave, Lawrence and Pennock, 2003). Other 
research has been conducted in analysing 
sentiment at a sentence level using bootstrapping 
techniques (Riloff and Wiebe, 2003), finding 
strength of opinions (Wilson, Wiebe and Hwa, 
2004), summing up orientations of opinion words 
in a sentence (Kim and Hovy, 2004), and 
identifying opinion holders (Stoyanov and Cardie, 
2006). Finally, fine grained, feature-based opinion 
summarization is defined in (Hu and Liu, 2004).  
2 Opinion Summarization System 
In order to tackle the OSP task, we considered the 
use of two different methods for opinion mining 
and summarization, differing mainly with respect 
to the use of the optional text snippets provided. 
Our first approach (the Snippet-driven Approach) 
Target : George Clooney 
Question: Why do people like George Clooney? 
Snippet 1: 1050 BLOG06-20060125-015-
0025581509 he is a great actor 
72
used these snippets, whereas the second one (Blog-
driven Approach) found the answers directly in the 
corresponding blogs. A general overview of the 
system?s architecture is shown in Figure 2, where 
three main parts can be distinguished: the question 
processing stage, the snippets processing stage 
(only carried out for the first approach), and the 
final summary generation module. Next, the main 
steps involved in each process will be explained in 
more detail.  
 
Figure 2. System architecture 
 
The first step was to determine the polarity of each 
question, extract the keywords from each of them 
and finally, build some patterns of reformulation. 
The latter were defined in order to give the final 
summary an abstract nature, rather than a simple 
joining of sentences. The polarity of the question 
was determined using a set of created patterns, 
whose goal was to extract for further classification 
the nouns, verbs, adverbs or adjectives indicating 
some kind of polarity (positive or negative). These 
extracted words, together with their determiners, 
were classified using the emotions lists in 
WordNet Affect (Strapparava and Valitutti, 2005), 
jointly with the emotions lists of attitudes, triggers 
resource (Balahur and Montoyo, 2008 [1]), four 
created lists of attitudes, expressing criticism, 
support, admiration and rejection and two 
categories for value (good and bad), taking for the 
opinion mining systems in (Balahur and Montoyo, 
2008 [2]). Moreover, the focus of each question 
was automatically extracted using the Freeling2 
Named Entity Recognizer module. This 
information was used to determine whether or not 
all the questions within the same topic had the 
same focus, as well as be able to decide later on 
which text snippet belonged to which question.  
Regarding the given text snippets, we also 
computed their polarity and their focus. The 
                                                           
2
 http://garraf.epsevg.upc.es/freeling/ 
polarity was calculated as a vector similarity 
between the snippets and vectors constructed from 
the list of sentences contained in the ISEAR corpus 
(Scherer and Wallbot, 1997), WordNet Affect 
emotion lists of anger, sadness, disgust and joy and 
the emotion triggers resource, using Pedersen's 
Text Similarity Package.3  
Concerning the blogs, our opinion mining and 
summarization system is focused only on plain 
text; therefore, as pre processing stage, we 
removed all unnecessary tags and irrelevant 
information, such as links, images etc. Further on, 
we split the remaining text into individual 
sentences. A matching between blogs' sentences 
and text snippets was performed so that a 
preliminary set of potential meaningful sentences 
was recorded for further processing. To achieve 
this, snippets not literally contained in the blogs 
were tokenized and stemmed using Porter's 
Stemmer,4 and stop words were removed in order 
to find the most similar possible sentence 
associated with it. Subsequently, by means of the 
same Pedersen Text Similarity Package as for 
computing the snippets' polarity, we computed the 
similarity between the given snippets and this 
created set of potential sentences. We extracted the 
complete blog sentences to which each snippet was 
related. Further on, we extracted the focus for each 
blog phrase sentence as well. Then, we filtered 
redundant sentences using a na?ve similarity based 
approach. Once we obtained the possible answers, 
we used Minipar5 to filter out incomplete 
sentences.  
Having computed the polarity for the questions and 
snippets, and set out the final set of sentences to 
produce the summary, we bound each sentence to 
its corresponding question, and we grouped all 
sentences which were related to the same question 
together, so that we could generate the language 
for this group, according to the patterns of 
reformulation previously mentioned. Finally, the 
speech style was changed to an impersonal one, in 
order to avoid directly expressed opinion 
sentences. A POS-tagger tool (TreeTagger6) was 
used to identify third person verbs and change 
them to a neutral style. A set of rules to identify 
                                                           
3http://www.d.umn.edu/~tpederse/text-similarity.html 
4http://tartarus.org/~martin/PorterStemmer/ 
5http://www.cs.ualberta.ca/~lindek/minipar.htm 
6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/ 
73
pronouns was created, and they were also changed 
to the more general pronoun ?they? and its 
corresponding forms, to avoid personal opinions.  
3 Evaluation 
Table 1 shows the final results obtained by our 
approaches in the TAC 2008 Opinion Pilot (the 
rank among the 36 participating systems is shown 
in brackets for each evaluation measure). Both of 
our approaches were totally automatic, and the 
only difference between them was the use of the 
given snippets in the first one (A1) and not in the 
second (A2). The column numbers stand for the 
following average scores: summarizerID (1); 
pyramid F-score (Beta=1) (2), grammaticality (3); 
non-redundancy (4); structure/coherence 
(including focus and referential clarity) (5); overall 
fluency/readability (6); overall responsiveness (7). 
 
1 2 3 4 5 6 7 
A1 0.357 
(7) 
4.727 
(8) 
5.364 
(28) 
3.409 
(4) 
3.636 
(16) 
5.045 
(5) 
A2 0.155 
(23) 
3.545 
(36) 
4.364 
(36) 
3.091 
(13) 
2.636 
(36) 
2.227 
(28) 
Table 1. Evaluation results 
 
As it can be noticed from Table 1, our system 
performed well regarding F-measure, the first run 
being classified 7th among the 36 evaluated. As far 
as the structure and coherence are concerned, the 
results were also good, placing the first approach 
in the fourth. Also worth mentioning is the good 
performance obtained regarding the overall 
responsiveness, where A1 ranked 5th. Generally 
speaking, the results for A1 showed well-balanced 
among all the criteria evaluated, except for non 
redundancy and grammaticality.  For the second 
approach, results were not as good, due to the 
difficulty in selecting the appropriate opinion blog 
sentence by only taking into account the keywords 
of the question.  
4 Post-competition tests, experiments 
and improvements 
When an exhaustive examination of the nuggets 
used for evaluating the summaries was done, we 
found some problems that are worth mentioning. 
 
a) Some nuggets with high score did not exist in 
the snippet list (e.g. ?When buying from 
CARMAX, got a better than blue book trade-in 
on old car? (0.9)).  
b) Some nuggets for the same target express the 
same idea, despite their not being identical 
(e.g. ?NAFTA needs to be renegotiated to 
protect Canadian sovereignty? and ?Green 
Party: Renegotiate NAFTA to protect 
Canadian Sovereignty?). 
c) The meaning of one nugget can be deduced 
from another's (e.g. ?reasonably healthy food? 
and ?sandwiches are healthy?). 
d) Some nuggets are not very clear in meaning 
(e.g. ?hot?, ?fun?). 
e) A snippet can be covered by several nuggets 
(e.g. both nuggets ?it is an honest book? and 
?it is a great book? correspond to the same 
snippet ?It was such a great book- honest and 
hard to read (content not language 
difficulty)?). 
 
On the other hand, regarding the use of the 
optional snippets, the main problem to address is to 
remove redundancy, because many of them are 
repeated for the same target, and we have to 
determine which snippet represents better the idea 
for the final summary, in order to avoid noisy 
irrelevant information. 
4.1 Measuring the Performance of a 
Generic Summarization System 
Several participants in the TAC 2008 edition 
performed the OSP task by using generic 
summarization systems. Most were adjusted by 
integrating an opinion classifier module so that the 
task could be fulfilled, but some were not (Bossard 
et al, 2008), (Hendrickx and Bosma, 2008). This 
fact made us realize that a generic summarizer 
could be used to achieve this task. We wanted to 
analyze the effects of such a kind of summarizer to 
produce opinion summaries. We followed the 
approach described in (Lloret et al, 2008). The 
main idea employed is to score sentences of a 
document with regard to the word frequency count 
(WF), which can be combined with a Textual 
Entailment (TE) module.  
Although the first approach suggested for opinion 
summarization obtained much better results in the 
evaluation than the second one (see Section 3.1), 
we decided to run the generic system over both 
approaches, with and without applying TE, to 
74
provide a more extent analysis and conclusions. 
After preprocessing the blogs and having all the 
possible candidate sentences grouped together, we 
considered these as the input for the generic 
summarizer. The goal of these experiments was to 
determine whether the techniques used for a 
generic summarizer would have a positive 
influence in selecting the main relevant 
information to become part of the final summary.  
4.2 Results and Discussion 
We re-evaluated the summaries generated by the 
generic system following the nuggets? list provided 
by the TAC 2008 organization, and counting 
manually the number of nuggets that were covered 
in the summaries. This was a tedious task, but it 
could not be automatically performed because of 
the fact that many of the provided nuggets were 
not found in the original blog collection. After the 
manual matching of nuggets and sentences, we 
computed the average Recall, Precision and F-
measure (Beta =1) in the same way as in the TAC 
2008 was done, according to the number and 
weight of the nuggets that were also covered in the 
summary. Each nugget had a weight ranging from 
0 to 1 reflecting its importance, and it was counted 
only once, even though the information was 
repeated within the summary.  
The average for each value was calculated taking 
into account the results for all the summaries in 
each approach. Unfortunately, we could not 
measure criteria such as readability or coherence as 
they were manually evaluated by human experts.  
Table 2 points out the results for all the approaches 
reported. We have also considered the results 
derived from our participation in the TAC 2008 
conference (OpSum-1 and OpSum-2), in order to 
analyze whether they have been improved or not. 
From these results it can be stated that the TE 
module in conjunction with the WF counts, have 
been very appropriate in selecting the most 
important information of a document. Although it 
can be thought that applying TE can remove some 
meaningful sentences which contained important 
information, results show the opposite. It benefits 
the Precision value, because a shorter summary 
contains greater ratio of relevant information. On 
the other hand, taking into consideration the F-
measure value only, it can be seen that the 
approach combining TE and WF, for the sentences 
in the first approach, has beaten significantly the 
best F-measure result among the participants of 
TAC 2008 (please see Table 3), increasing its 
performance by 20% (with respect to WF only), 
and improving by approximately 80% with respect 
to our first approach submitted to TAC 2008.    
However, a simple generic summarization system 
like the one we have used here is not enough to 
produce opinion oriented summaries, since 
semantic coherence given by the grouping of 
positive and negative opinions is not taken into 
account. Therefore, the opinion classification stage 
must be added in the same manner as used in the 
competition. 
 
SYSTEM RECALL PRECISION F-MEASURE 
OpSum-1 0.592 0.272 0.357 
OpSum-2 0.251 0.141 0.155 
WF-1 0.705 0.392 0.486 
TE+WF -1  0.684 0.630  0.639 
WF -2 0.322 0.234  0.241 
TE+WF-2 0.292 0.282 0.262 
Table 2. Comparison of the results 
4.3 Improving the quality of summaries 
In the evaluation performed by the TAC 
organization, a manual quality evaluation was also 
carried out. In this evaluation the important aspects 
were grammaticality, non-redundancy, structure 
and coherence, readability, and overall 
responsiveness. Although our participating systems 
obtained good F-measure values, in other scores, 
especially in grammaticality and non-redundancy, 
the results achieved were very low. Focusing all 
our efforts in improving the first approach, 
OpSum-1, non-redundancy and grammaticality 
verification had to be performed. In this approach, 
we wanted to test how much of the redundant 
information would be possible to remove by using 
a Textual Entailment system similar to (Iftene and 
Balahur-Dobrescu, 2007), without it affecting the 
quality of the remaining data. As input for the TE 
system, we considered the snippets retrieved from 
the original blog posts. We applied the entailment 
verification on each of the possible pairs, taking in 
turn all snippets as Text and Hypothesis with all 
other snippets as Hypothesis and Text, 
respectively. Thus, as output, we obtained the list 
of snippets from which we eliminated those that 
75
are entailed by any of the other snippets. We 
further eliminated those snippets which had a high 
entailment score with any of the remaining 
snippets. 
 
SYSTEM F-MEASURE 
Best system  0.534 
Second best system 0.490 
OpSum-1 + TE  0.530 
OpSum-1 0.357 
Table 3. F-measure results after improving the system 
 
Table 3 shows that applying TE before generating 
the final summary leads to very good results 
increasing the F-measure by 48.50% with respect 
to the original first approach. Moreover, it can be 
seen form Table 3 that our improved approach 
would have ranked in the second place among all 
the participants, regarding F-measure. The main 
problem with this approach is the long processing 
time. We can apply Textual Entailment in the 
manner described within the generic 
summarization system presented, successively 
testing the relation as Snippet1 entails Snippet2?, 
Snippet1+Snippet2 entails Snippet3? and so on. 
The problem then becomes the fact that this 
approach is random, since different snippets come 
from different sources, so there is no order among 
them. Further on, we have seen that many 
problems arise from the fact that extracting 
information from blogs introduces a lot of noise. In 
many cases, we had examples such as: 
At 4:00 PM John said Starbucks coffee tastes great 
John said Starbucks coffee tastes great, always get one 
when reading New York Times. 
To the final summary, the important information 
that should be added is ?Starbucks coffee tastes 
great?. Our TE system contains a rule specifying 
that the existence or not of a Named Entity in the 
hypothesis and its not being mentioned in the text 
leads to the decision of ?NO? entailment. For the 
example given, both snippets are maintained, 
although they contain the same data.  
Another issue to be addressed is the extra 
information contained in final summaries that is 
not scored as nugget. As we have seen from our 
data, much of this information is also valid and 
correctly answers the questions. Therefore, what 
methods can be employed to give more weight to 
some and penalize others automatically?  
Regarding the grammaticality criteria, once we had 
a summary generated we used the module 
Language Tool7 as a post-processing step. The 
errors that we needed correcting included the 
number matching between nouns and determiners 
as well as among subject and predicate, upper case 
for sentence start, repeated words or punctuation 
marks and lack of punctuation marks. The rules 
present in the module and that we ?switched off?, 
due to the fact that they produced more errors, 
were those concerning the limit in the number of 
consecutive nouns and the need for an article 
before a noun (since it always seemed to want to 
correct ?Vista? for ?the Vista? a.o.). We evaluated 
by observing the mistakes that the texts contained, 
and counting the number of remaining or 
introduced errors in the output. The results 
obtained can be seen in Table 4. 
 
Problem Rightly corrected 
 
Wrongly 
corrected 
Match S-P 90% 10% 
Noun-det 75% 25% 
Upper case 80% 20% 
Repeated words 100% 0% 
Repeated ?.? 80% 20% 
Spelling mistakes 60% 40% 
Unpaired ??/() 100% 0% 
Table 4. Grammaticality analysis 
 
The greatest problem encountered was the fact that 
bigrams are not detected and agreement is not 
made in cases in which the noun does not appear 
exactly after the determiner. All in all, using this 
module, the grammaticality of our texts was 
greatly improved. 
5 Conclusions and future work 
The Opinion Pilot in the TAC 2008 competition 
was a difficult task, involving the development of 
systems including components for QA, IR, polarity 
classification and summarization. Our contribution 
presented in this paper resides in proposing an 
opinion mining and summarization method using 
different approaches and resources, evaluating 
each of them in turn. We have shown that using a 
generic summarization system, we obtain 80% 
improvement over the results obtained in the 
competition, with coherence being maintained by 
using the same polarity classification mechanisms. 
                                                           
7http://community.languagetool.org/ 
76
Using redundancy removal with TE, as opposed to 
our initial polarity strength based sentence filtering 
improved the system performance by almost 50%.    
Finally, we showed that grammaticality can be 
checked and improved using an independent 
solution given by Language Tool.  
Further work includes the improvement of the 
polarity classification component by using 
machine learning over annotated corpora and other 
techniques, such as anaphora resolution. As we 
could see, the well functioning of this component 
ensures logic, structure and coherence to the 
produced summaries. Moreover, we plan to study 
the manner in which opinion sentences of 
blogs/bloggers can be coherently combined. 
References  
Balahur, A., Lloret, E., Ferr?ndez, ?., Montoyo, A., 
Palomar, M., Mu?oz, R., The DLSIUAES Team?s 
Participation in the TAC 2008 Tracks. In 
Proceedings of the Text Analysis Conference (TAC), 
2008. 
Balahur, A. and Montoyo, A. [1]. An Incremental 
Multilingual Approach to Forming a Culture 
Dependent Emotion Triggers Database. In 
Proceedings of the 8th International Conference on 
Terminology and Knowledge Engineering, 2008. 
Balahur, A. and Montoyo, A. [2]. Multilingual Feature--
driven Opinion Mining and Summarization from 
Customer Reviews. In Lecture Notes in Computer 
Science 5039, pg. 345-346. 
Bossard, A., G?n?reux, M. and  Poibeau, T.. Description 
of the LIPN systems at TAC 2008: Summarizing 
information and opinions. In Proceedings of the Text 
Analysis Conference (TAC), 2008. 
Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a 
Comparison between Supervised and Unsupervised 
Classification Approaches. In Proceedings of HICSS-
05, the 38th Hawaii International Conference on 
System Sciences. 
Cruz, F., Troyani, J.A., Ortega, J., Enr?quez, F. The 
Italica System at TAC 2008 Opinion Summarization 
Task. In Proceedings of the Text Analysis 
Conference (TAC), 2008. 
Cui, H., Mittal, V., Datar, M. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of the 21st National 
Conference on Artificial Intelligence AAAI 2006.  
Dave, K., Lawrence, S., Pennock, D. 2003. Mining the 
Peanut Gallery: Opinion Extraction and Semantic 
Classification of Product Reviews. In Proceedings of 
WWW-03.  
Lloret, E., Ferr??ndez, O., Mu?oz, R. and Palomar, M. A 
Text Summarization Approach under the Influence of 
Textual Entailment. In Proceedings of the 5th 
International Workshop on Natural Language 
Processing and Cognitive Science (NLPCS 2008), 
pages 22?31, 2008. 
Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E. 
2005. Mining Customer Opinions from Free Text. 
Lecture Notes in Computer Science. 
He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008: 
Proceeding on Using Semantic Method for 
Automated Summarization Yield. In Proceedings of 
the Text Analysis Conference (TAC), 2008. 
Hendrickx, I. and Bosma, W.. Using coreference links 
and sentence compression in graph-based 
summarization. In Proceedings of the Text Analysis 
Conference (TAC), 2008.     
Hu, M., Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of 19th National 
Conference on Artificial Intelligence AAAI. 
Iftene, A., Balahur-Dobrescu, A. Hypothesis 
Transformation and Semantic Variability Rules for 
Recognizing Textual Entailment. In Proceedings of 
the ACL 2007 Workshop on Textual Entailment and 
Paraphrasis, 2007. 
Kim, S.M., Hovy, E. 2004. Determining the Sentiment 
of Opinions. In Proceedings of COLING 2004. 
Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP-02, the 
Conference on Empirical Methods in Natural 
Language Processing. 
Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns 
for Subjective Expressions. In Proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing.  
Scherer, K. and Wallbott, H.G. The ISEAR 
Questionnaire and Codebook, 1997.  
Stoyanov, V., Cardie, C. 2006. Toward Opinion 
Summarization: Linking the Sources. In: COLING-
ACL 2006 Workshop on Sentiment and Subjectivity 
in Text. 
Strapparava, C. and Valitutti, A. "WordNet-Affect: an 
affective extension of WordNet". In Proceedings 
ofthe 4th International Conference on Language 
Resources and Evaluation, 2004, pp. 1083-1086.  
Turney, P., 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceedings of the 40th 
Annual Meeting of the ACL 
Varma, V., Pingali, P., Katragadda, R., Krisha, S., 
Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty, 
H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT 
Hyderabad at TAC 2008. In Proceedings of the Text 
Analysis Conference (TAC), 2008.  
Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are 
you? Finding strong and weak opinion clauses. In: 
Proceedings of AAAI 2004. 
77
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903?911,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Quantifying the Limits and Success of Extractive Summarization Systems
Across Domains
Hakan Ceylan and Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
{hakan,rada}@unt.edu
Umut ?Ozertem
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
umut@yahoo-inc.com
Elena Lloret and Manuel Palomar
Department of
Software and Computing Systems
University of Alicante
San Vicente del Raspeig
Alicante 03690, Spain
{elloret,mpalomar}@dlsi.ua.es
Abstract
This paper analyzes the topic identification
stage of single-document automatic text sum-
marization across four different domains, con-
sisting of newswire, literary, scientific and le-
gal documents. We present a study that ex-
plores the summary space of each domain
via an exhaustive search strategy, and finds
the probability density function (pdf) of the
ROUGE score distributions for each domain.
We then use this pdf to calculate the per-
centile rank of extractive summarization sys-
tems. Our results introduce a new way to
judge the success of automatic summarization
systems and bring quantified explanations to
questions such as why it was so hard for the
systems to date to have a statistically signifi-
cant improvement over the lead baseline in the
news domain.
1 Introduction
Topic identification is the first stage of the gener-
ally accepted three-phase model in automatic text
summarization, in which the goal is to identify the
most important units in a document, i.e., phrases,
sentences, or paragraphs (Hovy and Lin, 1999; Lin,
1999; Sparck-Jones, 1999). This stage is followed
by the topic interpretation and summary generation
steps where the identified units are further processed
to bring the summary into a coherent, human read-
able abstract form. The extractive summarization
systems, however, only employ the topic identifi-
cation stage, and simply output a ranked list of the
units according to a compression ratio criterion. In
general, for most systems sentences are the preferred
units in this stage, as they are the smallest grammat-
ical units that can express a statement.
Since the sentences in a document are reproduced
verbatim in extractive summaries, it is theoretically
possible to explore the search space of this problem
through an enumeration of all possible extracts for
a document. Such an exploration would not only
allow us to see how far we can go with extractive
summarization, but we would also be able to judge
the difficulty of the problem by looking at the dis-
tribution of the evaluation scores for the generated
extracts. Moreover, the high scoring extracts could
also be used to train a machine learning algorithm.
However, such an enumeration strategy has an
exponential complexity as it requires all possible
sentence combinations of a document to be gener-
ated, constrained by a given word or sentence length.
Thus the problem quickly becomes impractical as
the number of sentences in a document increases and
the compression ratio decreases. In this work, we try
to overcome this bottleneck by using a large cluster
of computers, and decomposing the task into smaller
problems by using the given section boundaries or a
linear text segmentation method. As a result of this
exploration, we generate a probability density func-
tion (pdf) of the ROUGE score (Lin, 2004) distri-
butions for four different domains, which shows the
distribution of the evaluation scores for the gener-
ated extracts, and allows us to assess the difficulty
of each domain for extractive summarization.
Furthermore, using these pdfs, we introduce a
new success measure for extractive summarization
systems. Namely, given a system?s average score
over a data set, we show how to calculate the per-
903
centile rank of this system from the corresponding
pdf of the data set. This allows us to see the true
improvement a system achieves over another, such
as a baseline, and provides a standardized scoring
scheme for systems performing on the same data set.
2 Related Work
Despite the large amount of work in automatic
text summarization, there are only a few studies
in the literature that employ an exhaustive search
strategy to create extracts, which is mainly due to
the prohibitively large search space of the prob-
lem. Furthermore, the research regarding the align-
ment of abstracts to original documents has shown
great variations across domains (Kupiec et al, 1995;
Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;
Ceylan and Mihalcea, 2009), which indicates that
the extractive summarization techniques are not ap-
plicable to all domains at the same level.
In order to automate the process of corpus
construction for automatic summarization systems,
(Marcu, 1999) used exhaustive search to generate
the best Extract from a given (Abstract, Text) tuple,
where the best Extract contains a set of clauses from
Text that have the highest similarity to the given Ab-
stract.
In addition, (Donaway et al, 2000) used exhaus-
tive search to create all the sentence extracts of
length three starting with 15 TREC Documents, in
order to judge the performance of several summary
evaluation measures suggested in their paper.
Finally, the study most similar to ours was done
by (Lin and Hovy, 2003), who used the articles with
less than 30 sentences from the DUC 2001 data set
to find oracle extracts of 100 and 150 (?5) words.
These extracts were compared against one summary
source, selected as the one that gave the highest
inter-human agreement. Although it was concluded
that a 10% improvement was possible for extrac-
tive summarization systems, which typically score
around the lead baseline, there was no report on how
difficult it would be to achieve this improvement,
which is the main objective of our paper.
3 Description of the Data Set
Our data set is composed of four different domains:
newswire, literary, scientific and legal. For all the
Domain ?Dw ?Sw ?R ?C ?Cw
Newswire 641 101 84% 1 641
Literary 4973 1148 77% 6 196
Scientific 1989 160 92% 9 221
Legal 3469 865 75% 18 192
Table 1: Statistical properties of the data set. ?Dw, and
?Sw represent the average number of words for each doc-
ument and summary respectively; ?R indicates the av-
erage compression ratio; and ?C and ?Cw represent the
average number of sections for each document, and the
average number of words for each section respectively.
domains we used 50 documents and only one sum-
mary for each document, except for newswire where
we used two summaries per document. For the
newswire domain, we selected the articles and their
summaries from the DUC 2002 data set,1. For the
literary domain, we obtained 10 novels that are lit-
erature classics, and available online in text format.
Further, we collected the corresponding summaries
for these novels from various websites such as
CliffsNotes (www.cliffsnotes.com) and SparkNotes
(www.sparknotes.com), which make available hu-
man generated abstracts for literary works. These
sources give a summary for each chapter of the
novel, so each chapter can be treated as a sepa-
rate document. Thus we evaluate 50 chapters in to-
tal. For the scientific domain, we selected the ar-
ticles from the medical journal Autoimmunity Re-
views2 were selected, and their abstracts are used
as summaries. Finally, for the legal domain, we
gathered 50 law documents and their corresponding
summaries from the European Legislation Website,3
which comprises four types of laws - Council Di-
rectives, Acts, Communications, and Decisions over
several topics, such as society, environment, educa-
tion, economics and employment.
Although all the summaries are human generated
abstracts for all the domains, it is worth mention-
ing that the documents and their corresponding sum-
maries exhibit a specific writing style for each do-
main, in terms of the vocabulary used and the length
of the sentences. We list some of the statistical prop-
erties of each domain in Table 1.
1http://www-nlpir.nist.gov/projects/duc/data.html
2http://www.elsevier.com/wps/product/cws home/622356
3http://eur-lex.europa.eu/en/legis/index.htm
904
4 Experimental Setup
As mentioned in Section 1, an exhaustive search
algorithm requires generating all possible sentence
combinations from a document, and evaluating each
one individually. For example, using the values from
Table 1, and assuming 20 words per sentence, we
find that the search space for the news domain con-
tains approximately
(32
5
)
? 50 = 10, 068, 800 sum-
maries. The same calculation method for the sci-
entific domain gives us
(99
8
)
? 50 = 8.56 ? 1012
summaries. Obviously the search space gets much
bigger for the legal and literary domains due to their
larger text size.
In order to be able to cope with such a huge
search space, the first thing we did was to modify
the ROUGE 1.5.54 Perl script by fixing the parame-
ters to those used in the DUC experiments,5 and also
by modifying the way it handles the input and output
to make it suitable for streaming on the cluster.
The resulting script evaluates around 25-30 sum-
maries per second on an Intel 2.33 GHz processor.
Next, we streamed the resulting ROUGE script for
each (document, summary) pair on a large cluster
of computers running on an Hadoop Map-Reduce
framework.6 Based on the size of the search space
for a (document, summary) pair, the number of com-
puters allocated in the cluster ranged from just a few
to more than one thousand.
Although the combination of a large cluster and a
faster ROUGE is enough to handle most of the doc-
uments in the news domain in just a few hours, a
simple calculation shows that the problem is still im-
practical for the other domains. Hence for the scien-
tific, legal, and literary domains, rather than consid-
ering each document as a whole, we divide them into
sections, and create extracts for each section such
that the length of the extract is proportional to the
length of the section in the original document. For
the legal and scientific domains, we use the given
section boundaries (without considering the subsec-
tions for scientific documents). For the novels, we
treat each chapter as a single document (since each
chapter has its own summary), which is further di-
vided into sections using a publicly available linear
4http://berouge.com
5
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0
6http://hadoop.apache.org/
text segmentation algorithm by (Utiyama and Isa-
hara, 2001).7 In all cases, we let the algorithm pick
the number of segments automatically.
To evaluate the sections, we modified ROUGE
further so that it applies the length constraint to the
extracts only, not to the model summaries. This is
due to the fact that we evaluate the extracts of each
section individually against the whole model sum-
mary, which is larger than the extract. This way,
we can get an overall ROUGE recall score for a
document extract, simply by summing up the re-
call scores of each section extracts. The precision
score for the entire document can also be found by
adding the weighted precision scores for each sec-
tion, where the weight is proportional to the length
of the section in the original document. In our study,
however, we only use recall scores.
Note that, since for the legal, scientific, and lit-
erary domains we consider each section of a doc-
ument independently, we are not performing a true
exhaustive search for these domains, but rather solv-
ing a suboptimal problem, as we divide the number
of words in the model summary to each section pro-
portional to the section?s length. However, we be-
lieve that this is a fair assumption, as it has been
shown repeatedly in the past that text segmentation
helps improving the performance of text summariza-
tion systems (yen Kan et al, 1998; Nakao, 2000;
Mihalcea and Ceylan, 2007).
5 Exhaustive Search Algorithm
Let Eik = Si1 , Si2 , ..., Sik be the ith extract that
has k sentences, and generated from a document
D with n sentences D = S1, S2, . . . , Sn. Further,
let len(Sj) give the number of words in sentence
Sj . We enforce that Eik satisfies the following con-
straints:
len(Eik) = len(Si1) + . . . + len(Sik) ? L
len(Eik?1) = len(Si1) + . . . + len(Sik?1) < L
where L is the length constraint on all the extracts
of document D. We note that for any Eik , the or-
der of the sentences in Eik?1 does not affect the
ROUGE scores, since only the last sentence may be
7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-
1.211.tar.gz
905
chopped off due to the length constraint.8 Hence, we
start generating sentence combinations
(n
r
)
in lexico-
graphic order, for r = 1...n, and for each combina-
tion Eik = Si1 , Si2 , ..., Sik where k > 1, we gener-
ate additional extracts E?ik by successfully swapping
Sij with Sik for j = 1, ..., k? 1 and checking to see
if the above constraints are still satisfied. Therefore
from a combination with k sentences that satisfies
the constraints, we might generate up to k ? 1 ad-
ditional extracts. Finally, we stop the process either
when r = n and the last combination is generated,
or we cannot find any extract that satisfies the con-
straints for r.
6 Generating pdfs
Once the extracts for a document are generated and
evaluated, we go through each result and assign its
recall score to a range, which we refer to as a bin.
We use 1, 000 equally spaced bins between 0 and
1. As an example, a recall score of 0.46873 would
be assigned to the bin [0.468, 0.469]. By keeping
a count for each bin, we are in fact building a his-
togram of scores for the document. Let this his-
togram be h, and h[j] be the value in the jth bin of
the histogram. We then define the normalized his-
togram h? as:
h?[j] =
N
?N
i=1 h[j]
h[j] (1)
where N = 1, 000 is the number of bins in the his-
togram. Note that since the width of each bin is 1N ,
the Riemann sum of the normalized histogram h? is
equal to 1, so h? can be used as an approximation
to the underlying pdf. As an example, we show the
histogram h? for the newswire document AP890323-
0218 in Figure 1.
We combine the normalized histograms of all the
documents in a domain in order to find the pdf for
that domain. This requires multiplying the value
of each bin in a document?s histogram, with all
the other possible combinations of bin values taken
from each of the remaining histograms, and assign-
ing the result to the average bin for each combina-
8Note that we do not take the coherence of extracts into ac-
count, i.e. the sentences in an extract do not need to be sorted
in order of their appearance in the original document. We also
do not change the position of the words in a sentence.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  100  200  300  400  500  600  700  800  900  1000
"AP890323-0218.dat"
Figure 1: The normalized histogram h? of ROUGE-1 re-
call scores for the newswire document AP890323-0218.
tion. This can be done iteratively by keeping a mov-
ing average. We illustrate this procedure in Algo-
rithm 1, where K represents the number of docu-
ments in a domain.
Algorithm 1 Combine h?i?s for i = 1, . . . ,K to cre-
ate hd, the histogram for domain d.
1: hd := {}
2: for i = 1 to N do
3: hd[i] := h?1[i]
4: end for
5: for i = 2 to K do
6: ht = {}
7: for j = 1 to N do
8: for k = 1 to N do
9: a = round(((k ? (i? 1)) + j)/i)
10: ht[a] = ht[a] + (hd[k] ? h?i[j])
11: end for
12: end for
13: hd := ht
14: end for
The resulting histogram hd, when normalized us-
ing Equation 1, is an approximation to the pdf for
domain d. Furthermore, we used the round() func-
tion in line 9, which rounds a number to the nearest
integer, as the bins are indexed by integers. Note
that this rounding introduces an error, which is dis-
tributed uniformly due to the nature of the round()
function. It is also possible to lower the affect of this
error with higher resolutions (i.e. larger number of
bins). In Figure 2, we show a sample hd, obtained
by combining 10 documents from the newswire do-
906
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
"newswire_10-ROUGE-1.dat"
Figure 2: An example pdf obtained by combining 10 doc-
ument histograms of ROUGE-1 recall scores from the
newswire domain. The x-axis is normalized to [0,1].
main.
Recall from Section 4 that the documents in
the literary, legal, and scientific domains are di-
vided into sections either by using the given section
boundaries or by applying a text segmentation al-
gorithm, and the extracts of each section are then
evaluated individually. Hence for these domains, we
first calculate the histogram of each section individ-
ually, and then combine them to find the histogram
of a document. The combination procedure for the
section histograms is similar to Algorithm 1, except
that in this case we do not keep a moving average,
but rather sum up the bins of the sections. Note
that when bin i and j are added, the resulting val-
ues should be expected to be half the times in bin
i + j, and half the times in i + j ? 1.
7 Calculating Percentile Ranks
Given a pdf for a domain, the success of a system
having a ROUGE recall score of S could be sim-
ply measured by finding the area bounded by S.
This gives us the percentile rank of the system in
the overall distribution. Assuming 0 ? S ? 1, let
S? = ?N ?S?, then the formula to calculate the per-
centile rank can be simply given as:
PR(S) =
100
N
bS?
i=1
h?d[i] (2)
ROUGE-1
Domain ? ? max min
Newswire 39.39 0.87 65.70 20.20
Literary 45.20 0.47 63.90 28.40
Scientific 45.99 0.68 71.90 24.20
Legal 72.82 0.28 82.40 62.80
ROUGE-2
Domain ? ? max min
Newswire 11.57 0.79 37.40 1.60
Literary 5.41 0.34 16.90 1.80
Scientific 10.98 0.60 33.30 1.30
Legal 28.74 0.29 40.90 19.60
ROUGE-SU4
Domain ? ? max min
Newswire 15.33 0.69 38.10 6.40
Literary 13.28 0.30 24.30 6.90
Scientific 16.13 0.50 35.80 6.20
Legal 35.63 0.25 45.70 28.70
Table 2: Statistical properties of the pdfs
8 Results
The ensemble distributions of ROUGE-1 recall
scores per document are shown in Figure 3. The
ensemble distributions tell us that the performance
of the extracts, especially for the news and the sci-
entific domains, are mostly uniform for each docu-
ment. This is due to the fact that documents in these
domains, and their corresponding summaries, are
written with a certain conventional style. There is
however a little scattering in the distributions of the
literary and the legal domains. This is an expected
result for the literary domain, as there is no specific
summarization style for these documents, but some-
how surprising for the legal domain, where the effect
is probably due to the different types of legal docu-
ments in the data set.
The pdf plots resulting from the ROUGE-1 recall
scores are shown in Figure 4.9 In order to analyze
the pdf plots, and better understand their differences,
Table 2 lists the mean (?) and the standard deviation
(?) measures of the pdfs, as well as the average min-
imum and maximum scores that an extractive sum-
marization system can get for each domain.
By looking at the pdf plots and the minimum and
maximum columns from Table 2, we notice that for
9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,
even if at a different scale.
907
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45  50
"Ensemble-Newswire-50-ROUGE-1.dat"
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Literary-50-Ensemble-ROUGE-1.dat"
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Medical-50-Ensemble-ROUGE-1.dat"
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Legal-50-Ensemble-ROUGE-1.dat"
Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,
respectively from left to right.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0.36  0.38  0.4  0.42  0.44
"Newswire-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Literary-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Medical-50-ROUGE-1.dat"
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.7  0.72  0.74  0.76  0.78  0.8
"Legal-50-ROUGE-1.dat"
Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and Legal
Domains, respectively from left to right. The resolution of the x-axis is increased to 0.1.
all the domains, the pdfs are long-tailed distribu-
tions. This immediately implies that most of the
extracts in a summary space are clustered around
the mean, which means that for automatic summa-
rization systems, it is very easy to get scores around
this range. Furthermore, we can judge the hardness
of each domain by looking at the standard devia-
tion values. A lower standard deviation indicates a
steeper curve, which implies that improving a sys-
tem would be harder. From the table, we can in-
fer that the legal domain is the hardest while the
newswire is the easiest.
Comparing Table 2 with the values in Table 1,
we also notice that the compression ratio affects the
performance differently for each domain. For ex-
ample, although the scientific domain has the high-
est compression ratio, it has a higher mean than
the literary and the newswire domains for ROUGE-
1 and ROUGE-SU4 recall scores. This implies
that although the abstracts of the medical journals
are highly compressed, they have a high overlap
with the document, probably caused by their writ-
ing style. This was in fact confirmed earlier by the
experiments in (Kupiec et al, 1995), where it was
found out that for a data set of 188 scientific arti-
cles, 79% of the sentences in the abstracts could be
perfectly matched with the sentences in the corre-
sponding documents.
Next, we confirm our experiments by testing three
different extractive summarization systems on our
data set. The first system that we implement is called
Random, and gives a random score between 1 and
100 to each sentence in a document, and then se-
lects the top scoring sentences. The second system,
Lead, implements the lead baseline method which
takes the first k sentences of a document until the
length limit is reached. Finally, the last system that
we implement is TextRank, which uses a variation of
the PageRank graph centrality algorithm in order to
identify the most important sentences in a document
(Page et al, 1999; Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). We selected TextRank as it has a
performance competitive with the top systems par-
ticipating in DUC ?02 (Mihalcea and Tarau, 2004).
We would also like to mention that for the literary,
scientific, and legal domains, the systems apply the
algorithms for each section and each section is eval-
uated independently, and their resulting recall scores
are summed up. This is needed in order to be con-
sistent with our exhaustive search experiments.
The ROUGE recall scores of the three systems are
shown in Table 3. As expected, for the literary and
legal domains, the Random, and the Lead systems
score around the mean. This is due to the fact that
the leading sentences for these two domains do not
indicate any significance, hence the Lead system just
behaves like Random. However for the scientific and
newswire domains, the leading sentences do have
908
ROUGE-1
Domain Random Lead TextRank
Newswire 39.13 45.63 44.43
Literary 45.39 45.36 46.12
Scientific 45.75 47.18 49.26
Legal 73.04 72.42 74.82
ROUGE-2
Domain Random Lead TextRank
Newswire 11.39 19.60 17.99
Literary 5.33 5.41 5.92
Scientific 10.73 12.07 12.76
Legal 28.56 28.92 31.06
ROUGE-SU4
Domain Random Lead TextRank
Newswire 15.07 21.58 20.46
Literary 13.21 13.28 13.81
Scientific 15.92 17.12 17.85
Legal 35.41 35.55 37.64
Table 3: ROUGE recall scores of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
importance so the Lead system consistently outper-
forms Random. Furthermore, although TextRank is
the best system for the literary, scientific, and legal
domains, it gets outperformed by the Lead system
on the newswire domain. This is also an expected re-
sult as none of the single-document summarization
systems were able to achieve a statistically signifi-
cant improvement over the lead baseline in the previ-
ous Document Understanding Conferences (DUC).
The ROUGE scoring scheme does not tell us how
much improvement a system achieved over another,
or how far it is from the upper bound. Since we now
have access to the pdf of each domain in our data set,
we can find this information simply by calculating
the percentile rank of each system using the formula
given in Equation 2.
The percentile ranks of all three systems for each
domain are shown in Table 4. Notice how different
the gap is between the scores of each system this
time, compared to the scores in Table 3. For ex-
ample, we see in Table 3 that TextRank on scientific
domain has only a 3.51 ROUGE-1 score improve-
ment over a system that randomly selects sentences
to include in the extract. However, Table 4 tells us
that this improvement is in fact 57.57%.
From Table 4, we see that both TextRank and
the Lead system are in the 99.99% percentile of
ROUGE-1
Domain Random Lead TextRank
Newswire %39.18 %99.99 %99.99
Literary %62.89 %62.89 %97.90
Scientific %42.30 %95.56 %99.87
Legal %79.47 %16.19 %99.99
ROUGE-2
Domain Random Lead TextRank
Newswire %39.57 %99.99 %99.99
Literary %42.20 %54.32 %94.34
Scientific %35.6 %96.03 %99.79
Legal %36.68 %75.38 %99.99
ROUGE-SU4
Domain Random Lead TextRank
Newswire %40.68 %99.99 %99.99
Literary %46.39 %46.39 %96.84
Scientific %36.37 %97.69 %99.94
Legal %23.53 %42.00 %99.99
Table 4: Percentile rankings of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
the newswire domain although the systems have
1.20, 1.61, and 1.12 difference in their ROUGE-1,
ROUGE-2, and ROUGE-SU4 scores respectively.
The high percentile for the Lead system explains
why it was so hard to improve over these baseline in
previous evaluations on newswire data (e.g., see the
evaluations from the Document Understanding Con-
ferences). Furthermore, we see from Table 2 that the
upper bounds corresponding to these scores are 65.7,
37.4, and 38.1 respectively, which are well above
both the TextRank and the Lead systems. There-
fore, the percentile rankings of the Lead and the Tex-
tRank systems for this domain do not seem to give
us clues about how the two systems compare to each
other, nor about their actual distance from the up-
per bounds. There are two reasons for this: First,
as we mentioned earlier, most of the summary space
consists of easy extracts, which make the distribu-
tion long-tailed.10 Therefore even though we have
quite a bit of systems achieving high scores, their
number is negligible compared to the millions of ex-
tracts that are clustered around the mean. Secondly,
we need a higher resolution (i.e. larger number of
bins) in constructing the pdfs in order to be able to
10This also accounts for the fact that even though we might
have two very close ROUGE scores that are not statistically sig-
nificant, their percentile rankings might differ quite a bit.
909
see the difference more clearly between the two sys-
tems. Finally, when comparing two successful sys-
tems using percentile ranks, we believe the use of
error reduction would be more beneficial.
As a final note, we also randomly sampled ex-
tracts from documents in the scientific and legal do-
mains, but this time without considering the section
boundaries and without performing any segmenta-
tion. We kept the number of samples for each doc-
ument equal to the number of extracts we generated
from the same document using a divide-and-conquer
approach. We evaluated the samples using ROUGE-
1 recall scores, and obtained pdfs for each domain
using the same strategy discussed earlier in the pa-
per. The resulting pdfs, although they exhibit simi-
lar characteristics, they have mean values (?) around
10% lower than the ones we listed in Table 2, which
supports the findings from earlier research that seg-
mentation is useful for text summarization.
9 Conclusions and Future Work
In this paper, we described a study that explores the
search space of extractive summaries across four dif-
ferent domains. For the news domain we generated
all possible extracts of the given documents, and
for the literary, scientific, and legal domains we fol-
lowed a divide-and-conquer approach by chunking
the documents into sections, handled each section
independently, and combined the resulting scores at
the end. We then used the distributions of the eval-
uations scores to generate the probability density
functions (pdfs) for each domain. Various statistical
properties of these pdfs helped us asses the difficulty
of each domain. Finally, we introduced a new scor-
ing scheme for automatic text summarization sys-
tems that can be derived from the pdfs. The new
scheme calculates a percentile rank of the ROUGE-
1 recall score of a system, which gives scores in the
range [0-100]. This lets us see how far each sys-
tem is from the upper bound, and thus make a better
comparison among the systems. The new scoring
system showed us that while there is a 20.1% gap
between the upper bound and the lead baseline for
the news domain, closing this gap is difficult, as the
percentile rank of the lead baseline system, 99.99%,
indicates that the system is already very close to the
upper bound.
Furthermore, except for the literary domain, the
percentile rank of the TextRank system is also very
close to the upperbound. This result does not sug-
gest that additional improvements cannot be made
in these domains, but that making further improve-
ments using only extractive summarization will be
considerably difficult. Moreover, in order to see
these future improvements, a higher resolution (i.e.
larger number of bins) will be needed when con-
structing the pdfs.
In all our experiments we used the ROUGE
(Lin, 2004) evaluation package and its ROUGE-
1, ROUGE-2, and ROUGE-SU4 recall scores. We
would like to note that since ROUGE performs its
evaluations based on the n-gram overlap between
the peer and the model summary, it does not take
other summary quality metrics such as coherence
and cohesion into account. However, our goal in this
paper was to analyze the topic-identification stage
only, which concentrates on selecting the right con-
tent from the document to include in the summary,
and the ROUGE scores were found to correlate well
with the human judgments on assessing the content
overlap of summaries.
In the future, we would like to apply a similar ex-
haustive search strategy, but this time with differ-
ent compression ratios, in order to see the impact
of compression ratios on the pdf of each domain.
Furthermore, we would also like to analyze the
high scoring extracts found by the exhaustive search,
in terms of coherence, position and other features.
Such an analysis would allow us to see whether these
extracts exhibit certain properties which could be
used in training machine learning systems.
Acknowledgments
The authors would like to thank the anonymous re-
viewers of NAACL-HLT 2010 for their feedback.
The work of the first author has been partly sup-
ported by an award from Google, Inc. The work of
the fourth and fifth authors has been supported by an
FPI grant (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation, under the project
TEXT-MESS (TIN2006-15265-C06-01) funded by
the Spanish Government, and the project PROME-
TEO Desarrollo de Tcnicas Inteligentes e Interacti-
vas de Minera de Textos (2009/119) from the Valen-
cian Government.
910
References
Hakan Ceylan and Rada Mihalcea. 2009. The decompo-
sition of human-written book summaries. In CICLing
?09: Proceedings of the 10th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 582?593, Berlin, Heidelberg. Springer-
Verlag.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In NAACL-
ANLP 2000 Workshop on Automatic summarization,
pages 69?78, Morristown, NJ, USA. Association for
Computational Linguistics.
G. Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22.
Eduard H. Hovy and Chin Yew Lin. 1999. Automated
text summarization in summarist. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?97. MIT Press.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28(4):527?543.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95: Pro-
ceedings of the 18th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 68?73, New York, NY, USA.
ACM.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In CIKM ?99: Proceedings of the eighth
international conference on Information and knowl-
edge management, pages 55?62, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 137?144, New
York, NY, USA. ACM.
Rada Mihalcea and Hakan Ceylan. 2007. Explorations in
automatic book summarization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 380?
389, Prague, Czech Republic, June. Association for
Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierarchy
detection. In ACL ?00: Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 302?309, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
InfoLab.
Karen Sparck-Jones. 1999. Automatic summarising:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?13. MIT Press.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scallable
Text Summarization, Madrid, Spain, July.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 491?498.
Min yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 1998. Linear segmentation and segment sig-
nificance. In In Proceedings of the 6th International
Workshop of Very Large Corpora, pages 197?205.
911
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107?115,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experiments on Summary-based Opinion Classification
Elena Lloret
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
elloret@dlsi.ua.es
Horacio Saggion
Department of Infomation and
Communication Technologies
Grupo TALN
Universitat Pompeu Fabra
C/Ta?nger, 122-134, 2nd floor
08018 Barcelona, Spain
horacio.saggion@upf.edu
Manuel Palomar
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
mpalomar@dlsi.ua.es
Abstract
We investigate the effect of text summarisa-
tion in the problem of rating-inference ? the
task of associating a fine-grained numerical
rating to an opinionated document. We set-up
a comparison framework to study the effect of
different summarisation algorithms of various
compression rates in this task and compare the
classification accuracy of summaries and doc-
uments for associating documents to classes.
We make use of SVM algorithms to associate
numerical ratings to opinionated documents.
The algorithms are informed by linguistic and
sentiment-based features computed from full
documents and summaries. Preliminary re-
sults show that some types of summaries could
be as effective or better as full documents in
this problem.
1 Introduction
Public opinion has a great impact on company and
government decision making. In particular, compa-
nies have to constantly monitor public perception of
their products, services, and key company represen-
tatives to ensure that good reputation is maintained.
Recent cases of public figures making headlines for
the wrong reasons have shown how companies take
into account public opinion to distance themselves
from figures which can damage their public image.
The Web has become an important source for find-
ing information, in the field of business intelligence,
business analysts are turning their eyes to the Web
in order to monitor public perception on products,
services, policies, and managers. The field of senti-
ment analysis has recently emerged (Pang and Lee,
2008) as an important area of research in Natural
Language Processing (NLP) which can provide vi-
able solutions for monitoring public perception on
a number of issues; with evaluation programs such
as the Text REtrieval Conference track on blog min-
ing 1, the Text Analysis Conference 2 track on opin-
ion summarisation, and the DEfi Fouille de Textes
program (Grouin et al, 2009) advances in the state
of the art have been produced. Although sentiment
analysis involves various different problems such as
identifying subjective sentences or identifying posi-
tive and negative opinions in text, here we concen-
trate on the opinion classification task; and more
specifically on rating-inference, the task of identify-
ing the author?s evaluation of an entity with respect
to an ordinal-scale based on the author?s textual eval-
uation of the entity (Pang and Lee, 2005). The spe-
cific problem we study in this paper is that of as-
sociating a fine-grained rating (1=worst,...5=best)
to a review. This is in general considered a dif-
ficult problem because of the fuzziness inherent of
mid-range ratings (Mukras et al, 2007). A consid-
erable body of research has recently been produced
to tackle this problem (Chakraborti et al, 2007; Fer-
rari et al, 2009) and reported figures showing accu-
racies ranging from 30% to 50% for such complex
task; most approaches derive features for the classi-
fication task from the full document. In this research
we ask whether extracting features from document
summaries could help a classification system. Since
text summaries are meant to contain the essential
content of a document (Mani, 2001), we investigate
whether filtering noise through text summarisation
is of any help in the rating-inference task. In re-
1http:trec.nist.gov/
2http://www.nist.gov/tac/
107
cent years, text summarisation has been used to sup-
port both manual and automatic tasks; in the SUM-
MAC evaluation (Mani et al, 1998), text summaries
were tested in document classification and ques-
tion answering tasks where summaries were consid-
ered suitable surrogates for full documents; Bagga
and Baldwin (1998) studied summarisation in the
context of a cross-document coreference task and
found that summaries improved the performance of
a clustering-based coreference mechanism; more re-
cently Latif and McGee (2009) have proposed text
summarisation as a preprocessing step for student
essay assessment finding that summaries could be
used instead of full essays to group ?similar? qual-
ity essays. Summarisation has been studied in the
field of sentiment analysis with the objective of pro-
ducing opinion summaries, however, to the best of
our knowlegde there has been little research on the
study of document summarisation as a text pro-
cessing step for opinion classification. This paper
presents a framework and extensive experiments on
text summarisation for opinion classification, and in
particular, for the rating-inference problem. We will
present results indicating that some types of sum-
maries could be as effective or better than the full
documents in this task.
The remainder of the paper is organised as fol-
lows: Section 2 will compile the existing work with
respect to the inference-rating problem; Section 3
and Section 4 will describe the corpus and the NLP
tools used for all the experimental set-up. Next, the
text summarisation approaches will be described in
Section 5, and then Section 6 will show the exper-
iments conducted and the results obtained together
with a discussion. Finally, we will draw some con-
clusions and address further work in Section 7.
2 Related Work
Most of the literature regarding sentiment analysis
addresses the problem either by detecting and clas-
sifying opinions at a sentence level (Wilson et al,
2005; Du and Tan, 2009), or by attempting to cap-
ture the overall sentiment of a document (McDonald
et al, 2007; Hu et al, 2008). Traditional approaches
tackle the task as binary classification, where text
units (e.g. words, sentences, fragments) are classi-
fied into positive vs. negative, or subjective vs. ob-
jective, according to their polarity and subjectivity
degree, respectively. However, sentiment classifica-
tion taking into account a finer granularity has been
less considered. Rating-inference is a particular task
within sentiment analysis, which aims at inferring
the author?s numerical rating for a review. For in-
stance, given a review and 5-star-rating scale (rang-
ing from 1 -the worst- to 5 -the best), this task should
correctly predict the review?s rating, based on the
language and sentiment expressed in its content.
In (Pang and Lee, 2005), the rating-inference
problem is analysed for the movies domain. In
particular, the utility of employing label and item
similarity is shown by analysing the performance
of three different methods based on SVM (one vs.
all, regression and metric labeling), in order to infer
the author?s implied numerical rating, which ranges
from 1 up to 4 stars, depending on the degree the au-
thor of the review liked or not the film. The approach
described in (Leung et al, 2006) suggests the use of
collaborative filtering algorithms together with sen-
timent analysis techniques to obtain user preferences
expressed in textual reviews, focusing also on movie
reviews. Once the opinion words from user reviews
have been identified, the polarity of those opinion
words together with their strength need to be com-
puted and mapped to the rating scales to be further
input to the collaborative input algorithms.
Apart from these approaches, this problem is
stated from a different point of view in (Shimada
and Endo, 2008). Here it is approached from the
perspective of rating different details of a product
under the same review. Consequently, they rename
the problem as ?seeing several stars? instead of only
one, corresponding to the overall sentiment of the
review. Also, in (Baccianella et al, 2009) the rating
of different features regarding hotel reviews (cleanli-
ness, location, staff, etc.) is addressed by analysing
several aspects involved in the generation of prod-
uct review?s representations, such as part-of-speech
and lexicons. Other approaches (Devitt and Ahmad,
2007), (Turney, 2002) face this problem by group-
ing documents with closer stars under the same cat-
egory, i.e. positive or negative, simplifying the task
into a binary classification problem.
Recently, due to the vast amount of on-line infor-
mation and the subjectivity appearing in documents,
the combination of sentiment analysis and summari-
108
sation task in tandem can result in great benefits
for stand-alone applications of sentiment analysis,
as well as for the potential uses of sentiment analy-
sis as part of other NLP applications (Stoyanov and
Cardie, 2006). Whilst there is much literature com-
bining sentiment analysis and text summarisation
focusing on generating opinion-oriented summaries
for the new textual genres, such as blogs (Lloret
et al, 2009), or reviews (Zhuang et al, 2006), the
use of summaries as substitutes of full documents in
tasks such as rating-inference has been not yet ex-
plored to the best of our knowledge. In contrast to
the existing literature, this paper uses summaries in-
stead of full reviews to tackle the rating-inference
task in the financial domain, and we carry out a pre-
liminary analysis concerning the potential benefits
of text summaries for this task.
3 Dataset for the Rating-inference Task
Since there is no standard dataset for carrying out
the rating-inference task, the corpus used for our ex-
periments was one associated to a current project on
business intelligence we are working on. These data
consisted of 89 reviews of several English banks
(Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, and
National Westminster) gathered from the Internet. In
particular the documents were collected from Ciao3,
a Website where users can write reviews about dif-
ferent products and services, depending on their own
experience.
Table 1 lists some of the statistical properties of
the data. It is worth stressing upon the fact that
the reviews have on average 2,603 words, which
means that we are dealing with long documents
rather than short ones, making the rating-inference
task even more challenging. The shortest document
contains 1,491 words, whereas the longest document
has more than 5,000 words.
# Reviews Avg length Max length Min length
89 2,603 5,730 1,491
Table 1: Corpus Statistics
Since the aim of the task we are pursuing focuses
on classifying correctly the star for a review (rang-
ing from 1 to 5 stars), it is necessary to study how
3http://www.ciao.co.uk/
many reviews we have for each class, in order to see
whether we have a balanced distribution or not. Ta-
ble 2 shows this numbers for each star-rating. It is
worth mentioning that one-third of the reviews be-
long to the 4-star class. In contrast, we have only 9
reviews that have been rated as 3-star, consisting of
the 10% of the corpus, which is a very low number.
Star-rating # reviews %
1-star 17 19
2-star 11 12
3-star 9 10
4-star 28 32
5-star 24 27
Table 2: Class Distribution
4 Natural Language Processing Tools
Linguistic analysis of textual input is carried out
using the General Architecture for Text Engineer-
ing (GATE) ? a framework for the development and
deployment of language processing technology in
large scale (Cunningham et al, 2002). We make use
of typical GATE components: tokenisation, parts of
speech tagging, and morphological analysis to pro-
duce document annotations. From the annotations
we produce a number of features for document rep-
resentation. Features produced from the annotations
are: string ? the original, unmodified text of each
token; root ? the lemmatised, lower-case form of
the token; category ? the part-of-speech (POS) tag, a
symbol that represents a grammatical category such
as determiner, present-tense verb, past-tense verb,
singular noun, etc.; orth ? a code representing the to-
ken?s combination of upper- and lower-case letters.
In addition to these basic features, ?sentiment? fea-
tures based on a lexical resource are computed as
explained below.
4.1 Sentiment Features
SentiWordNet (Esuli and Sebastiani, 2006) is a lexi-
cal resource in which each synset (set of synonyms)
of WordNet (Fellbaum, 1998) is associated with
three numerical scores obj (how objective the word
is), pos (how positive the word is), and neg (how
negative the word is). Each of the scores ranges
from 0 to 1, and their sum equals 1. SentiWord-
Net word values have been semi-automatically com-
puted based on the use of weakly supervised classi-
109
fication algorithms. In this work we compute the
?general sentiment? of a word in the following way:
given a word w we compute the number of times the
word w is more positive than negative (positive >
negative), the number of times is more negative than
positive (positive < negative) and the total number
of entries of word w in SentiWordNet, therefore we
can consider the overall positivity or negativity a
particular word has in SentiWordNet. We are in-
terested in words that are generally ?positive?, gen-
erally ?negative? or generally ?neutral? (not much
variation between positive and negative). For exam-
ple a word such as ?good? has many more entries
where the positive score is greater than the nega-
tivity score while a word such as ?unhelpful? has
more negative occurrences than positive. We use this
aggregated scores in our classification experiments.
Note that we do not apply any word sense disam-
biguation procedure here.
4.2 Machine Learning Tool
For the experiments reported here, we adopt a Sup-
port Vector Machine (SVM) learning paradigm not
only because it has recently been used with suc-
cess in different tasks in natural language processing
(Isozaki and Kazawa, 2002), but it has been shown
particularly suitable for text categorization (Kumar
and Gopal, 2009) where the feature space is huge, as
it is in our case. We rely on the support vector ma-
chines implementation distributed with the GATE
system (Li et al, 2009) which hides from the user
the complexities of feature extraction and conver-
sion from documents to the machine learning imple-
mentation. The tool has been applied with success
to a number of datasets for opinion classification and
rating-inference (Saggion and Funk, 2009).
5 Text Summarisation Approach
In this Section, three approaches for carrying out the
summarisation process are explained in detail. First,
a generic approach is taken as a basis, and then, it is
adapted into a query-focused and a opinion-oriented
approach, respectively.
5.1 Generic Summarisation
A generic text summarisation approach is first taken
as a core, in which three main stages can be distin-
guished: i) document preprocessing; ii) relevance
detection; and ii) summary generation. Since we
work with Web documents, an initial preprocessing
step is essential to remove all unnecessary tags and
noisy information. Therefore, in the first stage the
body of the review out of the whole Web page is
automatically delimitated by means of patterns, and
only this text is used as the input for the next sum-
marisation stages. Further on, a sentence relevance
detection process is carried out employing different
combinations of various techniques. In particular,
the techniques employed are:
Term frequency (tf ): this technique has been
widely used in different summarisation approaches,
showing the the most frequent words in a document
contain relevant information and can be indicative of
the document?s topic (Nenkova et al, 2006)
Textual entailment (te): a te module (Ferra?ndez
et al, 2007) is used to detect redundant information
in the document, by computing the entailment be-
tween two consecutive sentences and discarding the
entailed ones. The identification of these entailment
relations helps to avoid incorporating redundant in-
formation in summaries.
Code quantity principle (cqp): this is a linguis-
tic principle which proves the existence of a propor-
tional relation between how important the informa-
tion is, and the number of coding elements it has
(Givo?n, 1990). In this approach we assume that sen-
tences containing longer noun-phrases are more rel-
evant.
The aforementioned techniques are combined
together taking always into account the term-
frequency, leading to different summarisation strate-
gies (tf, te+tf, cqp+tf, te+cqp+tf ). Finally, the re-
sulting summary is produced by extracting the high-
est scored sentences up to the desired length, accord-
ing the techniques explained.
5.2 Query-focused Summarisation
Through adapting the generic summarisation ap-
proach into a query-focused one, we could benefit
from obtaining more specific sentences with regard
to the topic of the review. As a preliminary work, we
are going to assume that a review is about a bank,
and as a consequence, the name of the bank is con-
sidered to be the topic. It is worth mentioning that a
person can refer to a specific bank in different ways.
For example, in the case of ?The National Westmin-
110
ster Bank?, it can be referred to as ?National West-
minster? or ?NatWest?. Such different denomina-
tions were manually identified and they were used
to biased the content of the generated summaries,
employing the same techniques of tf, te and the cqp
combined together. One limitation of this approach
is that we do not directly deal with the coreference
problem, so for example, sentences containing pro-
nouns referring also to the bank, will not be taken
into consideration in the summarisation process. We
are aware of this limitation and for future work it
would be necessary to run a coreference algorithm
to identify all occurrences of a bank within a review.
However, since the main goal of this paper is to carry
out a preliminary analysis of the usefulness of sum-
maries in contrast to whole reviews in the rating-
inference problem, we did not take this problem into
account at this stage of the research. In addition,
when we do query-focused summarisation only we
rely on the SUMMA toolkit (Saggion, 2008) to pro-
duce a query similarity value for each sentence in the
review which in turn is used to rank sentences for an
extractive summary (qf ). This similarity value is the
cosine similarity between a sentence vector (terms
and weights) and a query vector (terms and weigths)
and where the query is the name of the entity being
reviewed (e.g. National Westminster).
5.3 Opinion-oriented Summarisation
Since reviews are written by people who want to
express their opinion and experience with regard
to a bank, in this particular case, either generic or
query-focused summaries can miss including some
important information concerning their sentiments
and feelings towards this particular entity. There-
fore, a sentiment classification system similar to the
one used in (Balahur-Dobrescu et al, 2009) is used
together with the summarisation approach, in order
to generate opinion-oriented summaries. First of all,
the sentences containing opinions are identified, as-
signing each of them a polarity (positive and neg-
ative) and a numerical value corresponding to the
polarity strength (the higher the negative score, the
more negative the sentence and similarly, the higher
the positive score, the more positive the sentence).
Sentences containing a polarity value of 0 are con-
sidered neutral and are not taken into account. Once
the sentences are classified into positives, negatives
and neutrals, they are grouped together according
to its type. Further on, the same combination of
techniques as for previously explained summarisa-
tion approaches are then used.
Additionally, a summary containing only the most
positive and negative sentences is also generated (we
have called this type of summaries sent) in order to
check whether the polarity strength on its own could
be a relevant feature for the summarisation process.
6 Evaluation Environment
In this Section we are going to describe in detail all
the experimental set-up. Firstly, we will explain the
corpus we used together with some figures regard-
ing some statistics computed. Secondly, we will de-
scribe in-depth all the experiments we ran and the re-
sults obtained. Finally, an extensive discussion will
be given in order to analyse all the results and draw
some conclusions.
6.1 Experiments and Results
The main objective of the paper is to investigate the
influence of summaries in contrast to full reviews for
the rating-inference problem.
The purpose of the experiments is to analyse the
performance of the different suggested text sum-
marisation approaches and compare them to the per-
formance of the full review. Therefore, the experi-
ments conducted were the following: for each pro-
posed summarisation approach, we experimented
with five different types of compression rates for
summaries (ranging from 10% to 50%). Apart from
the full review, we dealt with 14 different sum-
marisation approaches (4 for generic, 5 for query-
focused and 5 for opinion-oriented summarisation),
as well as 2 baselines (lead and final, taking the first
or the last sentences according to a specific compres-
sion rate, respectively). Each experiment consisted
of predicting the correct star of a review, either with
the review as a whole or with one of the summari-
sation approaches. As we previously said in Sec-
tion 4, for predicting the correct star-rating, we used
machine learning techniques. In particular, differ-
ent features were used to train a SVM classifier with
10-fold cross validation4 , using the whole review:
4The classifier used was the one integrated within the GATE
framework: http://gate.ac.uk/
111
the root of each word, its category, and the calcu-
lated value employing the SentiWordNet lexicon, as
well as their combinations. As a baseline for the full
document we took into account a totally uninformed
approach with respect to the class with higher num-
ber of reviews, i.e. considering all documents as if
they were scored with 4 stars. The different results
according different features can be seen in Table 3.
Feature F?=1
baseline 0.300
root 0.378
category 0.367
sentiWN 0.333
root+category 0.356
root+sentiWN 0.333
category+sentiWN 0.389
root+category+sentiWN 0.413
Table 3: F-measure results using the full review for clas-
sification
Regarding the features for training the summaries,
it is worth mentioning that the best performing fea-
ture when no sentiment-based features are taken into
account is the one using the root of the words. Con-
sequently, this feature was used to train the sum-
maries. Moreover, since the best results using the
full review were obtained using the combination of
the all the features (root+category+sentiWN), we
also selected this combination to train the SVM
classifier with our summaries. Conducting both
experiments, we could analyse to what extent the
sentiment-based feature benefit the classification
process.
The results obtained are shown in Table 4 and
Table 5, respectively. These tables show the F-
measure value obtained for the classification task,
when features extracted from summaries are used
instead from the full review. On the one hand,
results using the root feature extracted from sum-
maries can be seen in Table 4. On the other hand,
Table 5 shows the results when the combination
of all the linguistic and sentiment-based features
(root+category+sentiWN), that has been extracted
from summaries, are used for training the SVM clas-
sifier.
We also performed two statistical tests in order
to measure the significance for the results obtained.
The tests we performed were the one-way Analy-
sis of Variance (ANOVA) and the t-test (Spiegel and
Castellan, 1998). Given a group of experiments, we
first run ANOVA for analysing the difference be-
tween their means. In case some differences are
found, we run the t-test between those pairs.
6.2 Discussion
A first analysis derived from the results obtained in
Table 3 makes us be aware of the difficulty associ-
ated to the rating-inference task. As can be seen,
a baseline without any information from the docu-
ment at all, is performing around 30%, which com-
pared to the remaining approaches is not a very bad
number. However, we assumed that dealing with
some information contained in documents, the clas-
sification algorithm will do better in finding the cor-
rect star associated to a review. This was the rea-
son why we experimented with different features
alone or in combination. From these experiments,
we obtained that the combination of linguistic and
semantic-based features leads to the best results, ob-
taining a F-measure value of 41%. If sentiment-
based features are not taken into account, the best
feature is the root of the word on its own. Further-
more, in order to analyse further combinations, we
ran some experiments with bigrams. However, the
results obtained did not improve the ones we already
had, so they are not reported in this paper.
As far as the results is concerned comparing the
use of summaries to the full document, it is worth
mentioning that when using specific summarisation
approaches, such as query-focused summaries com-
bined with term-frequency, we get better results than
using the full document with a 90% confidence in-
terval, according to a t-test. In particular, qf for 10%
is significant with respect to the full document, us-
ing only root as feature for training. For the results
regarding the combination of root, category and Sen-
tiWordNet, qf for 10% and qf+tf for 10% and 20%
are significant with respect to the full document.
Concerning the different summarisation ap-
proaches, it cannot be claimed a general tendency
about which ones may lead to the best results. We
also performed some significance tests between dif-
ferent strategies, and in most of the cases, the t-
test and the ANOVA did not report significance
over 95%. Only a few approaches were significant
at a 95% confidence level, for instance, te+cqp+tf
and sent+te+cqp+tf with respect to sent+cqp+tf
112
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.411 0.378 0.367 0.311 0.322
final F?=1 0.322 0.389 0.300 0.467 0.456
tf F?=1 0.400 0.344 0.400 0.367 0.367
te+tf F?=1 0.367 0.422 0.411 0.389 0.322
cqp+tf F?=1 0.300 0.344 0.311 0.300 0.256
te+cqp+tf F?=1 0.422 0.356 0.333 0.300 0.322
qf F?=1 0.513 0.388 0.375 0.363 0.363
qf+tf F?=1 0.567 0.467 0.311 0.367 0.389
qf+te+tf F?=1 0.389 0.367 0.411 0.378 0.333
qf+cqp+tf F?=1 0.300 0.356 0.378 0.378 0.333
qf+te+cqp+tf F?=1 0.322 0.322 0.367 0.367 0.356
sent F?=1 0.344 0.380 0.391 0.290 0.336
sent+tf F?=1 0.378 0.425 0.446 0.303 0.337
sent+te+tf F?=1 0.278 0.424 0.313 0.369 0.347
sent+cqp+tf F?=1 0.333 0.300 0.358 0.358 0.324
sent+te+cqp+tf F?=1 0.446 0.334 0.358 0.292 0.369
Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;
tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focused
summaries; and sent = opinion-oriented summaries)
for 10%; sent+tf in comparison to sent+cqp+tf
for 20%; or sent with respect to cqp+tf for 40%
and 50% compression rates. Other examples of
the approaches that were significant at a 90%
level of confidence are qf for 10% with respect to
sent+te+cqp+tf. Due to the wide range of summari-
sation strategies tested in the experiments, the results
obtained vary a lot and, due to the space limitations,
it is not possible to report all the tables. What it
seems to be clear from the results is that the code
quantity principle (see Section 5) is not contributing
much to the summarisation process, thus obtaining
poor results when it is employed. Intuitively, this
can be due to the fact that after the first mention of
the bank, there is a predominant use of pronouns,
and as a consequence, the accuracy of the tool that
identifies noun-phrases could be affected. The same
reason could be affecting the term-frequency calcu-
lus, as it is computed based on the lemmas of the
words, not taking into account the pronouns that re-
fer also to them.
7 Conclusion and Future Work
This paper presented a preliminary study of
inference-rating task. We have proposed here a new
framework for comparison and extrinsic evaluation
of summaries in a text-based classification task. In
our research, text summaries generated using differ-
ent strategies were used for training a SVM classifier
instead of full reviews. The aim of this task was to
correctly predict the category of a review within a 1
to 5 star-scale. For the experiments, we gathered 89
bank reviews from the Internet and we generated 16
summaries of 5 different compression rates for each
of them (80 different summaries for each review,
having generated in total 7,120 summaries). We also
experimented with several linguistic and sentiment-
based features for the classifier. Although the re-
sults obtained are not significant enough to state
that summaries really help the rating-inference task,
we have shown that in some cases the use of sum-
maries (e.g. query/entity-focused summaries) could
offer competitive advantage over the use of full doc-
uments and we have also shown that some summari-
sation techniques do not degrade the performance of
a rating-inference algorithm when compared to the
use of full documents. We strongly believe that this
preliminary study could serve as a starting point for
future developments.
Although we have carried out extensive experi-
mentation with different summarisation techniques,
compression rates, and document/summary features,
there are many issues that we have not explored. In
the future, we plan to investigate whether the re-
sults could be affected by the class distribution of
the reviews, and in this line we would like to see the
distribution of the documents using clustering tech-
113
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.275 0.422 0.422 0.378 0.322
final F?=1 0.275 0.378 0.333 0.344 0.400
tf F?=1 0.411 0.422 0.411 0.378 0.378
te+tf F?=1 0.411 0.344 0.344 0.344 0.378
cqp+tf F?=1 0.358 0.267 0.333 0.222 0.289
te+cqp+tf F?=1 0.444 0.411 0.411 0.311 0.322
qf F?=1 0.563 0.488 0.400 0.375 0.350
qf+tf F?=1 0.444 0.411 0.433 0.367 0.356
qf+te+tf F?=1 0.322 0.367 0.356 0.344 0.344
qf+cqp+tf F?=1 0.292 0.322 0.367 0.333 0.356
qf+te+cqp+tf F?=1 0.356 0.378 0.356 0.367 0.356
sent F?=1 0.322 0.370 0.379 0.412 0.414
sent+tf F?=1 0.378 0.446 0.359 0.380 0.402
sent+te+tf F?=1 0.333 0.414 0.404 0.380 0.381
sent+cqp+tf F?=1 0.300 0.333 0.347 0.358 0.296
sent+te+cqp+tf F?=1 0.436 0.413 0.425 0.359 0.324
Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen-
tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle with
noun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)
niques. Moreover, we would also like to investigate
what it would happen if we consider the values of the
star-rating scale as ordinal numbers, and not only as
labels for categories. We will replicate the exper-
iments presented here using as evaluation measure
the ?mean square error? which has been pinpointed
as a more appropriate measure for categorisation in
an ordinal scale. Finally, in the medium to long-
term we plan to extent the experiments and analy-
sis to other available datasets in different domains,
such as movie or book reviews, in order to see if
the results could be influenced by the nature of the
corpus, allowing also further results for comparison
with other approaches and assessing the difficulty of
the task from a perspective of different domains.
Acknowledgments
This research has been supported by the project PROM-
ETEO ?Desarrollo de Te?cnicas Inteligentes e Interacti-
vas de Miner??a de Textos? (2009/119) from the Valencian
Government. Moreover, Elena Lloret is funded by the
FPI program (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation under the project TEXT-
MESS (TIN2006-15265-C06-01), and Horacio Saggion
is supported by a Ramo?n y Cajal Fellowship from the
Ministry of Science and Innovation, Spain. The authors
would also like to thank Alexandra Balahur for helping to
process the dataset with her Opinion Mining approach.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2009. Multi-
facet Rating of Product Reviews. In Proceedings of
the 31th European Conference on IR Research on Ad-
vances in Information Retrieval, pages 461?472.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the COLING-ACL, pages
79?85.
A. Balahur-Dobrescu, M. Kabadjov, J. Steinberger,
R. Steinberger, and A. Montoyo. 2009. Summarizing
Opinions in Blog Threads. In Proceedings of the Pa-
cific Asia Conference on Language, INformation and
Computation Conference, pages 606?613.
S. Chakraborti, R. Mukras, R. Lothian, N. Wiratunga,
S. Watt, and D Harper. 2007. Supervised Latent Se-
mantic Indexing using Adaptive Sprinkling. In Pro-
ceedings of IJCAI-07, pages 1582?1587.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphi-
cal Development Environment for Robust NLP Tools
and Applications. In Proceedings of the ACL.
A. Devitt and K. Ahmad. 2007. Sentiment Polarity Iden-
tification in Financial News: A Cohesion-based Ap-
proach. In Proceedings of the ACL, pages 984?991.
W. Du and S. Tan. 2009. An Iterative Reinforcement
Approach for Fine-Grained Opinion Mining. In Pro-
ceedings of the NAACL, pages 486?493.
A. Esuli and F. Sebastiani. 2006. SENTIWORDNET: A
Publicly Available Lexical Resource for Opinion Min-
ing. In Proceedings of LREC, pages 417?422.
114
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
O. Ferra?ndez, D. Micol, R. Mun?oz, and M. Palomar.
2007. A Perspective-Based Approach for Solving Tex-
tual Entailment Recognition. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 66?71, June.
S. Ferrari, T. Charnois, Y. Mathet, F. Rioult, and
D. Legallois. 2009. Analyse de Discours ?Evaluatif,
Mode`le Linguistique et Applications. In Fouille de
donne?es d?opinion, volume E-17, pages 71?93.
T. Givo?n, 1990. Syntax: A functional-typological intro-
duction, II. John Benjamins.
C. Grouin, M. Hurault-Plantet, P. Paroubek, and J. B.
Berthelin. 2009. DEFT?07 : Une Campagne
d?Avaluation en Fouille d?Opinion. In Fouille de
donne?es d?opinion, volume E-17, pages 1?24.
Y. Hu, W. Li, and Q. Lu. 2008. Developing Evalua-
tion Model of Topical Term for Document-Level Sen-
timent Classification. In Proceedings of the 10th Pa-
cific Rim International Conference on Artificial Intel-
ligence, pages 175?186.
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 390?396.
M. A. Kumar and M. Gopal. 2009. Text Categorization
Using Fuzzy Proximal SVM and Distributional Clus-
tering of Words. In Proceedings of the 13th Pacific-
Asia Conference on Advances in Knowledge Discovery
and Data Mining, pages 52?61.
S. Latif and M. McGee Wood. 2009. A Novel Technique
for Automated Linguistic Quality Assessment of Stu-
dents? Essays Using Automatic Summarizers. Com-
puter Science and Information Engineering, World
Congress on, 5:144?148.
C. W. K. Leung, S. C. F. Chan, and F. L. Chung.
2006. Integrating Collaborative Filtering and Sen-
timent Analysis: A Rating Inference Approach. In
Proceedings of The ECAI 2006 Workshop on Recom-
mender Systems, pages 62?66.
Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapt-
ing SVM for Data Sparseness and Imbalance: A Case
Study in Information Extraction. Natural Language
Engineering, 15(2):241?271.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo.
2009. Towards Building a Competitive Opinion Sum-
marization System: Challenges and Keys. In Proceed-
ings of the NAACL. Student Research Workshop and
Doctoral Consortium, pages 72?77.
I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim. 1998.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. Technical report, The Mitre Corporation.
I. Mani. 2001. Automatic Text Summarization. John
Benjamins Publishing Company.
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the
ACL, pages 432?439.
R. Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, and
D. Harper. 2007. Information Gain Feature Selection
for Ordinal Text Classification using Probability Re-
distribution. In Proceedings of the Textlink workshop
at IJCAI-07.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A Compositional Context Sensitive Multi-document
Summarizer: Exploring the Factors that Influence
Summarization. In Proceedings of the ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 573?580.
B. Pang and L. Lee. 2005. Seeing Stars: Exploiting
Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the ACL,
pages 115?124.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
H. Saggion and A. Funk. 2009. Extracting Opinions and
Facts for Business Intelligence. RNTI, E-17:119?146.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Languages, 49:103?125.
K. Shimada and T. Endo. 2008. Seeing Several Stars: A
Rating Inference Task for a Document Containing Sev-
eral Evaluation Criteria. In Proceedings of the 12th
Pacific-Asia Conference on Advances in Knowledge
Discovery and Data Mining, pages 1006?1014.
S. Spiegel and N. J. Castellan, Jr. 1998. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill
International.
V. Stoyanov and C. Cardie. 2006. Toward Opinion Sum-
marization: Linking the Sources. In Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 9?14.
P. D. Turney. 2002. Thumbs Up or Thumbs Down?: Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In Proceedings of the ACL, pages
417?424.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of the EMNLP, pages 347?
354.
L. Zhuang, F. Jing, and X. Y. Zhu. 2006. Movie Re-
view Mining and Summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
115
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 168?174,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Towards a Unified Approach for Opinion Question Answering and
Summarization
Elena Lloret and Alexandra Balahur and Manuel Palomar and Andre?s Montoyo
Department of Software and Computing Systems
University of Alicante
Alicante 03690, Spain
{elloret,abalahur, mpalomar, montoyo}@dlsi.ua.es
Abstract
The aim of this paper is to present an ap-
proach to tackle the task of opinion question
answering and text summarization. Follow-
ing the guidelines TAC 2008 Opinion Sum-
marization Pilot task, we propose new meth-
ods for each of the major components of the
process. In particular, for the information
retrieval, opinion mining and summarization
stages. The performance obtained improves
with respect to the state of the art by approxi-
mately 12.50%, thus concluding that the sug-
gested approaches for these three components
are adequate.
1 Introduction
Since the birth of the Social Web, users play a cru-
cial role in the content appearing on the Internet.
With this type of content increasing at an exponen-
tial rate, the field of Opinion Mining (OM) becomes
essential for analyzing and classifying the sentiment
found in texts.
Nevertheless, real-world applications of OM of-
ten require more than an opinion mining component.
On the one hand, an application should allow a user
to query about opinions in natural language. There-
fore, Question Answering (QA) techniques must be
applied in order to determine the information re-
quired by the user and subsequently retrieve and
analyze it. On the other hand, opinion mining of-
fers mechanisms to automatically detect and classify
sentiments in texts, overcoming the issue given by
the high volume of such information present on the
Internet. However, in many cases, even the result of
the opinion processing by an automatic system still
contains large quantities of information, which are
still difficult to deal with manually. For example,
for questions such as ?Why do people like George
Clooney?? we can find thousands of answers on the
Web. Therefore, finding the relevant opinions ex-
pressed on George Clooney, classifying them and
filtering only the positive opinions is not helpful
enough for the user. He/she will still have to sift
through thousands of texts snippets, containing rele-
vant, but also much redundant information. For that,
we need to use Text Summarization (TS) techniques.
TS provides a condensed version of one or several
documents (i.e., a summary) which can be used as a
substitute of the original ones (Spa?rck Jones, 2007).
In this paper, we will concentrate on proposing ad-
equate solutions to tackle the issue of opinion ques-
tion answering and summarization. Specifically, we
will propose methods to improve the task of ques-
tion answering and summarization over opinionated
data, as defined in the TAC 2008 ?Opinion Sum-
marization pilot?1. Given the performance improve-
ments obtained, we conclude that the approaches we
proposed for these three components are adequate.
2 Related Work
Research focused on building factoid QA systems
has a long tradition, however, it is only recently that
studies have started to focus on the creation and de-
velopment of opinion QA systems. Example of this
can be (Stoyanov et al, 2004) who took advantage of
opinion summarization to support Multi-Perspective
QA system, aiming at extracting opinion-oriented
information of a question. (Yu and Hatzivassiloglou,
2003) separated opinions from facts and summa-
rized them as answer to opinion questions. Apart
from these studies, specialized competitions for sys-
tems dealing with opinion retrieval and QA have
been organized in the past few years. The TAC
2008 Opinion Summarization Pilot track proposed
a mixed setting of factoid and opinion questions.
1http://www.nist.gov/tac/2008/summarization/
168
It is interesting to note that most of the participat-
ing systems only adapted their factual QA systems
to overcome the newly introduced difficulties re-
lated to opinion mining and polarity classification.
Other relevant competition focused on the treatment
of subjective data is the NTCIR MOAT (Multilin-
gual Opinion Analysis Test Collection). The ap-
proaches taken by the participants in this task are rel-
evant to the process of opinion retrieval, which is the
first step performed by an opinion mining question
answering system. For example, (Taras Zabibalov,
2008) used an almost unsupervised approach ap-
plied to two of the sub-tasks: opinionated sentence
and topic relevance detection.(Qu et al, 2008) ap-
plied a sequential tagging approach at the token level
and used the learned token labels in the sentence
level classification task and their formal run submis-
sion was is trained on MPQA (Wiebe et al, 2005).
3 Text Analysis Conferences
In 2008, the Opinion Summarization Pilot task at
the Text Analysis Conferences2 (TAC) consisted in
generating summaries from blogs, according to spe-
cific opinion questions provided by the TAC orga-
nizers. Given a set of blogs from the Blog06 col-
lection3 and a list of questions, participants had to
produce a summary that answered these questions.
The questions generally required determining opin-
ion expressed on a target, each of which dealt with a
single topic (e.g. George Clooney). Additionally, a
set of text snippets were also provided, which con-
tained the answers to the questions. Table 1 depicts
an example of target, question, and optional snippet.
Target: George Clooney
Questions: Why do people like George Clooney?Why do people dislike George Clooney?
Snippets: 1050 BLOG06-20060209-006-0013539097
he?s a great actor.
Table 1: Example of target, question, and snippet.
Following the results obtained in the evaluation
at TAC 2008 (Balahur et al, 2008), we propose
an opinion question answering and summarization
(OQA&S) approach, which is described in detail in
the following sections.
2www.nist.gov/tac/
3http://ir.dcs.gla.ac.uk/test collections/access to data.html
4 An Opinion Question Answering and
Summarization Approach
In order to improve the results of the OQA&S sys-
tem presented at TAC, we propose new methods for
each of the major components of the system: infor-
mation retrieval, opinion mining and text summa-
rization.
4.1 Opinion Question Answering and
Summarization Components
? Information Retrieval
JAVA Information Retrieval system (JIRS) is
a IR system especially suited for QA tasks
(Go?mez, 2007). Its purpose is to find frag-
ments of text (passages) with more probabil-
ity of containing the answer to a user question
made in natural language instead of finding rel-
evant documents for a query. To that end, JIRS
uses the own question structure and tries to
find an equal or similar expression in the docu-
ments. The more similar the structure between
the question and the passage is, the higher the
passage relevance.
JIRS is able to find question structures in a
large document collection quickly and effi-
ciently using different n-gram models. Subse-
quently, each passage is assessed depending on
the extracted n-grams, the weight of these n-
grams, and the relative distance between them.
Finally, it is worth noting that the number of
passages in JIRS is configurable, and in this
research we are going to experiment with pas-
sages of length 1 and 3.
? Opinion Mining
The first step we took in our approach was
to determine the opinionated sentences, as-
sign each of them a polarity (positive or neg-
ative) and a numerical value corresponding to
the polarity strength (the higher the negative
score, the more negative the sentence and vice
versa). In our first approximation (OMaprox1),
we employed a simple, yet efficient method,
presented in Balahur et al (Balahur et al,
2009). As lexicons for affect detection, we
used WordNet Affect (Strapparava and Vali-
tutti, 2004), SentiWordNet (Esuli and Sebas-
169
tiani, 2006), and MicroWNOp (Cerini et al,
2007). Each of the resources we employed
were mapped to four categories, which were
given different scores: positive (1), negative
(-1), high positive (4) and high negative (-4).
First, the score of each of the blog posts was
computed as the sum of the values of the words
that were identified. Subsequently, we per-
formed sentence splitting4 and classified the
sentences we thus obtained according to their
polarity, by adding the individual scores of the
affective words identified.
In the second approach (OMaprox2), we first
filter out the sentences that are associated to
the topic discussed, using LSA. Further on, we
score the sentences identified as relating to the
topic of the blog post, in the same manner as
in the previous approach. The aim of this ap-
proach is to select for further processing only
the sentences which contain opinions on the
post topic. In order to filter these sentences
in, we first create a small corpus of blog posts
on each of the topics included in our collec-
tion5. For each of the corpora obtained, we
apply LSA, using the Infomap NLP Software6.
Subsequently, we compute the 100 most asso-
ciated words with two of the terms that are most
associated with each of the topics and the 100
most associated words with the topic word. The
approach was proven to be successful in (Bal-
ahur et al, 2010).
? Text Summarization
The text summarization approach used in this
paper was presented in (Lloret and Palomar,
2009). In order to generate a summary, the
suggested approach first carries out a basic pre-
processing stage comprising HTML parsing,
sentence segmentation, tokenization, and stem-
ming. Once the input document or documents
have been pre-processed, a relevance detection
stage, which is the core part of the approach, is
applied. The objective of this step is to identify
4http://alias-i.com/lingpipe/
5These small corpora (30 posts for each of the top-
ics) are gathered using the search on topic words on
http://www.blogniscient.com/ and crawling the resulting pages.
6http://infomap-nlp.sourceforge.net/
potential relevant sentences in the document by
means of three techniques: textual entailment,
term frequency and the code quantity principle
(Givo?n, 1990). Then, each potential relevant
sentence is given a score which is computed
on the basis of the aforementioned techniques.
Finally, all sentences are ordered according
to their scores, and the highest ranked ones
(which mean those sentences contain more im-
portant information) are selected and extracted
up to the desired length, thus building the fi-
nal summary. It is worth stressing upon the fact
that in an attempt to maintain the coherence of
the original documents, sentences are shown in
the same order they appear in the original doc-
uments.
4.2 Experimental Framework
The objective of this section is to describe the corpus
used and the experiments performed with the data
provided in TAC 2008 Opinion Summarization Pi-
lot7 task. The approaches analyzed comprise:
? OQA&S: The three components explained
in the previous section (information retrieval,
opinion mining and summarization) were
bound together in order to produce summaries
that include the answer to opinionated ques-
tions. First, the most relevant passages of
length 1 and 3 are retrieved by the IR module,
as in the aforementioned approach, and then
the subjective information is found and classi-
fied within them using the OM approaches de-
scribed in the previous section. Further on, we
incorporate the TS module, to select and ex-
tract the most relevant opinionated facts from
the pool of subjective information identified
by the OM module. We generate opinion-
oriented summaries of compression rates rang-
ing from 10% to 50%. In the end, four dif-
ferent approaches result from the integration
of the three components: IRp1-OMaprox1-
TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-
TS; and IRp3-OMaprox2-TS.
Moreover, apart from these approaches, two base-
lines were also defined. On the one hand, we sug-
7http://www.nist.gov/tac/data/past-
blog06/2008/OpSummQA08.html#OpSumm
170
gest a baseline using the list of snippets provided by
the TAC organization (QA-snippets). This baseline
produces a summary by joining all the answers in the
snippets that related to the same topic On the other
hand, we took as a second baseline the approach
from our participation in TAC 2008 (DLSIUAES),
without not taking into account any information re-
trieval or question answering system to retrieve the
fragments of information which may be relevant to
the query. In contrast, this was performed by com-
puting the cosine similarity8 between each sentence
in the blog and the query. After all the potential rel-
evant sentences for the query were identified, they
were classified in terms of subjectivity and polarity,
and the most relevant ones were selected for the final
summary.
4.3 Evaluation Methodology
Since we used the corpus provided at the Opinion
Summarization Pilot task, and we followed simi-
lar guidelines, we should evaluate our OQA&S ap-
proach in the same way as participant systems were
assessed. However, the evaluation methodology
proposed differs slightly from the one carried out
in the competition. The reason why we took such
decision was due to the fact that the evaluation car-
ried out in TAC had some limitations, and therefore
was not suitable for our purposes. In this manner,
our evaluation is also based on the gold-standard
nuggets provided by TAC, but in addition we pro-
posed an extended version of them, by adding other
pieces of information that are also relevant to the
topics.
In this section, all the issues concerning the eval-
uation are explained. These comprise the original
evaluation method used in the Opinion Summariza-
tion Pilot task at TAC (Section 4.3.1) , its draw-
backs (Section 4.3.2), and the extended version for
the evaluation method we propose (Section 4.3.3).
Further on, the results obtained together with a wide
discussion, as well as its comparison with the base-
lines and the TAC participants is provided in Section
4.4.
4.3.1 Nugget-based Evaluation at TAC
Within the Opinion Summarization Pilot task,
each summary was evaluated according to its con-
8http://www.d.umn.edu/ tpederse/text-similarity.html
tent using the Pyramid method (Nenkova et al,
2007). A list of nuggets was provided and the asses-
sors used such list of nuggets to count the number
of nuggets a summary contained. Depending on the
number of nuggets the summary included and the
importance of each one given by their weight, the
values for recall, precision and F-measure were ob-
tained. An example of several nuggets correspond-
ing to different topics can be seen in Table 2, where
the weight for each one is also shown in brackets.
Topic Nugget (weight)
Carmax CARMAX prices are firm, the price is
the price (0.9)
Jiffy Lube They should have torque wrenches (0.2)
Talk show hosts Funny (0.78)
Table 2: Example of evaluation nuggets and associated
weights.
4.3.2 Limitations of the Nugget Evaluation
The evaluation method suggested at TAC requires
a lot of human effort when it comes to identify
the relevant fragments of information (nuggets) and
compute how many of them a summary contains, re-
sulting in a very costly and time-consuming task.
This is a general problem associated to the evalua-
tion of summaries, which makes the task of summa-
rization evaluation especially hard and difficult.
But, apart from this, when an exhaustive exam-
ination of the nuggets used in TAC is done, some
other problems arised which are worth mentioning.
The average number of nuggets for each topic is
27, and this would mean, that longer summaries
will be highly penalized, because it will contain
more useless information according to the nuggets.
After analyzing in detail all the provided nuggets,
we mainly classified the possible problems into six
groups, which are:
1. Some of the nuggets were expressed differently
from how they appeared in the original blogs.
Since most of the summarization systems are ex-
tractive, this fact forced that humans had to evaluate
the summaries, otherwise it would be very difficult
to account for the presence of such nugget in the
summary, if they are not using the same vocabulary
as the original blogs.
2. Some nuggets for the same topic express the
171
same idea, despite not being identical. In these
cases, we are counting a single piece of informa-
tion in the summary twice, if the idea that nuggets
expressed is included.
3. Moreover, the meaning of one nugget can be de-
duced from another?s, which is also related to the
problem stated before.
4. Some of the nuggets are not very clear in mean-
ing (e.g. ?hot?, ?fun?). This would mean that a
summary might include such terms in a different
context, thus, obtaining incorrectly that it is reve-
lant when might be out of context.
5. A sentence in the original blog can be covered by
several nuggets. For instance, both nuggets ?it is
an honest book? and ?it is a great book? correspond
to the same sentence ?It was such a great book-
honest and hard to read (content not language dif-
ficulty)?. In this case, it is not clear how to proceed
with the evaluation; whether to count both nuggets
or just one of them.
6. Some information which is also relevant for the
topic is not present in any nugget. For instance:
?I go to Starbucks because they generally provide
me better service?. Although it is relevant with re-
spect to the topic and it appears in a number of sum-
maries, it would be not counted because it has not
been chosen as a nugget.
4.3.3 Extended Nugget-based Evaluation
Since we are interested in testing a wide range of
approaches involving IR, OM and TS, sticking to the
rules to the original TAC evaluation would mean that
a lot of time as well as human effort will be required,
as well as not accounting for important information
that summaries may contain in addition to the one
expressed by the nuggets. Therefore, taking as a ba-
sis the nuggets provided at TAC, we set out a modi-
fied version of them.
The underlying idea behind this is to create an ex-
tended set of nuggets that serve as a reference for
assessing the content of the summaries. In this man-
ner, we will map each original nugget with the set of
sentences in the original blogs that are most similar
to it, thus generating a gold-standard summary for
each topic. For creating this extended gold-standard
nuggets we compute the cosine similarity9 between
9The cosine similarity was computed using Pedersen?s
every nugget and all the sentences in the blog related
to the same topic. We empirically established a sim-
ilarity threshold of 0.5, meaning that if a sentence
was equal or above such similarity value, it will be
considered also relevant. One main disadvantage of
such a lower threshold value is that we can consider
relevant sentences that share the same vocabulary
but in fact they are not relevant to the summary. In
order to avoid this, once we had identified all the
most similar sentences to each nugget, we carried
out a manual analysis to discard cases like this. Hav-
ing created the extended set of nuggets, we grouped
all of them pertaining to the same topic, and consid-
ered it a gold-standard summary. Now, the average
number of nuggets per topic is 53, which we have
increased by twice the number of original nuggets
provided at TAC.
Further on, our summaries are compared against
this new gold-standard using ROUGE (Lin, 2004).
This tool computes the number of different kinds
of overlap n-grams between an automatic summary
and a human-made summary. For our evaluation,
we compute ROUGE-1 (unigrams), ROUGE-2 (bi-
grams), ROUGE-SU4 (it measures the overlap of
skip-bigrams between a candidate summary and a
set of reference summaries with a maximum skip
distance of 4), and ROUGE-L (Longest Common
Subsequence between two texts). The results and
discussion are next provided.
4.4 Results and Discussion
This section contains the results obtained for our
OQA&S approach and all the sub-approaches tested.
IRpN refers to the length of the passage employed
in the information retrieval approach, whereas
OMaproxN indicates the approach used for the opin-
ion mining component. Firstly, we show and ana-
lyze the results of our different approaches, and then
we compared the best performing one with the base-
lines and the average Opinion Summarization Pilot
task participants results in TAC.
Table 3 shows the precision (Pre), recall (Rec) and
F-measure results of ROUGE-1 (R-1) for all the ap-
proaches we experimented with.
Generally speaking, the results obtained show
better figures for precision than for recall, and there-
Text Similarity Package: http://www.d.umn.edu/ tpederse/text-
similarity.html
172
Approach Summary length
Name R-1 10% 20% 30% 40% 50%
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 14.45 18.58 22.32 23.63 26.32
-OMaprox1-TS F?=1 16.53 20.65 24.58 25.75 28.12
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 16.90 20.02 23.36 24.15 26.77
-OMaprox2-TS F?=1 19.45 22.13 25.36 25.94 28.40
Pre 27.27 30.18 30.91 30.05 30.19
IRp3 Rec 20.56 24.76 28.25 31.67 34.47
-OMaprox1-TS F?=1 22.65 26.23 27.98 29.18 29.74
Pre 30.16 32.11 32.35 32.41 32.11
IRp3 Rec 20.64 24.03 27.25 29.78 32.68
-OMaprox2-TS F?=1 23.28 25.64 27.42 28.44 29.21
Table 3: Results of our OQA&S approaches
Approach Performance (ROUGE)
Name % R-1 R-2 R-L R-SU4
Pre 32.11 7.34 29.00 11.37
IRp3-OMaprox2 Rec 32.68 8.31 33.24 12.76
-TS (50%) F?=1 29.21 7.22 28.60 11.13
Pre 17.97 8.76 17.65 9.98
QA-snippets Rec 71.24 31.30 70.10 37.44
F?=1 24.73 11.58 24.29 13.45
Pre 20.54 7.00 19.46 9.29
DLSIUAES Rec 57.66 18.98 54.61 25.77
F?=1 27.04 9.10 25.59 12.22
Pre 23.74 8.35 22.72 10.81
Average TAC Rec 56.65 19.37 54.56 25.40
participants F?=1 27.45 9.64 26.33 12.46
Pre 20.42 6.06 19.55 8.62
Average TAC Rec 56.45 17.3 54.40 24.11
participants? F?=1 24.31 7.25 23.31 10.29
Table 4: Comparison with other systems
fore the F-measure value, which combines both val-
ues, will be affected. Good precision values means
that the information our approaches select is the cor-
rect one, despite not including all the relevant infor-
mation.
Our best performing approach in general is the
one which uses a length passage of 3 and, as far
as OM is concerned, when topic-sentiment analy-
sis is carried out (IRp3-OMaprox2-TS). This shows
that the approach dealing with topic-sentiment anal-
ysis in opinion mining is more suitable than the one
which does not consider topic relevance. Taking a
look at some individual results, we next try to eluci-
date the reasons why our approach performs better
at some approaches and not so good at others. Con-
cerning the IR module, it is important to mention
that a passage length of 1 always obtains poorer re-
sults that when it is increased to 3, meaning that the
longer the passage, the better.
Regarding the best summary length, we observed
that in general terms, the more content we allow
for the summary, the better. In other words, com-
pression rates of 50% get higher results than 20%
or 10%. However, there are cases in which shorter
summaries (10% and 20%) obtains better results
than longer ones (e.g. IRp3-OMaprox2-TS vs. IRp3-
OMaprox1-TS).
Although the results theirselves are not very high
(around 30%), they are in line with the state-of-the-
art, as can be seen in Table 4, where our best per-
forming approach is compared with respect to other
approaches.
Although the compression rate which obtains best
results is not very high (50%), indeed the final sum-
maries have an average length of 2,333 non-white
space characters. This is really low compared to the
length that TAC organization allowed for the Opin-
ion Summarization Pilot task, which was 7,000 non-
white space characters per question, and most of
the times there were two questions for each topic.
Whereas the results of TAC participants are much
better for the recall value than ours, if we take a look
at the precision, our approach outperforms them ac-
cording to this value in all of the cases. The longer
a summary is, the more chances it has to contain in-
formation related to the topic. However, not all this
information may be relevant, as it is shown in the
results for the precision values, which decrease con-
siderably compared to the recall ones. In contrast,
due to the fact that our approach is missing some
relevant information because we use a rather short
passage length (3 sentences), we do not obtain such
high values for the recall, but we obtain good preci-
sion results, which indicate that the information that
we keep is important.
Moreover, comparing those results with the ones
obtained by our approach, it is worth mentioning
that IRp3-OMaprox2-TS outperforms the F-measure
value for all the ROUGE metrics with respect to Av-
erage TAC participants?. More in detail, when the
ROUGE scores are averaged, IRp3-OMaprox2-TS
improves by 12.50% the Average TAC participants?
for the F-measure value.
173
5 Conclusion and Future Work
In this paper, we tackled the process of OQA&S.
In particular, we analyzed specific methods within
each component of this process, i.e., information
retrieval, opinion mining and text summarization.
These components are crucial in this task, since our
final goal was to provide users with the correct infor-
mation containing the answer of a question. How-
ever, contrary to most research work in question an-
swering, we focus on opinionated questions rather
than factual, increasing the difficulty of the task.
Our analysis comprises different configurations
and approaches: i) varying the length for retrieving
the passages of the documents in the retrieval infor-
mation stage; ii) studying a method that take into
consideration topic-sentiment analysis for detecting
and classifying opinions in the retrieved passages
and comparing it to another that does not; and iii)
generating summaries of different compression rates
(10% to 50%). The results obtained showed that
the proposed methods are appropriate to tackle the
OQA&S task, improving state of the art approaches
by 12.50% approximately.
In the future, we plan to continue investigating
suitable approaches for each of the proposed com-
ponents. Our final goal is to build an integrated and
complete approach.
Acknowledgments
This research work has been funded by the Spanish Gov-
ernment through the research program FPI (BES-2007-
16268) associated to the project TEXT-MESS (TIN2006-
1526-C06-01). Moreover, it has been also partially
funded by projects TEXT-MESS 2.0 (TIN2009-13391-
C04), and PROMETEO (PROMETEO/2009/199) from
the Spanish and the Valencian Government, respectively.
References
A. Balahur, E. Lloret, O. Ferra?ndez, A. Montoyo,
M. Palomar, and R. Mun?oz. 2008. The DLSIUAES
team?s participation in the tac 2008 tracks. In Pro-
ceedings of the Text Analysis Conference.
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
Bruno Pouliquen, and Mijai Kabadjov. 2009. Opinion
mining from newspaper quotations. In Proceedings of
the Workshop on Intelligent Analysis and Processing
of Web News Content.
A. Balahur, M. Kabadjov, and J. Steinberger. 2010.
Exploiting higher-level semantic information for the
opinion-oriented summarization of blogs. In Proceed-
ings of CICLing?2010.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Language re-
sources and linguistic theory: Typology, second lan-
guage acquisition, English linguistics.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceedings of LREC.
Talmy Givo?n, 1990. Syntax: A functional-typological in-
troduction, II. John Benjamins.
Jose? M. Go?mez. 2007. Recuperacio?n de Pasajes Multil-
ingu?e para la Bu?squeda de Respuestas. Ph.D. thesis.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. In Proceedings of ACL Text
Summarization Workshop, pages 74?81.
Elena Lloret and Manuel Palomar. 2009. A gradual com-
bination of features for building automatic summarisa-
tion systems. In Proceedings of TSD, pages 16?23.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2):4.
Lizhen Qu, Cigdem Toprak, Niklas jakob, and iryna
Gurevych. 2008. Sentence level subjectivity and sen-
timent analysis experiments in ntcir-7 moat challenge.
In Proceedings of NTCIR-7 Workshop meeting.
Karen Spa?rck Jones. 2007. Automatic summarising: The
State of the Art. Information Processing & Manage-
ment, 43(6):1449?1481.
V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe. 2004.
Evaluating an opinion annotation scheme using a new
multi-perspective question and answer corpus. In
AAAI Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1083?1086.
John Carroll Taras Zabibalov. 2008. Almost-
unsupervised cross-language opinion analysis at ntcis-
7. In Proceedings of NTCIR-7 Workshop meeting.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Language Resources and Evaluation, volume 39.
D. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
174
