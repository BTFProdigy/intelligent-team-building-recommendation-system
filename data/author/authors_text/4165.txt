Automatic Learning of Language Model Structure
Kevin Duh and Katrin Kirchhoff
Department of Electrical Engineering
University of Washington, Seattle, USA
{duh,katrin}@ee.washington.edu
Abstract
Statistical language modeling remains a challeng-
ing task, in particular for morphologically rich lan-
guages. Recently, new approaches based on factored
language models have been developed to address
this problem. These models provide principled ways
of including additional conditioning variables other
than the preceding words, such as morphological or
syntactic features. However, the number of possible
choices for model parameters creates a large space of
models that cannot be searched exhaustively. This
paper presents an entirely data-driven model selec-
tion procedure based on genetic search, which is
shown to outperform both knowledge-based and ran-
dom selection procedures on two different language
modeling tasks (Arabic and Turkish).
1 Introduction
In spite of novel algorithmic developments and
the increased availability of large text corpora,
statistical language modeling remains a diffi-
cult problem, particularly for languages with
rich morphology. Such languages typically ex-
hibit a large number of word types in relation
to word tokens in a given text, which leads
to high perplexity and a large number of un-
seen word contexts. As a result, probability es-
timates are often unreliable, even when using
standard smoothing and parameter reduction
techniques. Recently, a new language model-
ing approach, called factored language models
(FLMs), has been developed. FLMs are a gen-
eralization of standard language models in that
they allow a larger set of conditioning variables
for predicting the current word. In addition to
the preceding words, any number of additional
variables can be included (e.g. morphological,
syntactic, or semantic word features). Since
such features are typically shared across mul-
tiple words, they can be used to obtained bet-
ter smoothed probability estimates when train-
ing data is sparse. However, the space of pos-
sible models is extremely large, due to many
different ways of choosing subsets of condition-
ing word features, backoff procedures, and dis-
counting methods. Usually, this space cannot
be searched exhaustively, and optimizing mod-
els by a knowledge-inspired manual search pro-
cedure often leads to suboptimal results since
only a small portion of the search space can
be explored. In this paper we investigate the
possibility of determining the structure of fac-
tored language models (i.e. the set of condition-
ing variables, the backoff procedure and the dis-
counting parameters) by a data-driven search
procedure, viz. Genetic Algorithms (GAs). We
apply this technique to two different tasks (lan-
guage modeling for Arabic and Turkish) and
show that GAs lead to better models than ei-
ther knowledge-inspired manual search or ran-
dom search. The remainder of this paper is
structured as follows: Section 2 describes the
details of the factored language modeling ap-
proach. The application of GAs to the problem
of determining language model structure is ex-
plained in Section 3. The corpora used in the
present study are described in Section 4 and ex-
periments and results are presented in Section 5.
Section 6 compares the present study to related
work and Section 7 concludes.
2 Factored Language Models
A standard statistical language model com-
putes the probability of a word sequence W =
w1, w2, ..., wT as a product of conditional prob-
abilities of each word wi given its history, which
is typically approximated by just one or two pre-
ceding words (leading to bigrams, and trigrams,
respectively). Thus, a trigram language model
is described by
p(w1, ..., wT ) ?
T
?
i=3
p(wi|wi?1, wi?2) (1)
Even with this limitation, the estimation of
the required probabilities is challenging: many
word contexts may be observed infrequently
or not at all, leading to unreliable probabil-
ity estimates under maximum likelihood estima-
tion. Several techniques have been developed
to address this problem, in particular smooth-
ing techniques (Chen and Goodman, 1998) and
class-based language models (Brown and oth-
ers, 1992). In spite of such parameter reduc-
tion techniques, language modeling remains a
difficult task, in particular for morphologically
rich languages, e.g. Turkish, Russian, or Arabic.
Such languages have a large number of word
types in relation to the number of word tokens
in a given text, as has been demonstrated in
a number of previous studies (Geutner, 1995;
Kiecza et al, 1999; Hakkani-Tu?r et al, 2002;
Kirchhoff et al, 2003). This in turn results
in a high perplexity and in a large number of
out-of-vocabulary (OOV) words when applying
a trained language model to a new unseen text.
2.1 Factored Word Representations
A recently developed approach that addresses
this problem is that of Factored Language Mod-
els (FLMs) (Kirchhoff et al, 2002; Bilmes and
Kirchhoff, 2003), whose basic idea is to decom-
pose words into sets of features (or factors) in-
stead of viewing them as unanalyzable wholes.
Probabilistic language models can then be con-
structed over (sub)sets of word features instead
of, or in addition to, the word variables them-
selves. For instance, words can be decomposed
into stems/lexemes and POS tags indicating
their morphological features, as shown below:
Word: Stock prices are rising
Stem: Stock price be rise
Tag: Nsg N3pl V3pl Vpart
Such a representation serves to express lexical
and syntactic generalizations, which would oth-
erwise remain obscured. It is comparable to
class-based representations employed in stan-
dard class-based language models; however, in
FLMs several simultaneous class assignments
are allowed instead of a single one. In general,
we assume that a word is equivalent to a fixed
number (K) of factors, i.e. W ? f1:K . The task
then is to produce a statistical model over the
resulting representation - using a trigram ap-
proximation, the resulting probability model is
as follows:
p(f1:K1 , f1:K2 , ..., f1:KT ) ?
T
?
t=3
p(f1:Kt |f1:Kt?1 , f1:Kt?2 )
(2)
Thus, each word is dependent not only on a sin-
gle stream of temporally ordered word variables,
but also on additional parallel (i.e. simultane-
ously occurring) features. This factored repre-
sentation can be used in two different ways to
improve over standard LMs: by using a product
model or a backoff model. In a product model,
Equation 2 can be simplified by finding con-
ditional independence assumptions among sub-
sets of conditioning factors and computing the
desired probability as a product of individual
models over those subsets. In this paper we
only consider the second option, viz. using the
factors in a backoff procedure when the word
n-gram is not observed in the training data.
For instance, a word trigram that is found in
an unseen test set may not have any counts in
the training set, but its corresponding factors
(e.g. stems and morphological tags) may have
been observed since they also occur in other
words.
2.2 Generalized parallel backoff
Backoff is a common smoothing technique in
language modeling. It is applied whenever
the count for a given n-gram in the training
data falls below a certain threshold ? . In that
case, the maximum-likelihood estimate of the
n-gram probability is replaced with a probabil-
ity derived from the probability of the lower-
order (n ? 1)-gram and a backoff weight. N-
grams whose counts are above the threshold re-
tain their maximum-likelihood estimates, dis-
counted by a factor that re-distributes proba-
bility mass to the lower-order distribution:
pBO(wt|wt?1, wt?2) (3)
=
{
dcpML(wt|wt?1, wt?2) if c > ?3
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where c is the count of (wt, wt?1, wt?2), pML
denotes the maximum-likelihood estimate and
dc is a discounting factor that is applied to the
higher-order distribution. The way in which the
discounting factor is estimated determines the
actual smoothing method (e.g. Good-Turing,
Kneser-Ney, etc.) The normalization factor
?(wt?1, wt?2) ensures that the entire distribu-
tion sums to one. During standard backoff, the
most distant conditioning variable (in this case
wt?2) is dropped first, then the second most dis-
tant variable etc. until the unigram is reached.
This can be visualized as a backoff path (Fig-
ure 1(a)). If the only variables in the model are
words, such a backoff procedure is reasonable.
tW 1tW? 2tW? 3tW?
tW 1tW? 2tW?
tW 1tW?
tW
(a)
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
(b)
Figure 1: Standard backoff path for a 4-gram lan-
guage model over words (left) and backoff graph for
4-gram over factors (right).
However, if variables occur in parallel, i.e. do
not form a temporal sequence, it is not imme-
diately obvious in which order they should be
dropped. In this case, several backoff paths are
possible, which can be summarized in a backoff
graph (Figure 1(b)). In principle, there are sev-
eral different ways of choosing among different
paths in this graph:
1. Choose a fixed, predetermined backoff path
based on linguistic knowledge, e.g. always drop
syntactic before morphological variables.
2. Choose the path at run-time based on statis-
tical criteria.
3. Choose multiple paths and combine their
probability estimates.
The last option, referred to as parallel backoff,
is implemented via a new, generalized backoff
function (here shown for a 4-gram):
pGBO(f |f1, f2, f3) (4)
=
{
dcpML(f |f1, f2, f3) if c > ?4
?(f1, f2, f3)g(f, f1, f2, f3) otherwise
where c is the count of (f, f1, f2, f3),
pML(f |f1, f2, f3) is the maximum likeli-
hood distribution, ?4 is the count threshold,
and ?(f1, f2, f3) is the normalization factor.
The function g(f, f1, f2, f3) determines the
backoff strategy. In a typical backoff proce-
dure g(f, f1, f2, f3) equals pBO(f |f1, f2). In
generalized parallel backoff, however, g can be
any non-negative function of f, f1, f2, f3. In
our implementation of FLMs (Kirchhoff et al,
2003) we consider several different g functions,
including the mean, weighted mean, product,
and maximum of the smoothed probability
distributions over all subsets of the conditioning
factors. In addition to different choices for g,
different discounting parameters can be chosen
at different levels in the backoff graph. For
instance, at the topmost node, Kneser-Ney
discounting might be chosen whereas at a
lower node Good-Turing might be applied.
FLMs have been implemented as an add-on
to the widely-used SRILM toolkit1 and have
been used successfully for the purpose of
morpheme-based language modeling (Bilmes
and Kirchhoff, 2003), multi-speaker language
modeling (Ji and Bilmes, 2004), and speech
recognition (Kirchhoff et al, 2003).
3 Learning FLM Structure
In order to use an FLM, three types of para-
meters need to be specified: the initial con-
ditioning factors, the backoff graph, and the
smoothing options. The goal of structure learn-
ing is to find the parameter combinations that
create FLMs that achieve a low perplexity on
unseen test data. The resulting model space
is extremely large: given a factored word rep-
resentation with a total of k factors, there are
?k
n=1
(k
n
)
possible subsets of initial condition-
ing factors. For a set of m conditioning factors,
there are up to m! backoff paths, each with its
own smoothing options. Unless m is very small,
exhaustive search is infeasible. Moreover, non-
linear interactions between parameters make it
difficult to guide the search into a particular
direction, and parameter sets that work well
for one corpus cannot necessarily be expected
to perform well on another. We therefore need
an automatic way of identifying the best model
structure. In the following section, we describe
the application of genetic-based search to this
problem.
3.1 Genetic Algorithms
Genetic Algorithms (GAs) (Holland, 1975) are a
class of evolution-inspired search/optimization
techniques. They perform particularly well
in problems with complex, poorly understood
search spaces. The fundamental idea of GAs is
to encode problem solutions as (usually binary)
strings (genes), and to evolve and test successive
populations of solutions through the use of ge-
netic operators applied to the encoded strings.
Solutions are evaluated according to a fitness
function which represents the desired optimiza-
tion criterion. The individual steps are as fol-
1We would like to thank Jeff Bilmes for providing and
supporting the software.
lows:
Initialize: Randomly generate a set (popula-
tion) of strings.
While fitness improves by a certain threshold:
Evaluate fitness: calculate each string?s fitness
Apply operators: apply the genetic operators
to create a new population.
The genetic operators include the probabilis-
tic selection of strings for the next genera-
tion, crossover (exchanging subparts of differ-
ent strings to create new strings), and muta-
tion (randomly altering individual elements in
strings). Although GAs provide no guarantee
of finding the optimal solution, they often find
good solutions quickly. By maintaining a pop-
ulation of solutions rather than a single solu-
tion, GA search is robust against premature
convergence to local optima. Furthermore, solu-
tions are optimized based on a task-specific fit-
ness function, and the probabilistic nature of ge-
netic operators helps direct the search towards
promising regions of the search space.
3.2 Structure Search Using GA
In order to use GAs for searching over FLM
structures (i.e. combinations of conditioning
variables, backoff paths, and discounting op-
tions), we need to find an appropriate encoding
of the problem.
Conditioning factors
The initial set of conditioning factors F are
encoded as binary strings. For instance, a
trigram for a word representation with three
factors (A,B,C) has six conditioning variables:
{A?1, B?1, C?1, A?2, B?2, C?2} which can be
represented as a 6-bit binary string, with a bit
set to 1 indicating presence and 0 indicating ab-
sence of a factor in F . The string 10011 would
correspond to F = {A?1, B?2, C?2}.
Backoff graph
The encoding of the backoff graph is more dif-
ficult because of the large number of possible
paths. A direct approach encoding every edge
as a bit would result in overly long strings, ren-
dering the search inefficient. Our solution is to
encode a binary string in terms of graph gram-
mar rules (similar to (Kitano, 1990)), which
can be used to describe common regularities in
backoff graphs. For instance, a node with m
factors can only back off to children nodes with
m ? 1 factors. For m = 3, the choices for pro-
ceeding to the next-lower level in the backoff
1. {X1 X2 X3} ?> {X1 X2}
2. {X1 X2 X3} ?> {X1 X3}
3. {X1 X2 X3} ?> {X2 X3}
4.      {X1 X2}  ?> {X1}
5.      {X1 X2}  ?> {X2}
PRODUCTION RULES:
AB
ABC
AB
ABC
BC AB
ABC
BC
A B
AB
ABC
BC
A B
 
 







0
1
4 4
10110
3
(b) Generation of Backoff Graph by rules 1, 3, and 4
(a) Gene activates production rules
GENE:
Figure 2: Generation of Backoff Graph from pro-
duction rules selected by the gene 10110.
graph can thus be described by the following
grammar rules:
RULE 1: {x1, x2, x3} ? {x1, x2}
RULE 2: {x1, x2, x3} ? {x1, x3}
RULE 3: {x1, x2, x3} ? {x2, x3}
Here xi corresponds to the factor at the ith
position in the parent node. Rule 1 indicates
a backoff that drops the third factor, Rule 2
drops the second factor, etc. The choice of
rules used to generate the backoff graph is en-
coded in a binary string, with 1 indicating the
use and 0 indicating the non-use of a rule, as
shown schematically in Figure 2. The presence
of two different rules at the same level in the
backoff graph corresponds to parallel backoff;
the absence of any rule (strings consisting only
of 0 bits) implies that the corresponding backoff
graph level is skipped and two conditioning vari-
ables are dropped simultaneously. This allows
us to encode a graph using few bits but does not
represent all possible graphs. We cannot selec-
tively apply different rules to different nodes at
the same level ? this would essentially require
a context-sensitive grammar, which would in
turn increase the length of the encoded strings.
This is a fundamental tradeoff between the most
general representation and an encoding that is
tractable. Our experimental results described
below confirm, however, that sufficiently good
results can be obtained in spite of the above
limitation.
Smoothing options
Smoothing options are encoded as tuples of in-
tegers. The first integer specifies the discount-
ing method while second indicates the minimum
count required for the n-gram to be included in
the FLM. The integer string consists of succes-
sive concatenated tuples, each representing the
smoothing option at a node in the graph. The
GA operators are applied to concatenations of
all three substrings describing the set of factors,
backoff graph, and smoothing options, such that
all parameters are optimized jointly.
4 Data
We tested our language modeling algorithms on
two different data sets from two different lan-
guages, Arabic and Turkish.
The Arabic data set was drawn from the
CallHome Egyptian Conversational Arabic
(ECA) corpus (LDC, 1996). The training,
development, and evaluation sets contain
approximately 170K, 32K, and 18K words,
respectively. The corpus was collected for the
purpose of speech recognizer development for
conversational Arabic, which is mostly dialectal
and does not have a written standard. No
additional text material beyond transcriptions
is available in this case; it is therefore im-
portant to use language models that perform
well in sparse data conditions. The factored
representation was constructed using linguistic
information from the corpus lexicon, in combi-
nation with automatic morphological analysis
tools. It includes, in addition to the word, the
stem, a morphological tag, the root, and the
pattern. The latter two are components which
when combined form the stem. An example
of this factored word representation is shown
below:
Word:il+dOr/Morph:noun+masc-sg+article/
Stem:dOr/Root:dwr/Pattern:CCC
For our Turkish experiments we used a mor-
phologically annotated corpus of Turkish
(Hakkani-Tu?r et al, 2000). The annotation
was performed by applying a morphological
analyzer, followed by automatic morphological
disambiguation as described in (Hakkani-Tu?r
et al, 2002). The morphological tags consist
of the initial root, followed by a sequence of
inflectional groups delimited by derivation
boundaries (?DB). A sample annotation (for
the word yararlanmak, consisting of the root
yarar plus three inflectional groups) is shown
below:
yararmanlak:
yarar+Noun+A3sg+Pnon+Nom
?DB+Verb+Acquire+Pos
?DB+Noun+Inf+A3sg+Pnon+Nom
We removed segmentation marks (for titles
and paragraph boundaries) from the corpus
but included punctuation. Words may have
different numbers of inflectional groups, but
the FLM representation requires the same
number of factors for each word; we therefore
had to map the original morphological tags to
a fixed-length factored representation. This
was done using linguistic knowledge: according
to (Oflazer, 1999), the final inflectional group
in each dependent word has a special status
since it determines inflectional markings on
head words following the dependent word.
The final inflectional group was therefore
analyzed into separate factors indicating the
number (N), case (C), part-of-speech (P) and
all other information (O). Additional factors
for the word are the root (R) and all remaining
information in the original tag not subsumed
by the other factors (G). The word itself is
used as another factor (W). Thus, the above
example would be factorized as follows:
W:yararlanmak/R:yarar/P:NounInf-N:A3sg/
C:Nom/O:Pnon/G:NounA3sgPnonNom+Verb
+Acquire+Pos
Other factorizations are certainly possible;
however, our primary goal is not to find the
best possible encoding for our data but to
demonstrate the effectiveness of the FLM
approach, which is largely independent of the
choice of factors. For our experiments we used
subsets of 400K words for training, 102K words
for development and 90K words for evaluation.
5 Experiments and Results
In our application of GAs to language model
structure search, the perplexity of models with
respect to the development data was used as
an optimization criterion. The perplexity of
the best models found by the GA were com-
pared to the best models identified by a lengthy
manual search procedure using linguistic knowl-
edge about dependencies between the word fac-
tors involved, and to a random search procedure
which evaluated the same number of strings as
the GA. The following GA options gave good
results: population size 30-50, crossover proba-
bility 0.9, mutation probability 0.01, Stochastic
Universal Sampling as the selection operator, 2-
point crossover. We also experimented with re-
initializing the GA search with the best model
found in previous runs. This method consis-
tently improved the performance of normal GA
search and we used it as the basis for the results
reported below. Due to the large number of fac-
N Word Hand Rand GA ? (%)
Dev Set
2 593.8 555.0 556.4 539.2 -2.9
3 534.9 533.5 497.1 444.5 -10.6
4 534.8 549.7 566.5 522.2 -5.0
Eval Set
2 609.8 558.7 525.5 487.8 -7.2
3 545.4 583.5 509.8 452.7 -11.2
4 543.9 559.8 574.6 527.6 -5.8
Table 1: Perplexity for Turkish language models. N
= n-gram order, Word = word-based models, Hand
= manual search, Rand = random search, GA =
genetic search.
tors in the Turkish word representation, models
were only optimized for conditioning variables
and backoff paths, but not for smoothing op-
tions. Table 1 compares the best perplexity re-
sults for standard word-based models and for
FLMs obtained using manual search (Hand),
random search (Rand), and GA search (GA).
The last column shows the relative change in
perplexity for the GA compared to the better
of the manual or random search models. For
tests on both the development set and evalu-
ation set, GA search gave the lowest perplex-
ity. In the case of Arabic, the GA search was
N Word Hand Rand GA ? (%)
Dev Set
2 229.9 229.6 229.9 222.9 -2.9
3 229.3 226.1 230.3 212.6 -6.0
Eval Set
2 249.9 230.1 239.2 223.6 -2.8
3 285.4 217.1 224.3 206.2 -5.0
Table 2: Perplexity for Arabic language models
(w/o unknown words).
performed over conditioning factors, the back-
off graph, and smoothing options. The results
in Table 2 were obtained by training and test-
ing without consideration of out-of-vocabulary
(OOV) words. Our ultimate goal is to use these
language models in a speech recognizer with a
fixed vocabulary, which cannot recognize OOV
words but requires a low perplexity for other
N Word Hand Rand GA ? (%)
Dev Set
2 236.0 195.5 198.5 193.3 -1.1
3 237.0 199.0 202.0 188.1 -5.5
Eval Set
2 235.2 234.1 247.7 233.4 -0.7
3 253.9 229.2 219.0 212.2 -3.1
Table 3: Perplexity for Arabic language models
(with unknown words).
word combinations. In a second experiment,
we trained the same FLMs from Table 2 with
OOV words included as the unknown word to-
ken. Table 3 shows the results. Again, we see
that the GA outperforms other search methods.
The best language models all used parallel back-
off and different smoothing options at different
backoff graph nodes. The Arabic models made
use of all conditioning variables (Word, Stem,
Root, Pattern, and Morph) whereas the Turkish
models used only the W, P, C, and R variables
(see above Section 4).
6 Related Work
Various previous studies have investigated the
feasibility of using units other than words for
language modeling (e.g. (Geutner, 1995; C?arki
et al, 2000; Kiecza et al, 1999)). However,
in all of these studies words were decomposed
into linear sequences of morphs or morph-like
units, using either linguistic knowledge or data-
driven techniques. Standard language models
were then trained on the decomposed represen-
tations. The resulting models essentially ex-
press statistical relationships between morphs,
such as stems and affixes. For this reason, a
context larger than that provided by a trigram
is typically required, which quickly leads to
data-sparsity. In contrast to these approaches,
factored language models encode morphological
knowledge not by altering the linear segmenta-
tion of words but by encoding words as parallel
bundles of features.
The general possibility of using multiple con-
ditioning variables (including variables other
than words) has also been investigated by
(Dupont and Rosenfeld, 1997; Gildea, 2001;
Wang, 2003; Zitouni et al, 2003). Mostly, the
additional variables were general word classes
derived by data-driven clustering procedures,
which were then arranged in a backoff lattice
or graph similar to the present procedure. All
of these studies assume a fixed path through
the graph, which is usually obtained by an
ordering from more specific probability distri-
butions to more general distributions. Some
schemes also allow two or more paths to be
combined by weighted interpolation. FLMs, by
contrast, allow different paths to be chosen at
run-time, they support a wider range of combi-
nation methods for probability estimates from
different paths, and they offer a choice of dif-
ferent discounting options at every node in the
backoff graph. Most importantly, however, the
present study is to our knowledge the first to
describe an entirely data-driven procedure for
identifying the best combination of parameter
choices. The success of this method will facili-
tate the rapid development of FLMs for differ-
ent tasks in the future.
7 Conclusions
We have presented a data-driven approach to
the selection of parameters determining the
structure and performance of factored language
models, a class of models which generalizes
standard language models by including addi-
tional conditioning variables in a principled
way. In addition to reductions in perplexity ob-
tained by FLMs vs. standard language models,
the data-driven model section method further
improved perplexity and outperformed both
knowledge-based manual search and random
search.
Acknowledgments
We would like to thank Sonia Parandekar for the ini-
tial version of the GA code. This material is based
upon work supported by the NSF and the CIA un-
der NSF Grant No. IIS-0326276. Any opinions, find-
ings, and conclusions expressed in this material are
those of the authors and do not necessarily reflect
the views of these agencies.
References
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings of HLT/NACCL, pages 4?6.
P.F. Brown et al 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
K. C?arki, P. Geutner, and T. Schultz. 2000. Turkish
LVCSR: towards better speech recognition for ag-
glutinative languages. In Proceedings of ICASSP.
S. F. Chen and J. Goodman. 1998. An empirical
study of smoothing techniques for language mod-
eling. Technical Report Tr-10-98, Center for Re-
search in Computing Technology, Harvard Uni-
versity.
P. Dupont and R. Rosenfeld. 1997. Lattice based
language models. Technical Report CMU-CS-97-
173, Department of Computer Science, CMU.
P. Geutner. 1995. Using morphology towards better
large-vocabulary speech recognition systems. In
Proceedings of ICASSP, pages 445?448.
D. Gildea. 2001. Statistical Language Understand-
ing Using Frame Semantics. Ph.D. thesis, Uni-
versity of California, Berkeley.
D. Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r. 2000.
Statistical morphological disambiguation for ag-
glutinative languages. In Proceedings of COL-
ING.
D. Hakkani-Tu?r, K. Oflazer, and Go?khan Tu?r. 2002.
Statistical morphological disambiguation for ag-
glutinative languages. Journal of Computers and
Humanities, 36(4).
J.H. Holland. 1975. Adaptation in Natural and Ar-
tificial Systems. University of Michigan Press.
Gand Ji and Jeff Bilmes. 2004. Multi-speaker lan-
guage modeling. In Proceedings of HLT/NAACL,
pages 137?140.
D. Kiecza, T. Schultz, and A. Waibel. 1999. Data-
driven determination of appropriate dictionary
units for Korean LVCSR. In Proceedings of
ICASSP, pages 323?327.
K. Kirchhoff et al 2002. Novel speech recognition
models for Arabic. Technical report, Johns Hop-
kins University.
K. Kirchhoff et al 2003. Novel approaches to Ara-
bic speech recognition: Report from 2002 Johns-
Hopkins summer workshop. In Proceedings of
ICASSP, pages I?344?I?347.
Hiroaki Kitano. 1990. Designing neural networks
using genetic algorithms with graph generation
system. Complex Systems, pages 461?476.
LDC. 1996. http://www.ldc.upenn.edu/Catalog/-
LDC99L22.html.
K. Oflazer. 1999. Dependency parsing with an ex-
tended finite state approach. In Proceedings of the
37th ACL.
W. Wang. 2003. Factorization of language
models through backing off lattices. Com-
putation and Language E-print Archive,
oai:arXiv.org/cs/0305041.
I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierar-
chical class n-gram language models: towards bet-
ter estimation of unseen events in speech recogni-
tion. In Proceedings of Eurospeech - Interspeech,
pages 237?240.
Phrase-Based Backoff Models for Machine Translation of Highly Inflected
Languages
Mei Yang
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
yangmei@ee.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
We propose a backoff model for phrase-
based machine translation that translates
unseen word forms in foreign-language
text by hierarchical morphological ab-
stractions at the word and the phrase level.
The model is evaluated on the Europarl
corpus for German-English and Finnish-
English translation and shows improve-
ments over state-of-the-art phrase-based
models.
1 Introduction
Current statistical machine translation (SMT) usu-
ally works well in cases where the domain is
fixed, the training and test data match, and a large
amount of training data is available. Nevertheless,
standard SMT models tend to perform much bet-
ter on languages that are morphologically simple,
whereas highly inflected languages with a large
number of potential word forms are more prob-
lematic, particularly when training data is sparse.
SMT attempts to find a sentence e? in the desired
output language given the corresponding sentence
f in the source language, according to
e? = argmaxeP (f |e)P (e) (1)
Most state-of-the-art SMT adopt a phrase-based
approach such that e is chunked into I phrases
e?1, ..., e?I and the translation model is defined
over mappings between phrases in e and in f .
i.e. P (f? |e?). Typically, phrases are extracted from
a word-aligned training corpus. Different inflected
forms of the same lemma are treated as different
words, and there is no provision for unseen forms,
i.e. unknown words encountered in the test data
are not translated at all but appear verbatim in the
output. Although the percentage of such unseen
word forms may be negligible when the training
set is large and matches the test set well, it may rise
drastically when training data is limited or from
a different domain. Many current and future ap-
plications of machine translation require the rapid
porting of existing systems to new languages and
domains without being able to collect appropri-
ate training data; this problem can therefore be
expected to become increasingly more important.
Furthermore, untranslated words can be one of the
main factors contributing to low user satisfaction
in practical applications.
Several previous studies (see Section 2 below)
have addressed issues of morphology in SMT, but
most of these have focused on the problem of word
alignment and vocabulary size reduction. Princi-
pled ways of incorporating different levels of mor-
phological abstraction into phrase-based models
have mostly been ignored so far. In this paper we
propose a hierarchical backoff model for phrase-
based translation that integrates several layers of
morphological operations, such that more specific
models are preferred over more general models.
We experimentally evaluate the model on transla-
tion from two highly-inflected languages, German
and Finnish, into English and present improve-
ments over a state-of-the-art system. The rest of
the paper is structured as follows: The following
section discusses related background work. Sec-
tion 4 describes the proposed model; Sections 5
and 6 provide details about the data and baseline
system used in this study. Section 7 provides ex-
perimental results and discussion. Section 8 con-
cludes.
41
2 Morphology in SMT Systems
Previous approaches have used morpho-syntactic
knowledge mainly at the low-level stages of a ma-
chine translation system, i.e. for preprocessing.
(Niessen and Ney, 2001a) use morpho-syntactic
knowledge for reordering certain syntactic con-
structions that differ in word order in the source
vs. target language (German and English). Re-
ordering is applied before training and after gener-
ating the output in the target language. Normaliza-
tion of English/German inflectional morphology
to base forms for the purpose of word alignment is
performed in (Corston-Oliver and Gamon, 2004)
and (Koehn, 2005), demonstrating that the vocab-
ulary size can be reduced significantly without af-
fecting performance.
Similar morphological simplifications have
been applied to other languages such as Roma-
nian (Fraser and Marcu, 2005) in order to de-
crease word alignment error rate. In (Niessen
and Ney, 2001b), a hierarchical lexicon model is
used that represents words as combinations of full
forms, base forms, and part-of-speech tags, and
that allows the word alignment training procedure
to interpolate counts based on the different lev-
els of representation. (Goldwater and McCloskey,
2005) investigate various morphological modifi-
cations for Czech-English translations: a subset
of the vocabulary was converted to stems, pseu-
dowords consisting of morphological tags were in-
troduced, and combinations of stems and morpho-
logical tags were used as new word forms. Small
improvements were found in combination with a
word-to-word translation model. Most of these
techniques have focused on improving word align-
ment or reducing vocabulary size; however, it is
often the case that better word alignment does not
improve the overall translation performance of a
standard phrase-based SMT system.
Phrase-based models themselves have not ben-
efited much from additional morpho-syntactic
knowledge; e.g. (Lioma and Ounis, 2005) do not
report any improvement from integrating part-of-
speech information at the phrase level. One suc-
cessful application of morphological knowledge is
(de Gispert et al, 2005), where knowledge-based
morphological techniques are used to identify un-
seen verb forms in the test text and to generate
inflected forms in the target language based on
annotated POS tags and lemmas. Phrase predic-
tion in the target language is conditioned on the
phrase in the source language as well the corre-
sponding tuple of lemmatized phrases. This tech-
nique worked well for translating from a morpho-
logically poor language (English) to a more highly
inflected language (Spanish) when applied to un-
seen verb forms. Treating both known and un-
known verbs in this way, however, did not result
in additional improvements. Here we extend the
notion of treating known and unknown words dif-
ferently and propose a backoff model for phrase-
based translation.
3 Backoff Models
Generally speaking, backoff models exploit rela-
tionships between more general and more spe-
cific probability distributions. They specify under
which conditions the more specific model is used
and when the model ?backs off? to the more gen-
eral distribution. Backoff models have been used
in a variety of ways in natural language process-
ing, most notably in statistical language modeling.
In language modeling, a higher-order n-gram dis-
tribution is used when it is deemed reliable (deter-
mined by the number of occurrences in the train-
ing data); otherwise, the model backs off to the
next lower-order n-gram distribution. For the case
of trigrams, this can be expressed as:
pBO(wt|wt?1, wt?2) (2)
=
{
dcpML(wt|wt?1, wt?2) if c > ?
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where pML denotes the maximum-likelihood
estimate, c denotes the count of the triple
(wi, wi?1, wi?2) in the training data, ? is the count
threshold above which the maximum-likelihood
estimate is retained, and dN(wi,wi?1,wi?2) is a dis-
counting factor (generally between 0 and 1) that is
applied to the higher-order distribution. The nor-
malization factor ?(wi?1, wi?2) ensures that the
distribution sums to one. In (Bilmes and Kirch-
hoff, 2003) this method was generalized to a back-
off model with multiple paths, allowing the com-
bination of different backed-off probability esti-
mates. Hierarchical backoff schemes have also
been used by (Zitouni et al, 2003) for language
modeling and by (Gildea, 2001) for semantic role
labeling. (Resnik et al, 2001) used backoff trans-
lation lexicons for cross-language information re-
trieval. More recently, (Xi and Hwa, 2005) have
used backoff models for combining in-domain and
42
out-of-domain data for the purpose of bootstrap-
ping a part-of-speech tagger for Chinese, outper-
forming standard methods such as EM.
4 Backoff Models in MT
In order to handle unseen words in the test data
we propose a hierarchical backoff model that uses
morphological information. Several morphologi-
cal operations, in particular stemming and com-
pound splitting, are interleaved such that a more
specific form (i.e. a form closer to the full word
form) is chosen before a more general form (i.e. a
form that has undergone morphological process-
ing). The procedure is shown in Figure 1 and can
be described as follows: First, a standard phrase
table based on full word forms is trained. If an
unknown word fi is encountered in the test data
with context cfi = fi?n, ..., fi?1, fi+1, ..., fi+m,
the word is first stemmed, i.e. f ?i = stem(fi).
The phrase table entries for words sharing the
same stem are then modified by replacing the
respective words with their stems. If an en-
try can be found among these such that the
source language side of the phrase pair consists of
fi?n, ..., fi?1, stem(fi), fi+1, ..., fi+m, the corre-
sponding translation is used (or, if several pos-
sible translations occur, the one with the high-
est probability is chosen). Note that the con-
text may be empty, in which case a single-word
phrase is used. If this step fails, the model backs
off to the next level and applies compound split-
ting to the unknown word (further described be-
low), i.e.(f ??i1, f ??i2) = split(fi). The match with
the original word-based phrase table is then per-
formed again. If this step fails for either of the
two parts of f ??, stemming is applied again: f ???i1 =
stem(f ??i1) and f ???i2 = stem(f ??i2), and a match with
the stemmed phrase table entries is carried out.
Only if the attempted match fails at this level is the
input passed on verbatim in the translation output.
The backoff procedure could in principle be
performed on demand by a specialized decoder;
however, since we use an off-the-shelf decoder
(Pharaoh (Koehn, 2004)), backoff is implicitly en-
forced by providing a phrase-table that includes
all required backoff levels and by preprocessing
the test data accordingly. The phrase table will
thus include entries for phrases based on full word
forms as well as for their stemmed and/or split
counterparts.
For each entry with decomposed morphological
i
i i
i1 i2 i
i1 i1
i2 i2
i1 i2
Figure 1: Backoff procedure.
forms, four probabilities need to be provided: two
phrasal translation scores for both translation di-
rections, p(e?|f?) and p(f? |e?), and two correspond-
ing lexical scores, which are computed as a prod-
uct of the word-by-word translation probabilities
under the given alignment a:
plex(e?|f?) =
J
?
j=1
1
|j|a(i) = j|
I
?
a(i)=j
p(fj |ei) (3)
where j ranges of words in phrase f? and i ranges
of words in phrase e?. In the case of unknown
words in the foreign language, we need the prob-
abilities p(e?|stem(f?)), p(stem(f?)|e?) (where the
stemming operation stem(f?) applies to the un-
known words in the phrase), and their lexical
equivalents. These are computed by relative fre-
quency estimation, e.g.
p(e?|stem(f?)) = count(e?, stem(f?))count(stem(f?)) (4)
The other translation probabilities are computed
analogously. Since normalization is performed
over the entire phrase table, this procedure has
the effect of discounting the original probability
porig(e?|f?) since e? may now have been generated
by either f? or by stem(f?). In the standard formu-
lation of backoff models shown in Equation 3, this
amounts to:
pBO(e?|f?) (5)
=
{
de?,f?porig(e?|f?) if c(e?, f?) > 0
p(e?|stem(f?)) otherwise
43
where
de?,f? =
1 ? p(e?, stem(f?))
p(e?, f?) (6)
is the amount by which the word-based phrase
translation probability is discounted. Equiva-
lent probability computations are carried out for
the lexical translation probabilities. Similar to
the backoff level that uses stemming, the trans-
lation probabilities need to be recomputed for
the levels that use splitting and combined split-
ting/stemming.
In order to derive the morphological decompo-
sition we use existing tools. For stemming we
use the TreeTagger (Schmid, 1994) for German
and the Snowball stemmer1 for Finnish. A vari-
ety of ways for compound splitting have been in-
vestigated in machine translation (Koehn, 2003).
Here we use a simple technique that considers all
possible ways of segmenting a word into two sub-
parts (with a minimum-length constraint of three
characters on each subpart). A segmentation is ac-
cepted if the subparts appear as individual items
in the training data vocabulary. The only linguis-
tic knowledge used in the segmentation process is
the removal of final <s> from the first part of the
compound before trying to match it to an existing
word. This character (Fugen-s) is often inserted as
?glue? when forming German compounds. Other
glue characters were not considered for simplic-
ity (but could be added in the future). The seg-
mentation method is clearly not linguistically ad-
equate: first, words may be split into more than
two parts. Second, the method may generate mul-
tiple possible segmentations without a principled
way of choosing among them; third, it may gener-
ate invalid splits. However, a manual analysis of
300 unknown compounds in the German develop-
ment set (see next section) showed that 95.3% of
them were decomposed correctly: for the domain
at hand, most compounds need not be split into
more than two parts; if one part is itself a com-
pound it is usually frequent enough in the train-
ing data to have a translation. Furthermore, lexi-
calized compounds, whose decomposition would
lead to wrong translations, are also typically fre-
quent words and have an appropriate translation in
the training data.
1http://snowball.tartarus.org
5 Data
Our data consists of the Europarl training, devel-
opment and test definitions for German-English
and Finnish-English of the 2005 ACL shared data
task (Koehn and Monz, 2005). Both German
and Finnish are morphologically rich languages:
German has four cases and three genders and
shows number, gender and case distinctions not
only on verbs, nouns, and adjectives, but also
on determiners. In addition, it has notoriously
many compounds. Finnish is a highly agglutina-
tive language with a large number of inflectional
paradigms (e.g. one for each of its 15 cases). Noun
compounds are also frequent. On the 2005 ACL
shared MT data task, Finnish to English trans-
lation showed the lowest average performance
(17.9% BLEU) and German had the second low-
est (21.9%), while the average BLEU scores for
French-to-English and Spanish-to-English were
much higher (27.1% and 27.8%, respectively).
The data was preprocessed by lowercasing and
filtering out sentence pairs whose length ratio
(number of words in the source language divided
by the number of words in the target language,
or vice versa) was > 9. The development and
test sets consist of 2000 sentences each. In order
to study the effect of varying amounts of training
data we created several training partitions consist-
ing of random selections of a subset of the full
training set. The sizes of the partitions are shown
in Table 1, together with the resulting percentage
of out-of-vocabulary (OOV) words in the develop-
ment and test sets (?type? refers to a unique word
in the vocabulary, ?token? to an instance in the ac-
tual text).
6 System
We use a two-pass phrase-based statistical MT
system using GIZA++ (Och and Ney, 2000) for
word alignment and Pharaoh (Koehn, 2004) for
phrase extraction and decoding. Word alignment
is performed in both directions using the IBM-
4 model. Phrases are then extracted from the
word alignments using the method described in
(Och and Ney, 2003). For first-pass decoding we
use Pharaoh in n-best mode. The decoder uses a
weighted combination of seven scores: 4 transla-
tion model scores (phrase-based and lexical scores
for both directions), a trigram language model
score, a distortion score, and a word penalty. Non-
monotonic decoding is used, with no limit on the
44
German-English
Set # sent # words oov dev oov test
train1 5K 101K 7.9/42.6 7.9/42.7
train2 25K 505K 3.8/22.1 3.7/21.9
train3 50K 1013K 2.7/16.1 2.7/16.1
train4 250K 5082K 1.3/8.1 1.2/7.5
train5 751K 15258K 0.8/4.9 0.7/4.4
Finnish-English
Set # sent # words oov dev oov test
train1 5K 78K 16.6/50.6 16.4/50.6
train2 25K 395K 8.6/28.2 8.4/27.8
train3 50K 790K 6.3/21.0 6.2/20.8
train4 250K 3945K 3.1/10.4 3.0/10.2
train5 717K 11319K 1.8/6.2 1.8/6.1
Table 1: Training set sizes and percentages of
OOV words (types/tokens) on the development
and test sets.
dev test
Finnish-English 22.2 22.0
German-English 24.6 24.8
Table 2: Baseline system BLEU scores (%) on dev
and test sets.
number of moves. The score combination weights
are trained by a minimum error rate training pro-
cedure similar to (Och and Ney, 2003). The tri-
gram language model uses modified Kneser-Ney
smoothing and interpolation of trigram and bigram
estimates and was trained on the English side of
the bitext. In the first pass, 2000 hypotheses are
generated per sentence. In the second pass, the
seven scores described above are combined with
4-gram language model scores. The performance
of the baseline system on the development and test
sets is shown in Table 2. The BLEU scores ob-
tained are state-of-the-art for this task.
7 Experiments and Results
We first investigated to what extent the OOV rate
on the development data could be reduced by our
backoff procedure. Table 3 shows the percentage
of words that are still untranslatable after back-
off. A comparison with Table 1 shows that the
backoff model reduces the OOV rate, with a larger
reduction effect observed when the training set
is smaller. We next performed translation with
backoff systems trained on each data partition. In
each case, the combination weights for the indi-
German-English
dev set test set
train1 5.2/27.7 5.1/27.3
train2 2.0/11.7 2.0/11.6
train3 1.4/8.1 1.3/7.6
train4 0.5/3.1 0.5/2.9
train5 0.3/1.7 0.2/1.3
Finnish-English
dev set test set
train1 9.1/28.5 9.2/28.9
train2 3.8/12.4 3.7/12.3
train3 2.5/8.2 2.4/8.0
train4 0.9/3.2 0.9/3.0
train5 0.4/1.4 0.4/1.5
Table 3: OOV rates (%) on the development
and test sets under the backoff model (word
types/tokens).
vidual model scores were re-optimized. Table 4
shows the evaluation results on the dev set. Since
the BLEU score alone is often not a good indi-
cator of successful translations of unknown words
(the unigram or bigram precision may be increased
but may not have a strong effect on the over-
all BLEU score), position-independent word error
rate (PER) rate was measured as well. We see im-
provements in BLEU score and PERs in almost
all cases. Statistical significance was measured on
PER using a difference of proportions significance
test and on BLEU using a segment-level paired
t-test. PER improvements are significant almost
all training conditions for both languages; BLEU
improvements are significant in all conditions for
Finnish and for the two smallest training sets for
German. The effect on the overall development set
(consisting of both sentences with known words
only and sentences with unknown words) is shown
in Table 5. As expected, the impact on overall per-
formance is smaller, especially for larger training
data sets, due to the relatively small percentage of
OOV tokens (see Table 1). The evaluation results
for the test set are shown in Tables 6 (for the sub-
set of sentences with OOVs) and 7 (for the entire
test set), with similar conclusions.
The examples A and B in Figure 2 demon-
strate higher-scoring translations produced by the
backoff system as opposed to the baseline sys-
tem. An analysis of the backoff system output
showed that in some cases (e.g. examples C and
45
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.2 56.9 15.4 55.5
train2 16.3 55.2 17.3 51.8
train3 17.8 51.1 18.4 49.7
train4 19.6 51.1 19.9 47.6
train5 21.9 46.6 22.6 46.0
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
Set BLEU PER BLEU PER
train1 12.4 59.9 13.6 57.8
train2 13.0 61.2 13.9 59.1
train3 14.0 58.0 14.7 57.8
train4 17.4 52.7 18.4 50.8
train5 16.8 52.7 18.7 50.2
Table 4: BLEU (%) and position-independent
word error rate (PER) on the subset of the devel-
opment data containing unknown words (second-
pass output). Here and in the following tables,
statistically significant differences to the baseline
model are shown in boldface (p < 0.05).
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 56.4 16.3 55.1
train2 19.0 53.0 19.5 51.6
train3 20.0 49.9 20.5 49.3
train4 22.2 49.0 22.4 48.1
train5 24.6 46.5 24.7 45.6
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 13.1 59.3 14.4 57.4
train2 14.5 59.7 15.4 58.3
train3 16.0 56.5 16.5 56.5
train4 21.0 50.0 21.4 49.2
train5 22.2 50.5 22.5 49.7
Table 5: BLEU (%) and position-independent
word error rate (PER) for the entire development
set.
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 14.3 56.2 15.5 55.1
train2 17.1 54.3 17.6 50.7
train3 17.4 50.8 18.1 49.7
train4 18.9 49.8 18.8 48.2
train5 19.1 46.3 19.4 46.2
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.4 59.5 13.5 57.5
train2 13.3 60.7 14.2 59.0
train3 14.1 58.2 15.1 57.3
train4 17.2 54.0 18.4 50.2
train5 16.6 51.8 19.0 49.4
Table 6: BLEU (%) and position-independent
word error rate (PER) for the test set (subset with
OOV words).
D in Figure 2), the backoff model produced a
good translation, but the translation was a para-
phrase rather than an identical match to the ref-
erence translation. Since only a single reference
translation is available for the Europarl data (pre-
venting the computation of a BLEU score based
on multiple hand-annotated references), good but
non-matching translations are not taken into ac-
count by our evaluation method. In other cases
the unknown word was translated correctly, but
since it was translated as single-word phrase the
segmentation of the entire sentence was affected.
This may cause greater distortion effects since the
sentence is segmented into a larger number of
smaller phrases, each of which can be reordered.
We therefore added the possibility of translating
an unknown word in its phrasal context by stem-
ming up to m words to the left and right in the
original sentence and finding translations for the
entire stemmed phrase (i.e. the function stem()
is now applied to the entire phrase). This step
is inserted before the stemming of a single word
f in the backoff model described above. How-
ever, since translations for entire stemmed phrases
were found only in about 1% of all cases, there
was no significant effect on the BLEU score. An-
other possibility of limiting reordering effects re-
sulting from single-word translations of OOVs is
to restrict the distortion limit of the decoder. Our
46
German-English
baseline backoff
Set BLEU PER BLEU PER
train1 15.3 55.8 16.3 54.8
train2 19.4 52.3 19.6 50.9
train3 20.3 49.6 20.7 49.2
train4 22.5 48.1 22.5 47.9
train5 24.8 46.3 25.1 45.5
Finnish-English
baseline backoff
Set BLEU PER BLEU PER
train1 12.9 58.7 14.0 57.0
train2 14.5 59.5 15.3 58.4
train3 15.6 56.6 16.4 56.2
train4 20.6 50.3 21.0 49.6
train5 22.0 50.0 22.3 49.5
Table 7: BLEU (%) and position-independent
word error rate (PER) for the test set (entire test
set).
experiments showed that this improves the BLEU
score slightly for both the baseline and the backoff
system; the relative difference, however, remained
the same.
8 Conclusions
We have presented a backoff model for phrase-
based SMT that uses morphological abstractions
to translate unseen word forms in the foreign lan-
guage input. When a match for an unknown word
in the test set cannot be found in the trained phrase
table, the model relies instead on translation prob-
abilities derived from stemmed or split versions
of the word in its phrasal context. An evalua-
tion of the model on German-English and Finnish-
English translations of parliamentary proceedings
showed statistically significant improvements in
PER for almost all training conditions and signifi-
cant improvements in BLEU when the training set
is small (100K words), with larger improvements
for Finnish than for German. This demonstrates
that our method is mainly relevant for highly in-
flected languages and sparse training data condi-
tions. It is also designed to improve human accep-
tance of machine translation output, which is par-
ticularly adversely affected by untranslated words.
Acknowledgments
This work was funded by NSF grant no. IIS-
0308297. We thank Ilona Pitka?nen for help with
Example A: (German-English):
SRC: wir sind berzeugt davon, dass ein europa des friedens
nicht durch milita?rbu?ndnisse geschaffen wird.
BASE: we are convinced that a europe of peace, not by
milita?rbu?ndnisse is created.
BACKOFF: we are convinced that a europe of peace, not
by military alliance is created.
REF: we are convinced that a europe of peace will not be
created through military alliances.
Example B. (Finnish-English):
SRC: arvoisa puhemies, puhuimme ta?a?lla? eilisiltana
serviasta ja siella? tapahtuvista vallankumouksellisista
muutoksista.
BASE: mr president, we talked about here last night, on
the subject of serbia and there, of vallankumouksellisista
changes.
BACKOFF: mr president, we talked about here last
night, on the subject of serbia and there, of revolutionary
changes.
REF: mr. president, last night we discussed the topic of
serbia and the revolutionary changes that are taking place
there.
Example C. (Finnish-English):
SRC: toivon ta?lta? osin, etta? yhdistyneiden kansakuntien
alaisuudessa ka?yta?vissa? neuvotteluissa pa?a?sta?isiin sell-
aiseen lopputulokseen, etta? kyproksen kreikkalainen ja
turkkilainen va?esto?nosa voisivat yhdessa? nauttia liittymisen
mukanaan tuomista eduista yhdistetyssa? tasavallassa.
BASE: i hope that the united nations in the negotiations
to reach a conclusion that the greek and turkish accession
to the benefi t of the benefi ts of the republic of ydistetyssa?
brings together va?esto?nosa could, in this respect, under the
auspices.
BACKOFF: i hope that the united nations in the nego-
tiations to reach a conclusion that the greek and turkish
communities can work together to bring the benefi ts of the
accession of the republic of ydistetyssa?. in this respect,
under the
REF: in this connection, i would hope that the talks
conducted under the auspices of the united nations will be
able to come to a successful conclusion enabling the greek
and turkish cypriot populations to enjoy the advantages
of membership of the european union in the context of a
reunifi ed republic.
Example D. (German-English):
SRC:so sind wir beim durcharbeiten des textes verfahren,
wobei wir bei einer reihe von punkten versucht haben, noch
einige straffungen vorzunehmen.
BASE: we are in the durcharbeiten procedures of the text,
although we have tried to make a few straffungen to carry
out on a number of issues.
BACKOFF: we are in the durcharbeiten procedures, and
we have tried to make a few streamlining of the text in a
number of points.
REF: this is how we came to go through the text, and
attempted to cut down on certain items in the process.
Figure 2: Translation examples (SRC = source,
BASE = baseline system, BACKOFF = backoff
system, REF = reference). OOVs and their trans-
lation are marked in boldface.
47
the Finnish language.
References
J.A. Bilmes and K. Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 4?6, Edmonton, Canada.
S. Corston-Oliver and M. Gamon. 2004. Normaliz-
ing German and English inflectional morphology to
improve statistical word alignment. In Robert E.
Frederking and Kathryn Taylor, editors, Proceedings
of the Conference of the Association for Machine
Translation in the Americas, pages 48?57, Washing-
ton, DC.
A. de Gispert, J.B. Marin?o, and J.M. Crego. 2005. Im-
proving statistical machine translation by classifying
and generalizing inflected verb forms. In Proceed-
ings of 9th European Conference on Speech Commu-
nication and Technology, pages 3193?3196, Lisboa,
Portugal.
A. Fraser and D. Marcu. 2005. ISI?s participation in
the Romanian-English alignment task. In Proceed-
ings of the 2005 ACL Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Transla-
tion and Beyond, pages 91?94, Ann Arbor, Michi-
gan.
D. Gildea. 2001. Statistical Language Understanding
Using Frame Semantics. Ph.D. thesis, University of
California, Berkeley, California.
S. Goldwater and D. McCloskey. 2005. Improving sta-
tistical MT through morphological analysis. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 676?683, Vancou-
ver, British Columbia, Canada.
P. Koehn and C. Monz. 2005. Shared task: statistical
machine translation between European languages.
In Proceedings of the 2005 ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 119?124, Ann Ar-
bor, Michigan.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. the-
sis, Information Sciences Institute, USC, Los Ange-
les, California.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Robert E. Frederking and Kathryn Taylor, editors,
Proceedings of the Conference of the Association for
Machine Translation in the Americas, pages 115?
124, Washington, DC.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit X, Phuket, Thailand.
C. Lioma and I. Ounis. 2005. Deploying part-of-
speech patterns to enhance statistical phrase-based
machine translation resources. In Proceedings of the
2005 ACL Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, pages 163?166, Ann Arbor, Michigan.
S. Niessen and H. Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine trans-
lation. In Proceedings of MT Summit VIII, Santiago
de Compostela, Galicia, Spain.
S. Niessen and H. Ney. 2001b. Toward hierar-
chical models for statistical machine translation of
inflected languages. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in Ma-
chine Translation, pages 47?54, Toulouse, France.
F.J. Och and H. Ney. 2000. Giza++:
Training of statistical translation mod-
els. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
F.J. Och and H. Ney. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
P. Resnik, D. Oard, and G.A. Levow. 2001. Improved
cross-language retrieval using backoff translation.
In Proceedings of the First International Conference
on Human Language Technology Research, pages
153?155, San Diego, California.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
C. Xi and R. Hwa. 2005. A backoff model for boot-
strapping resources for non-English languages. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 851?858, Van-
couver, British Columbia, Canada.
I. Zitouni, O. Siohan, and C.-H. Lee. 2003. Hierar-
chical class n-gram language models: towards bet-
ter estimation of unseen events in speech recogni-
tion. In Proceedings of 8th European Conference on
Speech Communication and Technology, pages 237?
240, Geneva, Switzerland.
48
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 995?1002, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Vocal Joystick: A Voice-Based Human-Computer Interface for
Individuals with Motor Impairments?
Jeff A. Bilmes?, Xiao Li?, Jonathan Malkin?, Kelley Kilanski?, Richard Wright?,
Katrin Kirchhoff?, Amarnag Subramanya?, Susumu Harada?, James A.
Landay?, Patricia Dowden?, Howard Chizeck?
?Dept. of Electrical Engineering
?Dept. of Computer Science & Eng.
?Dept. of Linguistics
?Dept. of Speech & Hearing Science
University of Washington
Seattle, WA
Abstract
We present a novel voice-based human-
computer interface designed to enable in-
dividuals with motor impairments to use
vocal parameters for continuous control
tasks. Since discrete spoken commands
are ill-suited to such tasks, our interface
exploits a large set of continuous acoustic-
phonetic parameters like pitch, loudness,
vowel quality, etc. Their selection is opti-
mized with respect to automatic recogniz-
ability, communication bandwidth, learn-
ability, suitability, and ease of use. Pa-
rameters are extracted in real time, trans-
formed via adaptation and acceleration,
and converted into continuous control sig-
nals. This paper describes the basic en-
gine, prototype applications (in particu-
lar, voice-based web browsing and a con-
trolled trajectory-following task), and ini-
tial user studies confirming the feasibility
of this technology.
1 Introduction
Many existing human-computer interfaces (e.g.,
mouse and keyboard, touch screens, pen tablets,
etc.) are ill-suited to individuals with motor
impairments. Specialized (and often expensive)
human-computer interfaces that have been devel-
oped specifically for this target group include sip
and puff switches, head mice, eye-gaze devices, chin
joysticks and tongue switches. While many indi-
viduals with motor impairments have complete use
?This material is based on work supported by the National
Science Foundation under grant IIS-0326382.
of their vocal system, these devices make little use
of it. Sip and puff switches, for example, have low
communication bandwidth, making it impossible to
achieve more complex control tasks.
Natural spoken language is often regarded as
the obvious choice for a human-computer inter-
face. However, despite significant research efforts
in automatic speech recognition (ASR) (Huang et
al., 2001), existing ASR systems are still not suf-
ficiently robust to a wide variety of speaking condi-
tions, noise, accented speakers, etc. ASR-based in-
terfaces are therefore often abandoned by users after
a short initial trial period. In addition, natural speech
is optimal for communication between humans but
sub-optimal for manipulating computers, windows-
icons-mouse-pointer (WIMP) interfaces, or other
electro-mechanical devices (such as a prosthetic ro-
botic arm). Standard spoken language commands
are useful for discrete but not for continuous op-
erations. For example, in order to move a cursor
from the bottom-left to the upper-right of a screen,
the user might have to repeatedly utter ?up? and
?right? or ?stop? and ?go? after setting an initial tra-
jectory and rate, which is quite inefficient. For these
reasons, we are developing alternative and reusable
voice-based assistive technology termed the ?Vocal
Joystick? (VJ).
2 The Vocal Joystick
The VJ approach has three main characteristics:
1) Continuous control parameters: Unlike standard
speech recognition, the VJ engine exploits continu-
ous vocal characteristics that go beyond simple se-
quences of discrete speech sounds (such as syllables
or words) and include e.g., pitch, vowel quality, and
loudness, which are then mapped to continuous con-
995
trol parameters.
2) Discrete vocal commands: Unlike natural speech,
the VJ discrete input language is based on a pre-
designed set of sounds. These sounds are selected
with respect to acoustic discriminability (maximiz-
ing recognizer accuracy), pronounceability (reduc-
ing potential vocal strain), mnemonic characteris-
tics (reducing cognitive load), robustness to environ-
mental noise, and application appropriateness.
3) Reusable infrastructure: Our goal is not to create
a single application but to provide a modular library
that can be incorporated by developers into a variety
of applications that can be controlled by voice. The
VJ technology is not meant to replace standard ASR
but to enhance and be compatible with it.
2.1 Vocal Characteristics
Three continuous vocal characteristics are extracted
by the VJ engine: energy, pitch, and vowel qual-
ity, yielding four specifiable continuous degrees of
freedom. The first of these, localized acoustic en-
ergy, is used for voice activity detection. In addi-
tion, it is normalized relative to the current vowel
detected (see Section 3.3), and is used by our cur-
rent VJ-WIMP application (Section 4) to control the
velocity of cursor movements. For example, a loud
voice causes a faster movement than does a quiet
voice. The second parameter, pitch, is also extracted
but is currently not mapped to any control dimension
in the VJ-WIMP application but will be in the future.
The third parameter is vowel quality. Unlike conso-
nants, which are characterized by a greater degree of
constriction in the vocal tract, vowels have much in-
herent signal energy and are therefore well-suited to
environments where both high accuracy and noise-
robustness are crucial. Vowels can be characterized
using a 2-D space parameterized by F1 and F2, the
first and second vocal-tract formants (resonant fre-
quencies). We initially experimented with directly
extracting F1/F2 and using them for directly spec-
ifying 2-D continuous control. While we have not
ruled out the use of F1/F2 in the future, we have
so far found that even the best F1/F2 detection al-
gorithms available are not yet accurate enough for
precise real-time specification of movement. There-
fore, we classify vowels directly and map them onto
the 2-D vowel space characterized by degree of con-
striction (i.e., tongue height) and tongue body posi-
tion (Figure 1). In our VJ-WIMP application, we use
Deg
ree 
of C
ons
trict
ion Front Central Back
High
Mid
Low
Tongue Body Position
[iy ] [ix ] [uw ]
[ey] [ax ] [ow ]
[ae ] [a] [aa ]
Figure 1: Vowel configurations as a function of their
dominant articulatory configurations.
the four corners of this chart to map to the 4 princi-
ple directions of up, down, left, and right as shown
in Figure 2 (note that the two figures are flipped and
rotated with respect to each other). We have four
different VJ systems running: A) a 4-class system
allowing only the specification of the 4 principle di-
rections; B) a 5-class system that also includes the
phone [ax] to act as a carrier when wishing to vary
only pitch and loudness; C) a 8-class system that in-
cludes the four diagonal directions; and D) a 9-class
system that includes all phones and directions. Most
of the discussion in this paper refers to the 4-class
system.
A fourth vocal characteristic is also extracted
by the VJ engine, namely discrete sounds. These
sounds may correspond to button presses as on a
mouse or joystick. The choice of sounds depends
on the application and are chosen according to char-
acteristic 2 above.
3 The VJ Engine
Our system-level design goals are modularity, low
latency, and maximal computational efficiency. For
this reason, we share common signal processing
operations in multiple signal extraction modules,
which yields real-time performance but leaves con-
siderable computational headroom for the back-end
applications being driven by the VJ engine.
Figure 3 shows the VJ engine architecture having
three modules: signal processing, pattern recogni-
tion, and motion control.
3.1 Signal Processing
The goal of the signal processing module is to ex-
tract low-level acoustic features that can be used in
996
[iy ]
[ix ]
[uw ]
[ey]
[ow ]
[ae ]
[a]
[aa ][ax ]
Figure 2: Vowel-direction mapping: vowels corre-
sponding to directions.
AcousticWaveform FeatureExtraction
Features:EnergyNCCFF1/F2MFCC
SignalProcessing
Energy
VowelClassification
PatternRecognition
PitchTracking
Discrete SoundRecognition
MotionParameters:
xy-directions,Speed,Acceleration,
Motion Control
SpaceTransformationMotion
ComputerInterfaceDriver Adaptation
Figure 3: System organization
estimating the vocal characteristics. The features we
use are energy, normalized cross-correlation coeffi-
cients (NCCC), formant estimates, Mel-frequency
cepstral coefficients (MFCCs), and formant esti-
mates. To extract features, the speech signal is PCM
sampled at a rate of Fs =16,000Hz. Energy is mea-
sured on a frame-by-frame basis with a frame size
of 25ms and a frame step of 10ms. Pitch is ex-
tracted with a frame size of 40ms and a frame step of
10ms. Multiple pattern recognition tasks may share
the same acoustic features: for example, energy and
NCCCs are used for pitch tracking, and energy and
MFCCs can be used in vowel classification and dis-
crete sound recognition. Therefore, it is more ef-
ficient to decouple feature extraction from pattern
recognition, as is shown in Figure 3.
3.2 Pattern Recognition
The pattern recognition module uses the acoustic
features to extract desired parameters. The estima-
tion and classification system must simultaneously
perform energy computation (available from the in-
put), pitch tracking, vowel classification, and dis-
crete sound recognition.
Many state-of-the-art pitch trackers are based on
dynamic programming (DP). This, however, often
requires the meticulous design of local DP cost func-
tions. The forms of these cost functions are usu-
ally empirically determined and/or their parameters
are tuned by algorithms such as gradient descent
(D.Talkin, 1995). Since different languages and ap-
plications may follow very different pitch transition
patterns, the cost functions optimized for certain lan-
guages and applications may not be the most appro-
priate for others. Our VJ system utilizes a graphi-
cal model mechanism to automatically optimize the
parameters of these cost functions, and has been
shown to yield state-of-the-art performance (X.Li et
al., 2004; J.Malkin et al, 2005).
For frame-by-frame vowel classification, our de-
sign constraints are the need for extremely low la-
tency and low computational cost. Probability es-
timates for vowel classes thus need to be obtained
as soon as possible after the vowel has been uttered
or after any small change in voice quality has oc-
curred. It is well known that models of vowel clas-
sification that incorporate temporal dynamics such
as hidden Markov models (HMMs) can be quite ac-
curate. However, the frame-by-frame latency re-
quirements of VJ make HMMs unsuitable for vowel
classification since HMMs estimate the likelihood
of a model based on the entire utterance. An alter-
native is to utilize causal ?HMM-filtering?, which
computes likelihoods at every frame based on all
frames seen so far. We have empirically found,
however, that slightly non-causal and quite local-
ized estimates of the vowel category probability
is sufficient to achieve user satisfaction. Specifi-
cally, we obtain probability estimates of the form
p(Vt|Xt?? , . . . , Xt+? ), where V is a vowel class,
and Xt?? , . . . , Xt+? are feature frames within a
length 2? + 1 window of features centered at time
t. After several empirical trials, we decided on
neural networks for vowel classification because of
the availability of efficient discriminative training al-
gorithms and their computational simplicity. Specif-
ically we use a simple 2-layer multi-layer percep-
tron (Bishop, 1995) whose input layer consists of
26 ? 7 = 182 nodes, where 26 is the dimension of
Xt, the MFCC feature vector, and 2? + 1 = 7 is the
997
number of consecutive frames, and that has 50 hid-
den nodes (the numbers 7 and 50 were determined
empirically). The output layer has 4 output nodes
representing 4 vowel probabilities. During training,
the network is optimized to minimize the Kullback-
Leibler (K-L) divergence between the output and the
true label distribution, thus achieving the aforemen-
tioned probabilistic interpretation.
The VJ engine needs not only to detect that the
user is specifying a vowel (for continuous control)
but also a consonant-vowel-consonant (CVC) pat-
tern (for discrete control) quickly and with a low
probability of confusion (a VJ system also uses C
and CV patterns for discrete commands). Requir-
ing an initial consonant will phonetically distinguish
these sounds from the pure vowel segments used
for continuous control ? the VJ system constantly
monitors for changes that indicate the beginning of
one of the discrete control commands. The vowel
within the CV and CVC patterns, moreover, can help
prevent background noise from being mis-classified
as a discrete sound. Lastly, each such pattern cur-
rently requires an ending silence, so that the next
command (a new discrete sound or continuous con-
trol vowel) can be accurately initiated. In all cases, a
simple threshold-based rejection mechanism is used
to reduce false positives.
To recognize the discrete control signals, HMMs
are employed since, as in standard speech recogni-
tion, time warping is necessary to normalize for dif-
ferent signal durations corresponding to the same
class. Specifically, we embed phone HMMs into
?word? (C, CV, or CVC) HMMs. In this way, it
is possible to train phone models using a training
set that covers all possible phones, and then con-
struct an application-specific discrete command vo-
cabulary without retraining by recombining existing
phone HMMs into new word HMMs. Therefore,
each VJ-driven application can have its own appro-
priate discrete sound set.
3.3 Motion Control: Direction and Velocity
The VJ motion control module receives several pat-
tern recognition parameters and processes them to
produce output more appropriate for determining 2-
D movement in the VJ-WIMP application.
Initial experiments suggested that using pitch to
affect cursor velocity (Igarashi and Hughes, 2001)
would be heavily constrained by an individual?s vo-
cal range. Giving priority to a more universal user-
independent VJ system, we instead focused on rela-
tive energy. Our observation that users often became
quiet when trying to move small amounts confirmed
energy as a natural choice. Drastically different in-
trinsic average energy levels for each vowel, how-
ever, meant that comparing all sounds to a global av-
erage energy would create a large vowel-dependent
bias. To overcome this, we distribute the energy per
frame among the different vowels, in proportion to
the probabilities output by the neural network, and
track the average energy for each vowel indepen-
dently. By splitting the power in this way, there is
no effect when probabilities are close to 1, and we
smooth out changes during vowel transitions when
probabilities are more evenly distributed.
There are many possible options for determining
velocity (a vector capturing both direction and speed
magnitude) and ?acceleration? (a function determin-
ing how the control-to-display ratio changes based
on input parameters), and the different schemes have
a large impact on user satisfaction. Unlike a standard
mouse cursor, where the mapping is from 2-D hand
movement to a 2-D screen, the VJ system maps from
vocal-tract articulatory movement to a 2-D screen,
and the transformation is not as straightforward. All
values are for the current time frame t unless indi-
cated otherwise. First, a raw direction value is cal-
culated for each axis j ? {x, y} as
dj =
?
i
pi ? ?vi, ej? (1)
in which pi = p(Vt = i|Xt??,...,t+? ) is the proba-
bility for vowel i at time t, vi is a unit vector in the
direction of vowel i, ej is the unit-length positive di-
rectional basis vector along the j axis, and ?v, e? is
the projection of vector v onto unit vector e. To de-
termine movement speed, we first calculate a scalar
for each axis j as
sj =
?
i
max
[
0, gi
(
pi ? f(
E
?i
)
)]
? |?vi, ej?|
where E is the energy in the current frame, ?i is the
average energy for vowel i, and f(?) and gi(?) are
functions used for energy normalization and percep-
tual scaling (such as logs and/or cube-roots). This
therefore allocates frame energy to direction based
on the vowel probabilities. Lastly, we calculate the
velocity for axis j at the current frame as
Vj = ? ? s
?
j ? exp(?sj). (2)
998
where ? represents the overall system sensitivity and
the other values (? and ?) are warping constants, al-
lowing the user to control the shape of the accelera-
tion curve. Typically only one of ? and ? is nonzero.
Setting both to zero results in constant-speed move-
ment along each axis, while ? = 1 and ? = 0
gives a linear mapping that will scale motion with
energy but have no acceleration. The current user-
independent system uses ? = 0.6, ? = 1.0 and sets
? = 0. Lastly, the final velocity along axis j is Vjdj .
Future publications will report on systematic evalu-
ations of different f(?) and gi(?) functions.
3.4 Motion Control: User Adaptation
Since vowel quality is used for continuous control,
inaccuracies can arise due to speaker variability ow-
ing to different speech loudness levels, vocal tract
lengths, etc. Moreover, a vowel class articulated by
one user might partially overlap in acoustic space
with a different vowel class from another user. This
imposes limitations on a purely user-independent
vowel classifier. Differences in speaker loudness
alone could cause significant unpredictability. To
mitigate these problems, we have designed an adap-
tation procedure where each user is asked to pro-
nounce four pre-defined vowel sounds, each last-
ing 2-3 seconds, at the beginning of a VJ ses-
sion. We have investigated several novel adaptation
strategies utilizing both neural networks and support
vector machines (SVM). The fundamental idea be-
hind them both is that an initial speaker-independent
transformation of the space is learned using train-
ing data, and is represented by the first layer of a
neural network. Adaptation data then is used to
transform various parameters of the classifier (e.g.,
all or sub-portions of the neural network, or the para-
meters of the SVM). Further details of some of these
novel adaptation strategies appear in (X.Li et al,
2005), and the remainder will appear in forthcom-
ing publications. Also, the average energy values of
each vowel for each user are recorded and used to
normalize the speed control rate mentioned above.
Preliminary evaluations on the data so far collected
show very good results, with adaptation reducing the
vowel classification error rate by 18% for the 4-class
case, and 35% for the 8-class case. Moreover, infor-
mal studies have shown that users greatly prefer the
VJ system after adaptation than before.
4 Applications and Videos
Our overall intent is for VJ to interface with a va-
riety of applications, and our primary application
so far has been to drive a standard WIMP interface
with VJ controls, what we call the VJ-WIMP ap-
plication. The current VJ version allows left but-
ton clicks (press and release, using the consonant
[k]) as well as left button toggles (using consonant
[ch]) to allow dragging. Since WIMP interfaces
are so general, this allows us to indirectly control
a plethora of different applications. Video demon-
strations are available at the URL: http://ssli.
ee.washington.edu/vj.
One of our key VJ applications is vocal web
browsing. The video (dated 6/2005) shows exam-
ples of two web browsing tasks, one as an exam-
ple of navigating the New York Times web site, the
other using Google Maps to select and zoom in on a
target area. Section 5 describes a preliminary evalu-
ation on these tasks. We have also started using the
VJ engine to control video games (third video ex-
ample), have interfaced VJ with the Dasher system
(Ward et al, 2000) (we call it the ?Vocal Dasher?),
and have also used VJ for figure drawing.
Several additional direct VJ-applications have
also been developed. Specifically, we have directly
interfaced the VJ system into a simple blocks world
environment, where more precise object movement
is possible than via the mouse driver. Specifically,
this environment can draw arbitrary trajectories, and
can precisely measure user fidelity when moving an
object along a trajectory. Fidelity depends both on
positional accuracy and task duration. One use of
this environment shows the spatial direction corre-
sponding to vocal effort (useful for training, forth
video example). Another shows a simple robotic
arm being controlled by VJ. We plan to use this
environment to perform formal and precise user-
performance studies in future work.
5 Preliminary User Study
We conducted a preliminary user study1 to evaluate
the feasibility of VJ and to obtain feedback regard-
ing specific difficulties in using the VJ-WIMP sys-
tem. While this study is not accurate in that: 1) it
does not yet involve the intended target population
1The user study presented here used an earlier version of VJ
than the current improved one described in the preceding pages.
999
of individuals with motor impairments, and: 2) the
users had only a small amount of time to practice and
become adept at using VJ, the study is still indica-
tive of the VJ approach?s overall viability as a novel
voice-based human-computer interface method. The
study quantitatively compares VJ performance with
a standard desktop mouse, and provides qualitative
measurement of the user?s perception of the system.
5.1 Experiment Setup
We recruited seven participants ranging from age 22
to 26, none of whom had any motor impairment.
Of the seven participants, two were female and five
were male. All of them were graduate students in
Computer Science, although none of them had pre-
viously heard of or used VJ. Four of the participants
were native English speakers; the other three had an
Asian language as their mother tongue.
We used a Dell Inspiron 9100 laptop with a 3.2
GHz Intel Pentium IV processor running the Fedora
Core 2 operating system, with a 1280 x 800 24-bit
color display. The laptop was equipped with an ex-
ternal Microsoft IntelliMouse connected through the
USB port which was used for all of the tasks in-
volving the mouse. A head-mounted Amanda NC-
61 microphone was used as the audio input device,
while the audio feedback from the laptop was output
through the laptop speakers. The Firefox browser
was used for all of the tasks, with the browser screen
maximized such that the only portion of the screen
which was not displaying the contents of the web
page was the top navigation toolbar which was 30
pixels high.
5.2 Quantitative and Qualitative Evaluation
At the beginning of the quantitative evaluation, each
participant was given a brief description of the VJ
operations and was shown a demonstration of the
system by a practiced experimenter. The participant
was then guided through an adaptation process dur-
ing which she/he was asked to pronounce the four
directional vowels (Section 3.4). After adaptation,
the participant was given several minutes to practice
using a simple target clicking application. The quan-
titative portion of our evaluation followed a within-
participant design. We exposed each participant to
two experimental conditions which we refer to as
input modalities: the mouse and the VJ. Each par-
ticipant completed two tasks on each modality, with
one trial per task.
The first task was a link navigation task (Link),
in which the participants were asked to start from a
specific web page and follow a particular set of links
to reach a destination. Before the trial, the experi-
menter demonstrated the specified sequence of links
to the participant by using the mouse and clicking at
the appropriate links. The participant was also pro-
vided with a sheet of paper for their reference that
listed the sequence of links that would lead them to
the target. The web site we used was a Computer
Science Department student guide and the task in-
volved following six links with the space between
each successive link including both horizontal and
vertical components.
The second task was map navigation (Map), in
which the participant was asked to navigate an on-
line map application from a starting view (showing
the entire USA) to get to a view showing a partic-
ular campus. The size of the map was 400x400
pixels, and the set of available navigation controls
surrounding the map included ten discrete zoom
level buttons, eight directional panning arrows, and
a click inside the map causing the map to be centered
and zoomed in by one level. Before the trial, a prac-
ticed experimenter demonstrated how to locate the
campus map starting from the USA view to ensure
they were familiar with the geography.
For each task, the participants performed one trial
using the mouse, and one trial using a 4-class VJ.
The trials were presented to the participants in a
counterbalanced order. We recorded the completion
time for each trial, as well as the number of false
positives (system interprets a click when the user
did not make a click sound), missed recognitions
(the user makes a click sound but the system fails to
recognize it as a click), and user errors (whenever
the user clicks on an incorrect link). The recorded
trial times include the time used by all of the above
errors including recovery time.
After the completion of the quantitative evalu-
ation, the participants were given a questionnaire
which consisted of 14 questions related to the partic-
ipants? perception of their experience using VJ such
as the degree of satisfaction, frustration, and embar-
rassment. The answers were encoded on a 7-point
Likert scale. We also included a space where the
participants could write in any comments, and an in-
1000
010
20
30
40
50
60
70
80
90
100
Link Map
Task type
T
as
k 
co
m
p
le
ti
o
n
 t
im
e 
(s
ec
o
n
d
s)
Mouse
Vocal Joystick
Figure 4: Task complement times
0
2
4
6
8
10
12
14
16
18
20
M,
 K
ore
a
M,
 N
ort
he
as
t
M,
 M
idw
es
t
M,
 N
ort
he
as
t
F, 
Mi
d-A
tla
nti
c
F, 
Ch
ina
M,
 C
hin
a
Participant (Gender, Origin)
N
um
be
r o
f m
is
se
d 
re
co
gn
iti
on
s
Link
Map
Figure 5: Missed recognitions by participant
formal post-experiment interview was performed to
solicit further feedback.
5.3 Results
Figure 4 shows the task completion times for Link
and Map tasks, Figure 5 shows the breakdown of
click errors by individual participants, Figure 6
shows the average number of false positive and
missed recognition errors for each of the tasks.
There was no instance of user error in any trial. Fig-
ure 7 shows the median of the responses to each of
the fourteen questionnaire questions (error bars in
each plot show ? standard error). In our measure-
ment of the task completion times, we considered
the VJ?s recognition error rate as a fixed factor, and
thus did not subtract the time spent during those er-
rors from the task completion time.
There were several other interesting observations
that were made throughout the study. We noticed
that the participants who had the least trouble with
missed recognitions for the clicking sound were ei-
0
1
2
3
4
5
6
7
8
9
10
Link Map
Task type
N
um
be
r o
f e
rr
or
s
False positive
Missed Recognition
Figure 6: Average number of click errors per task
1.0
2.0
3.0
4.0
5.0
6.0
7.0
Ea
sy 
to l
ear
n
Ea
sy 
to u
se
Dif
f icu
lt to
 co
ntr
ol
Fru
stra
ting Fu
n
Tir
ing
Em
bar
ras
sin
g
Intu
itiv
e
Err
or 
pro
ne
Se
lf-c
ons
cio
us
Se
lf-c
ons
cio
usn
ess
 de
cre
ase
d
Vo
we
l so
und
s d
isti
ngu
ish
abl
e
Ma
p h
ard
er t
han
 se
arc
h
Mo
tion
 ma
tch
ed 
inte
ntio
n
Strongly
agree
Strongly
disagree
Figure 7: Questionnaire results
ther female or with an Asian language background,
as shown in Figure 5. Our hypothesis regarding the
better performance by female participants is that the
original click sound was trained on one of our fe-
male researcher?s voice. We plan also in future work
to determine how the characteristics of different na-
tive language speakers influence VJ, and ultimately
to correct for any bias.
All but one user explicitly expressed their confu-
sion in distinguishing between the [ae] and [aa] vow-
els. Four of the seven participants independently
stated that their performance would probably have
been better if they had been able to practice longer,
and did not attribute their perceived suboptimal per-
formance to the quality of the VJ?s recognition sys-
tem. Several participants reported that they felt their
vocal cords were strained due to having to produce a
loud sound in order to get the cursor to move at the
desired speed. We suspect this is due either to ana-
log gain problems or to their adapted voice being too
loud, and therefore the system calibrating the nor-
mal speed to correspond to the loud voice. We have
since removed this problem by adjusting our adapta-
1001
tion strategy to express preference for a quiet voice.
In summary, the results from our study suggest
that users without any prior experience were able
to perform basic mouse based tasks using the Vocal
Joystick system with relative slowdown of four to
nine times compared to a conventional mouse. We
anticipate that future planned improvements in the
algorithms underlying the VJ engine (to improve ac-
curacy, user-independence, adaptation, and speed)
will further increase the VJ system?s viability, and
combined with practice could improve VJ enough so
that it becomes a reasonable alternative compared to
a standard mouse?s performance.
6 Related Work
Related voice-based interface studies include
(Igarashi and Hughes, 2001; Olwal and Feiner,
2005). Igarashi & Hughes presented a system where
non-verbal voice features control a mouse system ?
their system requires a command-like discrete sound
to determine direction before initiating a movement
command, where pitch is used to control veloc-
ity. We have empirically found an energy-based
mapping for velocity (as used in our VJ system)
both more reliable (no pitch-tracking errors) and
intuitive. Olwal & Feiner?s system moves the mouse
only after recognizing entire words. de Mauro?s
?voice mouse? http://www.dii.unisi.it/
?maggini/research/voice mouse.html
focuses on continuous cursor movements similar
to the VJ scenario; however, the voice mouse
only starts moving after the vocalization has been
completed leading to long latencies, and it is not
easily portable to other applications. Lastly, the
commercial dictation program Dragon by ScanSoft
includes MouseGridTM(Dra, 2004) which allows
discrete vocal commands to recursively 9-partition
the screen, thus achieving log-command access to a
particular screen point. A VJ system, by contrast,
uses continuous aspects of the voice, has change
latency (about 60ms) not much greater than reaction
time, and allows the user to make instantaneous
directional change using one?s voice (e.g., a user
can draw a ?U? shape in one breath).
7 Conclusions
We have presented new voice-based assistive tech-
nology for continuous control tasks and have
demonstrated an initial system implementation of
this concept. An initial user study using a group
of individuals from the non-target population con-
firmed the feasibility of this technology. We plan
next to further improve our system by evaluating a
number of novel pattern classification techniques to
increase accuracy and user-independence, and to in-
troduce additional vocal characteristics (possibilities
include vibrato, degree of nasality, rate of change
of any of the above as an independent parameter)
to increase the available simultaneous degrees of
freedom controllable via the voice. Moreover, we
plan to develop algorithms to decouple unintended
user correlations of these parameters, and to further
advance both our adaptation and acceleration algo-
rithms.
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion. Clarendon Press, Oxford.
2004. Dragon naturally speaking, MousegridTM, Scan-
Soft Inc.
D.Talkin. 1995. A robust algorithm for pitch track-
ing (RAPT). In W.B.Kleign and K.K.Paliwal, editors,
Speech Coding and Synthesis, pp. 495?515, Amster-
dam. Elsevier Science.
X. Huang, A. Acero, and H.-W. Hon. 2001. Spoken Lan-
guage Processing: A Guide to Theory, Algorithm, and
System Development. Prentice Hall.
T. Igarashi and J. F. Hughes. 2001. Voice as sound: Us-
ing non-verbal voice input for interactive control. In
ACM UIST 2001, November.
J.Malkin, X.Li, and J.Bilmes. 2005. A graphical model
for formant tracking. In Proc. IEEE Intl. Conf. on
Acoustics, Speech, and Signal Processing.
A. Olwal and S. Feiner. 2005. Interaction techniques us-
ing prosodic feature of speech and audio localization.
In Proceedings of the 10th International Conference
on Intelligent User Interfaces, pp. 284?286.
D. Ward, A. F. Blackwell, and D. C. MacKay. 2000.
Dasher - a data entry interface using continuous ges-
tures and language models. In ACM UIST 2000.
X.Li, J.Malkin, and J.Bilmes. 2004. A graphical model
approach to pitch tracking. In Proc. Int. Conf. on Spo-
ken Language Processing.
X.Li, J.Bilmes, and J.Malkin. 2005. Maximum mar-
gin learning and adaptation of MLP classifers. In 9th
European Conference on Speech Communication and
Technology (Eurospeech?05), Lisbon, Portugal, Sep-
tember.
1002
Factored Language Models and Generalized Parallel Backoff
Jeff A. Bilmes Katrin Kirchhoff
SSLI-LAB, University of Washington, Dept. of Electrical Engineering
{bilmes,katrin}@ssli.ee.washington.edu
Abstract
We introduce factored language models
(FLMs) and generalized parallel backoff
(GPB). An FLM represents words as bundles
of features (e.g., morphological classes, stems,
data-driven clusters, etc.), and induces a prob-
ability model covering sequences of bundles
rather than just words. GPB extends standard
backoff to general conditional probability
tables where variables might be heterogeneous
types, where no obvious natural (temporal)
backoff order exists, and where multiple
dynamic backoff strategies are allowed. These
methodologies were implemented during the
JHU 2002 workshop as extensions to the
SRI language modeling toolkit. This paper
provides initial perplexity results on both
CallHome Arabic and on Penn Treebank Wall
Street Journal articles. Significantly, FLMs
with GPB can produce bigrams with signif-
icantly lower perplexity, sometimes lower
than highly-optimized baseline trigrams. In a
multi-pass speech recognition context, where
bigrams are used to create first-pass bigram
lattices or N-best lists, these results are highly
relevant.
1 Introduction
The art of statistical language modeling (LM) is to create
probability models over words and sentences that trade-
off statistical prediction with parameter variance. The
field is both diverse and intricate (Rosenfeld, 2000; Chen
and Goodman, 1998; Jelinek, 1997; Ney et al, 1994),
with many different forms of LMs including maximum-
entropy, whole-sentence, adaptive and cache-based, to
name a small few. Many models are simply smoothed
conditional probability distributions for a word given its
preceding history, typically the two preceding words.
In this work, we introduce two new methods for lan-
guage modeling: factored language model (FLM) and
generalized parallel backoff (GPB). An FLM considers a
word as a bundle of features, and GPB is a technique that
generalized backoff to arbitrary conditional probability
tables. While these techniques can be considered in iso-
lation, the two methods seem particularly suited to each
other ? in particular, the method of GPB can greatly fa-
cilitate the production of FLMs with better performance.
2 Factored Language Models
In a factored language model, a word is viewed as a vec-
tor of k factors, so that wt ? {f1t , f2t , . . . , fKt }. Fac-
tors can be anything, including morphological classes,
stems, roots, and other such features in highly in-
flected languages (e.g., Arabic, German, Finnish, etc.),
or data-driven word classes or semantic features useful
for sparsely inflected languages (e.g., English). Clearly,
a two-factor FLM generalizes standard class-based lan-
guage models, where one factor is the word class and
the other is words themselves. An FLM is a model over
factors, i.e., p(f1:Kt |f1:Kt?1:t?n), that can be factored as a
product of probabilities of the form p(f |f1, f2, . . . , fN ).
Our task is twofold: 1) find an appropriate set of factors,
and 2) induce an appropriate statistical model over those
factors (i.e., the structure learning problem in graphical
models (Bilmes, 2003; Friedman and Koller, 2001)).
3 Generalized Parallel Backoff
An individual FLM probability model can be seen as a di-
rected graphical model over a set of N + 1 random vari-
ables, with child variable F and N parent variables F1
through FN (if factors are words, then F = Wt and Fi =
Wt?i). Two features make an FLM distinct from a stan-
dard language model: 1) the variables {F, F1, . . . , FN}
can be heterogeneous (e.g., words, word clusters, mor-
phological classes, etc.); and 2) there is no obvious nat-
ural (e.g., temporal) backoff order as in standard word-
based language models. With word-only models, back-
off proceeds by dropping first the oldest word, then the
next oldest, and so on until only the unigram remains. In
p(f |f1, f2, . . . , fN ), however, many of the parent vari-
ables might be the same age. Even if the variables have
differing seniorities, it is not necessarily best to drop the
oldest variable first.
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
A
B C D
E F G
H
Figure 1: A backoff graph for F with three parent vari-
ables F1, F2, F3. The graph shows all possible single-
step backoff paths, where exactly one variable is dropped
per backoff step. The SRILM-FLM extensions, however,
also support multi-level backoff.
We introduce the notion of a backoff graph (Figure 1)
to depict this issue, which shows the various backoff
paths from the all-parents case (top graph node) to the
unigram (bottom graph node). Many possible backoff
paths could be taken. For example, when all variables
are words, the path A? B? E?H corresponds to tri-
gram with standard oldest-first backoff order. The path
A? D?G?H is a reverse-time backoff model. This
can be seen as a generalization of lattice-based language
modeling (Dupont and Rosenfeld, 1997) where factors
consist of words and hierarchically derived word classes.
In our GPB procedure, either a single distinct path
is chosen for each gram or multiple parallel paths are
used simultaneously. In either case, the set of back-
off path(s) that are chosen are determined dynamically
(at ?run-time?) based on the current values of the vari-
ables. For example, a path might consist of nodes
A? (BCD) ? (EF) ?G where node A backs off in par-
allel to the three nodes BCD, node B backs off to nodes
(EF), C backs off to (E), and D backs off to (F).
This can be seen as a generalization of the standard
backoff equation. In the two parents case, this becomes:
pGBO(f |f1, f2) =
{
dN(f,f1,f2)pML(f |f1, f2) if N(f, f1, f2) > ?
?(f1, f2)g(f, f1, f2) otherwise
where dN(f,f1,f2) is a standard discount (determining
the smoothing method), pML is the maximum likeli-
hood distribution, ?(f1, f2) are backoff weights, and
g(f, f1, f2) is an arbitrary non-negative backoff function
of its three factor arguments. Standard backoff occurs
with g(f, f1, f2) = pBO(f |f1), but the GPB procedures
can be obtained by using different g-functions. For exam-
ple, g(f, f1, f2) = pBO(f |f2) corresponds to a different
backoff path, and parallel backoff is obtained by using an
appropriate g (see below). As long as g is non-negative,
the backoff weights are defined as follows:
?(f1, f2) =
1 ?
?
f:N(f,f1,f2)>?
dN(f,f1,f2)pML(f |f1, f2)
?
f:N(f,f1,f2)<=?
g(f, f1, f2)
This equation is non-standard only in the denominator,
where one may no longer sum over the factors f only
with counts greater than ? . This is because g is not nec-
essarily a distribution (i.e., does not sum to unity). There-
fore, backoff weight computation can indeed be more ex-
pensive for certain g functions, but this appears not to be
prohibitive as demonstrated in the next few sections.
Table 1: CallHome Arabic Results.
LM parents backoff function/path(s) ppl
3-gram w1, w2 - / temporal [2, 1] 173
FLM 3-gram w1, w2,m1, s1 - / [2, 1, 4, 3] 178
GPB-FLM 3-gram w1, w2,m1, s1 g1 / [2, 1, (3, 4), 3, 4] 166
2-gram w1 - / temporal [1] 175
FLM 2-gram w1,m1 - / [2, 1] 173
FLM 2-gram w1,m1, s1 - / [1, 2, 3] 179
GPB-FLM 2-gram w1,m1, s1 g1 / [1, (2, 3), 2, 3] 167
4 SRILM-FLM extensions
During the recent 2002 JHU workshop (Kirchhoff et al,
2003), significant extensions were made to the SRI lan-
guage modeling toolkit (Stolcke, 2002) to support arbi-
trary FLMs and GPB procedures. This uses a graphical-
model like specification language, and where many dif-
ferent backoff functions (19 in total) were implemented.
Other features include: 1) all SRILM smoothing methods
at every node in a backoff graph; 2) graph level skipping;
and 3) up to 32 possible parents (e.g., 33-gram). Two of
the backoff functions are (in the three parents case):
g(f, f1, f2, f3) = pGBO(f |f`1 , f`2)
where
(`1, `2) = argmax
(m1,m2)?{(1,2),(1,3),(2,3)}
pGBO(f |fm1 , fm2)
(call this g1) or alternatively, where
(`1, `2) = argmax
(m1,m2)?{(1,2),(1,3),(2,3)}
N(f, fm1 , fm2 )
|{f : N(f, fm1 , fm2 ) > 0}|
(call this g2) where N() is the count function. Imple-
mented backoff functions include maximum/min (nor-
malized) counts/backoff probabilities, products, sums,
mins, maxs, (weighted) averages, and geometric means.
5 Results
GPB-FLMs were applied to two corpora and their per-
plexity was compared with standard optimized vanilla bi-
and trigram language models. In the following, we con-
sider as a ?bigram? a language model with a temporal
history that includes information from no longer than one
previous time-step into the past. Therefore, if factors are
deterministically derivable from words, a ?bigram? might
include both the previous words and previous factors as
a history. From a decoding state-space perspective, any
such bigram would be relatively cheap.
In CallHome-Arabic, words are accompanied with de-
terministically derived factors: morphological class (M),
Table 2: Penn Treebank WSJ Results.
LM parents Backoff function/path(s) ppl (?std. dev.)
3-gram w1, w2 - / temporal [2, 1] 258(?1.2)
2-gram w1 - / temporal [1] 320(?1.3)
GPB-FLM 2-gram A w1, d1, t1 g2 / [(1, 2, 3), (1, 2), (2, 3), (3, 1), 1, 2, 3] 266(?1.1)
GPB-FLM 2-gram B w1, d1, f1 g2 / [2, 1] 276(?1.3)
GPB-FLM 2-gram C w1, d1, c1 g2/ [1, (2, 3), 2, 3] 275(?1.2)
stems (S), roots (R), and patterns (P). Training data con-
sisted of official training portions of the LDC CallHome
ECA corpus plus the CallHome ECA supplement (100
conversations). For testing we used the official 1996 eval-
uation set. Results are given in Table 1 and show perplex-
ity for: 1) the baseline 3-gram; 2) a FLM 3-gram using
morphs and stems; 3) a GPB-FLM 3-gram using morphs,
stems and backoff function g1; 4) the baseline 2-gram;
5) an FLM 2-gram using morphs; 6) an FLM 2-gram us-
ing morphs and stems; and 7) an GPB-FLM 2-gram using
morphs and stems. Backoff path(s) are depicted by listing
the parent number(s) in backoff order. As can be seen, the
FLM alone might increase perplexity, but the GPB-FLM
decreases it. Also, it is possible to obtain a 2-gram with
lower perplexity than the optimized baseline 3-gram.
The Wall Street Journal (WSJ) data is from the Penn
Treebank 2 tagged (?88-?89) WSJ collection. Word
and POS tag information (Tt) was extracted. The sen-
tence order was randomized to produce 5-fold cross-
validation results using (4/5)/(1/5) training/testing sizes.
Other factors included the use of a simple determinis-
tic tagger obtained by mapping a word to its most fre-
quent tag (Ft), and word classes obtained using SRILM?s
ngram-class tool with 50 (Ct) and 500 (Dt) classes.
Results are given in Table 2. The table shows the baseline
3-gram and 2-gram perplexities, and three GPB-FLMs.
Model A uses the true by-hand tag information from the
Treebank. To simulate conditions during first-pass de-
coding, Model B shows the results using the most fre-
quent tag, and Model C uses only the two data-driven
word classes. As can be seen, the bigram perplexities
are significantly reduced relative to the baseline, almost
matching that of the baseline trigram. Note that none of
these reduced perplexity bigrams were possible without
using one of the novel backoff functions.
6 Discussion
The improved perplexity bigram results mentioned above
should ideally be part of a first-pass recognition step of a
multi-pass speech recognition system. With a bigram, the
decoder search space is not large, so any appreciable LM
perplexity reductions should yield comparable word er-
ror reductions for a fixed set of acoustic scores in a first-
pass. For N-best or lattice generation, the oracle error
should similarly improve. The use of an FLM with GPB
in such a first pass, however, requires a decoder that sup-
ports such language models. Therefore, FLMs with GPB
will be incorporated into GMTK (Bilmes, 2002), a gen-
eral purpose graphical model toolkit for speech recogni-
tion and language processing. The authors thank Dimitra
Vergyri, Andreas Stolcke, and Pat Schone for useful dis-
cussions during the JHU?02 workshop.
References
[Bilmes2002] J. Bilmes. 2002. The GMTK docu-
mentation. http://ssli.ee.washington.edu/
?bilmes/gmtk.
[Bilmes2003] J. A. Bilmes. 2003. Graphical models and au-
tomatic speech recognition. In R. Rosenfeld, M. Osten-
dorf, S. Khudanpur, and M. Johnson, editors, Mathematical
Foundations of Speech and Language Processing. Springer-
Verlag, New York.
[Chen and Goodman1998] S. F. Chen and J. Goodman. 1998.
An empirical study of smoothing techniques for language
modeling. Technical Report Tr-10-98, Center for Research
in Computing Technology, Harvard University, Cambridge,
Massachusetts, August.
[Dupont and Rosenfeld1997] P. Dupont and R. Rosenfeld.
1997. Lattice based language models. Technical Report
CMU-CS-97-173, Carnegie Mellon University, Pittsburgh,
PA 15213, September.
[Friedman and Koller2001] N. Friedman and D. Koller. 2001.
Learning Bayesian networks from data. In NIPS 2001 Tuto-
rial Notes. Neural Information Processing Systems, Vancou-
ver, B.C. Canada.
[Jelinek1997] F. Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
[Kirchhoff et al2003] K. Kirchhoff et al2003. Novel ap-
proaches to arabic speech recognition: Report from the 2002
johns-hopkins summer workshop. In Proc. IEEE Intl. Conf.
on Acoustics, Speech, and Signal Processing, Hong Kong.
[Ney et al1994] H. Ney, U. Essen, and R. Kneser. 1994. On
structuring probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
[Rosenfeld2000] R. Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here? Proceed-
ings of the IEEE, 88(8).
[Stolcke2002] A. Stolcke. 2002. SRILM- an extensible lan-
guage modeling toolkit. In Proc. Int. Conf. on Spoken Lan-
guage Processing, Denver, Colorado, September.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?4,
New York, June 2006. c?2006 Association for Computational Linguistics
Factored Neural Language Models
Andrei Alexandrescu
Department of Comp. Sci. Eng.
University of Washington
andrei@cs.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
katrin@ee.washington.edu
Abstract
We present a new type of neural proba-
bilistic language model that learns a map-
ping from both words and explicit word
features into a continuous space that is
then used for word prediction. Addi-
tionally, we investigate several ways of
deriving continuous word representations
for unknown words from those of known
words. The resulting model significantly
reduces perplexity on sparse-data tasks
when compared to standard backoff mod-
els, standard neural language models, and
factored language models.
1 Introduction
Neural language models (NLMs) (Bengio et al,
2000) map words into a continuous representation
space and then predict the probability of a word
given the continuous representations of the preced-
ing words in the history. They have previously been
shown to outperform standard back-off models in
terms of perplexity and word error rate on medium
and large speech recognition tasks (Xu et al, 2003;
Emami and Jelinek, 2004; Schwenk and Gauvain,
2004; Schwenk, 2005). Their main drawbacks are
computational complexity and the fact that only dis-
tributional information (word context) is used to
generalize over words, whereas other word prop-
erties (e.g. spelling, morphology etc.) are ignored
for this purpose. Thus, there is also no principled
way of handling out-of-vocabulary (OOV) words.
Though this may be sufficient for applications that
use a closed vocabulary, the current trend of porting
systems to a wider range of languages (esp. highly-
inflected languages such as Arabic) calls for dy-
namic dictionary expansion and the capability of as-
signing probabilities to newly added words without
having seen them in the training data. Here, we in-
troduce a novel type of NLM that improves gener-
alization by using vectors of word features (stems,
affixes, etc.) as input, and we investigate deriving
continuous representations for unknown words from
those of known words.
2 Neural Language Models
P(w  | w    ,w     )t?2t?1t
M
i
h
o
Wih Who
d columns
|V| rows
d = continuous space size
V = vocabulary
n?2w
n?1w
Figure 1: NLM architecture. Each word in the context maps
to a row in the matrix M . The output is next word?s probability
distribution.
A standard NLM (Fig. 1) takes as input the previ-
ous n ? 1 words, which select rows from a continu-
ous word representation matrix M . The next layer?s
input i is the concatenation of the rows in M cor-
responding to the input words. From here, the net-
work is a standard multi-layer perceptron with hid-
den layer h = tanh(i ? Wih + bh) and output layer
o = h ? Who + bo. where bh,o are the biases on the
respective layers. The vector o is normalized by the
softmax function fsoftmax(oi) = eoiP|V |
k=1 e
ok
. Back-
propagation (BKP) is used to learn model parame-
1
ters, including the M matrix, which is shared across
input words. The training criterion maximizes the
regularized log-likelihood of the training data.
3 Generalization in Language Models
An important task in language modeling is to pro-
vide reasonable probability estimates for n-grams
that were not observed in the training data. This
generalization capability is becoming increasingly
relevant in current large-scale speech and NLP sys-
tems that need to handle unlimited vocabularies and
domain mismatches. The smooth predictor func-
tion learned by NLMs can provide good generaliza-
tion if the test set contains n-grams whose individ-
ual words have been seen in similar context in the
training data. However, NLMs only have a simplis-
tic mechanism for dealing with words that were not
observed at all: OOVs in the test data are mapped
to a dedicated class and are assigned the singleton
probability when predicted (i.e. at the output layer)
and the features of a randomly selected singleton
word when occurring in the input. In standard back-
off n-gram models, OOVs are handled by reserv-
ing a small fixed amount of the discount probabil-
ity mass for the generic OOV word and treating it
as a standard vocabulary item. A more powerful
backoff strategy is used in factored language models
(FLMs) (Bilmes and Kirchhoff, 2003), which view
a word as a vector of word features or ?factors?:
w = ?f1, f2, . . . , fk? and predict a word jointly
from previous words and their factors: A general-
ized backoff procedure uses the factors to provide
probability estimates for unseen n-grams, combin-
ing estimates derived from different backoff paths.
This can also be interpreted as a generalization of
standard class-based models (Brown et al, 1992).
FLMs have been shown to yield improvements in
perplexity and word error rate in speech recogni-
tion, particularly on sparse-data tasks (Vergyri et
al., 2004) and have also outperformed backoff mod-
els using a linear decomposition of OOVs into se-
quences of morphemes. In this study we use factors
in the input encoding for NLMs.
4 Factored Neural Language Models
NLMs define word similarity solely in terms of their
context: words are assumed to be close in the contin-
uous space if they co-occur with the same (subset of)
words. But similarity can also be derived from word
shape features (affixes, capitalization, hyphenation
etc.) or other annotations (e.g. POS classes). These
allow a model to generalize across classes of words
bearing the same feature. We thus define a factored
neural language model (FNLM) (Fig. 2) which takes
as input the previous n ? 1 vectors of factors. Dif-
ferent factors map to disjoint row sets of the ma-
trix. The h and o layers are identical to the standard
NLM?s. Instead of predicting the probabilities for
n?1f
2
f 1n?1
n?1f
3
?  |V  | rows
M
i
h
o
Wih Who
n?2
f1
3
n?2f
fn?2
2
P(c   | c    ,c      ) t t?1 t?2
P(w  |c   )t t
d columns
d = continuous space size
k
k
k
V  =vocabulary of factor k
Figure 2: FNLM architecture. Input vectors consisting of
word and feature indices are mapped to rows in M. The final
multiplicative layer outputs the word probability distribution.
all words at the output layer directly, we first group
words into classes (obtained by Brown clustering)
and then compute the conditional probability of each
word given its class: P (wt) = P (ct) ? P (wt|ct).
This is a speed-up technique similar to the hierarchi-
cal structuring of output units used by (Morin and
Bengio, 2005), except that we use a ?flat? hierar-
chy. Like the standard NLM, the network is trained
to maximize the log-likelihood of the data. We use
BKP with cross-validation on the development set
and L2 regularization (the sum of squared weight
values penalized by a parameter ?) in the objective
function.
5 Handling Unknown Factors in FNLMs
In an FNLM setting, a subset of a word?s factors may
be known or can be reliably inferred from its shape
although the word itself never occurred in the train-
ing data. The FNLM can use the continuous repre-
sentation for these known factors directly in the in-
put. If unknown factors are still present, new contin-
uous representations are derived for them from those
of known factors of the same type. This is done by
averaging over the continuous vectors of a selected
subset of the words in the training data, which places
the new item in the center of the region occupied by
2
the subset. For example, proper nouns constitute a
large fraction of OOVs, and using the mean of the
rows in M associated with words with a proper noun
tag yields the ?average proper noun? representation
for the unknown word. We have experimented with
the following strategies for subset selection: NULL
(the null subset, i.e. the feature vector components
for unknown factors are 0), ALL (average of all
known factors of the same type); TAIL (averaging
over the least frequently encountered factors of that
type up to a threshold of 10%); and LEAST, i.e. the
representation of the single least frequent factors of
the same type. The prediction of OOVs themselves
is unaffected since we use a factored encoding only
for the input, not for the output (though this is a pos-
sibility for future work).
6 Data and Baseline Setup
We evaluate our approach by measuring perplex-
ity on two different language modeling tasks. The
first is the LDC CallHome Egyptian Colloquial Ara-
bic (ECA) Corpus, consisting of transcriptions of
phone conversations. ECA is a morphologically
rich language that is almost exclusively used in in-
formal spoken communication. Data must be ob-
tained by transcribing conversations and is therefore
very sparse. The present corpus has 170K words
for training (|V | = 16026), 32K for development
(dev), 17K for evaluation (eval97). The data was
preprocessed by collapsing hesitations, fragments,
and foreign words into one class each. The corpus
was further annotated with morphological informa-
tion (stems, morphological tags) obtained from the
LDC ECA lexicon. The OOV rates are 8.5% (de-
velopment set) and 7.7% (eval97 set), respectively.
Model ECA (?102) Turkish (?102)
dev eval dev eval
baseline 3gram 4.108 4.128 6.385 6.438
hand-optimized FLM 4.440 4.327 4.269 4.479
GA-optimized FLM 4.325 4.179 6.414 6.637
NLM 3-gram 4.857 4.581 4.712 4.801
FNLM-NULL 5.672 5.381 9.480 9.529
FNLM-ALL 5.691 5.396 9.518 9.555
FNLM-TAIL 10% 5.721 5.420 9.495 9.540
FNLM-LEAST 5.819 5.479 10.492 10.373
Table 1: Average probability (scaled by 102) of known words
with unknown words in order-2 context
The second corpus consists of Turkish newspa-
per text that has been morphologically annotated and
disambiguated (Hakkani-Tu?r et al, 2002), thus pro-
viding information about the word root, POS tag,
number and case. The vocabulary size is 67510
(relatively large because Turkish is highly aggluti-
native). 400K words are used for training, 100K
for development (11.8% OOVs), and 87K for test-
ing (11.6% OOVs). The corpus was preprocessed by
removing segmentation marks (titles and paragraph
boundaries).
7 Experiments and Results
We first investigated how the different OOV han-
dling methods affect the average probability as-
signed to words with OOVs in their context. Ta-
ble 1 shows that average probabilities increase com-
pared to the strategy described in Section 3 as
well as other baseline models (standard backoff tri-
grams and FLM, further described below), with the
strongest increase observed for the scheme using the
least frequent factor as an OOV factor model. This
strategy is used for the models in the following per-
plexity experiments.
We compare the perplexity of word-based and
factor-based NLMs with standard backoff trigrams,
class-based trigrams, FLMs, and interpolated mod-
els. Evaluation was done with (the ?w/unk? column
in Table 2) and without (the ?no unk? column) scor-
ing of OOVs, in order to assess the usefulness of our
approach to applications using closed vs. open vo-
cabularies. The baseline Model 1 is a standard back-
off 3-gram using modified Kneser-Ney smoothing
(model orders beyond 3 did not improve perplex-
ity). Model 2 is a class-based trigram model with
Brown clustering (256 classes), which, when inter-
polated with the baseline 3-gram, reduces the per-
plexity (see row 3). Model 3 is a 3-gram word-based
NLM (with output unit clustering). For NLMs,
higher model orders gave improvements, demon-
strating their better scalability: for ECA, a 6-gram
(w/o unk) and a 5-gram (w/unk) were used; for Turk-
ish, a 7-gram (w/o unk) and a 5-gram (w/unk) were
used. Though worse in isolation, the word-based
NLMs reduce perplexity considerably when interpo-
lated with Model 1. The FLM baseline is a hand-
optimized 3-gram FLM (Model 5); we also tested
an FLM optimized with a genetic algorithm as de-
3
# Model ECA dev ECA eval Turkish dev Turkish eval
no unk w/unk no unk w/unk no unk w/unk no unk w/unk
1 Baseline 3-gram 191 176 183 172 827 569 855 586
2 Class-based LM 221 278 219 269 1642 1894 1684 1930
3 1) & 2) 183 169 178 167 790 540 814 555
4 Word-based NLM 208 341 204 195 1510 1043 1569 1067
5 1) & 4) 178 165 173 162 758 542 782 557
6 Word-based NLM 202 194 204 192 1991 1369 2064 1386
7 1) & 6) 175 162 173 160 754 563 772 580
8 hand-optimized FLM 187 171 178 166 827 595 854 614
9 1) & 8) 182 167 174 163 805 563 832 581
10 genetic FLM 190 188 181 188 761 1181 776 1179
11 1) & 10) 183 166 175 164 706 488 720 498
12 factored NLM 189 173 190 175 1216 808 1249 832
13 1) & 12) 169 155 168 155 724 487 744 500
14 1) & 10) & 12) 165 155 165 154 652 452 664 461
Table 2: Perplexities for baseline backoff LMs, FLMs, NLMs, and LM interpolation
scribed in (Duh and Kirchhoff, 2004) (Model 6).
Rows 7-10 of Table 2 display the results. Finally, we
trained FNLMs with various combinations of fac-
tors and model orders. The combination was opti-
mized by hand on the dev set and is therefore most
comparable to the hand-optimized FLM in row 8.
The best factored NLM (Model 7) has order 6 for
both ECA and Turkish. It is interesting to note that
the best Turkish FNLM uses only word factors such
as morphological tag, stem, case, etc. but not the
actual words themselves in the input. The FNLM
outperforms all other models in isolation except the
FLM; its interpolation with the baseline (Model 1)
yields the best result compared to all previous inter-
polated models, for both tasks and both the unk and
no/unk condition. Interpolation of Model 1, FLM
and FNLM yields a further improvement. The pa-
rameter values of the (F)NLMs range between 32
and 64 for d, 45-64 for the number of hidden units,
and 362-1024 for C (number of word classes at the
output layer).
8 Conclusion
We have introduced FNLMs, which combine neu-
ral probability estimation with factored word repre-
sentations and different ways of inferring continuous
word features for unknown factors. On sparse-data
Arabic and Turkish language modeling task FNLMs
were shown to outperform all comparable models
(standard backoff 3-gram, word-based NLMs) ex-
cept FLMs in isolation, and all models when inter-
polated with the baseline. These conclusions apply
to both open and closed vocabularies.
Acknowledgments
This work was funded by NSF under grant no. IIS-
0326276 and DARPA under Contract No. HR0011-
06-C-0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the views of these agencies.
References
Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural
probabilistic language model. In NIPS.
J.A. Bilmes and K. Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
HLT-NAACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18(4).
K. Duh and K. Kirchhoff. 2004. Automatic learning of
language model structure. In COLING 2004.
A. Emami and F. Jelinek. 2004. Exact training of a neu-
ral syntactic language model. In ICASSP 2004.
D. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2002. Statistical
morphological disambiguation for agglutinative lan-
guages. Journal of Computers and Humanities, 36(4).
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In AISTATS.
H. Schwenk and J.L. Gauvain. 2004. Neural network
language models for conversational speech recogni-
tion. In ICSLP 2004.
H. Schwenk. 2005. Training neural network language
models on very large corpora. In HLT/EMNLP.
D. Vergyri, K. Kirchhoff, K. Duh, and A. Stolcke.
2004. Morphology-based language modeling for ara-
bic speech recognition. In ICSLP.
P. Xu, A. Emami, and F. Jelinek. 2003. Training connec-
tionist models for the structured language model. In
EMNLP 2003.
4
Proceedings of NAACL HLT 2007, pages 204?211,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Data-Driven Graph Construction for Semi-Supervised Graph-Based
Learning in NLP
Andrei Alexandrescu
Dept. of Computer Science and Engineering
University of Washington
Seattle, WA, 98195
andrei@cs.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
katrin@ee.washington.edu
Abstract
Graph-based semi-supervised learning has
recently emerged as a promising approach
to data-sparse learning problems in natu-
ral language processing. All graph-based
algorithms rely on a graph that jointly rep-
resents labeled and unlabeled data points.
The problem of how to best construct this
graph remains largely unsolved. In this
paper we introduce a data-driven method
that optimizes the representation of the
initial feature space for graph construc-
tion by means of a supervised classifier.
We apply this technique in the frame-
work of label propagation and evaluate
it on two different classification tasks, a
multi-class lexicon acquisition task and a
word sense disambiguation task. Signifi-
cant improvements are demonstrated over
both label propagation using conventional
graph construction and state-of-the-art su-
pervised classifiers.
1 Introduction
Natural Language Processing (NLP) applications
benefit from the availability of large amounts of an-
notated data. However, such data is often scarce,
particularly for non-mainstream languages. Semi-
supervised learning addresses this problem by com-
bining large amounts of unlabeled data with a small
set of labeled data in order to learn a classifica-
tion function. One class of semi-supervised learn-
ing algorithms that has recently attracted increased
interest is graph-based learning. Graph-based tech-
niques represent labeled and unlabeled data points
as nodes in a graph with weighted edges encoding
the similarity of pairs of samples. Various tech-
niques are then available for transferring class la-
bels from the labeled to the unlabeled data points.
These approaches have shown good performance in
cases where the data is characterized by an underly-
ing manifold structure and samples are judged to be
similar by local similarity measures. However, the
question of how to best construct the graph forming
the basis of the learning procedure is still an under-
investigated research problem. NLP learning tasks
present additional problems since they often rely on
discrete or heterogeneous feature spaces for which
standard similarity measures (such as Euclidean or
cosine distance) are suboptimal.
We propose a two-pass data-driven technique for
graph construction in the framework of label propa-
gation (Zhu, 2005). First, we use a supervised clas-
sifier trained on the labeled subset to transform the
initial feature space (consisting of e.g. lexical, con-
textual, or syntactic features) into a continuous rep-
resentation in the form of soft label predictions. This
representation is then used as a basis for measur-
ing similarity among samples that determines the
structure of the graph used for the second, semi-
supervised learning step. It is important to note that,
rather than simply cascading the supervised and the
semi-supervised learner, we optimize the combina-
tion with respect to the properties required of the
graph. We present several techniques for such op-
timization, including regularization of the first-pass
classifier, biasing by class priors, and linear combi-
204
nation of classifier predictions with known features.
The proposed approach is evaluated on a lexicon
learning task using the Wall Street Journal (WSJ)
corpus, and on the SENSEVAL-3 word sense dis-
ambiguation task. In both cases our technique sig-
nificantly outperforms our baseline systems (label
propagation using standard graph construction and
discriminatively trained supervised classifiers).
2 Background
Several graph-based learning techniques have re-
cently been developed and applied to NLP prob-
lems: minimum cuts (Pang and Lee, 2004), random
walks (Mihalcea, 2005; Otterbacher et al, 2005),
graph matching (Haghighi et al, 2005), and label
propagation (Niu et al, 2005). Here we focus on
label propagation as a learning technique.
2.1 Label propagation
The basic label propagation (LP) algorithm (Zhu and
Ghahramani, 2002; Zhu, 2005) has as inputs:
? a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)},
where xi are samples (feature vectors) and yi ?
{1, 2, . . . , C} are their corresponding labels;
? an unlabeled set {xn+1, . . . , xN};
? a distance measure d(i, j) i, j ? {1, . . . N} de-
fined on the feature space.
The goal is to infer the labels {yn+1, . . . , yN} for
the unlabeled set. The algorithm represents all N
data points as vertices in an undirected graph with
weighted edges. Initially, only the known data ver-
tices are labeled. The edge linking vertices i and j
has weight:
wij = exp
(
?d(i, j)
2
?2
)
(1)
where ? is a hyperparameter that needs to be empir-
ically chosen or learned separately. wij indicates the
label affinity of vertices: the larger wij is, the more
likely it is that i and j have the same label. The LP
algorithm constructs a row-normalized N ?N tran-
sition probability matrix P as follows:
Pij = P (i? j) =
wij
?N
k=1 wik
(2)
The algorithm probabilistically pushes labels from
the labeled nodes to the unlabeled nodes. To do so, it
defines the n?C hard labels matrix Y and the N?C
soft labels matrix f , whose first n rows are identical
to Y . The hard labels matrix Y is invariant through
the algorithm and is initialized with probability 1 for
the known label and 0 for all other labels:
Yic = ?(yi, C) (3)
where ? is Kronecker?s delta function. The algo-
rithm iterates as follows:
1. f ? ? P ? f
2. f ?[rows 1 to n] ? Y
3. If f ? ?= f , stop
4. f ? f ?
5. Repeat from step 1
In each iteration, step 2 fixes the known labels,
which might otherwise be overriden by propagated
labels. The resulting labels for each feature xi,
where i ? {n + 1, . . . , N}, are:
li = arg max
j=1,...,C
fij (4)
It is important that the distance measure is locally
accurate, i.e. nodes connected by an edge with a
high weight should have the same label. The global
distance is less relevant since label information will
be propagated from labeled points through the entire
space. This is why LP works well with a local dis-
tance measure that might be unsuitable as a global
distance measure.
Applications of LP include handwriting recogni-
tion (Zhu and Ghahramani, 2002), image classifi-
cation (Balcan et al, 2005) and retrieval (Qin et
al., 2005), and protein classification (Weston et al,
2003). In NLP, label propagation has been used for
word sense disambiguation (Niu et al, 2005), doc-
ument classification (Zhu, 2005), sentiment analy-
sis (Goldberg and Zhu, 2006), and relation extrac-
tion (Chen et al, 2006).
2.2 Graph construction
One of the main problems in LP, as well as other
graph-based learning techniques, is how to best con-
struct the graph. Currently, graph construction ?is
more of an art than science? (Zhu, 2005). Typically,
edge weights are derived from a simple Euclidean
or cosine distance measure, regardless of the nature
of the underlying features. Edges are then estab-
lished either by connecting all nodes, by applying
a single global threshold to the edge weights, or by
connecting each node to its k nearest neighbors ac-
cording to the edge weights. This procedure is often
suboptimal: Euclidean distance relies on a model of
normally distributed i.i.d. random variables; cosine
205
distance likewise assumes that the different feature
vector dimensions are uncorrelated. However, many
applications, particularly in NLP, rely on feature
spaces with correlated dimensions. Moreover, fea-
tures may have different ranges and different types
(e.g. continuous, binary, multi-valued), which en-
tails the need for normalization, binning, or scaling.
Finally, common distance measures do not take ad-
vantage of domain knowledge that might be avail-
able.
Some attempts have been made at improving the
standard method of graph construction. For in-
stance, in a face identification task (Balcan et al,
2005), domain knowledge was used to identify three
different edge sets based on time, color and face
features, associating a different hyperparameter with
each. The resulting graph was then created by super-
posing edge sets. Zhu (Zhu, 2005, Ch. 7) describes
graph construction using separate ? hyperparame-
ters for each feature dimension, and presents a data-
driven way (evidence maximization) for learning the
values of the parameters.
3 Data-driven graph construction
Unlike previous work, we propose to optimize the
feature representation used for graph construction
by learning it with a first-pass supervised classi-
fier. Under this approach, similarity of samples is
defined as similarity of the output values produced
by a classifier applied to the original feature repre-
sentation of the samples. This idea bears similar-
ity to classifier cascading (Alpaydin and Kaynak,
1998), where classifiers are trained around a rule-
exceptions paradigm; however, in our case, the clas-
sifiers work together, the first acting as a jointly op-
timized feature mapping function for the second.
1. Train a first-pass supervised classifier that out-
puts soft label predictions Zi for all sam-
ples i ? {1, . . . N}, e.g. a posterior prob-
ability distribution over target labels: Zi =
?pi1, pi2, . . . , piC?;
2. Apply postprocessing to Zi if needed.
3. Use vectors Zi and an appropriately chosen dis-
tance measure to construct a graph for LP.
4. Perform label propagation over the constructed
graph to find the labeling of the test samples.
The advantages of this procedure are:
? Uniform range and type of features: The out-
put from a first-pass classifier can produce well-
defined features, e.g. posterior probability distribu-
tions. This eliminates the problem of input features
of different ranges and types (e.g. binary vs. multi-
valued, continuous vs. categorical attributes) which
are often used in combination.
? Feature postprocessing: The transformation of
features into a different space also opens up pos-
sibilities for postprocessing (e.g. probability distri-
bution warping) depending on the requirements of
the second-pass learner. In addition, different dis-
tance functions (e.g. those defined on probability
spaces) can be used, which avoids violating assump-
tions made by metrics such as Euclidean and cosine
distance.
? Optimizing class separation: The learned repre-
sentation of labeled training samples might reveal
better clusters in the data than the original represen-
tation: a discriminatively-trained first pass classifier
will attempt to maximize the separation of samples
belonging to different classes. Moreover, the first-
pass classifier may learn a feature transformation
that suppresses noise in the original input space.
Difficulties with the proposed approach might arise
when the first-pass classifier yields confident but
wrong predictions, especially for outlier samples in
the original space. For this reason, the first-pass
classifier and the graph-based learner should not
simply be concatenated without modification, but
the first classifier should be optimized with respect
to the requirements of the second. In our case, the
choice of first-pass classifier and joint optimization
techniques are determined by the particular learning
task and are detailed below.
4 Tasks
4.1 Lexicon acquisition task
Our first task is a part-of-speech (POS) lexicon ac-
quisition task, i.e. the labels to be predicted are the
sets of POS tags associated with each word in a lex-
icon. Note that this is not a tagging task: we are not
attempting to identify the correct POS of each word
in running text. Rather, for each word in the vocab-
ulary, we attempt to infer the set of possible POS
tags. Our choice of this task is motivated by our
long-term goal of applying this technique to lexicon
acquisition for resource-poor languages: POS lexi-
206
cons are one of the most basic language resources,
which enable subsequent training of taggers, chun-
kers, etc. We assume that a small set of words can be
reliably annotated, and that POS-sets for the remain-
ing words can be inferred by semi-supervised learn-
ing. Rather than choosing a genuinely resource-poor
language for this task, we use the English Wall Street
Journal (WSJ) corpus and artificially limit the size
of the labeled set. This is because the WSJ corpus is
widely obtainable and allows easy replication of our
experiments.
We use sections 0-18 of the Wall Street Journal
corpus (N = 44, 492). Words have between 1 and
4 POS tags, with an average of 1.1 per word. The
number of POS tags is 36, and we treat every POS
combination as a unique class, resulting in C = 158
distinct labels. We use three different randomly se-
lected training sets of various sizes: 5000, 10000,
and 15000 words, representing about 11%, 22%, and
34% of the entire data set respectively; the rest of the
data was used for testing. In order to avoid experi-
mental bias, we run all experiments on five differ-
ent randomly chosen labeled subsets and report av-
erages and standard deviations. Due to the random
sampling of the data it is possible that some labels
never occur in the training set or only occur once.
We train our classifiers only on those labels that oc-
cur at least twice, which results in 60-63 classes. La-
bels not present in the training set will therefore not
be hypothesized and are guaranteed to be errors. We
delete samples with unknown labels from our unla-
beled set since their percentage is less than 0.5% on
average.
We use the following features to represent sam-
ples:
? Integer: the three-letter suffix of the word;
? Integer: The four-letter suffix of the word;
? Integer ? 4: The indices of the four most fre-
quent words that immediately precede the word
in the WSJ text;
? Boolean: word contains capital letters;
? Boolean: word consists only of capital letters;
? Boolean: word contains digits;
? Boolean: word contains a hyphen;
? Boolean: word contains other special charac-
ters (e.g. ?&?).
We have also experimented with shorter suffixes and
with prefixes but those features tended to degrade
performance.
4.2 SENSEVAL-3 word sense disambiguation
task
The second task is word sense disambiguation using
the SENSEVAL-3 corpus (Mihalcea et al, 2004), to
enable a comparison of our method with previously
published results. The goal is to disambiguate the
different senses of each of 57 words given the sen-
tences within which they occur. There are 7860 sam-
ples for training and 3944 for testing. In line with
existing work (Lee and Ng, 2002; Niu et al, 2005),
we use the following features:
? Integer ? 7: seven features consisting of the
POS of the previous three words, the POS of
the next three words, and the POS of the word
itself. We used the MXPOST tagger (Ratna-
parkhi, 1996) for POS annotation.
? Integer??variable length?: a bag of all words
in the surrounding context.
? Integer ? 15: Local collocations Cij (i, j are
the bounds of the collocation window)?word
combinations from the context of the word to
disambiguate. In addition to the 11 collocations
used in similar work (Lee and Ng, 2002), we
also used C?3,1, C?3,2, C?2,3, C?1,3.
Note that syntactic features, which have been used in
some previous studies on this dataset (Mohammad
and Pedersen, 2004), were not included. We apply a
simple feature selection method: a feature X is se-
lected if the conditional entropy H(Y |X) is above
a fixed threshold (1 bit) in the training set, and if X
also occurs in the test set (note that no label infor-
mation from the test data is used for this purpose).
5 Experiments
For both tasks we compare the performance of a su-
pervised classifier, label propagation using the stan-
dard input features and either Euclidean or cosine
distance, and LP using the output from a first-pass
supervised classifier.
5.1 Lexicon acquisition task
5.1.1 First-pass classifier
For this task, the first-pass classifier is a multi-
layer perceptron (MLP) with the topology shown
in Fig. 1. The input features are mapped to con-
207
x 2
x 4
x 1
x 3
P(y | x)
M
i
h
o
Wih Who
A
Figure 1: Architecture of first-pass supervised classifier (MLP)
for lexicon acquisition
.
tinuous values by a discrete-to-continuous mapping
layer M , which is itself learned during the MLP
training process. This layer connects to the hidden
layer h, which in turn is connected to the output
layer o. The entire network is trained via backprop-
agation. The training criterion maximizes the regu-
larized log-likelihood of the training data:
L = 1n
n
?
t=1
log P (yt|xt, ?) + R(?) (5)
The use of an additional continuous mapping layer
is similar to the use of hidden continuous word rep-
resentations in neural language modeling (Bengio et
al., 2000) and yields better results than a standard
3-layer MLP topology.
Problems caused by data scarcity arise when some
of the input features of the unlabeled words have
never been seen in the training set, resulting in un-
trained, randomly-initialized values for those fea-
ture vector components. We address this problem
by creating an approximation layer A that finds the
known input feature vector x? that is most similar
to x (by measuring the cosine similarity between
the vectors). Then xk is replaced with x?k, resulting
in vector x? = ?x1, . . . , xk?1, x?k, xk+1, . . . , xf ? that
has no unseen features and is closest to the original
vector.
5.1.2 LP Setup
We use a dense graph approach. The WSJ set
has a total of 44,492 words, therefore the P ma-
trix that the algorithm requires would have 44, 492?
44, 492 ?= 2? 109 elements. Due to the matrix size,
we avoid the analytical solution of the LP problem,
which requires inverting the P matrix, and choose
the iterative approach described above (Sec. 2.1) in-
stead. Convergence is stopped when the maximum
relative difference between each cell of f and the
corresponding cell of f ? is less than 1%.
Also for data size reasons, we apply LP in chunks.
While the training set stays in memory, the test
data is loaded in fixed-size chunks, labeled, and dis-
carded. This approach has yielded similar results
for various chunk sizes, suggesting that chunking is
a good approximation of whole-set label propaga-
tion.1 LP in chunks is also amenable to paralleliza-
tion: Our system labels different chunks in parallel.
We trained the ? hyperparameter by three-fold
cross-validation on the training data, using a geo-
metric progression with limits 0.1 and 10 and ratio
2. We set fixed upper limits of edges between an
unlabeled node and its labeled neighbors to 15, and
between an unlabeled node and its unlabeled neigh-
bors to 5. The approach of setting different limits
among different kinds of nodes is also used in re-
lated work (Goldberg and Zhu, 2006).
For graph construction we tested: (a) the original
discrete input representation with cosine distance;
(b) the classifier output features (probability distri-
butions) with the Jeffries-Matusita distance.
5.2 Combination optimization
The static parameters of the MLP (learning rate, reg-
ularization rate, and number of hidden units) were
optimized for the LP step by 5-fold cross-validation
on the training data. This process is important be-
cause overspecialization is detrimental to the com-
bined system: an overspecialized first-pass classi-
fier may output very confident but wrong predic-
tions for unseen patterns, thus placing such samples
at large distances from all correctly labeled sam-
ples. A strongly regularized neural network, by con-
trast, will output smoother probability distributions
for unseen patterns. Such outputs also result in a
smoother graph, which in turn helps the LP process.
Thus, we found that a network with only 12 hidden
units and relatively high R(?) in Eq. 5 (10% of the
weight value) performed best in combination with
LP (at an insignificant cost in accuracy when used
1In fact, experiments have shown that performance tends to
degrade for larger chunk sizes, suggesting that whole-set LP
might be affected by ?artifact? clusters that are not related to
the labels.
208
as an isolated classifier).
5.2.1 Results
We first conducted an experiment to measure the
smoothness of the underlying graph, S(G), in the
two LP experiments according to the following for-
mula: S(G) =
?
yi 6=yj ,(i>n?j>n)
wij (6)
where yi is the label of sample i. (Lower values are
better as they reflect less affinity between nodes of
different labels.) The value of S(G) was in all cases
significantly better on graphs constructed with our
proposed technique than on graphs constructed in
the standard way (see Table 1). Table 1 also shows
the performance comparison between LP over the
discrete representation and cosine distance (?LP?),
the neural network itself (?NN?), and LP over the
continuous representation (?NN+LP?), on all dif-
ferent subsets and for different training sizes. For
scarce labeled data (5000 samples) the neural net-
work, which uses a strictly supervised training pro-
cedure, is at a clear disadvantage. However, for a
larger training set the network is able to perform
more accurately than the LP learner that uses the
discrete features directly. The third, combined tech-
nique outperforms the first two significantly.2 The
differences are more pronounced for smaller train-
ing set sizes. Interestingly, the LP is able to extract
information from largely erroneous (noisy) distribu-
tions learned by the neural network.
5.3 Word Sense Disambiguation
We compare the performance of an SVM classifier,
an LP learner using the same input features as the
SVM, and an LP learner using the SVM outputs as
input features. To analyze the influence of train-
ing set size on accuracy, we randomly sample sub-
sets of the training data (25%, 50%, and 75%) and
use the remaining training data plus the test data
as unlabeled data, similarly to the procedure fol-
lowed in related work (Niu et al, 2005). The re-
sults are averaged over five different random sam-
plings. The samplings were chosen such that there
was at least one sample for each label in the training
set. SENSEVAL-3 sports multi-labeled samples and
2Significance was tested using a difference of proportions
significance test; the significance level is 0.01 or smaller in all
cases.
samples with the ?unknown? label. We eliminate all
samples labeled as unknown and retain only the first
label for the multi-labeled instances.
5.3.1 SVM setup
The use of SVM vs. MLP in this case was justi-
fied by the very small training data set. An MLP has
many parameters and needs a considerable amount
of data for effective training, so for this task with
only on the order of 102 training samples per classi-
fier, an SVM was deemed more appropriate. We use
the SVMlight package to build a set of binary clas-
sifiers in a one-versus-all formulation of the multi-
class classification problem. The features input to
each SVM consist of the discrete features described
above (Sec. 4.2) after feature selection. After train-
ing SVMs for each target label against the union of
all others, we evaluate the SVM approach against the
test set by using the winner-takes-all strategy: the
predicted label corresponds to the SVM that outputs
the largest value.
5.3.2 LP setup
Again we set up two LP systems: one using the
original feature space (after feature selection, which
benefited all of the tested systems) and one using the
SVM outputs. Both use a cosine distance measure.
The ? parameter (see Eq. 1) is optimized through
3-fold cross-validation on the training set.
5.4 Combination optimization
Unlike MLPs, SVMs do not compute a smooth out-
put distribution but base the classification decision
on the sign of the output values. In order to smooth
output values with a view towards graph construc-
tion we applied the following techniques:
1. Combining SVM predictions and perfect fea-
ture vectors: After training, the SVM actu-
ally outputs wrong label predictions for a small
number (? 5%) of training samples. These out-
puts could simply be replaced with the perfect
SVM predictions (1 for the true class, -1 else-
where) since the labels are known. However,
the second-pass learner might actually bene-
fit from the information contained in the mis-
classifications. We therefore linearly combine
the SVM predictions with the ?perfect? feature
209
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
5000 NN ? 50.70 59.22 63.77 60.09 54.58 57.67 ? 4.55
LP 451.54 58.37 59.91 60.88 62.01 59.47 60.13 ? 1.24
NN+LP 409.79 58.03 63.91 66.62 65.93 57.76 62.45 ? 3.83
10000 NN ? 65.86 60.19 67.52 65.68 65.64 64.98 ? 2.49
LP 381.16 58.27 60.04 60.85 61.99 62.06 60.64 ? 1.40
NN+LP 315.53 69.36 64.73 69.50 70.26 67.71 68.31 ? 1.97
15000 NN ? 69.85 66.42 70.88 70.71 72.18 70.01 ? 1.94
LP 299.10 58.51 61.00 60.94 63.53 60.98 60.99 ? 1.59
NN+LP 235.83 70.59 69.45 69.99 71.20 73.45 70.94 ? 1.39
Table 1: Accuracy results of neural classification (NN), LP with discrete features (LP), and combined (NN+LP), over 5 random
samplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task. S(G) is the smoothness of the graph
vectors v that contain 1 at the correct label po-
sition and -1 elsewhere:
s?i = ?si + (1? ?)vi (7)
where si, s?i are the i?th input and output feature
vectors and ? a parameter fixed at 0.5.
2. Biasing uninformative distributions: For some
training samples, although the predicted class
label was correct, the outputs of the SVM were
relatively close to one another, i.e. the decision
was borderline. We decided to bias these SVM
outputs in the right direction by using the same
formula as in equation 7.
3. Weighting by class priors: For each training
sample, a corresponding sample with the per-
fect output features was added, thus doubling
the total number of labeled nodes in the graph.
These synthesized nodes are akin to the ?don-
gle? nodes (Goldberg and Zhu, 2006). The dif-
ference is that, while dongle nodes are only
linked to one node, our artificial nodes are
treated like any other node and as such can con-
nect to several other nodes. The role of the arti-
ficial nodes is to serve as authorities during the
LP process and to emphasize class priors.
5.4.1 Results
As before, we measured the smoothness of the
graphs in the two label propagation setups and found
that in all cases the smoothness of the graph pro-
duced with our method was better when compared
to the graphs produced using the standard approach,
as shown in Table 3, which also shows accuracy re-
sults for the SVM (?SVM? label), LP over the stan-
dard graph (?LP?), and label propagation over SVM
outputs (?SVM+LP?). The latter system consistently
performs best in all cases, although the most marked
gains occur in the upper range of labeled samples
percentage. The gain of the best data-driven LP over
the knowledge-based LP is significant in the 100%
and 75% cases.
# System Acc. (%)
1 htsa3 (Grozea, 2004) 72.9
2 IRST-kernels (Strapparava et al, 2004) 72.6
3 nusels (Lee et al, 2004) 72.4
4 SENSEVAL-3 contest baseline 55.2
5 Niu et al (Niu et al, 2005) LP/J-S 70.3
6 Niu et al LP/cosine 68.4
7 Niu et al SVM 69.7
Table 2: Accuracy results of other published systems on
SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly com-
parably to our system.
For comparison purposes, Table 2 shows results
of other published systems against the SENSEVAL
corpus. The ?htsa3?, ?IRST-kernels?, and ?nusels?
systems were the winners of the SENSEVAL-3 con-
test and used extra input features (syntactic rela-
tions). The Niu et al work (Niu et al, 2005) is
most comparable to ours. We attribute the slightly
higher performance of our SVM due to our feature
selection process. The LP/cosine system is a system
similar to our LP system using the discrete features,
and the LP/Jensen-Shannon system is also similar
but uses a distance measure derived from Jensen-
Shannon divergence.
6 Conclusions
We have presented a data-driven graph construction
technique for label propagation that utilizes a first-
210
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
25% SVM ? 62.94 62.53 62.69 63.52 62.99 62.93 ? 0.34
LP 44.71 63.27 61.84 63.26 62.96 63.30 62.93 ? 0.56
SVM+LP 39.67 63.39 63.20 63.95 63.68 63.91 63.63 ? 0.29
50% SVM ? 67.90 66.75 67.57 67.44 66.79 67.29 ? 0.45
LP 33.17 67.84 66.57 67.35 66.52 66.35 66.93 ? 0.57
SVM+LP 24.19 67.95 67.54 67.93 68.21 68.11 67.95 ? 0.23
75% SVM ? 69.54 70.19 68.75 69.80 68.73 69.40 ? 0.58
LP 29.93 68.87 68.65 68.58 68.42 67.19 68.34 ? 0.59
SVM+LP 16.19 69.98 70.05 69.69 70.38 68.94 69.81 ? 0.49
100% SVM ? 70.74
LP 21.72 69.69
SVM+LP 13.17 71.72
Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagation
over SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set. The
improvements of SVM+LP are significant over LP in the 75% and 100% cases. S(G) is the graph smoothness
pass supervised classifier. The outputs from this
classifier (especially when optimized for the second-
pass learner) were shown to serve as a better repre-
sentation for graph-based semi-supervised learning.
Classification results on two learning tasks showed
significantly better performance compared to LP us-
ing standard graph construction and the supervised
classifier alone.
Acknowledgments This work was funded by
NSF under grant no. IIS-0326276. Any opinions,
findings and conclusions, or recommendations ex-
pressed herein are those of the authors and do not
necessarily reflect the views of this agency.
References
E. Alpaydin and C. Kaynak. 1998. Cascading classifiers. Ky-
bernetika, 34:369?374.
Balcan et al 2005. Person identification in webcam images. In
ICML Workshop on Learning with Partially Classified Train-
ing Data.
Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural prob-
abilistic language model. In NIPS.
J. Chen, D. Ji, C.L. Tan, and Z. Niu. 2006. Relation Extraction
Using Label Propagation Based Semi-supervised Learning.
In Proceedings of ACL, pages 129?136.
A. Goldberg and J. Zhu. 2006. Seeing stars when there aren?t
many stars: Graph-based semi-supervised learning for sen-
timent categorization. In HLT-NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
C. Grozea. 2004. Finding optimal parameter settings for high
performance word sense disambiguation. Proceedings of
Senseval-3 Workshop.
A. Haghighi, A. Ng, and C.D. Manning. 2005. Robust textual
inference via graph matching. Proceedings of EMNLP.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense disam-
biguation. In Proceedings of EMNLP, pages 41?48.
Y.K. Lee, H.T. Ng, and T.K. Chia. 2004. Supervised Word
Sense Disambiguation with Support Vector Machines and
Multiple Knowledge Sources. SENSEVAL-3.
R. Mihalcea, T. Chklovski, and A. Killgariff. 2004. The
Senseval-3 English Lexical Sample Task. In Proceedings
of ACL/SIGLEX Senseval-3.
R. Mihalcea. 2005. Unsupervised large-vocabulary word sense
disambiguation with graph-based algorithms for sequence
data labeling. In Proceedings of HLT/EMNLP, pages 411?
418.
S. Mohammad and T. Pedersen. 2004. Complementarity of
Lexical and Simple Syntactic Features: The SyntaLex Ap-
proach to Senseval-3. Proceedings of the SENSEVAL-3.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005. Word
sense disambiguation using label propagation based semi-
supervised learning. In ACL ?05.
J. Otterbacher, G. Erkan, and D.R. Radev. 2005. Using Ran-
dom Walks for Question-focused Sentence Retrieval. Pro-
ceedings of HLT/EMNLP, pages 915?922.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, pages 271?278.
T. Qin, T.-Y. Liu, X.-D. Zhang, W.-Y. Ma, and H.-J. Zhang.
2005. Subspace clustering and label propagation for active
feedback in image retrieval. In MMM, pages 172?179.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings of EMNLP, pages 133?142.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern
abstraction and term similarity for word sense disambigua-
tion: IRST at SENSEVAL-3. Proc. of SENSEVAL-3, pages
229?234.
J. Weston, C. Leslie, D. Zhou, A. Elisseeff, and W. Noble.
2003. Semi-supervised protein classification using cluster
kernels.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical report,
CMU-CALD-02.
Xiaojin Zhu. 2005. Semi-Supervised Learning with Graphs.
Ph.D. thesis, Carnegie Mellon University. CMU-LTI-05-
192.
211
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 119?127,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph-based Learning for Statistical Machine Translation
Andrei Alexandrescu
Dept. of Comp. Sci. Eng.
University of Washington
Seattle, WA 98195, USA
andrei@cs.washington.edu
Katrin Kirchhoff
Dept. of Electrical Eng.
University of Washington
Seattle, WA 98195, USA
katrin@ee.washington.edu
Abstract
Current phrase-based statistical machine
translation systems process each test sentence
in isolation and do not enforce global consis-
tency constraints, even though the test data
is often internally consistent with respect to
topic or style. We propose a new consistency
model for machine translation in the form
of a graph-based semi-supervised learning
algorithm that exploits similarities between
training and test data and also similarities
between different test sentences. The algo-
rithm learns a regression function jointly over
training and test data and uses the resulting
scores to rerank translation hypotheses. Eval-
uation on two travel expression translation
tasks demonstrates improvements of up to 2.6
BLEU points absolute and 2.8% in PER.
1 Introduction
Current phrase-based statistical machine translation
(SMT) systems commonly operate at the sentence
level?each sentence is translated in isolation, even
when the test data consists of internally coherent
paragraphs or stories, such as news articles. For
each sentence, SMT systems choose the translation
hypothesis that maximizes a combined log-linear
model score, which is computed independently of
all other sentences, using globally optimized com-
bination weights. Thus, similar input strings may
be translated in very different ways, depending on
which component model happens to dominate the
combined score for that sentence. This is illustrated
by the following example (from the IWSLT 2007
Arabic-English translation task):
Source 1: Asf lA ymknk *lk hnAk klfp HwAly vmAnyn
dwlAr lAlsAEp AlwAHdp
Ref: sorry you can?t there is a cost the charge is eighty
dollars per hour
1-best: i?m sorry you can?t there in the cost about eighty
dollars for a one o?clock
Source 2: E*rA lA ymknk t$gyl AltlfAz HtY tqlE
AlTA}rp
Ref: sorry you cannot turn the tv on until the plane has
taken off
1-best: excuse me i you turn tv until the plane departs
The phrase lA ymknk (you may not/you cannot)
is translated differently (and wrongly in the sec-
ond case) due to different segmentations and phrase
translations chosen by the decoder. Though differ-
ent choices may be sometimes appropriate, the lack
of constraints enforcing translation consistency of-
ten leads to suboptimal translation performance. It
would be desirable to counter this effect by encour-
aging similar outputs for similar inputs (under a suit-
ably defined notion of similarity, which may include
e.g. a context specification for the phrase/sentence).
In machine learning, the idea of forcing the out-
puts of a statistical learner to vary smoothly with the
underlying structure of the inputs has been formal-
ized in the graph-based learning (GBL) framework.
In GBL, both labeled (train) and unlabeled (test)
data samples are jointly represented as vertices in a
graph whose edges encode pairwise similarities be-
tween samples. Various learning algorithms can be
applied to assign labels to the test samples while en-
suring that the classification output varies smoothly
119
along the manifold defined by the graph. GBL has
been successfully applied to a range of problems in
computer vision, computational biology, and natu-
ral language processing. However, in most cases,
the learning tasks consisted of unstructured classi-
fication, where the input was represented by fixed-
length feature vectors and the output was one of a
finite set of discrete labels. In machine translation,
by contrast, both inputs and outputs consist of word
strings of variable length, and the number of possi-
ble outputs is not fixed and practically unlimited.
In this paper we propose a new graph-based learn-
ing algorithm with structured inputs and outputs to
improve consistency in phrase-based statistical ma-
chine translation. We define a joint similarity graph
over training and test data and use an iterative label
propagation procedure to regress a scoring function
over the graph. The resulting scores for unlabeled
samples (translation hypotheses) are then combined
with standard model scores in a log-linear transla-
tion model for the purpose of reranking. Our con-
tributions are twofold. First, from a machine trans-
lation perspective, we design and evaluate a global
consistency model enforcing that similar inputs re-
ceive similar translations. Second, from a machine
learning perspective, we apply graph-based learning
to a task with structured inputs and outputs, which
is a novel contribution in itself since previous ap-
plications of GBL have focused on predicting cat-
egorical labels. We evaluate our approach on two
machine translation tasks, the IWSLT 2007 Italian-
to-English and Arabic-to-English tasks, and demon-
strate significant improvements over the baseline.
2 Graph-Based Learning
GBL algorithms rely on a similarity graph consisting
of a set of nodes representing data samples xi (where
i ranges over 1, . . . , l labeled points and l+1, . . . , n
unlabeled points), and a set of weighted edges en-
coding pairwise similarities between samples. The
graph is characterized by a weight matrix W whose
elements Wij ? 0 are the similarity values for edges
between vertices xi and xj , and by its label vector
Y = (y1, . . . yl), yi ? {1, . . . , C} that defines la-
bels for the first l points. If there is no edge linking
nodes xi and xj , then Wij = 0. There is consider-
able freedom in choosing the weights. The similar-
ity measure used to compute the edge weights de-
termines the graph structure and is the most impor-
tant factor in successfully applying GBL. In most
applications of GBL, data samples are represented
by fixed-length feature vectors, and cosine similar-
ity or Euclidean distance-based measures are used
for edge weights.
Learning algorithms on similarity graphs include
e.g. min-cut (Blum and Chawla, 2001), spectral
graph transducer (Joachims, 2003), random walk-
based approaches (Szummer and Jaakkola, 2001),
and label propagation (Zhu and Ghahramani, 2002).
The algorithm proposed herein is based on the latter.
2.1 Label Propagation
Given a graph defined by a weight matrix W and
a label set Y , the basic label propagation algorithm
proceeds as follows:
1. Initialize the matrix P as Pij = Wij?WiiP
j Wij?Wii
2. Initialize a n? C matrix f with binary vectors
encoding the known labels for the first l rows:
fi = ?C(yi) ?i ? {1, 2, . . . , l}, where ?C(yi) is
the Kronecker vector of length C with 1 in po-
sition yi and 0 elsewhere. The remaining rows
of f can be zero.
3. f ? ? P ? f
4. Clamp already-labeled data rows: f ?i = ?C(yi)
?i ? {1, 2, . . . , l}
5. If f ? ?= f , stop.
6. f ? f ?
7. Repeat from step 3.
After convergence, f contains the solution in rows
l + 1 to n in the form of unnormalized label proba-
bility distributions. Hard labels can be obtained by
y?i = argmax
j?{1,...,C}
fij ?i ? {l + 1, . . . , n} (1)
The algorithm minimizes the following cost func-
tion (Zhu, 2005):
S =
C?
k=1
?
i>l ? j>l
Wij(fik ? fjk)2 (2)
S measures the smoothness of the learned function,
i.e., the extent to which the labeling allows large-
weight edges to link nodes of different labels. By
minimizing S , label propagation finds a labeling
120
that, to the extent possible, assigns similar soft labels
(identical hard labels) to nodes linked by edges with
large weights (i.e., highly similar samples). The
labeling decision takes into account not only sim-
ilarities between labeled and unlabeled nodes (as
in nearest-neighbor approaches) but also similarities
among unlabeled nodes. Label propagation has been
used successfully for various classification tasks,
e.g. image classification and handwriting recogni-
tion (Zhu, 2005). In natural language processing, la-
bel propagation has been used for document classifi-
cation (Zhu, 2005), word sense disambiguation (Niu
et al, 2005; Alexandrescu and Kirchhoff, 2007), and
sentiment categorization (Goldberg and Zhu, 2006).
3 Graph-Based Learning for Machine
Translation
Our goal is to exploit graph-based learning for im-
proving consistency in statistical phrase-based ma-
chine translation. Intuitively, a set of similar source
sentences should receive similar target-language
translations. This means that similarities between
training and test sentences should be taken into ac-
count, but also similarities between different test
sentences, which is a source of information currently
not exploited by standard SMT systems. To this
end we define a graph over the training and test sets
with edges between test and training sentences as
well as between different test sentences. In cases
where a test sentence does not have any connections
to training sentences but is connected to other test
sentences, helpful information about preferred trans-
lations can be propagated via these edges.
As mentioned above, the problem of machine
translation does not neatly fit into the standard
GBL framework. Given that our samples consist
of variable-length word strings instead of feature
vectors, the standard cosine or Euclidean-distance
based similarity measures cannot be used mean-
ingfully, and the number of possible ?labels??
correct translations?is unbounded and practically
very large. We thus need to modify both the graph
construction and the label propagation algorithms.
First, we handle the problem of unlimited out-
puts by applying GBL to rescoring only. In most
SMT systems, an N -best list (generated by a first de-
coding pass) approximates the search space of good
hypotheses reasonably well, provided N is large
enough. For all hypotheses of all sentences in the
test set (set we denote with H), the system learns a
ranking function r : H ? [0, 1]. Larger values of r
indicate better hypotheses. The corresponding loss
functional is
L(r) =?
i,j
Wij [r(xi)? r(xj)]2 (3)
L(r) measures the smoothness of r over the graph
by penalizing highly similar clusters of nodes that
have a high variance of r (in other words, simi-
lar input sentences that have very different transla-
tions). The smaller L(r), the ?smoother? r is over
the graph. Thus, instead of directly learning a clas-
sification function, we learn a regression function?
similar to (Goldberg and Zhu, 2006)?that is then
used for ranking the hypotheses.
3.1 Graph Construction
Each graph node represents a sentence pair (consist-
ing of source and target strings), and edge weights
represent the combined similarity scores computed
from comparing both the source sides and target
sides of a pair of nodes. Given a training set
with l source and target language sentence pairs
(s1, t1), . . . , (sl, tl) and a test set with l + 1, ..., n
source sentences, sl+1, . . . , sn, the construction of
the similarity graph proceeds as follows:
1. For each test sentence si, i = l + 1, . . . , n,
find a set Straini of similar training source
sentences and a set Stesti of similar test sen-
tences (excluding si and sentences identical to
it) by applying a string similarity function ? to
the source sides only and retaining sentences
whose similarity exceeds a threshold ?. Dif-
ferent ??s can be used for training vs. test sen-
tences; we use the same ? for both sets.
2. For each hypothesis hsi generated for si by a
baseline system, compute its similarity to the
target sides of all sentences in Straini . The
overall similarity is then defined by the com-
bined score
?ij = ? (?(si, sj), ?(hsi , tj)
) (4)
where i = l + 1, . . . n, j = 1, . . . , |Straini | and
? : R+ ? R+ ? R+ is an averaging function.
121
If ?ij > 0, establish graph nodes for hsi and tj
and link them with an edge of weight ?ij .
3. For each hypothesis hsi and each hypothe-
sis generated for each of the sentences sk ?
?testi , compute similarity on the target side and
use the combined similarity score as the edge
weight between nodes for hsi and hsk .
4. Finally,for each node xt representing a train-
ing sentence, assign r(xt) = 1 and also de-
fine its synthetic counterpart: a vertex x?t with
r(x?t) = 0. For each edge incident to xt of
weight Wth, define a corresponding edge of
weight 1?Wt?h.
The synthetic nodes and edges need to be added
to prevent the label propagation algorithm from con-
verging to the trivial solution that assigns r = 1 to
all points in the graph. This choice is theoretically
motivated?a similarity graph for regression should
have not only ?sources? (good nodes with high value
of r) but also ?sinks? (counterparts for the sources).
Figure 1 illustrates the connections of a test node.
Similarity Measure The similarity measure used
for comparing source and target sides is of prime
importance, as it determines the structure of the
graph. This has consequences for both computa-
tional efficiency (denser graphs require more com-
putation and memory) and the accuracy of the out-
come. A low similarity threshold results in a rich
graph with a large number of edges but possibly in-
troduces noise. A higher threshold leads to a small
graph emphasizing highly similar samples but with
too many disconnected components. The similarity
measure is also the means by which domain knowl-
edge can be incorporated into the graph construc-
tion process. Similarity may be defined at the level
of surface word strings, but may also include lin-
guistic information such as morphological features,
part-of-speech tags, or syntactic structures. Here,
we compare two similarity measures: the famil-
iar BLEU score (Papineni et al, 2002) and a score
based on string kernels. In using BLEU we treat
each sentence as a complete document. BLEU is not
symmetric?when comparing two sentences, differ-
ent results are obtained depending on which one is
considered the reference and which one is the hy-
pothesis. For computing similarities between train
and test translations, we use the train translation as
the reference. For computing similarity between two
test hypotheses, we compute BLEU in both direc-
tions and take the average. We note that more ap-
propriate distance measures are certainly possible.
Many previous studies, such as (Callison-Burch et
al., 2006), have pointed out drawbacks of BLEU,
and any other similarity measure could be utilized
instead. In particular, similarity measures that model
aspects of sentences that are ill handled by standard
phrase-based decoders (such as syntactic structure
or semantic information) could be useful here.
A more general way of computing similarity be-
tween strings is provided by string kernels (Lodhi et
al., 2002; Rousu and Shawe-Taylor, 2005), which
have been extensively used in bioinformatics and
email spam detection. String kernels map strings
into a feature space defined by all possible sub-
strings of the string up a fixed length k, and com-
puting the dot product between the resulting feature
vectors. Several variants of basic string kernels ex-
ist, notably those allowing gaps or mismatches, and
efficient implementations have been devised even
for large scale applications. Formally, we define a
sentence s as a concatenation of symbols from a fi-
nite alphabet ? (the vocabulary of the language) and
an embedding function from strings to feature vec-
tors, ? : ?? ? H. A kernel function K(s, t) com-
putes the distance between the resulting vectors for
two sentences s and t. In our case, the embedding
function is defined as
?ku(s) :=
?
i:u=s(i)
?|i| u ? ?k (5)
where k is the maximum length of substrings, |i| is
the length of i, and ? is a penalty parameter for each
gap encountered in the substring. K is defined as
K(s, t) =?
u
??u(s), ?u(t)?wu (6)
where w is a weight dependent on the length of the
substring u. Finally, the kernel score is normalized
by
?
K(s, s) ? K(t, t) to discourage long sentences
from being favored. Thus, our similarity measure is
a gapped, normalized string kernel, which is a more
general measure than BLEU in that is considers non-
contiguous substrings. We use a dynamic program-
ming implementation of string kernels (Rousu and
Shawe-Taylor, 2005).
122
For the combination of source-side and target-
side similarity scores (the function we denoted as ?)
we test two simple schemes, using either the ge-
ometric or the arithmetic mean of the individual
scores. In the first case, large edge weights only re-
sult when both source and target are close to each
other; the latter may produce high edge weights
when only one of them (typically the source score)
is high. More sophisticated combination schemes,
using e.g. weighted combination, could be used but
were not investigated in this study.
Scalability Poor scalability is often mentioned as
a drawback of graph-based learning. Straightfor-
ward implementations of GBL algorithms often rep-
resent the joint training and test data in working
memory and therefore do not scale well to large
data sets. However, we have developed several tech-
niques to improve scalability without impeding ac-
curacy. First, we construct separate graphs for each
test sentence without losing global connectivity in-
formation. The graph for a test sentence is com-
puted as the transitive closure of the edge set E over
the nodes containing all hypotheses for that test sen-
tence. This smaller graph does not affect the out-
come of the learning process for the chosen sentence
because in label propagation the learned value r(xi)
can be influenced by that of another node xj if and
only if xj is reachable from xi. In the worst the-
oretical case, the transitive closure could compre-
hend the entire graph, but in practice the edge set is
never that dense and can be easily pruned based on
the heuristic that faraway nodes connected through
low-weight edges have less influence on the result.
We use a simple embodiment of this heuristic in a
work-list approach: starting from the nodes of inter-
est (hypotheses for the focal sentence), we expand
the closure starting with the direct neighbors, which
have the largest influence; then add their neighbors,
which have less influence, and so forth. A thresh-
old on the number of added vertices limits undue
expansion while capturing either the entire closure
or a good approximation of it. Another practical
computational advantage of portioning work is that
graphs for different hypothesis sets can be trivially
created and used in parallel, whereas distributing
large matrix-vector multiplication is much more dif-
ficult (Choi, 1998). The disadvantage is that overall
1 0
1 0
. . . . . .
W2h
W1h 1?W1h1?W2h
Figure 1: Connections for hypothesis node xh. Similar-
ity edges with weights Wth link the node with train sen-
tences xt, for which r(xt) = 1. For each of these edges
we define a dissimilarity edge of weight 1?Wth, linking
the node with node x?t for which r(x?t) = 0. The vertex is
also connected to other test vertices (the dotted edges).
redundant computations are being made: incomplete
estimates of r are computed for the ancillary nodes
in the transitive closure and then discarded.
Second, we obtain a reduction in graph size of or-
ders of magnitude by collapsing all training vertices
of the same r that are connected to the same test
vertex into one and sum the edge weights. This is
equivalent to the full graph for learning purposes.
3.2 Propagation
Label propagation proceeds as follows:
1. Compute the transitive closure over the edges
starting from all hypothesis nodes of a given
sentence.
2. On the resulting graph, collapse all test-train
similarities for each test node by summing edge
weights. Obtain accumulated similarities in
row and column 1 of the similarity matrix W .
3. Normalize test-to-train weights such that?
j W1j =
?
j Wj1 = 1.
4. Initialize the matrix P as Pij = Wij1?Wi1+Pj Wij .
(The quantity 1?W1i in the denominator is the
weight of the dissimilarity edge.)
5. Initialize a column vector f of height n with
f1 = 1 (corresponding to node x1) and 0 in the
remaining positions.
6. f ? ? P ? f
7. Clamp f ?1: f ?1 = 1
8. If f ? ?= f , continue with step 11.
9. f ? f ?
10. Repeat from step 6.
11. The result r is in the slots of f that correspond
to the hypotheses of interest. Normalize per
sentence if needed, and rank in decreasing or-
der of r.
123
Convergence Our algorithm?s convergence proof
is similar to that for standard label propagation (Zhu,
2005, p. 6). We split P as follows:
P =
[ 0 PLU
PUL PUU
]
(7)
where PUL is a column vector holding global simi-
larities of test hypotheses with train sentences, PLU
is a horizontal vector holding the same similarities
(though PLU 6= P TUL due to normalization), and
PUU holds the normalized similarities between pairs
of test hypotheses. We also separate f :
f =
[ 1
fU
]
(8)
where we distinguish the first entry because it repre-
sents the training part of the data. With these nota-
tions, the iteration formula becomes:
f ?U = PUUfU + PUL (9)
Unrolling the iteration yields:
fU = limn??
[
(PUU )nf0U +
( n?
i=1
(PUU )i?1
)
PUL
]
It can be easily shown that the first term converges
to zero because of normalization in step 4 (Zhu,
2005). The sum in the second term converges to
(I ? PUU )?1, so the unique fixed point is:
fU = (I ? PUU )?1PUL (10)
Our system uses the iterative form. On the data sets
used, convergence took 61.07 steps on average.
At the end of the label propagation algorithm, nor-
malized scores are obtained for each N-best list (sen-
tences without any connections whatsoever are as-
signed zero scores). These are then used together
with the other component models in log-linear com-
bination. Combination weights are optimized on a
held-out data set.
4 Data and System
We evaluate our approach on the IWSLT 2007
Italian-to-English (IE) and Arabic-to-English (AE)
travel tasks. The first is a challenge task, where the
training set consists of read sentences but the de-
velopment and test data consist of spontaneous di-
alogues. The second is a standard travel expres-
sion translation task consisting entirely of read in-
put. For our experiments we chose the text input
(correct transcription) condition only. The data set
sizes are shown in Table 1. We split the IE develop-
ment set into two subsets of 500 and 496 sentences
each. The first set (dev-1) is used to train the system
parameters of the baseline system and as a training
set for GBL. The second is used to tune the GBL pa-
rameters. For each language pair, the baseline sys-
tem was trained with additional out-of-domain text
data: the Italian-English Europarl corpus (Koehn,
2005) in the case of the IE system, and 5.5M words
of newswire data (LDC Arabic Newswire, Multiple-
Translation Corpus and ISI automatically extracted
parallel data) in the case of the AE system.
Set # sent pairs # words # refs
IE train 26.5K 160K 1
IE dev-1 500 4308 1
IE dev-2 496 4204 1
IE eval 724 6481 4
AE train 23K 160K 1
AE dev4 489 5392 7
AE dev5 500 5981 7
AE eval 489 2893 6
Table 1: Data set sizes and reference translations count.
Our baseline is a standard phrase-based SMT
system based on a log-linear model with the fol-
lowing feature functions: two phrase-based trans-
lation scores, two lexical translation scores, word
count and phrase count penalty, distortion score,
and language model score. We use the Moses de-
coder (Koehn et al, 2007) with a reordering limit of
4 for both languages, which generates N -best lists
of up to 2000 hypotheses per sentence in a first pass.
The second pass uses a part-of-speech (POS) based
trigram model, trained on POS sequences generated
by a MaxEnt tagger (Ratnaparkhi, 1996). The lan-
guage models are trained on the English side using
SRILM (Stolcke, 2002) and modified Kneser-Ney
discounting for the first-pass models and Witten-
Bell discounting for the POS models. The baseline
system yields state-of-the-art performance.
124
Weighting dev-2 eval
none (baseline) 22.3/53.3 29.6/45.5
(a) 23.4/51.5 30.7/44.1
(b) 23.5/51.6 30.6/44.3
(c) 23.2/51.8 30.0/44.6
Table 2: GBL results (%BLEU/PER) on IE task
for different weightings of labeled-labeled vs. labeled-
unlabeled graph edges (BLEU-based similarity measure).
5 Experiments and Results
We started with the IE system and initially inves-
tigated the effect of only including edges between
labeled and unlabeled samples in the graph. This
is equivalent to using a weighted k-nearest neighbor
reranker that, for each hypothesis, computes average
similarity with its neighborhood of labeled points,
and uses the resulting score for reranking.
Starting with the IE task and the BLEU-based
similarity metric, we ran optimization experiments
that varied the similarity threshold and compared
sum vs. product combination of source and target
similarity scores, settling for ? = 0.7 and prod-
uct combination. We experimented with three dif-
ferent ways of weighting the contributions from
labeled-unlabeled vs. unlabeled-unlabeled edges:
(a) no weighting, (b) labeled-to-unlabeled edges
were weighted 4 times stronger than unlabeled-
unlabeled ones; and (c) labeled-to-unlabeled edges
were weighted 2 times stronger. The weighting
schemes do not lead to significantly different results.
The best result obtained shows a gain of 1.2 BLEU
points on the dev set and 1 point on the eval set, re-
flecting PER gains of 2% and 1.2%, respectively.
We next tested the string kernel based similarity
measure. The parameter values were 0.5 for the gap
penalty, a maximum substring length of k = 4, and
weights of 0, 0.1, 0.2, 0.7. These values were chosen
heuristically and were not tuned extensively due to
time constraints. Results (Table 3) show significant
improvements in PER and BLEU.
In the context of the BTEC challenge task it is
interesting to compare this approach to adding the
development set directly to the training set. Part of
the improvements may be due to utilizing kNN in-
formation from a data set that is matched to the test
System dev-2 eval
Baseline 22.3/53.3 29.6/45.5
GBL 24.3/51.0 32.2/42.7
Table 3: GBL results (%BLEU/PER) on IE tasks with
string-kernel based similarity measure.
set in terms of style. If this data were also used for
training the initial phrase table, the improvements
might disappear. We first optimized the log-linear
model combination weights on the entire dev07 set
(dev-1 and dev-2 in Table 1) before retraining the
phrase table using the combined train and dev07
data. The new baseline performance (shown in Ta-
ble 4) is much better than before, due to the im-
proved training data. We then added GBL to this
system by keeping the model combination weights
trained for the previous system, using the N-best
lists generated by the new system, and using the
combined train+dev07 set as a train set for select-
ing similar sentences. We used the GBL parameters
that yielded the best performance in the experiments
described above. As can be seen from Table 4, GBL
again yields an improvement of up to 1.2% absolute
in both BLEU and PER.
System BLEU (%) PER
Baseline 37.9 38.4
GBL 39.2 37.2
Table 4: Effect of GBL on IE system trained with
matched data (eval set).
For the AE task we used ? = 0.5; however, this
threshold was not tuned extensively. Results using
BLEU similarity are shown in Table 5. The best
result on the eval set yields an improvement of 1.2
BLEU points though only 0.2% reduction in PER.
Overall, results seem to vary with parameter settings
and nature of the test set (e.g. on dev5, used as a test
set, not for optimization, a surprisingly larger im-
provement in BLEU of 2.7 points is obtained!).
Overall, sentence similarities were observed to be
lower for this task. One reason may be that the AE
system includes statistical tokenization of the source
side, which is itself error-prone in that it can split the
same word in different ways depending on the con-
125
Method dev4 dev5 eval
Baseline 30.2/43.5 21.9/48.4 37.8/41.8
GBL 30.3/42.5 24.6/48.1 39.0/41.6
Table 5: AE results (%BLEU/PER, ? = 0.5)
text. Since our similarity measure is word-based,
this may cause similar sentences to fall below the
threshold. The string kernel does not yield any im-
provement over the BLEU-based similarity measure
on this task. One possible improvement would be to
use an extended string kernel that can take morpho-
logical similarity into account.
Example Below we give an actual example of a
translation improvement, showing the source sen-
tence, the 1-best hypotheses of the baseline system
and GBL system, respectively, the references, and
the translations of similar sentences in the graph
neighborhood of the current sentence.
Source: Al+ mE*rp Aymknk {ltqAT Swrp lnA
Baseline: i?m sorry could picture for us
GBL: excuse me could you take a picture of the us
Refs:
excuse me can you take a picture of us
excuse me could you take a photo of us
pardon would you mind taking a photo of us
pardon me could you take our picture
pardon me would you take a picture of us
excuse me could you take a picture of u
Similar sentences:
could you get two tickets for us
please take a picture for me
could you please take a picture of us
6 Related Work
GBL is an instance of semi-supervised learning,
specifically transductive learning. A different form
of semi-supervised learning (self-training) has been
applied to MT by (Ueffing et al, 2007). Ours is
the first study to explore a graph-based learning ap-
proach. In the machine learning community, work
on applying GBL to structured outputs is beginning
to emerge. Transductive graph-based regularization
has been applied to large-margin learning on struc-
tured data (Altun et al, 2005). However, scalability
quickly becomes a problem with these approaches;
we solve that issue by working on transitive closures
as opposed to entire graphs. String kernel represen-
tations have been used in MT (Szedmak, 2007) in
a kernel regression based framework, which, how-
ever, was an entirely supervised framework. Finally,
our approach can be likened to a probabilistic imple-
mentation of translation memories (Maruyana and
Watanabe, 1992; Veale and Way, 1997). Translation
memories are (usually commercial) databases of
segment translations extracted from a large database
of translation examples. They are typically used by
human translators to retrieve translation candidates
for subsequences of a new input text. Matches can
be exact or fuzzy; the latter is similar to the iden-
tification of graph neighborhoods in our approach.
However, our GBL scheme propagates similarity
scores not just from known to unknown sentences
but also indirectly, via connections through other un-
known sentences. The combination of a translation
memory and statistical translation was reported in
(Marcu, 2001); however, this is a combination of
word-based and phrase-based translation predating
the current phrase-based approach to SMT.
7 Conclusion
We have presented a graph-based learning scheme
to implement a consistency model for SMT that
encourages similar inputs to receive similar out-
puts. Evaluation on two small-scale translation tasks
showed significant improvements of up to 2.6 points
in BLEU and 2.8% PER. Future work will include
testing different graph construction schemes, in par-
ticular better parameter optimization approaches and
better string similarity measures. More gains can
be expected when using better domain knowledge
in constructing the string kernels. This may include
e.g. similarity measures that accommodate POS tags
or morphological features, or comparisons of the
syntax trees of parsed sentence. The latter could be
quite easily incorporated into a string kernel or the
related tree kernel similarity measure. Additionally,
we will investigate the effectiveness of this approach
on larger translation tasks.
Acknowledgments This work was funded by
NSF grant IIS-032676 and DARPA under Contract
No. HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of these agencies.
126
References
A. Alexandrescu and K. Kirchhoff. 2007. Data-Driven
Graph Construction for Semi-Supervised Graph-
Based Learning in NLP. In HLT.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Proceedings of NIPS 18.
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. Proc. 18th
International Conf. on Machine Learning, pages 19?
26.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In Proceedings of EACL.
Jaeyoung Choi. 1998. A new parallel matrix multi-
plication algorithm on distributed-memory concurrent
computers. Concurrency: Practice and Experience,
10(8):655?670.
A. Goldberg and J. Zhu. 2006. Seeing stars when
there aren?t many stars: Graph-based semi-supervised
learning for sentiment categorization. In HLT-NAACL
Workshop on Graph-based Algorithms for Natural
Language Processing.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Machine Translation
Summit X, pages 79?86, Phuket, Thailand.
H. Lodhi, J. Shawe-taylor, and N. Cristianini. 2002. Text
classification using string kernels. In Proceedings of
NIPS.
D. Marcu. 2001. Towards a unified approach to memory-
and statistical-based machine translation. In Proceed-
ings of ACL.
H. Maruyana and H. Watanabe. 1992. Tree cover search
algorithm for example-based translation. In Proceed-
ings of TMI, pages 173?184.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word sense disambiguation using label propagation
based semi-supervised learning method. In Proceed-
ings of ACL, pages 395?402.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proc.of (EMNLP).
J. Rousu and J. Shawe-Taylor. 2005. Efficient computa-
tion of gap-weighted string kernels on large alphabets.
Journal of Machine Learning Research, 6:1323?1344.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In ICSLP, pages 901?904.
Zhuoran Wang;John Shawe-Taylor;Sandor Szedmak.
2007. Kernel regression based machine translation. In
Proceedings of NAACL/HLT, pages 185?188. Associ-
ation for Computational Linguistics.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with markov random walks. In
Advances in Neural Information Processing Systems,
volume 14. http://ai.mit.edu/people/
szummer/.
N. Ueffing, G. Haffari, and A. Sarkar. 2007. Trans-
ductive learning for statistical machine translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation.
T. Veale and A. Way. 1997. Gaijin: a template-
based bootstrapping approach to example-based ma-
chine translation. In Proceedings of News Methods in
Natural Language Processing.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU-CALD-02.
Xiaojin Zhu. 2005. Semi-Supervised Learning with
Graphs. Ph.D. thesis, Carnegie Mellon University.
CMU-LTI-05-192.
127
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 37?40,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Beyond Log-Linear Models:
Boosted Minimum Error Rate Training for N-best Re-ranking
Kevin Duh?
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
kevinduh@u.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
katrin@ee.washington.edu
Abstract
Current re-ranking algorithms for machine
translation rely on log-linear models, which
have the potential problem of underfitting the
training data. We present BoostedMERT, a
novel boosting algorithm that uses Minimum
Error Rate Training (MERT) as a weak learner
and builds a re-ranker far more expressive than
log-linear models. BoostedMERT is easy to
implement, inherits the efficient optimization
properties of MERT, and can quickly boost the
BLEU score on N-best re-ranking tasks. In
this paper, we describe the general algorithm
and present preliminary results on the IWSLT
2007 Arabic-English task.
1 Introduction
N-best list re-ranking is an important component in
many complex natural language processing applica-
tions (e.g. machine translation, speech recognition,
parsing). Re-ranking the N-best lists generated from
a 1st-pass decoder can be an effective approach be-
cause (a) additional knowledge (features) can be in-
corporated, and (b) the search space is smaller (i.e.
choose 1 out of N hypotheses).
Despite these theoretical advantages, we have of-
ten observed little gains in re-ranking machine trans-
lation (MT) N-best lists in practice. It has often
been observed that N-best list rescoring only yields
a moderate improvement over the first-pass output
although the potential improvement as measured by
the oracle-best hypothesis for each sentence is much
?Work supported by an NSF Graduate Research Fellowship.
higher. This shows that hypothesis features are ei-
ther not discriminative enough, or that the reranking
model is too weak
This performance gap can be mainly attributed to
two problems: optimization error and modeling er-
ror (see Figure 1).1 Much work has focused on de-
veloping better algorithms to tackle the optimization
problem (e.g. MERT (Och, 2003)), since MT eval-
uation metrics such as BLEU and PER are riddled
with local minima and are difficult to differentiate
with respect to re-ranker parameters. These opti-
mization algorithms are based on the popular log-
linear model, which chooses the English translation
e of a foreign sentence f by the rule:
argmaxe p(e|f) ? argmaxe
?K
k=1 ?k?k(e, f)
where ?k(e, f) and ?k are the K features and
weights, respectively, and the argmax is over all hy-
potheses in the N-best list.
We believe that standard algorithms such as
MERT already achieve low optimization error (this
is based on experience where many random re-starts
of MERT give little gains); instead the score gap is
mainly due to modeling errors. Standard MT sys-
tems use a small set of features (i.e. K ? 10) based
on language/translation models.2 Log-linear mod-
els on such few features are simply not expressive
enough to achieve the oracle score, regardless of
how well the weights {?k} are optimized.
1Note that we are focusing on closing the gap to the oracle
score on the training set (or the development set); if we were
focusing on the test set, there would be an additional term, the
generalization error.
2In this work, we do not consider systems which utilize a
large smorgasbord of features, e.g. (Och and others, 2004).
37
BLEU=.40, achieved by re-ranking with MERT
BLEU=.56, achieved byselecting oracle hypotheses
Modeling problem:Log-linear model insufficient?
Optimization problem:Stuck in local optimum?
Figure 1: Both modeling and optimization problems in-
crease the (training set) BLEU score gap between MERT
re-ranking and oracle hypotheses. We believe that the
modeling problem is more serious for log-linear models
of around 10 features and focus on it in this work.
To truly achieve the benefits of re-ranking in MT,
one must go beyond the log-linear model. The re-
ranker should not be a mere dot product operation,
but a more dynamic and complex decision maker
that exploits the structure of the N-best re-ranking
problem.
We present BoostedMERT, a general framework
for learning such complex re-rankers using standard
MERT as a building block. BoostedMERT is easy to
implement, inherits MERT?s efficient optimization
procedure, and more effectively boosts the training
score. We describe the algorithm in Section 2, report
experiment results in Section 3, and end with related
work and future directions (Sections 4, 5).
2 BoostedMERT
The idea for BoostedMERT follows the boosting
philosophy of combining several weak classifiers
to create a strong overall classifier (Schapire and
Singer, 1999). In the classification case, boosting
maintains a distribution over each training sample:
the distribution is increased for samples that are in-
correctly classified and decreased otherwise. In each
boosting iteration, a weak learner is trained to opti-
mize on the weighted sample distribution, attempt-
ing to correct the mistakes made in the previous iter-
ation. The final classifier is a weighted combination
of weak learners. This simple procedure is very ef-
fective in reducing training and generalization error.
In BoostedMERT, we maintain a sample distribu-
tion di, i = 1 . . .M over the M N-best lists.3 In
3As such, it differs from RankBoost, a boosting-based rank-
ing algorithm in information retrieval (Freund et al, 2003). If
each boosting iteration t, MERT is called as as sub-
procedure to find the best feature weights ?t on di.4
The sample weight for an N-best list is increased if
the currently selected hypothesis is far from the ora-
cle score, and decreased otherwise. Here, the oracle
hypothesis for each N-best list is defined as the hy-
pothesis with the best sentence-level BLEU. The fi-
nal ranker is a combination of (weak) MERT ranker
outputs.
Algorithm 1 presents more detailed pseudocode.
We use the following notation: Let {xi} represent
the set of M training N-best lists, i = 1 . . .M . Each
N-best list xi contains N feature vectors (for N hy-
potheses). Each feature vector is of dimension K,
which is the same dimension as the number of fea-
ture weights ? obtained by MERT. Let {bi} be the
set of BLEU statistics for each hypothesis in {xi},
which is used to train MERT or to compute BLEU
scores for each hypothesis or oracle.
Algorithm 1 BoostedMERT
Input: N-best lists {xi}, BLEU scores {bi}
Input: Initialize sample distribution di uniformly
Input: Initialize y0 = [0], a constant zero vector
Output: Overall Ranker: fT
1: for t = 1 to T do
2: Weak ranker: ?t = MERT({xi},{bi},di)
3:
4: if (t ? 2): {yt?1} = PRED(f t?1, {xi})
5: {yt} = PRED(?t, {xi})
6: ?t = MERT([yt?1; yt],{bi})
7: Overall ranker: f t = yt?1 + ?tyt
8:
9: for i = 1 to M do
10: ai = [BLEU of hypothesis selected by f t]
divided by [BLEU of oracle hypothesis]
11: di = exp(?ai)/normalizer
12: end for
13: end for
applied on MT, RankBoost would maintain a weight for each
pair of hypotheses and would optimize a pairwise ranking met-
ric, which is quite dissimilar to BLEU.
4This is done by scaling each BLEU statistic, e.g. n-gram
precision, reference length, by the appropriate sample weights
before computing corpus-level BLEU. Alternatively, one could
sample (with replacement) the N-best lists using the distribu-
tion and use the resulting stochastic sample as input to an un-
modified MERT procedure.
38
The pseudocode can be divided into 3 sections:
1. Line 2 finds the best log-linear feature weights
on distribution di. MERT is invoked as a weak
learner, so this step is computationally efficient
for optimizing MT-specific metrics.
2. Lines 4-7 create an overall ranker by combin-
ing the outputs of the previous overall ranker
f t?1 and current weak ranker ?t. PRED is a
general function that takes a ranker and a M
N-best lists and generates a set of M N -dim
output vector y representing the predicted re-
ciprocal rank. Specifically, suppose a 3-best list
and a ranker predicts ranks (1,3,2) for the 1st,
2nd, and 3rd hypotheses, respectively. Then
y = (1/1,1/3,1/2) = (1,0.3,0.5).5
Finally, using a 1-dimensional MERT, the
scalar parameter ?t is optimized by maximiz-
ing the BLEU of the hypothesis chosen by
yt?1+?tyt. This is analogous to the line search
step in boosting for classification (Mason et al,
2000).
3. Lines 9-11 update the sample distribution di
such that N-best lists with low accuracies ai
are given higher emphasis in the next iteration.
The per-list accuracy ai is defined as the ratio of
selected vs. oracle BLEU, but other measures
are possible: e.g. ratio of ranks, difference of
BLEU.
The final classifier fT can be seen as a voting pro-
cedure among multiple log-linear models generated
by MERT. The weighted vote for hypotheses in an
N-best list xi is represented by the N-dimensional
vector: y? =
?T
t=1 ?
tyt =
?T
t=1 ?
t PRED(?t,xi).
We choose the hypothesis with the maximum value
in y?
Finally, we stress that the above algorithm
is an novel extension of boosting to re-ranking
problems. There are many open questions and
one can not always find a direct analog between
boosting for classification and boosting for rank-
ing. For instance, the distribution update scheme
5There are other ways to define a ranking output that are
worth exploring. For example, a hard argmax definition would
be (1,0,0); a probabilistic definition derived from the dot prod-
uct values can also be used. It is the definition of PRED that
introduces non-linearities in BoostedMERT.
of Lines 9-11 is recursive in the classification
case (i.e. di = di ? exp(LossOfWeakLearner)),
but due to the non-decompositional properties of
argmax in re-ranking, we have a non-recursive
equation based on the overall learner (di =
exp(LossOfOverallLearner)). This has deep impli-
cations on the dynamics of boosting, e.g. the distri-
bution may stay constant in the non-recursive equa-
tion, if the new weak ranker gets a small ?.
3 Experiments
The experiments are done on the IWSLT 2007
Arabic-to-English task (clean text condition). We
used a standard phrase-based statistical MT system
(Kirchhoff and Yang, 2007) to generated N-best lists
(N=2000) on Development4, Development5,
and Evaluation sub-sets. Development4 is
used as the Train set; N-best lists that have the same
sentence-level BLEU statistics for all hypotheses are
filtered since they are not important in impacting
training. Development5 is used as Dev set (in
particular, for selecting the number of iterations in
boosting), and Evaluation (Eval) is the blind
dataset for final ranker comparison. Nine features
are used in re-ranking.
We compare MERT vs. BoostedMERT. MERT is
randomly re-started 30 times, and BoostedMERT is
run for 30 iterations, which makes for a relatively
fair comparison. MERT usually does not improve
its Train BLEU score, even with many random re-
starts (again, this suggests that optimization error
is low). Table 1 shows the results, with Boosted-
MERT outperforming MERT 42.0 vs. 41.2 BLEU
on Eval. BoostedMERT has the potential to achieve
43.7 BLEU, if a better method for selecting optimal
iterations can be devised.
It should be noted that the Train scores achieved
by both MERT and BoostedMERT is still far from
the oracle (around 56). We found empirically that
BoostedMERT is somewhat sensitive to the size (M )
of the Train set. For small Train sets, BoostedMERT
can improve the training score quite drastically; for
the current Train set as well as other larger ones, the
improvement per iteration is much slower. We plan
to investigate this in future work.
39
MERT BOOST ?
Train, Best BLEU 40.3 41.0 0.7
Dev, Best BLEU 24.0 25.0 1.0
Eval, Best BLEU 41.2 43.7 2.5
Eval, Selected BLEU 41.2 42.0 0.8
Table 1: The first three rows show the BLEU score for
Train, Dev, and Eval from 30 iterations of BoostedMERT
or 30 random re-restarts of MERT. The last row shows
the actual BLEU on Eval when selecting the number
of boosting iterations based on Dev. Last column in-
dicates absolute improvements. BoostedMERT outper-
forms MERT by 0.8 points on Eval.
4 Related Work
Various methods are used to optimize log-linear
models in re-ranking (Shen et al, 2004; Venugopal
et al, 2005; Smith and Eisner, 2006). Although
this line of work is worthwhile, we believe more
gain is possible if we go beyond log-linear models.
For example, Shen?s method (2004) produces large-
margins but observed little gains in performance.
Our BoostedMERT should not be confused with
other boosting algorithms such as (Collins and Koo,
2005; Kudo et al, 2005). These algorithms are
called boosting because they iteratively choose fea-
tures (weak learners) and optimize the weights for
the boost/exponential loss. They do not, however,
maintain a distribution over N-best lists.
The idea of maintaining a distribution over N-
best lists is novel. To the best of our knowledge,
the most similar algorithm is AdaRank (Xu and Li,
2007), developed for document ranking in informa-
tion retrieval. Our main difference lies in Lines 4-7
in Algorithm 1: AdaRank proposes a simple closed
form solution for ? and combines only weak fea-
tures, not full learners (as in MERT). We have also
implemented AdaRank but it gave inferior results.
It should be noted that the theoretical training
bounds derived in the AdaRank paper is relevant
to BoostedMERT. Similar to standard boosting, this
bound shows that the training score can be improved
exponentially in the number of iterations. However,
we found that the conditions for which this bound is
applicable is rarely satisfied in our experiments.6
6The explanation for this is beyond the scope of this paper;
the basic reason is that our weak rankers (MERT) are not weak
in practice, so that successive iterations get diminishing returns.
5 Conclusions
We argue that log-linear models often underfit the
training data in MT re-ranking, and that this is the
reason we observe a large gap between re-ranker and
oracle scores. Our solution, BoostedMERT, creates
a highly-expressive ranker by voting among multiple
MERT rankers.
Although BoostedMERT improves over MERT,
more work at both the theoretical and algorithmic
levels is needed to demonstrate even larger gains.
For example, while standard boosting for classifica-
tion can exponentially reduce training error in the
number of iterations under mild assumptions, these
assumptions are frequently not satisfied in the algo-
rithm we described. We intend to further explore
the idea of boosting on N-best lists, drawing inspi-
rations from the large body of work on boosting for
classification whenever possible.
References
M. Collins and T. Koo. 2005. Discriminative reranking
for natural langauge parsing. Computational Linguis-
tics, 31(1).
Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. 2003.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4.
K. Kirchhoff and M. Yang. 2007. The UW machine
translation system for IWSLT 2007. In IWSLT.
T. Kudo, J. Suzuki, and H. Isozaki. 2005. Boosting-
based parse reranking with subtree features. In ACL.
L. Mason, J. Baxter, P. Bartless, and M. Frean. 2000.
Boosting as gradient descent. In NIPS.
F.J. Och et al 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT/NAACL.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3).
L. Shen, A. Sarkar, and F.J. Och. 2004. Discriminative
reranking for machine translation. In HLT-NAACL.
D. Smith and J. Eisner. 2006. Minimum risk anneal-
ing for training log-linear models. In Proc. of COL-
ING/ACL Companion Volume.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluating error minimization rules for SMT.
In ACL Workshop on Building/Using Parallel Texts.
J. Xu and H. Li. 2007. AdaRank: A boosting algorithm
for information retrieval. In SIGIR.
40
Directions For Multi-Party Human-Computer Interaction Research
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington, Seattle
katrin@ee.washington.edu
Mari Ostendorf
Department of Electrical Engineering
University of Washington, Seattle
mo@ee.washington.edu
Abstract
Research on dialog systems has so far concen-
trated on interactions between a single user and
a machine. In this paper we identify novel re-
search directions arising from multi-party hu-
man computer interaction, i.e. scenarios where
several human participants interact with a dia-
log system.
1 Introduction
Most current work on spoken human-computer interac-
tion (HCI) involves dialog systems. In recent years, spo-
ken dialog systems with system initiative have become
more commonplace in commercial telephony applica-
tions, and there have been important advances in mixed
initiative and multi-modal research systems. Telephone-
based systems have made it possible to collect large
amounts of human-computer interaction data, which has
benefited empirical research as well as methods based on
automatic training. In addition, evaluation frameworks
have improved beyond the single utterance accuracy mea-
sures used a decade ago to dialog-level subjective and
quantitative measures (Walker et al, 1998).
As dialog systems have advanced, a new area of re-
search has also been developing in automatic recog-
nition and analysis of multi-party human-human spo-
ken interactions, such as meetings, talk shows, court-
room proceedings, and industrial settings (Cohen et al,
2002). Multi-party interactions pose challenges for
speech recognition and speaker tracking because of fre-
quent talker overlap (Shriberg et al, 2001), noise and
room reverberation, but they also introduce new chal-
lenges for discourse modeling. Until recently, empirical
research was only possible using single-speaker and di-
alog corpora, but now there are many hours of data be-
ing collected in multi-talker environments (Morgan et al
2001; Schultz et al 2001).
While many challenges remain in dialog systems ?
from error handling and user modeling to response gen-
eration ? technology has advanced to the point where
one can also envision tackling the combined problem of
multi-party human-computer interaction. A key motiva-
tion for research in such a domain is supporting human-
human collaboration. We envision a scenario where a
computer plays a role as a conversational agent, much as
in a dialog system, except that it interacts with multiple
collaborating humans. The human participants may be
at distributed locations, perhaps with small subgroups at
each location, possibly with different platforms for input
and output. For example, one might imagine a group of
people in a facility with high-end computers interacting
with workers in the field with lightweight communication
clients, using the computer assistant to help gather vital
information or help plan a transportation route. A key dif-
ference from previous work in such scenarios is the idea
of computer initiative. The computer as a participant also
significantly changes the focus of research relative to that
involved in transcription and analysis of meetings, from
work aimed at indexing and summarization to a focus on
interaction.
Besides the application-oriented motivation for re-
search on multi-party human-computer interaction, the
scenario provides a useful technology pull. In current
dialog systems, there is a disincentive to explore user
initiative, simply because much better accuracy can be
achieved by ?controlling? the dialog. However, it would
be impractical for a system to try to constrain the in-
puts from a group of users. Secondly, current dialog sys-
tems generally assume a fixed platform, and hence the re-
sponse generation can be greatly simplified. With varying
platforms and participants with different needs, a more
complex output rendering strategy will be needed, which
will also have implications for future dialog systems as
well. In the follow section, we expand on these issues
and many more research questions that arise in the con-
text of multi-party HCI research.
2 RESEARCH ISSUES
Some of the most intensively pursued research questions
in single-party human-computer interaction are the fol-
lowing: initiative strategies (human vs. system vs. mixed
initiative); dialog planning, in particular the possibility
of learning probabilistic dialog models from data; han-
dling recognition/understanding errors and other system
failures or miscommunications; user modeling, i.e. find-
ing models of interaction patterns specific to certain
users in order to adapt the dialog system; and multi-
modal input/output strategies. A multi-party scenario ex-
tends these questions in various interesting ways but also
presents entirely new challenges, such as described be-
low.
Initiative. It needs to be asked how human-human
communication affects interaction with an automatic di-
alog system on the one hand, and how the presence of
the system influences communication among the human
participants on the other. Specifically, how is the fre-
quency and type of each user?s interaction with the sys-
tem determined? Does every user address the system on
a ?first come, first speak? basis, do users take turns, or do
they first communicate among themselves and then inter-
act with the system via a designated ?proxy? speaker?
How do these different interaction protocols affect the
outcome? For instance, communicative goals might be
achieved more frequently and more rapidly when the pro-
tocol is fixed in advance but users might be more satisfied
with a less structured interaction.
Two other questions are closely tied to the issue of in-
teraction and initiative protocols: (a) should the system
be an open-mike dialog system, i.e. able to record ev-
erything at all times, though responding only to specific
events? and (b) are users in the same physical location
or are they distributed (e.g. in videoconferencing)? In the
case of an open-mike system, special provisions need to
be made to distinguish between utterances addressed to
the dialog system and utterances that are exclusively part
of the human-human interaction. This additional chal-
lenge is offset by the possibility of gathering useful back-
ground information from the participants? conversation
that might enable the system to better respond to queries.
Dialog Modeling. Distributed scenarios, where differ-
ent subgroups of participants are separated from each
other physically, will typically lead to parallel subdialogs
evolving in the course of the interaction. In this case
the system needs to be able to track several subdialogs
simultaneously and to relate them to each other. The
possibility of having multiple concurrent subdialogs di-
rectly affects the dialog planning component. Different
user queries and dialog states might need to be tracked
simultaneously, and formal models of this type of in-
teraction need to be established. Recently, probabilis-
tic models of human-computer dialogs have become in-
creasingly popular. The most commonly used paradigm
is that of Markov decision processes and partially observ-
able Markov decision processes, where the entire dialog
is modelled as a sequence of states and associated actions,
each of which has a certain value (or reward) (Singh et
al., 1999; Roy et al, 2000). The goal is to to choose that
sequence of actions which maximizes the overall reward
in response to the user?s query. States can be thought
of as representing the underlying intentions of the user.
These are typically not entirely transparent but only indi-
rectly (or partially) observable through the speech input.
Multi-party dialogs might require extensions to this and
other modeling frameworks. For instance, it is unclear
whether multiple parallel subdialogs can be modelled by
a single state sequence (i.e. a single decision process), or
whether multiple, partially independent decision process
are required. The issue of how to acquire data to train
these models is a further problem, since parallel subdi-
alogs tend to occur spontaneously and can often not be
elicited in a natural way.
Error handling. The prevention and repair of system
errors and miscommunications may take on an entirely
new shape in the context of multi-party interactions.
First, the notion of what constitutes an error may change
since some participants might interpret a particular sys-
tem response as an error whereas others might not. Sec-
ond, the inherent susceptibility of the system to recogni-
tion and understanding errors will be higher in a group
than when interacting with a single user since both the
speech input and the interaction style exhibit greater vari-
ation. Third, error recovery strategies cannot necessarily
be tailored to a single user but need to take into account
the input from multiple participants, such as the sponta-
neous and possibly diverging reactions to a system recog-
nition error. Studies on different reactions to system er-
rors (e.g. building on related studies in single-party HCI
(Oviatt et al, 1998)) should be included in a roadmap for
multi-party HCI research.
User Modeling. User modeling has recently gained in-
creased importance in the field of dialog systems re-
search, as evidenced by the growing number of dialog-
related publications in e.g. the International Conference
on User Modeling, and the User Modeling and Adap-
tation journal. When multiple users are present, several
concurrent user models need to be established, unless in-
teraction is restricted to a proxy scenario as described
above. Here, too, the question is not only what individual
models should look like, but also how they can be learned
from data. Group interactions are typically dominated by
a small number of active speakers, whereas the remaining
participants provide fewer contributions, such that multi-
party data collections tend to be rather unbalanced with
respect to the amount of data per speaker. Furthermore,
individual users might behave differently in different sce-
narios, depending e.g. on other participants in the group.
Flexible ?Multi? Input/Output. Research on multi-
modal input/output for language-based dialog systems is
a relatively new field, though many contributions have
been made in recent years. Many developments will im-
pact both dialog and multi-party systems, but introducing
the multi-party dimension brings further challenges. For
example, the problem of coordinating speech and gesture
from one person is complicated by increasing the number
of people, making the problem of speaker tracking impor-
tant. For speech input, there are questions of whether to
use an open-mike system, as mentioned earlier, but there
may also be different requirements for participants with
distributed platforms/locations (e.g. noisy environments
may require push-to-talk). One could envision haptic de-
vices controlled simultaneously be multiple users. On the
output side, there is a problem of incorporating backchan-
nels and visual evidence of attentiveness (equivalent to
head nods), as well as turn-taking and interruption cues
for coordinating with other human speech. Coordina-
tion of different output modalities faces an added chal-
lenge when some platforms/environments preclude use
of all modalities. Considering language alone, the re-
sponse generator must provide alternatives depending on
whether voice output is available and on display size (i.e.
how much text and/or visual aids can be included). User
and context modeling should also impact response gener-
ation.
3 INFRASTRUCTURE
In order to study the research questions addressed above
we need appropriate resources. Currently, no publicly
available corpus of multi-party human-machine interac-
tions exists. Several corpora of human-human communi-
cation are available and may be used to study phenomena
such as negotiation of turn-taking but are clearly not suf-
ficient to support work on multi-party HCI.
Data collection mechanism. It would be desirable
to collect several corpora of multi-party human-
machine communication, representing different scenar-
ios, e.g. corporate meetings, remote collaboration of sci-
entific teams, or, remaining closer to existing scenarios,
collaborative travel planning of business partners. Care
should be taken to collect data from groups of various
sizes, co-located as well as distributed teams, technolog-
ically experienced vs. inexperienced users, different in-
put modalities, and teams working on a variety of dif-
ferent tasks. Ideally, data collection should be coordi-
nated across different sites with complementary exper-
tise. Data collections should be made available publicly,
e.g. through LDC. Existing multi-party recording facili-
ties (such as instrumented meeting rooms) could be lever-
aged for this effort.
New Evaluation Paradigms. One of the most impor-
tant research questions is how to evaluate the success of
multi-party HCI. Can we build on existing frameworks
for single-person dialogs? For example, can we extend
the Paradise framework (Walker et al, 1998) by introduc-
ing new quantitative measures (such as speaker tracking
error, computer interruption rate) and designing group
questionnaires or averaging responses to individual ques-
tionnaires? As in dialog system research, component-
level evaluations will continue to be a key driver of re-
search progress, but a multi-party system would likely
include new components relative to a dialog system. For
example, a natural task for the computer might be infor-
mation retrieval (IR), in which case there measures from
the IR community would be relevant. Additionally, we
can incorporate insights from more general (i.e. not nec-
essarily speech-specific) evaluation frameworks for col-
laborative systems (Drury et al 1999; Damianos et al
2000), which take into account factors such as group size,
social interaction parameters, and collaborative tasks.
Multi-party HCI represents a substantial step beyond
current research, but it is an important challenge that will
drive new ideas in many areas. Since multi-party HCI is
fundamentally about collaboration, it is an ideal problem
for fostering the type of multi-site and multi-discipline
interactions that will advance human communication
References
P.R. Cohen, R. Coulston, and K. Krout. 2002. Multiparty mul-
timodal interaction: A preliminary analysis. In Proc. of IC-
SLP, pages 201?204.
L. Damianos et al 2000. Evaluating multi-party multi-modal
systems. Technical paper, The MITRE Corporation.
J. Drury et al 1999. Methodology for evaluation of collabora-
tive systems, v.4.0. http://www.nist.gov/nist-icv.
N. Morgan et al 2001. The meeting project at ICSI. In Proc.
of HLT, pages 246?252.
S. Oviatt, G.A. Levow, E. Moreton, and M. MacEachern. 1998.
Modeling global and focal hyperarticulation during human-
computer error resolution. JASA, 104(5).
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialogue man-
agement using probabilistic reasoning. In Proc. of ACL.
T. Schultz et al 2001. The ISL meeting room system.
In Proc. Workshop on Hands-Free Speech Communication
(HSC-2001), Kyoto Japan.
E. Shriberg, A. Stolcke, and D. Baron. 2001. Observations on
overlap: Findings and implications for automatic processing
of multi-party conversation. In Proc. EUROSPEECH, pages
1359?1362.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999. Rein-
forcement learning for spoken dialog systems. In Advances
in Neural Processing Systems, volume 12.
M. Walker et al 1998. Evaluating spoken dialogue agents with
PARADISE: Two case studies. Computer Speech and Lan-
guage, 12(3):317?348.
Automatic diacritization of Arabic for Acoustic Modeling in
Speech Recognition
Dimitra Vergyri
Speech Technology and Research Lab.,
SRI International,
Menlo Park, CA 94025, USA
dverg@speech.sri.com
Katrin Kirchhoff
Department of Electrical Engineering,
University of Washington,
Seattle, WA 98195, USA
katrin@ee.washington.edu
Abstract
Automatic recognition of Arabic dialectal speech is
a challenging task because Arabic dialects are es-
sentially spoken varieties. Only few dialectal re-
sources are available to date; moreover, most avail-
able acoustic data collections are transcribed with-
out diacritics. Such a transcription omits essen-
tial pronunciation information about a word, such
as short vowels. In this paper we investigate var-
ious procedures that enable us to use such train-
ing data by automatically inserting the missing dia-
critics into the transcription. These procedures use
acoustic information in combination with different
levels of morphological and contextual constraints.
We evaluate their performance against manually dia-
critized transcriptions. In addition, we demonstrate
the effect of their accuracy on the recognition perfor-
mance of acoustic models trained on automatically
diacritized training data.
1 Introduction
Large-vocabulary automatic speech recognition
(ASR) for conversational Arabic poses several
challenges for the speech research community.
The most difficult problems in developing highly
accurate speech recognition systems for Arabic
are the predominance of non-diacritized text
material, the enormous dialectal variety, and
the morphological complexity.
Most available acoustic training material for
Arabic ASR is transcribed in the Arabic script
form, which does not include short vowels and
other diacritics that reflect differences in pro-
nunciation, such as the shadda, tanween, etc. In
particular, almost all additional text data that
can easily be obtained (e.g. broadcast news cor-
pora) is represented in standard script form. To
our knowledge, the only available corpus that
does include detailed phonetic information is
the CallHome (CH) Egyptian Colloquial Ara-
bic (ECA) corpus distributed by the Linguis-
tic Data Consortium (LDC). This corpus has
been transcribed in both the script form and
a so-called romanized form, which is an ASCII
representation that includes short vowels and
other diacritic information and thus has com-
plete pronunciation information. It is quite
challenging to create such a transcription: na-
tive speakers of Arabic are not used to writing
their language in a ?romanized? form, or even in
fully diacritized script form. Consequently, this
task is considered almost as difficult as phonetic
transcription. Transcribing a sufficiently large
amount of training data in this way is there-
fore labor-intensive and costly since it involves
(re)-training native speakers for this purpose.
The constraint of having mostly non-
diacritized texts as recognizer training material
leads to problems for both acoustic and lan-
guage modeling. First, it is difficult to train
accurate acoustic models for short vowels if
their identity and location in the signal is not
known. Second, the absence of diacritics leads
to a larger set of linguistic contexts for a given
word form; language models trained on non-
diacritized material may therefore be less pre-
dictive than those trained on diacritized texts.
Both of these factors may lead to a loss in
recognition accuracy. Previous work (Kirchhoff
et al, 2002; Lamel, 2003) has shown that ig-
noring available vowel information does indeed
lead to a significant increase in both language
model perplexity and word error rate. There-
fore, we are interested in automatically deriv-
ing a diacritized transcription from the Arabic
script representation when a manual diacritiza-
tion is not available. Some software companies
(Sakhr, Apptek, RDI) have developed commer-
cial products for the automatic diacritization of
Arabic. However, these products use only text-
based information, such as the syntactic context
and possible morphological analyses of words, to
predict diacritics. In the context of diacritiza-
tion for speech recognition, by contrast, acous-
tic data is available that can be used as an ad-
ditional knowledge source. Moreover, commer-
cial products concentrate exclusively on Modern
Standard Arabic (MSA), whereas a common ob-
jective of Arabic ASR is conversational speech
recognition, which is usually dialectal. For this
reason, a more flexible set of tools is required
in order to diacritize dialectal Arabic prior to
speech recognizer training.
In this work we investigate the relative ben-
efits of a variety of knowledge sources (acous-
tic, morphological, and contextual) to automat-
ically diacritize MSA transcriptions. We eval-
uate the different approaches in two different
ways: (a) by comparing the automatic output
against a manual reference diacritization and
computing the diacritization error rate, and (b)
by using automatically diacritized training data
in a cross-dialectal speech recognition applica-
tion.
The remainder of this paper is structured as
follows: Section 2 gives a detailed description of
the motivation as well as prior work. Section 3
describes the corpora used for the experiments
reported in this paper. The automatic diacriti-
zation procedures and results are explained in
Section 4. The speech recognition experiments
and results are reported in Section 5. Section 6
presents our conclusions.
2 Motivation and Prior Work
We first describe the Arabic writing system
and its inherent problems for speech recognizer
training, and then discuss previous attempts at
automatic diacritization.
2.1 The Arabic Writing System
The Arabic alphabet consists of twenty-eight
letters, twenty-five of which represent conso-
nants and three of which represent the long
vowels (/i:/,/a:/,/u:/). A distinguishing fea-
ture of Arabic-script based writing systems is
that short vowels are not represented by the
letters of the alphabet. Instead, they are
marked by so-called diacritics, short strokes
placed either above or below the preceding con-
sonant. Several other pronunciation phenom-
ena are marked by diacritics, such as consonant
doubling (phonemic in Arabic), which is indi-
cated by the ?shadda? sign, and the ?tanween?,
i.e. word-final adverbial markers that add /n/ to
the pronunciation of the word. These diacritics
are listed in Table 1. Arabic texts are almost
never fully diacritized; normally, diacritics are
used sparingly and only to prevent misunder-
standings. Exceptions are important religious
and/or political texts or beginners? texts for
MSA Symbol Name Meaning

@ fatHa /a/
@

kasra /i/

@ Damma /u/

P shadda consonant doubling
?

PX sukuun vowel absence

@ tanween al-fatHa /an/
@

tanween al-kasr /in/

@ tanween aD-Damm /un/
Table 1: Arabic diacritics
students of Arabic. The lack of diacritics may
lead to considerable lexical ambiguity that must
be resolved by contextual information, which
in turn presupposes knowledge of the language.
It was observed in (Debili et al, 2002) that
a non-diacritized dictionary word form has 2.9
possible diacritized forms on average and that
an Arabic text containing 23,000 word forms
showed an average ratio of 1:11.6. The form
I
.

J?, for instance, has 21 possible diacritiza-
tions. The correspondence between graphemes
and phonemes is relatively transparent com-
pared to other languages like English or French:
apart from certain special graphemes (e.g. laam
alif), the relationship is one to one. Finally,
it is worth noting that the writing system de-
scribed above is that of MSA. Arabic dialects
are primarily oral varieties in that they do not
have generally agreed-upon writing standards.
Whenever there is the need to write down di-
alectal speech, speakers will try to approximate
the standard system as far as possible and use a
phonetic spelling for non-MSA or foreign words.
The lack of diacritics in standard Arabic texts
makes it difficult to use non-diacritized text for
training since the location and identity of short
vowels and other phonetic segments are un-
known. One possible approach is to use acous-
tic models for long vowels and consonants only,
where the acoustic signal portions correspond-
ing to unwritten segments are implicitly incor-
porated into the acoustic models for consonants
(Billa et al 2002). However, this leads to less
discriminative acoustic and language models.
Previous work (Kirchhoff et al, 2002; Lamel,
2003) has compared the word error rates of
two CH ECA recognizers: one trained on script
transcriptions and another trained on roman-
ized transcriptions. It was shown that the loss
in information due to training on script forms
results in significantly worse performance: a rel-
ative increase in word error rate of almost 10%
was observed.
It seems clear that diacritized data should be
used for training Arabic ASR systems whenever
possible. As explained above, however, it is very
expensive to obtain manually transcribed data
in a diacritized form. Therefore, the corpora
that do include detailed transcriptions are fairly
small and any dialectal data that might become
available in the future will also very likely be
of limited size. By contrast, it is much easier
to collect publicly available data (e.g. broadcast
news data) and to transcribe it in script form.
In order to be able to take advantage of such
resources, we need to restore short vowels and
other missing diacritics in the transcription.
2.2 Prior Work
Various software companies have developed
automatic diacritization products for Arabic.
However, all of these are targeted towards MSA;
to our knowledge, there are no products for di-
alectal Arabic. In a previous study (Kirchhoff
et al, 2002) one of these products was tested
on three different texts, two MSA texts and one
ECA text. It was found that the diacritization
error rate (percentage of missing and wrongly
identified or inserted diacritics) on MSA ranged
between 9% and 28%, depending on whether or
not case vowel endings were counted. However,
on the ECA text, the diacritization software ob-
tained an error rate of 48%.
A fully automatic approach to diacritization
was presented in (Gal, 2002), where an HMM-
based bigram model was used for decoding
diacritized sentences from non-diacritized sen-
tences. The technique was applied to the Quran
and achieved 14% word error (incorrectly dia-
critized words).
A first attempt at developing an automatic
diacritizer for dialectal speech was reported in
(Kirchhoff et al, 2002). The basic approach
was to use a small set of parallel script and dia-
critized data (obtained from the ECA CallHome
corpus) and to derive diacritization rules in an
example-based way. This entirely knowledge-
free approach achieved a 16.6% word error rate.
Other studies (El-Imam, 2003) have ad-
dressed problems of grapheme-to-phoneme con-
version in Arabic, e.g. for the purpose of speech
synthesis, but have assumed that a fully dia-
critized version of the text is already available.
Several knowledge sources are available for
determining the most appropriate diacritization
of a script form: analysis of the morphological
structure of the word (including segmentation
into stems, prefixes, roots and patterns), con-
sideration of the syntactic context in which the
word form appears, and, in the context of speech
recognition, the acoustic data that accompanies
the transcription. Specific dictionary informa-
tion could in principle be added (such as infor-
mation about proper names), but this knowl-
edge source is ignored for the purpose of this
study. All of the approaches described above
make use of text-based information only and do
not attempt to use acoustic information.
3 Data
For the present study we used two different cor-
pora, the FBIS corpus of MSA speech and the
LDC CallHome ECA corpus.
The FBIS corpus is a collection of radio news-
casts from various radio stations in the Ara-
bic speaking world (Cairo, Damascus, Bagh-
dad) totaling approximately 40 hours of speech
(roughly 240K words). The transcription of the
FBIS corpus was done in Arabic script only
and does not contain any diacritic information.
There were a total of 54K different script forms,
with an average of 2.5 different diacritizations
per word.
The CallHome corpus, made available by
LDC, consists of informal telephone conversa-
tions between native speakers (friends and fam-
ily members) of Egyptian Arabic, mostly from
the Cairene dialect region. The corpus con-
sists of about 20 hours of training data (roughly
160K words) and 6 hours of test data. It is tran-
scribed in two different ways: (a) using stan-
dard Arabic script, and (b) using a romaniza-
tion scheme developed at LDC and distributed
with the corpus. The romanized transcription
contains short vowels and phonetic segments
corresponding to other diacritics. It is not en-
tirely equivalent to a diacritized Arabic script
representation since it includes additional in-
formation. For instance, symbols particular to
Egyptian Arabic were used (e.g. ?g? for /g/,
the ECA pronunciation of the MSA letter `),
whereas the script transcriptions contain MSA
letters only. In general, the romanized tran-
scription provides more information about ac-
tual pronunciation and is thus closer to a broad
phonetic transcription.
4 Automatic Diacritization
We describe three techniques for the automatic
diacritization of Arabic text data. The first
combines acoustic, morphological and contex-
tual information to predict the correct form, the
second ignores contextual information, and the
third is fully acoustics based. The latter tech-
nique uses no morphological or syntactic con-
straints, and allows for all possible items to be
inserted at every possible position.
4.1 Combination of Acoustic,
Morphological and Contextual
Information
Most Arabic script forms can have a number
of possible morphological interpretations, which
often correspond to different diacritized forms.
Our goal is to combine morphological knowledge
with contextual information in order to identify
possible diacritizations and assign probabilities
to them. Our procedure is as follows:
1. Generate all possible diacritized variants
for each word, along with their morphological
analyses (tags).
2. Train an unsupervised tagger to assign
probabilities to sequences of these morpholog-
ical tags.
3. Use the trained tagger to assign proba-
bilities to all possible diacritizations for a given
utterance.
For the first step we used the Buckwalter
stemmer, which is an Arabic morphological
analysis tool available from the LDC. The stem-
mer produces all possible morphological anal-
yses of a given Arabic script form; as a by-
product it also outputs the concomitant dia-
critized word forms. An example of the output
is shown in Figure 1. The next step was to train
an unsupervised tagger on the output to obtain
tag n-gram probabilities. The number of differ-
ent morphological tags generated by applying
the stemmer to the FBIS text was 763. In or-
der to obtain a smaller tag set and to be able
to estimate probabilities for tag sequences more
robustly, this initial tag needed to be conflated
to a smaller set. We adopted the set used in
the LDC Arabic TreeBank project, which was
also developed based on the Buckwalter mor-
phological analysis scheme. The FBIS tags were
mapped to TreeBank tags using longest com-
mon substring matching; this resulted in 392
tags. Further possible reductions of the tag
set were investigated but it was found that too
much clustering (e.g. of verb subclasses into a
LOOK-UP WORD: ?J
.

? (qbl)
SOLUTION 1: (qabola) qabola/PREP
(GLOSS): + before +
SOLUTION 2: (qaboli) qaboli/PREP
(GLOSS): + before +
SOLUTION 3: (qabolu) qabolu/ADV
(GLOSS): + before/prior +
SOLUTION 4:(qibal) qibal/NOUN
(GLOSS): + (on the) part of +
SOLUTION 5:(qabila)
qabil/VERB PERFECT+a/PVSUFF SUBJ:3MS
(GLOSS): + accept/receive/approve + he/it <verb>
SOLUTION 6: (qab?ala)
qab al/VERB PERFECT+a/PVSUFF SUBJ:3MS
(GLOSS): + kiss + he/it <verb>
Figure 1: Sample output of Buckwalter stem-
mer showing the possible diacritizations and
morphological analyses of the script form ?J
.

?
(qbl). Lower-case o stands for sukuun (lack of
vowel).
single verb class) could result in the loss of im-
portant information. For instance, the tense
and voice features of verbs are strong predictors
of the short vowel patterns and should therefore
be preserved in the tagset.
We adopted a standard statistical trigram
tagging model:
P (t0, . . . , tn|w0, . . . , wn) =
n?
i=0
P (wi|ti)P (ti|ti?1, ti?2) (1)
where t is a tag, w is a word, and n is the to-
tal number of words in the sentence. In this
model, words (i.e. non-diacritized script forms)
and morphological tags are treated as observed
random variables during training. Training is
done in an unsupervised way, i.e. the correct
morphological tag assignment for each word is
not known. Instead, all possible assignments
are initially considered and the Expectation-
Maximization (EM) training procedure itera-
tively trains the probability distributions in the
above model (the probability of word given
tag, P (wi|ti), and the tag sequence probabil-
ity, P (ti|ti?1, ti?2)) until convergence. During
testing, only the word sequence is known and
the best tag assignment is found by maximiz-
ing the probability in Equation 1. We used the
graphical modeling toolkit GMTK (Bilmes and
Zweig, 2002) to train the tagger. The trained
tagger was then used to assign probabilities to
all possible sequences of three successive mor-
phological tags and their associated diacritiza-
tions to all utterances in the FBIS corpus.
Using the resulting possible diacritizations
for each utterance we constructed a word-
pronunciation network with the probability
scores assigned by the tagger acting as transi-
tion weights. These word networks were used
as constraining recognition networks with the
acoustic models trained on the CallHome cor-
pus to find the most likely word sequence (a
process called alignment). We performed this
procedure with different weights on the tagger
probabilities to see how much this information
should be weighted compared to the acoustic
scores. Results for weights 1 and 5 are reported
below.
Since the Buckwalter stemmer does not pro-
duce case endings, the word forms obtained
by adding case endings were included as vari-
ants in the pronunciation dictionary used by the
aligner. Additional variants listed in the dictio-
nary are the taa marbuta alternations /a/ and
/at/. In some cases (approximately 1.5% of all
words) the Buckwalter stemmer was not able to
produce an analysis of the word form due to mis-
spellings or novel words. These were mapped to
a generic reject model.
4.2 Combination of Acoustic and
Morphological Constraints
We were interested in separately evaluating the
usefulness of the probabilistic contextual knowl-
edge provided by the tagger, and the morpho-
logical knowledge contributed by the Buckwal-
ter tool. To that end we used the word networks
produced by the method described above but
stripped the tagger probabilities, thus assigning
uniform probability to all diacritized forms pro-
duced by the morphological analyzer. We used
the same acoustic models to find the most likely
alignment from the word networks.
4.3 Using only Acoustic Information
Similarly, we wanted to evaluate the importance
of using morphological information versus only
acoustic information to constrain the possible
diacritizations. This is particularly interesting
since, as new dialectal speech data become avail-
able, the acoustics may be the only informa-
tion source. As explained above, existing mor-
phological analysis tools such as the Buckwalter
stemmer have been developed for MSA only.
For that purpose, we generated word net-
works that include all possible short vowels at
each allowed position in the word and allowed
all possible case endings. This means that af-
ter every consonant there are at least 5 dif-
ferent choices: no vowel (corresponding to the
sukuun diacritic), /i/, /a/, /u/, or consonant
doubling caused by a shadda sign. Combina-
tions of shadda and a short vowel are also pos-
sible. Since we do not use acoustic models for
doubled consonants in our speech recognizer, we
ignore the variants involving shadda and allow
only four possibilities after every word-medial
consonant: the three short vowels or absence of
a vowel. Finally, we include the three tanween
endings in addition to these four possibilities in
word-final position. As before, the taa marbuta
variants are also included.
In this way, many more possible ?pronuncia-
tions? are generated for a script form than could
ever occur. The number of possible variants in-
creases exponentially with the number of pos-
sible vowel slots in the word. For instance, for
a longer word with 7 possible positions, more
than 16K diacritized forms are possible, not
even counting the possible word endings. As be-
fore, we use these large pronunciation networks
to constrain our alignment with acoustic models
trained on CallHome data and choose the most
likely path as the output diacritization.
In principle it would also be possible to deter-
mine diacritization performance in the absence
of acoustic information, using only morphologi-
cal and contextual knowledge. This can be done
by selecting the best path from the weighted
word transition networks without rescoring the
network with acoustic models. However, this
would not lead to a valid comparison in our case
because case endings are only represented in the
pronunciation dictionary used by the acoustic
aligner; they are not present in the weighted
transition network and thus cannot be hypoth-
esized unless the acoustic aligner is used.
4.4 Autodiacritization Error Rates
We measured the performance of all three meth-
ods by comparing the output against hand tran-
scribed references on a 500 word subset of the
FBIS corpus. These references were fully dia-
critized script transcriptions created by a na-
tive speaker of Arabic who was trained in or-
thographic transcription but not in phonetic
transcription. The diacritization error rate was
measured as the percentage of wrong diacritiza-
tion decisions out of all possible decisions. In
particular, an error occurs when:
? a vowel is inserted although the reference
transcription shows either sukuun or no dia-
critic mark at the corresponding position (in-
sertion).
? no vowel is produced by the automatic pro-
cedure but the reference contains a vowel mark
at the corresponding position (deletion).
? the short vowel inserted does not match the
vowel at the corresponding position (substitu-
tion).
? in the case of tanween and taa marbuta end-
ings, either the required consonants or vowels
are missing or wrongly inserted. Thus, in the
case of a taa marbuwta ending with a following
case vowel /i/, for instance, both the /t/ and
the /i/ need to be present. If either is missing,
one error is assigned; if both are missing, two
errors are assigned.
Results are listed in Table 2. The first column
reports the error rate at the word level, i.e. the
percentage of words that contained at least one
diacritization mistake. The second column lists
the diacritization error computed as explained
above. The first three methods have a very sim-
ilar performance with respect to diacritization
error rate. The use of contextual information
(the tagger probabilities) gives a slight advan-
tage, although the difference is not statistically
significant. Despite these small differences, the
word error rate is the same for all three meth-
ods; this is because a word that contains at least
one mistake is counted as a word error, regard-
less of the total number of mistakes in the word,
which may vary from system to system. Using
only acoustic information doubles the diacriti-
zation error rate and increases the word error
rate to 50%. Errors result mostly from incorrect
insertions of vowels (e.g. X@

Y
	
?

K
.
? X@

Y

	
?

K
.
). Many
of these insertions may stem from acoustic ef-
fects created by neighbouring consonants, that
give a vowel-like quality to transitions between
consonants. The main benefit of using morpho-
logical knowledge lies in the prevention of such
spurious vowel insertions, since only those inser-
tions are permitted which result in valid words.
Even without the use of morphological infor-
mation, the vast majority of the missing vowels
are still identified correctly. Thus, this method
might be of use when diacritizing a variety of
Arabic for which morphological analysis tools
are not available. Note that the results obtained
here are not directly comparable to any of the
works described in Section 2.2 since we used a
data set with a much larger vocabulary size.
Word Character
Information used level level
acoustic + morphological
+ contextual 27.3 13.24
(tagger prob. weight=5)
acoustic + morphological
+ contextual 27.3 11.54
(tagger prob. weight=1)
acoustic + morphological
(tagger prob. weight=0) 27.3 11.94
acoustic only 50.0 23.08
Table 2: Automatic diacritization error rates
(%).
5 ASR Experiments
Our overall goal is to use large amounts of MSA
acoustic data to enrich training material for a
speech recognizer for conversational Egyptian
Arabic. The ECA recognizer was trained on the
romanized transcription of the CallHome cor-
pus described above and uses short vowel mod-
els. In order to be able to use the phonetically
deficient MSA transcriptions, we first need to
convert them to a diacritized form. In addition
to measuring autodiacritization error rates, as
above, we would like to evaluate the different
diacritization procedures by investigating how
acoustic models trained on the different outputs
affect ASR performance.
One motivation for using cross-dialectal data
is the assumption that infrequent triphones in
the CallHome corpus might have more training
samples in the larger MSA corpus. In (Kirch-
hoff and Vergyri, 2004) we demonstrated that
it is possible to get a small improvement in this
task by combining the scores of models trained
strictly on CallHome (CH) with models trained
on the combined FBIS+CH data, where the
FBIS data was diacritized using the method de-
scribed in Section 4.1. Here we compare that ex-
periment with the experiments where the meth-
ods described in Sections 4.2 and 4.3 were used
for diacritizing the FBIS corpus.
5.1 Baseline System
The baseline system was trained with only
CallHome data (CH-only). For these exper-
iments we used a single front-end (13 mel-
frequency cepstral coefficients with first and
second differences). Mean and variance as
well as Vocal Tract Length (VTL) normaliza-
tion were performed per conversation side for
CH and per speaker cluster (obtained auto-
matically) for FBIS. We trained non-crossword,
System dev96 eval03
simple CH-only 56.1 42.7
RT-2003 CH-only 52.6 39.7
Table 3: CH-only baseline WER (%)
continuous-density, genonic hidden Markov
models (HMMs) (Digalakis and Murveit, 1994),
with 128 gaussians per genone and 250 genones.
Recognition was done by SRI?s DECIPHERTM
engine in a multipass approach: in the first
pass, phone-loop adaptation with two Max-
imum Likelihood Linear Regression (MLLR)
transforms was applied. A recognition lexicon
with 18K words and a bigram language model
were used to generate the first pass recogni-
tion hypothesis. In the second pass the acoustic
models were adapted using constrained MLLR
(with 6 transformations) based on the previ-
ous hypothesis. Bigram lattices were generated
and then expanded using a trigram language
model. Finally, N-best lists were generated us-
ing the adapted models and the trigram lattices.
The final best hypothesis was obtained using N-
best ROVER (?). This system is simpler than
our best current recognition system (submitted
for the NIST RT-2003 benchmark evaluations)
(Stolcke et al, 2003) since we used a single front
end (instead of a combination of systems based
on different front ends) and did not include
HLDA, cross-word triphones, MMIE training
or a more complex language model. The lack
of these features resulted in a higher error rate
but our goal here was to explore exclusively the
effect of the additional MSA training data us-
ing different diacritization approaches. Table 3
shows the word error rates of the system used
for these experiments and the full system used
for the NIST RT-03 evaluations. Our full sys-
tem was about 2% absolute worse than the best
system submitted for that task. This shows that
even though the system is simpler we are not
operating far from the state-of-the-art perfor-
mance for this task.
5.2 ASR Systems Using FBIS Data
In order to investigate the effect of additional
MSA training data, we trained a system similar
to the baseline but used training data pooled
from both corpora (CH+FBIS). After perform-
ing alignment of the FBIS data with the net-
works described in Section 4.1, 10% of the data
was discarded since no alignments could be
found. This could be due to segmentation prob-
lems or noise in the acoustic files. The remain-
ing 90% were used for our experiments. In or-
der to account for the fact that we had much
more data, and also more dissimilar data, we
increased the model size to 300 genones.
For training the CH+FBIS acoustic models,
we first used the whole data set with weight
2 for CH utterances and 1 for FBIS utterances.
Models were then MAP adapted on the CH-only
data (Digalakis et al, 1995). Since training in-
volves several EM iterations, we did not want
to keep the diacritization fixed from the first
pass, which used CH-only models. At every it-
eration, we obtain better acoustic models which
can be used to re-align the data. Thus, for the
first two approaches, where the size of the pro-
nunciation networks is limited due to the use
of morphological information, the EM forward-
backward counts were collected using the whole
diacritization network and the best diacritiza-
tion path was allowed to change at every iter-
ation. In the last case, where only acoustic in-
formation was used, the pronunciation networks
were too large to be run efficiently. For this rea-
son, we updated the diacritized references once
during training by realigning the networks with
the newer models after the first training iter-
ation. As reported in (Kirchhoff and Vergyri,
2004) the CH+FBIS trained system by itself did
not improve much over the baseline (we only
found a small improvement on the eval03 test-
set) but it provided sufficiently different infor-
mation, so that ROVER combination (Fiscus,
1997) with the baseline yielded an improvement.
As we can see in Table 4, all diacritization pro-
cedures performed practically the same: there
was no significant difference in the word error
rates obtained after the combination with the
CH-only baseline. This suggests that we may
be able to obtain improvements with automat-
ically diacritized data even when using inaccu-
rate diacritization, produced without the use of
morphological constraints.
6 Conclusions
In this study we have investigated different op-
tions for automatically diacritizing Arabic text
for use in acoustic model training for ASR. A
comparison of the different approaches showed
that more linguistic information (morphology
and syntactic context) in combination with
the acoustics provides lower diacritization er-
ror rates. However, there is no significant dif-
ference among the word error rates of ASR sys-
dev96 eval03
System alone Rover with CH-only alone Rover with CH-only
CH-only 56.1 42.7
CH+FBIS1(weight 1) 56.3 55.3 42.2 41.6
CH+FBIS1(weight 5) 56.1 55.2 42.2 41.8
CH+FBIS2 56.2 55.3 42.4 41.6
CH+FBIS3 56.6 55.7 42.1 41.6
Table 4: Word error rates (%) obtained after the final recognition pass and with ROVER combina-
tion with the baseline system. FBIS1, FBIS2 and FBIS3 correspond to the diacritization procedures
described in Sections 4.1, 4.2 and 4.3 respectively. For the first approach we report results using
the tagger probabilities with weights 1 and 5.
tems trained on data resulting from the different
methods. This result suggests that it is pos-
sible to use automatically diacritized training
data for acoustic modeling, even if the data has
a comparatively high diacritization error rate
(23% in our case). Note, however, that one
reason for this may be that the acoustic mod-
els are finally adapted to the accurately tran-
scribed CH-only data. In the future, we plan to
apply knowledge-poor diacritization procedures
to other dialects of Arabic, for which morpho-
logical analyzers do not exist.
7 Acknowledgments
This work was funded by DARPA under con-
tract No. MDA972-02-C-0038. We are grateful
to Kathleen Egan for making the FBIS corpus
available to us, and to Andreas Stolcke and Jing
Zheng for valuable advice on several aspects of
this work.
References
J. Billa et al 2002. Audio indexing of Broad-
cast News. In Proceedings of ICASSP.
J. Bilmes and G. Zweig. 2002. The Graphical
Models Toolkit: An open source software sys-
tem for speech and time-series processing. In
Proceedings of ICASSP.
F. Debili, H. Achour, and E Souissi. 2002. De
l?e?tiquetage grammatical a` la voyellation au-
tomatique de l?arabe. Technical report, Cor-
respondances de l?Institut de Recherche sur
le Maghreb Contemporain.
V. Digalakis and H. Murveit. 1994.
GENONES: Optimizing the degree of
mixture tying in a large vocabulary hidden
markov model based speech recognizer. In
Proceeding of ICASSP, pages I?537?540.
V.V. Digalakis, D. Rtischev, and L. G.
Neumeyer. 1995. Speaker adaptation using
constrained estimation of gaussian mixtures.
IEEE Transactions SAP, 3:357?366.
Yousif A. El-Imam. 2003. Phonetization of
Arabic: rules and algorithms. Computer,
Speech and Language, in press, preprint avail-
able online at www.sciencedirect.com.
J. G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer
output voting error reduction (ROVER). In
Proceedings IEEE Automatic Speech Recog-
nition and Understanding Workshop, pages
347?352, Santa Barbara, CA.
Ya?akov Gal. 2002. An HMM approach to vowel
restoration in Arabic and Hebrew. In Pro-
ceedings of the Workshop on Computational
Approaches to Semitic Languages, pages 27?
33, Philadelphia, July. Association for Com-
putational Linguistics.
K. Kirchhoff and D. Vergyri. 2004. Cross-
dialectal acoustic data sharing for Ara-
bic speech recognition. In Proceedings of
ICASSP.
K. Kirchhoff, J. Bilmes, J. Henderson,
R. Schwartz, M. Noamany, P. Schone, G. Ji,
S. Das, M. Egan, F. He, D. Vergyri, D. Liu,
and N. Duta. 2002. Novel approaches to Ara-
bic speech recognition - final report from the
JHU summer workshop 2002. Technical re-
port, Johns Hopkins University.
L. Lamel. 2003. Personal communication.
A. Stolcke, Y. Konig, and M. Weintraub. 1997.
Explicit word error minimization in N-best
list rescoring. In Proceedings of Eurospeech,
volume 1, pages 163?166.
A. Stolcke et al 2003. Speech-to-text re-
search at sri-icsi-uw. Technical report, NIST
RT-03 Spring Workshop. availble online
http://www.nist.gov/speech/tests/rt/rt2003/
spring/presentations/sri+-rt03-stt.pdf.
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 55?62,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
POS Tagging of Dialectal Arabic: A Minimally Supervised Approach
Kevin Duh and Katrin Kirchhoff
Department of Electrical Engineering
University of Washington, Seattle, WA, 98195
{duh,katrin}@ee.washington.edu
Abstract
Natural language processing technology
for the dialects of Arabic is still in its
infancy, due to the problem of obtaining
large amounts of text data for spoken Ara-
bic. In this paper we describe the de-
velopment of a part-of-speech (POS) tag-
ger for Egyptian Colloquial Arabic. We
adopt a minimally supervised approach
that only requires raw text data from sev-
eral varieties of Arabic and a morpholog-
ical analyzer for Modern Standard Ara-
bic. No dialect-specific tools are used. We
present several statistical modeling and
cross-dialectal data sharing techniques to
enhance the performance of the baseline
tagger and compare the results to those
obtained by a supervised tagger trained
on hand-annotated data and, by a state-of-
the-art Modern Standard Arabic tagger ap-
plied to Egyptian Arabic.
1 Introduction
Part-of-speech (POS) tagging is a core natural lan-
guage processing task that can benefit a wide range
of downstream processing applications. Tagging
is often the first step towards parsing or chunking
(Osborne, 2000; Koeling, 2000), and knowledge
of POS tags can benefit statistical language mod-
els for speech recognition or machine translation
(Heeman, 1998; Vergyri et al, 2004). Many ap-
proaches for POS tagging have been developed in
the past, including rule-based tagging (Brill, 1995),
HMM taggers (Brants, 2000; Cutting and oth-
ers, 1992), maximum-entropy models (Rathnaparki,
1996), cyclic dependency networks (Toutanova et
al., 2003), memory-based learning (Daelemans et
al., 1996), etc. All of these approaches require ei-
ther a large amount of annotated training data (for
supervised tagging) or a lexicon listing all possible
tags for each word (for unsupervised tagging). Tag-
gers have been developed for a variety of languages,
including Modern Standard Arabic (MSA) (Khoja,
2001; Diab et al, 2004). Since large amount of text
material as well as standard lexicons can be obtained
in these cases, POS tagging is a straightforward task.
The dialects of Arabic, by contrast, are spoken
rather than written languages. Apart from small
amounts of written dialectal material in e.g. plays,
novels, chat rooms, etc., data can only be obtained
by recording and manually transcribing actual con-
versations. Moreover, there is no universally agreed
upon writing standard for dialects (though several
standardization efforts are underway); any large-
scale data collection and transcription effort there-
fore requires extensive training of annotators to en-
sure consistency. Due to this data acquisition bot-
tleneck, the development of NLP technology for di-
alectal Arabic is still in its infancy. In addition to the
problems of sparse training data and lack of writing
standards, tagging of dialectal Arabic is difficult for
the following reasons:
? Resources such as lexicons, morphological an-
alyzers, tokenizers, etc. are scarce or non-
existent for dialectal Arabic.
? Dialectal Arabic is a spoken language. Tagging
spoken language is typically harder than tag-
55
ging written language, due to the effect of dis-
fluencies, incomplete sentences, varied word
order, etc.
? The rich morphology of Arabic leads to a
large number of possible word forms, which
increases the number of out-of-vocabulary
(OOV) words.
? The lack of short vowel information results in
high lexical ambiguity.
In this paper we present an attempt at developing
a POS tagger for dialectal Arabic in a minimally
supervised way. Our goal is to utilize existing re-
sources and data for several varieties of Arabic in
combination with unsupervised learning algorithms,
rather than developing dialect-specific tools. The
resources available to us are the CallHome Egyp-
tian Colloquial Arabic (ECA) corpus, the LDC Lev-
antine Arabic (LCA) corpus, the LDC MSA Tree-
bank corpus, and a generally available morpholog-
ical analysis tool (the LDC-distributed Buckwalter
stemmer) for MSA. The target dialect is ECA, since
this is the only dialectal corpus for which POS an-
notations are available. Our general approach is
to bootstrap the tagger in an unsupervised way us-
ing POS information from the morphological ana-
lyzer, and to subsequently improve it by integrating
additional data from other dialects and by general
machine learning techniques. We compare the re-
sult against the performance of a tagger trained in a
supervised way and an unsupervised tagger with a
hand-developed ECA lexicon.
2 Data
The ECA corpus consists of telephone conversations
between family members or close friends, with one
speaker being located in the U.S. and the other in
Egypt. We use the combined train, eval96 and hub5
sections of the corpus for training, the dev set for
development and the eval97 set for evaluation. The
LCA data consists of telephone conversations on
pre-defined topics between Levantine speakers pre-
viously unknown to each other; all of the available
data was used. The Treebank corpus is a collection
of MSA newswire text from Agence France Press,
An Nahar News, and Unmah Press. We use parts 1
(v3.0), 2 (v2.0) and 3 (v1.0). The sizes of the vari-
ous corpora are shown in Table 1.
The ECA corpus was originally transcribed in a ?ro-
manized? form; a script representation was then de-
rived automatically from the romanized transcripts.
The script, therefore, does not entirely conform to
the MSA standard: romanized forms may repre-
sent actual pronunciations and contain such MSA
? ECA changes as /?/ ? /s/ or /t/ and /?/ ? /z/
or /d/. The resulting representation cannot be unam-
biguously mapped back to MSA script; the variants
/s/ or /t/, for instance, are represented by   or  ,
rather than  . This introduces additional noise into
the data, but it also mimics the real-world situation
of variable spelling standards that need to be handled
by a robust NLP system. We use the script represen-
tation of this corpus for all our experiments. The
ECA corpus is accompanied by a lexicon contain-
ing the morphological analysis of all words, i.e. an
analysis in terms of stem and morphological charac-
teristics such as person, number, gender, POS, etc.
Since the analysis is based on the romanized form, a
single tag can be assigned to the majority of words
(75% of all tokens) in the corpus. We use this assign-
ment as the reference annotation for our experiments
to evaluate the output of our tagger. The remaining
25% tokens (ambiguous words) have 2 or more tags
in the lexicon and are thus ignored during evaluation
since the correct reference tag cannot be determined.
Both the LCA and the MSA Treebank data sets
were transcribed in standard MSA script. The LCA
corpus only consists of raw orthographic transcrip-
tions; no further annotations exist for this data set.
Each word in the Treebank corpus is associated with
all of its possible POS tags; the correct tag has been
marked manually. We use the undecomposed word
forms rather than the forms resulting from splitting
off conjunctions, prepositions, and other clitics. Al-
though improved tokenization can be expected to
result in better tagging performance, tokenization
tools for dialectal Arabic are currently not available,
and our goal was to create comparable conditions
for tagging across all of our data sets. Preprocessing
of the data thus only included removing punctuation
from the MSA data and removing word fragments
from the spoken language corpora. Other disfluen-
cies (fillers and repetitions) were retained since they
are likely to have predictive value. Finally, single-
ton words (e.g. inconsistent spellings) were removed
56
from the LCA data. The properties of the different
data sets (number of words, n-grams, and ambigu-
ous words) are displayed in Table 1.
ECA LCA MSA
train dev test
sentences 25k 6k 2.7k 114k 20k
# tokens 156k 31k 12k 476k 552k
# types 15k 5k 1.5k 16k 65k
# bigrams 81k 20k 7k 180k 336k
# trigrams 125k 26k 10k 320k 458k
% ambig. 24.4 27.8 28.2 ? ?
Table 1: Corpus statistics for ECA, LCA and MSA.
The only resource we utilize in addition to raw
data is the LDC-distributed Buckwalter stemmer.
The stemmer analyzes MSA script forms and out-
puts all possible morphological analyses (stems and
POS tags, as well as diacritizations) for the word.
The analysis is based on an internal stem lexi-
con combined with rules for affixation. Although
the stemmer was developed primarily for MSA, it
can accommodate a certain percentage of dialectal
words. Table 2 shows the percentages of word types
and tokens in the ECA and LCA corpora that re-
ceived an analysis from the Buckwalter stemmer.
Since both sets contain dialectal as well as standard
MSA forms, it is not possible to determine precisely
how many of the unanalyzable forms are dialectal
forms vs. words that were rejected for other rea-
sons, such as misspellings. The higher percentage
of rejected word types in the ECA corpus is most
likely due to the non-standard script forms described
above.
Type Token
N ECA LCA ECA LCA
0 37.6 23.3 18.2 28.2
1 34.0 52.5 33.6 40.4
2 19.4 17.7 26.4 19.9
3 7.2 5.2 16.2 10.5
4 1.4 1.0 5.0 2.3
5 0.1 0.1 0.4 0.6
Table 2: Percentage of word types/tokens with N possible
tags, as determined by the Buckwalter stemmer. Words with
0 tags are unanalyzable.
The POS tags used in the LDC ECA annota-
tion and in the Buckwalter stemmer are rather fine-
grained; furthermore, they are not identical. We
therefore mapped both sets of tags to a unified, sim-
pler tagset consisting only of the major POS cate-
gories listed in Table 2. The mapping from the orig-
inal Buckwalter tag to the simplified set was based
on the conversion scheme suggested in (Bies, 2003).
The same underlying conversion rules were applica-
ble to most of the LDC tags; those cases that could
not be determined automatically were converted by
hand.
Symbol Gloss (%)
CC coordinating conjunction 7.15
DT determiner 2.23
FOR foreign word 1.18
IN preposition 7.46
JJ adjective 6.02
NN noun 19.95
NNP proper noun 3.55
NNS non-singular noun 3.04
NOTAG non-word 0.05
PRP pronoun 5.85
RB adverb 4.13
RP particle 9.71
UH disfluency, interjection 9.55
VBD perfect verb 6.53
VBN passive verb 1.88
VBP imperfect verb 10.62
WP relative pronoun 1.08
Table 3: Collapsed tagset and percentage of occur-
rence of each tag in the ECA corpus.
Prior to the development of our tagger we com-
puted the cross-corpus coverage of word n-grams
in the ECA development set, in order to verify our
assumption that utilizing data from other dialects
might be helpful. Table 4 demonstrates that the
n-gram coverage of the ECA development set in-
creases slightly by adding LCA and MSA data.
Types Tokens
1gr 2gr 3gr 1gr 2gr 3gr
ECA 64 33 12 94 58 22
LCA 31 9 1.4 69 20 3.7
ECA + LCA 68 35 13 95 60 23
MSA 32 3.7 0.2 66 8.6 0.3
ECA + MSA 71 34 12 95 60 22
Table 4: Percentages of n-gram types and tokens in ECA dev
set that are covered by the ECA training set, the LCA set, com-
bined ECA training + LCA set, and MSA sets. Note that adding
the LCA or MSA improves the coverage slightly.
57
3 Baseline Tagger
We use a statistical trigram tagger in the form of a
hidden Markov model (HMM). Let w0:M be a se-
quence of words (w0, w1, . . . , wM ) and t0:M be the
corresponding sequence of tags. The trigram HMM
computes the conditional probability of the word
and tag sequence p(w0:M , t0:M ) as:
P (t0:M |w0:M ) =
M
?
i=0
p(wi|ti)p(ti|ti?1, ti?2) (1)
The lexical model p(wi|ti) characterizes the dis-
tribution of words for a specific tag; the contex-
tual model p(ti|ti?1, ti?2) is trigram model over
the tag sequence. For notational simplicity, in
subsequent sections we will denote p(ti|ti?1, ti?2)
as p(ti|hi), where hi represents the tag history.
The HMM is trained to maximize the likelihood
of the training data. In supervised training, both
tag and word sequences are observed, so the max-
imum likelihood estimate is obtained by relative fre-
quency counting, and, possibly, smoothing. Dur-
ing unsupervised training, the tag sequences are
hidden, and the Expectation-Maximization Algo-
rithm is used to iteratively update probabilities based
on expected counts. Unsupervised tagging re-
quires a lexicon specifying the set of possible tags
for each word. Given a test sentence w?0:M , the
Viterbi algorithm is used to find the tag sequence
maximizing the probability of tags given words:
t?0:M = argmaxt0:M p(t0:M |w?0:M ). Our taggers
are implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Zweig, 2002), which allows
training a wide range of probabilistic models with
both hidden and observed variables.
As a first step, we compare the performance of
four different baseline systems on our ECA dev set.
First, we trained a supervised tagger on the MSA
treebank corpus (System I), in order to gauge how a
standard system trained on written Arabic performs
on dialectal speech. The second system (System II)
is a supervised tagger trained on the manual ECA
POS annotations. System III is an unsupervised tag-
ger on the ECA training set. The lexicon for this
system is derived from the reference annotations of
the training set ? thus, the correct tag is not known
during training, but the lexicon is constrained by
expert information. The difference in accuracy be-
tween Systems II and III indicates the loss due to the
unsupervised training method. Finally, we trained a
system using only the unannotated ECA data and a
lexicon generated by applying the MSA analyzer to
the training corpus and collecting all resulting tags
for each word. In this case, the lexicon is much less
constrained; moreover, many words do not receive
an output from the stemmer at all. This is the train-
ing method with the least amount of supervision and
therefore the method we are interested in most.
Table 5 shows the accuracies of the four systems
on the ECA development set. Also included is a
breakdown of accuracy by analyzable (AW), unana-
lyzable (UW), and out-of-vocabulary (OOV) words.
Analyzable words are those for which the stemmer
outputs possible analyses; unanalyzable words can-
not be processed by the stemmer. The percent-
age of unanalyzable word tokens in the dev set is
18.8%. The MSA-trained tagger (System I) achieves
an accuracy of 97% on a held-out set (117k words)
of MSA data, but performs poorly on ECA due to
a high OOV rate (43%). By contrast, the OOV
rate for taggers trained on ECA data is 9.5%. The
minimally-supervised tagger (System IV) achieves a
baseline accuracy of 62.76%. In the following sec-
tions, we present several methods to improve this
system, in order to approximate as closely as possi-
ble the performance of the supervised systems. 1
System Total AW UW OOV
I 39.84 55.98 21.05 19.21
II 92.53 98.64 99.09 32.20
III 84.88 90.17 99.11 32.64
IV 62.76 67.07 20.74 21.84
Table 5: Tagging accuracy (%) for the 4 baseline
systems. AW = analyzable words, UW unanalyzable
words, OOV = out-of-vocabulary words.
4 System Improvements
4.1 Adding Affix Features
The low accuracy of unanalyzable and OOV words
may significantly impact downstream applications.
1The accuracy of a naive tagger which labels all words with
the most likely tag (NN) achieves an accuracy of 20%. A tagger
which labels words with the most likely tag amongst its possible
tags achieves an accuracy of 52%.
58
One common way to improve accuracy is to add
word features. In particular, we are interested in
features that can be derived automatically from the
script form, such as affixes. Affix features are
added in a Naive Bayes fashion to the basic HMM
model defined in Eq.1. In addition to the lexical
model p(wi|ti) we now have prefix and suffix mod-
els p(ai|ti) and p(bi|ti), where a and b are the prefix
and suffix variables, respectively. The affixes used
are:   -,   -,  -,  -, 	
 -,   -,   -,  -, - , - , -  ,
-Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125?128,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improved Language Modeling for Statistical Machine Translation
Katrin Kirchhoff and Mei Yang
Department of Electrical Engineering
University of Washington, Seattle, WA, 98195
{katrin,yangmei}@ee.washington.edu
Abstract
Statistical machine translation systems
use a combination of one or more transla-
tion models and a language model. While
there is a significant body of research ad-
dressing the improvement of translation
models, the problem of optimizing lan-
guage models for a specific translation
task has not received much attention. Typ-
ically, standard word trigram models are
used as an out-of-the-box component in
a statistical machine translation system.
In this paper we apply language model-
ing techniques that have proved benefi-
cial in automatic speech recognition to the
ACL05 machine translation shared data
task and demonstrate improvements over a
baseline system with a standard language
model.
1 Introduction
Statistical machine translation (SMT) makes use of
a noisy channel model where a sentence e? in the de-
sired language can be conceived of as originating as
a sentence f? in a source language. The goal is to
find, for every input utterance f? , the best hypothesis
e?? such that
e?? = argmaxe?P (e?|f?) = argmaxe?P (f? |e?)P (e?)
(1)
P (f? |e?) is the translation model expressing proba-
bilistic constraints on the association of source and
target strings. P (e?) is a language model specifying
the probability of target language strings. Usually, a
standard word trigram model of the form
P (e1, ..., el) ?
l
?
i=3
P (ei|ei?1, ei?2) (2)
is used, where e? = e1, ..., el . Each word is predicted
based on a history of two preceding words.
Most work in SMT has concentrated on develop-
ing better translation models, decoding algorithms,
or minimum error rate training for SMT. Compara-
tively little effort has been spent on language mod-
eling for machine translation. In other fields, partic-
ularly in automatic speech recognition (ASR), there
exists a large body of work on statistical language
modeling, addressing e.g. the use of word classes,
language model adaptation, or alternative probabil-
ity estimation techniques. The goal of this study was
to use some of the language modeling techniques
that have proved beneficial for ASR in the past and
to investigate whether they transfer to statistical ma-
chine translation. In particular, this includes lan-
guage models that make use of morphological and
part-of-speech information, so-called factored lan-
guage models.
2 Factored Language Models
A factored language model (FLM) (Bilmes and
Kirchhoff, 2003) is based on a representation of
words as feature vectors and can utilize a variety of
additional information sources in addition to words,
such as part-of-speech (POS) information, morpho-
logical information, or semantic features, in a uni-
fied and principled framework. Assuming that each
125
word w can be decomposed into k features, i.e. w ?
f1:K , a trigram model can be defined as
p(f1:K1 , f1:K2 , ..., f 1:KT ) ?
T
?
t=3
p(f1:Kt |f1:Kt?1 , f1:Kt?2 )
(3)
Each word is dependent not only on a single stream
of temporally preceding words, but also on addi-
tional parallel streams of features. This represen-
tation can be used to provide more robust probabil-
ity estimates when a particular word n-gram has not
been observed in the training data but its correspond-
ing feature combinations (e.g. stem or tag trigrams)
has been observed. FLMs are therefore designed to
exploit sparse training data more effectively. How-
ever, even when a sufficient amount of training data
is available, a language model utilizing morpholog-
ical and POS information may bias the system to-
wards selecting more fluent translations, by boost-
ing the score of hypotheses with e.g. frequent POS
combinations. In FLMs, word feature information
is integrated via a new generalized parallel back-
off technique. In standard Katz-style backoff, the
maximum-likelihood estimate of an n-gram with too
few observations in the training data is replaced with
a probability derived from the lower-order (n ? 1)-
gram and a backoff weight as follows:
pBO(wt|wt?1, wt?2) (4)
=
{
dcpML(wt|wt?1, wt?2) if c > ?
?(wt?1, wt?2)pBO(wt|wt?1) otherwise
where c is the count of (wt, wt?1, wt?2), pML
denotes the maximum-likelihood estimate, ? is a
count threshold, dc is a discounting factor and
?(wt?1, wt?2) is a normalization factor. During
standard backoff, the most distant conditioning vari-
able (in this case wt?2) is dropped first, followed
by the second most distant variable etc., until the
unigram is reached. This can be visualized as a
backoff path (Figure 1(a)). If additional condition-
ing variables are used which do not form a tempo-
ral sequence, it is not immediately obvious in which
order they should be eliminated. In this case, sev-
eral backoff paths are possible, which can be sum-
marized in a backoff graph (Figure 1(b)). Paths in
this graph can be chosen in advance based on lin-
guistic knowledge, or at run-time based on statis-
tical criteria such as counts in the training set. It
tW 1tW? 2tW? 3tW?
tW 1tW? 2tW?
tW 1tW?
tW
(a)
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
(b)
Figure 1: Standard backoff path for a 4-gram lan-
guage model over words (left) and backoff graph
over word features (right).
is also possible to choose multiple paths and com-
bine their probability estimates. This is achieved by
replacing the backed-off probability pBO in Equa-
tion 2 by a general function g, which can be any
non-negative function applied to the counts of the
lower-order n-gram. Several different g functions
can be chosen, e.g. the mean, weighted mean, prod-
uct, minimum or maximum of the smoothed prob-
ability distributions over all subsets of conditioning
factors. In addition to different choices for g, dif-
ferent discounting parameters can be selected at dif-
ferent levels in the backoff graph. One difficulty in
training FLMs is the choice of the best combination
of conditioning factors, backoff path(s) and smooth-
ing options. Since the space of different combina-
tions is too large to be searched exhaustively, we use
a guided search procedure based on Genetic Algo-
rithms (Duh and Kirchhoff, 2004), which optimizes
the FLM structure with respect to the desired crite-
rion. In ASR, this is usually the perplexity of the
language model on a held-out dataset; here, we use
the BLEU scores of the oracle 1-best hypotheses on
the development set, as described below. FLMs have
previously shown significant improvements in per-
plexity and word error rate on several ASR tasks
(e.g. (Vergyri et al, 2004)).
3 Baseline System
We used a fairly simple baseline system trained us-
ing standard tools, i.e. GIZA++ (Och and Ney, 2000)
for training word alignments and Pharaoh (Koehn,
2004) for phrase-based decoding. The training data
126
was that provided on the ACL05 Shared MT task
website for 4 different language pairs (translation
from Finnish, Spanish, French into English); no
additional data was used. Preprocessing consisted
of lowercasing the data and filtering out sentences
with a length ratio greater than 9. The total num-
ber of training sentences and words per language
pair ranged between 11.3M words (Finnish-English)
and 15.7M words (Spanish-English). The develop-
ment data consisted of the development sets pro-
vided on the website (2000 sentences each). We
trained our own word alignments, phrase table, lan-
guage model, and model combination weights. The
language model was a trigram model trained us-
ing the SRILM toolkit, with modified Kneser-Ney
smoothing and interpolation of higher- and lower-
order ngrams. Combination weights were trained
using the minimum error weight optimization pro-
cedure provided by Pharaoh. We use a two-pass de-
coding approach: in the first pass, Pharaoh is run
in N-best mode to produce N-best lists with 2000
hypotheses per sentence. Seven different compo-
nent model scores are collected from the outputs,
including the distortion model score, the first-pass
language model score, word and phrase penalties,
and bidirectional phrase and word translation scores,
as used in Pharaoh (Koehn, 2004). In the second
pass, the N-best lists are rescored with additional
language models. The resulting scores are then com-
bined with the above scores in a log-linear fashion.
The combination weights are optimized on the de-
velopment set to maximize the BLEU score. The
weighted combined scores are then used to select
the final 1-best hypothesis. The individual rescoring
steps are described in more detail below.
4 Language Models
We trained two additional language models to be
used in the second pass, one word-based 4-gram
model, and a factored trigram model. Both were
trained on the same training set as the baseline sys-
tem. The 4-gram model uses modified Kneser-
Ney smoothing and interpolation of higher-order
and lower-order n-gram probabilities. The potential
advantage of this model is that it models n-grams
up to length 4; since the BLEU score is a combina-
tion of n-gram precision scores up to length 4, the
integration of a 4-gram language model might yield
better results. Note that this can only be done in a
rescoring framework since the first-pass decoder can
only use a trigram language model.
For the factored language models, a feature-based
word representation was obtained by tagging the text
with Rathnaparki?s maximum-entropy tagger (Rat-
naparkhi, 1996) and by stemming words using the
Porter stemmer (Porter, 1980). Thus, the factored
language models use two additional features per
word. A word history of up to 2 was considered (3-
gram FLMs). Rather than optimizing the FLMs on
the development set references, they were optimized
to achieve a low perplexity on the oracle 1-best hy-
potheses (the hypotheses with the best individual
BLEU scores) from the first decoding pass. This is
done to avoid optimizing the model on word combi-
nations that might never be hypothesized by the first-
pass decoder, and to bias the model towards achiev-
ing a high BLEU score. Since N-best lists differ for
different language pairs, a separate FLM was trained
for each language pair. While both the 4-gram lan-
guage model and the FLMs achieved a 8-10% reduc-
tion in perplexity on the dev set references compared
to the baseline language model, their perplexities on
the oracle 1-best hypotheses were not significantly
different from that of the baseline model.
5 N-best List Rescoring
For N-best list rescoring, the original seven model
scores are combined with the scores of the second-
pass language models using the framework of dis-
criminative model combination (Beyerlein, 1998).
This approach aims at an optimal (with respect to
a given error criterion) integration of different infor-
mation sources in a log-linear model, whose com-
bination weights are trained discriminatively. This
combination technique has been used successfully
in ASR, where weights are typically optimized to
minimize the empirical word error count on a held-
out set. In this case, we use the BLEU score of
the N-best hypothesis as an optimization criterion.
Optimization is performed using a simplex downhill
method known as amoeba search (Nelder and Mead,
1965), which is available as part of the SRILM
toolkit.
127
Language pair 1st pass oracle
Fi-En 21.8 29.8
Fr-En 28.9 34.4
De-En 23.9 31.0
Es-En 30.8 37.4
Table 1: First-pass (left column) and oracle results
(right column) on the dev set (% BLEU).
Language pair 4-gram FLM both
Fi-En 22.2 22.2 22.3
Fr-En 30.2 30.2 30.4
De-En 24.6 24.2 24.6
Es-En 31.4 31.0 31.3
Table 2: Second-pass rescoring results (% BLEU)
on the dev set for 4-gram LM, 3-gram FLM, and
their combination.
6 Results
The results from the first decoding pass on the de-
velopment set are shown in Table 1. The second
column in Table 1 lists the oracle BLEU scores for
the N-best lists, i.e. the scores obtained by always
selecting the hypothesis known to have the highest
individual BLEU score. We see that considerable
improvements can in principle be obtained by a bet-
ter second-pass selection of hypotheses. The lan-
guage model rescoring results are shown in Table 2,
for both types of second-pass language models indi-
vidually, and for their combination. In both cases we
obtain small improvements in BLEU score, with the
4-gram providing larger gains than the 3-gram FLM.
Since their combination only yielded negligible ad-
ditional improvements, only 4-grams were used for
processing the final evaluation sets. The evaluation
results are shown in Table 3.
Language pair baseline 4-gram
Fi-En 21.6 22.0
Fr-En 29.3 30.3
De-En 24.2 24.8
Es-En 30.5 31.0
Table 3: Second-pass rescoring results (% BLEU)
on the evaluation set.
7 Conclusions
We have demonstrated improvements in BLEU
score by utilizing more complex language models
in the rescoring pass of a two-pass SMT system.
We noticed that FLMs performed worse than word-
based 4-gram models. However, only trigram FLM
were used in the present experiments; larger im-
provements might be obtained by 4-gram FLMs.
The weights assigned to the second-pass language
models during weight optimization were larger than
those assigned to the first-pass language model, sug-
gesting that both the word-based model and the FLM
provide more useful scores than the baseline lan-
guage model. Finally, we observed that the overall
improvement represents only a small portion of the
possible increase in BLEU score as indicated by the
oracle results, suggesting that better language mod-
els do not have a significant effect on the overall sys-
tem performance unless the translation model is im-
proved as well.
Acknowledgements
This work was funded by the National Science
Foundation, Grant no. IIS-0308297. We are grate-
ful to Philip Koehn for assistance with Pharaoh.
References
P. Beyerlein. 1998. Discriminative model combination. In
Proc. ICASSP, pages 481?484.
J.A. Bilmes and K. Kirchhoff. 2003. Factored language mod-
els and generalized parallel backoff. In Proceedings of
HLT/NAACL, pages 4?6.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In Proceedings of COLING.
P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-
based statistical machine translation models. In Proceedings
of AMTA.
J.A. Nelder and R. Mead. 1965. A simplex method for function
minimization. Computing Journal, 7(4):308?313.
F.J. Och and H. Ney. 2000. Giza++: Training of sta-
tistical translation models. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
M.F. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proceedings EMNLP, pages 133?141.
D. Vergyri et al 2004. Morphology-based language modeling
for Arabic speech recognition. In Proceedings of ICSLP.
128
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 399?407,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Lexicon Acquisition for Dialectal Arabic Using Transductive Learning
Kevin Duh
Dept. of Electrical Engineering
University of Washington
Seattle, WA, USA
duh@ee.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
We investigate the problem of learn-
ing a part-of-speech (POS) lexicon for a
resource-poor language, dialectal Arabic.
Developing a high-quality lexicon is often
the first step towards building a POS tag-
ger, which is in turn the front-end to many
NLP systems. We frame the lexicon ac-
quisition problem as a transductive learn-
ing problem, and perform comparisons
on three transductive algorithms: Trans-
ductive SVMs, Spectral Graph Transduc-
ers, and a novel Transductive Clustering
method. We demonstrate that lexicon
learning is an important task in resource-
poor domains and leads to significant im-
provements in tagging accuracy for dialec-
tal Arabic.
1 Introduction
Due to the rising importance of globalization and
multilingualism, there is a need to build natu-
ral language processing (NLP) systems for an in-
creasingly wider range of languages, including
those languages that have traditionally not been
the focus of NLP research. The development of
NLP technologies for a new language is a chal-
lenging task since one needs to deal not only with
language-specific phenomena but also with a po-
tential lack of available resources (e.g. lexicons,
text, annotations). In this study we investigate the
problem of learning a part-of-speech (POS) lexi-
con for a resource-poor language, dialectal Arabic.
Developing a high-quality POS lexicon is the
first step towards training a POS tagger, which in
turn is typically the front end for other NLP appli-
cations such as parsing and language modeling. In
the case of resource-poor languages (and dialec-
tal Arabic in particular), this step is much more
critical than is typically assumed: a lexicon with
too few constraints on the possible POS tags for
a given word can have disastrous effects on tag-
ging accuracy. Whereas such constraints can be
obtained from large hand-labeled corpora or high-
quality annotation tools in the case of resource-
rich languages, no such resources are available for
dialectal Arabic. Instead, constraints on possible
POS tags must be inferred from a small amount
of tagged words, or imperfect analysis tools. This
can be seen as the problem of learning complex,
structured outputs (multi-class labels, with a dif-
ferent number of classes for different words and
dependencies among the individual labels) from
partially labeled data.
Our focus is on investigating several machine
learning techniques for this problem. In partic-
ular, we argue that lexicon learning in resource-
poor languages can be best viewed as transduc-
tive learning. The main contribution of this work
are: (1) a comprehensive evaluation of three trans-
ductive algorithms (Transductive SVM, Spectral
Graph Transducer, and a new technique called
Transductive Clustering) as well as an inductive
SVM on this task; and (2) a demonstration that
lexicon learning is a worthwhile investment and
leads to significant improvements in the tagging
accuracy for dialectal Arabic.
The outline of the paper is as follows: Section 2
describes the problem in more detail and discusses
the situation in dialectal Arabic. The transductive
framework and algorithms for lexicon learning are
elaborated in Section 3. Sections 4 and 5 describe
the data and system. Experimental results are pre-
sented in Section 6. We discuss some related work
in Section 7 before concluding in Section 8.
399
2 The Importance of Lexicons in
Resource-poor POS Tagging
2.1 Unsupervised Tagging
The lack of annotated training data in resource-
poor languages necessitates the use of unsuper-
vised taggers. One commonly-used unsuper-
vised tagger is the Hidden Markov model (HMM),
which models the joint distribution of a word se-
quence w0:M and tag sequence t0:M as:
P (t0:M , w0:M ) =
M
?
i=0
p(wi|ti)p(ti|ti?1, ti?2)
(1)
This is a trigram HMM. Unsupervised learn-
ing is performed by running the Expectation-
Maximization (EM) algorithm on raw text. In this
procedure, the tag sequences are unknown, and the
probability tables p(wi|ti) and p(ti|ti?1, ti?2) are
iteratively updated to maximize the likelihood of
the observed word sequences.
Although previous research in unsupervised
tagging have achieved high accuracies rivaling su-
pervised methods (Kupiec, 1992; Brill, 1995),
much of the success is due to the use of artifi-
cially constrained lexicons. Specifically, the lex-
icon is a wordlist where each word is annotated
with the set of all its possible tags. (We will call
the set of possible tags of a given word the POS-
set of that word; an example: POS-set of the En-
glish word bank may be {NN,VB}.) Banko and
Moore (2004) showed that unsupervised tagger ac-
curacies on English degrade from 96% to 77% if
the lexicon is not constrained such that only high
frequency tags exist in the POS-set for each word.
Why is the lexicon so critical in unsupervised
tagging? The answer is that it provides addi-
tional knowledge about word-tag distributions that
may otherwise be difficult to glean from raw text
alone. In the case of unsupervised HMM taggers,
the lexicon provides constraints on the probability
tables p(wi|ti) and p(ti|ti?1, ti?2). Specifically,
the lexical probability table is initialized such that
p(wi|ti) = 0 if and only if tag ti is not included in
the POS-set of word wi. The transition probability
table is initialized such that p(ti|ti?1, ti?2) = 0 if
and only if the tag sequence (ti, ti?1, ti?2) never
occurs in the tag lattice induced by the lexicon on
the raw text. The effect of these zero-probability
initialization is that they will always stay zero
throughout the EM procedure (modulo the effects
of smoothing). This therefore acts as hard con-
straints and biases the EM algorithm to avoid cer-
tain solutions when maximizing likelihood. If the
lexicon is accurate, then the EM algorithm can
learn very good predictive distributions from raw
text only; conversely, if the lexicon is poor, EM
will be faced with more confusability during train-
ing and may not produce a good tagger. In general,
the addition of rare tags, even if they are correct,
creates a harder learning problem for EM.
Thus, a critical aspect of resource-poor POS
tagging is the acquisition of a high-quality lexi-
con. This task is challenging because the lexicon
learning algorithm must not be resource-intensive.
In practice, one may be able to find analysis tools
or incomplete annotations such that only a partial
lexicon is available. The focus is therefore on ef-
fective machine learning algorithms for inferring
a full high-quality lexicon from a partial, possibly
noisy initial lexicon. We shall now discuss this sit-
uation in the context of dialectal Arabic.
2.2 Dialectal Arabic
The Arabic language consist of a collection of
spoken dialects and a standard written language
(Modern Standard Arabic, or MSA). The dialects
of Arabic are of considerable importance since
they are used extensively in almost all everyday
conversations. NLP technology for dialectal Ara-
bic is still in its infancy, however, due to the lack
of data and resources. Apart from small amounts
of written dialectal material in e.g. plays, novels,
chat rooms, etc., data can only be obtained by
recording and manually transcribing actual con-
versations. Annotated corpora are scarce because
annotation requires another stage of manual ef-
fort beyond transcription work. In addition, ba-
sic resources such as lexicons, morphological an-
alyzers, tokenizers, etc. have been developed for
MSA, but are virtually non-existent for dialectal
Arabic.
In this study, we address lexicon learning for
Levantine Colloquial Arabic. We assume that only
two resources are available during training: (1)
raw text transcriptions of Levantine speech and (2)
a morphological analyzer developed for MSA.
The lexicon learning task begins with a par-
tial lexicon generated by applying the MSA ana-
lyzer to the Levantine wordlist. Since MSA dif-
fers from Levantine considerably in terms of syn-
tax, morphology, and lexical choice, not all Lev-
antine words receive an analysis. In our data,
23% of the words are un-analyzable. Thus, the
400
goal of lexicon learning is to infer the POS-sets
of the un-analyzable words, given the partially-
annotated lexicon and raw text.
Details on the Levantine data and overall system
are provided in Sections 4 and 5. We discuss the
learning algorithms in the next section.
3 Learning Frameworks and Algorithms
Let us formally define the lexicon learning prob-
lem. We have a wordlist of size m + u. A portion
of these words (m) are annotated with POS-set la-
bels, which may be acquired by manual annotation
or an automatic analysis tool. The set of labeled
words {Xm} is the training set, also referred to as
the partial lexicon. The task is to predict the POS-
sets of the remaining u unlabeled words {Xu}, the
test set. The goal of lexicon learning is to label
{Xu} with low error. The final result is a full lex-
icon that contains POS-sets for all m + u words.
3.1 Transductive Learning with Structured
Outputs
We argue that the above problem formulation
lends itself to a transductive learning framework.
Standard inductive learning uses a training set of
fully labeled samples in order to learn a classi-
fication function. After completion of the train-
ing phase, the learned model is then used to clas-
sify samples from a new, previously unseen test
set. Semi-supervised inductive learning exploits
unlabeled data in addition to labeled data to better
learn a classification function. Transductive learn-
ing, first described by Vapnik (Vapnik, 1998) also
describes a setting where both labeled and unla-
beled data are used jointly to decide on a label as-
signment to the unlabeled data points. However,
the goal here is not to learn a general classifica-
tion function that can be applied to new test sets
multiple times but to achieve a high-quality one-
time labeling of a particular data set. Transduc-
tive learning and inductive semi-supervised learn-
ing are sometimes confused in the literature. Both
approaches use unlabeled data in learning ? the
key difference is that a transductive classifier only
optimizes the performance on the given unlabeled
data while an inductive semi-supervised classifier
is trained to perform well on any new unlabeled
data.
Lexicon learning fits in the transductive learn-
ing framework as follows: The test set {Xu},
i.e. the unlabeled words, is static and known dur-
NN?VB vs. ~NN?VB
NN?JJ vs. ~NN?JJ
VB vs. ~VB
NN vs. ~NN
VB?JJ vs. ~VB?JJ
..., etc.
0.8
0.6
?0.4
?0.4
0.7 argmax NN?JJ
NN vs.~NN
VB vs. ~VB
JJ vs. ~JJ
sample
sample
K independent classifiers + 1 overall classifier
SINGLE?LABEL FRAMWORK
COMPOUND?LABEL FRAMEWORK
1 multi?class classifier
(one?vs?rest implementation using N binary classifiers)
0.9
?0.8 {NN,JJ}
0.1
Classifier
2nd Stage
Figure 1: Learning with Structured Outputs using
single or compound labels
ing learning time; we are not interested in inferring
POS-sets for any words outside the word list.
An additional characterization of the lexicon
learning problem is that it is a problem of learn-
ing with complex, structured outputs. The label
for each word is its POS-set, which may contain
one to K POS tags (where K is the size of the
tagset, K=20 in our case). This differs from tra-
ditional classification tasks where the output is a
single scalar variable.
Structured output problems like lexicon learn-
ing can be characterized by the granularity of the
basic unit of labels. We define two cases: single-
label and compound-label. In the single-label
framework (see Figure 1), each individual POS tag
is the target of classification and we have K binary
classifiers each hypothesizing whether a word has
a POS tag k (k = 1, . . . ,K). A second-stage clas-
sifier takes the results of the K individual classi-
fiers and outputs a POS-set. This classifier can
simply take all POS tags hypothesized positive by
the individual binary classifiers to form the POS-
set, or use a more sophisticated scheme for deter-
mining the number of POS tags (Elisseeff and We-
ston, 2002).
The alternative compound-label framework
treats each POS-set as an atomic label for clas-
sification. A POS-set such as {?NN?, ?VB?} is
?compounded? into one label ?NN-VB?, which re-
sults in a different label than, say, ?NN? or ?NN-
JJ?. Suppose there exist N distinct POS-sets in the
401
training data; then we have N atomic units for la-
beling. Thus a (N -ary) multi-class classifier is em-
ployed to directly predict the POS-set of a word. If
only binary classifiers are available (i.e. in the case
of Support Vector Machines), one can use one-vs-
rest, pairwise, or error correcting code schemes to
implement the multi-class classification.
The single-label framework is potentially ill-
suited for capturing the dependencies between
POS tags. Dependencies between POS tags arise
since some tags, such as ?NN? and ?NNP? can of-
ten be tagged to the same word and therefore co-
occur in the POS-set label. The compound-label
framework implicitly captures tag co-occurrence,
but potentially suffers from training data fragmen-
tation as well as the inability to hypothesize POS-
sets that do not already exist in the training data.
In our initial experiments, the compound-label
framework gave better classification results; thus
we implemented all of our algorithms in the multi-
class framework (using the one-vs-rest scheme
and choosing the argmax as the final decision).
3.2 Transductive Clustering
How does a transductive algorithm effectively uti-
lize unlabeled samples in the learning process?
One popular approach is the application of the so-
called cluster assumption, which intuitively states
that samples close to each other (i.e. samples that
form a cluster) should have similar labels.
Transductive clustering (TC) is a simple algo-
rithm that directly implements the cluster assump-
tion. The algorithm clusters labeled and unlabeled
samples jointly, then uses the labels of labeled
samples to infer the labels of unlabeled words in
the same cluster. This idea is relatively straight-
forward, yet what is needed is a principled way
of deciding the correct number of clusters and the
precise way of label transduction (e.g. based on
majority vote vs. probability thresholds). Typ-
ically, such parameters are decided heuristically
(e.g. (Duh and Kirchhoff, 2005a)) or by tuning on
a labeled development set; for resource-poor lan-
guages, however, no such set may be available.
As suggested by (El-Yaniv and Gerzon, 2005),
the TC algorithm can utilize a theoretical error
bound as a principled way of determining the pa-
rameters. Let R?h(Xm) be the empirical risk of a
given hypothesis (i.e. classifier) on the training set;
let Rh(Xu) be the test risk. (Derbeko et al, 2004)
derive an error bound which states that, with prob-
ability 1??, the risk on the test samples is bounded
by:
Rh(Xu) ? R?h(Xm)
+
?
(m+u
u
)
(
u+1
u
)
(
ln 1p(h)+ln
1
?
2m
)
(2)
i.e. the test risk is bounded by the empirical risk on
the labeled data, R?h(Xm), plus a term that varies
with the prior p(h) of the hypothesis or classifier.
This is a PAC-Bayesian bound (McAllester, 1999).
The prior p(h) indicates ones prior belief on the
hypothesis h over the set of all possible hypothe-
ses. If the prior is low or the empirical risk is high,
then the bound is large, implying that test risk may
be large. A good hypothesis (i.e. classifier) will
ideally have a small value for the bound, thus pre-
dicting a small expected test risk.
The PAC-Bayesian bound is important because
it provides a theoretical guarantee on the quality
of a hypothesis. Moreover, the bound in Eq. 2 is
particularly useful because it is easily computable
on any hypothesis h, assuming that one is given
the value of p(h). Given two hypothesized label-
ings of the test set, h1 and h2, the one with the
lower PAC-Bayesian bound will achieve a lower
expected test risk. Therefore, one can use the
bound as a principled way of choosing the pa-
rameters in the Transductive Clustering algorithm:
First, a large number of different clusterings is cre-
ated; then the one that achieves the lowest PAC-
Bayesian bound is chosen. The pseudo-code is
given in Figure 2.
(El-Yaniv and Gerzon, 2005) has applied the
Transductive Clustering algorithm successfully to
binary classification problems and demonstrated
improvements over the current state-of-the-art
Spectral Graph Transducers (Section 3.4). We use
the algorithm as described in (Duh and Kirchhoff,
2005b), which adapts the algorithm to structured
output problems. In particular, the modification
involves a different estimate of the priors p(h),
which was assumed to be uniform in (El-Yaniv and
Gerzon, 2005). Since there are many possible h,
adopting a uniform prior will lead to small values
of p(h) and thus a loose bound for all h. Proba-
bility mass should only be spent on POS-sets that
are possible, and as such, we calculate p(h) based
on frequencies of compound-labels in the training
data (i.e. an empirical prior).
3.3 Transductive SVM
Transductive SVM (TSVM) (Joachims, 1999) is
an algorithm that implicitly implements the cluster
402
1 For ? = 2 : C (C is set arbitrarily to a large number)
2 Apply a clustering algorithm to generate ? clusters on Xm+u.
3 Generate label hypothesis h? (by labeling each cluster with the most frequent label among its labeled samples)
4 Calculate the bound for h? as defined in Eq. 2.
5 Choose the hypothesis h? with the lowest bound; output the corresponding classification of Xu.
Figure 2: Pseudo-code for transductive clustering.
assumption. In standard inductive SVM (ISVM),
the learning algorithm seeks to maximize the mar-
gin subject to misclassification constraints on the
training samples. In TSVM, this optimization is
generalized to include additional constraints on
the unlabeled samples. The resulting optimiza-
tion algorithm seeks to maximize the margin on
both labeled and unlabeled samples and creates a
hyperplane that avoids high-density regions (e.g.
clusters).
3.4 Spectral Graph Transducer
Spectral Graph Transducer (SGT) (Joachims,
2003) achieves transduction via an extension of
the normalized mincut clustering criterion. First,
a data graph is constructed where the vertices are
labeled or unlabeled samples and the edge weights
represent similarities between samples. The min-
cut criteria seeks to partition the graph such that
the sum of cut edges is minimized. SGT extends
this idea to transductive learning by incorporating
constraints that require samples of the same label
to be in the same cluster. The resulting partitions
decide the label of unlabeled samples.
4 Data
4.1 Corpus
The dialect addressed in this work is Levantine
Colloquial Arabic (LCA), primarily spoken in Jor-
dan, Lebanon, Palestine, and Syria. Our devel-
opment/test data comes from the Levantine Ara-
bic CTS Treebank provided by LDC. The train-
ing data comes from the Levantine CTS Audio
Transcripts. Both are from the Fisher collection
of conversational telephone speech between Lev-
antine speakers previously unknown to each other.
The LCA data was transcribed in standard MSA
script and transliterated into ASCII characters us-
ing the Buckwalter transliteration scheme1. No di-
acritics are used in either the training or develop-
ment/test data. Speech effects such as disfluencies
and noises were removed prior to our experiments.
1http://www.ldc.upenn.edu/myl/morph/buckwalter.html
The training set consists of 476k tokens and
16.6k types. It is not annotated with POS tags ?
this is the raw text we use to train the unsuper-
vised HMM tagger. The test set consists of 15k
tokens and 2.4k types, and is manually annotated
with POS tags. The development set is also POS-
annotated, and contains 16k tokens and 2.4k types.
We used the reduced tagset known as the Bies
tagset (Maamouri et al, 2004), which focuses on
major part-of-speech and excludes detailed mor-
phological information.
Using the compound-label framework, we
observe 220 and 67 distinct compound-labels
(i.e. POS-sets) in the training and test sets, respec-
tively. As mentioned in Section 3.1, a classifier
in the compound-label framework can never hy-
pothesize POS-sets that do not exist in the training
data: 43% of the test vocabulary (and 8.5% by to-
ken frequency) fall under this category.
4.2 Morphological Analyzer
We employ the LDC-distributed Buckwalter ana-
lyzer for morphological analyses of Arabic words.
For a given word, the analyzer outputs all possi-
ble morphological analyses, including stems, POS
tags, and diacritizations. The information regard-
ing possible POS tags for a given word is crucial
for constraining the unsupervised learning process
in HMM taggers.
The Buckwalter analyzer is based on an internal
stem lexicon combined with rules for affixation. It
was originally developed for the MSA, so only a
certain percentage of Levantine words can be cor-
rectly analyzed. Table 1 shows the percentages
of words in the LCA training text that received N
possible POS tags from the Buckwalter analyzer.
Roughly 23% of types and 28% of tokens received
no tags (N=0) and are considered un-analyzable.
5 System
Our overall system looks as follows (see Figure
3): In Step 1, the MSA (Buckwalter) analyzer
is applied to the word list derived from the raw
training text. The result is a partial POS lexicon,
403
word2 JJ?NN
word3 JJ
word4 ?
word5 ?
word1 NN?VB
HMM TaggerFull POS LexiconPartial POS Lexicon
RAW
TEXT
Buckwalter
Analyzer (1)
Transductive
Learning (2) Training (3)
EM
word2 JJ?NN
word3 JJ
word1 NN?VB
word4 NN?VB
word5 JJ
Figure 3: Overall System: (1) Apply Buckwalter Analyzer to dialectal Arabic raw text, obtaining a
partial POS lexicon. (2) Use Transductive Learning to infer missing POS-sets. (3) Unsupervised training
of HMM Tagger using both raw text and inferred lexicon.
N Type Token
0 23.3 28.2
1 52.5 40.4
2 17.7 19.9
3 5.2 10.5
4 1.0 2.3
5 0.1 0.6
Table 1: Percentage of word types/tokens with N
possible tags, as determined by the Buckwalter an-
alyzer. Words with 0 tags are un-analyzable.
which lists the set of possible POS tags for those
words for which the analyzer provided some out-
put. All possibilities suggested by the analyzer are
included.
The focus of Step 2 is to infer the POS-sets of
the remaining, unannotated words using one of the
automatic learning procedures described in Sec-
tion 3. Finally, Step 3 involves training an HMM
tagger using the learned lexicon. This is the stan-
dard unsupervised learning component of the sys-
tem. We use a trigram HMM, although modifica-
tions such as the addition of affixes and variables
modeling speech effects may improve tagging ac-
curacy. Our concern here is the evaluation of the
lexicon learning component in Step 2.
An important problem in this system setup is
the possibility of error propagation. In Step 1, the
MSA analyzer may give incorrect POS-sets to ana-
lyzable words. It may not posit the correct tag (low
recall), or it may give too many tags (low preci-
sion). Both have a negative effect on lexicon learn-
ing and EM training. For lexicon learning, Step
1 errors represent corrupt training data; For EM
training, Step 1 error may cause the HMM tagger
to never hypothesize the correct tag (low recall) or
have too much confusibility during training (low
precision). We attempted to measure the extent of
this error by calculating the tag precision/recall on
words that occur in the test set: Among the 12k
words analyzed by the analyzer, 1483 words oc-
cur in the test data. We used the annotations in
the test data and collected all the ?oracle? POS-
sets for each of these 1483 words.2 The aver-
age precision of the analyzer-generated POS-sets
against the oracle is 56.46%. The average recall
is 81.25%. Note that precision is low?this implies
that the partial lexicon is not very constrained. The
recall of 81.25% means that 18.75% of the words
may never receive the correct tag in tagging. In
the experiments, we will investigate to what ex-
tent this kind of error affects lexicon learning and
EM training.
6 Experiments
6.1 Lexicon learning experiments
We seek to answer the following three questions
in our experiments:
? How useful is the lexicon learning step in an
end-to-end POS tagging system? Do the ma-
chine learning algorithms produce lexicons
that result in higher tagging accuracies, when
compared to a baseline lexicon that simply
hypothesizes all POS tags for un-analyzable
words? The answer is a definitive yes.
? What machine learning algorithms perform
the best on this task? Do transductive learn-
ing outperform inductive learning? The em-
pirical answer is that TSVM performs best,
SGT performs worst, and TC and ISVM are
in the middle.
2Since the test set is small, these ?oracle? POS-sets may
be missing some tags. Thus the true precision may be higher
(and recall may be lower) than measured.
404
Orthographic features:
wi matches /?pre/, pre = {set of data-derived prefixes}
wi matches /suf$/, suf = {set of data-derived suffixes}
Contextual features:
wi?1 = voc, voc = {set of words in lexicon}
ti?1 = tag, tag = {set of POS tags}
ti+1 = tag, tag = {set of POS tags}
wi?1 is an un-analyzable word
wi+1 is an un-analyzable word
Table 2: Binary features used for predicting POS-
sets of un-analyzable words.
? What is the relative impact of errors from the
MSA analyzer on lexicon learning and EM
training? The answer is that Step 1 errors af-
fect EM training more, and lexicon learning
is comparably robust to these errors.
In our problem, we have 12k labeled samples
and 3970 unlabeled samples. We define the feature
of each sample as listed in Table 2. The contextual
features are generated by co-occurrence statistics
gleaned from the training data. For instance, for
a word foo, we collect all bigrams consisting of
foo from the raw text; all features [wt?1 = voc]
that correspond to the bigrams (voc, foo) are set
to 1. The idea is that words with similar ortho-
graphic and/or contextual features should receive
similar POS-sets.
All results, unless otherwise noted, are tagging
accuracies on the test set given by training a HMM
tagger on a specific lexicon. Table 3 gives tagging
accuracies of the four machine learning methods
(TSVM, TC, ISVM, SGT) as well as two base-
line approaches for generating a lexicon: (all tags)
gives all 20 possible tags to the un-analyzable
words, whereas (open class) gives only the sub-
set of open-class POS tags.3 The results are given
in descending order of overall tagging accuracy.4
With the exception of TSVM (63.54%) vs. TC
(62.89%), all differences are statistically signifi-
cant. As seen in the table, applying a machine
learning step for lexicon learning is a worthwhile
effort since it always leads to better tagging accu-
racies than the baseline methods.
3Not all un-analyzable words are open-class. Close-class
words may be un-analyzable due to dialectal spelling varia-
tions.
4Note that the unknown word accuracies do not follow
the same trend and are generally quite low. This might be
due to the fact that POS tags of unknown words are usually
best predicted by the HMM?s transition probabilities, which
may not be as robust due to the noisy lexicon.
Method Accuracy UnkAcc
TSVM 63.54 26.19
TC 62.89 26.71
ISVM 61.53 27.68
SGT 59.68 25.82
open class 57.39 27.08
all tags 55.64 25.00
Table 3: Tagging Accuracies for lexicons derived
by machine learning (TSVM, TC, ISVM, SGT)
and baseline methods. Accuracy=Overall accu-
racy; UnkAcc=Accuracy of unknown words.
The poor performance of SGT is somewhat sur-
prising since it is contrary to results presented in
other papers. We attributed this to the difficulty in
constructing the data graph. For instance, we con-
structed k-nearest-neighbor graphs based on the
cosine distance between feature vectors, but it is
difficult to decide the best distance metric or num-
ber of neighbors. Finally, we note that besides the
performance of SGT, transductive learning meth-
ods (TSVM, TC) outperform the inductive ISVM.
We also compute precision/recall statistics of
the final lexicon on the test set words (similar to
Section 5) and measure the average size of the
POS-sets (?POSset?). As seen in Table 4, POS-
set sizes of machine-learned lexicon is a factor of
2 or 3 smaller than that of the baseline lexicons.
On the other hand, recall is better for the baseline
lexicons. These observations, combined with the
fact that machine-learned lexicons gave better tag-
ging accuracy, suggests that we have a constrained
lexicon effect here: i.e. for EM training, it is better
to constrain the lexicon with small POS-sets than
to achieve high recall.
Method Precision Recall ?POSset?
TSVM 58.15 88.85 1.89
TC 59.19 87.88 1.80
ISVM 58.09 88.44 1.87
SGT 53.98 82.60 1.87
open class 54.03 96.77 3.39
all tags 53.31 98.53 5.17
Table 4: Statistics of the Lexicons in Table 3.
Next, we examined the effects of error propa-
gation from the MSA analyzer in Step 1. We at-
tempted to correct these errors by using POS-sets
of words derived from the development data. In
405
particular, of the 1562 partial lexicon words that
also occur in the development set, we found 1044
words without entirely matching POS-sets. These
POS-sets are replaced with the oracle POS-sets de-
rived from the development data, and the result is
treated as the (corrected) partial lexicon of Step 1.
In this procedure, the average POS-set size of the
partial lexicon decreased from 2.13 to 1.10, recall
increased from 82.44% to 100%, and precision in-
creased from 57.15% to 64.31%. We apply lexi-
con learning to this corrected partial lexicon and
evaluate tagging results, shown in Table 5. The
fact that all numbers in Table 5 represent signifi-
cant improvements over Table 3 implies that error
propagation is not a trivial problem, and automatic
error correction methods may be desired.
Method Accuracy UnkAcc
TSVM 66.54 27.38
ISVM 65.08 26.86
TC 64.05 28.20
SGT 63.78 27.23
all tags 62.96 27.91
open class 61.26 27.83
Table 5: Tag accuracies by correcting mistakes in
the partial lexicon prior to lexicon learning. In-
terestingly, we note ISVM outperforms TC here,
which differs from Table 3.
Finally, we determine whether error propaga-
tion impacts lexicon learning (Step 2) or EM train-
ing (Step 3) more. Table 6 shows the results of
TSVM for four scenarios: correcting analyzer er-
rors in the the lexicon: (A) prior to lexicon learn-
ing, (B) prior to EM training, (C) both, or (D)
none. As seen in Table 6, correcting the lexicon
at Step 3 (EM training) gives the most improve-
ments, indicating that analyzer errors affects EM
training more than lexicon learning. This implies
that lexicon learning is relatively robust to train-
ing data corruption, and that one can mainly focus
on improved estimation techniques for EM train-
ing (Wang and Schuurmans, 2005) if the goal is to
alleviate the impact of analyzer errors. The same
evaluation on the other machine learning methods
(TC, ISVM, SGT) show similar results.
6.2 Comparison experiments: Expert lexicon
and supervised learning
Our approach to building a resource-poor POS
tagger involves (a) lexicon learning, and (b) un-
Scenario Step2 Step3 TSVM
(B) N Y 66.70
(C) Y Y 66.54
(A) Y N 64.93
(D) N N 63.54
Table 6: Effect of correcting the lexicon in differ-
ent steps. Y=yes, lexicon corrected; N=no, POS-
set remains the same as analyzer?s output.
supervised training. In this section we examine
cases where (a) an expert lexicon is available, so
that lexicon learning is not required, and (b) sen-
tences are annotated with POS information, so that
supervised training is possible. The goal of these
experiments is to determine when alternative ap-
proaches involving additional human annotations
become worthwhile in this task.
(a) Expert lexicon: First, we build an expert
lexicon by collecting all tags per word in the de-
velopment set (i.e. ?oracle? POS-sets). Then, the
tagger is trained using EM by treating the develop-
ment set as raw text (i.e. ignoring the POS anno-
tations). This achieves an accuracy of 74.45% on
the test set. Note that this accuracy is significantly
higher than the ones in Table 3, which represent
unsupervised training on more raw text (the train-
ing set), but with non-expert lexicons derived from
the MSA analyzer and a machine learner. This re-
sult further demonstrates the importance of obtain-
ing an accurate lexicon in unsupervised training. If
one were to build this expert lexicon by hand, one
would need an annotator to label the POS-sets of
2450 distinct lexicon items.
(b) Supervised training: We build a super-
vised tagger by training on the POS annotations of
the development set, which achieves 82.93% accu-
racy. This improved accuracy comes at the cost of
annotating 2.2k sentences (16k tokens) with com-
plete POS information.
Finally, we present the same results with re-
duced data, taking first 50, 100, 200, etc. sen-
tences in the development set for lexicon or POS
annotation. The learning curve is shown in Table
7. One may be tempted to draw conclusions re-
garding supervised vs. unsupervised approaches
by directly comparing this table with the results
in Section 6.1; we avoid doing so since taggers in
Sections 6.1 and 6.2 are trained on different data
sets (training vs. development set) and the accu-
racy differences are compounded by issues such
406
Supervised Unsupervised, Expert
#Sentence Acc #Vocab Acc
50 47.82 123 47.13
100 55.32 188 54.65
200 61.17 299 57.37
400 69.17 497 64.36
800 76.92 953 70.36
1600 81.73 1754 72.99
2200 82.93 2450 74.45
Table 7: (1) Supervised training accuracies with
varying numbers of sentences. (2) Accuracies of
unsupervised training using a expert lexicon of
different vocabulary sizes.
as ngram coverage, data-set selection, and the way
annotations are done.
7 Related Work
There is an increasing amount of work in NLP
tools for Arabic. In supervised POS tagging, (Diab
et al, 2004) achieves high accuracy on MSA with
the direct application of SVM classifiers. (Habash
and Rambow, 2005) argue that the rich morphol-
ogy of Arabic necessitates the use of a morpho-
logical analyzer in combination with POS tag-
ging. This can be considered similar in spirit to
the learning of lexicons for unsupervised tagging.
The work done at a recent JHU Workshop
(Rambow and others, 2005) is very relevant in that
it investigates a method for improving LCA tag-
ging that is orthogonal to our approach. They do
not use the raw LCA text as we have done. Instead,
they train a MSA supervised tagger and adapt it to
LCA by a combination of methods, such using a
MSA-LCA translation lexicon and redistributing
the probabibility mass of MSA words to LCA.
8 Conclusion
In this study, we investigated several machine
learning algorithms on the task of lexicon learn-
ing and demonstrated its impact on dialectal Ara-
bic tagging. We achieve a POS tagging accuracy
of 63.54% using a transductively-learned lexicon
(TSVM), outperforming the baseline (57.39%).
This result brings us one step closer to the accu-
racies of unsupervised training with expert lexi-
con (74.45%) and supervised training (82.93%),
both of which require significant annotation effort.
Future work includes a more detailed analysis of
transductive learning in this domain and possible
solutions to alleviating error propagation.
Acknowledgments
We would like to thank Rebecca Hwa for discussions regard-
ing the JHU project. This work is funded in part by NSF
Grant IIS-0326276 and an NSF Graduate Fellowship for the
1st author. Any opinions, findings, and conclusions expressed
in this material are those of the authors and do not necessarily
reflect the views of these agencies.
References
M. Banko and R. Moore. 2004. Part-of-speech tagging in
context. In Proc. of COLING 2004.
E. Brill. 1995. Unsupervised learning of disambiguation
rules for part of speech tagging. In Proc. of the Third
Workshop on Very Large Corpora.
P. Derbeko, R. El-Yaniv, and R. Meir. 2004. Explicit learning
curves for transduction and application to clustering and
compression algorithms. Journal of Artificial Intelligence
Research, 22:117-142.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of Arabic text: from raw text to base phrase chunks.
In Proceedings of HLT/NAACL.
K. Duh and K. Kirchhoff. 2005a. POS tagging of dialectal
arabic: a minimally-supervised approach. In ACL 2005,
Semitic Languages Workshop.
K. Duh and K. Kirchhoff. 2005b. Structured multi-label
transductive learning. In NIPS Workshop on Advances in
Structured Learning for Text/Speech Processing.
R. El-Yaniv and L. Gerzon. 2005. Effective transductive
learning via objective model selection. Pattern Recogni-
tion Letters, 26(13):2104-2115.
A. Elisseeff and J. Weston. 2002. Kernel methods for multi-
labeled classification. In NIPS.
N. Habash and O. Rambow. 2005. Arabic tokenization, mor-
phological analysis, and part-of-speech tagging in one fell
swoop. In ACL.
T. Joachims. 1999. Transductive inference for text classifi-
cation using support vector machines. In ICML.
T. Joachims. 2003. Transductive learning via spectral graph
partitioning. In ICML.
J. Kupiec. 1992. Robust part-of-speech tagging using a hid-
den Markov model. Computer Speech and Language, 6.
M. Maamouri, A. Bies, and T. Buckwalter. 2004. The Penn
Arabic Treebank: Building a large-scale annotated Arabic
corpus. In NEMLAR Conf. on Arabic Language Resources
and Tools.
D. McAllester. 1999. Some PAC-Bayesian theorems. Ma-
chine Learning, 37(3):255-36.
O. Rambow et al 2005. Parsing Arabic dialects. Technical
report, Final Report, 2005 JHU Summer Workshop.
V. Vapnik. 1998. Statistical Learning Theory. Wiley Inter-
science.
Q. Wang and D. Schuurmans. 2005. Improved estimation for
unsupervised part-of-speech tagging. In IEEE NLP-KE.
407
Proceedings of the Third Workshop on Statistical Machine Translation, pages 123?126,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The University of Washington Machine Translation System for
ACL WMT 2008
Amittai Axelrod, Mei Yang, Kevin Duh, Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA 98195
{amittai,yangmei,kevinduh,katrin} @ee.washington.edu
Abstract
This paper present the University of Washing-
ton?s submission to the 2008 ACL SMT shared ma-
chine translation task. Two systems, for English-to-
Spanish and German-to-Spanish translation are de-
scribed. Our main focus was on testing a novel
boosting framework for N-best list reranking and
on handling German morphology in the German-to-
Spanish system. While boosted N-best list reranking
did not yield any improvements for this task, simpli-
fying German morphology as part of the preprocess-
ing step did result in significant gains.
1 Introduction
The University of Washington submitted systems
to two data tracks in the WMT 2008 shared task
competition, English-to-Spanish and German-to-
Spanish. In both cases, we focused on the in-domain
test set only. Our main interest this year was on in-
vestigating an improved weight training scheme for
N-best list reranking that had previously shown im-
provements on a smaller machine translation task.
For German-to-Spanish translation we additionally
investigated simplifications of German morphology,
which is known to be fairly complex due to a large
number of compounds and inflections. In the fol-
lowing sections we first describe the data, baseline
system and postprocessing steps before describing
boosted N-best list reranking and morphology-based
preprocessing for German.
2 Data and Basic Preprocessing
We used the Europarl data as provided (version 3b,
1.25 million sentence pairs) for training the transla-
tion model for use in the shared task. The data was
lowercased and tokenized with the auxiliary scripts
provided, and filtered according to the ratio of the
sentence lengths in order to eliminate mismatched
sentence pairs. This resulted in about 965k paral-
lel sentences for English-Spanish and 950k sentence
pairs for German-Spanish. Additional preprocess-
ing was applied to the German corpus, as described
in Section 5. For language modeling, we addition-
ally used about 82M words of Spanish newswire text
from the Linguistic Data Consortium (LDC), dating
from 1995 to 1998.
3 System Overview
3.1 Translation model
The system developed for this year?s shared task
is a state-of-the-art, two-pass phrase-based statisti-
cal machine translation system based on a log-linear
translation model (Koehn et al 2003). The trans-
lation models and training method follow the stan-
dard Moses (Koehn et al 2007) setup distributed as
part of the shared task. We used the training method
suggested in the Moses documentation, with lexical-
ized reordering (the msd-bidirectional-fe
option) enabled. The system was tuned via Mini-
mum Error Rate Training (MERT) on the first 500
sentences of the devtest2006 dataset.
123
3.2 Decoding
Our system used the Moses decoder to generate
2000 output hypotheses per input sentence during
the first translation pass. For the second pass, the
N-best lists were rescored with the additional lan-
guage models described below. We re-optimized the
model combination weights with a parallelized im-
plementation of MERT over 16 model scores on the
test2007 dataset. Two of these model scores for
each hypothesis were from the two language models
used in our second-pass system, and the rest corre-
spond to the 14 Moses model weights (for reorder-
ing, language model, translation model, and word
penalty).
3.3 Language models
We built all of our language models using the
SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney discounting and interpolating all n-
gram estimates of order > 1. For first-pass de-
coding we used a 4-gram language model trained
on the Spanish side of the Europarl v3b data. The
optimal n-gram order was determined by testing
language models with varying orders (3 to 5) on
devtest2006; BLEU scores obtained using the
various language models are shown in Table 1. The
4-gram model performed best.
Table 1: LM ngram size vs. output BLEU on the dev sets.
order devtest2006 test2007
3-gram 30.54 30.69
4-gram 31.03 30.94
5-gram 30.85 30.84
Two additional language models were used for
second pass rescoring. First, we trained a large out-
of-domain language model on Spanish newswire
text obtained from the LDC, dating from 1995 to
1998.
We used a perplexity-filtering method to filter out
the least relevant half of the out-of-domain text, in
order to significantly reduce the training time of
the large language model and accelerate the rescor-
ing process. This was done by computing the per-
plexity of an in-domain language model on each
newswire sentence, and then discarding all sen-
tences with greater than average perplexity. This
reduced the size of the training set from 5.8M sen-
tences and 166M tokens to 2.8M sentences and 82M
tokens. We then further restricted the vocabulary to
the union of the vocabulary lists of the Spanish sides
of the de-es and en-es parallel training corpora. The
remaining text was used to train the language model.
The second language model used for rescoring
was a 5-gram model over part-of-speech (POS) tags.
This model was built using the Spanish side of the
English-Spanish parallel training corpus. The POS
tags were obtained from the corpus using Freeling
v2.0 (Atserias et al 2006).
We selected the language models for our transla-
tion system were selected based on performance on
the English-to-Spanish task, and reused them for the
German-to-Spanish task.
4 Boosted Reranking
We submitted an alternative system, based on a
different re-ranking method, called BoostedMERT
(Duh and Kirchhoff, 2008), for each task. Boosted-
MERT is a novel boosting algorithm that uses Mini-
mum Error Rate Training (MERT) as a weak learner
to build a re-ranker that is richer than the standard
log-linear models. This is motivated by the obser-
vation that log-linear models, as trained by MERT,
often do not attain the oracle BLEU scores of the N-
best lists in the development set. While this may be
due to a local optimum in MERT, we hypothesize
that log-linear models based on our K re-ranking
features are also not sufficiently expressive.
BoostedMERT is inspired by the idea of Boosting
(for classification), which has been shown to achieve
low training (and generalization) error due to classi-
fier combination. In BoostedMERT, we maintain a
weight for each N-best list in the development set.
In each iteration, MERT is performed to find the best
ranker on weighted data. Then, the weights are up-
dated based on whether the current ranker achieves
oracle BLEU. For N-best lists that achieve BLEU
scores far lower than the oracle, the weights are in-
creased so that they become the emphasis of next
iteration?s MERT. We currently use the factor e?r
to update the N-best list distribution, where r is the
ratio of the oracle hypothesis? BLEU to the BLEU
of the selected hypothesis. The final ranker is a
124
weighted combination of many such rankers.
More precisely, let wi be the weights trained by
MERT at iteration i. Given any wi, we can gener-
ate a ranking yi over an N-best list where yi is an
N-dimensional vector of predicted ranks. The final
ranking vector is a weighted sum: y =
?T
i=1 ?iyi,
where ?i are parameters estimated during the boost-
ing process. These parameters are optimized for
maximum BLEU score on the development set. The
only user-specified parameter is T , the number of
boosting iterations. Here, we choose T by divid-
ing the dev set in half: dev1 and dev2. First, we
train BoostedMERT on dev1 for 50 iterations, then
pick the T with the best BLEU score on dev2. Sec-
ond, we train BoostedMERT on dev2 and choose the
optimal T from dev1. Following the philosophy of
classifier combination, we sum the final rank vectors
y from each of the dev1- and dev2-trained Boosted-
MERT to obtain our final ranking result.
5 German ? Spanish Preprocessing
German is a morphologically complex language,
characterized by a high number of noun compounds
and rich inflectional paradigms. Simplification of
morphology can produce better word alignment, and
thus better phrasal translations, and can also signifi-
cantly reduce the out-of-vocabulary rate. We there-
fore applied two operations: (a) splitting of com-
pound words and (b) stemming.
After basic preprocessing, the German half of the
training corpus was first tagged by the German ver-
sion of TreeTagger (Schmid, 1994), to identify part-
of-speech tags. All nouns were then collected into
a noun list, which was used by a simple compound
splitter, as described in (Yang and Kirchhoff, 2006).
This splitter scans the compound word, hypothesiz-
ing segmentations, and selects the first segmentation
that produces two nouns that occur individually in
the corpus. After splitting the compound nouns in
the filtered corpus, we used the TreeTagger again,
only this time to lemmatize the (filtered) training
corpus.
The stemmed version of the German text was used
to train the translation system?s word alignments
(through the end of step 3 in the Moses training
script). After training the alignments, they were pro-
jected back onto the unstemmed corpus. The parallel
phrases were then extracted using the standard pro-
cedure. Stemming is only used during the training
stage, in order to simplify word alignment. During
the evaluation phase, only the compound-splitter is
applied to the German input.
6 Results
6.1 English ? Spanish
The unofficial results of our 2nd-pass system for the
2008 test set are shown in Table 2, for recased, unto-
kenized output. We note that the basic second-pass
model was better than the first-pass system on the
2008 task, but not on the 2007 task, whereas Boost-
edMERT provided a minor improvement in the 2007
task but not the 2008 task. This is contrary to previ-
ous results in the Arabic-English IWSLT 2007 task,
where boosted MERT gave an appreciable improve-
ment. This result is perhaps due to the difference in
magnitude between the IWSLT and WMT transla-
tion tasks.
Table 2: En?Es system on the test2007 and test2008
sets.
System test2007 test2008
First-Pass 30.95 31.83
Second-Pass 30.94 32.72
BoostedMERT 31.05 32.62
6.2 German ? Spanish
As previously described, we trained two German-
Spanish translation systems: one via the default
method provided in the Moses scripts, and an-
other using word stems to train the word align-
ments and then projecting these alignments onto
the unstemmed corpus and finishing the training
process in the standard manner. Table 3 demon-
strates that the word alignments generated with
word-stems markedly improved first-pass transla-
tion performance on the dev2006 dataset. How-
ever, during the evaluation period, the worse of the
two systems was accidentally used, resulting in a
larger number of out-of-vocabulary words in the
system output and hence a poorer score. Rerun-
ning our German-Spanish translation system cor-
rectly yielded significantly better system results,
also shown in Table 3.
125
Table 3: De?Es first-pass system on the development
and 2008 test set.
System dev2006 test2008
Baseline 23.9 21.2
Stemmed Alignments 26.3 24.4
6.3 Boosted MERT
BoostedMERT is still in an early stage of experi-
mentation, and we were interested to see whether it
improved over traditional MERT in re-ranking. As it
turns out, the BLEU scores on test2008 and test2007
data for the En-Es track are very similar for both re-
rankers. In our post-evaluation analysis, we attempt
to understand the reasons for similar BLEU scores,
since the weights wi for both re-rankers are quali-
tatively different. We found that out of 2000 En-Es
N-best lists, BoostedMERT and MERT differed on
1478 lists in terms of the final hypothesis that was
chosen. However, although the rankers are choosing
different hypotheses, the chosen strings appear very
similar. The PER of BoostedMERT vs. MERT re-
sults is only 0.077, and manual observation indicates
that the differences between the two are often single
phrase differences in a sentence.
We also computed the sentence-level BLEU for
each ranker with respect to the true reference. This
is meant to check whether BoostedMERT improved
over MERT in some sentences but not others: if the
improvements and degradations occur in the same
proportions, a similar corpus-level BLEU may be
observed. However, this is not the case. For a major-
ity of the 2000 sentences, the sentence-level BLEU
for both systems are the same. Only 10% of sen-
tences have absolute BLEU difference greater than
0.1, and the proportion of improvement/degradation
is similar (each 5%). For BLEU differences greater
than 0.2, the percentage drops to 4%.
Thus we conclude that although BoostedMERT
and MERT choose different hypotheses quite of-
ten, the string differences between their hypotheses
are negligible, leading to similar final BLEU scores.
BoostedMERT has found yet another local optimum
during training, but has not improved upon MERT
in this dataset. We hypothesize that dividing up the
original development set into halves may have hurt
BoostedMERT.
7 Conclusion
We have presented the University of Washing-
ton systems for English-to-Spanish and German-to-
Spanish for the 2008 WMT shared translation task.
A novel method for reranking N-best lists based on
boosted MERT training was tested, as was morpho-
logical simplification in the preprocessing compo-
nent for the German-to-Spanish system. Our con-
clusions are that boosted MERT, though successful
on other translation tasks, did not yield any improve-
ment here. Morphological simplification, however,
did result in significant improvements in translation
quality.
Acknowledgements
This work was funded by NSF grants IIS-0308297
and IIS-0326276.
References
Atserias, J. et al 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP library.
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Genoa, Italy.
Duh, K., and Kirchhoff, K. 2008. Beyond Log-Linear
Models: Boosted Minimum Error Rate Training for
MT Re-ranking. To appear, Proceedings of the Associ-
ation for Computational Linguistics (ACL). Columbus,
Ohio.
Koehn, P. and Och, F.J. and Marcu, D. 2003. Statistical
phrase-based translation. Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, (HLT/NAACL). Edmonton, Canada.
Koehn, P. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation Proceedings of MT Summit.
Koehn, P. et al 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session. Prague, Czech Republic.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. International Conference on New
Methods in Language Processing, Manchester, UK.
Stolcke, A. 2002. SRILM - An extensible language mod-
eling toolkit. Proceedings of ICSLP.
Yang, M. and K. Kirchhoff. 2006. Phrase-based backoff
models for machine translation of highly inflected lan-
guages. Proceedings of the 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL 2006). Trento, Italy.
126
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1227?1235,
Beijing, August 2010
Contextual Modeling for Meeting Translation Using Unsupervised Word
Sense Disambiguation
Yang Mei
Department of Electrical Engineering
University of Washington
yangmei@u.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
katrin@ee.washington.edu
Abstract
In this paper we investigate the challenges
of applying statistical machine translation
to meeting conversations, with a particu-
lar view towards analyzing the importance
of modeling contextual factors such as the
larger discourse context and topic/domain
information on translation performance.
We describe the collection of a small cor-
pus of parallel meeting data, the develop-
ment of a statistical machine translation
system in the absence of genre-matched
training data, and we present a quantita-
tive analysis of translation errors result-
ing from the lack of contextual modeling
inherent in standard statistical machine
translation systems. Finally, we demon-
strate how the largest source of translation
errors (lack of topic/domain knowledge)
can be addressed by applying document-
level, unsupervised word sense disam-
biguation, resulting in performance im-
provements over the baseline system.
1 Introduction
Although statistical machine translation (SMT)
has made great progress over the last decade,
most SMT research has focused on the transla-
tion of structured input data, such as newswire
text or parliamentary proceedings. Spoken lan-
guage translation has mostly concentrated on two-
person dialogues, such as travel expressions or
patient-provider interactions in the medical do-
main. Recently, more advanced spoken-language
data has been addressed, such as speeches (Stu?ker
et al, 2007), lectures (Waibel and Fu?gen, 2008),
and broadcast conversations (Zheng et al, 2008).
Problems for machine translation in these genres
include the nature of spontaneous speech input
(e.g. disfluencies, incomplete sentences, etc.) and
the lack of high-quality training data. Data that
match the desired type of spoken-language inter-
action in topic, domain, and, most importantly, in
style, can only be obtained by transcribing and
translating conversations, which is a costly and
time-consuming process. Finally, many spoken-
language interactions, especially those involving
more than two speakers, rely heavily on the par-
ticipants? shared contextual knowledge about the
domain and topic of the discourse, relationships
between speakers, objects in the real-world en-
vironment, past interactions, etc. These are typ-
ically not modelled in standard SMT systems.
The problem of speech disfluencies has been
addressed by disfluency removal techniques that
are applied prior to translation (Rao et al, 2007;
Wang et al, 2010). Training data sparsity has been
addressed by adding data from out-of-domain re-
sources (e.g. (Matusov et al, 2004; Hildebrandt
et al, 2005; Wu et al, 2008)), exploiting com-
parable rather than parallel corpora (Munteanu
and Marcu, 2005), or paraphrasing techniques
(Callison-Burch et al, 2006). The lack of con-
textual modeling, by contrast, has so far not been
investigated in depth, although it is a generally
recognized problem in machine translation. Early
attempts at modeling contextual information in
machine translation include (Mima et al, 1998),
where information about the role, rank and gen-
der of speakers and listeners was utilized in a
transfer-based spoken-language translation sys-
tem for travel dialogs. In (Kumar et al, 2008)
1227
statistically predicted dialog acts were used in a
phrase-based SMT system for three different di-
alog tasks and were shown to improve perfor-
mance. Recently, contextual source-language fea-
tures have been incorporated into translation mod-
els to predict translation phrases for traveling do-
main tasks (Stroppa et al, 2007; Haque et al,
2009). However, we are not aware of any work ad-
dressing contextual modeling for statistical trans-
lation of spoken meeting-style interactions, not
least due to the lack of a relevant corpus.
The first goal of this study is to provide a quan-
titative analysis of the impact of the lack of con-
textual modeling on translation performance. To
this end we have collected a small corpus of par-
allel multi-party meeting data. A baseline SMT
system was trained for this corpus from freely
available data resources, and contextual transla-
tion errors were manually analyzed with respect
to the type of knowledge sources required to re-
solve them. Our analysis shows that the largest
error category consists of word sense disambigua-
tion errors resulting from a lack of topic/domain
modeling. In the second part of this study we
therefore present a statistical way of incorporat-
ing such knowledge by using a graph-based unsu-
pervised word sense disambiguation algorithm at
a global (i.e. document) level. Our evaluation on
real-world meeting data shows that this technique
improves the translation performance slightly but
consistently with respect to position-independent
word error rate (PER).
2 Data
2.1 Parallel Conversational Data
For our investigations we used a subset of the AMI
corpus (McCowan, 2005), which is a collection of
multi-party meetings consisting of approximately
100 hours of multimodal data (audio and video
recordings, slide images, data captured from dig-
ital whiteboards, etc.) with a variety of existing
annotations (audio transcriptions, topic segmenta-
tions, summaries, etc.). Meetings were recorded
in English and fall into two broad types: sce-
nario meetings, where participants were asked to
act out roles in a pre-defined scenario, and non-
scenario meetings where participants were not re-
stricted by role assignments. In the first case, the
scenario was a project meeting about the devel-
opment of a new TV remote control; participant
roles were project manager, industrial designer,
marketing expert, etc. The non-scenario meet-
ings are about the move of an academic lab to
a new location on campus. The number of par-
ticipants is four. For our study we selected 10
meetings (5 scenario meetings and 5 non-scenario
meetings) and had their audio transcriptions trans-
lated into German (our chosen target language) by
two native speakers each. Translators were able
to simultaneously read the audio transcription of
the meeting, view the video, and listen to the au-
dio, when creating the translation. The transla-
tion guidelines were designed to obtain transla-
tions that match the source text as closely as pos-
sible in terms of style ? for example, translators
were asked to maintain the same level of collo-
quial as opposed to formal language, and to gen-
erally ensure that the translation was pragmati-
cally adequate. Obvious errors in the source text
(e.g. errors made by non-native English speak-
ers among the meeting participants) were not ren-
dered by equivalent errors in the German transla-
tion but were corrected prior to translation. The
final translations were reviewed for accuracy and
the data were filtered semi-automatically by elim-
inating incomplete sentences, false starts, fillers,
repetitions, etc. Although these would certainly
pose problems in a real-world application of spo-
ken language translation, the goal of this study
is not to analyze the impact of speech-specific
phenomena on translation performance (which, as
discussed in Section 1, has been addressed be-
fore) but to assess the impact of contextual infor-
mation such as discourse and knowledge of the
real-world surroundings. Finally, single-word ut-
terances such as yeah, oh, no, sure, etc. were
downsampled since they are trivial to translate and
were very frequent in the corpus; their inclusion
would therefore bias the development and tuning
of the MT system towards these short utterances
at the expense of longer, more informative utter-
ances.
Table 1 shows the word counts of the trans-
lated meetings after the preprocessing steps de-
scribed above. As an indicator of inter-translator
1228
ID type # utter. # word S-BLEU
ES2008a S 224 2327 21.5
IB4001 NS 419 3879 24.5
IB4002 NS 447 3246 30.5
IB4003 NS 476 5118 24.1
IB4004 NS 593 5696 26.9
IB4005 NS 381 4719 30.4
IS1008a S 191 2058 25.8
IS1008b S 353 3661 24.1
IS1008c S 308 3351 19.6
TS3005a S 245 2339 28.1
Table 1: Sizes and symmetric BLEU scores for
translated meetings from the AMI corpus (S = sce-
nario meeting, NS = non-scenario meeting).
agreement we computed the symmetric BLEU
(S-BLEU) scores on the reference translations
(i.e. using one translation as the reference and the
other as the hypothesis, then switching them and
averaging the results). As we can see, scores are
fairly low overall, indicating large variation in the
translations. This is due to (a) the nature of con-
versational speech, and (b) the linguistic proper-
ties of the target language. Conversational data
contain a fair amount of colloquialisms, referen-
tial expressions, etc. that can be translated in a va-
riety of ways. Additionally, German as the target
language permits many variations in word order
that convey slight differences in emphasis, which
is turn is dependent on the translators? interpreta-
tion of the source sentence. German also has rich
inflectional morphology that varies along with the
choice of words and word order (e.g. verbal mor-
phology depends on which subject is chosen).
2.2 SMT System Training Data
Since transcription and translation of multi-
party spoken conversations is extremely time-
consuming and costly, it is unlikely that parallel
conversational data will ever be produced on a suf-
ficiently large scale for a variety of different meet-
ing types, topics, and target languages. In order to
mimic this situation we trained an initial English-
German SMT system on freely available out-of-
domain data resources. We considered the follow-
ing parallel corpora: news text (de-news1, 1.5M
words), EU parliamentary proceedings (Europarl
(Koehn, 2005), 24M words) and EU legal docu-
ments (JRC Acquis2, 35M words), as well as two
generic English-German machine-readable dictio-
naries3,4 (672k and 140k entries, respectively).
3 Translation Systems
We trained a standard statistical phrase-based
English-German translation system from the re-
sources described above using Moses (Hoang and
Koehn, 2008). Individual language models were
trained for each data source and were then lin-
early interpolated with weights optimized on the
development set. Similarly, individual phrase ta-
bles were trained and were then combined into a
single table. Binary indicator features were added
for each phrase pair, indicating which data source
it was extracted from. Duplicated phrase pairs
were merged into a single entry by averaging their
scores (geometric mean) over all duplicated en-
tries. The weights for binary indicator features
were optimized along with all other standard fea-
tures on the development set. Our previous ex-
perience showed that this method worked better
than the two built-in features in Moses for han-
dling multiple translation tables. We found that
the JRC corpus obtained very small weights; it
was therefore omitted from further system de-
velopment. Table 2 reports results from six dif-
ferent systems: the first (System 1) is a system
that only uses the parallel corpora but not the
external dictionaries listed in Section 2.2. Sys-
tem 2 additionally uses the external dictionar-
ies. All systems use two meetings (IB4002 and
IS1008b) as a development set for tuning model
parameters and five meetings for testing (IB4003-
5,IS1008c,TS3005a). For comparison we also
trained a version of the system where a small in-
domain data set (meetings ES2008a, IB4001, and
IS1008a) was added to the training data (System
3). Finally, we also compared our performance
against Google Translate, which is a state-of-the-
art statistical MT system with unconstrained ac-
1www.iccs.inf.ed.ac.uk/?pkoehn/publications/de-news2http://wt.jrc.it/lt/Acquis/
3http://www.dict.cc
4http://www-user.tu-chemnitz.de/?fri/ding
1229
System description
Dev set Eval set
OOV (%) Trans. Scores OOV (%) Trans. Scores
EN DE BLEU PER EN DE BLEU PER
System 1 OOD parallel data only 4.1 17.0 23.8 49.0 6.5 20.5 21.1 49.5
System 2 System 1 + dictionaries 1.5 15.9 24.6 47.3 2.8 16.3 21.7 48.4
System 3 System 1 + ID parallel data 3.5 13.4 24.7 47.2 5.8 19.7 21.9 48.3
System 4 System 2 + ID parallel data 1.2 12.9 25.4 46.1 2.5 15.9 22.0 48.2
System 5 System 4 + web data 1.2 12.8 26.0 45.9 2.5 15.8 22.1 48.1
System 6 Google Translate ? ? 25.1 49.1 ? ? 23.7 50.8
Table 2: System performance using out-of-domain (OOD) parallel data only vs. combination with a
small amount of in-domain (ID) data and generic dictionaries. For each of the development (DEV)
and evaluation (Eval) set, the table displays the percentages of unknown word types (OOV) for English
(EN) and German (DE), as well as the translation scores of BLEU (%) and PER.
cess to the web as training data (System 6). As
expected, translation performance is fairly poor
compared to the performance generally obtained
on more structured genres. The use of exter-
nal dictionaries helps primarily in reducing PER
scores while BLEU scores are only improved no-
ticeably by adding in-domain data. System 6
shows a more even performance across dev and
eval sets than our trained system, which may re-
flect some degree of overtuning of our systems
to the relatively small development set (about 7K
words). However, the PER scores of System 6 are
significantly worse compared to our in-house sys-
tems.
In order to assess the impact of adding web data
specifically collected to match our meeting corpus
we queried a web portal5 that searches a range of
English-German bilingual web resources and re-
turns parallel text in response to queries in either
English or German. As queries we used English
phrases from our development and evaluation sets
that (a) did not already have phrasal translations
in our phrase tables, (b) had a minimum length
of four words, and (c) occurred at least twice in
the test data. In those cases where the search en-
gine returned results with an exact match on the
English side, we word-aligned the resulting paral-
lel text (about 600k words) by training the word
alignment together with the news text corpus. We
then extracted new phrase pairs (about 3k) from
the aligned data. The phrasal scores assigned to
5http://www.linguee.com
the new phrase pairs were set to 1; the lexical
scores were computed from a word lexicon trained
over both the baseline data resources and the par-
allel web data. However, results (Row 5 in Ta-
ble 2) show that performance hardly improved,
indicating the difficulty in finding matching data
sources for conversational speech.
Table 2 also shows the impact of different data
resources on the percentages of unknown word
types (OOV) for both the source and target lan-
guages. The use of external dictionaries gave the
largest reduction of OOV rates (System 1 vs. Sys-
tem 2 and System 3 vs. System 4), followed by the
use of in-domain data (System 1 vs. System 3 and
System 2 vs. System 4). Since they were retrieved
by multi-word query phrases, adding the web data
did not lead to significant reduction on the OOV
rates (System 4 vs. System 5).
Finally, we also explored a hierarchical phrase-
based system as an alternative baseline system.
The system was trained using the Joshua toolkit
(Li et al, 2009) with the same word alignments
and language models as were used in the standard
phrase-based baseline system (System 4). After
extracting the phrasal (rule) tables for each data
source, they were combined into a single phrasal
(rule) table using the same combination approach
as for the basic phrase-based system. However,
the translation results (BLEU/PER of 24.0/46.6
(dev) and 20.8/47.6 (eval), respectively) did not
show any improvement over the basic phrase-
based system.
1230
4 Analysis of Baseline Translations:
Effect of Contextual Information
The output from System 5 was analyzed manu-
ally in order to assess the importance of model-
ing contextual information. Our goal was not to
determine how translation of meeting style data
can be improved in general ? better translations
could certainly be generated by better syntactic
modeling, addressing morphological variation in
German, and generally improving phrasal cover-
age, in particular for sentences involving collo-
quial expressions. However, these are fairly gen-
eral problems of SMT that have been studied pre-
viously. Instead, our goal was to determine the
relative importance of modeling different contex-
tual factors, such as discourse-level information or
knowledge of the real-world environment, which
have not been studied extensively.
We considered three types of contextual in-
formation: discourse coherence information (in
particular anaphoric relations), knowledge of the
topic or domain, and real-world/multimodal infor-
mation. Anaphoric relations affect the translation
of referring expressions in cases where the source
and target languages make different grammatical
distinctions. For example, German makes more
morphological distinctions in noun phrases than
English. In order to correctly translate an expres-
sion like ?the red one? the grammatical features
of the target language expression for the referent
need to be known. This is only possible if a suf-
ficiently large context is taken into account dur-
ing translation and if the reference is resolved cor-
rectly. Knowledge of the topic or domain is rele-
vant for correctly translating content words and is
closely related to the problem of word sense dis-
ambiguation. In our current setup, topic/domain
knowledge could be particularly helpful because
in-domain training data is lacking and many word
translations are obtained from generic dictionar-
ies that do not assign probabilities to compet-
ing translations. Finally, knowledge of the real-
world environment, such as objects in the room,
other speakers present, etc. determines translation
choices. If a speaker utters the expression ?that
one? while pointing to an object, the correct trans-
lation might depend on the grammatical features
Error type % (dev) % (eval)
Word sense 64.5 68.2
Exophora (addressee) 24.3 23.4
Anaphora 10.2 7.8
Exophora (other) 1.0 0.6
Table 3: Relative frequency of different error
types involving contextual knowledge. The total
number of errors is 715, for 315 sentences.
of the linguistic expression for that object; e.g. in
German, the translation could be ?die da?, ?der
da? or ?das da?. Since the participants in our
meeting corpus use slides and supporting docu-
ments we expect to see some effect of such ex-
ophoric references to external objects.
In order to quantify the influence of contextual
information we manually analyzed the 1-best out-
put of System 5, identified those translation errors
that require knowledge of the topic/domain, larger
discourse, or external environment for their res-
olution, classified them into different categories,
and computed their relative frequencies. We then
corrected these errors in the translation output to
match at least one of the human references, in or-
der to assess the maximum possible improvement
in standard performance scores that could be ob-
tained from contextual modeling. The results are
shown in Tables 3 and 4. We observe that out of all
errors that can be related to the lack of contextual
knowledge, word sense confusions are by far the
most frequent. A smaller percentage of errors is
caused by anaphoric expressions. Contrary to our
expectations, we did not find a strong impact of
exophoric references; however, there is one cru-
cial exception where real-world knowledge does
play an important role. This is the correct transla-
tion of the addressee you. In English, this form is
used for the second person singular, second per-
son plural, and the generic interpretation (as in
?one?, or ?people?). German has three distinct
forms for these cases and, additionally, formal and
informal versions of the second-person pronouns.
The required formal/informal pronouns can only
be determined by prior knowledge of the rela-
tionships among the meeting participants. How-
ever, the singular-plural-generic distinction can
potentially be resolved by multimodal informa-
1231
Original Corrected
BLEU (%) PER BLEU (%) PER
dev 26.0 45.9 27.5 44.0
eval 22.1 48.1 23.3 46.0
Table 4: Scores obtained by correcting errors due
to lack of contextual knowledge.
tion such as gaze, head turns, body movements,
or hand gestures of the current speaker. Since
these errors affect mostly single words as opposed
to larger phrases, the impact of the corrections on
BLEU/PER scores is not large. However, for prac-
tical applications (e.g. information extraction or
human browsing of meeting translations) the cor-
rect translation of content words and referring ex-
pressions would be very important. In the remain-
der of the paper we therefore describe initial ex-
periments designed to address the most important
source of contextual errors, viz. word sense con-
fusions.
5 Resolving Word Sense Disambiguation
Errors
The problem of word sense disambiguation
(WSD) in MT has received a fair amount of
attention before. Initial experiments designed
at integrating a WSD component into an MT
system (Carpuat and Wu, 2005) did not meet
with success; however, WSD was subsequently
demonstrated to be successful in data-matched
conditions (Carpuat and Wu, 2007; Chan et al,
2007). The approach pursued by these latter ap-
proaches is to train a supervised word sense clas-
sifier on different phrase translation options pro-
vided by the phrase table of an initial baseline sys-
tem (i.e. the task is to separate different phrase
senses rather than word senses). The input fea-
tures to the classifier consist of word features ob-
tained from the immediate context of the phrase
in questions, i.e. from the same sentence or from
the two or three preceding sentences. The classi-
fier is usually trained only for those phrases that
are sufficiently frequent in the training data.
By contrast, our problem is quite different.
First, many of the translation errors caused by
choosing the wrong word sense relate to words
obtained from an external dictionary that do not
occur in the parallel training data; there is also lit-
tle in-domain training data available in general.
For these reasons, training a supervised WSD
module is not an option without collecting addi-
tional data. Second, the relevant information for
resolving a word sense distinction is often not lo-
cated in the immediately surrounding context but
it is either at a more distant location in the dis-
course, or it is part of the participants? background
knowledge. For example, in many meetings the
opening remarks refer to slides and an overhead
projector. It is likely that subsequent mention-
ing of slide later on during the conversation also
refer to overhead slides (rather than e.g. slide in
the sense of ?playground equipment?), though the
contextual features that could be used to identify
this word sense are not located in the immedi-
ately preceding sentences. Thus, in contrast to su-
pervised, local phrase sense disambiguation em-
ployed in previous work, we propose to utilize
unsupervised, global word sense disambiguation,
in order to obtain better modeling of the topic
and domain knowledge that is implicitly present
in meeting conversations.
5.1 Unsupervised Word Sense
Disambiguation
Unsupervised WSD algorithms have been pro-
posed previously (e.g. (Navigli and Lapata, 2007;
Cheng et al, 2009)). The general idea is to ex-
ploit measures of word similarity or relatedness
to jointly tag all words in a text with their correct
sense. We adopted the graph-based WSD method
proposed in (Sinha and Mihalcea, 2007), which
represents all word senses in a text as nodes in an
undirected graph G = (V,E). Pairs of nodes are
linked by edges weighted by scores indicating the
similarity or relatedness of the words associated
with the nodes. Given such a graph, the likeli-
hood of each node is derived by the PageRank al-
gorithm (Brin and Page, 1998), which measures
the relative importance of each node to the entire
graph by considering the amount of ?votes? the
node receives from its neighboring nodes. The
PageRank algorithm was originally designed for
directed graphs, but can be easily extended to an
undirected graph. Let PR(vi) denote the PageR-
ank score of vi. The PageRank algorithm itera-
1232
tively updates this score as follows:
PR(vi) = (1 ? d) + d
?
(vi,vj)?E
PR(vj)
wij?
k wkj
where wij is the similarity weight of the undi-
rected edge (vi, vj) and d is a damping factor,
which is typically set to 0.85 (Brin and Page,
1998). The outcome of the PageRank algorithm
is numerical weighting of each node in the graph.
The sense with the highest score for each word
identifies its most likely word sense. For our
purposes, we modified the procedure as follows.
Given a document (meeting transcription), we first
identify all content words in the source document.
The graph is then built over all target-language
translation candidates, i.e. each node represents a
word translation. Edges are then established be-
tween all pairs of nodes for which a word similar-
ity measure can be obtained.
5.2 Word Similarity Measures
We follow (Zesch et al, 2008a) in computing
the semantic similarity of German words by ex-
ploiting the Wikipedia and Wiktionary databases.
We use the publicly available toolkits JWPL and
JWKTL (Zesch et al, 2008b) to retrieve relevant
articles in Wikipedia and entries in Wiktionary for
each German word ? these include the first para-
graphs of Wikipedia articles entitled by the Ger-
man word, the content of Wiktionary entries of
the word itself as well as of closely related words
(hypernyms, hyponyms, synonyms, etc.). We then
concatenate all retrieved material for each word to
construct a pseudo-gloss. We then lowercase and
lemmatize the pseudo-glosses (using the lemma-
tizer available in the TextGrid package 6), exclude
function words by applying a simple stop-word
list, and compute a word similarity measure for
a given pair of words by counting the number of
common words in their glosses.
We need to point out that one drawback in this
approach is the low coverage of German content
words in the Wikipedia and Wiktionary databases.
Although the English edition contains millions
of entries, the German edition of Wikipedia and
Wiktionary is much smaller ? the coverage of all
content words in our task ranges between 53% and
6http://www.textgrid.de/en/beta.html
56%, depending on the meeting, which leads to
graphs with roughly 3K to 5K nodes and 8M to
13M edges. Words that are not covered mostly in-
clude rare words, technical terms, and compound
words.
5.3 Experiments and Results
For each meeting, the derived PageRank scores
were converted into a positive valued feature, re-
ferred to as the WSD feature, by normalization
and exponentiation:
fWSD(wg|we) = exp
{
PR(wg)?
wg?H(we) PR(wg)
}
where PR(wg) is the PageRank score for the Ger-
man word wg and H(we) is the set of all transla-
tion candidates for the English word we. Since
they are not modeled in the graph-based method,
multi-words phrases and words that are not found
in the Wikipedia or Wiktionary databases will re-
ceive the default value 1 for their WSD feature.
The WSD feature was then integrated into the
phrase table to perform translation. The new sys-
tem was optimized as before.
It should be emphasized that the standard mea-
sures of BLEU and PER give an inadequate im-
pression of translation quality, in particular be-
cause of the large variation among the reference
translations, as discussed in Section 4. In many
cases, better word sense disambiguation does not
result in better BLEU scores (since higher gram
matches are not affected) or even PER scores
because although a feasible translation has been
found it does not match any words in the refer-
ence translations. The best way of evaluating the
effect of WSD is to obtain human judgments ?
however, since translation hypotheses change with
every change to the system, our original error an-
notation described in Section 4 cannot be re-used,
and time and resource constraints prevented us
from using manual evaluations at every step dur-
ing system development.
In order to loosen the restrictions imposed by
having only two reference translations, we uti-
lized a German thesaurus7 to automatically ex-
tend the content words in the references with syn-
onyms. This can be seen as an automated way of
7http://www.openthesaurus.de
1233
No WSD With WSD
BLEU (%) PER XPER BLEU (%) PER XPER
dev 25.4 46.1 43.4 25.4 45.6 42.9
eval 22.0 48.2 44.6 22.0 47.9 44.0
IB4003 21.4 48.3 44.4 21.4 47.5 43.8
IB4004 22.4 48.5 44.4 23.1 48.4 43.9
IB4005 25.4 45.9 42.4 25.3 45.6 42.2
IS1008c 15.9 52.9 50.0 14.9 52.3 48.6
TS3005a 23.1 45.2 41.9 23.2 45.3 41.7
Table 5: Performance of systems with and without WSD for dev and eval sets as well as individual
meetings in the eval set.
approximating the larger space of feasible trans-
lations that could be obtained by producing addi-
tional human references. Note that the thesaurus
provided synonyms for only roughly 50% of all
content words in the dev and eval set. For each
of them, on average three synonyms are found in
the thesaurus. We use these extended references
to recompute the PER score as an indicator of
correct word selection. All results (BLEU, PER
and extended PER (or XPER)) are shown in Table
5. As expected, BLEU is not affected but WSD
improves the PER and XPER slightly but consis-
tently. Note that this is despite the fact that only
roughly half of all content words received disam-
biguation scores.
Finally, we provide a concrete example of
translation improvements, with improved words
highlighted:
Source:
on the balcony
there?s that terrace
there?s no place inside the building
Translation, no WSD:
auf dem balkon
es ist das absatz
es gibt keinen platz innerhalb des geba?udes
Translation, with WSD:
auf dem balkon
es ist das terrasse
es gibt keinen platz geba?udeintern
References:
auf dem balkon / auf dem balkon
da gibt es die terrasse / da ist die terrasse
es gibt keinen platz im geba?ude / es gibt keinen
platz innen im geba?ude
6 Summary and Conclusions
We have presented a study on statistical transla-
tion of meeting data that makes the following con-
tributions: to our knowledge it presents the first
quantitative analysis of contextual factors in the
statistical translation of multi-party spoken meet-
ings. This analysis showed that the largest im-
pact could be obtained in the area of word sense
disambiguation using topic and domain knowl-
edge, followed by multimodal information to re-
solve addressees of you. Contrary to our ex-
pectations, further knowledge of the real-world
environment (such as objects in the room) did
not show an effect on translation performance.
Second, it demonstrates the application of unsu-
pervised, global WSD to SMT, whereas previ-
ous work has focused on supervised, local WSD.
Third, it explores definitions derived from col-
laborative Wiki sources (rather than WordNet or
existing dictionaries) for use in machine transla-
tion. We demonstrated small but consistent im-
provements even though word coverage was in-
complete. Future work will be directed at improv-
ing word coverage for the WSD algorithm, in-
vestigating alternative word similarity measures,
and exploring the combination of global and local
WSD techniques.
Acknowledgments
This work was funded by the National Science Foundation
under Grant IIS-0840461 and by a grant from the Univer-
sity of Washington?s Provost Office. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect the
views of the funding organizations.
1234
References
S. Brin and L. Page. 1998. ?The Anatomy of a Large-
Scale Hypertextual Web Search Engine?. Proceed-
ings of WWW7.
C. Callison-Burch, P. Koehn and M. Osborne. 2006.
?Improved Statistical Machine Translation Using
Paraphrases?. Proceedings of NAACL.
M. Carpuat and D. Wu. 2005. ?Word sense disam-
biguation vs. statistical machine translation?. Pro-
ceedings of ACL.
M. Carpuat and D. Wu. 2007. ?Improving statistical
machine translation using word sense disambigua-
tion?. Proceedings of EMNLP-CoNLL.
Y.S. Chan and H.T. Ng and D. Chiang 2007. ?Word
sense disambiguation improves statistical machine
translation?. Proceedings of ACL.
P. Chen, W. Ding, C. Bowes and D. Brown. 2009.
?A fully unsupervised word sense disambiguation
method using dependency knowledge?. Proceed-
ings of NAACL.
E. Gabrilovich and S. Markovitch. 2007 ?Computing
semantic relatedness usingWikipedia-based explicit
semantic analysis?. Proceedings of IJCAI.
R. Haque, S.K. Naskar, Y. Ma and A.Way. 2009. ?Us-
ing supertags as source language context in SMT?.
Proceedings of EAMT.
A.S. Hildebrandt, M. Eck, S. Vogel and A. Waibel.
2005. ?Adaptation of the Translation Model for Sta-
tistical Machine Translation using Information Re-
trieval?. Proceedings of EAMT.
H. Hoang and P. Koehn. 2008. ?Design of the Moses
decoder for statistical machine translation?. Pro-
ceedings of SETQA-NLP.
P. Koehn. 2005. ?Europarl: a parallel corpus for statis-
tical machine translation?. Proceedings of MT Sum-
mit.
V. Kumar, R. Sridhar, S. Narayanan and S. Bangalore.
2008. ?Enriching spoken language translation with
dialog acts?. Proceedings of HLT.
Z. Li et al. 2009. ?Joshua: An Open Source Toolkit
for Parsing-based Machine Translation?. Proceed-
ings of StatMT.
E. Matusov, M. Popovic?, R. Zens and H. Ney. 2004.
?Statistical Machine Translation of Spontaneous
Speech with Scarce Resources?. Proceedings of
IWSLT.
A. McCowan. 2005. ?The AMI meeting corpus?,
H. Mima, O. Furuse and H. Iida. 1998. ?Improving
Performance of Transfer-Driven Machine Transla-
tion with Extra-Linguistic Information from Con-
text, Situation and Environment?. Proceedings of
Coling. Proceedings of the International Confer-
ence on Methods and Techniques in Behavioral Re-
search.
D.S. Munteanu and D. Marcu. 2005. ?Improving
machine translation performance by exploiting non-
parallel corpora?. Computational Linguistics.
R. Navigli and M. Lapata. 2007. ?Graph Connectiv-
ity Measures for Unsupervised Word Sense Disam-
biguation?, Proceedings of IJCAI
S. Rao and I. Lane and T. Schultz. 2007. ?Improving
spoken language translation by automatic disfluency
removal?. Proceedings of MT Summit. 31(4).
R. Sinha and R. Mihalcea. 2007. ?Unsupervised
Graph-based Word Sense Disambiguation Using
Measures of Word Semantic Similarity?, Proceed-
ings of IEEE-ICSC
N. Stroppa, A. Bosch and A. Way. 2007. ?Exploiting
Source Similarity for SMT using Context-Informed
Features?. Proceedings of TMI.
S. Stu?ker, C. Fu?gen, F. Kraft and M. Wo?lfel. 2007.
?The ISL 2007 English Speech Transcription Sys-
tem for European Parliament Speeches?. Proceed-
ings of Interspeech.
A. Waibel and C. Fu?gen. 2008. ?Spoken Language
Translation ? Enabling cross-lingual human-human
communication?. Proceedings of Coling).
W. Wang, G. Tur, J. Zheng and N.F. Ayan. 2010.
?Automatic disfluency removal for improving spo-
ken language translation?. Proceedings of ICASSP.
IEEE Signal Processing Magazine
H. Wu, H. Wang and C. Zong. 2008. ?Domain adapta-
tion for statistical machine translation with domain
dictionary and monolingual corpora?,
T. Zesch, C. Mu?ller and Iryna Gurevych. 2008.
?Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary?. Proceedings of LREC.
T. Zesch, Christof Mu?ler and Iryna Gurevych. 2008.
?Using Wiktionary for Computing Semantic Relat-
edness?.
J. Zheng, W. Wang and N.F. Ayan. 2008. ?Devel-
opment of SRI?s translation systems for broadcast
news and broadcast conversations?. Proceedings of
Interspeech. Proceedings of AAAI.
1235
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131?141,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Submodularity for Data Selection in Statistical Machine Translation
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
kk2@u.washington.edu
Jeff Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
bilmes@u.washington.edu
Abstract
We introduce submodular optimization
to the problem of training data subset
selection for statistical machine translation
(SMT). By explicitly formulating data
selection as a submodular program, we ob-
tain fast scalable selection algorithms with
mathematical performance guarantees, re-
sulting in a unified framework that clarifies
existing approaches and also makes both
new and many previous approaches easily
accessible. We present a new class of
submodular functions designed specifically
for SMT and evaluate them on two differ-
ent translation tasks. Our results show that
our best submodular method significantly
outperforms several baseline methods,
including the widely-used cross-entropy
based data selection method. In addition,
our approach easily scales to large data sets
and is applicable to other data selection
problems in natural language processing.
1 Introduction
SMT has made significant progress over the last
decade, not least due to the availability of increas-
ingly larger data sets. Large-scale SMT systems
are now routinely trained on millions of sentences
of parallel data, and billions of words of mono-
lingual data for language modeling. Large data
sets are often beneficial, but they do create certain
other problems. First, they place higher demands
on computational resources (storage and compute).
Hence, existing software infrastructure may need
to be adapted and optimized to handle such large
data sets. Second, experimental turn-around time
is increased as well, making it more difficult to
quickly train, fine-tune, and evaluate novel model-
ing approaches. Most importantly, however, SMT
performance does not increase linearly with the
training data size but levels off after a certain point.
This is because the additional training data may be
noisy, irrelevant to the task at hand, or inherently
redundant. Thus, a linear increase in the amount of
training data typically leads to a sublinear increase
in performance, an effect known as diminishing
returns. Several recent papers (Bloodgood and
Callison-Burch, 2010; Turchi et al., 2012a; Turchi
et al., 2012b) have amply demonstrated this effect.
A way to counteract this is to perform data sub-
set selection, i.e., choose a subset of the available
training data to optimize a particular quality cri-
terion. One scheme is to select a subset that ex-
presses as much of the information in the original
data set as possible - i.e., the data set should be
?summarized? by excluding redundant information.
Another scheme, popular in the context of SMT, is
to subselect the original training set to match the
properties of a particular test set.
In this paper, we introduce submodularity for
subselecting SMT training data, a methodology
that follows both of the above schemes.
1
Sub-
modular functions (Fujishige, 2005) are a class
of discrete set functions having the property of di-
minishing returns. They occur naturally in a wide
range of problems in a diverse set of fields includ-
ing economics, game theory, operations research,
circuit theory, and more recently machine learn-
ing. Submodular functions share certain properties
with convexity (e.g., naturalness and mathematical
tractability) although submodularity is still quite
distinct from convexity.
We present a novel class of submodular func-
tions particularly suited for SMT subselection and
evaluate it against state-of-the-art baseline meth-
ods on two different translation tasks, showing that
our method outperforms them significantly in most
cases. While many approaches to SMT data se-
lection have been developed previously (a detailed
overview is provided in Section 3), many of them
are heuristic and do not offer performance guaran-
tees. Certain previous approaches, however, have
1
As far as we know, submodularity has not before been
explicitly utilized for SMT subset selection.
131
inadvertently made use of submodular methods.
This, in addition to our own positive results, pro-
vides strong evidence that submodularity is a natu-
ral and practical framework for data subset selec-
tion in SMT and related fields.
An additional advantage of this framework is
that many submodular programs (e.g., the greedy
procedure reviewed in Section 2) are fast and
scalable to large data sets. By contrast, trying
to solve a submodular problem using, say, an
integer-linear programming (ILP) procedure,
would lead to impenetrable scalability problems.
Initial value f(X) = 2 colors in urn.
Updated value f(X?{v}) = 3 with 
added blue ball.
Initial value f(Y) = 3 colors in urn.
Updated value f(Y?{v}) = 3 with 
added blue ball.
X
Y
v v
Figure 1: f (Y ) measures the number of distinct col-
ors in the set of balls Y , and hence is submodular.
This paper makes several contributions: First, we
present a brief overview of submodular functions
(Section 2) and their potential application to natural
language processing (NLP). Next we review pre-
vious approaches to MT data selection (Section 3)
and analyze them with respect to their submodular
properties. We find that some previous approaches
are submodular in nature although this connection
was not heretofore made explicit. Section 4 details
our new approach. We discuss desirable properties
of an SMT data selection objective and present a
new class of submodular functions tailored towards
this problem. Section 5 presents the data and
systems used for the experiments, and results are
reported in Section 6. Section 7 then concludes.
2 Submodular Functions/Optimization
Submodular functions (Edmonds, 1970; Fujishige,
2005), are widely used in mathematics, economics,
circuit theory (Narayanan, 1997), and operations
research. More recently, they have attracted much
interest in machine learning (e.g., (Narasimhan
and Bilmes, 2004; Kolmogorov and Zabih, 2004;
Krause et al., 2008; Krause and Guestrin, 2011;
Jegelka and Bilmes, 2011; Iyer and Bilmes, 2013)),
where they have been applied to a variety of prob-
lems. In natural language and speech processing,
they have been applied to document summariza-
tion (Lin and Bilmes, 2011; Lin and Bilmes, 2012)
and speech data selection (Wei et al., 2013).
We are given a finite size-n set of objects V (i.e.,
|V |= n). A valuation function f : 2
V
? R
+
is de-
fined that returns a non-negative real value for any
subset X ?V . The function f is said to be submodu-
lar if it satisfies the property of diminishing returns:
namely, for all X ? Y and v /? Y , we must have:
f (X ?{v})? f (X)? f (Y ?{v})? f (Y ). (1)
This means that the incremental value (or gain) of
element v decreases when the context in which v
is considered grows from X to Y ? X . We define
the ?gain? as f (v|X), f (X ?{v})? f (X). Hence,
f is submodular if f (v|X)? f (v|Y ). We note that
a function m : 2
V
? R
+
is said to be modular
if it satisfies the above with equality, meaning
m(v|X) = m(v|Y ) for all X ? Y ? V \ {v}. If m
is modular and m( /0) = 0, it can be written as
m(X) =
?
x?X
m(x) and, moreover, is seen simply
as a n-dimensional vector m ? R
V
.
As an example, suppose we have a set V of balls
and f (X) counts the number of colors present
in any subset X ? V . In Figure 1, |X | = 5 and
f (X) = 2, |Y | = 7 and f (Y ) = 3, and X ? Y .
Adding v (a blue ball) to X has a unity gain
f (v|X) = 1 but since a blue ball exists in Y , we
have f (v|Y ) = 0 < f (v|X) = 1.
Submodularity is a natural model for data subset
selection in SMT. In this case, each v ? V is a
distinct training data sentence and V corresponds
to a training set. An important characteristic of
any good model for this problem is that we wish
to decrease the ?value? of a sentence v ?V based
on how much that sentence has in common with
those sentences, say X , that have already been
chosen. The value f (v|X) of a given sentence
v in a context of previously chosen sentences
X ? V further diminishes as the context grows
Y ? X . When, for example, a sentence?s value is
represented as the value of its set of features (e.g.,
n-grams), it is natural for those features? values to
be discounted based on how much representation
of those features already exists in a previously
chosen subset. This corresponds to submodularity,
which can easily be expressed mathematically by
functions such as Eqn. (4) below.
Not only are submodular functions natural for
SMT subset selection, they can also be optimized
efficiently and scalably such that the result has
mathematical performance guarantees. In the re-
mainder of this paper we will assume that f is not
only submodular, but also non-negative ( f (X)? 0
for all X), and monotone non-decreasing ( f (X)?
f (Y ) for all X ? Y ). Such functions are trivial to
uselessly maximize, since f (V ) is the largest possi-
ble valuation. Typically, however, we wish to have
132
Algorithm 1: The Greedy Algorithm
1 Input: Submodular function f : 2
V
? R
+
,
cost vector m, budget b, finite set V .
2 Output: X
k
where k is the number of
iterations.
3 Set X
0
? /0 ; i? 0 ;
4 while m(X
i
)< b do
5 Choose v
i
as follows:
v
i
?
{
argmax
v?V\X
i
f ({v}|X
i
)
m(v)
}
;
6 X
i+1
? X
i
?{v
i
} ; i? i+1 ;
a valuable subset of bounded and small cost, where
cost is measured based on a modular function m(X).
For example, the cost m(v) of a sentence v ? V
might be its length, so m(X) =
?
x?X
m(x) is a sum
of sentence lengths. This leads to the following
optimization problem:
X
?
? argmax
X?V,m(X)?b
f (X), (2)
where b is a known budget. Solving this problem
exactly is NP-complete (Feige, 1998), and express-
ing it as an ILP procedure renders it impractical for
large data sizes. When f is submodular the cost is
just size (m(X) = |X |), then the simple greedy algo-
rithm (detailed below) will have a worst-case guar-
antee of f (
?
X
?
) ? (1? 1/e) f (X
opt
) ? 0.63 f (X
opt
)
where X
opt
is the optimal and
?
X
?
is the greedy so-
lution (Nemhauser et al., 1978).
This constant factor guarantee has practical im-
portance. First, a constant factor guarantee stays
the same as n grows, so the relative worst-case qual-
ity of the solution is the same for small and for big
problem instances. Second, the worst-case result
is achieved only by very contrived and unrealistic
function instances ? the typical case is almost al-
ways much better. Third, the worst-case guarantee
improves depending on the ?curvature? ? ? [0,1]
of the submodular function (Conforti and Cornue-
jols, 1984). When the submodular function is not
fully curved (? < 1, something true of the func-
tions used in this paper), the worst case guarantee
is better, namely
1
?
(1?e
??
) (e.g., a function f with
? = 0.2 has a worst-case guarantee of 0.91). Lastly,
when the cost m is not just cardinality but an arbi-
trary non-negative modular function, a greedy al-
gorithm has similar guarantees (Sviridenko, 2004),
and a scalable variant has a worst-case guarantee
of 1?1/
?
e (Lin and Bilmes, 2010).
The basic greedy algorithm has a very simple
form. Starting with X ? /0, we repeat the operation
X ? X ? argmax
v?V\X
f (v|X)
m(v)
until the budget is
exceeded (m(X) > b) and then backoff to the
previous iteration (complete details are given in
Algorithm 1). While the algorithm has complexity
O(n
2
), there is an accelerated instance of this
algorithm (Minoux, 1978; Leskovec et al., 2007)
that has empirical computational complexity of
O(n logn) where n = |V |. The greedy algorithm,
therefore, scales practically to very large n.
Recently, still much faster (Wei et al., 2014) and
also parallel distributed (Mirzasoleiman et al.,
2013) greedy procedures have been advanced
offering still better scalability.
There are many submodular functions that
are appropriate for subset selection (Lin and
Bilmes, 2011; Lin and Bilmes, 2012). Some
of them are graph-based, where we are given a
non-negative weighted graph G = (V,E,w) and
w : E?R
+
is a set of edge weights (i.e., w(x,y) is
a non-negative similarity score between sentences
x and y). A submodular function is obtained via
a graph cut function f (X) =
?
x?X ,y?V\X
w(x,y)
or via a monotone truncated graph cut
function f (X) =
?
v?V
min(C
v
(X),?C
v
(V ))
where ? ? (0,1) is a scalar parameter and
C
v
(X) =
?
x?X
w(v,x) is a v-specific modular
function. Alternatively, the class of facility loca-
tion functions f (X) =
?
v?V
max
x?X
w(x,v) have
been widely and successfully used in the field of
operations research, and are also applicable here.
In the worse case, the required graph construc-
tion has a worst-case complexity of O(n
2
). While
sparse graphs can be used, this can be prohibitive
when n = |V | gets large. Another class of sub-
modular functions that does not have this prob-
lem is based on a weighted bipartite graph G =
(V,U,E,w) where V are the left vertices, U are the
right vertices, E ? V ?U is a set of edges, and
w : U?R
+
is a set of non-negative weights on the
vertices U . For X ?V , the bipartite neighborhood
function is defined as:
f (X) = w({u ?U : ?x ? X with (x,u) ? E}) (3)
This function is interesting for NLP applications
since U can be seen as a set of ?features? of the ele-
ments v?V (i.e., if V is a set of sentences, U can be
the collective set of n-grams for multiple values of
n, and f (X) is the weight of the n-grams contained
collectively in sentences X).
2
Given a set X ? V ,
2
To be consistent with standard notation in previous liter-
ature, we overload the use of n in ?n-grams? and the size of
our set ?n = |V |?, even though the two ns have no relationship
with each other.
133
we get value from the features of the elements
x ? X , but we get credit for each feature only one
time ? once a given object x ? X has a given fea-
ture u?U , any additions to X by elements also hav-
ing feature u offer no further credit via that feature.
Another interesting class of submodular func-
tions, allowing additional credit from an element
even when its features already exist in X , are what
we call feature-based submodular functions. They
involve sums of non-decreasing concave functions
applied to modular functions (Stobbe and Krause,
2010) and take the following form:
f (X) =
?
u?U
w
u
?
u
(m
u
(X)) (4)
where w
u
> 0 is a feature weight, m
u
(X) =
?
x?X
m
u
(x) is a non-negative modular function
specific to feature u, m
u
(x) is a relevance score (a
non-negative scalar score indicating the relevance
of feature u in object x), and ?
u
is a u-specific
non-negative non-decreasing concave function.
The gain is f (v|X) =
?
u?U
(
?(m
u
(X ? {v}))?
?(m
u
(X))
)
, and thanks to ?
u
?s concavity, the
term ?(m
u
(X ?{v}))??(m
u
(X)) for each feature
u ?U is decaying as X grows. The rate of decay,
and hence the degree of diminishing returns and
ultimately the measure of redundancy of the
information provided by the feature, is controlled
by the concave function. The rate of decay is
also related to the curvature ? of the submodular
function (c.f. ?2), with more aggressive decay
having higher curvature (and a worse worst-case
guarantee). The decay is a modeling choice that
should be decided based on a given application.
Feature-based functions have the advantage that
they do not require the construction of a pairwise
graph; they have a cost of only O(n|U |), which is
linear in the data size and therefore scalable to
large data set sizes.
We utilize this class for our subset selection ex-
periments described in Section 4, where we use one
global concave function ?
u
= ? for all u ?U . In
this work we chose one particular set of features U .
However, given the large body of research into NLP
feature engineering (Jurafsky and Martin, 2009),
this class is extensible beyond just this set, which
makes it suitable for many other NLP applications.
Before describing our SMT-specific functions in
detail, we review previous work on subset selection
for SMT in the context of submodularity.
3 Previous Approaches
There have been many previous approaches to data
subset selection in SMT. In this section, we show
that some of them in fact correspond to submodular
methods, thus introducing a connection between
submodularity and the practical problem of SMT
data selection. The fact that submodularity is
implicitly and unintentionally used in previous
work suggests that it is natural for this problem.
A currently widely-used data selection method in
SMT (which we also use as a baseline in Section 6)
uses the cross-entropy between two language mod-
els (Moore and Lewis, 2010), one trained on the
test set of interest, and another trained on a large set
of generic or out-of-domain training data. We call
this the cross-entropy method. This method trains
a test-set specific (or in-domain) language model,
LM
in
, and a generic (out-of- or mixed-domain) lan-
guage model, LM
out
. Each sentence x ? V in the
training data is given a probability score with both
language models and then ranked in descending
order based on the log ratio
m
ce
(x) =
1
`(x)
log[Pr(x|LM
in
)/Pr(x|LM
out
)] (5)
where `(x) is the length of sentence x. Finally, the
top N sentences are chosen. In (Axelrod et al.,
2011) this method is extended to take both sides
of the parallel corpus into account rather than just
the source side. The cross-entropy approach values
each sentence individually, without regard to any in-
teraction with already selected sentences. This ap-
proach, therefore, is modular (a special case of sub-
modular) and values a set X via m(X) =
?
x?X
m(x).
Moreover, the thresholding method for choosing
a subset corresponds exactly to the optimization
problem in Eqn. (2) where f ? m and the budget
b is set to the sum of the top N sentence scores.
Thanks to modularity, the problem is no longer NP-
complete, and the threshold method solves Eqn. (2)
exactly. On the other hand, a modular function
does not have the diminishing returns property, and
thus has no chance to represent interaction or re-
dundancy between sentences. The chosen subset,
therefore, might have an enormous overrepresenta-
tion of one aspect of the training data while having
minimal or no representation of another aspect, a
major vulnerability of this approach.
Other methods use information retrieval (Hilde-
brand et al., 2005; L?u et al., 2007) which can also
be described as modular function optimization
(e.g., take the top k scoring sentences). Duplicate
134
sentence removal is easily represented by a feature-
based submodular function, Equation (4), where
there is one sentence-specific feature per sentence
and where ?
u
(m
u
(X)) = min(|X ?{u}|,1) ? once
a sentence is chosen, its contribution is saturated
so any duplicate sentence has a gain of zero. Also,
the unseen n-gram function of (Eck et al., 2005;
Bloodgood and Callison-Burch, 2010) corresponds
to a bipartite neighborhood submodular function,
with a weight function defined based on n-gram
counts. Moreover their functions are optimized
using the greedy algorithm; hence they in fact
have a 1? 1/e guarantee. Other methods have
noted and dealt with the existence of redundancy
in phrase-based systems (Ittycheriah and Roukos,
2007) by limiting the set of phrases ? submodular
optimization inherently removes redundancy. Also,
(Callison-Burch et al., 2005; Lopez, 2007) involve
modular functions but where selection is over
subsets of phrases (rather than sentences as in our
current work) and where multiple selections occur,
each specific to an individual test set sentence
rather than the entire test set.
In the feature-decay method, presented in (Bic?ici,
2011; Bic?ici and Yuret, 2011; Bic?ici, 2013), the
value of a sentence is based on its decomposition
into a set of feature values. As sentences are added
to a set, the feature decay approach in general di-
minishes the value of each feature depending on
how much of that feature has already been covered
by those sentences previously chosen ? the pa-
pers define a set of feature decay functions for this
purpose.
Our analysis of (Bic?ici, 2011; Bic?ici and Yuret,
2011; Bic?ici, 2013), from the perspective of sub-
modularity, has revealed an interesting connection.
The feature decay functions used in these papers
turn out to be derivatives of non-decreasing con-
cave functions. For example, in one case ?
?
(a) =
1/(1+ a) which is the derivative of the concave
function ?(a) = ln(1+a). We are given a constant
initialization w
u
for feature u ?U ? in the papers,
they set either w
u
? 1, or w
u
? log(m(V )/m
u
(V )),
or w
u
? log(m(V )/(1+m
u
(V ))), where m(V ) =
?
u
m
u
(V ), and where m
u
(X) =
?
x?X
m
u
(x) is the
count of feature u within the set of sentences
X ?V . This yields the submodular feature function
f
u
(X) = w
u
?(m
u
(X)). The value of sentence v as
measured by feature u in the context of X is the gain
f
u
(v|X), which is a discrete derivative correspond-
ing to w
u
/(1+m
u
(X ?{v})). An alternative decay
function they define is given as ?
?
(a) = 1/(1+b
a
)
for a base b (they set b? 2) which is the derivative
of the following non-decreasing concave function:
?(a) =
[
1?
1
ln(b)
ln
(
1+ exp
(
?a ln(b)
)
)]
(6)
We note that this function is saturating, meaning
that it quickly reaches its asymptote at its maxi-
mum possible value. We can, once again, define
a function specific for feature u ?U as f
u
(X) =
w
u
?(m
u
(X)) with a gain f
u
(v|X) being a discrete
derivative corresponding to w
u
/(1+b
m
u
(X?{v})
).
The connection between this work and submod-
ularity is not complete, however, without consider-
ing the method used for optimization. In fact, Algo-
rithm 1 of (Bic?ici and Yuret, 2011) is precisely the
accelerated greedy algorithm of (Minoux, 1978)
applied to the submodular function corresponding
to f (X) =
?
u?U
f
u
(X), and Algorithm 1 of (Bic?ici,
2013) is the cost-normalized variant of this greedy
algorithm corresponding to a knapsack constraint
(Sviridenko, 2004). Thus, our analysis shows that
these methods also have a 1? 1/e performance
guarantee and also the O(n logn) empirical com-
plexity mentioned in Section 2. This is an impor-
tant connection, as it furthers the evidence that
submodularity is natural for the problem of SMT
subset selection. This also increases the accessibil-
ity of this method since we may view it as a special
case of Equation (4).
Another class of approaches focuses on active
learning. In (Haffari et al., 2009) a large corpus
of noisy parallel data is created automatically; a
smaller set of samples is then selected from this
set that receive human translations. A combination
of several ?informativeness? scores is computed
on a sentence-level basis, and samples are selected
via hierarchical adaptive sampling (Dasgupta and
Hsu, 2008). In (Mandal et al., 2008) a measure
of disagreement between different MT systems, as
well as an entropy-based criterion are used to select
additional data for annotation. In (Bloodgood and
Callison-Burch, 2010) and (Ambati et al., 2010),
active learning is combined with crowd-sourced an-
notations to produce large, human-translated data
sets that are as informative as possible. In (Cao
and Khudanpur, 2012), samples are selected for
discriminative training of an MT system accord-
ing to a greedy algorithm that tries to maximize
overall quality. These methods address a differ-
ent scenario (data selection for annotation or dis-
criminative training) than the one considered here;
however, we also note that the actual selection tech-
niques employed in these papers do not appear to
be submodular.
135
4 Novel Submodular Functions for SMT
In this section, we design a parameterized class
of submodular functions useful for SMT training
data subset selection. By staying within the realm
of submodularity, we retain the advantages of the
greedy algorithm, its theoretical performance as-
surances, and its scalability properties. At the same
time this opens the door to a general framework for
quickly exploring a much larger class of functions
(with the same desirable properties) than before.
It is important to note that we are using sub-
modularity as a ?model? of the selection process,
and the submodular objective acts as a surrogate
for the actual SMT objective function. Thus, the
mathematical guarantee we have is in terms of the
surrogate objective rather than the true SMT ob-
jective. Evaluating one point of the actual SMT
objective would require the complete training and
testing of an SMT system, so even an algorithm as
efficient as Algorithm 1 would be infeasible, even
on small data. It is therefore important to design a
natural and scalable surrogate objective.
We do not consider the graph-based functions
discussed in Section 2 here since they require a
pairwise similarity matrix over all training sen-
tences and thus have O(n
2
) worst-case complexity.
For large tasks with millions or even billions of
sentences, this eventually becomes impractical.
Instead we focus on feature-based functions of the
type presented in Eqn. (4), where each sentence
is represented as a set of features rather than as a
vertex in a graph. In this function there are four
components to specify: 1) U , the linguistic feature
set; 2) m
u
(x), the relevance scores for each feature
u and sentence x; 3) w
u
, the feature weights; and
4) ? , the concave function (we use one concave
function, so ?
u
= ? for all u ?U).
Feature set: U is the set of n-grams from either
the source language U
src
, or from both the source
and target language U
src
?U
tgt
(see Section 6);
since we are interested in selecting a training set
that matches a given test set, we use the set of n-
grams that occur both in the training set and in
the development/test data (for target features, only
development set features are used). I.e., U
src
=
(U
src
dev
?U
src
test
)?U
src
train
and U
tgt
=U
tgt
dev
?U
tgt
train
.
Relevance scores: A feature u within a sentence
x should be valued based on how salient that fea-
ture is within the ?document? in which it occurs;
here, the ?document? is the set of training sen-
tences. This is a task well suited to TFIDF. As
an alternative to raw feature counts we thus also
consider scores of the form m
u
(x)? tfidf(u,x) =
tf(u,x)? idf
trn
(u), where tf(u,x) and idf
trn
(u) are
defined as usual.
Feature weights: We wish to select those training
samples that contain features occurring frequently
in the test data while avoiding the over-selection
of features that are very frequent in the training
data because those are likely to be translated
correctly anyway. This is similar to the approach
in (Moore and Lewis, 2010) (see Equation (5)),
where a log-probability ratio of in-domain to
out-of-domain language model is utilized. In the
present case, we need a value that is specific to
feature u ?U ; a natural approach is to use the ratio
of counts c
tst
(u)/c
trn
(u) where c
tst
(u) is the raw
count of u in the development/test data, and c
trn
(u)
is its raw count in the training data (note that
c
trn
(u) is never zero due to the way U is defined).
As an additional factor we allow feature length
to have an influence. In general, longer n-grams
might be considered more valuable since they
typically lead to better translations and are more
relevant for BLEU. Thus, we include a reward
term for longer n-grams in the form of ?
|u|
where
? ? 1 and |u| is the length of feature u. This gives
greater weight to longer n-grams when ? > 1.
Concave function: It is imperative to find the right
form of concave function since, as described in Sec-
tion 2, the concave shape determines the degree to
which redundancy and diminishing returns are rep-
resented. Intuitively, when the shape of the concave
function for a feature becomes ?flat? rapidly, that
feature quickly looses its ability to provide addi-
tional value to a candidate subset. Many different
concave functions were tested for ? , including one
of the two functions implicit in (Bic?ici and Yuret,
2011) and derived in Section 3, and a variety of
roots of the form ?(a) = a
?
for 0 < ? < 1. In
Table 2, for example, we find evidence that the
simple square root ?(a) =
?
a performs slightly
better than the log function. The square root is
much less curved and decays much more gradually
than either of the two functions implicit in (Bic?ici
and Yuret, 2011), of which one is a log form and
the other is even more curved and quickly satu-
rates (see ?3). The square root function yields a
less curved submodular function, in the sense of
(Conforti and Cornuejols, 1984), resulting in better
worst-case guarantees. Indeed, Table 1 in (Bic?ici
and Yuret, 2011) corroborates by showing that the
more curved saturating function does worse than
the less curved log function.
Four Components Together: Different instantia-
136
tions of the four components discussed above will
result in different submodular functions of the gen-
eral class defined in Eqn. (4). Particular settings
of these general parameters produce the methods
considered in (Bic?ici and Yuret, 2011), thus mak-
ing that approach easily accessible once the general
submodular framework is set up. As a very special
case, this is also true of the cross-entropy method
(Moore and Lewis, 2010), where |U |= 1, m
u
(x)?
exp(m
ce
(x)) of Equation (5)
3
, w
u
? 1, and ?(a) =
a is the identity function. In Section 6, we specify
the parameter settings used in our experiments.
Task Train Dev Test LM
NIST 189M 48k 49k 2.5B
Europarl 52.8M 57.7k 58.1k 53M
Table 1: Data set sizes (number of source-side
words) for MT tasks. LM = language model data.
5 Data and Systems
We evaluate our approach on the NIST Arabic-
English translation task, using the NIST 2006 set
for development and the NIST 2009 set for eval-
uation. The training data consists of all Modern
Standard Arabic-English parallel LDC corpora per-
mitted in the NIST evaluations (minus the restricted
time periods). Together these sets form a mixed-
domain training set containing relevant in-domain
data similar to the NIST data sets but also less rele-
vant data (e.g., the UN parallel corpora); we thus
expect data selection to work well on this task. Ad-
ditional English language modeling data was drawn
from several other LDC corpora (English Giga-
word, AQUAINT, HARD, ANC/DCI and the Amer-
ican National Corpus). Preprocessing included con-
version of the Arabic data to Buckwalter format,
tokenization, spelling normalization, and morpho-
logical segmentation using MADA (Habash et al.,
2009). Numbers and URLs were replaced with
variables. The English data was tokenized and
lowercased. Postprocessing involved recasing the
translation output, replacing variable names with
their original corresponding tokens, and normal-
izing spelling and stray punctuation marks. The
recasing model is an SMT system without reorder-
ing, trained on parallel cased and lowercased ver-
sions of the training data. The recasing model re-
mains fixed for all experiments and is not retrained
3
Due to modularity, any monotone increasing transforma-
tion from m
ce
(x) to m
u
(x) that ensures m
u
(x)? 0 is equivalent.
for different sizes of the training data. Evalua-
tion follows the NIST guidelines and was done by
computing BLEU scores using the official NIST
evaluation tool mteval-v13a.pl with the ?c flag
for case-sensitive scoring. In addition to the NIST
task we also applied our method to the Europarl
German-English translation task. The training data
comes from the Europarl-v7 collection
4
; the devel-
opment set is the 2006 dev set, and the test set is the
2007 test set. The number of reference translations
is 1. The German data was preprocessed by tok-
enization, lower-casing, splitting noun compounds
and lemmatization to address morphological vari-
ation in German. The English side was tokenized
and lowercased. Evaluation was done by comput-
ing BLEU on the lowercased versions of the data.
Since test and training data for this task come from
largely the same domain we expect the training
data to be less redundant or irrelevant; nevertheless
it will be interesting to see how much different data
selection methods can contribute even to in-domain
translation tasks. The sizes of the various data sets
are shown in Table 1.
All translation systems were trained using the
GIZA++/Moses infrastructure (Koehn et al., 2007).
The translation model is a standard phrase-based
model with a maximum phrase length of 7. Since a
large number of experiments had to be run for this
study, more complex hierarchical or syntax-based
translation models were deliberately excluded in
order to limit the experimental turn-around time
needed for each experiment. The reordering model
is a hierarchical model according to (Galley and
Manning, 2008). The feature weights in the log-
linear function were optimized on the development
set BLEU score using minimum error-rate training.
The language models for the NIST task (5-grams)
were trained on three different data sources (Gi-
gaword, GALE data, and all remaining corpora),
which were then interpolated into a single model.
The interpolation weights were optimized sepa-
rately for the two different genres present in the
NIST task (newswire and web text). All models
used Witten-Bell discounting and interpolation of
higher-order and lower-order models. Language
models remain fixed for all experiments, i.e., the
language model training data is not subselected
since we were interested in the effect of data subset
selection on the translation model only. The lan-
guage model for the Europarl system was a 5-gram
trained on Europarl data only.
4
http://http://www.statmt.org/europarl/
137
Method Data Subset Sizes
10% 20% 30% 40%
Rand 0.3991 (? 0.004) 0.4142 (? 0.003) 0.4205 (? 0.002) 0.4220 (? 0.002)
Xent 0.4235 (? 0.004) 0.4292 (? 0.002) 0.4290 (? 0.003) 0.4292 (? 0.001)
SM-1 0.4309 (? 0.000) 0.4367 (? 0.001) 0.4330 (? 0.004) 0.4351 (? 0.002)
SM-2 0.4330
?
(? 0.001) 0.4395
?
(? 0.003) 0.4333 (? 0.001) 0.4366
?
(? 0.003)
SM-3 0.4313
?
(? 0.002) 0.4338 (? 0.002) 0.4361
?
(? 0.002) 0.4351 (? 0.003)
SM-4 0.4276 (? 0.003) 0.4303 (? 0.002) 0.4324 (? 0.002) 0.4329 (? 0.000)
SM-5 0.4285 (? 0.004) 0.4356 (? 0.002) 0.4333 (? 0.003) 0.4324 (? 0.002)
SM-6 0.4302
?
(? 0.004) 0.4334 (? 0.003) 0.4371
?
(? 0.002) 0.4349 (? 0.003)
SM-7 0.4295 (? 0.002) 0.4374 (? 0.002) 0.4344 (? 0.001) 0.4314 (? 0.0004)
SM-8 0.4304
?
(? 0.002) 0.4323 (? 0.000) 0.4358 (? 0.003) 0.4337 (? 0.001)
100% 0.4257
Table 2: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand),
cross-entropy (Xent), and submodular (SM) data selection methods defined in Table 4. 100% = system
using all of the training data. Boldface numbers indicate a statistically significant improvement (p? 0.05)
over the median Xent system. Starred scores are also significantly better than SM-5.
Method Data Subset Sizes
10% 20% 30% 40%
Rand 0.2590 (? 0.003) 0.2652 (? 0.001) 0.2677 (? 0.002) 0.2697 (? 0.001)
Xent 0.2639 (? 0.002) 0.2687 (? 0.002) 0.2704 (? 0.001) 0.2723 (? 0.001)
SM-5 0.2653 (? 0.001) 0.2727 (? 0.000) 0.2697 (? 0.002) 0.2720 (? 0.002)
SM-6 0.2697? (? 0.001) 0.2700 (? 0.002) 0.2740? (? 0.002) 0.2723 (? 0.000)
100% 0.2651
Table 3: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross-
entropy (Xent), and submodular (SM) data selection methods. 100% = system using all of the training
data. Boldface numbers indicate a statistically significant improvement (p? 0.05) over the median Xent
system. Starred scores are significantly better than SM-5.
6 Experiments
Function parameters
w(u) ?(a) m
u
(x) U
SM-1
c
tst
(u)
c
trn
(u)
?
|u|
?
a tfidf(u,x) U
src
SM-2
?
c
tst
(u)
c
trn
(u)
?
|u|
?
a tfidf(u,x) U
src
?U
tgt
SM-3
c
tst
(u)
c
trn
(u)
?
|u|
?
a c(u,x) U
src
SM-4 c
tst
(u)
?
a tfidf(u,x) U
src
SM-5 1 ln(1+a) c(u,x) U
src
SM-6
?
c
tst
(u)
c
trn
(u)
?
a tfidf(u,x) U
src
SM-7
c
tst
(u)
c
trn
(u)
?
(a) tfidf(u,x) U
src
?U
tgt
SM-8
c
tst
(u)
c
trn
(u)
ln(1+a) tfidf(u,x) U
src
?U
tgt
Table 4: Different instantiations of the general sub-
modular function in Eq. 4 (? = 1.5 in all cases).
We first trained a baseline system on 100% of
the training data. Different data selection methods
were then used to select subsets of 10%, 20%, 30%
and 40% of the data. While not reported in the
tables, above 40%, the performance slowly drops
to the 100% performance.
The first baseline selection method utilizes ran-
dom data selection, for which 3 different data sets
of the specified size were drawn randomly from
the training data. Individual systems were trained
on all random subsets of the same size, and their
scores were averaged. The second baseline is
the cross-entropy method by (Moore and Lewis,
2010). In-domain language models were trained on
the combined development and test data, and out-
of-domain models were trained on an equivalent
amount of data drawn randomly from the training
set. Sentences were ranked by the function in Eq. 5,
and the top k percent were chosen. The order of the
n-gram models was optimized on the development
set and was found to be 3. Larger model orders
resulted in worse performance, possibly due to the
138
limited size of the data used for their training. Since
this method also involves random data selection,
we report the average BLEU score over 5 different
trials. For the submodular selection method, Ta-
ble 4 shows the different values that were tested for
the four components listed in Section 4. The combi-
nation was optimized on the development set. The
selection algorithm (Alg. 1) runs within a few min-
utes on our complete training set of 189M words.
Results on the NIST 2009 test set are shown in
Table 2. The scores for the submodular systems
are averages over 3 different runs of MERT tuning.
Random data subset selection (Row 1) falls short
of the baseline system using 100% of the training
data. The cross-entropy method (Row 2) surpasses
the performance of the baseline system at about
20% of the data, demonstrating that data subset
selection is a suitable technique for such mixed-
domain translation tasks. The following rows show
results for the various submodular functions shown
in Table 4. Out of these, SM-5 corresponds to the
best approach in (Bic?ici and Yuret, 2011). SM-6
is our own best-performing function, beating the
cross-entropy method by a statistically significant
margin (p? 0.05) under all conditions.
5
SM-6 is
also significantly better than SM-5 in two cases.
Finally, it surpasses the performance of the all-data
system at only 10% of the training data; possibly,
even smaller training data sets could be used
but this option was not investigated. While the
bilingual submodular functions SM-2 and SM-7)
yield an improvement of up to 0.015 BLEU points
on the dev set (not shown in the table), they do not
consistently outperform the monolingual functions
on the test set. Since test set target features cannot
be used in our scenario, bilingual features are
only helpful to the extent that the development set
closely matches the test set. However, target fea-
tures should be quite helpful when selecting data
from an out-of-domain set to match an in-domain
training set (as in e.g. (Axelrod et al., 2011)). We
found no gain from the length reward ?
|u|
.
The Europarl results (Table 3) show a similar
pattern. Although the differences in BLEU scores
are smaller overall (as expected on an in-domain
translation task), data subset selection improves
over the all-data baseline system in this case as
well. The cross-entropy method again outperforms
random data selection. On this task we only tested
our submodular function that worked best on the
5
Statistical significance was measured using the paired
bootstrap resampling test of (Koehn, 2004), applied to the
systems with the median BLEU scores.
NIST task; again we find that it outperforms the
cross-entropy method. In two conditions (10% and
30%) these differences are statistically significant.
10% of the training data suffices to outperform the
all-data system, and up to a full BLEU point can be
gained on this task using 20-30% of the data and a
submodular data selection method.
7 Conclusions
We have introduced submodularity to SMT data
subset selection, generalizing previous approaches
to this problem. Our method has theoretical perfor-
mance guarantees, comes with scalable algorithms,
and significantly improves over current, widely-
used data selection methods on two different trans-
lation tasks. There are many possible extensions
to this work. One strategy would be to extend the
feature set U with features representing different
types of linguistic information - e.g., when using
a syntax-based system it might be advantageous
to select training data that covers the set of syn-
tactic structures seen in the test data. Secondly,
the selected data was test data specific. In some
contexts, it is not possible to train test data spe-
cific systems dynamically; in that case, different
submodular functions could be designed to select
a representative ?summary? of the training data.
Finally, the use of submodular functions for subset
selection is applicable to other data sets that can
be represented as features or as a pairwise similar-
ity graph. Submodularity thus can be applied to a
wide range of problems in NLP beyond machine
translation.
Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263, and is also supported by the National Science
Foundation under Grant No. (IIS-1162606), and by
a Google, a Microsoft, and an Intel research award.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of In-
telligence Advanced Research Projects Activity
(IARPA) or the U.S. Government.
139
References
[Ambati et al.2010] V. Ambati, S. Vogel, and J. Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In Proceedings of LREC,
pages 2169?2174, Valletta, Malta.
[Axelrod et al.2011] A. Axelrod, X. He, and J. Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland.
[Bic?ici and Yuret2011] E. Bic?ici and D. Yuret. 2011.
Instance selection for machine translation using fea-
ture decay algorithms. In Proceedings of the 6th
Workshop on Statistical Machine Translation, pages
272?283.
[Bic?ici2013] E. Bic?ici. 2013. Feature decay algorithms
for fast deployment of accurate statistical machine
translation systems. In Proceedings of the 8th Work-
shop on Statistical Machine Translation, pages 78?
84.
[Bic?ici2011] E. Bic?ici. 2011. The Regression Model of
Machine Translation. Ph.D. thesis, KOC? University.
[Bloodgood and Callison-Burch2010] M. Bloodgood
and C. Callison-Burch. 2010. Bucking the trend:
large-scale cost-focused active learning for statis-
tical machine translation. In Proceedings of ACL,
pages 854?864.
[Callison-Burch et al.2005] Chris Callison-Burch,
Colin Bannard, and Josh Schroeder. 2005. Scaling
phrase-based statistical machine translation to larger
corpora and longer phrases. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 255?262. Association for
Computational Linguistics.
[Cao and Khudanpur2012] Y. Cao and S. Khudanpur.
2012. Sample selection for large-scale MT discrimi-
native training. In Proceedings of AMTA.
[Conforti and Cornuejols1984] M. Conforti and G. Cor-
nuejols. 1984. Submodular set functions, matroids
and the greedy algorithm: tight worst-case bounds
and some generalizations of the Rado-Edmonds the-
orem. Discrete Applied Mathematics, 7(3):251?
274.
[Dasgupta and Hsu2008] S. Dasgupta and D. Hsu.
2008. Hierarchical sampling for active learning. In
Proceedings of ICML.
[Eck et al.2005] M. Eck, S. Vogel, and A. Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram frequency and tf-idf. In Pro-
ceedings of the 10th Machine Translation Summit X,
pages 227?234.
[Edmonds1970] J. Edmonds, 1970. Combinatorial
Structures and their Applications, chapter Submodu-
lar functions, matroids and certain polyhedra, pages
69?87. Gordon and Breach.
[Feige1998] U. Feige. 1998. A threshold of ln n for ap-
proximating set cover. Journal of the ACM (JACM),
45(4):634?652.
[Fujishige2005] S. Fujishige. 2005. Submodular func-
tions and optimization. Annals of Discrete Mathe-
matics, volume 58. Elsevier Science.
[Galley and Manning2008] M. Galley and C. D. Man-
ning. 2008. A simple and effective hierarchical
phrase reordering model. In Proceedings of EMNLP,
pages 847?855.
[Habash et al.2009] N. Habash, O. Rambow, and
R. Roth. 2009. A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceed-
ings of the MEDAR conference, pages 102?109.
[Haffari et al.2009] G. Haffari, M. Roy, and A. Sarkar.
2009. Active learning for statistical machine transla-
tion. In Proceedings of HLT, pages 415?423.
[Hildebrand et al.2005] A. Hildebrand, M. Eck, S. Vo-
gel, and A. Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of EAMT,
pages 133?142.
[Ittycheriah and Roukos2007] A. Ittycheriah and
S. Roukos. 2007. Direct translation model 2. In
Proceedings of HLT/NAACL, page 5764.
[Iyer and Bilmes2013] R. Iyer and J. Bilmes. 2013.
Submodular optimization with submodular cover
and submodular knapsack constraints. In Neural In-
formation Processing Society (NIPS), Lake Tahoe,
CA, December.
[Jegelka and Bilmes2011] Stefanie Jegelka and Jeff A.
Bilmes. 2011. Submodularity beyond submodular
energies: coupling edges in graph cuts. In Computer
Vision and Pattern Recognition (CVPR), Colorado
Springs, CO, June.
[Jurafsky and Martin2009] D. Jurafsky and J. H. Mar-
tin. 2009. Speech and Language Processing:
An Introduction to Natural Language Processing,
Speech Recognition, and Computational Linguistics.
Prentice-Hall, 2nd edition.
[Koehn et al.2007] P. Koehn, H. Hoang, A. Birch,
C. Callison-Burch, M. Federico, N. Bertoldi,
B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer,
O. Bojar, A. Constantin, and E. Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL.
[Koehn2004] P. Koehn. 2004. Statistical significance
tests for machine translation evaluation. In Proceed-
ings of EMNLP.
[Kolmogorov and Zabih2004] V. Kolmogorov and
R. Zabih. 2004. What energy functions can
be minimized via graph cuts? IEEE TPAMI,
26(2):147?159.
140
[Krause and Guestrin2011] A. Krause and C. Guestrin.
2011. Submodularity and its applications in opti-
mized information gathering. ACM Transactions on
Intelligent Systems and Technology, 2(4).
[Krause et al.2008] A. Krause, H.B. McMahan,
C. Guestrin, and A. Gupta. 2008. Robust sub-
modular observation selection. Journal of Machine
Learning Research, 9:2761?2801.
[Leskovec et al.2007] J. Leskovec, A. Krause,
C. Guestrin, C. Faloutsos, J. VanBriesen, and
N. Glance. 2007. Cost-effective outbreak detection
in networks. In Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 420?429.
[Lin and Bilmes2010] H. Lin and J. Bilmes. 2010.
Multi-document summarization via budgeted maxi-
mization of submodular functions. In Proceedings
of NAACL-HLT, pages 2761?2801.
[Lin and Bilmes2011] H. Lin and J. Bilmes. 2011. A
class of submodular functions for document summa-
rization. In Proceedings of ACL, pages 510?520.
[Lin and Bilmes2012] H. Lin and J. Bilmes. 2012.
Learning mixtures of submodular shells with appli-
cation to document summarization. In Uncertainty
in Artifical Intelligence (UAI), Catalina Island, USA,
July. AUAI.
[Lopez2007] A. Lopez. 2007. Hierarchical phrase-
based translation with suffix arrays. In EMNLP-
CoNLL, pages 976?985.
[L?u et al.2007] Y. L?u, J. Huang, and Q. Liu. 2007. Im-
proving statistical machine translation performance
by training data selection and optimization. In Pro-
ceedings of EMNLP, pages 343?350.
[Mandal et al.2008] A. Mandal, D. Vergyri, W. Wang,
J. Zheng, A. Stolcke, D. Hakkani-T?ur G. T?ur, and
N.F. Ayan. 2008. Efficient data selection for ma-
chine translation. In Proceedings of the Spoken Lan-
guage Technology Workshop, pages 261?264.
[Minoux1978] M. Minoux. 1978. Accelerated greedy
algorithms for maximizing submodular functions.
In Lecture Notes in Control and Information Sci-
ences, volume 7, pages 234?243.
[Mirzasoleiman et al.2013] B. Mirzasoleiman, A. Kar-
basi, R. Sarkar, and A. Krause. 2013. Distributed
submodular maximization: Identifying representa-
tive elements in massive data. In Neural Information
Processing Systems (NIPS).
[Moore and Lewis2010] R. Moore and W. Lewis. 2010.
Intelligent selection of language model training data.
In Proceedings of the Association for Computational
Linguistics, pages 220?224.
[Narasimhan and Bilmes2004] M. Narasimhan and
J. Bilmes. 2004. PAC-learning bounded tree-width
graphical models. In Uncertainty in Artificial Intel-
ligence: Proceedings of the Twentieth Conference
(UAI-2004). Morgan Kaufmann Publishers, July.
[Narayanan1997] H. Narayanan. 1997. Submodular
functions and electrical networks. Elsevier.
[Nemhauser et al.1978] G.L. Nemhauser, L.A. Wolsey,
and M.L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions i.
Mathematical Programming, 14:265?294.
[Stobbe and Krause2010] P. Stobbe and A. Krause.
2010. Efficient minimization of decomposable sub-
modular functions. In NIPS.
[Sviridenko2004] M. Sviridenko. 2004. A note on
maximizing a submodular set function subject to a
knapsack constraint. Operations Research Letters,
32(1):41?43.
[Turchi et al.2012a] M. Turchi, T. de Bie, C. Goutte,
and N. Cristianini. 2012a. Learning to translate: A
statistical and computational analysis. Advances in
Artificial Intellligence, 2012:484580:15 pages.
[Turchi et al.2012b] M. Turchi, C. Goutte, and N. Cris-
tianini. 2012b. Learning machine translation from
in-domain and out-of-domain data. In Proceedings
of EAMT, Trento, Italy.
[Wei et al.2013] K. Wei, Y. Liu, K. Kirchhoff, and
J. Bilmes. 2013. Using document summarization
techniques for speech data subset selection. In Pro-
ceedings of NAACL, pages 721?726, Atlanta, Geor-
gia, June.
[Wei et al.2014] K. Wei, R. Iyer, and Jeff Bilmes. 2014.
Fast multi-stage submodular maximization. In Pro-
ceedings of ICML, Beijing, China.
141
Proceedings of NAACL-HLT 2013, pages 721?726,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Document Summarization Techniques for Speech Data Subset
Selection
Kai Wei?, Yuzong Liu?, Katrin Kirchhoff , Jeff Bilmes
Department of Eletrical Engineering
University of Washington
Seattle, WA 98195, USA
{kaiwei,yzliu,katrin,bilmes}@ee.washington.edu
Abstract
In this paper we leverage methods from sub-
modular function optimization developed for
document summarization and apply them to
the problem of subselecting acoustic data. We
evaluate our results on data subset selection
for a phone recognition task. Our framework
shows significant improvements over random
selection and previously proposed methods us-
ing a similar amount of resources.
1 Introduction
Present-day applications in spoken language technol-
ogy (speech recognizers, keyword spotters, etc.) can
draw on an unprecedented amount of training data.
However, larger data sets come with increased de-
mands on computational resources; moreover, they
tend to include redundant information as their size
increases. Therefore, the performance gain curves
of large-scale systems with respect to the amount of
training data often show ?diminishing returns?: new
data is often less valuable (in terms of performance
gain) when added to a larger pre-existing data set than
when added to a smaller pre-existing set (e.g.,(Moore,
2003)). Therefore it is of prime importance to de-
velop methods for data subset selection. We distin-
guish two data subselection scenarios: (a) a priori
selection of a data set before (re-)training a system;
in this case the goal is to subselect the existing data
set as well as possible, eliminating redundant infor-
mation; (b) selection for adaptation, where the goal
?These authors are joint first authors with equal contribu-
tions.
is to tune a system to a known development or test
set. While many studies have addressed the second
scenario, this paper investigates the first: our goal is
to select a smaller subset of the data that fits a given
?budget? (e.g. maximum number of hours of data) but
provides, to the extent possible, as much information
as the complete data set. Additionally, our selection
method should be a low-resource method that does
not require an already-trained complex system such
as an existing word recognizer.
This problem is akin to unsupervised data ?sum-
marization?. In (Lin and Bilmes, 2009) a novel class
of summarization techniques based on submodular
function optimization were proposed for extractive
document summarization. Interestingly, these meth-
ods can also be applied to speech data ?summariza-
tion? with only small modifications. In the following
sections we develop a submodular framework for
speech data summarization and evaluate it on a proof-
of-concept phone recognition task.
2 Related Work
Most approaches to data subset selection in speech
have relied on ?rank-and-select? approaches that de-
termine the utility of each sample in the data set,
rank all samples according to their utility scores, and
then select the top N samples. In weakly supervised
approaches (e.g.,(Kemp and Waibel, 1998; Lamel
et al, 2002; Hakkani-Tur et al, 2002), utility is re-
lated to the confidence of an existing word recognizer
on new data samples: untranscribed training data is
automatically transcribed using an existing baseline
speech recognizer, and individual utterances are se-
lected as additional training data if they have low
721
confidence. These are active learning approaches
suitable for a scenario where a well-trained speech
recognizer is already available and additional data
for retraining needs to be selected. However, we
would like to reduce available training data ahead of
time with a low-resource approach. In (Chen et al,
2009) individual samples are selected for the purpose
of discriminative training by considering phone ac-
curacy and the frame-level entropy of the Gaussian
posteriors. (Itoh et al, 2012) use a utility function
consisting of the entropy of word hypothesis N-best
lists and the representativeness of the sample using a
phone-based TF-IDF measure. The latter is compa-
rable to methods used in this paper, though the first
term in their objective function still requires a word
recognizer. In (Wu et al, 2007) acoustic training data
associated with transcriptions is subselected to max-
imize the entropy of the distribution over linguistic
units (phones or words). Most importantly, all these
methods select samples in a greedy fashion without
optimality guarantees. As we will explain in the next
section, greedy selection is near-optimal only when
applied to monotone submodular functions.
3 Submodular Functions
Submodular functions (Edmonds, 1970) have been
widely studied in mathematics, economics, and op-
erations research and have recently attracted interest
in machine learning (Krause and Guestrin, 2011). A
submodular function is defined as follows: Given a fi-
nite ground set of objects (samples) V = {v1, ..., vn}
and a function f : 2V ? R+ that returns a real value
for any subset S ? V , f is submodular if ?A ? B,
and v /? B, f(A+ v)? f(A) ? f(B + v)? f(B).
That is, the incremental ?value? of v decreases when
the set in which v is considered grows from A to B.
Powerful optimization guarantees exist for certain
subtypes of submodular functions. If, for example,
the function is monotone submodular, i.e. ?A ?
B, f(A) ? f(B), then it can be maximized, under
a cardinality constraint, by a greedy algorithm that
scales to extremely large data sets, and finds a solu-
tion guaranteed to approximate the optimal solution
to within a constant factor 1? 1/e (Nemhauser et al,
1978). Submodular functions can be considered the
discrete analog of convexity.
3.1 Submodular Document Summarization
In (Lin and Bilmes, 2011) submodular functions were
recently applied to extractive document summariza-
tion. The problem was formulated as a monotone
submodular function that had to be maximized sub-
ject to cardinality or knapsack constraints:
argmaxS?V {f(S) : c(S) ? K} (1)
where V is the set of sentences to be summarized, K
is the maximum number of sentences to be selected,
and c(?) ? 0 is sentence cost. f(S) was instantiated
by a form of saturated coverage:
fSC(S) =
?
i?V
min{Ci(S), ?Ci(V )} (2)
where Ci(S) =
?
j?S wij , and where wij ? 0 in-
dicates the similarity between sentences i and j ?
Ci : 2V ? R is itself monotone submodular (modu-
lar in fact) and 0 ? ? ? 1 is a saturation coefficient.
fSC(S) is monotone submodular and therefore has
the previously mentioned performance guarantees.
The weighting function w was implemented as the
cosine similarity between TF-IDF weighted n-gram
count vectors for the sentences in the dataset.
3.2 Submodular Speech Summarization
Similar to the procedure described above we can treat
the task of subselecting an acoustic data set as an
extractive summarization problem. For our a priori
data selection scenario we would like to extract those
training samples that jointly are representative of
the total data set. Initial explorations of submodular
functions for speech data can be found in (Lin and
Bilmes, 2009), where submodular functions were
used in combination with a purely acoustic similarity
measure (Fisher kernel). In addition Equation 2 the
facility location function was used:
ffac(S) =
?
i?V
max
j?S
wij (3)
Here our focus is on utilizing methods that move
beyond purely acoustic similarity measures and con-
sider kernels derived from discrete representations
of the acoustic signal. To this end we first run a to-
kenizer over the acoustic signal that converts it into
a sequence of discrete labels. In our case we use a
722
simple bottom-up monophone recognizer (without
higher-level constraints such as a phone language
model) that produces phone labels. We then use the
hypothesized sequence of phonetic labels to compute
two different sentence similarity measures: (a) co-
sine similarity using TF-IDF weighted phone n-gram
counts, and (b) string kernels. We compare their
performance to that of the Fisher kernel as a purely
acoustic similarity measure.
TF-IDF weighted cosine similarity
The cosine similarity between phone sequences si
and sj is computed as
simij =
?
w?si tfw,si ? tfw,sj ? idf
2
w
??
w?si tf
2
w,si idf
2
w
??
w?sj tf
2
w,sj idf
2
w
(4)
where tfw,si is the count of n-gram w in si and idfw
is the inverse document count of w (each sentence is
a ?document?). We use n = 1, 2, 3.
String kernel
The particular string kernel we use is a gapped,
weighted subsequence kernel of the type described in
(Rousu and Shawe-Taylor, 2005). Formally, we de-
fine a sentence s as a concatenation of symbols from
a finite alphabet ? (here the inventory of phones) and
an embedding function from strings to feature vec-
tors, ? : ?? ? H. The string kernel function K(s, t)
computes the distance between the resulting vectors
for two sentences si and sj . The embedding function
is defined as
?ku(s) :=
?
i:u=s(i)
?|i| u ? ?k (5)
where k is the maximum length of subsequences,
|i| is the length of i, and ? is a penalty parameter
for each gap encountered in the subsequence. K is
defined as
K(si, sj) =
?
u
??u(si), ?u(sj)?wu (6)
where w is a weight dependent on the length of
u, l(u). Finally, the kernel score is normalized by?
K(si, si) ? K(sj , sj) to discourage long sentences
from being favored.
Fisher kernel
The Fisher kernel is based on the vector of derivatives
UX of the log-likelihood of the acoustic data (X)
with respect to the parameters in the phone HMMs
?1, ..., ?m for m models, having similarity score:
simij = (max
i?,j?
di?j?)? dij , where dij = ||U ?i ? U
?
j ||1,
U ?X = 5? logP (X|?), and U
?
X = U
?1
X ? U
?2
x , ..., ?U
?m
X .
4 Data and Systems
We evaluate our approach on subselecting training
data from the TIMIT corpus for training a phone rec-
ognizer. Although this not a large-scale data task, it
is an appropriate proof-of-concept task for rapidly
testing different combinations of submodular func-
tions and similarity measures. Our goal is to focus
on acoustic modeling only; we thus look at phone
recognition performance and do not have to take into
account potential interactions with a language model.
We also chose a simple acoustic model, a monophone
HMM recognizer, rather than a more powerful but
computationally complex model in order to ensure
quick experimental turnaround time. Note that the
goal of this study is not to obtain the highest phone
accuracy possible; what is important is the relative
performance of the different subset selection meth-
ods, especially on small data subsets.
The sizes of the training, development and test data
are 4620, 200 and 192 utterances, respectively. Pre-
processing was done by extracting 39-dimensional
MFCC feature vectors every 10 ms, with a window
of 25.6ms. Speaker mean and variance normaliza-
tion was applied. A 16-component Gaussian mixture
monophone HMM system was trained on the full data
set to generate parameters for the Fisher kernel and
phone sequences for the string kernel and TF-IDF
based similarity measures.
Following the selection of subsets (2.5%, 5%, 10%,
20%, 30%, 40%, 50%, 60%, 70% and 80% of the
data, measured as percentage of non-silence speech
frames), we train a 3-state HMM monophone recog-
nizer for all 48 TIMIT phone classes on the result-
ing sets and evaluate the performance on the core
test set of 192 utterances, collapsing the 48 classes
into 39 in line with standard practice (Lee and Hon,
1989). The HMM state output distributions are mod-
eled by diagonal-covariance Gaussian mixtures with
the number of Gaussians ranging between 4 and 64,
depending on the data size.
As a baseline we perform 100 random draws of
the specified subset sizes and average the results.
723
The second baseline consists of the method in (Wu et
al., 2007), where utterances are selected to maximize
the entropy of the distribution over phones in the
selected subset.
5 Experiments
We tested the three different similarity measures de-
scribed above in combination with the submodular
functions in Equations 2 and 3. The parameters of
the gapped string kernel (i.e. the kernel order (k), the
gap penalty (?), and the contiguous substring length
l) were optimized on the development set. The best
values were ? = 0.1, k = 4, l = 3. We found that
facility location was superior to saturated cover func-
tion across the board.
Comparison of different data subset selection methods 
Phone Accuracy (%)
P
e
r
c
e
n
t
a
g
e
 
o
f
 
S
p
e
e
c
h
 
i
n
 
S
e
l
e
c
t
e
d
 
S
u
b
s
e
t
40# 45# 50# 55# 60# 65#
80#
70#
60#
50#
40#
30#
20#
10#
5#
2.5# string#kernel#TF7IDF#trigram#TF7IDF#bigram#TF7IDF#unigram#Fisher#kernel#entropy#random#
Figure 1: Phone accuracy for different subset sizes; each
block of bars lists, from bottom to top: random baseline,
entropy baseline, Fisher kernel, TF-IDF (unigram), TF-
IDF (bigram), TF-IDF (trigram), string kernel.
Figure 1 shows the performance of the random and
entropy-based baselines as well as the performance
of the facility location function with different sim-
ilarity measures. The entropy-based baseline beats
the random baseline for most percentage cases but
is otherwise the lowest-performing method overall.
Note that this baseline uses the true transcriptions in
line with (Wu et al, 2007) rather than the hypothe-
sized phone labels output by our recognizer. The low
performance and the fact that it is even outperformed
by the random baseline in the 2.5% and 70% cases
P
e
r
c
e
n
t
a
g
e
 
o
f
 
S
p
e
e
c
h
 
i
n
 
S
e
l
e
c
t
e
d
 
S
u
b
s
e
t
Phone Accuracy (%)
Comparison of different submodular functions
45
50
55
60
65
2.5 5 10 20 30 40 60 60 70 80
Figure 2: Phone accuracy obtained by random selection,
facility location function, and saturated coverage function
(string kernel similarity measure).
may be because the selection method encourages
highly diverse but not very representative subsets.
Furthermore, the entropy-based baseline utilizes a
non-submodular objective function with a heuristic
greedy search method. No theoretical guarantee of
optimality can be made for the subset found by this
method.
Among the different similarity measures the Fisher
kernel outperforms the baseline methods but has
lower performance than the TF-IDF kernel and the
string kernel. The best performance is obtained with
the string kernel, especially when using small train-
ing data sets (2.5%-10%). The submodular selection
methods yield significant improvements (p < 0.05)
over both the random baseline and over the entropy-
based method.
We also investigated using different submodular
functions, i.e. the facility location function and the
saturated coverage function. Figure 2 shows the per-
formance of the facility location (ffac) and saturated
coverage (fSC) functions in combination with the
string kernel similarity measure. The reason ffac
outperforms fSC is that fSC primarily controls for
over-coverage of any element not in the subset via the
? saturation hyper-parameter. However, it does not
ensure that every non-selected element has good rep-
resentation in the subset. fSC measures the quality of
the subset by how well each individual element out-
side the subset has a surrogate within the subset (via
724
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
TF-IDF bigramTF-IDF unigram
string kernelTF-IDF trigram
Figure 3: Phone accuracy for true vs. hypothesized phone
labels, for string-based similarity measures.
the max function) and hence tends to model complete
coverage better, leading to better results.
Finally we examined whether using hypothesized
phone sequences vs. the true transcriptions has nega-
tive effects. Figure 3 shows that this is not the case:
interestingly, the hypothesized labels even result in
slightly better results. This may be because the rec-
ognized phone sequences are a function of both the
underlying phonetic sequences that were spoken and
the acoustic signal characteristics, such as the speaker
and channel. The true transcriptions, on the other
hand, are able to provide information only about pho-
netic as opposed to acoustic characteristics.
6 Discussion
We have presented a low-resource framework for
acoustic data subset selection based on submodular
function optimization, which was previously devel-
oped for document summarization. Evaluation on a
proof-of-concept task has shown that the method is
successful at selecting data subsets that outperform
subsets selected randomly or by a previously pro-
posed low-resource method. We note that the best
selection strategies for the experimental conditions
tested here involve similarity measures based on a
discrete tokenization of the speech signal rather than
direct acoustic similarity measures.
Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of Intelli-
gence Advanced Research Projects Activity (IARPA)
or the U.S. Government.
References
B. Chen, S.H Liu, and F.H. Chu. 2009. Training data se-
lection for improving discriminative training of acoustic
models. Pattern Recognition Letters, 30:1228?1235.
J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69?87. Gordon and Breach.
G. Hakkani-Tur, G. Riccardi, and A. Gorin. 2002. Active
learning for automatic speech recognition. In Proc. of
ICASSP, pages 3904?3907.
N. Itoh, T.N. Sainath, D.N. Jiang, J. Zhou, and B. Ramab-
hadran. 2012. N-best entropy based data selection for
acoustic modeling. In Proceedings of ICASSP.
Thomas Kemp and Alex Waibel. 1998. Unsupervised
training of a speech recognizer using TV broadcasts.
In in Proceedings of the International Conference on
Spoken Language Processing (ICSLP-98), pages 2207?
2210.
A. Krause and C. Guestrin. 2011. Submodularity and its
applications in optimized information gathering. ACM
Transactions on Intelligent Systems and Technology,
2(4).
L. Lamel, J.L. Gauvain, and G. Adda. 2002. Lightly
supervised and unsupervised acoustic model training.
Computer, Speech and Language, 16:116 ? 125.
K.F. Lee and H.W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models. IEEE
Trans. ASSP, 37:1641?1648.
Hui Lin and Jeff A. Bilmes. 2009. How to select a good
training-data subset for transcription: Submodular ac-
tive selection for sequences. In Proc. Annual Confer-
ence of the International Speech Communication Asso-
ciation (INTERSPEECH), Brighton, UK, September.
H. Lin and J. Bilmes. 2011. A class of submodular
functions for document summarization. In Proceedings
of ACL.
R.K. Moore. 2003. A comparison of the data require-
ments of automatic speech recognition systems and
human listeners. In Proceedings of Eurospeech, pages
2581?2584.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978.
An analysis of approximations for maximizing submod-
ular functions-I. Math. Program., 14:265?294.
725
J. Rousu and J. Shawe-Taylor. 2005. Efficien computa-
tion of of gapped substring kernels for large alphabets.
Journal of Machine Leaning Research, 6:13231344.
Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection
for speech recognition. In Proceedings of ASRU.
726
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 306?313,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Hand Gestures in Disambiguating Types of You Expressions in Multiparty
Meetings
Tyler Baldwin
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
baldwin96@cse.msu.edu
Joyce Y. Chai
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
jchai@cse.msu.edu
Katrin Kirchhoff
Department of Electrical
Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
The second person pronoun you serves dif-
ferent functions in English. Each of these
different types often corresponds to a dif-
ferent term when translated into another
language. Correctly identifying different
types of you can be beneficial to machine
translation systems. To address this is-
sue, we investigate disambiguation of dif-
ferent types of you occurrences in multi-
party meetings with a new focus on the
role of hand gesture. Our empirical re-
sults have shown that incorporation of ges-
ture improves performance on differentiat-
ing between the generic use of you (e.g.,
refer to people in general) and the referen-
tial use of you (e.g., refer to a specific per-
son or a group of people). Incorporation
of gesture can also compensate for limi-
tations in automated language processing
(e.g., reliable recognition of dialogue acts)
and achieve comparable results.
1 Introduction
The second person pronoun you is one of the most
prevalent words in conversation and it serves sev-
eral different functions (Meyers, 1990). For ex-
ample, it can be used to refer to a single addressee
(i.e., the singular case) or multiple addressees (i.e.,
the plural case). It can also be used to represent
people in general (i.e., the generic case) or be used
idiomatically in the phrase ?you know?.
For machine translation systems, these differ-
ent types of you often correspond to different
translations in another language. For example,
in German, there are different second-person pro-
nouns for singular vs. plural you (viz. du vs. ihr);
in addition there are different forms for formal
vs. informal forms of address (du vs. Sie) and for
the generic use (man). The following examples
demonstrate different translations of you from En-
glish (EN) into German (DE):
? Generic you
EN: Sometimes you have meetings where the
decision is already taken.
DE: Manchmal hat man Meetings wo die
Entscheidung schon gefallen ist.
? Singular you:
EN: Do you want an extra piece of paper?
DE: Mo?chtest du noch ein Blatt Papier?
? Plural you:
EN: Hope you are all happy!
DE: Ich hoffe, ihr seid alle zufrieden!
These examples show that correctly identifying
different types of You plays an important role in
the correct translation of you in different context.
To address this issue, this paper investigates the
role of hand gestures in disambiguating different
usages of you in multiparty meetings. Although
identification of you type has been investigated
before in the context of addressee identification
(Gupta et al, 2007b; Gupta et al, 2007a; Framp-
ton et al, 2009; Purver et al, 2009), our work
here focuses on two new angles. First, because of
our different application on machine translation,
rather than processing you at an utterance level to
identify addressee, our work here concerns each
occurrence of you within each utterance. Second
and more importantly, our work investigates the
role of corresponding hand gestures in the disam-
biguation process. This aspect has not been exam-
ined in previous work.
When several speakers are conversing in a sit-
uated environment, they often overtly gesture at
one another to help manage turn order or explic-
itly direct a statement toward a particular partici-
pant (McNeill, 1992). For example, consider the
following snippet from a multiparty meeting:
A: ?Why is that??
B: ?Because, um, based on what ev-
306
erybody?s saying, right, [gestures at
Speaker D] you want something sim-
ple. You [gestures at Speaker C]want
basic stuff and [gestures at Speaker A]
you want something that is easy to use.
Speech recognition might not be the
simplest thing.?
The use of gesture in this example indicates that
each instance of the pronoun you is intended to
be referential, and gives some indication of the in-
dented addressee. Without the aid of gesture, it
would be difficult even for a human listener to be
able to interpret each instance correctly.
Therefore, we conducted an empirical study on
several meeting segments from the AMI meeting
corpus. We formulated our problem as a classifica-
tion problem for each occurrence of you, whether
it is a generic, singular, or plural type. We com-
bined gesture features with several linguistic and
discourse features identified by previous work and
evaluated the role of gesture in two different set-
tings: (1) a two stage classification that first dif-
ferentiates the generic type from the referential
type and then within the referential type distin-
guishes singular and plural usages; (2) a three way
classification between generic, singular, or plural
types. Our empirical results have shown that in-
corporation of gesture improves performance on
differentiating between the generic and the refer-
ential type. Incorporation of gesture can also com-
pensate for limitations in automated language pro-
cessing (e.g., reliable recognition of dialogue acts)
and achieve comparable results. These findings
have important implications for machine transla-
tion of you expressions from multiparty meetings.
2 Related Work
Psychological research on gesture usage in
human-human dialogues has shown that speakers
gesture for a variety of reasons. While speakers of-
ten gesture to highlight objects related to the core
conversation topic (Kendon, 1980), they also ges-
ture for dialogue management purposes (Bavelas
et al, 1995). While not all of the gestures pro-
duced relate directly to the resolution of the word
you, many of them give insight into which partici-
pant is being addressed, which has a close correla-
tion with you resolution. Our investigation here is
closely related to two areas of previous work: ad-
dressee identification based on you and the use of
gestures in coreference resolution.
Addressee Identification. Disambiguation of
you type in the context of addressee identifica-
tion has been examined in several papers in re-
cent years. Gupta et. al. (2007b) examined
two-party dialogues from the Switchboard corpus.
They modeled the problem as a binary classifi-
cation problem of differentiating between generic
and referential usages (referential usages include
the singular and plural types). This work has iden-
tified several important linguistic and discourse
features for this task (which was used and ex-
tended in later work and our work here). Later
work by the same group (Gupta et al, 2007a) ex-
amined the same problem on multiparty dialogue
data. They made adjustments to their previous
methods by removing some oracle features from
annotation and applying simpler and more realis-
tic features. A recent work (Frampton et al, 2009)
has examined both the generic vs. referential and
singular vs. plural classification tasks. A main
difference is that this work incorporated gaze fea-
ture information in both classification tasks (gaze
features are commonly used in addressee identi-
fication). More recent work (Purver et al, 2009)
discovered that large gains in performance can
be achieved by including n-gram based features.
However, they found that many of the most im-
portant n-gram features were topic specific, and
thus required training data consisting of meetings
about the same topic.
Gestures in Coreference Resolution. Eisen-
stein and Davis (2006; 2007) examined corefer-
ence resolution on a corpus of speaker-listener
pairs in which the speaker had to describe the
workings of a mechanical device to the listener,
with the help of visual aids. In this gesture heavy
dataset, they found gesture data to be helpful in re-
solving references. In our previous work (2009),
we examined gestures for the identification of
coreference on multparty meeting data. We found
that gestures only provided limited help in the
coreference identification task. Given the nature
of the meetings under investigation, although ges-
tures have not been shown to be effective in gen-
eral, they are potentially helpful in recognizing
whether two linguistic expressions refer to a same
participant.
Compared to these two areas of earlier work,
our investigation here has two unique aspects.
First, as mentioned earlier, previous work on ad-
dressee identification focused the problem at the
307
utterance level. Because the goal was to find the
addressee of an utterance, the assumption was that
all instances of you in an utterance were of the
same type. However, since several instances of
you in the same utterance may translate differently,
we instead examine the classification task at the
instance level. Second, our work here specifically
investigates the role of gestures in disambiguation
of different types of you. This aspect has not been
examined in previous work.
3 Data
The dataset used in our investigation was the
AMI meeting corpus (Popescu-Belis and Estrella,
2007), the same corpus used in previous work
(Gupta et al, 2007a; Frampton et al, 2009; Purver
et al, 2009; Baldwin et al, 2009). The AMI meet-
ing corpus is a large publicly available corpus of
multiparty design meetings. AMI meeting anno-
tations contain manual speech transcriptions, as
well as annotations of several additional modali-
ties, such as focus of attention and head and hand
gesture.
For this work, six AMI meeting segments
(IS1008a, IS1008b, IS1008c, IS1008d, ES2008a,
TS3005a) were used. These instances were cho-
sen because they contained manual annotations of
hand gesture data, which was not available for all
AMI meeting segments. These six meeting seg-
ments were from AMI ?scenario? meetings, in
which meeting participants had a specific task of
designing a hypothetical remote control.
All instances of the word you and its variants
were manually annotated as either generic, singu-
lar, or plural. This produced a small dataset of 533
instances. Agreement between two human anno-
tators was high (? = 0.9). The distribution of you
types is shown in Figure 1. The most prevalent
type in our data set was the generic type, which
accounted for 47% of all instances of you present.
Of the two referential types, the singular type ac-
counted for about 60% of the referential instances.
A total of 508 gestures are present in our data
set. Table 1 shows the distribution of gestures.
As shown, ?non-communicative gestures?, make
up nearly half (46%) of the gestures produced.
These are gestures that are produced without an
overt communicative intent, such as idly tapping
on the table. The other main categorization of
gestures is ?communicative gestures?, which ac-
counts for 45% of all gestures produced and is
made up of the ?pointing at participants?, ?point-
ing at objects?, ?interact with object?, and ?other
communicative? gesture types from Table 1. A to-
tal of 17% of the gestures produced were pointing
gestures that pointed to people, a type of gesture
that would likely be helpful for you type identifica-
tion. A small percentage of the gestures produced
were not recorded by the meeting recording cam-
eras (i.e., off camera), and thus are of unknown
type.
4 Methodology
Our general methodology followed previous work
and formulated this problem as a classification
problem. We evaluated how gesture data may
help you type identification using two different ap-
proaches: (1) two stage binary classification, and
(2) a single three class classification problem. In
two stage binary classification, we first attempt
to distinguish between instances of you that are
generic and those that are referential. We then take
those cases that are referential and attempt to sub-
divide them into instances that are intended to re-
fer to a single person and those that refer to several
people.
Our feature set includes features used by Gupta
et. al. (2007a) (Hereafter referred to as Gupta) and
Frampton et. al. (2009) (Hereafter Frampton), as
well as new features incorporating gestures. We
summarize these features as follows.
Sentential Features. We used several senten-
tial features to capture important phrase patterns.
Most of our sentential features were drawn from
Gupta (2007a). These features captured the pat-
terns ?you guys?, ?you know?, ?do you? (and sim-
ilar variants), ?which you? (and variants), ?if you?,
and ?you hear? (and variants). Another sentential
feature captured the number of times the word you
appeared in the sentence. Additionally, other fea-
tures captured sentence patterns not related to you,
such as the presence of the words ?I? and ?we?.
A few other sentential features were drawn from
Frampton et. al. (2009). These include the pattern
?<auxiliary> you? (a more general version of the
?do you? feature) and a count of the number of
total words in the utterances.
Part-of-Speech Features. Several features
based on automatic part-of-speech tagging of the
sentence containing you were used. Quality of au-
tomatic tagging was not assessed. From the tagged
results, we extracted 5 features based on sentence
308
Distrib
ution 
of You
 Types
050100150200250300
Generi
c
Singula
r
Plural
Type
Count
(a) Distribution of You types
Gestu
re Dis
tributi
on
050100150200250 Non- Comm
unicative Po
inting at Pa
rticipant
s Pointi
ng at Objec
ts Other Co
mmunica
tive Inte
ract with O
bject
Off_came
ra
Type
Count
(b) Distribution of gesture types
Figure 1: Data distributions
and tag patterns: whether or not the sentence that
contained you also contained I, or we followed by
a verb tag (3 separate features), and whether or
not the sentence contains a comparative JJR (ad-
jective) tag. All of these features were adapted
from Gupta (2007a).
Dialog Act Features. We used the manually an-
notated dialogue act tags provided by the AMI cor-
pus to produce our dialogue act features. Three di-
alogue act features were used: the dialogue act tag
of the current sentence, the previous sentence, and
the sentence prior to that. Dialog act tags were in-
corporated into the feature set in one of two differ-
ent ways: 1) using the full tag set provided by the
AMI corpus, and 2) using a binary feature record-
ing if the dialogue act tag was of the elicit type.
The latter way of dialogue act incorporation rep-
resents a simpler and more realistic treatment of
dialogue acts.
Question Mark Feature. The question mark
feature captures whether or not the current sen-
tence ends in a question mark. This feature cap-
tures similar information to the elicit dialogue act
tag and was used in Gupta as an automatically ex-
tractable replacement to the manually extracted di-
alogue act tags (2007a).
Backward Looking/Forward Looking Fea-
tures. Several features adapted from Frampton et.
al. (2009) used information about previous and
next sentences and speakers. These features con-
nected the current utterance with previous utter-
ances by the other participants in the room. For
each listener, a feature was recorded that indicated
how many sentences elapsed between the current
sentence and the last/next time the person spoke.
Additionally, two features captured the number of
speakers in the previous and next five sentences.
Gesture Features. Several different features
were used to capture gesture information. Three
types of gesture data were considered: all pro-
duced gestures, only those gestures that were
manually annotated as being communicative, and
only those gestures that were manually annotated
as pointing towards another meeting participant.
For each of these types, one gesture feature cap-
tures the total number of gestures that co-occur
with the current sentence, while another feature
records only whether or not a gesture co-occurs
with the utterance of you. Since previous work
(Kendon, 1980) has indicated that gesture produc-
tion tends to precede the onset of the expression,
gestures were considered to have co-occurred with
instances if they directly overlapped with them or
preceded them by a short window of 2.5 seconds.
Note that in this investigation, we used anno-
tated gestures provided by the AMI corpus. Al-
though automated extraction of reliable gesture
features can be challenging and should be pursued
in the future, the use of manual annotation allows
us to focus on our current goal, which is to under-
stand whether and to what degree hand gestures
may help disambiguation of you Type.
It is also important to note that although previ-
ous work (Purver et al, 2009) showed that n-gram
features produced large performance gains, these
features were heavily topic dependent. The AMI
meeting corpus provides several meetings on ex-
actly the same topic, which allowed the n-gram
features to learn topic-specific words such as but-
ton, channel, and volume. However, as real world
309
Accuracy
Majority Class Baseline 53.3%
Gupta automatic 70.7%
Gupta manual 74.7%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 74.3%
All (+ gesture) 79.0%
Table 1: Accuracy values for Generic vs. Referen-
tial Classification
meetings occur with a wider range of goals and
topics, we would like to build a topic and domain
independent model that does not require a corpus
of topic specific training data. As such, we have
excluded n-gram features from our study.
Additionally, we have not implemented gaze
features. Although previous work (Frampton et
al., 2009) showed that these features were able to
improve performance, we decided to focus solely
on gesture to the exclusion of other non-speech
modalities. However, we are currently in the pro-
cess of evaluating the overlap between gesture and
gaze feature coverage.
5 Results
Due to the small number of meeting segments in
our data, leave-one-out cross validation was pre-
formed for evaluation. Since a primary focus of
this paper is to understand whether and to what
degree gesture is able to aid in the you type iden-
tification task, experiments were run using a deci-
sion tree classifier due to its simplicity and trans-
parency 1.
5.1 Two Stage Classification
We first evaluated the role of gesture via two stage
binary classification. That is, we performed two
binary classification tasks, first differentiating be-
tween generic and referential instances, and then
further dividing the referential instances into the
singular and plural types. This provides a more
detailed analysis of where gesture may be helpful.
Results for the generic vs. referential and singu-
lar vs. plural binary classification tasks are shown
in Table 1 and Table 2, respectively. Tables 1
and 2 present several different configurations. The
1In order to get a more direct comparison to previous work
(Gupta et al, 2007a; Frampton et al, 2009), we also experi-
mented with classification via a bayesian network. We found
that the overall results were comparable to those obtained
with the decision tree.
Accuracy
Majority Class Baseline 59.5%
Gupta automatic 72.2%
Gupta manual 73.6%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 72.5%
All (+ gesture) 74.6%
Table 2: Accuracy values for Singular vs. Plural
Classification
?Gupta? feature configurations consist of all fea-
tures used by Gupta et. al. (2007a). These in-
clude all part-of-speech features, all dialogue act
features, the question mark feature, and all sen-
tential features except the ?<auxiliary> you? fea-
ture and the word count feature. Results from two
types of processing are presented: automatic and
manual.
? Automatic feature extraction (automatic) -
The automatic configurations consist of only
features that were automatically extracted
from the text. This includes all of the features
we examined except for the dialogue act and
gesture features. These features are extracted
from meeting transcriptions.
? Manual feature extraction (manual) - Manual
configurations apply manual annotations of
dialogue acts and gestures together with the
automatically extracted features.
The Frampton configurations add the addi-
tional sentential features as well as the backward-
looking and forward-looking features. As before,
results are presented for a manual and an auto-
matic run. The final configuration (?All?) includes
the entire feature set with the addition of gesture
features. The All configuration is the only config-
uration that includes gesture features.
Although they are not directly comparable, the
results for generic vs. referential classification
shown in Table 1 appear consistent with those re-
ported by Gupta (2007a). Adding additional fea-
tures from Frampton et. al. did not produce an
overall increase in performance when dialogue act
features were present. Including gesture features
leads to a significant increase in performance (Mc-
Nemar Test, p < 0.01), an absolute increase of
4.3% over the best performing feature set that does
not include gesture. This result seems to confirm
our hypothesis that, because gestures are likely
310
Accuracy
Majority Class Baseline 46.7%
Gupta automatic 61.5%
Gupta manual 66.2%
Gupta + Frampton automatic 63.6%
Gupta + Frampton manual 70.2%
All (+ gesture) 70.4%
Table 3: Accuracy values for several different fea-
ture configurations on the three class classification
problem.
to accompany referential instances of you but not
generic instances, gesture information is able to
help differentiate between the two. Manual in-
spection of the decision tree produced indicates
that gesture features were among the most dis-
criminative features.
The results on the singular vs. plural task shown
in Table 2 are less clear. Although (Gupta et al,
2007a) did not report results on singular vs. plural
classification, their feature set produced reason-
able classification accuracy of 73.6%. Including
gesture and other features did not produce a statis-
tically significant improvement in the overall ac-
curacy. This suggests that while gesture is helpful
for predicting referentiality, it does not appear to
be a reliable predictor of whether an instance of
you is singular or plural. Inspection on the deci-
sion tree confirms that gesture features were not
seen to be highly discriminative.
5.2 Three Class Classification
The results presented for singular vs. plural classi-
fication are based on performance on the subset of
you instances that are referential, which assumes
that we are able to filter out generic references
with 100% accuracy. While this gives us an eval-
uation of how well the singular vs. plural task can
be performed without the generic references pre-
senting a confounding factor, it presents unrealis-
tic performance for a real system. To account for
this, we present results on a three class problem of
determining whether an instance of you is generic,
singular, or plural. The results are shown in Table
3. A simple majority class classifier yields accu-
racy of 46.7% (In our data, the generic class was
the majority class).
As we can see from Table 3, adding addi-
tional features gives improved performance over
the original implementation by Gupta et. al., re-
sulting in an overall accuracy of about 70%. We
also observed that the dialogue act features were
important; manual configurations produced abso-
lute gains of about 7% accuracy over fully auto-
matic configurations. The gesture feature, how-
ever, did not provide a significant increase in per-
formance over the same feature set without gesture
information.
Table 4 shows the precision, recall, and F-
measure values for each you type for several dif-
ferent configurations. As shown, the generic class
proved to be the easiest for the classifiers to iden-
tify. This is not suprising, as not only are generic
instance our majority class, but many of the fea-
tures used were originally tailored towards the two
class problem of differentiating generic instances
from the other classes. The performance on the
plural and singular classes is comparable to one
another when the basic feature set is used. How-
ever, as more features are added, the performance
on the singular class increases while the perfor-
mance on the plural class does not. This seems
to suggest that future work should attempt to in-
clude more features that are indicative of plural
instances.
When manual dialogue acts are applied, it ap-
pears incorporation of gestures does not lead to
any overall performance improvement (as shown
in Table 3). One possible explanation is that ges-
ture features as they are incorporated here do pro-
vide some disambiguating information (as shown
in the two stage classification), but this informa-
tion is subsumed by other features, such as dia-
logue acts. To test this hypothesis, we ran an ex-
periment with a feature set that contained all fea-
tures except dialogue act features. That is, a fea-
ture set that contains all of the automatic features,
as well as gesture features. Results are shown in
Table 5.
Our ?automatic + gesture? feature configuration
produced accuracy of 66.2%. When compared to
the same feature set without gesture features (the
?Gupta + Frampton automatic? row in Table 3) we
see a statistically significant (p < 0.01) absolute
accuracy improvement of about 2.6%. This seems
to suggest that gesture features are providing some
small amount of relevant information that is not
captured by our automatically extractable features.
Up until this point we have incorporated dia-
logue acts using the full set of dialogue act tags
provided by the AMI corpus. As we have men-
311
Precision Recall F-Measure
Gupta automatic Plural 0.553 0.548 0.550
Singular 0.657 0.408 0.504
Generic 0.624 0.787 0.696
Gupta manual Plural 0.536 0.513 0.524
Singular 0.675 0.503 0.576
Generic 0.704 0.839 0.766
All (+ gesture) Plural 0.542 0.565 0.553
Singular 0.745 0.604 0.667
Generic 0.754 0.835 0.792
Table 4: Precision, recall, and F-measure results for each you type based on three class classification.
Accuracy
Gupta + Frampton automatic 63.6%
Gupta + Frampton automatic + gesture 66.2%
Gupta + Frampton automatic + simple dialogue act 66.6%
Gupta + Frampton automatic + simple dialogue act + gesture 69.0%
Table 5: Accuracy for 3-way classification by combining gesture information with automatically ex-
tracted features based on the Decision Tree model
tioned, this level of granularity may not be prac-
tically extractable for use in a current state-of-
the-art system. As a result, we implemented the
simpler dialogue act incorporation method pro-
posed by (Gupta et al, 2007a), in which only
the presence or absence of the elicit dialogue act
type is considered. Using this feature with the
automatically extracted features yielded accuracy
of 66.6%, a statistically significant improvement
(p < 0.01) of an absolute 3% over a fully auto-
matic run. Furthermore, if we incorporate gesture
features with this configuration, the performance
increases to 69.0% (statistically significantly, p <
0.01). This suggests that while gesture features
may be redundant with information provided by
the full set of dialogue act tags, it is largely com-
plementary with the simpler dialogue act incorpo-
ration. The incorporation of gesture along with
simpler and more reliable dialogue acts can po-
tentially approach the performance gained by in-
corporation of more complex dialogue acts, which
are often difficult to obtain. Of course, gesture fea-
tures themselves are often difficult to obtain. How-
ever, redundancy in two potentially error-prone
feature sources can be an asset, as data from one
source may help to compensate for errors in the
other. Although addressing a different problem of
multimodal integration, previous work (Oviatt et
al., 1997) appears to indicate that this is the case.
6 Conclusion
In this paper, we investigate the role of hand ges-
tures in disambiguating types of You expressions
in multiparty meetings for the purpose of machine
translation.
Our results have shown that on the binary
generic vs. referential classification problem, the
inclusion of gesture data provides a statistically
significant increase in performance over the same
feature set without gesture. This result is consis-
tent with our hypothesis that gesture data would be
helpful because speakers are more likely to gesture
when producing referential instances of you.
To produce results more akin to those that
would be expected during incorporation in a real
machine translation system, we experimented with
the type identification problem as a three class
classification problem. It was discovered that
when a full set of dialogue act tags were used as
features, the incorporation of gesture features does
not provide an increase in performance. However,
when simpler dialogue act tags are used, the in-
corporation of gestures helps to make up for lost
performance. Since it remains a difficult prob-
lem to automatically predict complex dialog acts
with high accuracy, the incorporation of gesture
features may prove beneficial to current systems.
312
7 Acknowledgement
This work was supported by IIS-0855131 (to the
first two authors) and IIS-0840461 (to the third au-
thor) from the National Science Foundation. The
authors would like to thank anonymous reviewers
for valuable comments and suggestions.
References
Tyler Baldwin, Joyce Y. Chai, and Katrin Kirchhoff.
2009. Communicative gestures in coreference iden-
tification in multiparty meetings. In ICMI-MLMI
?09: Proceedings of the 2009 international con-
ference on Multimodal interfaces, pages 211?218.
ACM.
J. B. Bavelas, N. Chovil, L. Coates, and L. Roe. 1995.
Gestures specialized for dialogue. Personality and
Social Psychology Bulletin, 21:394?405.
Jacob Eisenstein and Randall Davis. 2006. Gesture
improves coreference resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
37?40, New York City, USA, June. Association for
Computational Linguistics.
Jacob Eisenstein and Randall Davis. 2007. Condi-
tional modality fusion for coreference resolution. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 352?
359, Prague, Czech Republic, June. Association for
Computational Linguistics.
Matthew Frampton, Raquel Ferna?ndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ?you??: combining linguis-
tic and gaze features to resolve second-person refer-
ences in dialogue. In EACL ?09: Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 273?
281, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Dan Jurafsky. 2007a. Resolving you in multi-party
dialog. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007b. Disambiguating between generic and refer-
ential you in dialog. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Adam Kendon. 1980. Gesticulation and speech: Two
aspects of the process of utterance. In Mary Richie
Key, editor, The Relationship of Verbal and Nonver-
bal Communication, pages 207?227.
D. McNeill. 1992. Hand and Mind: What Gestures
Reveal about Thought. University of Chicago Press.
W. M. Meyers. 1990. Current generic pronoun usage.
American Speech, 65(3):228?237.
Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.
1997. Integration and synchronization of input
modes during multimodal human-computer interac-
tion. In CHI ?97: Proceedings of the SIGCHI con-
ference on Human factors in computing systems,
pages 415?422, New York, NY, USA. ACM.
Andrei Popescu-Belis and Paula Estrella. 2007. Gen-
erating usable formats for metadata and annotations
in a large meeting corpus. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 93?96,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthew Purver, Raquel Ferna?ndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In SIGDIAL ?09: Proceedings of the SIGDIAL 2009
Conference, pages 306?309, Morristown, NJ, USA.
Association for Computational Linguistics.
313
