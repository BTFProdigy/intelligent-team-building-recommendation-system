Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,
Dublin, Ireland, August 23-24, 2014.
DCU: Aspect-based Polarity Classification for SemEval Task 4
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
Abstract
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
1 Introduction
This paper describes DCU?s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
2 Sentiment Lexicons
The following four lexicons are employed:
1. MPQA
1
(Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet
2
(Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer
3
indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu?s Opinion Lexicon
4
(Hu and Liu,
1
http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2
http://sentiwordnet.isti.cnr.it/
3
http://www.wjh.harvard.edu/
?
inquirer/
inqtabs.txt
4
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html#lexicon
223
2004) indicates whether a word expresses
positive or negative sentiment.
2.1 Lexicon Combination
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
? MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
? SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
? General Inquirer and Bing Liu?s Opinion
Lexicon: 1 for positive and -1 for negative
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.
5
2.2 Lexicon Filtering
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
2.3 Adding Domain-Specific Words
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.
6
5
We also tried to vote over the four lexicon scores but this
did not improve over summing.
6
http://world-food-and-wine.com/
describing-food
2.4 Handling Variation
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus
7
among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
3 Methodology
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
3.1 Rule-Based Approach
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
? Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
7
The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
224
? Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
? Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)
8
,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n?t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.
9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
3.2 Machine Learning Approach
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka?s SMO
implementation (Platt, 1998; Hall et al., 2009).
10
8
To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9
We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10
We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
Transf. n c n-gram Freq.
-L? 2 2 cord with 1
AL? 2 2 <aspect> with 56
ALS? 1 4 <negu080> 595
ALSR- 1 4 <negu080> 502
AL? 2 4 and skip 1
ALSR- 2 4 and <negu080> 25
ALSRP 1 4 <negu080>/vb 308
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
3.2.1 Bag-of-N-gram Features
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
<ASPECT> (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
3.2.2 Adding Rule-Based Score Features
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
225
4. number of negative words in the sentence
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
3.2.3 Submitted Runs
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and ? of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
? = 0.003311 for the laptop data, C = 1.445,
? = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
4 Results and Analysis
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems? er-
rors fall into the following categories:
Dataset System Training Test
Laptop Baseline ? 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline ? 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
Table 2: Accuracy of the task baseline system, a
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
? Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
? Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
? Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.
11
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
11
We only classify one test instance as conflict.
226
chocolate raspberry cake is heavenly - not too
sweet, but full of flavor.
5 Related Work
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has ? to the
best of our knowledge ? not been applied before.
6 Efficiency
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.
12
After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
? The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
? The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
12
We validate documents using the XML schema defini-
tion provided on the shared task website.
227
7 Conclusion
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461?472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC?10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O?Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49?58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144?152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1?8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
228
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630?1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301?311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185?208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138?144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631?
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 77?
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
229
Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 11?18,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Mining Sentiments from Tweets
Akshat Bakliwal, Piyush Arora, Senthil Madhappan
Nikhil Kapre, Mukesh Singh and Vasudeva Varma
Search and Information Extraction Lab,
International Institute of Information Technology, Hyderabad.
{akshat.bakliwal, piyush.arora}@research.iiit.ac.in,
{senthil.m, nikhil.kapre, mukeshkumar.singh}@students.iiit.ac.in,
vv@iiit.ac.in
Abstract
Twitter is a micro blogging website, where
users can post messages in very short text
called Tweets. Tweets contain user opin-
ion and sentiment towards an object or per-
son. This sentiment information is very use-
ful in various aspects for business and gov-
ernments. In this paper, we present a method
which performs the task of tweet sentiment
identification using a corpus of pre-annotated
tweets. We present a sentiment scoring func-
tion which uses prior information to classify
(binary classification ) and weight various sen-
timent bearing words/phrases in tweets. Us-
ing this scoring function we achieve classifi-
cation accuracy of 87% on Stanford Dataset
and 88% on Mejaj dataset. Using supervised
machine learning approach, we achieve classi-
fication accuracy of 88% on Stanford dataset.
1 Introduction
With enormous increase in web technologies, num-
ber of people expressing their views and opinions
via web are increasing. This information is very
useful for businesses, governments and individuals.
With over 340+ million Tweets (short text messages)
per day, Twitter is becoming a major source of infor-
mation.
Twitter is a micro-blogging site, which is popular
because of its short text messages popularly known
as ?Tweets?. Tweets have a limit of 140 characters.
Twitter has a user base of 140+ million active users1
1As on March 21, 2012. Source:
http://en.wikipedia.org/wiki/Twitter
and thus is a useful source of information. Users
often discuss on current affairs and share their per-
sonals views on various subjects via tweets.
Out of all the popular social media?s like Face-
book, Google+, Myspace and Twitter, we choose
Twitter because 1) tweets are small in length, thus
less ambigious; 2) unbiased; 3) are easily accessible
via API; 4) from various socio-cultural domains.
In this paper, we introduce an approach which can
be used to find the opinion in an aggregated col-
lection of tweets. In this approach, we used two
different datasets which are build using emoticons
and list of suggestive words respectively as noisy la-
bels. We give a new method of scoring ?Popularity
Score?, which allows determination of the popular-
ity score at the level of individual words of a tweet
text. We also emphasis on various types and levels
of pre-processing required for better performance.
Roadmap for rest of the paper: Related work is
discussed in Section 2. In Section 3, we describe
our approach to address the problem of Twitter
sentiment classification along with pre-processing
steps.Datasets used in this research are discussed in
Section 4. Experiments and Results are presented in
Section 5. In Section 6, we present the feature vector
approach to twitter sentiment classification. Section
7 presents as discussion on the methods and we con-
clude the paper with future work in Section 8.
2 Related Work
Research in Sentiment Analysis of user generated
content can be categorized into Reviews (Turney,
2002; Pang et al, 2002; Hu and Liu, 2004), Blogs
(Draya et al, 2009; Chesley, 2006; He et al, 2008),
11
News (Godbole et al, 2007), etc. All these cat-
egories deal with large text. On the other hand,
Tweets are shorter length text and are difficult to
analyse because of its unique language and struc-
ture.
(Turney, 2002) worked on product reviews. Tur-
ney used adjectives and adverbs for performing
opinion classification on reviews. He used PMI-IR
algorithm to estimate the semantic orientation of the
sentiment phrase. He achieved an average accuracy
of 74% on 410 reviews of different domains col-
lected from Epinion. (Hu and Liu, 2004) performed
feature based sentiment analysis. Using Noun-Noun
phrases they identified the features of the products
and determined the sentiment orientation towards
each feature. (Pang et al, 2002) tested various ma-
chine learning algorithms on Movie Reviews. He
achieved 81% accuracy in unigram presence feature
set on Naive Bayes classifier.
(Draya et al, 2009) tried to identify domain spe-
cific adjectives to perform blog sentiment analysis.
They considered the fact that opinions are mainly
expressed by adjectives and pre-defined lexicons fail
to identify domain information. (Chesley, 2006) per-
formed topic and genre independent blog classifica-
tion, making novel use of linguistic features. Each
post from the blog is classified as positive, negative
and objective.
To the best of our knowledge, there is very less
amount of work done in twitter sentiment analy-
sis. (Go et al, 2009) performed sentiment analy-
sis on twitter. They identified the tweet polarity us-
ing emoticons as noisy labels and collected a train-
ing dataset of 1.6 million tweets. They reported an
accuracy of 81.34% for their Naive Bayes classi-
fier. (Davidov et al, 2010) used 50 hashtags and 15
emoticons as noisy labels to create a dataset for twit-
ter sentiment classification. They evaluate the effect
of different types of features for sentiment extrac-
tion. (Diakopoulos and Shamma, 2010) worked on
political tweets to identify the general sentiments of
the people on first U.S. presidential debate in 2008.
(Bora, 2012) also created their dataset based on
noisy labels. They created a list of 40 words (pos-
itive and negative) which were used to identify the
polarity of tweet. They used a combination of
a minimum word frequency threshold and Cate-
gorical Proportional Difference as a feature selec-
tion method and achieved the highest accuracy of
83.33% on a hand labeled test dataset.
(Agarwal et al, 2011) performed three class (pos-
itive, negative and neutral) classification of tweets.
They collected their dataset using Twitter stream
API and asked human judges to annotate the data
into three classes. They had 1709 tweets of each
class making a total of 5127 in all. In their research,
they introduced POS-specific prior polarity features
along with twitter specific features. They achieved
max accuracy of 75.39% for unigram + senti fea-
tures.
Our work uses (Go et al, 2009) and (Bora, 2012)
datasets for this research. We use Naive Bayes
method to decide the polarity of tokens in the tweets.
Along with that we provide an useful insight on how
preprocessing should be done on tweet. Our method
of Senti Feature Identification and Popularity Score
perform well on both the datasets. In feature vec-
tor approach, we show the contribution of individual
NLP and Twitter specific features.
3 Approach
Our approach can be divided into various steps.
Each of these steps are independent of the other but
important at the same time.
3.1 Baseline
In the baseline approach, we first clean the tweets.
We remove all the special characters, targets (@),
hashtags (#), URLs, emoticons, etc and learn the
positive & negative frequencies of unigrams in train-
ing. Every unigram token is given two probability
scores: Positive Probability (Pp) and Negative Prob-
ability (Np) (Refer Equation 1). We follow the same
cleaning process for the test tweets. After clean-
ing the test tweets, we form all the possible uni-
grams and check for their frequencies in the training
model. We sum up the positive and negative proba-
bility scores of all the constituent unigrams, and use
their difference (positive - negative) to find the over-
all score of the tweet. If tweet score is > 0 then it is
12
positive otherwise negative.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
Pp = Positive Probability of the token.
= Pf/(Pf + Nf )
Np = Negative Probability of the token.
= Nf/(Pf + Nf )
(1)
3.2 Emoticons and Punctuations Handling
We make slight changes in the pre-processing mod-
ule for handling emoticons and punctuations. We
use the emoticons list provided by (Agarwal et al,
2011) in their research. This list2 is built from
wikipedia list of emoticons3 and is hand tagged into
five classes (extremely positive, positive, neutral,
negative and extremely negative). In this experi-
ment, we replace all the emoticons which are tagged
positive or extremely positive with ?zzhappyzz? and
rest all other emoticons with ?zzsadzz?. We append
and prepend ?zz? to happy and sad in order to pre-
vent them from mixing into tweet text. At the end,
?zzhappyzz? is scored +1 and ?zzsadzz? is scored -1.
Exclamation marks (!) and question marks (?)
also carry some sentiment. In general, ?!? is used
when we have to emphasis on a positive word and
??? is used to highlight the state of confusion or
disagreement. We replace all the occurrences of ?!?
with ?zzexclaimzz? and of ??? with ?zzquestzz?. We
add 0.1 to the total tweet score for each ?!? and sub-
tract 0.1 from the total tweet score for each ???. 0.1
is chosen by trial and error method.
3.3 Stemming
We use Porter Stemmer4 to stem the tweet words.
We modify porter stemmer and restrict it to step 1
only. Step 1 gets rid of plurals and -ed or -ing.
3.4 Stop Word Removal
Stop words play a negative role in the task of senti-
ment classification. Stop words occur in both pos-
itive and negative training set, thus adding more
ambiguity in the model formation. And also, stop
2http://goo.gl/oCSnQ
3http://en.wikipedia.org/wiki/List of emoticons
4http://tartarus.org/m?artin/PorterStemmer/
words don?t carry any sentiment information and
thus are of no use to us. We create a list of stop
words like he, she, at, on, a, the, etc. and ignore
them while scoring. We also discard words which
are of length ? 2 for scoring the tweet.
3.5 Spell Correction
Tweets are written in random form, without any fo-
cus given to correct structure and spelling. Spell
correction is an important part in sentiment analy-
sis of user- generated content. Users type certain
characters arbitrary number of times to put more em-
phasis on that. We use the spell correction algo-
rithm from (Bora, 2012). In their algorithm, they
replace a word with any character repeating more
than twice with two words, one in which the re-
peated character is placed once and second in which
the repeated character is placed twice. For example
the word ?swwweeeetttt? is replaced with 8 words
?swet?, ?swwet?, ?sweet?, ?swett?, ?swweet?, and so
on.
Another common type of spelling mistakes oc-
cur because of skipping some of characters from the
spelling. like ?there? is generally written as ?thr?.
Such types of spelling mistakes are not currently
handled by our system. We propose to use phonetic
level spell correction method in future.
3.6 Senti Features
At this step, we try to reduce the effect of non-
sentiment bearing tokens on our classification sys-
tem. In the baseline method, we considered all the
unigram tokens equally and scored them using the
Naive Bayes formula (Refer Equation 1). Here, we
try to boost the scores of sentiment bearing words.
In this step, we look for each token in a pre-defined
list of positive and negative words. We use the list of
of most commonly used positive and negative words
provided by Twitrratr5. When we come across a to-
ken in this list, instead of scoring it using the Naive
Bayes formula (Refer Equation 1), we score the to-
ken +/- 1 depending on the list in which it exist. All
the tokens which are missing from this list went un-
der step 3.3, 3.4, 3.5 and were checked for their oc-
currence after each step.
5http://twitrratr.com/
13
3.7 Noun Identification
After doing all the corrections (3.3 - 3.6) on a word,
we look at the reduced word if it is being converted
to a Noun or not. We identify the word as a Noun
word by looking at its part of speech tag in English
WordNet(Miller, 1995). If the majority sense (most
commonly used sense) of that word is Noun, we
discard the word while scoring. Noun words don?t
carry sentiment and thus are of no use in our experi-
ments.
3.8 Popularity Score
This scoring method boosts the scores of the most
commonly used words, which are domain specific.
For example, happy is used predominantly for ex-
pressing the positive sentiment. In this method, we
multiple its popularity factor (pF) to the score of
each unigram token which has been scored in the
previous steps. We use the occurrence frequency of
a token in positive and negative dataset to decide on
the weight of popularity score. Equation 2 shows
how the popularity factor is calculated for each to-
ken. We selected a threshold 0.01 min support as the
cut-off criteria and reduced it by half at every level.
Support of a word is defined as the proportion of
tweets in the dataset which contain this token. The
value 0.01 is chosen such that we cover a large num-
ber of tokens without missing important tokens, at
the same time pruning less frequent tokens.
Pf = Frequency in Positive Training Set
Nf = Frequency in Negative Training Set
if(Pf ?Nf ) > 1000)
pF = 0.9;
elseif((Pf ?Nf ) > 500)
pF = 0.8;
elseif((Pf ?Nf ) > 250)
pF = 0.7;
elseif((Pf ?Nf ) > 100)
pF = 0.5;
elseif((Pf ?Nf < 50))
pF = 0.1;
(2)
Figure 1 shows the flow of our approach.
Figure 1: Flow Chart of our Algorithm
4 Datasets
In this section, we explain the two datasets used in
this research. Both of these datasets are built using
noisy labels.
4.1 Stanford Dataset
This dataset(Go et al, 2009) was built automat-
ically using emoticons as noisy labels. All the
tweets which contain ?:)? were marked positive and
tweets containing ?:(? were marked negative. Tweets
that did not have any of these labels or had both
were discarded. The training dataset has ?1.6 mil-
lion tweets, equal number of positive and negative
tweets. The training dataset was annotated into two
classes (positive and negative) while the testing data
was hand annotated into three classes (positive, neg-
ative and neutral). For our experimentation, we use
only positive and negative class tweets from the test-
ing dataset for our experimentation. Table 1 gives
the details of dataset.
Training Tweets
Positive 800,000
Negative 800,000
Total 1,600,000
Testing Tweets
Positive 180
Negative 180
Objective 138
Total 498
Table 1: Stanford Twitter Dataset
14
4.2 Mejaj
Mejaj dataset(Bora, 2012) was built using noisy la-
bels. They collected a set of 40 words and manually
categorized them into positive and negative. They
label a tweet as positive if it contains any of the pos-
itive sentiment words and as negative if it contains
any of the negative sentiment words. Tweets which
do not contain any of these noisy labels and tweets
which have both positive and negative words were
discarded. Table 2 gives the list of words which were
used as noisy labels. This dataset contains only two
class data. Table 3 gives the details of the dataset.
Positive Labels Negative Labels
amazed, amused,
attracted, cheerful,
delighted, elated,
excited, festive, funny,
hilarious, joyful,
lively, loving,
overjoyed, passion,
pleasant, pleased,
pleasure, thrilled,
wonderful
annoyed, ashamed,
awful, defeated,
depressed,
disappointed,
discouraged,
displeased,
embarrassed, furious,
gloomy, greedy,
guilty, hurt, lonely,
mad, miserable,
shocked, unhappy,
upset
Table 2: Noisy Labels for annotating Mejaj Dataset
Training Tweets
Positive 668,975
Negative 795,661
Total 1,464,638
Testing Tweets
Positive 198
Negative 204
Total 402
Table 3: Mejaj Dataset
5 Experiment
In this section, we explain the experiments carried
out using the above proposed approach.
5.1 Stanford Dataset
On this dataset(Go et al, 2009), we perform a series
of experiments. In the first series of experiments,
we train on the given training data and test on the
testing data. In the second series of experiments,
we perform 5 fold cross validation using the training
data. Table 4 shows the results of each of these ex-
periments on steps which are explained in Approach
(Section 3).
In table 4, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.2 Mejaj Dataset
Similar series of experiments were performed on
this dataset(Bora, 2012) too. In the first series of
experiments, training and testing was done on the
respective given datasets. In the second series of ex-
periments, we perform 5 fold cross validation on the
training data. Table 5 shows the results of each of
these experiments.
In table 5, we give results for each step emoticons
and punctuations handling, spell correction, stem-
ming and stop word removal mentioned in Approach
Section (Section 3). The Baseline + All Combined
results refers to combination of these steps (emoti-
cons, punctuations, spell correction, Stemming and
stop word removal) performed together. Series 2 re-
sults are average of accuracy of each fold.
5.3 Cross Dataset
To validate the robustness of our approach, we ex-
perimented with cross dataset training and testing.
We trained our system on one dataset and tested on
the other dataset. Table 6 reports the results of cross
dataset evaluations.
6 Feature Vector Approach
In this feature vector approach, we form features us-
ing Unigrams, Bigrams, Hashtags (#), Targets (@),
Emoticons, Special Symbol (?!?) and used a semi-
supervised SVM classifier. Our feature vector com-
prised of 11 features. We divide the features into
two groups, NLP features and Twitter specific fea-
tures. NLP features include frequency of positive
15
Method Series 1 (%) Series 2 (%)
Baseline 78.8 80.1
Baseline + Emoticons + Punctuations 81.3 82.1
Baseline + Spell Correction 81.3 81.6
Baseline + Stemming 81.9 81.7
Baseline + Stop Word Removal 81.7 82.3
Baseline + All Combined (AC) 83.5 85.4
AC + Senti Features (wSF) 85.5 86.2
wSF + Noun Identification (wNI) 85.8 87.1
wNI + Popularity Score 87.2 88.4
Table 4: Results on Stanford Dataset
Method Series 1 (%) Series 2 (%)
Baseline 77.1 78.6
Baseline + Emoticons + Punctuations 80.3 80.4
Baseline + Spell Correction 80.1 80.0
Baseline + Stemming 79.1 79.7
Baseline + Stop Word Removal 80.2 81.7
Baseline + All Combined (AC) 82.9 84.1
AC + Senti Features (wSF) 86.8 87.3
wSF + Noun Identification (wNI) 87.6 88.2
wNI + Popularity Score 88.1 88.1
Table 5: Results on Mejaj Dataset
Method Training Dataset Testing Dataset Accuracy
wNI + Popularity Score Stanford Mejaj 86.4%
wNI + Popularity Score Mejaj Stanford 84.7%
Table 6: Results on Cross Dataset evaluation
NLP Unigram (f1) # of positive and negative unigramBigram (f2) # of positive and negative Bigram
Twitter Specific
Hashtags (f3) # of positive and negative hashtags
Emoticons (f4) # of positive and negative emoticons
URLs (f5) Binary Feature - presence of URLs
Targets (f6) Binary Feature - presence of Targets
Special Symbols (f7) Binary Feature - presence of ?!?
Table 7: Features and Description
16
Feature Set Accuracy (Stanford)
f1 + f2 85.34%
f3 + f4 + f7 53.77%
f3 + f4 + f5 + f6 + f7 60.12%
f1 + f2 + f3 + f4 + f7 85.89%
f1 + f2 + f3 + f4 +
f5 + f6 + f7 87.64%
Table 8: Results of Feature Vector Classifier on Stanford
Dataset
unigrams matched, negative unigrams matched, pos-
itive bigrams matched, negative bigrams matched,
etc and Twitter specific features included Emoti-
cons, Targets, HashTags, URLs, etc. Table 7 shows
the features we have considered.
HashTags polarity is decided based on the con-
stituent words of the hashtags. Using the list of pos-
itive and negative words from Twitrratr6, we try to
find if hashtags contains any of these words. If so,
we assign the polarity of that to the hashtag. For
example, ?#imsohappy? contains a positive word
?happy?, thus this hashtag is considered as posi-
tive hashtag. We use the emoticons list provided
by (Agarwal et al, 2011) in their research. This
list7 is built from wikipedia list of emoticons8 and
is hand tagged into five classes (extremely positive,
positive, neutral, negative and extremely negative).
We reduce this five class list to two class by merging
extremely positive and positive class to single posi-
tive class and rest other classes (extremely negative,
negative and neutral) to single negative class. Ta-
ble 8 reports the accuracy of our machine learning
classifier on Stanford dataset.
7 Discussion
In this section, we present a few examples evaluated
using our system. The following example denotes
the effect of incorporating the contribution of emoti-
cons on tweet classification. Example ?Ahhh I can?t
move it but hey w/e its on hell I?m elated right now
:-D?. This tweet contains two opinion words, ?hell?
and ?elated?. Using the unigram scoring method,
this tweet is classified neutral but it is actually posi-
6http://twitrratr.com/
7http://goo.gl/oCSnQ
8http://en.wikipedia.org/wiki/List of emoticons
tive. If we incorporate the effect of emoticon ?:-D?,
then this tweet is tagged positive. ?:-D? is a strong
positive emoticon.
Consider this example, ?Bill Clinton Fail -
Obama Win??. In this example, there are two senti-
ment bearing words, ?Fail? and ?Win?. Ideally this
tweet should be neutral but this is tagged as a posi-
tive tweet in the dataset as well as using our system.
In this tweet, if we calculate the popularity factor
(pF) for ?Win? and ?Fail?, they come out to be 0.9
and 0.8 respectively. Because of the popularity fac-
tor weight, the positive score domniates the negative
score and thus the tweet is tagged as positive. It is
important to identify the context flow in the text and
also how each of these words modify or depend on
the other words of the tweet.
For calculating the system performance, we as-
sume that the dataset which is used here is correct.
Most of the times this assumption is true but there
are a few cases where it fails. For example, this
tweet ?My wrist still hurts. I have to get it looked
at. I HATE the dr/dentist/scary places. :( Time to
watch Eagle eye. If you want to join, txt!? is tagged
as positive, but actually this should have been tagged
negative. Such erroneous tweets also effect the sys-
tem performance.
There are few limitations with the current pro-
posed approach which are also open research prob-
lems.
1. Spell Correction: In the above proposed ap-
proach, we gave a solution to spell correction
which works only when extra characters are en-
tered by the user. It fails when users skip some
characters like ?there? is spelled as ?thr?. We
propose the use of phonetic level spell correc-
tion to handle this problem.
2. Hashtag Segmentation: For handling hashtags,
we looked for the existence of the positive or
negative words9 in the hashtag. But there can
be some cases where it may not work correctly.
For example, ?#thisisnotgood?, in this hashtag
if we consider the presence of positive and neg-
ative words, then this hashtag is tagged posi-
tive (?good?). We fail to capture the presence
and effect of ?not? which is making this hash-
9word list taken from http://twitrratr.com/
17
tag as negative. We propose to devise and use
some logic to segment the hashtags to get cor-
rect constituent words.
3. Context Dependency: As discussed in one of
the examples above, even tweet text which is
limited to 140 characters can have context de-
pendency. One possible method to address this
problem is to identify the objects in the tweet
and then find the opinion towards those objects.
8 Conclusion and Future Work
Twitter sentiment analysis is a very important and
challenging task. Twitter being a microblog suffers
from various linguistic and grammatical errors. In
this research, we proposed a method which incorpo-
rates the popularity effect of words on tweet senti-
ment classification and also emphasis on how to pre-
process the Twitter data for maximum information
extraction out of the small content. On the Stanford
dataset, we achieved 87% accuracy using the scor-
ing method and 88% using SVM classifier. On Me-
jaj dataset, we showed an improvement of 4.77% as
compared to their (Bora, 2012) accuracy of 83.33%.
In future, This work can be extended through in-
corporation of better spell correction mechanisms
(may be at phonetic level) and word sense disam-
biguation. Also we can identify the target and enti-
ties in the tweet and the orientation of the user to-
wards them.
Acknowledgement
We would like to thank Vibhor Goel, Sourav Dutta
and Sonil Yadav for helping us with running SVM
classifier on such a large data.
References
Agarwal, A., Xie, B., Vovsha, I., Rambow, O. and Pas-
sonneau, R. (2011). Sentiment analysis of Twitter
data. In Proceedings of the Workshop on Languages
in Social Media LSM ?11.
Bora, N. N. (2012). Summarizing Public Opinions in
Tweets. In Journal Proceedings of CICLing 2012,
New Delhi, India.
Chesley, P. (2006). Using verbs and adjectives to auto-
matically classify blog sentiment. In In Proceedings
of AAAI-CAAW-06, the Spring Symposia on Compu-
tational Approaches.
Davidov, D., Tsur, O. and Rappoport, A. (2010). En-
hanced sentiment learning using Twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters COL-
ING ?10.
Diakopoulos, N. and Shamma, D. (2010). Characterizing
debate performance via aggregated twitter sentiment.
In Proceedings of the 28th international conference on
Human factors in computing systems ACM.
Draya, G., Planti, M., Harb, A., Poncelet, P., Roche,
M. and Trousset, F. (2009). Opinion Mining from
Blogs. In International Journal of Computer Informa-
tion Systems and Industrial Management Applications
(IJCISIM).
Go, A., Bhayani, R. and Huang, L. (2009). Twitter Sen-
timent Classification using Distant Supervision. In
CS224N Project Report, Stanford University.
Godbole, N., Srinivasaiah, M. and Skiena, S. (2007).
Large-Scale Sentiment Analysis for News and Blogs.
In Proceedings of the International Conference on We-
blogs and Social Media (ICWSM).
He, B., Macdonald, C., He, J. and Ounis, I. (2008). An
effective statistical approach to blog post opinion re-
trieval. In Proceedings of the 17th ACM conference on
Information and knowledge management CIKM ?08.
Hu, M. and Liu, B. (2004). Mining Opinion Features in
Customer Reviews. In AAAI.
Miller, G. A. (1995). WordNet: A Lexical Database for
English. Communications of the ACM 38, 39?41.
Pang, B., Lee, L. and Vaithyanathan, S. (2002). Thumbs
up? Sentiment Classification using Machine Learning
Techniques.
Turney, P. D. (2002). Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In ACL.
18
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
