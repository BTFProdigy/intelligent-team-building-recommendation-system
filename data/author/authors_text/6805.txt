A Geometric View on Bilingual Lexicon Extraction from Comparable
Corpora
E. Gaussier?, J.-M. Renders?, I. Matveeva?, C. Goutte?, H. De?jean?
?Xerox Research Centre Europe
6, Chemin de Maupertuis ? 38320 Meylan, France
Eric.Gaussier@xrce.xerox.com
?Dept of Computer Science, University of Chicago
1100 E. 58th St. Chicago, IL 60637 USA
matveeva@cs.uchicago.edu
Abstract
We present a geometric view on bilingual lexicon
extraction from comparable corpora, which allows
to re-interpret the methods proposed so far and iden-
tify unresolved problems. This motivates three new
methods that aim at solving these problems. Empir-
ical evaluation shows the strengths and weaknesses
of these methods, as well as a significant gain in the
accuracy of extracted lexicons.
1 Introduction
Comparable corpora contain texts written in differ-
ent languages that, roughly speaking, ?talk about
the same thing?. In comparison to parallel corpora,
ie corpora which are mutual translations, compara-
ble corpora have not received much attention from
the research community, and very few methods have
been proposed to extract bilingual lexicons from
such corpora. However, except for those found in
translation services or in a few international organ-
isations, which, by essence, produce parallel docu-
mentations, most existing multilingual corpora are
not parallel, but comparable. This concern is re-
flected in major evaluation conferences on cross-
language information retrieval (CLIR), e.g. CLEF1,
which only use comparable corpora for their multi-
lingual tracks.
We adopt here a geometric view on bilingual lex-
icon extraction from comparable corpora which al-
lows one to re-interpret the methods proposed thus
far and formulate new ones inspired by latent se-
mantic analysis (LSA), which was developed within
the information retrieval (IR) community to treat
synonymous and polysemous terms (Deerwester et
al., 1990). We will explain in this paper the moti-
vations behind the use of such methods for bilin-
gual lexicon extraction from comparable corpora,
and show how to apply them. Section 2 is devoted to
the presentation of the standard approach, ie the ap-
proach adopted by most researchers so far, its geo-
metric interpretation, and the unresolved synonymy
1http://clef.iei.pi.cnr.it:2002/
and polysemy problems. Sections 3 to 4 then de-
scribe three new methods aiming at addressing the
issues raised by synonymy and polysemy: in sec-
tion 3 we introduce an extension of the standard ap-
proach, and show in appendix A how this approach
relates to the probabilistic method proposed in (De-
jean et al, 2002); in section 4, we present a bilin-
gual extension to LSA, namely canonical correla-
tion analysis and its kernel version; lastly, in sec-
tion 5, we formulate the problem in terms of prob-
abilistic LSA and review different associated simi-
larities. Section 6 is then devoted to a large-scale
evaluation of the different methods proposed. Open
issues are then discussed in section 7.
2 Standard approach
Bilingual lexicon extraction from comparable cor-
pora has been studied by a number of researchers,
(Rapp, 1995; Peters and Picchi, 1995; Tanaka and
Iwasaki, 1996; Shahzad et al, 1999; Fung, 2000,
among others). Their work relies on the assump-
tion that if two words are mutual translations, then
their more frequent collocates (taken here in a very
broad sense) are likely to be mutual translations as
well. Based on this assumption, the standard ap-
proach builds context vectors for each source and
target word, translates the target context vectors us-
ing a general bilingual dictionary, and compares the
translation with the source context vector:
1. For each source word v (resp. target word w),
build a context vector ??v (resp. ??w ) consisting
in the measure of association of each word e
(resp. f ) in the context of v (resp. w), a(v, e).
2. Translate the context vectors with a general
bilingual dictionary D, accumulating the con-
tributions from words that yield identical trans-
lations.
3. Compute the similarity between source word v
and target word w using a similarity measures,
such as the Dice or Jaccard coefficients, or the
cosine measure.
As the dot-product plays a central role in all these
measures, we consider, without loss of generality,
the similarity given by the dot-product between ??v
and the translation of ??w :
???v ,????tr(w)? =
?
e
a(v, e)
?
f,(e,f)inD
a(w, f)
=
?
(e,f)?D
a(v, e) a(w, f) (1)
Because of the translation step, only the pairs (e, f)
that are present in the dictionary contribute to the
dot-product.
Note that this approach requires some general
bilingual dictionary as initial seed. One way to cir-
cumvent this requirement consists in automatically
building a seed lexicon based on spelling and cog-
nates clues (Koehn and Knight, 2002). Another ap-
proach directly tackles the problem from scratch by
searching for a translation mapping which optimally
preserves the intralingual association measure be-
tween words (Diab and Finch, 2000): the under-
lying assumption is that pairs of words which are
highly associated in one language should have trans-
lations that are highly associated in the other lan-
guage. In this latter case, the association measure
is defined as the Spearman rank order correlation
between their context vectors restricted to ?periph-
eral tokens? (highly frequent words). The search
method is based on a gradient descent algorithm, by
iteratively changing the mapping of a single word
until (locally) minimizing the sum of squared differ-
ences between the association measure of all pairs
of words in one language and the association mea-
sure of the pairs of translated words obtained by the
current mapping.
2.1 Geometric presentation
We denote by si, 1 ? i ? p and tj , 1 ? j ? q the
source and target words in the bilingual dictionary
D. D is a set of n translation pairs (si, tj), and
may be represented as a p ? q matrix M, such that
Mij = 1 iff (si, tj) ? D (and 0 otherwise).2
Assuming there are m distinct source words
e1, ? ? ? , em and r distinct target words f1, ? ? ? , fr in
the corpus, figure 1 illustrates the geometric view of
the standard method.
The association measure a(v, e) may be viewed
as the coordinates of the m-dimensional context
vector ??v in the vector space formed by the or-
thogonal basis (e1, ? ? ? , em). The dot-product in (1)
only involves source dictionary entries. The corre-
sponding dimensions are selected by an orthogonal
2The extension to weighted dictionary entries Mij ? [0, 1]
is straightforward but not considered here for clarity.
projection on the sub-space formed by (s1, ? ? ? , sp),
using a p ? m projection matrix Ps. Note that
(s1, ? ? ? , sp), being a sub-family of (e1, ? ? ? , em), is
an orthogonal basis of the new sub-space. Similarly,
??w is projected on the dictionary entries (t1, ? ? ? , tq)
using a q ? r orthogonal projection matrix Pt. As
M encodes the relationship between the source and
target entries of the dictionary, equation 1 may be
rewritten as:
S(v, w) = ???v ,????tr(w)? = (Ps??v )> M (Pt??w ) (2)
where > denotes transpose. In addition, notice that
M can be rewritten as S>T , with S an n ? p and
T an n ? q matrix encoding the relations between
words and pairs in the bilingual dictionary (e.g. Ski
is 1 iff si is in the kth translation pair). Hence:
S(v, w)=??v>P>s S>TPt??w =?SPs??v , TPt??w ? (3)
which shows that the standard approach amounts to
performing a dot-product in the vector space formed
by the n pairs ((s1, tl), ? ? ? , (sp, tk)), which are as-
sumed to be orthogonal, and correspond to transla-
tion pairs.
2.2 Problems with the standard approach
There are two main potential problems associated
with the use of a bilingual dictionary.
Coverage. This is a problem if too few corpus
words are covered by the dictionary. However, if
the context is large enough, some context words
are bound to belong to the general language, so a
general bilingual dictionary should be suitable. We
thus expect the standard approach to cope well with
the coverage problem, at least for frequent words.
For rarer words, we can bootstrap the bilingual dic-
tionary by iteratively augmenting it with the most
probable translations found in the corpus.
Polysemy/synonymy. Because all entries on ei-
ther side of the bilingual dictionary are treated as or-
thogonal dimensions in the standard methods, prob-
lems may arise when several entries have the same
meaning (synonymy), or when an entry has sev-
eral meanings (polysemy), especially when only
one meaning is represented in the corpus.
Ideally, the similarities wrt synonyms should not
be independent, but the standard method fails to ac-
count for that. The axes corresponding to synonyms
si and sj are orthogonal, so that projections of a
context vector on si and sj will in general be uncor-
related. Therefore, a context vector that is similar to
si may not necessarily be similar to sj .
A similar situation arises for polysemous entries.
Suppose the word bank appears as both financial in-
stitution (French: banque) and ground near a river
Ps
e 2
e m
v
e 1 s 1
s p
v?
(s  ,t  )
t
t f
f
f(s  ,t  )1 1
(s  ,t  ) 2
1
r
w
w?
1
p
PtS T
p k
1 i
v"
w"
Figure 1: Geometric view of the standard approach
(French: berge), but only the pair (banque, bank)
is in the bilingual dictionary. The standard method
will deem similar river, which co-occurs with bank,
and argent (money), which co-occurs with banque.
In both situations, however, the context vectors of
the dictionary entries provide some additional infor-
mation: for synonyms si and sj , it is likely that ??si
and ??sj are similar; for polysemy, if the context vec-
tors
?????banque and ???bank have few translations pairs in
common, it is likely that banque and bank are used
with somewhat different meanings. The following
methods try to leverage this additional information.
3 Extension of the standard approach
The fact that synonyms may be captured through
similarity of context vectors3 leads us to question
the projection that is made in the standard method,
and to replace it with a mapping into the sub-space
formed by the context vectors of the dictionary en-
tries, that is, instead of projecting ??v on the sub-
space formed by (s1, ? ? ? , sp), we now map it onto
the sub-space generated by (??s1 , ? ? ? ,??sp). With this
mapping, we try to find a vector space in which syn-
onymous dictionary entries are close to each other,
while polysemous ones still select different neigh-
bors. This time, if ??v is close to ??si and ??sj , si and
sj being synonyms, the translations of both si and
sj will be used to find those words w close to v.
Figure 2 illustrates this process. By denoting Qs,
respectively Qt, such a mapping in the source (resp.
target) side, and using the same translation mapping
(S, T ) as above, the similarity between source and
target words becomes:
S(v, w)=?SQs??v , TQt??w ?=??v>Q>s S>TQt??w (4)
A natural choice for Qs (and similarly for Qt) is the
following m ? p matrix:
Qs = R>s =
?
??
a(s1, e1) ? ? ? a(sp, e1)
.
.
.
.
.
.
.
.
.
a(s1, em) ? ? ? a(sp, em)
?
??
3This assumption has been experimentally validated in sev-
eral studies, e.g. (Grefenstette, 1994; Lewis et al, 1967).
but other choices, such as a pseudo-inverse of Rs,
are possible. Note however that computing the
pseudo-inverse of Rs is a complex operation, while
the above projection is straightforward (the columns
of Q correspond to the context vectors of the dic-
tionary words). In appendix A we show how this
method generalizes over the probabilistic approach
presented in (Dejean et al, 2002). The above
method bears similarities with the one described
in (Besanc?on et al, 1999), where a matrix similar
to Qs is used to build a new term-document ma-
trix. However, the motivations behind their work
and ours differ, as do the derivations and the gen-
eral framework, which justifies e.g. the choice of
the pseudo-inverse of Rs in our case.
4 Canonical correlation analysis
The data we have at our disposal can naturally be
represented as an n ? (m + r) matrix in which
the rows correspond to translation pairs, and the
columns to source and target vocabularies:
C =
e1 ? ? ? em f1 ? ? ? fr
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (s(1), t(1))
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (s(n), t(n))
where (s(k), t(k)) is just a renumbering of the trans-
lation pairs (si, tj).
Matrix C shows that each translation pair sup-
ports two views, provided by the context vectors in
the source and target languages. Each view is con-
nected to the other by the translation pair it repre-
sents. The statistical technique of canonical corre-
lation analysis (CCA) can be used to identify direc-
tions in the source view (first m columns of C) and
target view (last r columns of C) that are maximally
correlated, ie ?behave in the same way? wrt the
translation pairs. We are thus looking for directions
in the source and target vector spaces (defined by
the orthogonal bases (e1, ? ? ? , em) and (f1, ? ? ? , fr))
such that the projections of the translation pairs on
these directions are maximally correlated. Intu-
itively, those directions define latent semantic axes
se
e
e
v
f
f
f(s  ,t  )1
2
1
r
w
1
tS T
em
e1
e2
m
1
2
s
s
s
s
(s  ,t  )
1(s  ,t  )
p
1
k
i
f
fr
2f t
t
t
t
1
2
w"
v"
1
2
p
k
q
i
v
wQ Q
Figure 2: Geometric view of the extended approach
that capture the implicit relations between transla-
tion pairs, and induce a natural mapping across lan-
guages. Denoting by ?s and ?t the directions in the
source and target spaces, respectively, this may be
formulated as:
? = max
?s,?t
?
i??s,??s (i)???t,
??t (i)???
i??s,??s (i)?
?
j??t,
??t (j)?
As in principal component analysis, once the first
two directions (?1s , ?1t ) have been identified, the pro-
cess can be repeated in the sub-space orthogonal
to the one formed by the already identified direc-
tions. However, a general solution based on a set of
eigenvalues can be proposed. Following e.g. (Bach
and Jordan, 2001), the above problem can be re-
formulated as the following generalized eigenvalue
problem:
B ? = ?D ? (5)
where, denoting again Rs and Rt the first m and last
r (respectively) columns of C, we define:
B =
( 0 RtR>t RsR>s
RsR>s RtR>t 0
)
,
D =
( (RsR>s )2 0
0 (RtR>t )2
)
, ? =
( ?s
?t
)
The standard approach to solve eq. 5 is to per-
form an incomplete Cholesky decomposition of a
regularized form of D (Bach and Jordan, 2001).
This yields pairs of source and target directions
(?1s , ?1t ), ? ? ? , (?ls, ?lt) that define a new sub-space in
which to project words from each language. This
sub-space plays the same role as the sub-space de-
fined by translation pairs in the standard method, al-
though with CCA, it is derived from the corpus via
the context vectors of the translation pairs. Once
projected, words from different languages can be
compared through their dot-product or cosine. De-
noting ?s =
[
?1s , . . . ?ls
]>
, and ?t =
[
?1t , . . . ?lt
]>
,
the similarity becomes (figure 3):
S(v, w) = ??s??v , ?t??w ? = ??v>?>s ?t??w (6)
The number l of vectors retained in each language
directly defines the dimensions of the final sub-
space used for comparing words across languages.
CCA and its kernelised version were used in (Vi-
nokourov et al, 2002) as a way to build a cross-
lingual information retrieval system from parallel
corpora. We show here that it can be used to in-
fer language-independent semantic representations
from comparable corpora, which induce a similarity
between words in the source and target languages.
5 Multilingual probabilistic latent
semantic analysis
The matrix C described above encodes in each row
k the context vectors of the source (first m columns)
and target (last r columns) of each translation pair.
Ideally, we would like to cluster this matrix such
that translation pairs with synonymous words ap-
pear in the same cluster, while translation pairs with
polysemous words appear in different clusters (soft
clustering). Furthermore, because of the symmetry
between the roles played by translation pairs and vo-
cabulary words (synonymous and polysemous vo-
cabulary words should also behave as described
above), we want the clustering to behave symmet-
rically with respect to translation pairs and vocabu-
lary words. One well-motivated method that fulfills
all the above criteria is Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999).
Assuming that C encodes the co-occurrences be-
tween vocabulary words w and translation pairs d,
PLSA models the probability of co-occurrence w
and d via latent classes ?:
P (w, d) =
?
?
P (?) P (w|?) P (d|?) (7)
where, for a given class, words and translation pairs
are assumed to be independently generated from
class-conditional probabilities P (w|?) and P (d|?).
Note here that the latter distribution is language-
independent, and that the same latent classes are
used for the two languages. The parameters of the
model are obtained by maximizing the likelihood of
the observed data (matrix C) through Expectation-
Maximisation algorithm (Dempster et al, 1977). In
ee
e
v
f
f
f
2
1
r
w
1
e
e1
e2
m
1
2
f
fr
2f
v"
v
w(CCA)
w"
(CCA)
m
(?1s , ?1t )
?1s
?is
?ls
?2s
(?ls, ?lt)
(?2s , ?2t ) ?1t
?lt
?s ?t
?2t
?it
Figure 3: Geometric view of the Canonical Correlation Analysis approach
addition, in order to reduce the sensitivity to initial
conditions, we use a deterministic annealing scheme
(Ueda and Nakano, 1995). The update formulas for
the EM algorithm are given in appendix B.
This model can identify relevant bilingual latent
classes, but does not directly define a similarity be-
tween words across languages. That may be done
by using Fisher kernels as described below.
Associated similarities: Fisher kernels
Fisher kernels (Jaakkola and Haussler, 1999) de-
rive a similarity measure from a probabilistic model.
They are useful whenever a direct similarity be-
tween observed feature is hard to define or in-
sufficient. Denoting `(w) = lnP (w|?) the log-
likelihood for example w, the Fisher kernel is:
K(w1, w2) = ?`(w1)>IF?1?`(w2) (8)
The Fisher information matrix IF =
E
(
?`(x)?`(x)>
)
keeps the kernel indepen-
dent of reparameterisation. With a suitable
parameterisation, we assume IF ? 1. For PLSA
(Hofmann, 2000), the Fisher kernel between two
words w1 and w2 becomes:
K(w1, w2) =
?
?
P (?|w1)P (?|w2)
P (?) (9)
+
?
d
P? (d|w1)P? (d|w2)
?
?
P (?|d,w1)P (?|d,w2)
P (d|?)
where d ranges over the translation pairs. The
Fisher kernel performs a dot-product in a vector
space defined by the parameters of the model. With
only one class, the expression of the Fisher kernel
(9) reduces to:
K(w1, w2) = 1 +
?
d
P? (d|w1)P? (d|w2)
P (d)
Apart from the additional intercept (?1?), this is
exactly the similarity provided by the standard
method, with associations given by scaled empir-
ical frequencies a(w, d) = P? (d|w)/
?
P (d). Ac-
cordingly, we expect that the standard method and
the Fisher kernel with one class should have simi-
lar behaviors. In addition to the above kernel, we
consider two additional versions, obtained:through
normalisation (NFK) and exponentiation (EFK):
NFK(w1, w2) =
K(w1, w2)?
K(w1)K(w2)
(10)
EFK(w1, w2) = e?
1
2 (K(w1)+K(w2)?2K(w1,w2))
where K(w) stands for K(w, w).
6 Experiments and results
We conducted experiments on an English-French
corpus derived from the data used in the multi-
lingual track of CLEF2003, corresponding to the
newswire of months May 1994 and December 1994
of the Los Angeles Times (1994, English) and Le
Monde (1994, French). As our bilingual dictionary,
we used the ELRA multilingual dictionary,4 which
contains ca. 13,500 entries with at least one match
in our corpus. In addition, the following linguis-
tic preprocessing steps were performed on both the
corpus and the dictionary: tokenisation, lemmatisa-
tion and POS-tagging. Only lexical words (nouns,
verbs, adverbs, adjectives) were indexed and only
single word entries in the dicitonary were retained.
Infrequent words (occurring less than 5 times) were
discarded when building the indexing terms and the
dictionary entries. After these steps our corpus con-
tains 34,966 distinct English words, and 21,140 dis-
tinct French words, leading to ca. 25,000 English
and 13,000 French words not present in the dictio-
nary.
To evaluate the performance of our extraction
methods, we randomly split the dictionaries into a
training set with 12,255 entries, and a test set with
1,245 entries. The split is designed in such a way
that all pairs corresponding to the same source word
are in the same set (training or test). All methods
use the training set as the sole available resource
and predict the most likely translations of the terms
in the source language (English) belonging to the
4Available through www.elra.info
test set. The context vectors were defined by com-
puting the mutual information association measure
between terms occurring in the same context win-
dow of size 5 (ie. by considering a neighborhood of
+/- 2 words around the current word), and summing
it over all contexts of the corpora. Different associ-
ation measures and context sizes were assessed and
the above settings turned out to give the best perfor-
mance even if the optimum is relatively flat. For
memory space and computational efficiency rea-
sons, context vectors were pruned so that, for each
term, the remaining components represented at least
90 percent of the total mutual information. After
pruning, the context vectors were normalised so that
their Euclidean norm is equal to 1. The PLSA-based
methods used the raw co-occurrence counts as asso-
ciation measure, to be consistent with the underly-
ing generative model. In addition, for the extended
method, we retained only the N (N = 200 is the
value which yielded the best results in our experi-
ments) dictionary entries closest to source and tar-
get words when doing the projection with Q. As
discussed below, this allows us to get rid of spuri-
ous relationships.
The upper part of table 1 summarizes the results
we obtained, measured in terms of F-1 score for
different lengths of the candidate list, from 20 to
500. For each length, precision is based on the num-
ber of lists that contain an actual translation of the
source word, whereas recall is based on the num-
ber of translations provided in the reference set and
found in the list. Note that our results differ from the
ones previously published, which can be explained
by the fact that first our corpus is relatively small
compared to others, second that our evaluation re-
lies on a large number of candidates, which can oc-
cur as few as 5 times in the corpus, whereas previous
evaluations were based on few, high frequent terms,
and third that we do not use the same bilingual dic-
tionary, the coverage of which being an important
factor in the quality of the results obtained. Long
candidate lists are justified by CLIR considerations,
where longer lists might be preferred over shorter
ones for query expansion purposes. For PLSA, the
normalised Fisher kernels provided the best results,
and increasing the number of latent classes did not
lead in our case to improved results. We thus dis-
play here the results obtained with the normalised
version of the Fisher kernel, using only one compo-
nent. For CCA, we empirically optimised the num-
ber of dimensions to be used, and display the results
obtained with the optimal value (l = 300).
As one can note, the extended approach yields
the best results in terms of F1-score. However, its
performance for the first 20 candidates are below
the standard approach and comparable to the PLSA-
based method. Indeed, the standard approach leads
to higher precision at the top of the list, but lower
recall overall. This suggests that we could gain in
performance by re-ranking the candidates of the ex-
tended approach with the standard and PLSA meth-
ods. The lower part of table 1 shows that this is
indeed the case. The average precision goes up
from 0.4 to 0.44 through this combination, and the
F1-score is significantly improved for all the length
ranges we considered (bold line in table 1).
7 Discussion
Extended method As one could expect, the ex-
tended approach improves the recall of our bilingual
lexicon extraction system. Contrary to the standard
approach, in the extended approach, all the dictio-
nary words, present or not in the context vector of a
given word, can be used to translate it. This leads to
a noise problem since spurious relations are bound
to be detected. The restriction we impose on the
translation pairs to be used (N nearest neighbors)
directly aims at selecting only the translation pairs
which are in true relation with the word to be trans-
lated.
Multilingual PLSA Even though theoretically
well-founded, PLSA does not lead to improved per-
formance. When used alone, it performs slightly
below the standard method, for different numbers
of components, and performs similarly to the stan-
dard method when used in combination with the
extended method. We believe the use of mere co-
occurrence counts gives a disadvantage to PLSA
over other methods, which can rely on more sophis-
ticated measures. Furthermore, the complexity of
the final vector space (several millions of dimen-
sions) in which the comparison is done entails a
longer processing time, which renders this method
less attractive than the standard or extended ones.
Canonical correlation analysis The results we ob-
tain with CCA and its kernel version are disappoint-
ing. As already noted, CCA does not directly solve
the problems we mentioned, and our results show
that CCA does not provide a good alternative to the
standard method. Here again, we may suffer from a
noise problem, since each canonical direction is de-
fined by a linear combination that can involve many
different vocabulary words.
Overall, starting with an average precision of 0.35
as provided by the standard approach, we were able
to increase it to 0.44 with the methods we consider.
Furthermore, we have shown here that such an im-
provement could be achieved with relatively simple
20 60 100 160 200 260 300 400 500 Avg. Prec.
standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35
Ext (N=500) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40
CCA (l=300) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25
NFK(k=1) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30
Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44
Ext + NFK(k=1) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42
Ext + NFK(k=4) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41
Ext + NFK (k=16) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40
Table 1: Results of the different methods; F-1 score at different number of candidate translations. Ext refers
to the extended approach, whereas NFK stands for normalised Fisher kernel.
methods. Nevertheless, there are still a number of
issues that need be addressed. The most impor-
tant one concerns the combination of the different
methods, which could be optimised on a validation
set. Such a combination could involve Fisher ker-
nels with different latent classes in a first step, and
a final combination of the different methods. How-
ever, the results we obtained so far suggest that the
rank of the candidates is an important feature. It is
thus not guaranteed that we can gain over the com-
bination we used here.
8 Conclusion
We have shown in this paper how the problem of
bilingual lexicon extraction from comparable cor-
pora could be interpreted in geometric terms, and
how this view led to the formulation of new solu-
tions. We have evaluated the methods we propose
on a comparable corpus extracted from the CLEF
colection, and shown the strengths and weaknesses
of each method. Our final results show that the com-
bination of relatively simple methods helps improve
the average precision of bilingual lexicon extrac-
tion methods from comparale corpora by 10 points.
We hope this work will help pave the way towards
a new generation of cross-lingual information re-
trieval systems.
Acknowledgements
We thank J.-C. Chappelier and M. Rajman who
pointed to us the similarity between our extended
method and the model DSIR (distributional seman-
tics information retrieval), and provided us with
useful comments on a first draft of this paper. We
also want to thank three anonymous reviewers for
useful comments on a first version of this paper.
References
F. R. Bach and M. I. Jordan. 2001. Kernel inde-
pendent component analysis. Journal of Machine
Learning Research.
R. Besanc?on, M. Rajman, and J.-C. Chappelier.
1999. Textual similarities based on a distribu-
tional approach. In Proceedings of the Tenth In-
ternational Workshop on Database and Expert
Systems Applications (DEX?99), Florence, Italy.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391?407.
H. Dejean, E. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
International Conference on Computational Lin-
guistics, COLING?02.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the
Royal Statistical Society, Series B, 39(1):1?38.
Mona Diab and Steve Finch. 2000. A statisti-
cal word-level translation model for compara-
ble corpora. In Proceeding of the Conference
on Content-Based Multimedia Information Ac-
cess (RIAO).
Pascale Fung. 2000. A statistical view on bilingual
lexicon extraction - from parallel corpora to non-
parallel corpora. In J. Ve?ronis, editor, Parallel
Text Processing. Kluwer Academic Publishers.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Construction. Kluwer Academic Pub-
lishers.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic analysis. In Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelli-
gence, pages 289?296. Morgan Kaufmann.
Thomas Hofmann. 2000. Learning the similarity of
documents: An information-geometric approach
to document retrieval and categorization. In Ad-
vances in Neural Information Processing Systems
12, page 914. MIT Press.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Advances in Neural Information Pro-
cessing Systems 11, pages 487?493.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora.
In ACL 2002 Workshop on Unsupervised Lexical
Acquisition.
P.A.W. Lewis, P.B. Baxendale, and J.L. Ben-
net. 1967. Statistical discrimination of the
synonym/antonym relationship between words.
Journal of the ACM.
C. Peters and E. Picchi. 1995. Capturing the com-
parable: A system for querying comparable text
corpora. In JADT?95 - 3rd International Con-
ference on Statistical Analysis of Textual Data,
pages 255?262.
R. Rapp. 1995. Identifying word translations in
nonparallel texts. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics.
I. Shahzad, K. Ohtake, S. Masuyama, and K. Ya-
mamoto. 1999. Identifying translations of com-
pound nouns using non-aligned corpora. In Pro-
ceedings of the Workshop MAL?99, pages 108?
113.
K. Tanaka and Hideya Iwasaki. 1996. Extraction of
lexical translations from non-aligned corpora. In
International Conference on Computational Lin-
guistics, COLING?96.
Naonori Ueda and Ryohei Nakano. 1995. Deter-
ministic annealing variant of the EM algorithm.
In Advances in Neural Information Processing
Systems 7, pages 545?552.
A. Vinokourov, J. Shawe-Taylor, and N. Cristian-
ini. 2002. Finding language-independent seman-
tic representation of text using kernel canonical
correlation analysis. In Advances in Neural In-
formation Processing Systems 12.
Appendix A: probabilistic interpretation of
the extension of standard approach
As in section 3, SQs??v is an n-dimensional vector,
defined over ((s1, tl), ? ? ? , (sp, tk)). The coordinate
of SQs??v on the axis corresponding to the transla-
tion pair (si, tj) is ???si ,??v ? (the one for TQt??w on
the same axis being ???tj ,??w ?). Thus, equation 4 can
be rewritten as:
S(v, w) =
?
(si,tj)
???si ,??v ????tj ,??w ?
which we can normalised in order to get a probabil-
ity distribution, leading to:
S(v, w) =
?
(si,tj)
P (v)P (si|v)P (w|tj)P (tj)
By imposing P (tj) to be uniform, and by denoting
C a translation pair, one arrives at:
S(v, w) ?
?
C
P (v)P (C|v)P (w|C)
with the interpretation that only the source, resp.
target, word in C is relevant for P (C|v), resp.
P (w|C). Now, if we are looking for those ws clos-
est to a given v, we rely on:
S(w|v) ?
?
C
P (C|v)P (w|C)
which is the probabilistic model adopted in (Dejean
et al, 2002). This latter model is thus a special case
of the extension we propose.
Appendix B: update formulas for PLSA
The deterministic annealing EM algorithm for
PLSA (Hofmann, 1999) leads to the following equa-
tions for iteration t and temperature ?:
P (?|w, d) = P (?)
?P (w|?)?P (d|?)??
?P (?)?P (w|?)?P (d|?)?
P (t+?)(?) = 1?
(w,d) n(w, d)
?
(w,d)
n(w, d)P (?|w, d)
P (t+?)(w|?) =
?
d n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)
P (t+?)(d|?) =
?
w n(w, d)P (?|w, d)?
(w,d) n(w, d)P (?|w, d)
where n(w, d) is the number of co-occurrences be-
tween w and d. Parameters are obtained by iterating
eqs 11?11 for each ?, 0 < ? ? 1.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 29?32, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Voice Enabled Procedure Browser
for the International Space Station
Manny Rayner, Beth Ann Hockey, Nikos Chatzichrisafis, Kim Farrell
ICSI/UCSC/RIACS/NASA Ames Research Center
Moffett Field, CA 94035?1000
mrayner@riacs.edu, bahockey@email.arc.nasa.gov
Nikos.Chatzichrisafis@web.de, kfarrell@email.arc.nasa.gov
Jean-Michel Renders
Xerox Research Center Europe
6 chemin de Maupertuis, Meylan, 38240, France
Jean-Michel.Renders@xrce.xerox.com
Abstract
Clarissa, an experimental voice enabled
procedure browser that has recently been
deployed on the International Space Sta-
tion (ISS), is to the best of our knowl-
edge the first spoken dialog system in
space. This paper gives background
on the system and the ISS procedures,
then discusses the research developed to
address three key problems: grammar-
based speech recognition using the Regu-
lus toolkit; SVM based methods for open
microphone speech recognition; and ro-
bust side-effect free dialogue management
for handling undos, corrections and con-
firmations.
1 Overview
Astronauts on the International Space Station (ISS)
spend a great deal of their time performing com-
plex procedures. Crew members usually have to
divide their attention between the task and a pa-
per or PDF display of the procedure. In addition,
since objects float away in microgravity if not fas-
tened down, it would be an advantage to be able
to keep both eyes and hands on the task. Clarissa,
an experimental speech enabled procedure navigator
(Clarissa, 2005), is designed to address these prob-
lems. The system was deployed on the ISS on Jan-
uary 14, 2005 and is scheduled for testing later this
year; the initial version is equipped with five XML-
encoded procedures, three for testing water quality
and two for space suit maintenance. To the best of
our knowledge, Clarissa is the first spoken dialogue
application in space.
The system includes commands for navigation:
forward, back, and to arbitrary steps. Other com-
mands include setting alarms and timers, record-
ing, playing and deleting voice notes, opening and
closing procedures, querying system status, and in-
putting numerical values. There is an optional mode
that aggressively requests confirmation on comple-
tion of each step. Open microphone speech recog-
nition is crucial for providing hands free use. To
support this, the system has to discriminate between
speech that is directed to it and speech that is not.
Since speech recognition is not perfect, and addi-
tional potential for error is added by the open micro-
phone task, it is also important to support commands
for undoing or correcting bad system responses.
The main components of the Clarissa system are
a speech recognition module, a classifier for exe-
cuting the open microphone accept/reject decision,
a semantic analyser, and a dialogue manager. The
rest of this paper will briefly give background on the
structure of the procedures and the XML representa-
tion, then describe the main research content of the
system.
2 Voice-navigable procedures
ISS procedures are formal documents that typically
represent many hundreds of person hours of prepa-
ration, and undergo a strict approval process. One
requirement in the Clarissa project was that the pro-
cedures should be displayed visually exactly as they
29
Figure 1: Adding voice annotations to a group of
steps
appear in the original PDF form. However, reading
these procedures verbatim would not be very useful.
The challenge is thus to let the spoken version di-
verge significantly from the written one, yet still be
similar enough in meaning that the people who con-
trol the procedures can be convinced that the two
versions are in practice equivalent.
Figure 1 illustrates several types of divergences
between the written and spoken versions, with
?speech bubbles? showing how procedure text is ac-
tually read out. In this procedure for space suit main-
tenance, one to three suits can be processed. The
group of steps shown cover filling of a ?dry LCVG?.
The system first inserts a question to ask which suits
require this operation, and then reads the passage
once for each suit, specifying each time which suit is
being referred to; if no suits need to be processed, it
jumps directly to the next section. Step 51 points the
user to a subprocedure. The spoken version asks if
the user wants to execute the steps of the subproce-
dure; if so, it opens the LCVG Water Fill procedure
and goes directly to step 6. If the user subsequently
goes past step 17 of the subprocedure, the system
warns that the user has gone past the required steps,
and suggests that they close the procedure.
Other important types of divergences concern en-
try of data in tables, where the system reads out an
appropriate question for each table cell, confirms the
value supplied by the user, and if necessary warns
about out-of-range values.
Rec Patterns Errors
Reject Bad Total
Text LF 3.1% 0.5% 3.6%
Text Surface 2.2% 0.8% 3.0%
Text Surface+LF 0.8% 0.8% 1.6%
SLM Surface 2.8% 7.4% 10.2%
GLM LF 1.4% 4.9% 6.3%
GLM Surface 2.9% 4.8% 7.7%
GLM Surface+LF 1.0% 5.0% 6.0%
Table 1: Speech understanding performance on six
different configurations of the system.
3 Grammar-based speech understanding
Clarissa uses a grammar-based recognition architec-
ture. At the start of the project, we had two main rea-
sons for choosing this approach over the more popu-
lar statistical one. First, we had no available training
data. Second, the system was to be designed for ex-
perts who would have time to learn its coverage, and
who moreover, as former military pilots, were com-
fortable with the idea of using controlled language.
Although there is not much to be found in the litera-
ture, an earlier study in which we had been involved
(Knight et al, 2001) suggested that grammar-based
systems outperformed statistical ones for this kind
of user. Given that neither of the above arguments is
very strong, we wanted to implement a framework
which would allow us to compare grammar-based
methods with statistical ones, and retain the option
of switching from a grammar-based framework to a
statistical one if that later appeared justified. The
Regulus and Alterf platforms, which we have devel-
oped under Clarissa and other earlier projects, are
designed to meet these requirements.
The basic idea behind Regulus (Regulus, 2005;
Rayner et al, 2003) is to extract grammar-based lan-
guage models from a single large unification gram-
mar, using example-based methods driven by small
corpora. Since grammar construction is now a
corpus-driven process, the same corpora can be used
to build statistical language models, facilitating a di-
rect comparison between the two methodologies.
On its own, however, Regulus only permits com-
parison at the level of recognition strings. Alterf
(Rayner and Hockey, 2003) extends the paradigm to
30
ID Rec Features Classifier Error rates
Classification Task
In domain Out Av
Good Bad
1 SLM Confidence Threshold 5.5% 59.1% 16.5% 11.8% 10.1%
2 GLM Confidence Threshold 7.1% 48.7% 8.9% 9.4% 7.0%
3 SLM Confidence + Lexical Linear SVM 2.8% 37.1% 9.0% 6.6% 7.4%
4 GLM Confidence + Lexical Linear SVM 2.8% 48.5% 8.7% 6.3% 6.2%
5 SLM Confidence + Lexical Quadratic SVM 2.6% 23.6% 8.5% 5.5% 6.9%
6 GLM Confidence + Lexical Quadratic SVM 4.3% 28.1% 4.7% 5.5% 5.4%
Table 2: Performance on accept/reject classification and the top-level task, on six different configurations.
the semantic level, by providing a trainable seman-
tic interpretation framework. Interpretation uses a
set of user-specified patterns, which can match ei-
ther the surface strings produced by both the statisti-
cal and grammar-based architectures, or the logical
forms produced by the grammar-based architecture.
Table 1 presents the result of an evaluation, car-
ried out on a set of 8158 recorded speech utterances,
where we compared the performance of a statisti-
cal/robust architecture (SLM) and a grammar-based
architecture (GLM). Both versions were trained off
the same corpus of 3297 utterances. We also show
results for text input simulating perfect recognition.
For the SLM version, semantic representations are
constructed using only surface Alterf patterns; for
the GLM and text versions, we can use either sur-
face patterns, logical form (LF) patterns, or both.
The ?Error? columns show the proportion of ut-
terances which produce no semantic interpretation
(?Reject?), the proportion with an incorrect seman-
tic interpretation (?Bad?), and the total.
Although the WER for the GLM recogniser is
only slightly better than that for the SLM recogniser
(6.27% versus 7.42%, 15% relative), the difference
at the level of semantic interpretation is considerable
(6.3% versus 10.2%, 39% relative). This is most
likely accounted for by the fact that the GLM ver-
sion is able to use logical-form based patterns, which
are not accessible to the SLM version. Logical-form
based patterns do not appear to be intrinsically more
accurate than surface (contrast the first two ?Text?
rows), but the fact that they allow tighter integration
between semantic understanding and language mod-
elling is intuitively advantageous.
4 Open microphone speech processing
The previous section described speech understand-
ing performance in terms of correct semantic inter-
pretation of in-domain input. However, open micro-
phone speech processing implies that some of the in-
put will not be in-domain. The intended behaviour
for the system is to reject this input. We would
also like it, when possible, to reject in-domain input
which has not been correctly recognised.
Surface output from the Nuance speech recog-
niser is a list of words, each tagged with a confidence
score; the usual way to make the accept/reject deci-
sion is by using a simple threshold on the average
confidence score. Intuitively, however, we should be
able to improve the decision quality by also taking
account of the information in the recognised words.
By thinking of the confidence scores as weights,
we can model the problem as one of classifying doc-
uments using a weighted bag of words model. It
is well known (Joachims, 1998) that Support Vec-
tor Machine methods are very suitable for this task.
We have implemented a version of the method de-
scribed by Joachims, which significantly improves
on the naive confidence score threshold method.
Performance on the accept/reject task can be eval-
uated directly in terms of the classification error. We
can also define a metric for the overall speech under-
standing task which includes the accept/reject deci-
sion, as a weighted loss function over the different
types of error. We assign weights of 1 to a false re-
ject of a correct interpretation, 2 to a false accept of
an incorrectly interpreted in-domain utterance, and 3
to a false accept of an out-of-domain utterance. This
31
captures the intuition that correcting false accepts is
considerably harder than correcting false rejects, and
that false accepts of utterances not directed at the
system are worse than false accepts of incorrectly
interpreted utterances.
Table 2 summarises the results of experiments
comparing performance of different recognisers and
accept/reject classifiers on a set of 10409 recorded
utterances. ?GLM? and ?SLM? refer respectively to
the best GLM and SLM recogniser configurations
from Table 1. ?Av? refers to the average classi-
fier error, and ?Task? to a normalised version of the
weighted task metric. The best SVM-based method
(line 6) outperforms the best naive threshold method
(line 2) by 5.4% to 7.0% on the task metric, a relative
improvement of 23%. The best GLM-based method
(line 6) and the best SLM-based method (line 5) are
equally good in terms of accept/reject classification
accuracy, but the GLM?s better speech understand-
ing performance means that it scores 22% better on
the task metric. The best quadratic kernel (line 6)
outscores the best linear kernel (line 4) by 13%. All
these differences are significant at the 5% level ac-
cording to the Wilcoxon matched-pairs test.
5 Side-effect free dialogue management
In an open microphone spoken dialogue application
like Clarissa, it is particularly important to be able
to undo or correct a bad system response. This
suggests the idea of representing discourse states
as objects: if the complete dialogue state is an ob-
ject, a move can be undone straightforwardly by
restoring the old object. We have realised this idea
within a version of the standard ?update seman-
tics? approach to dialogue management (Larsson
and Traum, 2000); the whole dialogue management
functionality is represented as a declarative ?update
function? relating the old dialogue state, the input
dialogue move, the new dialogue state and the out-
put dialogue actions.
In contrast to earlier work, however, we include
task information as well as discourse information in
the dialogue state. Each state also contains a back-
pointer to the previous state. As explained in detail
in (Rayner and Hockey, 2004), our approach per-
mits a very clean and robust treatment of undos, cor-
rections and confirmations, and also makes it much
simpler to carry out systematic regression testing of
the dialogue manager component.
Acknowledgements
Work at ICSI, UCSC and RIACS was supported
by NASA Ames Research Center internal fund-
ing. Work at XRCE was partly supported by the
IST Programme of the European Community, un-
der the PASCAL Network of Excellence, IST-2002-
506778. Several people not credited here as co-
authors also contributed to the implementation of
the Clarissa system: among these, we would par-
ticularly like to mention John Dowding, Susana
Early, Claire Castillo, Amy Fischer and Vladimir
Tkachenko. This publication only reflects the au-
thors? views.
References
Clarissa, 2005. http://www.ic.arc.nasa.gov/projects/clarissa/.
As of 26 April 2005.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning, Chemnitz, Germany.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Dia-
logue Systems Engineering, pages 323?340.
M. Rayner and B.A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in a
speech understanding architecture. In Proceedings of
the 10th EACL (demo track), Budapest, Hungary.
M. Rayner and B.A. Hockey. 2004. Side effect free
dialogue management in a voice enabled procedure
browser. In Proceedings of INTERSPEECH 2004, Jeju
Island, Korea.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL, Budapest, Hungary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 26 April 2005.
32
Combining labelled and unlabelled data: a case study on Fisher
kernels and transductive inference for biological entity recognition
Cyril Goutte, Herve Dejean, Eric Gaussier,
Nicola Cancedda and Jean-Michel Renders
Xerox Research Center Europe
6, chemin de Maupertuis
38240 Meylan, France
Abstract
We address the problem of using partially la-
belled data, eg large collections were only little
data is annotated, for extracting biological en-
tities. Our approach relies on a combination of
probabilistic models, which we use to model the
generation of entities and their context, and ker-
nel machines, which implement powerful cate-
gorisers based on a similarity measure and some
labelled data. This combination takes the form
of the so-called Fisher kernels which implement
a similarity based on an underlying probabilistic
model. Such kernels are compared with trans-
ductive inference, an alternative approach to
combining labelled and unlabelled data, again
coupled with Support Vector Machines. Exper-
iments are performed on a database of abstracts
extracted from Medline.
1 Introduction
The availability of electronic databases of ra-
pidly increasing sizes has encouraged the de-
velopment of methods that can tap into these
databases to automatically generate knowledge,
for example by retrieving relevant information
or extracting entities and their relationships.
Machine learning seems especially relevant in
this context, because it helps performing these
tasks with a minimum of user interaction.
A number of problems like entity extraction
or ltering can be mapped to supervised tech-
niques like categorisation. In addition, modern
supervised classication methods like Support
Vector Machines have proven to be ecient and
versatile. They do, however, rely on the avail-
ability of labelled data, where labels indicate
eg whether a document is relevant or whether
a candidate expression is an interesting entity.
This causes two important problems that mo-
tivate our work: 1) annotating data is often a
dicult and costly task involving a lot of hu-
man work
1
, such that large collections of la-
belled data are dicult to obtain, and 2) inter-
annotator agreement tends to be low in eg ge-
nomics collections (Krauthammer et al, 2000),
thus calling for methods that are able to deal
with noise and incomplete data.
On the other hand, unsupervised techniques
do not require labelled data and can thus be
applied regardless of the annotation problems.
Unsupervised learning, however, tend to be less
data-ecient than its supervised counterpart,
requiring many more examples to discover sig-
nicant features in the data, and is incapable
of solving the same kinds of problems. For ex-
ample, an ecient clustering technique may be
able to distribute documents in a number of
well-dened clusters. However, it will be unable
to decide which clusters are relevant without a
minimum of supervision.
This motivates our study of techniques that
rely on a combination of supervised and unsu-
pervised learning, in order to leverage the avail-
ability of large collections of unlabelled data and
use a limited amount of labelled documents.
The focus of this study is on a particular
application to the genomics literature. In ge-
nomics, a vast amount of knowledge still resides
in large collections of scientic papers such as
Medline, and several approaches have been pro-
posed to extract, (semi-)automatically, informa-
tion from such papers. These approaches range
from purely statistical ones to symbolic ones
relying on linguistic and knowledge processing
tools (Ohta et al, 1997; Thomas et al, 2000;
Proux et al, 2000, for example). Furthermore,
due to the nature of the problem at hand, meth-
1
If automatic annotation was available, we would ba-
sically have solved our Machine Learning problem
ods derived from machine learning are called
for, (Craven and Kumlien, 1999), whether su-
pervised, unsupervised or relying on a combi-
nation of both.
Let us insist on the fact that our work is pri-
marily concerned with combining labelled and
unlabelled data, and entity extraction is used
as an application in this context. As a conse-
quence, it is not our purpose at this point to
compare our experimental results to those ob-
tained by specic machine learning techniques
applied to entity extraction (Cali, 1999). Al-
though we certainly hope that our work can be
useful for entity extraction, we rather think of
it as a methodological study which can hope-
fully be applied to dierent applications where
unlabelled data may be used to improve the re-
sults of supervised learning algorithms. In addi-
tion, performing a fair comparison of our work
on standard information extraction benchmarks
is not straightforward: either we would need to
obtain a large amount of unlabelled data that is
comparable to the benchmark, or we would need
to \un-label" a portion of the data. In both
cases, comparing to existing results is dicult
as the amount of information used is dierent.
2 Classication for entity extraction
We formulate the following (binary) classica-
tion problem: given an input space X , and from
a dataset of N input-output pairs (x
k
; y
k
) 2
X  f 1; +1g, we want to learn a classier
h : X ! f 1; +1g so as to maximise the proba-
bility P (h(x) = y) over the xed but unknown
joint input-output distribution of (x; y) pairs.
In this setting, binary classication is essentially
a supervised learning problem.
In order to map this to the biological en-
tity recognition problem, we consider for each
candidate term, the following binary decision
problem: is the candidate a biological entity
2
(y = 1) or not (y =  1). The input space is a
high dimensional feature space containing lexi-
cal, morpho-syntactic and contextual features.
In order to assess the validity of combining
labelled and unlabelled data for the particular
task of biological entity extraction, we use the
following tools. First we rely on Suport Vec-
tor Machines together with transductive infer-
2
In our case, biological entities are proteins, genes
and RNA, cf. section 6.
ence (Vapnik, 1998; Joachims, 1999), a train-
ing technique that takes both labelled and unla-
belled data into account. Secondly, we develop
a Fisher kernel (Jaakkola and Haussler, 1999),
which derives the similarity from an underlying
(unsupervised) model of the data, used as a sim-
ilarity measure (aka kernel) within SVMs. The
learning process involves the following steps:
 Transductive inference: learn a SVM classi-
er h(x) using the combined (labelled and
unlabelled) dataset, using traditional ker-
nels.
 Fisher kernels:
1. Learn a probabilistic model of the data
P (xj) using combined unlabelled and
labelled data;
2. Derive the Fisher kernel K(x; z) ex-
pressing the similarity in X -space;
3. Learn a SVM classier h(x) using this
Fisher kernel and inductive inference.
3 Probabilistic models for
co-occurence data
In (Gaussier et al, 2002) we presented a gen-
eral hierarchical probabilistic model which gen-
eralises several established models like Nave
Bayes (Yang and Liu, 1999), probabilistic latent
semantic analysis (PLSA) (Hofmann, 1999) or
hierarchical mixtures (Toutanova et al, 2001).
In this model, data result from the observation
of co-occuring objects. For example, a docu-
ment collection is expressed as co-occurences
between documents and words; in entity extrac-
tion, co-occuring objects may be potential en-
tities and their context, for example. For co-
occuring objects i and j, the model is expressed
as follows:
P (i; j) =
X

P ()P (ij)
X

P (j)P (jj)
(1)
where  are latent classes for co-occurrences
(i; j) and  are latent nodes in a hierarchy gener-
ating objects j. In the case where no hierarchy
is needed (ie P (j) = ( = )), the model
reduces to PLSA:
P (i; j) =
X

P ()P (ij)P (jj) (2)
where  are now latent concepts over both i and
j. Parameters of the model (class probabilities
P () and class-conditional P (ij) and P (jj))
are learned using a deterministic annealing ver-
sion of the expectation-maximisation (EM) al-
gorithm (Hofmann, 1999; Gaussier et al, 2002).
4 Fisher kernels
Probabilistic generative models like PLSA and
hierarchical extensions (Gaussier et al, 2002)
provide a natural way to model the generation
of the data, and allow the use of well-founded
statistical tools to learn and use the model.
In addition, they may be used to derive a
model-based measure of similarity between ex-
amples, using the so-called Fisher kernels pro-
posed by Jaakkola and Haussler (1999). The
idea behind this kernel is that using the struc-
ture implied by the generative model will give
a more relevant similarity estimate, and allow
kernel methods like the support vector machines
or nearest neighbours to leverage the probabilis-
tic model and yield improved performance (Hof-
mann, 2000).
The Fisher kernel is obtained using the log-
likelihood of the model and the Fisher informa-
tion matrix. Let us consider our collection of
documents fx
k
g
k=1:::N
, and denote by `(x) =
logP (xj) the log-likelihood of the model for
data x. The expression of the Fisher kernel
(Jaakkola and Haussler, 1999) is then:
K(x
1
; x
2
) = r`(x
1
)
>
I
F
 1
r`(x
2
) (3)
The Fisher information matrix I
F
can be seen
as a way to keep the kernel expression inde-
pendent of parameterisation and is dened as
I
F
= E

r`(x)r`(x)
>

, where the gradient
is w.r.t.  and the expectation is taken over
P (xj). With a suitable parameterization, the
information matrix I is usually approximated by
the identity matrix (Hofmann, 2000), leading
to the simpler kernel expression: K(x
1
; x
2
) =
r`(x
1
)
>
r`(x
2
).
Depending on the model, the various log-
likelihoods and their derivatives will yield dif-
ferent Fisher kernel expressions. For PLSA (2),
the parameters are  = [P (); P (ij); P (jj)].
From the derivatives of the likelihood `(x) =
P
(i;j)2x
log P (i; j), we derive the following sim-
ilarity (Hofmann, 2000):
K(x
1
; x
2
) =
X

P (jd
i
)P (jd
j
)
P ()
(4)
+
X
w
b
P
wd
i
b
P
wd
j
X

P (jd
i
; w)P (jd
j
; w)
P (wj)
with
b
P
wd
i
,
b
P
wd
j
the empirical word distributions
in documents d
i
, d
j
.
5 Transductive inference
In standard, inductive SVM inference, the an-
notated data is used to infer a model, which is
then applied to unannotated test data. The in-
ference consists in a trade-o between the size
of the margin (linked to generalisation abilities)
and the number of training errors. Transductive
inference (Gammerman et al, 1998; Joachims,
1999) aims at maximising the margin between
positives and negatives, while minimising not
only the actual number of incorrect predictions
on labelled examples, but also the expected
number of incorrect predictions on the set of
unannotated examples.
This is done by including the unknown la-
bels as extra variables in the original optimisa-
tion problem. In the linearly separable case, the
new optimisation problem amounts now to nd
a labelling of the unannotated examples and a
hyperplane which separates all examples (anno-
tated and unannotated) with maximum margin.
In the non-separable case, slack variables are
also associated to unannotated examples and
the optimisation problem is now to nd a la-
belling and a hyperplane which optimally solves
the trade-o between maximising the margin
and minimising the number of misclassied ex-
amples (annotated and unannotated).
With the introduction of unknown labels as
supplementary optimisation variables, the con-
straints of the quadratic optimisation problem
are now nonlinear, which makes solving more
dicult. However, approximated iterative algo-
rithms exist which can eciently train Trans-
ductive SVMs. They are based on the principle
of gradually improving the solution by switching
the labels of unnannotated examples which are
misclassied at the current iteration, starting
from an initial labelling given by the standard
(inductive) SVM.
WUp Is the word capitalized?
WAllUp Is the word alls capitals?
WNum Does the word contain digits?
Table 1: Spelling features
6 Experiments
For our experiments, we used 184 abstracts from
the Medline site. In these articles, genes, pro-
teins and RNAs were manually annotated by a
biologist as part of the BioMIRE project. These
articles contain 1405 occurrences of gene names,
792 of protein names and 81 of RNA names. All
these entities are considered relevant biological
entities. We focus here on the task of identify-
ing names corresponding to such entities in run-
ning texts, without dierentiating genes from
proteins or RNAs. Once candidates for bio-
logical entity names have been identied, this
task amounts to a binary categorisation, rele-
vant candidates corresponding to biological en-
tity names. We divided these abstracts in a
training and development set (122 abstracts),
and a test set (62 abstracts). We then retained
dierent portions of the training labels, to be
used as labelled data, whereas the rest of the
data is considered unlabelled.
6.1 Denition of features
First of all, the abstracts are tokenised, tagged
and lemmatized. Candidates for biological en-
tity names are then selected on the basis of the
following heuristics: a token is considered a can-
didate if it appears in one of the biological lexicons
we have at our diposal, or if it does not belong to
our general English lexicon. This simple heuris-
tics allows us to retain 93% (1521 out of 1642)
of biological names in the training set (90% in
the test set), while considering only 21% of all
possible candidates (5845 out of 27350 tokens).
It thus provides a good pre-lter which signif-
icantly improves the performance, in terms of
speed, of our system. The biological lexicons
we use were provided by the BioMIRE project,
and were derived from the resources available
at: http://iubio.bio.indiana.edu/.
For each candidate, three types of features
were considered. We rst retained the part-of-
speech and some spelling information (table 1).
These features were chosen based on the inspec-
tion of gene and protein names in our lexicons.
LexPROTEIN Protein lexicon
LexGENE Gene lexicon
LexSPECIES Biological species lexicon
LEXENGLISH General English lexicon
Table 2: Features provided by lexicons.
The second type of features relates to the pres-
ence of the candidate in our lexical resources
3
(table 2). Lastly, the third type of features de-
scribes contextual information. The context we
consider contains the four preceding and the
four following words. However, we did not take
into account the position of the words in the
context, but only their presence in the right or
left context, and in addition we replaced, when-
ever possible, each word by a feature indicating
(a) whether the word was part of the gene lex-
icon, (b) if not whether it was part of the pro-
tein lexicon, (c) if not whether it was part of
the species lexicon, (d) and if not, whenever the
candidate was neither a noun, an adjective nor
a verb, we replaced it by its part-of-speech.
For example, the word hairless is associated
with the features given in Table 3, when en-
countered in the following sentence: Inhibition
of the DNA-binding activity of Drosophila sup-
pressor of hairless and of its human homolog,
KBF2/RBP-J kappa, by direct protein{protein
interaction with Drosophila hairless. The word
hairless appears in the gene lexicon and is
wrongly recognized as an adjective by our tag-
ger.
4
The word human, the fourth word of
the right context of hairless, belongs to the
species lexicon, ans is thus replaced by the fea-
ture RC SPECIES. Neither Drosophila nor sup-
pressor belong to the specialized lexicons we
use, and, since they are both tagged as nouns,
they are left unchanged. Prepositions and con-
junctions are replaced by their part-of-speech,
and prexes LC and RC indicate whether they
were found in left or right context. Note that
since two prepositions appear in the left context
of hairless, the value of the LC PREP feature
is 2.
Altogether, this amounts to a total of 3690
possible features in the input space X .
3
Using these lexicons alone, the same task with the
same test data, yields: precision = 22%, recall = 76%.
4
Note that no adaptaion work has been conducted on
our tagger, which explains this error.
Feature Value
LexGENE 1
ADJ 1
LC drosophila 1
LC suppressor 1
LC PREP 2
RC CONJ 1
RC SPECIES 1
RC PRON 1
RC PREP 1
Table 3: Features of hairless in \...of Drosophila
suppressor of hairless and of its human...".
6.2 Results
In our experiments, we have used the following
methods:
 SVM trained with inductive inference, and
using a linear kernel, a polynomial kernel of
degree d = 2 and the so-called \radial ba-
sis function" kernel (Scholkopf and Smola,
2002).
 SVM trained with transductive inference,
and using a linear kernel or a polynomial
kernel of degree d = 2.
 SVM trained with inductive inference us-
ing Fisher kernels estimated from the whole
training data (without using labels), with
dierent number of classes c in the PLSA
model (4).
The proportion of labelled data is indicated
in the tables of results. For SVM with induc-
tive inference, only the labelled portion is used.
For transductive SVM (TSVM), the remaining,
unlabelled portion is used (without the labels).
For the Fisher kernels (FK), an unsupervised
model is estimated on the full dataset using
PLSA, and a SVM is trained with inductive
inference on the labelled data only, using the
Fisher kernel as similarity measure.
6.3 Transductive inference
Table 4 gives interesting insight into the ef-
fect of transductive inference. As expected, in
the limit where little unannotated data is used
(100% in the table), there is little to gain from
using transductive inference. Accordingly, per-
formance is roughly equivalent
5
for SVM and
% annotated: 1.5% 6% 24% 100%
SVM (lin) 41.22 45.34 49.67 62.97
SVM (d=2) 40.97 46.78 52.12 62.69
SVM (rbf) 42.51 49.53 51.11 63.96
TSVM (lin) 38.63 51.64 61.84 62.91
TSVM (d=2) 43.88 52.38 55.36 62.72
Table 4: F
1
scores(in %) using dierent propor-
tions of annotated data for the following models:
SVM with inductive inference (SVM) and lin-
ear (lin) kernel, second degree polynomial ker-
nel (d=2), and RBF kernel (rbf); SVM with
transductive inference (TSVM) and linear (lin)
kernel or second degree polynomial (d=2) ker-
nel.
TSVM, with a slight advantage for RBF kernel
trained with inductive inference. Interestingly,
in the other limit, ie when very little annotated
data is used, transductive inference does not
seem to yield a marked improvement over in-
ductive learning. This nding seems somehow
at odds with the results reported by Joachims
(1999) on a dierent task (text categorisation).
We interpret this result as a side-eect of the
search strategy, where one tries to optimise
both the size of the margin and the labelling
of the unannotated examples. In practice, an
exact optimisation over this labelling is imprac-
tical, and when a large amount of unlabelled
data is used, there is a risk that the approxi-
mate, sub-optimal search strategy described by
Joachims (1999) may fail to yield a solution that
is markedly better that the result of inductive
inference.
For the two intermediate situation, however,
transductive inference seems to provide a size-
able performance improvement. Using only 24%
of annotated data, transductive learning is able
to train a linear kernel SVM that yields approxi-
mately the same performance as inductive infer-
ence on the full annotated dataset. This means
that we get comparable performance using only
what corresponds to about 30 abstracts, com-
pared to the 122 of the full training set.
6.4 Fisher kernels
The situation is somewhat dierent for SVM
trained with inductive inference, but using
5
Performance is not strictly equivalent because SVM
and TSVM use the data dierently when optimising the
trade-o parameter C over a validation set.
% annotated: 1.5% 6% 24% 100%
SVM (lin) 41.22 45.34 49.67 62.97
SVM (d=2) 40.97 46.78 52.12 62.69
lin+FK8 46.08 42.83 54.59 63.92
lin+FK16 44.43 40.92 55.70 63.76
lin+combi 46.38 38.10 52.74 63.08
Table 5: F
1
scores(in %) using dierent propor-
tions of annotated data for the following mod-
els: standard SVM with linear (lin) and second
degree polynomial kernel (d=2); Combination
of linear kernel and Fisher kernel obtained from
a PLSA with 4 classes (lin+FK4) or 8 classes
(lin+FK8), and combination of linear and all
Fisher kernels obtained from PLSA using 4, 8,
12 and 16 classes (lin+combi).
Fisher kernels obtained from a model of the
entire (non-annotated) dataset. As the use
of Fisher kernels alone was unable to consis-
tently achieve acceptable results, the similarity
we used is a combination of the standard lin-
ear kernel and the Fisher kernel (a similar solu-
tion was advocate by Hofmann (2000)). Table 5
summarises the results obtained using several
types of Fisher kernels, depending on how many
classes were used in PLSA. FK8 (resp. FK16)
indicates the model using 8 (resp. 16) classes,
while combi is a combination of the Fisher ker-
nels obtained using 4, 8, 12 and 16 classes.
The eect of Fisher kernels is not as clear-cut
as that of transductive inference. For fully an-
notated data, we obtain results that are similar
to the standard kernels, although often better
than the linear kernel. Results obtained using
1.5% and 6% annotated data seem somewhat in-
consistent, whith a large improvement for 1.5%,
but a marked degradation for 6%, suggesting
that in that case, adding labels actually hurts
performance. We conjecture that this may be
an artifact of the specic annotated set we se-
lected. For 24% annotated data, the Fisher ker-
nel provides results that are inbetween induc-
tive and transductive inference using standard
kernels.
7 Discussion
The results of our experiments are encouraging
in that they suggest that both transductive in-
ference and the use of Fisher kernels are poten-
tially eective way of taking unannotated data
into account to improve performance.
These experimental results suggest the follow-
ing remark. Note that Fisher kernels can be
implemented by a simple scalar product (lin-
ear kernel) between Fisher scores r`(x) (equa-
tion 3). The question arises naturally as to
whether using non-linear kernels may improve
results. One one hand, Fisher kernels are
derived from information-geometric arguments
(Jaakkola and Haussler, 1999) which require
that the kernel reduces to an inner-product of
Fisher scores. On the other hand, polynomial
and RBF kernels often display better perfor-
mance than a simple dot-product. In order to
test this, we have performed experiments using
the same features as in section 6.4, but with a
second degree polynomial kernel. Overall, re-
sults are consistently worse than before, which
suggest that the expression of the Fisher kernel
as the inner product of Fisher scores is theoret-
ically well-founded and empirically justied.
Among possible future work, let us mention
the following technical points:
1. Optimising the weight of the contributions
of the linear kernel and Fisher kernel, eg
as K(x; y) =  hx; yi + (1   )FK(x; y),
 2 [0; 1].
2. Understanding why the Fisher kernel alone
(ie without interpolation with the linear
kernel) is unable to provide a performance
boost, despite attractive theoretical prop-
erties.
In addition, the performance improvement
obtained by both transductive inference and
Fisher kernels suggest to use both in cunjunc-
tion. To our knowledge, the question of whether
this would allow to \bootstrap" the unlabelled
data by using them twice (once for estimating
the kernel, once in transductive learning) is still
an open research question.
Finally, regarding the application that we
have targeted, namely entity recognition, the
use of additional unlabelled data may help us
to overcome the current performance limit on
our database. None of the additional experi-
ments conducted internally using probabilisitc
models and symbolic, rule-based methods have
been able to yield F
1
scores higher than 63-64%
on the same data. In order to improve on this,
we have collected several hundred additional
abstracts by querying the MedLine database.
After pre-processing, this yields more than a
hundred thousand (unlabelled) candidates that
we may use with transductive inference and/or
Fisher kernels.
8 Conclusion
In this paper, we presented a comparison be-
tween two state-of-the-art methods to combine
labelled and unlabelled data: Fisher kernels and
transductive inference. Our experimental re-
sults suggest that both method are able to yield
a sizeable improvement in performance. For ex-
ample transductive learning yields performance
similar to inductive learning with only about a
quarter of the data. These results are very en-
couraging for tasks where annotation is costly
while unannotated data is easy to obtain, like
our task of biological entity recognition. In ad-
dition, it provides a way to benet from the
availability of large electronic databases in or-
der to automatically extract knowledge.
9 Acknowledgement
We thank Anne Schiller,

Agnes Sandor and Vi-
olaine Pillet for help with the data and re-
lated experimental results. This research was
supported by the European Commission un-
der the KerMIT project no. IST-2001-25431
and the French Ministry of Research under the
BioMIRE project, grant 00S0356.
References
M. E. Cali, editor. 1999. Proc. AAAI Work-
shop on Machine Learning for Information
Extraction. AAAI Press.
M. Craven and J. Kumlien. 1999. Construct-
ing biological knowledge bases by extract-
ing information from text sources. In Proc.
ISMB'99.
A. Gammerman, V. Vovk, and V. Vapnik. 1998.
Learning by transduction. In Cooper and
Morla, eds, Proc. Uncertainty in Articial In-
telligence, pages 145{155.Morgan Kaufmann.
Eric Gaussier, Cyril Goutte, Kris Popat, and
Francine Chen. 2002. A hierarchical model
for clustering and categorising documents.
In Crestani, Girolami, and van Rijsbergen,
eds, Advances in Information Retrieval|
Proc. ECIR'02, pages 229{247. Springer.
Thomas Hofmann. 1999. Probabilistic latent
semantic analysis. In Proc. Uncertainty in
Articial Intelligence, pages 289{296.Morgan
Kaufmann.
Thomas Hofmann. 2000. Learning the similar-
ity of documents: An information-geometric
approach to document retrieval and catego-
rization. In NIPS*12, page 914. MIT Press.
Tommi S. Jaakkola and David Haussler. 1999.
Exploiting generative models in discrimina-
tive classiers. In NIPS*11, pages 487{493.
MIT Press.
Thorsten Joachims. 1999. Transductive in-
ference for text classication using support
vector machine. In Bratko and Dzeroski,
eds, Proc. ICML'99, pages 200{209. Morgan
Kaufmann.
M. Krauthammer, A. Rzhetsky, P. Morozov,
and C. Friedman. 2000. Using blast for iden-
tifying gene and protein names in journal ar-
ticles. Gene.
Y. Ohta, Y. Yamamoto, T. Okazaki,
I. Uchiyama, and T. Takagi. 1997. Au-
tomatic constructing of knowledge base from
biological papers. In Proc. ISMB'97.
D. Proux, F. Reichemann, and L. Julliard.
2000. A pragmatic information extraction
strategy for gathering data on genetic inter-
actions. In Proc. ISMB'00.
Bernhard Scholkopf and Alexander J. Smola.
2002. Learning with Kernels. MIT Press.
J. Thomas, D. Milward, C. Ouzounis, S. Pul-
man, and M. Caroll. 2000. Automatic ex-
traction of protein interactions from scientic
abstracts. In Proc. PSB 2000.
Kristina Toutanova, Francine Chen, Kris Popat,
and Thomas Hofmann. 2001. Text classica-
tion in a hierarchical mixture model for small
training sets. In Proc. ACM Conf. Informa-
tion and Knowledge Management.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley.
Yiming Yang and Xin Liu. 1999. A re-
examination of text categorization methods.
In Proc. 22nd ACM SIGIR, pages 42{49.
