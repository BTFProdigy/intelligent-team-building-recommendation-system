An Evaluation Corpus For Temporal Summarization
Vikash Khandelwal, Rahul Gupta, and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
fvikas,rgupta,allang@cs.umass.edu
ABSTRACT
In recent years, a lot of work has been done in the eld of
Topic Tracking. The focus of this work has been on iden-
tifying stories belonging to the same topic. This might re-
sult in a very large number of stories being reported to the
user. It might be more useful to a user if a summary of the
main events in the topic rather than the entire collection of
stories related to the topic were presented. Though work
on such a ne-grained level has been started, there is cur-
rently no standard evaluation testbed available to measure
the accuracy of such techniques. We describe a scheme for
developing a testbed of user judgments which can be used
to evaluate the above mentioned techniques. The corpus
that we have created can also be used to evaluate single or
multi-document summaries.
1. THE PROBLEM
In recent years, a lot of progress has been made in the
eld of Topic Tracking ([2], [3], [8], etc). The focus of this
work has been on identifying news stories belonging to the
same topic. This might result in a very large number of
stories being reported to the user. It would be more useful
to a user if a summary of the main events/developments in
the topic rather than the entire collection of stories related
to the topic were presented. We can formulate the problem
as follows.
We are given a stream of chronologically ordered and top-
ically related stories. We strive to identify the shifts in the
topic which represent the developments within the topic.
For example, consider the topic \2000 Presidential Elec-
tions". On the night of November 7, there were reports of
Gore conceding defeat to Bush. The next morning, there
were reports claiming his retraction of the previous conces-
sion. Most of the stories on the next day would also contain
old information including details of Gore's rst phone call
to Bush. We want to present only the new development
(i.e., Gore's retraction) on the next day.
.
We assume that sentence extracts can identify such topic
shifts. At the very least, they can convey enough informa-
tion to a user to keep track of the developments within that
topic. For example, in Figure 1, the mappings indicate how
the sentences in a story correspond to events.
Human judgments are required to evaluate accuracy of
extracts. The approach usually taken is to have each such
extract evaluated by human beings but such a process is ex-
pensive and time consuming. We need an evaluation corpus
similar to the TDT or TREC corpora that can be used over
and over again to do such evaluations automatically. We
propose a new scheme for building such a corpus.
Summarization evaluation is di?cult because summaries
can be created for a range of purposes. The Tipster SUM-
MAC evaluation [7] required human assessors to evaluate
each summary, and most other evaluations have also re-
quired human checking of every summary [6]. There are
others who have attempted automatic evaluations ([5], [9])
but none of these evaluation schemes captures all the desir-
able properties in a summary.
The particular problem of summarizing shifts in a news
topic was attacked slightly dierently at a Summer 1999
workshop on Novelty Detection [4]. Those eorts towards
\new information detection" were a dead end because the
granularity of new information was too small, e.g., a men-
tion of a person's age might count as new information even
when it is not the focus of the story. Swan and Allan also
created an event-level summary \timeline" ([10], [11]) but
they did not develop any evaluation corpus for their work.
This paper is organised as follows. In Section 2, we dis-
cuss the desirable properties of such an evaluation corpus.
Section 3 discusses the entire annotation process, as well
as the interesting practical issues, the problems faced and
then the statistics of the corpus that we have built. Finally,
in Section 4, we discuss one possible way of utilising this
corpus.
2. DESIRABLE PROPERTIES OF THE
EVALUATION CORPUS
Any evaluation corpus of sentence extracts and events
which is to be used for the purpose of evaluating summaries
of topic shifts in a news stream should have the following
properties:
 It should be possible to identify all new events on a
periodic basis. This would be required to estimate the
recall of a system.
The Navy has ordered the discharge of sailor Timothy Mcveigh 
after he described himself gay in his America online user profile.
Civil rights groups and gay campaigners are outraged.
Mcveigh, who?s no relation of the convicted Oklahoma bomber,
is Lodging an appeal.
Paul Miller has more.
Timothy R. Mcveigh put "gay" in the marital status part of an aol
For "the world," I am Paul Miller in Washington.
user profile.
He did not use his full name or say he was in the Navy, referring
to himself only as "Tim of Honolulu".
The Navy?s personnel department says that?s violation of the 
Clinton administration?s Don?t ask/Don?t tell policy of Homo-
sexuals in the military and Mcveigh has been dismissed.
Many people are upset that the Navy asked aol for information
about the supposedly anonymous user and, a Naval investigat-
or says, the online service provided it.
Gay rights groups say it?s discrimination.
Privacy advocates say it?s a breach of confidentiality.
      
Sailor Mcveigh dis-
charged from Navy
Navy claims that he 
violated "Don?t ask/
Don?t tell" policy.
Discrimination against
                gays.
Breach of privacy by
            Navy.
           event
Not related to any
Figure 1: An example showing how sentence extracts can indicate events
 It should be possible to quantify the precision of a
summary, i.e., it should be possible to nd the pro-
portion of relevant sentences in the summary,
 It should be possible to identify redundancy in the
system output being evaluated. There should be some
way of assigning a marginal utility to sentences con-
taining relevant but redundant information
 It should be possible to quantify the \usefulness" of
a summary taking recall, precision as well as redun-
dancy into account.
 Sentence boundaries should be uniquely identied
(though they need not be perfect) because the aim
of the system is to identify the relevant portions in
the summary.
3. BUILDING AN EVALUATION CORPUS
3.1 The annotation process
We collect a stream of stories related to a certain topic
from the TDT-2 corpus of stories from January 1 to June
30 1998. We used stories that were judged \on-topic" by
annotators from LDC. The topics were selected from the
TDT 1998 and 1999 evaluations. The stories are parsed to
obtain sentence boundaries and all the sentences are given
unique identiers. We proceed with collecting the human
judgments in the following four steps.
1. Each judge reads all the stories and identies the im-
portant events.
2. The judges sit together to merge the events identied
by them, to form a single list of events for that topic.
All the events are given unique identiers.
3. Each judge goes through the stories again, connecting
sentences to the relevant events. Obviously, not all
sentences need to be related to any event. However,
if some sentence is relevant to more than one event, it
is linked to all those events.
4. Another judge now veries the mapping between the
sentences and the events. This gives us the nal map-
ping from sentences to events.
This way we obtain all the events mentioned within a
story and we can also nd out the events which nd their
rst mention within this story. The advantage of building
the evaluation corpus in this way is that these judgments
can be used both for summarizing topic shifts as well as
summarizing any given story by itself.
We have built a user interface in Java to allow judges to do
the above work systematically. Figure 2 shows a snapshot
of the interface used by the judges.
3.2 Statistics of the judgments obtained
We have obtained judgments for 22 topics. Three judges
worked on each topic. We summarize the results of the
annotation process for a subset of the topics in Table 1.
We dene the interjudge agreement for an event to be the
ratio of the number of sentences linked to that event, as
agreed upon by the third judge, to the number of sentences
in the union of the sentences individually marked by the
rst two judges for that event. For a topic, the interjudge
agreement is dened to be the average of the agreement for
all the events in that topic. It is to be noted that the Kappa
statistic is not applicable here in any standard form.
We found a large variance in the number of sentences
linked to dierent events. As an example, in Table 2, we
show the statistics for a group of news stories describing
Figure 2: A snapshot of the user interface used for annotating the topics
Topic id # of # of Time taken Inter-judge
stories events (in hours) Agreement
20008 49 10 4.5 0.91
20020 34 23 4.5 0.98
20021 48 9 2.5 0.97
20022 27 10 3.5 0.85
20024 38 12 2.75 0.98
20026 68 11 2.5 0.87
20031 34 15 2.5 0.62
20041 24 11 2 0.94
20042 28 14 2.5 0.66
20057 19 9 2 0.66
20065 57 16 2.33 0.94
20074 51 13 3 0.96
Average 39.75 12.75 2.88 0.86
Table 1: Annotation statistics for some of the topics
the damage due to tornados in Florida. We see that event 5
(\Relief agencies needed more than $300,000 to provide re-
lief") is linked to 4 sentences while event 1 (\At least 40 peo-
ple died in Florida due to 10-15 tornados.") is linked to 43
sentences. We may be able to use the number of sentences
linked to a event as an indicator of the weight/importance
of the event.
We have divided our corpus into two parts - one each for
training and testing respectively. Each part consists of 11
topics. Care was taken to ensure that both the parts had
topics of roughly the same size and time of occurrence. The
statistics of both parts of the corpus are given in Table 3.
3.3 Problems faced
 Sometimes our sentence parser broke up a valid sen-
tence into multiple parts. One judge linked only the
Event id # of Inter-judge
sentences Agreement
1 43 1.0
2 9 1.0
3 33 0.97
4 8 1.0
5 4 0.8
6 5 1.0
7 14 1.0
8 19 1.0
9 9 1.0
Table 2: Variance in the number of sentences linked
to dierent events for topic 20021
relevant part of the sentence to the corresponding
event while another linked all the parts to that event.
This happened in the case of three of the topics (top-
ics 20031, 20042 and 20057) before we detected the
problem.
 Sometimes when similar sentences occur in dierent
stories, one of the judges neglected the later occur-
rences of the sentence.
3.4 Interesting issues/judges? comments
We asked the judges for feedback on the annotation pro-
cess and the di?culties faced. Here are some of the inter-
esting issues which cropped up :
 Some ideas/events cannot be covered by any single
sentence but only by a group of sentences. By them-
selves, none of the sentences might be relevant to the
event. For example, Suppose, the event is The Navy
and AOL contradict each other and we have two sen-
tences - \the navy has said in sworn testimony that
Training Test All
Number of topics 11 11 22
Number of stories 474 470 944
per topic 43.1 42.7 42.9
Number of events 162 181 343
per topic 14.7 16.5 15.6
Number of sentences 8043 9006 17049
per topic 731.2 818.7 775.0
per story 17.0 19.2 18.1
O-event sentences 72% 70% 71%
Single-event sentences 24% 26% 25%
Multi-event sentences 4% 4% 4%
Table 3: Characteristics of the corpus. All numbers
except for the number of topics are averaged over
all topics included in that column.
this did happen." and \america online is saying this
never happened." Clearly, any one sentence does not
adequately represent the event. This can be easily
taken care of by considering groups of sentences rather
than single sentences.
 Abstract ideas : Sometimes the meaning of individ-
ual sentences is totally dierent from overall idea they
convey. Satirical articles are an example of this. These
kind of ideas cannot be represented by sentence ex-
tracts. We omitted such events.
 Sometimes dierent stories totally contradict each
other. For example, some stories (on the same day)
claim a lead for Bush while others claim Gore to be far
ahead. This is more of a summarization issue though
and need not be dealt with while building the evalua-
tion corpus.
4. USING THE EVALUATION CORPUS
We have used the corpus for evaluating our system which
produces temporal summaries in news stream ([1]). The
problem of temporal summarization can be formalized as
follows. A news topic is made up of a set of events and
is discussed in a sequence of news stories. Most sentences
of the news stories discuss one or more of the events in the
topic. Some sentences are not germane to any of the events.
Those sentences are called \o-event" sentences and con-
trast with \on-event" sentences. The task of the system is
to assign a score to every sentence that indicates the impor-
tance of the sentence in the summary. This scoring yields
a ranking on all sentences in the topic, including o- and
on-event sentences.
We will use measures that are analogues of recall and
precision. We are interested in multiple properties:
 Useful sentences are those that have the potential to
be a meaningful part of the summary. O-event sen-
tences are not useful, but all other sentences are.
 Novel sentences are those that are not redundant|
i.e., are new in the presentation. The rst sentence
about an event is clearly novel, but all following sen-
tences discussing the same event are not.
Figure 3: nu-recall vs nu-precision plot for the task
of summarizing topic shifts in a news stream
 Size of the summary is a typical measure used in sum-
marization research and we include it here.
Based on those properties, we could dene the following
measure to capture the combination of usefulness and nov-
elty:
nu   recall =
P
I(r(e
i
) > 0)
E
nu  precision =
P
I(r(e
i
) > 0)
S
r
where S
r
is the number of sentences retrieved, E is the
number of events in the topic, e
i
is event number i (1  i 
E), r(e
i
) is the number of sentences retrieved for event e
i
,
I(exp) is 1 if exp is true and 0 if not. All summations are
as i ranges over the set of events. Note that S
r
6=
P
r(e
i
)
since completely o-topic sentences might be retrieved.
The nu-recall measure is the proportion of the events that
have been mentioned in the summary, and nu-precision is
the proportion of sentences retrieved that are the rst men-
tions of an event.
We used this measure to evaluate the performance of
our system over the entire training corpus. The results for
the training corpus are shown in the nu-recall/nu-precision
graph in gure 3. This work is described in detail else-
where([1]).
This is just one of the possible ways of using the corpus.
We can dene a number of other similar measures which
could be easily computed using the data provided by such a
corpus. These same measures can also be used to evaluate a
system producing single or multi-document summaries too.
5. FUTURE WORK
We intend to complete collecting user judgments for more
topics soon. After analyzing the reliablity of these judg-
ments and correcting the few mistakes that we had made
initially, we will collect annotations for more topics. Ini-
tially, we had used a simple barebones sentence parser, since
that is mostly su?cient for the work such a corpus would be
put to. Nevertheless, in future annotations, we will need to
improve the sentence parser. We intend to continue using
these judgments to evaluate the performance of the systems
that we are currently building to identify and summarize
topic shifts in news streams.
Acknowledgements
This material is based on work supported in part by the
Library of Congress and Department of Commerce under
cooperative agreement number EEC-9209623 and in part
by SPAWARSYSCEN-SD grant number N66001-99-1-8912.
Any opinions, ndings and conclusions or recommendations
expressed in this material are the author(s) and do not nec-
essarily reect those of the sponsor.
6. REFERENCES
[1] J. Allan, R. Gupta, and V. Khandelwal. Temporal
Summaries of News Topics. Proceedings of SIGIR
2001 Conference, New Orleans, LA., 2001.
[2] J. Allan, V. Lavrenko, D. Frey, and V. Khandelwal.
UMASS at TDT2000. TDT 2000 Workshop notebook,
2000.
[3] J. Allan, R. Papka, and V. Lavrenko. On-line New
Event Detection and Tracking. Proceedings of SIGIR
1998, pp. 37-45, 1998.
[4] J. Allan et al Topic-based novelty detection. 1999
Summer Workshop at CLSP Final Report. Available
at http://www.clsp.jhu.edu/ws99/tdt, 1999.
[5] J. Goldstein, M. Kantrowitz, V. Mittal, and
J. Carbonell. Summarizing text documents: Sentence
Selection and Evaluation Metrics. Proceedings of
SIGIR 1999, August 1999.
[6] H. Jing, R. Barzilay, K. McKeown, and M. Elhadad.
Summarization Evaluation Methods: Experiments
and Analysis. Working notes, AAAI Spring
Symposium on Intelligent Text Summarization,
Stanford, CA, April, 1998.
[7] Inderjeet Mani and et al The TIPSTER SUMMAC
Text Summarization Evaluation Final Report. 1998.
[8] R. Papka, J. Allan, and V. Lavrenko. UMASS
Approaches to Detection and Tracking at TDT2.
Proceedings of the DARPA Broadcast News
Workshop, Herndon,VA, pp. 111-125, 1999.
[9] D. R. Radev, H. Jing, and M. Budzikowska.
Summarization of multiple documents: clustering,
sentence extraction, and evaluation. ANLP/NAACL
Workshop on Summarization, Seattle, WA, 2000.
[10] R. Swan and J. Allan. Extracting Signicant Time
Varying Features from Text. Proceedings of the Eighth
International Conference on Information and
Knowledge Management, pp.38-45, 1999.
[11] R. Swan and J. Allan. Automatic Generation of
Overview Timelines. Proceedings of SIGIR 2000
Conference, Athens, pp.49-56, 2000.
Monitoring the News: a TDT demonstration system
David Frey, Rahul Gupta, Vikas Khandelwal,
Victor Lavrenko, Anton Leuski, and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
ABSTRACT
We describe a demonstration system built upon Topic Detection
and Tracking (TDT) technology. The demonstration system moni-
tors a stream of news stories, organizes them into clusters that rep-
resent topics, presents the clusters to a user, and visually describes
the changes that occur in those clusters over time. A user may also
mark certain clusters as interesting, so that they can be ?tracked?
more easily.
1. TDT BACKGROUND
The Topic Detection and Tracking (TDT) research program in-
vestigates methods for organizing an arriving stream of news sto-
ries by the topics the stories discuss.[1, 4, 7, 8] Topics are de?ned
to be the set of stories that follow from some seminal event in the
world?this is in contrast to a broader subject-based notion of topic.
That is, stories about a particular airline crash fall into one topic,
and stories from other airline crashes will be in their own topics.
All organization is done as stories arrive, though variations of
the task allow ?nal organizational decisions to be postponed for
minutes, hours, or even days. The formal TDT evaluation program
includes the following research tasks:
1. Segmentation is used to separate a television or radio pro-
gram into distinct news stories. This process is not needed
for newswire services, since those stories arrive pre-segmented.
2. Detection is the task of putting all arriving news stories into
bins that represent broad news topics. If a new topic appears
in the news, the system must create a new bin. Neither the
set of bins nor the total number of them is known in advance.
This task is carried out without any supervision?i.e., the
system never knows whether or not the stories it is putting
together actually belong together.
3. Tracking is the task of ?nding all stories that follow are on
the same topic as an initial small set. This task is different
from detection in that the starting stories are known to be on
the same topic. Typically tracking is evaluated with 2-4 on-
topic stories.
.
The TDT research workshops also include a few other tasks (?rst
story detection, and story link detection). TDT has also inspired
other event-based organization methods, including automatic time-
line generation to visualize the temporal locality of topics[10], and
the identi?cation of new information within a topic?s discussion[3].
This demonstration system illustrates event-based news organi-
zation by visualizing the creation of, changes within, and relation-
ships between clusters created by the detection task. It leverages
the segmentation results so that audio stories are distinct stories,
but does not directly visualize the detection. Tracking is implicity
presented by allowing clusters to be marked so that they receive
special attention by the user.
2. ARCHITECTURE
The TDT demonstration system is based upon Lighthouse, an
interactive information retrieval system developed by Leuski.[6]
Lighthouse provides not only a typical ranked list search result, but
a visualization of inter-document similarities in 2- or 3-dimensions.
The user interface is a Java client that can run as an application or
an applet. Lighthouse uses http protocols to send queries to a server
and receive the ranked list, summary information about the docu-
ments, and the visualization data.
The TDTLighthouse system requires a TDT system running in
the background. In this version of the demonstration, the TDT sys-
tem is only running the segmentation and detection tasks described
above. Stories arrive and are put into clusters (bins).
The TDTLighthouse client can query its server to receive up-to-
date information about the clusters that the TDT system has found.
The server in turn queries the TDT system to get that information
and maintains state information so that changes (cluster growth,
additional clusters, etc.) can be highlighted.
3. DEMONSTRATION DATA
The data for this demonstration was taken from the our TDT
2000 evaluation output on the TDT cluster detection task [8]. The
sytem is running on the TDT-3 evaluation collection of news arti-
cles, approximately 40,000 news stories spanning October 1 through
December 31, 1998.
We simulated incremental arrival of the data as follows. At the
end of each day in the collection, we looked at the incremental
output of the TDT detection system. At this point, every story has
been classi?ed into a cluster. Every story seen to date is in one of
the clusters for that day, even if the cluster has the same contents as
it did yesterday.
The demonstration is designed to support text summarization
tools that could help a user understand the content of the cluster.
For our purposes, each cluster was analyzed to construct the fol-
lowing information:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 1: TDT demonstration system running on TDT-3 data, approximately four weeks into the collection.
1. The title was generated by selecting the 10 most commonly
occurring non-stopwords throughout the cluster. A better ti-
tle would probably be the headline of the most ?representa-
tive? news story, though this is an open research question.
2. The summary was generated by selecting the ?ve sentences
that were most representative of the entire cluster. Better ap-
proaches might generate a summary from the multiple doc-
uments [9] or summarize the changes from the previous day
[5, 2].
3. The contents of the cluster is just a list of every story in the
cluster, presented in reverse chronological order. Various
alternative presentations are possible, including leveraging
the multimedia (radio and television) that is the basis for the
TDT data.
The demonstration system was setup so that it could move from
between the days. All of the input to the client was generated au-
tomatically, but we saved the information so that it could be shown
more quickly. It typically takes a few minutes to generate all of the
presentation information for a single day?s clusters.
4. DEMONSTRATION SYSTEM
Figure 1 shows the client window. This snapshot shows the sys-
tem on October 31 at 10:00pm, approximately four weeks into the
data. The status line on the lower-left shows that at this point the
system has already encountered almost 16,000 stories and has bro-
ken them into about 2400 topic clusters.
The system is showing the 50 topics with the largest number of
stories. The ranked list (by size) starts on the upper-left, shows the
?rst 25, and the continues in the upper-right. The ?title? for each
of those topics is generated in this case by the most common words
within the cluster. Any system that does a better job of building
a title for a large cluster of stories could be used to improve this
capability.
In addition to the ranked list of topics, the system computes inter-
topic similarities and depicts that using the spheres in the middle.
If two topics are highly similar, their spheres will appear near each
other in the visualization. This allows related topics to be detected
quickly. Because the 50 largest topics are shown, the topics are
more unalike than they would be with a wider range, but it is still
possible to see, for example, that topics about the Clinton pres-
idency are near each other (the cyan pair of spheres overlapping
rank number 9, topic rank numbers 5 and 29). The spheres and the
ranked list are tightly integrated, so selecting one causes the other
to be highlighted.
Topics can be assigned colors to make them easier to pick out in
future sessions. In this case, the user has chosen to use the same
color for a range of related topics?e.g., red for sports topics, green
for weather topics, etc. The color selection is in the control of
the user and is not done automatically. However, once a color is
assigned to a topic, the color is ?sticky? for future sessions. A user
might choose to color a critical topic bright red so that changes to
it stand out in the future.
Figure 2 shows the same visualization, but here a summary of
a selected topic is shown in a pop-up balloon. This summary was
generated by selecting sentences that contained large numbers of
key concepts from the topic. Any summarization of a cluster could
be used here if it provided more useful information.
To illustrate how the demonstration system shows changes in
TDT clusters over time, Figure 3 shows an updated visualization
for two weeks later (November 14, 1998). The topic colors are
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 2: Similar to Figure 1, but showing a pop-up balloon.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 4: A 3-dimensional version of Figure 3.
persistent from Figure 1, though one of the marked topics (?Straw-
berry cancer colon Yankee?) is no longer in the largest 50 so does
not appear.
Most of the spheres include a small ?wedge? of yellow in them.
That indicates the proportion of the topic that is new stories (since
Figure 1). Some topics have large numbers of new stories, so have
a large yellow slice, whereas a few have a very small number of
new stories, so have only a thin wedge. The yellow wedge can be
as much as 50% of the sphere (which would represent an entirely
new topic), and only covers the top of the sphere. This restriction
ensures that the topic color is still visible.
The controls at the top of the screen are for moving between
queries, issuing a query, and returning the visualization to a ?home?
point. The next ?ve controls affect the layout of the display, includ-
ing allowing a 3-D display: a 3-D version of Figure 3 is shown in
Figure 4. The ?nal control enables a browsing wizard that can be
used to ?nd additional topics that are very similar to a selected topic
color (that set is chosen using the pull-down menu that has ?none?
in it).
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 3: TDT demonstration system running on TDT-3 data, approximately six weeks into the collection.
5. CONCLUSION AND FUTURE WORK
The demonstration system described above illustrates the effect
of TDT technology. It is also interesting in its own right, allow-
ing a user to track news topics of interest and to see how changes
occur over time. There is no reason that the same system could
not be used for non-TDT environments: any setting that clusters
documents might be appropriate for this system.
We are working to extend the demonstration system to include
some additional features.
? Considering the large number of topics (almost 3,000 in Fig-
ure 3), it is unlikely that all ?interesting? topics will be ?nd-
able. The query box at the top of the display will be used to
allow the user to ?nd topics that match a request. The ranked
list will display the top 50 topics that match the query.
? Related to querying, we hope to include an ?alert? feature
that will ?ag newly-created topics that match a query. For
example, an analyst interested in the Middle East might de-
velop a query that would identify topics in that region. When
such a topic appeared, it would be ?agged for the user (prob-
ably with a ?hot topic? color).
? We hope to allow user ?correction? of the topic breakdown
provided by the TDT system. The state-of-the-art in TDT
still makes mistakes, sometimes pulling two similar topics
together, and sometimes breaking a single topic into multiple
clusters. We intend that a user who sees such a mistake be
able to indicate it to the system. That information will, in
turn, to be relayed back to the TDT system to affect future
processing.
? We will be implementing an ?explode this topic? feature that
will show the stories within a topic analogously to the way
the current system shows the topics within the news. If the
topic is small enough, for example, the spheres would repre-
sent stories within the topic. If the topic is larger, the spheres
might represnt sub-clusters within the topic.
Acknowledgments
This material is based on work supported in part by the Library of
Congress and Department of Commerce under cooperative agree-
ment number EEC-9209623, and in part by SPAWARSYSCEN-SD
contract number N66001-99-1-8912. Any opinions, ?ndings and
conclusions or recommendations expressed in this material are the
authors? and do not necessarily re?ect those of the sponsor.
6. REFERENCES
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. Topic detection and tracking pilot study: Final
report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, pages 194?218,
1998.
[2] J. Allan, R. Gupta, and K. Khandelwal. Temporal summaries
of news topics. Technical Report IR-226, University of
Massachusetts, CIIR, 2001.
[3] J. Allan, H. Jin, M. Rajman, C. Wayne, D. Gildea,
V. Lavrenko, R. Hoberman, and D. Caputo. Topic-based
novelty detection: 1999 summer workshop at CLSP, ?nal
report. Available at http://www.clsp.jhu.edu/ws99/tdt, 1999.
[4] DARPA, editor. Proceedings of the DARPA Broadcast news
Workshop, Herndon, Virginia, February 1999.
[5] V. Khandelwal, R. Gupta, and J. Allan. An evaluation
scheme for summarizing topic shifts in news streams. In
Notebook proceedings of HLT 2001, 2001.
[6] A. Leuski and J. Allan. Lighthouse: Showing the way to
relevant information. In Proceedings of the IEEE Symposium
on Information Visualization (InfoVis), pages 125?130, 2000.
[7] NIST. Proceedings of the TDT 1999 workshop. Notebook
publication for participants only, March 2000.
[8] NIST. Proceedings of the TDT 2000 workshop. Notebook
publication for participants only, November 2000.
[9] D. R. Radev, H. Jing, and M. Budzikowska. Summarization
of multiple documents: clustering, sentence extraction, an d
evaluation. ANLP/NAACL Workshop on Summarization,
Seattle, WA, 2000.
[10] Russell Swan and James Allan. Automatic generation of
overview timelines. In Proceedings of SIGIR, pages 49?56,
Athens, Greece, 2000. ACM.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 121?128, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Names and Topics for New Event Detection
Giridhar Kumaran and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{giridhar,allan}@cs.umass.edu
Abstract
New Event Detection (NED) involves
monitoring chronologically-ordered news
streams to automatically detect the stories
that report on new events. We compare
two stories by finding three cosine simi-
larities based on names, topics and the full
text. These additional comparisons sug-
gest treating the NED problem as a bi-
nary classification problem with the com-
parison scores serving as features. The
classifier models we learned show statis-
tically significant improvement over the
baseline vector space model system on all
the collections we tested, including the lat-
est TDT5 collection.
The presence of automatic speech recog-
nizer (ASR) output of broadcast news in
news streams can reduce performance and
render our named entity recognition based
approaches ineffective. We provide a so-
lution to this problem achieving statisti-
cally significant improvements.
1 Introduction
The instant and automatic detection of new events
is very useful in situations where novel informa-
tion needs to be detected from a real-time stream
of rapidly growing data. These real-life situations
occur in scenarios like financial markets, news anal-
yses, and intelligence gathering. In this paper we
focus on creating a system to immediately identify
stories reporting new events in a stream of news
- a daunting task for a human analyst given the
enormous volume of data coming in from various
sources.
The Topic Detection and Tracking (TDT) pro-
gram, a DARPA funded initiative, seeks to develop
technologies that search, organize and structure mul-
tilingual news-oriented textual materials from a va-
riety of broadcast news media. One of the tasks in
this program, New Event Detection (NED), involves
constant monitoring of streams of news stories to
identify the first story reporting topics of interest.
A topic is defined as ?a seminal event or activity,
along with directly related events and activities? (Al-
lan, 2002). An earthquake at a particular place is
an example of a topic. The first story on this topic
is the story that first carries the report on the earth-
quake?s occurrence. The other stories that make up
the topic are those discussing the death toll, the res-
cue efforts, the reactions from different parts of the
world, scientific discussions, the commercial impact
and so on. A good NED system would be one that
correctly identifies the article that reports the earth-
quake?s occurrence as the first story.
NED is a hard problem. For example, to dis-
tinguish stories about earthquakes in two different
places, a vector space model system would rely on a
tf-idf weighting scheme that will bring out the dif-
ference by weighting the locations higher. More
often then not, this doesn?t happen as the differ-
ences are buried in the mass of terms in common
between stories describing earthquakes and their af-
termath. In this paper we reduce the dependence on
tf-idf weighting by showing the utility of creating
121
three distinct representations of each story based on
named entities. This allows us to view NED as a bi-
nary classification problem - i.e., each story has to
be classified into one of two categories - old or new,
based on features extracted using the three different
representations.
The paper starts by summarizing the previous
work on NED in Section 2. In Section 3, we explain
the rationale behind our intuition. Section 4 de-
scribes the experimental setup, data pre-processing,
and our baseline NED system. We then briefly de-
scribe the evaluation methodology for NED in Sec-
tion 5. Model creation and the results of applying
these models to test data are detailed in Section 6.
In the same section, we describe the effect on perfor-
mance if the manually transcribed version of broad-
cast news is replaced with ASR output. Since its
hard to recognize named entities from ASR data,
performance expectedly deteriorates. We follow a
novel approach to work around the problem result-
ing in statistically significant improvement in per-
formance. The results are analyzed in Section 7. We
wrap up with conclusions and future work in Sec-
tion 8.
2 Previous Research
Previous approaches to NED have concentrated on
developing similarity metrics or better document
representations or both. A summer workshop on
topic-based novelty detection held at Johns Hop-
kins University extensively studied the NED prob-
lem. Similarity metrics, effect of named entities,
pre-processing of data, and language and Hidden
Markov Models were explored (Allan et al, 1999).
Combinations of NED systems were also discussed.
In the context of this paper, selective re-weighting of
named entities didn?t bring about expected improve-
ment.
Improving NED by better comparison of stories
was the focus of following papers. In an approach
to solve on-line NED, when a new document was
encountered it was processed immediately to ex-
tract features and build up a query representation
of the document?s content (Papka and Allan, 1998).
The document?s initial threshold was determined by
evaluating it with the query. If the document did not
trigger any previous query by exceeding this partic-
ular threshold, it was marked as a new event. Un-
like the previous paper, good improvements on TDT
benchmarks were shown by extending a basic in-
cremental TF-IDF model to include source-specific
models, similarity score normalization techniques,
and segmentation of documents (Brants et al, 2003).
Other researchers have attempted to build better
document models. A combination of evidence de-
rived from two distinct representations of a docu-
ment?s content was used to create a new representa-
tion for each story (Stokes and Carthy, 2001). While
one of the representations was the usual free text
vector, the other made use of lexical chains (created
using WordNet) to obtain the most prevalent topics
discussed in the document. The two vectors were
combined in a linear fashion and a marginal increase
in effectiveness was observed.
NED approaches that rely on exploiting existing
news tracking technology were proved to inevitably
exhibit poor performance (Allan et al, 2000). Given
tracking error rates, the lower and upper bounds
on NED error rates were derived mathematically.
These values were found to be good approximations
of the true NED system error rates. Since track-
ing and filtering using full-text similarity compar-
ison approaches were not likely to make the sort
of improvements that are necessary for high-quality
NED results, the paper concluded that an alternate
approach to NED was required. This led to a se-
ries of research efforts that concentrated on building
multi-stage NED algorithms and new ways to com-
bine evidence from different sources.
In the topic-conditioned novelty detection ap-
proach, documents were classified into broad top-
ics and NED was performed within these categories
(Yang et al, 2002). Additionally, named entities
were re-weighted relative to the normal words for
each topic, and a stop list was created for each topic.
The experiments were done on a corpus different
from the TDT corpus and, apparently didn?t scale
well to the TDT setting.
The DOREMI research group treated named enti-
ties like people and locations preferentially and de-
veloped a new similarity measure that utilized the
semantics classes they came up with (Makkonen et
al., 2002). They explored various definitions of the
NED task and tested their system accordingly. More
recently, they utilized a perceptron to learn a weight
122
function on the similarities between different seman-
tic classes to obtain a final confidence score for each
story (Makkonen et al, 2004).
The TDT group at UMass introduced multiple
document models for each news story and modified
similarity metrics by splitting up stories into only
named entities and only terms other than named en-
tities (Kumaran and Allan, 2004). They observed
that certain categories of news were better tackled
using only named entities, while using only topic
terms for the others helped.
In approaches similar to named entity tagging,
part-of-speech tagging (Farahat et al, 2003) has also
been successfully used to improve NED.
Papers in the TDT2003 and TDT2004 work-
shops validated the hypothesis that ensemble single-
feature classifiers based on majority voting exhibited
better performance than single classifiers working
with a number of features on the NED task (Braun
and Kaneshiro, 2003; Braun and Kaneshiro, 2004).
Examples of features they used are cosine similarity,
text tiling output and temporally-weighted tf-idf.
Probabilistic models for online clustering of doc-
uments, with a mechanism for handling creation of
new clusters have been developed. Each cluster was
assumed to correspond to a topic. Experimental re-
sults did not show any improvement over baseline
systems (Zhang et al, 2005).
3 Features for NED
Pinning down the character of new stories is a tough
process. New events don?t follow any periodic cy-
cle, can occur at any instant, can involve only one
particular type of named entity (people, places, or-
ganizations etc.) or a combination, can be reported
in any language, and can be reported as a story of
any length by any source1 . Apart from the source,
date, and time of publication or broadcast of each
news story, the TDT corpora do not contain any
other clues like placement in the webpage, the num-
ber of sources reporting the same news and so on.
Given all these factors, we decided that the best fea-
1It could be argued that articles from a source, say NYTimes,
are much longer than news stories from CNN, and hence the
length of stories is a good candidate for use as a feature. How-
ever, when there is no pattern that indicates that either of the
two sources reports new stories preferentially, the use of length
as a feature is moot.
tures to use would be those that were not particular
to the story in question only, but those that measure
differences between the story and those it is com-
pared with.
Category-specific rules that modified the baseline
confidence score assigned to each story have been
developed (Kumaran and Allan, 2004). The mod-
ification was based on additional evidence in the
form of overlap of named entities and topic terms
(terms in the document not identified as named en-
tities) with the closest story reported by a base-
line system. We decided to use these three scores:
namely the baseline confidence score, named en-
tity overlap, and topic-term overlap as features. The
named entities considered were Event, Geopolitical
Entity, Language, Location, Nationality, Organiza-
tion, Person, Cardinal, Ordinal, Date, and Time.
These named entities were detected in stories using
BBN IdentiFinderTM(Bikel et al, 1999). Irrespec-
tive of their type, all named entities were pooled to-
gether to form a single named entity vector.
The intuition behind using these features is that
we believe every event is characterized by a set of
people, places, organizations, etc. (named entities),
and a set of terms that describe the event. While
the former can be described as the who, where, and
when aspects of an event, the latter relates to the
what aspect. If two stories were on the same topic,
they would share both named entities as well as topic
terms. If they were on different, but similar, topics,
then either named entities or topic terms will match
but not both.
We illustrate the above intuition with examples.
Terms in bold face are named entities common to
both stories, while those in italics are topic terms
in common. We start with an example showing
that for old stories both the named entities as well
as topic terms overlap with a story on the same topic.
Story 1. : Story on a topic already reported
While in Croatia today, Pope John Paul II called
on the international community to help end the
fighting in the Yugoslavia?s Kosovo province.
Story 2. : Story on the same topic
Pope John Paul II is urging the international
community to quickly help the ethnic Albanians in
Kosovo. He spoke in the coastal city of Split, where
he ended a three-day visit to Croatia.
123
Story 1 is an old story about Pope John Paul II?s
visit to Yugoslavia. Story 2 was the first story on the
topic and it shares both named entities likes Pope
John Paul II and Croatia and also topic terms like
international community and help.
Our next example shows that for new stories,
either the named entities or topic terms match with
an earlier story.
Story 3. : Topic not seen before
Turkey has sent 10,000 troops to its southern border
with Syria amid growing tensions between the two
neighbors, newspapers reported Thursday. Defense
Minister Ismet Sezgin denied any troop movement
along the border, but said Turkey?s patience was
running out. Turkey accuses Syria of harboring
Turkish Kurdish rebels fighting for autonomy in
Turkey?s southeast; it says rebel leader Abdullah
Ocalan lives in Damascus.
Story 4. : Closest Story due to Named Entities
A senior Turkish government official called Mon-
day for closer military cooperation with neighboring
Bulgaria. After talks with President Petar Stoyanov
at the end of his four-day visit, Turkish Deputy Pre-
mier and National Defense Minister Ismet Sezgin
expressed satisfaction with the progress of bilateral
relations and the hope that Bulgarian-Turkish
military cooperation will be promoted.
Story 3 is a new story about the rising ten-
sions between Turkey and Syria. The closest story
as reported by a (baseline) basic vector space model
NED system using cosine similarity is Story 4,
a story about Turkish-Bulgarian relations. The
named entities Turkey and Ismet Sezgin caused
this match. We see that none of the topic terms
match. However, the system reported with a high
confidence score that Story 3 is old. This is because
of the matching of high IDF-valued named entities.
Determining that the topic terms didn?t match would
have helped the system avoid this mistake.
4 Experimental Setup and Baseline
We used the TDT2, TDT3, TDT4, and TDT5 cor-
pora for our experiments. They contain a mix of
broadcast news (bn) and newswire (nwt) stories.
Only the English stories in the multi-lingual collec-
tions were considered for the NED task. The broad-
cast news material is provided in the form of an au-
dio sampled data signal, a manual transcription of
the audio signal (bn-man), and a transcription cre-
ated using an automatic speech recognizer (bn-asr).
We used version 3.0 of the open source Lemur
system2 to tokenize the data, remove stop words,
stem and create document vectors. We used the 418
stopwords included in the stop list used by InQuery
(Callan et al, 1992), and the Krovetz-stemmer algo-
rithm implementation provided as part of Lemur.
Documents were represented as term vectors with
incremental TF-IDF weighting (Brants et al, 2003;
Yang et al, 1998). We used the cosine similarity
metric to judge the similarity of a story S with those
seen in the past.
Sim(S, X) =
?
w weight(w, S) ? weight(w, X)
?
?
w weight(w,S)
2
?
?
w weight(w, X)
2
(1)
where
weight(w, d) =tf ? idf
tf =log(termfrequency + 1.0)
idf = log((docCount+1)(documentfreq+0.5)
The maximum similarity of the story S with stories
seen in the past was taken as the confidence score
that S was old. This constituted our baseline system.
We extracted three features for each incoming
story S. The first was the confidence score reported
by the baseline system. The second and third fea-
tures were the cosine similarity between only the
named entities in S and X and the cosine similarity
between only the topic terms in S and X. We trained
a Support Vector Machine (SVM) (Burges, 1998)
classifier on these features. We chose to use SVMs
as they are considered state-of-the-art for text clas-
sification purposes (Mladeni et al, 2004), and pro-
vide us with options to consider both linear and non-
linear decision boundaries. To develop SVM models
we used SV MLight(Joachims, 1999), which is an
implementation of SVMs in C. SV MLight is an im-
plementation of Vapnik?s Support Vector Machine
(Vapnik, 1995).
For training, we used the TDT3 and TDT4 cor-
pora. There were 115 and 70 topics respectively giv-
ing us a total of 185 positive examples (new stories)
2http://www.lemurproject.org
124
and 7800 negative examples (old stories). We bal-
anced the number of positive and negative examples
by oversampling the minority class until there were
equal number of positive and negative training in-
stances. Testing was done on the TDT2 and TDT5
corpora (96 and 126 topics resp.).
5 NED Evaluation
The official TDT evaluation requires a NED system
to assign a confidence score between 0 (new) and
1 (old) to every story upon its arrival in the time-
ordered news stream. This assignment of scores
is done either immediately upon arrival or after a
fixed look-ahead window of stories. To evaluate per-
formance, the stories are sorted according to their
scores, and a threshold sweep is performed. All
stories with scores above the threshold are declared
old, while those below it are considered new. At
each threshold value, the misses and false alarms are
identified, and a cost Cdet is calculated as follows.
Cdet = Cmiss ?Pmiss ?Ptarget + CFA ? PFA ? Pnon?target
where CMiss and CFA are the costs of a Miss
and a False Alarm, respectively, PMiss and PFA
are the conditional probabilities of a Miss and a
False Alarm, respectively, and Ptarget and Pnon?target
are the a priori target probabilities (Pnon?target = 1 -
Ptarget).
The threshold that results in the least cost is se-
lected as the optimum one. Different NED systems
are compared based on their minimum cost. In other
words, the lower the Cdet score reported by a system
on test data, the better the system.
6 Results
Our first set of experiments were performed on data
consisting of newswire text and manual transcrip-
tion of broadcast news (nwt+bn-man). We used
the features mentioned in Section 3 to build SVM
models in the classification mode. We experimented
with linear, polynomial, and RBF kernels. The out-
put from the SVM classifiers was normalized to fall
within the range zero and one.
We found that using certain kernels improved per-
formance over the baseline system significantly. The
results for both corpora, TDT2 and TDT5, were
consistently and significantly improved by using the
TDT2 TDT5
Kernel Type (nwt+bn-man) (nwt)
Baseline System 0.585 0.701
Linear Kernel 0.548 0.696
Poly. of deg. 1 0.548 0.696
Poly. of deg. 2 0.543 0.688
Poly. of deg. 3 0.545 0.684
Poly. of deg. 4 0.535 0.694
Poly. of deg. 5 0.533 0.688
Poly. of deg. 6 0.534 0.693
RBF with ? = 1 0.540 0.661
RBF with ? = 5 0.530 0.699
Table 1: Summary of the results of using SVM classifier mod-
els for NED on the TDT2 and TDT5 collections. The numbers
are the minimum cost (Cdet) values (lower is better). The sign
test, with ? = 0.05, was performed to compare the baseline sys-
tem with only a classifier using RBF kernels with ? = 1. For
both collections, the improvements were found to be statisti-
cally significant (shown in bold). While there are better per-
forming kernels for TDT2, we chose to perform significance
tests for only one kernel to show that significant improvement
over the baseline can be obtained using a single kernel across
different test collections.
classification models. The 2004 NED evaluations
conducted by the National Institute of Standards and
Technology was on the TDT5 collection. The large
size of the collection and existence of a large num-
ber of topics with a single story made the task very
challenging. The best system fielded by the partici-
pating sites was the baseline system used here. Ta-
ble 1 summarizes the results we obtained.
All statistical significance testing was done using
the sign test. We counted the number of topics for
which using the SVM classifier improved over the
baseline (in terms of detecting more previously un-
detected new and old stories), and also the num-
ber of topics for which using the SVM classifier
actually converted originally correct decisions into
wrong ones. These were used as input for the sign
test. The test were used to check whether improve-
ment in performance using the classifier-based sys-
tem was spread across a significant number of top-
ics, and not confined to a few. Table 2 gives some
examples of topics and the associated improvements
in detecting them.
125
Topic ID Number of Num. detected Num. detected Improvement
old stories by baseline system by SVM classifier (Higher the better)
55105 420 407 403 -4
55010 21 21 20 -1
55023 5 5 4 -1
55089 226 226 225 -1
55125 120 114 120 6
55107 331 327 331 7
55106 808 787 795 8
55200 196 185 193 8
Table 2: Examples of improvements due to using the SVM classifier on a per-topic basis. Shown here are the
four topics each in which the greatest degradation and improvements in performance were seen. The topics
vary in size. The SVM classifier resulted in overall (statistically significant, refer Table 1) improvement as
it corrected more errors than introduced them.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
20
25
30
35
Confidence Score
N
um
be
r o
f S
to
rie
s
TDT5 ? New Scores
Baseline New Story Scores
Classifier New Story Scores
Figure 1: Distribution of new story scores for the
baseline and SVM model systems.
7 Analysis
The main goal of our effort was to come up with a
way to correctly identify new stories based on fea-
tures we thought characterized them. To understand
what we had actually achieved by using these mod-
els, we studied the distribution of the confidence
scores assigned to new and old stories for the base-
line and a classifier-based NED system for the TDT5
collection (Figures 1 and 2 respectively).
We observe that the scores for a small fraction
of new stories that were initially missed (between
scores 0.8 and 1) are decreased by the model-based
NED system while a larger fraction (between scores
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
500
1000
1500
2000
2500
3000
Confidence Score
N
um
be
r o
f S
to
rie
s
TDT5 ? Old Stories
Baseline Old Story Scores
Classifier Old Story Scores
Figure 2: Distribution of old story scores for the
baseline and SVM model systems.
0.1 and 0.4) is also decreased by a small amount.
However, the major impact of using SVM model-
based NED systems appears to be in detecting old
stories. We observe that the scores of a significant
number of old stories (between scores 0.2 and 0.55)
have been increased to be closer to one. This had the
effect of increasing the score difference between old
and new stories, and hence improved classification
accuracy as measured by the minimum cost.
We investigated the relative importance of the
three features by looking that the linear kernel SVM
model. While the original cosine similarity metric
CS remained the prominent feature, the contribution
126
of the third feature non-NE-CS was slightly more
than if not equal to the contribution of named en-
tities NE-CS (Table 3). This explains why simple
re-weighting of named entities alone (Allan et al,
1999) doesn?t suffice to improved performance.
Feature CS NE-CS non-NE-CS
Weight 5.4 1.58 1.83
Table 3: Weights assigned to features by the linear
kernel SVM.
If this method of harnessing named entities and
topic terms were indeed so effective, then we should
have been able to detect every old story in every
topic. However, analysis reveals that this approach
makes an assumption about the way stories in a
topic are related. Not all topics are dense, with both
named entities and topic terms threading the stories
together. Examples of such topics are natural dis-
aster topics. While the first story might report on
the actual calamity and the region it affected, suc-
cessive stories might report on individual survivor
tales. These stories might be connected to the orig-
inal story of the topic by as tenuous a link as only
the name of the calamity, or the place. Such topic
structures are very common in newswire. Hence our
approach will fail in such topics with loosely con-
nected stories. Much more advanced processing of
story content is required in such cases. Mistakes
made by the named entity recognizer also impede
performance.
Given that its impractical to expect manual tran-
scriptions of all broadcast news, we tested our base-
line and classifier systems on a version of TDT2
with newswire stories and ASR output of the broad-
cast news (nwt+bn-asr). TDT5 was left out as it
doesn?t have any broadcast news. As shown in Ta-
ble 4, the baseline system performed significantly
worse when manual transcription was replaced with
ASR output. The classifier systems did even worse
than the nwt +bn-asr baseline result. An analysis
of the named entities extracted revealed that the ac-
curacy was very poor - worse than extraction from
bn-man documents. This was primarily because the
version of IdentiFinder (IdentiFinder-man) we used
was by default trained on nwt.
To alleviate this problem we re-trained Identi-
Kernel Type TDT2 (nwt+bn-asr)
Baseline System 0.640
IdentiFinder-man IdentiFinder-asr
Linear Kernel 0.653 0.608
Poly. of deg. 1 0.654 0.608
Poly. of deg. 2 0.658 0.619
Poly. of deg. 3 0.659 0.616
Poly. of deg. 4 0.671 0.632
Poly. of deg. 5 0.676 0.640
Poly. of deg. 6 0.682 0.652
RBF with ? = 1 0.649 0.636
RBF with ? = 5 0.668 0.679
Table 4: The baseline system was the same used for
the nwt+bn-man collection. We find that using a lin-
ear kernel for the procedure using IdentiFinder-asr
to tag named entities results in statistically signifi-
cant improvement.
Finder using a simulated ASR corpus with named
entities identified correctly. Since the amount of
training data required was huge, we obtained the
training data from the bn-man version of TDT3.
We ran IdentiFinder-man on the bn-man version of
TDT3 and tagged the named entities. We then re-
moved punctuation and converted all the text to up-
percase to simulate ASR to a limited degree. We
re-trained IdentiFinder on this simulated ASR cor-
pus and used it to tag named entities in only the
bn-asr stories in TDT2. We retained the use of
IdentiFinder-man for the nwt stories. The same three
features were then extracted and we re-ran the classi-
fiers. The results are shown in Table 4 in the column
titled IdentiFinder-asr.
8 Conclusions and Future Work
We have shown the applicability of machine learn-
ing classification techniques to solve the NED prob-
lem. Significant improvements were made over the
baseline systems on all the corpora tested on. The
features we engineered made extensive use of named
entities, and reinforced the importance and need to
effectively harness their utility to solve problems in
TDT. NED requires not only detection and report-
ing of new events, but also suppression of stories
that report old events. From the study of the distri-
butions of scores assigned to stories by the baseline
127
and SVM model systems, we can see that we now do
a better job of detecting old stories (reducing false
alarms). Thus we believe that attacking the prob-
lem as ?old story detection? might be a better and
more fruitful approach. We have shown the effects
of ASR output in the news stream, and demonstrated
a procedure to alleviate the problem.
A classifier with RBF kernel with ? set to one ex-
hibited the best performance. The reason for this
superior performance over other kernels needs to be
investigated. Engineering of better features is also
a definite priority. In the future NED can also be
extended to other interesting domains like scientific
literature to detect the emerge of new topics and in-
terests.
Acknowledgements
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by NSF grant
#IIS-9907018, and in part by SPAWARSYSCEN-
SD grant number N66001-02-1-8903. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are the author(s) and do
not necessarily reflect those of the sponsor.
References
J. Allan, Hubert Jin, Martin Rajman, Charles Wayne, Daniel
Gildea, Victor Lavrenko, Rose Hoberman, and David Ca-
puto. 1999. Topic-based novelty detection. Technical re-
port, Center for Language and Speech Processing, Johns
Hopkins University. Summer Workshop Final Report.
J. Allan, Victor Lavrenko, and Hubert Jin. 2000. First story
detection in tdt is hard. In Proceedings of the Ninth Interna-
tional Conference on Information and Knowledge Manage-
ment, pages 374?381. ACM Press.
J. Allan. 2002. Topic Detection and Tracking: Event-Based
Information Organization. Kluwer Academic Publishers.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
Thorsten Brants, Francine Chen, and Ayman Farahat. 2003. A
system for new event detection. In Proceedings of the 26th
Annual International ACM SIGIR Conference, pages 330?
337, New York, NY, USA. ACM Press.
Ronald K. Braun and Ryan Kaneshiro. 2003. Exploiting topic
pragmatics for new event detection in tdt-2004. Techni-
cal report, National Institute of Standards and Technology.
Topic Detection and Tracking Workshop.
Ronald K. Braun and Ryan Kaneshiro. 2004. Exploiting topic
pragmatics for new event detection in tdt-2004. Techni-
cal report, National Institute of Standards and Technology.
Topic Detection and Tracking Workshop.
Christopher J. C. Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data Mining and Knowl-
edge Discovery, 2(2):121?167.
James P. Callan, W. Bruce Croft, and Stephen M. Harding.
1992. The INQUERY retrieval system. In Proceedings of
DEXA-92, 3rd International Conference on Database and
Expert Systems Applications, pages 78?83.
Ayman Farahat, Francine Chen, and Thorsten Brants. 2003.
Optimizing story link detection is not equivalent to optimiz-
ing new event detection. In ACL, pages 232?239.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge, MA,
USA.
Giridhar Kumaran and J. Allan. 2004. Text classification
and named entities for new event detection. In Proceedings
of the 27th Annual International ACM SIGIR Conference,
pages 297?304, New York, NY, USA. ACM Press.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2002. Applying semantic classes in event
detection and tracking. In Proceedings of International
Conference on Natural Language Processing (ICON 2002),
pages 175?183.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3?4):347?368.
Dunja Mladeni, Janez Brank, Marko Grobelnik, and Natasa
Milic-Frayling. 2004. Feature selection using linear classi-
fier weights: interaction with classification models. In Pro-
ceedings of the 27th Annual International ACM SIGIR Con-
ference, pages 234?241, New York, NY, USA. ACM Press.
R. Papka and J. Allan. 1998. On-line new event detection using
single pass clustering. Technical Report UM-CS-1998-021.
Nicola Stokes and Joe Carthy. 2001. Combining semantic and
syntactic document classifiers to improve first story detec-
tion. In Proceedings of the 24th Annual International ACM
SIGIR Conference, pages 424?425, New York, NY, USA.
ACM Press.
Vladimir N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998. A study
of retrospective and on-line event detection. In Proceedings
of the 21st Annual International ACM SIGIR Conference,
pages 28?36, New York, NY, USA. ACM Press.
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin.
2002. Topic-conditioned novelty detection. In Proceedings
of the 8th ACM SIGKDD International Conference, pages
688?693. ACM Press.
Jian Zhang, Zoubin Ghahramani, and Yiming Yang. 2005. A
probabilistic model for online document clustering with ap-
plication to novelty detection. In Lawrence K. Saul, Yair
Weiss, and Le?on Bottou, editors, Advances in Neural In-
formation Processing Systems 17, pages 1617?1624. MIT
Press, Cambridge, MA.
128
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 451?458, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Matching Inconsistently Spelled Names in Automatic Speech Recognizer
Output for Information Retrieval
Hema Raghavan and James Allan
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{hema,allan}@cs.umass.edu
Abstract
Many proper names are spelled inconsis-
tently in speech recognizer output, posing
a problem for applications where locating
mentions of named entities is critical. We
model the distortion in the spelling of a
name due to the speech recognizer as the
effect of a noisy channel. The models fol-
low the framework of the IBM translation
models. The model is trained using a par-
allel text of closed caption and automatic
speech recognition output. We also test a
string edit distance based method. The ef-
fectiveness of these models is evaluated on
a name query retrieval task. Our methods
result in a 60% improvement in F1. We
also demonstrate why the problem has not
been critical in TREC and TDT tasks.
1 Introduction
Proper names are key to our understanding of topics
in news. For example, to determine that a news
story is on the 2004 elections in the United States,
the words President Bush, John Kerry and USA
are necessary features of the story. In other words,
names of people, places and organizations are key
entities of a news story. For many tasks, like in
topic detection and tracking (TDT), the entities
form an important feature for distinguishing topics
from one another. For example, it is the people
that distinguish stories on the 2004 election from
stories on the 2000 U.S election. Names, especially
rare and foreign ones are a problem for automatic
speech recognition (ASR) systems as they are often
out of vocabulary (OOV) i.e., they do not exist in
the lexicon of the ASR system. An OOV word is
replaced with the most similar word in the lexicon
of the speech recognizer. Sometimes, even if a
name is in the lexicon of the speech recognizer, it
may have multiple spelling variants. The following
is a sample ASR snippet from the TDT3 1 corpus
that demonstrates how the same entity may have
different spellings even within the same snippet of
ASR text.
...newspaper quotes qaddafi is saying they?ll
turn them over but only if they?re allowed ..leader
moammar gadhafi says he doesn?t want an interna-
tional confrontation over the suspects in the..
In this work, we aim to find methods by which to
cluster or group names in ASR text. We evaluate
a variety of techniques that range from a simple
string-edit distance model to generative models
using both intrinsic and extrinsic evaluations. We
get statistically significant improvements in results
for ad-hoc retrieval when the query is just the name
of a person. We also explain why the problem
of misspelled proper names in ASR has not been
an issue in the TREC spoken document retrieval
(SDR) track or in topic detection and tracking
(TDT). We demonstrate how the problem would be
of significance when the query is short, containing
mainly names with little or no context.
1http:///www.ldc.upenn.edu/projects/tdt3/
451
2 Related Work
That names can be spelled differently is a prob-
lem that has been addressed by the database com-
munity in great detail. They found that the prob-
lem was rising in significance with the increasing
interest in reconciling different databases. Differ-
ences in names due to spelling errors, spelling vari-
ants and transliteration errors have been dealt with
by different kinds of approximate string matching
techniques like Soundex, Phonix, and String Edit
distance (James C. French, 1997; Zobel and Dart,
1996). The nature of the problem is identical when
the domain consists of databases of documents but
in order to apply techniques that were developed for
names by the database community one would have
to first detect names in the corpus, and then normal-
ize them to some canonical form. This is the ap-
proach taken by Raghavan and Allan (Raghavan and
Allan, 2004) who showed that normalizing names
using Soundex codes resulted in a 10% improvement
on the TDT3 Story Link Detection Task. They tested
their method on newswire stories only. Their diffi-
culty in applying Soundex to the ASR documents
was that detecting names in ASR is too error prone
for their methods to be useful (Miller et al, 2000).
Spoken document retrieval was a track at the
TREC-6,7 and 8 (Voorhees and Harman, 1997;
Voorhees and Harman, 1998; Voorhees and Harman,
1999) conferences. At the TREC-8 SDR track the
conclusion was that ASR is not really an issue for
ad hoc retrieval. However, the queries in those tracks
were not centered on any entity. The TREC-8 pro-
ceedings also acknowledge that mean average preci-
sion dropped as named entity word error rate (NE-
WER) increased. A typical speech recognizer has a
lexicon of about 60K and for this size of a lexicon,
about 10% of the person names are out of vocabu-
lary (OOV).
The problem of alternate spellings of names has
also been explored by the cross lingual information
retrieval community (Virga and Khudanpur, 2003;
AbdulJaleel and Larkey, 2003). The problem with
names in machine translated text is quite similar to
the problem with names in ASR text, except that the
errors caused by a speech recognizer are often pho-
netic confusions, which is not necessarily the case
for machine translation errors. Spelling errors of
names in machine translated text are typically con-
sistent. A given word in the source language always
translates to the same word in the target language for
a given machine translation system. As seen earlier,
ASR systems do not exhibit such consistency.
Another problem that resembles the one we are
addressing in this paper is that of spelling correc-
tion. Spelling correction has been tackled in several
different ways (Durham et al, 1983), in some cases
with the use of contextual cues (Golding and Roth,
1999) and in some cases it has been modeled as a
?noisy channel problem? (Kernighan et al, 1990).
The latter approach is similar to ours because we
also approach the problem of spelling variations due
to speech recognizer errors as analogous to the er-
rors caused by a noisy channel. However, spelling
correction methods must rectify human errors (ty-
pographic errors and common confusions) whereas
speech recognizer errors are different.
Additionally, the argument that Jon Smith and
John Smythe may genuinely be different people and
should not be considered to be the same entity is
more of a cross-document co-reference problem.
The problem we are attempting to solve in this
paper is one of grouping names that ?sound like?
each other together, without considering the prob-
lem of cross document co-reference. For example,
the name Lewinsky has 199 occurrences in the TDT3
corpus, and also appears as Lewinski (1324 times),
and Lewenskey (171 times). Most of these occur-
rences refer to Monica Lewinsky. The aim is to
group all these variants together, without taking into
consideration which ones refer to the same person.
We then measure the effectiveness of our methods
on various retrieval tasks.
Perhaps the most similar work from the point of
view of the task is work in word spotting in audio
output (Amir et al, 2001). The queries are single
words and the task is to locate their mention in au-
dio. The starting point in that work is however, a
phonetic transcript of the audio signal and the em-
phasis is not on locating names. Our starting point
is automatic speech recognizer output, and we aim
to locate names in particular.
3 Our Approaches
In this section we explain the techniques by which
we group names together. One method uses string
edit distance to group names that are variants of each
452
other. The other techniques are some of the possible
generative models suitable to this task.
An equivalence class is defined as a group of
names such that any two names in that class are vari-
ants of each other and such that there exist no two
names from different equivalence classes that are
variants of each other. An equivalence class is rep-
resented as a set of names enclosed in curly braces
as {name-1 name-2 ...}
Four of our models are trained on a parallel text
of ASR and manual transcripts (or closed caption
depending on availability) in order to learn a proba-
bilistic model of ASR errors. The parallel text con-
sists of pairs of sentences: sentences from the ASR
output and the corresponding manual transcripts.
This is a common technique in machine translation
for which the IBM translation models are popular
methods (Brown et al, 1993).
As a convention, we use uppercase letters to de-
note ASR output and lowercase for manual tran-
scriptions. Given an input of parallel text of ASR
and manual transcriptions, the model learns a prob-
abilistic dictionary. The dictionary contains pairs
of closed caption and ASR words and the probabil-
ity that the closed caption word is generated from a
given word in ASR. Thus, the model might learn a
high probability for P(CAT|kate).
3.1 Overview of Methods
We generate equivalence classes of names by clus-
tering a list of names. The algorithm draws links be-
tween pairs of words and then clusters the words into
equivalence classes such that if a and b are linked
and b and c are linked then a, b and c are in the same
equivalence class. Links between words are gener-
ated in five different ways described below.
In the first of our methods we align manual tran-
scripts and ASR sentences using the IBM transla-
tion model (Brown et al, 1993) to obtain a proba-
bilistic dictionary. We give details of the translation
model in section 3.2. Names are grouped such that
if P(CAT|kate) is high (above some threshold) then
there is a link between CAT and kate. This is called
the Simple Aligned method. Some sample pairs of
words obtained by this technique are shown in fig-
ure 1.
We can also ask a human to create a list of equiv-
alence classes of names. We describe our method
african AFRICA albania ALBANIAN
alex ALEC cardoso CARDOZO
ann ANNE ching CHIANG
Figure 1: Example of pairs of words obtained by
Simple Aligned
of obtaining such a list in section 4. This method is
called the Supervised method.
Given a list of equivalence classes, pairs of names
that go together can easily be generated such that for
each pair, both words are obtained from the same
equivalence class. In this way equivalence classes
of names obtained from the Simple Aligned and Su-
pervised methods can be used to create a list of pairs
of names that form parallel text to train a charac-
ter level machine translation model. We would ex-
pect this model to learn a high probability for simi-
lar sounding alphabets, e.g., a high probability for
P (C|k). Depending on where the training set of
pairs of names for this method comes from, we get
two possible systems. These are called the Gener-
ative Unsupervised method and Generative Super-
vised method respectively. Note that the Genera-
tive Unsupervised method is not completely unsu-
pervised; we still need the parallel text of ASR and
manual transcripts, but we don?t need a human to
do the added grouping of names into equivalence
classes. A character level translation model helps
us generalize better to unseen words.
We also grouped together names that differ by a
string edit distance of one, giving a fifth system. In
particular, we use the Levenshtein distance (Lev-
enshtein, 1966), that is the number of insertions,
deletions and substitutions needed to convert one
string to the other. Many methods employed by the
database community build on string edit distance.
The method works well but has some disadvantages.
Consider a user who types in a query containing a
name such that the spelling, as typed by the user,
never occurs in the corpus. To employ string edit
distance, one would have to compare the query name
against all the words in the vocabulary of the cor-
pus to find the most similar strings. With a gener-
ative model, only the query needs to be expanded
using the translation model, thereby speeding up the
search process. The string edit distance model on the
453
other hand, is completely unsupervised and needs no
training in the form of parallel text. Both methods
have their advantages and disadvantages, and the use
of one method over the other is situation dependent.
3.2 Details
To learn alignments, translation probabilities, etc in
the first method we used work that has been done in
statistical machine translation (Brown et al, 1993),
where the translation process is considered to be
equivalent to a corruption of the source language text
to the target language text due to a noisy channel.
We can similarly consider that an ASR system cor-
rupts the spelling of a name as a result of a noisy
channel. To obtain the closed caption word c, of an
ASR word a, we want to find the string for which
the probability P (c|a) is highest. This is modeled as
P (c|a) = P (c)P (a|c)P (a) (1)
For a given name a, since P (a) is constant, the
problem reduces to one of maximizing P (c)P (a|c).
P (c) is called the language model. We need
to model P (a|c) as opposed to directly modeling
P (c|a) so that our model assigns more probability
to well formed English names.
Given a pair of sentences (c, a), an alignment
A(c, a) is defined as the mapping from the words
in c to the words in a. If there are l closed caption
words and m ASR words, there are 2lm alignments
in A(c, a). l ? A(c, a) can be denoted as a series
lm1 = l1, l2...lm where lj = i means that a word in
position j of the ASR string is aligned with a word in
position i of the closed caption string. Then P (a|c)
is computed as follows:
P (a|c) =
?
l
P (a, l|c)
P (a, l|c) = P (m|c)
m
?
j
P (lj |lj?11 , a
j?1
1 , m, e)
?P (aj |lj1, a
j?1
1 , m, c) (2)
where aj is a word in position j of the string a, and
aj1 is the series a1...aj . The model is generative in
the following way: we first choose for each word in
the closed caption string the number of ASR words
that will be connected to it, then we pick the identity
of those ASR words and finally we pick the actual
positions that these words will occupy. There are
five different IBM translation models (Brown et al,
1993). Models 3 and 4 build on the above equations,
and also incorporate the notion of fertility. Fertility
takes into account that a given word in closed cap-
tion may be omitted by an ASR system, or one word
may result in two or more, like Iraq? I ROCK (This
is a true example). The models are trained using Ex-
pectation Maximization. Further details are in the
original paper (Brown et al, 1993).
The IBM models have shown good performance
in machine translation, and especially so within cer-
tain families of languages, for example in translating
between French and English or between Sinhalese
and Tamil (Brown et al, 1993; Weerasinghe, 2004).
Pairs of closed caption and ASR sentences or words
(as the case may be) are akin to a pair of closely re-
lated languages.
For the Generative Unsupervised and Generative
Supervised methods, we use the same models, but in
this case the training set consists of pairs of words
obtained from the ASR and closed caption text as
opposed to sentences. In other words, the place of
words in the previous case is taken by characters.
Modeling fertility, etc, again fits very well in this
case. For example the terminal character e is often
dropped in ASR, and a single o in closed caption
may result in a double o in ASR or vice versa.
4 Experimental Set Up
4.1 Corpora
For experiments in this paper we used the TREC-6
and TREC-7 SDR track data (Voorhees and Harman,
1998). We also used the TDT2 and TDT3 corpora.
For TREC-6 we had the ASR output provided by
NIST (WER 34%). The TREC-7 corpus consists of
the output of the Dragon systems speech recognizer
(WER 29.5%). For the TDT sources we had the
ASR output of the BBN Byblos Speech recognizer
provided by the LDC. NIST provides human gener-
ated transcripts for the TREC corpora and LDC pro-
vides closed caption quality transcripts with a WER
of 14.5% for the TDT corpora. There are 3943,
23282, 1819 and 2866 ASR documents in the TDT2
TDT3, TREC-6 and TREC-7 corpora respectively.
454
4.2 Intrinsic Evaluation
The Paice evaluation (Paice, 1996) for stemming al-
gorithms (algorithms that reduce a word to its mor-
phological root), attempts to compare the equiva-
lence classes generated by our methods with human
judgments.
The Paice evaluation measures the performance
of a stemmer based on its understemming and over-
stemming indices (UI and OI respectively). UI
measures the total number of missed links between
words and OI measures the total number of false
alarm links. A perfect stemmer would have a UI and
OI value of zero.
We obtained a list of names to be grouped into
equivalence classes in the following way. We did
not use a named entity tagger on the corpus because
named entity taggers typically have very high word
error rates for ASR text (Bikel et al, 1999). Instead
we ran the Unix spell command on the corpus and
used the list of rejected words as the list of names
for the annotators to group into equivalence classes.
These 296 OOV words are taken to correspond to
the names in the corpus. We then obtained the set of
ground-truth equivalence classes by a method simi-
lar to Paice.
A group of undergraduate students was hired. The
list of names was provided to each student in a text
editor in alphabetical order. The purpose as ex-
plained to them was to group together names that
were alternate spellings of similar sounding names
together. The student was instructed to go through
the list systematically, and for each word to look
at the previous 10 words, as well as the following
10 words to see if there were any other variants. If
there was a word or a group where the current word
was likely to fit in, they were asked to cut the word
and paste it into the appropriate group. In this way,
groups were created such that no word could belong
to more than one group. The annotators were also
asked to mark the words that were indeed names. Of
the 296 OOV words, 292 were found to be actual
names.
4.3 Extrinsic evaluation
In addition to the Paice evaluation we propose two
extrinsic or task based evaluations for our methods.
In the first task, given a name as a query, we aim to
Query Equivalence class
1: {christy christie}
2: {christina christine}
3: {toney toni}
4: {michelle michel mitchell}
5: {columbia colombia colombian}
Figure 2: Some sample query equivalence classes
find all documents that have a mention of that name
or any of its variants. In order to obtain queries
and relevance judgments for this task we arbitrar-
ily chose 35 groups of names from the ground-truth
set of equivalence classes. The TDT3 corpus was
chosen to be the test corpus for this task. Hence we
eliminated those words that had no occurrence in the
TDT3 corpus from the 35 groups of names giving a
total of 76 names. Each of the 76 words formed a
query. For each name query we consider all docu-
ments that contain a mention of any of the names in
the equivalence class of the query as relevant to that
query. In this way we obtained relevance judgments
for the name query task. Some sample queries are
shown in figure 2. We use F1 (harmonic mean of the
precision and recall) as a measure of performance.
Our extrinsic evaluation is spoken document re-
trieval. The queries on the TREC-6 and TREC-7
corpora are standard TREC spoken document re-
trieval track queries. For the TDT2 corpus we use
one randomly chosen document from each topic as
the query. This document is like a long query with
plenty of entities and plenty of contextual informa-
tion. For the TDT3 corpus we use the topic de-
scriptions as provided by the LDC as the queries.
The LDC topic descriptions discuss the events that
describe a topic and the key entities and locations
involved in the event. These are representative of
shorter queries, rich in entities. LDC has provided
relevance judgments for both the TDT2 and TDT3
corpora. Mean average precision was used as the
measure of evaluation.
4.4 Implementation Details
We use GIZA++ (Och and Ney, 2003) to train the
machine translation system and the ISI ReWrite
Decoder (ISI, 2001) to do the actual translations.
The decoder takes as input the models learned by
455
GIZA++ and a sentence from the foreign language.
It can output the top n translations of the input sen-
tence. The ReWrite decoder can translate using IBM
Model-3 or Model-4. We found Model 3 to have
lower perplexity and hence chose it for our experi-
ments. In order to build the language model P (c),
we used the CMU Language Modeling toolkit 2.
All retrieval experiments were performed using the
LEMUR 3 toolkit, and using the traditional vector
space model. In the traditional vector space model
queries and documents are represented as vectors of
words. Each word in the vector is weighted using
a product of term frequency and inverse document
frequency. The similarity between a query and a
document is measured using the cosine of the angle
between the query and document vectors.
The Simple Aligned and Generative Unsuper-
vised methods require a parallel corpus of ASR and
closed caption for training. For the name query task
we used TDT2, TREC-6 and TREC-7 to train these
methods and TDT3 as the test corpus.
The Supervised and Generative Supervised meth-
ods require a human to provide pairs of words that
are variants of each other. We filtered out those
words from the human generated list of equivalence
classes that occurred exclusively in the test corpus
and in no other corpus. This is equivalent to asking
a human to group words in the training corpus. Sim-
ilarly we trained the Simple Aligned and Generative
Unsupervised models using ASR and closed caption
text from all other sources except those in the test
set.
The models were trained similarly for the SDR
experiments. The models were tested on each of
the four corpora in turn, and in each case they were
trained on everything but the test corpus.
5 Results
5.1 Intrinsic Experiments
Table 1 shows how the different methods perform on
the intrinsic evaluation. We also show the UI and OI
values for methods that use string edit distances of
2, 3, 4 and 5. Note that the Supervised method is the
ground truth for this evaluation, and hence it has a UI
and OI value of zero. A string edit distance of 1 has
2http://mi.eng.cam.ac.uk/prc14/toolkit documentation.html
3http://www.cs.cmu.edu/lemur
Method UI OI
Simple Aligned 0.236 0.004
Supervised 0 0
Gen Sup 0.393 0.023
Gen Uns 0.351 0.003
Str. Ed. (1) 0.229 0.000
Str. Ed. (2) 0.083 0.003
Str. Ed. (3) 0.039 0.001
Str. Ed. (4) 0.031 0.124
Str. Ed. (5) 0.023 0.336
Table 1: Understemming and Overstemming indices
for each of the methods (lower is better)
the lowest OI value, meaning there are very few false
alarms. Higher string edit distances have lower UI
values, with an increase in OI. We will interpret the
UI and OI values again after observing performance
on the retrieval tasks, so as to interpret the impact of
missed links and false alarm links for retrieval.
5.2 Name Query Retrieval experiments
The results of our experiments on the name query
task are given in table 2. We report both Macro
and Micro averaged (averaged over the equivalence
classes of the queries) F1 measures. They do not dif-
fer much since the equivalence classes have almost
the same number (2-3) of names.
From table 2, all methods improve the baseline
F1 score significantly (statistical significance mea-
sured using a two tailed t-test with a confidence of
95%). In general, the Simple Aligned, Generative
Unsupervised and string edit distance methods are
the best performing for this task. The string edit
distance improves the baseline by over 60%. The
Supervised method is also not as good as the other
four of our methods as it does not generalize well to
names that occur exclusively in the test set.
String edit distance performs very well on cer-
tain equivalence classes of names. For example, on
the equivalence class {Seigal, Segal, Siegal, Siegel}
the precision and recall are 100% each since all of
the words in the equivalence class differ from each
other by a string edit distance of one. In the case of
the equivalence class {Lewenskey Lewinski Lewin-
sky}, the term Lewenskey has a string edit distance
of 2 (greater than one) from the other two members,
456
Method Micro avg Micro avg Micro Macro avg Macro avg Macro
Recall Precision F1 Recall Precision F1
Baseline 0.401 1 0.573 0.400 1 0.571
Simple Aligned 0.632 0.933 0.754 0.608 0.925 0.734
Sup 0.477 0.961 0.638 0.463 0.960 0.625
Gen Sup 0.530 0.937 0.677 0.517 0.938 0.667
Gen Uns 0.590 0.921 0.720 0.576 0.913 0.706
Str. Ed 0.752 0.867 0.806 0.751 0.871 0.807
Table 2: Results on the Name Query Retrieval task
Lewinsky and Lewinski. The equivalence class of
{John Jon Joan} has very low precision and recall.
This is because both John and Jon differ by a string
edit distance of one from so many other names in the
corpus, such as Jong, resulting in lowered precision.
The Simple Aligned method fails on names it has
not seen in the training set. However, for cases
like {Greensborough Greensboro} the link between
these two names is detected using the simple aligned
method and by no other. The generative methods can
detect variations in spelling due to similar sounding
alphabets. For example it can detect the link be-
tween Sydney and Sidney. The generative models
were also able to learn that c and k are substitutable
for each other. Therefore these models could detect
the links between the words in the equivalence class
{Katherine Kathryn Catherine}.
The Simple Aligned model performs well on the
extrinsic evaluations although it has a high OI value.
The intrinsic evaluations use judgments by humans.
The Simple Aligned method would conflate Kofi and
Copy into one class if that was a genuine ASR error
and the alignment was correct, but these two words
would not be conflated into the same equivalence
class by our annotators and would actually count
as a false alarm on the intrinsic evaluations. There-
fore, although the OI is high for the Simple Aligned
Method, on closer examination we found that some
of the false alarms were actually representative of
ASR errors.
5.3 Spoken Document Retrieval
We now move on to discuss results on the SDR task.
For TDT3 we got statistically significant improve-
ments (an improvement in mean average precision
from 0.715 to 0.757) over the baseline using string
edit distance. On the remaining corpora we got little
or no improvement by our methods. We proceed to
explain why this is the case for each of the corpora.
The TREC-7 corpus has only 5 queries with a
mention of a name resulting in hardly any gains
overall. Similar was the case for TREC-6. Again
in the case of the TDT2 corpus, since we used en-
tire documents as stories, there are enough words in
the query that a few recognition errors can be toler-
ated and therefore traditional retrieval is good for the
task. There is evidence from previous TREC tracks
(Voorhees and Harman, 1999) that shorter queries
result in a decrease in retrieval performance and
hence we see some improvements for TDT3. Be-
sides, the TDT3 queries were rich in names.
We wanted to check how our methods performed
on outputs of different ASR systems. Spoken doc-
ument retrieval on the TREC-7 data with the out-
put of Dragon systems, which has a word error rate
of 29.5%, results in an improvement of 6% using
the Simple Aligned method. The NIST-B2 system
with a higher WER (46.6%) has an improvement in
Mean Average Precision of 6.5%. Similarly with the
CUHTK (WER 35.6%) and NIST-B1 (WER 33.8%)
and Sheffield (WER 24.6 %) systems we obtained
improvements of 1.6%, 0.39% and 0.05% respec-
tively using the Simple Aligned method. Thus, with
increasing WER, the named entity word error rate
increases significantly, and therefore the benefits of
our method are more apparent in such situations.
6 Discussion and Conclusions
We showed (both intrinsically and extrinsically) that
string edit distance is an effective technique for lo-
cating name variants. We also developed a set of
generative models and showed that they are almost
457
as effective at name finding and document retrieval,
but are probably more efficient than string edit dis-
tance. The generative models need to be trained on
parallel text and therefore require human effort for
training the models. The advantage of one method
over the other is dependent on the size of the corpus
and the availability of resources.
The problem has not been of significance in previ-
ous TREC tasks or in TDT, because we have always
escaped the problem of misspelled names by virtue
of the nature of those tasks. In the TREC tasks very
few queries are centered on an entity. In all the TDT
tasks, one is usually required to compare entire sto-
ries with each other. A story is long enough that
there are enough words that are in the vocabulary
(just like a very long query) or that are correctly rec-
ognized, that the ASR errors do not really matter.
Therefore, the TDT tasks also do not suffer as a re-
sult of these ASR errors.
We can improve and apply our methods to other
domains like Switchboard data (Godfrey et al,
1992). Our methods also generalize well across lan-
guages since there are no language specific tech-
niques employed.
7 Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903. Any opinions, findings and conclusions or
recommendations expressed in this material are the
author(s) and do not necessarily reflect those of the
sponsor.
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for english-arabic cross language information
retrieval. In Proceedings of the 12th CIKM conference,
pages 139?146. ACM Press.
Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001. Ad-
vances in phonetic word spotting. In CIKM ?01: Proceed-
ings of the tenth international conference on Information and
knowledge management, pages 580?582, New York, NY,
USA. ACM Press.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
P. F. Brown, Steven A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Lingustics, 19(2):263?311.
Ivor Durham, David A. Lamb, and James B. Saxe. 1983.
Spelling correction in user interfaces. Commun. ACM,
26(10):764?773.
J. Godfrey, E. Holiman, and J. McDaniel. 1992. Switchboard:
Telephone speech corpus for research and development. In
Proceedings of the International Conference on Acoustics,
Speech and Signa Processing pp. I-517-520, 1992, pages
517?520.
Andrew R. Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Machine
Learning, 34(1-3):107?130.
2001. ISI rewrite decoder, http://www.isi.edu/licensed-
sw/rewrite-decoder/.
Allison L. Powell James C. French. 1997. Applications of ap-
proximate word matching in information retrieval. In Pro-
ceedings of the Sixth CIKM Conference.
Mark D. Kernighan, Kenneth W. Church, , and William A. Gale.
1990. A spelling correction program based on a noisy chan-
nel model. In Proceedings of COLING-90, pages 205?210.
V. I. Levenshtein. 1966. Binary codes capable of correcing
deletions,insertions and reversals. Phs. Dokl., 6:707?710.
David Miller, Richard Schwartz, Ralph Weischedel, and Re-
becca Stone. 2000. Named entity extraction from broadcast
news.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Chris D. Paice. 1996. Method for evaluation of stemming al-
gorithms based on error counting. JASIS, 47(8):632?649.
Hema Raghavan and James Allan. 2004. Using soundex codes
for indexing names in asr documents. In Proceedings of the
HLT NAACL Workshop on Interdisciplinary Approaches to
Speech Indexing and Retrieval.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of
proper names in cross-language applications. In Proceed-
ings of the 26th ACM SIGIR conference, pages 365?366.
ACM Press.
E. M. Voorhees and D. K. Harman, editors. 1997. The Sixth
Text REtrieval Conference (TREC 6). NIST.
E. M. Voorhees and D. K. Harman, editors. 1998. The Seventh
Text REtrieval Conference (TREC 7). NIST.
E. M. Voorhees and D. K. Harman, editors. 1999. The Eighth
Text REtrieval Conference (TREC 8). NIST.
Ruvan Weerasinghe. 2004. A statistical machine translation
approach to Sinhala Tamil language translation. In SCALLA
2004.
Justin Zobel and Philip W. Dart. 1996. Phonetic string match-
ing: Lessons from information retrieval. In Proceedings of
the 19th ACM SIGIR Conference,(Special Issue of the SIGIR
Forum), pages 166?172.
458
Cross-Document Coreference on a Large Scale Corpus 
 
 
 
Abstract 
     In this paper, we will compare and evaluate the 
effectiveness of different statistical methods in the 
task of cross-document coreference resolution. We 
created entity models for different test sets and 
compare the following disambiguation and 
clustering techniques to cluster the entity models in 
order to create coreference chains: 
Incremental Vector Space  
KL-Divergence 
Agglomerative Vector Space  
1. Introduction 
     Coreference analysis refers to the process of 
determining whether or not two mentions of entities 
refer to the same person (Kibble and Deemter, 2000).  
For example, consider the following short passage of 
text: 
 
John Smith was appointed chair of the committee.  
Because of his past experience, Mr. Smith seemed the 
perfect choice.  His good friend John, however, was not 
considered. 
 
     Coreference analysis attempts to decide whether 
John Smith and Mr. Smith refer to the same person, and 
whether John is also the same person.  The task is often 
extended to include references such as his or even his 
good friend, though we do not explore that extension in 
this study. 
     Addressing this problem is important to support 
systems such as those that search for, extract, and 
process mentions of ?people of interest? in news or 
transcripts (BBN 2001), or for other information 
organization tasks that might benefit from precise 
knowledge of how names occur, such as Topic 
Detection and Tracking (Allan 2002). 
     Cross-document coreference analysis pushes the task 
into considering whether mentions of a name in 
different documents are the same.  The problem 
becomes more complex because documents might come  
 
 
 
 
 
 
 
 
from different sources, will probably have different 
authors and different writing conventions and 
styles(Bagga and Baldwin,1998), and may even be in 
different languages.    
     There has been little published work on cross-
document coreference analysis and that has generally 
been evaluated on a small corpus of documents.  A 
major contribution of this work is to develop a 
substantially larger (more than two orders of magnitude) 
corpus for evaluation.  We show that the previous 
approach is effective but that a variation on it, 
agglomerative vector space, provides improved and 
much more stable results. 
    We begin in Section 2 by describing how cross-
document coreference analysis is evaluated.  We sketch 
prior work in Section 3 and describe our two evaluation 
corpora in Section 4.  Section 5 discusses the three 
algorithms that we explore for this task and then Section 
6 describes our experimental results on both corpora.  In 
Section 7 we provide some additional analysis that 
attempts to explain some surprising results.  We 
conclude in Section 8 with a description of our plans for 
future work. 
2. Evaluation 
     Given a collection of named entities from 
documents, the coreferencing task is to put them into 
equivalence classes, where every mention in the same 
class refers to the same entity (person, location, 
organization, and so on).  The classes are referred to as 
?coreference chains? because the entities are chained 
together. 
     To evaluate the coreference chains emitted by a 
system, we need truth data: the chains of entities that are 
actually referring to the same person.  Evaluation then 
proceeds by comparing the true chains to the system?s 
hypothesized chains. 
     We use the B-CUBED scoring algorithm (Bagga and 
Baldwin 1998) because it is the one used in the 
published research.    The algorithm works as follows. 
     For each entity mention e in the evaluation set, we 
first locate the truth chain TC that contains that mention 
(it can be in only one truth chain) and the system?s 
hypothesized chain HC that contains it (again, there can 
Chung Heong Gooi and James Allan 
Center for Intelligent Information Retrieval 
Department of Computer Science 
University of Massachusetts 
Amherst, MA 01003 
{cgooi,allan}@cs.umass.edu 
be only one hypothesis chain).   We then compute a 
precision and recall score for those two chains.  
Precision is the proportion of mentions in HC that are 
also in TC and recall is the proportion of mentions in TC 
that are also in HC.  If the chains match perfectly, recall 
and precision will both be one.  If the hypothesis chain 
contains only the single mention e, then its precision 
will be one, and its recall will be 1/|TC|, the inverse of 
the size of the truth chain.  Note that it is not possible to 
have a precision or recall of zero since entity e is always 
in common between the two chains. Our 
implementation of the B-CUBED algorithm is used 
specifically to evaluate an existing set of coreference 
chains and does not utilize any smoothing to handle 
system output which contains no entities.  
     Overall precision and recall values are determined by 
averaging the individual values over all mentions in the 
evaluation set.  These are the primary evaluation 
measures for cross-document coreference analysis. 
3. Related Work 
     TIPSTER Phase III first identified cross-document 
coreference as an area for research since it is a central 
tool to drive the process of producing summaries from 
multiple documents and for information fusion (Bagga 
and Baldwin, 1998). The Sixth Message Understanding 
Conference (MUC-6) identified cross-document 
coreference as a potential task but it was not included 
because it was considered to be too difficult (Bagga and 
Baldwin, 1998).  
     ISOQuest?s NetOwl and IBM?s Textract attempted 
to determine whether multiple named entities refer to 
the same entity but neither had the ability to distinguish 
different entities with the same name. Entity detection 
and tracking looks at the same tasks as cross document 
coreferencing.  
     Much of the work in this study is based on that by 
Bagga and Baldwin (1998), where they presented a 
successful cross-document coreference resolution 
algorithm to resolve ambiguities between people having 
the same name using the vector space model.  We have 
implemented a simplified version of their algorithm that 
achieves roughly equivalent accuracy, but will show 
that the algorithm does not work as well when translated 
to a substantially larger corpus of documents. 
     There has been significant work recently in the 
information extraction community on a problem known 
as Entity Detection and Tracking within the Automatic 
Content Extraction (ACE) evaluations (NIST 2003).  
That work includes an optional sub-task referred to 
alternately as either Entity Tracking or Entity Mention 
Detection.  The goal is to pull together all mentions of 
the same entity across multiple documents.   This task is 
a small and optional part of the complete ACE 
evaluation and results from it do not appear to be 
published.  
4. Corpora 
     To evaluate the effectiveness of our various 
techniques for cross document coreference, we use the 
same ?John Smith? corpus created by Bagga and 
Baldwin (1998). In addition, we created a larger, richer 
and highly ambiguous corpus that we call the ?Person-x 
corpus.? 
4.1 John Smith Corpus 
     Bagga and Baldwin tested their coreference 
algorithm against a set of 197 articles from 1996 and 
1997 editions of the New York Times, all of which refer 
to ?John Smith?. All articles either contain the name 
John Smith or some variation with a middle name or 
initial. There are 35 different John Smiths mentioned in 
the articles.  Of these, 24 refer to a unique John Smith 
entity which is not mentioned in the other 173 articles 
(197 minus 24).  
     We present results on this corpus for comparison 
with past work, to show that our approximation of those 
algorithms is approximately as effective as the originals.  
The corpus also permits us to show how our additional 
algorithms compare on that data.  However, our 
primarily evaluation corpus is the larger corpus that we 
now discuss. 
4.2 Person-x Corpus 
     Since the task of annotating documents is time 
consuming, we used a different technique to create a 
large test set with different entities of the same name. 
The technique used to construct this corpus is similar to 
the well known technique of creating artificial sense 
tagged corpora. Artificial sense tagged corpora is used 
to evaluate word sense disambiguation algorithms and is 
created by adding ambiguity to a corpus. Similarly, we 
consider the task of coreferencing multiple occurrences 
of ?John Smith? to be similar to coreferencing multiple 
occurrences of ?person-x?, where the occurrences of 
?person-x? are a disguise of multiple named entities 
such as ?George Bush? and ?Saddam Hussein?.  This 
approach simplifies the task of looking for a large 
collection of ?John Smith? articles and obtaining the 
actual coreference links between the many ?John 
Smith? entities.  It also allows us to create a vastly 
larger corpus of documents mentioning the ?same 
person.? 
     We first obtained from 10,000 to 50,000 unique 
documents from the TREC 1, 2 and 3 volumes using the 
Inquery search engine from UMass Amherst for each of 
the following subjects: art, business, education, 
government, healthcare, movies, music, politics, 
Number of 
occurrences 
Percentage 
of entities 
1 46.66 
2 18.78 
3 9.03 
4 4.55 
5 1.86 
6 1.16 
7 0.83 
8 0.46 
9 or more 16.67 
 
Table 1: Breakdown of distribution by number of 
occurrences within the Person X corpus. 
religion, science and sports. Then, we ran the 
documents through Identifinder, a named entity 
extraction system developed by BBN, to tag the named 
entities in the documents. 
     Next, we selected one person entity randomly from 
each document.  Since Identifinder occasionally tags an 
entity incorrectly, we manually went through each 
selection to filter out entities that were not people?s 
names. We also manually filter out cases where the 
tagged entity is only one word (e.g., John, Alex, etc.). 
     We replaced the occurrences of the selected entity in 
each document with ?person-x? as follows: 
 
In the late 1970s, the company hired producers 
<enamex type="person">jon peters</enamex>  and 
<enamex type="person">peter guber </enamex>  to 
start a studio from scratch.      
 
In the late 1970s, the company hired producers 
<enamex type="person">jon peters</enamex>  and 
<enamex type="person"> person-x </enamex>  to start 
a studio from scratch.      
 
     We also replaced all additional occurrences of the 
same name and names that matched (except for a 
middle initial or name) in that document with ?person-
x?.  For example, in the case above, other occurrences 
of Peter Guber or names such as Peter X. Guber would 
be replaced by ?person-x?. 
     We now have a large set of documents containing a 
reference to ?Person X? and we know for each 
document ?which? person entity it is referring to. We 
actually verified that names of the same name were the 
same entity, though with the large number of entities, 
the task was potentially overwhelming.  However, since 
the entities are categorized according to domain (by the 
query that found the document), determining the actual 
coreference links becomes significantly easier. In an 
article discussing sports, the multiple occurrences of the 
name ?Michael Chang? are most likely to be pointing to 
the tennis player?and the same tennis player.   
     These mappings from ?Person X? to their true names 
will serve as our evaluation/true coreference chains. 
Since we know the name that ?Person X? replaced, we 
assume that if those names are identical, they refer to 
the same person.  So all references to ?Person X? that  
correspond to, say, ?Bill Clinton,? will be put into the 
same coreference chain. 
     We manually removed documents whose Person X 
entity pointed to a different person than the person in its 
corresponding chain. Consider the scenario where we 
have four documents, three of which contains Person X 
entities pointing to John Smith (president of General 
Electric Corporation) and the other pointing to John 
Smith (the character in Pocahontas). The last John 
Smith document will be removed from the chain and the 
entire corpus. The final Person X corpus contains 
34,404 unique documents. Hence, there are 34,404 
?Person X?s in the corpus and they point to 14,767 
different actual entities. 15.24% of the entities occur in 
more than one domain subject. 
     Table 1 displays the distribution of entities versus 
their occurrences in our corpus. Slightly over 46% of 
entities occur only once in the collection of 34,404 
entities.  That compares to about 12% in the John Smith 
corpus.  Of the total of 315,415 unique entities that 
Identifinder recognized in the entire corpus, just under 
49% occurred precisely once, so our sample appears to 
be representative of the larger corpus even if it does not 
represent how ?John Smith? appears.  
     A potential shortcoming that was noted is that 
variation of names such as ?Bob Matthews? versus 
?Robert Matthews? may have been missed during the 
construction of this corpus. However, this problem did 
not show up in a set entities randomly sampled for 
analysis.  
      
5. Methodology 
     In all cases, we represented the mention of an entity 
(i.e., an occurrence of ?John Smith? or ?Person-x? 
depending on the corpus used) by the words around all 
occurrences of the entity in a document.   Based on 
exploratory work on training data, we choose a window 
of 55 words centered on each mention, merged those, 
and called the result a ?snippet.?  (In many cases the 
snippet incorporates text from only a single occurrence 
of the entity, but there are documents that contain 2-3 
?person-x? instances, and those are merged together.)  
We then employ three different methods for comparing 
snippets to determine whether their corresponding 
mentions are or are not to the same entity.  In the 
remainder of this section, we describe the three 
methods: incremental vector space, KL divergence, and 
agglomerative vector space.      
 
 5.1. Incremental vector space  
 
     Our intent with the incremental vector space model 
is to approximate the work reported by Bagga and 
Baldwin (1998).   Their system takes as input properly 
formatted documents and uses the University of 
Pennsylvania?s CAMP system to perform within-
document coreference resolution, doing more careful 
work to find additional mentions of the entity in the 
document. It then extracts all sentences that are relevant 
for each entity of interest based on the within-document 
coreference chains produced by CAMP. The sentences 
extracted form a summary that represents the entity (in 
contrast to our 55-word snippets). The system then 
computes the similarity of that summary with each of 
the other summaries using the vector space model. If the 
similarity computed is above a predefined threshold, 
then the two summaries are considered to be coreferent. 
     Each of the summaries was stored as a vector of 
terms. The similarity between two summaries S1 and S2 
is computed as by the cosine of the angle between their 
corresponding vectors.  Terms are weighted by a tf-idf 
weight as tf?log(N/df), where tf is the number of times 
that a term occurs in the summary, N is the total number 
of documents in the collection, and df is the number of 
documents that contain the term.   
     Because we did not have the same within-document 
coreference tools, we opted for a simpler variation on 
Bagga and Baldwin?s approach.  In our implementation, 
we represent snippets (combined 55-word sections of 
text) as vectors and use this model to represent each 
entity.  We calculated term weights and similarity in the 
same way, however.  The only difference is the text 
used to represent each mention. 
     For both cases, the system operates incrementally on 
the list of entities as follows.  We first create one 
coreference chain containing a single entity mention 
(one vector).  We then take the next entity vector and 
compare it against the entity in the link. If the two 
vectors have a similarity above a pre-defined threshold, 
then they are regarded to be referring to the same entity 
and the latter will be added into the same chain. 
Otherwise, a new coreference link is created for the 
entity.  
     We continue creating links using this incremental 
approach until all of the entities have been clustered into 
a chain. At each step, a new entity is compared against 
all existing coreference chains and is added into the 
chain with the highest average similarity if it is above 
the predefined threshold.   Our implementation differs 
from that of Bagga and Baldwin in the following ways: 
 
Bagga and Baldwin use a single-link technique to 
compare an entity with the entities in a coreference 
chain. This means they include an entity into a chain as 
soon as they find one pair-wise entity to entity 
comparison that is above the predefined threshold. We 
utilize an average-link comparison and compared an 
entity to each other entity in a coreference chain, then 
used the average similarity to determine if the entity 
should be included into the chain. 
 
They utilized the CAMP system developed by the 
University of Pennsylvania to resolve within document 
coreferencing and extract a summary for each entity. In 
our system, we simply extract the snippets for each 
entity and do not depend on within document 
coreferencing of an entity. 
 
5.2 KL Divergence 
The second technique that we implemented for entity 
disambiguation was based on Kullback-Leibler 
Divergence. For this technique, we represent the 
snippets in the form of a probability distribution of 
words, creating a so-called entity language model (Allan 
and Raghavan, 2002). The KL divergence is a classic 
measure of the ?distance? between two probability 
distributions. The more dissimilar the distributions are, 
the higher the KL divergence.  It is given by the 
equation: 
?
=
x xr
xq
xqrqD )(
)(log)()||(
  
where x ranges over the entire vocabulary. The smaller 
the distance calculated by KL divergence, the more 
similar a document is with another. If the distance is 0, 
then the two distributions are identical.  To deal with 
zero probabilities, we need some type of smoothing, and 
we chose to use the asymmetric skew divergence, 
mixing one distribution with the other as determined by 
a ? (Lee,2001): D(r || ?q + (1 ? ?)r) 
     Skew divergence best approximates KL divergence 
when the parameter ? is set to a value close to 1. In our 
experiments, we let ?=0.9 
     We used the incremental approach of Section 5.1, 
but with probability distributions.  Each of the 
distributions created (from a snippet) was evaluated 
against the distributions for existing coreference chains. 
Smaller distances computed through skew divergence 
indicate that the entity is similar to the entities in the 
chain. If the distance computed is smaller than a 
predefined threshold, then the new entity is added into 
the coreference chain and the probabilistic distribution 
of the coreference chain?s model is updated accordingly. 
We start with one entity in one coreference chain and 
continue comparing, inserting, and creating coreference 
chains until all of the entities have been resolved.  
     Note that the KL divergence approach is modeled 
directly after the incremental vector space approach.  
The difference is that the vector is replaced by a 
probability distribution and the comparison uses 
divergence rather than cosine similarity. 
5.3 Agglomerative vector space 
     In our explorations with the previous algorithm, we 
noticed that if early coreference chains contained 
misplaced entities, those entities attracted other entities 
with high similarity and ?polluted? the coreference 
chain with entities that are not part of the truth chain.  
We therefore switched to an agglomerative approach 
that builds up the clusters in a way that is order 
independent.  This approach is typically known as 
bottom-up agglomerative clustering.  It is also done in 
the vector space model, so we again represent the 
snippets by vectors.  
     We first create a coreference chain containing one 
entity for every entity to be resolved. For each 
coreference chain, we then find its nearest neighbor by 
computing the similarity of the chain against all other 
chains using the technique described above in Section 
5.1. If the highest similarity computed is above a 
predefined threshold, then we merge those two chains 
together. If any merging was performed in this iteration, 
we repeat the whole process of looking for the most 
similar pair and merging then in the next iteration. We 
continue this until no more merging is done?i.e., the 
highest similarity is below the threshold. 
     The only difference between this approach and that 
in the previous section is that the agglomerative 
technique requires more comparisons and takes more 
time.  On the other hand, it minimizes problems caused 
by a single spurious match and it is order independent. 
 
6. Experiments and Results 
     To evaluate our various techniques for the task of 
cross-document coreferencing, we used the two test 
corpora mentioned in Section 4 and the three 
coreference approaches described in Section 5. The 
coreference chains are then evaluated using the B-
CUBED algorithm to measure precision and recall as 
described in Section 2.  We present the results by 
corpus. 
 
6.1 John Smith Corpus Results 
Our main goal for the John Smith corpus is to demon-
strate that we have successfully approximated the 
algorithm of Bagga and Baldwin (1998).  Figure 1 
shows how recall and precision trade off against each 
other as the decision threshold (should a name be put 
into a chain) varies in the incremental vector space 
approach.  This graph is nearly identical to the tradeoff 
curves shown by Bagga and Baldwin, so we believe our 
variation on their approach is sufficiently accurate to 
draw conclusions.  A key point to note about the graph 
is that although there is an excellent recall/precision 
tradeoff point, the results are not stable around that 
threshold.  If the threshold is shifted slightly higher, 
recall plummets; if it is lowered slightly, precision drops 
off rapidly. 
     Figure 2 provides an alternative view of the same 
information, and overlays the other algorithms on it.  In 
this case we show a recall/precision tradeoff curve.  
Again, in all cases the tradeoff drops off rapidly, though 
the agglomerative vector space approach takes longer to 
fall from high accuracy. 
   Figure 3 provides another comparison of the three 
approaches by highlighting how the F-measure varies 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pe
rc
e
n
ta
ge
Threshold
precision
recall
F-Measure
Figure 1: Results of cross-document coreferencing 
on the John Smith corpus using the incremental 
vector space approach.  
 20
 30
 40
 50
 60
 70
 80
 90
 100
 20  30  40  50  60  70  80  90  100
Pr
ec
isi
o
n
Recall
Incremental VS
KL Divergence
Agglomerative VS
Breck and Bagga
Figure 2: Recall and precision tradeoff of three 
algorithms on the John Smith Corpus.  Results from 
Baldwin and Bagga (1998) are estimated and 
overlaid onto the graph. 
with the threshold.  Note that the agglomerative vector 
space approach has the highest measure and has a 
substantially less ?pointed? curve: it is much less 
sensitive to threshold selection and therefore more 
stable. 
     The agglomerative vector space achieved a peak F 
measure of 88.2% in comparison to the incremental 
approach that peaked at 84.3% (comparable to Bagga 
and Baldwin?s reported 84.6%).  We also created a 
single-link version of our incremental algorithm.  It 
achieved a peak F measure of only 81.4%, showing the 
advance of average link (when compared to our 
approach) and the advantage of using within-document 
coreference to find related sentences (when compared to 
their work). 
6.2 Person-x Results 
     We next evaluated the same three algorithms on the 
much larger Person X corpus. The recall/precision graph 
in Figure 4, when compared to that in Figure 2, clearly 
demonstrates that the larger corpus has made the task 
much harder.  However, the agglomerative vector space 
approach has been impacted the least and maintains 
excellent performance. 
        Figure 5 shows the F-measure graph.  In 
comparison to Figure 3, all of the techniques are less 
sensitive to threshold selection, but the two vector space 
approaches are less sensitive than the KL divergence 
approach.  It is unclear why this is, though may reflect 
problems with using the skewed divergence for 
smoothing.  
7. Further Exploration 
     We conducted additional analysis to explore the 
issues surrounding cross-document coreferencing. We 
ran experiments with the John Smith corpus to explore  
 
the question of the effectiveness of a model based on the 
amount of text used to represent an entity. 
7.1 Window size and recall/precision 
Allan and Raghavan (2002) showed that the size of the 
snippets correlates positively with the ?clarity? (non-
ambiguity) of the model. As the size of the snippet 
increases, the ambiguity of the model increases, 
presumably because it is more likely to include 
extraneous information from the surrounding text.  
     In our experiment with the John Smith corpus, we 
used the incremental vector space approach with a 
threshold of 0.1 and evaluated precision/recall using 
various window sizes for the snippets. Figure 6 shows 
the variation.  We discovered that the F-Measure peaks 
at 84.3% with a window size of 55 words. This is the 
window size that we used for all of our other 
experiments. 
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
e
ci
si
o
n
Recall
Incremental VS
KL Divergence
Agglomerative VS
Figure 4: Recall and precision tradeoff for the 
person-x corpus. 
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
F-
M
e
a
su
re
Threshold
Breck and Baldwin
Incremental VS
KL Divergence
Agglomerative VS
Figure 3: Comparison of F-Measure on the John 
Smith Corpus.    
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
F-
M
e
a
su
re
Threshold
Incremental VS
KL Divergence
Agglomerative VS
Figure 5: Comparison of F-Measure on the Person-x 
Corpus.    
7.2 Domain-specific sub-corpora 
 
     The person-x corpus may appear to be biased due to 
the manner of its construction. Since the documents 
were selected by subject, one may argue that the task of 
clustering entities will be much easier if the entities are 
clearly from different genres. However, if this is true, 
then it may account for about 85% of the entities in the 
person-x corpus that occur only in one domain subject. 
We hypothesized that coreferencing entities in the same 
genre domain can be considered to be harder in terms 
of achieving high precision because the consistency of 
the contents between documents in the same genre 
domain makes it significantly harder to create a unique 
model for each entity to aid the task of distinguishing 
one entity from another.  
     In order to see how our techniques measure up 
against this, we reevaluated the effectiveness of our 
methods of cross-document coreference resolution on a 
modified version of the person-x corpus. We clustered 
the documents into their original genre domain (recall 
that they were created using simple information 
retrieval queries). Then, we evaluated the 
precision/recall for each of the clusters and averaged 
the results to obtain a final precision/recall score. This 
eliminates the potential bias that clustering the entities 
becomes easier if the entities are clearly from different 
genres. Hypothetically, it also makes the task of cross-
document coreferencing more challenging than in 
reality when performed on actual corpora that is not 
clustered according to genre. Table 2 shows the 
breakdown of documents and entities in each genre.  
     The results of the experiments show that clustering 
documents by their domain specific attributes such as 
domain genre will hurt cross-document coreferencing. 
The highest F-Measure that was achieved with 
agglomerative vector space dropped 6% to 77% and 
incremental dropped a similar 5%.  The KL divergence 
approach, on the other hand, showed a modest increase 
of 3% to 77%, equaling the agglomerative approach. 
The reason for this may be because KL divergence 
relies more on the global property of the corpus and this 
approach is more effective when the nearest neighbor 
computation is degraded by the consistency of the word 
distributions between documents in the same genre 
domain.  
 
 
7.3 Runtime comparison. 
 
    An important observation in our comparison among 
the algorithms is running time. While we have shown 
that the agglomerative vector space approach produced 
the best results in our experiments, it is also important 
to  note  that  it  was  noticeably  slower.  The  estimated  
 
 
running time for the agglomerative vector space 
experiment  on  the  large  corpus was  approximately  3  
times longer than that of the incremental vector space 
and KL-Divergence. The runtimes of incremental 
approaches are linear whereas the runtime of our 
agglomerative vector space approach is O(n?).  
     Is the improvement in our results worth the 
difference in runtime? The noticeable run time 
difference in our experiment is caused by the need to 
cluster a large number of Person-x entities (34,404 
entities). In reality, it would be rare to find such a large 
number of entities across documents with the same 
name. In the analysis of our reasonably large corpus, 
less than 16% of entities occur more than 10 times. If 
the mean size of entities to be disambiguated is 
relatively small, then there will not be a significant 
degrade in runtime on the agglomerative approach. Thus, 
Genre 
Number of 
Documents  
Number of person-
x entities 
Art      3346       1455 
Business        315         182 
Education      6177       2351 
Government      3374         945 
Healthcare        914         405 
Movies        677       2292 
Music        976         366 
Politics      4298         949 
Religion      2699       1030 
Science      7211       2783 
Sports      4417       2009 
 
Table 2: Breakdown of document and entity 
distribution in the domain subject specific clusters. 
 
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  10  20  30  40  50  60  70  80  90  100
Pe
rc
e
n
ta
ge
Window Size
Precision
Recall
F-Measure
 
Figure 6: Relationship between the window size of the 
snippet and recall/precision on the John Smith corpus. 
 
our conclusion is that the tradeoff between coreference 
quality versus runtime in our agglomerative approach is 
definitely worthwhile if the number of same-named 
entities to be disambiguated is relatively small. 
8. Conclusion and Future Work 
 
     We were able to compare and contrast our results 
directly with previous work of Bagga and Baldwin by 
using the same corpus and evaluation technique. In 
order to perform a careful excursion into the limited 
work on cross document coreferencing, we deployed 
different information retrieval techniques for entity 
disambiguation and clustering. In our experiments, we 
have shown that the agglomerative vector space 
clustering algorithm consistently yields better precision 
and recall throughout most of the tests. It outperforms 
the incremental vector space disambiguation model and 
is much more stable with respect to the decision 
threshold.  Both vector space approaches outperform 
KL divergence except when the entities to be clustered 
belong to the same genre.    
     We are pleased that our snippet approach worked 
well on the task of cross document coreferencing since 
it was easier than running a within document 
coreference analyzer first.  It was also interesting to 
discover that previous techniques that worked well on a 
smaller corpus did not show the same promising recall 
and precision tradeoff on a larger corpus.  
     We are interested in continuing these evaluations in 
two ways.  First, colleagues of ours are working on a 
more realistic corpus that is not just large but also 
contains a much richer set of marked up entities.  We 
look forward to trying out techniques on that data when 
it is available.  Second, we intend to extend our work to 
include new comparison and clustering approaches.  It 
appears that sentence-based snippets and within-
document coreference information may provide a small 
gain.  And the subject information has apparently value 
in some cases, so we hope to determine how to use the    
information more broadly. 
Acknowledgements 
The John Smith corpus was provided by Breck Baldwin, 
and we are grateful to him for digging through his 
archives to find the data. This work was supported in 
part by the Center for Intelligent Information Retrieval, 
in part by SPAWARSYSCEN-SD grant number 
N66001-02-1-8903 and in part by Advanced Research 
and Development Activity under contract number 
MDA904-01-C-0984. Any opinions, findings and 
conclusions or recommendations expressed in this 
material are the author(s) and do not necessarily reflect 
those of the sponsor. 
References 
Allan, James, ed.  Topic Detection and Tracking: Event-
based Information Organization, Kluwer Academic 
Publishers, 2002. 
Allan, James and Raghavan, Hema. Entity Models: 
Construction and Application. Center for Intelligent 
Information Retrieval, Department of Computer 
Science, University of Massachusetts, 2002. 
Bagga, Amit and Breck Baldwin. Algorithms for 
Scoring Coreference Chains. In Proceedings of the 
Linguistic Coreference Workshop at The First 
International Conference on Language Resources and 
Evaluation (LREC'98), pp563-566, 1998. 
Bagga, Amit and Breck Baldwin. Entity-Based Cross-  
Document Coreferencing Using the Vector Space 
Model. Proceedings of the 36th Annual Meeting of 
the Association for Computational Linguistics and 
the 17th International Conference on Computational 
Linguistics (COLING-ACL'98),pp.79-85, 1998. 
Bagga, Amit and Breck Baldwin. Coreference as the 
Foundations for Link Analysis Over Free Text 
Databases. In Proceedings of the COLING-ACL'98 
Content Visualization and Intermedia 
Representations Workshop (CVIR'98), pp. 19-24, 
1998. 
Bagga, Amit, and Biermann, Alan. A Methodology for 
Cross-Document Coreference.  In Proceedings of the 
Fifth Joint Conference on Information Sciences 
(JCIS 2000), pp. 207-210, 2000. 
Bagga, Amit and Baldwin, Breck. How Much 
Processing Is Required for Cross-Document 
Coreference? In Proceedings of the Linguistic 
Coreference Workshop (LREC'98), pp. 563-566, 
1998. 
BBN Technologies.  Rough ?n? Ready?: Audio Index-
ing for Meetings and News.   http://www.bbn.com/-
speech/roughnready.html (2001) 
Kibble, Rodger and Kees, van Deemter. Coreference 
Annotation: Whither? Proceedings of LREC2000, 
Athens, pp. 1281-1286, 2000. 
Lee, Lillian. On the Effectiveness of the Skew 
Divergence for Statistical Language Analysis, 
Technical Report, Department of Computer Science, 
Cornell University, 2001. 
Proceedings of NAACL HLT 2007, pages 89?96,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Information Retrieval On Empty Fields
Victor Lavrenko, Xing Yi and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003-4610, USA
{lavrenko,yixing,allan}@cs.umass.edu
Abstract
We explore the problem of retrieving
semi-structured documents from a real-
world collection using a structured query.
We formally develop Structured Rele-
vance Models (SRM), a retrieval model
that is based on the idea that plausible
values for a given field could be inferred
from the context provided by the other
fields in the record. We then carry out a
set of experiments using a snapshot of the
National Science Digital Library (NSDL)
repository, and queries that only mention
fields missing from the test data. For such
queries, typical field matching would re-
trieve no documents at all. In contrast, the
SRM approach achieves a mean average
precision of over twenty percent.
1 Introduction
This study investigates information retrieval on
semi-structured information, where documents con-
sist of several textual fields that can be queried in-
dependently. If documents contained subject and
author fields, for example, we would expect to see
queries looking for documents about theory of rela-
tivity by the author Einstein.
This setting suggests exploring the issue of inex-
act match?is special theory of relativity relevant??
that has been explored elsewhere (Cohen, 2000).
Our interest is in an extreme case of that problem,
where the content of a field is not corrupted or in-
correct, but is actually absent. We wish to find rele-
vant information in response to a query such as the
one above even if a relevant document is completely
missing the subject and author fields.
Our research is motivated by the challenges we
encountered in working with the National Science
Digital Library (NSDL) collection.1 Each item in
the collection is a scientific resource, such as a re-
search paper, an educational video, or perhaps an
entire website. In addition to its main content, each
resource is annotated with metadata, which provides
information such as the author or creator of the re-
source, its subject area, format (text/image/video)
and intended audience ? in all over 90 distinct fields
(though some are very related). Making use of such
extensive metadata in a digital library paves the way
for constructing highly-focused models of the user?s
information need. These models have the potential
to dramatically improve the user experience in tar-
geted applications, such as the NSDL portals. To
illustrate this point, suppose that we are running
an educational portal targeted at elementary school
teachers, and some user requests teaching aids for
an introductory class on gravity. An intelligent
search system would be able to translate the request
into a structured query that might look something
like: subject=?gravity? AND audience=?grades 1-4?
AND format=?image,video? AND rights=?free-for-
academic-use?. Such a query can be efficiently an-
swered by a relational database system.
Unfortunately, using a relational engine to query a
semi-structured collection similar to NSDL will run
into a number of obstacles. The simplest problem is
1http://www.nsdl.org
89
that natural language fields are filled inconsistently:
e.g., the audience field contains values such as K-
4, K-6, second grade, and learner, all of which are
clearly semantically related.
A larger problem, and the one we focus on in this
study, is that of missing fields. For example 24%
of the items in the NSDL collection have no sub-
ject field, 30% are missing the author information,
and over 96% mention no target audience (reading
level). This means that a relational query for ele-
mentary school material will consider at most 4% of
all potentially relevant resources in the NSDL col-
lection.2
The goal of our work is to introduce a retrieval
model that will be capable of answering complex
structured queries over a semi-structured collection
with corrupt and missing field values. This study
focuses on the latter problem, an extreme version
of the former. Our approach is to use a generative
model to compute how plausible a word would ap-
pear in a record?s empty field given the context pro-
vided by the other fields in the record.
The remainder of this paper is organized as fol-
lows. We survey previous attempts at handling semi-
structured data in section 2. Section 3 will provide
the details of our approach, starting with a high-level
view, then providing a mathematical framework, and
concluding with implementation details. Section 4
will present an extensive evaluation of our model on
the large set of queries over the NSDL collection.
We will summarize our results and suggest direc-
tions for future research in Section 5.
2 Related work
The issue of missing field values is addressed in a
number of recent publications straddling the areas of
relational databases and machine learning. In most
cases, researchers introduce a statistical model for
predicting the value of a missing attribute or relation,
based on observed values. Friedman et al(1999) in-
troduce a technique called Probabilistic Relational
Models (PRM) for automatically learning the struc-
ture of dependencies in a relational database. Taskar
2Some of the NSDL metadata fields overlap substantially in
meaning, so it might be argued that the overlapping fields will
cover the collection better. Under the broadest possible inter-
pretation of field meanings, more than 7% of the documents
still contain no subject and 95% still contain no audience field.
et al(2001) demonstrate how PRM can be used to
predict the category of a given research paper and
show that categorization accuracy can be substan-
tially improved by leveraging the relational structure
of the data. Heckerman et al(2004) introduce the
Probabilistic Entity Relationship (PER) model as an
extension of PRM that treats relations between enti-
ties as objects. Neville at al (2003) discuss predict-
ing binary labels in relational data using Relational
Probabilistic Trees (RPT). Using this method they
successfully predict whether a movie was a box of-
fice hit based on other movies that share some of
the properties (actors, directors, producers) with the
movie in question.
Our work differs from most of these approaches in
that we work with free-text fields, whereas database
researchers typically deal with closed-vocabulary
values, which exhibit neither the synonymy nor the
polysemy inherent in natural language expressions.
In addition, the goal of our work is different: we aim
for accurate ranking of records by their relevance to
the user?s query, whereas database research has typ-
ically focused on predicting the missing value.
Our work is related to a number of existing ap-
proaches to semi-structured text search. Desai et
al (1987) followed by Macleod (1991) proposed us-
ing the standard relational approach to searching
unstructured texts. The lack of an explicit rank-
ing function in their approaches was partially ad-
dressed by Blair (1988). Fuhr (1993) proposed the
use of Probabilistic Relational Algebra (PRA) over
the weights of individual term matches. Vasan-
thukumar et al(1996) developed a relational imple-
mentation of the inference network retrieval model.
A similar approach was taken by de Vries and
Wilschut (1999), who managed to improve the ef-
ficiency of the approach. De Fazio et al(1995) in-
tegrated IR and RDBMS technology using an ap-
proached called cooperative indexing. Cohen (2000)
describes WHIRL ? a language that allows efficient
inexact matching of textual fields within SQL state-
ments. A number of relevant works are also pub-
lished in the proceedings of the INEX workshop.3
The main difference between these endeavors and
our work is that we are explicitly focusing on the
cases where parts of the structured data are missing
3http://inex.is.informatik.uni-duisburg.de/index.html
90
or mis-labeled.
3 Structured Relevance Model
In this section we will provide a detailed description
of our approach to searching semi-structured data.
Before diving into the details of our model, we want
to clearly state the challenge we intend to address
with our system.
3.1 Task: finding relevant records
The aim of our system is to identify a set of
records relevant to a structured query provided by
the user. We assume the query specifies a set of
keywords for each field of interest to the user, for
example Q: subject=?physics,gravity? AND audi-
ence=?grades 1-4?4. Each record in the database is
a set of natural-language descriptions for each field.
A record is considered relevant if it could plausibly
be annotated with the query fields. For example, a
record clearly aimed at elementary school students
would be considered relevant to Q even if it does not
contain ?grades 1-4? in its description of the target
audience. In fact, our experiments will specifically
focus on finding relevant records that contain no di-
rect match to the specified query fields, explicitly
targeting the problem of missing data and inconsis-
tent schemata.
This task is not a typical IR task because the
fielded structure of the query is a critical aspect of
the processing, not one that is largely ignored in fa-
vor of pure content based retrieval. On the other
hand, the approach used is different from most DB
work because cross-field dependencies are a key
component of the technique. In addition, the task
is unusual for both communities because it consid-
ers an unusual case where the fields in the query do
not occur at all in the documents being searched.
3.2 Overview of the approach
Our approach is based on the idea that plausible val-
ues for a given field could be inferred from the con-
text provided by the other fields in the record. For
instance, a resource titled ?Transductive SVMs? and
containing highly technical language in its descrip-
tion is unlikely to be aimed at elementary-school stu-
4For this paper we will focus on simple conjunctive queries.
Extending our model to more complex queries is reserved for
future research.
dents. In the following section we will describe a
statistical model that will allow us to guess the val-
ues of un-observed fields. At the intuitive level, the
model takes advantage of the fact that records sim-
ilar in one respect will often be similar in others.
For example, if two resources share the same author
and have similar titles, they are likely to be aimed at
the same audience. Formally, our model is based on
the generative paradigm. We will describe a proba-
bilistic process that could be viewed, hypothetically,
as the source of every record in our collection. We
will assume that the query provided by our user is
also a sample from this generative process, albeit a
very short one. We will use the observed query fields
(e.g. audience and subject) to estimate the likely val-
ues for other fields, which would be plausible in the
context of the observed subject and audience. The
distributions over plausible values will be called rel-
evance models, since they are intended to mimic the
kind of record that might be relevant to the observed
query. Finally, all records in the database will be
ranked by their information-theoretic similarity to
these relevance models.
3.3 Definitions
We start with a set of definitions that will be used
through the remainder of this paper. Let C be a
collection of semi-structured records. Each record
w consists of a set of fields w1. . .wm. Each
field wi is a sequence of discrete variables (words)
wi,1. . .wi,ni , taking values in the field vocabulary
Vi.5 When a record contains no information for the
i?th field, we assume ni=0 for that record. A user?s
query q takes the same representation as a record
in the database: q={qi,j?Vi : i=1..m, j = 1..ni}.
We will use pi to denote a language model over Vi,
i.e. a set of probabilities pi(v)?[0, 1], one for each
word v, obeying the constraint ?vpi(v) = 1. The
set of all possible language models over Vi will be
denoted as the probability simplex IPi. We define
pi : IP1?? ? ??IPm?[0, 1] to be a discrete measure
function that assigns a probability mass pi(p1. . .pm)
to a set of m language models, one for each of the
m fields present in our collection.
5We allow each field to have its own vocabulary Vi, since we
generally do not expect author names to occur in the audience
field, etc. We also allow Vi to share same words.
91
3.4 Generative Model
We will now present a generative process that will be
viewed as a hypothetical source that produced ev-
ery record in the collection C. We stress that this
process is purely hypothetical; its only purpose is to
model the kinds of dependencies that are necessary
to achieve effective ranking of records in response to
the user?s query. We assume that each record w in
the database is generated in the following manner:
1. Pick m distributions p1. . .pm according to pi
2. For each field i = 1. . .m:
(a) Pick the length ni of the i?th field of w
(b) Draw i.i.d. words wi,1. . .wi,ni from pi
Under this process, the probability of observing a
record {wi,j : i=1..m, j=1..ni} is given by the fol-
lowing expression:
?
IP1...IPm
[ m?
i=1
ni?
j=1
pi(wi,j)
]
pi(p1. . .pm)dp1. . .dpm (1)
3.4.1 A generative measure function
The generative measure function pi plays a critical
part in equation (1): it specifies the likelihood of us-
ing different combinations of language models in the
process of generating w. We use a non-parametric
estimate for pi, which relies directly on the combi-
nations of language models that are observed in the
training part of the collection. Each training record
w1. . .wm corresponds to a unique combination of
language models pw1 . . .pwm defined by the following
equation:
pwi (v) = #(v,wi) + ?icvni + ?i (2)
Here #(v,wi) represents the number of times the
word v was observed in the i?th field of w, ni
is the length of the i?th field, and cv is the rela-
tive frequency of v in the entire collection. Meta-
parameters ?i allow us to control the amount of
smoothing applied to language models of different
fields; their values are set empirically on a held-out
portion of the data.
We define pi(p1. . .pm) to have mass 1N when
its argument p1. . .pm corresponds to one of the N
records w in the training part Ct of our collection,
and zero otherwise:
pi(p1. . .pm) = 1N
?
w?Ct
m?
i=1
1pi=pwi (3)
Here pwi is the language model associated with the
training record w (equation 2), and 1x is the Boolean
indicator function that returns 1 when its predicate x
is true and zero when it is false.
3.4.2 Assumptions and limitations of the model
The generative model described in the previous
section treats each field in the record as a bag of
words with no particular order. This representation
is often associated with the assumption of word in-
dependence. We would like to stress that our model
does not assume word independence, on the con-
trary, it allows for strong un-ordered dependencies
among the words ? both within a field, and across
different fields within a record. To illustrate this
point, suppose we let ?i?0 in equation (2) to re-
duce the effects of smoothing. Now consider the
probability of observing the word ?elementary? in
the audience field together with the word ?differen-
tial? in the title (equation 1). It is easy to verify that
the probability will be non-zero only if some train-
ing record w actually contained these words in their
respective fields ? an unlikely event. On the other
hand, the probability of ?elementary? and ?differen-
tial? co-occurring in the same title might be consid-
erably higher.
While our model does not assume word indepen-
dence, it does ignore the relative ordering of the
words in each field. Consequently, the model will
fail whenever the order of words, or their proximity
within a field carries a semantic meaning. Finally,
our generative model does not capture dependencies
across different records in the collection, each record
is drawn independently according to equation (1).
3.5 Using the model for retrieval
In this section we will describe how the generative
model described above can be used to find database
records relevant to the structured query provided by
the user. We are given a structured query q, and
a collection of records, partitioned into the training
portion Ct and the testing portion Ce. We will use
the training records to estimate a set of relevance
92
records average unique
covered length words
title 655,673 (99%) 7 102,772
description 514,092 (78%) 38 189,136
subject 504,054 (77%) 12 37,385
content 91,779 (14%) 743 575,958
audience 22,963 (3.5%) 4 119
Table 1: Summary statistics for the five NSDL fields
used in our retrieval experiments.
models R1. . .Rm, intended to reflect the user?s in-
formation need. We will then rank testing records by
their divergence from these relevance models. A rel-
evance Ri(v) specifies how plausible it is that word
v would occur in the i?th field of a record, given
that the record contains a perfect match to the query
fields q1. . .qm:
Ri(v) = P (q1. . .v?qi. . .qm)P (q1. . .qi. . .qm) (4)
We use v?qi to denote appending word v to the
string qi. Both the numerator and the denomina-
tor are computed using equation (1). Once we have
computed relevance models Ri for each of the m
fields, we can rank testing records w? by their sim-
ilarity to these relevance models. As a similarity
measure we use weighted cross-entropy, which is an
extension of the ranking formula originally proposed
by (Lafferty and Zhai, 2001):
H(R1..m;w1..m) =
m?
i=1
?i
?
v?Vi
Ri(v) logpwi (v) (5)
The outer summation goes over every field of inter-
est, while the inner extends over all the words in the
vocabulary of the i?th field. Ri are computed accord-
ing to equation (4), while pwi are estimated from
equation (2). Meta-parameters ?i allow us to vary
the importance of different fields in the final rank-
ing; the values are selected on a held-out portion of
the data.
4 Experiments
4.1 Dataset and queries
We tested the performance of our model on a Jan-
uary 2005 snapshot of the National Science Digi-
tal Library repository. The snapshot contains a to-
tal of 656,992 records, spanning 92 distinct (though
sometimes related) fields. 6Only 7 of these fields
are present in every record, and half the fields are
present in less than 1% of the records. An average
record contains only 17 of the 92 fields. Our experi-
ments focus on a subset of 5 fields (title, description,
subject, content and audience). These fields were
selected for two reasons: (i) they occur frequently
enough to allow a meaningful evaluation and (ii)
they seem plausible to be included in a potential
query.7 Of these fields, title represents the title of the
resource, description is a very brief abstract, content
is a more detailed description (but not the full con-
tent) of the resource, subject is a library-like clas-
sification of the topic covered by the resource, and
audience reflects the target reading level (e.g. ele-
mentary school or post-graduate). Summary statis-
tics for these fields are provided in Table 1.
The dataset was randomly split into three sub-
sets: the training set, which comprised 50% of the
records and was used for estimating the relevance
models as described in section 3.5; the held-out set,
which comprised 25% of the data and was used to
tune the smoothing parameters ?i and the bandwidth
parameters ?i; and the evaluation set, which con-
tained 25% of the records and was used to evaluate
the performance of the tuned model8.
Our experiments are based on a set of 127 auto-
matically generated queries. We randomly split the
queries into two groups, 64 for training and 63 for
evaluation. The queries were constructed by com-
bining two randomly picked subject words with two
audience words, and then discarding any combi-
nation that had less than 10 exact matches in any
of the three subsets of our collection. This proce-
dure yields queries such as Q91={subject:?artificial
intelligence? AND audience=?researchers?}, or
Q101={subject:?philosophy? AND audience=?high
school?}.
4.2 Evaluation paradigm
We evaluate our model by its ability to find ?rele-
vant? records in the face of missing values. We de-
6As of May 2006, the NSDL contains over 1.5 million doc-
uments.
7The most frequent NSDL fields (id, icon, url, link and 4
brand fields) seem unlikely to be used in user queries.
8In real use, typical pseudo relevance feedback scheme can
be followed: retrieve top-k documents to build relevance mod-
els then perform IR again on the same whole collection
93
fine a record w to be relevant to the user?s query q
if every keyword in q is found in the corresponding
field of w. For example, in order to be relevant to
Q101 a record must contain the word ?philosophy? in
the subject field and words ?high? and ?school? in the
audience field. If either of the keywords is missing,
the record is considered non-relevant.9
When the testing records are fully observable,
achieving perfect retrieval accuracy is trivial: we
simply return all records that match all query key-
words in the subject and audience fields. As we
stated earlier, our main interest concerns the sce-
nario when parts of the testing data are missing. We
are going to simulate this scenario in a rather ex-
treme manner by completely removing the subject
and audience fields from all testing records. This
means that a straightforward approach ? matching
query fields against record fields ? will yield no rel-
evant results. Our approach will rank testing records
by comparing their title, description and content
fields against the query-based relevance models, as
discussed in section 3.5.
We will use the standard rank-based evaluation
metrics: precision and recall. Let NR be the total
number of records relevant to a given query, sup-
pose that the first K records in our ranking contain
NK relevant ones. Precision at rank K is defined
as NKK and recall is defined as
NK
NR . Average preci-
sion is defined as the mean precision over all ranks
where relevant items occur. R-precision is defined
as precision at rank K=NR.
4.3 Baseline systems
Our experiments will compare the ranking perfor-
mance of the following retrieval systems:
cLM is a cheating version of un-structured text
search using a state-of-the-art language-modeling
approach (Ponte and Croft, 1998). We disregard
the structure, take all query keywords and run them
against a concatenation of all fields in the testing
records. This is a ?cheating? baseline, since the con-
9This definition of relevance is unduly conservative by the
standards of Information Retrieval researchers. Many records
that might be considered relevant by a human annotator will be
treated as non-relevant, artificially decreasing the accuracy of
any retrieval algorithm. However, our approach has the advan-
tage of being fully automatic: it allows us to test our model on
a scale that would be prohibitively expensive with manual rele-
vance judgments.
catenation includes the audience and subject fields,
which are supposed to be missing from the testing
records. We use Dirichlet smoothing (Lafferty and
Zhai, 2001), with parameters optimized on the train-
ing data. This baseline mimics the core search capa-
bility currently available on the NSDL website.
bLM is a combination of SQL-like structured
matching and unstructured search with query ex-
pansion. We take all training records that contain
an exact match to our query and select 10 highly-
weighted words from the title, description, and con-
tent fields of these records. We run the resulting 30
words as a language modeling query against the con-
catenation of title, description, and content fields in
the testing records. This is a non-cheating baseline.
bMatch is a structured extension of bLM. As in
bLM, we pick training records that contain an ex-
act match to the query fields. Then we match 10
highly-weighted title words, against the title field of
testing records, do the same for the description and
content fields, and merge the three resulting ranked
lists. This is a non-cheating baseline that is similar
to our model (SRM). The main difference is that this
approach uses exact matching to select the training
records, whereas SRM leverages a best-match lan-
guage modeling algorithm.
SRM is the Structured Relevance Model, as de-
scribed in section 3.5. For reasons of both effec-
tiveness and efficiency, we firstly run the original
query to retrieve top-500 records, then use these
records to build SRMs. When calculating the cross
entropy(equ. 5), for each field we only include the
top-100 words which will appear in that field with
the largest probabilities.
Note that our baselines do not include a standard
SQL approach directly on testing records. Such
an approach would have perfect performance in a
?cheating? scenario with observable subject and au-
dience fields, but would not match any records when
the fields are removed.
4.4 Experimental results
Table 2 shows the performance of our model (SRM)
against the three baselines. The model parameters
were tuned using the 64 training queries on the train-
ing and held-out sets. The results are for the 63 test
queries run against the evaluation corpus. (Similar
results occur if the 64 training queries are run against
94
cLM bMatch bLM SRM %change improved
Rel-ret: 949 582 914 861 -5.80 26/50
Interpolated Recall - Precision:
at 0.00 0.3852 0.3730 0.4153 0.5448 31.2 33/49
at 0.10 0.3014 0.3020 0.3314 0.4783 44.3 42/56
at 0.20 0.2307 0.2256 0.2660 0.3641 36.9 40/59
at 0.30 0.2105 0.1471 0.2126 0.2971 39.8 36/58
at 0.40 0.1880 0.1130 0.1783 0.2352 31.9 36/58
at 0.50 0.1803 0.0679 0.1591 0.1911 20.1 32/57
at 0.60 0.1637 0.0371 0.1242 0.1439 15.8 27/51
at 0.70 0.1513 0.0161 0.1001 0.1089 8.7 21/42
at 0.80 0.1432 0.0095 0.0901 0.0747 -17.0 18/36
at 0.90 0.1292 0.0055 0.0675 0.0518 -23.2 12/27
at 1.00 0.1154 0.0043 0.0593 0.0420 -29.2 9/23
Avg.Prec. 0.1790 0.1050 0.1668 0.2156 29.25 43/63
Precision at:
5 docs 0.1651 0.2159 0.2413 0.3556 47.4 32/43
10 docs 0.1571 0.1651 0.2063 0.2889 40.0 34/48
15 docs 0.1577 0.1471 0.1841 0.2360 28.2 32/49
20 docs 0.1540 0.1349 0.1722 0.2024 17.5 28/47
30 docs 0.1450 0.1101 0.1492 0.1677 12.4 29/50
100 docs 0.0913 0.0465 0.0849 0.0871 2.6 37/57
200 docs 0.0552 0.0279 0.0539 0.0506 -6.2 33/53
500 docs 0.0264 0.0163 0.0255 0.0243 -4.5 26/48
1000 docs 0.0151 0.0092 0.0145 0.0137 -5.8 26/50
R-Prec. 0.1587 0.1204 0.1681 0.2344 39.44 31/49
Table 2: Performance of the 63 test queries retrieving 1000 documents on the evaluation data. Bold figures
show statistically significant differences. Across all 63 queries, there are 1253 relevant documents.
the evalution corpus.)
The upper half of Table 2 shows precision at
fixed recall levels; the lower half shows precision
at different ranks. The %change column shows rel-
ative difference between our model and the base-
line bLM. The improved column shows the num-
ber of queries where SRM exceeded bLM vs. the
number of queries where performance was different.
For example, 33/49 means that SRM out-performed
bLM on 33 queries out of 63, underperformed on
49?33=16 queries, and had exactly the same per-
formance on 63?49=14 queries. Bold figures in-
dicate statistically significant differences (according
to the sign test with p < 0.05).
The results show that SRM outperforms three
baselines in the high-precision region, beating
bLM?s mean average precision by 29%. User-
oriented metrics, such as R-precision and precision
at 10 documents, are improved by 39.4% and 44.3%
respectively. The absolute performance figures are
also very encouraging. Precision of 28% at rank 10
means that on average almost 3 out of the top 10
records in the ranked list are relevant, despite the re-
quested fields not being available to the model.
We note that SRM continues to outperform bLM
until very high recall and until the 100-document
cutoff. After that, SRM degrades rapidly with re-
spect to bLM. We feel the drop in effectiveness is of
marginal interest because precision is already well
below 10% and few users will be continuing to that
depth in the list.
It is encouraging to see that SRM outperforms
both cLM, the cheating baseline that takes advantage
of the field values that are supposed to be ?miss-
ing?, and bMatch, suggesting that best-match re-
trieval provides a superior strategy for selecting a set
of appropriate training records.
5 Conclusions
We have developed and empirically validated a new
retrieval model for semi-structured text. The model
is based on the idea that missing or corrupted val-
ues for one field can be inferred from values in other
fields of the record. The cross-field inference makes
it possible to find documents in response to a struc-
tured query when those query fields do not exist in
the relevant documents at all.
We validated the SRM approach on a large
95
archive of the NSDL repository. We developed a
large set of structured Boolean queries that had rel-
evant documents in the test portion of collection.
We then indexed the documents without the fields
used in the queries. As a result, using standard field
matching approaches, not a single document would
be returned in response to the queries?in particular,
no relevant documents would be found.
We showed that standard information retrieval
techniques and structured field matching could be
combined to address this problem, but that the SRM
approach outperforms them. We note that SRM
brought two relevant documents into the top five?
again, querying on missing fields?and achieved an
average precision of 23%, a more than 35% im-
provement over a state-of-the-art relevance model
approach combining the standard field matching.
Our work is continuing by exploring methods
for handling fields with incorrect or corrupted val-
ues. The challenge becomes more than just inferring
what values might be there; it requires combining
likely missing values with confidence in the values
already present: if an audience field contains ?under-
graduate?, it should be unlikely that ?K-6? would be
a plausible value, too.
In addition to using SRMs for retrieval, we are
currently extending the ideas to provide field valida-
tion and suggestions for data entry and validation:
the same ideas used to find documents with miss-
ing field values can also be used to suggest potential
values for a field and to identify values that seem
inappropriate. We have also begun explorations to-
ward using inferred values to help a user browse
when starting from some structured information?
e.g., given values for two fields, what values are
probable for other fields.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
the Defense Advanced Research Projects Agency
(DARPA) under contract number HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors? and do not necessarily reflect those of the
sponsor.
References
D.C. Blair. 1988. An extended relational document re-
trieval model. Inf. Process. Manage., 24(3):349?371.
W.W. Cohen. 2000. WHIRL: A word-based informa-
tion representation language. Artificial Intelligence,
118(1?2):163?196.
S. DeFazio, A. Daoud, L. A. Smith, and J. Srinivasan.
1995. Integrating IR and RDBMS Using Cooperative
Indexing. In Proceedings of SIGIR, pages 84?92.
B. C. Desai, P. Goyal, and F. Sadri. 1987. Non-first nor-
mal form universal relations: an application to infor-
mation retrieval systems. Inf. Syst., 12(1):49?55.
N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. 1999.
Learning probabilistic relational models. In IJCAI,
pages 1300?1309.
N. Fuhr. 1993. A probabilistic relational model for the
integration of IR and databases. In Proceedings of SI-
GIR, pages 309?317.
D. Heckerman, C. Meek, and D. Koller. 2004. Proba-
bilistic models for relational data. Technical Report
MSR-TR-2004-30, Microsoft Research.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proceedings of SIGIR, pages 111?
119.
I. Macleod. 1991. Text retrieval and the relational model.
Journal of the American Society for Information Sci-
ence, 42(3):155?165.
J. Neville, D. Jensen, L. Friedland, and M. Hay. 2003.
Learning relational probability trees. In Proceedings
of ACM KDD, pages 625?630, New York, NY, USA.
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
SIGIR, pages 275?281.
B. Taskar, E. Segal, and D. Koller. 2001. Probabilistic
classification and clustering in relational data. In Pro-
ceedings of IJCAI, pages 870?876.
S. R. Vasanthakumar, J.P. Callan, and W.B. Croft. 1996.
Integrating INQUERY with an RDBMS to support text
retrieval. IEEE Data Eng. Bull., 19(1):24?33.
A.D. Vries and A. Wilschut. 1999. On the integration of
IR and databases. In Proceedings of IFIP 2.6 Working
Conf. on Data Semantics, Rotorua, New Zealand.
96
Proceedings of NAACL HLT 2007, pages 220?227,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Case for Shorter Queries, and Helping Users Create Them
Giridhar Kumaran and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{giridhar,allan}@cs.umass.edu
Abstract
Information retrieval systems are fre-
quently required to handle long queries.
Simply using all terms in the query or re-
lying on the underlying retrieval model
to appropriately weight terms often leads
to ineffective retrieval. We show that re-
writing the query to a version that com-
prises a small subset of appropriate terms
from the original query greatly improves
effectiveness. Targeting a demonstrated
potential improvement of almost 50% on
some difficult TREC queries and their as-
sociated collections, we develop a suite of
automatic techniques to re-write queries
and study their characteristics. We show
that the shortcomings of automatic meth-
ods can be ameliorated by some simple
user interaction, and report results that are
on average 25% better than the baseline.
1 Introduction
Query expansion has long been a focus of infor-
mation retrieval research. Given an arbitrary short
query, the goal was to find and include additional
related and suitably-weighted terms to the original
query to produce a more effective version. In this pa-
per we focus on a complementary problem ? query
re-writing. Given a long query we explore whether
there is utility in modifying it to a more concise ver-
sion such that the original information need is still
expressed.
The Y!Q beta1 search engine allows users to se-
lect large portions of text from documents and issue
them as queries. The search engine is designed to
encourage users to submit long queries such as this
example from the web site ?I need to know the gas
mileage for my Audi A8 2004 model?. The moti-
vation for encouraging this type of querying is that
longer queries would provide more information in
the form of context (Kraft et al, 2006), and this ad-
ditional information could be leveraged to provide
a better search experience. However, handling such
long queries is a challenge. The use of all the terms
from the user?s input can rapidly narrow down the
set of matching documents, especially if a boolean
retrieval model is adopted. While one would ex-
pect the underlying retrieval model to appropriately
assign weights to different terms in the query and
return only relevant content, it is widely acknowl-
edged that models fail due to a variety of reasons
(Harman and Buckley, 2004), and are not suited to
tackle every possible query.
Recently, there has been great interest in personal-
ized search (Teevan et al, 2005), where the query is
modified based on a user?s profile. The profile usu-
ally consists of documents previously viewed, web
sites recently visited, e-mail correspondence and so
on. Common procedures for using this large amount
of information usually involve creating huge query
vectors with some sort of term-weighting mecha-
nism to favor different portions of the profile.
The queries used in the TREC ad-hoc tracks con-
sist of title, description and narrative sections, of
progressively increasing length. The title, of length
1http://yq.search.yahoo.com/
220
ranging from a single term to four terms is consid-
ered a concise query, while the description is consid-
ered a longer version of the title expressing the same
information need. Almost all research on the TREC
ad-hoc retrieval track reports results using only the
title portion as the query, and a combination of the
title and description as a separate query. Most re-
ported results show that the latter is more effective
than the former, though in the case of some hard col-
lections the opposite is true. However, as we shall
show later, there is tremendous scope for improve-
ment. Formulating a shorter query from the descrip-
tion can lead to significant improvements in perfor-
mance.
In the light of the above, we believe there is great
utility in creating query-rewriting mechanisms for
handling long queries. This paper is organized in
the following way. We start with some examples
and explore ways by which we can create concise
high-quality reformulations of long queries in Sec-
tion 2. We describe our baseline system in Section 3
and motivate our investigations with experiments in
Section 4. Since automatic methods have shortfalls,
we present a procedure in Section 5 to involve users
in selecting a good shorter query from a small selec-
tion of alternatives. We report and discuss the results
of this approach in Section 6. Related work is pre-
sented in Section 7. We wrap up with conclusions
and future directions in Section 8.
2 Selecting sub-queries
Consider the following query:
Define Argentine and British international rela-
tions.
When this query was issued to a search engine,
the average precision (AP, Section 3) of the results
was 0.424. When we selected subsets of terms (sub-
queries) from the query, and ran them as distinct
queries, the performance was as shown in Table 1. It
can be observed that there are seven different ways
of re-writing the original query to attain better per-
formance. The best query, also among the shortest,
did not have a natural-language flavor to it. It how-
ever had an effectiveness almost 50% more than the
original query. This immense potential for improve-
ment by query re-writing is the motivation for this
paper.
Query AP
.... ....
international relate 0.000
define international relate 0.000
.... ....
define argentina 0.123
international relate argentina 0.130
define relate argentina 0.141
relate argentina 0.173
define britain international relate argentina 0.424
define britain international argentina 0.469
britain international relate argentina 0.490
define britain relate argentina 0.494
britain international argentina 0.528
define britain argentina 0.546
britain relate argentina 0.563
britain argentina 0.626
Table 1: The results of using all possible subsets (ex-
cluding singletons) of the original query as queries.
The query terms were stemmed and stopped.
Analysis of the terms in the sub-queries and the
relationship of the sub-queries with the original
query revealed a few interesting insights that had po-
tential to be leveraged to aid sub-query selection.
1. Terms in the original query that a human would
consider vital in conveying the type of infor-
mation desired were missing from the best sub-
queries. For example, the best sub-query for
the example was britain argentina, omitting
any reference to international relations. This
also reveals a mismatch between the user?s
query and the way terms occurred in the corpus,
and suggests that an approximate query could
at times be a better starting point for search.
2. The sub-query would often contain only terms
that a human would consider vital to the query
while the original query would also (naturally)
contain them, albeit weighted lower with re-
spect to other terms. This is a common prob-
lem (Harman and Buckley, 2004), and the fo-
cus of efforts to isolate the key concept terms
in queries (Buckley et al, 2000; Allan et al,
1996).
3. Good sub-queries were missing many of the
noise terms found in the original query. Ideally
the retrieval model would weight them lower,
but dropping them completely from the query
appeared to be more effective.
221
4. Sub-queries a human would consider as an in-
complete expression of information need some-
times performed better than the original query.
Our example illustrates this point.
Given the above empirical observations, we ex-
plored a variety of procedures to refine a long query
into a shorter one that retained the key terms. We ex-
pected the set of terms of a good sub-query to have
the following properties.
A. Minimal Cardinality: Any set that contains
more than the minimum number of terms to retrieve
relevant documents could suffer from concept drift.
B. Coherency: The terms that constitute the sub-
query should be coherent, i.e. they should buttress
each other in representing the information need. If
need be, terms that the user considered important but
led to retrieval of non-relevant documents should be
dropped.
Some of the sub-query selection methods we ex-
plored with these properties in mind are reported be-
low.
2.1 Mutual Information
Let X and Y be two random variables, with joint
distribution P (x, y) and marginal distributions P (x)
and P (y) respectively. The mutual information is
then defined as:
I(X;Y ) =
?
x
?
y
p(x, y)log p(x, y)p(x)p(y)
(1)
Intuitively, mutual information measures the infor-
mation about X that is shared by Y . If X and Y are
independent, then X contains no information about
Y and vice versa and hence their mutual information
is zero. Mutual Information is attractive because it is
not only easy to compute, but also takes into consid-
eration corpus statistics and semantics. The mutual
information between two terms (Church and Hanks,
1989) can be calculated using Equation 2.
I(x, y) = log
n(x,y)
N
n(x)
N
n(y)
N
(2)
n(x, y) is the number of times terms x and y oc-
curred within a term window of 100 terms across the
corpus, while n(x) and n(y) are the frequencies of
x and y in the collection of size N terms.
To tackle the situation where we have an arbi-
trary number of variables (terms) we extend the two-
variable case to the multivariate case. The extension,
called multivariate mutual information (MVMI) can
be generalized from Equation 1 to:
I(X1;X2;X3; ...;XN ) =
N
?
i=1
(?1)i?1
?
X?(X1,X2,X3,...,XN),|X|=k
H(X) (3)
The calculation of multivariate information using
Equation 3 was very cumbersome, and we instead
worked with the approximation (Kern et al, 2003)
given below.
I(X1;X2;X3; ...;XN ) = (4)
?
i,j={1,2,3,...,N ;i6=j}
I(Xi;Xj) (5)
For the case involving multiple terms, we calcu-
lated MVMI as the sum of the pair-wise mutual in-
formation for all terms in the candidate sub-query.
This can be also viewed as the creation of a com-
pletely connected graph G = (V,E), where the ver-
tices V are the terms and the edges E are weighted
using the mutual information between the vertices
they connect.
To select a score representative of the quality of
a sub-query we considered several options includ-
ing the sum, average, median and minimum of the
edge weights. We performed experiments on a set
of candidate queries to determine how well each of
these measures tracked AP, and found that the aver-
age worked best. We refer to the sub-query selection
procedure using the average score as Average.
2.2 Maximum Spanning Tree
It is well-known that an average is easily skewed
by outliers. In other words, the existence of one or
more terms that have low mutual information with
every other term could potentially distort results.
This problem could be further compounded by the
fact that mutual information measured using Equa-
tion 2 could have a negative value. We attempted
222
to tackle this problem by considering another mea-
sure that involved creating a maximum spanning tree
(MaxST) over the fully connected graph G, and us-
ing the weight of the identified tree as a measure rep-
resentative of the candidate query?s quality (Rijsber-
gen, 1979). We used Kruskal?s minimum spanning
tree (Cormen et al, 2001) algorithm after negating
the edge weights to obtain a MaxST. We refer to the
sub-query selection procedure using the weight of
the maximum spanning tree as MaxST.
2.3 Named Entities
Named entities (names of persons, places, organiza-
tions, dates, etc.) are known to play an important
anchor role in many information retrieval applica-
tions. In our example from Section 2, sub-queries
without Britain or Argentina will not be effective
even though the mutual information score of the
other two terms international and relations might
indicate otherwise. We experimented with another
version of sub-query selection that considered only
sub-queries that retained at least one of the named
entities from the original query. We refer to the vari-
ants that retained named entities as NE Average and
NE MasT.
3 Experimental Setup
We used version 2.3.2 of the Indri search engine, de-
veloped as part of the Lemur2 project. While the
inference network-based retrieval framework of In-
dri permits the use of structured queries, the use
of language modeling techniques provides better es-
timates of probabilities for query evaluation. The
pseudo-relevance feedback mechanism we used is
based on relevance models (Lavrenko and Croft,
2001).
To extract named entities from the queries, we
used BBN Identifinder (Bikel et al, 1999). The
named entities identified were of type Person, Lo-
cation, Organization, Date, and Time.
We used the TREC Robust 2004 and Robust 2005
(Voorhees, 2006) document collections for our ex-
periments. The 2004 Robust collection contains
around half a million documents from the Finan-
cial Times, the Federal Register, the LA Times, and
FBIS. The Robust 2005 collection is the one-million
2http://www.lemurproject.org
document AQUAINT collection. All the documents
were from English newswire. We chose these col-
lections because they and their associated queries
are known to be hard, and hence present a chal-
lenging environment. We stemmed the collections
using the Krovetz stemmer provided as part of In-
dri, and used a manually-created stoplist of twenty
terms (a, an, and, are, at, as, be, for, in, is, it, of, on,
or, that, the, to, was, with and what). To determine
the best query selection procedure, we analyzed 163
queries from the Robust 2004 track, and used 30 and
50 queries from the 2004 and 2005 Robust tracks re-
spectively for evaluation and user studies.
For all systems, we report mean average preci-
sion (MAP) and geometric mean average precision
(GMAP). MAP is the most widely used measure in
Information Retrieval. While precision is the frac-
tion of the retrieved documents that are relevant, av-
erage precision (AP) is a single value obtained by
averaging the precision values at each new relevant
document observed. MAP is the arithmetic mean of
the APs of a set of queries. Similarly, GMAP is the
geometric mean of the APs of a set of queries. The
GMAP measure is more indicative of performance
across an entire set of queries. MAP can be skewed
by the presence of a few well-performing queries,
and hence is not as good a measure as GMAP from
the perspective of measure comprehensive perfor-
mance.
4 Experiments
We first ran two baseline experiments to record the
quality of the available long query and the shorter
version. As mentioned in Section 1, we used the
description and title sections of each TREC query
as surrogates for the long and short versions re-
spectively of a query. The results are presented in
the first two rows, Baseline and Pseudo-relevance
Feedback (PRF), of Table 2. Measured in terms of
MAP and GMAP (Section3), using just the title re-
sults in better performance than using the descrip-
tion. This clearly indicates the existence of terms in
the description that while elaborating an information
need hurt retrieval performance. The result of using
pseudo-relevance feedback (PRF) on both the title
and description show moderate gains - a known fact
about this particular collection and associated train-
223
MAP GMAP
Long Query Baseline 0.243 0.136
(Description) PRF 0.270 0.124
Short Query Baseline 0.249 0.154
(Title) PRF 0.269 0.148
Best sub-query Baseline 0.342 0.270
(Combination) PRF 0.343 0.241
Table 2: Results across 163 training queries on the
Robust 2004 collection. Using the best sub-query
results in almost 50% improvement over the baseline
ing queries.
To show the potential and utility of query re-
writing, we first present results that show the upper
bound on performance that can obtained by doing
so. We ran retrieval experiments with every combi-
nation of query terms. For a query of length n, there
are 2n combinations. We limited our experiments
to queries of length n ? 12. Selecting the perfor-
mance obtained by the best sub-query of each query
revealed an upper bound in performance almost 50%
better than the baseline (Table 2).
To evaluate the automatic sub-query selection
procedures developed in Section 2, we performed
retrieval experiments using the sub-queries selected
using them. The results, which are presented in Ta-
ble 3, show that the automatic sub-query selection
process was a failure. The results of automatic se-
lection were worse than even the baseline, and there
was no significant difference between using any of
the different sub-query selection procedures.
The failure of the automatic techniques could be
attributed to the fact that we were working with the
assumption that term co-occurrence could be used
to model a user?s information need. To see if there
was any general utility in using the procedures to
select sub-queries, we selected the best-performing
sub-query from the top 10 ranked by each selection
procedure (Table 4). While the effectiveness in each
case as measured by MAP is not close to the best
possible MAP, 0.342, they are all significantly better
than the baseline of 0.243.
5 Interacting with the user
The final results we presented in the last section
hinted at a potential for user interaction. We envi-
MAP GMAP
Baseline 0.243 0.136
Average 0.172 0.025
MaxST 0.172 0.025
NE Average 0.170 0.023
NE MaxST 0.182 0.029
Table 3: Score of the highest rank sub-query by var-
ious measures.
MAP GMAP
Baseline 0.243 0.136
AverageTop10 0.296 0.167
MaxSTTop10 0.293 0.150
NE AverageTop10 0.278 0.156
NE MaxSTTop10 0.286 0.159
Table 4: Score of the best sub-query in the top 10
ranked by various measures
sioned providing the user with a list of the top 10
sub-query candidates using a good ranking proce-
dure, and asking her to select the sub-query she felt
was most appropriate. This additional round of hu-
man intervention could potentially compensate for
the inability of the ranking measures to select the
best sub-query automatically.
5.1 User interface design
We displayed the description (the long query) and
narrative portion of each TREC query in the inter-
face. The narrative was provided to help the partic-
ipant understand what information the user who is-
sued the query was interested in. The title was kept
hidden to avoid influencing the participant?s choice
of the best sub-query. A list of candidate sub-queries
was displayed along with links that could be clicked
on to display a short section of text in a designated
area. The intention was to provide an example of
what would potentially be retrieved with a high rank
if the candidate sub-query were used. The partici-
pant used this information to make two decisions -
the perceived quality of each sub-query, and the best
sub-query from the list. A facility to indicate that
none of the candidates were good was also included.
224
Percentage of candidates
better than baseline
Average 28.5%
MaxST 35.5%
NE Average 31.1%
NE MaxST 36.6%
Table 5: Number of candidates from top 10 that ex-
ceeded the baseline
5.2 User interface content issues
The two key issues we faced while determining the
content of the user interface were:
A. Deciding which sub-query selection procedure
to use to get the top 10 candidate sub-queries: To
determine this in the absence of any significant dif-
ference in performance due to the top-ranked can-
didate selected by each procedure, we looked at the
number of candidates each procedure brought into
the top 10 that were better than the baseline query,
as measured by MAP. This was guided by the belief
that greater the number of better candidates in the
top 10, the higher the probability that the user would
select a better sub-query. Table 5 shows how each of
the selection procedures compared. The NE MaxST
ranking procedure had the most number of better
sub-queries in the top 10, and hence was chosen.
B. Displaying context: Simply displaying a list
of 10 candidates without any supportive information
would make the task of the user difficult. This was in
contrast to query expansion techniques (Anick and
Tipirneni, 1999) where displaying a list of terms suf-
ficed as the task of the user was to disambiguate
or expand a short query. An experiment was per-
formed in which a single user worked with a set of
30 queries from Robust 2004, and an accompanying
set of 10 candidate sub-queries each, twice - once
with passages providing context and one with snip-
pets providing context. The top-ranked passage was
generated by modifying the candidate query into
one that retrieved passages of fixed length instead
of documents. Snippets, like those seen along with
links to top-ranked documents in the results from
almost all popular search engines, were generated
after a document-level query was used to query the
collection. The order in which the two contexts were
presented to the user was randomized to prevent the
MAP GMAP
Snippet as Context 0.348 0.170
Passage as Context 0.296 0.151
Table 6: Results showing the MAP over 19 of 30
queries that the user provided selections for using
each context type.
user from assuming a quality order. We see that pre-
senting the snippet led to better MAP that presenting
the passage (Table 6). The reason for this could be
that the top-ranking passage we displayed was from
a document ranked lower by the document-focussed
version of the query. Since we finally measure MAP
only with respect to document ranking, and the snip-
pet was generated from the top-ranked document,
we hypothesize that this led to the snippet being a
better context to display.
6 User Evaluation
We conducted an exploratory study with five par-
ticipants - four of them were graduate students in
computer science while the fifth had a background
in the social sciences and was reasonably proficient
in the use of computers and internet search engines.
The participants worked with 30 queries from Ro-
bust 2004, and 50 from Robust 20053. The baseline
values reported are automatic runs with the descrip-
tion as the query.
Table 7 shows that all five participants4 were
able to choose sub-queries that led to an improve-
ment in performance over the baseline (TREC title
query only). This improvement is not only on MAP
but also on GMAP, indicating that user interaction
helped improve a wide spectrum of queries. Most
notable were the improvements in P@5 and P@10.
This attested to the fact that the interaction tech-
nique we explored was precision-enhancing. An-
other interesting result, from # sub-queries selected
was that participants were able to decide in a large
number of cases that re-writing was either not useful
for a query, or that none of the options presented to
them were better. Showing context appears to have
helped.
3Participant 4 looked that only 34 of the 50 queries presented
4The p value for testing statistical significance of MAP im-
provement for Participant 5 was 0.053 - the result very narrowly
missed being statistically significant.
225
# Queries # sub-queries % sub-queries MAP GMAP P@5 p@10
selected better
Baseline 0.203 0.159 0. 476 0.507
1 50 26 80.7% With Interaction 0.249 0.199 0.615 0.580
Upper Bound 0.336 0.282 0.784 0.719
Baseline 0.224 0.156 0.484 0.526
2 50 19 78.9% With Interaction 0.277 0.209 0.652 0.621
Upper Bound 0.359 0.293 0.810 0.742
Baseline 0.217 0.126 0.452 0.432
3 80 53 73.5% With Interaction 0.276 0.166 0.573 0.501
Upper Bound 0.354 0.263 0.762 0.654
Baseline 0.192 0.142 0.462 0.525
4 50(34) 19 68.7% With Interaction 0.255 0.175 0.612 0.600
Upper Bound 0.344 0.310 0.862 0.800
Baseline 0.206 0.111 0.433 0.410
5 80 65 61.5% With Interaction 0.231 0.115 0.486 0.429
Upper Bound 0.341 0.245 0.738 0.640
Table 7: # Queries refers to the number of queries that were presented to the participant while # sub-queries
selected refers to the number of queries for which the participant chose a sub-query. All scores including
upper bounds were calculated only considering the queries for which the participant selected a sub-query.
An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was
measured using a paired t-test, with ? set to 0.05.
7 Related Work
Our interest in finding a concise sub-query that ef-
fectively captures the information need is reminis-
cent of previous work in (Buckley et al, 2000).
However, the focus was more on balancing the ef-
fect of query expansion techniques such that differ-
ent concepts in the query were equally benefited.
Mutual information has been used previously in
(Church and Hanks, 1989) to identify collocations of
terms for identifying semantic relationships in text.
Experiments were confined to bigrams. The use of
MaST over a graph of mutual information values
to incorporate the most significant dependencies be-
tween terms was first noted in (Rijsbergen, 1979).
Extensions can be found in a different field - image
processing (Kern et al, 2003) - where multivariate
mutual information is frequently used.
Work done by (White et al, 2005) provided a ba-
sis for our decision to show context for sub-query se-
lection. The useful result that top-ranked sentences
could be used to guide users towards relevant mate-
rial helped us design an user interface that the par-
ticipants found very convenient to use.
A related problem addressed by (Cronen-
Townsend et al, 2002) was determining query qual-
ity. This is known to be a very hard problem, and
various efforts (Carmel et al, 2006; Vinay et al,
2006) have been made towards formalizing and un-
derstanding it.
Previous work (Shapiro and Taksa, 2003) in the
web environment attempted to convert a user?s natu-
ral language query into one suited for use with web
search engines. However, the focus was on merg-
ing the results from using different sub-queries, and
not selection of a single sub-query. Our approach
of re-writing queries could be compared to query re-
formulation, wherein a user follows up a query with
successive reformulations of the original. In the web
environment, studies have shown that most users
still enter only one or two queries, and conduct lim-
ited query reformulation (Spink et al, 2002). We hy-
pothesize that the techniques we have developed will
be well-suited for search engines like Ask Jeeves
where 50% of the queries are in question format
226
(Spink and Ozmultu, 2002). More experimentation
in the Web domain is required to substantiate this.
8 Conclusions
Our results clearly show that shorter reformulations
of long queries can greatly impact performance. We
believe that our technique has great potential to be
used in an adaptive information retrieval environ-
ment, where the user starts off with a more general
information need and a looser notion of relevance.
The initial query can then be made longer to express
a most focused information need.
As part of future work, we plan to conduct a more
elaborate study with more interaction strategies in-
cluded. Better techniques to select effective sub-
queries are also in the pipeline. Since we used mu-
tual information as the basis for most of our sub-
query selection procedures, we could not consider
sub-queries that comprised of a single term. We plan
to address this issue too in future work.
9 Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
the Defense Advanced Research Projects Agency
(DARPA) under contract number HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors and do not necessarily reflect those of the
sponsor. We also thank the anonymous reviewers
for their valuable comments.
References
James Allan, James P. Callan, W. Bruce Croft, Lisa Ballesteros,
John Broglio, Jinxi Xu, and Hongming Shu. 1996. Inquery
at TREC-5. In TREC.
Peter G. Anick and Suresh Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative infor-
mation seeking. In 22nd ACM SIGIR Proceedings, pages
153?159.
Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine
Learning, 34(1-3):211?231.
Chris Buckley, Mandar Mitra, Janet Walz, and Claire Cardie.
2000. Using clustering and superconcepts within smart:
TREC 6. Information Processing and Management,
36(1):109?131.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg.
2006. What makes a query difficult? In 29th ACM SIGIR
Proceedings, pages 390?397.
Kenneth Ward Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In 27th
ACL Proceedings, pages 76?83.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,
and Clifford Stein. 2001. Introduction to Algorithms, Sec-
ond Edition. The MIT Electrical Engineering and Computer
Science Series. The MIT Press.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002.
Predicting query performance. In 25th ACM SIGIR Proceed-
ings, pages 299?306.
Donna Harman and Chris Buckley. 2004. The NRRC reliable
information access (RIA) workshop. In 27th ACM SIGIR
Proceedings, pages 528?529.
Jeffrey P. Kern, Marios Pattichis, and Samuel D. Stearns. 2003.
Registration of image cubes using multivariate mutual infor-
mation. In Thirty-Seventh Asilomar Conference, volume 2,
pages 1645?1649.
Reiner Kraft, Chi Chao Chang, Farzin Maghoul, and Ravi Ku-
mar. 2006. Searching with context. In 15th International
CIKM Conference Proceedings, pages 477?486.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance based
language models. In 24th ACM SIGIR Conference Proceed-
ings, pages 120?127.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA, USA, 2 edition.
Jacob Shapiro and Isak Taksa. 2003. Constructing web search
queries from the user?s information need expressed in a nat-
ural language. In Proceedings of the 2003 ACM Symposium
on Applied Computing, pages 1157?1162.
Amanda Spink and H. Cenk Ozmultu. 2002. Characteristics of
question format web queries: An exploratory study. Infor-
mation Processing and Management, 38(4):453?471.
Amanda Spink, Bernard J. Jansen, Dietmar Wolfram, and Tefko
Saracevic. 2002. From e-sex to e-commerce: Web search
changes. Computer, 35(3):107?109.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Per-
sonalizing search via automated analysis of interests and ac-
tivities. In 28th ACM SIGIR Proceedings, pages 449?456.
Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Ken
Wood. 2006. On ranking the effectiveness of searches. In
29th ACM SIGIR Proceedings, pages 398?404.
Ellen M. Voorhees. 2006. The TREC 2005 robust track. SIGIR
Forum, 40(1):41?48.
Ryen W. White, Joemon M. Jose, and Ian Ruthven. 2005. Us-
ing top-ranking sentences to facilitate effective information
access: Book reviews. JAIST, 56(10):1113?1125.
227
Proceedings of NAACL HLT 2007, pages 532?539,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Question Answering using Integrated Information Retrieval and
Information Extraction
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027
bschiff,kathy@cs.columbia.edu
Ralph Grishman
Department of Computer Science
New York University
New York, NY 10003
grishman@cs.nyu.edu
James Allan
University of Massachusetts
Department of Computer Science
Amherst, MA 01003
allan@cs.umass.edu
Abstract
This paper addresses the task of provid-
ing extended responses to questions re-
garding specialized topics. This task is an
amalgam of information retrieval, topical
summarization, and Information Extrac-
tion (IE). We present an approach which
draws on methods from each of these ar-
eas, and compare the effectiveness of this
approach with a query-focused summa-
rization approach. The two systems are
evaluated in the context of the prosecution
queries like those in the DARPA GALE
distillation evaluation.
1 Introduction
As question-answering systems advance from han-
dling factoid questions to more complex requests,
they must be able to determine how much informa-
tion to include while making sure that the informa-
tion selected is indeed relevant. Unlike factoid ques-
tions, there is no clear criterion that defines the kind
of phrase that answers the question; instead, there
may be many phrases that could make up an answer
and it is often unclear in advance, how many. As
system developers, our goal is to yield high recall
without sacrificing precision.
In response to questions about particular events of
interest that can be enumerated in advance, it is pos-
sible to perform a deeper semantic analysis focusing
on the entities, relations, and sub-events of interest.
On the other hand, the deeper analysis may be error-
ful and will also not always provide complete cov-
erage of the information relevant to the query. The
challenge, therefore, is to blend a shallower, robust
approach with the deeper approach in an effective
way.
In this paper, we show how this can be achieved
through a synergistic combination of information re-
trieval and information extraction. We interleave in-
formation retrieval (IR) and response generation, us-
ing IR in high precision mode in the first stage to
return a small number of documents that are highly
likely to be relevant. Information extraction of enti-
ties and events within these documents is then used
to pinpoint highly relevant sentences and associated
words are selected to revise the query for a sec-
ond pass of retrieval, improving recall. As part of
this process, we approximate the relevant context by
measuring the proximity of the target name in the
query and extracted events.
Our approach has been evaluated in the frame-
work of the DARPA GALE1 program. One of the
GALE evaluations involves responding to questions
based on a set of question templates, ranging from
broad questions like ?Provide information on X?,
where X is an organization, to questions focused on
particular classes of events. For the experiments pre-
sented here, we used the GALE program?s prosecu-
tion class of questions. These are given in the fol-
lowing form: ?Describe the prosecution of X for Y,?
where X is a person and Y is a crime or charge. Our
results show that we are able to achieve higher accu-
1Global Autonomous Language Exploitation
532
racy with a system that exploits the justice events
identified by IE than with an approach based on
query-focused summarization alone.
In the following sections, we first describe the
task and then review related work in question-
answering. Section 3 details our procedure for find-
ing answers as well as performing the information
retrieval and information extraction tasks. Section 4
compares the results of the two approaches. Finally,
we present our conclusion and plans for future work.
1.1 The Task
The language of the question immediately raises the
question of what is meant by prosecution. Unlike a
question such as ?When was X born??, which is ex-
pected to be answered by a clear, concrete phrase,
the prosecution question asks for a much greater
range of material. The answer is in no way limited
to the statements and activities of the prosecuting at-
torney, although these would certainly be part of a
comprehensive answer.
In the GALE relevance guidelines2 , the answer
can include many facets of the case:
? Descriptions of the accused?s involvement in
the crime.
? Descriptions of the activities, motivations, and
involvement in the crime.
? Descriptions of the person as long as they are
related to the trial.
? Information about the defense of the suspect.
? Information about the sentencing of the person.
? Information about similar cases involving the
person.
? Information about the arrest of the person and
statements made by him or her.
? Reactions of people involved in the trial, as
well as statements by officials or reactions by
the general public.
2BAE Systems Advanced Information Technologies, ?Rele-
vance Guidelines for Distillation Evaluation for GALE: Global
Autonomous Language Exploitation?, Version 2.2, January 25,
2007
The guidelines also provide a catchall instruction
to ?include reported information believed to be rele-
vant to the case, but deemed inadmissible in a court
of law.?
It is easy to see that the use of a few search terms
alone will be insufficient to locate a comprehensive
answer.
We took a broad view of the question type and
consider that any information about the investiga-
tion, accusation, pursuit, capture, trial and punish-
ment of the individual, whether a person or organi-
zation, would be desireable in the answer.
1.2 Overview
The first step in our procedure sends a query tai-
lored to this question type to the IR system to ob-
tain a small number of high-quality documents with
which we can determine what name variations are
used in the corpus and estimate how many docu-
ments contain references to the individual. In the
future we will expand the type of information we
want to glean from this small set of documents. A
secondary search is issued to find additional docu-
ments that refer to the individual, or individuals.
Once we have the complete document retrieval,
the foundation for finding these types of events
lies in the Proteus information extraction compo-
nent (Grishman et al, 2005). We employ an IE sys-
tem trained for the tasks of the 2005 Automatic Con-
tent Extraction evaluation, which include entity and
event extraction. ACE defines a number of general
event types, including justice events, which cover in-
dictments, accusations, arrests, trials, and sentenc-
ings. The union of all these specific categories gives
us many of the salient events in a criminal justice
case from beginning to end. The program uses the
events, as well as the entities, to help identify the
passages that respond to the question.
The selection of sentences is based on the as-
sumption that the co-occurrence of the target indi-
vidual and a judicial event indicates that the target
is indeed involved in the event, but these two do not
necesssarily occur in the same sentence.
2 Related Work
A large body of work in question-answering has fol-
lowed from the opening of the Text Retrieval Con-
533
ference?s Q&A track in 1999. The task started as a
group of factoid questions and expanded from there
into more sophisticated problems. TREC provides
a unique testbed of question-answer pairs for re-
searchers and this data has been influential in fur-
thering progress.
In TREC 2006, there was a new secondary task
called ?complex, interactive Question Answering,?
(Dang et al, 2006) which is quite close to the GALE
problem, though it incorporated interaction to im-
prove results. Questions are posed in a canonical
form plus a narrative elaborating on the kind of in-
formation requested. An example question (from the
TREC guidelines) asks, ?What evidence is there for
transport of [drugs] from [Bonaire] to the [United
States]?? Our task is most similar to the fully-
automatic baseline runs of the track, which typically
took the form of passage retrieval with query ex-
pansion (Oard et al, 2006) or synonym processing
(Katz et al, 2006), and not the deeper processing
employed in this work.
Within the broader QA task, the other question
type is closest to the requirements in GALE, but it
is too open ended. In TREC, the input for other
questions is the name or description of the target,
and the response is supposed to be all information
that did not fit in the answers to the previous ques-
tions. While a few GALE questions have similar in-
put, most, including the prosecution questions, pro-
vide more detail about the topic in question.
A number of systems have used techniques in-
spired by information extraction. One of the top sys-
tems in the other questions category at the 2004 and
2005 evaluations generated lexical-syntactic pat-
terns and semantic patterns (Schone et al, 2004).
But they build these patterns from the question. In
our task, we took advantage of the structured ques-
tion format to make use of extensive work on the
semantics of selected domains. In this way we
hope to determine whether we can obtain better per-
formance by adding more sophisticated knowledge
about these domains. The Language Computer Cor-
poration (LCC) has long experimented with incorpo-
rating information extraction techniques. Recently,
in its system for the other type questions at TREC
2005, LCC developed search patterns for 33 target
classes (Harabagiu et al, 2005). These patterns were
learned with features from WordNet, stemming and
named entity recognition.
More and more systems are exploiting the size
and redundancy of the Web to help find answers.
Some obtain answers from the Web and then
project the answer back to the test corpus to find
a supporting document (Voorhees and Dang, 2005).
LCC used ?web boosting features? to add to key
words (Harabagiu et al, 2005). Rather than go to
the Web and enhance the question terms, we made
a beginning at examining the corpus for specific bits
of information, in this prototype, to determine alter-
native realizations of names.
3 Implementation
As stated above, the system takes a query in the
XML format required by the GALE program. The
query templates allow users to amplify their requests
by specifying a timeframe for the information and/or
a locale. In addition, there are provisions for en-
tering synonyms or alternate terms for either of the
main arguments, i.e. the accused and the crime, and
for related but less important terms.
Since this system is a prototype written especially
for the GALE evaluation in July 2006, we paid close
attention to the way example questions were given,
as well as to the evaluation corpus, which consisted
of more than 600,000 short news articles. The goal
in GALE was to offer comprehensive results to the
user, providing all snippets, or segments of texts,
that responded to the information request. This re-
quired us to develop a strategy that balanced pre-
cision against recall. A system that reported only
high-confidence answers was in danger of having no
answers or far fewer answers than other systems,
while a system that allowed lower confidence an-
swers risked producing answers with a great deal of
irrelevant material. Another way to look at this bal-
ancing act was that it was necessary for a system to
know when to quit. For this reason, we sought to
obtain a good estimate of the number of documents
we wanted to scan for answers.
Answer selection focused first on the name of the
suspect, which was always given in the query tem-
plate. In many of the training cases, the suspect was
in the news only because of a criminal charge against
him; and in most, the charge specified was the only
accusation reported in the news. Both location and
534
date constraints seemed to be largely superfluous,
and so we ignored these. But we did have a mecha-
nism for obtaining supplementary answers keyed to
the brief description of the crime and other related
words
The first step in the process is to request a seed
collection of 10 documents from the IR system.
This number was established experimentally. The
IR query combines terms tailored to the prosecution
template and the specific template parameters for a
particular question. The 10 documents returned are
then examined to produce a list of name variations
that substantially match the name as rendered in the
query template. The IR system is then asked for the
number of times that the name appears in the cor-
pus. This figure is adjusted by the frequency per
document in the seed collection and a new query is
submitted, set to obtain the N documents in which
we expect to find the target?s name.
3.1 Information Retrieval
The goal of the information retrieval component of
the system was to locate relevant documents that the
summarization system could then use to construct an
answer. All search, whether high-precision or high-
recall, was performed using the Indri retrieval sys-
tem 3 (Strohman et al, 2005).
Indri provides a powerful query language that
is used here to combine numerous aspects of the
query. The Indri query regarding Saddam Hus-
sein?s prosecution for crimes against humanity in-
cludes the following components: source restric-
tions, prosecution-related words, mentions of Sad-
dam Hussein, justice events, dependence model
phrases (Metzler and Croft, 2005) regarding the
crime, and a location constraint.
The first part of the query located references to
prosecutions by looking for the keywords prosecu-
tion, defense, trial, sentence, crime, guilty, or ac-
cuse, all of which were determined on training data
to occur in descriptions of prosecutions. These
words were important to have in documents for them
to be considered relevant, but the individual?s name
and the description of the crime were far more im-
portant (by a factor of almost 19 to 1).
The more heavily weighted part of the query,
3http://lemurproject.org/indri
then, was a ?justice event? marker found using in-
formation extraction (Section 3.2) and the more de-
tailed description of that event based on phrases ex-
tracted from the crime (here crimes against human-
ity). Those phrases give more probability of rele-
vance to documents that use more terms from the
crime. It also included a location constraint (here,
Iraq) that boosted documents referring to that lo-
cation. And it captured user-provided equivalent
words such as Saddam Hussein being a synonym for
former President of Iraq.
The most complex part of the query handled ref-
erences to the individual. The extraction system had
annotated all person names throughout the corpus.
We used the IR system to index all names across
all documents and used Indri to retrieve any name
forms that matched the individual. As a result, we
were able to find references to Saddam, Hussein,
and so on. This task could have also been accom-
plished with cross-document coreference technol-
ogy but our approach appeared to compensate for
incorrectly translated names slightly better than the
coreference system we had available at the time. For
example, Present rust Hussein was one odd form
that was matched by our simple approach.The final query looked like the following:
#filreq( #syn( #1(AFA).source ... #1(XIE).source )
#weight(
0.05 #combine( prosecution defense trial sentence
crime guilty accuse )
0.95 #combine(
#any:justice
#weight(1.0 #combine(humanity against crimes)
1.0 #combine(
#1(against humanity)
#1(crimes against)
#1(crimes against humanity))
1.0 #combine
#uw8(against humanity)
#uw8(crimes humanity)
#uw8(crimes against)
#uw12(crimes against humanity)))
Iraq
#syn( #1(saddam hussein)
#1(former president iraq))
#syn( #equals( entity 126180 ) ...))))
The actual query is much longer because it con-
tains 100 possible entities and numerous sources.
The processing is described in more detail else-
where (Kumaran and Allan, 2007).
3.2 Information Extraction
The Proteus system produces the full range of anno-
tations as specified for the ACE 2005 evaluation, in-
cluding entities, values, time expressions, relations,
535
and events. We focus here on the two annotations,
entities and events, most relevant to our question-
answering task. The general performance on entity
and event detection in news articles is within a few
percentage points of the top-ranking systems from
the evaluation.
The extraction engine identifies seven semantic
classes of entities mentioned in a document, of
which the most frequent are persons, organizations,
and GPE?s (geo-political entities ? roughly, regions
with a government). Each entity will have one or
more mentions in the document; these mentions in-
clude names, nouns and noun phrases, and pro-
nouns. Text processing begins with an HMM-based
named entity tagger, which identifies and classifies
the names in the document. Nominal and pronomi-
nal mentions are identified either with a chunker or
a full Penn-Treebank parser. A rule-based coref-
erence component identifies coreference relations,
forming entities from the mentions. Finally, a se-
mantic classifier assigns a class to each entity based
on the type of the first named mention (if the entity
includes a named mention) or the head of the first
nominal mention (using statistics gathered from the
ACE training corpus).
The ACE annotation guidelines specify 33 differ-
ent event subtypes, organized into 8 major types.
One of the major types is justice events, which in-
clude arrest, charge, trial, appeal, acquit, convict,
sentence, fine, execute, release, pardon, sue, and ex-
tradite subtypes. In parallel to entities, the event
tagger first identifies individual event mentions and
then uses event coreference to form events. For the
ACE evaluation, an annotated corpus of approxi-
mately 300,000 words is used to train the event tag-
ger.
For each event mention in the corpus, we collect
the trigger word (the main word indicating the event)
and a pattern recording the path from the trigger
to each event argument. These paths are recorded
in two forms: as the sequence of heads of maxi-
mal constituents between the trigger and the argu-
ment, and as the sequence of predicate-argument re-
lations connecting the trigger to the argument4 . In
4These predicate argument relations are based on a repre-
sentation called GLARF (Grammatical-Logical Argument Rep-
resentation Framework), which incorporates deep syntactic re-
lations and the argument roles from PropBank and NomBank.
addition, a set of maximum-entropy classifiers are
trained: to distinguish events from non-events, to
classify events by type and subtype, to distinguish
arguments from non-arguments, and to classify ar-
guments by argument role. In tagging new data, we
first match the context of each instance of a trig-
ger word against the collected patterns, thus iden-
tifying some arguments. The argument classifier is
then used to collect additional arguments within the
sentence. Finally, the event classifier (which uses
the proposed arguments as features) is used to re-
ject unlikely events. The patterns provide somewhat
more precise matching, while the argument classi-
fiers improve recall, yielding a tagger with better
performance than either strategy separately.
3.3 Answer Generation
Once the final batch of documents is received,
the answer generator module selects candidate pas-
sages. The names, with alternate renderings, are lo-
cated through the entity mentions by the IE system.
All sentences that contain a justice event and that
fall within a mention of a target by no more than
n sentences, where n is a settable parameter, which
was put at 5 for this evaluation, form the core of the
system?s answer.
The tactic takes the place of topic segmentation,
which we used for other question types in GALE
that did not have the benefit of the sophisticated
event recognition offered by the IE system. Segmen-
tation is used to give users sufficient context in the
answer without needing a means of identifying dif-
ficult definite nominal resolution cases that are not
handled by extraction.
In order to increase recall, in keeping with the
need for a comprehensive answer in the GALE eval-
uation, we added sentences that contain the name of
the target in documents that have justice events and
sentences that contain words describing the crime.
However, we imposed a limitation on the growth of
the answer size. When the target individual is well-
known, he or she will be mentioned in numerous
contexts, reducing the likelihood that this additional
mention will be relevant. Thus, when the size of the
answer grew too rapidly, we stopped including these
additional sentences, and produced sentences only
from the justice events. The threshold for triggering
this shift was 200 sentences.
536
3.4 Summarization
As a state-of-the-art baseline, we used a generic
multidocument summarization system that has been
tested in numerous contexts. It is, indeed, the
backup answer generator for several question types,
including the prosecution questions, in our GALE
system, and has been been tested in the topic-based
tasks of the 2005 and 2006 Document Understand-
ing Conferences.
A topic statement is formed by collapsing the
template arguments into one list, e.g., ?saddam hus-
sein crimes against humanity prosecution?, and the
answer generation module proceeds by using a hy-
brid approach that combines top-down strategies
based on syntactic patterns, alongside a suite of
summarization methods which guide content in a
bottom-up manner that clusters and combines the
candidate sentences (Blair-Goldensohn and McKe-
own, 2006).
4 Evaluation
The results of our evaluation are shown in Table 1.
We increased the number of test questions over the
number used in the official GALE evaluation and we
used only previously unseen questions. Documents
for the baseline system were selected without use of
the event annotations from Proteus.
We paired the 25 questions for judges, so that both
the system?s answer and the baseline answer were
assigned to the same person. We provided explicit
instructions on the handling on implicit references,
allowing the judges to use the context of the ques-
tion and other answer sentences to determine if a
sentence was relevant ? following the practice of the
GALE evaluation.
Our judges were randomly assigned questions
and asked whether the snippets, which in our case
were individual sentences, were relevant or not;
they could respond Relevant, Not Relevant or Don?t
Know. In cases where references were unclear, the
judges were asked to choose Don?t Know and these
were removed from the scoring.5
5In the GALE evaluation, the snippets are broken down by
hand into nuggets ? discrete pieces of information ? and the
answers are scored on that basis. However, we scored our re-
sponses on the basis of snippets (sentences) only, as it is much
more efficient, and therefore more feasible to repeat in the fu-
ture.
Our system using IE event detection and en-
tity tracking outperformed the summarization-based
baseline, with average precision of 68% compared
with 57%. Moreover, the specialized system sus-
tained that level of precision although it returned a
much larger number of snippets, totaling 2,086 over
the 25 questions, compared with 363 for the base-
line system. We computed a relative recall score, us-
ing the union of the sentences found by the systems
and judged relevant as the ground truth. For recall,
the specialized system scored an average 89% ver-
sus 17% for the baseline system. Computing an F-
measure weighting precision and recall equally, the
specialized system outperformed the baseline sys-
tem 75% to 23%. The difference in relative recall
and F-measure are both statisticaly significant under
a two-tailed, paired t-test, with p < 0.001.
5 Conclusion and Future Work
Our results show that the specialized system statis-
tically outperforms the baseline, a well-tested query
focused summarization approach, on precision. The
specialized system produced a much larger answer
on average (Table 1). Moreover, our answer gener-
ator seemed to adapt well to information in the cor-
pus. Of the six cases where it returned fewer than
10 sentences, the baseline found no additional sen-
tences four times (Questions B006, B011, B015 and
B022). We regard this as an important property in
the question-answering task.
A major challenge is to ascertain whether the
mention of the target is indeed involved in the rec-
ognized justice event. Our event recognition system
was developed within the ACE program and only
seeks to assigns roles within the local context of a
single sentence. We currently use a threshold to con-
sider whether an entity mention is reliable, but we
will experiment with ways to measure the likelihood
that a particular sentence is about the prosecution or
some other issue. We are planning to obtain vari-
ous pieces of information from additional secondary
queries to the search engine. Within the GALE pro-
gram, we are limited to the defined corpus, but in the
general case, we could add more varied resources.
In addition, we are working to produce answers
using text generation, to bring more sophisticated
summarization techniques to make a better presen-
537
QID System with IE Baseline System
Precision Recall F-meas Count Precision Recall F-meas Count
B001 0.728 0.905 0.807 92 0.818 0.122 0.212 11
B002 0.713 0.906 0.798 108 0.889 0.188 0.311 18
B003 0.770 0.942 0.848 148 0.875 0.058 0.109 8
B004 0.930 0.879 0.904 86 1.000 0.154 0.267 14
B005 0.706 0.923 0.800 34 0.400 0.231 0.293 15
B006 1.000 1.000 1.000 3 0.000 0.000 0.000 17
B007 0.507 1.000 0.673 73 0.421 0.216 0.286 19
B008 0.791 0.909 0.846 201 0.889 0.091 0.166 18
B009 0.759 0.960 0.848 158 0.941 0.128 0.225 17
B010 1.000 0.828 0.906 24 0.500 0.276 0.356 16
B011 0.500 1.000 0.667 6 0.000 0.000 0.000 18
B012 0.338 0.714 0.459 74 0.765 0.371 0.500 17
B013 0.375 0.900 0.529 120 0.700 0.280 0.400 20
B014 0.571 0.800 0.667 7 0.062 0.200 0.095 16
B015 0.500 1.000 0.667 2 0.000 0.000 0.000 10
B016 1.000 0.500 0.667 5 0.375 0.600 0.462 16
B017 1.000 1.000 1.000 13 0.125 0.077 0.095 7
B018 0.724 0.993 0.837 199 0.875 0.048 0.092 8
B019 0.617 0.954 0.749 201 0.684 0.100 0.174 19
B020 0.923 0.727 0.814 26 0.800 0.364 0.500 15
B021 0.562 0.968 0.711 162 0.818 0.096 0.171 11
B022 0.667 1.000 0.800 6 0.000 0.000 0.000 18
B023 0.684 0.950 0.795 196 0.778 0.050 0.093 9
B024 0.117 0.636 0.197 60 0.714 0.455 0.556 7
B025 0.610 0.943 0.741 82 0.722 0.245 0.366 18
Aver 0.684 0.893 0.749 83 0.566 0.174 0.229 14
Table 1: The table compares results of our answer generator combining the Indri and the Proteus ACE sys-
tem, against the focused-summarization baseline. This experiment is over 25 previously unseen questions.
The differences between the two systems are statistically significant (p < 0.001) for recall and f-measure by
a two-tailed, paired t-test. A big difference between the two systems is that the answer generator produces
a total of 2,086 answer sentences while sustaining an average precision of 0.684. In only three cases, does
the precision fall below 0.5. In contrast, the baseline system produced only 362, one-sixth the number of
answer sentences. While its average precision was not significantly worse than the answer-generator?s, its
precision varied widely, failing to find any correct sentences four times.
538
tation than an unordered list of sentences.
Finally, we will look into applying the techniques
used here on other topics. The first test would rea-
sonably be Conflict events, for which the ACE pro-
gram has training data. But ultimately, we would
like to adapt our system to arbitrary topic areas.
Acknowledgements
This material is based in part upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
Sasha Blair-Goldensohn and Kathleen McKeown. 2006.Integrating rhetorical-semantic relation models forquery-focused summarization. In Proceedings of 6th
Document Understanding Conference (DUC2006).
Hoa Trang Dang, Jimmy Lin, and Diane Kelly. 2006.Overview of the TREC 2006 question answering track.In Proceedings TREC. Forthcoming.
Ralph Grishman, David Westbrook, and Adam Meyers.2005. NYU?s english ACE 2005 system descrip-tion. In ACE 05 Evaluation Workshop. On-line athttp://nlp.cs.nyu.edu/publication.
Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005. Employing two question answering systems in
TREC 2005. In Proceedings of the Fourteenth Text
Retrieval Conference.
B. Katz, G. Marton, G. Borchardt, A. Brownell,S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox.2006. External knowledge sources for question an-swering. In Proceedings of TREC. On-line at
http://www.trec.nist.gov.
Giridhar Kumaran and James Allan. 2007. Informationretrieval techniques for templated queries. In Proceed-
ings of RIAO. Forthcoming.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. In Proceedings of
ACM SIGIR, pages 472?479.
D. Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels,
J. Lin, and D. Soergel. 2006. Trec 2006 at maryland:Blog, enterprise, legal and QA tracks. In Proceedings
of TREC. On-line at http://www.trec.nist.gov.
Patrick Schone, Gary Ciany, Paul McNamee, James
Mayeld, Tina Bassi, and Anita Kulman. 2004. Ques-tion answering with QACTIS at TREC-2004. In Pro-
ceedings of the Thirteenth Text Retrieval Conference.
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language-model based search enginefor complex queries (extended version). Technical Re-port IR-407, CIIR, UMass Amherst.
Ellen M. Voorhees and Hoa Trang Dang. 2005.
Overview of the TREC 2005 question answering track.In Proceedings of the Fourteenth Text Retrieval Con-
ference.
539
Using Soundex Codes for Indexing Names in ASR documents
Hema Raghavan
hema@cs.umass.edu
James Allan
allan@cs.umass.edu
Abstract
In this paper we highlight the problems that
arise due to variations of spellings of names
that occur in text, as a result of which links be-
tween two pieces of text where the same name
is spelt differently may be missed. The problem
is particularly pronounced in the case of ASR
text. We propose the use of approximate string
matching techniques to normalize names in or-
der to overcome the problem. We show how we
could achieve an improvement if we could tag
names with reasonable accuracy in ASR.
1 Introduction
Proper names are often key to our understanding of the
information conveyed by a document. This is particu-
larly the case when the domain is news. For example, a
document with several mentions of George W. Bush, Dick
Cheney, Baghdad and Saddam Hussein, gives us a good
sense of what the contents of the document may be. In
comparison, other regular English words like death, scud
and missiles, may be good indicators of more general top-
ics like war, but may not give us any indication of the
exact event being discussed. Linking stories that discuss
the same event, like the Attack on Iraq is very useful for
a news filtering systems. When topics are primarily de-
termined by specific events, it is easy to see why names
of entities- people places and organizations, play such a
critical role in discriminating between events that discuss
a topic.
However, when one considers a real life scenario
where news is from different media (print and broad-
cast) and in many different languages, proper names
pose many different problems. The problem with proper
names is that they often have different spelling variations.
For example, the names Arafat, Araafat, and Arafaat may
all refer to the same entity. Human beings can also vary
in their spellings of a named entity. Besides that, the out-
put of ASR and Machine Translation systems can also re-
sult in different spelling variations of a name. Such slight
spelling variations may be acceptable and discernible by
humans, but for a machine they are harder to match. A
user who issues a query with the term Arafat in it may
never find a document that discusses Araafat, using cur-
rent TF-IDF matching techniques, even though the docu-
ment may be pertinent to his or her query. Although this
loss may not be critical to some applications, one cannot
assume that the problem does not exist. The problem has
been addressed by the data-base community in the past by
the use of approximate string matching techniques, but in
pure-text, we have the added problem of detecting names.
In this paper, we demonstrate with examples how
sometimes we may not be able to draw connections be-
tween two pieces of text without the use of approximate
string matching techniques. We indicate the problems we
encounter while detecting names, and propose ways to
address those issues. In the discussion of previous work
in the next section we describe some tasks that use ASR
output, and which may have been benefited by the use
of approximate string matching techniques. We describe
some preliminary experiments and their results. We then
discuss the bottlenecks, in the proposed methodology,
and how they may be overcome.
2 Past Work
2.1 Stemming
Stemming (Porter, 1980; Krovetz, 1993) is a method in
which the corpus is processed so that semantically and
morphologically related words are reduced to a common
stem. Thus, race, racing, and racer are all reduced to a
single root ? race. Stemming has been found to be ef-
fective for Information Retrieval, TDT and other related
tasks. Current stemming algorithms work only for regu-
lar English words and not names. In this paper we look
at addressing the problem of grouping together and nor-
malizing proper names in the same way that stemming
groups together regular English words.
2.2 Approximate String Matching
There has been some past work (French et al, 1997; Zo-
bel and Dart, 1996) that has addressed the problem that
proper names can have different spellings. Each of those
works, however, only addresses the question of how ef-
fectively one can match a name to its spelling variants.
They measure their performance in terms of the preci-
sion and recall with which they are able to retrieve other
names which are variants of a given query name. Essen-
tially, the primary motivation of those works was in find-
ing good approximate string matching techniques. Those
techniques are directly applicable only in applications
that retrieve tuples from a database record.
However, there is no work that evaluates the effec-
tiveness of approximate string matching techniques for
names in an information retrieval or related task. We
know of no work that attempts to detect names automati-
cally, and then index names that should go together, in the
same way that words of the same stem class are indexed
by one common term.
2.3 The TREC SDR and the TDT Link Detection
tasks
A single news-source may spell all mentions of a given
name identically. However, this consistency is lost when
there are multiple sources of news, where sources span
languages and modes (broadcast and print). The TDT3
corpus (ldc, 2003) is representative of such real-life data.
The corpus consists of English, Arabic and Mandarin
print and broadcast news. ASR output is used in the case
of the broadcast sources and in the case of non-English
stories machine translated output is used for comparing
stories. For both ASR systems and Machine Transla-
tion systems, proper names are often out-of-vocabulary
(OOV). A typical speech recognizer has a lexicon of
about 60K, and for a lexicon of this size about 10% of
the person names are OOV. The OOV problem is usually
solved by the use of transliteration and other such tech-
niques. A breakdown of the OOV rates for names for
different lexicon sizes is given in (Miller et al, 2000).
We believe the problem of spelling errors is of impor-
tance when one wants to index and retrieve ASR docu-
ments. For example, Monica Lewinsky is commonly re-
ferred to in the TDT3 corpus. The corpus has closed- cap-
tion transcripts for TV broadcasts. Closed caption suf-
fers from typing errors. The name Lewinsky is also often
misspelt as Lewinskey in the closed caption text. In the
ASR text some of the variants that appear are Lewenskey,
Linski, Lansky and Lewinsky. This example is typical,
with the errors in the closed caption text highlighting how
humans themselves can vary in their spelling of a name
and the errors in ASR demonstrating how a single ASR
system can output different spellings for the same name.
The ASR errors are largely because ASR systems rely
on phonemes for OOV words, and each of the different
variations in the spellings of the same name is probably
a result of different pronounciations and other such fac-
tors. The result of an ASR system then, is several dif-
ferent spelling variations of each name. It is easy to see
why it would help considerably to group names that refer
to the same entity together, and index them as one en-
tity. We can exploit the fact that these different spelling
variations of a given name exhibit strong similarity us-
ing approximate string matching techniques. We propose
that in certain domains, where the issue that proper names
exist with many different variations is dominant, the use
of approximate string matching techniques to determine
which names refer to the same entity will help improve
the accuracy with which we can detect links between sto-
ries. Figure 1 shows a snippet of closed caption text and
its ASR counterpart. The names Lewinskey and Tripp
are misspelt in the ASR text. The two documents how-
ever have high similarity, because of the other words that
the ASR system gets right. Allan (Allan, 2002) showed
how ASR errors can cause misses in TDT tasks, and can
sometimes be beneficial, resulting in a minimal average
impact on performance in TDT. In the case of Spoken
Document Retrieval (Garofolo et al, 2000) also it was
found that a few ASR errors per document did not re-
sult in a big difference to performance as long as we get
a reasonable percentage of the words right. Of course,
factors such as the length of the two pieces of text being
compared make a difference. Barnett et al(Barnett et al,
1997), showed how short queries were affected consid-
erably by Word Error rate. ASR errors may not cause a
significant drop in performance for any of the Topic De-
tection and Tracking tasks. But, consider a system where
retrieving all documents mentioning Lewinskey and Tripp
is critical, and it is not unrealistic to assume there exist
systems with such needs, the ASR document in the above
mentioned example would be left out. We therefore, be-
lieve that the problem we are addressing in this paper is
an important one. The preliminary experiments in this
paper, which are on the TDT corpus, only highlight how
our approach can help.
3 Story Link Detection
3.1 Task Definition
The Story Link Detection Task is key to all the other tasks
in TDT. The system is handed a set of story pairs, and
for each pair it is asked to judge whether both the stories
discuss the same topic or different topics. In addition to
a YES/NO decision the system is also expected to output
a confidence score, where a low confidence score implies
that the system is more in favor of the NO decision.
3.2 Our Approach
Simply stated our approach to the SLD task, is to use ap-
proximate string matching techniques to compare entities
between two pieces of text. The two pieces of text may be
a query and a document, or two documents, depending on
the task. We first need to identify entities in the two doc-
uments. There exist several techniques to automatically
identify names. For properly punctuated text, heuristics
like capitalization work sufficiently well. However, for
ASR text we often do not have sentence boundaries or
even punctuation. Hence we rely on a Hidden Markov
Model based named entity recognizer (Bikel et al, 1999)
for our task.
A simple strategy that incorporates an approximate
string matching technique is to first preprocess the cor-
pus, and then normalize all mentions of a named entity
to a given canonical form, where the canonical form is
independent of mentions of other entities in the two doc-
uments being compared. Soundex, Phonix, and other
such codes offer us a means of normalizing a word to
its phonetic form. The Soundex code is a combination of
the first letter of the word and a three digit code which
is representative of its phonetic sound. Hence, similar
sounding names like ?Lewinskey? and ?Lewinsky? are
both reduced to the same soundex code ?l520?. We can
pre-process the corpus so that all the named entities are
replaced by their Soundex codes. We then compute the
similarity between documents in the new corpus as op-
posed to the old one, using conventional similarity met-
rics like Cosine or TF-IDF.
4 Experimental Set up
4.1 Data
The corpus (ldc, 2003) has 67111 documents from mul-
tiple sources of news in multiple languages (English
Chinese and Arabic) and media (broadcast news and
newswire). The English sources are Associated Press and
New York Times, PRI, Voice of America etc. For the
broadcast news sources we have ASR output and for TV
we have both ASR output as well as closed caption data.
Additionally we have the following Mandarin news-wire,
web and broadcast sources - Xinhua news, Zaobao, and
Voice of America (Mandarin). For all the Mandarin doc-
uments we have the original documents in the native lan-
guage as well the English output of Systran- a machine
translation system. The data has been collected by LDC
by sampling from the above mentioned sources in the pe-
riod from October to December 1998.
The LDC has annotated 60 topics in the TDT3 corpus.
A topic is determined by an event. For example topic
30001 is the Cambodian Government Coalition. Each
topic has key entities associated with it and a description
of the topic. A subset of the documents are annotated as
being on-topic or not according to a well formed strategy
as defined by the LDC.
4.2 Story Link Detection
To compute the similarity of two documents, that is, the
YES/NO decision threshold, we used the the traditional
cosine similarity metric. To give some leverage to doc-
uments that were very similar even before named entity
normalization, we average the similarity scores between
documents before and after the named entities have been
normalized by their Soundex codes as follows:
Sim(D
1
; D
2
) =
1
2
(Cos(D
1
; D
2
) + Cos(D
0
1
; D
0
2
)) (1)
Where D
1
and D
2
are the original documents and D0
1
and D0
2
are the documents after the names have been nor-
malized.
4.3 Evaluation
An ROC curve is plotted by making a parameter sweep of
the YES/NO decision thresholds, and plotting the Misses
and False Alarms at each point. At each point the cost
is computed using the following empirically determined
formula (Fiscus et al, 1998).
C
det
= 0:02P (miss) + 0:098P (fa) (2)
This cost function is standard across all tasks. The point
of minimum cost serves as the comparison between vari-
ous systems.
5 Results
We tested our idea on the TDT3 corpus for the Story Link
Detection Task, using the Cosine similarity metric, and
found that performance actually degraded. On investiga-
tion we found that the named entity recognizer performs
poorly on Machine Translated and ASR source data. Our
named entity recognizer relies considerably on sentence
structure, to make its predictions. Machine translated out-
put often lacks grammatical structure, and ASR output
does not have punctuation, which results in a lot of named
entity tagging errors.
We therefore decided to test our idea for newswire text.
We created our own test set of 4752 pairs of stories from
newswire sources. This test set was created by randomly
picking on and off-topic stories for each topic using the
same policy as employed by the LDC (Fiscus, 2003). On
these pairs, we obtained about 10% improvement (Fig-
ure 2), suggesting that there is merit in Soundex normal-
ization of names. However, the problem of poor named
entity recognition is a bottle-neck for ASR. We discuss
1
2
5
10
20
40
60
80
90
.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
False Alarms probability (in %)
SLD using soundex codes on Newswire-Newswire pairs
Random Performance
Using Soundex
TW Min DET Norm(Cost) = 0.1588
Baseline
TW Min DET Norm(Cost) = 0.1709
Figure 1: Story Link Detection performance
alternative strategies of how to deal with this, and other
ways of using approximate string matching in the next
section.
6 Alternative strategies
6.1 To not use an entity recognizer
We were not able to benefit from our approach on the
ASR documents because of the poor performance of the
named entity recognizer on those types of document.
An example of a randomly picked named entity tagged
ASR document is given below. The tagging errors are
underlined.
< DOC >
< DOCNO > CNN19981001:0130:0000 < /DOCNO >
< TEXT >
< ENAMEX TYPE=?ORGANIZATION? >
BUDGET SURPLUS < /ENAMEX> AND FIGHTING
OVER WHETHER IT?S GOING DOOR POCKETS WILL
TELL YOU THE < ENAMEX TYPE=?ORGANIZATION?
> VEHICLES CLIMBED DATES THEREAFTER <
/ENAMEX > AND IF YOU?RE REQUIRED TO PAY
CHILD SUPPORT INFORMATION THAT YOUR
JOB AND COME AND ADDRESS NOW PART
HAVE < ENAMEX TYPE=?ORGANIZATION? >
A NATIONAL REGISTRY THE HEADLINE < /ENAMEX
> NEWS I?M < ENAMEX TYPE=?PERSON?>KIMBERLY
KENNEDY </ENAMEX> THOSE STORIES IN A MO-
MENT BUT FIRST </TEXT></DOC >
We need a better performing recognizer, but that may
be hard. Instead we might be able to use other informa-
tion from the speech recognizer to overcome this prob-
lem. We did not have confidence scores for the words in
the ASR output. If we had had that information, or if we
were able to obtain information about which words were
OOV, we could possibly index all words with low confi-
dence scores or all OOV words by their Soundex codes.
Or else, one could normalize all words in the ASR out-
put, that are not part of the regular English vocabulary by
their Soundex codes.
6.2 Other ways of grouping entities
Another direction of research to pursue is the way in
which approximate string matching is used to compare
documents. The way we used approximate string match-
ing in this paper was fairly simple. However, it loses
out on some names that ought to go together particularly
when two names differ in their first alphabet - for example
Katherine and Catherine. The Soundex codes are k365
and c365 respectively. This is by virtue of the nature of
the Soundex code of word.
There are other ways to compute the similarity be-
tween two documents like the Levenshtein distance or
edit distance which is a measure of the number of string
edit operations required to convert one string to the other.
The words Katherine and Catherine have an edit distance
of 1. Given two documents D
1
and D
2
, we can compute
the distance between them by computing the distance be-
tween all pairs of names that occur in the two documents,
and using the distances to group entities and finally to find
the similarity of the two documents. However this means
that each entity in D
1
has to be compared to all entities in
D
1
and D
2
. Besides, this method brings with it the ques-
tion of how to use the distances between the names so as
to group together similar names. This method is probably
a good direction for future research, because the Leven-
shtein distance could possibly be a better string matching
technique. Another plausible strategy would be to use the
edit-distance of the Soundex codes of the names, when
comparing documents. Katherine and Catherine would
have a distance of 1 in this case too.
Using cross document coreference resolution tech-
niques to find equivalence classes of entities would be yet
another alternative approach. In Cross document corefer-
ence, two mentions of the same name, may or may not be
included in the same group depending on whether or not
the context of the two mentions is the same or is different.
7 Conclusions and Future Directions
In this paper we highlighted an important problem that
occurs with names in ASR text. We showed how a name
may be spelt differently by humans. In ASR the same
name had many more different spellings.
We proposed a simple indexing strategy for names,
wherein a name was indexed by its Soundex code. We
found that our strategy did not work for ASR, but the
problem was not with the approach, but because we could
not do a good job of identifying names in ASR text.If
we could detect names with reasonable accuracy in ASR
text we should be able to achieve reasonable improve-
ment. We did not have a named entity recognizer that
performed well on ASR text. We therefore verified our
idea on news-wire text, which is grammatical, well punc-
tuated text. In the news-wire domain, in spite of there be-
ing reasonable consistency in spellings of names, we get
about 10% improvement in minimum cost, and a consis-
tent improvement at all points in the ROC curve. Hence,
a simple technique like Soundex served as a useful nor-
malization technique for names. We proposed alternative
mechanisms that could be applied to ASR text, wherein
all OOV words could be normalized by their Soundex
codes. We also outlined further directions for research in
the way that approximate string matching may be used.
We think the general results of past works that has con-
sidered the problems due to ASR errors to be insignificant
cannot be assumed to transfer across to other problems.
There will arise situations when this problem is material
and research needs to be done in this direction.
8 Acknowledgements
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903. Any opinions, findings and conclusions or recom-
mendations expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
James Allan. 2002. Detecting and tracking topics in
broadcast news,att speechdays 2002.
James Barnett, Steve Anderson, John Broglio, Mona
Singh, R. Hudson, and S. W. Kuo. 1997. Experiments
in spoken queries for document retrieval. In Proc. Eu-
rospeech ?97, pages 1323?1326, Rhodes, Greece.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
J. Fiscus, G. Doddington, J. Garofolo, and A. Martin.
1998. Nist?s 1998 topic detection and tracking eval-
uation.
John Fiscus. 2003. Personal communication.
J. C. French, A. L. Powell, and E. Schulman. 1997. Ap-
plications of approximate word matching in informa-
tion retrieval. In Proceedings of the Sixth International
Conference on Knowledge and Information Manage-
ment, pages 9?15, New York, NY. ACM Press.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
trec spoken document retrieval track: A success story.
R. Krovetz. 1993. Viewing Morphology as an Inference
Process,. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 191?203.
2003. http://www.ldc.upenn.edu/tdt/.
David Miller, Richard Schwartz, Ralph Weischedel, and
Rebecca Stone. 2000. Named entity extraction from
broadcast news.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program.
J. Zobel and P. W. Dart. 1996. Phonetic string match-
ing: Lessons from information retrieval. In H.-P. Frei,
D. Harman, P. Scha?ble, and R. Wilkinson, editors,
Proceedings of the 19th International Conference on
Research and Development in Information Retrieval,
pages 166?172, Zurich, Switzerland. ACM Press.
