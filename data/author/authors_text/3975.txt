Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 417?424
Manchester, August 2008
A Local Alignment Kernel in the Context of NLP
Sophia Katrenko
Informatics Institute
University of Amsterdam
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
Informatics Institute
University of Amsterdam
the Netherlands
pietera@science.uva.nl
Abstract
This paper discusses local alignment ker-
nels in the context of the relation extrac-
tion task. We define a local alignment
kernel based on the Smith-Waterman mea-
sure as a sequence similarity metric and
proceed with a range of possibilities for
computing a similarity between elements
of sequences. We propose to use distri-
butional similarity measures on elements
and by doing so we are able to incorporate
extra information from the unlabeled data
into a learning task. Our experiments sug-
gest that a LA kernel provides promising
results on some biomedical corpora largely
outperforming a baseline.
1 Introduction
Relation extraction is one of the tasks in the
natural language processing which is constantly
revisited. To date, there are many methods
which have been proposed to tackle it. Such ap-
proaches often benefit from using syntactic infor-
mation (Bunescu and Mooney, 2006) and back-
ground knowledge (Sekimizu et al, 1998). How-
ever, it would be interesting to employ additional
information not necessarily contained in the train-
ing set. This paper presents a contribution to the
work on relation extraction by combining statisti-
cal information with string distance measures. In
particular, we propose to use a local alignment ker-
nel to detect relations.
The paper is organized as follows. We start with
the definition of a local alignment kernel and show
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
how it is defined on the Smith-Waterman measure.
We proceed by discussing how a substitution ma-
trix can be constructed in the context of natural
language processing tasks. Once a method is de-
scribed, we turn to the task of relation extraction
and present an experimental part. We conclude by
mentioning possible future directions.
2 A Local Aligment Kernel
Kernel methods are widely used for a variety of
natural language processing task, starting from
PoS tagging to information extraction. Many of
the approaches employ the idea of combining ker-
nels together which leads to a convolution kernel
(Haussler, 1999). The examples of convolution
methods being successfully used in NLP are ker-
nels based on dependency trees and shallow pars-
ing (Moschitti, 2006; Zelenko et al, 2003). Local
alignment (LA) kernels also belong to the family
of convolution kernels but have not yet been ap-
plied to NLP problems.
Although the approaches listed above proved to
be accurate, they only use kernels which are de-
signed by computing inner products between vec-
tors of sequences. Intuitively, methods using more
elaborate measures of similarity could provide bet-
ter results but kernels defined on such measures
are not necessarily positive semi-definite. Recent
work in the biomedical field shows that it is pos-
sible to design valid kernels based on a similarity
measure by solving the diagonal dominance prob-
lem to ensure the semi-definiteness (Saigo et al,
2006). To illustrate it, Saigo et al (2004) con-
sider the Smith-Waterman (SW) similarity mea-
sure (Smith and Waterman, 1981) which has of-
ten been used to compare two sequences of amino
acids. The original Smith-Waterman score is cal-
culated to achieve the best local alignment allow-
417
ing gaps.
The Smith-Waterman measure belongs to the
string distance metrics which can be divided into
term-based, edit-distance and HMM based metrics
(Cohen et al, 2003). Term-based distances such
as metrics based on TF-IDF score, consider a pair
of word sequences as two sets of words neglecting
their order. In contrast, edit string distances treat
the entire sequences and, by comparing them, cal-
culate the minimal number of the transformation
operations converting a sequence x into a sequence
y. Examples of string edit distances are Leven-
shtein, Needleman-Wunsch and Smith-Waterman
metrics. Levenshtein distance has been used in
natural language processing field as a component
in the variety of tasks, including semantic role la-
beling (Tjong Kim Sang et al, 2005), construc-
tion of the paraphrase corpora (Dolan et al, 2004),
evaluation of machine translation output (Leusch
et al, 2003), and others. Smith-Waterman distance
is mostly used in the biological domain, there are,
however, some applications of a modified Smith-
Waterman distance to the text data as well (Monge
and Elkan, 1996), (Cohen et al, 2003). HMM
based measures present probabilistic extensions of
edit distances.
According to the definition of a LA kernel,
two strings (sequences) are considered similar if
they have many local alignments with high scores
(Saigo et al, 2006). Given two sequences x =
x
1
x
2
. . . x
n
and y = y
1
y
2
. . . y
m
of length n and m
respectively, Smith-Waterman distance is defined
as the local alignment score of their best align-
ment:
SW (x, y) = max
pi?A(x,y)
s(x, y, pi) (1)
In the equation above, s(x, y, pi) is a score of a
local alignment pi of sequence x and y and A de-
notes the set of all possible alignments. This defi-
nition can be rewritten by means of dynamic pro-
gramming as follows:
SW (i, j) = max
8
>
<
>
:
0
SW (i? 1, j ? 1) + d(x
i
, y
j
)
SW (i? 1, j)?G
SW (i, j ? 1)?G
(2)
In Equation 2, d(x
i
, y
j
) denotes a substitution
score between two elements x
i
and y
j
and G
stands for a gap penalty.
Unfortunately, the direct application of the
Smith-Waterman score will not result in the valid
kernel. A valid kernel based on the Smith-
Waterman distance can be defined by summing up
the contribution of all possible alignments as fol-
lows (Saigo et al, 2004):
K
LA
=
?
pi?A(x,y)
?
??s(x,y,pi) (3)
It is shown that in the limit a LA kernel ap-
proaches the Smith-Waterman score:
lim
???
ln
(
1
?
K
LA
(x, y)
)
= SW (x, y) (4)
The results in the biological domain suggest that
kernels based on the Smith-Waterman distance are
more relevant for the comparison of amino acids
than string kernels. It is not clear whether this
holds when applied to natural language process-
ing tasks. In our view, it depends on the param-
eters which are used, such as a substitution ma-
trix and the penalty gaps. It has been shown by
Saigo (2006) that given a substitution matrix which
is equal to the identity matrix and no penalty gap,
the Smith-Waterman score is a string kernel.
2.1 How to define a substitution matrix
d(?, ?)?
In order to use Smith-Waterman distance for our
purposes, it is necessary to define a substitution
matrix. Unlike a matrix in the original Smith-
Waterman measure defined by the similarity of
amino acids or a substitution matrix in (Monge and
Elkan, 1996) based on the exact and approximate
match of two characters (for instance, m and n),
we introduce a matrix based on the distributional
similarity measures. In our view, they are the most
natural measures for the text data. In other words,
if we are to compare any two words given two se-
quences of words, the elements sharing the same
contexts should be more similar to each other than
those that do not. In the context of the LA kernel,
such metrics can be especially useful. Consider,
for instance, the labeled sequences of words which
are used as input for a machine learning method.
To compare the sequences, we have to be able to
compare their elements, i.e. words. Now, if there
are some words in the test data that do not occur
in the training set, it is still possible to carry out a
418
comparison if additional evidence is present. Such
evidence can be provided by the distributional sim-
ilarity metrics.
There are a number of measures proposed over
the years, including such metrics as cosine, dice
coefficient, and Jaccard distance. Distributional
similarity measures have been extensively studied
in (Lee, 1999; Weeds et al, 2004).
We have chosen the following metrics: dice, co-
sine and l2 (euclidean) whose definitions are given
in Table 1. Here, x
i
and y
j
denote two words and
c stands for a context. Similarly to (Lee, 1999),
we use unsmoothed relative frequencies to derive
probability estimates P . In the definition of the
dice coefficient, F (x
i
) = {c : P (c|x
i
) > 0}.
We are mainly interested in the symmetric mea-
sures (d(x
i
, y
j
) = d(y
j
, x
i
)) because of a symmet-
ric positive semi-definite matrix required by ker-
nel methods. Consequently, such measures as the
skew divergence were excluded from the consider-
ation (Lee, 1999).
The Euclidean measure as defined in Table 1
does not necessarily vary from 0 to 1. It was there-
fore normalized by dividing an l2 score in Table 1
by a maximum score and retracting it from 1.
Measure Formula
cosine d(x
i
, y
j
) =
P
c
P (c|x
i
)?P (c|y
j
)
?
P
c
P (c|x
i
)
2
P
c
P (c|y
j
)
2
dice d(x
i
, y
j
) =
2?F (x
i
)?F (y
j
)
F (x
i
)?F (y
j
)
l2 d(x
i
, y
j
) =
p
P
c
(P (c|x
i
)? P (c|y
j
))
2
Table 1: Distributional similarity measures.
3 A relation extraction task
Many approaches to relation extraction consider
syntactic information. In this paper we focus on
dependency parsing. The experiments in the past
have already shown syntactic analysis to be useful
for relation learning. Like other work we extract
a path between two nodes which correspond to the
arguments of a binary relation. We also assume
that each analysis results in a tree and since it is an
acyclic graph, there exists only one path between
each pair of nodes. We do not consider, however,
the other structures that might be derived from the
full syntactic analysis as in, for example, subtree
kernels (Moschitti, 2006).
Consider, for instance, an example of interac-
tion among proteins (5) whose syntactic analysis
is given in Fig. 1. Here, there is a relation between
Cbf3 and three proteins, Cbf3a, Cbf3b and Cbf3c
expressed by a verb contain. We believe that this
partial information extracted from the dependency
trees should be sufficient for relation learning and
can be used as a representation for the learning
method.
(5) Cbf3 contains three proteins, Cbf3a, Cbf3b
and Cbf3c.
contains
nsubj
dobj
Cbf3 proteins
conj and
num
conj and
conj and
Cbf3a three Cbf3b Cbf3c
Figure 1: Stanford parser output
Representation: dependency paths
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3a
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3b
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3c
4 Experiments
4.1 Set-up
Data We use two corpora which both come from
the biomedical field and contain annotations of
either interacting proteins BC-PPI (1,000 sen-
tences)1 or the interactions among proteins and
genes LLL2 (77 sentences in the training set and
87 in the test set) (Ne?dellec, 2005). The BC-PPI
corpus was created by sampling sentences from the
BioCreAtive challenge, the LLL corpus was com-
posed by querying Medline with the term Bacillus
subtilis. The difference between the two corpora
lies in the directionality of interactions. The for-
mer corpus contains both symmetric and asymmet-
ric interactions while in the latter they are strictly
asymmetric. We analyzed the BC corpus with the
Stanford parser. 3 The LLL corpus has already
been preprocessed by the Link parser.
To estimate distributional similarity, we use
TREC 2006 Genomics collection (Hersch,
2006) which contains 162,259 documents from
1available from http://www2.informatik.
hu-berlin.de/?hakenber/
2available from http://genome.jouy.inra.fr/
texte/LLLchallenge/
3available from http://nlp.stanford.edu/
software/lex-parser.shtml\#Download
419
49 journals. All documents have been prepro-
cessed by removing HTML-tags, citations in the
text and reference sections and stemmed by the
Porter stemmer (van Rijsbergen et al, 1980).
Furthermore, the query-likelihood approach with
Dirichlet smoothing (Chen, 1996) is used to
retrieve document passages given a query. All
words occurring in the set of input sequences are
fed as queries. Immediate context surrounding
each pair of words is used as features to calculate
distributional similarity of these words. We set the
context window to ?2 (2 tokens to the right and 2
tokens to the left of a word in focus) and do not
perform any kind of further preprocessing such as
PoS tagging.
Recall that in Section 2.1 we defined a substi-
tution matrix solely based on the words. How-
ever, the representation we employ also contains
information on syntactic functions and directions
(Fig. 1). To take this into account, we revise
the definition of d(?, ?). We assume sequences
x = x
1
x
2
. . . x
n
and y = y
1
y
2
. . . y
m
to contain
words (x
i
?W ) and syntactic functions accompa-
nied by direction (x
i
/?W ). Then,
d
?
(x
i
, y
j
) =
8
>
>
<
>
>
:
d(x
i
, y
j
) x
i
, y
j
?W
1 x
i
, y
j
/?W & x
i
= y
j
0 x
i
, y
j
/?W & x
i
6= y
j
0 x
i
?W & y
j
/?W
0 x
i
/?W & y
j
?W
(6)
Baseline To test how well local alignment ker-
nels perform compared to the kernels proposed in
the past, we implemented a method described in
(Bunescu and Mooney, 2005) as a baseline. Here,
similarly to our approach, the shortest path be-
tween relation arguments is extracted and a ker-
nel between two sequences (paths) x and y is com-
puted as follows:
K(x, y) =
{
0 m 6= n
?
n
i=1
f(x
i
, y
i
) m = n
(7)
In Eq. 7, f(x
i
, y
i
) is the number of com-
mon features shared by x
i
and y
i
. Bunescu and
Mooney (2005) use several features such as word
(protesters), part of speech tag (NNS), gener-
alized part of speech tag (Noun), and entity type
(e.g., PERSON ) if applicable. In addition, a di-
rection feature (? or?) is employed. In our ex-
periments we also use lemma, part of speech tag
and direction but we do not consider an entity type
or negative polarity of items.
Kernels that we compute are used together with
LibSVM (Chang and Lin, 2001) to detect hyper-
planes separating positive examples from the neg-
ative ones. Before plugging all kernel matrices into
LibSVM, they were normalized as in Eq. 8.
K(x
?
, y
?
) =
K(x, y)
?
K(x, x)K(y, y)
+ 1 (8)
To compute LA matrices we use the distributed
ASCI supercomputer 3 (DAS-3) 4 which allows us
to speed up the process of sequence comparison.
In particular, because of symmetricity of the result-
ing matrices for n sequences we need to carry out
n(n?1)/2 comparisons to build a matrix. Compu-
tations are done in parallel by reserving a number
of nodes of DAS-3 and concatenating the outputs
later on.
4.2 Experiment I: Distributional measures
and their impact on the final performance
Distributional similarity measures have been used
for various tasks in the past. For instance, (Lee,
1999) employs them to detect similar nouns based
on the verb-object cooccurrence pairs. The results
suggest the Jaccard?s coefficient to be one of the
best performing measures followed by some others
including cosine. Euclidean distance fell into the
group with the largest error rates. It is of consider-
able interest to test whether these metrics have an
impact on the performance of a LA kernel. We do
not employ Jaccard?s measure but the dice coeffi-
cient is monotonic in it.
While computing a distributional similarity, it
may happen that a given word x does not occur
in the corpus. To handle such cases, we always set
d(x, x) = 1. To estimate distributional similarity,
a number of hits returned by querying the TREC
collection is set to 500. Gaps are defined through
the gaps opening and extension costs. In our ex-
periments, the gap opening cost is set to 1.2, the
extension cost to 0.2 and the scaling parameter ?
to 1.
The 10-fold cross-validation results on the
BC-PPI corpus are presented in Table 2 and on
the LLL training data set in Table 3. The LA kernel
4http://www.cs.vu.nl/das3
420
based on the distributional similarity measures per-
forms significantly better than the baseline. In con-
trast to the baseline, it is able to handle sequences
of different lengths including gaps. According to
the Eq. 7, a comparison of any two sequences of
different lengths results in the 0-score. Neverthe-
less it still yields high recall while precision is
much lower. Interestingly, the results of the short-
est path approach on the ACE corpus (Bunescu and
Mooney, 2005) were reversed by boosting preci-
sion while decreasing recall.
Method Pr,% R,% F
1
,%
LAK-dice 75.56 79.72 77.56
LAK-cosine 76.4 80.66 78.13
LAK-l2 77.56 79.31 78.42
Baseline 32.04 75.63 45.00
Table 2: Results on the BC-PPI data set
At first glance, the LA kernel based on the distri-
butional similarity measures that we selected pro-
vides similar performance. We can notice that the
l2 metric seems to be the best performing measure.
On the BC-PPI data, the method based on the l2
measure outperforms the methods based on dice
and on cosine but the differences are not signifi-
cant.
On the LLL data set, the LA method using distri-
butional similarity measures significantly outper-
forms both baselines and also yields better results
than an approach based on shallow linguistic in-
formation (Giuliano et al, 2006). Giuliano et al
(2006) use no syntactic information. Recent work
reported in (Fundel, 2007) also uses dependency
information but in contrast to our method, it serves
as representation on which extraction rules are de-
fined.
The choice of the distributional measure does
not seem to affect the overall performance very
much. But in contrast to the BC-PPI data set, the
kernels which use dice and cosine measures sig-
nificantly outperform the one based on l2 (paired
t-test, ? = 0.01).
Method Pr,% R,% F
1
,%
LAK-dice 74.25 87.94 80.51
LAK-cosine 73.99 88.23 80.48
LAK-l2 69.28 87.6 77.37
(Fundel, 2007) 68 83 75
(Giuliano et al, 2006) 62.10 61.30 61.70
Baseline 39.02 100.00 56.13
Table 3: Results on the LLL data set
coreferences Pr,% R,% F
1
,%
with (LAK-dice) 60.00 31.00 40.90
w/o (LAK-dice) 71.00 50.00 58.60
with (Giuliano et al, 2006) 29.00 31.00 30.00
w/o (Giuliano et al, 2006) 54.80 62.90 58.60
Table 4: Results on the LLL test data set
We also verified how well our method performs
on the LLL test data. Surpisingly, precision is
still high (for both subsets, with co-references and
without them) while recall suffers. We hypothesize
that it is due to the fact that for some sentences
only incomplete parses are provided and, conse-
quently, no dependency paths between the entities
are found. For 91 out of 567 possible interaction
pairs generated on the test data, there is no depen-
dency path extracted. In contrast, work reported in
(Giuliano et al, 2006) does not make use of syn-
tactic information which on the data without coref-
erences yields higher recall.
4.2.1 Experiment Ia: Impact of distributional
measures estimation
We believe that accuracy of LA kernel crucially
depends on the substitution matrix, i.e. an accu-
rate estimate of distributional similarity. In most
cases, to obtain accurate estimates it is needed to
use a large corpus. However, it is unclear whether
differences in the estimates derived from corpora
of different sizes would affect the overall perfor-
mance of the LA kernel. To investigate it, we con-
ducted several experiments by varying a number of
retrieved passages.
Table 5 contains the most similar words to ad-
here, expression and sigF detected by the dice
measure in descending order (by varying the num-
ber of passages retrieved per query). While the or-
der of the most similar words for sigF does not
change very much from one setting to another, es-
timates for adhere and expression depend more on
the number of passages retrieved. Moreover, not
only the actual ordering changes, but also the num-
ber of similar words does. For instance, while
there are only four words similar to adhere found
when 100 passages per each query are used, al-
ready 12 similar words to adhere are detected
when the count of extracted documents is set to
1,000 passages per query.
We also notice that the most similar words to
421
adhere expression sigF
dice@100 contribute, belong, bind, map processing, overlap, production, cotC, tagA,
localization, sequestration rocG, tagF, whiG
dice@500 contribute, belong, bind, end, localization, presence, sigE, comK,
occur, result processing, absence cotC, sigG, tagA
dice@1,000 contribute, bind, convert, presence, assembly, localization, sigE, comK,
occur, belong processing, activation cotC, sigG, tagA
dice@1,500 bind, contribute, convert, localization, assembly, presence, sigE, comK,
correspond, belong activation, processing cotC, sigG, tagA
Table 5: Top 5 similar words (LLL data set)
sigF are all named entities. Even though sigF does
not occur in the training data, we can still hypoth-
esize that it is likely to be a target of the relation
because of sigE, cotC and tagA. These three genes
can be found in the training set and they are usually
targets (second argument) of the interaction rela-
tion.
Table 6 shows results on the LLL data set by
varying the size of the data used for estimation of
distributional similarity (dice measure). We ob-
serve the decrease in precision and in recall when
increasing the number of hits to 1,500. Changing
the number of hits from 500 to 1,000 results in a
subtle increase in recall.
Size Pr,% R,% F
1
,%
dice@500 74.25 87.94 80.51
dice@1,000 74.38 88.02 80.62
dice@1,500 69.87 86.85 77.43
Table 6: Estimation settings for the LLL data set
Size Pr,% R,% F
1
,%
dice@100 75.56 79.72 77.58
dice@500 76.72 81.01 78.8
dice@1,000 76.56 80.78 78.61
Table 7: Estimation settings for the BC-PPI data
set
The results on the BC-PPI data set show a simi-
lar tendency. However the observed differences are
not statistically significant in the latter case. These
subtle changes in recall and precision can be at-
tributed to the relatively low absolute values of the
similarity scores. For instance, even though an or-
der of similar words in Table 5 changes while in-
creasing the data used for estimation, a difference
between the absolute values can be quite small.
4.2.2 Experiment Ib: Impact of the scaling
parameter ?
Saigo et al (2004) have already shown that the
parameter ? has the significant impact on the re-
sults accuracy. We have also carried out some pre-
liminary experiments by setting the opening gap to
12, the extension gap to 2 and by varying the pa-
rameter ?. The kernel matrices were normalized
as in Eq. 8. The results on the BC-PPI data set
(dice500) are given in Table 8.
? Pr,% R,% F
1
,%
0.5 17.72 94.87 29.85
1 38.84 89.42 54.14
10 67.72 76.67 71.90
Table 8: Impact of ? on the performance on the
BC-PPI data set
The results indicate that decreasing ? leads to
the decrease in the overall performance. However,
if the values of gap penalties are lower and ? is set
to 1, the results are better. This suggests that the
final performance of the LA kernel is influenced
by a combination of parameters and their choice is
crucial for obtaining the good performance.
5 Related Work
We have already mentioned some relevant work
on relation extraction while introducing the local
alignment kernel. Most work done for relation
extraction considers binary relations in sentential
context (McDonald, 2005). Current techniques for
relation extraction include hand-written patterns
(Sekimizu et al, 1998), kernel methods (Zelenko
et al, 2003), pattern induction methods (Snow et
al., 2005), and finite-state automata (Pustejovsky
et al, 2002).
Kernel methods have become very popular
in natural language processing in general and
422
for learning relations, in particular (Culotta and
Sorensen, 2004). There are many kernels defined
for the text data. For instance, string kernels are
special kernels which consider inner products of
all subsequences from the given sequences of el-
ements (Lodhi et al, 2002). They can be further
extended to syllable kernels which proved to per-
form well for text categorization (Saunders et al,
2002).
For relation learning, Zelenko et al(2003) use
shallow parsing in conjunction with contiguous
and non-contiguous kernels to learn relations.
Bunescu et al(2006) define several kernels to ac-
complish the same task. First, they introduce
the sequential kernels and show that such method
out-performs the longest match approach. Next,
Bunescu et al (2006) propose kernels for the paths
in dependency trees (which is referred to as a short-
est path between two arguments of a given rela-
tion). In this paper we used their method based on
dependency parsing as one of the baselines. Giu-
liano (2006) takes this approach further by defin-
ing several kernels using local context and senten-
tial context. An advantage of Giuliano?s method
(2006) lies in the simpler representation which
does not use syntactic structure. In this case, even
if parsing fails on certain sentences, it is still pos-
sible to handle them.
6 Conclusions
We presented a novel approach to relation extrac-
tion which is based on the local alignments of se-
quences. To compare two sequences, additional
information is used which is not necessarily con-
tained in the training data. By employing distribu-
tional measures we obtain a considerable improve-
ment over two baselines and work reported before.
The choice of a distributional similarity measure
does not seem to affect the overall performance
very much. Based on the experiments we have
conducted, we conclude that the LA kernel using
dice and cosine measures perform similarly on the
LLL data set and the BC-PPI corpus. On the LLL
corpus, the LA kernel employing l2 shows a sig-
nificant decrease in performance. But concerning
statistical significance, the method using dice sig-
nificantly outperforms the one based l2 measure
only on LLL corpus while there is no significant
improvement on the BC-PPI data set noticed.
We use contextual information to measure dis-
tributional similarity. In this setting any two words
can be compared no matter which parts of speech
they belong to. As dependency paths contain vari-
ous words along with nouns and verbs, other meth-
ods often mentioned in the literature would be
more difficult to use. However, in the future we
are going to extend this approach by using syntac-
tically analyzed corpora and by estimating distri-
butional similarity from it. It would allow us to use
more accurate estimates and to discriminate be-
tween lexically ambiguous words. Similarity mea-
sures on the words that belong to other parts of
speech can be still estimated using the local con-
text only.
Acknowledgments
The authors thank Vera Hollink, Victor de Boer
and anonymous reviewers for their valuable com-
ments. This work was carried out in the con-
text of the Virtual Laboratory for e-Science project
(www.vl-e.nl). This project is supported by a
BSIK grant from the Dutch Ministry of Education,
Culture and Science (OC&W) and is part of the
ICT innovation program of the Ministry of Eco-
nomic Affairs (EZ).
References
Yevgeny (Eugene) Agichtein. 2005. Extracting Re-
lations from Large Text Collections. Ph.D. Thesis,
Columbia University.
Razvan C. Bunescu and Raymond J.Mooney. 2006.
Extracting Relations from Text. From Word Se-
quences to Dependency Paths. In book ?Text Mining
and Natural Language Processing?, Anne Kao and
Steve Poteet (Eds).
Razvan C. Bunescu and Raymond J.Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proceedings of the Joint Conference
on Human Language Technology / Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
Vancouver, BC.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm
Stanley F. Chen and Joshua Goodman. 1996. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. In ACL?96.
William W. Cohen, Pradeep Ravikumar and Stephen
Fienberg. 2003. A Comparison of String Distance
Metrics for Name-Matching Tasks. In IIWeb 2003,
pages 73-78.
423
Aron Culotta and Jeffrey Sorensen. 2003. Dependency
Tree Kernels for Relation Extraction. In ACL 2003.
William B. Dolan, Chris Quirk and Chris Brockett.
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel News
Sources. In Proceedings of COLING 2004, Geneva,
Switzerland.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer. 2007.
RelEx - Relation Extraction using dependency parse
trees. In Bioinformatics, vol. 23, no. 3.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting Shallow Linguistic Infor-
mation for Relation Extraction from Biomedical Lit-
erature. In EACL 2006.
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. UC Santa Cruz Technical Report
UCS-CRL-99-10.
William Hersch, Aaron M. Cohen, Phoebe Roberts and
Hari K. Rakapalli. 2006. TREC 2006 Genomics
Track Overview. In Proceedings of the 15th Text Re-
trieval Conference.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 25-32.
G. Leusch, N. Ueffing and H. Ney. 2003. A Novel
String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Machine
Translation Summit IX, New Orleans, LO, pages
240-247.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Christianini, and Chris Watkins. 2002. Text
Classification using String Kernels. In Journal of
Machine Learning Research, 2, pages 419-444.
Ryan McDonald. 2005. Extracting Relations from Un-
structured Text. Technical Report: MS-CIS-05-06.
Alvaro E. Monge and Charles Elkan. 1996. The Field
Matching Problem: Algorithms and Applications. In
KDD 1996, pages 267-270.
Alessandro Moschitti. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees. In
ECML 2006, pages 318-329.
Cl. Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In Proceed-
ings of the Learning Language in Logic workshop.
J. Pustejovsky, J. Castano, J. Zhang, B. Cochran, M.
Kotecki. 2002. Robust Relational Parsing over
Biomedical Literature: Extracting Inhibit Relations.
Pacific Symposium on Biocomputing.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
James Dowdall, Christos Andronis, Andreas Per-
sidis, and Ourania Konstanti. 2004. Mining re-
lations in the GENIA corpus. In ?Second Euro-
pean Workshop on Data Mining and Text Mining for
Bioinformatics?, in conjunction with ECML/PKDD
2004, September.
Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda and
Tatsuya Akutsu. 2004. Protein homology detection
using string alignment kernels. In ?Bioinformatics?,
vol. 20 no. 11, pages 1682-1689.
Hiroto Saigo, Jean-Philippe Vert, and Tatsuya Akutsu.
2006. Optimizing amino acid substitution matrices
with a local alignment kernel. In ?BMC Bioinfor-
matics?, 7:246.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other String Kernel Extensions. In
Proceesings of the Nineteenth International Confer-
ence on Machine Learning (ICML?02).
T. Sekimizu, H. Park, and J. Tsujii. 1998. Identify-
ing the interaction between genes and gene products
based on frequently seen verbs in medline abstracts.
In Genome Informatics.
T. F. Smith and M. S. Waterman. 1987. Identifica-
tion of Common Molecular Subsequences. In J. Mol.
Biol. 147, 195?197.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. NIPS 17.
Erik Tjong Kim Sang, Sander Canisius, Antal van den
Bosch and Toine Bogers. 2005. Applying spelling
error correction techniques for improving semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Natural Language Learning, CoNLL-2005,
June 29-30, 2005, Ann Arbor, MI.
Lonneke van der Plas and Jo?rg Tiedemann. 2006. Find-
ing Synonyms Using Automatic Word Alignment
and Measures of Distributional Similarity. In Pro-
ceedings of ACL/Coling.
C. J. van Rijsbergen, S. E. Robertson and M. F. Porter.
1980. New models in probabilistic information re-
trieval. London: British Library. (British Library
Research and Development Report, no. 5587).
Julie Weeds, David Weir and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of CoLing 2004.
Dmitry Zelenko, Ch. Aone, and A. Richardella. 2003.
Kernel Methods for Relation Extraction. Journal of
Machine Learning Research 3 (2003), 1083-1106.
424
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 185?188,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Types of Some Generic Relation Arguments:
Detection and Evaluation
Sophia Katrenko
Institute of Informatics
University of Amsterdam
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
Institute of Informatics
University of Amsterdam
the Netherlands
pietera@science.uva.nl
Abstract
This paper presents an approach to detec-
tion of the semantic types of relation argu-
ments employing the WordNet hierarchy. Us-
ing the SemEval-2007 data, we show that
the method allows to generalize relation ar-
guments with high precision for such generic
relations as Origin-Entity, Content-Container,
Instrument-Agency and some other.
1 Introduction and Motivation
A common approach to learning relations is com-
posed from two steps, identification of arguments
and relation validation. This methodology is widely
used in different domains, such as biomedical. For
instance, in order to extract instances of a relation of
protein interactions, one has to first identify all pro-
tein names in text and, second, verify if a relation
between them holds.
Clearly, if arguments are already given, accuracy
of relation validation is higher compared to the sit-
uation when the arguments have to be identified au-
tomatically. In either case, this methodology is ef-
fective for the domain-dependent relations but is not
considered for more generic relation types. If a rela-
tion is more generic, such as Part-Whole, it is more
difficult to identify its arguments because they can
be of many different semantic types. An exam-
ple below contains a causality relation (virus causes
flu). Note that syntactic information is not sufficient
to be able to detect such relation mention and the
background knowledge is needed.
A person infected with a particular flu virus
strain develops antibody against that virus.
In this paper we propose a method to detect se-
mantic types of the generic relation arguments. For
the Part-Whole relation, it is known that it embraces
such subtypes as Member-Collection or Place-Area
while there is not much information on the other re-
lation types. We do not claim semantic typing to
be sufficient to recognize relation mentions in text,
however, it would be interesting to examine the ac-
curacy of relation extraction when the background
knowledge only is used. Our aim is therefore to dis-
cover precise generalizations per relation type rather
than to cover all possible relation mentions.
2 A Method: Making Semantic Types of
Arguments Explicit
We propose a method for generalizing relation argu-
ment types based on the positive and negative exam-
ples of a given relation type. It is also necessary that
the arguments of a relation are annotated using some
semantic taxonomy, such as WordNet (Fellbaum,
1998). Our hypothesis is as follows: because of
the positive and negative examples, it should be pos-
sible to restrict semantic types of arguments using
negative examples. If negative examples are nearly
positive, the results of such generalization should be
precise. Or, in machine learning terms, such neg-
ative examples are close to the decision boundary
and if used during generalization, precision will be
boosted. If negative examples are far from the de-
cision boundary, their use will most likely not help
to identify semantic types and will result in over-
generalization.
To test this hypothesis, we use an idea borrowed
from induction of the deterministic finite automata.
185
Gx1 Gy1
Gy2
Gy3
Gx4 Gy4
Gx1 LCSGy1 ,Gy2 ,Gy3
Gx4
Gy2
Gy4
Figure 1: Generalization process.
More precisely, to infer deterministic finite automata
(DFA) from positive and negative examples, one first
builds the maximal canonical automaton (MCA)
(Pernot et al, 2005) with one starting state and a
separate sequence of states for each positive exam-
ple and then uses a merging strategy such that no
negative examples are accepted.
Similarly, for a positive example < xi, yi > we
collect all f hyperonyms Hxi = h1xi , h
2
xi , . . . , h
f
xi
for xi where h1xi is an immediate hyperonym and h
f
xi
is the most general hyperonym. The same is done for
yi. Next, we use all negative examples to find Gxi
and Gyi which are generalization types of the argu-
ments of a given positive example < xi, yi >. In
other words, we perform generalization per relation
argument in a form of one positive example vs. all
negative examples. Because of the multi-inheritance
present in WordNet, it is possible to find more hy-
peronymy paths than one. To take it into account,
the most general hyperonym hfxi equals to a splitting
point/node.
It is reasonable to assume that the presence of a
general semantic category of one argument will re-
quire a more specific semantic category for the other.
Generalization per argument is, on the one hand,
useful because none of the arguments share a seman-
tic category with the corresponding arguments of all
negative examples. On the other hand, it is too re-
strictive if one aims at identification of the relation
type. To avoid this, we propose to generalize seman-
tic category of one argument by taking into account
a semantic category of the other. In particular, one
can represent a binary relation as a bipartite graph
where the corresponding nodes (relation arguments)
are connected. A natural way of generalizing would
be to combine the nodes which differ on the basis of
their similarity. In case of WordNet, we can use a
least common subsumer (LCS) of the nodes. Given
the bipartite graph in Figure 1, it can be done as fol-
lows. For every vertex Gxi in one part which is con-
nected to several vertices Gy1 , . . . , Gyk in the other,
we compute LCS of Gy1 , . . . , Gyk . Note that we re-
quire the semantic contrains on both arguments to be
satisfied in order to validate a given relation. Gener-
alization via LCS is carried out in both directions.
This step is described in more detail in Algorithm 1.
Algorithm 1 Generalization via LCS
1: MemoryM = ?
2: Direction: ?
3: for all < Gxi , Gyi >? G do
4: Collect all < Gxj , Gyj >, j = 0, . . . , l s. t.
Gxi = Gxj
5: if exists < Gxk , Gyj > s. t. Gxi 6= Gxk then
6: G = G ? {< Gxj , Gyj >}
7: end if
8: Compute L = LCSGy0 ,...,Gyl
9: Replace < Gxj , Gyj >,j = 0, . . . , l with <
Gxj ,L > in G
10: M =M? {< Gxj ,L >}
11: end for
12: Direction: ?
13: for all < Gxi , Gyi >? G do
14: Collect all < Gxj , Gyj >, j = 0, . . . , l s. t. Gyi =
Gyj and
< Gxj , Gyj >/?M
15: Compute L = LCSGx0 ,...,Gxl
16: Replace < Gxj , Gyj >, j = 0, . . . , l with <
L, Gyj > in G
17: end for
18: return G
Example Consider, for instance, two sentences
from the SemEval data (Instrument-Agency rela-
tion).
013 ?The test is made by inserting the
end of a <e1>jimmy</e1> or other
<e2>burglar</e2>?s tool and endeavouring
to produce impressions similar to those which
have been found on doors or windows.?
WordNet(e1) = ?jimmy%1:06:00::?, Word-
Net(e2) = ?burglar%1:18:00::?, Instrument-
Agency(e1, e2) = ?true?
040 ?<e1>Thieves</e1> used a
<e2>blowtorch</e2> and bolt cutters
to force their way through a fenced area
186
topped with razor wire.? WordNet(e1) =
?thief%1:18:00::?, WordNet(e2) = ?blow-
torch%1:06:00::?, Instrument-Agency(e2, e1)
= ?true?
First, we find the sense keys corresponding
to the relation arguments, (?jimmy%1:06:00::?,
?burglar%1:18:00::?) = (jimmy#1, burglar#1)
and (?blowtorch%1:06:00::?, ?thief%1:18:00::?) =
(blowtorch#1, thief#1).By using negative exam-
ples, we obtain the following pairs: (apparatus#1,
bad person#1) and (bar#3, bad person#1). These
pairs share the second argument and it makes
it possible to apply generalization in the direc-
tion ?. LCS of apparatus#1 and bar#3 is
instrumentality#3 and hence the generalized pair
becomes (instrumentality#3, bad person#1).
Note that an order in which the directions are cho-
sen in Algorithm 1 does not affect the resulting gen-
eralizations. Keeping all generalized pairs in the
memory M ensures that whatever direction (? or
?) a user chooses first, the output of the algorithm
will be the same.
Until now, we have considered generalization in
one step only. It would be natural to extend this ap-
proach to the iterative generalization such that it is
performed until no further generalization steps can
be made (it corresponds either to the two specific ar-
gument types or to the situation when the top of the
hierarchy is reached). However, such method would
most likely result in overgeneralization by boost-
ing recall but drastically decreasing precision. As
an alternative we propose to use memory MI de-
fined over the iterations. After each iteration step
every generalized pair < Gxi , Gyi > is applied to
the training set and if it accepts at least one negative
example, it is either removed from the set G (first
iteration) or this generalization pair is decomposed
back into the pairs it was formed from (all other it-
erations). By employing backtracking we guarantee
that empirical error on the training set Eemp = 0.
3 Evaluation
Data For semantic type detection, we use 7 binary
relations from the training set of the SemEval-2007
competition, all definitions of which share the re-
quirement of the syntactic closeness of the argu-
ments. Further, their definitions have various restric-
tions on the nature of the arguments. Short descrip-
tion of the relation types we study is given below.
Cause-Effect(X,Y) This relation takes place if, given
a sentence S, it is possible to entail that X is the cause
of Y . Y is usually not an entity but a nominal denoting
occurrence (activity or event).
Instrument-Agency(X,Y) This relation is true if S en-
tails the fact that X is the instrument of Y (Y uses X).
Further, X is an entity and Y is an actor or an activity.
Product-Producer(X,Y) X is a product of Y , or Y
produces X , where X is any abstract or concrete object.
Origin-Entity(X,Y) X is the origin of Y where X can
be spatial or material and Y is the entity derived from the
origin.
Theme-Tool(X,Y) The tool Y is intended for X is ei-
ther its result or something that is acted upon.
Part-Whole(X,Y) X is part of Y and this rela-
tion can be one of the following five types: Place-
Area, Stuff-Object, Portion-Mass, Member-Collection
and Component-Integral object.
Content-Container(X,Y) A sentence S entails the
fact that X is stored inside Y . Moreover, X is not a com-
ponent of Y and can be removed from it.
We hypothesize that Cause-Effect and Part-Whole
are the relation types which may require sentential
information to be detected. These two relations al-
low a greater variety of arguments and the seman-
tic information alone might be not sufficient. Such
relation types as Product-Producer or Instrument-
Agency are likely to benefit more from the external
knowledge. Our method depends on the positive and
negative examples in the training set and on the se-
mantic hierarchy we use. If some parts of the hierar-
chy are more flat, the resulting patterns may be too
general.
As not all examples have been annotated with
the information from WordNet, we removed them
form the test data while conducting this experiment.
Content-Container turned out to be the only rela-
tion type whose examples are fully annotated. In
contrast, Product-Producer is a relation type with
the most information missing (9 examples removed).
There is no reason to treat relation mentions as mu-
tually exclusive, therefore, only negative example
provided for a particular relation type are used to
determine semantic types of its arguments.
Discussion The entire generalization process re-
sults in a zero-error on the training set. It does
not, however, guarantee to hold given a new data
set. The loss in precision on the unseen exam-
187
Relation type P, % R, % A, % B-A, %
Origin-Entity 100 26.5 67.5 55.6
Content-Container 81.8 47.4 67.6 51.4
Cause-Effect 100 2.8 52.7 51.2
Instrument-Agency 78.3 48.7 67.6 51.3
Product-Producer 77.8 38.2 52.4 66.7
Theme-Tool 66.7 8.3 65.2 59.2
Part-Whole 66.7 15.4 66.2 63.9
avg. 81.6 26.8 62.7 57.0
Table 1: Performance on the test data
ples can be caused by the generalization pairs where
both arguments are generalized to the higher level
in the hierarchy than it ought to be. To check
how the algorithm behaves, we first evaluate the
specialization step on the test data from the Se-
mEval challenge. Among all the relation types,
only Instrument-Agency, Part-Whole and Content-
Container fail to obtain 100% precision after the
specialization step. It means that, already at this
stage, there are some false positives and the contex-
tual classification is required to achieve better per-
formance.
The results of the method introduced here are pre-
sented in Table 1. Systems which participated in
SemEval were categorized depending on the input
information they have used. The category Word-
Net implies that WordNet was employed but it does
not exclude a possibility of using other resources.
Therefore, to estimate how well our method per-
forms, we calculated accuracy and compared it
against a baseline that always returns the most fre-
quent class label (B-A). Given the results of the
teams participating in the challenge, the organizers
mention Product-Producer as one of the easiest rela-
tions, while Origin-Entity and Theme-Tool are con-
sidered to be ones of the hardest to detect (Girju
et al, 2007). Interestingly, Origin-Entity obtains
the highest precision compared to the other relation
types while using our approach.
Table 2 contains some examples of the semantic
types we found for each relation. Some of them
are quite specific (e.g., Origin-Entity), while the
other arguments may be very general (e.g., Cause-
Effect). The examples of the patterns for Part-
Whole can be divided in several subtypes, such as
Member-Collection (person#1, social group#1),
Place-Area (top side#1, whole#2) or Stuff-Object
(germanium#1, mineral#1).
Relation (GX , GY )
Content-
Container
(physical entity#1, vessel#3)
Instrument- (instrumentality#3, bad person#1)
Agency (printing machine#1, employee#1)
Cause- (cognitive operation#1, joy#1)
Effect (entity#1, harm#2)
(cognitive content#1,
communication#2)
Product- (knowledge#1, social unit#1)
Producer (content#2, individual#1)
(instrumentality#3,
business organisation#1)
Origin- (article#1, section#1)
Entity (vegetation#1, plant part#1)
(physical entity#1, fat#1)
Theme- (abstract entity#1, implementation#2)
Tool (animal#1, water#6)
(nonaccomplishment#1,
human action#1)
Part- (top side#1, whole#2)
Whole (germanium#1, mineral#1)
(person#1, social group#1)
Table 2: Some examples per relation type.
4 Conclusions
As expected, the semantic types derived for such
relations as Origin-Entity, Content-Container and
Instrument-Agency provide high precision on the
test data. In contrast, precision for Theme-Tool is
the lowest which has been noted by the participants
of the SemEval-2007. In terms of accuracy, Cause-
Effect seems to obtain 100% precision but low recall
and accuracy. An explanation for that might be a
fact that causation can be characterized by a great
variety of argument types many of which have been
absent in the training data. Origin-Entity obtains the
maximal precision with accuracy much higher than
baseline.
References
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Nicholas Pernot, Antoine Cornue?jols, and Michele Se-
bag. 2005. Phase transition within grammatical infer-
ence. In Proceedings of IJCAI 2005.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In ACL 2007.
188
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 88?93,
Prague, June 2007. c?2007 Association for Computational Linguistics
Named Entity Recognition for Ukrainian: A Resource-Light Approach
Sophia Katrenko
HCSL, University of Amsterdam,
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
HCSL, University of Amsterdam,
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
pitera@science.uva.nl
Abstract
Named entity recognition (NER) is a subtask
of information extraction (IE) which can be
used further on for different purposes. In this
paper, we discuss named entity recognition
for Ukrainian language, which is a Slavonic
language with a rich morphology. The ap-
proach we follow uses a restricted number of
features. We show that it is feasible to boost
performance by considering several heuris-
tics and patterns acquired from the Web data.
1 Introduction
The information extraction task has proved to be dif-
ficult for a variety of domains (Riloff, 1995). The
extracted information can further be used for ques-
tion answering, information retrieval and other ap-
plications. Depending on the final purpose, the ex-
tracted information can be of different type, e.g.,
temporal events, locations, etc. The information cor-
responding to locations and names, is referred to as
the information about named entities. Hence, named
entity recognition constitutes a subtask of the infor-
mation extraction in general.
It is especially challenging to extract the named
entities from the text sources written in languages
other than English which, in practice, is supported
by the results of the shared tasks on the named entity
recognition (Tjong Kim Sang, 2002).
Named entity recognition for the languages with
a rich morphology and a free word order is difficult
because of several reasons. The entropy of texts in
such languages is usually higher than the entropy
of English texts. It is either needed to use such
resources as morphological analyzers to reduce the
data sparseness or to annotate a large amount of data
in order to obtain a good performance. Luckily, the
free word order is not crucial for the named entity
recognition task as the local context of a named en-
tity should be sufficient for its detection. Besides,
a free word order usually implies a free order of
constituents (such as noun phrases or verb phrases)
rather than words as such. For instance, although
(1)1 is grammatically correct and can occur in the
data, it would be less frequent than (2).
 
	


Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 472?475,
Prague, June 2007. c?2007 Association for Computational Linguistics
UVAVU: WordNet Similarity and Lexical Patterns
for Semantic Relation Classication
Willem Robert van Hage
TNO Science & Industry
Stieltjesweg 1, 2628CK Delft
the Netherlands
wrvhage@few.vu.nl
Sophia Katrenko
HCSL, University of Amsterdam
Kruislaan 419, 1098VA Amsterdam
the Netherlands
katrenko@science.uva.nl
Abstract
The system we propose to learning seman-
tic relations consists of two parallel com-
ponents. For our final submission we used
components based on the similarity mea-
sures defined over WordNet and the patterns
extracted from the Web and WMTS. Other
components using syntactic structures were
explored but not used for the final run.
1 Experimental Set-up
The system we used to classify the semantic rela-
tions consists of two parallel binary classifiers. We
ran this system for each of the seven semantic re-
lations separately. Each classifier predicts for each
instance of the relation whether it holds or not. The
predictions of all the classifiers are aggregated for
each instance by disjunction. That is to say, each in-
stance is predicted to be false by default unless any
of the classifiers gives evidence against this.
To generate the submitted predictions we used
two parallel classifiers: (1) a classifier that com-
bines eleven WordNet-based similarity measures,
see Sec. 2.1, and (2) a classifier that learns lexical
patterns from Google and the Waterloo Multi-Text
System (WMTS)(Turney, 2004) snippets and ap-
plies these on the same corpora, see Sec. 2.2.
Three other classifiers we experimented with, but
that were not used to generate the submitted predic-
tions: (3) a classifier that uses string kernel methods
on the dependency paths of the training sentences,
see Sec. 3.1, (4) a classifier that uses string kernels
on the local context of the subject and object nom-
inals in the training sentences, see Sec. 3.2 and (5)
a classifier that uses hand-made lexical patterns on
Google and WMTS, see Sec. 3.3.
2 Submitted Run
2.1 WordNet-based Similarity Measures
WordNet 3.0 (Fellbaum, 1998) is the most fre-
quently used lexical database of English. As this re-
source consists of lexical and semantic relations, its
use constitutes an appealing option to learning rela-
tions. In particular, we believe that given two men-
tions of the same semantic relation, their arguments
should also be similar. Or, in analogy learning terms,
if R1(X1,Y1) and R2(X2,Y2) are relation mentions ofthe same type, then X1 :: Y1 as X2 :: Y2. Our prelim-inary experiments with WordNet suggested that few
arguments of each relation are connected by imme-
diate hyperonymy or meronymy relations. As a re-
sult, we decided to use similarity measures defined
over WordNet (Pedersen et al, 2004). The Word-
Net::Similarity package (Pedersen et al, 2004) in-
cludes 11 different measures, which mostly use ei-
ther the WordNet glosses (lesk or vector measures)
or the paths between a pair of concepts (lch; wup) to
determine their relatedness.
To be able to use WordNet::Similarity, we
mapped all WordNet sense keys from the training
and test sets to the earlier WordNet version (2.1).
Given a relation R(X ,Y ), we computed the related-
ness scores for each pair of arguments X and Y . The
scores together with the sense keys of arguments
were further used as features for the machine learn-
ing method. As there is no a priori knowledge on
what measures are the most important for each rela-
472
tion, all of them were used and no feature selection
step has been taken.
We experimented with a number of machine
learning methods such as k-nearest neighbour al-
gorithm, logistic regression, bayesian networks and
others. For each relation a method performing best
on the training set was selected (using 5-fold cross-
validation).
2.2 Learnt Lexical Patterns
This classifier models the intuition that when a pair
of nominals is used in similar phrases as another pair
they share at least one relation, and when no such
phrases can be found they do not share any relation.
Applied to the semantic relation classification prob-
lem this means that when a pair in the test set can be
found in the same patterns as pairs from the training
set, the classification for the pair will be true.
To find the patterns we followed step 1 to 6 de-
scribed in (Turney, 2006), with the exception that
we used both Google and the WMTS to compute
pattern frequency.
First we extracted the pairs of nominals ?X ,Y ?
from the training sentences and created one Google
query and a set of WMTS queries for each pair.
The Google queries were of the form "X * Y"
OR "Y * X". Currently, Google performs mor-
phological normalization on every query, so we
did not make separate queries for various endings
of the nominals. For the WMTS we did make
separate queries for various morphological varia-
tions. We used the following set of suffixes: ?-
tion(s|al)?, ?-ly?, ?-ist?, ?-ical?, ?-y?, ?-ing?, ?-ed?,
?-ies?, and ?-s?. For this we used Peter Turney?s
pairs Perl package. The WMTS queries looked
like [n]>([5].."X"..[i].."Y"..[5]) and
[n]>([5].."Y"..[i].."X"..[5]) for i =
1,2,3 and n = i+12, and for each variation of X and
Y . Then we extracted sentences from the Google
snippets and cut out a context of size 5, so that
we were left with similar text segments as those
returned by the WMTS queries. We merged the
lists of text segments and counted all n-grams that
contained both nominals for n = 1 to 6. We sub-
stituted the nominals by variables in the n-grams
with a count greater than 10 and used these as pat-
terns for the classifier. An example of such a pat-
tern for the Cause-Effect relation is "generation
of Y by X". After this we followed step 3 to
6 of (Turney, 2006), which left us with a matrix
for each of the seven semantic relations, where each
row represented a pair of nominals and each column
represented the frequency of a pattern, and where
each pair was classified as either true or false. The
straightforward way to find pattern frequencies for
the pairs in the test set would be to fill in these pat-
terns with the pairs of nominals from the test set.
This was not feasible given the time limitation on
the task. So instead, for each pair of nominals in
the test set we gathered the top-1000 snippets and
computed pattern frequencies by counting how of-
ten the nominals occur in every pattern on this set
text segments. We constructed a matrix from these
frequencies in the same way as for the training set,
but without classifications for the pairs. We experi-
mented with various machine learning algorithms to
predict the classes of the pairs. We chose to use k-
nearest neighbors, because it was the only algorithm
that gave more subtle predictions than true for every
pair or false for every pair. For each semantic rela-
tion we used the value of k that produced the highest
F1 score on 5-fold cross validation on the trainingdata.
3 Additional Runs
3.1 String Kernels on Dependency Paths
It has been a long tradition to use syntactic structures
for relation extraction task. Some of the methods
as in (Katrenko and Adriaans, 2004) have used in-
formation extracted from the dependency trees. We
followed similar approach by considering the paths
between each pair of arguments X and Y . Ideally, if
each syntactic structure is a tree, there is only one
path from one node to the other. After we have ex-
tracted paths, we used them as input for the string
kernel methods (Hal Daum? III, 2004). The advan-
tage of using string kernels is that they can handle
sequences of different lengths and already proved to
be efficient for a number of tasks.
All sentences in the training data were parsed
using MINIPAR (Lin, 1998). From each depen-
dency tree we extracted a dependency path (if any)
between the arguments by collecting all lemmas
(nodes) and syntactic functions (edges). The se-
quences we obtained were fed into string kernel.
473
To assess the results, we carried out 5-fold cross-
validation. Even by optimizing the parameters of
the kernel (such as the length of subsequences) for
each relation, the highest accuracy we obtained was
equal 61,54% (on Origin-Entity relation) and the
lowest was accuracy for the Instrument-Agency re-
lation (50,48%).
3.2 String Kernels on Local Context
Alternatively to syntactic information, we also ex-
tracted the snippets of the fixed length from each
sentence. For each relation mention of R(X ,Y ), all
tokens between the relation arguments X and Y were
collected along with at most three tokens to the left
and to the right. Unfortunately, the results we ob-
tained on the training set were comparable to those
obtained by string kernels on dependency paths and
less accurate than the results provided by WordNet
similarity measures or patterns extracted from the
Web and WMTS. As a consequence, string kernel
methods were not used for the final submission.
3.3 Manually-created Lexical Patterns
The results of the method described in Sec. 2.2 are
quite far below what we expected given earlier re-
sults in the literature (Turney, 2006; van Hage, Ka-
trenko, and Schreiber, 2005; van Hage, Kolb, and
Schreiber, 2006; Berland and Charniak, 2006; Et-
zioni et al, 2004). We think this is caused by
the fact that many pairs in the training set are non-
stereotypical examples. So often the most com-
monly described relation of such a pair is not the re-
lation we try to classify with the pair. For example,
common associations with the pair ?body,parents?
are that it is the parents? body, or that the parents
are member of some organizing body, while it is a
positive example for the Product-Producer relation.
We wanted to see if this could be the case by testing
whether more intuitive patterns give better results on
the test set. The patterns we manually created for
each relation are shown in Table 1. If a pair gives
any results for these patterns on Google or WMTS,
we classify the pair as true, otherwise we classify
it as false. The results are shown in Table 2. We
did not use these results for the submitted run, be-
cause only automatic runs were permitted. The man-
ual patterns did not yield many useful results at all.
Apparently intuitive patterns do not capture what is
required to classify the relations in the test set. The
patterns we used for the Part-Whole (6) relation had
an average Precision of .50, which is much lower
than the average Precision found in (van Hage, Kolb,
and Schreiber, 2006), which was around 0.88. We
conclude that both the sets of training and test ex-
amples capture different semantics of the relations
than the intuitive ones, which causes common sense
background knowledge, such as Google to produce
bad results.
rel. patterns
1. X causes Y, X caused by Y, X * cause Y
2. X used Y, X uses Y, X * with a Y
3. X made by Y, X produced by Y, Y makes X,
Y produces X
4. Y comes from X, X * source of Y, Y * from * X
5. Y * to * X, Y * for * X, used Y for * X
6. X in Y, Y contains X, X from Y
7. Y contains X, X in Y, X containing Y, X into Y
Table 1: Hand-written patterns.
relation N Prec. Recall F1 Acc.1. Cause-Effect 6 1 0.15 0.25 0.56
2. Instr.-Agency 2 1 0.05 0.10 0.54
3. Prod.-Prod. 4 0.75 0.05 0.09 0.35
4. Origin-Ent. 6 0.33 0.05 0.09 0.35
5. Theme-Tool 2 0 0 0 0.56
6. Part-Whole 16 0.50 0.31 0.38 0.64
7. Cont.-Cont. 11 0.54 0.16 0.24 0.50
Table 2: Results for hand-written lexical patterns on
Google and WMTS.
4 Results
4.1 WordNet-based Similarity Measures
Table 3 shows the results of the WordNet-based sim-
ilarity measure method. In the ?methods? column,
the abbreviation LR stands for logistic regression,
K-NN stands for k-nearest neighbour, and DT stands
for decision trees.
relation method Prec. Recall F1 Acc.1. Cause-Effect LR 0.48 0.51 0.49 0.45
2. Instr.-Agency DT 0.65 0.63 0.64 0.62
3. Prod.-Prod. DT 0.67 0.50 0.57 0.46
4. Origin-Ent. LR 0.50 0.47 0.49 0.49
5. Theme-Tool LR 0.54 0.52 0.53 0.62
6. Part-Whole DT 0.54 0.73 0.62 0.67
7. Cont.-Cont. 2-NN 0.66 0.55 0.60 0.62
Table 3: Results for similarity-measure methods.
474
4.2 Learnt Lexical Patterns
Table 4 shows the results of the learnt lexical pat-
terns method. For all relations we used the k-nearest
neighbour method.
relation method Prec. Recall F1 Acc.1. Cause-Effect 3-NN 0.53 0.76 0.63 0.54
2. Instr.-Agency 2-NN 0.47 0.89 0.62 0.46
3. Prod.-Prod. 2-NN 0 0 0 0.33
4. Origin-Ent. 2-NN 0.47 0.22 0.30 0.54
5. Theme-Tool 3-NN 0.39 0.93 0.55 0.38
6. Part-Whole 2-NN 0.36 1 0.53 0.36
7. Cont.-Cont. 2-NN 0.51 0.97 0.67 0.51
Table 4: Results for learnt lexical patterns on Google
and WMTS.
5 Discussion
Our methods had the most difficulty with classify-
ing relation 1, 3 and 4. We wanted to see if hu-
man assessors perform less consistent for those re-
lations. If so, then those relations would simply be
harder to classify. Otherwise, our system performed
worse for those relations. We manually assessed ten
sample sentences from the test set, five of which
were positive examples and five were false exam-
ples. The result of a comparison with the test set is
shown in Table 5. The numbers listed there repre-
sent the fraction of examples on which we agreed
with the judges of the test set. There was quite a
inter-judge agreement
relation judge 1 judge 2
1. Cause-Effect 0.93 0.93
2. Instrument-Agency 0.77 0.77
3. Product-Producer 0.87 0.80
4. Origin-Entity 0.80 0.77
5. Theme-Tool 0.80 0.77
6. Part-Whole 0.97 1.00
7. Content-Container 0.77 0.77
Table 5: Inter-judge agreement.
large variation in the inter-judge agreement, but for
relation 1 and 3 the consensus was high. We con-
clude that the reason for our low performance on
those relations are not caused by the difficulty of
the sentences, but due to other reasons. Our intu-
ition is that the sentences, especially those of rela-
tion 1 and 3, are easily decidable by humans, but
that they are non-stereotypical examples of the re-
lation, and thus hard to learn. The following ex-
ample sentence breaks common-sense domain and
range restrictions: Product-Producer #142 ?And, of
course, everyone wants to prove the truth of their be-
liefs through experience, but the <e1>belief</e1>
begets the <e2>experience</e2>.? The common-
sense domain and range restriction of the Product-
Producer relation are respectively something like
?Entity? and ?Agent?. However, ?belief? is generally
not considered to be an entity, and ?experience? not
an agent. The definition of Product-Producer rela-
tion used for the Challenge is more flexible and al-
lows therefore many examples which are difficult to
find by such common-sense resources as Google or
WordNet.
References
Matthew Berland and Eugene Charniak. 1999. Finding
Parts in Very Large Corpora. In Proceedings of ACL1999.
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Hal Daum? III. 2004. SVMsequel Tutorial Manual.Available at http://www.cs.utah.edu/
?hal/SVMsequel/svmsequel.pdf
Oren Etzioni et al 2004. Methods for Domain-
Independent Information Extraction from the Web: AnExperimental Comparison. In Proceedings of AAAI2004.
Willem Robert van Hage, Sophia Katrenko, and GuusSchreiber. 2005. A Method to Combine Linguis-tic Ontology-Mapping Techniques. In Proceedings ofISWC 2005.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber.2006. A Method for Learning Part-Whole Relations.In Proceedings of ISWC 2006.
Sophia Katrenko and Pieter Adriaans. 2007. Learn-ing Relations from Biomedical Corpora Using Depen-
dency Trees. In KDECB, LNBI, vol. 4366.
Dekang Lin. 1998. Dependency-based Evaluation ofMINIPAR. In Workshop on the Evaluation of ParsingSystems, Granada, Spain.
Ted Pedersen, Patwardhan, and Michelizzi. 2004. Word-Net::Similarity - Measuring the Relatedness of Con-
cepts. In the Proceedings of AAAI-04, San Jose, CA.
Peter Turney. 2006. Expressing Implicit SemanticRelations without Supervision. In Proceedings ofCOLING-ACL 2006.
Peter Turney. 2004. The MultiText Project Home Page,University of Waterloo, School of Computer Science,
http://www.multitext.uwaterloo.ca
475
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 49?53,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
?Could you make me a favour and do coffee, please??:
Implications for Automatic Error Correction in English and Dutch
Sophia Katrenko
UiL-OTS
Utrecht University
s.katrenko@uu.nl
Abstract
The correct choice of words has proven chal-
lenging for learners of a second language and
errors of this kind form a separate category
in error typology. This paper focuses on one
known example of two verbs that are often
confused by non-native speakers of Germanic
languages, to make and to do. We conduct ex-
periments using syntactic information and im-
mediate context for Dutch and English. Our
results show that the methods exploiting syn-
tactic information and distributional similarity
yield the best results.
1 Introduction
When learning a second language, non-native speak-
ers make errors at all levels of linguistic analy-
sis, from pronunciation and intonation to language
use. Word choice errors form a substantial part
of all errors made by learners and may also be
observed in writing or speech of native speak-
ers. This category of errors includes homophones.
Some commonly known confusions in English are
accept-except, advice-advise, buy-by-bye, ate-eight,
to name but a few. Other errors can be explained
by a non-native speaker?s inability to distinguish be-
tween words because there exists only one corre-
sponding word in their native language. For ex-
ample, Portuguese and Spanish speakers have diffi-
culties to differentiate between te doen (to do) and
te maken (to make), and Turkish between kunnen
(can), weten (to know) and kennen (to know) in
Dutch (Coenen et al, 1979). Adopting terminol-
ogy from Golding and Roth (1999) and Rozovskaya
and Roth (2010), do/make and kunnen/kennen/weten
form two confusion sets. However, unlike the case
of kunnen/kennen/weten, where the correct choice is
often determined by syntactic context 1, the choice
between to make and to do can be motivated by
semantic factors. It has been argued in the litera-
ture that the correct use of these verbs depends on
what is being expressed: to do is used to refer to
daily routines and activities, while to make is used to
describe constructing or creating something. Since
word choice errors have different nature, we hypoth-
esize that there may exist no uniform approach to
correct them.
State-of-the-art spell-checkers are able to detect
spelling and agreement errors but fail to find words
used incorrectly, e.g. to distinguish to make from to
do. Motivated by the implications that the correct
prediction of two verbs of interest may have for au-
tomatic error correction, we model the problem of
choosing the correct verb in a similar vein to selec-
tional preferences. The latter has been considered
for a variety of applications, e. g. semantic role la-
beling (Zapirain et al, 2009). Words such as be or
do have been often excluded from consideration be-
cause they are highly polysemous and ?do not select
strongly for their arguments? (McCarthy and Car-
roll, 2003). In this paper, we study whether semantic
classes of arguments may be used to determine the
correct predicate (e.g., to make or to do) and con-
sider the following research questions:
1. Can information on semantic classes of direct
1Kunnen is a modal verb followed by the main verb, kennen
takes a direct object as in, e.g., to know somebody, and weten is
often followed by a clause (as in I know that).
49
objects potentially help to correct verb choice
errors?
2. How do approaches using contextual and syn-
tactic information compare when predicting to
make vs. to do?
The paper is organised as follows. Section 2.1
discusses the methods, followed by Section 2.2 on
data. The experimental findings are presented in
Section 2.3. We conclude in Section 3.
2 Experiments
We re-examine several approaches to selectional
preferences in the context of error correction. Ex-
isting methods fall into one of two categories, either
those relying on information from WordNet (Mc-
Carthy and Carroll, 2003), or data-driven (Erk,
2007; Schulte im Walde, 2010; Pado et al, 2007).
For the purpose of our study, we focus on the latter.
2.1 Methods
For each verb in question, we have a frequency-
based ranking list of nouns co-occurring with it
(verb-object pairs) which we use for the first two
methods.
Latent semantic clustering (LSC) Rooth et
al. (1999) have proposed a soft-clustering method to
determine selectional preferences, which models the
joint distribution of nouns n and verbs v by condi-
tioning them on a hidden class c. The probability of
a pair (v, n) then equals
P (v, n) =
?
c?C
P (c)P (v|c)P (n|c) (1)
Similarity-based method The next classifier we
use combines similarity between nouns with rank-
ing information and is a modification of the method
described in (Pado et al, 2007). First, for all words
ni on the ranking list their frequency scores are nor-
malised between 0 and 1, fi. Then, they are weighed
by the similarity score between a new noun nj and a
corresponding word on the ranking list, ni, and the
noun with the highest score (1-nearest neighbour) is
selected:
argmax
ni
fi ? sim(nj , ni) (2)
Finally, two highest scores for each verb?s ranking
list are compared and the verb with higher score is
selected as a preferred one.
In addition, if we sum over all seen words instead
of choosing the nearest neighbour, this will lead to
the original approach by Pado et al (2007). In the
experimental part we consider both approaches (the
original method is referred to as SMP while the
nearest neighbour approach is marked by SMknn)
and study whether there is any difference between
the two when a verb that allows many different ar-
guments is considered (e.g., it may be better to use
the nearest neighbour approach for to do rather than
aggregating over all similarity scores).
Bag-of-words (BoW) approach This widely used
approach to document classification considers con-
textual words and their frequencies to represent doc-
uments (Zellig, 1954). We restrict the length of the
context around two verbs (within a window of ?2
and ?3 around the focus word, make or do) and
build a Naive Bayes classifier.
2.2 Data
Both verbs, to make and to do, license complements
of various kinds, e. g. they can be mono-transitive,
ditransitive, and complex transitive (sentences 1, 2,
and 3, respectively). Furthermore, make can be part
of idiomatic ditransitives (e.g., make use of, make
fun of, make room for) and phrasal mono-transitives
(e.g., make up) .
1. Andrew made [a cake]dobj .
2. Andrew made [his mum]iobj [a cake]dobj .
3. Andrew made [his mum]dobj happy.
For English, we use one of the largest cor-
pora available, the PukWAC (over 2 billion words,
30GB) (Baroni et al, 2009), which has been parsed
by MaltParser (Nivre and Scholz, 2004). We extract
all sentences with to do or to make (based on lem-
mata). The verb to make occurs in 2,13% of sen-
tences, and the verb to do in 3,27% of sentences in
the PukWAC corpus. Next, we exclude from con-
sideration phrasal mono-transitives and select sen-
tences where verb complements are nouns (Table 1).
For experiments in Dutch, we use the ?Wikipedia
Dump Of 2010? corpus, which is a part of Lassy
Large corpus (159 million tokens), and is parsed by
50
LANG # sent # dobj (to make) # dobj (to do)
EN 181,813,571 1,897,747 881,314
NL 8,639,837 15,510 6,197
Table 1: The number of sentences in English (EN) and Dutch (NL) corpora (the last two columns correspond to the
number of sentences where direct objects are nouns).
the Alpino parser (Bouma et al, 2001). Unlike in
English data, to make occurs here more often than
to do (3,3% vs. 1%). This difference can be ex-
plained by the fact that to do is also an auxiliary verb
in English which leads to more occurrences in to-
tal. Similarly to the English data set, phrasal mono-
transitives are filtered out. Finally, the sentences
that contain either to make or to do from wiki01 up
to wiki07 (19,847 sentences in total) have been se-
lected for training and wiki08 (1,769 sentences in
total) for testing. To be able to compare our results
against the performance on English data, we sample
a subset from PukWAC which is of the same size as
Dutch data set and is referred to as EN (sm).
To measure distributional similarity for the near-
est neighbour method, we use first-order and
second-order similarity based on Lin?s information
theoretic measure (Lin, 1998). For both languages,
similarity scores have been derived given a subset
of Wikipedia (276 million tokens for English and
114 million tokens for Dutch) using the DISCO
API (Kolb, 2009).
2.3 Results
Table 2 and Table 3 summarize our results. When re-
ferring to similarity-based methods, the symbols (f)
and (s) indicate first-order and second-order similar-
ity. For the BoW models, ?2 and ?3 corresponds
to the context length. The performance is measured
by true positive rate (TP) per class, overall accuracy
(Acc) and coverage (Cov). The former indicates in
how many cases the correct class label (make or do)
has been predicted, while the latter shows how many
examples a system was able to classify. Coverage is
especially indicative for LCS and semantic similar-
ity approaches because they may fail to yield pre-
dictions. For these methods, we provide two evalua-
tions. First, in order to be able to compare results
against the BoW approach, we measure accuracy
and coverage on all test examples. In such a case,
if some direct objects occur very often in the test set
and are classified correctly, accuracy scores will be
boosted. Therefore, we also provide the second eval-
uation where we measure accuracy and coverage on
(unique) test examples regardless of how frequent
they are. This evaluation will give us a better in-
sight into how well LCS and similarity-based meth-
ods work. Finally, we tested several settings for the
LSC method and the results presented here are ob-
tained for 20 clusters and 50 iterations. We remove
stop words 2 but do not take any other preprocessing
steps.
For both languages, it is more difficult to predict
to do than to make, although the differences in per-
formance on Dutch data (NL) are much smaller than
on English data (EN (sm)). An interesting obser-
vation is that using second-order similarity slightly
boosts performance for to make but is highly unde-
sirable for predicting to do (decrease in accuracy for
around 15%) in Dutch. This may be explained by the
fact that the objects of to do are already very generic.
Our findings on English data are that the similarity-
based approach is more sensitive to the choice of
aggregating over all words in the training set or se-
lecting the nearest neighbour. In particular, we ob-
tained better performance when choosing the nearest
neighbour for to do but aggregating over all scores
for to make. The results on Dutch and English data
are in general not always comparable. In addition
to the differences in performance of similarity-based
methods, the BoW models work better for predicting
to do in English but to make in Dutch.
As expected, similarity-based approaches yield
higher coverage than LSC, although the latter is su-
perior in terms of accuracy (in all cases but to do
in English). Since LSC turned out to be the most
computationally efficient method, we have also run
it on larger subsets of the PukWAC data set, up to
the entire corpus. We have not noticed any signifi-
2We use stop word lists for English and Dutch from http:
//snowball.tartarus.org/algorithms/.
51
LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)
EN (all) LSC 91.70 98.75 73.40 97.16 85.90 98.24
EN (sm) LSC 89.81 90.00 75.81 86.70 86.91 89.30
SMP (f) 84.89 98.82 69.89 95.14 81.78 98.03
SMP (s) 82.92 98.82 55.65 95.14 77.27 98.03
SMknn (f) 62.61 98.82 91.13 95.14 68.52 98.03
SMknn (s) 4.36 98.82 99.46 95.14 24.07 98.03
BoW ?2 36.41 100 82.21 100 46.01 100
BoW ?3 32.26 100 84.10 100 43.13 100
NL LSC 98.75 91.79 95.74 93.37 98.09 92.13
SMP (f) 95.64 95.82 92.97 98.14 95.06 96.32
SMP (s) 97.52 95.82 76.75 98.14 93.00 96.32
SMknn (f) 94.14 95.82 92.97 98.14 93.89 96.32
SMknn (s) 96.09 95.82 78.64 98.14 92.30 96.32
BoW ?2 89.34 100 61.19 100 83.44 100
BoW ?3 91.06 100 54.18 100 83.32 100
Table 2: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)
and Dutch (NL) data.
LANG Method TP (to make) Cov (to make) TP (to do) Cov (to do) Acc (all) Cov (all)
EN (sm) LSC 80.88 77.12 52.60 74.76 73.73 76.51
SMP (f) 73.17 97.29 45.99 90.78 66.49 95.60
SMP (s) 77.00 97.29 33.69 90.78 66.36 95.60
SMknn (f) 31.18 97.29 82.35 90.78 43.76 95.60
SMknn (s) 4.36 98.82 98.93 90.78 25.76 95.60
NL LSC 94.85 63.40 86.59 76.64 92.39 66.83
SMP (f) 87.55 81.37 77.00 93.45 84.24 84.50
SMP (s) 91.16 81.37 54.00 93.45 80.52 84.50
SMknn (f) 80.72 81.37 76.00 93.45 79.66 84.50
SMknn (s) 85.54 81.37 55.00 93.45 76.79 84.50
Table 3: True positive rate (TP, %), accuracy (Acc, %) and coverage (Cov, %) for the experiments on English (EN)
and Dutch (NL) unique direct objects.
cant changes in performance; the results for the en-
tire data set, EN (all), are given in the first row of
Table 2. Table 3 shows the results for the methods
using direct object information on unique objects,
which gives a more realistic assessment of their per-
formance. At closer inspection, we noticed that
many non-classified cases in Dutch refer to com-
pounds. For instance, bluegrassmuziek (bluegrass
music) cannot be compared against known words in
the training set. In order to cover such cases, existing
methods may benefit from morphological analysis.
3 Conclusions
In order to predict the use of two often confused
verbs, to make and to do, we have compared two
methods to modeling selectional preferences against
the bag-of-words approach. The BoW method is al-
ways outperformed by LCS and similarity-based ap-
proaches, although the differences in performance
are much larger for to do in Dutch and for to make
in English. In this study, we do not use any corpus of
non-native speakers? errors and explore how well it
is possible to predict one of two verbs provided that
the context words have been chosen correctly. In the
future work, we plan to label all incorrect uses of to
make and to do and to correct them.
Acknowledgments
The author thanks anonymous reviewers for their valu-
able comments. This work is supported by a VICI grant
number 277-80-002 by the Netherlands Organisation for
Scientific Research (NWO).
52
References
Marco Baroni and Silvia Bernardini and Adriano Fer-
raresi and Eros Zanchetta. 2009. The WaCky Wide
Web: A Collection of Very Large Linguistically Pro-
cessed Web-Crawled Corpora. Language Resources
and Evaluation 43(3), pp. 209-226.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage Computational Analysis
of Dutch. In Computational Linguistics in the Nether-
lands 2000. Enschede.
Jose?e A. Coenen, W. van Wiggen, and R. Bok-Bennema.
1979. Leren van fouten: een analyse van de meest
voorkomende Nederlandse taalfouten, die gemaakt
worden door Marokkaanse, Turkse, Spaanse en Por-
tugese kinderen. Amsterdam: Stichting ABC, Contac-
torgaan voor de Innovatie van het Onderwijs.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL 2007.
Prague, Czech Republic, 2007.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
Based Approach to Context-Sensitive Spelling Correc-
tion. Machine Learning 34(1-3), pp. 107-130.
Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Pro-
ceedings of the 17th Nordic Conference on Compu-
tational Linguistics - NODALIDA ?09, Odense, Den-
mark, May 2009.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-ACL
1998, Montreal.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4), pp. 639-654.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING 04.
Sebastian Pado?, Ulrike Pado? and Katrin Erk. 2007. Flex-
ible, Corpus-Based Modelling of Human Plausibility
Judgements. In Proceedings of EMNLP/CoNLL 2007.
Prague, Czech Republic, pp. 400-409.
Mats Rooth, Stefan Riezler and Detlef Prescher. 1999.
Inducing a Semantically Annotated Lexicon via EM-
Based Clustering. In Proceedings of ACL 99.
Anna Rozovskaya and Dan Roth. 2010. Generating
Confusion Sets for Context-Sensitive Error Correction.
In Proceedings of EMNLP, pp. 961-970.
Sabine Schulte im Walde. 2010. Comparing Com-
putational Approaches to Selectional Preferences ?
Second-Order Co-Occurrence vs. Latent Semantic
Clusters. In Proceedings of the 7th International Con-
ference on Language Resources and Evaluation, Val-
letta, Malta, pp. 1381?1388.
Ben?at Zapirain, Eneko Agirre and Llu??s Ma`rquez. 2009.
Generalizing over Lexical Features: Selectional Pref-
erences for Semantic Role Classification. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers. Suntec, Singapore, pp. 73-76.
Harris Zellig. 1954. Distributional Structure. Word 10
(2/3), p. 146-62.
53
