Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 121?130,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Rich Feature Vector for Protein-Protein Interaction Extraction from
Multiple Corpora
Makoto Miwa1 Rune S?tre1 Yusuke Miyao1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,rune.saetre,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Because of the importance of protein-
protein interaction (PPI) extraction from
text, many corpora have been proposed
with slightly differing definitions of pro-
teins and PPI. Since no single corpus is
large enough to saturate a machine learn-
ing system, it is necessary to learn from
multiple different corpora. In this paper,
we propose a solution to this challenge.
We designed a rich feature vector, and we
applied a support vector machine modi-
fied for corpus weighting (SVM-CW) to
complete the task of multiple corpora PPI
extraction. The rich feature vector, made
from multiple useful kernels, is used to
express the important information for PPI
extraction, and the system with our fea-
ture vector was shown to be both faster
and more accurate than the original kernel-
based system, even when using just a sin-
gle corpus. SVM-CW learns from one cor-
pus, while using other corpora for support.
SVM-CW is simple, but it is more effec-
tive than other methods that have been suc-
cessfully applied to other NLP tasks ear-
lier. With the feature vector and SVM-
CW, our system achieved the best perfor-
mance among all state-of-the-art PPI ex-
traction systems reported so far.
1 Introduction
The performance of an information extraction pro-
gram is highly dependent on various factors, in-
cluding text types (abstracts, complete articles, re-
ports, etc.), exact definitions of the information to
be extracted, shared sub-topics of the text collec-
tions from which information is to be extracted.
Even if two corpora are annotated in terms of the
same type of information by two groups, the per-
formance of a program trained by one corpus is
unlikely to be reproduced in the other corpus. On
the other hand, from a practical point of view, it is
worth while to effectively use multiple existing an-
notated corpora together, because it is very costly
to make new annotations.
One problem with several different corpora is
protein-protein interaction (PPI) extraction from
text. While PPIs play a critical role in un-
derstanding the working of cells in diverse bio-
logical contexts, the manual construction of PPI
databases such as BIND, DIP, HPRD, IntAct, and
MINT (Mathivanan et al, 2006) is known to be
very time-consuming and labor-intensive. The au-
tomatic extraction of PPI from published papers
has therefore been a major research topic in Natu-
ral Language Processing for Biology (BioNLP).
Among several PPI extraction task settings, the
most common is sentence-based, pair-wise PPI ex-
traction. At least four annotated corpora have been
provided for this setting: AIMed (Bunescu et al,
2005), HPRD50 (Fundel et al, 2006), IEPA (Ding
et al, 2002), and LLL (Ne?dellec, 2005). Each of
these corpora have been used as the standard cor-
pus for training and testing PPI programs. More-
over, several corpora are annotated for more types
of events than just for PPI. Such examples include
BioInfer (Pyysalo et al, 2007), and GENIA (Kim
et al, 2008a), and they can be reorganized into PPI
corpora. Even though all of these corpora were
made for PPI extraction, they were constructed
based on different definitions of proteins and PPI,
which reflect different biological research inter-
ests (Pyysalo et al, 2008).
Research on PPI extraction so far has revealed
that the performance on each of the corpora could
121
benefit from additional examples (Airola et al,
2008). Learning from multiple annotated cor-
pora could lead to better PPI extraction perfor-
mance. Various research paradigms such as induc-
tive transfer learning (ITL) and domain adaptation
(DA) have mainly focused on how to effectively
use corpora annotated by other groups, by reduc-
ing the incompatibilities (Pan and Yang, 2008).
In this paper, we propose the extraction of PPIs
from multiple different corpora. We design a rich
feature vector, and as an ITL method, we ap-
ply a support vector machine (SVM) modified for
corpus weighting (SVM-CW) (Schweikert et al,
2008), in order to evaluate the use of multiple cor-
pora for the PPI extraction task. Our rich feature
vector is made from multiple useful kernels, each
of which is based on multiple parser inputs, pro-
posed by Miwa et al (2008). The system with our
feature vector was better than or at least compa-
rable to the state-of-the-art PPI extraction systems
on every corpus. The system is a good starting
point to use the multiple corpora. Using one of the
corpora as the target corpus, SVM-CW weights
the remaining corpora (we call them the source
corpora) with ?goodness? for training on the tar-
get corpus. While SVM-CW is simple, we show
that SVM-CW can improve the performance of the
system more effectively and more efficiently than
other methods proven to be successful in other
NLP tasks earlier. As a result, SVM-CW with our
feature vector is comprised of a PPI system with
five different models, of which each model is su-
perior to the best model in the original PPI extrac-
tion task, which used only the single corpus.
2 Related Works
While sentence-based, pair-wise PPI extraction
was initially tackled by using simple methods
based on co-occurrences, lately, more sophisti-
cated machine learning systems augmented by
NLP techniques have been applied (Bunescu et al,
2005). The task has been tackled as a classifica-
tion problem. To pull out useful information from
NLP tools including taggers and parsers, several
kernels have been applied to calculate the similar-
ity between PPI pairs. Miwa et al (2008) recently
proposed the use of multiple kernels using multi-
ple parsers. This outperformed other systems on
the AIMed, which is the most frequently used cor-
pus for the PPI extraction task, by a wide margin.
To improve the performance using external
Classification
Result
Training
Data
Feature 
vector
Raw Texts
Parsers
Classifier
Test 
Data
Raw Texts
Model
Pair Information
Pair Information
Label
Figure 1: Overview of our PPI extraction system
training data, many ITL and DA methods have
been proposed. Most of ITL methods assume that
the feature space is same, and that the labels may
be different in only some examples, while most of
DA methods assume that the labels are the same,
and that the feature space is different. Among the
methods, we use adaptive SVM (aSVM) (Yang et
al., 2007), singular value decomposition (SVD)
based alternating structure optimization (SVD-
ASO) (Ando et al, 2005), and transfer AdaBoost
(TrAdaBoost) (Dai et al, 2007) to compare with
SVM-CW. We do not use semi-supervised learn-
ing (SSL) methods, because it would be consid-
erably costly to generate enough clean unlabeled
data needed for SSL (Erkan et al, 2007). aSVM
is seen as a promising DA method among sev-
eral modifications of SVM including SVM-CW.
aSVM tries to find a model that is close to the one
made from other classification problems. SVD-
ASO is one of the most successful SSL, DA, or
multi-task learning methods in NLP. The method
tries to find an additional useful feature space by
solving auxiliary problems that are close to the tar-
get problem. With well-designed auxiliary prob-
lems, the method has been applied to text clas-
sification, text chunking, and word sense disam-
biguation (Ando, 2006). The method was reported
to perform better than or comparable to the best
state-of-the-art systems in all of these tasks. TrAd-
aBoost was proposed as an ITL method. In train-
ing, the method reduces the effect of incompatible
examples by decreasing their weights, and thereby
tries to use useful examples from source corpora.
The method has been applied to text classifica-
tion, and the reported performance was better than
SVM and transductive SVM (Dai et al, 2007).
3 PPI Extraction System
The target task of our system is a sentence-based,
pair-wise PPI extraction. It is formulated as a clas-
sification problem that judges whether a given pair
122
XPGp1 protein interacts with multiple subunits of
TFIIHprot and with CSBp2 protein.
Figure 2: A sentence including an interacting pro-
tein pair (p1, p2). (AIMed PMID 8652557, 9th
sentence, 3rd pair)
BOW
v-walks
e-walks
Graph BOW
v-walks
e-walks
Graph
Normalization
Parsers
KSDEPEnju
a sentence including a pair
feature vector
BOW Graph BOW
v-walks
e-walks
Graph
v-walks
e-walks
Figure 3: Extraction of a feature vector from the
target sentence
of proteins in a sentence is interacting or not. Fig-
ure 2 shows an example of a sentence in which the
given pair (p1 and p2) actually interacts.
Figure 1 shows the overview of the proposed
PPI extraction system. As a classifier using a sin-
gle corpus, we use the 2-norm soft-margin lin-
ear SVM (L2-SVM) classifier, with the dual co-
ordinate decent (DCD) method, by Hsieh et al
(2008). In this section, we explain the two main
features: the feature vector, and the corpus weight-
ing method for multiple corpora.
3.1 Feature Vector
We propose a feature vector with three types of
features, corresponding to the three different ker-
nels, which were each combined with the two
parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao
et al, 2008); this feature vector is used because the
kernels with these parsers were shown to be effec-
tive for PPI extraction by Miwa et al (2008), and
because it is important to start from a good per-
formance single corpus system. Both parsers were
retrained using the GENIA Treebank corpus pro-
vided by Kim et al (2003). By using our linear
feature vector, we can perform calculations faster
by using fast linear classifiers like L2-SVM, and
we also obtain a more accurate extraction, than by
using the original kernel method.
Figure 3 summarizes the way in which the fea-
ture vector is constructed. The system extracts
Bag-of-Words (BOW), shortest path (SP), and
graph features from the output of two parsers. The
PROT M:1, and M:1, interact M:1, multiple M:1,
of M:1, protein M:1, subunit M:1, with M:2, pro-
tein A:1
Figure 4: Bag-of-Words features of the pair in Fig-
ure 2 with their positions (B:Before, M:in the Mid-
dle of, A:After) and frequencies.
NMOD SBJ
rNMOD
ENTITY1 protein interact ENTITY2protein protein
ENTITY1 protein interacts with multiple and with ENTITY2 protein .
NMOD SBJ
COOD
COORD
NMOD
PMOD
NMOD SBJ
rNMOD
protein interact protein
SBJ rCOOD
rPMOD
V-walks  
E-walks
???
???
???
Figure 5: Vertex walks, edge walks in the upper
shortest path between the proteins in the parse tree
by KSDEP. The walks and their subsets are used
as the shortest path features of the pair in Figure 2.
output is grouped according to the feature-type
and parser, and each group of features is separately
normalized by the L2-norm1. Finally, all values
are put into a single feature vector, and the whole
feature vector is then also normalized by the L2-
norm. The features are constructed by using pred-
icate argument structures (PAS) from Enju, and by
using the dependency trees from KSDEP.
3.1.1 Bag-of-Words (BOW) Features
The BOW feature includes the lemma form of a
word, its relative position to the target pair of pro-
teins (Before, Middle, After), and its frequency in
the target sentence. BOW features form the BOW
kernel in the original kernel method. BOW fea-
tures for the pair in Figure 2 are shown in Figure 4.
3.1.2 Shortest Path (SP) Features
SP features include vertex walks (v-walks), edge
walks (e-walks), and their subsets (Kim et al,
2008b) on the target pair in a parse structure, and
represent the connection between the pair. The
features are the subsets of the tree kernels on the
shortest path (S?tre et al, 2007). Figure 5 illus-
trates the shortest path between the pair in Fig-
ure 2, and its v-walks and e-walks extracted from
the shortest path in the parse tree by KSDEP. A
v-walk includes two lemmas and their link, while
1The vector normalized by the L2-norm is also called a
unit vector.
123
an e-walk includes a lemma and its two links. The
links indicates the predicate argument relations for
PAS, and the dependencies for dependency trees.
3.1.3 Graph Features
Graph features are made from the all-paths graph
kernel proposed by Airola et al (2008). The ker-
nel represents the target pair using graph matrices
based on two subgraphs, and the graph features are
all the non-zero elements in the graph matrices.
The two subgraphs are a parse structure sub-
graph (PSS) and a linear order subgraph (LOS).
Figure 6 describes the subgraphs of the sentence
parsed by KSDEP in Figure 2. PSS represents the
parse structure of a sentence. PSS has word ver-
tices or link vertices. A word vertex contains its
lemma and its part-of-speech (POS), while a link
vertex contains its link. Additionally, both types
of vertices contain their positions relative to the
shortest path. The ?IP?s in the vertices on the
shortest path represent the positions, and the ver-
tices are differentiated from the other vertices like
?P?, ?CC?, and ?and:CC? in Figure 6. LOS repre-
sents the word sequence in the sentence. LOS has
word vertices, each of which contains its lemma,
its relative position to the target pair, and its POS.
Each subgraph is represented by a graph matrix
G as follows:
G = L
T
?
?
n=1
A
n
L, (1)
where L is a N?L label matrix, A is an N?N
edge matrix, N represents the number of vertices,
and L represents the number of labels. The la-
bel of a vertex includes all information described
above (e.g. ?ENTITY1:NN:IP? in Figure 6). If
two vertices have exactly same information, the
labels will be same. G can be calculated effi-
ciently by using the Neumann Series (Airola et al,
2008). The label matrix represents the correspon-
dence between labels and vertices. L
ij
is 1 if the
i-th vertex corresponds to the j-th label, and 0 oth-
erwise. The edge matrix represents the connection
between the pairs of vertices. A
ij
is a weight w
ij
(0.9 or 0.3 in Figure 6 (Airola et al, 2008)) if the
i-th vertex is connected to the j-th vertex, and 0
otherwise. By this calculation, G
ij
represent the
sum of the weights of all paths between the i-th
label and the j-th label.
A B H I L
positive 1,000 2,534 163 335 164
all 5,834 9,653 433 817 330
Table 1: The sizes of used PPI corpora. A:AIMed,
B:BioInfer, H:HPRD50, I:IEPA, and L:LLL.
50
60
70
80
90
100
0 20 40 60 80 100
% examples
AImed (F)
BioInfer (F)
AImed (AUC)
BioInfer (AUC)
Figure 7: Learning curves on two large corpora.
The x-axis is related to the percentage of the ex-
amples in a corpus. The curves are obtained by a
10-fold CV with a random split.
3.2 Corpus Weighting for Mixing Corpora
Table 1 shows the sizes of the PPI corpora that we
used. Their widely-ranged differences including
the sizes were manually analyzed by Pyysalo et
al. (2008). While AIMed, HPRD50, IEPA, and
LLL were all annotated as PPI corpora, BioInfer in
its original form contains much more fine-grained
information than does just the PPI. BioInfer was
transformed into a PPI corpus by a program, so
making it the largest of the five. Among them,
AIMed alone was created by annotating whole ab-
stracts, while the other corpora were made by an-
notating single sentences selected from abstracts.
Figure 7 shows the learning curves on two large
corpora: AIMed and BioInfer. The curves are
obtained by performing a 10-fold cross valida-
tion (CV) on each corpus, with random splits, us-
ing our system. The curves show that the perfor-
mances can benefit from the additional examples.
To get a better PPI extraction system for a chosen
target, we need to draw useful shared information
from external source corpora. We refer to exam-
ples in the source corpora as ?source examples?,
and examples in a target corpus as ?target exam-
ples?. Among the corpora, we assume that the la-
bels in some examples are incompatible, and that
their distributions are also different, but that the
feature space is shared.
In order to draw useful information from the
source corpora to get a better model for the target
124
ENTITY1
NN
IP
protein
NN
IP
interact
VBZ
IP
with
IN
IP
multiple
JJ
subunit
NNS
of
IN
PROT
NN
and
CC
with
IN
IP
ENTITY2
NN
IP
protein
NN
IP
.
.
NMOD
IP
SBJ
IP
COOD
IP
PMOD
NMOD NMOD
PMOD
CC
COORD
IP
NMOD
IP
PMOD
IP
P
ENTITY1
NN
protein
NN
M
interact
VBZ
M
with
IN
M
multiple
JJ
M
subunit
NNS
M
of
IN
M
PROT
NN
M
and
CC
M
with
IN
M
ENTITY2
NN
protein
NN
A
.
.
0.9,            0.3
IP: In shortest Path, B:Before, M:in the Middle of, A:After
Figure 6: Parse structure subgraph and linear order subgraph to extract graph features of the pair in
Figure 2. The parse structure subgraph is from the parse tree by KSDEP.
corpus, we use SVM-CW, which has been used
as a DA method. Given a set of instance-label
pairs (xi, yi), i = 1, . . ., ls + lt, xi?Rn, and
y
i
?{?1,+1}, we solve the following problem:
min
w
1
2
w
T
w + C
s
ls
?
i=1
`
i
+ C
t
ls+lt
?
i=ls+1
`
i
, (2)
where w is a weight vector, ` is a loss function,
and ls and lt are the numbers of source and target
examples respectively. C
s
? 0 and C
t
? 0 are
penalty parameters. We use a squared hinge loss
`
i
= max(0, 1? y
i
w
T
xi)2. Here, the source cor-
pora are treated as one corpus. The problem, ex-
cluding the second term, is equal to L2-SVM. The
problem can be solved using the DCD method.
As an ITL method, SVM-CW weights each cor-
pus, and tries to benefit from the source corpora,
by adjusting the effect of their compatibility and
incompatibility. For the adjustment, these penalty
parameters should be set properly. Since we are
unaware of the widely ranged differences among
the corpora, we empirically estimated them by
performing 10-fold CV on the training data.
4 Evaluation
4.1 Evaluation Settings
We used five corpora for evaluation: AIMed,
BioInfer, HPRD50, IEPA, and LLL. For the com-
parison with other methods, we report the F-
score (%), and the area under the receiver op-
erating characteristic (ROC) curve (AUC) (%)
using (abstract-wise) a 10-fold CV and a one-
answer-per-occurrence criterion. These measures
are commonly used for the PPI extraction tasks.
The F-score is a harmonic mean of Precision and
Recall. The ROC curve is a plot of a true posi-
tive rate (TPR) vs a false positive rate (FPR) for
different thresholds. We tuned the regularization
parameters of all classifiers by performing a 10-
fold CV on the training data using a random split.
The other parameters were fixed, and we report the
highest of the macro-averaged F-scores as our fi-
nal F-score. For 10-fold CV, we split the corpora
as recommended by Airola et al (2008).
4.2 PPI Extraction on a Single Corpus
In this section, we evaluate our system on a single
corpus, in order to evaluate our feature vector and
to justify the use of the following modules: nor-
malization methods and classification methods.
First, we compare our preprocessing method
with other preprocessing methods to confirm how
our preprocessing method improves the perfor-
mance. Our method produced 64.2% in F-score
using L2-SVM on AIMed. Scaling all features in-
dividually to have a maximal absolute value of 1,
produced only 44.2% in the F-score, while nor-
malizing the feature vector by L2-norm produced
61.5% in the F-score. Both methods were inferior
to our method, because the values of features in
the same group should be treated together, and be-
cause the values of features in the different groups
should not have a big discrepancy. Weighting each
125
L2 L1 LR AP CW
F 64.2 64.0 64.2 62.7 63.0
AUC 89.1 88.8 89.0 88.5 87.8
Table 2: Classification performance on AIMed us-
ing five different linear classifiers. The F-score (F)
and Area Under the ROC curve (AUC) are shown.
L2 is L2-SVM, L1 is L1-SVM, LR is logistic re-
gression, AP is averaged perceptron, and CW is
confidence weighted linear classification.
group with different values can produce better re-
sults, as will be explored in our future work.
Next, using our feature vector, we applied
five different linear classifiers to extract PPI
from AIMed: L2-SVM, 1-norm soft-margin
SVM (L1-SVM), logistic regression (LR) (Fan
et al, 2008), averaged perceptron (AP) (Collins,
2002), and confidence weighted linear classifica-
tion (CW) (Dredze et al, 2008). Table 2 indicates
the performance of these classifiers on AIMed.
We employed better settings for the task than did
the original methods for AP and CW. We used a
Widrow-Hoff learning rule (Bishop, 1995) for AP,
and we performed one iteration for CW. L2-SVM
is as good as, if not better, than other classifiers (F-
score and AUC). In the least, L2-SVM is as fast as
these classifiers. AP and CW are worse than the
other three methods, because they require a large
number of examples, and are un-suitable for the
current task. This result indicates that all linear
classifiers, with the exception of AP and CW, per-
form almost equally, when using our feature vec-
tor.
Finally, we implemented the kernel method by
Miwa et al (2008). For a 10-fold CV on AIMed,
the running time was 9,507 seconds, and the per-
formance was 61.5% F-score and 87.1% AUC.
Our system used 4,702 seconds, and the perfor-
mance was 64.2% F-score and 89.1% AUC. This
result displayed that our system, with L2-SVM,
and our new feature vector, is better, and faster,
than the kernel-based system.
4.3 Evaluation of Corpus Weighting
In this section, we first apply each model from a
source corpus to a target corpus, to show how dif-
ferent the corpora are. We then evaluate SVM-CW
by comparing it with three other methods (see Sec-
tion 2) with limited features, and apply it to every
corpus.
0
10
20
30
40
50
60
70
80
90
AIMed BioInfer HPRD50 IEPA LLL
F
Target corpus
AIMed
BioInfer
HPRD50
IEPA
LLL
co-occ
Model
Figure 8: F-score on a target corpus using a model
on a source corpus. For the comparison, we show
the 10-fold CV result on each target corpus and
co-occurrences. The regularization parameter was
fixed to 1.
First, we apply the model from a source corpus
to a target corpus. Figure 8 shows how the model
from a source corpus performs on the target cor-
pus. Interestingly, the model from IEPA performs
better on LLL than the model from LLL itself. All
the results showed that using different corpora (ex-
cept IEPA) is worse than just using the same cor-
pora. However, the cross-corpora scores are still
better than the co-occurrences base-line, which in-
dicates that the corpora share some information,
even though they are not fully compatible.
Next, we compare SVM-CW with three other
methods: aSVM, SVD-ASO, and TrAdaBoost.
For this comparison, we used our feature vec-
tor without including the graph features, because
SVD-ASO and TrAdaBoost require large compu-
tational resources. We applied SVD-ASO and
TrAdaBoost in the following way. As for SVD-
ASO, we made 400 auxiliary problems from the
labels of each corpus by splitting features ran-
domly, and extracted 50 additional features each
for 4 feature groups. In total, we made new 200
additional features from 2,000 auxiliary problems.
As recommended by Ando et al (2005), we re-
moved negative weights, performed SVD to each
feature group, and iterated ASO once. Since Ad-
aBoost easily overfitted with our rich feature vec-
tor, we applied soft margins (Ratsch et al, 2001)
to TrAdaBoost. The update parameter for source
examples was calculated using the update param-
eter on the training data in AdaBoost and the orig-
inal parameter in TrAdaBoost. This ensures that
the parameter would be the same as the original
parameter, when the C value in the soft margin ap-
proaches infinity.
126
aSVM SVD-ASO TrAdaBoost SVM-CW L2-SVM
F AUC F AUC F AUC F AUC F AUC
AIMed 63.6 88.4 62.9 88.3 63.4 88.4 64.0 88.6 63.2 88.4
BioInfer 66.5 85.2 65.7 85.1 66.1 85.2 66.7 85.4 66.2 85.1
HPRD50 71.2 84.3 68.7 80.8 72.6 85.3 72.7 86.4 67.2 80.7
IEPA 73.8 85.4 72.3 83.8 74.3 86.3 75.2 85.9 73.0 84.7
LLL 85.9 89.2 79.3 85.5 86.5 88.8 86.9 90.3 80.3 86.3
Table 3: Comparison of methods on multiple corpora. Our feature vector without graph features is used.
The source corpora with the best F-scores are reported for aSVM, TrAdaBoost, and SVM-CW.
F-score AUC
A B H I L all A B H I L all
A (64.2) 64.0 64.7 65.2 63.7 64.2 (89.1) 89.5 89.2 89.3 89.0 89.4
B 67.9 (67.6) 67.9 67.9 67.7 68.3 86.2 (86.1) 86.2 86.3 86.2 86.4
H 71.3 71.2 (69.7) 74.1 70.8 74.9 84.7 85.0 (82.8) 85.0 83.4 87.9
I 74.4 75.6 73.7 (74.4) 74.4 76.6 86.7 87.1 85.4 (85.6) 86.9 87.8
L 83.2 85.9 82.0 86.7 (80.5) 84.1 86.3 87.1 87.4 90.8 (86.0) 86.2
Table 4: F-score and AUC by SVM-CW. Rows correspond to a target corpus, and columns a source
corpus. A:AIMed, B:BioInfer, H:HPRD50, I:IEPA, and L:LLL corpora. ?all? signifies that all source
corpora are used as one source corpus, ignoring the differences among the corpora. For the comparison,
we show the 10-fold CV result on each target corpus.
In Table 3, we demonstrate the results of the
comparison. SVM-CW improved the classifica-
tion performance at least as much as all the other
methods. The improvement is mainly attributed to
the aggressive use of source examples while learn-
ing the model. Some source examples can be used
as training data, as indicated in Figure 8. SVM-
CW does not set the restriction between C
s
and
C
t
in Equation (2), so it can use source exam-
ples aggressively while learning the model. Since
aSVM transfers a model, and SVD-ASO transfers
an additional feature space, aSVM and SVD-ASO
do not use the source examples while learning the
model. In addition to the difference in the data us-
age, the settings of aSVM and SVD-ASO do not
match the current task. As for aSVM, the DA as-
sumption (that the labels are the same) does not
match the task. In SVD-ASO, the numbers of both
source examples and auxiliary problems are much
smaller than those reported by Ando et al (2005).
TrAdaBoost uses the source examples while learn-
ing the model, but never increases the weight of
the examples, and it attempts to reduce their ef-
fects.
Finally, we apply SVM-CW to all corpora using
all features. Table 4 summarizes the F-score and
AUC by SVM-CW with all features. SVM-CW
is especially effective for small corpora, show-
ing that SVM-CW can adapt source corpora to a
small annotated target corpus. The improvement
on AIMed is small compared to the improvement
on BioInfer, even though these corpora are sim-
ilar in size. One of the reasons for this is that
whole abstracts are annotated in AIMed, therefore
making the examples biased. The difference be-
tween L2-SVM and SVM-CW + IEPA on AIMed
is small, but statistically, it is significant (McNe-
mar test (McNemar, 1947), P = 0.0081). In the
cases of HPRD50 + IEPA, LLL + IEPA, and two
folds in BioInfer + IEPA, C
s
is larger than C
t
in
Equation (2). This is worth noting, because the
source corpus is more weighted than the target cor-
pus, and the prediction performance on the tar-
get corpus is improved. Most methods put more
trust in the target corpus than in the source cor-
pus, and our results show that this setting is not al-
ways effective for mixing corpora. The results also
indicate that IEPA contains more useful informa-
tion for extracting PPI than other corpora, and that
using source examples aggressively is important
for these combinations. We compared the results
of L2-SVM and SVM-CW + IEPA on AIMed,
and found that 38 pairs were described as ?inter-
action? or ?binding? in the sentences among 61
127
SVM-CW L2-SVM Airola et al
F AUC F AUC F AUC
A 65.2 89.3 64.2 89.1 56.4 84.8
B 68.3 86.4 67.6 86.1 61.3 81.9
H 74.9 87.9 69.7 82.8 63.4 79.7
I 76.6 87.8 74.4 85.6 75.1 85.1
L 86.7 90.8 80.5 86.0 76.8 83.4
Table 6: Comparison with the results by Airola
et al (2008). A:AIMed, B:BioInfer, H:HPRD50,
I:IEPA, and L:LLL corpora. The results with the
highest F-score from Table 4 are reported as the
results for SVM-CW.
newly found pairs. This analysis is evidence that
IEPA contains instances to help find such inter-
actions, and that SVM-CW helps to collect gold
pairs that lack enough supporting instances in a
single corpus, by adding instances from other cor-
pora. SVM-CW missed coreferential relations that
were also missed by L2-SVM. This can be at-
tributed to the fact that the coreferential informa-
tion is not stored in our current feature vector; so
we need an even more expressive feature space.
This is left as future work.
SVM-CW is effective on most corpus combi-
nations, and all the models from single corpora
can be improved by adding other source corpora.
This result is impressive, because the baselines by
L2-SVM on just single corpora are already better
than or at least comparable to other state-of-the-art
PPI extraction systems, and also because the vari-
ety of the differences among different corpora is
quite wide depending on various factors including
annotation policies of the corpora (Pyysalo et al,
2008). The results suggest that SVM-CW is useful
as an ITL method.
4.4 Comparison with Other PPI Systems
We compare our system with other previously
published PPI extraction systems. Tables 5 and
6 summarize the comparison. Table 5 summa-
rizes the comparison of several PPI extraction sys-
tems evaluated on the AIMed corpus. As indi-
cated, the performance of the heavy kernel method
is lower than our fast rich feature-vector method.
Our system is, to the extent of our knowledge, the
best performing PPI extraction system evaluated
on the AIMed corpus, both in terms of AUC and
F-scores. Airola et al (2008) first reported results
using all five corpora. We cannot directly com-
pare our result with the F-score results, because
they tuned the threshold, but our system still out-
performs the system by Airola et al (2008) on ev-
ery corpus in AUC values. The results also indi-
cate that our system outperforms other systems on
all PPI corpora, and that both the rich feature vec-
tor and the corpus weighting are effective for the
PPI extraction task.
5 Conclusion
In this paper, we proposed a PPI extraction system
with a rich feature vector, using a corpus weight-
ing method (SVM-CW) for combining the mul-
tiple PPI corpora. The feature vector extracts as
much information as possible from the main train-
ing corpus, and SVM-CW incorporate other exter-
nal source corpora in order to improve the perfor-
mance of the classifier on the main target corpus.
To the extent of our knowledge, this is the first ap-
plication of ITL and DA methods to PPI extrac-
tion. As a result, the system, with SVM-CW and
the feature vector, outperformed all other PPI ex-
traction systems on all of the corpora. The PPI
corpora share some information, and it is shown
to be effective to add other source corpora when
working with a specific target corpus.
The main contributions of this paper are: 1)
conducting experiments in extracting PPI using
multiple corpora, 2) suggesting a rich feature
vector using several previously proposed features
and normalization methods, 3) the combination of
SVM with corpus weighting and the new feature
vector improved results on this task compared with
prior work.
There are many differences among the corpora
that we used, and some of the differences are still
unresolved. For further improvement, it would be
necessary to investigate what is shared and what
is different among the corpora. The SVM-CW
method, and the PPI extraction system, can be ap-
plied generally to other classification tasks, and
to other binary relation extraction tasks, without
the need for modification. There are several other
tasks in which many different corpora, which at
first glance seem compatible, exist. By apply-
ing SVM-CW to such corpora, we will analyze
which differences can be resolved by SVM-CW,
and what differences require a manual resolution.
For the PPI extraction system, we found many
false negatives that need to be resolved. For fur-
ther improvement, we need to analyze the cause
128
positive all P R F AUC
SVM-CW 1,000 5,834 60.0 71.9 65.2 89.3
L2-SVM 1,000 5,834 62.7 66.6 64.2 89.1
(Miwa et al, 2008) 1,005 5,648 60.4 69.3 64.2 (61.5) 87.9 (87.1)
(Miyao et al, 2008) 1,059 5,648 54.9 65.5 59.5
(Airola et al, 2008) 1,000 5,834 52.9 61.8 56.4 84.8
(S?tre et al, 2007) 1,068 5,631 64.3 44.1 52.0
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0
(Bunescu and Mooney, 2005) 65.0 46.4 54.2
Table 5: Comparison with previous PPI extraction results on the AIMed corpus. The numbers of positive
and all examples, precision (P), recall (R), F-score (F), and AUC are shown. The result with the highest
F-score from Table 4 is reported as the result for SVM-CW. The scores in the parentheses of Miwa et al
(2008) indicate the result using the same 10-fold splits as our result, as indicated in Section 4.2.
of these false negatives more deeply, and design a
more discriminative feature space. This is left as a
future direction of our work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Scientific Research (C) (General) (MEXT, Japan).
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross corpus learn-
ing. BMC Bioinformatics.
Rie Kubota Ando, Tong Zhang, and Peter Bartlett.
2005. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. Jour-
nal of Machine Learning Research, 6:1817?1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL-X),
pages 77?84, June.
C. M. Bishop. 1995. Neural Networks for Pattern
Recognition. Oxford University Press.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS 2005.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP 2002,
pages 1?8.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong
Yu. 2007. Boosting for transfer learning. In ICML
2007, pages 193?200.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML 2008, pages 264?271.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extract-
ing protein interaction sentences using dependency
parsing. In EMNLP 2007.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear SVM. In ICML 2008, pages 408?415.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19:i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008a. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
129
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Suresh Mathivanan, Balamurugan Periaswamy, TKB
Gandhi, Kumaran Kandasamy, Shubha Suresh, Riaz
Mohmood, YL Ramachandra, and Akhilesh Pandey.
2006. An evaluation of human protein-protein inter-
action data in the public domain. BMC Bioinformat-
ics, 7 Suppl 5:S19.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157, June.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining mul-
tiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining
in Biomedicine (SMBM 2008), pages 101?108.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya
Matsuzaki, and Jun?ichi Tsujii. 2008. Task-
oriented evaluation of syntactic parsers and their
representations. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguistics
(ACL?08:HLT).
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Sinno Jialin Pan and Qiang Yang. 2008. A survey on
transfer learning. Technical Report HKUST-CS08-
08, Department of Computer Science and Engineer-
ing, Hong Kong University of Science and Technol-
ogy, Hong Kong, China, November.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Gunnar Ratsch, Takashi Onoda, and Klaus-Robert
Muller. 2001. Soft margins for adaboost. Machine
Learning, 42(3):287?320.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Syntactic features for protein-protein interaction ex-
traction. In LBM 2007 short papers.
Gabriele Schweikert, Christian Widmer, Bernhard
Scho?lkopf, and Gunnar Ra?tsch. 2008. An empir-
ical analysis of domain adaptation algorithms for
genomic sequence analysis. In NIPS, pages 1433?
1440.
Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007. Cross-domain video concept detection using
adaptive SVMs. In MULTIMEDIA ?07: Proceed-
ings of the 15th international conference on Multi-
media, pages 188?197.
130
Proceedings of the Workshop on BioNLP: Shared Task, pages 103?106,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
From Protein-Protein Interaction to Molecular Event Extraction
Rune S?tre?, Makoto Miwa?, Kazuhiro Yoshida? and Jun?ichi Tsujii?
{rune.saetre,mmiwa,kyoshida,tsujii}@is.s.u-tokyo.ac.jp
?Department of Computer Science
?Information Technology Center
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
Abstract
This document describes the methods and re-
sults for our participation in the BioNLP?09
Shared Task #1 on Event Extraction. It also
contains some error analysis and a brief dis-
cussion of the results. Previous shared tasks in
the BioNLP community have focused on ex-
tracting gene and protein names, and on find-
ing (direct) protein-protein interactions (PPI).
This year?s task was slightly different, since
the protein names were already manually an-
notated in the text. The new challenge was
to extract biological events involving these
given gene and gene products. We modi-
fied a publicly available system (AkanePPI)
to apply it to this new, but similar, protein
interaction task. AkanePPI has previously
achieved state-of-the-art performance on all
existing public PPI corpora, and only small
changes were needed to achieve competitive
results on this event extraction task. Our of-
ficial result was an F-score of 36.9%, which
was ranked as number six among submissions
from 24 different groups. We later balanced
the recall/precision by including more predic-
tions than just the most confident one in am-
biguous cases, and this raised the F-score on
the test-set to 42.6%. The new Akane program
can be used freely for academic purposes.
1 Introduction
With the increasing number of publications report-
ing on protein interactions, there is also a steadily
increasing interest in extracting information from
Biomedical articles by using Natural Language Pro-
cessing (BioNLP). There has been several shared
tasks arranged by the BioNLP community to com-
pare different ways of doing such Information Ex-
traction (IE), as reviewed in Krallinger et al(2008).
Earlier shared tasks have dealt with Protein-
Protein Interaction (PPI) in general, but this
task focuses on more specific molecular events,
such as Gene expression, Transcription, Pro-
tein catabolism, Localization and Binding, plus
(Positive or Negative) Regulation of proteins or
other events. Most of these events are related to PPI,
so our hypothesis was that one of the best perform-
ing PPI systems would perform well also on this
new event extraction task. We decided to modify a
publicly available system with flexible configuration
scripting (Miwa et al, 2008). Some adjustments had
to be made to the existing system, like adding new
types of Named Entities (NE) to represent the events
mentioned above. The modified AkaneRE (for Re-
lation Extraction) can be freely used in academia1.
2 Material and Methods
The event extraction system is implemented in a
pipeline fashion (Fig. 1).
2.1 Tokenization and Sentence Boundary
Detection
The text was split into single sentences by a sim-
ple sentence detection program, and then each sen-
tence was split into words (tokens). The tokeniza-
tion was done by using white-space as the token-
separator, but since all protein names are known dur-
ing both training and testing, some extra tokeniza-
tion rules were applied. For example, the protein
1http://www-tsujii.is.s.u-tokyo.ac.jp/?satre/akane/
103
Recursive Template  
Output
POS tagging
Parsing
(Enju & GDep)
Event Clueword 
Recognition
Event Template 
Extraction
Machine 
Learning (ML)
Training Data
ML Filtering
POS tagging
Event Clueword 
Recognition
Event Template 
Filling
Test Data
Models with 
Templates
Parsing
(Enju & GDep)
Tokenization Tokenization
Figure 1: System Overview
name ?T cell factor 1? is treated as a single token,
?T cell factor 1?, and composite tokens including a
protein name, like ?(T cell factor 1)?, are split into
several tokens, like ?(?, ?T cell factor 1? and ?)?, by
adding space around all given protein names. Also,
punctuation (commas, periods etc.) were treated as
separate tokens.
2.2 POS-tagging and Parsing
We used Enju2 and GDep3 to parse the text. These
parsers have their own built-in Part-of-Speech (POS)
taggers, and Enju also provides a normalized lemma
form for each token.
2.3 Event Clue-word tagging
Event clue-word detection was performed by a Ma-
chine Learning (ML) sequence labeling program.
This named-entity tagger program is based on a first
order Maximum Entropy Markov Model (MEMM)
and is described in Yoshida and Tsujii (2007). The
clue-word annotation of the shared-task training set
was converted into BIO format, and used to train the
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
3http://www.cs.cmu.edu/?sagae/parser/gdep/
MEMM model. The features used in the MEMM
model was extracted from surface strings and POS
information of the words corresponding to (or ad-
jacent to) the target BIO tags. The clue-word tag-
ger was applied to the development and test sets to
obtain the marginal probability that each word is a
clue-word of a certain category. The probabilities
were obtained by marginalizing the n-best output of
the MEMM tagger. We later also created clue-word
probability annotation of the training set, to enable
the template extraction program to access clue-word
probability information in the training phase.
2.4 Event Template Extraction
The training data was used to determine which
events to extract. As input to the system, a list of
Named Entity (NE) types and the Roles they can
play were provided. The roles can be thought of as
slots for arguments in event-frames, and in this task
the roles were Event (clue), Theme and Cause. In
the original AkanePPI (based on the AIMed corpus),
the only NE type was Protein, and the only role was
Theme (p1 and p2). All the (PPI) events were pair-
wise interactions, and there was no explicit event-
clue role. This means that all the events could be
represented with the single template shown first in
Table 1.
The BioNLP shared task used eight other NE
types, in addition to manually annotated Proteins,
namely Binding, Gene expression, Localization,
Protein catabolism, Transcription, Regulation, Pos-
itive Regulation and Negative Regulation. The first
five events have only Theme slots, which can only
be filled by Proteins, while the last three regulation
events are very diverse. They also have one Theme
slot, but they can have a Cause slot as well, and each
role/slot can be filled with either Proteins, or other
Events. See the first half of Table 1.
148 templates were extracted and clustered into
nine homogeneous groups which were classified
as nine separate sub-problems. The grouping was
based on whether the templates had an Event or a
Protein in the same role-positions. This way of orga-
nizing the groups was motivated by the fact that the
Proteins are 100% certain, while the accuracy of the
clue-word recognizer is only around 50% (estimated
on the training data). The bottom of Table 1 shows
the resulting nine general interaction templates.
104
2.5 Machine Learning with Maximum Entropy
Models
We integrated Maximum Entropy (ME) modeling,
also known as Logistic Regression, into AkaneRE.
This was done by using LIBLINEAR4, which han-
dles multi-class learning and prediction. Gold tem-
plates were extracted during training, and each tem-
plate was matched with all legal combinations of
Named Entities (including gold proteins/clue-words
and other recognized clue-word candidates) in each
sentence. The positive training examples were la-
beled as gold members of the template, and all other
combinations matching a given template were la-
beled as negative examples within that specific tem-
plate class. The templates were grouped into the
nine general templates shown in the bottom of Ta-
ble 1. Using one-vs-rest logistic regression, we
trained one multi-class classifier for each of the nine
groups individually. The ML features are shown in
Table 2.
In the test-phase, we extracted and labeled all re-
lation candidates matching all the templates from the
training-phase. The ML component was automati-
cally run independently for each of the nine groups
listed in the bottom of Table 1. Each time, all the
candidate template-instances in the current group
were assigned a confidence score by the classifier for
that group. This score is the probability that a can-
didate is a true relation, and a value above a certain
threshold means that the extracted relation will be
predicted as a true member of its specific template.
LIBLINEAR?s C-value parameter and the prediction
threshold were selected by hand to produce a good
F-score (according to the strict matching criterion)
on the development-test set.
2.6 Filtering and recursive output of the most
confident template instances
After machine learning, all the template instances
were filtered based on their confidence score. Af-
ter tuning the threshold to the development test-set,
we ended up using 1 as our C-value, and 3.5% as
our confidence threshold. Because the prediction
of Regulation Events were done independent from
the sub-events (or proteins) affected by that event,
some sub-events had to be included for complete-
4http://www.csie.ntu.edu.tw/?cjlin/liblinear/
ness, even if their confidence score was below the
threshold.
3 Results and Discussion
Our final official result was an F-score of 36.9%,
which was ranked as number six among the sub-
missions from 24 different groups. This means that
the AkanePPI system can achieve good results when
used on other PPI-related relation-extraction tasks,
such as this first BioNLP event recognition shared
task. The most common error was in predicting reg-
ulation events with other events as Theme or Cause.
The problem is that these events involve more than
one occurrence of event-trigger words, so the perfor-
mance is more negatively affected by our imperfect
clue-word detection system.
Since the recall was much lower on the test-set
than on the development test-set, we later allowed
the system to predict multiple confident alternatives
for a single event-word, and this raised our score on
the test-set from 36.9% to 42.6%. In hindsight, this
is obvious since there are many such examples in
the training data: E.g. ?over-express? is both posi-
tive regulation and Gene expression. The new sys-
tem, named AkaneRE (for Relation Extraction), can
be used freely for academic purposes.
As future work, we believe a closer integration
between the clue-word recognition and the template
prediction modules can lead to better performance.
Acknowledgments
?Grant-in-Aid for Specially Promoted Research?
and ?Genome Network Project?, MEXT, Japan.
References
Martin Krallinger et al 2008. Evaluation of text-mining
systems for biology: overview of the second biocre-
ative community challenge. Genome Biology, 9(S2).
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining multi-
ple layers of syntactic information for protein-protein
interaction extraction. In Proceedings of SMBM 2008,
pages 101?108, Turku, Finland, September.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of the Workshop on BioNLP 2007, June. Prague,
Czech Republic.
105
Freq Event Theme1 Theme2 Theme3 Theme4 Cause
- PPI Protein Protein
613 Binding Protein
213 Binding Protein Protein
3 Binding Protein Protein Protein
2 Binding Protein Protein Protein Protein
217 Regulation Protein Protein
12 Regulation Binding Protein
48 +Regulation Transcription Protein
4 +Regulation Phosphorylation Binding
5 -Regulation +Regulation Protein
... ... ... ...
Total 148 Templates
Count General Templates Theme1 Theme2 Theme3 Theme4 Cause
9 event templates Protein
1 event template Protein Protein
1 event template Protein Protein Protein
1 event template Protein Protein Protein Protein
3 event templates Protein Protein
12 event templates Protein Event
27 event templates Event
26 event templates Event Protein
68 event templates Event Event
Table 1: Interaction Templates from the training-set. Classic PPI at the top, compared to Binding and Regulation
events in the middle. 148 different templates were automatically extracted from the training data by AkaneRE. At
the bottom, the Generalized Interaction Templates are shown, with proteins distinguished from other Named Entities
(Events)
Feature Example
Text The binding of the most prominent factor, named TCF-1 ( T cell factor 1 ),
is correlated with the proto-enhancer activity of TCEd.
BOW B The
BOW M0 -comma- -lparen- factor most named of prominent PROTEIN the
BOW A -comma- -rparen- activity correlated is of proto-enhancer the TCEd with
Enju PATH (ENTITY1) (<prep arg12arg1) (of) (prep arg12arg2>) (factor)
(<verb arg123arg2) (name) (verb arg123arg3>) (ENTITY2)
pairs (ENTITY1 <prep arg12arg1) (<prep arg12arg1 of) (of prep arg12arg2>) ...
triples (ENTITY1 <prep arg12arg1 of) (<prep arg12arg1 of prep arg12arg2>) ...
GDep PATH (ENTITY1) (<NMOD) (name) (<VMOD) (ENTITY2)
pairs/triples (ENTITY1 <NMOD) (<NMOD name) ... (ENTITY1 <NMOD name) ...
Vector BOW B BOW M0...BOW M4 BOW A Enju PATH GDep PATH
Table 2: Bag-Of-Words (BOW) and shortest-path features for the machine learning. Several BOW feature groups were
created for each template, based on the position of the words in the sentence, relative to the position of the template?s
Named Entities (NE). Specifically, BOW B was made by the words from the beginning of the sentence to the first NE,
BOW A by the words between the last NE and the end of the sentence, and BOW M0 to BOW M4 was made by the
words between the main event clue-word and the NE in slot 0 through 4 respectively. The path features are made from
one, two or three neighbor nodes. We also included certain specific words, like ?binding?, as features.
106
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779?787,
Beijing, August 2010
Evaluating Dependency Representation for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo
2School of Computer Science, University of Manchester
3National Center for Text Mining
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The detailed analyses of sentence struc-
ture provided by parsers have been applied
to address several information extraction
tasks. In a recent bio-molecular event ex-
traction task, state-of-the-art performance
was achieved by systems building specif-
ically on dependency representations of
parser output. While intrinsic evalua-
tions have shown significant advances in
both general and domain-specific pars-
ing, the question of how these translate
into practical advantage is seldom con-
sidered. In this paper, we analyze how
event extraction performance is affected
by parser and dependency representation,
further considering the relation between
intrinsic evaluation and performance at
the extraction task. We find that good
intrinsic evaluation results do not always
imply good extraction performance, and
that the types and structures of differ-
ent dependency representations have spe-
cific advantages and disadvantages for the
event extraction task.
1 Introduction
Advanced syntactic parsing methods have been
shown to effective for many information extrac-
tion tasks. The BioNLP 2009 Shared Task, a re-
cent bio-molecular event extraction task, is one
such task: analysis showed that the application of
a parser correlated with high rank in the task (Kim
et al, 2009). The automatic extraction of bio-
molecular events from text is important for a num-
ber of advanced domain applications such as path-
way construction, and event extraction thus a key
task in Biomedical Natural Language Processing
(BioNLP).
Methods building feature representations and
extraction rules around dependency representa-
tions of sentence syntax have been successfully
applied to a number of tasks in BioNLP. Several
parsers and representations have been applied in
high-performing methods both in domain studies
in general and in the BioNLP?09 shared task in
particular, but no direct comparison of parsers or
representations has been performed. Likewise,
a number of evaluation of parser outputs against
gold standard corpora have been performed in the
domain, but the broader implications of the results
of such intrinsic evaluations are rarely considered.
The BioNLP?09 shared task involved documents
contained also in the GENIA treebank (Tateisi et
al., 2005), creating an opportunity for direct study
of intrinsic and task-oriented evaluation results.
As the treebank can be converted into various de-
pendency formats using existing format conver-
sion methods, evaluation can further be extended
to cover the effects of different representations.
In this this paper, we consider three types of de-
pendency representation and six parsers, evaluat-
ing their performance from two different aspects:
dependency-based intrinsic evaluation, and effec-
tiveness for bio-molecular event extraction with a
state-of-the-art event extraction system. Compar-
ison of intrinsic and task-oriented evaluation re-
779
	








	
 


 
	

 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788?796,
Beijing, August 2010
Entity-Focused Sentence Simplification for Relation Extraction
Makoto Miwa1 Rune S?tre1 Yusuke Miyao2 Jun?ichi Tsujii1,3,4
1Department of Computer Science, The University of Tokyo
2National Institute of Informatics
3School of Computer Science, University of Manchester
4National Center for Text Mining
mmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,
yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jp
Abstract
Relations between entities in text have
been widely researched in the natu-
ral language processing and information-
extraction communities. The region con-
necting a pair of entities (in a parsed
sentence) is often used to construct ker-
nels or feature vectors that can recognize
and extract interesting relations. Such re-
gions are useful, but they can also incor-
porate unnecessary distracting informa-
tion. In this paper, we propose a rule-
based method to remove the information
that is unnecessary for relation extraction.
Protein?protein interaction (PPI) is used
as an example relation extraction problem.
A dozen simple rules are defined on out-
put from a deep parser. Each rule specif-
ically examines the entities in one target
interaction pair. These simple rules were
tested using several PPI corpora. The PPI
extraction performance was improved on
all the PPI corpora.
1 Introduction
Relation extraction (RE) is the task of finding a
relevant semantic relation between two given tar-
get entities in a sentence (Sarawagi, 2008). Some
example relation types are person?organization
relations (Doddington et al, 2004), protein?
protein interactions (PPI), and disease?gene as-
sociations (DGA) (Chun et al, 2006). Among
the possible RE tasks, we chose the PPI extrac-
tion problem. PPI extraction is a major RE task;
around 10 corpora have been published for train-
ing and evaluation of PPI extraction systems.
Recently, machine-learning methods, boosted
by NLP techniques, have proved to be effec-
tive for RE. These methods are usually intended
to highlight or select the relation-related regions
in parsed sentences using feature vectors or ker-
nels. The shortest paths between a pair of enti-
ties (Bunescu and Mooney, 2005) or pair-enclosed
trees (Zhang et al, 2006) are widely used as focus
regions. These regions are useful, but they can in-
clude unnecessary sub-paths such as appositions,
which cause noisy features.
In this paper, we propose a method to remove
information that is deemed unnecessary for RE.
Instead of selecting the whole region between
a target pair, the target sentence is simplified
into simpler, pair-related, sentences using general,
task-independent, rules. By addressing particu-
larly the target entities, the rules do not affect im-
portant relation-related expressions between the
target entities. We show how rules of two groups
can be easily defined using the analytical capabil-
ity of a deep parser with specific examination of
the target entities. Rules of the first group can re-
place a sentence with a simpler sentence, still in-
cluding the two target entities. The other group of
rules can replace a large region (phrase) represent-
ing one target entity, with just a simple mention of
that target entity. With only a dozen simple rules,
we show that we can solve several simple well-
known problems in RE, and that we can improve
the performance of RE on all corpora in our PPI
test-set.
788
2 Related Works
The general paths, such as the shortest path or
pair-enclosed trees (Section 1), can only cover
a part of the necessary information for relation
extraction. Recent machine-learning methods
specifically examine how to extract the missing
information without adding too much noise. To
find more representative regions, some informa-
tion from outside the original regions must be
included. Several tree kernels have been pro-
posed to extract such regions from the parse
structure (Zhang et al, 2006). Also the graph
kernel method emphasizes internal paths with-
out ignoring outside information (Airola et al,
2008). Composite kernels have been used to com-
bine original information with outside informa-
tion (Zhang et al, 2006; Miwa et al, 2009).
The approaches described above are useful,
but they can include unnecessary information that
distracts learning. Jonnalagadda and Gonzalez
(2009) applied bioSimplify to relation extraction.
BioSimplify is developed to improve their link
grammar parser by simplifying the target sentence
in a general manner, so their method might re-
move important information for a given target re-
lation. For example, they might accidentally sim-
plify a noun phrase that is needed to extract the
relation. Still, they improved overall PPI extrac-
tion recall using such simplifications.
To remove unnecessary information from a sen-
tence, some works have addressed sentence sim-
plification by iteratively removing unnecessary
phrases. Most of this work is not task-specific;
it is intended to compress all information in a tar-
get sentence into a few words (Dorr et al, 2003;
Vanderwende et al, 2007). Among them, Vickrey
and Koller (2008) applied sentence simplification
to semantic role labeling. With retaining all argu-
ments of a verb, Vickrey simplified the sentence
by removing some information outside of the verb
and arguments.
3 Entity-Focused Sentence
Simplification
We simplify a target sentence using simple rules
applicable to the output of a deep parser called
Mogura (Matsuzaki et al, 2007), to remove noisy
information for relation extraction. Our method
relies on the deep parser; the rules depend on the
Head-driven Phrase Structure Grammar (HPSG)
used by Mogura, and all the rules are written for
the parser Enju XML output format. The deep
parser can produce deep syntactic and semantic
information, so we can define generally applica-
ble comprehensive rules on HPSG with specific
examination of the entities.
For sentence simplification in relation extrac-
tion, the meaning of the target sentence itself is
less important than maintaining the truth-value of
the relation (interact or not). For that purpose,
we define rules of two groups: clause-selection
rules and entity-phrase rules. A clause-selection
rule constructs a simpler sentence (still includ-
ing both target entities) by removing noisy infor-
mation before and after the relevant clause. An
entity-phrase rule simplifies an entity-containing
region without changing the truth-value of the re-
lation. By addressing the target entities particu-
larly, we can define rules for many applications,
and we can simplify target sentences with less
danger of losing relation-related mentions. The
rules are summarized in Table 1.
Our method is different from the sentence sim-
plification in other systems (ref. Section 2). First,
our method relies on the parser, while bioSimplify
by Jonnalagadda and Gonzalez (2009) is devel-
oped for the improvement of their parser. Second,
our method tries to keep only the relation-related
regions, unlike other general systems including
bioSimplify which tried to keep all information in
a sentence. Third, our entity-phrase rules modify
only the entity-containing phrases, while Vickrey
and Koller (2008) tries to remove all information
outside of the target verb and arguments.
3.1 Clause-selection Rules
In compound or complex sentences, it is natural
to assume that one clause includes both the target
entities and the relation-related information. It can
also be assumed that the remaining sentence parts,
outside the clause, contain less related (or noisy)
information. The clause-selection rules simplify a
sentence by retaining only the clause that includes
the target entities (and by discarding the remain-
der of the sentence). We define three types of
789
Rule Group Rule Type # Example (original? simplified )
Sentence Clause 1 We show that A interacts with B.? A interacts with B.
Clause Selection Relative Clause 2 ... A that interacts with B.? A interacts with B.
Copula 1 A is a protein that interacts with B.? A interacts with B.
Apposition 2 a protein, A? A
Entity Phrase Exemplification 4 proteins, such as A? AParentheses 2 a protein (A)? A
Coordination 3 protein and A? A
Table 1: Rules for Sentence Simplification. (# is the rule count. A and B are the target entities.)
(a) S
bbbbbbb \\\\\\\
... VP
bbbbbbb \\\\\\\
N*
ccccc [[[[[
Vcc
77
(copular) ...
bbbbbbb \\\\\\\
... ENTITY ... N* S-REL
bbbbbbb \\\\\\\
NP-REL
NN
...
ccccc [[[[[
... ENTITY ...
A is a protein that interacts with B .
(b) S
bbbbbbb \\\\\\\
N*
ccccc [[[[[
...
ccccc [[[[[
... ENTITY ... ... ENTITY ...
A interacts with B .
Figure 1: Copula Rule. (a) is simplified to (b).
The arrows represent predicate?argument rela-
tions.
(a) N*
bbbbbbb \\\\\\\
N* ...
bbbbbbb ]]]]]]]]]]]]]
PN
RR
55(apposition) N*
ccccc [[[[[
... ENTITY ...
protein , A
(b) N*
ccccc [[[[[
... ENTITY ...
A
Figure 2: Apposition Rule.
clause-selection rules for sentence clauses, rela-
tive clauses, and copula. The sentence clause rule
finds the (smallest) clause that includes both tar-
get entities. It then replaces the original sentence
with the clause. The relative clause rules con-
struct a simple sentence from a relative clause and
the antecedent. If this simple sentence includes
the target entities, it is used instead of the orig-
inal sentence. We define two rules for the case
where the antecedent is the subject of the relative
clause. One rule is used when the relative clause
includes both the target entities. The other rule is
used when the antecedent includes one target en-
tity and the relative clause includes the other tar-
get entity. The copula rule is for sentences that
include copular verbs (e.g. be, is, become, etc).
The rule constructs a simple sentence from a rel-
ative clause with the subject of the copular verb
as the antecedent subject of the clause. The rule
replaces the target sentence with the constructed
sentence, if the relative clause includes one target
entity and the subject of a copular verb includes
the other target entity, as shown in Figure 1.
3.2 Entity-phrase Rules
Even the simple clauses (or paths between two
target entities) include redundant or noisy expres-
sions that can distract relation extraction. Some
of these expressions are related to the target enti-
ties, but because they do not affect the truth-value
of the relation, they can be deleted to make the
path simple and clear. The target problem affects
which expressions can be removed. We define
four types of rules for appositions, exemplifica-
tions, parentheses, and coordinations. Two appo-
sition rules are defined to select the correct ele-
ment from an appositional expression. One ele-
ment modifies or defines the other element in ap-
position, but the two elements represent the same
information from the viewpoint of PPI. If the tar-
get entity is in one of these elements, removing the
other element does not affect the truth-value of the
interaction. Many of these apposition expressions
are identified by the deep parser. The rule to se-
lect the last element is presented in Figure 2. Four
exemplification rules are defined for the two ma-
jor types of expressions using the phrases ?includ-
ing? or ?such as?. Exemplification is represented
by hyponymy or hypernymy. As for appositions,
the truth-value of the interaction does not change
whether we use the specific mention or the hyper-
class that the mention represents. Two parenthe-
ses rules are defined. Parentheses are useful for
synonyms, hyponyms, or hypernyms (ref. the two
790
1: S ? input sentence
2: repeat
3: reset rules {apply all the rules again}
4: P ? parse S
5: repeat
6: r ? next rule {null if no more rules}
7: if r is applicable to P then
8: P ? apply r to P
9: S ? sentence extracted from P
10: break (Goto 3)
11: end if
12: until r is null
13: until r is null
14: return S
Figure 3: Pseudo-code for sentence simplifica-
tion.
former rules). Three coordination rules are de-
fined. Removing other phrases from coordinated
expressions that include a target entity does not
affect the truth-value of the target relation. Two
rules are defined for simple coordination between
two phrases (e.g. select left or right phrase), and
one rule is defined to (recursively) remove one
element from lists of more than two coordinated
phrases (while maintaining the coordinating con-
junction, e.g. ?and?).
3.3 Sentence Simplification
To simplify a sentence, we apply rules repeatedly
until no more applications are possible as pre-
sented in Figure 3. After one application of one
rule, the simplified sentence is re-parsed before
attempting to apply all the rules again. This is be-
cause we require a consistent parse tree as a start-
ing point for additional applications of the rules,
and because a parser can produce more reliable
output for a partly simplified sentence than for the
original sentence. Using this method, we can also
backtrack and seek out conversion errors by exam-
ining the cascade of partly simplified sentences.
4 Evaluation
To elucidate the effect of the sentence simplifi-
cation, we applied the rules to five PPI corpora
and evaluated the PPI extraction performance. We
then analyzed the errors. The evaluation settings
will be explained in Section 4.1. The results of the
PPI extraction will be explained in Section 4.2. Fi-
nally, the deeper analysis results will be presented
in Section 4.3.
4.1 Experimental Settings
The state-of-the-art PPI extraction system
AkaneRE by Miwa et al (2009) was used to
evaluate our approach. The system uses a com-
bination of three feature vectors: bag-of-words
(BOW), shortest path (SP), and graph features.
Classification models are trained with a support
vector machine (SVM), and AkaneRE (with
Mogura) is used with default parameter settings.
The following two systems are used for a state-
of-the-art comparison: AkaneRE with multiple
parsers and corpora (Miwa et al, 2009), and
Airola et al (2008) single-parser, single-corpus
system.
The rules were evaluated on the BioIn-
fer (Pyysalo et al, 2007), AIMed (Bunescu et al,
2005), IEPA (Ding et al, 2002), HPRD50 (Fun-
del et al, 2006), and LLL (Ne?dellec, 2005) cor-
pora1. Table 2 shows the number of positive (in-
teracting) vs. all pairs. One duplicated abstract in
the AIMed corpus was removed.
These corpora have several differences in their
definition of entities and relations (Pyysalo et al,
2008). In fact, BioInfer and AIMed target al oc-
curring entities related to the corpora (proteins,
genes, etc). On the other hand, IEPA, HPRD50,
and LLL only use limited named entities, based
either on a list of entity names or on a named en-
tity recognizer. Only BioInfer is annotated for
other event types in addition to PPI, including
static relations such as protein family member-
ship. The sentence lengths are also different. The
duplicated pair-containing sentences contain the
following numbers of words on average: 35.8 in
BioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 in
HPRD50, and 33.4 in LLL.
For BioInfer, AIMed, and IEPA, each corpus is
split into training, development, and test datasets2.
The training dataset from AIMed was the only
training dataset used for validating the rules. The
development datasets are used for error analysis.
The evaluation was done on the test dataset, with
models trained using training and development
1http://mars.cs.utu.fi/PPICorpora/
GraphKernel.html
2This split method will be made public later.
791
BioInfer AIMed IEPA HPRD50 LLL
pos all pos all pos all pos all pos all
training 1,848 7,108 684 4,072 256 630 - - - -
development 256 928 102 608 23 51 - - - -
test 425 1,618 194 1,095 56 136 - - - -
all 2,534 9,653 980 5,775 335 817 163 433 164 330
Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.
BioInfer AIMed IEPA
Rule Applied F AUC Applied F AUC Applied F AUC
No Application 0 62.5 83.0 0 61.2 87.9 0 73.4 82.5
Clause Selection 4,313 63.5 83.9 2,569 62.5 88.2 307 75.0 83.7
Entity Phrase 22,066 60.5 80.9 7,784 61.2 86.1 1,031 72.7 83.3
ALL 26,281 62.9 82.1 10,783 60.2 85.7 1,343 75.4 85.7
Table 3: Performance of PPI Extraction on test datasets. ?Applied? represents the number of times the
rules are applied on the corpus. ?No Application? means PPI extraction without sentence simplification.
ALL is the case all rules are used. The top scores for each corpus are shown in bold.
datasets). Ten-fold cross-validation (CV) was
done to facilitate comparison with other existing
systems. For HPRD50 and LLL, there are insuf-
ficient examples to split the data, so we use these
corpora only for comparing the scores and statis-
tics. We split the corpora for the CV, and mea-
sured the F -score (%) and area under the receiver
operating characteristic (ROC) curve (AUC) as
recommended in (Airola et al, 2008). We count
each occurrence as one example because the cor-
rect interactions must be extracted for each occur-
rence if the same protein name occurs multiple
times in a sentence.
In the experiments, the rules are applied in the
following order: sentence?clause, exemplifica-
tion, apposition, parentheses, coordination, cop-
ula, and relative-clause rules. Furthermore, if the
same rule is applicable in different parts of the
parse tree, then the rule is first applied closest to
the leaf-nodes (deepest first). The order of the
rules is arbitrary; changing it does not affect the
results much. We conducted five experiments us-
ing the training and development dataset in IEPA,
each time with a random shuffling of the order of
the rules; the results were 77.8?0.26 in F -score
and 85.9?0.55 in AUC.
4.2 Performance of PPI Extraction
The performance after rule application was bet-
ter than the baseline (no application) on all the
corpora, and most rules could be frequently ap-
plied. We show the PPI extraction performance on
Rule Applied F AUC
No Application 0 72.9 84.5
Sentence Clause 145 71.6 83.8
Relative Clause 7 73.3 84.1
Copula 0 72.9 84.5
Clause Selection 152 71.4 83.4
Apposition 64 73.2 84.6
Exemplification 33 72.9 84.7
Parentheses 90 72.9 85.1
Coordination 417 73.6 85.4
Entity Phrase 605 74.1 86.6
ALL 763 75.0 86.6
Table 4: Performance of PPI Extraction on
HPRD50.
Rule Applied F AUC
No Application 0 79.0 84.6
Sentence Clause 135 81.3 85.2
Relative Clause 42 78.8 84.6
Copula 0 79.0 84.6
Clause Selection 178 81.0 85.6
Apposition 197 79.6 83.9
Exemplification 0 79.0 84.6
Parentheses 56 79.5 85.8
Coordination 322 84.2 89.4
Entity Phrase 602 83.8 90.1
ALL 761 82.9 90.5
Table 5: Performance of PPI Extraction on LLL.
BioInfer, AIMed, and IEPA with rules of different
groups in Table 3. The effect of using rules of
different types for PPI extraction from HPRD50
and LLL is reported in Table 4 and Table 5. Ta-
ble 6 shows the number of times each rule was
applied in an ?apply all-rules? experiment. The
usability of the rules depends on the corpus, and
different combinations of rules produce different
792
Rule B AIMed IEPA H LLL
S. Cl. 3,960 2,346 300 150 135
R. Cl. 287 212 17 5 24
Copula 60 57 1 0 0
Cl. Sel. 4,307 2,615 318 155 159
Appos. 3,845 1,100 99 69 198
Exempl. 383 127 11 33 0
Paren. 2,721 2,158 235 91 88
Coord. 15,025 4,783 680 415 316
E. Foc. 21,974 8,168 1,025 608 602
Sum 26,281 10,783 1,343 763 761
Table 6: Distribution of the number of rules ap-
plied when all rules are applied. B:BioInfer, and
H:HPRD50 corpora.
Rules Miwa et al Airola et al
F AUC F AUC F AUC
B 60.0 79.8 68.3 86.4 61.3 81.9
A 54.9 83.7 65.2 89.3 56.4 84.8
I 77.8 88.7 76.6 87.8 75.1 85.1
H 75.0 86.6 74.9 87.9 63.4 79.7
L 82.9 90.5 86.7 90.8 76.8 83.4
Table 7: Comparison with the results by Miwa et
al. (2009) and Airola et al (2008). The results
with all rules are reported.
results. For the clause-selection rules, the per-
formance was as good as or better than the base-
line for all corpora, except for HPRD50, which
indicates that the pair-containing clauses also in-
clude most of the important information for PPI
extraction. Clause selection rules alone could im-
prove the overall performance for the BioInfer and
AIMed corpora. Entity-phrase rules greatly im-
proved the performance on the IEPA, HPRD50,
and LLL corpora, although these rules degraded
the performance on the BioInfer and AIMed cor-
pora. These phenomena hold even if we use small
parts of the two corpora, so this is not because of
the size of the corpora.
We compare our results with the results by
Miwa et al (2009) and Airola et al (2008) in Ta-
ble 7. On three of five corpora, our method pro-
vides better results than the state-of-the-art system
by Airola et al (2008), and also provides com-
parable results to those obtained using multiple
parsers and corpora (Miwa et al, 2009) despite
the fact that our method uses one parser and one
corpus at a time. We cannot directly compare our
result with Jonnalagadda and Gonzalez (2009) be-
cause the evaluation scheme, the baseline system,
[FP?TN][Sentence, Parenthesis, Coordination] To
characterize the AAV functions mediating this effect,
cloned AAV type 2 wild-type or mutant genomes were
transfected into simian virus 40 (SV40)-transformed
hamster cells together with the six HSV replication genes
(encoding UL5, UL8, major DNA-binding protein, DNA
polymerase, UL42 , and UL52) which together are
necessary and sufficient for the induction of SV40 DNA
amplification (R. Heilbronn and H. zur Hausen, J. Virol.
63:3683-3692, 1989). (BioInfer.d760.s0)
[TP?FN][Coordination] Both the GT155-calnexin and
the GT155-CAP-60 interactions were dependent on the
presence of a correctly modified oligosaccharide group
on GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)
[TN?TN][Coordination, Parenthesis] Leptin may act as
a negative feedback signal to the hypothalamic control of
appetite through suppression of neuropeptide Y (NPY)
secretion and stimulation of cocaine and amphetamine
regulated transcript (CART) . (IEPA.d190.s454)
Figure 4: A rule-related error, a critical error, and
a parser-related error. Regions removed by the
rules are underlined, and target proteins are shown
in bold. Predictions, applied rules, and sentence
IDs are shown.
[FN?TP][Sentence, Coordination] WASp contains a
binding motif for the Rho GTPase CDC42Hs as well as
verprolin / cofilin-like actin-regulatory domains , but no
specific actin structure regulated by CDC42Hs-WASp has
been identified. (BioInfer.d795.s0)
[FN?TP][Parenthesis, Apposition] The protein Raf-1 , a
key mediator of mitogenesis and differentiation, associates
with p21ras (refs 1-3) . (AIMed.d124.s1055)
[FN?TP][Sentence, Parenthesis] On the basis of
far-Western blot and plasmon resonance (BIAcore)
experiments, we show here that recombinant bovine
prion protein (bPrP) (25-242) strongly interacts with the
catalytic alpha/alpha? subunits of protein kinase CK2
(also termed ?casein kinase 2?) (IEPA.d197.s479)
Figure 5: Correctly simplified cases. The first
sentence is a difficult (not PPI) relation, which is
typed as ?Similar? in the BioInfer corpus.
and test parts differ.
4.3 Analysis
We trained models using the training datasets
and classified the examples in the development
datasets. Two types of analysis were performed
based on these results: simplification-based and
classification-based analysis.
For the simplification-based analysis, we com-
pared positive (interacting) and negative pair sen-
tences that produce the exact same (inconsistent)
sentence after protein names normalization and
793
BioInfer AIMed IEPA
Before simplification FN FP TP TN FN FP TP TN FN FP TP TN Not AffectedAfter simplification TP TN FN FP TP TN FN FP TP TN FN FP
No Error 18 2 3 35 14 21 21 8 3 2 0 4 32
No Application 3 2 0 3 0 7 8 0 0 1 0 1 7
Number of Errors 0 2 0 32 4 2 1 4 0 0 0 0 1
Number of Pairs 21 6 3 70 18 30 30 12 3 3 0 5 40
Coordination 0 0 0 20 4 2 1 0 0 0 0 0 1
Sentence 0 2 0 4 0 0 0 4 0 0 0 0 0
Parenthesis 0 0 0 5 0 0 0 0 0 0 0 0 0
Exemplification 0 0 0 2 0 0 0 0 0 0 0 0 0
Apposition 0 0 0 1 0 0 0 0 0 0 0 0 0
Table 8: Distribution of sentence simplification errors compared to unsimplified predictions with their
types (on the three development datasets). TP, True Positive; TN, True Negative; FN, False Negative;
FP, False Positive. ?No Error? means that simplification was correct; ?No Application? means that no
rule could be applied; Other rule names mean that an error resulted from that rule application. ?Not
Affected? means that the prediction outcome did not change.
simplification in the training dataset. The numbers
of such inconsistent sentences are 7 for BioIn-
fer, 78 for AIMed, and 1 for IEPA. The few in-
consistencies in BioInfer and IEPA are from er-
rors by the rules, mainly triggered by parse errors.
The frequent inconsistencies in AIMed are mostly
from inconsistent annotations. For example, even
if all coordinated proteins are either interacting or
not, only the first protein mention is annotated as
interacting.
For the classification-based analysis, we
specifically examine simplified pairs that were
predicted differently before and after the simplifi-
cation. Pairs predicted differently before and after
rule application were selected: 100 random pairs
from BioInfer and all 90 pairs from AIMed. For
IEPA, all 51 pairs are reported. Simplified results
are classified as errors when the rules affect a re-
gion unrelated to the entities in the smallest sen-
tence clause. The results of analysis are shown in
Table 8. There were 34 errors in BioInfer, and 11
errors in AIMed. Among the errors, there were
five critical errors (in two sentences, in AIMed).
Critical errors mean that the pairs lost relation-
related mentions, and the errors are the only er-
rors which caused the changes in the truth-value
of the relation. There was also a rule-related er-
ror (in BioInfer), which means that rules with cor-
rect parse results affect a region unrelated to the
entities, and parse errors (parser-related errors).
Figure 4 shows the rule-related error in BioInfer,
one critical error in AIMed, and one parser-related
error in IEPA.
5 Discussion
Our end goal is to provide consistent relation
extraction for real tasks. Here we discuss the
?safety? of applying our simplification rules, the
difficulties in the BioInfer and AIMed corpora, the
reduction of errors, and the requirements for such
a general (PPI) extraction system.
Our rules are applicable to sentences, with little
danger of changing the relation-related mentions.
Figure 5 shows three successfully simplified cases
(?No Error? cases from Table 8). The sentence
simplification leaves sufficient information to de-
termine the value of the relation in these exam-
ples. Relation-related mentions remained for most
of the simplification error cases. There were only
five critical errors, which changed the truth-value
of the relation, out of 46 errors in 241 pairs shown
in Table 8. Please note that some rules can be
dangerous for other relation extraction tasks. For
example, the sentence clause rule could remove
modality information (negation, speculation, etc.)
modifying the clause, but there are few such cases
in the PPI corpora (see Table 8). Also, the task of
hedge detection (Morante and Daelemans, 2009)
can be solved separately, in the original sentences,
after the interacting pairs have been found. For
example, in the BioNLP shared task challenge
and the BioInfer corpus, interaction detection and
modality are treated as two different tasks. Once
other NLP tasks, like static relation (Pyysalo et
794
al., 2009) or coreference resolution, become good
enough, they can supplement or even substitute
some of the proposed rules.
There are different difficulties in the BioInfer
and AIMed corpora. BioInfer includes more com-
plicated sentences and problems than the other
corpora do, because 1) the apposition, coordi-
nation, and exemplification rules are more fre-
quently used in the BioInfer corpus than in the
other corpora (shown in Table 6), 2) there were
more errors in the BioInfer corpus than in other
corpora among the simplified sentences (shown
in Table 8), and 3) BioInfer has more words per
sentence and more relation types than the other
corpora. AIMed contains several annotation in-
consistencies as explained in Section 4.3. These
inconsistencies must be removed to properly eval-
uate the effect of our method.
Simplification errors are mostly caused by
parse errors. Our rule specifically examines a part
of parser output; a probability is attached to the
part. The probability is useful for defining the or-
der of rule applications, and the n-best results by
the parser are useful to fix major errors such as co-
ordination errors. By using these modifications of
rule applications and by continuous improvement
in parsing technology for the biomedical domain,
the performance on the BioInfer and AIMed cor-
pora will be improved also for the all rules case.
The PPI extraction system lost the ability to
capture some of the relation-related expressions
left by the simplification rules. This indicates
that the system used to extract some relations (be-
fore simplification) by using back-off features like
bag-of-words. The system can reduce bad effects
caused by parse errors, but it also captures the an-
notation inconsistencies in AIMed. Our simpli-
fication (without errors) can capture more general
expressions needed for relation extraction. To pro-
vide consistent PPI relation extraction in a general
setting (e.g. for multiple corpora or for other pub-
lic text collections), the parse errors must be dealt
with, and a relation extraction system that can cap-
ture (only) general relation-related expressions is
needed.
6 Conclusion
We proposed a method to simplify sentences, par-
ticularly addressing the target entities for relation
extraction. Using a few simple rules applicable
to the output of a deep parser called Mogura,
we showed that sentence simplification is effec-
tive for relation extraction. Applying all the rules
improved the performance on three of the five
corpora, while applying only the clause-selection
rules raised the performance for the remaining two
corpora as well. We analyzed the simplification
results, and showed that the simple rules are ap-
plicable with little danger of changing the truth-
values of the interactions.
The main contributions of this paper are: 1) ex-
planation of general sentence simplification rules
using HPSG for relation extraction, 2) presenting
evidence that application of the rules improve re-
lation extraction performance, and 3) presentation
of an error analysis from two viewpoints: simpli-
fication and classification results.
As future work, we are planning to refine and
complete the current set of rules, and to cover
the shortcomings of the deep parser. Using these
rules, we can then make better use of the parser?s
capabilities. We will also attempt to apply our
simplification rules to other relation extraction
problems than those of PPI.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientific Research (C) (General) (MEXT,
Japan).
795
References
Airola, Antti, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of the BioNLP 2008 work-
shop.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
724?731.
Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dictio-
naries and machine learning. In The Pacific Sympo-
sium on Biocomputing (PSB), pages 4?15.
Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) program: Tasks, data, and evalua-
tion. In Proceedings of LREC?04, pages 837?840.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In in Proceedings of Work-
shop on Automatic Summarization, pages 1?8.
Fundel, Katrin, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine, pages 109?114, November.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and cfg-filtering. In IJCAI?07: Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 1671?1676, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Miwa, Makoto, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, June.
Morante, Roser and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Ne?dellec, Claire. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Pyysalo, Sampo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece
in the biomedical information extraction puzzle.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1(3):261?
377.
Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606?1618.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 825?832. Association for
Computational Linguistics.
796
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2270?2279, Dublin, Ireland, August 23-29 2014.
Comparable Study of Event Extraction in Newswire and Biomedical
Domains
Makoto Miwa
?,?
Paul Thompson
?
Ioannis Korkontzelos
?
Sophia Ananiadou
?
?
National Centre for Text Mining and School of Computer Science,
University of Manchester, United Kingdom
?
Graduate School of Engineering, Toyota Technological Institute, Japan
{makoto.miwa, paul.thompson, ioannis.korkontzelos, sophia.ananiadou}@manchester.ac.uk
Abstract
Event extraction is a popular research topic in natural language processing. Several event extrac-
tion tasks have been defined for both the newswire and biomedical domains. In general, different
systems have been developed for the two domains, despite the fact that the tasks in both domains
share a number of characteristics. In this paper, we analyse the commonalities and differences
between the tasks in the two domains. Based on this analysis, we demonstrate how an event
extraction method originally designed for the biomedical domain can be adapted for application
to the newswire domain. The performance is state-of-the-art for both domains, with F-scores of
52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primary
evaluation metrics.
1 Introduction
Research into event extraction was initially focussed on the general language domain, largely driven by
the Message Understanding Conferences (MUC) series (e.g., Chinchor (1998)) and the Automated Con-
tent Extraction (ACE) evaluations
1
. More recently, the focus of research has been widened to the biomed-
ical domain, motivated by the ongoing series of biomedical natural language processing (BioNLP) shared
tasks (STs) (e.g., Kim et al. (2013)).
Although the textual characteristics and the types of relevant events to be extracted can vary consid-
erably between domains, the same general features of events normally hold across domains. An event
usually consists of a trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nom-
inalised verb that denotes the presence of the event in the text, while the arguments are usually entities.
In general, arguments are assigned semantic roles that characterise their contribution towards the event
description.
Until now, however, there has been little, if any, effort by researchers working on event extraction in
different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008))
and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006))
and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit cross-
domain features of events to develop more adaptable event extraction systems is an under-studied area.
Consequently, although there is a large number of published studies on event extraction, proposing many
different methods, no work has previously been reported that aims to adapt an event extraction method
developed for one domain to a new domain.
In response to the above, we have investigated the feasibility of adapting an event extraction method
developed for the biomedical domain to the newswire domain. To facilitate this, we firstly carry out a
detailed static analysis of the differences that hold between event extraction tasks in the newswire and
biomedical domains. Specifically, we consider the ACE 2005 event extraction task (Walker et al., 2006)
for the newswire domain and the Genia Event Extraction task (GENIA) in BioNLP ST 2013 (Kim et al.,
2013) for the biomedical domain. Based on the results of this analysis, we adapt the biomedical event
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
itl.nist.gov/iad/mig/tests/ace
2270
Jim McMahon was body slammed to the ground in the mid 80's about five seconds after he had released a pass.
PER_Individual Conflict_Attack ?? timex2 PER_Individual
timex2
Target Time-Within
Time-At-End
Figure 1: ACE 2005 event example (ID: MARKBACKER 20041220.0919)
p300 immunoprecipitated Foxp3 when both proteins were overexpressed in HEK 293T cells
Pro Binding Pro +Reg
+Reg
Gene expression
Gene expressionTheme Theme2 Cause
CauseTheme
Theme
Theme
Theme
Figure 2: GENIA event example (ID: PMC-1447668-08-Results)
extraction method to the task of extracting events in the newswire domain, according to the specification
of the ACE 2005 event extraction task. The original method consists of a classification pipeline that has
previously been applied to extract events according to task descriptions that are similar to GENIA. In
order to address the differences between this task and the ACE task, we have made a number of changes
to the original method, including modifications to the classification labels assigned, the pipeline itself
and the features used. We retrained the model of the adapted system on the ACE task, compared the
performance, and empirically analysed the differences between the two tasks in terms of entity-related
information. We demonstrate that the resulting system achieves state-of-the-art performance for tasks in
both domains.
2 Related Work
In this section, we introduce the two domain specific event extraction tasks on which we will focus, i.e.,
the ACE 2005 event extraction task, which concerns events in the newswire domain, and the GENIA
event task from the BioNLP ST 2013, which deals with biomedical event extraction. We also examine
state-of-the-art systems that have been developed to address each task.
2.1 Newswire Event Extraction
The extraction of events from news-related texts has been widely researched, largely due to motivation
from the various MUC and ACE shared tasks. Whilst MUC focussed on filling a single event template
on a single topic by gathering information from different parts of a document, ACE defined a more
comprehensive task, involving the recognition of multiple fine-grained and diverse types of entities and
associated intra-sentential events within each document.
A common approach to tackling the MUC template filling task has involved the employment of
pattern-based methods, e.g., Riloff (1996). In contrast, supervised learning approaches have constituted
a more popular means of approaching the ACE tasks
2
. In this paper, we choose to focus on adapting
our biomedical-focussed event extraction method to the ACE 2005 task. Our choice is based on the task
definition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than the
MUC event template task definition.
In terms of the characteristics of state-of-the-art event extraction systems designed according to the
ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006).
Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role
types, and event triggers. This pipeline approach has been further extended in several subsequent studies.
For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence
of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to
ensure cross-entity consistency.
2
Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for
the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper.
2271
Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments
(together with their role types) using a structured perceptron model. The system outperformed the best
results reported for the ACE 2005 task in the literature, without the use of any external resources.
2.2 Biomedical Event Extraction
The task of event extraction has received a large amount of attention from BioNLP researchers in recent
years. Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained through
the organisation of further STs in 2011 and 2013. The STs consist of a number of different sub-tasks, the
majority of which concern the extraction of events from biomedical papers from the PubMed database.
Events generally concern interactions between biomedical entities, such as proteins, cells and chemicals.
Similarly to newswire event extraction systems, pipeline-based methods have constituted a popular
approach to extracting events in the biomedical domain (Bj?orne and Salakoski, 2013; Miwa et al., 2012).
The pipeline developed by Miwa et al. (2012) consists of a number of modules, which sequentially
detect event triggers, event arguments, event structures and hedges (i.e., speculations and negations).
The system has been applied to several event extraction tasks, and has achieved the best performance on
most of these, in comparison to other systems. It should be noted that the ordering of the components
in biomedical event extraction pipelines often differs from pipelines designed for news event extraction,
e.g., Grishman et al. (2005), which was described above.
As in newswire event detection, some joint (non pipeline-based) approaches have also been proposed
for biomedical event extraction. For example, McClosky et al. (2012) used a stacking model to combine
the results of applying two different methods to event extraction. The first method is a joint method,
similar to Li et al. (2013), that detects triggers, arguments and their roles. However, in contrast to
the structured perceptron employed in Li et al. (2013), McClosky et al. (2012) use a dual-decomposition
approach for the detection. The second method is based on dependency parsing and treats event structures
as dependency trees.
3 Adaptation of Biomedical Event Extraction to Newswire Event Extraction
In this section, we firstly analyse the differences between the domain-specific ACE 2005 and GENIA
event extraction tasks. Based on our findings, we propose an approach to adapting an existing event ex-
traction method, originally developed for biomedical event extraction, to the ACE 2005 task, by resolving
the observed differences between the two task definitions.
3.1 Differences in event extraction tasks
Both the ACE 2005 and GENIA tasks concern the task of event extraction, i.e., the identification of
relationships between entities. For both tasks, the requirement is to extract events from text that conform
to the general event description introduced earlier, i.e., a trigger and its arguments, each of which is
assigned a semantic role. Despite this high-level similarity between the tasks, their finer-grained details
diverge in a number of ways. Apart from the different textual domain, the tasks adopt varying annotation
schemes. The exact kinds of annotations provided at training time are also different, as are the evaluation
settings.
Several variants of the official task setting for the ACE 2005 corpus have been defined. This is partly
due to the demanding nature of the official task definition, which requires the detection of events from
scratch, including the recognition of named entities participating in events, together with the resolution
of coreferences. Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010)))
generally simplify the official task definition, e.g., by omitting the requirement to perform coreference
resolution. A further issue is that the test data sets for the official task setting have not been made publicly
available. As a result of the multiple existing variations of the ACE 2005 task definition that have been
employed by different research efforts, direct comparison of our results with those obtained by other
state-of-the art systems is problematic. The solution we have chosen is to adopt the same ACE 2005
event extraction task specification that has been adopted in recent research, by Hong et al. (2011) and Li
et al. (2013). For GENIA, we follow the specification of the original GENIA event extraction task.
2272
ACE 2005 GENIA
# of entity types 13 (type) / 53 (subtype) 2
Argument Entity/Nominal/Value/Time Entity
# of event types 8 (type) / 33 (subtype) 13
# of argument role types 35 7
Max # of arguments for an event 11 4
Nested events None Possible
Overlaps of events None Possible
Correspondences of arguments None Possible
Entity Available (Given) Available (Partially given)
Entity attributes Available (Given) Not available
Event attributes Available (Not given) Available (Not given)
Entity coreference Available (Given) Available (Not given)
Event coreference Available (Not given) Not available
Evaluation Trigger/Role Event
Table 1: Comparison of event definitions and event extraction tasks. ?Available annotations? are annota-
tions available in the corresponding corpus, while ?Given annotations? are annotations provided during
(training and) prediction. ?Given annotations? do not need to be predicted during event extraction.
Event annotation examples for ACE 2005 and GENIA are shown in Figures 1 and 2, respectively.
Table 1 summarises the following comparison between the two event extraction tasks.
Semantic types There are more event, role and entity types and a greater potential number of arguments
in ACE 2005 events than in GENIA events. There is also a hierarchy of event types and entity types
in ACE 2005. For example, the Life event type has Be-Born, Marry, Divorce, Injure, Die event
subtypes. Some GENIA event types can also be arranged to have a hierarchy but they are limited.
Events in ACE 2005 can take non-entity arguments, e.g., Time.
Nested events/Overlapping events Event structures are flat in ACE 2005, but they can be nested in
GENIA, i.e., an event can take other events as its arguments. Events in GENIA can also be over-
lapping, in the sense that a particular word or phrase can be a trigger for multiple events. Figure 2
illustrates both nesting and overlapping in GENIA events. These properties of GENIA events are
not addressed by methods developed for event extraction according to the ACE 2005 specification,
making direct application of these methods to the GENIA task impossible.
Links amongst arguments A specific feature of the GENIA event extraction task, which is completely
absent from the ACE 2005 task, is that links amongst arguments sometimes have to be identified.
For example, the Binding event type in the GENIA task can take the following argument role types:
Theme, Theme2, Site and Site2. The number 2 is attached to differentiate specific linkages between
arguments: Site is the location of Theme, while Site2 is the location of Theme2.
Entities, events and their attributes Entities in ACE 2005 have rich attributes associated with them.
For example, the Time entity type has an attribute to store a normalised temporal format (e.g., 2003-
03-04 for entities ?20030304?, ?March 4? and ?Tuesday?) while the GPE (Geo-Political Entity)
type has attributes such as subtypes (e.g., Nation), mention type (proper name, common noun or
pronoun), roles (location of a group or person) and style (literal or metonymic). In contrast, GENIA
entities have no attributes
3
. In ACE 2005, all entities are provided (gold) in the training and test
data and they do not need to be predicted. In GENIA, some named entities (i.e., Proteins) are also
provided, but other types of named or non-named entities that can constitute event arguments, such
as locations and sites of proteins, are not provided in the test data and thus need to be predicted
as part of the extraction process. Events in both corpora also have associated attributes: modality,
3
Types are not counted as attributes in this paper.
2273
polarity, genericity and tense in ACE 2005 and negation and speculation in GENIA. The GENIA
task definition requires event attributes to be predicted, but the ACE 2005 task definition does not.
Coreference Both entity and event coreference are annotated in ACE 2005, but only entity coreference is
annotated in GENIA. Events in ACE 2005 can take non-entity mentions, such as pronouns, as their
arguments. However, events in GENIA can take only entity mentions as arguments. Thus, instead
of non-entity mentions, coreferent entity mentions that are the closest to triggers are annotated as
arguments in GENIA. For example, in Figure 2, ?p300? and ?Foxp3? are annotated as Themes of
Gene expression events instead of ?both proteins?.
Evaluation In ACE 2005, the accuracy of extracted events is evaluated at the level of individual ar-
guments and their roles. Completeness of events is not taken into consideration (Li et al., 2013),
presumably because each event can take many arguments. Evaluation is performed by taking into
account the 33 event subtypes, rather than the 8 coarser-grained event types. In contrast, evaluation
of events according to the GENIA specification considers only the correctness of complete events,
after nested events have been broken down.
In summary, the ACE 2005 task is in some respects more complex than the GENIA task, because it
concerns a greater number event types, whose arguments may constitute a greater range of entity types,
and whose semantic roles are drawn from a larger set, some of which are specific to particular event
types and entities. In other respects, the task is more straightforward than the GENIA task, because of
the simpler nature of the event structures in ACE 2005, i.e., there are no nested or overlapping event
structures.
3.2 Adaptation of event extraction method
Since event structures are simpler in ACE 2005 than GENIA, we choose to adapt a biomedical event
extraction method to the ACE 2005 task rather than the other way around. The inverse adaptation,
starting from a newswire event extraction method, is considered more complex, since we would need to
extend the method to capture the more complex event structures required in the GENIA task. It would
additionally be inappropriate to employ domain adaptation methods (Daum?e III and Marcu, 2006; Pan
and Yang, 2010) to allow GENIA-trained models to be applied to the ACE 2005 tasks. This is because
such methods require that there is at least a certain degree of overlap between the target information
types, which is not the case in this scenario.
We employ the biomedical event extraction pipeline method described in Miwa et al. (2012) as our
starting point. Our motivation is that, due to their modular nature, pipeline approaches are often easier
to adapt to other task settings than joint approaches, e.g., (McClosky et al., 2012; Li et al., 2013).
In addition, the method has previously been shown to achieve state-of-the-art performance in several
biomedical event extraction tasks (Miwa et al., 2012).
The pipeline consists of four detectors, i.e., trigger/entity, event role, event structure, and hedge de-
tectors. The trigger/entity detector finds triggers and entities in text. The event role detector determines
which triggers/entities constitute arguments of events, links them to the appropriate event trigger and as-
signs semantic roles to the arguments. The event structure detector merges trigger-argument pairs into all
possible complete event structures, and determines which of these structures constitute actual events. The
same detector determines links between arguments, such as Theme2 and Site2. The hedge detector finds
negation and speculation information associated with events. Each detector solves multi-label multi-
class classification problems using lexical and syntactic features obtained from multiple parsers. These
features include character n-grams, word n-grams, and shortest paths between triggers and participants
within parse structures. More detailed information can be found in Miwa et al. (2012).
We have updated the original method by simplifying the format of the classification labels used by
both the event role detector and event structure detector modules. We refer to this method as BioEE,
which we have applied to the GENIA task. We use only the role types (e.g., Theme) as classification
labels for instances in the event role detector, instead of the more complex labels used in the original
version of the module, which combined event types, roles and semantic entity types of arguments (e.g.,
2274
Binding:Theme-Protein). Similarly, in the event structure detector, we use only two labels (?EVENT?
or ?NOT-EVENT?), instead of the previously used composite labels, which consisted of the event type,
together with the roles and semantic entity types of all arguments of the event (e.g., Regulation:Cause-
Protein:Theme-Protein.) We employed the simplified labels, since they increase the number of training
instances for each label. The use of such labels, compared to the more complex ones, could reduce the
potential of carrying out detailed modelling of specific aspects of the task. However, this was found not
to be an issue, since the use of the simplified labels improved the performance of the pipeline in detecting
events within the GENIA development data set (about 1% improvement in F-score). The simplification of
the set of classification labels was also vital to ensure the tractability of the classification problems within
the context of the ACE 2005 task. For example, using the same conventions to formulate classification
labels as in the original system would result in 345 possible labels (compared to 91 in GENIA) to be
predicted by the event role detector (and an even greater number of labels for the event structure detector),
based on event-role-semantic type combinations found in the ACE training/development sets.
In order to adapt the system to extract events according to the ACE 2005 specification, we modified
BioEE in several ways, making changes to both the pipeline itself and the features employed by the
different modules. We refer to this method as Adapted BioEE, and we applied this method to the ACE
2005 task. These changes were made in an attempt to address the two major differences between the
GENIA and ACE 2005 tasks, i.e., the simpler event structures and the availability of entity attribute and
coreference information in ACE.
The pipeline-based modifications consisted of removing certain modules from the original pipeline,
such that only two modules remained, i.e., the trigger/entity and event role detectors. The other two
modules of the original pipeline, i.e., the event structure and hedge detectors, were designed to deal with
problems that do not exist in the ACE 2005 extraction task, and thus their usage would be redundant.
Instead of using the event structure detector to piece the different elements of an event, we simply aggre-
gate all the arguments of the same trigger into a single event structure, after the event role detector has
been applied.
As mentioned above, the ACE 2005 task definition includes rich information about entities, including
attributes and coreference information. Existing systems developed to address this task have exploited
this information to generate rich feature sets for classification (Liao and Grishman, 2010; Li et al.,
2013). Based on the demonstrated utility of this information within the context of event extraction, we
also choose to use it, by adding binary feature that indicate the presence of base forms, entity subtypes,
and attributes of the entities and their coreferent entities to features in both detectors above. We choose
to use base forms, since surface forms of entities are not used by most biomedical event extraction
systems, including BioEE. We also add the features for Brown clusters (Brown et al., 1992) following Li
et al. (2013). Further details can be found in Li et al. (2013).
4 Evaluation
4.1 Evaluation settings
To assess the performance of Adapted BioEE on the ACE 2005 task, we followed the evaluation process
and settings used in previously reported studies (Hong et al., 2011; Li et al., 2013). ACE 2005 consists
of 599 documents. In order to facilitate direct comparison with other systems trained on the same data,
we conducted a blind test on the same 40 newswire documents that were used for evaluation in (Ji and
Grishman, 2008; Li et al., 2013), and used the remaining documents as training/development sets. We
use precision (P), recall (R) and F-score (F) to report the performance of the adapted system in classifying
triggers and argument roles. We use the latter F-score as our primary metric for comparing our system
with other systems, since this score better reflects the performance of the extraction of event structures.
GENIA consists of 34 full paper articles (Kim et al., 2013). To evaluate the performance of BioEE
on the GENIA task, we followed the task setting in BioNLP ST 2013 and used the official evaluation
systems provided by the organisers. We also used the same partitioning of data that was employed in
the official BioNLP ST 2013 evaluation, with 20 articles being used as the training/development set, and
the remaining 14 articles being held back as the test set. For brevity, we show the only the primary P,
2275
Arg. Role Decomposition Event Detection
P R F P R F (%)
BioEE 71.76 47.44 57.12 64.36 44.62 52.71
BioEE (+Entity) 69.47 46.94 56.02 61.81 44.11 51.48
EVEX 64.30 48.51 55.30 58.03 45.44 50.97
TEES-2.1 62.69 49.40 55.26 56.32 46.17 50.74
Table 2: Overall performance of BioEE on the GENIA data set
Trigger Classification Arg. Role Classification Event Detection
P R F P R F P R F (%)
Adapted BioEE 59.9 72.6 65.7 54.2 50.2 52.1 20.7 21.7 21.2
Adapted BioEE (-Entity) 57.9 71.5 64.0 51.0 48.1 49.5 19.7 19.3 19.5
Li et al. (2013) 73.7 62.3 67.5 64.7 44.4 52.7 - - -
Hong et al. (2011) 72.9 64.3 68.3 51.6 45.5 48.4 - - -
Table 3: Overall performance of Adapted BioEE on the ACE 2005 data set
R and F scores in the shared task, i.e., the EVENT TOTAL results obtained using the approximate span
& recursive evaluation method, as recommended by the organisers. The method individually evaluates
each complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxed
span matching, after nested events have been broken down as explained in Section 3.1. Note that the
scores do not count the non-named entities, hedges, and links between arguments, since only core events
are considered in the official evaluation.
We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagae
and Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for the
GENIA task. We also employed the GENIA sentence splitter (S?tre et al., 2007) for sentence splitting,
and the snowball (Porter2) stemmer
4
for stemming. We did not make use of any other external resources,
such as dictionaries, since this would hinder direct comparison of the two versions of the system.
4.2 Evaluation on GENIA
The ?Event Detection? column in Table 2 shows evaluation results of BioEE on GENIA. The effects
on performance by including entity-related features, i.e., entity base forms and Brown clustering, as
introduced in Section 3.2, are shown as ?BioEE (+Entity)?. The inclusion of these features slightly
degrades the performance.
For completeness, we also show in Table 2 the best and second best performing systems that took
part in the official BioNLP 2013 ST evaluation: EVEX (Hakala et al., 2013) and TEES-2.1 (Bj?orne and
Salakoski, 2013). TEES-2.1 consists of a modular pipeline similar to BioEE, but it uses a different set
of features. EVEX enhances the output of TEES-2.1, by using information obtained from the results of
large-scale event extraction. The comparison shows that BioEE achieves state-of-the-art event extraction
performance on the GENIA task.
4.3 Evaluation on ACE 2005
The ?Trigger Classification? and ?Arg. Role Classification? columns of Table 3 summarise the evaluation
results of the Adapted BioEE system (as described in Section 3.2) on the ACE 2005 task.
We analysed the effects of incorporating features based on entity-related information into the extrac-
tion process, by repeating the experiments with such features omitted (-Entity). As can be observed in
Table 3, the removal of entity-related features led to 3% performance decrease in F-score.
For completeness, Table 3 also illustrates the results of state-of-the-art systems that were specifi-
cally developed for ACE 2005: the system based on a joint approach (Li et al., 2013) and the pipeline-
based system enhanced with web-gathered information (Hong et al., 2011). The difference between the
4
snowball.tartarus.org
2276
Adapted BioEE and the best system is small and insignificant and the Adapted BioEE achieved perfor-
mance that is comparable to or better than these other systems, in terms of the F-scores in argument role
classification.
5 Discussion
To further investigate the differences in performance of the BioEE and Adapted BioEE systems on the
two tasks, we evaluate the scores achieved for each task using the evaluation criteria originally designed
for the other task. Specifically, we apply the ACE 2005 argument role classification criteria to the out-
put of GENIA task, and we apply the complete event-based evaluation, originally used to evaluate the
GENIA task, to the events extracted for the ACE 2005 task. The ?Arg. Role Decomposition? column of
Table 2 depicts the former evaluation, while the ?Event Detection? column of Table 3 shows the latter.
Table 2 also shows the performance of the other biomedical event extraction systems introduced above
in carrying out argument role classification, since such information was provided as ?Decomposition?
within the results of the original task evaluation
5
. Although the results shown for ?Arg. Role Decompo-
sition? in Table 2 are not directly comparable to those shown for ?Arg. Role Classification? in Table 3
(given the different characteristics of GENIA and ACE 2005 tasks), the scores are broadly comparable.
This demonstrates that the task of argument role classifications is equally challenging for both tasks.
The ?Event Detection? column of Table 3 illustrates event-based evaluation scores on ACE 2005.
The event structure detector was added to the pipeline to facilitate comparison of the results of the two
different tasks in a similar setting, and performance was evaluated according to the GENIA evaluation
criteria. Evaluation scores on ACE 2005 are unexpectedly low compared to those in Table 2. Considering
that the performance of argument role classification is similar in both tasks, this low performance is likely
to be due to the large number of potential event arguments in ACE 2005. This means that, in comparison
to GENIA events, which have a small number of possible argument types, there is a greater chance that
some arguments of more complex ACE 2005 events will fail to be detected. According to the GENIA
evaluation criteria, even if the majority of arguments has been correctly identified, the complete event
structure will still be evaluated as incorrect. This helps to explain why such evaluation criteria may have
been deemed inappropriate in the original ACE 2005 evaluations.
Subsequently, we analysed the effects of utilising entity-related features. We show the results obtained
by adding entity information (+Entity) in Table 2 and the results obtained by removing entity information
(-Entity) in Table 3. The positive or negative effect on performance of adding or removing these features
is consistent across all subtask evaluations shown in the two tables, although the exact level of perfor-
mance improvement or degradation depends on the subtask under evaluation. Overall, the inclusion of
the features degraded the performance of BioEE on the GENIA task, but improved the performance of
Adapted BioEE on the ACE 2005 task. These differences may be due to the increased richness of en-
tity information in the ACE 2005 corpus, suggesting that enriching entities in the GENIA corpus with
attribute information could be a possible way to further improve the performance of the system on this
task.
6 Conclusions and Future Work
In this paper, we have described our adaptation of a biomedical event extraction method to the newswire
domain. We firstly evaluated the method on a biomedical event extraction task (GENIA), and showed
that its performance was superior to other state-of-the-art systems designed for the task. We then adapted
the method to a newswire event extraction task (ACE 2005), by addressing the major differences between
the tasks. With only a small number of adaptations, the resulting system was also able to achieve state-of-
the-art performance on the newswire extraction task. These results show that there is no need to develop
separate systems for event extraction tasks in different domains, as long as the types of tasks being
addressed exhibit domain-independent features. However, further discussion and evaluation is needed to
better understand how different potential methods for adapting such tools from one domain to another
can be used and/or combined effectively.
5
bionlp-st.dbcls.jp/GE/2013/results
2277
As future work, we intend to further investigate the adaptation of alternative methods proposed for
use in one domain to another domain. Several interesting approaches have been described, such as the
utilisation of contextual information beyond the boundaries of individual sentences in the newswire do-
main (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in the
biomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet been
investigated. We also intend to investigate the possibility of discovering and utilising shared information
between the two domains (Goldwasser and Roth, 2013). Encouraging greater levels of communication
between researchers working on NLP tasks in different domains will help to stimulate such new direc-
tions of research, both for event extraction and for other related information extraction tasks, such as
relation extraction and coreference resolution.
Acknowledgements
This work was supported by the Arts and Humanities Research Council (AHRC) [grant number
AH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Commu-
nity?s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-in-
Aid for Young Scientists (B) [grant number 25730129].
References
David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning
about Time and Events, pages 1?8, Sydney, Australia, July. ACL.
Jari Bj?orne and Tapio Salakoski. 2013. Tees 2.1: Automated annotation scheme learning in the bionlp 2013 shared
task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16?25, Sofia, Bulgaria, August. ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2. In Proceedings of the 7th Message Understanding
Conference (MUC-7/MET-2).
Hal Daum?e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Simple information extraction (sie): A portable
and effective ie system. In Proceedings of the Workshop on Adaptive Text Extraction and Mining (ATEM 2006),
pages 9?16, Trento, Italy, April. Association for Computational Linguistics.
Dan Goldwasser and Dan Roth. 2013. Leveraging domain-independent information in semantic parsing. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 462?466, Sofia, Bulgaria, August. Association for Computational Linguistics.
Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU?s english ACE 2005 system description. In
Proceedings of ACE 2005 Evaluation Workshop, Washington, US.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. Evex in st?13:
Application of a large-scale text mining resource to event extraction and network construction. In Proceedings
of the BioNLP Shared Task 2013 Workshop, pages 26?34, Sofia, Bulgaria, August. ACL.
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity in-
ference to improve event extraction. In Proceedings of the 49th ACL-HLT, pages 1127?1136, Portland, Oregon,
USA, June. ACL.
Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings
of ACL-08: HLT, pages 254?262, Columbus, Ohio, June. ACL.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori. 2013. The genia event extraction shared task, 2013 edition
- overview. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15, Sofia, Bulgaria, August.
ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proceedings of the 51st ACL, pages 73?82, Sofia, Bulgaria, August. ACL.
2278
Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction.
In Proceedings of the 48th ACL, pages 789?797, Uppsala, Sweden, July. ACL.
Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
835?844, Jeju Island, Korea, July. Association for Computational Linguistics.
David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher Manning. 2012. Com-
bining joint models for biomedical event extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational
Linguistics, 34(1):35?80, March.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
Longhua Qian and Guodong Zhou. 2012. Tree kernel-based protein?protein interaction extraction from biomedi-
cal literature. Journal of biomedical informatics, 45(3):535?543.
Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the
national conference on artificial intelligence, pages 1044?1049.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, YusukeMiyao, Yuichiro Matsubayashi, and Tomoko Ohta. 2007.
AKANE System: Protein-protein interaction pairs in BioCreAtIvE2 Challenge, PPI-IPS subtask. In Proceed-
ings of the Second BioCreative Challenge Evaluation Workshop, pages 209?212, CNIO, Madrid, Spain, April.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. ACL.
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training
corpus. Linguistic Data Consortium.
2279
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372?1376,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Simple Customization of Recursive Neural Networks
for Semantic Relation Classification
Kazuma Hashimoto?, Makoto Miwa??, Yoshimasa Tsuruoka?, and Takashi Chikayama?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp
??The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Abstract
In this paper, we present a recursive neural
network (RNN) model that works on a syn-
tactic tree. Our model differs from previous
RNN models in that the model allows for an
explicit weighting of important phrases for the
target task. We also propose to average param-
eters in training. Our experimental results on
semantic relation classification show that both
phrase categories and task-specific weighting
significantly improve the prediction accuracy
of the model. We also show that averaging the
model parameters is effective in stabilizing the
learning and improves generalization capacity.
The proposed model marks scores competitive
with state-of-the-art RNN-based models.
1 Introduction
Recursive Neural Network (RNN) models are
promising deep learning models which have been
applied to a variety of natural language processing
(NLP) tasks, such as sentiment classification, com-
pound similarity, relation classification and syntactic
parsing (Hermann and Blunsom, 2013; Socher et al,
2012; Socher et al, 2013). RNN models can repre-
sent phrases of arbitrary length in a vector space of
a fixed dimension. Most of them use minimal syn-
tactic information (Socher et al, 2012).
Recently, Hermann and Blunsom (2013) pro-
posed a method for leveraging syntactic informa-
tion, namely CCG combinatory operators, to guide
composition of phrases in RNN models. While their
models were successfully applied to binary senti-
ment classification and compound similarity tasks,
there are questions yet to be answered, e.g., whether
such enhancement is beneficial in other NLP tasks
as well, and whether a similar improvement can
be achieved by using syntactic information of more
commonly available types such as phrase categories
and syntactic heads.
In this paper, we present a supervised RNN model
for a semantic relation classification task. Our model
is different from existing RNN models in that impor-
tant phrases can be explicitly weighted for the task.
Syntactic information used in our model includes
part-of-speech (POS) tags, phrase categories and
syntactic heads. POS tags are used to assign vec-
tor representations to word-POS pairs. Phrase cate-
gories are used to determine which weight matrices
are chosen to combine phrases. Syntactic heads are
used to determine which phrase is weighted during
combining phrases. To incorporate task-specific in-
formation, phrases on the path between entity pairs
are further weighted.
The second contribution of our work is the intro-
duction of parameter averaging into RNN models.
In our preliminary experiments, we observed that
the prediction performance of the model often fluc-
tuates significantly between training iterations. This
fluctuation not only leads to unstable performance
of the resulting models, but also makes it difficult to
fine-tune the hyperparameters of the model. Inspired
by Swersky et al (2010), we propose to average the
model parameters in the course of training. A re-
cent technique for deep learning models of similar
vein is dropout (Hinton et al, 2012), but averaging
is simpler to implement.
Our experimental results show that our model per-
1372
Figure 1: A recursive representations of a phrase ?a
word vector? with POS tags of the words (DT, NN and
NN respectively). For example, the two word-POS pairs
?word NN? and ?vector NN? with a syntactic category
N are combined to represent the phrase ?word vector?.
forms better than standard RNN models. By av-
eraging the model parameters, our model achieves
performance competitive with the MV-RNN model
in Socher et al (2012), without using computation-
ally expensive word-dependent matrices.
2 An Averaged RNN Model with Syntax
Our model is a supervised RNN that works on a bi-
nary syntactic tree. As our first step to leverage in-
formation available in the tree, we distinguish words
with the same spelling but POS tags in the vector
space. Our model also uses different weight ma-
trices dependent on the phrase categories of child
nodes (phrases or words) in combining phrases. Our
model further weights those nodes that appear to be
important.
Compositional functions of our model follow
those of the SU-RNN model in Socher et al (2013).
2.1 Word-POS Vector Representations
Our model simply assigns vector representations to
word-POS pairs. For example, a word ?caused?
can be represented in two ways: ?caused VBD? and
?caused VBN?. The vectors are represented as col-
umn vectors in a matrix We ? Rd?|V|, where d is
the dimension of a vector and V is a set of all word-
POS pairs we consider.
2.2 Compositional Functions with Syntax
In construction of parse trees, we associate each of
the tree node with its d-dimensional vector represen-
tation computed from vector representations of its
subtrees. For leaf nodes, we look up word-POS vec-
tor representations in V. Figure 1 shows an example
of such recursive representations. A parent vector
p ? Rd?1 is computed from its direct child vectors
cl and cr? Rd?1:
p = tanh(?lW
Tcl ,Tcr
l cl+?rW
Tcl ,Tcrr cr+bTcl ,Tcr ),
where W Tcl ,Tcrl and W
Tcl ,Tcrr ? Rd?d are weight
matrices that depend on the phrase categories of cl
and cr. Here, cl and cr have phrase categories Tcl
and Tcr respectively (such as N, V, etc.). bTcl ,Tcr ?
Rd?1 is a bias vector. To incorporate the impor-
tance of phrases into the model, two subtrees of a
node may have different weights ?l ? [0, 1] and
?r(= 1 ? ?l), taking phrase importance into ac-
count. The value of ?l is manually specified and
automatically applied to all nodes based on prior
knowledge about the task. In this way, we can com-
pute vector representations for phrases of arbitrary
length. We denote a set of such matrices as Wlr and
bias vectors as b.
2.3 Objective Function and Learning
As with other RNN models, we add on the top of a
node x a softmax classifier. The classifier is used to
predict a K-class distribution d(x) ? RK?1 over a
specific task to train our model:
d(x) = softmax(W labelx+ blabel), (1)
where W label ? RK?d is a weight matrix and
blabel ? RK?1 is a bias vector. We denote t(x) ?
RK?1 as the target distribution vector at node x.
t(x) has a 0-1 encoding: the entry at the correct la-
bel of t(x) is 1, and the remaining entries are 0. We
then compute the cross entropy error between d(x)
and t(x):
E(x) = ?
K
?
k=1
tk(x)logdk(x),
and define an objective function as the sum of E(x)
over all training data:
J(?) =
?
x
E(x) + ?
2
???2,
where ? = (We,Wlr, b,W label, blabel) is the set of
our model parameters that should be learned. ? is a
vector of regularization parameters.
1373
To compute d(x), we can directly leverage any
other nodes? feature vectors in the same tree. We
denote such additional feature vectors as x?i ? Rd?1,
and extend Eq. (1):
d(x) = softmax(W labelx+
?
i
W addi x?i +blabel),
where W addi ? RK?d are weight matrices for addi-
tional features. We denote these matrices W addi as
W add. We also add W add to ?:
? = (We,Wlr, b,W label,W add, blabel).
The gradient of J(?)
?J(?)
??
=
?
x
?E(x)
??
+ ??
is efficiently computed via backpropagation through
structure (Goller and Ku?chler, 1996). To minimize
J(?), we use batch L-BFGS1 (Hermann and Blun-
som, 2013; Socher et al, 2012).
2.4 Averaging
We use averaged model parameters
? = 1
T + 1
T
?
t=0
?t
at test time, where ?t is the vector of model parame-
ters after t iterations of the L-BFGS optimization.
Our preliminary experimental results suggest that
averaging ? except We works well.
3 Experimental Settings
We used the Enju parser (Miyao and Tsujii, 2008)
for syntactic parsing. We used 13 phrase categories
given by Enju.
3.1 Task: Semantic Relation Classification
We evaluated our model on a semantic relation clas-
sification task: SemEval 2010 Task 8 (Hendrickx et
al., 2010). Following Socher et al (2012), we re-
garded the task as a 19-class classification problem.
There are 8,000 samples for training, and 2,717 for
1We used libLBFGS provided at http://www.
chokkan.org/software/liblbfgs/.
Figure 2: Classifying the relation between two entities.
test. For the validation set, we randomly sampled
2,182 samples from the training data.
To predict a class label, we first find the minimal
phrase that covers the target entities and then use the
vector representation of the phrase (Figure 2).
As explained in Section 2.3, we can directly con-
nect features on any other nodes to the softmax clas-
sifier. In this work, we used three such internal fea-
tures: two vector representations of target entities
and one averaged vector representation of words be-
tween the entities2.
3.2 Weights on Phrases
We tuned the weight ?l (or ?r) introduced in Sec-
tion 2.2 for this particular task. There are two fac-
tors: syntactic heads and syntactic path between tar-
get entities. Our model puts a weight ? ? [0.5, 1]
on head phrases, and 1 ? ? on the others. For re-
lation classification tasks, syntactic paths between
target entities are important (Zhang et al, 2006), so
our model also puts another weight ? ? [0.5, 1] on
phrases on the path, and 1 ? ? on the others. When
both child nodes are on the path or neither of them
on the path, we set ? = 0.5. The two weight fac-
tors are summed up and divided by 2 to be the final
weights ?l and ?r to combine the phrases. For ex-
ample, we set ?l = (1??)+?2 and ?r =
?+(1??)
2
when the right child node is the head and the left
child node is on the path.
3.3 Initialization of Model Parameters and
Tuning of Hyperparameters
We initialized We with 50-dimensional word vec-
tors3 trained with the model of Collobert et
2Socher et al (2012) used richer features including words
around entity pairs in their implementation.
3The word vectors are provided at http://ronan.
collobert.com/senna/. We used the vectors without any
modifications such as normalization.
1374
Method F1 (%)
Our model 79.4
RNN 74.8
MV-RNN 79.1
RNN w/ POS, WordNet, NER 77.6
MV-RNN w/ POS, WordNet, NER 82.4
SVM w/ bag-of-words 73.1
SVM w/ lexical and semantic features 82.2
Table 1: Comparison of our model with other methods on
SemEval 2010 task 8.
Method F1 (%)
Our model 79.4
Our model w/o phrase categories (PC) 77.7
Our model w/o head weights (HW) 78.8
Our model w/o path weights (PW) 78.7
Our model w/o averaging (AVE) 76.9
Our model w/o PC, HW, PW, AVE 74.1
Table 2: Contributions of syntactic and task-specific in-
formation and averaging.
al. (2011), and Wlr with I2 + ?, where I ? Rd?d
is an identity matrix. Here, ? is zero-mean gaussian
random variable with a variance of 0.01. The ini-
tialization of Wlr is the same as that of Socher et
al. (2013). The remaining model parameters were
initialized with 0.
We tuned hyperparameters in our model using the
validation set for each experimental setting. The hy-
perparameters include the regularization parameters
for We, Wlr, W label and W add, and the weights ?
and ?. For example, the best performance for our
model with all the proposed methods was obtained
with the values: 10?6, 10?4, 10?3, 10?3, 0.7 and
0.9 respectively.
4 Results and Discussion
Table 1 shows the performance of our model and that
of previously reported systems on the test set. The
performance of an SVM system with bag-of-words
features was reported in Rink and Harabagiu (2010),
and the performance of the RNN and MV-RNN
models was reported in Socher et al (2012). Our
model achieves an F1 score of 79.4% and outper-
forms the RNN model (74.8% F1) as well as the
simple SVM-based system (73.1% F1). More no-
Figure 3: F1 vs Training iterations.
tably, the score of our model is competitive with that
of the MV-RNN model (79.1% F1), which is com-
putationally much more expensive. Readers are re-
ferred to Hermann and Blunsom (2013) for the dis-
cussion about the computational complexity of the
MV-RNN model. We improved the performance of
RNN models on the task without much increasing
the complexity. This is a significant practical advan-
tage of our model, although its expressive power is
not the same as that of the MV-RNN model.
Our model outperforms the RNN model with one
lexical and two semantic external features: POS
tags, WordNet hypernyms and named entity tags
(NER) of target word pairs (external features). The
MV-RNN model with external features shows bet-
ter performance than our model. An SVM with rich
lexical and semantic features (Rink and Harabagiu,
2010) also outperforms ours. Note, however, that
this is not a fair comparison because those mod-
els use rich external resources such as WordNet and
named entity tags.
4.1 Contributions of Proposed Methods
We conducted additional experiments to quantify the
contributions of phrase categories, heads, paths and
averaging to our classification score. As shown in
Table 2, our model without phrase categories, heads
or paths still outperforms the RNN model with ex-
ternal features. On the other hand, our model with-
out averaging yields a lower score than the RNN
model with external features, though it is still bet-
1375
ter than the RNN model alone. Without utiliz-
ing these four properties, our model obtained only
74.1% F1. These results indicate that syntactic and
task-specific information and averaging contribute
to the performance improvement. The improvement
is achieved by a simple modification of composi-
tional functions in RNN models.
4.2 Effects of Averaging in Training
Figure 3 shows the training curves in terms of F1
scores. These curves clearly demonstrate that pa-
rameter averaging helps to stabilize the learning and
improve generalization capacity.
5 Conclusion
We have presented an averaged RNN model for se-
mantic relation classification. Our experimental re-
sults show that syntactic information such as phrase
categories and heads improves the performance, and
the task-specific weighting is also beneficial. The
results also demonstrate that averaging the model
parameters not only stabilizes the learning but also
improves the generalization capacity of the model.
As future work, we plan to combine deep learning
models with richer information such as predicate-
argument structures.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful comments.
References
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
In JMLR, 12:2493?2537.
Christoph Goller and Andreas Ku?chler. 1996. Learning
Task-Dependent Distributed Representations by Back-
propagation Through Structure. In ICNN.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano and Stan Szpakowicz.
2010. SemEval-2010 Task 8: Multi-Way Classication
of Semantic Relations Between Pairs of Nominals. In
SemEval 2010.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional Se-
mantics. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever and Ruslan R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. In arXiv:1207.0580.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, 34(1):35?80, MIT Press.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical and
Semantic Resources. In SemEval 2010.
Richard Socher, Brody Huval, Christopher D. Manning
and Andrew Y. Ng. 2012. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning and
Andrew Y. Ng. 2013. Parsing with Compositional
Vector Grammars. In ACL.
Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-
itas. 2010. A tutorial on stochastic approximation al-
gorithms for training Restricted Boltzmann Machines
and Deep Belief Nets. In ITA workshop.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006.
A Composite Kernel to Extract Relations between En-
tities with Both Flat and Structured Features. In COL-
ING/ACL.
1376
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544?1555,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Jointly Learning Word Representations and Composition Functions
Using Predicate-Argument Structures
Kazuma Hashimoto?, Pontus Stenetorp?, Makoto Miwa?, and Yoshimasa Tsuruoka?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp
?Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
makoto-miwa@toyota-ti.ac.jp
Abstract
We introduce a novel compositional lan-
guage model that works on Predicate-
Argument Structures (PASs). Our model
jointly learns word representations and
their composition functions using bag-
of-words and dependency-based con-
texts. Unlike previous word-sequence-
based models, our PAS-based model com-
poses arguments into predicates by using
the category information from the PAS.
This enables our model to capture long-
range dependencies between words and
to better handle constructs such as verb-
object and subject-verb-object relations.
We verify this experimentally using two
phrase similarity datasets and achieve re-
sults comparable to or higher than the pre-
vious best results. Our system achieves
these results without the need for pre-
trained word vectors and using a much
smaller training corpus; despite this, for
the subject-verb-object dataset our model
improves upon the state of the art by as
much as ?10% in relative performance.
1 Introduction
Studies on embedding single words in a vector
space have made notable successes in capturing
their syntactic and semantic properties (Turney
and Pantel, 2010). These embeddings have also
been found to be a useful component for Natural
Language Processing (NLP) systems; for exam-
ple, Turian et al. (2010) and Collobert et al. (2011)
demonstrated how low-dimensional word vectors
learned by Neural Network Language Models
(NNLMs) are beneficial for a wide range of NLP
tasks.
Recently, the main focus of research on vector
space representation is shifting from word repre-
sentations to phrase representations (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Combining the ideas of NNLMs and se-
mantic composition, Tsubaki et al. (2013) intro-
duced a novel NNLM incorporating verb-object
dependencies. More recently, Levy and Goldberg
(2014) presented a NNLM that integrated syntac-
tic dependencies. However, to the best of our
knowledge, there is no previous work on integrat-
ing a variety of syntactic and semantic dependen-
cies into NNLMs in order to learn composition
functions as well as word representations. The fol-
lowing question thus arises naturally:
Can a variety of dependencies be used to
jointly learn both stand-alone word vectors
and their compositions, embedding them in
the same vector space?
In this work, we bridge the gap between
purely context-based (Levy and Goldberg, 2014;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013) and compositional (Tsubaki et al., 2013)
NNLMs by using the flexible set of categories
from Predicate-Argument-Structures (PASs).
More specifically, we propose a Compositional
Log-Bilinear Language Model using PASs (PAS-
CLBLM), an overview of which is shown in
Figure 1. The model is trained by maximizing
the accuracy of predicting target words from their
bag-of-words and dependency-based context,
which provides information about selectional
preference. As shown in Figure 1 (b), one of the
advantages of the PAS-CLBLM is that the model
can treat not only word vectors but also composed
vectors as contexts. Since the composed vectors
1544
Figure 1: An overview of the proposed model: PAS-CLBLM. (a) The PAS-LBLM predicts target words
from their bag-of-words and dependency-based context words. (b) The PAS-CLBLM predicts target
words using not only context words but also composed vector representations derived from another level
of predicate-argument structures. Underlined words are target words and we only depict the bag-of-
words vector for the PAS-CLBLM.
are treated as input to the language model in
the same way as word vectors, these composed
vectors are expected to become similar to word
vectors for words with similar meanings.
Our empirical results demonstrate that the pro-
posed model has the ability to learn meaning-
ful representations for adjective-noun, noun-noun,
and (subject-) verb-object dependencies. On three
tasks of measuring the semantic similarity be-
tween short phrases (adjective-noun, noun-noun,
and verb-object), the learned composed vectors
achieve scores (Spearman?s rank correlation ?)
comparable to or higher than those of previ-
ous models. On a task involving more complex
phrases (subject-verb-object), our learned com-
posed vectors achieve state-of-the-art performance
(? = 0.50) with a training corpus that is an order
of magnitude smaller than that used by previous
work (Tsubaki et al., 2013; Van de Cruys et al.,
2013). Moreover, the proposed model does not
require any pre-trained word vectors produced by
external models, but rather induces word vectors
jointly while training.
2 Related Work
There is a large body of work on how to represent
the meaning of a word in a vector space. Distri-
butional approaches assume that the meaning of
a word is determined by the contexts in which it
appears (Firth, 1957). The context of a word is of-
ten defined as the words appearing in a window
of fixed-length (bag-of-words) and a simple ap-
proach is to treat the co-occurrence statistics of a
word w as a vector representation for w (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010); al-
ternatively, dependencies between words can be
used to define contexts (Goyal et al., 2013; Erk
and Pado?, 2008; Thater et al., 2010).
In contrast to distributional representations,
NNLMs represent words in a low-dimensional
vector space (Bengio et al., 2003; Collobert et al.,
2011). Recently, Mikolov et al. (2013b) and Mnih
and Kavukcuoglu (2013) proposed highly scalable
models to learn high-dimensional word vectors.
Levy and Goldberg (2014) extended the model of
Mikolov et al. (2013b) by treating syntactic depen-
dencies as contexts.
Mitchell and Lapata (2008) investigated a vari-
ety of compositional operators to combine word
vectors into phrasal representations. Among these
operators, simple element-wise addition and mul-
tiplication are now widely used to represent short
phrases (Mitchell and Lapata, 2010; Blacoe and
Lapata, 2012). The obvious limitation with these
simple approaches is that information about word
order and syntactic relations is lost.
To incorporate syntactic information into com-
position functions, a variety of compositional
models have been proposed. These include recur-
sive neural networks using phrase-structure trees
(Socher et al., 2012; Socher et al., 2013b) and
models in which words have a specific form of
parameters according to their syntactic roles and
composition functions are syntactically dependent
on the relations of input words (Baroni and Zam-
parelli, 2010; Grefenstette and Sadrzadeh, 2011;
Hashimoto et al., 2013; Hermann and Blunsom,
2013; Socher et al., 2013a).
More recently, syntactic dependency-based
1545
compositional models have been proposed (Pa-
perno et al., 2014; Socher et al., 2014; Tsub-
aki et al., 2013). One of the advantages of these
models is that they are less restricted by word or-
der. Among these, Tsubaki et al. (2013) intro-
duced a novel compositional NNLM mainly fo-
cusing on verb-object dependencies and achieved
state-of-the-art performance for the task of mea-
suring the semantic similarity between subject-
verb-object phrases.
3 PAS-CLBLM: A Compositional
Log-Bilinear Language Model Using
Predicate-Argument Structures
In some recent studies on representing words as
vectors, word vectors are learned by solving word
prediction tasks (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013). More specifically, given tar-
get words and their context words, the basic idea
is to train classifiers to discriminate between each
target word and artificially induced negative tar-
get words. The feature vector of the classifiers are
calculated using the context word vectors whose
values are optimized during training. As a result,
vectors of words in similar contexts become simi-
lar to each other.
Following these studies, we propose a novel
model to jointly learn representations for words
and their compositions by training word predic-
tion classifiers using PASs. In this section, we
first describe the predicate-argument structures as
they serve as the basis of our model. We then
introduce a Log-Bilinear Language Model us-
ing Predicate-Argument Structures (PAS-LBLM)
to learn word representations using both bag-of-
words and dependency-based contexts. Finally,
we propose integrating compositions of words into
the model. Figure 1 (b) shows the overview of the
proposed model.
3.1 Predicate-Argument Structures
Due to advances in deep parsing technologies,
syntactic parsers that can produce predicate-
argument structures are becoming accurate and
fast enough to be used for practical applications.
In this work, we use the probabilistic HPSG
parser Enju (Miyao and Tsujii, 2008) to obtain the
predicate-argument structures of individual sen-
tences. In its grammar, each word in a sentence
is treated as a predicate of a certain category with
zero or more arguments. Table 1 shows some ex-
Category predicate arg1 arg2
adj arg1 heavy rain
noun arg1 car accident
verb arg12 cause rain accident
prep arg12 at eat restaurant
Table 1: Examples of predicates of different cate-
gories from the grammar of the Enju parser. arg1
and arg2 denote the first and second arguments.
amples of predicates of different categories.1 For
example, a predicate of the category verb arg12
expresses a verb with two arguments. A graph can
be constructed by connecting words in predicate-
argument structures in a sentence; in general, these
graphs are acyclic.
One of the merits of using predicate-argument
structures is that they can capture dependencies
between more than two words, while standard syn-
tactic dependency structures are limited to depen-
dencies between two words. For example, one of
the predicates in the phrase ?heavy rain caused car
accidents? is the verb ?cause?, and it has two ar-
guments (?rain? and ?accident?). Furthermore, ex-
actly the same predicate-argument structure (pred-
icate: cause, first argument: rain, second argu-
ment: accident) is extracted from the passive form
of the above phrase: ?car accidents were caused
by heavy rain?. This is helpful when capturing
semantic dependencies between predicates and ar-
guments, and in extracting facts or relations de-
scribed in a sentence, such as who did what to
whom.
3.2 A Log-Bilinear Language Model Using
Predicate-Argument Structures
3.2.1 PAS-based Word Prediction
The PAS-LBLM predicts a target word given its
PAS-based context. We assume that each word
w in the vocabulary V is represented with a d-
dimensional vector v(w). When a predicate of
category c is extracted from a sentence, the PAS-
LBLM computes the predicted d-dimensional vec-
tor p(w
t
) for the target word w
t
from its context
words w
1
, w
2
, . . . , w
m
:
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
)
)
, (1)
1The categories of the predicates in the Enju parser are
summarized at http://kmcs.nii.ac.jp/
?
yusuke/
enju/enju-manual/enju-output-spec.html.
1546
where hc
i
? R
d?1 are category-specific weight
vectors and ? denotes element-wise multiplica-
tion. f is a non-linearity function; in this work
we define f as tanh.
As an example following Figure 1 (a), when
the predicate ?cause? is extracted with its first
and second arguments ?rain? and ?accident?, the
PAS-LBLM computes p(cause) ? Rd following
Eq. (1):
p(cause) = f(h
verb arg12
arg1
? v(rain)+
h
verb arg12
arg2
? v(accident)).
(2)
In Eq. (2), the predicate is treated as the target
word, and its arguments are treated as the con-
text words. In the same way, an argument can be
treated as a target word:
p(rain) = f(h
verb arg12
verb
? v(cause)+
h
verb arg12
arg2
? v(accident)).
(3)
Relationship to previous work. If we omit the
the category-specific weight vectors hc
i
in Eq. (1),
our model is similar to the CBOW model in
Mikolov et al. (2013a). CBOW predicts a tar-
get word given its surrounding bag-of-words con-
text, while our model uses its PAS-based context.
To incorporate the PAS information in our model
more efficiently, we use category-specific weight
vectors. Similarly, the vLBL model of Mnih and
Kavukcuoglu (2013) uses different weight vec-
tors depending on the position relative to the tar-
get word. As with previous neural network lan-
guage models (Collobert et al., 2011; Huang et al.,
2012), our model and vLBL can use weight ma-
trices rather than weight vectors. However, as dis-
cussed by Mnih and Teh (2012), using weight vec-
tors makes the training significantly faster than us-
ing weight matrices. Despite the simple formula-
tion of the element-wise operations, the category-
specific weight vectors efficiently propagate PAS-
based context information as explained next.
3.2.2 Training Word Vectors
To train the PAS-LBLM, we use a scoring function
to evaluate how well the target word w
t
fits the
given context:
s(w
t
, p(w
t
)) = v?(w
t
)
T
p(w
t
), (4)
where v?(w
t
) ? R
d?1 is the scoring weight vector
for w
t
. Thus, the model parameters in the PAS-
LBLM are (V, ?V ,H). V is the set of word vec-
tors v(w), and ?V is the set of scoring weight vec-
tors v?(w). H is the set of the predicate-category-
specific weight vectors hc
i
.
Based on the objective in the model of Collobert
et al. (2011), the model parameters are learned by
minimizing the following hinge loss:
N
?
n=1
max(1? s(w
t
, p(w
t
)) + s(w
n
, p(w
t
)), 0),
(5)
where the negative sample w
n
is a randomly sam-
pled word other than w
t
, and N is the number
of negative samples. In our experiments we set
N = 1. Following Mikolov et al. (2013b), nega-
tive samples were drawn from the distribution over
unigrams that we raise to the power 0.75 and then
normalize to once again attain a probability distri-
bution. We minimize the loss function in Eq. (5)
using AdaGrad (Duchi et al., 2011). For further
training details, see Section 4.5.
Relationship to softmax regression models.
The model parameters can be learned by maximiz-
ing the log probability of the target word w
t
based
on the softmax function:
p(w
t
|context) =
exp(s(w
t
, p(w
t
)))
?
|V|
i=1
exp(s(w
i
, p(w
t
)))
. (6)
This is equivalent to a softmax regression model.
However, when the vocabulary V is large, com-
puting the softmax function in Eq. (6) is compu-
tationally expensive. If we do not need probabil-
ity distributions over words, we are not necessar-
ily restricted to using the probabilistic expressions.
Recently, several methods have been proposed to
efficiently learn word representations rather than
accurate language models (Collobert et al., 2011;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013), and our objective follows the work of Col-
lobert et al. (2011). Mikolov et al. (2013b) and
Mnih and Kavukcuoglu (2013) trained their mod-
els using word-dependent scoring weight vectors
which are the arguments of our scoring function
in Eq. (4). During development we also trained
our model using the negative sampling technique
of Mikolov et al. (2013b); however, we did not ob-
serve any significant performance difference.
Intuition behind the PAS-LBLM. Here we
briefly explain how each class of the model pa-
rameters of the PAS-LBLM contributes to learning
word representations at each stochastic gradient
1547
decent step. The category-specific weight vectors
provide the PAS information for context word vec-
tors which we would like to learn. During train-
ing, context word vectors having the same PAS-
based syntactic roles are updated similarly. The
word-dependent scoring weight vectors propagate
the information on which words should, or should
not, be predicted. In effect, context word vectors
making similar contributions to word predictions
are updated similarly. The non-linear function f
provides context words with information on the
other context words in the same PAS. In this way,
word vectors are expected to be learned efficiently
by the PAS-LBLM.
3.3 Learning Composition Functions
As explained in Section 3.1, predicate-argument
structures inherently form graphs whose nodes are
words in a sentence. Using the graphs, we can in-
tegrate relationships between multiple predicate-
argument structures into our model.
When the context word w
i
in Eq. (1), excluding
predicate words, has another predicate-argument
of category c? as a dependency, we replace v(w
i
)
with the vector produced by the composition func-
tion for the predicate category c?. For example,
as shown in Figure 1 (b), when the first argument
?rain? of the predicate ?cause? is also the argu-
ment of the predicate ?heavy?, we first compute
the d-dimensional composed vector representation
for ?heavy? and ?rain?:
g
c
?
(v(heavy), v(rain)), (7)
where c? is the category adj arg1, and g
c
? is a func-
tion to combine input vectors for the predicate-
category c?. We can use any composition func-
tion that produces a representation of the same
dimensionality as its inputs, such as element-
wise addition/multiplication (Mitchell and Lap-
ata, 2008) or neural networks (Socher et al.,
2012). We then replace v(rain) in Eq. (2) with
g
c
?
(v(heavy), v(rain)). When the second argu-
ment ?accident? in Eq. (2) is also the argument
of the predicate ?car?, v(accident) is replaced
with g
c
??
(v(car), v(accident)). c
?? is the predi-
cate category noun arg1. These multiple relation-
ships of predicate-argument structures should pro-
vide richer context information. We refer to the
PAS-LBLM with composition functions as PAS-
CLBLM.
3.4 Bag-of-Words Sensitive PAS-CLBLM
Both the PAS-LBLM and PAS-CLBLM can take
meaningful relationships between words into ac-
count. However, at times, the number of context
words can be limited and the ability of other mod-
els to take ten or more words from a fixed con-
text in a bag-of-words (BoW) fashion could com-
pensate for this sparseness. Huang et al. (2012)
combined local and global contexts in their neural
network language models, and motivated by their
work, we integrate bag-of-words vectors into our
models. Concretely, we add an additional input
term to Eq. (1):
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
) + h
c
BoW
? v(BoW)
)
,
(8)
where hc
BoW
? R
d?1 are additional weight vec-
tors, and v(BoW) ? Rd?1 is the average of the
word vectors in the same sentence. To construct
the v(BoW) for each sentence, we average the
word vectors of nouns and verbs in the same sen-
tence, excluding the target and context words.
4 Experimental Settings
4.1 Training Corpus
We used the British National Corpus (BNC) as our
training corpus, extracted 6 million sentences from
the original BNC files, and parsed them using the
Enju parser described in Section 3.1.
4.2 Word Sense Disambiguation Using
Part-of-Speech Tags
In general, words can have multiple syntactic us-
ages. For example, the word cause can be a
noun or a verb depending on its context. Most
of the previous work on learning word vectors
ignores this ambiguity since word sense disam-
biguation could potentially be performed after the
word vectors have been trained (Huang et al.,
2012; Kartsaklis and Sadrzadeh, 2013). Some re-
cent work explicitly assigns an independent vec-
tor for each word usage according to its part-of-
speech (POS) tag (Hashimoto et al., 2013; Kart-
saklis and Sadrzadeh, 2013). Alternatively, Baroni
and Zamparelli (2010) assigned different forms of
parameters to adjectives and nouns.
In our experiments, we combined each word
with its corresponding POS tags. We used the
base-forms provided by the Enju parser rather than
1548
Figure 2: Two PAS-CLBLM training samples.
the surface-forms, and used the first two charac-
ters of the POS tags. For example, VB, VBP,
VBZ, VBG, VBD, VBN were all mapped to VB.
This resulted in two kinds of cause: cause NN and
cause VB and we used the 100,000 most frequent
lowercased word-POS pairs in the BNC.
4.3 Selection of Training Samples Based on
Categories of Predicates
To train the PAS-LBLM and PAS-CLBLM, we
could use all predicate categories. However, our
preliminary experiments showed that these cate-
gories covered many training samples which are
not directly relevant to our experimental setting,
such as determiner-noun dependencies. We thus
manually selected the categories used in our ex-
periments. The selected predicates are listed in
Table 1: adj arg1, noun arg1, prep arg12, and
verb arg12. These categories should provide
meaningful information on selectional preference.
For example, the prep arg12 denotes prepositions
with two arguments, such as ?eat at restaurant?
which means that the verb ?eat? is related to the
noun ?restaurant? by the preposition ?at?. Prepo-
sitions are one of the predicates whose arguments
can be verbs, and thus prepositions are important
in training the composition functions for (subject-)
verb-object dependencies as described in the next
paragraph.
Another point we had to consider was how
to construct the training samples for the PAS-
CLBLM. We constructed compositional training
samples as explained in Section 3.3 when c? was
adj arg1, noun arg1, or verb arg12. Figure 2
shows two examples in addition to the example
in Figure 1 (b). Using such training samples, the
PAS-CLBLM could, for example, recognize from
the two predicate-argument structures, ?eat food?
and ?eat at restaurant?, that eating foods is an ac-
tion that occurs at restaurants.
Model Composition Function
Add
l
v(w
1
) + v(w
2
)
Add
nl
tanh(v(w
1
) + v(w
2
))
Wadd
l
m
c
adj
? v(w
1
) + m
c
arg1
? v(w
2
)
Wadd
nl
tanh(m
c
adj
?v(w
1
)+m
c
arg1
?v(w
2
))
Table 2: Composition functions used in this work.
The examples are shown as the adjective-noun de-
pendency between w
1
=?heavy? and w
2
=?rain?.
4.4 Selection of Composition Functions
As described in Section 3.3, we are free to se-
lect any composition functions in Eq. (7). To
maintain the fast training speed of the PAS-
LBLM, we avoid dense matrix-vector multiplica-
tion in our composition functions. In Table 2,
we list the composition functions used for the
PAS-CLBLM. Add
l
is element-wise addition and
Add
nl
is element-wise addition with the non-
linear function tanh. The subscripts l and nl de-
note the words linear and non-linear. Similarly,
Wadd
l
is element-wise weighted addition and
Wadd
nl
is element-wise weighted addition with
the non-linear function tanh. The weight vec-
tors mc
i
? R
d?1 in Table 2 are predicate-category-
specific parameters which are learned during train-
ing. We investigate the effects of the non-linear
function tanh for these composition functions.
In the formulations of the backpropagation algo-
rithm, non-linear functions allow the input vectors
to weakly interact with each other.
4.5 Initialization and Optimization of Model
Parameters
We assigned a 50-dimensional vector for each
word-POS pair described in Section 4.2 and ini-
tialized the vectors and the scoring weight vec-
tors using small random values. In part inspired
by the initialization method of the weight matrices
in Socher et al. (2013a), we initialized all values
in the compositional weight vectors of the Wadd
l
and Wadd
nl
as 1.0. The context weight vectors
were initialized using small random values.
We minimized the loss function in Eq. (5) us-
ing mini-batch SGD and AdaGrad (Duchi et al.,
2011). Using AdaGrad, the SGD?s learning rate
is adapted independently for each model parame-
ter. This is helpful in training the PAS-LBLM and
PAS-CLBLM, as they have conditionally depen-
dent model parameters with varying frequencies.
1549
The mini-batch size was 32 and the learning rate
was 0.05 for each experiment, and no regulariza-
tion was used. To verify the semantics captured by
the proposed models during training and to tune
the hyperparameters, we used the WordSim-3532
word similarity data set (Finkelstein et al., 2001).
5 Evaluation on Phrase Similarity Tasks
5.1 Evaluation Settings
The learned models were evaluated on four tasks
of measuring the semantic similarity between
short phrases. We performed evaluation using the
three tasks (AN, NN, and VO) in the dataset3 pro-
vided by Mitchell and Lapata (2010), and the SVO
task in the dataset4 provided by Grefenstette and
Sadrzadeh (2011).
The datasets include pairs of short phrases ex-
tracted from the BNC. AN, NN, and VO con-
tain 108 phrase pairs of adjective-noun, noun-
noun, and verb-object. SVO contains 200 pairs of
subject-verb-object phrases. Each phrase pair has
multiple human-ratings: the higher the rating is,
the more semantically similar the phrases. For ex-
ample, the subject-verb-object phrase pair of ?stu-
dent write name? and ?student spell name? has a
high rating. The pair ?people try door? and ?peo-
ple judge door? has a low rating.
For evaluation we used the Spearman?s rank
correlation ? between the human-ratings and the
cosine similarity between the composed vector
pairs. We mainly used non-averaged human-
ratings for each pair, and as described in Section
5.3, we also used averaged human-ratings for the
SVO task. Each phrase pair in the datasets was an-
notated by more than two annotators. In the case
of averaged human ratings, we averaged multiple
human-ratings for each phrase pair, and in the case
of non-averaged human-ratings, we treated each
human-rating as a separate annotation.
With the PAS-CLBLM, we represented each
phrase using the composition functions listed in
Table 2. When there was no composition present,
we represented the phrase using element-wise ad-
dition. For example, when we trained the PAS-
CLBLM with the composition function Wadd
nl
,
2http://www.cs.technion.ac.il/
?
gabr/
resources/data/wordsim353/
3http://homepages.inf.ed.ac.uk/
s0453356/share
4http://www.cs.ox.ac.uk/activities/
compdistmeaning/GS2011data.txt
Model AN NN VO
PAS-CLBLM (Add
l
) 0.52 0.44 0.35
PAS-CLBLM (Add
nl
) 0.52 0.46 0.45
PAS-CLBLM (Wadd
l
) 0.48 0.39 0.34
PAS-CLBLM (Wadd
nl
) 0.48 0.40 0.39
PAS-LBLM 0.41 0.44 0.39
word2vec 0.52 0.48 0.42
BL w/ BNC 0.48 0.50 0.35
HB w/ BNC 0.41 0.44 0.34
KS w/ ukWaC n/a n/a 0.45
K w/ BNC n/a n/a 0.41
Human agreement 0.52 0.49 0.55
Table 3: Spearman?s rank correlation scores ? for
the three tasks: AN, NN, and VO.
the composed vector for each phrase was com-
puted using the Wadd
nl
function, and when we
trained the PAS-LBLM, we used the element-wise
addition function. To compute the composed vec-
tors using the Wadd
l
and Wadd
nl
functions, we
used the categories of the predicates adj arg1,
noun arg1, and verb arg12 listed in Table 1.
As a strong baseline, we trained the Skip-gram
model of Mikolov et al. (2013b) using the pub-
licly available word2vec5 software. We fed the
POS-tagged BNC into word2vec since our models
utilize POS tags and trained 50-dimensional word
vectors using word2vec. For each phrase we then
computed the representation using vector addition.
5.2 AN, NN, and VO Tasks
Table 3 shows the correlation scores ? for the AN,
NN, and VO tasks. Human agreement denotes the
inter-annotator agreement. The word2vec baseline
achieves unexpectedly high scores for these three
tasks. Previously these kinds of models (Mikolov
et al., 2013b; Mnih and Kavukcuoglu, 2013) have
mainly been evaluated for word analogy tasks and,
to date, there has been no work using these word
vectors for the task of measuring the semantic sim-
ilarity between phrases. However, this experimen-
tal result suggests that word2vec can serve as a
strong baseline for these kinds of tasks, in addi-
tion to word analogy tasks.
In Table 3, BL, HB, KS, and K denote the work
of Blacoe and Lapata (2012), Hermann and Blun-
som (2013), Kartsaklis and Sadrzadeh (2013), and
Kartsaklis et al. (2013) respectively. Among these,
5https://code.google.com/p/word2vec/
1550
Averaged Non-averaged
Model Corpus SVO-SVO SVO-V SVO-SVO SVO-V
PAS-CLBLM (Add
l
) 0.29 0.34 0.24 0.28
PAS-CLBLM (Add
nl
) 0.27 0.32 0.24 0.28
PAS-CLBLM (Wadd
l
) BNC 0.25 0.26 0.21 0.23
PAS-CLBLM (Wadd
nl
) 0.42 0.50 0.34 0.41
PAS-LBLM 0.21 0.06 0.18 0.08
word2vec BNC 0.12 0.32 0.12 0.28
Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a
Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a
Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37
Human agreement 0.75 0.62
Table 4: Spearman?s rank correlation scores ? for the SVO task. Averaged denotes the ? calculated by
averaged human ratings, and Non-averaged denotes the ? calculated by non-averaged human ratings.
only Kartsaklis and Sadrzadeh (2013) used the
ukWaC corpus (Baroni et al., 2009) which is an or-
der of magnitude larger than the BNC. As we can
see in Table 3, the PAS-CLBLM (Add
nl
) achieves
scores comparable to and higher than those of the
baseline and the previous state-of-the-art results.
In relation to these results, the Wadd
l
and Wadd
nl
variants of the PAS-CLBLM do not achieve great
improvements in performance. This indicates that
simple word vector addition can be sufficient to
compose representations for phrases consisting of
word pairs.
5.3 SVO Task
Table 4 shows the correlation scores ? for the SVO
task. The scores ? for this task are reported for
both averaged and non-averaged human ratings.
This is due to a disagreement in previous work
regarding which metric to use when reporting re-
sults. Hence, we report the scores for both settings
in Table 4. Another point we should consider is
that some previous work reported scores based on
the similarity between composed representations
(Grefenstette and Sadrzadeh, 2011; Van de Cruys
et al., 2013), and others reported scores based on
the similarity between composed representations
and word representations of landmark verbs from
the dataset (Tsubaki et al., 2013; Van de Cruys et
al., 2013). For completeness, we report the scores
for both settings: SVO-SVO and SVO-V in Table 4.
The results show that the weighted addition
model with the non-linear function tanh (PAS-
CLBLM (Wadd
nl
)) is effective for the more com-
plex phrase task. While simple vector addition is
sufficient for phrases consisting of word pairs, it is
clear from our experimental results that they fall
short for more complex structures such as those
involved in the SVO task.
Our PAS-CLBLM (Wadd
nl
) model outperforms
the previous state-of-the-art scores for the SVO
task as reported by Tsubaki et al. (2013) and
Van de Cruys et al. (2013). As such, there are three
key points that we would like to emphasize:
(1) the difference of the training corpus size,
(2) the necessity of the pre-trained word vectors,
(3) the modularity of deep learning models.
Tsubaki et al. (2013) and Van de Cruys et al.
(2013) used the ukWaC corpus. This means our
model works better, despite using a considerably
smaller corpora. It should also be noted that, like
us, Grefenstette and Sadrzadeh (2011) used the
BNC corpus.
The model of Tsubaki et al. (2013) is based on
neural network language models which use syn-
tactic dependencies between verbs and their ob-
jects. While their novel model, which incorpo-
rates the idea of co-compositionality, works well
with pre-trained word vectors produced by exter-
nal models, it is not clear whether the pre-trained
vectors are required to achieve high scores. In
contrast, we have achieved state-of-the-art results
without the use of pre-trained word vectors.
Despite our model?s scalability, we trained 50-
dimensional vector representations for words and
their composition functions and achieved high
scores using this low dimensional vector space.
1551
model d AN NN VO SVO
Add
l
50 0.52 0.44 0.35 0.24
1000 0.51 0.51 0.43 0.31
Add
nl
50 0.52 0.46 0.45 0.24
1000 0.51 0.50 0.45 0.31
Wadd
l
50 0.48 0.39 0.34 0.21
1000 0.50 0.49 0.43 0.32
Wadd
nl
50 0.48 0.40 0.39 0.34
1000 0.51 0.48 0.48 0.34
Table 5: Comparison of the PAS-CLBLM between
d = 50 and d = 1000.
This maintains the possibility to incorporate re-
cently developed deep learning composition func-
tions into our models, such as recursive neural
tensor networks (Socher et al., 2013b) and co-
compositional neural networks (Tsubaki et al.,
2013). While such complex composition functions
slow down the training of compositional models,
richer information could be captured during train-
ing.
5.4 Effects of the Dimensionality
To see how the dimensionality of the word vectors
affects the scores, we trained the PAS-CLBLM for
each setting using 1,000-dimensional word vectors
and set the learning rate to 0.01. Table 5 shows
the scores for all four tasks. Note that we only re-
port the scores for the setting non-averaged SVO-
SVO here. As shown in Table 5, the scores consis-
tently improved with a few exceptions. The scores
? = 0.51 for the NN task and ? = 0.48 for the
VO task are the best results to date. However, the
score ? = 0.34 for the SVO task did not improve
by increasing the dimensionality. This means that
simply increasing the dimensionality of the word
vectors does not necessarily lead to better results
for complex phrases.
5.5 Effects of Bag-of-Words Contexts
Lastly, we trained the PAS-CLBLM without the
bag-of-words contexts described in Section 3.4
and used 50-dimensional word vectors. As can be
seen in Table 6, large score improvements were
observed only for the VO and SVO tasks by in-
cluding the bag-of-words contexts and the non-
linearity function. It is likely that the results de-
pend on how the bag-of-words contexts are con-
structed. However, we leave this line of analysis
as future work. Both adjective-noun and noun-
model BoW AN NN VO SVO
Add
l
w/ 0.52 0.44 0.35 0.24
w/o 0.48 0.46 0.38 0.23
Add
nl
w/ 0.52 0.46 0.45 0.24
w/o 0.50 0.47 0.41 0.15
Wadd
l
w/ 0.48 0.39 0.34 0.21
w/o 0.47 0.39 0.38 0.21
Wadd
nl
w/ 0.48 0.40 0.39 0.34
w/o 0.52 0.42 0.33 0.26
Table 6: Scores of the PAS-CLBLM with and
without BoW contexts.
noun phrase are noun phrases, and (subject-) verb-
object phrases can be regarded as complete sen-
tences. Therefore, different kinds of context infor-
mation might be required for both groups.
6 Qualitative Analysis on Composed
Vectors
An open question that remains is to what ex-
tent composition affects the representations pro-
duced by our PAS-CLBLM model. To evalu-
ate this we assigned a vector for each composed
representation. For example, the adjective-noun
dependency ?heavy rain? would be assigned an
independent vector. We added the most fre-
quent 100,000 adjective-noun, noun-noun, and
(subject-) verb-object tuples to the vocabulary and
the resulting vocabulary contained 400,000 to-
kens (100,000+3?100,000). A similar method
for treating frequent neighboring words as single
words was introduced by Mikolov et al. (2013b).
However, some dependencies, such as (subject-)
verb-object phrases, are not always captured when
considering only neighboring words.
Table 7 (No composition) shows some examples
of predicate-argument dependencies with their
closest neighbors in the vector space according
to the cosine similarity. The table shows that the
learned vectors of multiple words capture seman-
tic similarity. For example, the vector of ?heavy
rain? is close to the vectors of words which ex-
press the phenomena heavily raining. The vector
of ?new york? captures the concept of a major city.
The vectors of (subject-) verb-object dependencies
also capture the semantic similarity, which is the
main difference to previous approaches, such as
that of Mikolov et al. (2013b), which only consider
neighboring words. These results suggest that the
PAS-CLBLM can learn meaningful composition
1552
Query No composition Composition
rain rain
(AN) thunderstorm sunshine
heavy downpour storm
rain blizzard drizzle
much rain chill
general manager executive
(AN) vice president director
chief executive director representative
executive project manager officer
managing director administrator
second war war
(NN) plane crash world
world riot race
war last war holocaust
great war warfare
oslo york
(NN) paris toronto
new birmingham paris
york moscow edinburgh
madrid glasgow
make order make
(VO) carry survey allow
make pay tax demand
payment pay produce
impose tax bring
achieve objective solve
(VO) bridge gap alleviate
solve improve quality overcome
problem deliver information resolve
encourage development circumvent
hold meeting take
(SVO) event take place get
meeting end season win
take discussion take place put
place do work gain
Table 7: Nearest neighbor vectors for multiple
words. POS-tags are not shown for simplicity.
category predicate arg1 arg2
adj arg1 2.38 6.55 -
noun arg1 3.37 5.60 -
verb arg12 6.78 2.57 2.18
Table 8: L2-norms of the 50-dimensional weight
vectors of the composition function Wadd
nl
.
functions since the composition layers receive the
same error signal via backpropagation.
We then trained the PAS-CLBLM using Wadd
nl
to learn composition functions. Table 7 (Compo-
sition) shows the nearest neighbor words for each
composed vector, and as we can see, the learned
composition function emphasizes the head words
and captures some sort of semantic similarity. We
then inspected the L2-norms of the weight vectors
of the composition function. As shown in Table 8,
head words are strongly emphasized. Emphasiz-
ing head words is helpful in representing com-
posed meanings, but in the case of verbs it may
not always be sufficient. This can be observed in
Table 3 and Table 4, which demonstrates that verb-
related tasks are more difficult than noun-phrase
tasks.
While No composition captures the seman-
tic similarity well using independent parameters,
there is the issue of data sparseness. As the size of
the vocabulary increases, the number of tuples of
word dependencies increases rapidly. In this ex-
periment, we used only the 300,000 most frequent
tuples. In contrast to this, the learned composi-
tion functions can capture similar information us-
ing only word vectors and a small set of predicate
categories.
7 Conclusion and Future Work
We have presented a compositional log-bilinear
language model using predicate-argument struc-
tures that incorporates both bag-of-words and
dependency-based contexts. In our experiments
the learned composed vectors achieve state-of-the-
art scores for the task of measuring the semantic
similarity between short phrases. For the subject-
verb-object phrase task, the result is achieved
without any pre-trained word vectors using a cor-
pus an order of magnitude smaller than that of the
previous state of the art. For future work, we will
investigate how our models and the resulting vec-
tor representations can be helpful for other unsu-
pervised and/or supervised tasks.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This work was
supported by JSPS KAKENHI Grant Number
13F03041.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
1553
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159.
Katrin Erk and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the Tenth In-
ternational World Wide Web Conference.
John Rupert Firth. 1957. A synopsis of linguistic
theory 1930-55. In Studies in Linguistic Analysis,
pages 1?32.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A Structured Distributional Seman-
tic Model : Integrating Structure with Semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 20?
29.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394?
1404.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1372?1376.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 894?904.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior Disambiguation of Word Tensors for Con-
structing Sentence Vectors. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1590?1601.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating Disambiguation from
Composition in Distributional Semantics. In Pro-
ceedings of 17th Conference on Natural Language
Learning (CoNLL), pages 114?123.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
302?308.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at the International Conference on Learning
Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1439.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems 26, pages 2265?2273.
Andriy Mnih and Yee Whye Teh. 2012. A fast
and simple algorithm for training neural probabilis-
tic language models. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 1751?1758.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
1554
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 90?99.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201?1211.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with Compo-
sitional Vector Grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
455?465.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1631?1642.
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2014. Grounded Compositional
Semantics for Finding and Describing Images with
Sentences. Transactions of the Association for Com-
putational Linguistics, 2:207?218.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 948?
957.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and Learning Se-
mantic Co-Compositionality through Prototype Pro-
jections and Neural Networks. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 130?140.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1142?1151.
1555
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858?1869,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Modeling Joint Entity and Relation Extraction with Table Representation
Makoto Miwa and Yutaka Sasaki
Toyota Technological Institute
2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511, Japan
{makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jp
Abstract
This paper proposes a history-based struc-
tured learning approach that jointly ex-
tracts entities and relations in a sentence.
We introduce a novel simple and flexible
table representation of entities and rela-
tions. We investigate several feature set-
tings, search orders, and learning meth-
ods with inexact search on the table. The
experimental results demonstrate that a
joint learning approach significantly out-
performs a pipeline approach by incorpo-
rating global features and by selecting ap-
propriate learning methods and search or-
ders.
1 Introduction
Extraction of entities and relations from texts has
been traditionally treated as a pipeline of two sep-
arate subtasks: entity recognition and relation ex-
traction. This separation makes the task easy to
deal with, but it ignores underlying dependencies
between and within subtasks. First, since entity
recognition is not affected by relation extraction,
errors in entity recognition are propagated to re-
lation extraction. Second, relation extraction is
often treated as a multi-class classification prob-
lem on pairs of entities, so dependencies between
pairs are ignored. Examples of these dependen-
cies are illustrated in Figure 1. For dependencies
between subtasks, a Live in relation requires PER
and LOC entities, and vice versa. For in-subtask
dependencies, the Live in relation between ?Mrs.
Tsutayama? and ?Japan? can be inferred from the
two other relations.
Figure 1 also shows that the task has a flexible
graph structure. This structure usually does not
cover all the words in a sentence differently from
other natural language processing (NLP) tasks
such as part-of-speech (POS) tagging and depen-
Mrs. Tsuruyama is from Kumamoto Prefecture in Japan .PER LOC LOC
Live_in Located_in
Live_in
Figure 1: An entity and relation example (Roth
and Yih, 2004). Person (PER) and location (LOC)
entities are connected by Live in and Located in
relations.
dency parsing, so local constraints are considered
to be more important in the task.
Joint learning approaches (Yang and Cardie,
2013; Singh et al., 2013) incorporate these de-
pendencies and local constraints in their models;
however most approaches are time-consuming and
employ complex structures consisting of multi-
ple models. Li and Ji (2014) recently proposed
a history-based structured learning approach that
is simpler and more computationally efficient than
other approaches. While this approach is promis-
ing, it still has a complexity in search and restricts
the search order partly due to its semi-Markov rep-
resentation, and thus the potential of the history-
based learning is not fully investigated.
In this paper, we introduce an entity and relation
table to address the difficulty in representing the
task. We propose a joint extraction of entities and
relations using a history-based structured learning
on the table. This table representation simplifies
the task into a table-filling problem, and makes
the task flexible enough to incorporate several en-
hancements that have not been addressed in the
previous history-based approach, such as search
orders in decoding, global features from relations
to entities, and several learning methods with in-
exact search.
2 Method
In this section, we first introduce an entity and re-
lation table that is utilized to represent the whole
1858
entity and relation structures in a sentence. We
then overview our model on the table. We finally
explain the decoding, learning, search order, and
features in our model.
2.1 Entity and relation table
The task we address in this work is the extraction
of entities and their relations from a sentence. En-
tities are typed and may span multiple words. Re-
lations are typed and directed.
We use words to represent entities and relations.
We assume entities do not overlap. We employ
a BILOU (Begin, Inside, Last, Outside, Unit) en-
coding scheme that has been shown to outperform
the traditional BIO scheme (Ratinov and Roth,
2009), and we will show that this scheme induces
several label dependencies between words and be-
tween words and relations in ?2.3.2. A label is
assigned to a word according to the relative posi-
tion to its corresponding entity and the type of the
entity. Relations are represented with their types
and directions. ? denotes a non-relation pair, and
? and? denote left-to-right and right-to-left re-
lations, respectively. Relations are defined on not
entities but words, since entities are not always
given when relations are extracted. Relations on
entities are mapped to relations on the last words
of the entities.
Based on this representation, we propose an en-
tity and relation table that jointly represents en-
tities and relations in a sentence. Figure 2 illus-
trates an entity and relation table corresponding to
an example in Figure 1. We use only the lower tri-
angular part because the table is symmetric, so the
number of cells is n(n + 1)/2 when there are n
words in a sentence. With this entity and relation
table representation, the joint extraction problem
can be mapped to a table-filling problem in that
labels are assigned to cells in the table.
2.2 Model
We tackle the table-filling problem by a history-
based structured learning approach that assigns la-
bels to cells one by one. This is mostly the same as
the traditional history-based model (Collins, 2002)
except for the table representation.
Let x be an input table, Y(x) be all possible
assignments to the table, and s(x,y) be a scoring
function that assesses the assignment of y ? Y(x)
to x. With these definitions, we define our model
to predict the most probable assignment as fol-
lows:
y
?
= argmax
y?Y(x)
s(x,y) (1)
This scoring function is a decomposable function,
and each decomposed function assesses the as-
signment of a label to a cell in the table.
s(x,y) =
|x|
?
i=1
s(x,y, 1, i) (2)
Here, i represents an index of a cell in the table,
which will be explained in ?2.3.1. The decom-
posed function s(x,y, 1, i) corresponds to the i-th
cell. The decomposed function is represented as a
linear model, i.e., an inner product of features and
their corresponding weights.
s(x,y, 1, i) = w?f(x,y, 1, i) (3)
The scoring function are further divided into two
functions as follows:
s(x,y, 1, i) = s
local
(x,y, i) + s
global
(x,y, 1, i)
(4)
Here, s
local
(x,y, i) is a local scoring func-
tion that assesses the assignment to the i-th
cell without considering other assignments, and
s
global
(x,y, 1, i) is a global scoring function that
assesses the assignment in the context of 1st to
(i ? 1)-th assignments. This global scoring func-
tion represents the dependencies between entities,
between relations, and between entities and rela-
tions. Similarly, features f are divided into local
features f
local
and global features f
global
, and they
are defined on its target cell and surrounding con-
texts. The features will be explained in ?2.5. The
weights w can also be divided, but they are tuned
jointly in learning as shown in ?2.4.
2.3 Decoding
The scoring function s(x,y, 1, i) in Equation (2)
uses all the preceding assignments and does not
rely on the Markov assumption, so we cannot em-
ploy dynamic programming.
We instead employ a beam search to find the
best assignment with the highest score (Collins
and Roark, 2004). The beam search assigns la-
bels to cells one by one with keeping the top K
best assignments when moving from a cell to the
next cell, and it returns the best assignment when
labels are assigned to all the cells. The pseudo
code for decoding with the beam search is shown
in Figure 3.
1859
Mrs. Tsutayama is from Kumamoto Prefecture in Japan .
Mrs. B-PER
Tsutayama ? L-PER
is ? ? O
from ? ? ? O
Kumamoto ? ? ? ? B-LOC
Prefecture ? Live in? ? ? ? L-LOC
in ? ? ? ? ? ? O
Japan ? Live in? ? ? ? Located in? ? U-LOC
. ? ? ? ? ? ? ? ? ?
Figure 2: The entity and relation table for the example in Figure 1.
INPUT: x: input table with no assignment,
K: beam size
OUTPUT: best assignment y
?
for x
1: b? [x]
2: for i = 1 to |x| do
3: T ? ?
4: for k = 1 to |b| do
5: for a ?A(i, b[k]) do
6: T ? T ? append(a, b[k])
7: end for
8: end for
9: b? top K tables from T using the scoring
function in Equation (2)
10: end for
11: return b[0]
Figure 3: Decoding with the beam search. A(i, t)
returns possible assignments for i-th cell of a table
t, and append(a, t) returns a table t updated with
an assignment a.
We explain how to map the table to a sequence
(line 2 in Figure 3), and how to calculate possible
assignments (line 6 in Figure 3) in the following
subsections.
2.3.1 Table-to-sequence mapping
Cells in an input table are originally indexed in
two dimensions. To apply our model in ?2.2 to the
cells, we need to map the two-dimensional table
to a one-dimensional sequence. This is equivalent
to defining a search order in the table, so we will
use the terms ?mapping? and ?search order? inter-
changeably.
Since it is infeasible to try all possible map-
pings, we define six promising static mappings
(search orders) as shown in Figure 4. Note that the
?left? and ?right? directions in the captions cor-
respond to not word orders, but tables. We de-
1 3 6
A B C
A 1
B 2 3
C 4 5 6
A B C
52 4
(a) Up to
down, left to
right
1 2 4
A B C
A 1
B 3 2
C 6 5 4
A B C
53 6
(b) Up to
down, right
to left
4 2 1
A B C
A 4
B 5 2
C 6 3 1
A B C
35 6
(c) Right to
left, up to
down
6 3 1
A B C
A 6
B 5 3
C 4 2 1
A B C
25 4
(d) Right to
left, down to
up
1 2 3
A B C
A 1
B 4 2
C 6 5 3
A B C
54 6
(e) Close-
first, left to
right
3 2 1
A B C
A 3
B 5 2
C 6 4 1
A B C
45 6
(f) Close-
first, right to
left
Figure 4: Static search orders.
fine two mappings (Figures 4(a) and 4(b)) with the
highest priority on the ?up to down? order, which
checks a sentence forwardly (from the beginning
of a sentence). Similarly, we also define two map-
pings (Figures 4(c) and 4(d)) with the highest pri-
ority on the ?right to left? order, which check a
sentence backwardly (from the end of a sentence).
From another point of view, entities are detected
before relations in Figures 4(b) and 4(c) whereas
the order in a sentence is prioritized in Figures 4(a)
1860
Condition Possible labels on w
i
Relation(s) on w
i?1
B-*, O, U-*
Relation(s) on w
i
L-*, U-*
Table 1: Label dependencies from relations to en-
tities. * indicates any type.
Label on w
i
Relations from/to w
i
B-*, I-*, O ?
L-*, U-* *
Label on w
i+1
Relations from/to w
i
I-*, L-* ?
B-*, U-*, O *
Table 2: Label dependencies from entities to rela-
tions.
and 4(d). We further define two close-first map-
pings (Figures 4(e) and 4(f)) since entities are
easier to find than relations and close relations are
easier to find than distant relations.
We also investigate dynamic mappings (search
orders) with an easy-first policy (Goldberg and El-
hadad, 2010). Dynamic mappings are different
from the static mappings above, since we reorder
the cells before each decoding
1
. We evaluate the
cells using the local scoring function, and assign
indices to the cells so that the cells with higher
scores have higher priorities. In addition to this
na??ve easy-first policy, we define two other dy-
namic mappings that restricts the reordering by
combining the easy-first policy with one of the fol-
lowing two policies: entity-first (all entities are de-
tected before relations) and close-first (closer cells
are detected before distant cells) policies.
2.3.2 Label dependencies
To avoid illegal assignments to a table, we have
to restrict the possible assignments to the cells ac-
cording to the preceding assignments. This restric-
tion can also reduce the computational costs.
We consider all the dependencies between cells
to allow the assignments of labels to the cells in
an arbitrary order. Our representation of entities
and relations in ?2.1 induces the dependencies be-
tween entities and between entities and relations.
Tables 1-3 summarize these dependencies on the i-
th wordw
i
in a sentence. We can further utilize de-
pendencies between entity types and relation types
if some entity types are involved in a limited num-
1
It is also possible to reorder the cells during decoding,
but it greatly increases the computational costs.
Label on w
i?2
Possible labels on w
i
B-TYPE B-*, I-TYPE, L-TYPE, O, U-*
I-TYPE B-*, I-TYPE, L-TYPE, O, U-*
L-TYPE B-*, I-*, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
Label on w
i?1
Possible labels on w
i
B-TYPE I-TYPE, L-TYPE
I-TYPE I-TYPE, L-TYPE
L-TYPE B-*, O, U-*
O B-*, O, U-*
U-TYPE B-*, O, U-*
O/S B-*, O, U-*
Label on w
i+1
Possible labels on w
i
B-TYPE L-*, O, U-*
I-TYPE B-TYPE, I-TYPE
L-TYPE B-TYPE, I-TYPE
O L-*, O, U-*
U-TYPE L-*, O, U-*
O/S L-*, O, U-*
Label on w
i+2
Possible labels on w
i
B-TYPE B-*, I-*, L-*, O, U-*
I-TYPE B-TYPE, I-TYPE, L-*, O, U-*
L-TYPE B-TYPE, I-TYPE, L-*, O, U-*
O B-*, I-*, L-*, O, U-*
U-TYPE B-*, I-*, L-*, O, U-*
O/S B-*, I-*, L-*, O, U-*
Table 3: Label dependencies between entities.
TYPE represents an entity type, and O/S means
the word is outside of a sentence.
ber of relation types or vice versa. We note that
the dependencies between entity types and rela-
tion types include not only words participating in
relations but also their surrounding words. For ex-
ample, the label on w
i?1
can restrict the types of
relations involving w
i
. We employ these type de-
pendencies in the evaluation, but we omit these de-
pendencies here since these dependencies are de-
pendent on the tasks.
2.4 Learning
The goal of learning is to minimize errors between
predicted assignments y
?
and gold assignments
y
gold
by tuning the weights w in the scoring func-
tion in Equation 3. We employ a margin-based
structured learning approach to tune the weights
w. The pseudo code is shown in Figure 5. This ap-
proach enhances the traditional structured percep-
1861
INPUT: training sets D = {(x
i
,y
i
)}
N
i=1
,
T: iterations
OUTPUT: weights w
1: w? 0
2: for t = 1 to T do
3: for x,y ? D do
4: y
?
? best assignment for x using decod-
ing in Figure 3 with s
?
in Equation (5)
5: if y
?
?= y
gold
then
6: m? argmax
i
{s
?
(x,y
gold
, 1, i)?
s
?
(x,y
?
, 1, i)}
7: w? update(w, f(x,y
gold
, 1,m),
f(x,y
?
, 1,m))
8: end if
9: end for
10: end for
11: return w
Figure 5: Margin-based structured learn-
ing approach with a max-violation update.
update(w, f(x,y
gold
, 1,m), f(x,y
?
, 1,m))
depends on employed learning methods.
tron (Collins, 2002) in the following ways. Firstly,
we incorporate a margin ? into the scoring func-
tion as follows so that wrong assignments with
small differences from gold assignments are pe-
nalized (lines 4 and 6 in Figure 5) (Freund and
Schapire, 1999).
s
?
(x,y) = s(x,y) + ?(y,y
gold
) (5)
Similarly to the scoring function s, the margin ?
is defined as a decomposable function using 0-1
loss as follows:
?(y,y
gold
) =
|x|
?
i=1
?(y
i
, y
gold
i
),
?(y
i
, y
gold
i
) =
{
0 if y
i
= y
gold
i
1 otherwise
(6)
Secondly, we update the weights w based on a
max-violation update rule following Huang et al.
(2012) (lines 6-7 in Figure 5). Finally, we em-
ploy not only perceptron (Collins, 2002) but also
AROW (Mejer and Crammer, 2010; Crammer et
al., 2013), AdaGrad (Duchi et al., 2011), and
DCD-SSVM (Chang and Yih, 2013) for learning
methods (line 7 in Figure 5.) We employ parame-
ter averaging except for DCD-SSVM. AROW and
AdaGrad store additional information for covari-
ance and feature counts respectively, and DCD-
SSVM keeps a working set and performs addi-
tional updates in each iteration. Due to space limi-
tations, we refer to the papers for the details of the
learning methods.
2.5 Features
Here, we explain the local features f
local
and the
global features f
global
introduced in ?2.2.
2.5.1 Local features
Our focus is not to exploit useful local features
for entities and relations, so we incorporate several
features from existing work to realize a reasonable
baseline. Table 4 summarizes the local features.
Local features for entities (or words) are similar
to the features used by Florian et al. (2003), but
some features are generalized and extended, and
gazetteer features are excluded. For relations (or
pairs of words), we employ and extend features in
Miwa et al. (2009).
2.5.2 Global features
We design global features to represent dependen-
cies among entities and relations. Table 5 summa-
rizes the global features
2
. These global features
are activated when all the information is available
during decoding.
We incorporate label dependency features like
traditional sequential labeling for entities. Al-
though our model can include other non-local fea-
tures between entities (Ratinov and Roth, 2009),
we do not include them expecting that global fea-
tures on entities and relations can cover them. We
design three types of global features for relations.
These features are activated when all the partic-
ipating relations are not ? (non-relations). Fea-
tures except for the ?Crossing? category are simi-
lar to global relation features in Li and Ji (2014).
We further incorporate global features for both en-
tities and relations. These features are activated
when the relation label is not ?. These features
can act as a bridge between entities and relations.
3 Evaluation
In this section, we first introduce the corpus and
evaluation metrics that we employed for evalua-
tion. We then show the performance on the train-
ing data set with explaining the parameters used
2
We tried other ?Entity+Relation? features to represent a
relation and both its participating entities, but they slightly
degraded the performance in our preliminary experiments.
1862
Target Category Features
Word Lexical Character n-grams (n=2,3,4)
(Entity) Attributes by parsers (base form, POS)
Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, all-
digits-or-puncts)
Contextual Word n-grams (n=1,2,3) within a context window size of 2
Word pair Entity Entity lexical features of each word
(Relation) Contextual Word n-grams (n=1,2,3) within a context window size of 2
Shortest
path
Walk features (word-dependency-word or dependency-word-
dependency) on the shortest paths in parsers? outputs
n-grams (n=2,3) of words and dependencies on the paths
n-grams (n=1,2) of token modifier-modifiee pairs on the paths
The length of the paths
Table 4: Local features.
Target Category Details
Entity Bigram Bigrams of labels
Combinations of two labels and their corresponding POS tags
Combinations of two labels and their corresponding words
Trigram Trigrams of labels
Combinations of three labels and each of their corresponding POS tags
Combinations of three labels and each of their corresponding words
Entity Combinations of a label and its corresponding entity
Relation Entity-
sharing
Combinations of two relation labels that share a word (i.e., relations in
same columns or same rows in a table)
Combinations of two relation labels and the shared word
Relation shortest path features between non-shared words, augmented by
a combination of relation labels and the shared word
Cyclic Combinations of three relation labels that make a cycle
Crossing Combinations of two relation labels that cross each other
Entity + Entity- Relation label and the label of its participating entity
Relation relation Relation label and the label and word of its participating entity
Table 5: Global features.
for the test set evaluation, and show the perfor-
mance on the test data set.
3.1 Evaluation settings
We used an entity and relation recognition corpus
by Roth and Yih (2004)
3
. The corpus defines four
named entity types Location, Organization, Per-
son, andOther and five relation typesKill, Live In,
Located In, OrgBased In and Work For.
All the entities were words in the original cor-
pus because all the spaces in entities were replaced
with slashes. Previous systems (Roth and Yih,
2007; Kate and Mooney, 2010) used these word
3
conll04.corp at http://cogcomp.cs.illinois.
edu/page/resource_view/43
boundaries as they were, treated the boundaries as
given, and focused the entity classification prob-
lem alone. Differently from such systems, we re-
covered these spaces by replacing these slashes
with spaces to evaluate the entity boundary detec-
tion performance on this corpus. Due to this re-
placement and the inclusion of the boundary de-
tection problem, our task is more challenging than
the original task, and our results are not compara-
ble with those by the previous systems.
The corpus contains 1,441 sentences that con-
tain at least one relation. Instead of 5-fold cross
validation on the entire corpus by the previous sys-
tems, we split the data set into training (1,153 sen-
tences) and blind test (288 sentences) data sets and
1863
developed the system on the training data set. We
tuned the hyper-parameters using a 5-fold cross
validation on the training data set, and evaluated
the performance on the test set.
We prepared a pipeline approach as a baseline.
We first trained an entity recognition model using
the local and global features, and then trained a
relation extraction model using the local features
and global features without global ?Relation? fea-
tures in Table 5. We did not employ the global
?Relation? features in this baseline since it is com-
mon to treat relation extraction as a multi-class
classification problem.
We extracted features using the results from two
syntactic parsers Enju (Miyao and Tsujii, 2008)
and LRDEP (Sagae and Tsujii, 2007). We em-
ployed feature hashing (Weinberger et al., 2009)
and limited the feature space to 2
24
. The num-
bers of features greatly varied for categories and
targets. They also caused biased predictions that
prefer entities to relations in our preliminary ex-
periments. We thus chose to re-scale the features
as follows. We normalized local features for each
feature category and then for each target. We also
normalized global features for each feature cate-
gory, but we did not normalize them for each target
since normalization was impossible during decod-
ing. We instead scaled the global features, and the
scaling factor was tuned by using the same 5-fold
cross validation above.
We used the F1 score on relations with entities
as our primary evaluation measure and used it for
tuning parameters. In this measure, a relation with
two entities is considered correct when the offsets
and types of the entities and the type of the relation
are all correct. We also evaluated the F1 scores for
entities and relations individually on the test data
set by checking their corresponding cells. An en-
tity is correct when the offset and type are correct,
and a relation is correct when the type is correct
and the last words of two entities are correct.
3.2 Performance on Training Data Set
It is infeasible to investigate all the combinations
of the parameters, so we greedily searched for a
default parameter setting by using the evaluated
results on the training data set. The default pa-
rameter setting was the best setting except for the
beam size. We show learning curves on the train-
ing data set in Figure 6 when we varied each pa-
rameter from the default parameter setting. We
employed 5-fold cross validation. The default pa-
rameter setting used DCD-SSVM as the learning
method, entity-first, easy-first as the search order,
local and global features, and 8 as the beam size.
This section discusses how these parameters affect
the performance on the training data set and ex-
plains how the parameter setting was selected for
the test set.
Figure 6(a) compares the learning methods in-
troduced in ?2.4. DCD-SSVM and AdaGrad per-
formed slightly better than perceptron, which has
often been employed in history-based structured
learning. AROW did not show comparable per-
formance to the others. We ran 100 iterations to
find the number of iterations that saturates learn-
ing curves. The large number of iterations took
time and the performance of DCD-SSVM almost
converged after 30 iterations, so we employed 50
iterations for other evaluation on the training data
set. AdaGrad got its highest performance more
quickly than other learning methods and AROW
converged slower than other methods, so we em-
ployed 10 for AdaGrad, 90 for AROW, and 50 it-
erations for other settings on the test data set.
The performance was improved by widening
the beam as in Figure 6(b), but the improvement
was gradually diminished as the beam size in-
creased. Since the wider beam requires more train-
ing and test time, we chose 8 for the beam size.
Figure 6(c) shows the effects of joint learning
as well as features explained in ?2.5. We show the
performance of the pipeline approach (Pipeline)
introduced in ?3.1, and the performance with lo-
cal features alone (Local), local and global fea-
tures without global ?Relation? features in Table 5
(Local+global (?relation)) and all local and global
features (Local+global). We note that Pipeline
shows the learning curve of relation extraction in
the pipeline approach. Features in ?Local+global
(?relation)? are the same as the features in the
pipeline approach, and the result shows that the
joint learning approach performed slightly better
than the pipeline approach. The incorporation
of global ?Entity? and ?Entity+Relation? features
improved the performance as is common with the
existing pipeline approaches, and relation-related
features further improved the performance.
Static search orders in ?2.3.1 also affected the
performance as shown in Figure 6(d), although
search orders are not investigated in the joint en-
tity and relation extraction. Surprisingly, the gap
1864
(a) Learning methods (b) Beam sizes
(c) Features and pipeline / joint approaches (d) Static search orders
(e) Dynamic search orders
Figure 6: Learning curves of entity and relation extraction on the training data set using 5-fold cross
validation.
between the performances with the best order and
worst order was about 0.04 in an F1 score, which
is statistically significant, and the performance can
be worse than the pipeline approach in Figure 6(c).
This means improvement by joint learning can be
easily cancelled out if we do not carefully con-
sider search order. It is also surprising that the sec-
ond worst order (Figure 4(b)) is the most intuitive
?left-to-right? order, which is closest to the order
in Li and Ji (2014) among the six search orders.
Figure 6(e) shows the performance with dy-
namic search orders. Unfortunately, the easy-first
policy did not work well on this entity and relation
task, but, with the two enhancements, dynamic or-
ders performed as well as the best static order in
Figure 6(d). This shows that entities should be de-
1865
tected earlier than relations on this data set.
3.3 Performance on Test Data Set
Table 6 summarizes the performance on the test
data set. We employed the default parameter set-
ting explained in ?3.2, and compared parameters
by changing the parameters shown in the first col-
umn. We performed a statistical test using the ap-
proximate randomization method (Noreen, 1989)
on our primary measure (?Entity+Relation?). The
results are almost consistent with the results on the
training data set with a few exceptions.
Differently from the results on the training data
set, AdaGrad and AROW performed significantly
worse than perceptron and DCD-SSVM and they
performed slightly worse than the pipeline ap-
proach. This result shows that DCD-SSVM per-
forms well with inexact search and the selection of
learning methods can significantly affect the entity
and relation extraction performance.
The joint learning approach showed a signifi-
cant improvement over the pipeline approach with
relation-related global features, although the joint
learning approach alone did not show a signif-
icant improvement over the pipeline approach.
Unfortunately, no joint learning approach outper-
formed the pipeline approach in entity recognition.
This may be partly because hyper-parameters were
tuned to the primary measure. The results on the
pipeline approach also indicate that the better per-
formance on entity recognition does not necessar-
ily improve the relation extraction performance.
Search orders also affected the performance,
and the worst order (right to left, down to up) and
best order (close-first, left to right) were signifi-
cantly different. The performance of the worst or-
der was worse than that of the pipeline approach,
although the difference was not significant. These
results show that it is necessary to carefully select
the search order for the joint entity and relation
extraction task.
3.4 Comparison with Other Systems
To compare our model with the other sys-
tems (Roth and Yih, 2007; Kate and Mooney,
2010), we evaluated the performance of our model
when the entity boundaries were given. Differ-
ently from our setting in ?3.1, we used the gold
entity boundaries encoded in the BILOU scheme
and assigned entity labels to the boundaries. We
performed 5-fold cross validation on the data set
following Roth and Yih (2007) although the split
was different from theirs since their splits were not
available. We employed the default parameter set-
ting in ?3.2 for this comparison.
Table 7 shows the evaluation results. Although
we cannot directly compare the results, our model
performs better than the other models. Compared
to Table 6, Table 7 also shows that the inclusion
of entity boundary detection degrades the perfor-
mance about 0.09 in F-score.
4 Related Work
Search order in structured learning has been stud-
ied in several NLP tasks. Left-to-right and right-
to-left orderings have been often investigated in
sequential labeling tasks (Kudo and Matsumoto,
2001). Easy-first policy was firstly introduced
by Goldberg and Elhadad (2010) for dependency
parsing, and it was successfully employed in sev-
eral tasks, such as joint POS tagging and depen-
dency parsing (Ma et al., 2012) and co-reference
resolution (Stoyanov and Eisner, 2012). Search
order, however, has not been focused in relation
extraction tasks.
Named entity recognition (Florian et al., 2003;
Nadeau and Sekine, 2007) and relation extrac-
tion (Zelenko et al., 2003; Miwa et al., 2009)
have often been treated as separate tasks, but
there are some previous studies that treat enti-
ties and relations jointly in learning. Most stud-
ies built joint learning models upon individual
models for subtasks, such as Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2007; Yang and
Cardie, 2013) and Card-Pyramid Parsing (Kate
and Mooney, 2010). Our approach does not re-
quire such individual models, and it also can de-
tect entity boundaries that these approaches except
for Yang and Cardie (2013) did not treat. Other
studies (Yu and Lam, 2010; Singh et al., 2013)
built global probabilistic graphical models. They
need to compute distributions over variables, but
our approach does not. Li and Ji (2014) proposed
an approach to jointly find entities and relations.
They incorporated a semi-Markov chain in repre-
senting entities and they defined two actions dur-
ing search, but our approach does not employ such
representation and actions, and thus it is more sim-
ple and flexible to investigate search orders.
5 Conclusions
In this paper, we proposed a history-based struc-
tured learning approach that jointly detects enti-
1866
Parameter Entity Relation Entity+Relation
Perceptron 0.809 / 0.809 / 0.809 0.760 / 0.547 / 0.636 0.731 / 0.527 / 0.612
?
AdaGrad 0.801 / 0.790 / 0.795 0.732 / 0.486 / 0.584 0.716 / 0.476 / 0.572
AROW 0.810 / 0.802 / 0.806 0.797 / 0.468 / 0.590 0.758 / 0.445 / 0.561
DCD-SSVM
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
Pipeline 0.823 / 0.814 / 0.818 0.672 / 0.542 / 0.600 0.647 / 0.522 / 0.577
Local 0.819 / 0.812 / 0.815 0.844 / 0.399 / 0.542 0.812 / 0.384 / 0.522
Local + global (?relation) 0.809 / 0.799 / 0.804 0.784 / 0.481 / 0.596 0.747 / 0.458 / 0.568
Local + global
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
(a) Up to down, left to right 0.824 / 0.801 / 0.813 0.821 / 0.433 / 0.567 0.787 / 0.415 / 0.543
(b) Up to down, right to left 0.828 / 0.808 / 0.818 0.850 / 0.461 / 0.597 0.822 / 0.445 / 0.578
(c) Right to left, up to down 0.823 / 0.799 / 0.811 0.826 / 0.448 / 0.581 0.789 / 0.427 / 0.554
(d) Right to left, down to up 0.811 / 0.784 / 0.797 0.774 / 0.445 / 0.565 0.739 / 0.425 / 0.540
(e) Close-first, left to right 0.821 / 0.806 / 0.813 0.807 / 0.522 / 0.634 0.780 / 0.504 / 0.612
?
(f) Close-first, right to left 0.817 / 0.801 / 0.809 0.832 / 0.491 / 0.618 0.797 / 0.471 / 0.592
Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540
Entity-first, easy-first
?
0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610
?
Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581
Table 6: Performance of entity and relation extraction on the test data set (precision / recall / F1 score).
The
?
denotes the default parameter setting in ?3.2 and
?
represents a significant improvement over the
underlined ?Pipeline? baseline (p<0.05). Labels (a)-(f) correspond to those in Figure 4.
Kate and Mooney (2010) Roth and Yih (2007) Entity-first, easy-first
Person 0.921 / 0.942 / 0.932 0.891 / 0.895 / 0.890 0.931 / 0.948 / 0.939
Location 0.908 / 0.942 / 0.924 0.897 / 0.887 / 0.891 0.922 / 0.939 / 0.930
Organization 0.905 / 0.887 / 0.895 0.895 / 0.720 / 0.792 0.903 / 0.896 / 0.899
All entities - - 0.924 / 0.924 / 0.924
Located In 0.675 / 0.567 / 0.583 0.539 / 0.557 / 0.513 0.821 / 0.549 / 0.654
Work For 0.735 / 0.683 / 0.707 0.720 / 0.423 / 0.531 0.886 / 0.642 / 0.743
OrgBased In 0.662 / 0.641 / 0.647 0.798 / 0.416 / 0.543 0.768 / 0.572 / 0.654
Live In 0.664 / 0.601 / 0.629 0.591 / 0.490 / 0.530 0.819 / 0.532 / 0.644
Kill 0.916 / 0.641 / 0.752 0.775 / 0.815 / 0.790 0.933 / 0.797 / 0.858
All relations - - 0.837 / 0.599 / 0.698
Table 7: Results of entity classification and relation extraction on the data set using the 5-fold cross
validation (precision / recall / F1 score).
ties and relations. We introduced a novel entity
and relation table that jointly represents entities
and relations, and showed how the entity and re-
lation extraction task can be mapped to a simple
table-filling problem. We also investigated search
orders and learning methods that have been fixed
in previous research. Experimental results showed
that the joint learning approach outperforms the
pipeline approach and the appropriate selection of
learning methods and search orders is crucial to
produce a high performance on this task.
As future work, we plan to apply this approach
to other relation extraction tasks and explore more
suitable search orders for relation extraction tasks.
We also plan to investigate the potential of this ta-
ble representation in other tasks such as semantic
parsing and co-reference resolution.
Acknowledgments
We thank Yoshimasa Tsuruoka and Yusuke Miyao
for valuable discussions, and the anonymous re-
viewers for their insightful comments. This work
was supported by the TTI Start-Up Research
Support Program and the JSPS Grant-in-Aid for
Young Scientists (B) [grant number 25730129].
1867
References
Ming-Wei Chang and Wen-Tau Yih. 2013. Dual coor-
dinate descent algorithms for efficient large margin
structured prediction. Transactions of the Associa-
tion for Computational Linguistics, 1:207?218.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine learning, 91(2):155?187.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans andMiles Osborne, editors, Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, pages 168?171.
Yoav Freund and Robert E Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine learning, 37(3):277?296.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750, Los Angeles, California,
June. Association for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151, Montr?eal, Canada, June. Association for
Computational Linguistics.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning, pages
203?212, Uppsala, Sweden, July. Association for
Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage Technologies, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402?412, Baltimore, Maryland, June.
Association for Computational Linguistics.
Ji Ma, Tong Xiao, Jingbo Zhu, and Feiliang Ren.
2012. Easy-first Chinese POS tagging and depen-
dency parsing. In Proceedings of COLING 2012,
pages 1731?1746, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 971?981, Cambridge, MA, October. As-
sociation for Computational Linguistics.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 121?130, Singapore, August. Association
for Computational Linguistics.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80, March.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, ed-
itors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learning
(CoNLL-2004), pages 1?8, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Roth and Wen-Tau Yih, 2007. Global Inference
for Entity and Relation Identification via a Linear
Programming Formulation. MIT Press.
1868
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In Pro-
ceedings of the 2013 workshop on Automated knowl-
edge base construction, pages 1?6. ACM.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519?2534, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning, ICML ?09, pages 1113?
1120, New York, NY, USA. ACM.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Coling 2010:
Posters, pages 1399?1407, Beijing, China, August.
Coling 2010 Organizing Committee.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083?1106.
1869
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 88?92, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UTTime: Temporal Relation Classification using Deep Syntactic Features
Natsuda Laokulrat
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
natsuda@logos.t.u-tokyo.ac.jp
Makoto Miwa
The University of Manchester
131 Princess Street,
Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Yoshimasa Tsuruoka
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
tsuruoka@logos.t.u-tokyo.ac.jp
Takashi Chikayama
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
chikayama@logos.t.u-tokyo.ac.jp
Abstract
In this paper, we present a system, UTTime,
which we submitted to TempEval-3 for Task
C: Annotating temporal relations. The sys-
tem uses logistic regression classifiers and ex-
ploits features extracted from a deep syntactic
parser, including paths between event words in
phrase structure trees and their path lengths,
and paths between event words in predicate-
argument structures and their subgraphs. UT-
Time achieved an F1 score of 34.9 based
on the graphed-based evaluation for Task C
(ranked 2nd) and 56.45 for Task C-relation-
only (ranked 1st) in the TempEval-3 evalua-
tion.
1 Introduction
Temporal annotation is the task of identifying tem-
poral relationships between pairs of temporal enti-
ties, namely temporal expressions and events, within
a piece of text. The temporal relationships are im-
portant to support other NLP applications such as
textual entailment, document summarization, and
question answering. The temporal annotation task
consists of several subtasks, including temporal ex-
pression extraction, event extraction, and temporal
link identification and relation classification.
In TempEval-3, there are three subtasks of the
temporal annotation process offered, i.e., Task A:
Temporal expression extraction and normalization,
Task B: Event extraction, and Task C: Annotating
temporal relations. This paper presents a system
to handle Task C. Based on the annotated data pro-
vided, this subtask requires identifying pairs of tem-
poral entities and classifying the pairs into one of the
14 relation types according to TimeML (Pustejovsky
et al, 2005), i.e., BEFORE, AFTER, IMMEDIATELY BE-
FORE, IMMEDIATELY AFTER, INCLUDES, IS INCLUDED,
DURING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY.
The motivation behind our work is to utilize syn-
tactic and semantic relationships between a pair of
temporal entities in the temporal relation classifica-
tion task, since we believe that these relationships
convey the temporal relation. In addition to general
features, which are easily extracted from sentences
(e.g., part of speech tags, lemmas, synnonyms), we
use features extracted using a deep syntactic parser.
The features from the deep parser can be divided into
two groups: features from phrase structure trees and
features from predicate-argument structures. These
features are only applicable in the case that the tem-
poral entities appear in the same sentence, so we use
only the general features for inter-sentence relations.
Predicate-argument structure expresses semantic
relations between words. This information can be
extracted from a deep syntactic parser. Features
from predicate-argument structures can capture im-
portant temporal information (e.g., prepositions of
time) from sentences effectively.
The remaining part of this paper is organized as
follows. We explain our approach in detail in Sec-
tion 2 and then show the evaluation and results in
Section 3. Finally, we conclude with directions for
future work in Section 4.
2 Approach
Our system, UTTime, is based on a supervised ma-
chine learning approach. UTTime performs two
tasks; TLINK identification and classification. In
88
other words, UTTime identifies pairs of temporal en-
tities and classifies these pairs into temporal relation
types.
2.1 TLINK identification
A pair of temporal entities that have a temporal rela-
tion is called a TLINK. The system first determines
which pairs of temporal entities are linked by using
a ruled-based approach as a baseline approach.
All the TempEval-3?s possible pairs of temporal
entities are extracted by a set of simple rules; pairs
of temporal entities that satisfy one of the following
rules are considered as TLINKs.
? Event and document creation time
? Events in the same sentence
? Event and temporal expression in the same sen-
tence
? Events in consecutive sentences
2.2 TLINK classification
Each TLINK is classified into a temporal relation
type. We use a machine learning approach for the
temporal relation classification. Two L2-regularized
logistic regression classifiers, LIBLINEAR (Fan et
al., 2008), are used; one for event-event TLINKs,
and another one for event-time TLINKs. In addition
to general features at different linguistic levels, fea-
tures extracted by a deep syntactic parser are used.
The general features we employed are:
? Event and timex attributes
All attributes associated with events (class,
tense, aspect, modality, and polarity) and
temporal expressions (type, value, func-
tionInDocument, and temporalFunction) are
used. For event-event TLINKs, we also use
tense/class/aspect match, tense/class/aspect bi-
grams as features (Chambers et al, 2007).
? Morphosyntactic information
Words, part of speech tags, lemmas within a
window before/after event words are extracted
using Stanford coreNLP (Stanford NLP Group,
2012).
? Lexical semantic information
Figure 1: Phrase structure tree
Synonyms of event word tokens from WordNet
lexical database (Fellbaum, 1998) are used as
features.
? Event-Event information
For event-event TLINKs, we use
same sentence feature to differentiate pairs
of events in the same sentence from pairs of
events from different sentences (Chambers et
al., 2007).
In the case that temporal entities of a particu-
lar TLINK are in the same sentence, we extract
two new types of sentence-level semantic informa-
tion from a deep syntactic parser. We use the Enju
parser (Miyao and Tsujii, 2008). It analyzes syn-
tactic/semantic structures of sentences and provides
phrase structures and predicate-argument structures.
The features we extract from the deep parser are
? Paths between event words in the phrase struc-
ture tree, and up(?)/down(?) lengths of paths.
We use 3-grams of paths as features instead of
full paths since these are too sparse. An ex-
ample is shown in Figure 1. In this case, the
path between the event words, estimates and
worth, is VBZ?, VX?, VP?, VP?, VP, PP?, PX?, IN?.
The 3-grams of the path are, therefore, {VBZ?-
VX?-VP?, VX?-VP?-VP?, VP?-VP?-VP, VP?-VP-PP?,
VP-PP?-PX?, PP?-PX-?-IN?}. The up/down path
89
Figure 2: Predicate argument structure
lengths are 4 (VBZ?, VX?, VP?, VP?) and 3 (PP?,
PX?, IN?) respectively.
? Paths between event words in predicate-
argument structure, and their subgraphs.
For the previous example, we can express the
relations in predicate-argument structure repre-
sentation as
? verb arg12: estimate (she, properties)
? prep arg12: worth (estimate, dollars)
In this case, the path between the event words,
estimates and worth, is?prep arg12:arg1. That
is, the type of the predicate worth is prep arg12
and it has estimate as the first argument (arg1).
The path from estimate to worth is in reverse
direction (?).
The next example sentence, John saw mary be-
fore the meeting, gives an idea of a more com-
plex predicate-argument structure as shown
in Figure 2. The path between the event
words, saw and meeting is ?prep arg12:arg1,
prep arg12:arg2.
We use (v, e, v) and (e, v, e) tuples of the
edges and vertices on the path as features.
For example, in Figure 2, the (v,e,v) tuples
are (see, ?prep arg12:arg1, before) and (be-
fore, prep arg12:arg2, meeting). In the same
way, the (e,v,e) tuple is (?prep arg12:arg1,
before, prep arg12:arg2). The subgraphs
of (v, e, v) and (e, v, e) tuples are also
used, including (see, ?prep arg12:arg1,
*), (*, ?prep arg12:arg1, before), (*,
?prep arg12:arg1, *), (*, prep arg12:arg2,
meeting), (before, prep arg12:arg2, *), (*,
prep arg12:arg2, *), (*, before, prep arg12:arg2),
(?prep arg12:arg1, before, *), (*, before, *).
From the above example, the features from pred-
icate argument structure can properly capture the
preposition before. It can also capture a preposi-
tion from a compound sentence such as John met
Mary before he went back home. The path between
the event words met and went are (?conj arg12:arg1,
conj arg12:arg2) and the (v, e, v) and (e, v, e)
tuples are (met, ?conj arg12:arg1, before), (before,
conj arg12:arg2, went), and (?prep arg12:arg1, be-
fore, prep arg12:arg2).
2.3 Hybrid approach
The rule-based approach described in Section 2.1
produces many unreasonable and excessive links.
We thus use a machine learning approach to filter
out those unreasonable links by training the model
in Section 2.2 with an additional relation type, UN-
KNOWN, for links that satisfy the rules in Section
2.1 but do not appear in the training data.
In this way, for Task C, we first extract all the links
that satisfy the rules and classify the relation types of
those links. After classifying temporal relations, we
remove the links that are classified as UNKNOWN.
3 Evaluation
The scores are calculated by the graph-based eval-
uation metric proposed by UzZaman and Allen
(2011). We trained the models with TimeBank and
AQUAINT corpora. We also trained our models on
the training set with inverse relations. The perfor-
mance analysis is based on 10-fold cross validation
on the development data.
3.1 Task C
In Task C, a system has to identify appropriate tem-
poral links and to classify each link into one tempo-
ral relation type. For Task C evaluation, we compare
the results of the models trained with and without the
features from the deep parser. The results are shown
in Table 1. The rule-based approach gives a very low
precision.
3.2 Task C-relation-only
Task C-relation-only provides a system with all the
appropriate temporal links and only needs the sys-
tem to classify the relation types. Since our goal is to
exploit the features from the deep parser, in Task C-
relation-only, we measured the contribution of those
features to temporal relation classification in Table
2.
90
Features F1 P R
gen. (rule) 22.51 14.32 52.58
gen. + ph. + pas. (rule) 22.61 14.30 54.01
gen. + ph. + pas. (hyb.) 33.52 36.23 31.19
gen. + ph. + pas. (hyb. + inv.) 39.53 37.56 41.70
Table 1: Result of Task C. (rule: rule-based approach,
hyb.: hybrid approach, gen.: general features, ph.:phrase
structure tree features, pas.:predicate-argument structure
features, and inv.: Inverse relations are used for training.)
Features F1 P R
gen. 64.42 64.59 64.25
gen. + ph. 65.24 65.42 65.06
gen. + pas. 66.40 66.55 66.25
gen. + ph. + pas. 66.39 66.55 66.23
gen. + ph. + pas. (inv.) 65.30 65.39 65.20
Table 2: Result of Task C-relation-only. (gen.:
general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
The predicate-argument-structure features con-
tributed to the improvement more than those of
phrase structures in both precision and recall. The
reason is probably that the features from phrase
structures that we used did not imply a temporal re-
lation of events in the sentence. For instance, the
sentence ?John saw Mary before the meeting? gives ex-
actly the same path as of the sentence ?John sawMary
after the meeting?.
3.3 Results on test data
Tables 3 and 4 show the results on the test data,
which were manually annotated and provided by the
TempEval-3 organizer. We also show the scores of
the other systems in the tables. For the evaluation
on the test data, we used the models trained with
general features, phrase structure tree features, and
predicate-argument structure features.
UTTime-5 ranked 2nd best in Task C. Interest-
ingly, training the models with inverse relations im-
proved the system only when using the hybrid ap-
proach. This means that the inverse relations did not
improve the temporal classification but helped the
system filter out unreasonable links (UNKNOWN)
in the hybrid approach. As expected, the ruled-based
approach got a very high recall score at the expense
of precision. UTTime-1, although it achieved the F1
Approach F1 P R
rule (UTTime-1) 24.65 15.18 65.64
rule + inv (UTTime-3) 24.28 15.1 61.99
hyb. (UTTime-4) 28.81 37.41 23.43
hyb. + inv. (UTTime-5) 34.9 35.94 33.92
cleartk 36.26 37.32 35.25
NavyTime 31.06 35.48 27.62
JU-CSE 26.41 21.04 35.47
KUL-KULTaskC 24.83 23.35 26.52
Table 3: Result of Tack C on test data. (rule: rule-based
approach, hyb.: hybrid approach, and inv.: Inverse rela-
tions are used for training.)
Approach F1 P R
gen. + ph. + pas. (UTTime-1) 56.45 55.58 57.35
gen. + ph. + pas. (UTTime-2) 54.26 53.2 55.36
gen. + ph. + pas. (inv.) (UTTime-3) 54.7 53.85 55.58
NavyTime 46.83 46.59 47.07
JU-CSE 34.77 35.07 34.48
Table 4: Result of Task C-relation-only on test data.
(gen.: general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
score of only 24.65, got the highest recall among all
the systems.
For Task C-relation-only, we achieved the highest
F1 score, precision, and recall. UTTime-2 basically
had the same models as that of UTTime-1, but we
put different weights for each relation type. The re-
sults show that using the weights did not improve
the score in graph-based evaluation.
4 Conclusion
The system, UTTime, identifying temporal links and
classifying temporal relation, is proposed. The links
were identified based on the rule-based approach
and then some links were filtered out by a classi-
fier. The filtering helped improve the system consid-
erably. For the relation classification task, the fea-
tures extracted from phrase structures and predicate-
argument structures were proposed, and the features
improved the classification in precision, recall, and
F-score.
In future work, we hope to improve the classifica-
tion performance by constructing timegraphs (Miller
and Schubert, 1999), so that the system can use in-
formation from neighbor TLINKs as features.
91
References
James Pustejovsky, Robert Ingria, Roser Saur??, Jose?
Castan?o, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, Inderjeet Mani 2005. The spec-
ification language TimeML. The Language of Time: A
reader, pages 545?557
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification.
Nathanael Chambers, Shan Wang and Dan Jurafsky.
2007. Classifying Temporal Relations between
Events. In ACL 2007, pages 173?176.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics. 34(1). pages 35?80, MIT Press.
Naushad UzZaman and James F. Allen. 2011. Temporal
Evaluation. In ACL 2011, pages 351?356.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time Revisited. In Computational Intelligence 6,
pages 108?118.
92
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 19?27,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Event Extraction for Post-Translational Modifications
Tomoko Ohta? Sampo Pyysalo? Makoto Miwa? Jin-Dong Kim? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,smp,mmiwa,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We consider the task of automatically
extracting post-translational modification
events from biomedical scientific publica-
tions. Building on the success of event
extraction for phosphorylation events in
the BioNLP?09 shared task, we extend the
event annotation approach to four major
new post-transitional modification event
types. We present a new targeted corpus of
157 PubMed abstracts annotated for over
1000 proteins and 400 post-translational
modification events identifying the modi-
fied proteins and sites. Experiments with
a state-of-the-art event extraction system
show that the events can be extracted with
52% precision and 36% recall (42% F-
score), suggesting remaining challenges
in the extraction of the events. The an-
notated corpus is freely available in the
BioNLP?09 shared task format at the GE-
NIA project homepage.1
1 Introduction
Post-translational-modifications (PTM), amino
acid modifications of proteins after translation, are
one of the posterior processes of protein biosyn-
thesis for many proteins, and they are critical
for determining protein function such as its ac-
tivity state, localization, turnover and interac-
tions with other biomolecules (Mann and Jensen,
2003). Since PTM alter the properties of a pro-
tein by attaching one or more biochemical func-
tional groups to amino acids, understanding of
the mechanism and effects of PTM are a major
goal in the recent molecular biology, biomedicine
and pharmacology fields. In particular, epige-
netic (?outside conventional genetics?) regulation
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
of gene expression has a crucial role in these fields
and PTM-like modifications of biomolecules are a
burning issue. For instance, tissue specific or con-
text dependent expression of many proteins is now
known to be controlled by specific PTM of his-
tone proteins, such as Methylation and Acetylation
(Jaenisch and Bird, 2003). This Methylation and
Acetylation of specific amino acid residues in his-
tone proteins are strongly implicated in unwinding
the nucleosomes and exposing genes to transcrip-
tion, replication and DNA repairing machinery.
The recent BioNLP?09 Shared Task on Event
Extraction (Kim et al, 2009a) (below, BioNLP
shared task) represented the first community-wide
step toward the extraction of fine-grained event
representations of information from biomolecular
domain publications (Ananiadou et al, 2010). The
nine event types targeted in the task included one
PTM type, Phosphorylation, whose extraction in-
volved identifying the modified protein and, when
stated, the specific phosphorylated site. The re-
sults of the shared task showed this PTM event to
be single most reliably extracted event type in the
data, with the best-performing system for the event
type achieving 91% precision and 76% recall
(83% F-score) in the extraction of phosphorylation
events (Buyko et al, 2009). The results suggest
both that the event representation is well applica-
ble to PTM and that current extraction methods are
capable of reliable PTM extraction. Most of the
proposed state-of-the-art methods for event extrac-
tion are further largely machine-learning based.
This suggest that the coverage of many existing
methods could be straightforwardly extended to
new event types and domains by extending the
scope of available PTM annotations and retrain-
ing the methods on newly annotated data. In this
study, we take such an annotation-based approach
to extend the extraction capabilities of state of the
art event extraction methods for PTM.
19
Term Count
Phosphorylation 172875 50.90%
Methylation 49780 14.66%
Glycosylation 36407 10.72%
Hydroxylation 20141 5.93%
Acetylation 18726 5.51%
Esterification 7836 2.31%
Ubiquitination 6747 1.99%
ADP-ribosylation 5259 1.55%
Biotinylation 4369 1.29%
Sulfation 3722 1.10%
. . .
TOTAL 339646 100%
Table 1: PTM mentions in PubMed. The number
of citations returned by the PubMed search engine
for each PTM term shown together with the frac-
tion of the total returned for all searches. Searches
were performed with the terms as shown, allow-
ing MeSH term expansion and other optimizations
provided by the Entrez search.
2 Corpus Annotation
We next discuss the selection of the annotated
PTM types and source texts and present the rep-
resentation and criteria used in annotation.
2.1 Event Types
A central challenge in the automatic extraction
of PTMs following the relatively data-intensive
BioNLP shared task model is the sheer number
of different modifications: the number of known
PTM types is as high as 300 and constantly grow-
ing (Witze et al, 2007). Clearly, the creation of
a manually annotated resource with even mod-
est coverage of statements of each of the types
would be a formidable undertaking. We next
present an analysis of PTM statement occurrences
in PubMed as the first step toward resolving this
challenge.
We estimated the frequency of mentions of
prominent PTM types by combining MeSH
ontology2 PTM terms with terms occurring
in the post-translational protein
modification branch of the Gene Ontology
(The Gene Ontology Consortium, 2000). After
removing variants (e.g. polyamination for amina-
tion or dephosphorylation for phosphorylation)
and two cases judged likely to occur frequently
2http://www.nlm.nih.gov/mesh/meshhome.
html
in non-PTM contexts (hydration and oxidation),
we searched PubMed for the remaining 31 PTM
types. The results for the most frequent types
are shown in Table 1. We find a power-law
- like distribution with phosphorylation alone
accounting for over 50% of the total, and the top
6 types together for over 90%. By contrast, the
bottom ten types together represent less than a
percent of total occurrences.
This result implies that fair coverage of individ-
ual PTM event mentions can be achieved without
considering even dozens of different PTM event
types, let alne hundreds. Thus, as a step toward
extending the coverage of event extraction systems
for PTM, we chose to focus limited resources on
annotating a small selection of types so that a num-
ber of annotations sufficient for supervised learn-
ing and stable evaluation can be provided. To
maximize the utility of the created annotation, the
types were selected based on their frequency of oc-
currence.
2.2 Text Selection
Biomedical domain corpora are frequently anno-
tated from selections of texts chosen as a sample
of publications in a particular subdomain of inter-
est. While several areas in present-day molecu-
lar biology are likely to provide ample source data
for PTM statements, a sample of articles from any
subdomain is unlikely to provide a well-balanced
distribution of event types: for example, the most
frequent PTM event type annotated in the GENIA
event corpus occurs more than 10 times as often
as the second most frequent (Kim et al, 2008).
Further, avoiding explicit subdomain restrictions
is not alone sufficient to assure a balanced distri-
bution of event types: in the BioInfer corpus, for
which sentences were selected on the basis of their
containing mentions of protein pairs known to in-
teract, the most frequent PTM type is again anno-
tated nearly four times as often as the second most
frequent (Pyysalo et al, 2007).
To focus annotation efforts on texts relevant to
PTM and to guarantee that the annotation results
in relatively balanced numbers of PTM events of
each targeted type, we decided to annotate a tar-
geted set of source texts instead of a random sam-
ple of texts for a particular subdomain. This type
of targeted annotation involves a risk of introduc-
ing bias: a badly performed selection could pro-
duce a corpus that is not representative of the
20
PTM type AB FT
Acetylation 103 128
Glycosylation 226 336
Methylation 72 69
Phosphorylation 186 76
Hydroxylation 71 133
Table 2: Number of abstracts (AB) and full-text ar-
ticles (FT) tagged in PIR as containing PTM state-
ments.
statements expressing PTMs in text and thus poor
material for either meaningful evaluation or for
training methods with good generalization perfor-
mance.3 To avoid such bias, we decided to base
our selection of the source texts on an indepen-
dently annotated PTM resource with biological (as
opposed to textual) criteria for inclusion. Owing
in part to the recent interest in PTMs, there are
currently a wealth of resources providing different
levels of annotation for PTMs.
Here, we have chosen to base initial annotation
on corpora provided by the Protein Information
Resource4 (PIR) (Wu et al, 2003). These corpora
contain annotation for spans with evidence for five
different PTM types (Table 2), corresponding to
the five PTMs found above to occur in PubMed
with the highest frequency. A key feature setting
this resource apart from others we are aware of is
that it provides text-bound annotations identifying
the statement by which a PTM record was made in
the context of the full publication abstracts. While
this annotation is less specific and detailed than
the full BioNLP shared task markup, it could both
serve as an initial seed for annotation and assure
that the annotation agrees with relevant database
curation criteria. The PIR corpora have also been
applied in previous PTM extraction studies (e.g.
(Hu et al, 2005; Narayanaswamy et al, 2005)).
We judged that the annotated Phosphorylation
events in the BioNLP shared task data provide
sufficient coverage for the extraction of this PTM
type, and chose to focus on producing annota-
tion for the four other PTM types in the PIR data.
As the high extraction performance for phospho-
rylation events in the BioNLP shared task was
3One could easily gather PTM-rich texts by performing
protein name tagging and searching for known patterns such
as ?[PROTEIN] methylates [PROTEIN]?, but a corpus cre-
ated in this way would not necessarily provide significant
novelty over the original search patterns.
4http://pir.georgetown.edu
Protein Site PTM Count
collagen lysine Hydroxylate 44
myelin arginine Methylate 17
M protein N-terminal Glycosylate 2
EF-Tu lysine Methylate 1
Actobindin NH2 terminus Acetylate 0
Table 3: Example queried triples and match counts
from Medie.
achieved with annotated training data containing
215 PTM events, in view of the available resources
we set as an initial goal the annotation of 100
events of each of the four PTM types. To assure
that the annotated resource can be made publicly
available, we chose to use only the part of the PIR
annotations that identified sections of PubMed ab-
stracts, excluding full-text references and non-
PubMed abstracts. Together with the elimination
of duplicates and entries judged to fall outside of
the event annotation criteria (see Section 2.4), this
reduced the number of source texts below our tar-
get, necessitating a further selection strategy.
For further annotation, we aimed to select ab-
stracts that contain specific PTM statements iden-
tifying both the name of a modified protein and the
modified site. As for the initial selection, we fur-
ther wished to avoid limiting the search by search-
ing for any specific PTM expressions. To imple-
ment this selection, we used the Medie system5
(Ohta et al, 2006; Miyao et al, 2006) to search
PubMed for sentences where a specific protein and
a known modified site were found together in a
sentence occurring in an abstract annotated with a
specific MeSH term. The (protein name, modified
site, MeSH term) triples were extracted from PIR
records, substituting the appropriate MeSH term
for each PTM type. Some examples with the num-
ber of matching documents are shown in Table 3.
As most queries returned either no documents or a
small number of hits, we gave priority to responses
to queries that returned a small number of docu-
ments to avoid biasing the corpus toward proteins
whose modifications are frequently discussed.
We note that while the PIR annotations typically
identified focused text spans considerably shorter
than a single sentence and sentence-level search
was used in the Medie-based search to increase the
likelihood of identifying relevant statements, after
selection all annotation was performed to full ab-
stracts.
5http://www-tsujii.is.s.u-tokyo.ac.jp/
medie/
21
Event type Count
Protein modification 38
Phosphorylation 546
Dephosphorylation 28
Acetylation 7
Deacetylation 1
Ubiquitination 6
Deubiquitination 0
Table 4: GENIA PTM-related event types and
number of events in the GENIA event corpus.
Type names are simplified: the full form of e.g.
the Phosphorylation type in the GENIA event on-
tology is Protein amino acid phosphorylation.
Event type Arguments Count
Protein modification Theme 31
Phosphorylation Theme 261
Phosphorylation Theme, Site 230
Phosphorylation Site 20
Phosphorylation Theme, Cause 14
Dephosphorylation Theme 16
Table 5: GENIA PTM-related event arguments.
Only argument combinations appearing more than
10 times in the corpus shown.
2.3 Representation
The employed event representation can capture
the association of varying numbers of participants
in different roles. To apply an event extraction
approach to PTM, we must first define the tar-
geted representation, specifying the event types,
the mandatory and optional arguments, and the ar-
gument types ? the roles that the participants play
in the events. In the following, we discuss alterna-
tives and present the representation applied in this
work.
The GENIA Event ontology, applied in the
annotation of the GENIA Event corpus (Kim
et al, 2008) that served as the basis of the
BioNLP shared task data, defines a general Pro-
tein modification event type and six more specific
modification subtypes, shown in Table 4. While
the existing Acetylation type could thus be applied
together with the generic Protein modification
type to capture all the annotated PTMs, we be-
lieve that identification of the specific PTM type
is not only important to users of extracted PTM
events but also a relatively modest additional bur-
den for automatic extraction, owing to the unam-
biguous nature of typical expressions used to state
Figure 1: Alternative representations for PTM
statements including a catalyst in GENIA Event
corpus. PTM events can be annotated with a di-
rect Cause argument (top, PMID 9374467) or us-
ing an additional Regulation event (middle, PMID
10074432). The latter annotation can be applied
also in cases where there is no expression directly
?triggering? the secondary event (bottom, PMID
7613138).
PTMs in text. We thus chose to introduce three
additional specific modification types, Glycosyla-
tion, Hydroxylation and Methylation for use in the
annotation.
The GENIA Event corpus annotation allows
PTM events to take Theme, Site and Cause argu-
ments specifying the event participants, where the
Theme identifies the entity undergoing the mod-
ification, Site the specific region being modified,
and Cause an entity or event leading to the modi-
fication. Table 5 shows frequent argument combi-
nations appearing in the annotated data. We note
that while Theme is specified in the great majority
of events and Site in almost half, Cause is anno-
tated for less than 5% of the events. However, the
relative sparsity of Cause arguments in modifica-
tion events does not imply that e.g. catalysts of the
events are stated only very rarely, but instead re-
flects also the use of an alternative representation
for capturing such statements without a Cause ar-
gument for the PTM event. The GENIA event an-
notation specifies a Regulation event (with Posi-
tive regulation and Negative regulation subtypes),
used to annotate not only regulation in the biolog-
ical sense but also statements of general causality
between events: Regulation events are used gen-
erally to connect entities or events stated to other
events that they are stated to cause. Thus, PTM
22
events with a stated cause (e.g. a catalyst) can be
alternatively represented with a Cause argument
on the PTM event or using a separate Regulation
event (Figure 1). The interpretation of these event
structures is identical, and from an annotation per-
spective there are advantages to both. However,
for the purpose of automatic extraction it is impor-
tant to establish a consistent representation, and
thus only one should be used.
In this work, we follow the latter representation,
disallowing Cause arguments for annotated PTM
events and applying separate Regulation events
to capture e.g. catalyst associations. This choice
has the benefits of providing an uniform repre-
sentation for catalysis and inhibition (one involv-
ing a Positive regulation and the other a Nega-
tive regulation event), reducing the sparseness of
specific event structures in the data, and matching
the representation chosen in the BioNLP shared
task, thus maintaining compatibility with exist-
ing event extraction methods. Finally, we note
that while we initially expected that glycosylation
statements might frequently identify specific at-
tached side chains, necessitating the introduction
of an additional argument type to accurately cap-
ture all the stated information regarding Glycosy-
lation events, the data contained too few examples
for either training material or to justify the mod-
ification of the event model. We adopt the con-
straints applied in the BioNLP shared task regard-
ing the entity types allowed as specific arguments.
Thus, the representation we apply here annotated
PTM events with specific types, taking as Theme
argument a gene/gene product type entity and as
Site argument a physical (non-event) entity that
does not need to be assigned a specific type.
2.4 Annotation criteria
To create PTM annotation compatible with the
event extraction systems introduced for the
BioNLP shared task, we created annotation fol-
lowing the GENIA Event corpus annotation cri-
teria (Kim et al, 2008), as adapted for the shared
task. The criteria specify that annotation should be
applied to statements that involve the occurrence
of a change in the state of an entity ? even if stated
as having occurred in the past, or only hypotheti-
cally ? but not in cases merely discussing the state
or properties of entities, even if these can serve as
the basis for inference that a specific change has
occurred. We found that many of the spans an-
notated in PIR as evidence for PTM did not ful-
fill the criteria for event annotation. The most fre-
quent class consisted of cases where the only evi-
dence for a PTM was in the form of a sequence of
residues, for example
Characterization [. . . ] gave the follow-
ing sequence, Gly-Cys-Hyp-D-Trp-Glu-
Pro-Trp-Cys-NH2 where Hyp = 4-trans-
hydroxyproline. (PMID 8910408)
Here, the occurrence of hydroxyproline in the se-
quence implies that the protein has been hydrox-
ylated, but as the hydroxylation event is only im-
plied by the protein state, no event is annotated.
Candidates drawn from PIR but not fulfilling
the criteria were excluded from annotation. While
this implies that the general class of event extrac-
tion approaches considered here will not recover
all statements providing evidence of PTM to bi-
ologists (per the PIR criteria), several factors mit-
igate this limitation of their utility. First, while
PTMs implied by sequence only are relatively fre-
quent in PIR, its selection criteria give emphasis
to publications initially reporting the existence of a
PTM, and further publications discussing the PTM
are not expected to state it as sequence only. Thus,
it should be possible to extract the correspond-
ing PTMs from later sources. Similarly, one of
the promises of event extraction approaches is the
potential to extract associations of multiple enti-
ties and extract causal chains connecting events
with others (e.g. E catalyzes the hydroxylation of
P, leading to . . . ), and the data indicates that the
sequence-only statements typically provide little
information on the biological context of the modi-
fication beyond identifying the entity and site. As
such non-contextual PTM information is already
available in multiple databases, this class of state-
ments may not be of primary interest for event ex-
traction.
2.5 Annotation results
The new PTM annotation covers 157 PubMed
abstracts. Following the model of the BioNLP
shared task, all mentions of specific gene or gene
product names in the abstracts were annotated, ap-
plying the annotation criteria of (Ohta et al, 2009).
This new named entity annotation covers 1031
gene/gene product mentions, thus averaging more
than six mentions per annotated abstract. In to-
tal, 422 events of which 405 are of the novel PTM
23
Event type Count
Glycosylation 122
Hydroxylation 103
Methylation 90
Acetylation 90
Positive reg. 12
Phosphorylation 3
Protein modification 2
TOTAL 422
Table 6: Statistics of the introduced event annota-
tion.
Arguments Count
Theme, Site 363
Theme 36
Site 6
Table 7: Statistics for the arguments of the anno-
tated PTM events.
types were annotated, matching the initial annota-
tion target in number and giving a well-balanced
distribution of the specific PTM types (Table 6).
Reflecting the selection of the source texts, the
argument structures of the annotated PTM events
(Table 7) show a different distribution from those
annotated in the GENIA event corpus (Table 5):
whereas less than half of the GENIA event corpus
PTM events include a Site argument, almost 90%
of the PTM events in the new data include a Site.
PTM events identifying both the modified protein
and the specific modified site are expected to be
of more practical interest. However, we note that
the greater number of multi-argument events is ex-
pected to make the dataset more challenging as an
extraction target.
3 Evaluation
To estimate the capacity of the newly annotated
resource to support the extraction of the targeted
PTM events and the performance of current event
extraction methods at open-domain PTM extrac-
tion, we performed a set of experiments using an
event extraction method competitive with the state
of the art, as established in the BioNLP shared task
on event extraction (Kim et al, 2009a; Bjo?rne et
al., 2009).
3.1 Methods
We adopted the recently introduced event extrac-
tion system of Miwa et al (2010). The system
applies a pipeline architecture consisting of three
supervised classification-based modules: a trig-
ger detector, an event edge detector, and an event
detector. In evaluation on the BioNLP shared
task test data, the system extracted phosphory-
lation events at 75.7% precision and 85.2% re-
call (80.1% F-score) for Task 1, and 75.7% preci-
sion and 83.3% recall (79.3% F-score) for Task 2,
showing performance comparable to the best re-
sults reported in the literature for this event class
(Buyko et al, 2009). We assume three precondi-
tions for the PTM extraction: proteins are given,
all PTMs have Sites, and all arguments in a PTM
co-occur in sentence scope. The first of these is
per the BioNLP shared task setup, the second fixed
based the corpus statistics, and the third a property
intrinsic to the extraction method, which builds on
analysis of sentence structure.6 In the experiments
reported here, only the four novel PTM event types
with Sites in the corpus are regarded as a target for
the extraction.
The system extracted PTMs as follows: the
trigger detector detected the entities (triggers and
sites) of the PTMs, the event edge detector de-
tected the edges in the PTMs, and the event de-
tector detected the PTMs. The evaluation setting
was the same as the evaluation in (Miwa et al,
2010) except for the threshold. The thresholds in
the three modules were tuned with the develop-
ment data set.
Performance evaluation is performed using the
BioNLP shared task primary evaluation criteria,
termed the ?Approximate Span Matching? crite-
rion. This criterion relaxes the requirements of
strict matching in accepting extracted event trig-
gers and entities as correct if their span is inside
the region of the corresponding region in the gold
standard annotation.
3.2 Data Preparation
The corpus data was split into training and test sets
on the document level with a sampling strategy
that aimed to preserve a roughly 3:1 ratio of oc-
currences of each event type between training and
test data. The test data was held out during sys-
tem development and parameter selection and only
applied in a single final experiment. The event ex-
traction system was trained using the 112 abstracts
of the training set, further using 24 of the abstracts
6We note that in the BioNLP shared task data, all argu-
ments were contained within single sentences for 95% of
events.
24
Figure 2: Performance of PTM extraction on the
development data set.
Event type Prec Rec F
Acetylation 69.6% 36.7% 48.1%
Methylation 50.0% 34.2% 40.6%
Glycosylation 36.7% 42.5% 39.4%
Hydroxylation 57.1% 29.3% 38.7%
Overall 52.1% 35.7% 42.4%
Table 8: Event extraction results on the test set.
as a development test set.
3.3 Results
We first performed parameter selection, setting the
machine learning method parameter by estimating
performance on the development data set. Figure 2
shows the performance of PTM extraction on the
development data set with different values of pa-
rameter. The threshold value corresponding to the
best performance (0.3) was then applied for an ex-
periment on the held-out test set.
Performance on the test set was evaluated as
52% precision and 36% recall (42% F-score),
matching estimates on the development data. A
breakdown by event type (Table 8) shows that
Acetylation is most reliably extracted with extrac-
tion for the other three PTM types showing sim-
ilar F-scores despite some variance in the preci-
sion/recall balance. We note that while these re-
sults fall notably below the best result reported
for Phosphorylation events in the BioNLP shared
task, they are comparable to the best results re-
ported in the task for Regulation and Binding
events (Kim et al, 2009a), suggesting that the
dataset alows the extraction of the novel PTM
events with Theme and Site arguments at levels
comparable to multi-argument shared task events.
Figure 3: Learning curve of PTM extraction on the
development data set.
Further, a learning curve (Figure 3) plotted on
the development data suggests roughly linearly
increasing performance over most of the curve.
While the increase appears to be leveling off to
an extent when using all of the available data, the
learning curve indicates that performance can be
further improved by increasing the size of the an-
notated dataset.
4 Discussion
Post-translational modifications have been a fo-
cus of interest in the biomedical text mining com-
munity, and a number of resources and systems
targeting PTM have been proposed. The GE-
NIES and GeneWays systems (Friedman et al,
2001; Rzhetsky et al, 2004) targeted PTM events
such as phosphorylation and dephosphorylation
under the more general createbond and breakbond
types. Hu et al (2005) introduce the RLIMS-P
rule-based system for mining the substrates and
sites for phosphorylation, which is extended with
the capacity to extract intra-clausal statements by
Narayanaswamy et al (2005). Saric et al (2006)
present an extension of their rule-based STRING-
IE system for extracting regulatory networks to
capture phosphorylation and dephosphorylation
events. Lee et al (2008) present E3Miner, a tool
for automatically extracting information related to
ubiquitination, and Kim et al (2009b) present a
preliminary study adapting the E3Miner approach
to the mining of acetylation events.
It should be noted that while studies target-
ing single specific PTM types report better re-
sults than found in the initial evaluation presented
here (in many cases dramatically so), different
25
extraction targets and evaluation criteria compli-
cate direct comparison. Perhaps more importantly,
our aim here is to extend the capabilities of gen-
eral event extraction systems targeting multiple
types of structured events. Pursuing this broader
goal necessarily involves some compromise in the
ability to focus on the extraction of individual
event types, and it is expected that highly focused
systems will provide better performance than re-
trained general systems.
The approach to PTM extraction adopted here
relies extensively on the availability of annotated
resources, the creation of which requires consider-
able effort and expertise in understanding the tar-
get domain as well as the annotation methodology
and tools. The annotation created in this study,
performed largely on the basis of partial existing
annotations drawn from PIR data, involved an es-
timated three weeks of full-time effort from an ex-
perienced annotator. As experiments further in-
dicated that a larger corpus may be necessary for
reliable annotation, we can estimate that extending
the approach to sufficient coverage of each of hun-
dreds of PTM types without a partial initial anno-
tation would easily require several person-years of
annotation efforts. We thus see a clear need for the
development of unsupervised or semisupervised
methods for PTM extraction to extend the cover-
age of event extraction systems to the full scale of
different PTM types. Nevertheless, even if reliable
methods for PTM extraction that entirely avoid the
need for annotated training data become available,
a manually curated reference standard will still be
necessary for reliable estimation of their perfor-
mance. To efficiently support the development of
event extraction systems capable of capturing the
full variety of PTM events, it may be beneficial to
reverse the approach taken here: instead of anno-
tating hundreds of examples of a small number of
PTM types, annotate a small number of each of
hundreds of PTM types, thus providing both seed
data for semisupervised approaches as well as ref-
erence data for the evaluation of broad-coverage
PTM event extraction systems.
5 Conclusions and Future Work
We have presented an event extraction approach
to automatic PTM recognition, building on the
model introduced in the BioNLP shared task on
event extraction. By annotating a targeted cor-
pus for four prominent PTM types not considered
in the BioNLP shared task data, we have created
a resource that can be straightforwardly used to
extend the capability of event extraction systems
for PTM extraction. We estimated that while sys-
tems trained on the original shared task dataset
could not recognize more than 50% of PTM men-
tions due to their types, the introduced annotation
increases this theoretical upper bound to nearly
90%. An initial experiment on the newly intro-
duced dataset using a state-of-the-art method indi-
cated that straightforward adoption of the dataset
as training data to extend coverage of PTM events
without specific adaptations of the method is feasi-
ble, although the measured performance indicates
remaining challenges for reliable extraction. Fur-
ther, while the experiments were performed on a
dataset selected to avoid bias toward e.g. a partic-
ular subdomain or specific forms of event expres-
sions, it remains an open question how extraction
performance generalizes to biomedical literature
beyond the selected sample. As experiments in-
dicated clear remaining potential for the improve-
ment of extraction performance from more train-
ing data, the extension of the annotated dataset is
a natural direction for future work. We considered
also the possiblity of extending annotation to cover
small numbers of each of a large variety of PTM
types, which would place focus on the challenges
of event extraction with little or no training data
for specific event types.
The annotated corpus covering over 1000 gene
and gene product entities and over 400 events is
freely available in the widely adopted BioNLP
shared task format at the GENIA project home-
page.7
Acknowledgments
We would like to thank Goran Topic for automat-
ing Medie queries to identify target abstracts.
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japan-Slovenia Research Cooperative
Program (JSPS, Japan and MHEST, Slovenia).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
7http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
26
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 19?27, Boulder, Colorado, June. Association
for Computational Linguistics.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: A natural-language processing system for the
extraction of molecular pathways from journal arti-
cles. Bioinformatics, 17(Suppl. 1):S74?S82.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
Rudolf Jaenisch and Adrian Bird. 2003. Epigenetic
regulation of gene expression: how the genome in-
tegrates intrinsic and environmental signals. Nature
Genetics, 33:245?254.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009a. Overview
of bionlp?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Youngrae Kim, Hodong Lee, and Gwan-Su Yi. 2009b.
Literature mining for protein acetylation. In Pro-
ceedings of LBM?09.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein
ligases. Nucl. Acids Res., 36(suppl.2):W416?422.
Matthias Mann and Ole N. Jensen. 2003. Proteomic
analysis of post-translational modifications. Nature
Biotechnology, 21:255?261.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131?146, February.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya, and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proceedings of
COLING-ACL 2006, pages 1017?1024.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2005. Beyond the clause: extraction
of phosphorylation information from medline ab-
stracts. Bioinformatics, 21(suppl.1):i319?327.
Tomoko Ohta, Yusuke Miyao, Takashi Ninomiya,
Yoshimasa Tsuruoka, Akane Yakushiji, Katsuya
Masuda, Jumpei Takeuchi, Kazuhiro Yoshida, Ta-
dayoshi Hara, Jin-Dong Kim, Yuka Tateisi, and
Jun?ichi Tsujii. 2006. An Intelligent Search Engine
and GUI-based Efficient MEDLINE Search Tool
Based on Deep Syntactic Parsing. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, pages 17?20.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Jasmin Saric, Lars Juhl Jensen, Rossitza Ouzounova,
Isabel Rojas, and Peer Bork. 2006. Extraction
of regulatory gene/protein networks from Medline.
Bioinformatics, 22(6):645?650.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25:25?29.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
27
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 37?45,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Syntactic Parsers for Event Extraction
Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The extraction of bio-molecular events
from text is an important task for a number
of domain applications such as pathway
construction. Several syntactic parsers
have been used in Biomedical Natural
Language Processing (BioNLP) applica-
tions, and the BioNLP 2009 Shared Task
results suggest that incorporation of syn-
tactic analysis is important to achieving
state-of-the-art performance. Direct com-
parison of parsers is complicated by to dif-
ferences in the such as the division be-
tween phrase structure- and dependency-
based analyses and the variety of output
formats, structures and representations ap-
plied. In this paper, we present a task-
oriented comparison of five parsers, mea-
suring their contribution to bio-molecular
event extraction using a state-of-the-art
event extraction system. The results show
that the parsers with domain models using
dependency formats provide very similar
performance, and that an ensemble of dif-
ferent parsers in different formats can im-
prove the event extraction system.
1 Introduction
Bio-molecular events are useful for modeling and
understanding biological systems, and their au-
tomatic extraction from text is one of the key
tasks in Biomedical Natural Language Process-
ing (BioNLP). In the BioNLP 2009 Shared Task
on event extraction, participants constructed event
extraction systems using a variety of different
parsers, and the results indicated that the use of
a parser was correlated with high ranking in the
task (Kim et al, 2009). By contrast, the results
did not indicate a clear preference for a particular
parser, and there has so far been no direct compar-
ison of different parsers for event extraction.
While the outputs of parsers applying the same
out format can be compared using a gold standard
corpus, it is difficult to perform meaningful com-
parison of parsers applying different frameworks.
Additionally, it is still an open question to what ex-
tent high performance on a gold standard treebank
correlates with usefulness at practical tasks. Task-
based comparisons of parsers provide not only a
way to asses parsers across frameworks but also a
necessary measure of their practical applicability.
In this paper, five different parsers are com-
pared on the bio-molecular event extraction task
defined in the BioNLP 2009 Shared Task using a
state-of-the-art event extraction system. The data
sets share abstracts with GENIA treebank, and the
treebank is used as an evaluation standard. The
outputs of the parsers are converted into two de-
pendency formats with the help of existing conver-
sion methods, and the outputs are compared in the
two dependency formats. The evaluation results
show that different syntactic parsers with domain
models in the same dependency format achieve
closely similar performance, and that an ensemble
of different syntactic parsers in different formats
can improve the performance of an event extrac-
tion system.
2 Bio-molecular Event Extraction with
Several Syntactic Parsers
This paper focuses on the comparison of several
syntactic parsers on a bio-molecular event extrac-
tion task with a state-of-the-art event extraction
system. This section explains the details of the
comparison. Section 2.1 presents the event ex-
37
traction task setting, following that of the BioNLP
2009 Shared Task. Section 2.2 then summa-
rizes the five syntactic parsers and three formats
adopted for the comparison. Section 2.3 described
how the state-of-the-art event extraction system of
Miwa et al (2010) is modified and used for the
comparison.
2.1 Bio-molecular Event Extraction
The bio-molecular event extraction task consid-
ered in this study is that defined in the BioNLP
2009 Shared Task (Kim et al, 2009)1. The shared
task provided common and consistent task defi-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task consists of
three subtasks: core event extraction (Task 1),
augmenting events with secondary arguments
(Task 2), and the recognition of speculation and
negation of the events (Task 3) (Kim et al, 2009).
In this paper we consider Task 1 and Task 2. The
shared task defined nine event types, which can be
divided into five simple events (Gene expression,
Transcription, Protein catabolism, Phosphoryla-
tion, and Localization) that take one core argu-
ment, a multi-participant binding event (Bind-
ing), and three regulation events (Regulation, Pos-
itive regulation, and Negative regulation) that can
take other events as arguments.
In the two tasks considered, events are repre-
sented with a textual trigger, type, and arguments,
where the trigger is a span of text that states the
event in text. In Task 1 the event arguments that
need to be extracted are restricted to the core ar-
guments Theme and Cause, and secondary argu-
ments (locations and sites) need to be attached in
Task 2.
2.2 Parsers and Formats
Five parsers and three formats are adopted for
the evaluation. The parsers are GDep (Sagae and
Tsujii, 2007)2, the Bikel parser (Bikel) (Bikel,
2004)3, the Charniak-Johnson reranking parser,
using David McClosky?s self-trained biomedi-
cal parsing model (MC) (McClosky, 2009)4, the
C&C CCG parser, adapted to biomedical text
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
2http://www.cs.cmu.edu/?sagae/parser/
gdep/
3http://www.cis.upenn.edu/?dbikel/
software.html
4http://www.cs.brown.edu/?dmcc/
biomedical.html
	 
     Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 114?123,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Towards Exhaustive Protein Modification Event Extraction
Sampo Pyysalo? Tomoko Ohta? Makoto Miwa? Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?Microsoft Research Asia, Beijing, China
{smp,okap,mmiwa}@is.s.u-tokyo.ac.jp, jtsujii@microsoft.com
Abstract
Protein modifications, in particular post-
translational modifications, have a central role
in bringing about the full repertoire of pro-
tein functions, and the identification of spe-
cific protein modifications is important for
understanding biological systems. This task
presents a number of opportunities for the au-
tomatic support of manual curation efforts.
However, the sheer number of different types
of protein modifications is a daunting chal-
lenge for automatic extraction that has so far
not been met in full, with most studies focus-
ing on single modifications or a few prominent
ones. In this work, aim to meet this challenge:
we analyse protein modification types through
ontologies, databases, and literature and intro-
duce a corpus of 360 abstracts manually anno-
tated in the BioNLP Shared Task event repre-
sentation for over 4500 mentions of proteins
and 1000 statements of modification events of
nearly 40 different types. We argue that to-
gether with existing resources, this corpus pro-
vides sufficient coverage of modification types
to make effectively exhaustive extraction of
protein modifications from text feasible.
1 Introduction
In the decade following the sequencing of the hu-
man genome, the critical role of protein modifica-
tions in establishing the full set of protein functions
from forms transcribed from the fixed DNA is in-
creasingly appreciated, reflected in the rise of pro-
teomics as an extension and complement to genetics
in efforts to understand gene and protein functions.
The mapping of the space of modifications of spe-
cific proteins is a formidable undertaking: the num-
ber of known types of post-translational modifica-
tions (PTMs) is as high as 300 (Witze et al, 2007)
with new types identified regularly (e.g. (Brennan
and Barford, 2009)), and the number of specific
molecular variants of proteins in cells may be several
orders of magnitude larger than that encoded in the
genome; up to millions for humans (Walsh, 2006).
Automatic extraction of protein modifications from
the massive literature on the topic could contribute
significantly to addressing these challenges.
Biomedical information extraction (IE) has ad-
vanced substantially in recent years, shifting from
the detection of simple binary associations such
as protein-protein interactions toward resources and
methods for the extraction of multiple types of struc-
tured associations of varying numbers participants in
specific roles. These IE approaches are frequently
termed event extraction (Ananiadou et al, 2010).
While protein modifications have been considered
in numerous IE studies in the domain (e.g. (Fried-
man et al, 2001; Rzhetsky et al, 2004; Hu et al,
2005; Narayanaswamy et al, 2005; Saric et al,
2006; Yuan et al, 2006; Lee et al, 2008; Ohta et
al., 2010), event extraction efforts have brought in-
creased focus also on the extraction of protein modi-
fications: in the BioNLP Shared Task series that has
popularized event extraction, the 2009 shared task
(Kim et al, 2009) involved the extraction of nine
event types including one PTM, and in the 2011
follow-up event (Kim et al, 2011) the Epigenet-
ics and Post-translational modifications (EPI) task
(Ohta et al, 2011) targeted six PTM types, their re-
114
verse reactions, and statements regarding their catal-
ysis. The results of these tasks were promising, sug-
gesting that the single PTM type could be extracted
at over 80% F-score (Buyko et al, 2009) and the
core arguments of the larger set at nearly 70% F-
score (Bjo?rne and Salakoski, 2011).
The increasing availability of systems capable of
detailed IE for protein modifications, their high per-
formance also for multiple modifications types, and
demonstrations of the scalability of the technology
to the full scale of the literature (Bjo?rne et al, 2010)
are highly encouraging for automatic extraction of
protein modifications. However, previous efforts
have been restricted by the relatively narrow scope
of targeted modification types. In the present study,
we seek to address the task in full by identifying
all modifications of substantial biological signifi-
cance and creating an annotated resource with effec-
tively complete type-level coverage. We addition-
ally present preliminary extraction results to assess
the difficulty of exhaustive modification extraction.
2 Event representation
To be able to benefit from the substantial number of
existing resources and systems for event extraction,
we apply the event representation of the BioNLP
Shared Task (ST) for annotating protein modifica-
tions. Specifically, we directly extend the approach
of the BioNLP ST 2011 EPI task (Ohta et al, 2011).
In brief, in the applied representation, each event
is marked as being expressed by a specific span of
text (the event trigger) and assigned a type from a
fixed ontology defining event types. Events can take
a conceptually open-ended number of participants,
each of which is similarly bound to a specific tex-
tual expression and marked as participating in the
event in a specific role. In this work, we apply three
roles: Theme identifies the entity or event that is af-
fected by the event (e.g. the protein that is modified),
Cause its cause, and Site specifies a specific part on
the Theme participant that is affected, i.e. the mod-
ification site or region. Further, events are primary
objects of annotation and can thus in turn be par-
ticipants in other events as well as being marked as
e.g. explicitly negated (?is not phosphorylated?) or
stated speculatively (?may be phosphorylated?). An
event annotation example is shown in Figure 1.
Figure 1: Illustration of the event representation. An
event of type ADP-RIBOSYLATION (expressed through
the text ?ADP-ribosylation?) with a PROTEIN (?P2X7?)
participant in the Theme role is in turn the Theme of a
CATALYSIS event with another PROTEIN (?ART2?) as its
Cause.
3 Protein Modifications
We next present our selection of protein modifica-
tion types relevant to event annotation and an ex-
tended analysis of their relative prominence.
3.1 Protein Modifications in Ontologies
For mapping and structuring the space of protein
modification concepts, we primarily build on the
community-standard Gene Ontology (GO) (Ash-
burner et al, 2000). GO has substantial represen-
tation of protein modifications: the sub-ontology
rooted at protein modification process
(GO:0006464) in the GO biological process ontol-
ogy contains 805 terms1 (including both leaf and in-
ternal nodes). This set of terms is the starting point
for our selection of modifications types to target.
First, many specific GO terms can be excluded
due to the different approach to semantic representa-
tion taken in event annotation: while GO terms rep-
resent detailed concepts without explicit structure
(see e.g. (Ogren et al, 2004)), the event representa-
tion is structured, allowing more general terms to be
applied while capturing the same information. For
example, many GO modification terms have child
nodes that identify the target (substrate) of modifica-
tion, e.g. protein phosphorylation has the
child actin phosphorylation. In the event
representation, the target of modification is cap-
tured through the Theme argument. Similarly, GO
terms may identify the site or region of modifica-
tion, which becomes a Site argument in the event
representation (see Figure 2). To avoid redundancy,
we exclude GO terms that differ from a more gen-
eral included term only in specifying a substrate or
modification site. We similarly exclude terms that
specify a catalyst or refer to regulation of modifi-
1GO structure and statistics from data retrieved Dec. 2010.
115
Figure 2: Comparison of hypothetical text-bound GO an-
notation with specific terms (top) and event annotation
with general GO terms (bottom).
cation, as these are captured using separate events
in the applied representation, as illustrated in Fig-
ure 1. For an analogous reason, we do not separately
include type-level distinctions for ?magnitude?
variants of terms (e.g. monoubiquitination,
polyubiquitination) as these can be system-
atically modeled as aspects that can mark any event
(cf. the low/neutral/high Manner of Nawaz et al
(2010)).
Second, a number of the GO terms identify reac-
tions that are in scope of previously defined (non-
modification) event types in existing resources. To
avoid introducing redundant or conflicting annota-
tion with e.g. the GENIA Event corpus (Kim et al,
2008) or BioNLP ST resources, we excluded terms
that involve predominantly (or exclusively) non-
covalent binding (included in the scope of the event
type BINDING) and terms involving the removal of
or binding between the amino acids of a protein, in-
cluding protein maturation by peptide bond cleav-
age (annotated ? arguably somewhat inaccurately ?
as PROTEIN CATABOLISM in GENIA/BioNLP ST
data). By contrast, we do differentiate between re-
actions involving the addition of chemical groups or
small proteins and those involving their removal, in-
cluding e.g. PALMITOYLATION and DEPALMITOY-
LATION as distinct types. To preserve the ontology
structure, we further include also internal nodes ap-
pearing in GO for the purposes of structuring the
ontology (e.g. small protein conjugation
or removal), although we only apply more spe-
cific leaf nodes in event annotation.
This selection, aiming to identify the maximal
subset of the protein modification branch of the GO
ontology relevant to event annotation, resulted in
the inclusion of 74 terms, approximately 9% of the
branch total. Table 1 shows the relevant part of
the GO protein modification subontology
term structure, showing each term only once2 and
excluding very rare terms for space. (A detailed de-
scription of other information in the table is given in
the following sections.)
In addition to GO, we consider protein modifica-
tions in the MeSH ontology,3 used to index PubMed
citations with concepts relevant to them. Further, for
resolving cases not appearing in GO, we refer to the
Uniprot controlled vocabulary of posttranslational
modifications4 and the Proteomics Standards Ini-
tiative Protein Modification Ontology5 (PSI-MOD)
(Montecchi-Palazzi et al, 2008).
3.2 Protein Modifications in Databases
A substantial number of databases tracking pro-
tein modifications from a variety of perspectives ex-
ist, and new ones are introduced regularly. The
databases range from the specific (e.g. (Gupta et al,
1999; Diella et al, 2004; Zhang et al, 2010)) to the
broad in scope (Lee et al, 2005; Li et al, 2009). In-
formation on protein modifications is also found in
general protein knowledge resources such as Swiss-
Prot (Boeckmann et al, 2003) and PIR (Wu et al,
2003). The relative number of entries relevant to
each protein modification in such resources is one
possible proxy for the biological significance of the
various modifications. We apply two such estimates
in this work.
One of the primary applications of GO is the use
of the ontology terms to annotate gene products,
identifying their functions. These annotations, pro-
vided by a variety of groups in different efforts (e.g.
(Camon et al, 2004)), are readily available in GO
and used in various GO tools as a reflection of the
prominence of each of the ontology concepts. As
GO is a community standard with wide participa-
tion and a primary source in this work, we give these
annotation numbers priority in introducing an addi-
tional filter: we exclude from detailed analysis any
term that has no gene product association annota-
tions, taking this as an indication that the modifica-
2GO allows multiple inheritance, and e.g. protein
palmitoylation occurs under both protein
lipidation and protein acylation reflecting
the biological definition.
3http://www.nlm.nih.gov/mesh/meshhome.
html
4http://www.uniprot.org/docs/ptmlist
5http://www.psidev.info/MOD
116
G
PA
Sy
sP
TM
Pu
bM
ed
G
EN
IA
O
ht
a?
10
EP
I
Th
is
st
ud
y
Term GO ID
phosphorylation GO:0006468 8246 24705 93584 546 3 130 85
small protein conj./removal GO:0070647
small protein conjugation GO:0032446
ubiquitination GO:0016567 1724 439 4842 6 - 340 52
sumoylation GO:0016925 121 260 886 - - - 101
neddylation GO:0045116 66 2 100 - - - 52
ufmylation GO:0071569 33 - 1 - - - -
urmylation GO:0032447 16 - 7 - - - -
pupylation GO:0070490 11 - 15 - - - -
small protein removal GO:0070646
deubiquitination GO:0016579 360 - 206 0 - 17 2
deneddylation GO:0000338 45 - 39 - - - 8
desumoylation GO:0016926 20 - 45 - - - 3
dephosphorylation GO:0006470 1479 121 8339 28 - 3 1
glycosylation GO:0006486 1145 2982 12619 - 122 347 62
acylation GO:0043543 1 - 1728 - - - 71
acetylation GO:0006473 522 2000 4423 7 90 337 17
palmitoylation GO:0018345 49 198 1009 - - - 187
myristoylation GO:0018377 27 150 895 - - - 34
octanoylation GO:0018190 4 - 11 - - - -
palmitoleylation GO:0045234 3 - 0 - - - -
alkylation GO:0008213 0
methylation GO:0006479 552 499 9749 - 90 374 18
lipidation GO:0006497 34 51 258 - - - 16
prenylation GO:0018342 64 111 822 - - - 71
farnesylation GO:0018343 19 - 118 - - - 48
geranylgeranylation GO:0018344 26 - 79 - - - 30
deacylation GO:0035601 1 - 331 - - - 1
deacetylation GO:0006476 320 6 1056 1 - 50 4
depalmitoylation GO:0002084 9 - 81 - - - 9
ADP-ribosylation GO:0006471 261 9 3113 - - - 52
cofactor linkage GO:0018065
lipoylation GO:0009249 53 - 49 - - - 14
FAD linkage GO:0018293 46 - 6 - - - -
pyridoxal-5-phosphate linkage GO:0018352 6 - 0 - - - -
dealkylation GO:0008214 0
demethylation GO:0006482 116 - 1465 - - 13 1
deglycosylation GO:0006517 22 1 1204 - - 27 0
ISG15-protein conjugation GO:0032020 20 - 3 - - - -
arginylation GO:0016598 20 - 46 - - - -
hydroxylation GO:0018126 20 226 2948 - 103 139 3
sulfation GO:0006477 18 132 960 - - - 37
carboxylation GO:0018214 17 7 595 - - - 34
nucleotidylation GO:0018175 0
adenylylation GO:0018117 16 - 116 - - - -
uridylylation GO:0018177 1 - 105 - - - -
polyglycylation GO:0018094 17 - 14 - - - -
de-ADP-ribosylation GO:0051725 16 - 7 - - - 5
nitrosylation GO:0017014 14 - 670 - - - -
glutathionylation GO:0010731 11 - 279 - - - -
biotinylation GO:0009305 8 - 1247 - - - 4
deglutathionylation GO:0080058 3 - 42 - - - -
delipidation GO:0051697 3 - 303 - - - -
oxidation GO:0018158 3 475 23413 - - - 21
phosphopantetheinylation GO:0018215 3 - 26 - - - -
tyrosinylation GO:0018322 2 - 2 - - - -
deamination GO:0018277 1 - 840 - - - -
esterification GO:0018350 1 - 1180 - - - -
glucuronidation GO:0018411 1 - 705 - - - -
polyamination GO:0018184 1 - 13 - - - -
Table 1: Protein modifications and protein modification resources. GO terms shown abbreviated, mostly by removing
?protein? (e.g. ?acylation? instead of ?protein acylation?). Terms with 0 GPA not shown except when required for
structure. Columns: GPA: number of Gene Product Associations for each term in GO (not including counts of more
specific child nodes), SysPTM: number of SysPTM modification entries (excluding sites), PubMed: PubMed query
matches (see Section 3.3), GENIA: GENIA corpus (Kim et al, 2008), Ohta?10: corpus introduced in Ohta et al
(2010), EPI: BioNLP ST?11 EPI task corpus (Ohta et al, 2011) (excluding test set).
117
tion is not presently established as having high bio-
logical significance.6
In addition to the GO associations, we include
an estimate based on dedicated protein modification
databases. We chose to use the integrated SysPTM
resource (Li et al, 2009), which incorporates data
from five databases, four webservers, and manual
extraction from the literature. In its initial release,
SysPTM included information on ?nearly 50 modifi-
cation types? on over 30,000 proteins. The columns
labeled GPA and SysPTM in Table 1 show the num-
ber of gene product associations for each selected
type in GO and entries per type in SysPTM, respec-
tively.
3.3 Protein Modifications in domain literature
As a final estimate of the relative prominence of the
various protein modification types, we estimated the
relative frequency with which they are discussed in
the literature through simple PubMed search, query-
ing the Entrez system for each modification in its
basic nominalized form (e.g. phosphorylation) in a
protein-related article. Specifically, for each modifi-
cation string MOD we searched Entrez for
?MOD?[TIAB] AND ?protein?[TIAB]
The modifier [TIAB] specifies to search the title and
abstract. The literal string ?protein? is included to
improve the estimate by removing references that
involve the modification of non-proteins or related
concepts that happen to share the term.7 While this
query is far from a perfect estimate of the actual
number of protein modifications, we expect it to be
a useful as a rough indicator of their relative fre-
quencies and more straightforward to assess than
more involved statistical analyses (e.g. (Pyysalo et
al., 2010)). The results for these queries are given in
the PubMed column of Table 1.
6We are also aware that GO coverage of protein modifica-
tions is not perfect: for example, citrullination, eliminylation,
sialylation, as well as a number of reverse reactions for addi-
tion reactions in the ontology (e.g. demyristoylation) are not
included at the time of this writing. As for terms with no gene
product associations, we accept these omissions as indicating
that these modifications are not biologically prominent.
7For example, search for only dehydration ? a modification
with zero GPA in GO ? matches nearly 10 times as many doc-
uments as search including protein, implying that most of the
hits for the former query likely do not concern protein modi-
fication by dehydration. By contrast, the majority of hits for
phosphorylation match also phosphorylation AND protein.
3.4 Protein Modifications in Event Resources
The rightmost four columns of Table 1 present the
number of annotations for each modification type
in previously introduced event-annotated resources
following the BioNLP ST representation as well as
those annotated in the present study. While modi-
fication annotations are found also in other corpora
(e.g. (Wu et al, 2003; Pyysalo et al, 2007)), we
only include here resources readily compatible with
the BioNLP ST representation.
Separating for the moment from consideration the
question of what level of practical extraction per-
formance can be supported by these event annota-
tions, we can now provide an estimate of the up-
per bound on the coverage of relevant modifica-
tion statements for each of the three proxies (GO
GPA, SysPTM DB entries, PubMed query hits) sim-
ply by dividing the sum of instances of modifica-
tions for which annotations exist by the total. Thus,
for example, there are 8246 GPA annotations for
Phosphorylation and a total of 15597 GPA an-
notations, so the BioNLP ST?09 data (containing
only PHOSPHORYLATION events) could by the GPA
estimate cover 8246/15597, or approximately 53%
of individual modifications.8
For the total coverage of the set of types for which
event annotation is available given the corpus in-
troduced in this study, the coverage estimates are:
GO GPA: 98.2%, SysPTM 99.6%, PubMed 97.5%.
Thus, we estimate that correct extraction of the in-
cluded types would, depending on whether one takes
a gene association, database entry, or literature men-
tion point of view, cover between 97.5% to 99.6%
of protein modification instances ? a level of cov-
erage we suggest is effectively exhaustive for most
practical purposes. We next briefly describe our an-
notation effort before discarding the assumption that
correct extraction is possible and measuring actual
extraction performance.
4 Annotation
This section presents the entity and event annotation
approach, document selection, and the statistics of
the created annotation.
8The remarkably high coverage for a single type reflects the
Zipfian distribution of the modification types; see e.g. Ohta et
al. (2010).
118
4.1 Entity and Event Annotation
To maximize compatibility with existing event-
annotated resources, we chose to follow the gen-
eral representation and annotation guidelines ap-
plied in the annotation of GENIA/BioNLP ST re-
sources, specifically the BioNLP ST 2011 EPI task
corpus. Correspondingly, we followed the GE-
NIA gene/gene product (Ohta et al, 2009) annota-
tion guidelines for marking protein mentions, ex-
tended the GENIA event corpus guidelines (Kim et
al., 2008) for the annotation of protein modification
events, and marked CATALYSIS events following the
EPI task representation. For compatibility, we also
marked event negation and speculation as in these
resources. We followed the GO definitions for in-
dividual modification types, and in the rare cases
where a modification discussed in text had no ex-
isting GO definition, we extrapolated from the way
in which protein modifications are generally defined
in GO, consulting other domain ontologies and re-
sources (Section 3.1) as necessary.
4.2 Document Selection
As the distribution of protein modifications in
PubMed is extremely skewed, random sampling
would recover almost solely instances of major
types such as phosphorylation. As we are inter-
ested also in the extraction of very rare modifica-
tions, we applied a document selection strategy tar-
geted at individual modification types. We applied
one of two primary strategies depending on whether
each targeted modification type had a correspond-
ing MeSH term or not. If a MeSH term specific
to the modification exists, we queried PubMed for
the MeSH term, thus avoiding searches for spe-
cific forms of expression that might bias the search.
In cases where no specific MeSH term existed,
we searched the text of documents marked with
the generic MeSH term protein processing,
post-translational for mentions of likely
forms of expression for the modification.9 Fi-
nally, in a few isolated instances we applied cus-
tom text-based PubMed searches with broader cov-
9Specifically, we applied a regular expression incorporating
the basic form of modification expression and allowing variance
through relevant affixes and inflections derived from an initial
set of annotations for documents for which MeSH terms were
defined.
Item Count
Abstract 360
Word 76806
Protein 4698
Event type 37
Event instance 1142
Table 2: Annotation statistics.
erage. Then, as many of the modifications are not
limited to protein substrates, to select documents re-
lating specifically to protein modification we pro-
ceeded to tagged a large random sample of selected
documents with the BANNER named entity tagger
(Leaman and Gonzalez, 2008) trained on the GENE-
TAG corpus (Tanabe et al, 2005) and removed doc-
uments with fewer than five automatically tagged
gene/protein-related entities. The remaining docu-
ments were then randomly sampled for annotation.10
4.3 Corpus Statistics
We initially aimed to annotate balanced numbers of
modification types in order of their estimated promi-
nence, with particular focus on previously untar-
geted reaction types involving the addition of chem-
ical groups or small proteins. However, it became
apparent in the annotation process that the extreme
rarity of some of the modifications as well as the
tendency for more frequent modifications to be dis-
cussed in texts mentioning rare ones made this im-
possible. Thus, while preserving the goal of es-
tablishing broadly balanced numbers of major new
modifications, we allowed the number of rare reac-
tions to remain modest.
Table 2 summarizes the statistics of the final cor-
pus, and the rightmost column of Table 1 shows
per-type counts. We note that as reactions involv-
ing the removal of chemical groups or small pro-
teins were not separately targeted, only few events
of such types were annotated. We did not sepa-
rately measure inter-annotator agreement for this ef-
fort, but note that this work is an extension of the
EPI corpus annotation, for which comparison of in-
dependently created event annotations indicated an
F-score of 82% for the full task and 89% for the core
targets (see Section 5.1) (Ohta et al, 2011).
10This strategy, including MeSH-based search, was applied
also in the BioNLP Shared Task 2011 EPI task document selec-
tion.
119
5 Experiments
To assess actual extraction performance, we per-
formed experiments using a state-of-the art event ex-
traction system.
5.1 Experimental Setup
We first split the corpus into a training/development
portion and a held out set for testing, placing half of
the abstracts into each set. The split was stratified
by event type to assure that relatively even numbers
of each event type were present in both sets. All
development was performed using cross-validation
on the visible portion of the data, and a single final
experiment was performed on the test dataset.
To assure that our results are comparable with
those published in recent event extraction stud-
ies, we adopted the standard evaluation crite-
ria of the BioNLP Shared Task. The evalua-
tion is event instance-based and uses the standard
precision/recall/F1-score metrics. We modified the
shared task evaluation software to support the newly
defined event types and ran experiments with the
standard approximate span matching and partial re-
cursive matching criteria (see (Kim et al, 2009)).
We further follow the EPI task evaluation in re-
porting results separately for the extraction of only
Theme and Cause arguments (core task) and for the
full argument set.
5.2 Event extraction method
We applied the EventMine event extraction system
(Miwa et al, 2010a; Miwa et al, 2010b), an SVM-
based pipeline system using an architecture similar
to that of the best-performing system in the BioNLP
ST?09 (Bjo?rne et al, 2009); we refer to the studies
of Miwa et al for detailed description of the base
system. For analysing sentence structure, we applied
the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and
GDep beta2 (Sagae and Tsujii, 2007) parsers.
For the present study, we modified the base Event-
Mine system as follows. First, to improve efficiency
and generalizability, instead of using all words as
trigger candidates as in the base system, we filtered
candidates using a dictionary extracted from train-
ing data and expanded by using the UMLS specialist
lexicon (Bodenreider, 2004) and the ?hypernyms?
and ?similar to? relations in WordNet (Fellbaum,
1998). Second, to allow generalization across ar-
gument types, we added support for solving a single
classification problem for event argument detection
instead of solving multiple classification problems
separated by argument types. Finally, to facilitate
the use of other event resources for extraction, we
added functionality to incorporate models trained by
other corpora as reference models, using predictions
from these models as features in classification.
5.3 Experimental results
We first performed a set of experiments to determine
whether models can beneficially generalize across
different modification event types. The EventMine
pipeline has separate classification stages for event
trigger detection, event-argument detection, and the
extraction of complete event structures. Each of
these stages involves a separate set of features and
output labels, some of which derive directly from
the involved event types: for example, in deter-
mining whether a specific entity is the Theme of
an event triggered by the string ?phosphorylation?,
the system by default uses the predicted event type
(PHOSPHORYLATION) among its features. It is pos-
sible to force the model to generalize across event
types by replacing specific types with placehold-
ers, for example replacing PHOSPHORYLATION,
METHYLATION, etc. with MODIFICATION.
In preliminary experiments on the development
set, we experimented with a number of such gener-
alizations. Results indicated that while some gen-
eralization was essential for achieving good ex-
traction performance, most implementation variants
produced broadly comparable results. We chose the
following generalizations for the final test: in the
trigger detection model, no generalization was per-
formed (allowing specific types to be extracted), for
argument detection, all instances of event types were
replaced with a generic type (EVENT), and for event
structure prediction, all instances of specific modi-
fication event types (but not CATALYSIS) were re-
placed with a generic type (MODIFICATION). Re-
sults comparing the initial, ungeneralized model to
the generalized one are shown in the top two rows
of Table 3. The results indicate that generalization is
clearly beneficial: attempting to learn each of the
event types in isolation leaves F-score results ap-
proximately 4-5% points lower than when general-
120
Core Full
Initial 39.40 / 46.36 / 42.60 31.39 / 38.88 / 34.74
Generalized 39.02 / 61.18 / 47.65 31.07 / 51.89 / 38.87
+Model 41.28 / 61.28 / 49.33 33.66 / 53.06 / 41.19
+Ann 38.46 / 66.99 / 48.87 32.36 / 59.17 / 41.84
+Model +Ann 41.84 / 66.17 / 51.26 33.98 / 56.00 / 42.30
Test data 45.69 / 62.35 / 52.74 38.03 / 54.57 / 44.82
Table 3: Experimental results.
izing across types. A learning curve for the gen-
eralized model is shown in Figure 3. While there
is some indication of decreasing slope toward use
of the full dataset, the curve suggests performance
could be further improved through additional anno-
tation efforts.
In a second set of experiments, we investigated
the compatibility of the newly introduced annota-
tions with existing event resources by incorporat-
ing their annotations either directly as training data
(+Ann) or indirectly through features from predic-
tions from a model trained on existing resources
(+Model), as well as their combination. We per-
formed experiments with the BioNLP Shared Task
2011 EPI task corpus11 and the generalized setting.
The results of these experiments are given in the
middle rows of Table 3. We find substantial bene-
fit from either form of existing resource integration
alone, and, interestingly, an indication that the ben-
efits of the two approaches can be combined. This
result indicates that the newly introduced corpus is
compatible with the EPI corpus, a major previously
introduced resource for protein modification event
extraction. Evaluation on the test data (bottom row
of Table 3) confirmed that development data results
were not overfit and generalized well to previously
unseen data.
6 Discussion and Conclusions
We have presented an effort to directly address the
challenges involved in the exhaustive extraction of
protein modifications in text. We analysed the Gene
Ontology protein modification process
subontology from the perspective of event extraction
for information extraction, arguing that due largely
to the structured nature of the event representation,
11When combining EPI annotations directly as additional
training abstracts, we filtered out abstracts including possible
?missing? annotations for modification types not annotated in
EPI data using a simple regular expression.
Figure 3: Learning curve.
74 of the 805 ontology terms suffice to capture the
general modification types included. Through an
analysis of the relative prominence of protein modi-
fications in ontology annotations, domain databases,
and literature, we then filtered and prioritized these
types, estimating that correct extraction of the most
prominent half of these types would give 97.5%-
99.6% coverage of protein modifications, a level that
is effectively exhaustive for practical purposes.
To support modification event extraction and to
estimate actual extraction performance, we then
proceeded to manually annotate a corpus of 360
PubMed abstracts selected for relevance to the se-
lected modification types. The resulting corpus an-
notation marks over 4500 proteins and over 1000 in-
stances of modification events and more than triples
the number of specific protein modification types for
which text-bound event annotations are available.
Experiments using a state-of-the-art event extraction
system showed that a machine learning method can
beneficially generalize features across different pro-
tein modification event types and that incorporation
of BioNLP Shared Task EPI corpus annotations can
improve performance, demonstrating the compati-
bility of the created resource with existing event cor-
pora. Using the best settings on the test data, we
found that the core extraction task can be performed
at 53% F-score.
The corpus created in this study is freely available
for use in research from http://www-tsujii.
is.s.u-tokyo.ac.jp/GENIA.
Acknowledgments
We would like to thank Yo Shidahara and Yoshihiro
Okuda of NalaPro Technologies for their efforts in
creating the corpus annotation. This work was sup-
ported by Grant-in-Aid for Specially Promoted Re-
search (MEXT, Japan).
121
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,
JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-
pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,
S Lewis, JC Matese, JE Richardson, M Ringwald,
GM Rubin, and G Sherlock. 2000. Gene ontology:
tool for the unification of biology. Nature genetics,
25:25?29.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of BioNLP?09 Shared
Task, pages 10?18.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Scaling up biomed-
ical event extraction to the entire pubmed. In Pro-
ceedings of the 2010 Workshop on Biomedical Natural
Language Processing, pages 28?36.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32(Database
issue):D267?70.
B. Boeckmann, A. Bairoch, R. Apweiler, M.C. Blat-
ter, A. Estreicher, E. Gasteiger, M.J. Martin, K. Mi-
choud, C. O?Donovan, I. Phan, et al 2003. The
SWISS-PROT protein knowledgebase and its supple-
ment TrEMBL in 2003. Nucleic acids research,
31(1):365.
D.F. Brennan and D. Barford. 2009. Eliminylation:
a post-translational modification catalyzed by phos-
phothreonine lyases. Trends in biochemical sciences,
34(3):108?114.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In Proceedings of the BioNLP?09
Shared Task, pages 19?27, Boulder, Colorado, June.
Association for Computational Linguistics.
Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-
vian Lee, Emily Dimmer, John Maslen, David Binns,
Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.
2004. The Gene Ontology Annotation (GOA)
Database: sharing knowledge in Uniprot with Gene
Ontology. Nucl. Acids Res., 32(suppl 1):D262?266.
Francesca Diella, Scott Cameron, Christine Gemund,
Rune Linding, Allegra Via, Bernhard Kuster, Thomas
Sicheritz-Ponten, Nikolaj Blom, and Toby Gibson.
2004. Phospho.elm: A database of experimentally
verified phosphorylation sites in eukaryotic proteins.
BMC Bioinformatics, 5(1):79.
C. Fellbaum. 1998. Wordnet: an electronic lexical
database. In International Conference on Computa-
tional Linguistics.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: A natural-language processing system for the
extraction of molecular pathways from journal articles.
Bioinformatics, 17(Suppl. 1):S74?S82.
Ramneek Gupta, Hanne Birch, Kristoffer Rapacki, Sren
Brunak, and Jan E. Hansen. 1999. O-glycbase version
4.0: a revised database of o-glycosylated proteins. Nu-
cleic Acids Research, 27(1):370?372.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of bionlp
shared task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Proceedings of PSB?08,
pages 652?663.
Tzong-Yi Lee, Hsien-Da Huang, Jui-Hung Hung, Hsi-
Yuan Huang, Yuh-Shyong Yang, and Tzu-Hao Wang.
2005. dbPTM: an information repository of protein
post-translational modification. Nucleic Acids Re-
search, 34(suppl 1):D622?D627.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein lig-
ases. Nucl. Acids Res., 36(suppl.2):W416?422.
Hong Li, Xiaobin Xing, Guohui Ding, Qingrun Li, Chuan
Wang, Lu Xie, Rong Zeng, and Yixue Li. 2009.
Sysptm: A systematic resource for proteomic research
on post-translational modifications. Molecular & Cel-
lular Proteomics, 8(8):1839?1849.
Takuya Matsuzaki and Yusuke Miyao. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
In In Proceedings of IJCAI-07, pages 1671?1676.
122
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. Evaluating dependency rep-
resentations for event extraction. In Proceedings of
Coling?10, pages 779?787.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146.
Luisa Montecchi-Palazzi, Ron Beavis, Pierre-Alain Binz,
Robert Chalkley, John Cottrell, David Creasy, Jim
Shofstahl, Sean Seymour, and John Garavelli. 2008.
The PSI-MOD community standard for representation
of protein modification data. Nature Biotechnology,
26:864?866.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2005. Beyond the clause: extraction of
phosphorylation information from medline abstracts.
Bioinformatics, 21(suppl.1):i319?327.
R. Nawaz, P. Thompson, J. McNaught, and S. Ananiadou.
2010. Meta-Knowledge Annotation of Bio-Events.
Proceedings of LREC 2010, pages 2498?2507.
P.V. Ogren, K.B. Cohen, G.K. Acquaah-Mensah, J. Eber-
lein, and L. Hunter. 2004. The compositional struc-
ture of Gene Ontology terms. In Pacific Symposium
on Biocomputing, page 214.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
GENETAG-style annotation to GENIA corpus. In
Proceedings of BioNLP?09, pages 106?107.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii. 2010.
An analysis of gene/protein associations at pubmed
scale. In Proceedings of the fourth International Sym-
posium for Semantic Mining in Biomedicine (SMBM
2010).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Morris,
Hong Yu, Pablo Ariel Duboue?, Wubin Weng, W. John
Wilbur, Vasileios Hatzivassiloglou, and Carol Fried-
man. 2004. GeneWays: A system for extracting, ana-
lyzing, visualizing, and integrating molecular pathway
data. Journal of Biomedical Informatics, 37(1):43?53.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of EMNLP-CoNLL?07,
pages 1044?1050.
Jasmin Saric, Lars Juhl Jensen, Rossitza Ouzounova, Is-
abel Rojas, and Peer Bork. 2006. Extraction of regu-
latory gene/protein networks from Medline. Bioinfor-
matics, 22(6):645?650.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Christopher Walsh. 2006. Posttranslational modification
of proteins: expanding nature?s inventory. Roberts &
Company Publishers.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
X. Yuan, ZZ Hu, HT Wu, M. Torii, M. Narayanaswamy,
KE Ravikumar, K. Vijay-Shanker, and CH Wu. 2006.
An online literature mining tool for protein phospho-
rylation. Bioinformatics, 22(13):1668.
Yan Zhang, Jie Lv, Hongbo Liu, Jiang Zhu, Jianzhong Su,
Qiong Wu, Yunfeng Qi, Fang Wang, and Xia Li. 2010.
Hhmd: the human histone modification database. Nu-
cleic Acids Research, 38(suppl 1):D149?D154.
123
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 94?98,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
NaCTeM EventMine for BioNLP 2013 CG and PC tasks
Makoto Miwa and Sophia Ananiadou
National Centre for Text Mining, University of Manchester, United Kingdom
School of Computer Science, University of Manchester, United Kingdom
{makoto.miwa,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper describes NaCTeM entries for
the Cancer Genetics (CG) and Pathway
Curation (PC) tasks in the BioNLP Shared
Task 2013. We have applied a state-of-
the-art event extraction system EventMine
to the tasks in two different settings: a
single-corpus setting for the CG task and
a stacking setting for the PC task. Event-
Mine was applicable to the two tasks with
simple task specific configuration, and it
produced a reasonably high performance,
positioning second in the CG task and first
in the PC task.
1 Introduction
With recent progress in biomedical natural lan-
guage processing (BioNLP), automatic extraction
of biomedical events from texts becomes practi-
cal and the extracted events have been success-
fully employed in several applications, such as
EVEX (Bjo?rne et al, 2012; Van Landeghem et
al., 2013) and PathText (Miwa et al, 2013a).
The practical applications reveal a problem in that
both event types and structures need to be cov-
ered more widely. The BioNLP Shared Task 2013
(BioNLP-ST 2013) offers several tasks addressing
the problem, and especially in the Cancer Genetics
(CG) (Pyysalo et al, 2013) and Pathway Curation
(PC) (Ohta et al, 2013) tasks, new entity/event
types and biomedical problems are focused.
Among dozens of extraction systems proposed
during and after the two previous BioNLP shared
tasks (Kim et al, 2011; Kim et al, 2012; Pyysalo
et al, 2012b), EventMine (Miwa et al, 2012)1
has been applied to several biomedical event ex-
traction corpora, and it achieved the state-of-the-
art performance in several corpora (Miwa et al,
2013b). In these tasks, an event associates with
1http://www.nactem.ac.uk/EventMine/
a trigger expression that denotes its occurrence
in text, has zero or more arguments (entities or
other events) that are identified with their roles
(e.g., Theme, Cause) and may be assigned hedge
attributes (e.g., Negation).
This paper describes how EventMine was ap-
plied to the CG and PC tasks in the BioNLP-ST
2013. We configured EventMine minimally for
the CG task and submit the results using the mod-
els trained on the training and development data
sets with no external resources. We employed a
stacking method for the PC task; the method ba-
sically trained the models on the training and de-
velopment data sets, but it also employed features
representing prediction scores of models on seven
external corpora.
We will first briefly describe EventMine and its
task specific configuration in the next section, then
show and discuss the results, and finally conclude
the paper with future work.
2 EventMine for CG and PC Tasks
This section briefly introduces EventMine and the
PC and CG tasks, and then explains its task spe-
cific configuration.
2.1 EventMine
EventMine (Miwa et al, 2012) is an SVM-based
pipeline event extraction system. For the de-
tails, we refer the readers to Miwa et al (2012;
2013b). EventMine consists of four modules: a
trigger/entity detector, an argument detector, a
multi-argument detector and a hedge detector.
The trigger/entity detector finds words that match
the head words (in their surfaces, base forms
by parsers, or stems by a stemmer) of trig-
gers/entities in the training data, and the detector
classifies each word into specific entity types (e.g.,
DNA domain or region), event types (Regulation)
or a negative type that represents the word does
not participate in any events. The argument
94
detector enumerates all possible pairs among
triggers and arguments that match the semantic
type combinations of the pairs in the training data,
and classifies each pair into specific role types
(e.g., Binding:Theme-Gene or gene product) or
a negative type. Similarly, the multi-argument
detector enumerates all possible combina-
tions of pairs that match the semantic type
structures of the events in the training data,
and classifies each combination into an event
structure type (e.g., Positive regulation:Cause-
Gene or gene product:Theme-Phosphorylation)
or a negative type. The hedge detector attaches
hedges to the detected events by classifying the
events into specific hedge types (Speculation and
Negation) or a negative type.
All the classifications are performed by one-vs-
rest support vector machines (SVMs). The detec-
tors use the types mentioned above as their clas-
sification labels. Labels with scores larger than
the separating hyper-plane of SVM and the label
with the largest value are selected as the predicted
labels; the classification problems are treated as
multi-class multi-label classification problems and
at least one label (including a negative type) needs
to be selected in the prediction.
Features for the classifications include charac-
ter n-grams, word n-grams, shortest paths among
event participants on parse trees, and word n-
grams and shortest paths between event partici-
pants and triggers/entities outside of the events on
parse trees. The last features are employed to cap-
ture the dependencies between the instances. All
gold entity names are replaced with their types,
the feature space is compressed to 220 by hash-
ing to reduce space cost, the positive instances are
weighted to reduce class imbalance problems, the
feature vectors are normalised, and the C parame-
ter for SVM is set to 1.
In the pipeline approach, there is no way to de-
tect instances if the participants are missed by the
preceding modules. EventMine thus aims high
recall in the modules by the multi-label setting
and weighting positive instances. EventMine also
avoids training on instances that cannot be de-
tected by generating the training instances based
on predictions by the preceding modules since the
training and test instances should be similar.
EventMine is flexible and applicable to several
event extraction tasks with task specific configura-
tion on entity, role and event types. This configu-
ration is described in a separate file2.
2.2 CG and PC Tasks
The CG task (Pyysalo et al, 2013) aims to extract
information on the biological processes relating to
the development and progression of cancer. The
annotation is built on the Multi-Level Event Ex-
traction (MLEE) corpus (Pyysalo et al, 2012a),
which EventMine was once applied to. The PC
task (Ohta et al, 2013), on the other hand, aims
to support the curation of bio-molecular pathway
models, and the corpus texts are selected to cover
both signalling and metabolic pathways.
Both CG and PC tasks offer more entity, role
and event types than most previous tasks like GE-
NIA (Kim et al, 2012) does, which may make the
classification problems more difficult.
2.3 Configuration for CG and PC Tasks
We train models for the CG and PC tasks in simi-
lar configuration, except for the incorporation of a
stacking method for the PC task. We first explain
the configuration applied to both tasks and then in-
troduce the stacking method for the PC task.
We employ two kinds of type generalisations
for both tasks: one for the classification labels
and features and the other for the generation of in-
stances. After the disambiguation of trigger/entity
types by the trigger/entity detector, we reduce the
number of event role labels and event structure
labels by the former type generalisations. The
generalisations are required to reduce the com-
putational costs that depend on the number of
the classification labels. Unfortunately, we can-
not evaluate the effect of the generalisations on
the performance since there are too many pos-
sible labels in the tasks. The generalisations
may alleviate the data sparseness problem but
they may also induce over-generalised features
for the problems with enough training instances.
For event roles, we generalise regulation types
(e.g., Positive regulation, Regulation) into a single
REGULATION type and post-transcriptional mod-
ification (PTM) types (e.g., Acetylation, Phos-
phorylation) into a single PTM type for trigger
types, numbered role types into a non-numbered
role type (e.g., Participant2?Participant) for role
2This file is not necessary since the BioNLP ST data for-
mat defines where these semantic types are described, but this
file is separated for the type generalisations explained later
and the specification of gold triggers/entities without repro-
ducing a1/a2 files.
95
types, and event types into a single EVENT type
and entity types into a single ENTITY type for
argument types. For event structures, we apply
the same generalisations except for the general-
isations of numbered role types since the num-
bered role types are important in differentiating
events. Unlike other types, the numbered role
types in events are not disambiguated by any other
modules. The generalisations are also applied to
the features in all the detectors when applicable.
These generalisations are the combination of the
generalisations for the GENIA, Epigenetics and
Post-translational Modifications (EPI), and Infec-
tious Diseases (ID) (Pyysalo et al, 2012b) of the
BioNLP-ST 2011 (Miwa et al, 2012).
The type generalisations on labels and fea-
tures are not directly applicable to generate pos-
sible instances in the detectors since the gen-
eralisations may introduce illegal or unrealis-
tic event structures. Instead, we employ sep-
arate type generalisations to expand the possi-
ble event role pair and event structure types and
cover types, which do not appear in the training
data. For example, if there are Regulation:Theme-
Gene expression instances but there are no Posi-
tive regulation:Theme-Gene expression instances
in the training data, we allow the creation of the
latter instances by generalising the triggers, i.e.,
REGULATION:Theme-Gene expression, and we
used all the created instances for classification.
The type generalisations may incorporate noisy in-
stances but they pose the possibility to find unan-
notated event structures. To avoid introducing un-
expected event structures, we apply the generali-
sations only to the regulation trigger types.
We basically follow the setting for EPI in
Miwa et al (2012). We employ a deep syntactic
parser Enju (Miyao and Tsujii, 2008) and a de-
pendency parser GDep (Sagae and Tsujii, 2007).
We utilise liblinear-java (Fan et al, 2008)3 with
the L2-regularised L2-loss linear SVM setting for
the SVM implementation, and Snowball4 for the
stemmer. We, however, use no external resources
(e.g., dictionaries) or tools (e.g., a coreference
resolver) except for the external corpora in the
stacked models for the PC task.
We train models for the CG task using the con-
figuration described above. For PC, in addition
to the configuration, we incorporated a stacking
3http://liblinear.bwaldvogel.de/
4http://snowball.tartarus.org/
Setting Recall Precision F-score
? 42.87 47.72 45.16
+Exp. 43.37 46.42 44.84
+Exp.+Stack. 43.59 48.77 46.04
Table 1: Effect of the type generalisations for ex-
panding possible instances (+Exp.) and stacking
method (+Stack.) on the PC development data set.
method (Wolpert, 1992) using the models with the
same configuration for seven other available cor-
pora: GENIA, EPI, ID, DNA methylation (Ohta
et al, 2011a), Exhaustive PTM (Pyysalo et al,
2011), mTOR (Ohta et al, 2011b) and CG. The
prediction scores of all the models are used as ad-
ditional features in the detectors. Although some
corpora may not directly relate to the PC task and
models trained on such corpora can produce noisy
features, we use all the corpora without selection
since the stacking often improve the performance,
e.g., (Pyysalo et al, 2012a; Miwa et al, 2013b).
3 Evaluation
We first evaluate the type generalisations for ex-
panding possible event structures and the stack-
ing method in Table 1. The scores were calcu-
lated using the evaluation script provided by the
organisers with the official evaluation metrics (soft
boundary and partial recursive matching). The
generalisations improved recall with the loss of
precision, and they slightly degraded the F-score
in total. The generalisations were applied to the
test set in the submission since this result was ex-
pected as explained in Section 2.3 and the slightly
high recall is favourable for the practical applica-
tions like semantic search engines (Miwa et al,
2013a). Although the improvement by the stack-
ing method (+Exp.+Stack. compared to +Exp.) is
not statistically significant (p=0.14) using the ap-
proximate randomisation method (Noreen, 1989;
Kim et al, 2011), this slight improvement indi-
cates that the corpus in the PC task shares some
information with the other corpora.
Tables 2 and 3 show the official scores of our
entries on the test data sets for the CG and PC
tasks5. EventMine ranked second in the CG task
and first in the PC task. The scores of the best sys-
tem among the other systems (TEES-2.1 (Bjo?rne
and Salakoski, 2013)) are shown for reference.
5We refer to the websites of the tasks for the details of the
event categories.
96
Task System Rec. Prec. F-Score
CG EventMine 48.83 55.82 52.09
TEES-2.1 48.76 64.17 55.41
PC EventMine 52.23 53.48 52.84
TEES-2.1 47.15 55.78 51.10
Table 2: Official best and second best scores on
the CG and PC tasks. Higher scores are shown in
bold.
Task Category EventMine TEES-2.1
CG ANATOMY 71.31 77.20
PATHOL 59.78 67.51
MOLECUL 72.77 72.60
GENERAL 53.08 52.20
REGULAT 39.79 43.08
PLANNED 40.51 39.43
MOD 29.95 34.66
PC SIMPLE 65.60 63.92
NON-REG 65.72 63.37
REGULAT 40.10 39.39
MOD 28.05 28.73
Table 3: F-scores on the CG and PC tasks for event
categories. Higher scores are shown in bold.
EventMine achieved the highest recall for both
tasks, and this is favourable as mentioned above.
This high recall is reasonable since EventMine
solved the problems as multi-label classification
tasks, corrected the class imbalance problem as
explained in Section 2.1 and incorporated the type
generalisations for expanding possible event struc-
tures. The performance (in F-score) on both CG
and PC tasks is slightly lower than the perfor-
mance on the GENIA and ID tasks in the BioNLP-
ST 2011 (Miwa et al, 2012), and close to the per-
formance on the EPI task. This may be partly be-
cause the GENIA and ID tasks deal with a fewer
number of event types than the other tasks.
EventMine performed worse than the best sys-
tem in the CG task, but this result is promis-
ing considering that we did not incorporate any
other resources and tune the parameters (e.g., C
in SVM). The detailed comparison with TEES-
2.1 shows that EventMine performed much worse
than TEES-2.1 in anatomical and pathological
event categories, which contained relatively new
event types. This indicates EventMine missed
some of the new structures in the new event types.
The range of the scores is similar to the
scores on the MLEE corpus (52.34?53.43% in F-
Score (Pyysalo et al, 2012a)) although we can-
not directly compare the results. The ranges of
the scores are around 60% to 70% for non-nested
events (e.g., SIMPLE), 40% for nested events
(e.g., REGULAT) and 30% for modifications (e.g.,
MOD). This large spread of the scores may be
caused by a multiplication of errors in predicting
their participants, since similar spread was seen
in the previous tasks (e.g., (Miwa et al, 2012)).
These results indicate that we may not be able
to improve the performance just by increasing the
training instances.
These results show that EventMine performed
well on the PC task that is a completely novel task
for EventMine, and the stacking would also work
effectively on the test set.
4 Conclusions
This paper explained how EventMine was ap-
plied to the CG and PC tasks in the BioNLP-
ST 2013. EventMine performed well on these
tasks and achieved the second best performance
in the CG task and the best performance in the
PC task. We show the usefulness of incorporat-
ing other existing corpora in the PC task. The
success of this application shows that the Event-
Mine implementation is flexible enough to treat
the new tasks. The performance ranges, however,
shows that we may need to incorporate other novel
techniques/linguistic information to produce the
higher performance.
As future work, we will investigate the cause
of the missed events. We also would like to ex-
tend and apply other functions in EventMine, such
as co-reference resolution, and seek a general ap-
proach that can improve the event extraction per-
formance on all the existing corpora, using the
training data along with external resources.
Acknowledgement
This work is supported by the Biotechnology and
Biological Sciences Research Council (BBSRC)
[BB/G53025X/1] and the Grant-in-Aid for Young
Scientists B [25730129] of the Japan Science and
Technology Agency (JST).
References
Jari Bjo?rne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bioNLP
97
2013 shared task. In Proceedings of BioNLP Shared
Task 2013 Workshop, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Jari Bjo?rne, Sofie Van Landeghem, Sampo Pyysalo,
Tomoko Ohta, Filip Ginter, Yves Van de Peer,
Sophia Ananiadou, and Tapio Salakoski. 2012.
Pubmed-scale event extraction for post-translational
modifications, epigenetics and protein structural re-
lations. In BioNLP: Proceedings of the 2012 Work-
shop on Biomedical Natural Language Processing,
pages 82?90, Montre?al, Canada, June. Association
for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011. Extract-
ing Bio-Molecular Events from Literature ? the
BioNLP?09 Shared Task. Computational Intelli-
gence, 27(4):513?540.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia Event and Protein Coreference tasks of
the BioNLP Shared Task 2011. BMC Bioinformat-
ics, 13(Suppl 11):S1.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Makoto Miwa, Tomoko Ohta, Rafal Rak, Andrew
Rowley, Douglas B. Kell, Sampo Pyysalo, and
Sophia Ananiadou. 2013a. A method for integrat-
ing and ranking the evidence for biochemical path-
ways by mining reactions from text. Bioinformatics.
(In Press).
Makoto Miwa, Sampo Pyysalo, Tomoko Ohta, and
Sophia Ananiadou. 2013b. Wide coverage biomedi-
cal event extraction using multiple partially overlap-
ping corpora. BMC Bioinformatics, 14(1):175.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80, March.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2011a. Event extraction for dna
methylation. Journal of Biomedical Semantics,
2(Suppl 5):S2.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsu-
jii. 2011b. From pathways to biomolecular
events: Opportunities and challenges. In Proceed-
ings of BioNLP?11, pages 105?113, Portland, Ore-
gon, USA. ACL.
Tomoko Ohta, Sampo Pyysalo, Rafal Rak, Andrew
Rowley, Hong-Woo Chun, Sung-Jae Jung, Sung-Pil
Choi, and Sophia Ananiadou. 2013. Overview of
the pathway curation (PC) task of bioNLP shared
task 2013. In Proceedings of BioNLP Shared Task
2013 Workshop, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and
Jun?ichi Tsujii. 2011. Towards exhaustive event ex-
traction for protein modifications. In Proceedings
of BioNLP?11, pages 114?123, Portland, Oregon,
USA, June. ACL.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-
Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou.
2012a. Event extraction across multiple levels of bi-
ological organization. Bioinformatics, 28(18):i575?
i581.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2012b.
Overview of the ID, EPI and REL tasks of BioNLP
Shared Task 2011. BMC Bioinformatics, 13(Suppl
11):S2.
Sampo Pyysalo, Tomoko Ohta, and Sophia Ananiadou.
2013. Overview of the cancer genetics (CG) task
of bioNLP shared task 2013. In Proceedings of
BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. ACL.
S. Van Landeghem, J. Bjorne, C. H. Wei, K. Hakala,
S. Pyysalo, S. Ananiadou, H. Y. Kao, Z. Lu,
T. Salakoski, Y. Van de Peer, and F. Ginter.
2013. Large-scale event extraction from literature
with multi-level gene normalization. PLoS One,
8(4):e55814.
David H Wolpert. 1992. Stacked generalization. Neu-
ral networks, 5(2):241?259.
98
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6?14,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Timegraphs in Temporal Relation Classification
Natsuda Laokulrat
?
, Makoto Miwa
?
, and Yoshimasa Tsuruoka
?
?
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{natsuda,tsuruoka}@logos.t.u-tokyo.ac.jp
?
Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
miwa@toyota-ti.ac.jp
Abstract
Most of the recent work on machine
learning-based temporal relation classifi-
cation has been done by considering only
a given pair of temporal entities (events or
temporal expressions) at a time. Entities
that have temporal connections to the pair
of temporal entities under inspection are
not considered even though they provide
valuable clues to the prediction. In this
paper, we present a new approach for ex-
ploiting knowledge obtained from nearby
entities by making use of timegraphs and
applying the stacked learning method to
the temporal relation classification task.
By performing 10-fold cross validation
on the Timebank corpus, we achieved an
F1 score of 59.61% based on the graph-
based evaluation, which is 0.16 percent-
age points higher than that of the local
approach. Our system outperformed the
state-of-the-art system that utilizes global
information and achieved about 1.4 per-
centage points higher accuracy.
1 Introduction
Temporal relationships between entities, namely
temporal expressions and events, are regarded as
important information for deep understanding of
documents. Being able to predict temporal re-
lations between events and temporal expressions
within a piece of text can support various NLP ap-
plications such as textual entailment (Bos et al.,
2005), multi-document summarization (Bollegala
et al., 2010), and question answering (Ravichan-
dran and Hovy, 2002).
Temporal relation classification, which is one of
the subtasks TempEval-3 (UzZaman et al., 2013),
aims to classify temporal relationships between
pairs of temporal entities into one of the 14 re-
lation types according to the TimeML specifica-
tion (Pustejovsky et al., 2005), e.g., BEFORE, AF-
TER, DURING, and BEGINS.
The Timebank corpus introduced by Puste-
jovsky et al. (2003) has enabled the machine
learning-based classification of temporal relation-
ship. By learning from the annotated relation
types in the documents, it is possible to predict
the temporal relation of a given pair of temporal
entities (Mani et al., 2006).
However, most of the existing machine
learning-based systems use local information
alone, i.e., they consider only a given pair of tem-
poral entities at a time. Entities that have tem-
poral connections to the entities in the given pair
are not considered at all even though they provide
valuable clues to the prediction. Hence, the lo-
cal approach often produces contradictions. For
instance, the system may predict that A happens
before B, that B happens before C, and that A hap-
pens after C, which are mutually contradictory.
In order to tackle the contradiction problem,
global approaches have been proposed by Cham-
bers and Jurafsky (2008) and Yoshikawa et al.
(2009). Chamber and Jurafsky proposed a global
model based on Integer Linear Programming that
combines the output of local classifiers and max-
imizes the global confidence scores. While they
focused only on the temporal relations between
events, Yoshikawa et al. proposed a Markov Logic
model to jointly predict the temporal relations be-
tween events and time expressions.
In this paper, we propose an approach that
utilizes timegraphs (Miller and Schubert, 1999),
which represent temporal connectivity of all tem-
poral entities in each document, for the relation
classification. Our method differs from the pre-
vious work in that their methods used transition
rules to enforce consistency within each triplet of
relations, but our method can also work with a set
consisting of more than three relations. Moreover,
6
Figure 1: An example from the Timebank corpus
in our work, the full set of temporal relations spec-
ified in TimeML are used, rather than the reduced
set used in the previous work.
We evaluate our method on the TempEval-3?s
Task C-relation-only data, which provides a sys-
tem with all the appropriate temporal links and
only needs the system to classify the relation
types. The result shows that by exploiting the
timegraph features in the stacked learning ap-
proach, the classification performance improves
significantly. By performing 10-fold cross valida-
tion on the Timebank corpus, we can achieve an F1
score of 59.61% based on the graph-based evalu-
ation, which is 0.16 percentage points (pp) higher
than that of the local approach. We compared the
results of our system to those of Yoshikawa et al.
(2009) and achieved about 1.4 pp higher accuracy.
The remainder of the paper is organized as fol-
lows. Section 2 explains the temporal relation
classification task and the pairwise classifier. Sec-
tion 3 and Section 4 describe our proposed time-
graph features and the application to the stacked
learning approach. Section 5 shows the experi-
ment setup and presents the results. Finally, we
discuss the results in 6 and conclude with direc-
tions for future work in Section 7.
2 Temporal Relation Classification
According to TempEval-3, a temporal annotation
task consists of several subtasks, including tempo-
ral expression extraction (Task A), event extrac-
tion (Task B), and temporal link identification and
relation classification (Task C). Our work, as with
the previous work mentioned in Section 1, only
focuses on the relation classification task (Task C-
relation only). The system does not extract events
and temporal expressions automatically.
A pair of temporal entities, including events and
temporal expressions, that is annotated as a tem-
poral relation is called a TLINK. Temporal rela-
tion classification is a task to classify TLINKs into
temporal relation types.
Following TempEval-3, all possible TLINKs
are between:
? Event and Document Creation Time (DCT)
? Events in the same sentence
? Event and temporal expression in the same
sentence
? Events in consecutive sentences
2.1 The Timebank corpus
The Timebank corpus is a human-annotated cor-
pus commonly used in training and evaluating a
temporal relation classifier. It is annotated follow-
ing the TimeML specification to indicate events,
temporal expressions, and temporal relations. It
also provides five attributes, namely, class, tense,
aspect, modality, and polarity, associated with
each event (EVENT), and four attributes, namely,
type, value, functionInDocument, and temporal-
Function, associated with each temporal expres-
sion (TIMEX3). An example of the annotated event
and temporal expression is shown in Figure 1.
The sentence is brought from wsj 0292.tml in the
Timebank corpus.
There is no modal word in the sentence, so the
attribute modality does not appear.
We use the complete set of the TimeML rela-
tions, which has 14 types of temporal relations in-
cluding BEFORE, AFTER, IMMEDIATELY BEFORE, IM-
MEDIATELY AFTER, INCLUDES, IS INCLUDED, DUR-
ING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY. However,
in TempEval-3, SIMULTANEOUS and IDENTITY are
regarded as the same relation type, so we change
all IDENTITY relations into SIMULTANEOUS.
Given the example mentioned above, the tem-
poral relation is annotated as shown in the last line
of Figure 1. From the annotated relation, the event
rose (e30) happens DURING the temporal expres-
sion the first nine months (t88).
7
Feature E-E E-T Description
Event attributes
Class X X
All attributes associated with events. The ex-
planation of each attribute can be found in
(Pustejovsky et al., 2005).
Tense X X
Aspect X X
Modality X X
Polarity X X
Timex attributes
Type X
All attributes associated with temporal ex-
pressions. The explanation of each attribute
can be found in (Pustejovsky et al., 2005).
Value X
FunctionInDocument X
TemporalFunction X
Morphosyntactic information
Words X X
Words, POS, lemmas within a window be-
fore/after event words extracted using Stan-
ford coreNLP (Stanford NLP Group, 2012)
Part of speech tags X X
Lemmas X X
Lexical semantic information
Synonyms of event word tokens X X
WordNet lexical database (Fellbaum, 1998)
Synonyms of temporal expressions X
Event-Event information
Class match X
Details are described in (Chambers et al.,
2007)
Tense match X
Aspect match X
Class bigram X
Tense bigram X
Aspect bigram X
Same sentence X X True if both temporal entities are in the same
sentence
Deep syntactic information
Phrase structure X X Deep syntactic information extracted from
Enju Parser (Miyao and Tsujii, 2008). The
details are described in (Laokulrat et al.,
2013)
Predicate-argument structure X X
Table 1: Local features
Feature E-E E-T Description
Adjacent nodes and links X X
The details are described in Subsection 3.2
Other paths X X
Generalized paths X X
(E,V,E) tuples X X
(V,E,V) tuples X X
Table 2: Timegraph features
8
Figure 2: path length ? 2
Figure 3: path length ? 3
3 Proposed method
Rather than using only local information on
two entities in a TLINK, our goal is to exploit
more global information which can be extracted
from a document?s timegraph. Our motivation
is that temporal relations of nearby TLINKs in
a timegraph provide very useful information for
predicting the relation type of a given TLINK. For
instance, consider the following sentence and the
temporal connectivity shown in Figure 2.
About 500 people attended (e1) a Sunday
night memorial for the Buffalo-area physician
who performed abortions, one year (t1) after he
was killed (e2) by a sniper?s bullet.
It can be seen that the relation between e1 and
t1 and the relation between t1 and e2 are useful
for predicting the relation between e1 and e2.
Another more-complicated example is shown
below with temporal connectivity in Figure 3.
?The Congress of the United States is af-
fording(e1) Elian Gonzalez what INS and this
administration has not, which is his legal right
and his right to due process,? said(e2) Jorge
Mas Santos, chairman of the Cuban American
National Foundation. ?This gives(e3) him the
protection that he will not be repatriated(e4) to
Cuba between now and Feb. 10.?
Figure 5: Local pairwise classification. Each
TLINK is classified separately.
Figure 6: Timegraph constructed from a docu-
ment?s TLINKs
Again, the relation between e4 and e3
can be inferred from the nearby relations,
i.e., (1) e4 AFTER e2 and e2 AFTER e1
imply e4 AFTER e1, (2) e4 AFTER e1 and
e1 SIMULTANEOUS e3 imply e4 AFTER e3.
3.1 Overview of our framework
Our framework is based on the stacked learn-
ing method (Wolpert, 1992), which employs two
stages of classification as illustrated in Figure 4.
3.1.1 Local pairwise model
In a local pairwise model, temporal relation clas-
sification is done by considering only a given pair
of temporal entities at a time as illustrated in Fig-
ure 5. We use a supervised machine learning ap-
proach and employ the basic feature set that can
be easily extracted from the document?s text and
the set of features proposed in our previous work
(Laokulrat et al., 2013), which utilizes deep syn-
tactic information, as baselines. The local features
at different linguistic levels are listed in Table 1.
Two classifiers are used: one for Event-Event
TLINKs (E-E), and the other for Event-Time
TLINKs (E-T).
3.1.2 Stacked learning
Stacked learning is a machine learning method
that enables the learner to be aware of the labels
of nearby examples.
9
Figure 4: Stacked learning. The output from the first stage is treated as features for the second stage.
The final output is predicted using label information of nearby TLINKs.
The first stage, as shown in Figure 5, uses the
local classifiers and predicts the relation types of
all TLINKs. In the second stage, the document?s
timegraph is constructed and the output from the
first stage is associated with TLINKs in the graph.
The classifiers in the second stage use the infor-
mation from the nearby TLINKs and predict the
final output. We exploit features extracted from
the documents? timegraphs, as listed in Section 3.2
in the second stage of the stacked learning.
An example of a document?s timegraph is
shown in Figure 6.
3.2 Timegraph features
We treat timegraphs as directed graphs and double
the number of edges by adding new edges with
opposite relation types/directions to every existing
edge. For example, if the graph contains an edge
e1 BEFORE e2, we add a new edge e2 AFTER e1.
Our proposed timegraph features are described
below.
? Adjacent nodes and links
The features are the concatenation of the di-
rections to the adjacent links to the pair of en-
tities, the relation types of the links, and the
information on the adjacent nodes, i.e., word
tokens, part of speech tags, lemmas. For ex-
ample, the features for predicting the relation
between e1 and e2 in Figure 6 are SRC OUT-
IS INCLUDED-(Type of t0), DEST IN-BEFORE-
(Type of t0), and so on.
In this work, only Type of temporal expres-
sion (an attribute given in the Timebank cor-
pus), Tense and Part-of-speech tag are ap-
plied but other attributes could also be used.
? Other paths
Paths with certain path lengths (in this work,
2 ? path length ? 4) between the temporal
entities are used as features. The paths must
not contain cycles. For example, the path
features of the relation between e1 and e2
are IS INCLUDED-BEFORE and SIMULTANEOUS-
BEFORE-BEFORE.
? Generalized paths
A generalized version of the path features,
e.g., the IS INCLUDED-BEFORE path is gener-
alized to *-BEFORE and IS INCLUDED-*.
? (E,V,E) tuples
The (E,V,E) tuples of the edges and ver-
tices on the path are used as features, e.g.,
IS INCLUDED (Type of t0) BEFORE.
? (V,E,V) tuples
The (V,E,V) tuples of the edges and vertices
on the path are used as features, e.g., (Tense
of e1) IS INCLUDED (Type of t0) and (Type of
t0) BEFORE (Tense of e2).
The summary of the timegraph features is
shown in Table 2.
4 Relation inference and time-time
connection
We call TLINKs that have more than one path be-
tween the temporal entities ?multi-path TLINKs?.
The coverage of the multi-path TLINKs is pre-
sented in Table 3. The annotated entities in
10
the Timebank corpus create loosely connected
timegraphs as we can see from the table that only
5.65% of all the annotated TLINKs have multiple
paths between given pairs of temporal entities.
Since most of the timegraph features are only
applicable for multi-path TLINKs, it is important
to have dense timegraphs. In order to increase
the numbers of connections, we employ two ap-
proaches: relation inference and time-time con-
nection.
4.1 Relation inference
We create new E-E and E-T connections between
entities in a timegraph by following a set of infer-
ence rules. For example, if e1 happens AFTER e2
and e2 happens IMMEDIATELY AFTER e3, then we
infer a new temporal relation ?e1 happens AFTER
e3?. In this paper, we add a new connection only
when the inference gives only one type of tem-
poral relation as a result from the relation infer-
ence. Figure 7b shows the timegraph after adding
new inference relations to the original timegraph
in Figure 7a.
4.2 Time-time connection
As with Chambers et al. (2007) and Tatu and
Srikanth (2008), we also create new connections
between time entities in a timegraph by applying
some rules to normalized values of time entities
provided in the corpus.
Figure 7c shows the timegraph after adding a
time-time link and new inference relations to the
original timegraph in Figure 7a. When the nor-
malized value of t2 is more than the value of t1,
a TLINK with the relation type AFTER is added
between them. After that, as introduced in Sub-
section 4.2, new inference relations (e1-e2, e1-e3,
e2-e3) are added.
As the number of relations grows too large af-
ter performing time-time connection and infer-
ence relation recursively, we limited the number of
TLINKs for each document?s timegraph to 10,000
relations. The total number of TLINKs for all doc-
uments in the corpus is presented in Table 4. The
first row is the number of the human-annotated re-
lations. The second and third rows show the to-
tal number after performing relation inference and
time-time connection.
(a) Original timegraph
(b) After relation inference. Two relations (e1-e2, e1-e3)
are added.
after 
after 
(c) After time-time connection (t1-t2) and relation inference.
Three relations (e1-e2, e1-e3, e2-e3) are added.
Figure 7: Increasing number of TLINKs
No. of TLINKs E-E E-T Total
All TLINKs 2,520 2,463 4,983
Multi-path TLINKs 119 163 282
Percentage 4.72 6.62 5.65
Table 3: Coverage of multi-path TLINKs
11
Approach
Graph-based evaluation
F1(%) P(%) R(%)
Local - baseline features 58.15 58.17 58.13
Local - baseline + deep features 59.45 59.48 59.42
Stacked - baseline features 58.33 58.37 58.29
Stacked (inference) - baseline features 58.30 58.32 58.27
Stacked (inference, time-time) - baseline features 58.29 58.31 58.27
Stacked - baseline + deep features 59.55 59.51 59.58
Stacked (inference) - baseline + deep features 59.55 59.57 59.52
Stacked (inference, time-time) - baseline + deep features 59.61 59.63 59.58
Table 5: Ten-fold cross validation results on the training set
No. of TLINKs Total
Annotated 4,983
+Inference 24,788
+Inference + time-time connection 87,992
Table 4: Number of TLINKs in the Timebank cor-
pus
5 Evaluation
For the baselines and both stages of the stacked
learning, we have used the LIBLINEAR (Fan
et al., 2008) and configured it to work as L2-
regularized logistic regression classifiers.
We trained our models on the Timebank corpus,
introduced in Subsection 2.1, which was provided
by the TempEval-3 organiser. The corpus contains
183 newswire articles in total.
5.1 Results on the training data
The performance analysis is performed based on
10-fold cross validation over the training data. The
classification F1 score improves by 0.18 pp and
0.16 pp compared to the local pairwise models
with/without deep syntactic features.
We evaluated the system using a graph-based
evaluation metric proposed by UzZaman and
Allen (2011). Table 5 shows the classification
accuracy over the training set using graph-based
evaluation.
The stacked model affected the relation classi-
fication output of the local model, changing the
relation types of 390 (out of 2520) E-E TLINKs
and 169 (out of 2463) E-T TLINKs.
5.2 Comparison with the state of the art
We compared our system to that of Yoshikawa
et al. (2009) which uses global information to
improve the accuracy of temporal relation clas-
sification. Their system was evaluated based on
TempEval-2?s rules and data set (Verhagen et al.,
2007), in which the relation types were reduced to
six relations: BEFORE, OVERLAP, AFTER, BEFORE-
OR-OVERLAP, OVERLAP-OR-AFTER, and VAGUE. The
evaluation was done using 10-fold cross validation
over the same data set as that of their reported re-
sults.
According to TempEval-2?s rules, there are
three tasks as follows:
? Task A: Temporal relations between events
and all time expressions appearing in the
same sentence.
? Task B: Temporal relations between events
and the DCT.
? Task C: Temporal relations betweeen main
verbs of adjacent sentences.
The number of TLINKs annotated by the orga-
nizer, after relation inference, and after time-time
connection for each task is summarized in Table
7. Table 8 shows the number of TLINKs after per-
forming relation inference and time-time connec-
tion.
As shown in Table 6, our system can achieve
better results in task B and C even without deep
syntactic features but performs worse than their
system in task A. Compared to the baselines, the
overall improvement is statistically significant* (p
< 10
?4
, McNemar?s test, two-tailed) without deep
syntactic features and gets more statistically sig-
nificant** (p< 10
?5
, McNemar?s test, two-tailed)
when applying deep syntactic information to the
system. The overall result has about 1.4 pp higher
accuracy than the result from their global model.
Note that Yoshikawa et al. (2009) did not apply
deep syntactic features in their system.
12
Approach Task A Task B Task C Overall
Yoshikawa et al. (2009) (local) 61.3 78.9 53.3 66.7
Yoshikawa et al. (2009) (global) 66.2 79.9 55.2 68.9
Our system (local) - baseline features 59.9 80.3 58.5 68.5
Our system (local) - baseline + deep features 62.1 80.3 58.4 69.0
Our system (stacked) - baseline features 59.5 79.9 58.5 68.2
Our system (stacked, inference) - baseline features 59.9 80.0 59.7 68.7
Our system (stacked, inference, time-time) - baseline fea-
tures
63.8 80.0 58.9 69.5*
Our system (stacked) - baseline + deep features 63.5 79.4 58.0 68.9
Our system (stacked, inference) - baseline + deep features 63.7 80.3 59.2 69.7
Our system (stacked, inference, time-time) - baseline +
deep features
65.9 80.5 58.9 70.3**
Table 6: Comparison of the stacked model to the state of the art and to our local model (F1 score(%))
No. of TLINKs Task A Task B Task C
Annotated 1,490 2,556 1,744
Table 7: TempEval-2 data set
No. of TLINKs Total
Annotated 5,970
+Inference 156,654
+Inference + time-time connection 167,875
Table 8: Number of relations in TempEval-2 data
set
The stacked model enhances the classification
accuracy of task A when timegraphs are dense
enough. Deep syntactic features can be extracted
only when temporal entities are in the same sen-
tences so they improve the model for task A
(event-time pairs in the same sentences) but these
features clearly lower the accuracy of task C, since
there are very few event-event pairs that appear
in the same sentences (and break the definition
of task C). This is probably because the sparse-
ness of the deep features degrades the performance
in task C. Moreover, these features do not help
task B in the local model because we cannot ex-
tract any deep syntactic features from TLINKs be-
tween events and DCT. However, they contribute
slightly to the improvement in the stacked model
since deep syntactic features increase the accuracy
of the prediction of task A in the first stage of the
stacked model. As a result, timegraph features ex-
tracted from the output of the first stage are better
than those extracted from the local model trained
on only baseline features.
6 Discussion
As we can see from Table 5 and 6, although
deep syntactic features can improve the classifi-
cation accuracy significantly, some additional pre-
processing is required. Moreover, deep parsers
are not able to parse sentences in some specific
domains. Thus, sometimes it is not practical to
use this kind of features in real-world temporal
relation classification problems. By applying the
stacked learning approach to the temporal relation
classification task, the system with only baseline
features is able to achieve good classification re-
sults compared to the system with deep syntactic
features.
Again, from Table 5 and 6, the inference and
time-time connection, described in Section 4,
sometimes degrade the performance. This is pre-
sumably because the number of features increases
severely as the number of TLINKs increased.
The stacked model also has another advantage
that it is easy to build and does not consume too
much training time compared to MLNs used by
Yoshikawa et al. (2009), which are, in general,
computationally expensive and infeasible for large
training sets.
7 Conclusion
In this paper, we present an approach for exploit-
ing timegraph features in the temporal relation
classification task. We employ the stacked learn-
ing approach to make use of information obtained
from nearby entities in timegraphs. The results
13
show that our system can outperform the state-of-
the-art system and achieve good accuracy by us-
ing only baseline features. We also apply the rela-
tion inference rules and the time-time connection
to tackle the timegraphs? sparseness problem.
In future work, we hope to improve the classi-
fication performance by making use of probability
values of prediction results obtained from the first
stage of the stacked learning and applying the full
set of inference relations to the system.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for their insightful comments and sugges-
tions, which were helpful in improving the quality
of the paper.
References
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. In In-
formation Processing & Management, Volume 46,
Issue 1, January 2010, pages 89?109.
Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
HLT/EMNLP 2005, pages 628?635.
Nathanael Chambers, Shan Wang and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In ACL 2007, pages 173?176.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In EMNLP 2008, pages 698?706.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka and Takashi Chikayama. 2013. UTTime:
Temporal relation classification using deep syntac-
tic features. In SemEval 2013, pages 89?92.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong
Min Lee and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In ACL 2006,
pages 753?760.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time revisited. In Computational Intelligence 6,
pages 108?118.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. In Com-
putational Linguistics. 34(1). pages 35?80, MIT
Press.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
dew See, Rob Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus.
In Proceedings of Corpus Linguistics 2003 (March
2003), pages 545?557.
James Pustejovsky, Robert Ingria, Roser Saur??, Jos?e
Casta?no, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz and Inderjeet Mani. 2005. The
specification language TimeML. In The Language
of Time: A reader, pages 545?557.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL 2002, pages 41?47.
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING 2008, pages 857?864.
Naushad UzZaman and James F. Allen. 2011. Tempo-
ral evaluation. In ACL 2011, pages 351?356.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TempEval-3:
Evaluating time expressions, events, and temporal
relations. In SemEval 2013, pages 2?9.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In SemEval 2007, pages 75?
80.
David H. Wolpert. 1992. Stacked generalization. In
Neural Networks, volume 5, pages 241?259.
Katsumasa Yoshikawa, Sebastian Riedel ,Masayuki
Asahara and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with Markov Logic. In
ACL 2009, pages 405?413.
14
