BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 96?97,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Natural Language Processing to Classify Suicide Notes
John P. Pestian*, Pawel Matykiewicz, Jacqueline Grupp-Phelan,
Sarah Arszman Lavanier, Jennifer Combs, and Robert Kowatch
Cincinnati Children?s Hospital Medical Center
Cincinnati, OH 45220, USA
john.pestian@cchmc.org
Abstract
We hypothesize that machine-learning algo-
rithms (MLA) can classify completer and
simulated suicide notes as well as mental
health professionals (MHP). Five MHPs
classified 66 simulated or completer notes;
MLAs were used for the same task. Results:
MHPs were accurate 71% of the time; using
the sequential minimization optimization
algorithm (SMO) MLAs were accurate 78%
of the time. There was no significant differ-
ence between the MLA and MPH classifiers.
This is an important first step in developing
an evidence based suicide predictor for
emergency department use.
1 Problem
Suicide is the third leading cause of death in
adolescents and a leading cause of death in the
United States1. Those who attempt suicide usually
arrive at the Emergency Department seeking help.
These individuals are at risk for a repeated attempt,
that may lead to a completed suicide2. We know
of no evidence-based risk assessment tool for pre-
dicting repeated suicide attempts. Thus, Emergency
Medicine clinicians are often left to manage suicidal
patients by clinical judgment alone. This research
focuses on the initial stage for constructing such
an evidence based tool, the Psychache3 Index.
Our efforts herein posit that suicide notes are an
artifact of a victim?s thoughts and that the thoughts
between completers and attempters are different.
Using natural language processing we attempt to
distinguish between completer notes and notes that
have been simulated by individuals who match the
profile of the completer. Understanding how to
optimize classification methods between these types
of notes prepares us for future work that can include
clinical and biological factors.
2 Methods
Suicidal patients are classified into three categories:
ideators ?those who think about committing sui-
cide, attempters ?those who attempt suicide, and
completers ?those who complete suicide. This re-
search focuses on the completers and a group of in-
dividuals called simulators. These simulators were
matched to completers by age, gender and socioe-
conomic status and asked to write a suicide note4.
Suicide notes from 33 completers and 33 simulators
were annotated with linguistic characteristics using
a perl-program with the EN:Lingua:Tagger module.
Emotional characteristics were annotated by assign-
ing terms in the note to a suicide-emotion ontology
that was developed from a meta analysis of 2,166
suicide related manuscripts and validated with ex-
pert opinion. This ontology includes such classes
as: affection, anger, depression, and worthlessness.
Each class had multiple concepts, i.e, affection?
love, concern for others, and gratitude. Three MHPs
read each note and tagged emotion-words found in
the notes with the appropriate classes and concepts.
Analysis of variance between structures was con-
ducted to insure that there actually was a difference
that could be detected. Emotional annotations were
used for machine-learning.
96
We then tested the hypothesis that MLAs could
distinguish between completer and simulated notes
as well as MHPs. Copies of the notes were given
to five MHPs who classified them as either written
by a completer or an simulator. MLA feature space
was defined by matrix of selected characteristics
from four sources: words, parts of speech, concepts,
and readability indexes. Collinearity was eliminated
by removing highly correlated features. The final
feature space included: specific words (such as
?love?, ?life?, ?no?), specific parts of speech (such
as, personal pronouns, verbs) Kincaid readability
index and emotional concepts (such as anger,
and hopelessness). We then tested the following
algorithms? ability to distinguish between completer
and simulator notes: decision trees - J48, C4.5,
LMT, DecisionStump, M5P; classification rules -
JRip, M5, OneR, PART; function models - SMO,
logistic builds, multinomial logistic regression,
linear regression; lazy learners and meta learners5.
3 Results
A significant difference was found between the
linguistic and emotional characteristics of the notes.
Linguistic differences (completer/simulated): word
count 120/66 p=0.007, verbs 25/13 p=0.012, nouns
28/12 p=0.0001, and prepositions 20/10 p=0.005.
This difference justified testing the classification
hypothesis. Emotionally, completers gave away
their possessions 20% of the time, simulators, never
did. Mental health experts accurately classified the
notes 71% of the time. The MLAs were accurate
60-79% of the time with SMO giving the highest
results when the word count, part-of-speech, and
readability vectors were included. Performance
weakened when the emotional vector was included,
yet the emotional vector was the primary source of
data for the MHPs.
4 Conclusion
Machine learning methods for classifying suicide
and non-suicide notes are promising. Future efforts
to represent the thoughts of the suicidal patient will
require larger sample sizes, inclusion of attempters
response to open-ended questions, biological and
clinical characteristics.
5 Acknowledgements
We acknowledge Drs. AA Leenaars, ES Shneidman,
the divisions of Biomedical Informatics, Emergency
Medicine and Psychiatry at Cincinnati Children?s
Hospital Medical Center, University of Cincinnati
and Ohio Third Frontier program for their generous
support of this work.
References:
[1] Jeffrey A Bridge, Tina R Goldstein, and David A Brent.
Adolescent suicide and suicidal behavior. J Child Psychol
Psychiatry, 47(3-4):372?394, 2006.
[2] P M Lewinsohn, P Rohde, and J R Seeley. Psychosocial
risk factors for future adolescent suicide attempts. J
Consult Clin Psychol, 62(2):297?305, 1994.
[3] E S Shneidman. Suicide as psychache. J Nerv Ment Dis,
181(3):145?147, 1993.
[4] ES Shneidman and NL Farberow. Clues to Suicide.
McGraw Hill Paperbacks, 1957.
[5] I.H. Witten and E. Frank. Data Mining: Practical
Machine Learning Tools ad Techniques. Morgan
Kaufman, 2nd edition, 2005.
97
Proceedings of the Workshop on BioNLP, pages 179?184,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Clustering semantic spaces of suicide notes and newsgroups articles
P. Matykiewicz1,2, W. Duch2, J. Pestian1
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Nicolaus Copernicus University, Torun?, Poland.
Abstract
Historically, suicide risk assessment has re-
lied on question-and-answer type tools. These
tools, built on psychometric advances, are
widely used because of availability. Yet there
is no known tool based on biologic and cogni-
tive evidence. This absence often cause a vex-
ing clinical problem for clinicians who ques-
tion the value of the result as time passes. The
purpose of this paper is to describe one exper-
iment in a series of experiments to develop a
tool that combines Biological Markers (Bm)
with Thought Markers (Tm), and use machine
learning to compute a real-time index for as-
sessing the likelihood repeated suicide attempt
in the next six-months. For this study we fo-
cus using unsupervised machine learning to
distinguish between actual suicide notes and
newsgroups. This is important because it gives
us insight into how well these methods dis-
criminate between real notes and general con-
versation.
1 Introduction
It is estimated that each year 800,000 die by suicide
worldwide (World Health Organization, 2001). In
the United States, suicide ranks second as the lead-
ing cause of death among 25-34 year-olds and the
third leading cause of death among 15-25 year-olds
(Kung et al, 2008). The challenge for those who
care for suicide attempters, such as an Emergency
Medicine clinicians, is to assess the likelihood of an-
other attempt, a more lethal one. We believe to fully
asses this risk a tool must be developed that mea-
sures both the biological and cognitive state of the
patient. Such a tool will include Biological Mark-
ers (Bm): measured by the concentration of cer-
tain biochemical markers, Thought Markers (Tm):
measured by artifacts of thought that have been re-
duced to writing or transcribe speech, and Clini-
cal Markers (Cm): measured by traditional clinical
risk factors. In this study we focus on the Tm be-
cause of BioNLP?s important role. Here, we employ
machine-learning analysis to examine suicide notes
and how these notes compare to newsgroups. This
is one experiment in a series of experiments that are
intended to provide insight into how best to apply
linguistic tools when responding to suicidal patients.
To gain insight into the suicidal mind, researchers
have suggested empirically analyzing national mor-
tality statistics, psychological autopsies, nonfatal
suicide attempts and documents such as suicide
notes (Shneidman and Farberow, 1957; Maris,
1981). Most suicide notes analysis has focused
on classification and theoretical-conceptual analysis.
Content analysis has been limited to extracting ex-
plicit information from a suicide note, e.g., length of
the message, words, and parts of speech (Ogilvie et
al., 1969). Classification analysis uses data such as
age, sex, marital status, educational level, employ-
ment status and mental disorder (Ho et al, 1998;
Girdhar et al, 2004; Chavez et al, 2006; Demirel
et al, 2007). Only a very few studies have utilized
theoretical-conceptual analysis , despite the asser-
tion in the first formal study of suicide notes (Shnei-
dman and Farberow, 1957) that such an analysis has
much promise. So, the inconclusive nature of the
methods of analysis has limited their application to
patient care.
179
Our own research has taken a different approach.
In particular we first wanted to determine if mod-
ern machine learning methods could be applied to
free-text from those who committed suicide. Our
first experiment focused on the the ability of ma-
chine learning to distinguish between real suicide
notes and elicited suicide notes as well as mental
health professionals. This is an important question
since all current care is based on a mental health pro-
fession?s interpretation. Our findings showed that
mental health professionals accurately selected gen-
uine suicide notes 50% of the time and the super-
vised machine learning methods were accurate 78%
(Pestian et al, 2008). In this study we shift from
supervised to unsupervised machine learning meth-
ods. Even though these methods have rich history
we know of no research that has applied them to
suicide notes. Our rationale for this study, then, is
that since our ultimate goal is to create a Suicide
Risk Index that incorporates biological and thought
markers it is important to determine if unsupervised
methods can distinguish between suicidal and non-
suicidal writings. To conduct this research we de-
veloped a corpus of over 800 suicide notes from in-
dividuals who had committed suicide, as opposed to
those who attempted or ideated about suicide. This
is an important contribution and, as far as we know,
it is the largest ever developed. It spans 70 years of
notes, and now includes multiple languages. Details
of this corpus are described below. We also created
a corpus of data from various newsgroups that acted
as non-suicidal writings. These corpora were used
to conduct the analysis. The sections below describe
the cluster analysis process and results.
2 Data
Suicide Notes Corpus
Data for the suicide note database were collected
from around the United States. They were either
in a hand written or typed written form. Once the
note was acquired it was scanned into the database.
Optical character recognition was attempted on the
typed written notes, but not accurate, so the notes
were read from the scanned version and type into the
database exactly as seen. A second person reviewed
what was typed. There were limitation in collecting
deceased demographics. The table 1 provides vari-
ous descriptive statistics.
Newsgroup Corpus
Newsgroup data was selected because it was conve-
nient and as close to normal discourse as we could
find. We understood that and ideal comparison
group would be composed of Internet blogs or e-
mails that were written by suicide ideators. True,
a Google query of ?suicide blog? yields millions
of response, a review of many of these responses
shows that the data are of little use for this analy-
sis. In our opinion, the next suitable corpora was
found in a 20 newsgroup collection from the Uni-
versity of California in Irvine (UCI) machine learn-
ing repository1 . Most of the newsgroups have no
relevance to suicide notes. Since our hypothesis
is that unsupervised learning methods can tell the
difference between suicidal and non-suicidal writ-
ing we selected discussions that we believed may
have some similarity to suicide writings. This se-
lection was based on reviewing the newsgroups
with experts. We had conjectured that if an unsu-
pervised method could distinguish between similar
clusters those methods could distinguish between
dissimilar clusters. The newsgroups ultimately se-
lected were talk.politics.guns, talk.politics.mideast,
talk.politics.misc, talk.religion.misc. Each news-
group contains 1000 articles (newsgroup postings).
Headers and quotes from other postings were re-
moved.
3 Methods
Basic statistics are calculated using variables ex-
tracted by Linguistic Inquiry and Word Count ver-
sion 2007 software (LIWC2007) (Chung and Pen-
nebaker, 2007). J. W. Pennebaker, C. K. Chung, M.
Ireland, A. Gonzales, and R. J. Booth created an an-
notated dictionary. Each word in the dictionary is
assigned to at least one of the following high level
category: linguistic process, psychological process,
personal concern, or spoken category. These cat-
egories provide an efficient and effective method
for studying the various emotional, cognitive, and
structural components present in individuals? verbal
and written speech samples (Chung and Pennebaker,
2007; Pennebaker et al, 2001). Here it is used to
analyze differences between suicide notes and news-
1http://archive.ics.uci.edu/ml/
180
group articles.
Feature space was prepared using open source al-
gorithms available in Perl language2 . First, Brian
Duggan spell checking software that uses aspell li-
brary was used (Text::SpellChecker module3). Then,
tokenizer created by Aaron Coburn was used (Lin-
gua::EN::Tagger module2) to extract words was ap-
plied. After that, words were filtered with 319 ele-
ment stop word list 4. Next, the Richardson/Franz
English stemmer was included in the pre-processing
stage (Lingua::Stem module2). Features that ap-
peared in less than 10 documents or in more than 500
documents were removed. Documents that had less
than 10 features or more than 500 were removed.
Finally, columns and rows were normalized to have
unitary lengths. These last steps of pre-processing
are used to reduce outliers.
Calculations are done using open source software
called R5. Clustering is done with the following al-
gorithms: expectation maximization (EM) (Witten
and Frank, 2000), simple k-means with euclidean
distance (SKM) (Witten and Frank, 2000), and
sequential information bottleneck algorithm (sIB)
(Slonim et al, 2002). The last approach has been
shown to work well work well when clustering doc-
uments. Specificity, sensitivity and F1 measure are
used as performance measures (Rijsbergen, 1979).
Multidimensional scaling with euclidean distance
measures is used for visualization purposes (Cox
and Cox, 1994).
To extract features that represent each cluster,
Pearson correlation coefficient is used. The correla-
tion coefficient r is calculated between each feature
and each cluster separately r(wi, cj) where wi is ith
word and cj is jth cluster. N best features with the
highest values for each cluster are selected as most
representative.
4 Results
Descriptive statistics for the data sets are listed in
table 1. It shows syntactic differences between lan-
guage use in suicide notes and newsgroups when
Lingua::EN::Tagger is used.
2http://www.perl.org
3http://search.cpan.org
4http://www.dcs.gla.ac.uk/idom/ir resources/
/linguistic utils/stop words
5http://www.r-project.org
Table 1: Descriptive statistics of suicide note corpus and
newsgroups.
suicide
corpus
newsgroups
Sample Size 866 4000 (1000
per group)
Collection Years 1945-2009 1992-1993
Avg tokens per record
(SD)
105 (154) 243 (582)
Range of tokens per
record
1-1837 0-11024
Average (SD) nouns 25.21 (34.81) 77.19
(181.63)
Average (SD) pronouns 16.58 (26.69) 18.05 (63.18)
Average (SD) verbs 21.07 (32.82) 41.31
(109.23)
Average (SD) adjec-
tives
6.43 (9.81) 16.92 (36.45)
Table 2 summarizes information about the lin-
guistic and psychological processes of the data.
The idea of ?process? is derived from the Lin-
guistic Inquiry and Word Count (LIWC2007) soft-
ware (Chung and Pennebaker, 2007). This software
conducts traditional natural language processing by
placing various word into categories. For example,
sixltrs includes words that are at least six letters in
length. A full description of this software, dictio-
naries, reliability and validity tests can be found on
LIWC?s website. 6. Table 2 shows that suicide notes
are, in many ways, different than normal text. For
our study this provides inspiration for continued re-
search.
Table 2: Mean and standard deviation in linguistic and
psychological processes. Selected categories with small-
est p-values (<0.0001) are shown.
suicide guns mideast politics religion
artcl 3.31 (2.79) 7.80 (3.52) 7.37 (3.34) 7.21 (3.40) 7.07 (3.51)
sixltrs 14.20 (7.34) 21.22 (6.32) 23.24 (7.03) 22.41 (7.13) 21.37 (7.87)
prnoun 16.75 (6.82) 11.96 (5.15) 10.64 (4.92) 11.77 (5.18) 13.21 (5.76)
prepos 10.61 (4.35) 12.13 (3.97) 12.89 (3.89) 12.21 (3.97) 11.75 (4.07)
verb 14.69 (5.99) 12.75 (4.72) 11.54 (4.74) 12.72 (4.63) 13.54 (4.97)
biolog 2.70 (3.04) 0.93 (1.27) 0.85 (1.50) 1.59 (2.08) 1.10 (1.75)
affctiv 7.71 (5.39) 4.83 (2.87) 4.77 (3.45) 4.90 (3.18) 5.10 (3.93)
cognitv 12.68 (5.76) 16.14 (5.93) 14.72 (5.62) 16.00 (5.49) 17.14 (6.17)
social 10.45 (5.86) 8.10 (4.20) 8.43 (4.71) 8.76 (4.37) 9.06 (5.17)
The four newsgroup data sets are combined
as follows: talk.politics.guns + suicide notes
= guns, talk.politics.mideast + suicide notes =
mideast, talk.politics.misc + suicide notes = politics,
6http://www.liwc.net/liwcdescription.php#index1
181
talk.religion.misc + suicide notes = religion. Each
data set contained 1866 documents before document
and feature selection is applied. Table 3 has final
number of features while table 4 has final number of
documents. In general sIB clustering algorithm per-
formed best for all data sets with respect to F1 mea-
sure (mean = 0.976, sd = 0.008). The average score
also did not change when the number of clusters var-
ied from two to six (mean = 0.973, sd = 0.012). Per-
formance of k-means and expectation maximization
algorithm was much worse. If number of clusters
was varied between two and six for different data
sets the algorithms achieved F1 measure 0.146 lower
than sIB (SKM mean = 0.831, sd = 0.279, EM mean
= 0.824, sd = 0.219). Table 3 summarizes perfor-
mance of best algorithms for each data set if two
clusters are chosen.
Table 3: Best clustering algorithms for each newsgroup
when clustered with suicide notes in case of two clus-
ters (alg = clustering algorithm, sens = sensitivity, spec
= specificity, F1 = F1 measure, #f = number of features,
sIB = sequential information bottleneck, SKM = simple
k-means).
dataset al sens spec F1 #f
guns sIB .9689 .9834 .9721 1658
mideast sIB .9837 .9942 .9877 2023
politics SKM .9705 .9889 .9769 1694
religion sIB .9787 .9700 .9692 1553
If the desired number of clusters is increased to
four then two major sub-groups are discovered in
suicide notes: emotional (represented by words like:
love, forgive, hope, and want) and non-emotional
(represented by words like: check, bank, and no-
tify). Example of the first type of note might be
(suicide note was annonymized and misspellings left
unchanged):
Jane I am bitterly sorry for what I have done to
you. Please try to forgive me. I can?t live with-
out you and you don?t want me. I can?t blame you
though. But I love you very much. I didn?t act like it
but I did and still do. Please try to be happy, Jane.
That is all I ask. I try hope for the best for you and
I guess that is all there is for me to say. Good by.
John Johnson. Please mail this to Mom. Mrs. Jane
Johnson. Cincinnati, OH.
Example of a non-emotional suicide note might be:
There is no use living in pains. That arthritis and
hardening of the arteries are too much for me. There
are two hundred and five dollars in the bank, and
here are fifty- five dollars and eight cents. I hope that
will be enough for my funeral. You have to notify the
Old Age Assistance Board. Phone - 99999.
Table 4 shows best five ranked features for each clus-
ter for each data set according to correlation coeffi-
cient CC . Features are in the order of rank so that
feature with the highest CC is first. Even though
that we use different newsgroups as control groups
same sub-groups of suicide notes are discovered.
sIB is the most stable and best performing algorithm
in this experiment so it was used to discover those
clusters. Stemmed word that appear in best five
ranked features in at least three data sets are marked
bold.
Figures 1, 2, 3, and 4 show high-dimensional doc-
ument/stemmed word feature space projected on a
two dimensional plane using multidimensional scal-
ing (MDS) initialized by principal component analy-
sis. Each figure has different rotation but the shapes
are similar. In addition MDS shows very little mix-
ing of suicide notes and newsgroups which is also
explained by results in the table 3.
Figure 1: MDS showing suicide notes and
talk.politics.guns articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
182
Table 4: Best five features when four clusters are created
by the sIB algorithm (#c = cluster number, #a = number
of newsgroup articles in a cluster, #s = number of suicide
notes in a cluster). Stemmed word that appear in best five
ranked features in at least three data sets are marked bold.
dataset #c stemmed words #a #s
guns 1 address, bank, bond, notifi,
testam
28 204
guns 2 clinton, fbi, foreign, jim,
spea
318 2
guns 3 forgiv, god, hope, love,
want
4 381
guns 4 crime, firearm, gun, law,
weapon
541 8
mideast 1 appressian, armenia, arme-
nian, ohanu, proceed
464 5
mideast 2 arab, congress, isra, israel,
jew
379 4
mideast 3 bank, check, funer, insur,
testam
10 233
mideast 4 forgiv, good, hope, love,
want
2 355
politics 1 compound, disclaim, fbi,
govern, major
593 12
politics 2 clayton, cramer, optilink,
relat, uunet
274 1
politics 3 bank, box, check, funer,
notifi
11 258
politics 4 forgiv, good, hope, life,
love
11 330
religion 1 bank, bond, check, notifi,
paper
36 192
religion 2 frank, object, observ, the-
ori, valu
279 0
religion 3 activ, christian, jesu, ko-
resh, net
502 10
religion 4 forgiv, hope, love, sorri,
want
12 395
5 Conclusions
Our findings suggest that unsupervised methods can
distinguish between suicide notes and newsgroups,
our proxy for general discussion. This is important
because it is helpful in determining if NLP can be
useful when integrating thought markers with bio-
logical and clinical markers (f(Bm, Tm, Cm)). In
other words, can an NLP tools accurately distin-
guish between suicidal and normal thought markers
(T Sm 6= TNm )? Moreover these unsupervised meth-
ods have shown an ability to find sub-groups of sui-
cide notes even when other types of newsgroups are
present. In our analysis, one subgroup showed no
Figure 2: MDS showing suicide notes and
talk.politics.mideast articles (s character in the fig-
ure means suicide notes while a character depicts
newsgroup article, colors are used as cluster numbers).
Figure 3: MDS showing suicide notes and
talk.politics.misc articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
emotional content while the other was emotionally
charged. This finding is consistent with Tuckman?s,
1959 work that showed suicide notes fall into six
emotional categories: emotionally neutral, emotion-
ally positive, emotionally negative directed inward,
emotionally negative directed outward, emotionally
negative directed inward and outward (Tuckman et
al., 1959). The next step in developing a Suicide
Risk Index is to conduct a clinical trail in the Emer-
gency Department that will collect Bm, Tm, Cm
and test multiple methods for computing the Suicide
183
Figure 4: MDS showing suicide notes and
talk.religion.misc articles (s character in the figure
means suicide note while a character depicts newsgroup
article, colors are used as cluster numbers).
Risk Index.
References
A. Chavez, D. Paramo-Castillo, A. Leenaars, and
L. Leenaars. 2006. Suicide notes in mexico: What do
they tell us? Suicide and Life-Threatening Behavior,
36:709?715.
C.K. Chung and J.W. Pennebaker, 2007. The
psychological functions of function words, pages 343?
359. New York: Psychology Press.
T. F. Cox and M. A. A. Cox. 1994. Multidimensional
Scaling. Chapman and Hall.
B. Demirel, T. Akar, A. Sayin, S. Candansayar, and
A Leenaars. 2007. Farewell to the world: Sui-
cide notes from turkey. Suicide and Life-Threatening
Behavior, 38:123?128.
S. Girdhar, A. Leenaars, T.D. Dogra, L. Leenaars, and
G. Kumar. 2004. Suicide notes in india: what do they
tell us? Archives of Suicide Research, 8:179?185.
T. Ho, P. Yip, C. Chiu, and P. Halliday. 1998. Sui-
cide notes: what do they tell us? Acta Psychiatrica
Scandinavica, 98:467?473.
Hsiang-Ching Kung, Donna L. Hoyert, Jiaquan Xu, and
Sherry L. Murphy. 2008. Deaths: Final data for 2005.
National Vital Statistics Report, 56:1?121.
R. Maris. 1981. Pathways to suicide. John Hopkins
University Press, Baltimore, MD.
D Ogilvie, P. Stone, and E. Shneidman. 1969. Some
characteristics of genuine versus simulated suicide
notes. Bulletin of Suicidology, 1:17?26.
J. W. Pennebaker, M. E. Francis, and R. J. Booth. 2001.
Linguistic Inquiry and Word Count: LIWC. Lawrence
Erlbaum Associates, Mahwah, NJ, 2nd edition.
J. P. Pestian, P. Matykiewicz, J. Grupp-Phelan,
S. Arszman-Lavanier, J. Combs, and Robert Kowatch.
2008. Using natural language processing to clas-
sify suicide notes. In AMIA Annual Symposium
Proceedings, volume 2008. American Medical Infor-
matics Association.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA, USA.
E Shneidman and N Farberow. 1957. Clues to Suicide.
McGraw Hill Paperbacks.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.
Unsupervised document classification using sequential
information maximization. In Proceedings of the 25th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 129?
136.
Jacob Tuckman, Robert J. Kleiner, and Martha Lavell.
1959. Emotional content of suicide notes. Am J
Psychiatry, 116(1):59?63.
Ian H. Witten and Eibe Frank. 2000. Data mining:
practical machine learning tools and techniques with
Java implementations. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
World Health Organization, 2001. Burden of mental and
behavioral disorders, pages 19?45. World Health Or-
ganization, Geneva.
184
BioNLP 2007: Biological, translational, and clinical language processing, pages 97?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Shared Task Involving Multi-label Classification of Clinical Free Text
John P. Pestian1, Christopher Brew2, Pawe? Matykiewicz1,4,
DJ Hovermale2, Neil Johnson1, K. Bretonnel Cohen3,
W?odzis?aw Duch4
1Cincinnati Children?s Hospital Medical Center, University of Cincinnati,
2Ohio State University, Department of Linguistics,
3University of Colorado School of Medicine,
4Nicolaus Copernicus University, Torun?, Poland.
Abstract
This paper reports on a shared task involving
the assignment of ICD-9-CM codes to radi-
ology reports. Two features distinguished
this task from previous shared tasks in the
biomedical domain. One is that it resulted in
the first freely distributable corpus of fully
anonymized clinical text. This resource is
permanently available and will (we hope) fa-
cilitate future research. The other key fea-
ture of the task is that it required catego-
rization with respect to a large and commer-
cially significant set of labels. The number
of participants was larger than in any pre-
vious biomedical challenge task. We de-
scribe the data production process and the
evaluation measures, and give a preliminary
analysis of the results. Many systems per-
formed at levels approaching the inter-coder
agreement, suggesting that human-like per-
formance on this task is within the reach of
currently available technologies.
1 Introduction
Clinical free text (primary data about patients, as op-
posed to journal articles) poses significant technical
challenges for natural language processing (NLP).
In addition, there are ethical and social demands
when working with such data, which is intended for
use by trained medical practitioners who appreciate
the constraints that patient confidentiality imposes.
State-of-the-art NLP systems handle carefully edited
text better than fragmentary notes, and clinical lan-
guage is known to exhibit unique sublanguage char-
acteristics (Hirschman and Sager, 1982; Friedman
et al, 2002; Stetson et al, 2002) (e.g. verbless
sentences, domain-specific punctuation semantics,
and unusual metonomies) that may limit the perfor-
mance of general NLP tools. More importantly, the
confidentiality requirements take time and effort to
address, so it is not surprising that much work in
the biomedical domain has focused on edited jour-
nal articles (and the genomics domain) rather than
clinical free text in medical records. The fact re-
mains, however, that the automation of healthcare
workflows can bring important benefits to treatment
(Hurtado et al, 2001) and reduce administrative bur-
den, and that free text is a critical component of
these workflows. There are economic motivations
for the task, as well. The cost of adding labels like
ICD-9-CM to clinical free text and the cost of re-
pairing associated errors is approximately $25 bil-
lion per year in the US (Lang, 2007). For these
(and many other) reasons, there have been consis-
tent attempts to overcome the obstacles which hin-
der the processing of clinical text (Uzuner et al,
2006). This paper discusses one such attempt?
The 2007 Computational Medicine Challenge, here-
after referred to as ?the Challenge?. There were two
main reasons for conducting the Challenge. One
is to facilitate advances in mining clinical free text;
shared tasks in other biomedical domains have been
shown to drive progress in the field in multiple ways
(see (Hirschman and Blaschke, 2006; Hersh et al,
2005; Uzuner et al, 2006; Hersh et al, 2006) for a
comprehensive review of biomedical challenge tasks
and their contributions). The other is a ground-
97
breaking distribution of useful, reusable, carefully
anonymized clinical data to the research commu-
nity, whose data use agreement is simply to cite the
source. The remaining sections of this paper de-
scribe how the data were prepared, the methods for
scoring, preliminary results [to be updated if sub-
mission is accepted?results are currently still under
analysis], and some lessons learned.
2 Corpus collection and coding process
Supervised methods for machine learning require
training data. Yet, due to confidentiality require-
ments, spotty electronic availability, and variance in
recording standards, the requisite clinical training
data are difficult to obtain. One goal of the chal-
lenge was to create a publicly available ?gold stan-
dard? that could serve as the seed for a larger, open-
source clinical corpus. For this we used the follow-
ing guiding principles: individual identity must be
expunged to meet United States HIPAA standards,
(U.S. Health, 2002) and approved for release by the
local Institutional Review Board (IRB); the sample
must represent problems that medical records coders
actually face; the sample must have enough data for
machine-learning-based systems to do well; and the
sample must include proportionate representations
of very low-frequency classes.
Data for the corpus were collected from the
Cincinnati Children?s Hospital Medical Center?s
(CCHMC) Department of Radiology. CCHMC?s
Institutional Review Board approved release of the
data. Sampling of all outpatient chest x-ray and re-
nal procedures for a one-year period was done us-
ing a bootstrap method (Walters, 2004). These data
are among those most commonly used, and are de-
signed to provide enough codes to cover a substan-
tial proportion of pediatric radiology activity. Ex-
punging patient identity to meet HIPAA standards
included three steps: disambiguation, anonymiza-
tion, and data scrubbing (Pestian et al, 2005).
Ambiguity and Anonymization. Not surprisingly,
some degree of disambiguation is needed to carry
out effective anonymization (Uzuner et al, 2006;
Sibanda and Uzuner, 2006). The reason is that clini-
cal text is dense with medical jargon, abbreviations,
and acronyms, many of which turn out to be ambigu-
ous between a sense that needs anonymization and a
different sense that does not. For example, in a clin-
ical setting, FT can be an abbreviation for full-term,
fort (as in Fort Bragg), feet, foot, field test, full-time
or family therapy. Fort Bragg, being a place name,
and a possible component of an address, could indi-
rectly lead to identification of the patient. Until such
occurrences are disambiguated, it is not possible to
be certain that other steps to anonymize data are ad-
equate. To resolve the relevant ambiguities found in
this free text, we relied on previous efforts that used
expert input to develop clinical disambiguation rules
(Pestian et al, 2004).
Anonymization. To assure patient privacy, clin-
ical text that is used for non-clinical reasons must
be anonymized. However, to be maximally useful
for machine-learning, this must be done in a par-
ticular way. Replacing personal names with some
unspecific value such as ?*? would lose potentially
useful information. Our goal is to replace the sensi-
tive fields with like values that obscure the identity
of the individual (Cho et al, 2002). We found that
the amount of sensitive information routinely found
in unstructured free text data is limited. In our case,
these data included patient and physician names and
sometimes dates or geographic locations, but little or
no other sensitive information turned up in the rele-
vant database fields. Using our internally developed
encryption broker software, we replaced all female
names with ?Jane?, all male names with ?John?, and
all surnames with ?Johnson?. Dates were randomly
shifted.
Manual Inspection. Once the data were disam-
biguated and anonymized, they were manually re-
viewed for the presence of any Protected Health In-
formation (PHI). If a specific token was perceived to
potentially violate PHI regulations, the entire record
was deleted from the dataset. In some case, how-
ever, a general geographic area was changed and
not deleted. For example if the data read ?patient
lived near Mr. Roger?s neighborhood? it would be
deleted, because it may be traceable. On the other
hand, if the data read ?patient was from Cincinnati?
it may have been changed to read ?patient was from
the Covington? After this process, a corpus of 2,216
records was obtained (See Table 2 for details).
ICD-9-CM Assignment. A radiology report has
multiple components. Two parts in particular are
essential for the assignment of ICD-9-CM codes:
98
clinical history?provided by an ordering physician
before a radiological procedure, and impression?
reported by a radiologist after the procedure. In the
case of radiology reports, ICD-9-CM codes serve as
justification to have a certain procedure performed.
There are official guidelines for radiology ICD-9-
CM coding (Moisio, 2000). These guidelines note
that every disease code requires a minimum num-
ber of digits before reimbursement will occur; that
a definite diagnosis should always be coded when
possible; that an uncertain diagnosis should never
be coded; and that symptoms must never be coded
when a definite diagnosis is available. Particular
hospitals and insurance companies typically aug-
ment these principles with more specific internal
guidelines and practices for coding. For these rea-
sons of policy, and because of natural variation in
human judgment, it is not uncommon for multiple
annotators to assign different codes to the same text.
Understanding the sources of this variation is impor-
tant; so too is the need to create a definite gold stan-
dard for use in the challenge. To do so, data were
annotated by the coding staff of CCHMC and two
independent coding companies: COMPANY Y and
COMPANY Z.
Majority annotation. A single gold standard was
created from these three sets of annotations. There
was no reason to adopt any a priori preference for
one annotator over another, so the democratic princi-
ple of assigning a majority annotation was used. The
majority annotation consists of those codes assigned
to the document by two or more of the annotators.
There are, however, several possible problems with
this approach. For example, it could be that the ma-
jority annotation will be empty. This will be rare
(126 records out of 2,216 in our case), because it
only happens when the codes assigned by the three
annotators form disjoint sets. In most hospital sys-
tems, including our own, the coders are required to
indicate a primary code. By convention, the primary
code is listed as the record?s first code, and has an
especially strong impact on the billing process. For
simplicity?s sake, the majority annotation process ig-
nores the distinction between primary and secondary
codes. There is space for a better solution here, but
we have not seriously explored it. We have, how-
ever, conducted an analysis of agreement statistics
(not further discussed here) that suggests that the
overall effect of the majority method is to create a
coding that shares many statistical properties with
the originals, except that it reduces the effect of the
annotators? individual idiosyncrasies. The majority
annotation is illustrated in Table 1.
Our evaluation strategy makes the simplistic as-
sumption that the majority annotation is a true gold
standard and a worthwhile target for emulation. This
is debatable, and is discussed below, but for the sake
of definiteness we simply stipulate that submissions
will be compared against the majority annotation,
and that the best possible performance is to exactly
replicate said majority annotation.
3 Evaluation
Micro- and macro-averaging. Although we rank
systems for purposes of determining the top three
performers on the basis of micro-averaged F1, we
report a variety of performance data, including the
micro-average, macro-average, and a cost-sensitive
measure of loss. Jackson and Moulinier comment
(for general text classification) that: ?No agree-
ment has been reached...on whether one should pre-
fer micro- or macro-averages in reporting results.
Macro-averaging may be preferred if a classification
system is required to perform consistently across all
classes regardless of how densely populated these
are. On the other hand, micro-averaging may be
preferred if the density of a class reflects its impor-
tance in the end-user system? (Jackson and Moulin-
ier, 2002):160-161. For the present medical ap-
plication, we are more interested in the number of
patients whose cases are correctly documented and
billed than in ensuring good coverage over the full
range of diagnostic codes. We therefore emphasize
the micro-average.
A cost-sensitive accuracy measure. While F-
measure is well-established as a method for ranking,
there are reasons for wanting to augment this with
a cost-sensitive measure. An approach that allows
penalties for over-coding (a false positive) and
under-coding (a false negative) to be manipulated
has important implications. The penalty for under-
coding is simple?the hospital loses the amount of
revenue that it would have earned if it had assigned
the code. The regulations under which coding is
done enforce an automatic over-coding penalty of
99
Table 1: Majority Annotation
Hospital Company Y Company Z Majority
Document 1 AB BC AB AB
Document 2 BC ABD CDE BCD
Document 3 EF EF E EF
Document 4 ABEF ACEF CDEF ACEF
three times what is earned from the erroneous code,
with the additional risk of possible prosecution
for fraud. This motivates a generalized version of
Jaccard?s similarity metric (Gower and Legendre,
1986), which was introduced by Boutell, Shen, Luo
and Brown (Boutell et al, 2003).
Suppose that Yx is the set of correct labels for a test
set and Px is the set of labels predicted by some
participating system. Define Fx = Px ? Yx and
Mx = Yx ? Px , i.e. Fx is the set of false positives,
and Mx is the set of missed labels or false negatives.
The score is given by
score(Px) =
(
1?
?|Mx|+ ?|Fx|
|Yx ? Px|
)?
(1)
As noted in (Boutell et al, 2003), if ? = ? = 1 this
formula reduces to the simpler case of
score(Px) =
(
1?
|Yx ? Px|
|Yx ? Px|
)?
(2)
The discussion in (Boutell et al, 2003) points out
that constraints are necessary on ? and ? to ensure
that the inner term of the expression is non-negative.
We do not understand the way that they formulate
these constraints, but note that non-negativity will be
ensured if 0 ? ? ? 1 and 0 ? ? ? 1 . Since over-
coding is three times as bad as undercoding, we use
? = 1.0 , ? = 0.33 . Varying the value of ? would
affect the range of the scores, but does not alter the
rankings of individual systems. We therefore used
? = 1 . This measure does not represent the pos-
sibility of prosecution for fraud, because the costs
involved are incommensurate with the ones that are
represented. With these parameter settings, the cost-
sensitive measure produces rankings that differ con-
siderably from those produced by macro-averaged
balanced F-measure. For example, we shall see that
the system ranked third in the competition by macro-
averaged F-measure assigns a total of 1167 labels,
where the second-ranked assigns 1232, and the cost-
sensitive measure rewards this conservatism in as-
signing labels by reversing the ranking of the two
systems. In either case, the difference between the
systems is small (0.86% difference in F-measure,
0.53% difference in the cost-sensitive measure).
4 The Data
We selected for the challenge a subset of the com-
prehensive data set described above. The subset was
created by stratified sampling, such that it contains
20% of the documents in each category. Thus, the
proportion of categories in the sample is the same as
the proportion of categories in the full data set. We
included in the initial sample only those categories
to which 100 or more documents from the compre-
hensive data set were assigned. After the process
summarized in Table 2, the data were divided into
two partitions: a training set with 978 documents,
and a testing set with 976. Forty-five ICD-9-CM
labels (e.g 780.6) are observed in these data sets.
These labels form 94 distinct combinations (e.g. the
combination 780.6, 786.2). We required that any
combination have at least two exemplars in the data,
and we split each combination between the train-
ing and the test sets. So, there may be labels and
combinations of labels that occur only one time in
the training data, but participants can be sure that
no combination will occur in the test data that has
not previously occurred at least once in the train-
ing data. Our policy here has the unintended con-
sequence that any combination that appears exactly
once in the training data is highly likely to appear
exactly once in the test data. This gives unnecessary
information to the participants. In future challenges
we will drop the requirement for two occurrences in
the data, but ensure that single-occurrence combina-
tions are allocated to the training set rather than the
100
test set. This maintains the guarantee that there will
be no unseen combinations in the test data. The full
data set may be downloaded from the official chal-
lenge web-site.
5 Results
Notice of the Challenge was distributed using elec-
tronic mailing lists supplied by the Association of
Computational Linguistics, IEEE Computer Intelli-
gence and Data Mining, and American Medical In-
formatics Association?s Natural Language Process-
ing special interest group. Interested participants
were asked to register at the official challenge web-
site. Registration began February 1, 2007 and ended
February 28, 2007. Approximately 150 individu-
als registered from 22 countries and six continents.
Upon completing registration, an automated e-mail
was sent with the location of the training data. On
March 1, 2007 participants received notice of the
location of the testing data. Participants were en-
couraged to use the data for other purposes as long
as it was non-commercial and the appropriate cita-
tion was made. There were no other data use re-
strictions. Participants had until March 18, 2007
to submit their results and an explanation of their
model. Approximately 33% (50) of the partici-
pants submitted results. During the course of the
Challenge participants asked a range of questions.
These were posted to the official challenge web-site
- www.computationalmedicine.org/challenge.
The figure below is a scatterplot relating micro-
averaged F1 to the cost-sensitive measure described
above. Each point represents a system. The top-
performing systems achieved 0.8908, the minimum
was 0.1541, and the mean was 0.7670, with a SD
of 0.1340. There are 21 systems with a micro-
averaged F1 between 0.81 and 0.90. Another 14
have F1 > 0.70 . It is noticeable that the systems
are not ranked identically by the cost-sensitive and
the micro-averaged measure, but the differences are
small in each case.
A preliminary screening using a two-factor ANOVA
with system identity and diagnostic code as predic-
tive factors for balanced F-measure revealed a sig-
nificant main effect of both system and code. Pair-
wise t-tests using Holm?s correction for multiple
comparisons revealed no statistically significant dif-
Figure 1: Scatter plot of evaluation measures
ferences between the systems performing at F=0.70
or higher. Differences between the top system and a
system with a microaveraged F-measure of 0.66 do
come out significant on this measure.
We have also calculated (Table 3) the agreement
figures for the three individual annotations that
went into the majority gold standard. We see
that CCHMC outranks COMPANY Y on the cost-
sensitive measure, but the reverse is true for micro-
and macro-averaged F1, with the agreement be-
tween the hospital and the gold standard being espe-
cially low for the macro-averaged version. To under-
stand these figures, it is necessary to recall that the
gold standard is a majority annotation that is formed
from the the three component annotations. It appears
that for rare codes, which have a disproportionate
effect on the macro-averaged F, the majority anno-
tation is dominated by cases where company Y and
company Z assign the same code, one that CCHMC
did not assign.
The agreement figures are comparable to those of
the best automatic systems. If submitted to the
competition, the components of the majority anno-
tation would not have outranked the best systems,
even though the components contributed to the ma-
jority opinion. It is tempting to conclude that the
automated systems are close to human-level perfor-
mance. Recall, however, that while the hospital and
the companies did not have the luxury of exposure
to the majority annotation, the systems did have that
access, which allowed them to explicitly model the
properties of that majority annotation. A more mod-
erate conclusion is that the hospital and the compa-
nies might be able to improve (or at least adjust)
their annotation practices by studying the majority
101
Table 2: Characteristics of the data set through the development process.
Step Removed Total documents
One-year collection of documents 20,275
20 percent sample of one-year collection 4,055
Manual inspection for anonymization problems 1,839 2,216
Removal of records with no majority code 126 2,090
Removal of records with a code occurring only once 136 1,954
Table 3: Comparison of human annotators against majority.
Annotator Cost-sensitive Micro-averaged F1 Macro-averaged F1
HOSPITAL 0.9056 0.8264 0.6124
COMPANY Y 0.8997 0.8963 0.8973
COMPANY Z 0.8621 0.8454 0.8829
annotation and adapting as appropriate.
6 Discussion
Compared to other recent text classification shared
tasks in the biomedical domain (Uzuner et al, 2006;
Hersh et al, 2004; Hersh et al, 2005), this task re-
quired categorization with respect to a set of labels
more than an order of magnitude larger than previ-
ous evaluations. This increase in the size of the set
of labels is an important step forward for the field?
systems that perform well on smaller sets of cate-
gories do not necessarily perform well with larger
sets of categories (Jackson and Moulinier, 2002), so
the data set will allow for more thorough text cat-
egorization system evaluations than have been pos-
sible in the past. Another important contribution of
the work reported here may be the distribution of
the data?the first fully distributable, freely usable
data set of clinical text. The high number of partici-
pants and final submissions was a pleasant surprise;
we attribute this, among other things, to the fact that
this was an applied challenge, that real data were
supplied, and that participants were free to use these
data in other venues.
Participants utilized a diverse range of approaches.
These system descriptions are based on brief com-
ments entered into the submission box, and are ob-
viously subject to revision. The three highest scor-
ers all mentioned ?negation,? all seemed to be us-
ing the structure of UMLS in a serious way. The
better systems frequently mentioned ?hypernyms?
or ?synonyms,? and were apparently doing signifi-
cant amounts of symbolic processing. Two of the
top three had machine-learning components, while
one of the top three used purely symbolic methods.
The most common approach seems to be thought-
ful and medically-informed feature engineering fol-
lowed by some variety of machine learning. The
top-performing system used C4.5, suggesting that
use of the latest algorithms is not a pre-requisite for
success. SVMs and related large-margin approaches
to machine learning were strongly represented, but
did not seem to be reliably predictive of high rank-
ing.
6.1 Observations on running the task and the
evaluation
The most frequently viewed question of the FAQ
was related to a script to calculate the evaluation
score. This was supplied both as a downloadable
script and as an interactive web-page with a form for
submission. In retrospect, we realize that we had not
fully thought through what would happen as people
began to use this script. If we run a similar contest
in the future, we will be better prepared for the con-
fusion that this can cause.
A novel aspect of this task was that although we only
scored a single run on the test data, we allowed par-
ticipants to submit their ?final? run up to 10 times,
and to see their score each time. Note that although
102
participants could see how their score varied on suc-
cessive submissions, they did not have access to the
actual test data or to the correct answers, and so there
were no opportunities for special-purpose hacks to
handle special cases in the test data. The average
participant tried 5.27 (SD 3.17) submissions against
the test data. About halfway through the submis-
sion period we began to realize that in a competi-
tive situation, there are risks in providing the type
of feedback given on the submission form. In fu-
ture challenges, we will be judicious in selecting the
number of attempts allowed and the provision of any
type of feedback. As far as we can tell our general
assumption that the scientific integrity of the partic-
ipants was greater than the need to game the system
is true. It is good policy for those administering the
contest, however, to keep temptations to a minimum.
Our current preference would be to provide only the
web-page interface with no more than five attempts,
and to tell participants only whether their submis-
sion had been accepted, and if so, how many items
and how many codes were recognized.
We provided an XML schema as a precise and pub-
licly visible description of the submission format.
Although we should not have been, we were sur-
prised when changes to the schema were required
in order to accommodate small but unexpected vari-
ations in participant submissions. An even simpler
submission format would have been good. The ad-
vantage of the approach that we took was that XML
validation gave us a degree of sanity-checking at lit-
tle cost. The disadvantage was that some of the nec-
essary sanity-checking went beyond what we could
see how to do in a schema.
The fact that numerous participants generated sys-
tems with high performance indicates that the task
was reasonable, and that sufficient information
about the coding task was either provided by us or
inferred by the participants to allow them to do their
work. Since this is a first attempt, it is not yet clear
what the upper limits on performance are for this
task, but preliminary indications are that automated
systems are or will soon be viable as a component of
deployed systems for this kind of application.
7 Acknowledgements
The authors thank Aaron Cohen of the Oregon
Health and Science University for observations on
the inter-rater agreement between the three sources
and its relationship to the majority assignments, and
also for his input on testing for statistically signif-
icant differences between systems. We also thank
PERSON of ORGANIZATION for helpful com-
ments on the manuscript. Most importantly we
thank all the participants for their on-going commit-
ment, professional feedback and scientific integrity.
References
[Boutell et al, 2003] Boutell M., Shen X., Luo J. and
Brown C. 2003. Multi-label Semantic Scene Clas-
sification, Technical Report 813. Department of Com-
puter Science, University of Rochester September.
[Cho et al, 2002] Cho P. S., Taira R. K., and Kangarloo
H. 2002 Text boundary detection of medical reports.
Proceedings of the Annual Symposium of the American
Medical Informatics Association, 998.
[Friedman et al, 2002] Friedman C., Kra P., and Rzhetsky
A. 2002. Two biomedical sublanguages: a descrip-
tion based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
[Gower and Legendre, 1986] Gower J. C. and Legendre P.
1986. Metric and euclidean properties of dissimilarity
coefficient. Journal of Classification, 3:5?48.
[Hersh et al, 2004] Hersh W., Bhupatiraju R. T., Ross L.,
Roberts P., Cohen A. M., and Kraemer D. F. 2004.
TREC 2004 Genomics track overview. Proceedings of
the 13th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hersh et al, 2006] Hersh W., Cohen A. M., Roberts P.,
and Rekapalli H. K. 2006. TREC 2006 Genomics
track overview. Proceedings of the 15th Annual Text
Retrieval Conference National Institute of Standards
and Technology.
[Hersh et al, 2005] Hersh W., Cohen A. M., Yang J.,
Bhupatiraju R. T., Roberts P., and Hearst M. 2005.
TREC 2005 Genomics track overview. Proceedings of
the 14th Annual Text Retrieval Conference. National
Institute of Standards and Technology.
[Hirschman and Blaschke, 2006] Hirschman L. and
Blaschke C. 2006. Evaluation of text mining in
biology. Text mining for biology and biomedicine,
Chapter 9. Ananiadou S. and McNaught J., editors.
Artech House.
103
[Hirschman and Sager, 1982] Hirschman L. and Sager S.
1982. Automatic information formatting of a medi-
cal sublanguage. Sublanguage: studies of language in
restricted semantic domains, Chapter 2. Kittredge R.
and Lehrberger J., editors. Walter de Gruyter.
[Hurtado et al, 2001] Hurtado M. P, Swift E. K., and Cor-
rigan J. M. 2001. Crossing the Quality Chasm: A
New Health System for the 21st Century. Institute of
Medicine, National Academy of Sciences.
[Jackson and Moulinier, 2002] Jackson P. and Moulinier
I. 2002. Natural language processing for online appli-
cations: text retrieval, extraction, and categorization.
John Benjamins Publishing Co.
[Lang, 2007] Lang, D. 2007. CONSULTANT REPORT
- Natural Language Processing in the Health Care In-
dustry. Cincinnati Children?s Hospital Medical Cen-
ter, Winter 2007.
[Moisio, 2000] Moisio M. 2000. A Guide to Health Care
Insurance Billing. Thomson Delmar Learning, Clifton
Park.
[Pestian et al, 2005] Pestian J. P., Itert L., Andersen C. L.,
and Duch W. 2005. Preparing Clinical Text for Use in
Biomedical Research. Journal of Database Manage-
ment, 17(2):1-12.
[Pestian et al, 2004] Pestian J. P., Itert L., and Duch W.
2004. Development of a Pediatric Text-Corpus for
Part-of-Speech Tagging. Intelligent Information Pro-
cessing and Web Mining, Advances in Soft Computing,
219?226 New York, Springer Verlag.
[Sammuelsson and Wiren, 2000] Sammuelsson C. and
Wiren M. 2000. Parsing Techniques. Handbook of
Natural Language Processing, 59?93. Dale R., Moisl
H., Somers H., editors. New York, Marcel Deker.
[Sibanda and Uzuner, 2006] Sibanda T. and Uzuner O.
2006. Role of local context in automatic deidentifica-
tion of ungrammatical, fragmented text. Proceedings
of the Human Language Technology conference of the
North American chapter of the Association for Com-
putational Linguistics, 65?73.
[Stetson et al, 2002] Stetson P. D., Johnson S. B., Scotch
M., and Hripcsak G. 2002. The sublanguage of cross-
coverage. Proceedings of the Annual Symposium of
the American Medical Informatics Association, 742?
746.
[U.S. Health, 2002] U.S. Heath & Human Services.
2002. 45 CFR Parts 160 and 164 Standards for Privacy
of Individually Identifiable Health Information Final
Rule Federal Register, 67(157):53181?53273.
[Uzuner et al, 2006] Uzuner O., Szolovits P., and Kohane
I. 2006. i2b2 workshop on natural language process-
ing challenges for clinical records. Proceedings of the
Fall Symposium of the American Medical Informatics
Association.
[Walters, 2004] Walters S. J. 2004. Sample size and
power estimation for studies with health related quality
of life outcomes: a comparison of four methods using
the SF-36 Health and Quality of Life Outcomes, 2:26.
104
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 193?201,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Effect of small sample size on text categorization with support vector
machines
Pawe? Matykiewicz
Biomedical Informatics
Cincinnati Children?s Hospital
3333 Burnet Ave
Cincinnat, OH 45220, USA
pawel.matykiewicz@gmail.com
John Pestian
Biomedical Informatics
Cincinnati Children?s Hospital
3333 Burnet Ave
Cincinnat, OH 45220, USA
john.pestian@cchmc.org
Abstract
Datasets that answer difficult clinical ques-
tions are expensive in part due to the need for
medical expertise and patient informed con-
sent. We investigate the effect of small sample
size on the performance of a text categoriza-
tion algorithm. We show how to determine
whether the dataset is large enough to train
support vector machines. Since it is not pos-
sible to cover all aspects of sample size cal-
culation in one manuscript, we focus on how
certain types of data relate to certain proper-
ties of support vector machines. We show that
normal vectors of decision hyperplanes can
be used for assessing reliability and internal
cross-validation can be used for assessing sta-
bility of small sample data.
1 Introduction
Every patient visit generates data, some on paper,
some stored in databases as structured form fields,
some as free text. Regardless of how they are
stored, all such data are to be used strictly for pa-
tient care and for billing, not for research. Patient
health records are maintained securely according to
the provisions of the Health Insurance Portability
and Accountability Act (HIPAA). Investigators must
obtain informed consent from patients whose data
will be used for other purposes. This means defin-
ing which data will be used and how they will be
used. In addition to writing protocols and obtain-
ing consent from patients, medical experts must ei-
ther manually codify important information or teach
a machine how to do it. All of these labor-intensive
tasks are expensive. No one wants to collect more
data than is necessary.
Our research focuses on answering difficult neu-
ropsychiatric questions such as, ?Who is at higher
risk of dying by suicide?? or ?Who is a good
candidate for epilepsy surgery evaluation?? Large
amounts of data that might answer these questions
exist in the form of text dictated by clinicians or
written by patients and thus unavailable. Parallel
to the collection of such data, we explored whether
small datasets can be used to build reliable methods
of making this information available. Here, we in-
vestigate how text classification training size relates
to certain aspects of linear support vector machines.
We hypothesize that a sufficiently large training sub-
set will generate stable and reliable performance es-
timates of a classifier. On the other hand, if the
dataset is too small, then even small changes to
the training size will change the performance of a
classifier and manifest unstable and unreliable esti-
mates. We introduce quantitive definitions for sta-
bility and reliability and give empirical evidence on
how they work.
2 Background
How much data is needed for reliable and stable
analysis? This question has been answered for most
univariate problems, and a few solutions exist for
multivariate problems, but no widely accepted an-
swer is available for sparse and high-dimensional
data. Nonetheless, we will review the few sample
size calculation methods that have been used for ma-
chine learning.
193
Hsieh et al (1998) described a method for calcu-
lating the sample size needed for logistic and lin-
ear regression models. The multivariate problem
was simplified to a series of univariate two-sample t-
tests on the input variables. A variance inflation fac-
tor was used to correct for the multi-dimensionality
which quantifies the severity of multicollinearity in
the least squares regression: collinearity deflates
and non-collinearity inflates sample size estima-
tion. Computer simulations were done on low-
dimensional and continuous data, so it is not known
whether the method is applicable to text categoriza-
tion.
Guyon et al (1998) addressed the problem of de-
termining what size test set guarantees statistically
significant results in a character recognition task, as
a function of the expected error rate. This method
does not assume which learner will be used. Instead,
it requires specific parameters that describe hand-
writing data collection properties such as between-
writers variance and within-writer variance. The
downside of this method is that it must assume the
worst-case scenario: a large variance in data and a
low error rate for the classifier. For this reason larger
datasets are recommended.
Dobbin et al (2008) and Jianhua Hu (2005) fo-
cused only on sample size for a classifier that learns
from gene expression data. No assumptions were
made about the classifier, only about the data struc-
ture. All gene expressions were measured on a con-
tinuous scale that denotes some luminescence cor-
responding to the relative abundance of nucleic acid
sequences in the target DNA strand. The data, re-
gardless of size, can be qualified using just one pa-
rameter, fold change, which measures changes in the
expression level of a gene under two different con-
ditions. Furthermore, the fold change can be stan-
dardized for compatibility with other biological ex-
periments: with a lower standardized fold change,
more samples are needed, and with more genes,
more samples are needed. There is a strong assump-
tion about data makeup, but no assumption is made
about the classifier. This solution allows for small
sample sizes but does not generalize to text classifi-
cation data.
Way et al (2010) evaluated the performance of
various classifiers and featured a selection technique
in the presence of different training sample sizes.
Experiments were conducted on synthetic data, with
two classes drawn from multivariate Gaussian dis-
tributions with unequal means and either equal or
unequal covariance matrices. The conclusion was
that support vector machines with a radial kernel
performed slightly better than the LDA when the
training sample size was small. Only certain combi-
nations of feature selection and classification meth-
ods work well with small sample sizes. We will use
similar assumptions for sparse and high-dimensional
data.
Most recently, Juckett (2012) developed a method
for determining the number of documents needed for
a gold standard corpus. The sample size calculation
was based on the concept of capture probabilities.
It is defined as the normalized sum of probabilities
over all words of interest. For example, if the re-
quired capture probability is 0.95 for a set of med-
ical words, when using larger corpora that contain
these words, it must first be calculated how many
documents are needed to capture the same probabil-
ity in the target corpus. This method is specific to
linguistic research on annotated corpora, where the
probabilities of individual words in the sought cor-
pora must match the probabilities of words in the
target domain. This method focuses solely on the
data structure and does not assume an algorithm or
the task that it will serve. The downside is a higher
sample size.
When reviewing various methods for sample size
calculation, we found that as more assumptions can
be made, fewer data are needed for meaningful anal-
ysis. Assumptions can be made about data structure
and quality, the task the data serve, feature selection,
and the classifier. Our approach exploits a scenario
where the task, the feature selection, and the classi-
fier are known.
3 Data
We used four data sets to test our hypothesis: ver-
sicolor and virginica samples from the Iris dataset
(VV), newswires about corn and wheat from the
ModApte split of the Reuters-21578 dataset (WCT
and WCE), suicide notes reprinted in Shneidman
and Farberow (1957) (SN), and ubiquitous question-
naire patient interviews (UQ). Properties of these
data are summarized in Table 1.
194
The first dataset was created by Anderson (1935)
and introduced to the world of statistics by Fisher
(1936). Since then it has been used on countless oc-
casions to benchmark machine learning algorithms.
Each row of data has four variables to describe the
shape of an iris calyx: sepal length, sepal width,
petal length, and petal width. The dataset contains
50 measurements for each of three subspecies of the
iris flower: setosa, versicolor, and virginica. All
measurements of the setosa calyx are separable from
the rest of the data and thus were not used in our ex-
periments. Instead, we used data corresponding to
versicolor and virginica (VV), which is more inter-
esting because of a small class overlap. The noise is
introduced mostly by sepal width and sepal length.
The second dataset was created by Lewis and
Ringuette (1994) and is the one most commonly
used to benchmark text classification algorithms.
The collection is composed of 21,578 short news
stories from the Reuters news agency. Some stories
have manually assigned topics, like ?earn,? ?acq,? or
?money-fx,? and others do not. In order to make the
dataset comparable across different uses, a ?Modi-
fied Apte? (?ModApte?) split was proposed by Apte?
et al (1994). It has 9,603 training and 3,299 exter-
nal testing documents, a total of 135 distinct topics,
with at least one topic per document. The most fre-
quent topic is ?earn,? which appears in 3,964 docu-
ments. Here, we used only the ?wheat? and ?corn?
categories, which appear 212 and 181 times in the
training set alng with 71 and 56 cases in the test
set. These topics are semantically related, so it is
no surprise that 59 documents in the training set
and 22 documents in test set have both labels. This
gives a total of 335 unique training instances and
105 unique test instances. Interestingly, it is eas-
ier to distinguish ?corn? news from ?not corn just
wheat? news than it is to distinguish ?wheat? from
?not wheat just corn.? The latter seems to be a good
dataset for benchmarking sample size calculation.
We will refer to the ?wheat? versus ?not wheat?
training set as WCT and the ?wheat? versus ?not
wheat? external test set as WCE.
The third dataset was extracted from the appendix
in Shneidman and Farberow (1957). It contains 66
suicide notes (SN) organized into two categories: 33
genuine and 33 simulated. The authors of the notes
were matched in both groups by gender (male), race
(white), religion (Protestant), nationality (native-
born U.S. citizens), and age (25-59). Authors of the
simulated suicide notes were screened for personal-
ity disorders or tendencies toward morbid thoughts
that would exclude them from the study. Individu-
als enrolled in the study were asked to write a sui-
cide note as if they were going to take their own life.
Notes were anonymized, digitized, and prepared for
text processing (Pestian et al, 2010).
The fourth dataset was collected in a clinical con-
trolled trial at Cincinnati Children?s Hospital Med-
ical Center Emergency Department. Sixty patients
were enrolled, 30 with suicidal behavior and 30 con-
trols from the orthopedic service. The suicidal be-
havior group comprised 15 females and 15 males
with an average age of ? 15.7 years (SD ? 1.15).
The control group included 15 females and 15 males
with an average age of ? 14.3 years (SD ? 1.21).
The interview consisted of five open-ended ubiqui-
tous questions (UQ): ?Does it hurt emotionally??
?Do you have any fear?? ?Are you angry?? ?Do
you have any secrets?? and ?Do you have hope??
The interviews were recorded in an audio format,
transcribed by a medical transcriptionist, and pre-
pared for analysis by removing the sections of the
interview where the questions were asked. To pre-
serve the UQ structure, n-grams from each of the
five questions were separated (Pestian et al, 2012).
VV SN UQ WCT WCE
Samples (m) 100 66 60 335 105
Classes 2 2 2 2 2
Class balance 100% 100% 100% 58% 48%
Min row freq 100 2 2 3 0
Max row freq 100 66 60 335 105
Min cell value 1 0 0 0 0
Max cell value 7.9 102.045 64 117 892
Features (n) 4 60 7,282 7,132 7,132
Sparsity 0% 60% 92.3% 97% 98%
Table 1: Four very different benchmark data: versicolor
and virginica (VV) from iris data, representing a dense,
low-dimensional dataset; suicide notes (SN) from Clues
to Suicide (Shneidman and Farberow, 1957), represent-
ing a mildly sparse, high-dimensional dataset; ubiquitous
questionnaires, (UQ) representing a sparse, extremely
high-dimensional dataset; and ?wheat? versus ?not wheat
just corn? (WCT and WCE) from the ?ModApte? split
of Reuters-21578 data, representing an unbalanced, ex-
tremely sparse, high-dimensional dataset.
195
4 Methods
Feature extraction. Every text classification algo-
rithm starts with feature engineering. Documents
in the UQ, WCT, and WCE sets were represented
by a bag-of-n-grams model (Manning and Schuetze,
1999; Manning et al, 2008). Every document was
tokenized, and frequencies of unigrams, bigrams,
and trigrams were calculated. All digit numbers
that appeared in a document were converted to the
same token (?NUMB?). Documents become row
vectors and n-grams become column vectors in a
large sparse matrix. Each n-gram has its own dimen-
sion, with the exception of UQ data, where n-grams
are represented separately for each of the five ques-
tions. Neither stemming nor a stop word list were
applied to the textual data. Suicide notes (SN) were
not represented by n-grams. In previous studies, we
found that the structure of the note and its emotional
content are indicative of suicidality, not its seman-
tic content. Hence, the SN dataset is represented
by the frequency of 23 emotions assigned by men-
tal health professionals, the frequency of 34 parts of
speech, and by three readability scores: Flesch, Fog,
and Kincaid.
Feature weighting. Term weighting was chosen
ad hoc. UQ, WCT, and WCE had a logarithmic
term frequency (log-tf) as local weighting and an in-
verse document frequency (idf) as global weighting
but were derived only from the training data (Salton
and Buckley, 1988; Nakov et al, 2001).
Feature selection. To speed up calculations, the
least frequent features were removed from the SN,
UQ, WCT, and WCE datasets (see minimum row
frequency in Table 1). Further optimization of the
feature space was done using an information gain
filter (Guyon and Elisseeff, 2003; Yang and Peder-
sen, 1997). Depending on the experiment, some of
the features with the lowest information gain were
removed. For example, IG = 0.4 means that 40%
of the features, those with a higher information gain,
were kept, and the other 60%, those with a lower in-
formation gain, were removed. Lastly, all row vec-
tors in UQ, WCT, and WCE were normalized to
unit length (Joachims, 1998).
Learning algorithm. We used linear support vec-
tor machines (SVM) to learn from the data. Sup-
port vector machines are described in great detail in
Figure 1: Normal vector w of a hyperplane.
Schlkopf and Smola (2001). We will focus on just
two aspects: properties of the normal vector of de-
cision hyperplane (see Figure 1) and internal cross-
validation (see Figure 2). SVM is in essence a sim-
ple linear classifier:
f(x) = sgn(?w,x?+ b) (1)
where x is an input vector that needs to be classified,
??, ?? is the inner product, w is a weight vector with
the same dimensionality as x, and b is a scalar. The
function f outputs +1 if x belongs to the first class
or ?1 if x belongs to the second class. SVM differs
from other linear classifiers on how w is computed.
Contrary to other classifiers, it does not solve w di-
rectly. Instead, it uses convex optimization to find
vectors from the training set that can be used for cre-
ating the largest margin between training examples
from the first and second class. Hence, the solution
to w is in the form of the linear combination of co-
efficients and training vectors:
w =
m?
i=1
?iyixi (2)
where m is the number of training vectors, ?i ? 0
are Lagrange multipliers, yi ? {?1, 1} are numer-
ical codes for class labels, and xi are training row
vectors. Vector w is perpendicular to the decision
boundary, and its proper name in the context of
SVM is the normal vector of decision hyperplane1
(see Figure 1). One of the properties of SVM is that
outlying training vectors are not used in w. These
vectors have the corresponding coefficient ?i = 0.
In fact, these vectors can be removed from the train-
ing set and the convex optimization procedure will
1If R with SVM from the e1071 package is used,
the command to obtain the normal vector is w =
c(t(model$coefs)% ?%model$SV).
196
result in exactly the same solution. We can use this
property to probe how reliable training data are for
the classification task. If we have enough data that
we can randomly remove some, what is left will re-
sult in w? ? w. On the other hand, if we do not
have enough data, then random removal of training
data will result in a very different equation, because
the decision boundary changes and w? 6= w.
Reliability of performance. The relationship be-
tween w? and w can be measured. We introduce the
SVM reliability index (SRI):
SRI(w?,w) = |r(w?,w)| (3)
=
|
?n
i=1(w
?
i ?w
?)(wi ?w)|
??n
i=1(w
?
i ?w
?)2
??n
i=1(wi ?w)
2
which is the absolute value of the Pearson product-
moment correlation coefficient between convex op-
timization solution w? corresponding to a training
subset and w corresponding to the full dataset2.
Pearson?s correlation coefficient discovers linear de-
pendency between two normally distributed random
variables and has its domain on a continuous seg-
ment between ?1 and +1. In our case, we are
looking for a strong linear dependency between con-
stituents of the training weight vector w?i and con-
stituents of the full dataset weight vector wi. Some
numerical implementations of SVM cause the out-
put values for the class labels to switch. We cor-
rected for this effect by applying absolute value to
the Pearson?s coefficient, resulting in SRI ? [0, 1].
We did not have a formal proof on how SRI relates
to SVM performance. Instead, we showed empir-
ical evidence for the relationship based on a few
small benchmark data. Stability of performance.
SVM generalization performance is usually mea-
sured using cross-validation accuracy. In particu-
lar, we use balanced accuracy because it gives bet-
ter evidence for a drop in performance when solving
unbalanced problems. Following Guyon and Elis-
seeff (2003) and many others, we divided the data
into three sets: test, training, and validation. Mean
test balanced accuracy aT is estimated using strati-
fied Monte Carlo cross-validation (MCCV), where
2We experimented with Pearson?s correlation, Spearman?s
correlation, one-way intraclass correlation, Cosine correlation,
Cronbach?s coefficient, and Krippendorff?s coefficients and
found that Pearson?s correlation coefficient works well with
both low-dimensional and high-dimensional spaces.
Figure 2: Estimation and resampling: mean test balanced
accuracy and mean validation balanced accuracy should
match. To prevent overfitting, tuning machine learning
should be guided by mean validation accuracy and con-
firmed by mean test accuracy. This procedure requires
the ?develop? set to be large enough to give reliable and
stable estimates.
the proportion of the training set to the test set is
varied between 0.06 and 0.99. Mean validation bal-
anced accuracy aV (MVA) is estimated using K-
fold cross-validation (also known as internal cross-
validation), where K = m2 and m is the number
of training cases. In the case of the ?wheat? versus
?not wheat just corn? dataset, we have, in addition,
the external validation set WCE and corresponding
mean external balanced accuracy aE . Correct esti-
mation of the learner?s generalization performance
should result in all three accuracies being equal:
aT ? aV ? aE . Furthermore, we want all three ac-
curacies to be the same regardless of the amount of
data. If we have enough data that we can randomly
remove some, what is left will result in aV
?
? aV
??
.
On the other hand, if we do not have enough data,
then random removal of training data will result in
very different accuracy estimations: aV
?
6= aV
??
.
Sample size calculation. We do not have a good
way of predicting how much data will be needed to
solve a problem with a small p-value, but this is a
matter of convenience. Rather than looking to the
future, we can simply ask if what we have now is
enough. If we can build a classifier that gives re-
liable and stable estimates of performance, we can
stop collecting data. Reliability is measured by SRI,
while stability is measured by MVA, not as a single
value but merely as a function of the training size:
SRI(t) = |r(wtm,wm)| and (4)
aT (t) = aT
tm
(5)
where t is a proportion of the training data, t ?
(0, 1), m is size of the full dataset, and tm is the
actual number of training instances. To quantify the
197
ability of the dataset to produce classification mod-
els with reliable and stable performance estimates,
we need two more measures: sample dispersion of
SRI and sample dispersion of MVA:
cSRI(t ? p) =
sSRI(t?p)
SRI(t ? p)
and (6)
cMVA(t ? p) =
saT (t?p)
aT (t ? p)
(7)
defined as the coefficient of variation of all SRI or
MVA measurements for training data sizes greater
than pm?. For example, we want to know if our 10-
fold cross-validation (CV) for a dataset that has 400
training samples is reliable and stable. 10-fold CV
is 0.9 of training data, so we need to measure SRI
and MVA for different proportions of training data,
t = {0.90, 0.91, . . . , 0.99}, and then calculate dis-
persion for cSRI(t ? 0.9) and cMVA(t ? 0.9). Nu-
merical calculations will give us sense of good and
bad dispersion across different datasets.
5 Results
Do I have enough data? The first set of experi-
ments was done with untuned algorithms. We set the
SVM parameter to C = 1 and did not use any fea-
ture selection. Figure 3 shows four examples of how
SVM performance depends on the training set size.
The performance was measured using mean test bal-
anced accuracy, MVA, and SRI. Numerical calcu-
lations showed that VV needs at least 30 randomly
selected training examples to produce reliable and
stable results with high accuracy. cSRI(t ? 0.75)
is 0.005 and cMVA(t ? 0.75) is 0.016. SN was
not encouraging regarding the estimated accuracy;
SRI dropped, suggesting that the SVM decision hy-
perplanes are unreliable. Mental health profession-
als can distinguish between genuine and simulated
notes about 63% of time. Machine learning does
it correctly about 73% of time if text structure and
emotional content are used. Even so, the sample
size calculation yields high dispersion (cSRI(t ?
0.75) = 0.134 and cMVA(t ? 0.75) = 0.082).
UQ is small and high-dimensional, and yet the re-
sults were reliable and stable (cSRI(t ? 0.75) =
0.015 and cMVA(t ? 0.75) = 0.023). Patients
enrolled in the UQ study also received the Sui-
cide Ideation Questionnaire (Raynolds, 1987) and
the Columbia-Suicide Severity Rating Scale (Pos-
ner et al, 2011). We found that UQ was no dif-
ferent from the structured questionnaires. UQ de-
tects suicidality mostly by emotional pain and hope-
lessness, which were mildly present in four control
patients. Other instruments returned errors because
the same few teenagers reported risky behavior and
morbid thoughts. WCT produced reliable and sta-
ble accuracy estimates, but no large amounts of data
could be removed (cSRI(t ? 0.75) = 0.010 and
cMVA(t ? 0.75) = 0.053). It seems that WCE
is somehow different from WCT, or it might be a
case of overfitting, which causes the mean test ac-
curacy to diverge from MVA as the training dataset
gets smaller. Algorithm tuning. No results should
be regarded as satisfactory until a thorough param-
eter space search has been completed. Each step of
a text classification algorithm can be improved. To
attempt a complete description of the dependency
of a minimal viable sample size on text classifica-
tion would be both impossible and futile, since new
methods are discovered every day. However, to start
somewhere, we focused only on the feature selection
and SVM parameter C 3. Feature selection removes
noise from data. Parameter C informs the convex
optimization process about the expected noise level.
If both parameters are set correctly, we should see
an improvement in the reliability and stability of
the results. There are several methods for tuning
SVM; the most commonly used but computation-
ally expensive is internal cross-validation (Duan et
al., 2003; Chapelle et al, 2002). Figure 5 shows
the results of the parameter tuning procedure. VV
and SN are not extremely high-dimensional, so we
tuned just parameter C. MVA maxima were found
at C = 0.45 with VV, C = 0.05 with SN, C = 0.4
and IG = 0.1584 with UQ, and C = 2.5 and
IG = 0.8020 with WCT. Do I have enough data
after algorithm tuning? Internal cross-validation
(MVA) did not improve dispersion universally (see
Table 2). VV improved on reliability but not stabil-
ity. SN scored much better on both measures, but
we do not yet know what the cutoff for having a
low enough dispersion is. UQ did worse on all mea-
sures after tuning. WCT improved greatly on mean
3Please note that most SVM implementations do not allow
for simultaneous feature selection and internal cross-validation.
198
VV SN UQ WCT and WCE
Figure 3: SRI index (S), MVA accuracy (V) and mean test accuracy (T) averaged over 120 repetitions and different
training data sizes. Linear SVM with C = 1 and no feature selection. VV (cSRI(t ? 0.75) = 0.005 and cMVA(t ?
0.75) = 0.016), UQ (cSRI(t ? 0.75) = 0.015 and cMVA(t ? 0.75) = 0.023), and WCT (cSRI(t ? 0.75) = 0.010
and cMVA(t ? 0.75) = 0.053) gave stable and reliable estimates, but SN did not (cSRI(t ? 0.75) = 0.134 and
cMVA(t ? 0.75) = 0.082).
VV SN UQ WCT
Figure 4: MVA (internal cross-validation) parameter tuning results. Maxima were found at C = 0.45 with VV,
C = 0.05 with SN, C = 0.4 and IG = 0.1584 with UQ, and C = 2.5 and IG = 0.8020 with WCT.
VV SN UQ WCT and WCE
Figure 5: SRI index (S), MVA accuracy (V), and mean test accuracy (T) averaged over 60 repetitions and different
training data sizes. Tuned classification algorithms: VV with C = 0.45 and no feature selection, SN with C = 0.05
and no feature selection, UQ with C = 0.4 and IG = 0.1584, and WCT with C = 2.5 and IG = 0.8020. Stability
and reliability: VV had cSRI(t ? 0.75) = 0.003 and cMVA(t ? 0.75) = 0.018), SN had cSRI(t ? 0.75) = 0.085
and cMVA(t ? 0.75) = 0.075, UQ had cSRI(t ? 0.75) = 0.025 and cMVA(t ? 0.75) = 0.024, and WCT had
cSRI(t ? 0.75) = 0.025 and cMVA(t ? 0.75) = 0.011.
199
test accuracy, mean external validation, and stability
dispersion (see Figure 5). It would be interesting to
see if improvement on both reliability dispersion and
stability dispersion would bring mean test accuracy
and mean external validation even closer together.
aT (t ? 0.75) cSRI(t ? 0.75) cMVA(t ? 0.75)
VV no tuning 0.965 0.005 0.016
SN no tuning 0.744 0.134 0.082
UQ no tuning 0.946 0.015 0.023
WCT no tuning 0.862 0.010 0.053
VV with tuning 0.970 0.003 0.018
SN with tuning 0.755 0.085 0.075
UQ with tuning 0.941 0.025 0.024
WCT with tuning 0.946 0.025 0.011
Table 2: Sample size calculation before and after tuning
with internal cross-validation (MVA). Even though mean
test accuracy (aT (t ? 0.75)) improved for VV, SN, and
WCT, reliability and stability did not improve univer-
sally. Internal cross-validation alone might not be ade-
quate for tuning classification algorithms for all data.
6 Discussion
Sample size calculation data for a competition
and for problem-solving. In general, there might be
two conflicting objectives when calculating whether
what we have collected is a large enough dataset. If
the objective is to have a shared task with many par-
ticipants and, thus, many unknowns, the best course
of action is to assume the weakest classifier: uni-
grams with no feature weighting or selection trained
using the simplest logistic regression. On the other
hand, if the problem is to be solved with only one
classifier and the least amount of data, then the
strongest assumptions about the data and the algo-
rithm are required.
The fallacy of untuned algorithms. After years
of working with classification algorithms to solve
difficult patient care problems, we have found that
a large amount of data is not needed; usually sam-
ples measured in the hundreds will suffice, but this
is only possible when a thorough parameter space
search is conducted. It seems that reliability and
stability dispersions are good measures of how well
the algorithm is tuned to the data without overfitting.
Moreover, we now have a new direction for thinking
about optimizing classification algorithms: instead
of focusing solely on accuracy, we can also measure
the dispersion and see whether this is a better indi-
cator of what would happen with unevaluated data.
There is a great deal of data available, but very little
that can be used for training.
What to measure? VC-bound, span-bound, ac-
curacy, F1, reliability, and stability dispersions are
just a few examples of indicators of how well our
models fit. What we have outlined here is how
one of the many properties of SVM, the property
of the normal vector, can be used to obtain insights
into data. Normal vectors are constructed using La-
grangian multipliers and support vectors; accuracy
is constructed using a sign function on decision val-
ues. It is feasible that other parts of SVM may be
more suited to algorithm tuning and calculation of
minimum viable training size.
7 Conclusion
Power and sample size calculations are very impor-
tant in any domain that requires extensive expertise.
We do not want to collect more data than necessary.
There is, however, a scarcity of research in sample
size calculation for machine learning. Nonetheless,
the existing results are consistent: the more that can
be assumed about the data, the problem and the al-
gorithm, the fewer data are needed.
We proposed two independent measures for eval-
uating whether available datasets are sufficiently
large: reliability and stability dispersions. Reliabil-
ity dispersion measures indirectly whether the deci-
sion hyperplane is always similar and how much it
varies, while stability dispersion measures how well
we are generalizing and how much variability there
is. If the sample size is large enough, we should
always get the same decision hyperplane with the
same generalization accuracy.
With little empirical evidence, we can conclude
that classifier performance measured by just a single
K in a cross-validation test is not sufficient. K must
be be varied, and other measures must be present,
such as the SVM reliability index, that support or
contradict the generalization accuracy estimates. We
suggest that other measures for sample size calcula-
tion and algorithm tuning may exist and there is still
much to be learned about the mechanics of support
vector machines.
200
References
Edgar Anderson. 1935. The irises of the gaspe peninsula.
Bulletin of the American Iris Society, 59:2?5.
Chidanand Apte?, Fred Damerau, and Sholom M. Weiss.
1994. Automated learning of decision rules for text
categorization. ACM Trans. Inf. Syst., 12(3):233?251,
July.
Olivier Chapelle, Vladimir Vapnik, Olivier Bousquet, and
Sayan Mukherjee. 2002. Choosing multiple parame-
ters for support vector machines. Machine Learning,
46:131?159.
Kevin K. Dobbin, Yingdong Zhao, and Richard M. Si-
mon. 2008. How large a training set is needed to de-
velop a classifier for microarray data? Clinical cancer
research : an official journal of the American Associ-
ation for Cancer Research, 14(1):108?114, January.
Kaibo Duan, S. Sathiya Keerthi, and Aun Neow Poo.
2003. Evaluation of simple performance measures
for tuning svm hyperparameters. Neurocomputing,
51:41?59.
Ronald A. Fisher. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7:179?188.
Isabelle Guyon and Andre Elisseeff. 2003. An introduc-
tion to variable and feature selection. J. Mach. Learn.
Res., 3:1157?1182, March.
Isabelle Guyon, John Makhoul, Richard Schwartz, and
Vladimir Vapnik. 1998. What size test set gives good
error rate estimates? Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 20(1):52?64, Jan-
uary.
Fushing Y. Hsieh, Daniel A. Bloch, and Michael D.
Larsen. 1998. A simple method of sample size cal-
culation for linear and logistic regression. Statistics in
Medicine, 17(14):1623?1634, December.
Fred A. Wright Jianhua Hu, Fei Zou. 2005. Practical fdr-
based sample size calculations in microarray experi-
ments. Bioinformatics, 21(15):3264?3272, August.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Claire Ndellec and Cline Rouveirol, ed-
itors, Machine Learning: ECML-98, volume 1398,
pages 137?142. Springer-Verlag, Berlin/Heidelberg.
David Juckett. 2012. A method for determining the num-
ber of documents needed for a gold standard corpus.
Journal of Biomedical Informatics, page In Press, Jan-
uary.
David D. Lewis and Marc Ringuette. 1994. A com-
parison of two learning algorithms for text categoriza-
tion. In Third Annual Symposium on Document Anal-
ysis and Information Retrieval, pages 81?93.
Christopher D. Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, 1 edition, June.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Preslav Nakov, Antonia Popova, and Plamen Mateev.
2001. Weight functions impact on lsa performance.
In EuroConference RANLP?2001 (Recent Advances in
NLP), pages 187?193.
John Pestian, Henry Nasrallah, Pawel Matykiewicz, Au-
rora Bennett, and Antoon Leenaars. 2010. Suicide
note classification using natural language processing:
A content analysis. Biomedical Informatics Insights,
pages 19?28, August.
John Pestian, Jacqueline Grupp-Phelan, Pawel
Matkiewicz, Linda Richey, Gabriel Meyers,
Christina M. Canter, and Michael Sorter. 2012.
Suicidal thought markers: A controlled trail exam-
ining the language of suicidal adolescents. To Be
Determined, In Preparation.
Kelly Posner, Gregory K. Brown, Barbara Stanley,
David A. Brent, Kseniya V. Yershova, Maria A.
Oquendo, Glenn W. Currier, Glenn A. Melvin, Lau-
rence Greenhill, Sa Shen, and J. John Mann. 2011.
The ColumbiaSuicide severity rating scale: Initial va-
lidity and internal consistency findings from three mul-
tisite studies with adolescents and adults. The Amer-
ican Journal of Psychiatry, 168(12):1266?1277, De-
cember.
William M. Raynolds, 1987. Suicidal Ideation Question-
naire - Junior. Odessa, FL: Psychological Assessment
Resources.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing & Management, 24(5):513?523.
Bernhard Schlkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. The MIT
Press, 1st edition, December.
Edwin S. Shneidman and Norman Farberow. 1957.
Clues to Suicide. McGraw Hill Paperbacks.
Ted W. Way, Berkman Sahiner, Lubomir M. Hadjiiski,
and Heang-Ping Chan. 2010. Effect of finite sample
size on feature selection and classification: a simula-
tion study. Medical Physics, 37(2):907?920, February.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the Fourteenth International Con-
ference on Machine Learning, ICML ?97, pages 412?
420, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
201
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 1?9,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Earlier Identification of Epilepsy Surgery Candidates Using Natural
Language Processing
Pawel Matykiewicz1, Kevin Bretonnel Cohen2, Katherine D. Holland1, Tracy A. Glauser1,
Shannon M. Standridge1, Karin M. Verspoor3,4, and John Pestian1?
1 Cincinnati Children?s Hospital Medical Center, Cincinnati OH USA
2 University of Colorado, Denver, CO
3 National ICT Australia and 4The University of Melbourne, Melbourne, Australia
?corresponding author: john.pestian@cchmc.org
Abstract
This research analyzed the clinical notes
of epilepsy patients using techniques from
corpus linguistics and machine learning
and predicted which patients are can-
didates for neurosurgery, i.e. have in-
tractable epilepsy, and which are not.
Information-theoretic and machine learn-
ing techniques are used to determine
whether and how sets of clinic notes
from patients with intractable and non-
intractable epilepsy are different. The re-
sults show that it is possible to predict
from an early stage of treatment which pa-
tients will fall into one of these two cate-
gories based only on text data. These re-
sults have broad implications for develop-
ing clinical decision support systems.
1 Introduction and Significance
Epilepsy is a disease characterized by recurrent
seizures that may cause irreversible brain damage.
While there are no national registries, epidemiolo-
gists have shown that roughly three million Amer-
icans require $17.6 billion USD in care annually
to treat their epilepsy (Epilepsy Foundation, 2012;
Begley et al, 2000). Epilepsy is defined by the
occurrence of two or more unprovoked seizures
in a year. Approximately 30% of those individ-
uals with epilepsy will have seizures that do not
respond to anti-epileptic drugs (Kwan and Brodie,
2000). This population of individuals is said to
have intractable or drug-resistant epilepsy (Kwan
et al, 2010).
Select intractable epilepsy patients are candi-
dates for a variety of neurosurgical procedures that
ablate the portion of the brain known to cause the
seizure. On average, the gap between the ini-
tial clinical visit when the diagnosis of epilepsy
is made and surgery is six years. If it were pos-
sible to predict which patients should be consid-
ered candidates for referral to surgery earlier in the
course of treatment, years of damaging seizures,
under-employment, and psychosocial distress may
be avoided. It is this gap that motivates this re-
search.
In this study, we examine the differences be-
tween the clinical notes of patients early in their
treatment course with the intent of predicting
which patients will eventually be diagnosed as in-
tractable versus which will be amenable to drug-
based treatment. The null hypothesis is that
there will be no detectable differences between
the clinic notes of patients who go on to a di-
agnosis of intractable epilepsy and patients who
do not progress to the diagnosis of intractable
epilepsy (figure 1). To further elucidate the phe-
nomenon, we look at both the patient?s earli-
est clinical notes and notes from a progression
of time points. Here we expect to gain insight
into how the linguistic characteristics (and natu-
ral language processing-based classification per-
formance) evolve over treatment course. We also
study the linguistic features that characterize the
differences between the document sets from the
two groups of patients. We anticipate that this ap-
proach will ultimately be adapted for various clin-
ical decision support systems.
2 Background
2.1 Related work
Although there has been extensive work on build-
ing predictive models of disease progression and
of mortality risk, few models take advantage of
natural language processing in addressing this
task.
(Abhyankar et al, 2012) used univariate anal-
ysis, multivariate logistic regression, sensitivity
analyses, and Cox proportional hazards models to
predict 30-day and 1-year survival of overweight
1
and obese Intensive Care Unit patients. As one of
the features in their system, they used smoking sta-
tus extracted from patient records by natural lan-
guage processing techniques.
(Himes et al, 2009) used a Bayesian network
model to predict which asthma patients would go
on to develop chronic obstructive pulmonary dis-
ease. As one of their features, they also used
smoking status extracted from patient records by
natural language processing techniques.
(Huang et al, under review) is the work most
similar to our own. They evaluated the ability of
a Naive Bayesian classifier to predict future diag-
noses of depression six months prior and twelve
months prior to the actual diagnoses. They used
a number of feature types, including fielded data
such as billing codes, ICD-9 CM diagnoses, and
others, as well as data drawn from natural lan-
guage processing.
In particular, they used an optimized version of
the NCBO Annotator (Jonquet et al, 2009) to rec-
ognize terms from 22 clinically relevant ontolo-
gies and classify them additionally as to whether
they were negated or related to the patient?s fam-
ily history. Their system demonstrated an ability
to predict diagnoses of depression both six months
and one year prior to the actual diagnoses at a rate
that exceeds the success of primary care practi-
tioners in diagnosing active depression.
Considering this body of work overall, natural
language processing techniques have played a mi-
nor role, providing only a fraction of a much larger
set of features?just one feature, in the first two
studies discussed. In contrast, in our work natu-
ral language processing is the central aspect of the
solution.
2.2 Theoretical background to the
approaches used in this work
In comparing the document sets from the two pa-
tient populations, we make use of two lines of in-
quiry. In the first, we use information-theoretic
methods to determine whether or not the contents
of the data sets are different, and if they are dif-
ferent, to characterize the differences. In the sec-
ond, we make use of a practical method from ap-
plied machine learning. In particular, we deter-
mine whether it is possible to train a classifier to
distinguish between documents from the two sets
of patients, given an appropriate classification al-
gorithm and a reasonable set of features.
From information-theoretic methods, we take
Kullback-Leibler divergence as a way to deter-
mine whether the contents of the two sets of docu-
ments are the same or different. Kullback-Leibler
divergence is the relative entropy of two probabil-
ity mass functions??a measure of how different
two probability distributions (over the same event
space) are? (Manning and Schuetze, 1999). This
measure has been previously used to assess the
similarity of corpora (Verspoor et al, 2009). De-
tails of the calculation of Kullback-Leibler diver-
gence are given in the Methods section. Kullback-
Leibler divergence has a lower bound of zero; with
a value of zero, the two document sets would be
identical. A value of 0.005 is assumed to corre-
spond to near-identity.
From practical applications of machine learn-
ing, we test whether or not it is possible to train a
classifier to distinguish between documents from
the two document sets. The line of thought here is
that provided that we have an appropriate classifi-
cation algorithm and a reasonable feature set, then
if clinic notes from the two document sets are in-
deed different, it should be possible to train a clas-
sifier to distinguish between them with reasonable
accuracy.
3 Materials and methods
3.1 Materials
The experimental protocol was approved by our
local Institutional Review Board (#2012-1646).
Neurology clinic notes were extracted from the
electronic medical record system. Records were
sampled from two groups of patients: 1) those
with intractable epilepsy referred for and eventu-
ally undergoing epilepsy surgery and 2) those with
epilepsy who were responsive to medications and
never referred for surgical evaluation. They were
also sampled at three time periods before the ?zero
point?, the date at which patients were either re-
ferred for surgery or the date of last seizure for the
non-intractable group. Table 1 shows the distribu-
tion of patients and clinic notes.
3.2 Methods
As described in the introduction, we applied
information-theoretic and machine learning tech-
niques to determine whether the two document
collections were different (or differentiable).
2
Non-Intractable Intractable
-12 to 0 355 (127) 641 (155)
-6 to +6 453 (128) 898 (155)
0 to +12 months 454 (132) 882 (149)
Table 1: Progress note and patient counts (in
parentheses) for each time period. A minus sign
indicates the period before surgery referral date
for intractable epilepsy patients and before last
seizure for non-intractable patients. A plus sign
indicates the period after surgery referral for in-
tractable epilepsy patients and after last seizure for
non-intractable patients. Zero is the surgery refer-
ral date or date of last seizure for the two popula-
tions, respectively.
3.2.1 Feature extraction
Features for both the calculation of Kullback-
Leibler divergence and the machine learning
experiment were unigrams, bigrams, tri-
grams, and quadrigrams. We applied the
National Library of Medicine stopword list
http://mbr.nlm.nih.gov/Download/
2009/WordCounts/wrd_stop. All words
were lower-cased, all numerals were substituted
with the string NUMB for abstraction, and all
non-ASCII characters were removed.
3.3 Information-theoretic approach
Kullback-Leibler divergence compares probability
distribution of words or n-grams between different
datasets DKL(P ||Q). In particular, it measures
how much information is lost if distribution Q is
used to approximate distribution P . This method,
however, gives an asymmetric dissimilarity mea-
sure. Jensen-Shannon divergence is probably the
most popular symmetrization of DKL and is de-
fined as follows:
DJS =
1
2
DKL(P ||Q) +
1
2
DKL(Q||P ) (1)
where
DKL(P ||Q) =
?
w?P?Q
(
p(w|cP ) log
p(w|cP )
p(w|cQ)
)
(2)
By Zipf?s law any corpus of natural language will
have a very long tail of infrequent words. To ac-
count for this effect we use DJS for the top N
most frequent words/n-grams. We use Laplace
smoothing to account for words or n-grams that
did not appear in one of the corpora.
We also aim to uncover terms that distinguish
one corpus from another. We use a metamor-
phic DJS test, log-likelihood ratios, and weighted
SVM features. Log-likelihood score will help us
understand where precisely the two corpora differ.
nij =
kij
kiP + kiA
(3)
mij =
kPj + kQj
kQP + kPP + kQA + kPA
(4)
LL(w) = 2
?
i,j
kij log
nij
mij
(5)
3.4 Machine learning
For the classification experiment, we used an im-
plementation of the libsvm support vector ma-
chine package that was ported to R (Dimitriadou
et al, 2011). Features were extracted as described
above in Section 3.2.1. We used a cosine kernel.
The optimal C regularization parameter was esti-
mated on a scale from 2?1 to 215.
3.5 Characterizing differences between the
document sets
We used a variety of methods to characterize
differences between the document sets: log-
likelihood ratio, SVM normal vector components,
and a technique adapted from metamorphic test-
ing.
3.5.1 Applying metamorphic testing to
Kullback-Leibler divergence
As one of our methods for characterizing differ-
ences between the two document sets, we used an
adaptation of metamorphic testing, inspired by the
work of (Murphy and Kaiser, 2008) on applying
metamorphic testing to machine learning applica-
tions. The intuition behind metamorphic testing is
that given some output for a given input, it should
be possible to predict in general terms what the
effect of some alternation in the input should be
on the output. For example, given some Kullback-
Leibler divergence for some set of features, it is
possible to predict how Kullback-Leibler diver-
gence will change if a feature is added to or sub-
tracted from the feature vector. We adapted this
observation by iteratively subtracting all features
one by one and ranking them according to how
much of an effect on the Kullback-Leibler diver-
gence its removal had.
3
Figure 1: Two major paths in epilepsy care. At
the begining of epilepsy care two groups of pa-
tients are indistinguishable. Subsequently, the two
groups diverge.
4 Results
4.1 Kullback-Leibler (Jensen-Shannon)
divergence
Table 2 shows the Kullback-Leibler divergence,
calculated as Jensen-Shannon divergence, for
three overlapping time periods?the year preced-
ing surgery referral, the period from 6 months be-
fore surgery referral to six months after surgery re-
ferral, and the year following surgery referral, for
the intractable epilepsy patients; and, for the non-
intractable epilepsy patients, the same time peri-
ods with reference to the last seizure date.
As can be seen in the left-most column (-12 to
0), at one year prior, the clinic notes of patients
who will require surgery and patients who will
not require surgery cannot easily be discriminated
by Kullback-Leibler divergence?the divergence
is only just above the .005 near-identity threshold
even when 8000 unique n-grams are considered. If
the -6 to +6 and 0 to +12 time periods are exam-
ined, we see that the divergence increases as we
reach and then pass the period of surgery (or move
into the year following the last seizure, for the non-
intractable patients), indicating that the difference
between the two collections becomes more pro-
nounced as treatment progresses. The divergence
for these time periods does pass the assumed near-
identity threshold for larger numbers of n-grams,
n-grams -12 to 0
months
-6 to +6
months
0 to +12
months
125 0.00125 0.00193 0.00244
250 0.00167 0.00229 0.00286
500 0.00266 0.00326 0.00389
1000 0.00404 0.00494 0.00585
2000 0.00504 0.00618 0.00718
4000 0.00535 0.00657 0.00770
8000 0.00555 0.00681 0.00796
Table 2: Kullback-Leibler divergence (calculated
as Jensen-Shannon divergence) for difference be-
tween progress notes of the two groups of patients.
Results are shown for the period 1 year before, 6
months before and 6 months after, and one year
after surgery referral for the intractable epilepsy
patients and the last seizure for non-intractable pa-
tients. 0 represents the date of surgery referral for
the intractable epilepsy patients and date of last
seizure for the non-intractable patients.
largely accounted for by terms that are unique to
one notes set or the other.
4.2 Classification with support vector
machines
Table 3 shows the results of building support vec-
tor machines to classify individual notes as be-
longing to the intractable epilepsy or the non-
intractable epilepsy patient population. Three time
periods are evaluated, as described above. The
number of features is varied by row. For each
cell, the average F-measure from 20-fold cross-
validation is shown.
As can be seen in the left-most column (-12 to
0), at one year prior to referral to surgery refer-
ral date or last seizure, the patients who will be-
come intractable epilepsy patients can be distin-
guished from the patients who will become non-
intractable epilepsy patients purely on the basis of
natural language processing-based classification
with an F-measure as high as 0.95. This supports
the conclusion that the two document sets are in-
deed different, and furthermore illustrates that this
difference can be used to predict which patients
will require surgical intervention.
4.3 Characterizing the differences between
clinic notes from the two patient
populations
Tables 4 and 5 show the results of three meth-
ods for differentiating between the document col-
4
n-grams -12 to 0
months
-6 to +6
months
0 to +12
months
125 0.8885 0.9217 0.9476
250 0.8928 0.9297 0.9572
500 0.9107 0.9367 0.9667
1000 0.9245 0.9496 0.9692
2000 0.9417 0.9595 0.9789
4000 0.9469 0.9661 0.9800
8000 0.9510 0.9681 0.9810
Table 3: Average F1 for the three time periods
described above, with increasing numbers of fea-
tures. Values are the average of 20-fold cross-
validation. See Figure 2 for an explanation of the
time periods.
lections representing the two patient populations.
The methodology for each is described above. The
most strongly distinguishing features when just
the 125 most frequent features are used are shown
in Table 4, and the most strongly distinguishing
features when the 8,000 most frequent features are
used are shown in Table 5. Impressionistically,
two trends emerge. One is that more clearly clini-
cally significant features are shown to have strong
discriminatory power when the 8,000 most fre-
quent features are used than when the 125 most
frequent features are used. This result is sup-
ported by the Kullback-Leibler divergence results,
which demonstrated the most divergent vocabular-
ies with larger numbers of n-grams. The other
trend is that the SVM classifier does a better job
of picking out clinically relevant features. This
has implications for the design of clinical decision
support systems that utilize our approach.
5 Discussion
5.1 Behavior of Kullback-Leibler divergence
Kullback-Leibler divergence varies with the num-
ber of words considered. When the vocabularies
of two document sets are merged and the words
are ordered by overall frequency, the further down
the list we go, the higher the Kullback-Leibler
divergence can be expected to be. This is be-
cause the highest-frequency words in the com-
bined set will generally be frequent in both source
corpora, and therefore carry similar probability
mass. As we progress further down the list of
frequency-ranked words, we include progressively
less-common words, with diverse usage patterns,
which are likely to reflect the differences between
the two document sets, if there are any. Thus, the
Kullback-Leibler divergence will rise.
To understand the intuition here, imagine look-
ing at the Kullback-Leibler divergence when just
the 50 most-common words are considered. These
will be primarily function words, and their distri-
butions are unlikely to differ much between the
two document sets unless the syntax of the two
corpora is radically different. Beyond this set of
very frequent common words will be words that
may be relatively frequent in one set as compared
to the other, contributing to divergence between
the sets.
In Table 2, the observed behavior for our two
document collections follows this expected pat-
tern. However, the divergence between the vocab-
ularies remains close to the assumed near-identity
threshold of 0.005, even when larger numbers of
n-grams are considered. The divergence never ex-
ceeds 0.01; this level of divergence for larger num-
bers of n-grams is consistent with prior analyses of
highly similar corpora (Verspoor et al, 2009).
We attribute this similarity to two factors. The
first is that both document sets derive from a single
department within a single hospital; a relatively
small number of doctors are responsible for au-
thoring the notes and there may exist specific hos-
pital protocols related to their content. The second
is that the clinical contexts from which our two
document sets are derived are highly related, in
that all the patients are epilepsy patients. While we
have demonstrated that there are clear differences
between the two sets, it is also to be expected that
they would have many words in common. The
nature of clinical notes combined with the shared
disease context results in generally consistent vo-
cabulary and hence low overall divergence.
5.2 Behavior of classifier
Table 3 demonstrates that classifier performance
increases as the number of features increases. This
indicates that as more terms are considered, the
basis for differentiating between the two different
document collections is stronger.
Examining the SVM normal vector components
(SVMW) in Tables 4 and 5, we find that unigrams,
bigrams and trigrams are useful in differentiation
between the two patient populations. While no
quadrigrams appear in this table, they may in fact
contribute to classifier performance. We will per-
form an ablation study in future work to quantify
5
JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-
nents (SVMW)
family = -0.000114 none = 623.702323 bilaterally = -19.009380
normal = -0.000106 family = -445.117177 age.NUMB = 17.981459
seizure = -0.000053 NUMB.NUMB.NUMB.NUMB
= 422.953816
review = 17.250652
problems = -0.000053 normal = -244.603033 based = -14.846495
none = 0.000043 problems = -207.021130 family.history = -14.659653
detailed = -0.000037 left = 176.434519 NUMB = -14.422525
including = -0.000036 bid = 142.105691 lower = -13.553434
risks = -0.000033 NUMB = 136.255678 mother = -13.436694
NUMB = 0.000032 detailed = -133.012908 first = -13.001744
concerns = -0.000032 right = 120.453596 including = -12.800433
NUMB.NUMB.NUMB.NUMB
= 0.000031
seizure = -120.047686 extremities = 11.709199
additional = -0.000029 including = -119.061518 documented = -11.441394
brain = -0.000026 risks = -116.543250 awake = -11.418535
NUMB.NUMB = 0.000022 concerns = -101.366110 hpi = 11.121019
minutes = -0.000021 additional = -95.880792 follow = -10.550802
NUMB.minutes = -0.000020 clear = 83.848170 neurology = -10.533895
reviewed = -0.000018 brain = -74.267220 call = -10.422606
history = -0.000017 seizures = 71.937757 effects = 10.298221
noted = -0.000017 one = 65.203819 brain = -9.900864
upper = -0.000017 epilepsy = 46.383564 weight = 9.819712
well = -0.000015 hpi = 45.932630 patient.s = -9.603531
side = -0.000015 minutes = -45.278770 discussed = -9.473544
bilaterally = -0.000014 NUMB.NUMB.NUMB =
43.320354
today = 9.390896
motor.normal = -0.000014 negative = 42.914770 allergies = -9.346146
notes = -0.000014 NUMB.minutes = -42.909968 NUMB.NUMB.NUMB.NUMB
= 9.342800
Spearman correlation between
JSMT and LLR = 0.912454
Spearman correlation between
LLR and SVMW = 0.086784
Spearman correlation between
SVMW and JSMT = 0.101965
Table 4: Comparison of three different methods for finding the strongest differentiating features. This
table shows features for the -12 to 0 periods with the 125 most frequent features. The JSMT and LLR
statistics give values greater than zero. We add sign to indicate which corpus has higher relative fre-
quency of the feature: a positive value indicates that the relative frequency of the feature is greater in the
intractable group, while a negative value indicates that the relative frequency of the feature is greater in
the non-intractable group. The last row shows the correlation between two different ranking statistics.
6
JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-
nents (SVMW)
family = -0.000118 family = -830.329965 john = -4.645071
normal = -0.000109 normal = -745.882086 lamotrigine = 4.320412
seizure = -0.000057 problems = -386.238711 surgery = 4.299546
problems = -0.000057 seizure = -369.342334 jane = 4.091609
none = 0.000047 none = 337.461504 epilepsy.surgery = 4.035633
including = -0.000040 detailed = -262.240496 janet = -3.970101
detailed = -0.000040 including = -255.076808 excellent.control = -3.946283
additional.concerns = -0.000038 additional.concerns.noted =
-246.603655
excellent = -3.920620
additional.concerns.noted =
-0.000038
concerns.noted = -246.603655 NUMB.seizure = -3.886997
concerns.noted = -0.000038 additional.concerns = -
243.353912
mother = -3.801364
NUMB = -0.000036 NUMB.NUMB.NUMB.NUMB
= 238.065700
jen = 3.568809
concerns = -0.000036 risks = -232.741511 back = -3.319477
risks = -0.000036 concerns = -228.805299 visit = -3.264600
NUMB.NUMB.NUMB.NUMB
= 0.000035
additional = -204.462411 james = 3.174763
additional = -0.000033 brain = -182.413340 NUMB.NUMB.NUMB.normal
= -3.024471
brain = -0.000030 NUMB = -162.992065 continue = -3.011293
NUMB.NUMB = -0.000026 surgery = 153.646067 idiopathic.localization = -
2.998177
minutes = -0.000025 minutes = -142.761961 idiopathic.localization.related =
-2.998177
surgery = 0.000024 NUMB.minutes = -134.048116 increase = 2.948187
NUMB.minutes = -0.000023 diff = -131.388230 diastat = -2.937431
diff = -0.000023 NUMB.NUMB = -125.067347 taking = -2.902673
history = -0.000021 reviewed = -116.013417 lamictal = 2.898987
reviewed = -0.000021 noted = -114.241532 going = 2.862764
noted = -0.000021 idiopathic = -112.331060 described = 2.844830
upper = -0.000020 shaking = -112.186858 epilepsy = 2.745872
Spearman correlation between
JSMT and LLR = 0.782918
Spearman correlation between
LLR and SVMW = 0.039860
Spearman correlation between
SVMW and JSMT = 0.165159
Table 5: Comparison of three different methods for finding the strongest differentiating features. This
table shows features for the -12 to 0 periods with the 8,000 most frequent features. The JSMT and
LLR statistics give values greater than zero. We add sign to indicate which corpus has higher relative
frequency of the feature: a positive value indicates that the relative frequency of the feature is greater in
the intractable group, while a negative value indicates that the relative frequency of the feature is greater
in the non-intractable group. The last row shows the correlation between two different ranking statistics.
7
the contribution of the different feature sets. In ad-
dition, we find that table 5 shows many clinically
relevant terms, such as seizure frequency (?ex-
cellent [seizure] control?), epilepsy type (?local-
ization related [epilepsy]?), etiology classification
(?idiopathic [epilepsy]?), and drug names (?lamot-
rigine?, ?diastat?, ?lamictal?), giving nearly com-
plete history of the present illness.
6 Conclusion
The classification results from our machine learn-
ing experiments support rejection of the null hy-
pothesis of no detectable differences between the
clinic notes of patients who will progress to the
diagnosis of intractable epilepsy and patients who
do not progress to the diagnosis of intractable
epilepsy. The results show that we can predict
from an early stage of treatment which patients
will fall into these two classes based only on tex-
tual data from the neurology clinic notes. As intu-
ition would suggest, we find that the notes become
more divergent and the ability to predict outcome
improves as time progresses, but the most impor-
tant point is that the outcome can be predicted
from the earliest time period.
SVM classification demonstrates a stronger re-
sult than the information-theoretic measures, uses
less data, and needs just a single run. However, it
is important to note that we cannot entirely rely
on the argument from classification as the sole
methodology in testing whether or not two doc-
ument sets are similar or different. If the find-
ing is positive, i.e., it is possible to train a classi-
fier to distinguish between documents drawn from
the two document sets, then interpreting the re-
sults is straightforward. However, if documents
drawn from the two document sets are not found
to be distinguishable by a classifier, one must
consider the possibility of multiple possible con-
founds, such as selection of an inappropriate clas-
sification algorithm, extraction of the wrong fea-
tures, bugs in the feature extraction software, etc.
Having established that the two sets of clinical
notes differ, we noted some identifying features of
clinic notes from the two populations, particularly
when more terms were considered.
The Institute of Medicine explains that ?. . . to
accommodate the reality that although profes-
sional judgment will always be vital to shaping
care, the amount of information required for any
given decision is moving beyond unassisted hu-
man capacity (Olsen et al, 2007).? This is surely
the case for those who care for the epileptic pa-
tient. Technology like natural language processing
will ultimately serve as a basis for stable clinical
decision support tools. It, however, is not a deci-
sion making tool. Decision making is the respon-
sibility of professional judgement. That judge-
ment will labor over such questions as: what is
the efficacy of neurosurgery, what will be the long
term outcome, will there be any lasting damage,
are we sure that all the medications have been
tested, and how the family will adjust to a poor
outcome. In the end, it is that judgement that will
decide what is best; that decision will be supported
by research like what is presented here.
7 Acknowledgements
This work was supported in part by the National
Institutes of Health, Grants #1R01LM011124-
01,and 1R01NS045911-01; the Cincinnati Chil-
dren?s Hospital Medical Center?s: Research Foun-
dation, Department of Pediatric Surgery and the
Department of Paediatrics?s divisions of Neurol-
ogy and Biomedical Informatics. We also wish
to acknowledge the clinical and surgical wisdom
provided by Drs. John J. Hutton & Hansel M.
Greiner, MD. K. Bretonnel Cohen was supported
by grants XXX YYY ZZZ. Karin Verspoor was
supported by NICTA, which is funded by the Aus-
tralian Government as represented by the Depart-
ment of Broadband, Communications and the Dig-
ital Economy and the Australian Research Coun-
cil.
References
[Abhyankar et al2012] Swapna Abhyankar, Kira Leis-
hear, Fiona M. Callaghan, Dina Demner-Fushman,
and Clement J. McDonald. 2012. Lower short- and
long-term mortality associated with overweight and
obesity in a large cohort study of adult intensive care
unit patients. Critical Care, 16.
[Begley et al2000] Charles E Begley, Melissa Famu-
lari, John F Annegers, David R Lairson, Thomas F
Reynolds, Sharon Coan, Stephanie Dubinsky,
Michael E Newmark, Cynthia Leibson, EL So, et al
2000. The cost of epilepsy in the united states: An
estimate from population-based clinical and survey
data. Epilepsia, 41(3):342?351.
[Dimitriadou et al2011] Evgenia Dimitriadou, Kurt
Hornik, Friedrich Leisch, David Meyer, and An-
dreas Weingessel, 2011. e1071: Misc Func-
tions of the Department of Statistics (e1071), TU
8
Wien. http://CRAN.R-project.org/package=e1071.
R package version 1.5.
[Epilepsy Foundation2012] Epilepsy Foundation,
2012. What is Epilepsy: Incidence and Prevalence.
http://www.epilepsyfoundation.org/ aboutepilepsy
/whatisepilepsy/ statistics.cfm.
[Himes et al2009] Blanca E. Himes, Yi Dai, Isaac S.
Kohane, Scott T. Weiss, and Marco F. Ramoni.
2009. Prediction of chronic obstructive pulmonary
disease (copd) in asthma patients using electronic
medical records. Journal of the American Medical
Informatics Association, 16(3):371?379.
[Huang et alunder review] Sandy H. Huang, Paea LeP-
endu, Srinivasan V Iyer, Anna Bauer-Mehren, Cliff
Olson, and Nigam H. Shah. under review. Develop-
ing computational models for predicting diagnoses
of depression. In American Medical Informatics As-
sociation.
[Jonquet et al2009] Clement Jonquet, Nigam H. Shah,
Cherie H. Youn, Mark A. Musen, Chris Callendar,
and Margaret-Anne Storey. 2009. NCBO Annota-
tor: Semantic annotation of biomedical data. In 8th
International Semantic Web Conference.
[Kwan and Brodie2000] Patrick Kwan and Martin J
Brodie. 2000. Early identification of refrac-
tory epilepsy. New England Journal of Medicine,
342(5):314?319.
[Kwan et al2010] Patrick Kwan, Alexis Arzimanoglou,
Anne T Berg, Martin J Brodie, W Allen Hauser,
Gary Mathern, Solomon L Moshe?, Emilio Perucca,
Samuel Wiebe, and Jacqueline French. 2010. Defi-
nition of drug resistant epilepsy: consensus proposal
by the ad hoc task force of the ilae commission on
therapeutic strategies. Epilepsia, 51(6):1069?1077.
[Manning and Schuetze1999] Christopher Manning
and Hinrich Schuetze. 1999. Foundations of
statistical natural language processing. MIT Press.
[Murphy and Kaiser2008] Christian Murphy and Gail
Kaiser. 2008. Improving the dependability of ma-
chine learning applications.
[Olsen et al2007] LeighAnne Olsen, Dara Aisner, and
J Michael McGinnis. 2007. The learning healthcare
system.
[Verspoor et al2009] K. Verspoor, K.B. Cohen, and
L. Hunter. 2009. The textual characteristics of tradi-
tional and open access scientific journals are similar.
BMC Bioinformatics, 10(1):183.
9
