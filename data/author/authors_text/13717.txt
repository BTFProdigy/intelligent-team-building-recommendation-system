Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320?1331,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Semantic Role Induction with Graph Partitioning
Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we present a method for unsuper-
vised semantic role induction which we for-
malize as a graph partitioning problem. Ar-
gument instances of a verb are represented as
vertices in a graph whose edge weights quan-
tify their role-semantic similarity. Graph par-
titioning is realized with an algorithm that it-
eratively assigns vertices to clusters based on
the cluster assignments of neighboring ver-
tices. Our method is algorithmically and con-
ceptually simple, especially with respect to
how problem-specific knowledge is incorpo-
rated into the model. Experimental results on
the CoNLL 2008 benchmark dataset demon-
strate that our model is competitive with other
unsupervised approaches in terms of F1 whilst
attaining significantly higher cluster purity.
1 Introduction
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the se-
mantic relations that hold between a predicate and
its arguments (e.g., ?who? did ?what? to ?whom?,
?when?, ?where?, and ?how?) abstracting over sur-
face syntactic configurations.
In the example sentences below, window occu-
pies different syntactic positions ? it is the object of
broke in sentences (1a,b), and the subject in (1c) ?
while bearing the same semantic role, i.e., the phys-
ical object affected by the breaking event. Analo-
gously, ball is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
(1) a. [Jim]A0 broke the [window]A1 with a
[ball]A2.
b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.
The semantic roles in the examples are labeled in
the style of PropBank (Palmer et al, 2005), a broad-
coverage human-annotated corpus of semantic roles
and their syntactic realizations. Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that predi-
cate1 and a set of adjunct roles such as location or
time whose interpretation is common across predi-
cates (e.g., last night in sentence (1c)).
The availability of PropBank and related re-
sources (e.g., FrameNet; Ruppenhofer et al (2006))
has sparked the development of great many seman-
tic role labeling systems most of which conceptu-
alize the task as a supervised learning problem and
rely on role-annotated data for model training. Most
of these systems implement a two-stage architec-
ture consisting of argument identification (determin-
ing the arguments of the verbal predicate) and ar-
gument classification (labeling these arguments with
semantic roles). Despite being relatively shallow, se-
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
1320
mantic role analysis has the potential of benefiting a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al, 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al, 2005).
Current approaches have high performance ? a
system will recall around 81% of the arguments cor-
rectly and 95% of those will be assigned a cor-
rect semantic role (see Ma`rquez et al (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al, 2008). Unfortu-
nately, the reliance on role-annotated data which is
expensive and time-consuming to produce for every
language and domain, presents a major bottleneck to
the widespread application of semantic role labeling.
In this paper we argue that unsupervised meth-
ods offer a promising yet challenging alternative. If
successful, such methods could lead to significant
savings in terms of annotation effort and ultimately
yield more portable semantic role labelers that re-
quire overall less engineering effort. Our approach
formalizes semantic role induction as a graph parti-
tioning problem. Given a verbal predicate, it con-
structs a weighted graph whose vertices correspond
to argument instances of the verb and whose edge
weights quantify the similarity between these in-
stances. The graph is partitioned into vertex clus-
ters representing semantic roles using a variant of
Chinese Whispers, a graph-clustering algorithm pro-
posed by Biemann (2006). The algorithm iteratively
assigns cluster labels to graph vertices by greedily
choosing the most common label amongst the neigh-
bors of the vertex being updated. Beyond extend-
ing Chinese Whispers to the semantic role induc-
tion task, we also show how it can be understood
as a type of Gibbs sampling when our graph is inter-
preted as a Markov random field.
Experimental results on the CoNLL 2008 bench-
mark dataset demonstrate that our method, de-
spite its simplicity, improves upon competitive ap-
proaches in terms of F1 and achieves significantly
higher cluster purity.
2 Related Work
Although the bulk of previous work on semantic role
labeling has primarily focused on supervised meth-
ods (Ma`rquez et al, 2008), a few semi-supervised
and unsupervised approaches have been proposed
in the literature. The majority of semi-supervised
models have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fu?rstenau and Lapata, 2009) or across different lan-
guages (Pado? and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) pro-
pose to increase the coverage of PropBank to un-
seen verbs by finding syntactically similar (labeled)
verbs and using their annotations as surrogate train-
ing data.
Swier and Stevenson (2004) were the first to intro-
duce an unsupervised semantic role labeling system.
Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances
is iteratively expanded using a classifier trained on
previously labeled instances. Their method starts
with a dataset containing no role annotations at all,
but crucially relies on VerbNet (Kipper et al, 2000)
for identifying the arguments of predicates and mak-
ing initial role assignments. VerbNet is a manually
constructed lexicon of verb classes each of which is
explicitly associated with argument realization and
semantic role specifications.
Subsequent work has focused on unsupervised
methods for argument identification and classifica-
tion. Abend et al (2009) recognize the arguments of
predicates by relying solely on part of speech anno-
tations whereas Abend and Rappoport (2010) distin-
guish between core and adjunct roles, using an unsu-
pervised parser and part-of-speech tagger. Grenager
and Manning (2006) address the role induction prob-
lem and propose a directed graphical model which
relates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Following up on this work, Lang and Lapata
(2010) formulate role induction as the process of de-
1321
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that takes
advantage of the close relationship between syntac-
tic functions and semantic roles. More recently,
Lang and Lapata (2011) propose a clustering algo-
rithm which first splits the argument instances of
a verb into fine-grained clusters based on syntac-
tic cues and then executes a series of merge steps
(mainly) based on lexical cues. The split phase cre-
ates a large number of small clusters with high purity
but low collocation, i.e., while the instances in a par-
ticular cluster typically belong to the same role the
instances for a particular role are commonly scat-
tered amongst many clusters. The subsequent merge
phase conflates clusters with the same role in order
to increase collocation.
Like Grenager and Manning (2006) and Lang
and Lapata (2010; 2011), this paper describes an
unsupervised method for semantic role induction,
i.e., one that does not require any role annotated data
or additional semantic resources for training. Con-
trary to these previous approaches, we conceptualize
role induction in a novel way, as a graph partitioning
problem. Our method is simple, computationally ef-
ficient, and does not rely on hidden variables. More-
over, the graph-based representation for verbs and
their arguments affords greater modeling flexibility.
A wide range of methods exist for finding partitions
in graphs (Schaeffer, 2007), besides Chinese Whis-
pers (Biemann, 2006), which could be easily applied
to the semantic role induction problem. However,
we leave this to future work.
Graph-based methods are popular in natural lan-
guage processing, especially with unsupervised
learning problems (Chen and Ji, 2010). The Chinese
Whispers algorithm itself (Biemann, 2006) has been
previously applied to several tasks including word
sense induction (Klapaftis and M., 2010) and unsu-
pervised part-of-speech tagging (Christodoulopou-
los et al, 2010). The same algorithm is also de-
scribed in Abney (2007, pp. 146-147) under the
name ?clustering by propagation?. The term makes
explicit the algorithm?s connection to label propa-
gation, a general framework2 for semi-supervised
learning (Zhu et al, 2003) with applications to
machine translation (Alexandrescu and Kirchhoff,
2009), information extraction (Talukdar and Pereira,
2010) and structured part-of-speech tagging (Sub-
ramanya et al, 2010). The basic idea behind la-
bel propagation is to represent labeled and unlabeled
instances as vertices in an undirected graph with
edges whose weights express similarity (and possi-
bly dissimilarity) between the instances. Label in-
formation is then propagated between the vertices
in such a way that similar instances tend to be as-
signed the same label. Analogously, Chinese Whis-
pers works by propagating cluster membership in-
formation along the edges of a graph, even though
the graph does not contain any human-labeled in-
stance vertices.
3 Problem Setting
We adopt the standard architecture of supervised se-
mantic role labeling systems where argument identi-
fication and argument classification are treated sep-
arately. Our role labeler is fully unsupervised with
respect to both tasks ? it does not rely on any role
annotated data or semantic resources. However, our
system does not learn from raw text. In common
with most semantic role labeling research, we as-
sume that the input is syntactically analyzed in the
form of dependency trees.
We view argument identification as a syntactic
processing step that can be largely undertaken deter-
ministically through structural analysis of the depen-
dency tree. We therefore use a small set of rules to
detect arguments with high precision and recall (see
Section 4). Argument classification is more chal-
lenging and must take into account syntactic as well
as lexical-semantic information. Both types of in-
formation are incorporated into our model through
a similarity function that assigns similarity scores
to pairs of argument instances. Following previous
work (Lang and Lapata, 2010; Grenager and Man-
ning, 2006), our system outputs verb-specific roles
by grouping argument instances into clusters and la-
beling each argument instance with an identifier cor-
2For example, Haffari and Sarkar (2007) use label propa-
gation to analyze other semi-supervised algorithms such as the
Yarowsky (1995) algorithm.
1322
responding to the cluster it has been assigned to.
Such identifiers are similar to PropBank-style core
labels (e.g., A0, A1).
4 Argument Identification
Supervised semantic role labelers often employ a
classifier in order to decide for each node in the
parse tree whether or not it represents a semantic
argument. Nodes classified as arguments are then
assigned a semantic role. In the unsupervised set-
ting, we slightly reformulate argument identification
as the task of discarding as many non-semantic ar-
guments as possible. This means that the argument
identification component does not make a final posi-
tive decision for any of the argument candidates; in-
stead, a final decision is only made in the subsequent
argument classification stage.
We discard or select argument candidates us-
ing the set of rules developed in Lang and Lap-
ata (2011). These are mainly based on the parts
of speech and syntactic relations encountered when
traversing a dependency tree from the predicate
node to the argument node. For each candidate,
rules are considered in a prespecified order and the
first matching rule is applied. When evaluated on
its own, the argument identification component ob-
tained 88.1% precision (percentage of semantic ar-
guments out of those identified) and 87.9% recall
(percentage of identified arguments out of all gold
arguments).
5 Argument Classification
After identifying likely arguments for each verb,
the next step is to infer a label for each argument
instance. Since we aim to induce verb-specific
roles (see Section 3), we construct an undirected,
weighted graph for each verb. Vertices corre-
spond to verb argument instances and edge weights
quantify the similarities between them. This
argument-instance graph is then partitioned into
clusters of vertices representing semantic roles and
each argument instance is assigned a label that indi-
cates the cluster it belongs to. In what follows we
first describe how the graph is constructed and then
provide the details of our graph partitioning algo-
rithm.
CA
E D
B
0.4 0.1
0.8 ?1
1
0.3
0.2 0.7
Figure 1: Simplified example of an argument-instance
graph. All pairs of vertices with non-zero similarity are
connected through edges that are weighted with a simi-
larity score ?(vi,v j). Upon updating the label for a vertex
all neighboring vertices propagate their label to the vertex
being updated. The score for each label is determined by
summing together the weighted votes for that label and
the label with the maximal score is chosen.
5.1 Graph Construction
For each verb we construct an undirected, weighted
graph G = (V,E,?) with vertices V , edges E, and
edge weight function ? as follows. Each argu-
ment instance in the corpus that belongs to the
verb is added as a vertex. Then, for each possi-
ble pair of vertices (vi,v j) we compute a weight
?(vi,v j) ? R according to the function ?. If the
weight is non-zero, an undirected edge e = (vi,v j)
with weight ?(vi,v j) is added to the graph. The func-
tion ? quantifies the similarity or dissimilarity be-
tween instances; positive values indicate that roles
are likely to be the same, negative values indicate
that roles are likely to differ, and zero values indicate
that there is no evidence for either case. Our simi-
larity function is symmetric, i.e., ?(vi,v j) = ?(v j,vi)
and permits negative values (see Section 5.4 for a
detailed description).
Figure 1 shows an example of a graph for a verb
with five argument instances (vertices A?E). Edges
are drawn between pairs of vertices with non-zero
similarity values. For instance, vertex D is con-
nected to vertex A with weight 0.2, to vertex E
with 1, and vertex C with?1. Since edges are drawn
between all pairs of vertices with non-zero simi-
larity, the resulting graphs tend to be densely con-
nected, which for large datasets may be prohibitively
1323
inefficient. A solution would be to sample a subset
from all possible pairs, but we did not make use of
any kind of edge pruning in our experiments.
5.2 Graph Partitioning
Graph partitioning is realized with a variant of Chi-
nese Whispers (Biemann, 2006) whose details are
given below. In addition, we discuss how our algo-
rithm relates to other graph-based models in order to
help provide a better theoretical understanding.
We assume each vertex vi is assigned a label
li ? {1 . . .L} indicating the cluster it belongs to. Ini-
tially, each vertex belongs to its own cluster, i.e., we
let the number of clusters L = |V | and set li? i.
Given this initial vertex labeling, the algorithm pro-
ceeds by iteratively updating the label for each ver-
tex. The update is based on the labels of neighbor-
ing vertices and reflects their similarity to the vertex
being updated. Intuitively, each neighboring vertex
votes for the cluster it is currently assigned to, where
the strength of the vote is determined by the similar-
ity (i.e., edge weight) to the vertex being updated.
The label li of vertex vi is thus updated according to
the following equation:
li? arg maxl?{1...L} ?v j?Ni(l)?(vi,v j) (2)
where Ni(l) = {v j|(vi,v j) ? E ? l = l j} denotes the
set of vi?s neighbors with label l. In other words,
for each label we compute a score by summing to-
gether the weights of edges to neighboring vertices
with that label and select the label with the maximal
score. Note that negative edges decrease the score
for a particular label, thus demoting the label.
Consider again Figure 1. Assume we wish to up-
date vertex A. In addition, assume that B and E are
currently assigned the same label (i.e., they belong
to the same cluster) whereas C and D are each in
different clusters. The score for cluster {B,E} is
0.4+0.8 = 1.2, the score for cluster {C} is 0.3 and
the score for cluster {D} is 0.2. We would thus as-
sign A to cluster {B,E} as it has the highest score.
The algorithm is run for several iterations. At
each iteration it passes over all vertices, and the up-
date order of the vertices is chosen randomly. As
the updates proceed, labels can disappear from the
graph, whereby the number of clusters decreases.
Empirically, we observe that for sufficiently many
iterations the algorithm converges to a fixed labeling
or oscillates between labelings that differ only in a
few vertices. The result of the algorithm is a hard
partitioning of the given graph, where the number of
clusters is determined automatically.
5.3 Propagation Prioritization
We make one important modification to the basic al-
gorithm described so far based on the intuition that
higher scores for a label indicate more reliable prop-
agations. More precisely, when updating vertex vi to
label l we define the confidence of the update as the
average similarity to neighbors with label l:
con f (li? l) = 1|Ni(l)| ?v j?Ni(l)?(vi,v j) (3)
We can then prioritize high-confidence updates by
setting a threshold ? and allowing only updates with
confidence greater or equal to ?. The threshold is
initially set to 1 (i.e., the maximal possible confi-
dence) and then lowered by some small constant ?
after each iteration until it reaches a minimum ?min,
at which point the algorithm terminates. This im-
proves the resulting clustering, since it promotes
reliable updates in earlier phases of the algorithm
which in turn has a positive effect on successive up-
dates.
5.4 Argument-Instance Similarity
As described earlier, the edge weights in our graph
are similarity scores, with positive values indicating
similarity and negative values indicating dissimilar-
ity. Determining the similarity function ? without
access to labeled training data poses a major diffi-
culty which we resolve by relying on prior linguis-
tic knowledge. Specifically, we measure the sim-
ilarity of argument instances based on three sim-
ple and intuitive criteria: (1) whether the instances
are lexically similar; (2) whether the instances oc-
cur in the same syntactic position; and (3) whether
the instances occur in the same frame (i.e., are argu-
ments in the same clause). The same criteria were
used in (Lang and Lapata, 2011) and shown effec-
tive in quantifying role-semantic similarity between
clusters of argument instances. Lexical and syntac-
tic similarity are scored through functions lex(vi,v j)
and syn(vi,v j) with range [?1,1], whereas the third
criterion enters the scoring function directly:
1324
?(vi,v j)=
{
?? if vi and v j are in same frame (4)
?lex(vi,v j)+(1??)syn(vi,v j) otherwise.
The first case in the function expresses a com-
mon linguistic assumption, i.e., that two argument
instances vi and v j occurring in the same frame can-
not have the same semantic role. The function im-
plements this constraint by returning??.3 The syn-
tactic similarity function s(vi,v j) indicates whether
two argument instances occur in a similar syntactic
position. We define syntactic positions through four
cues: the relation of the argument head word to its
governor, verb voice (active/passive), the linear po-
sition of the argument relative to the verb (left/right)
and the preposition used for realizing the argument
(if any). The score is S4 where S is the number of cueswhich agree, i.e., have the same value. The syntac-
tic score is set to zero when the governor relation
of the arguments is not the same. Lexical similar-
ity l(vi,v j) is measured in terms of the cosine of the
angle between vectors hi and h j representing the ar-
gument head words:
lex(vi,v j) = cos(hi,h j) = hi?h j?hi??h j? (5)
We obtain hi and h j from a simple semantic space
model (Turney and Pantel, 2010) which requires no
supervision (Section 6 describes the details of the
model used in our experiments).
Our similarity function weights the contribution
of syntax vs. semantics equally, i.e., ? is set to 0.5.
This reflects the linguistic intuition that lexical and
syntactic information are roughly of equal impor-
tance.
5.5 Relation to Other Models
This section briefly points out some connections to
related models. The averaging procedure used for
updating the graph vertices (Equation 2) appears in
some form in most label propagation algorithms (see
Talukdar (2010) for details). Label propagation al-
gorithms are commonly interpreted as random walks
3Formally, ? has range ran(?) = [?1,1] ? {??} and for
x ? ran(?) we define x+(??) =??. This means that the over-
all score computed for a label (Equation 2) is ?? if one of the
summands is ??.
2?
3 3
1
Figure 2: The update rule (Equation 2) can be under-
stood as choosing a minimal edge-cut, thereby greedily
maximizing intra-cluster similarity and minimizing inter-
cluster similarity. Assuming equal weight for all edges
above, label 3 is chosen for the vertex being updated such
that the sum of weights of edges crossing the cut is mini-
mal.
on graphs. In our case such an interpretation is
not directly possible due to the presence of negative
edge weights. This could be changed by transform-
ing the edge weights onto a non-negative scale, but
we find the current setup more expedient for model-
ing dissimilarity.
Our model could be also transformed into a prob-
abilistic graphical model that specifies a distribution
over vertex labels. In the transformed model each
vertex corresponds to a random variable over labels
and edges are associated with binary potential func-
tions over vertex-pairs. Let 1(vi = v j) denote an in-
dicator function which takes value 1 iff. li = l j and
value 0, otherwise. Then pairwise potentials can be
defined in terms of the original edge weights4 as
?(vi,v j) = exp(1(vi = v j)?(vi,v j)). A Gibbs sam-
pler used to sample from the distribution of the
resulting pairwise Markov random field (Bishop,
2006; Wainwright and Jordan, 2008) would employ
almost the same update procedure as in Equation 2,
the difference being that labels would be sampled
according to their probabilities, rather than chosen
deterministically based on scores.
A third way of understanding the update rule
is as a heuristic for maximizing intra-cluster sim-
ilarity and minimizing inter-cluster similarity. By
4Including weights with value zero and thus connecting all
vertex pairs.
1325
assigning the label with maximal score to vi, we
greedily maximize the sum of intra-cluster edge
weights while minimizing the sum of inter-cluster
edge weights, i.e., the weight of the edge-cut. This
is illustrated in Figure 2. Cut-based methods are
a common method in graph clustering (Schaeffer,
2007) and are also used for inference in pairwise
Markov random fields like the one described in the
previous paragraph (Boykov et al, 2001).
Note that while it would be possible to transform
our model into a model with a formal probabilistic
interpretation (either as a graph random walk or as a
probabilistic graphical model) this would not change
the non-empirical nature of the similarity function,
which is unavoidable in the unsupervised setting and
is also common in the semi-supervised methods dis-
cussed in Section 2.
6 Experimental Setup
In this section we describe how we assessed the
performance of our model. We discuss the dataset
on which our experiments were carried out, explain
how our system?s output was evaluated and present
the methods used for comparison with our approach.
Data We compared the output of our model
against the PropBank gold standard annotations con-
tained in the CoNLL 2008 shared task dataset (Sur-
deanu et al, 2008). The latter was taken from the
Wall Street Journal portion of the Penn Treebank
and converted into a dependency format (Surdeanu
et al, 2008). In addition to gold standard depen-
dency parses, the dataset alo contains automatic
parses obtained from the MaltParser (Nivre et al,
2007). The dataset provides annotations for ver-
bal and nominal predicate-argument constructions,
but we only considered the former, following previ-
ous work on semantic role labeling (Ma`rquez et al,
2008). All the experiments described in this paper
use the CoNLL 2008 training dataset.
Evaluation Metrics For each verb, we determine
the extent to which argument instances in the clus-
ters share the same gold standard role (purity) and
the extent to which a particular gold standard role is
assigned to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can be
then written as:
PU = 1N ?i maxj |G j ?Ci| (6)
Collocation is defined as follows. For each gold
role, we determine the cluster with the largest num-
ber of instances for that role (the role?s primary clus-
ter) and then compute the percentage of instances
that belong to the primary cluster for each gold role:
CO = 1N ?j maxi |G j ?Ci| (7)
Per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the
micro-average obtained by weighting the scores for
individual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
F1 = 2?CO?PUCO+PU (8)
Model Parameters Recall that our algorithm pri-
oritizes updates with confidence higher than a
threshold ?. Initially, ? is set to 1 and its value
decreases at each iteration by a small constant ?
which we set to 0.0025. The algorithm terminates
when a minimum confidence ?min is reached. While
choosing a value for ? is straightforward ? it sim-
ply has to be a small fraction of the maximal pos-
sible confidence ? specifying ?min on the basis of
objective prior knowledge is less so. And although
a human judge could determine the optimal termina-
tion point based on several criteria such as clustering
quality or the number of clusters, we used a develop-
ment set instead for the sake of reproducibility and
comparability. Specifically, we optimized ?min on
the CoNLL test set and obtained best results with
?min = 13 . This value was used for all our experi-ments and was also kept fixed for all verbs. Impor-
tantly, the development set was not used for any kind
of supervised training.
1326
Syntactic Function Latent Logistic Split-Merge Graph Partitioning
PU CO F1 PU CO F1 PU CO F1 PU CO F1
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 82.5 68.8 75.0
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 84.0 73.5 78.4
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 87.4 65.9 75.2
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 88.6 70.7 78.6
Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline
that assigns arguments to clusters based on their syntactic function.
0 10 20 30 40 50 60 70Average number of clusters per verb60
70
80
90
100
Clus
ter p
urity
 (%)
Figure 3: Purity (vertical axis) against average number
of clusters per verb (horizontal axis) on the auto/auto
dataset.
Recall that one of the components in our simi-
larity function is lexical similarity which we mea-
sure using a vector-based model (see Section 5.4).
We created such a model from the Google N-Grams
corpus (Brants and Franz, 2006) using a context
window of two words on both sides of the target
word and co-occurrence frequencies as vector com-
ponents (no weighting was applied). The large size
of this corpus allows us to use bigram frequencies,
rather than frequencies of individual words and to
distinguish between left and right bigrams. We used
randomized algorithms (Ravichandran et al, 2005)
to build the semantic space efficiently.
Comparison Models We compared our graph par-
titioning algorithm against three competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as a point of com-
parison by other unsupervised semantic role induc-
tion systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
0 100 200 300 400 500Number of iterations0
20
40
60
80
F1 sc
ore (%
)
Syntactic FunctionGraph Partitioning
Figure 4: F1 (vertical axis) against number of iterations
(horizontal axis) on the auto/auto dataset.
Our implementation allocates up to N = 21 clusters5
for each verb, one for each of the 20 most frequent
syntactic functions and a default cluster for all other
functions. We also compared our approach to Lang
and Lapata (2010) using the same model settings
(with 10 latent variables) and feature set proposed
in that paper. Finally, our third comparison model
is Lang and Lapata?s (2011) split-merge clustering
algorithm. Again we used the same parameters and
number of clusters (on average 10 per verb). Our
graph partitioning method uses identical cues for as-
sessing role-semantic similarity as the method de-
scribed in Lang and Lapata (2011).
7 Results
Our results are summarized in Table 1. We report
cluster purity (PU), collocation (CO) and their har-
monic mean (F1) for the baseline (Syntactic Func-
tion), our two previous models (the Latent Logistic
classifier and Split-Merge) and the graph partition-
ing algorithm on four datasets. These result from the
combination of automatic parses with automatically
identified arguments (auto/auto), gold parses with
5This is the number of gold standard roles.
1327
Syntactic Function
PU 91.4 68.6 45.1 59.7 62.4 61.9 63.5 75.9 76.7 69.6 63.1 53.7
CO 91.3 71.9 56.0 68.4 72.7 76.8 65.6 79.7 76.0 63.8 73.4 58.9
F1 91.4 70.2 49.9 63.7 67.1 68.6 64.5 77.7 76.3 66.6 67.9 56.2
Graph Partitioning
PU 95.6 83.5 72.3 75.4 83.3 84.4 74.8 84.8 89.5 83.0 73.2 66.3
CO 89.1 62.7 42.1 64.2 56.2 66.3 57.2 73.2 64.1 54.3 66.0 57.7
F1 92.2 71.6 53.2 69.4 67.1 74.3 64.8 78.5 74.7 65.7 69.4 61.7
Verb say make go increase know tell consider acquire meet send open break
Freq 15238 4250 2109 1392 983 911 753 704 574 506 482 246
Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the
syntactic function baseline; the scores were taken from a single run.
automatic arguments (gold/auto), automatic parses
with gold arguments (auto/gold) and gold parses
with gold arguments (gold/gold). Table 1 reports
averages across multiple runs. This was necessary
in order to ensure that the results of our randomized
graph partitioning algorithm are stable.6 The argu-
ments for the auto/auto and gold/auto datasets were
identified using the rules described in Lang and Lap-
ata (2011) (see Section 4). Bold-face is used to high-
light the best performing system under each measure
(PU, CO, or F1) on each dataset.
Compared to the Syntactic Function baseline,
the Graph Partitioning algorithm has higher F1 on
the auto/auto and auto/gold datasets but lags be-
hind by 0.5 points on the gold/auto dataset and
by 0.9 points on the gold/gold dataset. It attains
highest purity on all datasets except for gold/gold,
where it is 0.1 points below Split-Merge. When con-
sidering F1 in conjunction with purity and colloca-
tion, we observe that Graph Partitioning can attain
higher purity than the comparison models by trading
off collocation. If we were to hand label the clusters
output by our system, purity would correspond to the
quality of the resulting labeling, while collocation
would determine the labeling effort. The relation-
ship is illustrated more explicitly in Figure 3, which
plots purity against the average number of clusters
per verb on the auto/auto dataset. As the algorithm
6For example, on the auto/auto dataset and over 10 runs,
the standard deviation in F1 was 0.11 points in collocation 0.16
points and in purity 0.08 points. The worst F1 was 0.20 points
below the average, the worst collocation was 0.32 points be-
low the average and the worst purity was 0.17 points below the
average.
proceeds the number of clusters is reduced which
results in a decrease of purity. The latter decreases
more rapidly once the number of 20 clusters per verb
is reached. This is accompanied by a decreasing
tradeoff ratio between collocation and purity: at this
stage decreasing purity by one point increases collo-
cation by roughly one point, whereas in earlier itera-
tions a decrease of purity by one point goes together
with several points increase in collocation. This is
most likely due to the fact that the number of gold
standard classes is around 20.
Figure 4 shows the complete learning curve of our
graph partitioning method on the auto/auto dataset
(F1 is plotted against the number of iterations).
The algorithm naturally terminates at iteration 266
(when ?min = 1/3), but we have also plotted itera-
tions beyond that point. Since lower values of ? per-
mit unreliable propagations, F1 eventually falls be-
low the baseline (see Section 5.2). The importance
of our propagation prioritization mechanism is fur-
ther underlined by the fact that when it is not em-
ployed (i.e., when using the vanilla Chinese Whis-
pers algorithm without any modifications), it per-
forms substantially worse than the comparison mod-
els. On the auto/auto dataset, F1 converges to 59.1
(purity is 55.5 and collocation 63.2) within 10 itera-
tions.
Finally, Table 2 shows how performance varies
across verbs. We report results for the Syntac-
tic Function baseline and Graph Partitioning on the
auto/auto dataset for 12 verbs. These were selected
so as to exhibit varied occurrence frequencies and
alternation patterns. As can be seen, the macro-
1328
scopic result ? increase in F1 and purity ? also
holds across verbs.
8 Conclusions
In this paper we described an unsupervised method
for semantic role induction, in which argument-
instance graphs are partitioned into clusters repre-
senting semantic roles. The approach is conceptu-
ally and algorithmically simple and novel in its for-
malization of role induction as a graph partitioning
problem. We believe this constitutes an interesting
alternative for two reasons. Firstly, eliciting and
encoding problem-specific knowledge in the form
of instance-wise similarity judgments can be easier
than encoding it into model structure e.g., by mak-
ing statistical independence assumptions or assump-
tions about latent structure. Secondly, the approach
is general and amenable to other graph partitioning
algorithms and relates to well-known graph-based
semi-supervised learning methods.
The similarity function in this paper is by neces-
sity rudimentary, since it cannot be estimated from
data. Nevertheless, the resulting system attains com-
petitive F1 and notably higher purity than the com-
parison models. Arguably, performance could be
improved by developing a better similarity function.
Therefore, in the future we intend to investigate how
our system performs in a weakly supervised setting,
where the similarity function is estimated from a
small amount of labeled instances, since this would
allow us to incorporate richer syntactic features and
result in more precise similarity scores.
Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).
References
O. Abend and A. Rappoport. 2010. Fully unsupervised
core-adjunct argument classification. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 226?236, Uppsala,
Sweden.
O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28?36, Singapore.
S. Abney. 2007. Semisupervised Learning for Computa-
tional Linguistics. Chapman & Hall/CRC.
A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
119?127, Boulder, Colorado.
C. Biemann. 2006. Chinese Whispers: an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the First Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast Ap-
proximate Energy Minimization via Graph Cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222?1239.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia.
Z. Chen and H. Ji. 2010. Graph-based clustering for
computational linguistics: A survey. In Proceedings of
TextGraphs-5 - 2010 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 1?9, Up-
psala, Sweden.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 575?584, Cambridge, MA.
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547?619.
H. Fu?rstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11?20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245?288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192?199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1?8, Sydney, Australia.
1329
G. Haffari and A. Sarkar. 2007. Analysis of Semi-
Supervised Learning with the Yarowsky Algorithm. In
Proceedings of the 23rd Conference on Uncertainty in
Artificial Intelligence, Vancouver, BC.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691?696. AAAI Press / The MIT Press.
I. Klapaftis and Suresh M. 2010. Word sense induction
& disambiguation using hierarchical random graphs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 745?
755, Cambridge, MA.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939?
947, Los Angeles, California.
J. Lang and M. Lapata. 2011. Unsupervised Semantic
Role Induction via Split-Merge Clustering. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, Portland, Oregon.
To appear in.
L. Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145?
159.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95?135.
S. Pado? and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307?340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289?310.
D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Function for High Speed Noun Clustering.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 622629,
Ann Arbor, Michigan.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice, version 1.3. Technical report, In-
ternational Computer Science Institute, Berkeley, CA,
USA.
S. Schaeffer. 2007. Graph clustering. Computer Science
Review, 1(1):27?64.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12?21,
Prague, Czech Republic.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 167?176, Cambridge, MA.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8?15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159?177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95?102, Barcelona, Spain.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1473?1481, Uppsala, Sweden.
P. Talukdar. 2010. Graph-Based Weakly Supervised
Methods for Information Extraction & Integration.
Ph.D. thesis, CIS Department, University of Pennsyl-
vania.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
M. Wainwright and M. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Foundations and Trends in Machine Learning, 1(1-
2):1?305.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
1330
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13?16, Boulder, Colorado.
D. Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. In Proceedings of the 20th Interna-
tional Conference on Machine Learning, Washington,
DC.
1331
Similarity-Driven Semantic Role Induction
via Graph Partitioning
Joel Lang?
University of Geneva
Mirella Lapata??
University of Edinburgh
As in many natural language processing tasks, data-driven models based on supervised learning
have become the method of choice for semantic role labeling. These models are guaranteed to
perform well when given sufficient amount of labeled training data. Producing this data is
costly and time-consuming, however, thus raising the question of whether unsupervised methods
offer a viable alternative. The working hypothesis of this article is that semantic roles can
be induced without human supervision from a corpus of syntactically parsed sentences based
on three linguistic principles: (1) arguments in the same syntactic position (within a specific
linking) bear the same semantic role, (2) arguments within a clause bear a unique role, and
(3) clusters representing the same semantic role should be more or less lexically and distribu-
tionally equivalent. We present a method that implements these principles and formalizes the
task as a graph partitioning problem, whereby argument instances of a verb are represented as
vertices in a graph whose edges express similarities between these instances. The graph consists
of multiple edge layers, each one capturing a different aspect of argument-instance similarity,
and we develop extensions of standard clustering algorithms for partitioning such multi-layer
graphs. Experiments for English and German demonstrate that our approach is able to induce
semantic role clusters that are consistently better than a strong baseline and are competitive with
the state of the art.
1. Introduction
Recent years have seen increased interest in the shallow semantic analysis of natural
language text. The term is often used to describe the automatic identification and
labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky
2002). Semantic roles describe the relations that hold between a predicate and its
arguments (e.g., ?who? did ?what? to ?whom?, ?when?, ?where?, and ?how?)
abstracting over surface syntactic configurations. This type of semantic information
? Department of Computer Science, University of Geneva, 7 route de Drize, 1227 Carouge, Switzerland,
E-mail: Joel.Lang@unige.ch.
?? Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh,
10 Crichton Street, EH8 9AB, E-mail: mlap@inf.ed.ac.uk.
Submission received: 26 December 2012; revised version received: 19 September 2013; accepted for
publication: 20 November 2013.
doi:10.1162/COLI a 00195
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
is shallow but relatively straightforward to infer automatically and useful for the
development of broad-coverage, domain-independent language understanding
systems. Indeed, the analysis produced by existing semantic role labelers has been
shown to benefit a wide spectrum of applications ranging from information extraction
(Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine
translation (Wu and Fung 2009) and summarization (Melli et al. 2005).
In the example sentences below, window occupies different syntactic positions?it is
the object of broke in sentences (1a,b), and the subject in (1c). In all instances, it bears the
same semantic role, that is, the patient or physical object affected by the breaking event.
Analogously, ball is the instrument of break both when realized as a prepositional phrase
in (1a) and as a subject in (1b).
(1) a. [Jim]A0 broke the [window]A1 with a [ball]A2.
b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.
Also notice that all three instances of break in Example (1) have apparently similar
surface syntax with a subject and a noun directly following the predicate. However,
in sentence (1a) the subject of break expresses the agent role, in (1b) it expresses the
instrument role, and in (1c) the patient role.
The examples illustrate the fact that predicates can license several alternate map-
pings or linkings between their semantic roles and their syntactic realization. Pairs of
linkings allowed by a single predicate are often called diathesis alternations (Levin
1993). Sentence pair (1a,b) is an example of the instrument subject alternation, and
pair (1b,c) illustrates the causative alternation. Resolving the mapping between the
syntactic dependents of a predicate (e.g., subject, object) and the semantic roles that they
each express is one of the major challenges faced by semantic role labelers.
The semantic roles in the examples are labeled in the style of PropBank (Palmer,
Gildea, and Kingsbury 2005), a broad-coverage human-annotated corpus of semantic
roles and their syntactic realizations. Under the PropBank annotation framework each
predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose
interpretations are specific to that predicate1 and a set of adjunct roles such as location or
time whose interpretation is common across predicates (e.g., last night in sentence (1c)).
The availability of PropBank and related resources (e.g., FrameNet; Ruppenhofer et al.
2006) has sparked the development of a variety semantic role labeling systems, most
of which conceptualize the task as a supervised learning problem and rely on role-
annotated data for model training. Most of these systems implement a two-stage ar-
chitecture consisting of argument identification (determining the arguments of the
verbal predicate) and argument classification (labeling these arguments with semantic
roles). Current approaches deliver reasonably good performance?a system will recall
around 81% of the arguments correctly and 95% of those will be assigned a correct
semantic role (see Ma`rquez et al. [2008] for details), although only on languages and
domains for which large amounts of role-annotated training data are available.
Unfortunately, the reliance on labeled data, which is both difficult and expensive
to produce, presents a major obstacle to the widespread application of semantic role
labeling across different languages and text genres. Although corpora with semantic
1 More precisely, A0 and A1 have a common interpretation across predicates as proto-agent and proto-patient
in the sense of Dowty (1991).
634
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
role annotations exist nowadays in other languages (e.g., German, Spanish, Catalan,
Chinese, Korean), they tend to be smaller than their English equivalents and of limited
value for modeling purposes. Even within English, a language for which two major
annotated corpora are available, systems trained on PropBank demonstrate a marked
decrease in performance (approximately by 10%) when tested on out-of-domain data
(Pradhan, Ward, and Martin 2008). The data requirements for supervised systems and
the current paucity of such data has given impetus to the development of unsupervised
methods that learn from unlabeled data. If successful, unsupervised approaches could
lead to significant resource savings and the development of semantic role labelers
that require less engineering effort. Besides being interesting on their own right, from
a theoretical and linguistic perspective, unsupervised methods can provide valuable
features for downstream (supervised) processing and serve as a preprocessing step for
applications that require broad coverage understanding. In this article we study the
potential of unsupervised methods for semantic role labeling. As in the supervised case,
we decompose the problem into an argument identification step and an argument clas-
sification step. Our work primarily focuses on argument classification, which we term
role induction, because there is no predefined set of semantic roles in the unsupervised
case, and these must be induced from data. The goal is to assign argument instances to
clusters such that each cluster contains arguments corresponding to a specific semantic
role and each role corresponds to exactly one cluster.
Unsupervised learning is known to be challenging for many natural language
processing problems and role induction is no exception. Firstly, it is difficult to define
a learning objective function whose optimization will yield an accurate model. This
contrasts with the supervised setting, where the objective function can directly reflect
training error (i.e., some estimate of the mismatch between model output and the gold
standard) and the model can be tuned to replicate human output for a given input under
mathematical guarantees regarding the accuracy of the trained model. Secondly, it is
also more difficult to incorporate rich feature sets into an unsupervised model (Berg-
Kirkpatrick et al. 2010). Unless we explicitly know exactly how features interact, more
features may not necessarily lead to a more accurate model and may even decrease
performance. In the supervised setting, feature interactions relevant for a particular
learning task can be determined to a large extent automatically and thus a large number
of them can be included even if their significance is not clear a priori.
The lack of an extensional definition (in the form of training examples) of the target
concept makes a strong case for the development of unsupervised methods that use
problem specific prior knowledge. The idea is to derive a strong inductive bias (Gordon
and Desjardins 1995) based on this prior knowledge that will guide the learning towards
the correct target concept. For semantic role induction, we propose to build on the
following linguistic principles:
1. Semantic roles are unique within a particular frame.
2. Arguments occurring in a specific syntactic position within a specific linking
all bear the same semantic role.
3. The (asymptotic) distribution over argument heads is the same for two
clusters that represent the same semantic role.
We hypothesize that these three principles are, at least in theory, sufficient for
inducing high-quality semantic role clusters. A challenge, of course, lies in adequately
operationalizing them so that they guide the unsupervised learner towards meaningful
635
Computational Linguistics Volume 40, Number 3
solutions. The approach taken in this article translates these principles into estimates of
similarity (or dissimilarity) between argument instances and/or clusters of argument
instances. Principle (1) states that argument instances occurring in the same frame
(i.e., clause) cannot bear the same semantic role, and are thus dissimilar. From Prin-
ciple (2) it follows that arguments occurring in the same syntactic position within the
same linking can be considered similar (leaving aside for the moment the difficulty of
representing linkings through syntactic cues observable in a corpus). Principle (3) states
that two clusters of instances containing similar distributions over head words should
be considered similar.
Based on these similarity estimates we construct a graph whose vertices represent
argument instances and whose edges express similarities between these instances. The
graphs consist of multiple edge layers, each capturing one particular type of argument-
instance similarity. For example, one layer will be used to represent whether argument
instances occur in the same frame, and another layer will represent whether two argu-
ments have a similar head word, and so on. Given this graph representation of the data,
we formalize role induction as the problem of partitioning the graph into clusters of sim-
ilar vertices. We present two algorithms for partitioning multi-layer graphs, which are
adaptations of standard graph partitioning algorithms to the multi-layer setting. The al-
gorithms differ in the way they exploit the similarity information encoded in the graph.
The first one is based on agglomeration, where two clusters containing similar instances
are grouped into a larger cluster. The second one is based on propagation, where role-
label information is transferred from one cluster to another based on their similarity.
To understand how the aforementioned principles might allow us to handle the
ambiguity stemming from alternate linkings, consider again Example (1). The most
important thing to note is that, whereas the subject position is ambiguous with respect
to the semantic roles it can express (it can be A0, A1, or A2), we can resolve the
ambiguity by exploiting overt syntactic cues of the underlying linking. For example,
the predicate break is transitive in sentences (1a) and (1b), and intransitive in sentence
(1c). Thus, by taking into account the argument?s syntactic position and the predicate?s
transitivity, we can guess that the semantic role expressed by the subject in sentence (1c)
is different from the roles expressed by the subjects in sentences (1a,b). Now consider
the more difficult case of distinguishing between the subjects in sentences (1a) and (1b).
One linking cue that could help here is the prepositional phrase in sentence (1a), which
results in a syntactic frame different from sentence (1b). Were the prepositional phrase
omitted, we would attempt to disambiguate the linkings by resorting to lexical-semantic
cues (e.g., by taking into account whether the subject is animate). In sum, if we encode
sufficiently many linking cues, then the resulting fine-grained syntactic information will
discriminate ambiguous semantic roles. In cases where syntactic cues are not discerning
enough, we can exploit lexical information and group arguments together based on
their lexical content.
The remainder of this article is structured as follows. Section 2 provides an overview
of unsupervised methods for semantic role labeling. Sections 3 and 4 present the details
of our method, that is, how the graphs are constructed and partitioned. Role induction
experiments in English and German are described in sections 5 and 6, respectively.
Discussion of future work concludes in section 7.
2. Related Work
The bulk of previous work on semantic role labeling has focused on supervised methods
(Ma`rquez et al. 2008), although a few semi-supervised and unsupervised approaches
636
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
have been proposed. The majority of semi-supervised models have been developed
within a framework known as annotation projection. The idea is to combine labeled
and unlabeled data by projecting annotations from a labeled source sentence onto
an unlabeled target sentence within the same language (Fu?rstenau and Lapata 2009)
or across different languages (Pado? and Lapata 2009). Beyond annotation projection,
Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen
verbs by finding syntactically similar (labeled) verbs and using their annotations as
surrogate training data.
Swier and Stevenson (2004) were the first to introduce an unsupervised se-
mantic role labeling system. Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances is iteratively expanded using
a classifier trained on previously labeled instances. Their method starts with a data
set containing no role annotations at all, but crucially relies on VerbNet (Kipper,
Dang, and Palmer 2000) for identifying the arguments of predicates and making
initial role assignments. VerbNet is a manually constructed lexicon of verb classes,
each of which is explicitly associated with argument realization and semantic role
specifications.
In this article we will not assume the availability of any role-semantic resources,
although we do assume that sentences are syntactically analyzed. There have been
two main approaches to role induction from parsed data. Under the first approach,
semantic roles are modeled as latent variables in a (directed) graphical model that
relates a verb, its semantic roles, and their possible syntactic realizations (Grenager
and Manning 2006). Role induction here corresponds to inferring the state of the
latent variables representing the semantic roles of arguments. Following up on this
work, Lang and Lapata (2010) reformulate role induction as the process of detecting
alternations and finding a canonical syntactic form for them. Verbal arguments are
then assigned roles, according to their position in this canonical form, because each
position references a specific role. Their model extends the logistic classifier with
hidden variables and is trained in a manner that takes advantage of the close re-
lationship between syntactic functions and semantic roles. More recently, Garg and
Henderson (2012) extend the latent-variable approach by modeling the sequential order
of roles.
The second approach is similarity-driven and based on clustering. Lang and Lapata
(2011a) propose an algorithm that first splits the set of all argument instances of a verb
according to their syntactic position within a particular linking and then iteratively
merges clusters. A different clusstering algorithm is adopted in Lang and Lapata
(2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex
in the graph corresponds to an argument instance and edges represent a heuristically
defined measure of their lexical and syntactic similarity. The similarity-driven approach
has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian
clustering algorithm based on the Chinese Restaurant Process. In addition, they present
a method that shares linking preferences across verbs using a distance-dependent
Chinese Restaurant Process prior which encourages similar verbs to have similar
linking preferences. Titov and Klementiev (2012b) further introduce the use of multi-
lingual data for improving role induction.
There has also been work on unsupervised methods for argument identification.
Abend, Reichart, and Rappoport (2009) devise a method for recognizing the arguments
of predicates that relies solely on part of speech annotations, whereas Abend and
Rappoport (2010a) distinguish between core and adjunct roles, using an unsupervised
parser and part-of-speech tagger. More generally, shallow semantic representations
637
Computational Linguistics Volume 40, Number 3
induced from syntactic information are commonly used in lexicon acquisition and
information extraction tasks. For example, Lin and Pantel (2001) cluster syntactic re-
lations between pairs of words as expressed by parse tree paths into semantic relations
by exploiting lexical distributional similarity. Although not compatible with PropBank
or semantic roles as such, Poon and Domingos (2009) and Titov and Klementiev (2011)
also induce semantic information from dependency parses and apply it to a question
answering task for the biomedical domain. Another example is the work by Gamallo,
Agustini, and Lopes (2005), who cluster similar syntactic positions in order to develop
models of selectional preferences to be used for word sense induction and the resolution
of attachment ambiguities.
The work described here unifies the two clustering methods presented in Lang and
Lapata (2011a and 2011b) by reformulating them as graph partitioning algorithms. It
also extends them by utilizing multi-layer graphs which separate the similarities be-
tween instances on different features (e.g., part-of-speech, argument head) into different
layers. This has the advantage that similarity scores on individual features do not have
to be eagerly combined into a similarity score between instances. Instead, one can first
aggregate the similarity scores on each feature layer between two clusters and then
combine them into a similarity score between clusters. This is more robust, as the feature-
wise similarity scores between clusters can be computed in a principled way and the
heuristic combination step is deferred to the end (see Section 4 for details). Besides
providing a general modeling framework for semantic role induction, we discuss in
detail the linguistic principles guiding our modeling choices and assess their applica-
bility across languages. Specifically, we show that the framework presented here (and
the aforementioned principles) can be readily applied to English and German with
identical parametrizations for both languages and without fundamentally changing
the underlying model features, despite major syntactic differences between the two
languages.
3. Graph Construction
We begin by explaining how we construct a graph that represents verbs and their
arguments. Next, we describe how edge weights are computed?these translate to
similarity scores between argument instances?and then move on to provide the details
of our graph-partitioning algorithms.
As mentioned earlier, we formalize semantic role induction as a clustering problem.
Clustering algorithms (see Jain, Murty, and Flynn [1999] for an overview) commonly
take a matrix of pairwise similarity scores between instances as input and produce
a set of output clusters, often satisfying some explicitly defined optimality criterion.
The success or failure of the clustering approach is closely tied to the adequacy of
the employed similarity function for the task at hand. The graph partitioning view
of clustering (see Schaeffer [2007] for a detailed treatment) arises when instances are
represented as the vertices of a graph and the similarity matrix is interpreted as the
weight matrix of the graph. For semantic role induction, a straightforward application
of clustering would be to construct a graph for each verbal predicate such that vertices
correspond to argument instances of the verb and edge weights quantify the similarity
between these instances.
Lang and Lapata (2011b) hand-craft an instance similarity function by taking into
account different features such as the argument head or its syntactic position. Defin-
ing an appropriate instance-wise similarity function is nevertheless problematic as
638
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 1
A multi-layer graph consists of multiple edge layers, one for each similarity feature. Multi-layer
graph partitioning algorithms exploit this representation by computing separate similarity
scores between clusters for each feature layer and then combining them into a single overall
similarity score. This is advantageous over single-layer graph partitioning because it avoids
eagerly combining the similarity scores for individual features into a heuristic instance-wise
similarity score.
weights have to be chosen heuristically. Instead, we will represent similarities with
respect to different features on separate edge layers in the graph. For example, one
layer will represent the similarity between the head words of arguments and another
one will represent the similarity between pars of speech. So, given M features, the
graph will consist of M layers, one for each feature. Edge weights on a particular
layer quantify the similarity between the instances with respect to that feature. This
is illustrated in Figure 1 for two argument instances and three features. Formally, a
multi-layer graph is defined as a pair (V, {E1, . . . , EM}) consisting of vertices V and
a set of edge layers Ef for f = 1 . . .M. The set of vertices V = {v1, . . . , vN} consists
of all N argument instances for a particular verb. The edge layer Ef for feature f
is constructed by connecting all vertex-pairs with non-zero similarity with respect
to f :
Ef = {(vi, vj) ? V ? V|?f (vi, vj) = 0}. (2)
where ?f (vi, vj) is a similarity function for feature f , whose form will be discussed in the
next section. Each edge (vi, vj) ? Ef in layer f is weighted by ?f (vi, vj).
3.1 Feature Similarity Functions
Similarities for a specific feature f are measured with a function ?f (vi, vj) which assigns
a [?1, 1] value to any pair of instances (vi, vj). We assume similarities are measured on
an interval scale?that is, while sums, differences, and averages of the values of some
similarity function ?f express meaningful quantities, products and ratios do not. More-
over, the values of two distinct similarity functions cannot necessarily be meaningfully
compared without rescaling. Positive similarity values indicate that the semantic roles
are likely to be the same, negative values indicate that roles are likely to differ, and zero
values indicate that there is no evidence for either case. The magnitude of ?f expresses
the degree of confidence in the similarity judgment, with extreme values (i.e., ?1 and 1)
indicating maximal confidence.
In our model, we simply use indicator functions which output either 1 or ?1 iff
feature values are equal and 0 otherwise. Specifically, we define four feature similarity
functions that we derive from the principles discussed in Section 1. Our similarity
functions are based on the following features: the argument head words and their parts
639
Computational Linguistics Volume 40, Number 3
of speech,2 the frame constraint, and the syntactic position within a particular linking.
We measure lexical and part-of-speech similarity as follows:
?lex(vi, vj) =
{
1 if vlexi = v
lex
j
0 otherwise
?pos(vi, vj) =
{
1 if vposi = v
pos
j
0 otherwise.
(3)
The constraint that two argument instances vi and vj occurring in the same frame
cannot have the same semantic role is captured by the following similarity function:
?frame(vi, vj) =
{
?1 if vframei = v
frame
j
0 otherwise.
(4)
Finally, we also measure syntactic similarity through an indicator function
?syn(vi, vj), which assumes value 1 if two instances occur in the same syntactic position
within the same linking:
?syn(vi, vj) =
{
1 if vsyni = v
syn
j
0 otherwise.
(5)
The syntactic position of an argument is directly given by the parse tree and can
be encoded, for example, by the full path from predicate to argument head, or for
practical purposes, in order to reduce sparsity, simply through the relation governing
the argument head and its linear position relative to the predicate (left or right). In
contrast, linkings are not directly observed, but we can resort to overt syntactic cues
as a proxy. Examples include the verb?s voice (active/passive), whether it is transitive,
the part-of-speech of the subject, and so on. We argue that in principle, if sufficiently
many cues are taken into account, they will capture one particular linking, although
there may be several encodings for the same linking. Note that syntactic similarity is
not used to construct another graph layer; rather, it will be used for deriving initial
clusters of instances, as we explain in Section 4.1.
4. Graph Partitioning
The graph partitioning problem consists of finding a set of clusters {c1, . . . , cS} that
form a partition of the vertex-set, namely, ?ici = V and ci ? cj = ? for all i = j, such that
(ideally) each cluster contains argument instances of only one particular semantic role,
and the instances for a particular role are all assigned to one and the same cluster. In the
following sections we provide two algorithms for multi-layer graph partitioning, based
on standard clustering algorithms for single-layer graphs. Both algorithms operate
on the same graph but differ in terms of the underlying clustering mechanism they
use. The first algorithm is an adaptation of agglomerative clustering (Jain, Murty, and
Flynn 1999) to the multi-layer setting: Starting from an initial clustering, the algorithm
2 We include parts of speech as a simple means of alleviating the sparsity of head words.
640
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
iteratively merges vertex clusters in order to arrive at increasingly accurate representa-
tions of semantic roles. Rather than greedily merging clusters, our second algorithm is
based on propagating cluster membership information among the set of initial clusters
(Abney 2007).
4.1 Agglomerative Graph Partitioning
The agglomerative algorithm induces clusters in a bottom?up manner starting from an
initial cluster assignment that we will subsequently discuss in detail. Our initialization
results in a clustering that has high purity but low collocation, that is, argument
instances in each cluster tend to belong to the same role but argument instances of
a particular role are scattered among many clusters.3 The algorithm then improves
collocation by iteratively merging pairs of clusters. The agglomeration procedure is
described in Algorithm 1. As can be seen, pairs of clusters are merged iteratively until
a termination criterion is met. The decision of which cluster pair to merge at each
step is made by scoring a set of candidate cluster pairs and choosing the highest one
(line 5). The scoring function s(ci, cj? ) quantifies how likely two clusters are to contain
arguments of the same role. A key question is how to define this scoring function on
the basis of the underlying graph representation, that is, with reference to the instance
similarities expressed by the edges. In order to collect evidence for or against a merge,
we take into account the connectivity of a cluster pair at each feature layer of the graph.
This crucially involves aggregating over all edges that connect the two clusters, and
allows us to infer a cluster-level similarity score from the individual instance-level
similarities encoded in the edges. The evidence collected at each layer is then combined
together in order to arrive at an overall decision (see Figure 1 for an illustration).
3 We define the terms purity and collocation more formally in Section 5.4.
641
Computational Linguistics Volume 40, Number 3
Although it would be possible to enumerate and score all possible cluster pairs
at each step, we apply a more efficient and effective procedure in which the set of
candidates consists of pairs formed by combining a fixed cluster ci with all clusters c?j
larger than ci. This requires comparing only O(|C|) rather than O(|C|2) scores and, more
importantly, it favors merges between large clusters whose score can be computed more
reliably. As mentioned earlier, our scoring function implements an averaging procedure
over the instances contained in the clusters, and thus yields less noisy scores when
clusters are large (i.e., contain many instances). This prioritization promotes reliable
merges over less reliable ones in the earlier phases of the algorithm with a positive
effect on merges in the later phases. Moreover, by keeping ci fixed, we only require that
scores s(ci, x) and s(ci, z) are comparable (i.e., where one cluster is argument in both
scores), rather than comparisons between arbitrary cluster pairs (e.g., s(w, x) and s(y, z)).
In the following, we will provide details on the initialization of the algorithm and the
computation of the similarity scoring function.
A standard agglomerative clustering algorithm forms clusters bottom?up by ini-
tially placing each item of interest in its own cluster. In our case, initializing the algo-
rithm with as many clusters as argument instances would result in a clustering with
maximal purity and minimal collocation. There are two reasons that justify a more
sophisticated initialization procedure for our problem. Firstly, the scoring function we
use is more reliable for larger clusters than for smaller clusters (see the subsequent
discussion). In fact, the standard initialization that creates clusters with a single instance
would not yield useful results as our scoring function crucially relies on initial clusters
containing several instances on average. Secondly, the similarity scores for different
features are not directly comparable. Recall from Section 3.1 that we introduced different
types of similarities based on the arguments? head words (?lex), parts-of-speech (?pos),
syntactic positions (?syn), and frame constraints (?frame). As discussed earlier, engineer-
ing a scoring function that integrates these into a single score without resorting to
heuristic judgments on how to weight them poses a major challenge. In particular, it
is difficult to weight the contribution of the two forms of positive evidence given by
lexical and syntactic similarity. This motivates the idea of using syntactic similarity
for initialization, and lexical similarity (as well as the frame constraint) for scoring.
This separation avoids the difficulty of defining the exact interaction between the two.
Specifically, we obtain an initial clustering by grouping together all instances which
occur in the same syntactic position within a linking?that is, all pairs (vi, vj) for which
?syn(vi, vj) = 1 are grouped into the same cluster, assuming that arguments occurring in
a specific syntactic position under a specific linking share the same role.
We specify the syntactic position of an argument using four cues: the verb?s voice
(active/passive), the argument?s linear position relative to the predicate (left/right),
the syntactic relation of the argument to its governor (e.g., subject or object), and the
preposition used for realizing the argument (if any). Each argument is assigned a four-
tuple consisting of these cues and two syntactic positions are assumed equal iff they
agree on all cues.
Whereas the similarity functions defined in Section 3.1 measure role-semantic
similarity between instances on a particular feature, the scoring function measures role-
semantic similarity between clusters. Naturally, the similarity between two clusters is
defined in terms of the similarities of the instances contained in the clusters. This
involves two aggregation stages. Initially, instance similarities are aggregated in each
feature layer, resulting in an aggregate score for each feature. These layer-specific scores
are then integrated into a single score, which quantifies the overall similarity between
the two clusters (see Figure 1).
642
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
An obvious way to determine the similarity between two clusters (with respect to
a particular feature f ) would be to analyze their connectivity. For example, we could
use edge density (Schaeffer 2007) to average over the weights of edges between two
clusters. However, edge density is an inappropriate measure of similarity in our case,
because we cannot assume that arbitrary pairs of instances are similar with respect to
a particular feature, even if two clusters represent the same semantic role. Consider for
example lexical similarity: Most head words will not agree (even within a cluster) and
therefore averaging between all pairs would yield low scores, regardless of whether
the clusters represent the same role or not. Analogously, the vast majority of instance
pairs from any two clusters will belong to different frames, and thus averaging over
all possible pairs of instances would not yield indicative scores.
We therefore adopt an averaging procedure which finds, for each instance in one
cluster, the instance in the other cluster that is maximally similar or dissimilar and
averages over the scores of these alignments:
sf (ck, cl) =
1
Nk + Nl
?
?
?
vi?ck
abs max
vj?cl
?f (vi, vj) +
?
vj?cl
abs max
vi?ck
?f (vi, vj)
?
? (6)
Here, abs max is a functional that returns the extreme value of its argument, either
positive or negative: abs maxx?X g(x) = g(arg maxx?X |g(x)|). Note that the alignments
are unconstrained in the sense that va ? ck can be aligned to vb ? cl in the first term
of Equation (6), while vb can be aligned to some other instance in the second term.
Moreover, alignments in each term are many-to-one, namely, multiple instances from ck
can be aligned to the same vb ? cl in the first term and likewise in the second term. This
means that score aggregation does not reflect the distributional properties of clusters
(e.g., the frequency of head words in each cluster). Consider for example two clusters
with an identical set of head words. Because many-to-one alignments are allowed, each
instance can be aligned with maximal score to some other instance regardless of the
frequencies of these words.
As an alternative, we also use the well-known cosine similarity function?although
only for the features based on argument head words (lex) and parts of speech (pos):
sf (ck, cl) =
x fk ? x
f
l
?x fk ??x
f
l ?
. (7)
Here x fk and x
f
l are vector representations of the cluster containing as components
the occurrence frequencies of a particular value of the feature f (i.e., lex and pos in
our case). Another solution would be to enforce one-to-one alignments and redefine
Equation (6) as the optimal bipartite matching between the two clusters. Although
this solution adheres to the graph formulation (in contrast to Equation (7)) we see no
theoretical reason that makes it superior to cosine similarity. Moreover, its computation
would require cubic runtime in the number of vertices using the Hungarian algorithm
(Munkres 1957), which is prohibitively slow for sufficiently large clusters.
Layer-specific similarity scores must be combined into an overall cluster similar-
ity score. Because similarity scores and their aggregates for different features are not
directly comparable, their combination through summation would require weighting
each layer score according to its relative strength. Due to the difficulty of specifying
these weights without access to labeled training data, we propose an alternative scheme
643
Computational Linguistics Volume 40, Number 3
that is based on the distinction between positive and negative evidence. Negative
evidence is used to rule out a merge, whereas positive evidence provided by the lexical
score is used to score merges that have not yet been ruled out:
s(ck, cl) =
?
?
?
?
?
?
?
?1 if sframe(ck, cl) < ?
?1 if spos(ck, cl) < ?
slex(ck, cl) if slex(ck, cl) > ?
0 otherwise.
(8)
When the part-of-speech similarity is below a certain threshold ?, or when clause-level
constraints are satisfied to a lesser extent than threshold ?, the score takes value ?1
and the merge is ruled out. If the merge is not ruled out, the lexical similarity score
determines the magnitude of the overall score, provided that it is above threshold ?.
Otherwise, the function returns 0, indicating that neither strong positive nor negative
evidence is available. The cluster-similarity scoring function can be viewed as the
decision function of a binary classifier for deciding on whether to merge a particular
pair of clusters. The classifier is informed by the similarity scores for each feature
layer and outputs a confidence-weighted decision (positive/negative), where the sign
sgn(?f (vi, vj)) indicates the decision and the absolute value |?f (vi, vj)| quantifies confi-
dence. The scoring function in Equation (8) essentially implements a simple decision list
classifier, whose decision rules are sequentially inspected from top to bottom, applying
the first matching rule.
Although our definition avoids weighting, it has introduced threshold parame-
ters ?, ?, and ? that we need to somehow estimate. We propose a scheme in which
parameters ? and ? are iteratively adjusted, and ?, the threshold determining the
extent to which the frame constraints can be violated, is kept fixed. We heuristically set
? to ?0.05, based on the intuition that in principle frame constraints must be satisfied
although in practice, due to noise we expect a small number of violations (i.e., at most
5% of instances can violate the constraint). Parameters ? and ? are initially set to their
maximal value 1, thereby ruling out all merges except those with maximal confidence.
The parameters then decrease iteratively according to a routine whose pseudo-code
is specified in Algorithm 2. The parameter ? decreases at each iteration by a small
amount (0.025) until it reaches  = 0.025, at which point its value is reset to 1.0 and ?
is discounted by a factor close to one (0.9). This is repeated until ? falls below , upon
which the algorithm terminates.
644
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Runtime Analysis. As described in the previous section, Algorithm 1 stops when the
threshold ? falls below some small value . Both ? and ? iteratively decrease based on
a fixed scheme. The outer loop and starting in line 1 is therefore computed in constant
time T. Each pass through the inner loop starting at line 4 iterates over O(|C|) clusters
and for each one of them a score with O(|C|) other clusters is computed. Assume that
fi denotes the fraction of all V instances in cluster ci, namely, fiV = |ci| and
?
|C|
i=1 fi = 1.
Then, overall, the number of instance-wise similarities we need to evaluate is at most
O(|V|2):
|C|
?
i=1
|C|
?
j=i+1
(fi|V|)(fj|V|) =
1
2
|C|
?
i=1
|C|
?
j=1
(fi|V|)(fj|V|) ?
1
2
|C|
?
i=1
(fi|V|)
2
?|V|2
|C|
?
i=1
|C|
?
j=1
fifj = |V|
2
|C|
?
i=1
fi
|C|
?
j=1
fj = |V|
2
The total runtime in terms of the input is therefore O(T ? |V|2). Although this could be
prohibitively inefficient for large data sets, we did not observe long runtimes in our
experiments. Various optimizations are conceivable?for example, the cluster similarity
scores in line 5 of Algorithm 1 can be cached such that they only need to be recomputed
when a cluster changes (i.e., it is merged with another cluster).
4.2 Multi-Layer Label Propagation
Our second graph partitioning algorithm is based on the idea of propagating cluster
membership information along the edges of a graph, subsequently referred to as propa-
gation graph. As we explain in more detail subsequently, compared with agglomerative
clustering, this algorithm in principle is less prone to making false greedy decisions
that cannot be later revoked. Moreover, it has lower runtime and thus scales better to
larger data sets.
The propagation graph is created by collapsing vertices of the initial multi-layer
graph. Vertices in the propagation graph represent an atomic set of instances of the
original graph, that is, a group of instances that are always assigned to the same
cluster. For our induction problem, the vertices of the propagation graph correspond
to the initial clusters of the agglomerative algorithm discussed in Section 4.1. More
formally, let ai ? A denote the i-th vertex of the propagation graph, which references
an atomic cluster of vertices {vi1 . . . viNi} of the original graph that occur in the same
syntactic position within the same linking. Because each vertex of the propagation graph
corresponds to a cluster of vertices in the original graph, the edges of the propagation
graph can be defined in terms of the edges between these vertices in the original graph.
We reuse Equations (6) and (7) to define the edge weights of the propagation graph
as aggregates over the edge weights in the original graph. For each feature layer we
define the set of edges as:
Bf = {(ai, aj) ? A ? A|sf (ai, aj) = 0} (9)
Each edge (ai, aj) ? Bf in layer f is accordingly weighted by sf (ai, aj). Each vertex ai
is associated with a label li, indicating the partition that ai and all the vertices in the
original graph that have been collapsed into ai belongs to.
645
Computational Linguistics Volume 40, Number 3
Note that the label propagation algorithm is informed by the same similarity func-
tions as agglomerative clustering and uses an identical initialization procedure but pro-
vides an alternative means of cluster inference. Initially, each vertex of the propagation
graph belongs to its own cluster, that is, we let the number of clusters L = |A| and set
li ? i. Given this initial vertex labeling, the algorithm proceeds by iteratively updating
the label for each vertex (lines 4?10 in Algorithm 3). This crucially relies on a scoring
procedure in which a score s(l) is computed for each possible label l. We discuss the
details of the scoring procedure below.
The label scoring procedure required in line 5 of Algorithm 3 has parallels to the
cluster pair scoring procedure of the agglomerative algorithm. It also consists of two
stages: Initially, evidence is collected independently on each feature layer by computing
label score aggregates with respect to each feature and then these feature scores are
combined in order to arrive at an overall score.
Assume we are updating vertex ai. The first step is to compute the score for each
feature f and each label l:
sf (l) =
?
aj?Ni(l)
sf (ai, aj) (10)
where Ni(l) = {aj|(ai, aj) ? Bf ? l = lj ? |aj| > |ai|} denotes the set of ai?s neighbors with
label l that are larger than ai. Intuitively, each neighboring vertex votes for the cluster it
is currently assigned to, where the strength of the vote is determined by the similarity
to the vertex (i.e., edge weight) being updated. The votes of all (larger) neighboring
vertices are counted together, resulting in a score for each possible label. The condition
of including only larger vertices for computing the score is analogous to the prioriti-
zation mechanism of the agglomerative algorithm (only merges with larger clusters are
considered for a given candidate cluster). We impose this restriction for the same reason,
namely, that scores for larger clusters are more reliable.
Given the scores sf (l) for a particular label l on each layer f , our goal then is to com-
bine them into a single overall score s(l) for the label. As in agglomerative partitioning,
combining these scores through summation is not possible without ?guessing? their
weights, and therefore we use a sequential combination instead:
s(l) =
?
?
?
?
?
?
?
?1 if sframe(l) < ?
?1 if spos(l) < ?
slex(l) if slex(l) > ?
0 otherwise.
(11)
Analogously to Equation (8), negative evidence that stems from part-of-speech informa-
tion or frame constraints can veto a propagation, whereas positive evidence stemming
from argument head words can promote a propagation. If neither strong evidence
(positive or negative) is available, the label is assigned a zero score. Note that the
scoring function has three parameters with an identical interpretation to those in the
scoring function of the agglomerative algorithm. The threshold update that takes place
in line 11 of Algorithm 3 is therefore the same as the one described in Section 4.1 for
the agglomerative algorithm.
We now analyze the runtime of our algorithm. Let T denote the number of iterations
of the outer loop starting at line 1 of Algorithm 3. The inner loop starting at line 4 iterates
over |A| clusters and for each one of them it has to evaluate at most |A| neighboring
646
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
nodes. Additionally, there are the one-time costs of computing the similarities between
atomic clusters which take O(|V|2) time. The total runtime is therefore O(T|A|2 + |V|2).
Because |A|2 << |V|2, label propagation is substantially faster than agglomerative
clustering.
4.3 Relationship to Single-Layer Graph Partitioning
Clustering algorithms typically assume instance-wise similarities as input (i.e., single-
layer graphs). For our role induction problem, this would require a heuristically defined
similarity function that combines the similarities on individual features into a single
similarity score between instances. In other words, we would collapse the multiple
graph layers into a single layer and then partition the resulting single-layer graph
according to a standard clustering algorithm. A main difference between the two ap-
proaches is the order in which similarities are aggregated: Whereas multi-layer graph
partitioning aggregates similarities on each feature layer first and then combines them
into an overall cluster-wise similarity score, in the single-layer case feature similarities
are eagerly combined into an overall instance-wise similarity score and then aggregated.
Thus, in the multi-layer setting, aggregation can be done in a principled way by con-
sidering the individual feature layers in isolation. For large clusters the resulting scores
for each feature layer will provide reliable evidence for or against a merge. Combining
these cluster-wise similarity scores is much less error-prone than the eager combination
at the instance-level used by the single-layer approach. We experimentally confirm this
intuition (see Section 5.5) by comparing against the single-layer partitioning algorithm
presented in Lang and Lapata (2011b).
5. Role Induction Experiments on English
We adopt the general architecture of supervised semantic role labeling systems where
argument identification and argument classification are treated separately. Our role
labeler is fully unsupervised with respect to both tasks?it does not rely on any role
647
Computational Linguistics Volume 40, Number 3
annotated data or semantic resources. However, our system does not learn from raw
text. In common with most semantic role labeling research, we assume that the input is
syntactically analyzed. Our approach is not tied to a specific syntactic representation?
both constituent- and dependency-based representations can be used. The bulk of
our experiments focus on English data and a dependency-based representation that
simplifies argument identification considerably and is consistent with the CoNLL 2008
benchmark data set used for evaluation in our experiments. To show that our method
can be applied to other languages and across varying syntactic representations, we
also report experiments on German using a constituent-based representation (see
Section 6).
Given the parse of a sentence, our system identifies argument instances and as-
signs them to clusters. Thereafter, argument instances can be labeled with an identifier
corresponding to the cluster they have been assigned to, similar to PropBank core labels
(e.g., A0, A1). We view argument identification as a syntactic processing step that can be
largely undertaken deterministically through analysis of the syntactic tree. We therefore
use a small set of rules to detect arguments with high precision and recall. In the follow-
ing, we first describe the data set (Section 5.1) on which our experiments were carried
out. Next, we present the argument identification component of our system (Section 5.2)
and the method used for comparison with our approach. Finally, we explain how system
output was evaluated (Section 5.4).
5.1 Data
For evaluation purposes, we ran our method on the CoNLL 2008 shared task data set
(Surdeanu et al. 2008), which provides PropBank style gold standard annotations. As
our algorithm induces verb-specific roles, PropBank annotations are a natural choice of
gold standard for our problem. The data set contains annotations for verbal and nominal
predicate-argument constructions, but we only considered the former. The CoNLL data
set was taken from the Wall Street Journal portion of the Penn Treebank and converted
into a dependency format (Surdeanu et al. 2008). Input sentences are represented in
the dependency syntax specified by the CoNLL 2008 shared task (see Figure 2 for an
example). In addition to gold standard dependency parses, the data set also contains
automatic parses obtained from the MaltParser (Nivre et al. 2007), which we will use
as an alternative in our experiments in order to assess the impact of parse quality. For
each argument only the head word is annotated with the corresponding semantic role,
rather than the whole constituent. We assume that argument heads are content words
(e.g., the head of a prepositional phrase is the nominal head rather than the preposition).
We do not treat split arguments or co-referential arguments (e.g., in relative clauses).
Specifically, we ignore arguments with roles preceded by the C- or R- prefix in the
gold standard. All argument lemmas were normalized to lower case; we also replaced
numerical quantities with a placeholder; to further reduce data sparsity, we identified
Figure 2
A sample dependency parse with dependency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative complement), PRD (predicative complement),
and IM (infinitive marker).
648
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 1
Argument identification rules for English.
1. Discard a candidate if it is a coordinating conjunction or punctuation.
2. Discard a candidate if the path of relations from predicate to candidate ends with
coordination, subordination, etc. (see Appendix A for the full list of relations).
3. Keep a candidate if it is the closest subject (governed by the subject-relation) to the left of
a predicate and the relations from predicate p to the governor g of the candidate are all
upward-leading (directed as g ? p).
4. Discard a candidate if the path between the predicate and the candidate, excluding the
last relation, contains a subject relation, adjectival modifier relation, etc. (see Appendix A
for the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if it is directly connected to the predicate.
7. Keep a candidate if the path from predicate to candidate leads along several verbal nodes
(verb chain) and ends with an arbitrary relation.
8. Discard all remaining candidates.
the head of proper noun phrases heuristically as the most frequent lemma contained in
the phrase.
5.2 Argument Identification
In the supervised setting, a classifier is used in order to decide for each node in the parse
tree whether it represents a semantic argument or not. Nodes classified as arguments
are then assigned a semantic role. In the unsupervised setting, we slightly reformulate
argument identification as the task of discarding as many non-semantic arguments as
possible. This means that the argument identification component does not make a final
positive decision for any of the argument candidates; instead, this decision is deferred to
role induction.4 We assume here that predicate identification is a precursor to argument
identification and can be done relatively straightforwardly based on part-of-speech
information.
The rules given in Table 1 are used to discard or select argument candidates for
English. They primarily take into account the parts of speech and the syntactic relations
encountered when traversing the dependency tree from predicate to argument. A priori,
all words in a sentence are considered argument candidates for a given predicate. Then,
for each candidate, the rules are inspected sequentially and the first matching rule is
applied. We will exemplify how the argument identification component works for the
predicate expect in the sentence The company said it expects its sales to remain steady whose
parse tree is shown in Figure 2. Initially, all words except the predicate itself are treated
as argument candidates. Then, the rules from Table 1 are applied as follows. Firstly,
the words the and to are discarded based on their part of speech (Rule 1); then, remain
is discarded because the path ends with the relation IM and said is discarded as the
4 A few supervised systems implement a similar definition (Koomen et al. 2005), although in most cases
the argument identification component makes a final positive or negative decision regarding the status of
an argument candidate.
649
Computational Linguistics Volume 40, Number 3
path ends with an upward-leading OBJ relation (Rule 2). Rule 3 matches to it, which is
therefore added as a candidate. Next, steady is discarded because there is a downward-
leading OPRD relation along the path and the words company and its are also discarded
because of the OBJ relations along the path (Rule 4). Rule 5 does not apply but the word
sales is kept as a likely argument (Rule 6). Finally, Rule 7 does not apply, because there
are no candidates left.
On the CoNLL 2008 training set, our argument identification rules obtain a pre-
cision of 87.0% and a recall of 92.1% on gold standard parses. On automatic parses,
precision is 79.3% and recall 84.8%. Here, precision measures the percentage of selected
arguments that are actual semantic arguments, and recall measures the percentage of
actual arguments that are not filtered out.
Grenager and Manning (2006) also devise rules for argument identification, un-
fortunately without providing any details on their implementation. More recently, at-
tempts have been made to identify arguments without relying on a treebank-trained
parser (Abend and Rappoport 2010b; Abend, Reichart, and Rappoport 2009). The idea
is to combine a part-of-speech tagger and an unsupervised parser in order to identify
constituents. Likely arguments can be in turn identified based on a set of rules and
the degree of collocation with the predicate. Perhaps unsurprisingly, this method does
not match the quality of a rule-based component operating over trees produced by a
supervised parser.
5.3 Baseline Method for Semantic Role Induction
The linking between semantic roles and syntactic positions is not arbitrary; specific
semantic roles tend to map onto specific syntactic positions such as subject or object
(Levin and Rappaport 2005; Merlo and Stevenson 2001). We further illustrate this
observation in Table 2, which shows how often individual semantic roles map onto
certain syntactic positions. The latter are simply defined as the relations governing the
argument. The frequencies in the table were obtained from the CoNLL 2008 data set and
are aggregates across predicates. As can be seen, semantic roles often approximately
correspond to a single syntactic position. For example, A0 is commonly mapped onto
subject (SBJ), whereas A1 is often realized as object (OBJ).
This motivates a baseline that directly assigns instances to clusters according to
their syntactic position. The pseudo-code is given in Algorithm 4. For each verb we
allocate N = 22 clusters (the maximal number of gold standard clusters together with a
default cluster). Apart from the default cluster, each cluster is associated with a syntactic
position and all instances occurring in that position are mapped into the cluster. Despite
being relatively simple, this baseline has been previously used as a point of comparison
by other unsupervised semantic role labeling systems (Grenager and Manning 2006;
Lang and Lapata 2010) and shown difficult to outperform. This is partly due to the fact
that almost two thirds of the PropBank arguments are either A0 or A1. Identifying these
two roles correctly is therefore the most important distinction to make, and because this
can be largely achieved on the basis of the arguments? syntactic position (see Table 2),
the baseline yields high scores.
5.4 Evaluation
In this section we describe how we assess the quality of a role induction method that
assigns labels to units that have been identified as likely arguments. We also discuss
how we measure whether differences in model performance are statistically significant.
650
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 2
Contingency table between syntactic position and semantic roles. Only the eight most frequent
syntactic positions and their labels are listed (i.e., SBJ (Subject), OBJ (Object), ADV (Adverbial),
TMP (Temporal), PMOD (Preposition and its child), OPRD (Object complement), LOC
(Location), DIR (Direction)). Counts were obtained from the CoNLL 2008 training data set using
gold standard parses. The marginals in the right-most column include all syntactic positions
(not only the eight most frequent ones). Boldface highlights the most frequent role per syntactic
position (e.g., SBJ is frequently A0, OBJ is A1).
SBJ OBJ ADV TMP PMOD OPRD LOC DIR Total
A0 50,473 3,350 145 4 2,464 28 12 0 60,398
A1 18,090 50,986 3,207 45 4,819 3,489 118 170 83,535
A2 1,344 2,741 6,413 74 774 2,440 606 800 19,585
A3 88 254 1,208 37 116 114 63 940 3,359
A4 6 20 351 7 79 34 28 2,089 2,687
A5 0 0 19 0 1 3 0 28 67
AA 10 1 0 0 1 0 0 0 13
ADV 7 46 7,364 33 55 31 103 2 8,070
CAU 3 6 215 14 5 0 8 0 1,178
DIR 0 3 304 2 5 1 19 639 1,123
DIS 0 3 3,326 47 2 0 15 0 4,823
EXT 1 6 418 0 6 3 23 4 621
LOC 18 32 358 15 127 2 5,076 9 5,831
MNR 7 54 2,285 22 59 36 154 6 6,238
MOD 9 2,130 77 22 69 3 6 0 9,030
NEG 0 0 3,078 39 0 0 0 0 3,172
PNC 1 11 458 4 4 292 8 4 2,231
PRD 0 2 41 0 0 11 2 0 66
PRT 0 0 0 0 0 0 0 0 2
REC 0 5 8 0 0 0 0 0 14
TMP 14 93 969 14,465 141 1 42 15 16,086
Total 70,071 59,744 30,248 14,830 8,730 6,488 6,285 4,706 228,129
Arguments are labeled based on the cluster they have been assigned to, which
means that in contrast to the supervised setting we cannot verify the correctness of
these labels directly (e.g., by comparing them to the gold standard). Instead, we will
look at the induced clusters as a whole and assess their quality in terms of how well
they reflect the assumed gold standard. Specifically, for each verb, we determine the
extent to which argument instances in the clusters share the same gold standard role
(purity) and the extent to which a particular gold standard role is assigned to a single
cluster (collocation).
More formally, for each group of verb-specific clusters we measure cluster purity
as the percentage of instances belonging to the majority gold class in their respective
cluster. Let N denote the total number of instances, Gj the set of instances belonging to
the j-th gold class, and Ci the set of instances belonging to the i-th cluster. Purity can be
then written as
PU = 1N
?
i
max
j
|Gj ? Ci| (12)
Collocation is the inverse of purity (van Rijsbergen 1974) and defined as follows.
For each gold role, we determine the cluster with the largest number of instances for
651
Computational Linguistics Volume 40, Number 3
that role (the role?s primary cluster) and then compute the percentage of instances that
belong to the primary cluster for each gold role:
CO = 1N
?
j
max
i
|Gj ? Ci| (13)
Per-verb scores are aggregated into an overall score by averaging over all verbs. We
use the micro-average obtained by weighting the scores for individual verbs propor-
tionately to the number of instances for that verb. Finally, we use the harmonic mean of
purity and collocation as a single measure of clustering quality:
F1 = 2?CO?PU
CO + PU
(14)
Purity and collocation measure essentially the same data traits as precision and
recall, which in the context of clustering are, however, defined on pairs of instances
(Manning, Raghavan, and Schu?tze 2008), which makes them a bit harder to grasp
intuitively. We therefore prefer purity and collocation, arguing that these should be
assessed in combination or together with F1 because they can be traded off against each
other. Purity can be trivially maximized by mapping each instance into its own cluster,
and collocation can be trivially maximized by mapping all instances into a single cluster.
Although it is desirable to report performance with a single score such as F1, it
is equally important to assess how purity and collocation contribute to this score. In
particular, if a hypothetical system were to be used for automatically annotating data,
low collocation would result in higher annotation effort and low purity would result in
lower data quality. Therefore high purity is imperative for an effective system whereas
652
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
high collocation contributes to efficient data labeling. For assessing our methods we
therefore introduce the following terminology. If a model attains higher purity than
the baseline, we will say that it is adequate, because it induced roles that adequately
represent semantic roles. If a model attains higher F1 than the baseline, we will say
that it is non-trivial, because it strikes a tradeoff between collocation and purity that is
non-trivial. Our goal then is to find models that are both adequate and non-trivial.
In order to assess whether differences in performance between two models are
statistically significant, we used a sign test. Specifically, we obtained a series of score
pairs by testing two methods on a subsample of the test data. Each subsample corre-
sponds to a random selection of M = 2, 000. We consider the resulting samples to be
?sufficiently? independent to obtain indicative results from the test. As null hypothesis
(H0) we assume that a model m attains scores equal to another model b. Under H0 the
probability that model m outperforms model b on a particular test set is 12 . The random
variable S counting the number of times that scorem > scoreb in a sample of N score pairs
is binomially distributed:
S =
N
?
i=1
1[score(i)m > score
(i)
b ] Bin(
1
2 , N) (15)
We can therefore use S as our test statistic and reject the null hypothesis H0 if S >> N2 .
5.5 Results
Our results are summarized in Tables 3?5, which report cluster purity (PU), collocation
(CO), and their harmonic mean (F1) for the baseline and our two multi-layer graph
partitioning algorithms. We present scores on four data sets that result from the combi-
nation of automatic parses with automatically identified arguments (auto/auto), gold
parses with automatic arguments (gold/auto), automatic parses with gold arguments
(auto/gold), and gold parses with gold arguments (gold/gold). We show how per-
formance varies for our methods when measuring cluster similarity in the two ways
described above: (a) by finding for each instance in one cluster the instance in the
other cluster that is maximally similar or dissimilar and averaging over the scores of
these alignments (avgmax) and (b) by using cosine similarity (see Section 4.1). We also
report results for the single-layer algorithm proposed in Lang and Lapata (2011b).5
Given a verbal predicate, they construct a single-layer graph whose edge weights
express instance-wise similarities directly. The graph is partitioned into vertex clusters
representing semantic roles using a variant of Chinese Whispers, a graph clustering
algorithm proposed by Biemann (2006). The algorithm iteratively assigns cluster labels
to graph vertices by greedily choosing the most common label among the neighbors of
the vertex being updated.
Both agglomerative partitioning and multi-layered label propagation algorithms
systematically achieve higher F1 scores than the baseline?that is, induce non-trivial
clusterings and more adequate semantic roles (by attaining higher purity). For exam-
ple, on the auto/auto data set, the agglomerative algorithm using cosine similarity
5 The results in Table 5 differ slightly from those published in Lang and Lapata (2011b). This is due to a
small change in the preprocessing of the data. For all English experiments reported here, we removed
arguments with R- and C- role prefixes and replaced numbers with a placeholder.
653
Computational Linguistics Volume 40, Number 3
Table 3
Results for agglomerative partitioning (for avgmax and cosine similarity). F1 improvements
over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the
best performing system according to purity, collocation, and F1.
Parse/Arg Agglomerative
Baseline avgmax cosine
PU CO F1 PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 75.3 69.2 72.1 75.5 69.5 72.4
gold/auto 74.9 78.5 76.6 80.3 73.8 76.9 80.7 74.0 77.2
auto/gold 77.0 71.5 74.1 84.9 70.8 77.2 85.6 71.9 78.1
gold/gold 81.6 78.1 79.8 87.4 75.3 80.9 87.9 75.6 81.3
Table 4
Results for multi-layered label propagation (for avgmax and cosine similarity). F1 improvements
over the baseline are statistically significant in all settings (q < 0.001). Boldface highlights the
best performing system according to purity, collocation, and F1.
Parse/Arg Multi-Layer Label Propagation
Baseline avgmax cosine
PU CO F1 PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 73.8 70.3 72.0 74.0 70.3 72.1
gold/auto 74.9 78.5 76.6 78.8 74.3 76.5 79.2 74.3 76.7
auto/gold 77.0 71.5 74.1 82.9 72.8 77.5 83.6 73.1 78.0
gold/gold 81.6 78.1 79.8 85.6 75.8 80.4 86.3 76.1 80.9
Table 5
Results for single-layered label propagation using a heuristic similarity function.
F1 improvements over the baseline are statistically significant (q < 0.001) in the auto/gold
and gold/gold settings. Boldface highlights the best performing system according to purity,
collocation, and F1.
Parse/Arg Baseline Label Propagation
PU CO F1 PU CO F1
auto/auto 68.3 72.1 70.1 70.1 70.4 70.2
gold/auto 74.9 78.5 76.6 76.4 77.2 76.8
auto/gold 77.0 71.5 74.1 79.6 72.6 75.9
gold/gold 81.6 78.1 79.8 83.7 78.2 80.9
increases F1 by 2.3 points over the baseline and by 7.2 points in terms of purity. This
increase in purity is achieved by trading off against collocation, although in a favorable
ratio as indicated by the overall higher F1. All improvements over the baseline are
statistically significant (q < 0.001 according to the test described in Section 5.4). In
general, we observe that cosine similarity outperforms avgmax similarity. We conjecture
that cosine is a more appropriate measure of cluster similarity for features where it
is beneficial to capture the distributional similarity of clusters. The two algorithms
perform comparably?differences in F1 are not statistically significant (except in the
gold/auto setting). Nevertheless, agglomerative partitioning obtains higher purity and
F1 than label propagation. The latter trades off more purity and in return obtains
higher collocation. The single-layer method is inferior to the multi-layer algorithms,
654
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
in particular because it is less robust to noise, as demonstrated by the markedly worse
results on automatic parses. On the auto/auto data set the single-layered algorithm is on
a par with the baseline and marginally outperforms it on the auto/gold and gold/gold
configurations.
To help put our results in context, we compare our methods with Titov
and Klementiev?s (2012a) Bayesian clustering models. They report results on the
CoNLL 2008 data sets with two model variants, a factored model that models each verb
independently and a coupled model where model parameters are shared across verbs.
In an attempt to reduce the sparsity of the argument fillers, they also present variants
of the factored and coupled models where the argument heads have been replaced by
lexical cluster ids stemming from Brown et al.?s (1992) clustering algorithm on the RCV1
corpus. In Table 6 we follow Titov and Klementiev (2012a) and show results on the
gold/gold and gold/auto settings. As can be seen, both the agglomerative clustering
and label propagation perform comparably to their coupled model, even though they
do not implement any specific mechanism for sharing clustering preferences across
verbs. Versions of their models that use Brown word clusters (i.e., Factored+Br and
Coupled+Br) yield overall best results. We expect this type of preprocessing to also
increase the performance of our models, however we leave this to future work. Finally,
we should point out that Titov and Klementiev (2012a) do not cluster adjunct-like
modifier arguments that are already explicitly represented in syntax (e.g., TMP, LOC,
DIR). Thus, their Coupled+Mods model is most comparable to ours in terms of the
clustering objective as it treats both core and adjunct arguments and does not make
use of the Brown clustering. Table 6 shows the performance of Coupled+Mods on the
gold/gold setting only because auto/gold results are not reported.
We further examined the output of the baseline and our best performing model
in order to better understand where the performance gains are coming from. Table 7
shows how the two approaches differ when it comes to individual roles. We observe
that the agglomerative clustering algorithm performs better than the baseline on all
core roles. There are some adjunct roles for which the baseline obtains a higher F1.
This is not surprising because the parser directly outputs certain labels such as LOC
and TMP which results in high baseline scores for these roles. A word of caution is
necessary here since core roles are defined individually for each verb and need not have
a uniform corpus-wide interpretation. Thus, conflating per-role scores across verbs is
only meaningful to the extent that these labels actually signify the same role (which is
mostly true for A0 and A1). Furthermore, the purity scores we provide in this context
are averages over the clusters for which the specified role is the majority role.
Table 6
Semantic role induction with graph partitioning and Bayesian clustering.
Model gold/gold auto/gold
PU CO F1 PU CO F1
Baseline 81.6 78.1 79.8 77.0 71.5 74.1
Agglomerative 87.9 75.6 81.3 85.6 71.9 78.1
Multi-Layer LP 86.3 76.1 80.9 83.6 73.1 78.0
Factored 88.1 77.1 82.2 85.1 71.8 77.9
Coupled 89.3 76.6 82.5 86.7 71.2 78.2
Coupled+Mods 89.2 74.0 80.9 ? ? ?
Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6
Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8
655
Computational Linguistics Volume 40, Number 3
Table 7
Results for individual roles on the auto/auto data set; comparison between the baseline and the
agglomerative clustering algorithm with the cosine similarity function. Boldface highlights the
best performing system according to purity, collocation, and F1.
Role Freq Baseline Agglomerative
PU CO F1 PU CO F1
A0 49,956 68.2 89.6 77.5 71.1 90.0 79.4
A1 72,032 77.5 75.2 76.3 80.7 76.9 78.7
A2 16,795 65.7 71.4 68.4 79.1 68.3 73.3
A3 2,860 45.4 81.8 58.4 71.7 80.1 75.7
A4 2,471 61.6 86.1 71.8 81.6 85.1 83.3
A5 44 46.4 59.1 52.0 92.5 84.1 88.1
AA 9 46.7 100.0 63.6 50.0 100.0 66.7
ADV 5,824 33.8 86.3 48.6 67.7 41.9 51.8
CAU 878 67.5 79.3 72.9 81.5 73.9 77.5
DIR 811 51.5 71.6 59.9 66.9 58.9 62.7
DIS 3,022 36.1 90.4 51.6 57.5 75.7 65.3
EXT 536 46.9 91.0 61.9 70.2 92.2 79.7
LOC 4,481 65.1 76.5 70.4 74.2 58.4 65.3
MNR 5,066 62.0 64.6 63.3 84.3 48.3 61.5
MOD 8,064 80.2 44.1 56.9 90.3 89.3 89.8
NEG 2,952 38.7 98.6 55.6 53.5 98.7 69.4
PNC 1,682 67.9 71.8 69.8 77.8 70.6 74.1
PRD 56 39.1 92.9 55.1 80.4 85.7 83.0
REC 9 25.0 100.0 40.0 75.0 100.0 85.7
TMP 12,928 71.1 78.7 74.7 73.1 43.1 54.2
NONE 49,663 57.1 47.3 51.8 71.6 44.8 55.1
We further investigated the degree to which the baseline and the agglomera-
tive clustering algorithm agree in their role assignments. The overall mean overlap
was 46.03%. Figure 3a shows the percentage of verbs for which the baseline and our
algorithm have no, some, or complete overlap. We discretized overlap into 10 bins of
equal size ranging from 0 to 100. We observe that the role assignments produced by
the two methods have nothing in common for approximately 13.6% verbs, whereas
assignments are identical for 18.1% verbs. Aside from these two bins (see 0 and 100
in Figure 3), a large number of verbs seems to exhibit overlap in the range of 40?60%.
Figure 3b shows how the overlap in the cluster assignments varies with verb frequency.
Perhaps unsurprisingly, we can see that overlap is higher for least frequent and there-
fore less ambiguous verbs. In general, although the two methods have some degree
of overlap, agglomerative clustering does indeed manage to change and improve the
original role assignments of the baseline.
An interesting question concerns precisely the type of changes affected by the
agglomerative clustering algorithm over the assignments of the baseline. To be able
to characterize these changes we first examined the consistency of the role assignments
created by the two algorithms. Specifically, we would expect a verb-argument pair to be
mostly assigned to the same cluster (i.e., an argument to bear the same role label for the
same verb). Of course this is not a hard constraint as arguments and predicates can be
ambiguous and their roles may vary in specific syntactic configurations and contexts.
To give an idea of an upper bound, in our gold standard, an argument instance of the
same verb bears on average 2.23 distinct roles. For comparison, the baseline creates
(on average) 2.9 role clusters for an argument, whereas agglomerative clustering yields
more consistent assignments, with an average of 2.34 role clusters per argument.
656
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 3
Role assignment overlap between the baseline and agglomerative clustering on the auto/
auto data set. Figure 3a shows the percentage of verbs with no overlap (0%), 10% overlap,
20% overlap, 30% overlap, and so on. Figure 3b shows how role overlap varies with verb
frequency. Results are reported on the auto/auto data set.
We further grouped the verbs in our data set into different bins according to their
polysemy and allowable argument realizations. Specifically, we followed Levin?s (1993)
taxonomy and grouped verbs according to the number of semantic classes they inhabit
(e.g., one, two, and so on). We also binned verbs according to the number of alternations
they exhibit. To give an example, the verb donate is a member of the CONTRIBUTE class
and participates in the causative/inchoative and dative alternations, whereas the verb
shower is a member of four classes (i.e., SPRAY/LOAD, PELT, DRESS, and WEATHER) and
participates in the understood reflexive object and spray/load alternations. Figures 4a,b
show the overlap in role assignments between the baseline and agglomerative clus-
tering and how it varies according to verb class ambiguity and argument structure;
figures 4c,d illustrate the same for role assignments and their consistency. As can be
seen, there is less overlap between the two methods when the verbs in question are
more polysemous (Figures 4a) or exhibit more variation in their argument structure
(Figure 4b). As far as consistency in role assignments is concerned, agglomerative
clustering appears overall more consistent than the baseline. As expected, the mean
657
Computational Linguistics Volume 40, Number 3
role assignment is slightly higher for polysemous verbs because differences in meaning
manifest themselves in different argument realizations.
Figure 5 shows how purity, collocation, and F1 vary across alternations and verb
classes. Perhaps unsurprisingly, performance is generally better for least ambiguous
verbs exhibiting a small number of alternations. In general, agglomerative clustering
achieves higher purity across the board whereas the baseline achieves higher collo-
cation. Although agglomerative clustering achieves a consistently higher F1 over the
baseline, the performance of the two algorithms converges for the most polysemous
verbs (i.e., those inhabiting more than six semantic classes; see Figure 5f). Interestingly,
also note that F1 is comparable for verbs with less varied argument structure (i.e., verbs
inhabiting one alternation; see Figure 5c). For such verbs the performance gap between
the baseline and the agglomerative algorithm is narrower both in terms of purity and
collocation. Overall, we observe that agglomerative clustering is able to change some of
the role assignments of the baseline for verbs exhibiting a good degree of alternations
and polysemy.
Table 8 reports results for 12 individual verbs for the best performing method
(i.e., agglomerative partitioning using cosine similarity) on the auto/auto data set.
These verbs were selected so as to exhibit varied occurrence frequencies and alternation
patterns. As can be seen, the macroscopic result?higher F1 due to significantly higher
purity?seems to consistently hold also across verbs. An important exception is the verb
Figure 4
Comparison between the baseline and the agglomerative clustering algorithm in terms of
role assignment overlap (a and b) and consistency (c and d). Verbs are grouped according to
polysemy (a and c) and number of alternations (b and d). All results are reported on the
auto/auto data set.
658
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Figure 5
Comparison between the baseline and the agglomerative clustering algorithm across
alternations (a?c) and verb classes (d?f) using purity, collocation, and F1. All results are reported
on the auto/auto data set.
say, for which the baseline attains high scores due to little variation in its syntactic
realization within the corpus. Example output is given in Table 9, which shows the
five largest clusters produced by the baseline and agglomerative partitioning for the
verb increase. For each cluster we list the 10 most frequent argument head lemmas.
In this case, our method managed to induce an A0 cluster that is not present in the
top five clusters of the baseline, although the cluster also incorrectly contains some A1
arguments that stem from a false merge.
6. Role Induction Experiments on German
The applicability of our method to arbitrary languages is important from a theoretical
and practical perspective. On the one hand, linguistic theory calls for models which are
universal and generalize across languages. This is especially true for models operating
on the (frame-) semantic level, which is a generalization over surface structure and
should therefore be less language specific (Boas 2005). On the other hand, a language-
independent model can be applied to arbitrary languages, genres, and domains and
is thus of greater practical benefit. Because our approach is based on the language-
independent principles discussed in Section 1, we argue that it can easily generalize
to other languages. To test this claim, we further applied our methods to German data.
659
Computational Linguistics Volume 40, Number 3
Table 8
Results for individual verbs on the auto/auto data set; comparison between the baseline and our
agglomerative clustering algorithm with the cosine similarity function. Boldface highlights the
best performing system according to purity, collocation, and F1.
Verb Freq Baseline Agglomerative
PU CO F1 PU CO F1
say 16,698 86.7 90.8 88.7 85.8 90.4 88.0
make 4,589 63.3 71.0 67.0 66.4 71.0 68.6
go 2,331 47.3 56.0 51.3 55.7 55.3 55.5
increase 1,425 58.0 69.0 63.0 59.2 71.5 64.8
know 1,083 58.3 70.8 63.9 58.6 62.0 60.2
tell 969 59.0 76.8 66.7 71.4 68.0 69.7
consider 799 60.7 65.3 62.9 71.0 60.2 65.1
acquire 761 70.7 78.4 74.4 72.0 77.8 74.8
meet 616 70.0 72.2 71.1 78.9 68.3 73.2
send 515 68.3 67.4 67.9 75.9 64.9 70.0
open 528 55.3 67.8 60.9 61.9 55.1 58.3
break 274 51.1 59.1 54.8 62.8 55.8 59.1
Table 9
Five largest clusters created by the baseline and agglomerative partitioning for the verb increase.
Symbols $ and CD are used as placeholders for monetary units and cardinal numbers,
respectively.
Role Baseline
A0 it, sales, revenue, company, profit, rates, they, earnings, we, number
A1 number, reserves, stake, sales, costs, will, board, demand, rates, capacity
A4 $, %, CD, yen, cent, #, member, earlier, kronor, years
ADV $, not, CD, also, be, increase, greatly, month, %, thus
A2 %, $, CD, average, significantly, penny, yen, days, slightly, share
Role Agglomerative
A1 %, number, costs, sales, reserves, demand, stake, competition, pressure, size
A0 it, sales, revenue, company, profit, rates, earnings, we, they, line
A4 $, %, CD, yen, cent, member, result, #, kronor, barrels
A3 $, CD, %, yen, cent, earlier, period, #, member, quarter
TMP year, quarter, month, years, period, september, CD, week, example, instance
Although on a high-level, German clauses do not differ drastically from English
ones with respect to their frame-semantic make-up, there are differences in terms of
how frame elements are mapped onto specific positions on the linear surface structure
of a sentence, beyond any variations observed among English verbs. In general, German
places fewer constraints on word order (more precisely phrase order) and instead relies
on richer morphology to help disambiguate the grammatical functions of linguistic
units. In particular, verbal nominal arguments are marked with a grammatical case6
that directly indicates their grammatical function. Although in main declarative clauses
the inflected part of the verb has to occur in second position, German is commonly
6 German has (partially ambiguous) markers for Nominative, Accusative, Dative, and Genitive.
660
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
considered a verb-final language. This is because the verb often takes the final position
in subordinate clauses, as do infinitive verbs (Brigitta 1996).
6.1 Data
We conducted our experiments on the SALSA corpus (Burchardt et al. 2006), a lexical
resource for German, which, like FrameNet for English, associates predicates with
frames. SALSA is built as an extra annotation layer over the TIGER corpus (Brants
et al. 2002), a treebank for German consisting of approximately 40,000 sentences (700,000
tokens) of newspaper text taken from the Frankfurter Rundschau, although to date not all
predicate-argument structures have been annotated. The frame and role inventory of
SALSA was taken from FrameNet, but has been extended and adapted where necessary
due to lack of coverage and cross-lingual divergences.
The syntactic structure of a sentence is represented through a constituent tree whose
terminal nodes are tokens and non-terminal nodes are phrases (see Figure 6). In addition
to labeling each node with a constituent type such as Sentence, Noun Phrase, and Verb
Phrase, the edges between a parent and a child node are labeled according to the function
of the child within the parent constituent, for example, Accusative Object, Noun Kernel,
or Head. Edges can cross, allowing local and non-local dependencies to be encoded in a
uniform way and eliminating the need for traces. This approach has significant advan-
tages for non-configurational languages such as German, which exhibit a rich inventory
of discontinuous constituents and considerable freedom with respect to word order
(Smith 2003). Compared with the Penn TreeBank (Marcus, Santorini, and Marcinkiewicz
1993), tree structures are relatively flat. For example, the tree does not encode whether
a constituent is a verbal argument or adjunct; this information is encoded through the
edge labels instead.
The frame annotations contained in SALSA do not cover all of the predicate-
argument structures of the underlying TIGER corpus. Only a subset of around
550 predicates with approximately 18,000 occurrences in the corpus have been an-
notated. Moreover, only core roles are annotated, whereas adjunct roles are not, re-
sulting in a smaller number of arguments per predicate (1.96 on average) compared
with the CoNLL 2008 data set (2.57 on average) described in section 5.1. Because our
Figure 6
A sample parse tree for the sentence Pra?sident Jelzin verliert die Mach ans Ku?chenkabinett und
wird die Wahlen kaum gewinnen ko?nnen [translated in English as President Jelzin loses power to the
kitchen cabinet and will hardly be able to win the elections]. The parse tree contains phrase labels NP
(Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Sentence), and CS (Coordinated
Sentence). The dependency labels are NK (Noun Kernel), SB (Subject), AO (Object Accusative),
HD (Head), MO (Modifier), AC (Adpositional Case Marker), CJ (Conjunct), and OC (Clausal
Object).
661
Computational Linguistics Volume 40, Number 3
method is designed to induce verb-specific frames, we converted the SALSA frames
into PropBank-like frames by splitting each frame into several verb-specific frames and
accordingly mapping frame roles onto verb-specific roles. Our data set is comparable
to the German data set released as part of the CoNLL 2009 shared task (Hajic? et al.
2009), which was also derived from the SALSA corpus. However, we did not convert
the original constituent-based SALSA representation into dependencies, as we wanted
to assess whether our methods are also compatible with phrase structure trees.
6.2 Experimental Set-up
Although we follow the same experimental set-up as described in Section 5 for En-
glish, there are some deviations due to differences in the data sets utilized for the two
languages. Firstly, in contrast to the CoNLL 2008 data set, the SALSA data set (and
the underlying TIGER corpus) does not supply automatic parse trees and we therefore
conducted our experiments on gold parses only. Moreover, because adjunct arguments
are not annotated in SALSA, and because argument identification is not the central issue
of this work, we chose to also consider only the gold argument identification. Thus,
all our experiments for German were carried out on the gold/gold data set.
A substantial linguistic difference between the German and English data sets is the
sparsity of the argument head lemmas, which is significantly higher for German than
for English: In the CoNLL 2008 data set, the average number of distinct head lemmas
per verb is only 3.69, whereas in the SALSA data set it is 20.12. This is partly due to
the fact that the Wall Street Journal text underlying the English data is topically more
focused than the Rundschau newspaper text, which covers a broader range of news
beyond economics and politics. Moreover, noun compounding is more commonly used
in German than in English (Corston-Oliver and Gamon 2004), which leads to higher
lexical sparsity.
Data sparsity affects our method, which crucially relies on lexical similarity for
determining the role-equivalence of clusters. Therefore, we reduced the number of
syntactic cues used for cluster initialization in order to avoid creating too many small
clusters for which similarities cannot be reliably computed. Specifically, only the syn-
tactic position and function word served as cues to initialize our clusters. Note that, as
in English, the relatively small number of syntactic cues that determine the syntactic
position within a linking is a consequence of the size of our evaluation data set (which
is rather small) and not an inherent limitation of our method. On larger data sets, more
syntactic cues could and should be incorporated in order to increase performance.
In our experiments we compared the baseline introduced in Section 5.3 against
agglomerative partitioning and the label propagation algorithm using both cosine- and
avgmax-similarity. The parameters ?, ?, and ?, which determine the thresholds used
in defining overall similarity scores, were set and updated identically as for English
(i.e., these parameters can be considered language-independent).
6.3 Results
Table 10 reports results for the baseline and our role induction methods, namely, ag-
glomerative clustering and multi-layered label propagation (using the avgmax and
cosine similarity functions) on the SALSA gold/gold data set. For comparison, we
also include results on the English CoNLL-2008 gold/gold data set. As can be seen,
the baseline obtains a similar F1 for German and English, although the contributions
of purity and collocation are different for the two languages. In English, purity is
662
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Table 10
Results of agglomerative partitioning and label propagation for cosine and avgmax similarity
on German. For comparison purposes results for English on the gold/gold data set are also
tabulated. All improvements over the baseline are statistically significant at significance level
q < 0.001.
Model German English
PU CO F1 PU CO F1
Baseline 75.0 81.7 78.2 81.6 78.1 79.8
Agglomerative (avgmax) 77.6 80.8 79.2 87.3 75.3 80.9
Agglomerative (cosine) 77.6 80.8 79.2 87.9 75.6 81.3
Label Propagation (avgmax) 77.4 80.9 79.1 85.6 75.8 80.4
Label Propagation (cosine) 77.5 81.0 79.2 86.3 76.0 80.9
noticeably higher than in German, whereas collocation is higher in German. This is not
surprising when taking into account the distribution of syntactic relations governing an
argument. A few frequent relation labels absorb most of the probability mass in German
(see Figure 7b), whereas the mass is distributed more evenly among the labels in English
(Figure 7a), thus leading to higher purity but lower collocation.
Figure 7
Distribution of syntactic relations governing an argument in English and German data sets.
Only the most frequent relations are shown (a key for the English relations is given in Table 2;
in German the relations are SB (Subject), OA (Object Accusative), CJ (Conjunct), DA (Dative),
CD (Coordinator), MO (Modifier), RE (Subordinate Clause), RS (Reported Speech), OC
(Object Clausal), OP (Object Prepositional), NK (Noun Kernel), and CVC (Collocational
Verb Construction).
663
Computational Linguistics Volume 40, Number 3
Table 11
Results for individual verbs on the gold/gold SALSA data set; comparison between the baseline
and the agglomerative clustering algorithm with the cosine similarity function.
Verb Freq Baseline Agglomerative (cosine)
PU CO F1 PU CO F1
sagen [say] 2,076 96.3 89.0 92.5 97.3 97.7 97.5
wissen [know] 487 79.7 76.0 77.8 80.1 80.3 80.2
berichten [report] 438 79.5 78.3 78.9 80.0 81.3 80.7
nehmen [take] 420 49.8 70.2 58.3 51.9 72.4 60.5
verurteilen [convict] 265 70.9 83.4 76.7 70.6 81.9 75.8
erho?hen [increase] 120 58.3 70.8 64.0 70.8 73.3 72.1
schlie?sen [close] 93 40.9 72.0 52.1 53.8 78.5 63.8
brechen [break] 45 40.0 91.1 55.6 44.4 91.1 59.7
schauen [watch] 35 82.9 91.4 86.9 85.7 71.4 77.9
plazieren [place] 18 55.6 83.3 66.7 66.7 61.1 63.8
treffen [meet] 14 100.0 100.0 100.0 100.0 100.0 100.0
regnen [rain] 12 66.7 83.3 74.1 83.3 50.0 62.5
In German, our role induction algorithms improve over the baseline in terms of
F1. All four methods perform comparably and manage to strike a tradeoff between
collocation and purity that is non-trivial and represents semantic roles adequately.
Compared with English, the difference between the baseline and our algorithms is
narrower. This is because we use fewer syntactic cues for initialization in German, due
to the increased data sparsity discussed in the previous section. This also explains why
there is little variation in the collocation and purity results across methods. However,
qualitatively the tradeoff between purity and collocation is the same as for English (i.e.,
purity is increased at the cost of collocation).
Tables 11 and 12 show per-verb and per-role results, respectively, for agglomerative
clustering using cosine similarity. We report per-verb scores for a selection of 10 verbs
Table 12
Results for individual roles on gold/gold SALSA data set; comparison between the baseline and
the agglomerative clustering algorithm with the cosine similarity function.
Role Freq Baseline Agglomerative (cosine)
PU CO F1 PU CO F1
Agent 1,908 70.4 92.8 80.1 70.5 93.9 80.5
Theme 1,637 69.1 79.2 73.8 69.2 79.7 74.1
Cognizer 1,244 75.7 94.3 84.0 76.2 94.6 84.4
Entity 1,195 79.7 85.9 82.7 78.6 86.7 82.4
Content 1,136 87.2 65.2 74.6 88.7 66.8 76.2
Goal 1,071 62.0 81.0 70.2 87.0 67.2 75.9
Topic 477 85.2 69.4 76.5 86.8 58.9 70.2
Source 267 71.6 94.0 81.3 66.1 76.0 70.7
Goods 171 73.0 68.4 70.6 74.8 66.7 70.5
Buyer 121 65.0 90.1 75.5 70.4 88.4 78.4
Employee 63 50.4 98.4 66.7 50.4 98.4 66.7
Required Situation 56 60.3 78.6 68.3 52.1 82.1 63.8
Opinion 50 66.7 50.0 57.1 69.0 62.0 65.3
Leader 29 86.7 69.0 76.8 86.7 65.5 74.6
Financed 25 79.3 64.0 70.8 80.0 64.0 71.1
664
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
(see Table 12a), which in some cases are translations of the verbs used for English. With
respect to per-role scores, we make use of the fact that roles have a common meaning
across predicates (like A0 and A1 in PropBank), and report scores for a selection of
15 different roles (Table 12b) with varied occurrence frequencies. Per-verb results con-
firm that data sparsity affects performance in German. As can be seen, agglomerative
clustering outperforms the baseline on high-frequency verbs that are less affected by
sparsity, although this is not always the case on lower-frequency verbs. Analogously,
the method tends to perform better on high-frequency roles, whereas there is no clear
trend on lower-frequency roles. In contrast to English, for more than half of the verbs
the method manages to outperform the baseline in terms of both purity and collocation,
which is consistent with our macroscopic result, where the tradeoff between purity and
collocation is not as strong as for English.
The experiments show that our methods can be successfully applied to languages
other than English, thereby supporting the claim that they are based on a set of
language-independent assumptions and principles. Despite substantial differences be-
tween German and English grammar, both generally and in terms of the specific syn-
tactic representation that was used, our methods increased F1 over the baseline for both
languages and resulted in a similar tradeoff between purity and collocation. Improve-
ments were observed in spite of pronounced data sparsity in the case of German. Recall
that we had to reduce the number of syntactic initialization cues in order to be able
to obtain results on the relatively small amount of gold-standard data. We would also
like to note that porting our system to German did not require any additional feature
engineering or algorithmic changes.
7. Conclusions
In this article we described an unsupervised method for semantic role induction in
which argument-instance graphs are partitioned into clusters representing semantic
roles. A major hypothesis underlying our work has been that semantic roles can be
induced without human supervision from a corpus of syntactically parsed sentences
based on three linguistic principles : (1) arguments in the same syntactic position (within
a specific linking) bear the same semantic role, (2) arguments within a clause bear a
unique role, and (3) clusters representing the same semantic role should be more or less
lexically and distributionally equivalent. Based on these principles we have formulated
a similarity-driven model and introduced a multi-layer graph partitioning approach that
represents similarity between clusters on multiple feature layers, whose connectiv-
ity can be analyzed separately and then combined into an overall cluster-similarity
score.
Our work has challenged the established view that supervised learning is the
method of choice for the semantic role labeling task. Although the proposed unsuper-
vised models do yet achieve results comparable to their supervised counterparts, we
have been able to show that they consistently outperform the syntactic baseline across
several data sets that combine automatic and gold parses, with gold and automatic
argument identification in English and German. Our methods obtain F1 scores that are
systematically above the baseline and the purity of the induced clusters is considerably
higher, although in most cases this increase in purity is achieved by decreasing colloca-
tion. In sum, these results provide strong empirical evidence towards the soundness of
our method and the principles they are based on.
In terms of modeling, we have contributed to the body of work on similarity-
driven models by demonstrating their suitability for this problem, their effectiveness,
665
Computational Linguistics Volume 40, Number 3
and their computational efficiency. The models are based on judgments regarding
the similarity of argument instances with respect to their semantic roles. We showed
that these judgments are comparatively simple to formulate and incorporate into a
graph representation of the data. We have introduced the idea of separating different
similarity features into different graph layers, which resolves the problem faced by
many similarity-based approaches of having to heuristically define an instance-wise
similarity function and brings the advantage that cluster similarities can be computed
in a more principled way. Beyond semantic role labeling, we hope that the multi-layered
graph representation described here might be of relevance to other unsupervised prob-
lems such part-of-speech tagging or coreference resolution. The approach is general
and amenable to other graph partitioning algorithms besides agglomeration and label
propagation.
There are two forms of data sparsity that arise in the context of our work, namely,
the lexical sparsity of argument head lemmas and the sparsity of specific combinations
of linking and syntactic position. As our methods are unsupervised, the conceptually
simple solution to sparsity is to train on larger data sets. Because, with some modifica-
tions, our graph partitioning approaches could be scaled to larger data sets (in terms of
orders of magnitude), this is an obvious next step and would address both instances of
data sparsity. Firstly, it would allow us to incorporate a richer set of syntactic features
for initialization and would therefore necessarily result in initial clusterings of higher
purity. Secondly, the larger size of clusters would result in more reliable similarity
scores. Augmenting the data set would therefore almost surely increase the quality of
induced clusterings; however, we leave this to future work.
Another interesting future direction would be to eliminate the model?s reliance on a
syntactic parser that prohibits its application to languages for which parsing resources
are not available. It would therefore be worthwhile, albeit challenging, to build models
that operate on more readily available forms of syntactic analysis or even raw text. For
example, existing work (Abend and Rappoport 2010b; Abend, Reichart, and Rappoport
2009) attempts to identify arguments and distinguish them into core and adjunct ones
through unsupervised part of speech and grammar induction. As much as making our
model more unsupervised it would also be interesting to see whether some form of
weak supervision might help induce higher-quality semantic roles without incurring a
major labeling effort. The ideas conveyed in this article and the proposed methods
extend naturally to this setting: Introducing labels on some of the graph vertices would
translate into a semi-supervised graph-based learning task, akin to Zhu, Ghahramani,
and Lafferty (2003).
Appendix A. Argument Identification Rules
This appendix specifies the full set of relations used by Rules (2) and (4) of the argument
identification rules given for English in Section 5.2, Table 1. The symbols ? and ? denote
the direction of the dependency relation (upward and downward, respectively). The
dependency relations are explained in Surdeanu et al. (2008), in their Table 4.
The relations in Rule (2) from Table 1 are IM??, PRT?, COORD??, P??, OBJ?, PMOD?,
ADV?, SUB??, ROOT?, TMP?, SBJ?, OPRD?.
The relations in Rule (4) are ADV??, AMOD??, APPO??, BNF??-, CONJ??, COORD??,
DIR??, DTV??-, EXT??, EXTR??, HMOD??, IOBJ??, LGS??, LOC??, MNR??, NMOD??,
OBJ??, OPRD??, POSTHON??, PRD??, PRN??, PRP??, PRT??, PUT??, SBJ??, SUB??,
SUFFIX?? TMP??, VOC?? .
666
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Acknowledgments
We are grateful to the anonymous referees,
whose feedback helped to substantially
improve this article. We also thank the
members of the Probabilistic Models reading
group at the University of Edinburgh for
helpful discussions and comments. We
acknowledge the support of EPSRC (grant
EP/K017845/1).
References
Abend, O. and A. Rappoport. 2010a. Fully
unsupervised core-adjunct argument
classification. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Abend, O. and A. Rappoport. 2010b. Fully
unsupervised core-adjunct argument
classification. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics, pages 226?236,
Uppsala.
Abend, O., R. Reichart, and A. Rappoport.
2009. Unsupervised argument
identification for semantic role labeling.
In Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 28?36, Suntec.
Abney, S. 2007. Semisupervised Learning for
Computational Linguistics. Chapman &
Hall/CRC.
Berg-Kirkpatrick, T., A. Bouchard-Co?te?,
J. DeNero, and D. Klein. 2010. Painless
unsupervised learning with features. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 582?590,
Los Angeles, CA.
Biemann, C. 2006. Chinese Whispers: An
efficient graph clustering algorithm and its
application to natural language processing
problems. In Proceedings of TextGraphs: The
First Workshop on Graph Based Methods for
Natural Language Processing, pages 73?80,
New York, NY.
Boas, H. 2005. Semantic frames as
interlingual representations for
multilingual lexical databases. International
Journal of Lexicography, 18(4):445?478.
Brants, S., S. Dipper, S. Hansen, W. Lezius,
and G. Smith. 2002. The TIGER treebank.
In Proceedings of the 1st Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brigitta, H. 1996. Deutsch ist eine
V/2-Sprache mit Verbendstellung
und freier Wortfolge. In E. Lang and
G. Zifonun, editors, Deutsch U? typologisch,
pages 121?141. Walter de Gruyter.
Brown, P. F., V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. Class-
based n-gram models of natural language.
Computational Linguistics, 18(4):283?298.
Burchardt, A., K. Erk, A. Frank, A. Kowalski,
S. Pado?, and M. Pinkal. 2006. The SALSA
corpus: a German corpus resource for
lexical semantics. In Proceedings of the
International Conference on Language
Resources and Evaluation, pages 969?974,
Genoa.
Corston-Oliver, S. and M. Gamon. 2004.
Normalizing German and English
inflectional morphology to improve
statistical word alignment. In Robert
Frederking and Kathryn Taylor, editors,
Machine Translation: From Real Users to
Research, volume 3265 of Lecture Notes
in Computer Science. Springer, Berlin
Heidelberg, pages 48?57.
Dowty, D. 1991. Thematic proto roles
and argument selection. Language,
67(3):547?619.
Fu?rstenau, H. and M. Lapata. 2009. Graph
alignment for semi-supervised semantic
role labeling. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 11?20,
Singapore.
Gamallo, P., A. Agustini, and G. Lopes. 2005.
Clustering syntactic positions with similar
semantic requirements. Computational
Linguistics, 31(1):107?146.
Garg, N. and J. Henderson. 2012.
Unsupervised semantic role induction
with global role ordering. In Proceedings of
the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short
Papers), pages 145?149, Jeju Island.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Gordon, A. and R. Swanson. 2007.
Generalizing semantic role annotations
across syntactically similar verbs. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 192?199, Prague.
Gordon, D. and M. Desjardins. 1995.
Evaluation and selection of biases in
machine learning. Machine Learning,
20:5?22.
Grenager, T. and C. Manning. 2006.
Unsupervised discovery of a statistical
verb lexicon. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 1?8, Sydney.
667
Computational Linguistics Volume 40, Number 3
Hajic?, J., M. Ciaramita, R. Johansson,
D. Kawahara, M. A. Mart??, L. Ma`rquez,
A. Meyers, J. Nivre, S. Pado?, J. S?te?pa?nek,
P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL 2009 shared
task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009):
Shared Task, pages 1?18, Boulder, CO.
Jain, A., M. Murty, and P. Flynn. 1999. Data
clustering: A review. ACM Computing
Surveys, 31(3):264?323.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 691?696,
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Conference on
Computational Natural Language Learning,
pages 181?184, Ann Arbor, MI.
Lang, J. and M. Lapata. 2010. Unsupervised
induction of semantic roles. In Proceedings
of the North American Chapter of the Association
for Computational Linguistics Conference,
pages 939?947, Los Angeles, CA.
Lang, J. and M. Lapata. 2011a. Unsupervised
induction of semantic roles via split-merge
clustering. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 1,117?1,126, Portland, OR.
Lang, J. and M. Lapata. 2011b. Unsupervised
semantic role induction with graph
partitioning. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing, pages 1,320?1,331, Edinburgh.
Levin, B. and M. Rappaport. 2005. Argument
Realization. Cambridge University Press.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Lin, D. and P. Pantel. 2001. Discovery of
inference rules for question-answering.
Natural Langugae Engineering, 7:343?360.
Manning, C., P. Raghavan, and H. Schu?tze.
2008. Introduction to Information Retrieval.
Cambridge University Press.
Marcus, M., B. Santorini, and
M. Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, L., X. Carras, K. Litkowski,
and S. Stevenson. 2008. Semantic role
labeling: An introduction to the special
issue. Computational Linguistics,
34(2):145?159.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 summarization task.
In Proceedings of the Human Language
Technology Conference and the Conference
on Empirical Methods in Natural Language
Processing Document Understanding
Workshop, Vancouver.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27:373?408.
Munkres, J. 1957. Algorithms for the
assignment and transportation problems.
Journal of the Society for Industrial and
Applied Mathematics, 5(1):32?38.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit,
A. Chanev, S. Ku?bler, S. Marinov, and
E. Marsi. 2007. Malt-Parser: A language-
independent system for data-driven
dependency parsing. Natural Language
Engineering, 13(2):95?135.
Pado?, S. and M. Lapata. 2009. Cross-lingual
annotation projection of semantic roles.
Journal of Artificial Intelligence Research,
36:307?340.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?106.
Poon, H. and P. Domingos. 2009.
Unsupervised semantic parsing. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 1?10, Singapore.
Pradhan, S., W. Ward, and J. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Ruppenhofer, J., M. Ellsworth, M. Petruck,
C. Johnson, and J. Scheffczyk. 2006.
FrameNet II: Extended theory and practice,
version 1.3. Technical Report, International
Computer Science Institute, Berkeley, CA.
Schaeffer, S. 2007. Graph clustering. Computer
Science Review, 1(1):27?64.
Shen, D. and M. Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007
Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 12?21, Prague.
Smith, G. 2003. A Brief Introduction to the
TIGER Treebank, Version 1. Technical
Report, University of Potsdam.
668
Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics, pages 8?15, Sapporo.
Surdeanu, M., R. Johansson, A. Meyers, and
L. Ma`rquez. 2008. The CoNLL-2008 shared
task on joint parsing of syntactic and
semantic dependencies. In Proceedings of
the Conference on Natural Language Learning,
pages 159?177, Manchester.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 95?102, Barcelona.
Titov, I. and A. Klementiev. 2011. A
Bayesian model for unsupervised
semantic parsing. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 1,445?1,455,
Portland, OR.
Titov, I. and A. Klementiev. 2012a. A
Bayesian approach to unsupervised
semantic role induction. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational
Linguistics, pages 12?22, Avignon.
Titov, I. and A. Klementiev. 2012b.
Crosslingual induction of semantic roles.
In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 647?656,
Jeju Island.
van Rijsbergen, C. 1974. Foundation of
evaluation. Journal of Documentation,
30(4):265?374.
Wu, D. and P. Fung. 2009. Semantic roles
for SMT: A hybrid two-pass model. In
Proceedings of Human Language Technologies:
The Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, Companion Volume: Short Papers,
pages 13?16, Boulder, CO.
Zhu, X., Z. Ghahramani, and J. Lafferty.
2003. Semi-supervised learning using
Gaussian fields and harmonic functions.
In Proceedings of the International Conference
on Machine Learning, pages 912?919,
Washington, DC.
669

This article has been cited by:
1. Dan Jurafsky. 2014. Charles J. Fillmore. Computational Linguistics 40:3, 725-731. [Citation] [Full
Text] [PDF] [PDF Plus]
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 939?947,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Induction of Semantic Roles
Joel Lang and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Datasets annotated with semantic roles are
an important prerequisite to developing high-
performance role labeling systems. Unfortu-
nately, the reliance on manual annotations,
which are both difficult and highly expen-
sive to produce, presents a major obstacle to
the widespread application of these systems
across different languages and text genres. In
this paper we describe a method for induc-
ing the semantic roles of verbal arguments di-
rectly from unannotated text. We formulate
the role induction problem as one of detecting
alternations and finding a canonical syntactic
form for them. Both steps are implemented in
a novel probabilistic model, a latent-variable
variant of the logistic classifier. Our method
increases the purity of the induced role clus-
ters by a wide margin over a strong baseline.
1 Introduction
Semantic role labeling (SRL, Gildea and Jurafsky
2002) is the task of automatically classifying the ar-
guments of a predicate with roles such as Agent, Pa-
tient or Location. These labels capture aspects of the
semantics of the relationship between the predicate
and the argument while abstracting over surface syn-
tactic configurations. SRL has received much atten-
tion in recent years (Surdeanu et al, 2008; Ma`rquez
et al, 2008), partly because of its potential to im-
prove applications that require broad coverage se-
mantic processing. Examples include information
extraction (Surdeanu et al, 2003), question answer-
ing (Shen and Lapata, 2007), summarization (Melli
et al, 2005), and machine translation (Wu and Fung,
2009).
Given sentences (1-a) and (1-b) as input, an SRL
system would have to identify the verb predicate
(shown in boldface), its arguments (Michael and
sandwich) and label them with semantic roles (Agent
and Patient).
(1) a. [Michael]Agent eats [a sandwich]Patient.
b. [A sandwich]Patient is eaten [by
Michael]Agent.
Here, sentence (1-b) is an alternation of (1-a).
The verbal arguments bear the same semantic role,
even though they appear in different syntactic posi-
tions: sandwich is the object of eat in sentence (1-a)
and its subject in (1-b) but it is in both instances as-
signed the role Patient. The example illustrates the
passive alternation. The latter is merely one type
of alternation, many others exist (Levin, 1993), and
their computational treatment is one of the main
challenges faced by semantic role labelers.
Most SRL systems to date conceptualize semantic
role labeling as a supervised learning problem and
rely on role-annotated data for model training. Prop-
Bank (Palmer et al, 2005) has been widely used for
the development of semantic role labelers as well as
FrameNet (Fillmore et al, 2003). Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that pred-
icate1 and a set of adjunct roles (e.g., Location or
Time) whose interpretation is common across predi-
cates. In addition to large amounts of role-annotated
data, SRL systems often make use of a parser to ob-
tain syntactic analyses which subsequently serve as
input to a pipeline of components concerned with
1More precisely, A0 and A1 have a common interpreta-
tion across predicates as proto-agent and proto-patient (Dowty,
1991).
939
identifying predicates and their arguments (argu-
ment identification) and labeling them with semantic
roles (argument classification).
Supervised SRL methods deliver reasonably good
performance (a system will recall around 81% of the
arguments correctly and 95% of those will be as-
signed a correct semantic role; see Ma`rquez et al
2008 for details). Unfortunately, the reliance on la-
beled training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text genres.
And although corpora with semantic role annota-
tions exist nowadays in other languages (e.g., Ger-
man, Spanish, Catalan, Chinese, Korean), they tend
to be smaller than their English equivalents and of
limited value for modeling purposes. Moreover, the
performance of supervised systems degrades consid-
erably (by 10%) on out-of-domain data even within
English, a language for which two major annotated
corpora are available. Interestingly, Pradhan et al
(2008) find that the main reason for this are errors
in the assignment of semantic roles, rather than the
identification of argument boundaries. Therefore, a
mechanism for inducing the semantic roles observed
in the data without additional manual effort would
enhance the robustness of existing SRL systems and
enable their portability to languages for which anno-
tations are unavailable or sparse.
In this paper we describe an unsupervised ap-
proach to argument classification or role induction2
that does not make use of role-annotated data. Role
induction can be naturally formalized as a cluster-
ing problem where argument instances are assigned
to clusters. Ideally, each cluster should contain argu-
ments corresponding to a specific semantic role and
each role should correspond to exactly one cluster. A
key insight in our approach is that many predicates
are associated with a standard linking. A linking is
a deterministic mapping from semantic roles onto
syntactic functions such as subject, or object. Most
predicates will exhibit a standard linking, i.e., they
will be predominantly used with a specific map-
ping. Alternations occur when a different linking
is used. In sentence (1-a) the predicate eat is used
with its standard linking (the Agent role is mapped
onto the subject function and the Patient onto the
object), whereas in sentence (1-b) eat is used with
2We use the term role induction rather than argument clas-
sification for the unsupervised setting.
its passive-linking (the Patient is mapped onto sub-
ject and the Agent appears as a prepositional phrase).
When faced with such alternations, we will attempt
to determine for each argument the syntactic func-
tion it would have had, had the standard linking been
used. We will refer to this function as the arguments?
canonical function, and use the term canonicaliza-
tion to describe the process of inferring these canon-
ical functions in the case of alternations. So, in sen-
tence (1-b) the canonical functions of the arguments
by Michael and sandwich are subject and object, re-
spectively.
Since linkings are injective, i.e., no two seman-
tic roles are mapped onto the same syntactic func-
tion, the canonical function of an argument uniquely
references a specific semantic role. We define a
probabilistic model for detecting non-standard link-
ings and for canonicalization. The model specifies a
distribution p(F) over the possible canonical func-
tions F of an argument. We present an extension of
the logistic classifier with the addition of latent vari-
ables which crucially allow to learn generalizations
over varying syntactic configurations. Rather than
using manually labeled data, we train our model on
observed syntactic functions which can be obtained
automatically from a parser. These training instances
are admittedly noisy but readily available and as
we show experimentally a useful data source for
inducing semantic roles. Application of the model
to a benchmark dataset yields improvements over a
strong baseline.
2 Related Work
Much previous work on SRL relies on supervised
learning methods for both argument identification
and argument classification (see Ma`rquez et al 2008
for an overview). Most systems use manually anno-
tated resources to train separate classifiers for dif-
ferent SRL subtasks (e.g., Surdeanu et al 2008).
A few approaches adopt semi-supervised learning
methods. The idea here is to to alleviate the data
requirements for semantic role labeling by extend-
ing existing resources through the use of unlabeled
data. Swier and Stevenson (2004) induce role la-
bels with a bootstrapping scheme in which the set
of labeled instances is iteratively expanded using
a classifier trained on previously labeled instances.
Pado? and Lapata (2009) project role-semantic anno-
tations from an annotated corpus in one language
onto an unannotated corpus in another language.
And Fu?rstenau and Lapata (2009) propose a method
940
in which annotations are projected from a source
corpus onto a target corpus, however within the
same language.
Unsupervised approaches to SRL have been few
and far between. Early work on lexicon acquisition
focuses on identifying verbal alternations rather than
their linkings. This is often done in conjunction with
hand-crafted resources such as a taxonomy of possi-
ble alternations (McCarthy and Korhonen, 1998) or
WordNet (McCarthy, 2002). Lapata (1999) proposes
a corpus-based method that is less reliant on taxo-
nomic resources, however focuses only on two spe-
cific verb alternations. Other work attempts to clus-
ter verbs into semantic classes (e.g., Levin 1993) on
the basis of their alternation behavior (Schulte im
Walde and Brew, 2002).
More recently, Abend et al (2009) propose an
unsupervised algorithm for argument identifica-
tion that relies only on part-of-speech annotations,
whereas Grenager and Manning (2006) focus on
role induction which they formalize as probabilis-
tic inference in a Bayesian network. Their model
defines a joint probability distribution over the par-
ticular linking used together with a verb instance
and for each verbal argument, its lemma, syntactic
function as well as semantic role. Parameters in this
model are estimated using the EM algorithm as the
training instances include latent variables, namely
the semantic roles and linkings. To make inference
tractable they limit the set of linkings to a small
number and do not distinguish between different
types of adjuncts. Our own work also focuses on
inducing the semantic roles and the linkings used
by each verb. Our approach is conceptually sim-
pler and computationally more tractable. Our model
is a straightforward extension of the logistic classi-
fier with latent variables applied to all roles not just
coarse ones.
3 Problem Formulation
We treat role induction as a clustering problem.
The goal is to assign argument instances (i.e., spe-
cific arguments, occurring in an input sentence) into
clusters such that each cluster contains instances
with the same semantic role, and each semantic
role is found in exactly one cluster. As we as-
sume PropBank-style roles (Palmer et al, 2005),
our model will allocate a separate set of clusters for
each predicate and assign the arguments of a specific
predicate to one of the clusters associated with it.
As mentioned earlier (Section 1) a linking is a de-
A0 A1 TMP MNR
SBJ 54514 19684 15 7
OBJ 3359 51730 93 54
ADV 162 3506 976 2308
TMP 5 60 15167 22
PMOD 2466 4860 142 62
OPRD 37 5554 1 36
LOC 17 145 43 157
DIR 0 178 15 6
MNR 5 48 13 3312
PRP 9 50 11 6
LGS 2168 36 2 2
PRD 413 830 31 38
NMOD 422 388 25 59
EXT 0 20 2 12
DEP 18 150 25 65
SUB 3 84 4 2
CONJ 198 331 22 8
ROOT 62 147 84 2
64517 88616 16803 6404
Table 1: Contingency table between syntactic func-
tion and semantic role for two core roles Agent (A0)
and Patient (A1) and two adjunct roles, Time (TMP)
and Manner (MNR). Only syntactic functions occur-
ring more than 1000 times are listed. Counts were
obtained from the CoNLL 2008 training dataset us-
ing gold standard parses (the marginals in the bottom
row also include counts of unlisted co-occurrences).
terministic mapping from semantic roles onto syn-
tactic functions. Table 1 shows how frequently in-
dividual semantic roles map onto certain syntactic
functions. The frequencies were obtained from the
CoNLL 2008 dataset (see Surdeanu et al 2008 for
details) and constitute an aggregate across predi-
cates. As can be seen, there is a clear tendency for
a semantic role to be mapped onto a single syntac-
tic function. This is true across predicates and even
more so for individual predicates. For example, A0
is commonly mapped onto subject (SBJ), whereas
A1 is often realized as object (OBJ). There are two
reasons for this. Firstly, a predicate is often asso-
ciated with a standard linking which is most fre-
quently used. Secondly, the alternate linkings of a
given predicate often differ from the standard link-
ing only with respect to a few roles. Importantly, we
do not assume that a single standard linking is valid
941
for all predicates. Rather, each predicate has its own
standard linking. For example, in the standard link-
ing for the predicate fall, A1 is mapped onto subject
position, whereas in the standarad linking for eat,
A1 is mapped onto object position.
When an argument is attested with a non-standard
linking, we wish to determine the syntactic func-
tion it would have had if the standard linking had
been used. This canonical function of the argument
uniquely references a specific semantic role, i.e., the
semantic role that is mapped onto the function under
the standard linking. We can now specify an indi-
rect method for partitioning argument instances into
clusters:
1. Detect arguments that are linked in a non-
standard way (detection).
2. Determine the canonical function of these argu-
ments (canonicalization). For arguments with
standard linkings, their syntactic function cor-
responds directly to the canonical function.
3. Assign arguments to a cluster according to their
canonical function.
We distinguish between detecting non-standard link-
ings and canonicalization because in principle two
separate models could be used. In our probabilis-
tic formulation, both detection and canonicaliza-
tion rely on an estimate of the probability distribu-
tion p(F) over the canonical function F of an ar-
gument. When the most likely canonical function
differs from the observed syntactic function this in-
dicates that a non-standard linking has been used
(detection). This most likely canonical function can
be taken as the canonical function of the argument
(canonicalization).
Arguments are assigned to clusters based on
their inferred canonical function. Since we assume
predicate-specific roles, we induce a separate clus-
ter for each predicate. Given K clusters, we use the
following scheme for determining the mapping from
functions to clusters:
1. Order the functions by occurrence frequency.
2. For each of the K ? 1 most frequent functions
allocate a separate cluster.
3. Assign all remaining functions to the K-th clus-
ter.
4 Model
The detection of non-standard linkings and canon-
icalization both rely on a probabilistic model p(F)
which specifies the distribution over the canonical
functions F of an argument. As is the case with most
SRL approaches, we assume to be given a syntactic
parse of the sentence from which we can extract la-
beled dependencies, corresponding to the syntactic
functions of arguments. To train the model we ex-
ploit the fact that most observed syntactic functions
will correspond to canonical functions. This enables
us to use the parser?s output for training even though
it does not contain semantic role annotations.
Critically, the features used to determine the
canonical function must be restricted so that they
give no cues about possible alternations. If they
would, the model could learn to predict alternations,
and therefore produce output closer to the observed
syntactic rather than canonical function of an argu-
ment. To avoid this pitfall we only use features at
or below the node representing the argument head in
the parse tree apart from the predicate lemma (see
Section 5 for details).
Given these local argument features, a simple so-
lution would be to use a standard classifier such as
the logistic classifier (Berger et al, 1996) to learn
the canonical function of arguments. However, this
is problematic, because in our setting the training
and application of the classifier happen on the same
dataset. The model will over-adapt to the observed
targets (i.e., the syntactic functions) and fail to learn
appropriate canonical functions. Lexical sparsity is
a contributing factor: the parameters associated with
sparse lexical features will be unavoidably adjusted
so that they are highly indicative of the syntactic
function they occur with.
One way to improve generalization is to incor-
porate a layer of latent variables into the logistic
classifier, which mediates between inputs (features
defined over parse trees) and target (the canonical
function). As a result, inputs and target are no longer
directly connected and the information conveyed by
the features about the target must be transferred via
the latent layer. The model is shown in plate notation
in Figure 1a. Here, Xi represents the observed in-
put features, Y the observed target, and Z j the latent
variables. The number of latent variables influences
the generalization properties of the model. With too
few latent variables too little information will be
transferred via the latent variables, whereas with too
many latent variables generalization will degrade.
The model defines a probability distribution over
the target variable Y and the latent variables Z, con-
942
YZ j
M
Xi
N
Y
Z1 Z2
X2X1 X3
(a) (b)
Figure 1: The logistic classifier with latent variables
(shaded nodes) illustrated as a graphical model using
(a) plate notation and (b) in unrolled form for M = 2
and N = 3.
ditional on the input variables X :
p(y,z|x,?) =
1
P(x,?)
exp
(
?
k
?k?k(x,y,z)
)
(1)
We will assume that the latent variables Zi are bi-
nary. Each of the feature functions ?k is associated
with a parameter ?k. The partition function normal-
izes the distribution:
P(x,?) =?
y
?
z
exp
(
?
k
?k?k(x,y,z)
)
(2)
Note that this model is a special case of a conditional
random field with latent variables (Sutton and Mc-
Callum, 2007) and resembles a neural network with
one hidden layer (Bishop, 2006).
Let (c,d) denote a training set of inputs and corre-
sponding targets. The maximum-likelihood parame-
ters can then be obtained by finding the ? maximiz-
ing:
l(?) = log p(d|c)
= ?i log?z p(di,z|ci)
= ?i log
?z exp(?k ?k?k(ci,di,z))
P(ci,?)
(3)
And the gradient is given by:
(?l)k = ???k l(?)
= ?i?z p(z|di,ci)?k(ci,di,z)
??i?y,z p(y,z|ci)?k(ci,y,z)
(4)
where the first term is the conditional expected fea-
ture count and the second term is the expected fea-
ture count.
Thus far, we have written the equations in a
generic form for arbitrary conditional random fields
with latent variables (Sutton and McCallum, 2007).
In our model we have two types of pairwise suffi-
cient statistics: ?(x,z) : R?{0,1}? R, between a
single input variable and a single latent variable, and
?(y,z) : Y ?{0,1}? R, between the target and a la-
tent variable. Then, we can more specifically write
the gradient component of a parameter associated
with a sufficient statistic ?(x j,zk) as:
?
i
?
zk
p(zk|di,ci)?(ci, j,zk)??
i
?
zk
p(zk|ci)?(ci, j,zk) (5)
And the gradient component of a parameter associ-
ated with a sufficient statistic ?(y,zk) is:
?
i
?
zk
p(zk|di,ci)?(di,zk)??
i
?
y,zk
p(y,zk|ci)?(y,zk) (6)
To obtain maximum-a-posteriori parameter esti-
mates we regularize the equations. Like for the stan-
dard logistic classifier this results in an additional
term of the target function and each component
of the gradient (see Sutton and McCallum 2007).
Computing the gradient requires computation of the
marginals which can be performed efficiently using
belief propagation (Yedidia et al, 2003). Note that
due to the fact, that there are no edges between the
latent variables, the inference graph is tree structured
and therefore inference yields exact results. We use
a stochastic gradient optimization method (Bottou,
2004) to optimize the target. Optimization is likely
to result in a local maximum, as the likelihood func-
tion is not convex due to the latent variables.
5 Experimental Design
In this section we discuss the experimental design
for assessing the performance of the model de-
scribed above. We give details on the dataset, fea-
tures and evaluation measures employed and present
the baseline methods used for comparison with our
model.
943
Figure 2: Dependency graph (simplified) of a sample sentence from the corpus.
Data Our experiments were carried out on the
CoNLL 2008 (Surdeanu et al, 2008) training dataset
which contains both verbal and nominal predicates.
However, we focused solely on verbal predicates,
following most previous work on semantic role la-
beling (Ma`rquez et al, 2008). The CoNLL dataset
is taken form the Wall Street Journal portion of
the Penn Treebank corpus (Marcus et al, 1993).
Role semantic annotations are based on PropBank
and have been converted from a constituent-based
to a dependency-based representation (see Surdeanu
et al 2008). For each argument of a predicate only
the head word is annotated with the correspond-
ing semantic role, rather than the whole constituent.
In this paper we are only concerned with role in-
duction, not argument identification. Therefore, we
identify the arguments of each predicate by consult-
ing the gold standard.
The CoNLL dataset alo supplies an automatic
dependency parse of each input sentence obtained
from the MaltParser (Nivre et al, 2007). The target
and features used in our model are extracted from
these parses. Syntactic functions occurring more
than 1,000 times in the gold standard are shown
in Table 1 (for more details we refer the interested
reader to Surdeanu et al 2008). Syntactic func-
tions were further modified to include prepositions if
specified, resulting in a set of functions with which
arguments can be distinguished more precisely. This
was often the case with functions such as ADV,
TMP, LOC, etc. Also, instead of using the prepo-
sition itself as the argument head, we used the ac-
tual content word modifying the preposition. We
made no attempt to treat split arguments, namely in-
stances where the semantic argument of a predicate
has several syntactic heads. These are infrequent in
the dataset, they make up for less than 1% of all ar-
guments.
Model Setup The specific instantiation of the
model used in our experiments has 10 latent vari-
ables. With 10 binary latent variables we can en-
code 1024 different target values, which seems rea-
sonable for our set of syntactic functions which
comprises around 350 elements.
Features representing argument instances were
extracted from dependency parses like the one
shown in Figure 2. We used a relatively small feature
set consisting of: the predicate lemma, the argument
lemma, the argument part-of-speech, the preposition
involved in dependency between predicate and argu-
ment (if there is one), the lemma of left-most/right-
most child of the argument, the part-of-speech of
left-most/right-most child of argument, and a key
formed by concatenating all syntactic functions of
the argument?s children. The features for the argu-
ment maker in Figure 2 are [sell, maker, NN, ?, the,
auto, DT, NN, NMOD+NMOD]. The target for this
instance (and observed syntactic function) is SBJ.
Evaluation Evaluating the output of our model
is no different from other clustering problems. We
can therefore use well-known measures from the
clustering literature to assess the quality of our
role induction method. We first created a set of
gold-standard role labeled argument instances which
were obtained from the training partition of the
CoNLL 2008 dataset (corresponding to sections
02?21 of PropBank). We used 10 clusters for each
predicate and restricted the set of predicates to those
attested with more than 20 instances. This rules out
simple cases with only few instances relative to the
number of clusters, which trivially yield high scores.
We compared the output of our method against
the gold-standard using the following common mea-
sures. Let K denote the number of clusters, ci the set
of instances in the i-th cluster and g j the set of in-
stances having the j-th gold standard semantic role
label. Cluster purity (PU) is defined as:
PU =
1
K ?i
max
j
|ci ?g j| (7)
We also used cluster accuracy (CA, Equation 8),
944
PU CA CP CR CF1
Mic Mac Mic Mac Mic Mac Mic Mac Mic Mac
SyntFunc 73.2 75.8 82.0 80.9 67.6 65.3 55.7 50.1 61.1 56.7
LogLV 72.5 74.0 81.1 79.4 64.3 60.6 59.7 56.3 61.9 58.4
UpperBndS 94.7 96.1 96.9 97.0 97.4 97.6 90.4 100 93.7 93.8
UpperBndG 98.8 99.4 99.9 99.9 99.7 99.9 100 100 99.8 100
Table 2: Clustering results using our model (LogLV) against the baseline (SyntFunc) and upper bounds
(UpperBndS and UpperBndG).
cluster precision (CP, Equation 9), and cluster recall
(CR, Equation 9). Cluster F1 (CF1) is the harmonic
mean of precision and recall.
CA =
T P+T N
T P+FP+T N +FN
(8)
CP =
T P
T P+FP
CR =
T P
T P+FN
(9)
Here T P is the number of pairs of instances which
have the same role and are in the same cluster, T N is
the number of pairs of instances which have different
roles and are in different clusters, FP is the number
of pairs of instances with different roles in the same
cluster and FN the number of pairs of instances with
the same role in different clusters.
Baselines and Upper Bound We compared our
model against a baseline that assigns arguments to
clusters based on their syntactic function. Here, no
attempt is made to correct the roles of arguments in
non-standard linkings. We would also like to com-
pare our model against a supervised system. Unfor-
tunately, this is not possible, as we are using the des-
ignated CoNLL training set as our test set, and any
supervised system trained on this data would achieve
unfairly high scores. Therefore, we approximate the
performance of a supervised system by clustering in-
stances according to their gold standard role after
introducing some noise. Specifically, we randomly
selected 5% of the gold standard roles and mapped
them to an erroneous role. This roughly corresponds
to the clustering which would be induced by a state-
of-the-art supervised system with 95% precision. Fi-
nally, we also report the results of the true upper
bound obtained by clustering the arguments, based
on their gold standard semantic role (again using 10
clusters per verb).
6 Results
Our results are summarized in Table 2. We report
cluster purity, accuracy, precision, recall, and F1 for
our latent variable logistic classifier (LogLV) and a
baseline that assigns arguments to clusters accord-
ing to their syntactic function (SyntFunc). The table
also includes the gold standard upper bound (Up-
perBndG) and its supervised proxy (UpperBndS).
We report micro- and macro-average scores.3
Model scores are quite similar to the baseline,
which might suggest that the model is simply repli-
cating the observed data. However, this is not the
case: canonical functions differ from observed func-
tions for approximately 27% of the argument in-
stances. If the baseline treated these instances cor-
rectly, we would expect it to outperform our model.
The fact that it does not, indicates that the baseline
error rate is higher precisely on these instances. In
other words, the model can help in detecting alter-
nate linkings and thus baseline errors.
We further analyzed our model?s ability to de-
tect alternate linkings. Specifically, if we assume a
standard linking where model and observation agree
and an alternate linking where they disagree, we
obtain the following. The number of true positives
(correctly detected alternate linkings) is 27,606, the
number of false positives (incorrectly marked al-
ternations) is 32,031, the number of true negatives
(cases where the model correctly did not detect an
alternate linking) is 132,556, and the number of false
negatives (alternate linkings that the model should
have detected but did not) is 32,516.4. The analysis
shows that 46% of alternations (baseline errors) are
detected.
3Micro-averages are computed over instances while macro-
averages are computed over verbs.
4Note that the true/false positives/negatives here refer to al-
ternate linkings, not to be confused with the true/false positives
in equations (8) and (9).
945
PU CA CP CR CF1
Mic Mac Mic Mac Mic Mac Mic Mac Mic Mac
SyntFunct 73.9 77.8 82.1 81.3 68.0 66.5 55.9 50.3 61.4 57.3
LogLV 82.6 83.7 87.4 85.5 79.1 74.5 73.3 68.5 76.1 71.4
Table 3: Clustering results using our model to detect alternate linkings (LogLV) against the baseline (Synt-
Func).
We can therefore increase cluster purity by clus-
tering only those instances where the model does
not indicate an alternation. The results are shown
in Table 3. Using less instances while keeping the
number of clusters the same will by itself tend to
increase performance. To compensate for this, we
also report results for the baseline on a reduced
dataset. The latter was obtained from the origi-
nal dataset by randomly removing the same num-
ber of instances.5 By using the model to detect al-
ternations, scores improve over the baseline across
the board. We observe performance gains for pu-
rity which increases by 8.7% (micro-average; com-
pare Tables 2 and 3). F1 also improves considerably
by 13% (micro-average). These results are encour-
aging indicating that detecting alternate linkings is
an important first step towards more accurate role
induction.
We also conducted a more detailed error analysis
to gain more insight into the behavior of our model.
In most cases, alternate linkings where A1 occurs in
subject position and A0 in object position are canon-
icalized correctly (with 96% and 97% precision, re-
spectively). Half of the detected non-standard link-
ings involve adjunct roles. Here, the model has much
more difficulty with canonicalization and is success-
ful approximately 25% of the time. For example, in
the phrase occur at dawn the model canonicalizes
LOC to ADV, whereas TMP would be the correct
function. About 75% of all false negatives are due to
core roles and only 25% due to adjunct roles. Many
false negatives are due to parser errors, which are
reproduced by the model. This indicates overfitting,
and indeed many of the false negatives involve in-
frequent lexical items (e.g., juxtapose or Odyssey).
Finally, to put our evaluation results into context,
we also wanted to compare against Grenager and
Manning?s (2006) related system. A direct compar-
ison is somewhat problematic due to the use of dif-
5This was repeated several times to ensure that the results
are stable across runs.
ferent datasets and the fact that we induce labels for
all roles whereas they collapse adjunct roles to a sin-
gle role. Nevertheless, we made a good-faith effort
to evaluate our system using their evaluation setting.
Specifically, we ran our system on the same test set,
Section 23 of the Penn Treebank (annotated with
PropBank roles), using gold standard parses with six
clusters for each verb type. Our model achieves a
cluster purity score of 90.3% on this dataset com-
pared to 89.7% reported in Grenager and Manning.
7 Conclusions
In this paper we have presented a novel framework
for unsupervised role induction. We conceptualized
the induction problem as one of detecting alternate
linkings and finding their canonical syntactic form,
and formulated a novel probabilistic model that per-
forms these tasks. The model extends the logis-
tic classifier with latent variables and is trained on
parsed output which is used as a noisy target for
learning. Experimental results show promise, alter-
nations can be successfully detected and the quality
of the induced role clusters can be substantially en-
hanced.
We argue that the present model could be use-
fully employed to enhance the performance of other
models. For example, it could be used in an active
learning context to identify argument instances that
are difficult to classify for a supervised or semi-
supervised system and would presumably benefit
from additional (manual) annotation. Importantly,
the framework can incorporate different probabilis-
tic models for detection and canonicalization which
we intend to explore in the future. We also aim to
embed and test our role induction method within a
full SRL system that is also concerned with argu-
ment identification. Eventually, we also intend to re-
place the treebank-trained parser with a chunker.
946
References
Abend, O., R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of ACL-IJCNLP. Singapore,
pages 28?36.
Berger, A., S. Della Pietra, and V. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics 22(1):39?71.
Bishop, C. 2006. Pattern Recognition and Machine
Learning. Springer.
Bottou, L. 2004. Stochastic Learning. In Advanced Lec-
tures on Machine Learning, Springer Verlag, Lecture
Notes in Artificial Intelligence, pages 146?168.
Dowty, D. 1991. Thematic Proto Roles and Argument
Selection. Language 67(3):547?619.
Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International Journal
of Lexicography 16:235?250.
Fu?rstenau, H. and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of EMNLP. Singapore, pages 11?20.
Gildea, D. and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics
28(3):245?288.
Grenager, T. and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of EMNLP. Sydney, Australia, pages 1?8.
Lapata, M. 1999. Acquiring Lexical Generalizations
from Corpora: A Case Study for Diathesis Alterna-
tions. In Proceedings of the 37th ACL. pages 397?404.
Levin, B. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. The University of Chicago
Press.
Marcus, M., B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: the
Penn Treebank. Computational Linguistics 19(2):313?
330.
Ma`rquez, L., X. Carreras, K. Litkowski, and S. Steven-
son. 2008. Semantic Role Labeling: an Introduc-
tion to the Special Issue. Computational Linguistics
34(2):145?159.
McCarthy, D. 2002. Using Semantic Preferences to Iden-
tify Verbal Participation in Role Switching Alterna-
tions. In Proceedings of the 1st NAACL. Seattle, WA,
pages 256?263.
McCarthy, D. and A. Korhonen. 1998. Detecting Verbal
Participation in Diathesis Alternations. In Proceed-
ings of COLING/ACL. Montre?al, Canada, pages 1493?
1495.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Descrip-
tion of SQUASH, the SFU Question Answering Sum-
mary Handler for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP Document Under-
standing Workshop. Vancouver, Canada.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering 13(2):95?135.
Pado?, S. and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research 36:307?340.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics 31(1):71?106.
Pradhan, S. S., W. Ward, and J. H. Martin. 2008. Towards
Robust Semantic Role Labeling. Computational Lin-
guistics 34(2):289?310.
Schulte im Walde, S. and C. Brew. 2002. Inducing
German Semantic Verb Classes from Purely Syntac-
tic Subcategorisation Information. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 223?230.
Shen, D. and M. Lapata. 2007. Using Semantic Roles to
Improve Question Answering. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 12?
21.
Surdeanu, M., S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 8?15.
Surdeanu, M., R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL. Manchester, England, pages
159?177.
Sutton, C. and A. McCallum. 2007. An Introduction to
Conditional Random Fields for Relational Learning.
In L. Getoor and B. Taskar, editors, Introduction to
Statistical Relational Learning, MIT Press, pages 93?
127.
Swier, R. and S. Stevenson. 2004. Unsupervised Se-
mantic Role Labelling. In Proceedings of EMNLP.
Barcelona, Spain, pages 95?102.
Wu, D. and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of
NAACL HLT 2009: Short Papers. Boulder, Colorado,
pages 13?16.
Yedidia, J., W. Freeman, and Y. Weiss. 2003. Understand-
ing Belief Propagation and its Generalizations. Mor-
gan Kaufmann Publishers Inc., pages 239?269.
947
Proceedings of NAACL-HLT 2013, pages 772?776,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Graph-Based Seed Set Expansion for Relation Extraction Using
Random Walk Hitting Times
Joel Lang
University of Geneva
7 Route de Drize
1227 Carouge, Switzerland
joel.lang@unige.ch
James Henderson
Xerox Research Centre Europe
6 Chemin de Maupertuis
38240 Meylan, France
james.henderson@xrce.xerox.com
Abstract
Iterative bootstrapping methods are
widely employed for relation extraction,
especially because they require only a
small amount of human supervision.
Unfortunately, a phenomenon known
as semantic drift can affect the accuracy
of iterative bootstrapping and lead to
poor extractions. This paper proposes
an alternative bootstrapping method,
which ranks relation tuples by measuring
their distance to the seed tuples in a
bipartite tuple-pattern graph. In contrast
to previous bootstrapping methods, our
method is not susceptible to semantic
drift, and it empirically results in better
extractions than iterative methods.
1 Introduction
The goal of relation extraction is to extract tu-
ples of a particular relation from a corpus of
natural language text. A widely employed ap-
proach to relation extraction is based on iter-
ative bootstrapping (Brin, 1998; Agichtein and
Gravano, 2000; Pasca et al, 2006; Pantel and
Pennacchiotti, 2006), which can be applied with
only small amounts of supervision and which
scales well to very large datasets.
A well-known problem with iterative boot-
strapping is a phenomenon known as seman-
tic drift (Curran et al, 2007): as bootstrap-
ping proceeds it is likely that unreliable pat-
terns will lead to false extractions. These extrac-
tion errors are amplified in the following itera-
tions and the extracted relation will drift away
from the intended target. Semantic drift often
results in low precision extractions and there-
fore poses a major limitation of iterative boot-
strapping algorithms. Previous work on itera-
tive bootstrapping has addressed the issue of re-
ducing semantic drift for example by bagging
the results of various runs employing differing
seed tuples, constructing filters which identify
false tuples or patterns and adding further con-
straints to the bootstrapping process (T. McIn-
tosh, 2010; McIntosh and Curran, 2009; Curran
et al, 2007).
However, the analysis of Komachi et al
(2008) has shown that semantic drift is an in-
herent property of iterative bootstrapping algo-
rithms and therefore poses a fundamental prob-
lem. They have shown that iterative bootstrap-
ping without pruning corresponds to an eigen-
vector computation and thus as the number of
iterations increases the resulting ranking will al-
ways converge towards the same static ranking
of tuples, regardless of the particular choice of
seed instances.
In this paper, we describe an alternative
method, that is not susceptible to semantic drift.
We represent our data as a bipartite graph,
whose vertices correspond to patterns and tu-
ples respectively and whose edges capture cooc-
currences and then measure the distance of a
tuple to the seed set in terms of random walk
hitting times. Experimental results confirm that
semantic drift is avoided by our method and
show that substantial improvements over iter-
ative forms of bootstrapping are possible.
772
2 Scoring with Hitting Times
From a given corpus, we extract a dataset con-
sisting of tuples and patterns. Tuples are pairs
of co-occurring strings in the corpus, such as
(Bill Gates, Microsoft), which potentially belong
to a particular relation of interest. In our case,
patterns are simply the sequence of tokens oc-
curring between tuple elements, e.g. ?is the
founder of?. We represent all the tuple types1
X and all the extraction pattern types Y con-
tained in a given corpus through an undirected,
weighted, bipartite graph G = (V,E) with ver-
tices V = X ? Y and edges E ? X ? Y , where
an edge (x, y) ? E indicates that tuple x oc-
currs with pattern y somewhere in the corpus.
Edge weights are defined through a weight ma-
trix W which holds the weight Wi,j = w(vi, vj)
for edges (vi, vj) ? E. Specifically, we use the
count of how many times a tuple occurs with
a pattern in the corpus and weights for uncon-
nected vertices are zero.
Our goal is to compute a score vector ? hold-
ing a score ?i = ?(xi) for each tuple xi ? X,
which quantifies how well the tuple matches the
seed tuples. Higher scores indicate that the tu-
ple is more likely to belong to the relation de-
fined through the seeds and thus the score vec-
tor effectively provides a ranking of the tuples.
We define scores of tuples based on their dis-
tance2 to the seed tuples in the graph. The dis-
tance of some tuple x to the seed set S can
be naturally formalized in terms of the aver-
age time it takes until a random walk starting
in S reaches x, the hitting time. The random
walk is defined through the probability distri-
bution over start vertices and through a ma-
trix of transition probabilities. Edge weights
are constrained to be non-negative, which al-
lows us to define the transition matrix P with
Pi,j = p(vj |vi) = 1dvi
w(vi, vj), where dv =
?
vk?V
w(v, vk) is the degree of a vertex v ? V .
The distance of two vertices is measured in
terms of the average time of a random walk be-
1Note that we are using tuple and pattern types rather
than particular mentions in the corpus.
2The term is used informally. In particular, hitting times
are not a distance metric, since they can be asymmetric.
tween the two. Specifically, we adopt the notion
of T-truncated hitting time (Sarkar and Moore,
2007) defined as the expected number of steps
it takes until a random walk of at most T steps
starting at vi reaches vj for the first time:
hT (vj |vi) =
{
0 iff. vj = vi or T=0
1 +
?
vk?V
p(vk|vi)hT?1(vj |vk)
The truncated hitting time hT (vj |vi) can be
approximately computed by sampling M inde-
pendent random walks starting at vi of length T
and computing
h?T (vj |vi) =
1
M
m?
k=1
tk + (1?
m
M
)T (1)
where {t1 . . . tm} are the sampled first-hit times
of random walks which reach vj within T steps
(Sarkar et al, 2008).
The score ?HT (v) of a vertex v /? S to the seed
set S is then defined as the inverse of the aver-
age T -truncated hitting time of random walks
starting at a randomly chosen vertex s ? S:
1
?HT (v)
= hT (v|S) =
1
|S|
?
s?S
hT (v|s) (2)
3 Experiments
We extracted tuples and patterns from the fifth
edition of the Gigaword corpus (Parker et al,
2011), by running a named entity tagger and
extracting all pairs of named entities and ex-
tracting occurring within the same sentence
which do not have another named entity stand-
ing between them. Gold standard seed and test
tuples for a set of relations were obtained from
YAGO (Suchanek et al, 2007). Specifically, we
took all relations for which there are at least
300 tuples, each of which occurs at least once
in the corpus. This resulted in the set of rela-
tions shown in Table 1, plus the development
relation hasWonPrize.
For evaluation, we use the percentile rank of
the median test set element (PRM, see Francois
et al 2007), which reflects the quality of the
773
full produced ranking, not just the top N ele-
ments and is furthermore computable with only
a small set of labeled test tuples 3.
We compare our proposed method based on
hitting times (HT) with two variants of iterative
bootstrapping. The first one (IB1) does not em-
ploy pruning and corresponds to the algorithm
described in Komachi et al (2008). The sec-
ond one (IB2) corresponds to a standard boot-
strapping algorithm which employs pruning af-
ter each step in order to reduce semantic drift.
Specifically, scores are pruned after projecting
from X onto Y and from Y onto X, retaining
only the top N (t) = N0t scores at iteration t and
setting all other scores to zero.
3.1 Parametrizations
The experiments in this section were conducted
on the held out development relation hasWon-
Prize. The ranking produced by both forms of
iterative bootstrapping IB1 and IB2 depend on
the number of iterations, as shown in Figure 1.
IB1 achieves an optimal ranking after just one
iteration and thereafter scores get worse due to
semantic drift. In contrast, pruning helps avoid
semantic drift for IB2, which attains an optimal
score after 2 iterations and achieves relatively
constant scores for several iterations. However,
during iteration 9 an incorrect pattern is kept
and this at once leads to a drastic loss in ac-
curacy, showing that semantic drift is only de-
ferred and not completely eliminated.
Our method HT has parameter T , correspond-
ing to the truncation time, i.e., maximal number
of steps of a random walk. Figure 2 shows the
PRM of our method for different values of T .
Performance gets better as T increases and is
optimal for T = 12, whereas for larger values,
the performance gets slightly worse again. The
figure shows that, if T is large enough (> 5), the
PRM is relatively constant and there is no phe-
nomenon comparable to semantic drift, which
causes instability in the produced rankings.
3other common metrics do not satisfy these conditions.
Figure 1: PRM for iterative bootstrapping with-
out pruning (IB1) and with pruning (IB2). A
lower PRM is better.
Figure 2: PRM for our method based on hitting
times, for different values of the truncation time
parameter T.
3.2 Method Comparison
To evaluate the methods, firstly the parameters
for each method were set to the optimal values
as determined in the previous section. For the
experiments here, we again use 200 randomly
chosen tuples as the seeds for each relation. All
the remaining gold standard tuples are used for
testing.
Table 1 shows the PRM for the three methods.
For a majority of the relations (12/16) HT at-
tains the best, i.e. lowest, PRM, which confirms
that hitting times constitute an accurate way of
measuring the distance of tuples to the seed set.
IB1 and IB2 each perform best on 2/16 of the
relations. A sign test on these results yields that
774
Relation IB1 IB2 HT
created 1.82 1.71 0.803
dealsWith 0.0262 0.107 0.0481
diedIn 30.5 18.4 20.4
directed 0.171 0.238 0.166
hasChild 7.66 32.2 4.26
influences 5.93 5.48 6.60
isAffiliatedTo 1.54 2.01 1.30
isCitizenOf 1.74 1.87 1.68
isLeaderOf 1.37 1.91 0.401
isMarriedTo 4.69 4.14 1.27
isPoliticianOf 0.0117 0.110 0.0409
livesIn 3.17 2.48 1.70
owns 11.0 2.10 2.07
produced 1.55 0.967 0.240
wasBornIn 11.3 9.37 8.42
worksAt 1.52 2.21 0.193
Table 1: PRM in percent for all relations, for all
three models. A lower PRM corresponds to a
better model, with the best score indicated in
bold.
Figure 3: PRM for the three methods, as a func-
tion of the size of the seed set for the relation
created.
HT is better than both IB1 and IB2 at signifi-
cance level ? < 0.01.
Moreover, the ranking produced by HT is sta-
ble and not affected by semantic drift, given that
even where results are worse than for IB1 or
IB2, they are still close to the best performing
method. In contrast, when semantic drift oc-
curs, the performance of IB1 and IB2 can dete-
riorate drastically, e.g. for the worksAt relation,
where both IB1 and IB2 produce rankings that
are a lot worse than the one produced by HT.
3.3 Sensitivity to Seed Set Size
Figure 3 shows the PRM for each of the three
methods as a function of the size of the seed set
for the relation created. For small seed sets, the
performance of the iterative methods can be in-
creased by adding more seeds. However, from
a seed set size of 50 onwards, performance re-
mains relatively constant. In other words, iter-
ative bootstrapping is not benefitting from the
information provided by the additional labeled
data, and thus has a poor learning performance.
In contrast, for our method based on hitting
times, the performance continually improves as
the seed set size is increased. Thus, also in terms
of learning performance, our method is more
sound than iterative bootstrapping.
4 Conclusions
The paper has presented a graph-based method
for seed set expansion which is not susceptible
to semantic drift and on most relations outper-
forms iterative bootstrapping. The method mea-
sures distance between vertices through random
walk hitting times. One property which makes
hitting times an appropriate distance measure
is their ability to reflect the overall connectivity
structure of the graph, in contrast to measures
such as the shortest path between two vertices.
The hitting time will decrease when the num-
ber of paths from the start vertex to the tar-
get vertex increases, when the length of paths
decreases or when the likelihood (weights) of
paths increases. These properties are particu-
larly important when the observed graph edges
must be assumed to be merely a sample of all
plausible edges, possibly perturbated by noise.
This has also been asserted by previous work,
which has shown that hitting times successfully
capture the notion of similarity for other natural
language processing problems such as learning
paraphrases (Kok and Brockett, 2010) and re-
lated problems such as query suggestion (Mei
et al, 2008). Future work will be aimed to-
wards employing our hitting time based method
in combination with a richer feature set.
775
References
Agichtein, E. and Gravano, L. (2000). Snow-
ball: Extracting Relations from Large Plain-
text Collections. In Proceedings of the Fifth
ACM Conference on Digital Libraries.
Brin, S. (1998). Extracting Patterns and Rela-
tions from the World-Wide Web. In Proceed-
ings of the 1998 International Workshop on the
Web and Databases.
Curran, J., Murphy, T., and Scholz, B. (2007).
Minimising Semantic Drift with Mutual Exclu-
sion Bootstrapping. In Proceedings of the 10th
Conference of the Pacific Association for Com-
putational Linguistics.
Francois, F., Pirotte, A., Renders, J., and
Saerens, M. (2007). Random-Walk Computa-
tion of Similarities between Nodes of a Graph
with Application to Collaborative Recommen-
dation. IEEE Transactions on Knowledge and
Data Engineering, 19(3):355 ?369.
Kok, S. and Brockett, C. (2010). Hitting the
Right Paraphrases in Good Time. In Proceed-
ings of the Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics.
Komachi, M., Kudo, T., Shimbo, M., and Mat-
sumoto, Y. (2008). Graph-based Analysis
of Semantic Drift in Espresso-like Bootstrap-
ping Algorithms. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing.
McIntosh, T. and Curran, J. (2009). Reduc-
ing Semantic Drift with Bagging and Distri-
butional Similarity. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL.
Mei, Q., Zhou, D., and Church, K. (2008). Query
Suggestion Using Hitting Time. In Proceed-
ings of the 17th ACM Conference on Informa-
tion and Knowledge Management.
Pantel, P. and Pennacchiotti, M. (2006).
Espresso: Leveraging Generic Patterns for Au-
tomatically Harvesting Semantic Relations. In
Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th
Annual Meeting of the Association for Compu-
tational Linguistics.
Parker, R., Graff, D., Kong, J., Chen, K., and
Maeda, K. (2011). English Gigaword Fifth
Edition. Technical report, Linguistic Data
Consortium.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., and
Jain, A. (2006). Organizing and Searching
the World Wide Web of Facts ? Step One: the
One-million Fact Extraction Challenge. In Pro-
ceedings of the 21st National Conference on Ar-
tificial Intelligence (AAAI).
Sarkar, P. and Moore, A. (2007). A Tractable
Approach to Finding Closest Truncated-
commute-time Neighbors in Large Graphs. In
Proceedings of the 23rd Conference on Uncer-
tainty in Artificial Intelligence.
Sarkar, P., Moore, A., and Prakash, A. (2008).
Fast Incremental Proximity Search in Large
Graphs. In Proceedings of the 25th Interna-
tional Conference on Machine Learning.
Suchanek, F., Kasneci, G., and Weikum, G.
(2007). Yago: A Core of Semantic Knowl-
edge. In Proceedings of the International World
Wide Web Conference (WWW).
T. McIntosh (2010). Unsupervised Discovery
of Negative Categories in Lexicon Bootstrap-
ping. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Pro-
cessing.
776
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117?1126,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Semantic Role Induction via Split-Merge Clustering
Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we describe an unsupervised
method for semantic role induction which
holds promise for relieving the data acqui-
sition bottleneck associated with supervised
role labelers. We present an algorithm that it-
eratively splits and merges clusters represent-
ing semantic roles, thereby leading from an
initial clustering to a final clustering of bet-
ter quality. The method is simple, surpris-
ingly effective, and allows to integrate lin-
guistic knowledge transparently. By com-
bining role induction with a rule-based com-
ponent for argument identification we obtain
an unsupervised end-to-end semantic role la-
beling system. Evaluation on the CoNLL
2008 benchmark dataset demonstrates that
our method outperforms competitive unsuper-
vised approaches by a wide margin.
1 Introduction
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the re-
lations that hold between a predicate and its argu-
ments, abstracting over surface syntactic configura-
tions. In the example sentences below. window oc-
cupies different syntactic positions ? it is the object
of broke in sentences (1a,b), and the subject in (1c)
? while bearing the same semantic role, i.e., the
physical object affected by the breaking event. Anal-
ogously, rock is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
(1) a. [Joe]A0 broke the [window]A1 with a
[rock]A2.
b. The [rock]A2 broke the [window]A1.
c. The [window]A1 broke.
The semantic roles in the examples are labeled
in the style of PropBank (Palmer et al, 2005), a
broad-coverage human-annotated corpus of seman-
tic roles and their syntactic realizations. Under the
PropBank annotation framework (which we will as-
sume throughout this paper) each predicate is as-
sociated with a set of core roles (named A0, A1,
A2, and so on) whose interpretations are specific to
that predicate1 and a set of adjunct roles (e.g., loca-
tion or time) whose interpretation is common across
predicates. This type of semantic analysis is admit-
tedly shallow but relatively straightforward to auto-
mate and useful for the development of broad cov-
erage, domain-independent language understanding
systems. Indeed, the analysis produced by existing
semantic role labelers has been shown to benefit a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al, 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al, 2005).
Since both argument identification and labeling
can be readily modeled as classification tasks, most
state-of-the-art systems to date conceptualize se-
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
1117
mantic role labeling as a supervised learning prob-
lem. Current approaches have high performance ?
a system will recall around 81% of the arguments
correctly and 95% of those will be assigned a cor-
rect semantic role (see Ma`rquez et al (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al, 2008).
Unfortunately, the reliance on role-annotated data
which is expensive and time-consuming to produce
for every language and domain, presents a major
bottleneck to the widespread application of semantic
role labeling. Given the data requirements for super-
vised systems and the current paucity of such data,
unsupervised methods offer a promising alternative.
They require no human effort for training thus lead-
ing to significant savings in time and resources re-
quired for annotating text. And their output can be
used in different ways, e.g., as a semantic prepro-
cessing step for applications that require broad cov-
erage understanding or as training material for su-
pervised algorithms.
In this paper we present a simple approach to un-
supervised semantic role labeling. Following com-
mon practice, our system proceeds in two stages.
It first identifies the semantic arguments of a pred-
icate and then assigns semantic roles to them. Both
stages operate over syntactically analyzed sentences
without access to any data annotated with semantic
roles. Argument identification is carried out through
a small set of linguistically-motivated rules, whereas
role induction is treated as a clustering problem. In
this setting, the goal is to assign argument instances
to clusters such that each cluster contains arguments
corresponding to a specific semantic role and each
role corresponds to exactly one cluster. We formu-
late a clustering algorithm that executes a series of
split and merge operations in order to transduce an
initial clustering into a final clustering of better qual-
ity. Split operations leverage syntactic cues so as to
create ?pure? clusters that contain arguments of the
same role whereas merge operations bring together
argument instances of a particular role located in
different clusters. We test the effectiveness of our
induction method on the CoNLL 2008 benchmark
dataset and demonstrate improvements over compet-
itive unsupervised methods by a wide margin.
2 Related Work
As mentioned earlier, much previous work has
focused on building supervised SRL systems
(Ma`rquez et al, 2008). A few semi-supervised ap-
proaches have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fu?rstenau and Lapata, 2009) or across different lan-
guages (Pado? and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) attempt
to increase the coverage of PropBank by leveraging
existing labeled data. Rather than annotating new
sentences that contain previously unseen verbs, they
find syntactically similar verbs and use their annota-
tions as surrogate training data.
Swier and Stevenson (2004) induce role labels
with a bootstrapping scheme where the set of la-
beled instances is iteratively expanded using a clas-
sifier trained on previously labeled instances. Their
method is unsupervised in that it starts with a dataset
containing no role annotations at all. However, it re-
quires significant human effort as it makes use of
VerbNet (Kipper et al, 2000) in order to identify the
arguments of predicates and make initial role assign-
ments. VerbNet is a broad coverage lexicon orga-
nized into verb classes each of which is explicitly
associated with argument realization and semantic
role specifications.
Abend et al (2009) propose an algorithm that
identifies the arguments of predicates by relying
only on part of speech annotations, without, how-
ever, assigning semantic roles. In contrast, Lang
and Lapata (2010) focus solely on the role induction
problem which they formulate as the process of de-
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that makes
use of the close relationship between syntactic func-
tions and semantic roles. Grenager and Manning
1118
(2006) propose a directed graphical model which re-
lates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Our own work also follows the unsupervised
learning paradigm. We formulate the induction of
semantic roles as a clustering problem and propose a
split-merge algorithm which iteratively manipulates
clusters representing semantic roles. The motiva-
tion behind our approach was to design a concep-
tually simple system, that allows for the incorpo-
ration of linguistic knowledge in a straightforward
and transparent manner. For example, arguments
occurring in similar syntactic positions are likely to
bear the same semantic role and should therefore
be grouped together. Analogously, arguments that
are lexically similar are likely to represent the same
semantic role. We operationalize these notions us-
ing a scoring function that quantifies the compatibil-
ity between arbitrary cluster pairs. Like Lang and
Lapata (2010) and Grenager and Manning (2006)
our method operates over syntactically parsed sen-
tences, without, however, making use of any infor-
mation pertaining to semantic roles (e.g., in form of
a lexical resource or manually annotated data). Per-
forming role-semantic analysis without a treebank-
trained parser is an interesting research direction,
however, we leave this to future work.
3 Learning Setting
We follow the general architecture of supervised se-
mantic role labeling systems. Given a sentence and
a designated verb, the SRL task consists of identify-
ing the arguments of the verbal predicate (argument
identification) and labeling them with semantic roles
(role induction).
In our case neither argument identification nor
role induction relies on role-annotated data or other
semantic resources although we assume that the in-
put sentences are syntactically analyzed. Our ap-
proach is not tied to a specific syntactic representa-
tion ? both constituent- and dependency-based rep-
resentations could be used. However, we opted for a
dependency-based representation, as it simplifies ar-
gument identification considerably and is consistent
with the CoNLL 2008 benchmark dataset used for
evaluation in our experiments.
Given a dependency parse of a sentence, our sys-
tem identifies argument instances and assigns them
to clusters. Thereafter, argument instances can be
labeled with an identifier corresponding to the clus-
ter they have been assigned to, similar to PropBank
core labels (e.g., A0, A1).
4 Argument Identification
In the supervised setting, a classifier is employed
in order to decide for each node in the parse tree
whether it represents a semantic argument or not.
Nodes classified as arguments are then assigned a se-
mantic role. In the unsupervised setting, we slightly
reformulate argument identification as the task of
discarding as many non-semantic arguments as pos-
sible. This means that the argument identification
component does not make a final positive decision
for any of the argument candidates; instead, this de-
cision is deferred to role induction. The rules given
in Table 1 are used to discard or select argument can-
didates. They primarily take into account the parts of
speech and the syntactic relations encountered when
traversing the dependency tree from predicate to ar-
gument. For each candidate, the first matching rule
is applied.
We will exemplify how the argument identifica-
tion component works for the predicate expect in the
sentence ?The company said it expects its sales to
remain steady? whose parse tree is shown in Fig-
ure 1. Initially, all words save the predicate itself
are treated as argument candidates. Then, the rules
from Table 1 are applied as follows. Firstly, words
the and to are discarded based on their part of speech
(rule (1)); then, remain is discarded because the path
ends with the relation IM and said is discarded as
the path ends with an upward-leading OBJ relation
(rule (2)). Rule (3) does not match and is therefore
not applied. Next, steady is discarded because there
is a downward-leading OPRD relation along the path
and the words company and its are discarded be-
cause of the OBJ relations along the path (rule (4)).
Rule (5) does not apply but words it and sales are
kept as likely arguments (rule (6)). Finally, rule (7)
does not apply, because there are no candidates left.
1119
1. Discard a candidate if it is a determiner, in-
finitival marker, coordinating conjunction, or
punctuation.
2. Discard a candidate if the path of relations
from predicate to candidate ends with coordi-
nation, subordination, etc. (see the Appendix
for the full list of relations).
3. Keep a candidate if it is the closest subject
(governed by the subject-relation) to the left
of a predicate and the relations from predi-
cate p to the governor g of the candidate are
all upward-leading (directed as g? p).
4. Discard a candidate if the path between the
predicate and the candidate, excluding the last
relation, contains a subject relation, adjectival
modifier relation, etc. (see the Appendix for
the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if the predicate is its parent.
7. Keep a candidate if the path from predicate
to candidate leads along several verbal nodes
(verb chain) and ends with arbitrary relation.
8. Discard all remaining candidates.
Table 1: Argument identification rules.
5 Split-Merge Role Induction
We treat role induction as a clustering problem with
the goal of assigning argument instances (i.e., spe-
cific arguments occurring in an input sentence) to
clusters such that these represent semantic roles. In
accordance with PropBank, we induce a separate set
of clusters for each verb and each cluster thus repre-
sents a verb-specific role.
Our algorithm works by iteratively splitting and
merging clusters of argument instances in order to
arrive at increasingly accurate representations of se-
mantic roles. Although splits and merges could be
arbitrarily interleaved, our algorithm executes a sin-
gle split operation (split phase), followed by a se-
ries of merges (merge phase). The split phase par-
titions the seed cluster containing all argument in-
stances of a particular verb into more fine-grained
(sub-)clusters. This initial split results in a clustering
with high purity but low collocation, i.e., argument
instances in each cluster tend to belong to the same
role but argument instances of a particular role are
Figure 1: A sample dependency parse with depen-
dency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative com-
plement), PRD (predicative complement), and IM
(infinitive marker). See Surdeanu et al (2008) for
more details on this variant of dependency syntax.
located in many clusters. The degree of dislocation
is reduced in the consecutive merge phase, in which
clusters that are likely to represent the same role are
merged.
5.1 Split Phase
Initially, all arguments of a particular verb are placed
in a single cluster. The goal then is to partition this
cluster in such a way that the split-off clusters have
high purity, i.e., contain argument instances of the
same role. Towards this end, we characterize each
argument instance by a key, formed by concatenat-
ing the following syntactic cues:
? verb voice (active/passive);
? argument linear position relative to predicate
(left/right);
? syntactic relation of argument to its governor;
? preposition used for argument realization.
A cluster is allocated for each key and all argument
instances with a matching key are assigned to that
cluster. Since each cluster encodes fine-grained syn-
tactic distinctions, we assume that arguments occur-
ring in the same position are likely to bear the same
semantic role. The assumption is largely supported
by our empirical results (see Section 7); the clusters
emerging from the initial split phase have a purity
of approximately 90%. While the incorporation of
additional cues (e.g., indicating the part of speech
of the subject or transitivity) would result in even
greater purity, it would also create problematically
small clusters, thereby negatively affecting the suc-
cessive merge phase.
1120
5.2 Merge Phase
The split phase creates clusters with high purity,
however, argument instances of a particular role are
often scattered amongst many clusters resulting in a
cluster assignment with low collocation. The goal
of the merge phase is to improve collocation by ex-
ecuting a series of merge steps. At each step, pairs
of clusters are considered for merging. Each pair is
scored by a function that reflects how likely the two
clusters are to contain arguments of the same role
and the best scoring pair is chosen for merging. In
the following, we will specify which pairs of clus-
ters are considered (candidate search), how they are
scored, and when the merge phase terminates.
5.2.1 Candidate Search
In principle, we could simply enumerate and score
all possible cluster pairs at each iteration. In practice
however, such a procedure has a number of draw-
backs. Besides being inefficient, it requires a scoring
function with comparable scores for arbitrary pairs
of clusters. For example, let a, b, c, and d denote
clusters. Then, score(a,b) and score(c,d) must be
comparable. This is a stronger requirement than de-
manding that only scores involving some common
cluster (e.g., score(a,b) and score(a,c)) be com-
parable. Moreover, it would be desirable to ex-
clude pairings involving small clusters (i.e., with
few instances) as scores for these tend to be unre-
liable. Rather than considering all cluster pairings,
we therefore select a specific cluster at each step and
score merges between this cluster and certain other
clusters. If a sufficiently good merge is found, it is
executed, otherwise the clustering does not change.
In addition, we prioritize merges between large clus-
ters and avoid merges between small clusters.
Algorithm 1 implements our merging procedure.
Each pass through the inner loop (lines 4?12) selects
a different cluster to consider at that step. Then,
merges between the selected cluster and all larger
clusters are considered. The highest-scoring merge
is executed, unless all merges are ruled out, i.e., have
a score below the threshold ?. After each comple-
tion of the inner loop, the thresholds contained in
the scoring function (discussed below) are adjusted
and this is repeated until some termination criterion
is met (discussed in Section 5.2.3).
Algorithm 1: Cluster merging procedure. Oper-
ation merge(Li,L j) merges cluster Li into cluster
L j and removes Li from the list L.
1 while not done do
2 L? a list of all clusters sorted by number
of instances in descending order
3 i? 1
4 while i < length(L) do
5 j? arg max
0? j?<i
score(Li,L j?)
6 if score(Li,L j)? ? then
7 merge(Li,L j)
8 end
9 else
10 i? i+1
11 end
12 end
13 adjust thresholds
14 end
5.2.2 Scoring Function
Our scoring function quantifies whether two clusters
are likely to contain arguments of the same role and
was designed to reflect the following criteria:
1. whether the arguments found in the two clus-
ters are lexically similar;
2. whether clause-level constraints are satisfied,
specifically the constraint that all arguments
of a particular clause have different semantic
roles, i.e., are assigned to different clusters;
3. whether the arguments present in the two clus-
ters have similar parts of speech.
Qualitatively speaking, criteria (2) and (3) provide
negative evidence in the sense that they can be used
to rule out incorrect merges but not to identify cor-
rect ones. For example, two clusters with drastically
different parts of speech are unlikely to represent
the same role. However, the converse is not neces-
sarily true as part of speech similarity does not im-
ply role-semantic similarity. Analogously, the fact
that clause-level constraints are not met provides ev-
idence against a merge, but the fact that these are
satisfied is not reliable evidence in favor of a merge.
In contrast, lexical similarity implies that the clus-
1121
ters are likely to represent the same semantic role.
It is reasonable to assume that due to selectional re-
strictions, verbs will be associated with lexical units
that are semantically related and assume similar syn-
tactic positions (e.g., eat prefers as an object edible
things such as apple, biscuit, meat), thus bearing the
same semantic role. Unavoidably, lexical similarity
will be more reliable for arguments with overt lex-
ical content as opposed to pronouns, however this
should not impact the scoring of sufficiently large
clusters.
Each of the criteria mentioned above is quantified
through a separate score and combined into an over-
all similarity function, which scores two clusters c
and c? as follows:
score(c,c?) =
?
??
??
0 if pos(c,c?) < ?,
0 if cons(c,c?) < ?,
lex(c,c?) otherwise.
(2)
The particular form of this function is motivated by
the distinction between positive and negative evi-
dence. When the part-of-speech similarity (pos) is
below a certain threshold ? or when clause-level
constraints (cons) are satisfied to a lesser extent than
threshold ?, the score takes value zero and the merge
is ruled out. If this is not the case, the lexical similar-
ity score (lex) determines the magnitude of the over-
all score. In the remainder of this section we will
explain how the individual scores (pos, cons, and
lex) are defined and then move on to discuss how
the thresholds ? and ? are adjusted.
Lexical Similarity We measure lexical similar-
ity between two clusters through cosine similarity.
Specifically, each cluster is represented as a vec-
tor whose components correspond to the occurrence
frequencies of the argument head words in the clus-
ter. The similarity on such vectors x and y is then
quantified as:
lex(x,y) = cossim(x,y) =
x?y
?x??y?
(3)
Clause-Level Constraints Arguments occurring
in the same clause cannot bear the same role. There-
fore, clusters should not merge if the resulting clus-
ter contains (many) arguments of the same clause.
For two clusters c and c? we assess how well they
satisfy this clause-level constraint by computing:
cons(c,c?) = 1?
2? viol(c,c?)
NC +NC?
(4)
where viol(c,c?) refers to the number of pairs of in-
stances (d,d?) ? c? c? for which d and d? occur in
the same clause (each instance can participate in at
most one pair) and NC and NC? are the number of
instances in clusters c and c?, respectively.
Part-of-speech Similarity Part-of-speech similar-
ity is also measured through cosine-similarity (equa-
tion (3)). Clusters are again represented as vectors x
and y whose components correspond to argument
part-of-speech tags and values to their occurrence
frequency.
5.2.3 Threshold Adaptation and Termination
As mentioned earlier the thresholds ? and ? which
parametrize the scoring function are adjusted at each
iteration. The idea is to start with a very restrictive
setting (high values) in which the negative evidence
rules out merges more strictly, and then to gradually
relax the requirement for a merge by lowering the
threshold values. This procedure prioritizes reliable
merges over less reliable ones.
More concretely, our threshold adaptation pro-
cedure starts with ? and ? both set to value 0.95.
Then ? is lowered by 0.05 at each step, leaving ?
unchanged. When ? becomes zero, ? is lowered
by 0.05 and ? is reset to 0.95. Then ? is iteratively
decreased again until it becomes zero, after which ?
is decreased by another 0.05. This is repeated until ?
becomes zero, at which point the algorithm termi-
nates. Note that the termination criterion is not tied
explicitly to the number of clusters, which is there-
fore determined automatically.
6 Experimental Setup
In this section we describe how we assessed the per-
formance of our system. We discuss the dataset
on which our experiments were carried out, explain
how our system?s output was evaluated and present
the methods used for comparison with our approach.
Data For evaluation purposes, the system?s out-
put was compared against the CoNLL 2008 shared
task dataset (Surdeanu et al, 2008) which provides
1122
Syntactic Function Lang and Lapata Split-Merge
PU CO F1 PU CO F1 PU CO F1
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1
Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang and
Lapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function.
PropBank-style gold standard annotations. The
dataset was taken from the Wall Street Journal por-
tion of the Penn Treebank corpus and converted into
a dependency format (Surdeanu et al, 2008). In
addition to gold standard dependency parses, the
dataset alo contains automatic parses obtained from
the MaltParser (Nivre et al, 2007). Although the
dataset provides annotations for verbal and nominal
predicate-argument constructions, we only consid-
ered the former, following previous work on seman-
tic role labeling (Ma`rquez et al, 2008).
Evaluation Metrics For each verb, we determine
the extent to which argument instances in a cluster
share the same gold standard role (purity) and the
extent to which a particular gold standard role is as-
signed to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can then
be written as:
PU =
1
N ?i
max
j
|G j ?Ci| (5)
Collocation is defined as follows. For each gold role,
we determine the cluster with the largest number of
instances for that role (the role?s primary cluster)
and then compute the percentage of instances that
belong to the primary cluster for each gold role as:
CO =
1
N ?j
max
i
|G j ?Ci| (6)
The per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the micro-
average obtained by weighting the scores for indi-
vidual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
F1 =
2?CO?PU
CO+PU
(7)
Comparison Models We compared our split-
merge algorithm against two competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as point of com-
parison by other unsupervised semantic role label-
ing systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
Our implementation allocates up to N = 21 clus-
ters2 for each verb, one for each of the 20 most fre-
quent functions in the CoNLL dataset and a default
cluster for all other functions. The second compar-
ison model is the one proposed in Lang and Lapata
(2010) (see Section 2). We used the same model set-
tings (with 10 latent variables) and feature set pro-
posed in that paper. Our method?s only parameter is
the threshold ? which we heuristically set to 0.1. On
average our method induces 10 clusters per verb.
7 Results
Our results are summarized in Table 2. We re-
port cluster purity (PU), collocation (CO) and their
harmonic mean (F1) for the baseline (Syntactic
Function), Lang and Lapata?s (2010) model and
our split-merge algorithm (Split-Merge) on four
2This is the number of gold standard roles.
1123
Syntactic Function Split-Merge
Verb Freq PU CO F1 PU CO F1
say 15238 91.4 91.3 91.4 93.6 81.7 87.2
make 4250 68.6 71.9 70.2 73.3 72.9 73.1
go 2109 45.1 56.0 49.9 52.7 51.9 52.3
increase 1392 59.7 68.4 63.7 68.8 71.4 70.1
know 983 62.4 72.7 67.1 63.7 65.9 64.8
tell 911 61.9 76.8 68.6 77.5 70.8 74.0
consider 753 63.5 65.6 64.5 79.2 61.6 69.3
acquire 704 75.9 79.7 77.7 80.1 76.6 78.3
meet 574 76.7 76.0 76.3 88.0 69.7 77.8
send 506 69.6 63.8 66.6 83.6 65.8 73.6
open 482 63.1 73.4 67.9 77.6 62.2 69.1
break 246 53.7 58.9 56.2 68.7 53.3 60.0
Table 3: Clustering results for individual verbs with
our split-merge algorithm and the syntactic function
baseline.
datasets. These result from the combination of au-
tomatic parses with automatically identified argu-
ments (auto/auto), gold parses with automatic argu-
ments (gold/auto), automatic parses with gold argu-
ments (auto/gold) and gold parses with gold argu-
ments (gold/gold). Bold-face is used to highlight the
best performing system under each measure on each
dataset (e.g., auto/auto, gold/auto and so on).
On all datasets, our method achieves the highest
purity and outperforms both comparison models by
a wide margin which in turn leads to a considerable
increase in F1. On the auto/auto dataset the split-
merge algorithm results in 9% higher purity than the
baseline and increases F1 by 2.8%. Lang and Lap-
ata?s (2010) logistic classifier achieves higher collo-
cation but lags behind our method on the other two
measures.
Not unexpectedly, we observe an increase in per-
formance for all models when using gold standard
parses. On the gold/auto dataset, F1 increases
by 2.7% for the split-merge algorithm, 2.7% for the
logistic classifier, and 5.5% for the syntactic func-
tion baseline. Split-Merge maintains the highest pu-
rity and levels the baseline in terms of F1. Perfor-
mance also increases if gold standard arguments are
used instead of automatically identified arguments.
Consequently, each model attains its best scores on
the gold/gold dataset.
We also assessed the argument identification com-
Syntactic Function Split-Merge
Role PU CO F1 PU CO F1
A0 74.5 87.0 80.3 79.0 88.7 83.6
A1 82.3 72.0 76.8 87.1 73.0 79.4
A2 65.0 67.3 66.1 82.8 66.2 73.6
A3 48.7 76.7 59.6 79.6 76.3 77.9
ADV 37.2 77.3 50.2 78.8 37.3 50.6
CAU 81.8 74.4 77.9 84.8 67.2 75.0
DIR 62.7 67.9 65.2 71.0 50.7 59.1
EXT 51.4 87.4 64.7 90.4 87.2 88.8
LOC 71.5 74.6 73.0 82.6 56.7 67.3
MNR 62.6 58.8 60.6 81.5 44.1 57.2
TMP 80.5 74.0 77.1 80.1 38.7 52.2
MOD 68.2 44.4 53.8 90.4 89.6 90.0
NEG 38.2 98.5 55.0 49.6 98.8 66.1
DIS 42.5 87.5 57.2 62.2 75.4 68.2
Table 4: Clustering results for individual semantic
roles with our split-merge algorithm and the syntac-
tic function baseline.
ponent on its own (settings auto/auto and gold/auto).
It obtained a precision of 88.1% (percentage of se-
mantic arguments out of those identified) and recall
of 87.9% (percentage of identified arguments out of
all gold arguments). However, note that these fig-
ures are not strictly comparable to those reported
for supervised systems, due to the fact that our ar-
gument identification component only discards non-
argument candidates.
Tables 3 and 4 shows how performance varies
across verbs and roles, respectively. We compare the
syntactic function baseline and the split-merge sys-
tem on the auto/auto dataset. Table 3 presents results
for 12 verbs which we selected so as to exhibit var-
ied occurrence frequencies and alternation patterns.
As can be seen, the macroscopic result ? increase
in F1 (shown in bold face) and purity ? also holds
across verbs. Some caution is needed in interpret-
ing the results in Table 43 since core roles A0?A3
are defined on a per-verb basis and do not necessar-
ily have a uniform corpus-wide interpretation. Thus,
conflating scores across verbs is only meaningful to
the extent that these labels actually signify the same
3Results are shown for four core roles (A0?A3) and all sub-
types of the ArgM role, i.e., adjuncts denoting general purpose
(ADV), cause (CAU), direction (DIR), extent (EXT), location
(LOC), manner (MNR), and time (TMP), modal verbs (MOD),
negative markers (NEG), and discourse connectives (DIS).
1124
role (which is mostly true for A0 and A1). Further-
more, the purity scores given here represent the av-
erage purity of those clusters for which the specified
role is the majority role. We observe that for most
roles shown in Table 4 the split-merge algorithm im-
proves upon the baseline with regard to F1, whereas
this is uniformly the case for purity.
What are the practical implications of these re-
sults, especially when considering the collocation-
purity tradeoff? If we were to annotate the clus-
ters induced by our system, low collocation would
result in higher annotation effort while low purity
would result in poorer data quality. Our system im-
proves purity substantially over the baselines, with-
out affecting collocation in a way that would mas-
sively increase the annotation effort. As an exam-
ple, consider how our system could support humans
in labeling an unannotated corpus. (The following
numbers are derived from the CoNLL dataset4 in the
auto/auto setting.) We might decide to annotate all
induced clusters with more than 10 instances. This
means we would assign labels to 74% of instances in
the dataset (excluding those discarded during argu-
ment identification) and attain a role classification
with 79.4% precision (purity).5 However, instead
of labeling all 165,662 instances contained in these
clusters individually we would only have to assign
labels to 2,869 clusters. Since annotating a cluster
takes roughly the same time as annotating a single
instance, the annotation effort is reduced by a factor
of about 50.
8 Conclusions
In this paper we presented a novel approach to un-
supervised role induction which we formulated as a
clustering problem. We proposed a split-merge al-
gorithm that iteratively manipulates clusters repre-
senting semantic roles whilst trading off cluster pu-
rity with collocation. The split phase creates ?pure?
clusters that contain arguments of the same role
whereas the merge phase attempts to increase col-
location by merging clusters which are likely to rep-
resent the same role. The approach is simple, intu-
4Of course, it makes no sense to label this dataset as it is
already labeled.
5Purity here is slightly lower than the score reported in Ta-
ble 2 (auto/auto setting), because it is computed over a different
number of clusters (only those with at least 10 instances).
itive and requires no manual effort for training. Cou-
pled with a rule-based component for automatically
identifying argument candidates our split-merge al-
gorithm forms an end-to-end system that is capable
of inducing role labels without any supervision.
Our approach holds promise for reducing the data
acquisition bottleneck for supervised systems. It
could be usefully employed in two ways: (a) to cre-
ate preliminary annotations, thus supporting the ?an-
notate automatically, correct manually? methodol-
ogy used for example to provide high volume anno-
tation in the Penn Treebank project; and (b) in com-
bination with supervised methods, e.g., by providing
useful out-of-domain data for training. An important
direction for future work lies in investigating how
the approach generalizes across languages as well as
reducing our system?s reliance on a treebank-trained
parser.
Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).
Appendix
The relations in Rule (2) from Table 1 are IM??,
PRT?, COORD??, P??, OBJ?, PMOD?, ADV?,
SUB??, ROOT?, TMP?, SBJ?, OPRD?. The sym-
bols ? and ? denote the direction of the dependency
arc (upward and downward, respectively).
The relations in Rule (3) are ADV??, AMOD??,
APPO??, BNF??-, CONJ??, COORD??, DIR??,
DTV??-, EXT??, EXTR??, HMOD??, IOBJ??,
LGS??, LOC??, MNR??, NMOD??, OBJ??,
OPRD??, POSTHON??, PRD??, PRN??, PRP??,
PRT??, PUT??, SBJ??, SUB??, SUFFIX??. De-
pendency labels are abbreviated here. A detailed
description is given in Surdeanu et al (2008), in
their Table 4.
References
O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28?36, Singapore.
1125
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547?619.
H. Fu?rstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11?20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245?288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192?199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1?8, Sydney, Australia.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691?696. AAAI Press / The MIT Press.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939?
947, Los Angeles, California.
L. Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145?
159, June.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95?135.
S. Pado? and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307?340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289?310.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12?21,
Prague, Czech Republic.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8?15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159?177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95?102, Barcelona, Spain.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13?16, Boulder, Colorado.
1126
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625?630,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Scalable Probabilistic Classifier for Language Modeling
Joel Lang
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk
Abstract
We present a novel probabilistic classifier,
which scales well to problems that involve a
large number of classes and require training on
large datasets. A prominent example of such a
problem is language modeling. Our classifier
is based on the assumption that each feature
is associated with a predictive strength, which
quantifies how well the feature can predict the
class by itself. The predictions of individual
features can then be combined according to
their predictive strength, resulting in a model,
whose parameters can be reliably and effi-
ciently estimated. We show that a generative
language model based on our classifier consis-
tently matches modified Kneser-Ney smooth-
ing and can outperform it if sufficiently rich
features are incorporated.
1 Introduction
A Language Model (LM) is an important compo-
nent within many natural language applications in-
cluding speech recognition and machine translation.
The task of a generative LM is to assign a probabil-
ity p(w) to a sequence of words w = w1 . . . wL. It
is common to factorize this probability as
p(w) =
L?
i=1
p(wi|wi?N+1 . . . wi?1) (1)
Thus, the central problem that arises from this
formulation consists of estimating the probability
p(wi|wi?N+1 . . . wi?1). This can be viewed as a
classification problem in which the target word Wi
corresponds to the class that must be predicted,
based on features extracted from the conditioning
context, e.g. a word occurring in the context.
This paper describes a novel approach for mod-
eling such conditional probabilities. We propose a
classifier which is based on the assumption that each
feature has a predictive strength, quantifying how
well the feature can predict the class (target word)
by itself. Then the predictions made by individual
features can be combined into a mixture model, in
which the prediction of each feature is weighted ac-
cording to its predictive strength. This reflects the
fact that certain features (e.g. certain context words)
are much more predictive than others but the pre-
dictive strength for a particular feature often doesn?t
vary much across classes and can thus be assumed
constant. The main advantage of our model is that it
is straightforward to incorporate rich features with-
out sacrificing scalability or reliability of parame-
ter estimation. In addition, it is simple to imple-
ment and no feature selection is required. Section 3
shows that a generative1 LM built with our classi-
fier is competitive to modified Kneser-Ney smooth-
ing and can outperform it if sufficiently rich features
are incorporated.
The classification-based approach to language
modeling was introduced by Rosenfeld (1996) who
proposed an optimized variant of the maximum-
entropy classifier (Berger et al, 1996) for the task.
Unfortunately, data sparsity resulting from the large
number of classes makes it difficult to obtain reli-
able parameter estimates, even on large datasets and
the high computational costs make it difficult train
models on large datasets in the first place2. Scal-
1While the classifier itself is discriminative, i.e. condition-
ing on the contextual features, the resulting LM is generative.
See Roark et al (2007) for work on discriminative LMs.
2For example, using a vocabulary of 20000 words Rosen-
feld (1994) trained his model on up to 40M words, however
employing heavy feature pruning and indicating that ?the com-
putational load, was quite severe for a system this size?.
625
ability is however very important, since moving to
larger datasets is often the simplest way to obtain
a better model. Similarly, neural probabilistic LMs
(Bengio et al, 2003) don?t scale very well to large
datasets. Even the more scalable variant proposed
by Mnih and Hinton (2008) is trained on a dataset
consisting of only 14M words, also using a vocabu-
lary of around 20000 words. Van den Bosch (2005)
proposes a decision-tree classifier which has been
applied to training datasets with more than 100M
words. However, his model is non-probabilistic and
thus a standard comparison with probabilistic mod-
els in terms of perplexity isn?t possible.
N-Gram models (Goodman, 2001) obtain esti-
mates for p(wi|wi?N+1 . . . wi?1) using counts of
N-Grams. Because directly using the maximum-
likelihood estimate would result in poor predictions,
smoothing techniques are applied. A modified inter-
polated form of Kneser-Ney smoothing (Kneser and
Ney, 1995) was shown to consistently outperform
a variety of other smoothing techniques (Chen and
Goodman, 1999) and currently constitutes a state-
of-the-art3 generative LM.
2 Model
We are concerned with estimating a probability dis-
tribution p(Y |x) over a categorical class variable
Y with range Y , conditional on a feature vector
x = (x1, . . . , xM ), containing the feature values xi
of M features. While generalizations are conceiv-
able, we will restrict the features Xk to be binary,
i.e. xk ? {0, 1}. For language modeling the class
variable Y corresponds to the target word Wi which
is to be predicted and thus ranges over all possible
words of some vocabulary. The binary input fea-
tures x are extracted from the conditioning context
wi?N+1 . . . wi?1. The specific features we use for
language modeling are given in Section 3.
We assume sparse features, such that typically
only a small number of the binary features take value
1. These features are referred to as the active fea-
tures and predictions are based on them. We in-
troduce a bias feature which is active for every in-
stance, in order to ensure that the set of active fea-
tures is non-empty for each instance. Individually,
each active feature Xk is predictive of the class vari-
able and predicts the class through a categorical dis-
3The model of Wood et al (2009) has somewhat higher per-
formance, however, again due to high computational costs the
model has only been trained on training sets of at most 14M
words.
tribution4 distribution, which we denote as p(Y |xk).
Since instances typically have several active features
the question is how to combine the individual pre-
dictions of these features into an overall prediction.
To this end we make the assumption that each fea-
ture Xk has a certain predictive strength ?k ? R,
where larger values indicate that the feature is more
likely to predict correctly. The individual predic-
tions can then be combined into a mixture model,
which weights individual predictions according to
their predictive strength:
p(Y |x, ?) =
?
k?A(x)
vk(x)p(Y |xk) (2)
where
vk(x) =
e?k
?
k?A(x)
e?k
(3)
Here A(x) denotes the index-set of active features
for instance (y, x). Note that since the set of active
features varies across instances, so do the mixing
proportions vk(x) and thus this is not a conventional
mixture model, but rather a variable one. We will
therefore refer to our model as the variable mixture
model (VMM). In particular, our model differs from
linear or log-linear interpolation models (Klakow,
1998), which combine a typically small number of
components that are common across instances.
In order to compare our model to the maximum-
entropy classifier and other (generalized) linear
models, it is beneficial to rewrite Equation 2 as
p(Y = y|x, ?) =
1
Q(x)
M?
k=1
|Y|?
j=1
?j,k(y, x)?j,k (4)
=
1
Q(x)
?>?(y, x) (5)
where ?j,k(y, x) is a sufficient statistics indicating
whether feature Xk is active and class y = yj and
?j,k = e
?k+log p(yj |xk) (6)
Q(x) =
?
k?A(x)
e?k (7)
Table 1 shows the main differences between the
VMM, the maximum-entropy classifier and the per-
ceptron (Collins, 2002).
4commonly referred to as a multinomial distribution
626
VMM Maximum Entropy Perceptron
p(y|x, ?) = 1Q(x)?
>?(y, x) p(y|x, ?) = 1Q(x)e
?>?(y,x) score(y|x, ?) = ?>?(y, x)
Q(x) =
?
k?A(x) e
?k Q(x) =
?|Y|
j=1 e
?>?(yj ,x)
Table 1: A comparison between the VMM, the maximum-entropy classifier and the perceptron. Like the perceptron
and in contrast to the maximum-entropy classifier, the VMM directly uses a predictor ?>?(y, x). For the VMM the
sufficient statistics ?(y, x) correspond to binary indicator variables and the parameters ? are constrained according to
Equation 6. This results in a partition function Q(x) which can be efficiently computed, in contrast to the partition
function of the maximum-entropy classifier, which requires a summation over all classes.
2.1 Parameter Estimation
The VMM has two types of parameters:
1. the categorical parameters ?j,k = p(yj |xk)
which determine the likelihood of class yj in
presence of feature Xk;
2. the parameters ?k quantifying the predictive
strength of each feature Xk.
The two types of parameters are estimated from a
training dataset, consisting of instances (y(h), x(h)).
Parameter estimation proceeds in two separate
stages, resulting in a simple and efficient procedure.
In a first stage, the categorical parameters are com-
puted independently for each feature, as the maxi-
mum likelihood estimates, smoothed using absolute
discounting (Chen and Rosenfeld, 2000):
?j,k = p(yj |xk) =
c?j,k
ck
where c?j,k is the smoothed count of how many times
Y takes value yj when Xk is active, and ck is
the count of how many times Xk is active. The
smoothed count is computed as
c?j,k =
{
cj,k ?D if cj,k > 0
D?NZk
Zk
if cj,k = 0
where cj,k is the raw count for class yj and fea-
ture Xk, NZk is the number of classes for which
the raw count is non-zero, and Zk is the number of
classes for which the raw count is zero. D is the
discount constant chosen in [0, 1]. The smoothing
thus subtracts D from each non-zero count and re-
distributes the so-obtained mass evenly amongst all
zero counts. If all counts are non-zero no mass is
redistributed.
Once the categorical parameters have been com-
puted, we proceed by estimating the predictive
strengths ? = (?1, . . . , ?M ). We can do so by con-
ducting a search for the parameter vector ?? which
maximizes the log-likelihood of the training data:
?? = argmax
?
ll(?)
= argmax
?
?
h
log p(y(h)|x(h), ?)
While any standard optimization method could
be applied, we use stochastic gradient ascent (SGA,
Bottou (2004)) as this results in a particularly conve-
nient and efficient procedure that requires only one
iteration over the data (see Section 3). SGA is an
online optimization method which iteratively com-
putes the gradient ? for each instance and takes a
step of size ? in the direction of that gradient:
?(t+1) ? ?(t) + ?? (8)
The gradient ? = (?ll
(h)
??1
, . . . , ?ll
(h)
??M
) computed for
SGA contains the first-order derivatives of the data
log-likelihood of a particular instance with respect
to the ?-parameters which are given by
?
??k
log p(y|x, ?) =
vk(x)
p(y|x, ?)
[p(y|xk)? p(y|x, ?)]
(9)
The resulting parameter-update Equation 8 has
the following intuitive interpretation. If the predic-
tion of a particular active feature Xk is higher than
the current overall prediction, the term in square
brackets in Equation 9 becomes positive and thus
the predictive strength ?k for that feature is increased
and conversely for the case where the prediction is
below the overall prediction. The magnitude of the
627
Type Extracted Features
Standard N-Grams (BA,SR,LR)
* * * (bias)
Mr Thompson said
* Thompson said
* * said
Skip N-Grams (SR,LR)
Mr * said
Mr Thompson *
Mr * *
* Thompson *
Unigram Bag Features (SR,LR)
Mr
Thompson
said
Long-Range Unigram Bag Features (LR)
Yesterday
at
the
press
conference
Table 2: Feature types and examples for a model of order
N=4 and for the context Yesterday at the press
conference Mr Thompson said. For each fea-
ture type we write in parentheses the feature sets which
include that type of feature. The wildcard symbol * is
used as a placeholder for arbitrary regular words. The
bias feature, which is active for each instance is written
as * * *. In standard N-Gram models the bias feature
corresponds to the unigram distribution.
update depends on how much overall and feature
prediction differ and on the scaling factor vk(x)p(y|x,?) .
In order to improve generalization, we estimate
the categorical parameters based on the counts from
all instances, except the one whose gradient is being
computed for the online update (leave-one-out). In
other words, we subtract the counts for a particular
instance before computing the update (Equation 8)
and add them back when the update has been ex-
ecuted. In total, training only requires two passes
over the data, as opposed to a single pass (plus
smoothing) required by N-Gram models.
3 Experiments
All experiments were conducted using the SRI Lan-
guage Modeling Toolkit (SRILM, Stolcke (2002)),
i.e. we implemented5 the VMM within SRILM and
compared to default N-Gram models supplied with
SRILM. The experiments were run on a 64-bit, 2.2
GHz dual-core machine with 8GB RAM.
Data The experiments were carried out on data
from the Reuters Corpus Version 1 (Lewis et al,
5The code can be downloaded from http://code.
google.com/p/variable-mixture-model .
2004), which was split into sentences, tokenized and
converted to lower case, not removing punctuation.
All our models were built with the same 30367-
word vocabulary, which includes the sentence-end
symbol and a special symbol for out-of-vocabulary
words (UNK). The vocabulary was compiled by se-
lecting all words which occur more than four times
in the data of week 31, which was not otherwise
used for training or testing. As development set we
used the articles of week 50 (4.1M words) and as
test set the articles of week 51 (3.8M words). For
training we used datasets of four different sizes: D1
(week 1, 3.1M words), D2 (weeks 1-3, 10M words),
D3 (weeks 1-10, 37M words) and D4 (weeks 1-30,
113M words).
Features We use three different feature sets in our
experiments. The first feature set (basic, BA) con-
sists of all features also used in standard N-Gram
models, i.e. all subsequences up to a length N ? 1
immediately preceding the target word. The sec-
ond feature set (short-range, SR) consists of all ba-
sic features as well as all skip N-Grams (Ney et al,
1994) that can be formed with theN ?1 length con-
text. Moreover, all words occurring in the context
are included as bag features, i.e. as features which
indicate the occurrence of a word but not the partic-
ular position. The third feature set (long-range, LR)
is an extension of SR which also includes longer-
distance features. Specifically, this feature set ad-
ditionally includes all unigram bag features up to a
distance d = 9. The feature types and examples of
extracted features are given in Table 2.
Model Comparison We compared the VMM to
modified Kneser-Ney (KN, see Section 1). The or-
der of a VMM is defined through the length of the
context from which the basic and short-range fea-
tures are extracted. In particular, VM-BA of a cer-
tain order uses the same features as the N-Gram
models of the same order and VM-SR uses the same
conditioning context as the N-Gram models of the
same order. VM-LR in addition contains longer-
distance features, beyond the order of the corre-
sponding N-Gram models. The order of the models
was varied between N = 2 . . . 5, however, for the
larger two datasets D3 and D4 the order 5 models
would not fit into the available RAM which is why
for order 5 we can only report scores for D1 and D2.
We could resort to pruning, but since this would have
an effect on performance it would invalidate a direct
comparison, which we want to avoid.
628
D1 D2 D3 D4
Model N 3.1M 10M 37M 113M
KN
2 209.2 178.2 155.3 139.3
3 164.9 127.7 98.9 78.1
4 160.9 122.2 91.4 68.4
5 164.5 124.6 ? ?
VM-BA
2 217.9 209.8 162.8 144.7
3 174.1 159.7 114.3 87.3
4 164.9 147.7 102.7 78.2
5 163.2 144.2 ? ?
VM-SR
2 215.1 210.1 161.9 144.4
3 180.1 137.3 112.7 84.6
4 157.8 117.7 94.8 68.8
5 147.8 109.7 ? ?
VM-LR
2 207.5 170.8 147.4 128.2
3 160.6 124.7 103.2 79.3
4 146.7 112.1 89.8 66.0
5 141.4 107.1 ? ?
Table 3: The test set perplexities of the models for orders
N=2..5 on training datasets D1-D4.
Model Parametrization We used the develop-
ment set to determine the values for the absolute dis-
counting parameter D (defined in Section 2.1) and
the number of iterations for stochastic gradient as-
cent. This resulted in a value D = 0.1. Stochas-
tic gradient yields best results with a single pass
through all instances. More iterations result in over-
fitting, i.e. decrease training data log-likelihood but
increase the log-likelihood on the development data.
The step size was kept fixed at ? = 1.0.
Results The results of our experiments are given
in Table 3, which shows that for sufficiently high
orders VM-SR matches KN on each dataset. As ex-
pected, the VMM?s strength partly stems from the
fact that compared to KN it makes better use of
the information contained in the conditioning con-
text, as indicated by the fact that VM-SR matches
KN whereas VM-BA doesn?t. At orders 4 and 5,
VM-LR outperforms KN on all datasets, bringing
improvements of around 10% for the two smaller
training datasets D1 and D2. Comparing VM-BA
and VM-SR at order 4 we see that the 7 additional
features used by VM-SR for every instance signifi-
cantly improve performance and the long-range fea-
tures further improve performance. Thus richer fea-
ture sets consistently lead to higher model accuracy.
Similarly, the performance of the VMM improves as
one moves to higher orders, thereby increasing the
amount of contextual information. For orders 2 and
3 VM-SR is inferior to KN, because the SR feature
set at order 2 contains no additional features over
KN and at order 3 it only contains one additional
feature per instance. At order 4 VM-SR matches
KN and, while KN gets worse at order 5, the VMM
improves and outperforms KN by around 14%.
The training time (including disk IO) of the or-
der 4 VM-SR on the largest dataset D4 is about 30
minutes, whereas KN takes about 6 minutes to train.
4 Conclusions
The main contribution of this paper consists of a
novel probabilistic classifier, the VMM, which is
based on the idea of combining predictions made by
individual features into a mixture model whose com-
ponents vary from instance to instance and whose
mixing proportions reflect the predictive strength of
each component. The main advantage of the VMM
is that it is straightforward to incorporate rich fea-
tures without sacrificing scalability or reliability of
parameter estimation. Moreover, the VMM is sim-
ple to implement and works ?out-of-the-box? with-
out feature selection, or any special tuning or tweak-
ing.
Applied to language modeling, the VMM re-
sults in a state-of-the-art generative language model
whose relative performance compared to N-Gram
models gets better as one incorporates richer fea-
ture sets. It scales almost as well to large datasets
as standard N-Gram models: training requires only
two passes over the data as opposed to a single pass
required by N-Gram models. Thus, the experiments
provide empirical evidence that the VMM is based
on a reasonable set of modeling assumptions, which
translate into an accurate and scalable model.
Future work includes further evaluation of the
VMM, e.g. as a language model within a speech
recognition or machine translation system. More-
over, optimizing memory usage, for example via
feature pruning or randomized algorithms, would al-
low incorporation of richer feature sets and would
likely lead to further improvements, as indicated by
the experiments in this paper. We also intend to eval-
uate the performance of the VMM on other lexical
prediction tasks and more generally, on other classi-
fication tasks with similar characteristics.
Acknowledgments I would like to thank
Mirella Lapata and Charles Sutton for their
feedback on this work and Abby Levenberg for the
preprocessed datasets.
629
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research, 3:1137?1155.
A. Berger, V. Della Pietra, and S. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39?71.
L. Bottou. 2004. Stochastic Learning. In Advanced
Lectures on Machine Learning, Lecture Notes in Ar-
tificial Intelligence, pages 146?168. Springer Verlag,
Berlin/Heidelberg.
S. Chen and J. Goodman. 1999. An Empirical Study of
Smoothing Techniques for Language Modeling. Com-
puter Speech and Language, 13:359?394.
S. Chen and R. Rosenfeld. 2000. A Survey of Smooth-
ing Techniques for ME Models. IEEE Transactions on
Speech and Audio Processing, 8(1):37?50.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?8, Philadelphia, PA, USA.
J. Goodman. 2001. A Bit of Progress in Language Mod-
eling (Extended Version). Technical report, Microsoft
Research, Redmond, WA, USA.
D. Klakow. 1998. Log-Linear Interpolation of Language
Models. In Proceedings of the 5th International Con-
ference on Spoken Language Processing, pages 1694?
1698, Sydney, Australia.
R. Kneser and H. Ney. 1995. Improved Backing-off
for M-Gram Language Modeling. In Proceedings of
the International Conference on Acoustics, Speech and
Signal Processing, pages 181?184, Detroit, MI, USA.
D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research,
5:361?397.
A. Mnih and G. Hinton. 2008. A Scalable Hierarchical
Distributed Language Model. In Advances in Neural
Information Processing Systems 21.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modeling. Computer, Speech and Language, 8:1?38.
B. Roark, M. Saraclar, and M. Collins. 2007. Discrimi-
native n-gram Language Modeling. Computer, Speech
and Language, 21:373?392.
R. Rosenfeld. 1994. Adaptive Statistical Language Mod-
elling: A Maximum Entropy Approach. Ph.D. thesis,
Carnegie Mellon University.
R. Rosenfeld. 1996. A Maximum Entropy Approach to
Adaptive Statistical Language Modeling. Computer,
Speech and Language, 10:187?228.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO, USA.
A. Van den Bosch. 2005. Scalable Classification-based
Word Prediction and Confusible Correction. Traite-
ment Automatique des Langues, 42(2):39?63.
F. Wood, C. Archambeau, J. Gasthaus, L. James, and
Y. Teh. 2009. A Stochastic Memoizer for Sequence
Data. In Proceedings of the 24th International Con-
ference on Machine learning, pages 1129?1136, Mon-
treal, Quebec, Canada.
630
