Efficient Deep Processing of Japanese 
 
Melanie SIEGEL 
DFKI GmbH 
Stuhlsatzenhausweg 3 
66123 Saarbr?cken, Germany 
siegel@dfki.de 
Emily M. BENDER 
CSLI Stanford 
220 Panama Street 
Stanford, CA, 94305-4115, USA  
bender@csli.stanford.edu  
 
Abstract  
We present a broad coverage Japanese 
grammar written in the HPSG formalism 
with MRS semantics. The grammar is 
created for use in real world applications, 
such that robustness and performance issues 
play an important role. It is connected to a 
POS tagging and word segmentation tool. 
This grammar is being developed in a 
multilingual context, requiring MRS 
structures that are easily comparable across 
languages. 
Introduction 
Natural language processing technology has 
recently reached a point where applications that 
rely on deep linguistic processing are becoming 
feasible.  Such applications (e.g. message 
extraction systems, machine translation and 
dialogue understanding systems) require natural 
language understanding, or at least an 
approximation thereof.  This, in turn, requires 
rich and highly precise information as the output 
of a parse.  However, if the technology is to 
meet the demands of real-world applications, 
this must not come at the cost of robustness.  
Robustness requires not only wide coverage by 
the grammar (in both syntax and semantics), but 
also large and extensible lexica as well as 
interfaces to preprocessing systems for named 
entity recognition, non-linguistic structures such 
as addresses, etc.  Furthermore, applications 
built on deep NLP technology should be 
extensible to multiple languages.  This requires 
flexible yet well-defined output structures that 
can be adapted to grammars of many different 
languages.  Finally, for use in real-world 
applications, NLP systems meeting the above 
desiderata must also be efficient. 
In this paper, we describe the development of 
a broad coverage grammar for Japanese that is 
used in an automatic email response application. 
The grammar is based on work done in the 
Verbmobil project (Siegel 2000) on machine 
translation of spoken dialogues in the domain of 
travel planning. It has since been greatly 
extended to accommodate written Japanese and 
new domains. 
The grammar is couched in the theoretical 
framework of Head-Driven Phrase Structure 
Grammar (HPSG) (Pollard & Sag 1994), with 
semantic representations in Minimal Recursion 
Semantics (MRS) (Copestake et al 2001).  
HPSG is well suited to the task of multilingual 
development of broad coverage grammars: It is 
flexible enough (analyses can be shared across 
languages but also tailored as necessary), and 
has a rich theoretical literature from which to 
draw analyzes and inspiration.  The 
characteristic type hierarchy of HPSG also 
facilitates the development of grammars that are 
easy to extend.  MRS is a flat semantic 
formalism that works well with typed feature 
structures and is flexible in that it provides 
structures that are under-specified for scopal 
information. These structures give compact 
representations of ambiguities that are often 
irrelevant to the task at hand.    
HPSG and MRS have the further advantage 
that there are practical and useful open-source 
tools for writing, testing, and efficiently 
processing grammars written in these 
formalisms. The tools we are using in this 
project include the LKB system (Copestake 
2002) for grammar development, [incr tsdb()] 
(Oepen & Carroll 2000) for testing the grammar 
and tracking changes, and PET (Callmeier 
2000), a very efficient HPSG parser, for 
processing.  We also use the ChaSen tokenizer 
and POS tagger (Asahara & Matsumoto 2000).  
While couched within the same general 
framework (HPSG), our approach differs from 
that of Kanayama et al(2000).  The work 
described there achieves impressive coverage 
(83.7% on the EDR corpus of newspaper text) 
with an underspecified grammar consisting of a 
small number of lexical entries, lexical types 
associated with parts of speech, and six 
underspecified grammar rules.  In contrast, our 
grammar is much larger in terms of the number 
of lexical entries, the number of grammar rules, 
and the constraints on both,1 and takes 
correspondingly more effort to bring up to that 
level of coverage.  The higher level of detail 
allows us to output precise semantic 
representations as well as to use syntactic, 
semantic and lexical information to reduce 
ambiguity and rank parses. 
1 Japanese HPSG Syntax 
The fundamental notion of an HPSG is the sign. 
A sign is a complex feature structure 
representing information of different linguistic 
levels of a phrase or lexical item. The attribute-
value matrix of a sign in the Japanese HPSG is 
quite similar to a sign in the LinGO English 
Resource Grammar (henceforth ERG) 
(Flickinger 2000), with information about the 
orthographical realization of the lexical sign in 
PHON, syntactic and semantic information in 
SYNSEM, information about the lexical status in 
LEX, nonlocal information in NONLOC, head 
information that goes up the tree in HEAD and 
information about subcategorization in SUBCAT.  
The grammar implementation is based on a 
system of types. There are 900 lexical types that 
define the syntactic, semantic and pragmatic 
properties of the Japanese words, and 188 types 
that define the properties of phrases and lexical 
rules. The grammar includes 50 lexical rules for 
inflectional and derivational morphology and 47 
phrase structure rules. The lexicon contains 5100 
stem entries. As the grammar is developed for 
use in applications, it treats a wide range of 
                                                     
1 We do also make use of generic lexical entries for 
certain parts of speech as a means of extending our 
lexicon.  See section 3 below. 
basic constructions of Japanese.  Only some of 
these phenomena can be described here. 
1.1 Subcategorization 
The structure of SUBCAT is different from the 
ERG SUBCAT structure. This is due to 
differences in subcategorization between 
Japanese and English. A fundamental difference 
is the fact that, in Japanese, verbal arguments are 
frequently omitted. For example, arguments that 
refer to the speaker, addressee, and other 
arguments that can be inferred from context are 
often omitted in spoken language. Additionally, 
optional verbal arguments can scramble. On the 
other hand, some arguments are not only 
obligatory, but must also be realized adjacent to 
the selecting head. 
To account for this, our subcategorization 
contains the attributes SAT and VAL. The SAT 
value encodes whether a verbal argument is 
already saturated (such that it cannot be 
saturated again), optional or adjacent. VAL 
contains the agreement information for the 
argument. When an argument is realized, its 
SAT value on the mother node is specified as sat 
and its SYNSEM is unified with its VAL value on 
the subcategorizing head. The VAL value on the 
mother is none. Adjacency must be checked in 
every rule that combines heads and arguments or 
adjuncts. This is the principle of adjacency, 
stated as follows: 
In a headed phrase, the SUBCAT.SAT value 
on the non-head daughter must not contain 
any adjacent arguments. In a head-
complement structure, the SUBCAT.SAT 
value of the head daughter must not contain 
any adjacent arguments besides the non-
head daughter. In a head-adjunct structure, 
the SUBCAT.SAT value of the head daughter 
must not contain any adjacent arguments. 
1.2 Verbal inflection 
Japanese verb stems combine with endings that 
provide information about honorification, tense, 
aspect, voice and mode. Inflectional rules for the 
different types of stems prepare the verb stems 
for combination with the verbal endings. For 
example, the verb stem yomu must be inflected 
to yon to combine with the past tense ending da. 
Morphological features constrain the 
combination of stem and ending. In the above 
example, the inflectional rule changes the mu 
character to the n character and assigns the value 
nd-morph to the morphological feature 
RMORPH-BIND-TYPE. The ending da selects 
for a verbal stem with this value.  
Endings can be combined with other endings, 
as in -sase-rare-mashi-ta (causative-potential-
honorific-past), but not arbitrarily: 
 *-sase-mashi-rare-ta 
 *-sase-ta-mashi-rare 
 -sase-ta 
 -rare-mashi-ta 
This is accounted for with two kinds of rules 
which realize mutually selected elements. In the 
combination of stem and ending, the verb stem 
selects for the verbal ending via the head feature 
SPEC. In the case of the combination of two 
verbal endings, the first ending selects for the 
second one via the head feature MARK. In both 
cases, the right element subcategorizes for the 
left one via SUBCAT.VAL.SPR. Using this 
mechanism, it is possible to control the sequence 
of verbal endings: Verb stems select verbal 
endings via SPEC and take no SPR, derivational 
morphemes (like causative or potential) select 
tense endings or other derivational morphemes 
via MARK and subcategorize for verb stems 
and/or verb endings via SPR (sase takes only 
verb stems), and tense endings take verb stems 
or endings as SPR and take no MARK or SPEC 
(as they occur at the end of the sequence).  
1.3 Complex Predicates 
A special treatment is needed for Japanese 
verbal noun + light verb constructions. In these 
cases, a word that combines the qualities of a 
noun with those of a verb occurs in a 
construction with a verb that has only marginal 
semantic information. The syntactic, semantic 
and pragmatic information on the complex is a 
combination of the information of the two.  
Consider example 1. The verbal noun 
benkyou contains subcategorization information 
(transitive), as well as semantic information (the 
benkyou-relation and its semantic arguments). 
The light verb shi-ta supplies tense information 
(past). Pragmatic information can be supplied by 
both parts of the construction, as in the formal 
form o-benkyou shi-mashi-ta. The rule that 
licenses this type of combination is the vn-light-
rule, a subtype of the head-marker-rule. 
Example 1: 
Benkyou shi-ta. 
study do-past 
'Someone has studied.' 
Japanese auxiliaries combine with verbs and 
provide either aspectual or perspective 
information or information about honorification. 
In a verb-auxiliary construction, the information 
about subcategorization is a combination of the 
SUBCAT information of verb and auxiliary, 
depending on the type of auxiliary. The rule 
responsible for the information combination in 
these cases is the head-specifier-rule. We have 
three basic types of auxiliaries.  The first type is 
aspect auxiliaries.  These are treated as raising 
verbs, and include such elements as iru (roughly, 
progressive) and aru (roughly, perfective), as 
can be seen in example 2.  The other two classes 
of auxiliaries provide information about 
perspective or the point of view from which a 
situation is being described.  Both classes of 
auxiliaries add a ni (dative) marked argument to 
the argument structure of the whole predicate.  
The classes differ in how they relate their 
arguments to the arguments of the verb.  One 
class (including kureru 'give'; see example 3) are 
treated as subject control verbs.  The other class 
(including morau 'receive', see example 4) 
establishes a control relation between the ni-
marked argument and the embedded subject. 
Example 2: 
Keeki wo tabe-te iru. 
cake ACC eat progressive 
'Someone is eating cake.' 
Example 3: 
Sensei wa watashi ni hon wo  
teacher TOP  I DAT book ACC  
katte kure-ta. 
buy give-past 
'The teacher bought me a book.' 
Example 4: 
Watashi  ga sensei ni hon wo 
I  NOM teacher DAT book ACC
katte morat-ta. 
buy get-past 
'The teacher bought me a book.' 
1.4 Particles in a type hierarchy 
The careful treatment of Japanese particles is 
essential, because they are the most frequently 
occurring words and have various central 
functions in the grammar. It is difficult, because 
one particle can fulfill more than one function 
and they can co-occur, but not arbitrarily. The 
Japanese grammar thus contains a type hierarchy 
of 44 types for particles. See Siegel (1999) for a 
more detailed description of relevant phenomena 
and solutions. 
1.5 Numeral Expressions 
Number names, such as sen kyuu hyaku juu 
'1910' constitute a notable exception to the 
general head-final pattern of Japanese phrases. 
We found Smith's (1999) head-medial analysis 
of English number names to be directly 
applicable to the Japanese system as well 
(Bender 2002).  This analysis was easily 
incorporated into the grammar, despite the 
oddity of head positioning, because the type 
hierarchy of HPSG is well suited to express the 
partial generalizations that permeate natural 
language. 
On the other hand, number names in 
Japanese contrast sharply with number names in 
English in that they are rarely used without a 
numeral classifier.  
Example 5:  
Juu *(hiki no) neko ga ki-ta. 
ten   CL GEN cat NOM arrive-past 
'Ten cats arrived.' 
The grammar provides for 'true' numeral 
classifiers like hon, ko, and hiki, as well as 
formatives like en 'yen' and do 'degree' which 
combine with number names just like numeral 
classifiers do, but never serve as numeral 
classifiers for other nouns.  In addition, there are 
a few non-branching rules that allow bare 
number names to surface as numeral classifier 
phrases with specific semantic constraints.  
1.6 Pragmatic information 
Spoken language and email correspondence both 
encode references to the social relation of the 
dialogue partners. Utterances can express social 
distance between addressee and speaker and 
third persons. Honorifics can even express 
respect towards inanimates. Pragmatic 
information is treated in the CONTEXT layer of 
the complex signs. Honorific information is 
given in the CONTEXT.BACKGROUND and 
linked to addressee and speaker anchors.  
The expression of empathy or in-group vs. 
out-group is quite prevalent in Japanese. One 
means of expressing empathy is the perspective 
auxiliaries discussed above. For example, two 
auxiliaries meaning roughly 'give' (ageru and 
kureru) contrast in where they place the 
empathy.  In the case of ageru, it is with the 
giver.  In the case of kureru, it is with the 
recipient. We model this within the sign by 
positing a feature EMPATHY within CONTEXT 
and linking it to the relevant arguments' indices. 
2 Japanese MRS Semantics  
In the multilingual context in which this 
grammar has been developed, a high premium is 
placed on parallel and consistent semantic 
representations between grammars for different 
languages.  Ensuring this parallelism enables the 
reuse of the same downstream technology, no 
matter which language is used as input.  
Integrating MRS representations parallel to 
those used in the ERG into the Japanese 
grammar took approximately 3 months.  Of 
course, semantic work is on-going, as every new 
construction treated needs to be given a suitable 
semantic representation.  For the most part, 
semantic representations developed for English 
were straightforwardly applicable to Japanese.  
This section provides a brief overview of those 
cases where the Japanese constructions we 
encountered led to innovations in the semantic 
representations and/or the correspondence 
between syntactic and semantic structures.  Due 
to space limitations, we discuss these analyses in 
general terms and omit technical details. 
2.l Nominalization and Verbal Nouns 
Nominalization is of course attested in English 
and across languages.  However, it is much more 
prevalent in Japanese than in English, primarily 
because of verbal nouns.  As noted in Section 
1.3 above, a verbal noun like benkyou 'study' can 
appear in syntactic contexts requiring nouns, or, 
in combination with a light verb, in contexts 
requiring verbs.  One possible analysis would 
provide two separate lexical entries, one with 
nominal and one with verbal semantics.  
However, this would not only be redundant 
(missing the systematic relationship between 
these uses of verbal nouns) but would also 
contradict the intuition that even in its nominal 
use, the arguments of benkyou are still present. 
Example 6: 
Nihongo no benkyou wo hajimeru. 
Japanese GEN study ACC begin 
'Someone begins the study of Japanese.' 
In order to capture this intuition, we opted for an 
analysis that essentially treats verbal nouns as 
underlyingly verbal.  The nominal uses are 
produced by a lexical rule which nominalizes the 
verbal nouns.  The semantic effect of this rule is 
to provide a nominal relation which introduces a 
variable which can in turn be bound by 
quantifiers. The nominal relation subordinates 
the original verbal relation supplied by the 
verbal noun.  The rule is lexical as we have not 
yet found any cases where the verb's arguments 
are clearly filled by phrases in the syntax.  If 
they do appear, it is with genitive marking (e.g., 
nihongo no in the example above).  In order to 
reduce ambiguity, we leave the relationship 
between these genitive marked NPs and the 
nominalized verbal noun underspecified.  There 
is nothing in the syntax to disambiguate these 
cases, and we find that they are better left to 
downstream processing, where there may be 
access to world knowledge. 
2.2 Numeral Classifiers 
As noted in Section1.5, the internal syntax of 
number names is surprisingly parallel between 
English and Japanese, but their external syntax 
differs dramatically. English number names can 
appear directly as modifiers of NPs and are 
treated semantically as adjectives in the ERG.  
Japanese number names can only modify nouns 
in combination with numeral classifiers. In 
addition, numeral classifier phrases can appear 
in NP positions (akin to partitives in English). 
Finally, some numeral-classifier-like elements 
do not serve the modifier function but can only 
head phrases that fill NP positions.   
This constellation of facts required the 
following innovations: a representation of 
numbers that doesn't treat them as adjectives (in 
MRS terms, a feature structure without the ARG 
feature), a representation of the semantic 
contribution of numeral classifiers (a relation 
between numbers and the nouns they modify, 
this time with an ARG feature), and a set of 
rules for promoting numeral classifier phrases to 
NPs that contribute the appropriate nominal 
semantics (underspecified in the case of ordinary 
numeral classifiers or specific in the case of 
words like en 'yen'). 
2.3 Relative Clauses and Adjectives 
The primary issue in the analysis of relative 
clauses and adjectives is the possibility of 
extreme ambiguity, due to several intersecting 
factors:  Japanese has rampant pro-drop and 
does not have any relative pronouns.  In 
addition, a head noun modified by a relative 
clause need not correspond to any gap in the 
relative clause, as shown by examples like the 
following (Matsumoto 1997): 
Example 7: 
atama ga yoku naru hon 
head NOM better become book 
'a book that makes one smarter' 
Therefore, if we were to posit an attributive 
adjective + noun construction (distinct from the 
relative clause + noun possibility) we would 
have systematic ambiguities for NPs like akai 
hon ('red book'), ambiguities which could never 
be resolved based on information in the 
sentence.  Instead, we have opted for a relative 
clause analysis of any adjective + noun 
combination in which the adjective could 
potentially be used predicatively.  Furthermore, 
because of gapless relative clauses like the one 
cited above, we have opted for a non-extraction 
analysis of relative clauses.2  
Nonetheless, the well-formedness constraints 
on MRS representations require that there be 
                                                     
2 There is in fact some linguistic evidence for 
extraction in some relative clauses in Japanese  (see 
e.g., Baldwin 2001).  However, we saw no practical 
need to allow for this possibility in our grammar, and 
particularly not one that would justify the increase in 
ambiguity. There is also evidence that some 
adjectives are true attributives and cannot be used 
predicatively (Yamakido 2000). These are handled by 
a separate adjective + noun rule restricted to just 
these cases. 
some relationship between the head noun and 
the relative clause.  We picked the topic relation 
for this purpose (following Kuno 1973).  The 
topic relation is introduced into the semantics by 
the relative clause rule.  As with main clause 
topics (which we also give a non-extraction 
analysis), we rely on downstream anaphora 
resolution to refine the relationship.   
2.4 Summary 
For the most part, semantic representations and 
the syntax-semantic interface already worked 
out in the ERG were directly applicable to the 
Japanese grammar.  In those cases where 
Japanese presented problems not yet 
encountered (or at least not yet tackled) in 
English, it was fairly straightforward to work out 
suitable MRS representations and means of 
building them up.  Both of these points illustrate 
the cross-linguistic validity and practical utility 
of MRS representations. 
3 Integration of a Morphological 
Analyzer 
As Japanese written text does not have word 
segmentation, a preprocessing system is 
required. We integrated ChaSen (Asahara & 
Matsumoto 2000), a tool that provides word 
segmentation as well as POS tags and 
morphological information such as verbal 
inflection. As the lexical coverage of ChaSen is 
higher than that of the HPSG lexicon, default 
part-of-speech entries are inserted into the 
lexicon. These are triggered by the part-of-
speech information given by ChaSen, if there is 
no existing entry in the lexicon. These specific 
default entries assign a type to the word that 
contains features typical to its part-of-speech. It 
is therefore possible to restrict the lexicon to 
those cases where the lexical information 
contains more than the typical information for a 
certain part-of-speech. This default mechanism 
is often used for different kinds of names and 
'ordinary' nouns, but also for adverbs, 
interjections and verbal nouns (where we 
assume a default transitive valence pattern).3 
                                                     
3 Kanayama et al (2000) use a similar mechanism for 
most words. They report only 105 grammar-inherent 
lexical entries.  
The ChaSen lexicon is extended with a domain-
specific lexicon, containing, among others, 
names in the domain of banking. 
For verbs and adjectives, ChaSen gives 
information about stems and inflection that is 
used in a similar way. The inflection type is 
translated to an HPSG type. These types interact 
with the inflectional rules in the grammar such 
that the default entries are inflected just as 
'known' words would be. 
In addition to the preprocessing done by 
ChaSen, an additional (shallow) preprocessing 
tool recognizes numbers, date expressions, 
addresses, email addresses, URLs, telephone 
numbers and currency expressions.  The output 
of the preprocessing tool replaces these 
expressions in the string with placeholders.  The 
placeholders are parsed by the grammar using 
special placeholder lexical entries. 
4 Robustness and Performance Issues 
The grammar is aimed at working with real-
world data, rather than at experimenting with 
linguistic examples. Therefore, robustness and 
performance issues play an important role. 
While grammar development is carried out in 
the LKB (Copestake 2002), processing (both in 
the application domain and for the purposes of 
running test suites) is done with the highly 
efficient PET parser (Callmeier 2000). Figures 1 
and 2 show the performance of PET parsing of 
hand-made and real data, respectively. 
Phenomenon  items 
# 
etasks 
? 
filter 
% 
edges 
? 
first 
? (s) 
total 
? (s) 
tcpu
? (s)
gc 
? (s)
space 
? (kb) 
Total 742 946 95.7 303 0.06 0.11 0.11 0 833 
Fig.1 Performance parsing banking data, generated 
by [incr tsdb()] 
 
Phenomenon items 
# 
etasks 
? 
filter 
% 
edges 
? 
first 
? (s) 
total 
? (s) 
tcpu 
? (s)
tgc 
? (s) 
space
? (kb)
Total 316 2020 96.5 616 0.23 0.26 0.26 0 1819
Fig.2 Performance parsing document request data, 
generated by [incr tsdb()]  
 One characteristic of real-world data is the 
variety of punctuation marks that occur and the 
potential for ambiguity that they bring. In our 
grammar, certain punctuation marks are given 
lexical entries and processed by grammar rules. 
Take, for example, quotation marks. Ignoring 
them (as done in most development-oriented 
grammars and smaller grammars), leads to a 
significant loss of structural information: 
Example 8: 
"Botan wo osu" to it-ta 
button ACC push COMPL say-past 
'Someone said: ?push the button. "?  
The formative to is actually ambiguous between 
a complementizer and a conjunction.  Since the 
phrase before to is a complete sentence, this 
string is ambiguous if one ignores the quotation 
marks.  With the quotation marks, however, only 
the complementizer to is possible.  Given the 
high degree of ambiguity inherent in broad-
coverage grammars, we have found it extremely 
useful to parse punctuation rather than ignore it. 
The domains we have been working on (like 
many others) contain many date and number 
expressions. While a shallow tool recognizes 
general structures, the grammar contains rules 
and types to process these.  
Phenomena occurring in semi-spontaneous 
language (email correspondence), such as 
interjections (e.g. maa 'well'), contracted verb 
forms (e.g. tabe-chatta < tabete-shimatta 
'(someone) ate it all up'), fragmentary sentences 
(e.g. bangou: 1265 'number: 1265') and NP 
fragments (e.g. bangou? 'number?') must be 
covered as well as the 'ordinary' complete 
sentences found in more carefully edited text.  
Our grammar includes types, lexical entries, and 
grammar rules for dealing with such phenomena. 
Perhaps the most important performance 
issue for broad coverage grammars is ambiguity. 
At one point in the development of this 
grammar, the average number of readings 
doubled in two months of work. We currently 
have two strategies for addressing this problem: 
First, we include a mechanism into the grammar 
rules that chooses left-branching rules in cases 
of compounds, genitive modification and 
conjuncts, as we don?t have enough lexical-
semantic information represented to choose the 
right dependencies in these cases.4 Secondly, we 
use a mechanism for hand-coding reading 
preferences among rules and lexical entries. 
                                                     
4Consider, for example, genitive modification: The 
semantic relationship between modifier and modifiee 
is dependent on their semantic properties: toukyou no 
kaigi - 'the meeting in Tokyo', watashi no hon - 'my 
book'. More lexical-semantic information is needed  
to choose the correct parse in more complex 
structures, such as in watashi no toukyou no imooto ? 
?My sister in Tokyo?. 
Restrictions like head-complement preferred to 
head-adjunct are quite obvious. Others require 
domain-specific mechanisms that shall be 
subject of further work.  Stochastic 
disambiguation methods being developed for the 
ERG by the Redwoods project at Stanford 
University (Oepen et al 2002) should be 
applicable to this grammar as well.  
5 Evaluation 
The grammar currently covers 93.4% of 
constructed examples for the banking domain 
(747 sentences) and 78.2% of realistic email 
correspondence data (316 sentences), concerning 
requests for documents. During three months of 
work, the coverage in the banking domain 
increased 48.49%. The coverage of the 
document request data increased 51.43% in the 
following two weeks. 
Phenomenon total 
items 
# 
positive
items 
# 
word
string
% 
lexical 
items 
? 
parser 
analyses 
   ? 
total 
results 
# 
overall 
coverage 
% 
Total 747 747 101 75.24 6.54 698 93.4 
Fig.3 Coverage of banking data, generated by 
 [incr tsdb()] 
Phenomenon total 
items 
# 
positive
items 
# 
word
string
% 
lexical 
items 
? 
parser 
analyses 
   ? 
total 
results 
# 
overall 
coverage 
% 
Total 316 316 1.00 83.90 39.91 247 78.2 
Fig.4 Coverage of document request data, generated 
by [incr tsdb()] 
We applied the grammar to unseen data in one 
of the covered domains, namely the FAQ site of 
a Japanese bank. The coverage was 61%. 91.2% 
of the parses output were associated with all 
well-formed MRSs. That means that we could 
get correct MRSs in 55.61% of all sentences.  
Conclusion 
We described a broad coverage Japanese 
grammar, based on HPSG theory. It encodes 
syntactic, semantic, and pragmatic information. 
The grammar system is connected to a 
morphological analysis system and uses default 
entries for words unknown to the HPSG lexicon.  
Some basic constructions of the Japanese 
grammar were described. As the grammar is 
aimed at working in applications with real-world 
data, performance and robustness issues are 
important.  
The grammar is being developed in a 
multilingual context, where much value is 
placed on parallel and consistent semantic 
representations.  The development of this 
grammar constitutes an important test of the 
cross-linguistic validity of the MRS formalism. 
The evaluation shows that the grammar is at 
a stage where domain adaptation is possible in a 
reasonable amount of time. Thus, it is a 
powerful resource for linguistic applications for 
Japanese. 
In future work, this grammar could be further 
adapted to another domain, such as the EDR 
newspaper corpus (including a headline 
grammar).  As each new domain is approached, 
we anticipate that the adaptation will become 
easier as resources from earlier domains are 
reused.  Initial evaluation of the grammar on 
new domains and the growth curve of grammar 
coverage should bear this out.  
References  
Asahara, Masayuki and Yuji Matsumoto (2000). 
Extended Models and Tools for High-performance 
Part-of-speech Tagger. In Proceedings of the 18th 
International Conference on Computational 
Linguistics, Coling 2000, 21-27. Saarbr?cken, 
Germany. 
Baldwin, Timothy (2001). Making Lexical Sense of 
Japanese-English Machine Translation: A 
Disambiguation Extravaganza. PhD thesis, Tokyo 
Institute of Technology. 
Bender, Emily M. (2002). Number Names in 
Japanese: A Head-Medial Construction in a Head-
Final Language.  Paper presented at the 76th 
annual meeting of the LSA, San Francisco. 
Callmeier, Ulrich (2000). PET ? a platform for 
experimentation with efficient HPSG processing 
techniques. Journal of Natural Language 
Engineering, Special Issue on Efficient Processing 
with HPSG: Methods, Systems, Evaluation, pages 
99-108. 
Copestake, Ann (2002). Implementing Typed 
Feature-Structure Grammars. Stanford: CSLI. 
Copestake, Ann, Alex Lascarides, and Dan Flickinger 
(2001). An Algebra for Semantic Construction in 
Constraint-based Grammars. Proceedings of the 
39th Annual Meeting of the Association for 
Computational Linguistics (ACL 2001), Toulouse, 
France. 
Flickinger, Dan (2000).  On Building a More 
Efficient Grammar by Exploiting Types.  Natural 
Language Engineering 6(1) (Special Issue on 
Efficient Processing with HPSG), pages 15-28. 
Kanayama, Hiroshi, Kentaro Torisawa, Yutaka 
Mitsuishi and Jun?ichi Tsujii (2000). A Hybrid 
Japanese Parser with Hand-crafted Grammar and 
Statistics. In Proceedings of the 18th International 
Conference on Computational Linguistics, Coling 
2000. Saarbr?cken, Germany. 
Kuno, Susumu (1973). The Structure of the Japanese 
Language. Cambridge, MA: The MIT Press. 
Matsumoto, Yoshiko (1997). Noun-Modifying 
Constructions in Japanese: A Frame Semantic 
Approach. John Benjamins.  
Oepen, Stephan and John Carroll (2000). 
Performance Profiling for Parser Engineering. 
Journal of Natural Language Engineering, Special 
Issue on Efficient Processing with HPSG: Methods, 
Systems, Evaluation, pages 81-97. 
Oepen, Stephan, Kristina Toutanova, Stuart Shieber, 
Chris Manning, Dan Flickinger and Thorsten 
Brants (2002). The LinGO Redwoods Treebank. 
Motivation and Preliminary Applications. In 
Proceedings of the 19th International Conference 
on Computational Linguistics, Coling 2002. Tapei, 
Taiwan. . 
Pollard, Carl and Ivan A. Sag (1994). Head-Driven 
Phrase Structure Grammar. University of Chicago 
Press. 
Siegel, Melanie (1999). The Syntactic Processing of 
Particles in Japanese Spoken Language. In: Wang, 
Jhing-Fa and Wu, Chung-Hsien (eds.): 
Proceedings of the 13th Pacific Asia Conference 
on Language, Information and Computation, 
Taipei 1999.  
Siegel, Melanie (2000). HPSG Analysis of Japanese. 
In: W. Wahlster (ed.): Verbmobil: Foundations of 
Speech-to-Speech Translation. Springer Verlag.  
Smith, Jeffrey D. (1999). English number names in 
HPSG. In Gert Webelhuth, Andreas Kathol, and 
Jean-Pierre Koenig (eds.), Lexical and 
Constructional Aspects of Linguistic Explanation. 
Stanford: CSLI. 145-160. 
Yamakido, Hiroko (2000). Japanese attributive 
adjectives are not (all) relative clauses.  In Roger 
Billerey and Brook Danielle Lillehaugen (eds.), 
WCCFL 19: Proceedings of the 19th West Coast 
Conference on Formal Linguistics. Somerville, 
MA: Cascadilla Press.  588-602. 
 
The Grammar Matrix: An Open-Source Starter-Kit for the Rapid
Development of Cross-Linguistically Consistent Broad-Coverage Precision
Grammars
Emily M. Bender and Dan Flickinger and Stephan Oepen
Center for the Study of Language and Information
Stanford University
fbender jdan joeg@csli.stanford.edu
Abstract
The grammar matrix is an open-source
starter-kit for the development of broad-
coverage HPSGs. By using a type hierar-
chy to represent cross-linguistic generaliza-
tions and providing compatibility with other
open-source tools for grammar engineering,
evaluation, parsing and generation, it facil-
itates not only quick start-up but also rapid
growth towards the wide coverage necessary
for robust natural language processing and
the precision parses and semantic represen-
tations necessary for natural language under-
standing.
1 Introduction
The past decade has seen the development of
wide-coverage implemented grammars represent-
ing deep linguistic analysis of several languages
in several frameworks, including Head-Driven
Phrase Structure Grammar (HPSG), Lexical-
Functional Grammar (LFG), and Lexicalized Tree
Adjoining Grammar (LTAG). In HPSG, the most ex-
tensive grammars are those of English (Flickinger,
2000), German (Mu?ller & Kasper, 2000), and
Japanese (Siegel, 2000; Siegel & Bender, 2002).
Despite being couched in the same general frame-
work and in some cases being written in the
same formalism and consequently being compati-
ble with the same parsing and generation software,
these grammars were developed more or less inde-
pendently of each other. They each represent be-
tween 5 and 15 person years of research efforts,
and comprise 35?70,000 lines of code. Unfor-
tunately, most of that research is undocumented
and the accumulated analyses, best practices for
grammar engineering, and tricks of the trade are
only available through painstaking inspection of
the grammars and/or consultation with their au-
thors. This lack of documentation holds across
frameworks, with certain notable exceptions, in-
cluding Alshawi (1992), Mu?ller (1999), and Butt,
King, Nin?o, & Segond (1999).
Grammars which have been under development
for many years tend to be very difficult to mine for
information, as they contain layers upon layers of
interacting analyses and decisions made in light of
various intermediate stages of the grammar. As a
result, when embarking on the creation of a new
grammar for another language, it seems almost
easier to start from scratch than to try to model it on
an existing grammar. This is unfortunate?being
able to leverage the knowledge and infrastructure
embedded in existing grammars would greatly ac-
celerate the process of developing new ones. At the
same time, these grammars represent an untapped
resource for the bottom-up exploration of language
universals.
As part of the LinGO consortium?s multi-lingual
grammar engineering effort, we are developing a
?grammar matrix? or starter-kit, distilling the wis-
dom of existing grammars and codifying and doc-
umenting it in a form that can be used as the basis
for new grammars.
In the following sections, we outline the inven-
tory of a first, preliminary version of the grammar
matrix, discuss the interaction of basic construc-
tion types and semantic composition in unification
grammars by means of a detailed example, and
consider extensions to the core inventory that we
foresee and an evaluation methodology for the ma-
trix proper.
2 Preliminary Development of Matrix
We have produced a preliminary version of the
grammar matrix relying heavily on the LinGO
project?s English Resource Grammar, and to a
lesser extent on the Japanese grammar developed
jointly between DFKI Saarbru?cken (Germany) and
YY Technologies (Mountain View, CA). This early
version of the matrix comprises the following com-
ponents:
 Types defining the basic feature geometry and
technical devices (e.g., for list manipulation).
 Types associated with Minimal Recursion Se-
mantics (see, e.g., Copestake, Lascarides, &
Flickinger, 2001), a meaning representation
language which has been shown to be well-
suited for semantic composition in typed fea-
ture structure grammars. This portion of the
grammar matrix includes a hierarchy of rela-
tion types, types and constraints for the prop-
agation of semantic information through the
phrase structure tree, a representation of illo-
cutionary force, and provisions for grammar
rules which make semantic contributions.
 General classes of rules, including deriva-
tional and inflectional (lexical) rules, unary
and binary phrase structure rules, headed and
non-headed rules, and head-initial and head-
final rules. These rule classes include im-
plementations of general principles of HPSG,
like, for example, the Head Feature and Non-
Local Feature Principles.
 Types for basic constructions such as head-
complement, head-specifier, head-subject,
head-filler, and head-modifier rules, coordi-
nation, as well as more specialized classes
of constructions, such as relative clauses and
noun-noun compounding. Unlike in specific
grammars, these types do not impose any or-
dering on their daughters in the grammar ma-
trix.
Included with the matrix are configuration and
parameter files for the LKB grammar engineering
environment (Copestake, 2002).
Although small, this preliminary version of
the matrix already reflects the main goals of
the project: (i) Consistent with other work in
HPSG, semantic representations and in particular
the syntax-semantics interface are developed in de-
tail; (ii) the types of the matrix are each represen-
tations of generalizations across linguistic objects
and across languages; and (iii) the richness of the
matrix and the incorporation of files which connect
it with the LKB allow for extremely quick start-up
as the matrix is applied to new languages.
Since February 2002, this preliminary version of
the matrix has been in use at two Norwegian uni-
versities, one working towards a broad-coverage
reference implementation of Norwegian (NTNU),
the other?for the time being?focused on specific
aspects of clause structure and lexical description
(Oslo University). In the first experiment with
the matrix, at NTNU, basic Norwegian sentences
were parsing and producing reasonable semantics
within two hours of downloading the matrix files.
Linguistic coverage should scale up quickly, since
the foundation supplied by the matrix is designed
not only to provide a quick start, but also to support
long-term development of broad-coverage gram-
mars. Both initiatives have confirmed the utility of
the matrix starter kit and already have contributed
to a series of discussions on cross-lingual HPSG
design aspects, specifically in the areas of argu-
ment structure representations in the lexicon and
basic assumptions about constituent structure (in
one view, Norwegian exhibits a VSO topology in
the main clause). The user groups have suggested
refinements and extensions of the basic inventory,
and it is expected that general solutions, as they are
identified jointly, will propagate into the existing
grammars too.
3 A Detailed Example
As an example of the level of detail involved in
the grammar matrix, in this section we consider
the analysis of intersective and scopal modifica-
tion. The matrix is built to give Minimal Recursion
Semantics (MRS; Copestake et al, 2001; Copes-
take, Flickinger, Sag, & Pollard, 1999; Copestake,
Flickinger, Malouf, Riehemann, & Sag, 1995) rep-
resentations. The two English examples in (1)
exemplify the difference between intersective and
scopal modification:1
(1) a. Keanu studied Kung Fu on a spaceship.
b. Keanu probably studied Kung Fu.
The MRSs for (1a-b) (abstracting away from
agreement information) are given in (2) and (3).
The MRSs are ordered tuples consisting of a top
handle (h1 in both cases), an instance or event vari-
able (e in both cases), a bag of elementary predica-
tions (eps), and a bag of scope constraints (in these
cases, QEQ constraints or ?equal modulo quanti-
fiers?). In a well-formed MRS, the handles can be
1These examples also differ in that probably is a pre-
head modifier while on a spaceship is a post-head modifier.
This word-order distinction cross-cuts the semantic distinc-
tion, and our focus is on the latter, so we won?t consider the
word-order aspects of these examples here.
identified in one or more ways respecting the scope
constraints such that the dependencies between the
eps form a tree. For a detailed description of MRS,
see the works cited above. Here, we will focus on
the difference between the intersective modifier on
(a spaceship) and the scopal modifier probably.
In (2), the ep contributed by on (?on-rel?) shares
its handle (h7) with the ep contributed by the verb
it is modifying (?study-rel?). As such, the two will
always have the same scope; no quantifier can in-
tervene. Further, the second argument of the on-rel
(e) is the event variable of the study-rel. The first
argument, e0, is the event variable of the on-rel and
the third argument, z, is the instance variable of the
spaceship-rel.
(2) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?), h7:study-rel(e, x, y),
h8:def-np-rel(y, h9, h10),
h11:named-rel(y, ?Kung Fu?), h7:on-rel(e0, e, z),
h12:a-quant-rel(z, h13, h14),
h15:spaceship-rel(z) g,
f h2 QEQ h7, h4 QEQ h6, h19 QEQ h11,
h13 QEQ h15 g i
In (3), the ep contributed by the scopal modifier
probably (?probably-rel?) has its own handle (h7)
which is not shared by anything. Furthermore, it
takes a handle (h8) rather than the event variable
of the study-rel as its argument. h8 is equal mod-
ulo quantifiers (QEQ) to the handle of the study-rel
(h9), and h7 is equal modulo quantifiers to the ar-
gument of the prpstn-rel (h2). The prpstn-rel is the
ep representing the illocutionary force of the whole
expression. This means that quantifiers associated
with the NPs Keanu and Kung Fu can scope inside
or outside probably.
(3) h h1, e,
f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),
h6:named-rel(x, ?Keanu?),
h7:probably-rel(h8), h9:study-rel(e, x, y),
h10:def-np-rel(y, h11, h12),
h13:named-rel(y, ?Kung Fu?) g,
f h2 QEQ h7, h4 QEQ h6, h8 QEQ h9,
h11 QEQ h13 g i
While the details of modifier placement, which
parts of speech can modify which kinds of phrases,
etc., differ across languages, we believe that all
languages display a distinction between scopal and
intersective modification. Accordingly, the types
isect-mod-phrase := head-mod-phr-simple &
[ HEAD-DTR.SYNSEM.LOCAL
[ CONT [ TOP #hand,
INDEX #index ],
KEYS.MESSAGE 0-dlist ],
NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL isect-mod ]>,
CONT.TOP #hand ],
C-CONT.INDEX #index ].
Figure 1: TDL description of isect-mod-phrase
scopal-mod-phrase := head-mod-phr-simple &
[ NON-HEAD-DTR.SYNSEM.LOCAL
[ CAT.HEAD.MOD <[ LOCAL scopal-mod ]>,
CONT.INDEX #index ],
C-CONT.INDEX #index ].
Figure 2: TDL description of scopal-mod-phrase
necessary for describing these two kinds of modi-
fication are included in the matrix.
The types isect-mod-phrase and scopal-mod-
phrase (shown in Figures 1 and 2) encode the in-
formation necessary to build up in a compositional
manner the modifier portions of the MRSs in (2)
and (3).
These types are embedded in the type hierar-
chy of the matrix. Through their supertype head-
mod-phr-simple they inherit information common
to many types of phrases, including the basic fea-
ture geometry, head feature and non-local feature
passing, and semantic compositionality. These
types also have subtypes in the matrix specifying
the two word-order possibilities (pre- or post-head
modifiers), giving a total of four subtypes.2
The most important difference between these
types is in the treatment of the handle of the head
daughter?s semantics, to distinguish intersective
and scopal modification. In isect-mod-phrase, the
top handles (TOP) of the head and non-head (i.e.,
modifier) daughters are identified (#hand). This
allows for MRSs like (2) where the eps contributed
by the head (?study-rel?) and the modifier (?on-rel?)
take the same scope. The type scopal-mod-phrase
bears no such constraint. This allows for MRSs
like (3) where the modifier?s semantic contribution
(?probably-rel?) takes the handle of the head?s se-
mantics (?study-rel?) as its argument, so that the
modifier outscopes the head. In both types of mod-
2All four subtypes are provided on the theory that most
languages will make use of all or most of them.
ifier phrase, a constraint inherited from the super-
type ensures that the handle of the modifier is also
the handle of the whole phrase.
The constraints on the LOCAL value inside
the modifier?s MOD value regulate which lexi-
cal items can appear in which kind of phrase.
Intersective modifiers specify lexically that they
are [ MOD h [ LOCAL isect-mod ] i] and sco-
pal modifiers specify lexically that they are
[ MOD h [ LOCAL scopal-mod ] i].3 These con-
straints exemplify the kind of information that will
be developed in the lexical hierarchy of the matrix.
It is characteristic of broad-coverage grammars
that every particular analysis interacts with many
other analyses. Modularization is an on-going con-
cern, both for maintainability of individual gram-
mars, and for providing the right level of abstrac-
tion in the matrix. For the same reasons, we have
only been able to touch on the highlights of the se-
mantic analysis of modification here, but hope that
this quick tour will suffice to illustrate the extent
of the jump-start the matrix can give in the devel-
opment of new grammars.
4 Future Extensions
The initial version of the matrix, while sufficient to
support some useful grammar work, will require
substantial further development on several fronts,
including lexical representation, syntactic gener-
alization, sociolinguistic variation, processing is-
sues, and evaluation. This first version drew most
heavily from the implementation of the English
grammar, with some further insights drawn from
the grammar of Japanese. Extensions to the ma-
trix will be based on careful study of existing im-
plemented grammars for other languages, notably
German, Spanish and Japanese, as well as feed-
back from those using the first version of the ma-
trix.
For lexical representation, one of the most ur-
gent needs is to provide a language-independent
type hierarchy for the lexicon, at least for major
parts of speech, establishing the mechanisms used
for linking syntactic subcategorization to seman-
tic predicate-argument structure. Lexical rules pro-
vide a second mechanism for expressing general-
3Note that there are no further subtypes of LOCAL values
beyond isect-mod and scopal-mod. Since these grammars do
not make extensive use of subtypes of LOCAL values, they
were available for encoding this distinction. Alternative solu-
tions include positing a new feature.
izations within the lexicon, and offer ready oppor-
tunities for cross-linguistic abstractions for both
inflectional and derivational regularities. Work is
also progressing on establishing a standard rela-
tional database (using PostgreSQL) for storing in-
formation for the lexical entries themselves, im-
proving both scalability and clarity compared to
the current simple text file representation. Form-
based tools will be provided both for constructing
lexical entries and for viewing the contents of the
lexicon.
The primary focus of work on syntactic general-
ization in the matrix is to support more freedom
in word order, for both complements and modi-
fiers. The first step will be a relatively conserva-
tive extension along the lines of Netter (1996), al-
lowing the grammar writer more control over how
a head combines with complements of different
types, and their interleaving with modifier phrases.
Other areas of immediate cross-linguistic interest
include the hierarchy of head types, control phe-
nomena, clitics, auxiliary verbs, noun-noun com-
pounds, and more generally, phenomena that in-
volve the word/phrase distinction, such as noun in-
corporation. A study of the existing grammars for
English, German, Japanese, and Spanish reveals
a high degree of language-specificity for several
of these phenomena, but also suggests promise of
reusable abstractions.
Several kinds of sociolinguistic variation require
extensions to the matrix, including grammaticized
aspects of pragmatics such as politeness and em-
pathy, as well as dialect and register alternations.
The grammar of Japanese provides a starting point
for representations of both empathy and politeness.
Implementations of familiar vs. formal verb forms
in German and Spanish provide further instances
of politeness to help build the cross-linguistic ab-
stractions. Extensions for dialect variation will
build on some exploratory work in adapting the
English grammar to support American, British,
and Australian regionalisms, both lexical and syn-
tactic, while restricting dialect mixture in genera-
tion and associated spurious ambiguity in parsing.
While the development of the matrix will be
built largely on the LKB platform, support will also
be needed for using the emerging grammars on
other processing platforms, and for linking to other
packages for pre-processing the linguistic input.
Several other platforms exist which can efficiently
parse text using the existing grammars, includ-
ing the PET system developed in C++ at Saarland
University (Germany) and the DFKI (Callmeier,
2000); the PAGE system developed in Lisp at the
DFKI (Uszkoreit et al, 1994); the LiLFeS system
developed at Tokyo University (Makino, Yoshida,
Torisawa, & Tsujii, 1998), and a parallel process-
ing system developed in Objective C at Delft Uni-
versity (The Netherlands; van Lohuizen, 2002).
As part of the matrix package, sample configura-
tion files and documentation will be provided for
at least some of these additional platforms.
Existing pre-processing packages can also sig-
nificantly reduce the effort required to develop
a new grammar, particularly for coping with the
morphology/syntax interface. For example, the
ChaSen package for segmenting Japanese input
into words and morphemes (Asahara & Mat-
sumoto, 2000) has been linked to at least the LKB
and PET systems. Support for connecting im-
plementations of language-specific pre-processing
packages of this kind will be preserved and ex-
tended as the matrix develops. Likewise, config-
uration files are included to support generation, at
least within the LKB, provided that the grammar
conforms to certain assumptions about semantic
representation using the Minimal Recursion Se-
mantics framework.
Finally, a methodology is under development for
constructing and using test suites organized around
a typology of linguistic phenomena, using the im-
plementation platform of the [incr tsdb()] profil-
ing package (Oepen & Flickinger, 1998; Oepen
& Callmeier, 2000). These test suites will enable
better communication about current coverage of a
given grammar built using the matrix, and serve as
the basis for identifying additional phenomena that
need to be addressed cross-linguistically within the
matrix. Of course, the development of the typol-
ogy of phenomena is itself a major undertaking
for which a systematic cross-linguistic approach
will be needed, a discussion of which is outside
the scope of this report. But the intent is to seed
this classification scheme with a set of relatively
coarse-grained phenomenon classes drawn from
the existing grammars, then refine the typology as
it is applied to these and new grammars built using
the matrix.
5 Case Studies
One important part of the matrix package will be a
library of phenomenon-based analyses drawn from
the existing grammars and over time from users of
the matrix, to provide working examples of how
the matrix can be applied and extended. Each case
study will be a set of grammar files, simplified for
relevance, along with documentation of the anal-
ysis, and a test suite of sample sentences which
define the range of data covered by the analysis.
This library, too, will be organized around the ty-
pology of phenomena introduced above, but will
also make explicit reference to language families,
since both similarities and differences among re-
lated languages will be of interest in these case
studies. Examples to be included in the first re-
lease of this library include numeral classifiers in
Japanese, subject pro drop in Spanish, partial-VP
fronting in German, and verb diathesis in Norwe-
gian.
6 Evaluation and Evolution
The matrix itself is not a grammar but a collec-
tion of generalizations across grammars. As such,
it cannot be tested directly on corpora from partic-
ular languages, and we must find other means of
evaluation. We envision overall evaluation of the
matrix based on case studies of its performance
in helping grammar engineers quickly start new
grammars and in helping them scale those gram-
mars up. Evaluation in detail will based on au-
tomatable deletion/substitution metrics, i.e., tools
that determine which types from the matrix get
used as is, which get used with modifications, and
which get ignored in various matrix-derived gram-
mars. Furthermore, if the matrix evolves to include
defeasible constraints, these tools will check which
constraints get overridden and whether the value
chosen is indeed common enough to be motivated
as a default value. This evaluation in detail should
be paired with feedback from the grammar engi-
neers to determine why changes were made.
The main goal of evaluation is, of course, to im-
prove the matrix over time. This raises the ques-
tion of how to propagate changes in the matrix to
grammars based on earlier versions. The following
three strategies (meant to be used in combination)
seem promising: (i) segregate changes that are im-
portant to sync to (e.g., changes that affect MRS
outputs, fundamental changes to important anal-
yses), (ii) develop a methodology for communi-
cating changes in the matrix, their motivation and
their implementation to the user community, and
(iii) develop tools for semi-automating resynching
of existing grammars to upgrades of the matrix.
These tools could use the type hierarchy to predict
where conflicts are likely to arise and bring these
to the engineer?s attention, possibly inspired by the
approach under development at CSLI for the dy-
namic maintenance of the LinGO Redwoods tree-
bank (Oepen et al, 2002).
Finally, while initial development of the ma-
trix has been and will continue to be highly cen-
tralized, we hope to provide support for proposed
matrix improvements from the user community.
User feedback will already come in the form of
case studies for the library as discussed in Sec-
tion 5 above, but also potentially in proposals for
modification of the matrix drawing on experiences
in grammar development. In order to provide
users with some cross-linguistic context in which
to develop and evaluate such proposals themselves,
we intend to provide some sample matrix-derived
grammars and corresponding testsuites with the
matrix. A user could thus make a proposed change
to the matrix, run the testsuites for several lan-
guages using the supplied grammars which draw
from that changed matrix, and use [incr tsdb()]
to determine which phenomena have been affected
by the change. It is clear that full automation of
this evaluation process will be difficult, but at least
some classes of changes to the matrix will per-
mit this kind of quick cross-linguistic feedback to
users with only a modest amount of additional in-
frastructure.
7 Conclusion
This project carries linguistic, computational, and
practical interest. The linguistic interest lies in the
HPSG community?s general bottom-up approach
to language universals, which involves aiming for
good coverage of a variety of languages first, and
leaving the task of what they have in common for
later. (Of course, theory building is never purely
data-driven, and there are substantive hypotheses
within HPSG about language universals.) Now
that we have implementations with fairly extensive
coverage for a somewhat typologically diverse set
of languages, it is a good time to take the next step
in this program, working to extract and generalize
what is similar across these existing wide-coverage
grammars. Moreover, the central role of types in
the representation of linguistic generalizations en-
ables the kind of underspecification which is useful
for expressing what is common among related lan-
guages while allowing for the further specializa-
tion which necessarily distinguishes one language
from another.
The computational interest is threefold. First
there is the question of what formal devices the
grammar matrix will require. Should it include
defaults? What about domain union (linearization
theory)? The selection and deployment of formal
devices should be informed by on-going research
on processing schemes, and here the crosslinguis-
tic perspective can be particularly helpful. Where
there are several equivalent analyses of the same
linguistic phenomena (e.g., morphosyntactic am-
biguity or optionality), the choice of analysis can
have processing implications that aren?t necessar-
ily apparent in a single grammar. Second, having
a set of wide-coverage HPSGs with fairly standard-
ized fundamentals could prove interesting for re-
search on stochastic processing and disambigua-
tion, especially if the languages differ in gross ty-
pological features such as word order. Finally,
there are also computational issues involved in
how the grammar matrix would evolve over time
as it is used in new grammars. The matrix en-
ables the developer of a grammar for a new lan-
guage to get a quick start on producing a system
that parses and generates with non-trivial seman-
tics, while also building the foundation for a wide-
coverage grammar of the language. But the matrix
itself may well change in parallel with the devel-
opment of the grammar for a particular language,
so appropriate mechanisms must be developed to
support the merging of enhancements to both.
There is also practical industrial benefit to this
project. Companies that are consumers of these
grammars benefit when grammars of multiple lan-
guages work with the same parsing and generation
algorithms and produce standardized semantic rep-
resentations derived from a rich, linguistically mo-
tivated syntax-semantics interface. More impor-
tantly, the grammar matrix will help to remove one
of the primary remaining obstacles to commercial
deployment of grammars of this type and indeed of
the commercial use of deep linguistic analysis: the
immense cost of developing the resource.
Acknowledgements
Since the grammar matrix draws on prior re-
search and existing grammars, it necessarily re-
flects contributions from many people. Rob
Malouf, Jeff Smith, John Beavers, and Kathryn
Campbell-Kibler have contributed to the LinGO
ERG; Melanie Siegel is the original developer for
the Japanese grammar. Tim Baldwin, Ann Copes-
take, Ivan Sag, Tom Wasow, and other members
of the LinGO Laboratory at CSLI have had a great
deal of influence on the design of the grammatical
analyses and corresponding MRS representations.
Warmest thanks to Lars Hellan and his colleagues
at NTNU and Jan Tore L?nning and his students
at Oslo University for their cooperation, patience,
and tolerance.
References
Alshawi, H. (Ed.). (1992). The Core Language Engine.
Cambridge, MA: MIT Press.
Asahara, M., & Matsumoto, Y. (2000). Extended mod-
els and tools for high-performance part-of-speech
tagger. In Proceedings of the 18th International
Conference on Computational Linguistics (pp. 21 ?
27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stanford,
CA: CSLI Publications.
Callmeier, U. (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering, 6 (1) (Spe-
cial Issue on Efficient Processing with HPSG), 99 ?
108.
Copestake, A. (2002). Implementing typed feature
structure grammars. Stanford, CA: CSLI Publica-
tions.
Copestake, A., Flickinger, D., Malouf, R., Riehemann,
S., & Sag, I. (1995). Translation using minimal re-
cursion semantics. In Proceedings of the Sixth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. Leuven,
Belgium.
Copestake, A., Flickinger, D. P., Sag, I. A., & Pol-
lard, C. (1999). Minimal Recursion Semantics. An
introduction. in preparation, CSLI Stanford, Stan-
ford, CA.
Copestake, A., Lascarides, A., & Flickinger, D. (2001).
An algebra for semantic construction in constraint-
based grammars. In Proceedings of the 39th Meet-
ing of the Association for Computational Linguistics.
Toulouse, France.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Process-
ing with HPSG), 15 ? 28.
van Lohuizen, M. (2002). Efficient and thread-safe
unification with LinGO. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collaborative
language engineering. A case study in efficient
grammar-based processing. Stanford, CA: CSLI
Publications. (forthcoming)
Makino, T., Yoshida, M., Torisawa, K., & Tsujii, J.
(1998). LiLFeS ? towards a practical HPSG parser.
In Proceedings of the 17th International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Lin-
guistics (pp. 807 ? 11). Montreal, Canada.
Mu?ller, S. (1999). Deutsche syntax deklarativ. Head-
Driven Phrase Structure Grammar fu?r das Deutsche.
Tu?bingen, Germany: Max Niemeyer Verlag.
Mu?ller, S., & Kasper, W. (2000). HPSG analysis of
German. In W. Wahlster (Ed.), Verbmobil. Foun-
dations of speech-to-speech translation (Artificial
Intelligence ed., pp. 238 ? 253). Berlin, Germany:
Springer.
Netter, K. (1996). Functional categories in an HPSG
for German. Unpublished doctoral dissertation,
Saarland University, Saarbru?cken, Germany.
Oepen, S., & Callmeier, U. (2000). Measure for mea-
sure: Parser cross-fertilization. Towards increased
component comparability and exchange. In Pro-
ceedings of the 6th International Workshop on Pars-
ing Technologies (pp. 183 ? 194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards sys-
tematic grammar profiling. Test suite technology ten
years after. Journal of Computer Speech and Lan-
guage, 12 (4) (Special Issue on Evaluation), 411 ?
436.
Oepen, S., Toutanova, K., Shieber, S., Manning, C.,
Flickinger, D., & Brants, T. (2002). The LinGO
Redwoods treebank. Motivation and preliminary ap-
plications. In Proceedings of the 19th International
Conference on Computational Linguistics. Taipei,
Taiwan.
Siegel, M. (2000). HPSG analysis of Japanese.
In W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelligence
ed., pp. 265 ? 280). Berlin, Germany: Springer.
Siegel, M., & Bender, E. M. (2002). Efficient deep
processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguis-
tics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Diagne,
A. K., Hinkelman, E. A., Kasper, W., Kiefer, B.,
Krieger, H.-U., Netter, K., Neumann, G., Oepen, S.,
& Spackman, S. P. (1994). DISCO ? an HPSG-
based NLP system and its application for appoint-
ment scheduling. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics.
Kyoto, Japan.
Parallel Distributed Grammar Engineering for Practical Applications
Stephan Oepen?, Emily M. Bender?, Uli Callmeier?, Dan Flickinger??, Melanie Siegel?
?CSLI Stanford ?YY Technologies ?DFKI GmbH
Stanford (CA) Mountain View (CA) Saarbru?cken (Germany)
?
?
?
oe
bender
dan
?
?
?
@csli.stanford.edu
{
uc
dan
}
@yy.com siegel@dfki.de
Abstract
Based on a detailed case study of paral-
lel grammar development distributed across
two sites, we review some of the require-
ments for regression testing in grammar en-
gineering, summarize our approach to sys-
tematic competence and performance profil-
ing, and discuss our experience with gram-
mar development for a commercial applica-
tion. If possible, the workshop presentation
will be organized around a software demon-
stration.
1 Background
The production of large-scale constraint-based
grammars and suitable processing environments is
a labour- and time-intensive process that, maybe,
has become somewhat of a growth industry over
the past few years, as companies explore products
that incorporate grammar-based language process-
ing. Many broad-coverage grammars have been
developed over several years, sometimes decades,
typically coordinated by a single grammarian who
would often draw on additional contributors (e.g.
the three HPSG implementations developed as part
of the VerbMobil effort, see Flickinger, Copes-
take, & Sag, 2000, Mu?ller & Kasper, 2000, and
Siegel, 2000; or the LFG implementations devel-
oped within the ParGram consortium, Butt, King,
Nin?o, & Segond, 1999).
More recently, we also find genuinely shared
and distributed development of broad-coverage
grammars, and we will use one such initiative as an
example?viz. an open-source HPSG implementa-
tion for Japanese jointly developed between DFKI
Saarbru?cken (Germany) and YY Technologies
(Mountain View, CA)?to demonstrate the techno-
logical and methodological challenges present in
distributed grammar and system engineering.
2 Parallel Distributed Grammar
Development?A Case Study
The Japanese grammar builds on earlier work per-
formed jointly between DFKI and the Computa-
tional Linguistics Department at Saarland Univer-
sity (Germany) within VerbMobil; much like for
the German VerbMobil grammar, two people were
contributing to the grammar in parallel, one build-
ing out syntactic analyses, the other charged with
integrating semantic composition into the syntax.
This relatively strict separation of responsibilities
mostly enabled grammarians to serialize incre-
mental development of the resource: the syntacti-
cian would supply a grammar with extended cov-
erage to the semanticist and, at the onset of the fol-
lowing iteration, start subsequent work on syntax
from the revised grammar.
In the DFKI ? YY cooperation the situation was
quite different. Over a period of eight months,
both partners had a grammarian working on syn-
tax and semantics simultaneously on a day-to-
day basis; both grammarians were submitting
changes to a joint, version-controlled source repos-
itory and usually would start the work day by re-
trieving the most recent revisions. At the same
time, product building and the development of
so-called ?domain libraries? (structured collections
of knowledge about a specific domain that is in-
stantiated from semantic representations delivered
from grammatical analysis) at YY already incorpo-
rated the grammar and depended on it for actual,
customer-specific contracts. Due to a continuous
demand for improvements in coverage and analy-
sis accuracy, the grammar used in the main product
line would be updated from the current develop-
ment version about once or twice a week. Parallel
to work on the Japanese grammar (and simultane-
ous work on grammars for English and Spanish),
both the grammar development environment (the
open-source LKB system; Copestake, 2002) and
the HPSG run-time component powering the YY
linguistic analysis engine (the open-source PET
parser; Callmeier, 2002) continued to evolve, as
did the YY-proprietary mapping of meaning repre-
sentations extracted from the HPSG grammars into
domain knowledge?all central parts of a complex
system of interacting components and constraints.
As has been argued before (see, for exam-
ple, Oepen & Flickinger, 1998), the nature of a
large-scale constraint-based grammar and the sub-
tle interactions of lexical and constructional con-
straints make it virtually impossible to predict how
a change in one part of the grammar affects over-
all system behaviour. A relatively minor repair in
one lexical class, numeral adjectives as in ?three
books were ordered? for instance, will have the po-
tential of breaking the interaction of that class with
the construction deriving named (numeric) entities
from a numeral (e.g. as in ?three is my favourite
number?) or the partitive construction (e.g. as in
?three have arrived already?). A ripple effect of
a single change can thus corrupt the semantics
produced for any of these cases and in the con-
sequence cause failure or incorrect behaviour in
the back-end system. In addition to these qual-
ity assurance requirements on grammatical cover-
age and correctness, the YY application (like most
applications for grammar-based linguistic analy-
sis) utilizes a set of hand-constructed parse rank-
ing heuristics that enables the parser to operate
in best-first search mode and to return only one
reading, i.e. the analysis that is ranked best by the
heuristic component. The parse ranking machin-
ery builds on preferences that are associated with
individual or classes of lexical items and construc-
tions. The set of preferences is maintained in par-
allel to the grammar, in a sense providing a layer
of performance-oriented annotations over the basic
building blocks of the core competence grammar.
Without discussing the details of the parse ranking
approach, it creates an additional element of un-
certainty in assessing grammar changes: since the
preference for a specific analysis results implic-
itly from a series of local preferences (of lexical
items and constructions contributing to the com-
plete derivation), introducing additional elements
(i.e. new local or global ambiguity) into the search
space and subjecting them to the partial ordering
can quickly skew the overall result.
Summing up, the grammar and application engi-
neering example presented here illustrates a num-
ber of highly typical requirements on the engi-
neering environment. First, all grammarians and
system engineers participating in the development
process need to keep frequent, detailed, and accu-
rate records of a large number of relevant parame-
ters, including but not limited to grammatical cov-
erage, correctness of syntactic analyses and cor-
responding semantic forms, parse selection accu-
racy, and overall system performance. Second, as
modifications to the system as a whole are made
daily?and sometimes several times each day?all
developers must be able to assess the impact of
recent changes and track their effects on all rele-
vant parameters; gathering the data and analyzing
it must be simple, fast, and automated as much as
possible. Third, not all modifications (to the gram-
mar or underlying software) will result in ?mono-
tonic? or backwards-compatible effects. A change
in the treatment of optional nominal complements,
for example, may affect virtually all derivation
trees and render a comparison of results at this
level uninformative. At the same time, a primarily
syntactic change of this nature will not cause an ef-
fect in associated meaning representations, so that
a semantic equivalence test over analyses should
be expected to yield an exact match to earlier re-
sults. Hence, the machinery for representation and
comparison of relevant parameters needs to facil-
itate user-level specification of informative tests
and evolution criteria. Finally, the metrics used in
tracking grammar development cannot be isolated
from measurements of system resource consump-
tion and overall performance (specific properties
of a grammar may trigger idiosyncrasies or soft-
ware bugs in a particular version of the process-
ing system); therefore, and to enable exchange of
reference points and comparability of experiments,
grammarians and system developers alike should
use the same, homogenuous set of relevant param-
eters.
3 Integrated Competence and
Performance Profiling
The integrated competence and performance pro-
filing methodology and associated engineering
platform, dubbed [incr tsdb()] (Oepen & Callmeier,
2000)1 and reviewed in the remainder of this sec-
1See ?http://www.coli.uni-sb.de/itsdb/?
for the (draft) [incr tsdb()] user manual, pronunciation rules,
and instructions on obtaining and installing the package.
tion, was designed to meet al of the requirements
identified in the DFKI ? YY case study. Generally
speaking, the [incr tsdb()] environment is an in-
tegrated package for diagnostics, evaluation, and
benchmarking in practical grammar and system
engineering. The toolkit implements an approach
to grammar development and system optimization
that builds on precise empirical data and system-
atic experimentation, as it has been advocated by,
among others, Erbach & Uszkoreit (1990), Erbach
(1991), and Carroll (1994). [incr tsdb()] has been
integrated with, as of June 2002, nine different
constraint-based grammar development and pars-
ing systems (including both environments in use at
YY, i.e. the LKB and PET), thus providing a pre-
standard reference point for a relatively large (and
growing) community of NLP developers. The [incr
tsdb()] environment builds on the following com-
ponents and modules:
? test and reference data stored with annota-
tions in a structured database; annotations
can range from minimal information (unique
test item identifier, item origin, length et al)
to fine-grained linguistic classifications (e.g.
regarding grammaticality and linguistic phe-
nomena presented in an item), as they are rep-
resented in the TSNLP test suites, for example
(Oepen, Netter, & Klein, 1997);
? tools to browse the available data, identify
suitable subsets and feed them through the
analysis component of processing systems
like the LKB and PET, LiLFeS (Makino,
Yoshida, Torisawa, & Tsujii, 1998), TRALE
(Penn, 2000), PAGE (Uszkoreit et al, 1994),
and others;
? the ability to gather a multitude of precise and
fine-grained (grammar) competence and (sys-
tem) performance measures?like the num-
ber of readings obtained per test item, various
time and memory usage statistics, ambigu-
ity and non-determinism metrics, and salient
properties of the result structures?and store
them in a uniform, platform-independent data
format as a competence and performance pro-
file; and
? graphical facilities to inspect the resulting
profiles, analyze system competence (i.e.
grammatical coverage and overgeneration)
and performance (e.g. cpu time and memory
usage, parser search space, constraint solver
'
&
$
%
Parser 3Parser 2Parser 1Grammar 3Grammar 2Grammar 1
TestSet 3TestSet 2TestSet 1
ParallelVirtualMachine
C and Lisp APIRelationalDBMS Batch ControlStatistics UserInterface
ANSI C Common-Lisp Tcl/Tk
Figure 1: Rough sketch of [incr tsdb()] architec-
ture: the core engine comprises the database man-
agement, batch control and statistics component,
and the user interface.
workload, and others) at variable granulari-
ties, aggregate, correlate, and visualize the
data, and compare among profiles obtained
from previous grammar or system versions or
other processing environments.
As it is depicted in Figure 1, the [incr tsdb()]
architecture can be broken down into three major
parts: (i) the underlying database management sys-
tem (DBMS), (ii) the batch control and statistics
kernel (providing a C and Lisp application pro-
gram interface to client systems that can be dis-
tributed across the network), and (iii) the graphi-
cal user interface (GUI). Although, historically, the
DBMS was developed independently and the ker-
nel can be operated without the GUI, the full func-
tionality of the integrated competence and perfor-
mance laboratory?as demonstrated below?only
emerges from the combination of all three com-
ponents. Likewise, the flexibility of a clearly de-
fined API to client systems and its ability to par-
allelize batch processing and distribute test runs
across the network have greatly contributed to the
success of the package. The following paragraphs
review some of the fundamental aspects in more
detail, sketch essential functionality, and comment
on how they have been exploited in the DFKI ? YY
cooperation.
Abstraction over Processors The [incr tsdb()]
environment, by virtue of its generalized pro-
file format, abstracts over specific processing en-
vironments. While grammar engineers in the
DFKI ? YY collaboration regularly use both the
LKB (primarily for interactive development) and
PET (mostly for batch testing and the assessment
of results obtained in the YY production envi-
ronment), usage of the [incr tsdb()] profile anal-
ysis routines in most aspects hides the specifics
of the token processor used in obtaining a profile.
Both platforms interprete the same typed feature
structure formalism, load the same set of gram-
mar source files, and (unless malfunctioning) pro-
duce equivalent results. Using [incr tsdb()], gram-
marians can obtain summary views of grammati-
cal coverage and overgeneration, inspect relevant
subsets of the available data, break down analysis
views according to various aggregation schemes,
and zoom in on specific aggregates or individual
test items as appropriate. Moreover, processing
results obtained from the (far more efficient) PET
parser (that has no visualization or debugging sup-
port built in), once recorded as an [incr tsdb()] pro-
file, can be used in conjunction with the LKB (con-
tingent on the use of identical grammars), thereby
facilitating graphical inspection of parse trees and
semantic formulae.
Parallelization of Test Runs The [incr tsdb()] ar-
chitecture (see Figure 1) separates the batch con-
trol and statistics kernel from what is referred to
as client processors (i.e. parsing systems like the
LKB or PET) through an application program inter-
face (API) and the Parallel Virtual Machine (PVM;
Geist, Bequelin, Dongarra, Manchek, & Sun-
deram, 1994) message-passing protocol layer. The
use of PVM?in connection with task scheduling,
error recovery, and roll-over facilities in the [incr
tsdb()] kernel?enables developers to transparently
parallelize and distribute execution of batch pro-
cessing. At YY, grammarians had a cluster of net-
worked Linux compute servers configured as a sin-
gle PVM instance, so that execution of a test run?
using the efficient PET run-time engine?could be
completed as a matter of a few seconds. The com-
bination of near-instantaneous profile creation and
[incr tsdb()] facilities for quick, semi-automated as-
sessment of relevant changes (see below) enabled
developers to pursue a strongly empiricist style of
grammar engineering, assessing changes and their
effects on actual system behavior in small incre-
ments (often many times per hour).
Structured Comparison One of the facilities
that has proven particularly useful in the dis-
tributed grammar engineering setup outlined in
Section 2 above is the flexible comparison of com-
petence and performance profiles. The [incr tsdb()]
package eases comparison of results on a per-
item basis, using an approach similar to Un?x
diff(1), but generalized for structured data sets.
By selection of a set of parameters for intersec-
tion (and optionally a comparison predicate), the
user interface allows browsing the subset of test
items (and associated results) that fail to match
in the selected properties. One dimension that
grammarians found especially useful in intersect-
ing profiles is on the number of readings assigned
per item?detecting where coverage was lost or
added?and on derivation trees (bracketed struc-
tures labeled with rule names and identifiers of lex-
ical items) associated with each parser analysis?
assessing where analyses have changed. Addition-
ally, using a user-supplied equivalence predicate,
the same technique was regularly used at YY to
track the evolution of meaning representations (as
they form the interface from linguistic analysis into
the back-end knowledge processing engine), both
for all readings and the analysis ranked best by the
parse selection heuristics.
Zooming and Interactive Debugging In
analysing a new competence and performance
profile, grammarians typically start from summary
views (overall grammatical coverage, say), then
single out relevant (or suspicious) subsets of
profile data, and often end up zooming in to
the level of individual test items. For most [incr
tsdb()] analysis views the ?success? criteria can be
varied according to user decisions: in assessing
grammatical coverage, for example, the scoring
function can refer to virtually arbitrary profile
elements?ranging from the most basic coverage
measure (assigning at least one reading) to more
refined or application-specific metrics, the produc-
tion of a well-formed meaning representation, say.
Although the general approach allows output an-
notations on the test data (full or partial constituent
structure descriptions, for example), developers so
far have found the incremental, semi-automated
comparison against earlier results a more adequate
means of regression testing. It would appear
that, especially in an application-driven and
tightly scheduled engineering situation like the
DFKI ? YY partnership, the pace of evolution
and general lack of locality in changes (see the
examples discussed in Section 2) precludes the
construction of a static, ?gold-standard? target for
comparison. Instead, the structured comparison
facilities of [incr tsdb()] enable developers to
incrementally approximate target results and, even
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
40
45
50
55
60
65
70
75
80
85
90
95
Grammatical Coverage (Per Cent)
(generated by [incr tsdb()] at 29-jun-2002 (20:49 h))
?
? ???
? ??
?
???
?
?
?
?
?
?
?
?
?
???
??
?
??
?
?
? ?? ??
?
?
?
??
?? ? ?
?
?
?
?
?
?
?
? ???
?
?
????
??
?? ????
?? ?banking?
?? ?trading?
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
0
10
20
30
40
50
60
70
80
90
Ambiguity (Average Number of Analyses)
(generated by [incr tsdb()] at 29-jun-2002 (20:59 h))
?
?
? ? ?
?
?? ?? ? ???? ?? ? ?
?
?
?
?
?
? ?
?
??
??? ?
????
?? ?banking?
?? ?trading?
Figure 2: Evolution of grammatical coverage and average ambiguity (number of readings per test item) over
a five-month period; ?banking? and ?trading? are two data sets (of some 700 and 400 sentences, respectively)
of domain data.
in a highly dynamic environment where grammar
and processing environment evolve in parallel,
track changes and identify regression with great
confidence.
4 Looking Back?Quantifying Evolution
Over time, the [incr tsdb()] profile storage accu-
mulates precise data on the grammar development
process. Figure 2 summarizes two aspects of
grammatical evolution compiled over a five-month
period (and representing some 130 profiles that
grammarians put aside for future reference): gram-
matical coverage over two representative samples
of customer data?one for an on-line banking ap-
plication, the other from an electronic stock trad-
ing domain?is contrasted with the development
of global ambiguity (i.e. the average number of
analyses assinged to each test item). As should
be expected, grammatical coverage on both data
sets increases significantly as grammar develop-
ment focuses on these domains (?banking? for the
first three months, ?trading? from there on). While
the collection of available profiles, apparently, in-
cludes a number of data points corresponding to
?failed? experiments (fairly dramatic losses in cov-
erage), the larger picture shows mostly monotonic
improvement in coverage. As a control experi-
ment, the coverage graph includes another data
point for the ?banking? data towards the end of the
reporting period. Two months of focussed devel-
opment on the ?trading? domain have not nega-
tively affected grammatical coverage on the data
set used earlier. Corresponding to the (desirable)
increase in coverage, the graph on the right of Fig-
ure 2 depicts the evolution of grammatical ambi-
guity. As hand-built linguistic grammars put great
emphasis on the precision of grammatical analy-
sis and the exclusion of ungrammatical input, the
overall average of readings assigned to each sen-
tence varies around relatively small numbers. For
the moderately complex email data2 the grammar
often assigns less than ten analyses, rarely more
than a few dozens. However, not surprisingly
the addition of grammatical coverage comes with
a sharp increase in ambiguity (which may indi-
cate overgeneration): the graphs in Figure 2 clearly
show that, once coverage on the ?trading? data was
above eighty per cent, grammarians shifted their
engineering focus on ?tightening? the grammar, i.e.
the elimination of spurious ambiguity and overgen-
eration (see Siegel & Bender, 2002, for details on
the grammar).
Another view on grammar evolution is pre-
sented in Figure 3, depicting the ?size? of the
Japanese grammar over the same five-month de-
velopment cycle. Although measuring the size of
2Quantifying input complexity for Japanese is a non-
trivial task, as the count of the number of input words would
depend on the approach to string segmentation used in a spe-
cific system (the fairly aggressive tokenizer of ChaSen, Asa-
hara & Matsumoto, 2000, in our case); to avoid potential for
confusion, we report input complexity in the (overtly system-
specific) number of lexical items stipulated by the grammar
instead: around 50 and 80, on average, for the ?banking? and
?trading? data sets, respectively (as of February 2002).
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
8800
9000
9200
9400
9600
9800
10000
10200
88
90
92
94
96
98
100
102
104
106
Grammar Size
(generated by [incr tsdb()] at 30-jun-2002 (16:09 h))????
???
???
????
??
?????
?
?
?
?
??
?
?
? ???????
?????? ?
??























 





 

?? types
? rules
Figure 3: Evolution of grammar size (in the num-
bers of types, plotted against the left axis, and
grammar rules, plotted against the right axis) over
a five-month period.
computational grammars is a difficult challenge,
for the HPSG framework two metrics suggest them-
selves: the number of types (i.e. the size of the
grammatical ontology) and the number of gram-
mar rules (i.e. the inventory of construction types).
As would be expected, both numbers increase
more or less monotonically over the reporting pe-
riod, where the shift of focus from the ?banking?
into the ?trading? domain is marked with a sharp
increase in (primarily lexical) types. Contrasted
to the significant gains in grammatical coverage
(a relative improvement of more than seventy per
cent on the ?banking? data), the increase in gram-
mar size is moderate, though: around fifteen and
twenty per cent in the number of types and rules,
respectively.
5 Conclusions
At YY and cooperating partners (primarily DFKI
Saarbru?cken and CSLI Stanford), grammarians
(for all languages) as well as developers of both the
grammar development tools and of the production
system all used the competence and performance
profiling environment as part of their daily engi-
neering toolbox. The combination of [incr tsdb()]
facilities to parallelize test run processing and a
break-through in client system efficiency (using
the PET parser; Callmeier, 2002) has created an ex-
perimental development environment where gram-
marians can obtain near-instantaneous feedback on
the effects of changes they explore.
For the Japanese grammar specifically, the
grammar developers at both ends would typically
spend the first ten to twenty minutes of the day ob-
taining fresh profiles for a number of shared test
sets and diagnostic corpora, thereby assessing the
most recent set of changes through empirical anal-
ysis of their effects. In conjunction with a certain
rigor in documentation and communication, it was
the ability of both partners to regularly, quickly,
and semi-automatically monitor the evolution of
the joint resource with great confidence that has
enabled truly parallel development of a single,
shared HPSG grammar across continents. Within
a relatively short time, the partners succeeded
in adapting an existing grammar to a new genre
(email rather than spoken language) and domain
(customer service requests rather than appointment
scheduling), greatly extending grammatical cov-
erage (from initially around forty to above ninety
per cent on representative customer corpora), and
incorporating the grammar-based analysis engine
into a commercial product. And even though in
February 2002, for business reasons, YY decided
to reorganize grammar development for Japanese,
the distributed, parallel grammar development ef-
fort positively demonstrates that methodological
and technological advances in constraint-based
grammar engineering have enabled commercial
development and deployment of broad-coverage
HPSG implementations, a paradigm that until re-
cently was often believed to still lack the maturity
for real-world applications.
Acknowledgements
The DFKI ? YY partnership involved a large group
of people at both sites. We would like to thank
Kirk Oatman, co-founder of YY and first CEO,
and Hans Uszkoreit, Scientific Director at DFKI,
for their initiative and whole-hearted support to
the project; it takes vision for both corporate and
academic types to jointly develop an open-source
resource. Atsuko Shimada (from Saarland Uni-
versity), as part of a two-month internship at YY,
has greatly contributed to the preparation of repre-
sentative data samples, development of robust pre-
processing rules, and extensions to lexical cover-
age. Our colleague and friend Asahara-san (of the
Nara Advanced Institute of Technology, Japan),
co-developer of the open-source ChaSen tokenizer
and morphological analyzer for Japanese, was in-
strumental in the integration of ChaSen into the
YY product and also helped a lot in adapting and
(sometimes) fixing tokenization and morphology.
References
Asahara, M., & Matsumoto, Y. (2000). Extended
models and tools for high-performance part-of-
speech tagger. In Proceedings of the 18th In-
ternational Conference on Computational Lin-
guistics (pp. 21 ? 27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stan-
ford, CA: CSLI Publications.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in ef-
ficient grammar-based processing. Stanford,
CA: CSLI Publications. (forthcoming)
Carroll, J. (1994). Relating complexity to practi-
cal performance in parsing with wide-coverage
unification grammars. In Proceedings of the
32nd Meeting of the Association for Computa-
tional Linguistics (pp. 287 ? 294). Las Cruces,
NM.
Copestake, A. (2002). Implementing typed fea-
ture structure grammars. Stanford, CA: CSLI
Publications.
Erbach, G. (1991). An environment for exper-
imenting with parsing strategies. In J. My-
lopoulos & R. Reiter (Eds.), Proceedings of
IJCAI 1991 (pp. 931 ? 937). San Mateo, CA:
Morgan Kaufmann Publishers.
Erbach, G., & Uszkoreit, H. (1990). Grammar
engineering. Problems and prospects (CLAUS
Report # 1). Saarbru?cken, Germany: Compu-
tational Linguistics, Saarland University.
Flickinger, D., Copestake, A., & Sag, I. A. (2000).
HPSG analysis of English. In W. Wahlster
(Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed.,
pp. 321 ? 330). Berlin, Germany: Springer.
Geist, A., Bequelin, A., Dongarra, J., Manchek, W.
J. R., & Sunderam, V. (Eds.). (1994). PVM ?
parallel virtual machine. A users? guide and tu-
torial for networked parallel computing. Cam-
bridge, MA: The MIT Press.
Makino, T., Yoshida, M., Torisawa, K., & Tsu-
jii, J. (1998). LiLFeS ? towards a practical
HPSG parser. In Proceedings of the 17th In-
ternational Conference on Computational Lin-
guistics and the 36th Annual Meeting of the
Association for Computational Linguistics (pp.
807 ? 11). Montreal, Canada.
Mu?ller, S., & Kasper, W. (2000). HPSG analy-
sis of German. In W. Wahlster (Ed.), Verbmo-
bil. Foundations of speech-to-speech transla-
tion (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Callmeier, U. (2000). Measure for
measure: Parser cross-fertilization. Towards
increased component comparability and ex-
change. In Proceedings of the 6th International
Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards
systematic grammar profiling. Test suite tech-
nology ten years after. Journal of Computer
Speech and Language, 12 (4) (Special Issue on
Evaluation), 411 ? 436.
Oepen, S., Netter, K., & Klein, J. (1997). TSNLP
? Test Suites for Natural Language Process-
ing. In J. Nerbonne (Ed.), Linguistic Databases
(pp. 13 ? 36). Stanford, CA: CSLI Publica-
tions.
Penn, G. (2000). Applying constraint handling
rules to HPSG. In Proceedings of the first in-
ternational conference on computational logic
(pp. 51 ? 68). London, UK.
Siegel, M. (2000). HPSG analysis of Japanese. In
W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelli-
gence ed., pp. 265 ? 280). Berlin, Germany:
Springer.
Siegel, M., & Bender, E. M. (2002). Efficient
deep processing of japanese. In Proceedings
of the 19th International Conference on Com-
putational Linguistics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Di-
agne, A. K., Hinkelman, E. A., Kasper, W.,
Kiefer, B., Krieger, H.-U., Netter, K., Neu-
mann, G., Oepen, S., & Spackman, S. P.
(1994). DISCO ? an HPSG-based NLP
system and its application for appointment
scheduling. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics. Kyoto, Japan.
Rapid Prototyping of Scalable Grammars: Towards Modularity in
Extensions to a Language-Independent Core
Emily M. Bender
Department of Linguistics
University of Washington
Box 354340
Seattle WA 98195-4340 USA
ebender@u.washington.edu
Dan Flickinger
Center for the Study of Language and Information
Stanford University
Stanford CA 94305-2150 USA
danf@csli.stanford.edu
Abstract
We present a new way to simplify the
construction of precise broad-coverage
grammars, employing typologically-
motivated, customizable extensions to
a language-independent core grammar.
Each ?module? represents a salient di-
mension of cross-linguistic variation,
and presents the grammar developer
with simple choices that result in auto-
matically generated language-specific
software. We illustrate the approach for
several phenomena and explore the in-
terdependence of the modules.
1 Introduction
Manual development of precise broad-coverage
grammar implementations, useful in a range of
natural language processing/understanding tasks,
is a labor-intensive undertaking, requiring many
years of work by highly trained linguists. Many
recent efforts toward reducing the time and level
of expertise needed to produce a new grammar
have focused on adapting an existing grammar of
another language (Butt et al, 2002; Kim et al,
2003; Bateman et al, ip). Our work on the ?Gram-
mar Matrix? has pursued an alternative approach,
identifying a set of language-independent gram-
mar constraints to which language-specific con-
straints can be added (Bender et al, 2002). This
approach has the hitherto unexploited potential
to benefit from the substantial theoretical work
on language typology. In this paper, we present
a prototype Grammar Matrix customization sys-
tem. This system draws on phenomenon-specific
modules encoding dimensions of linguistic varia-
tion, presents the grammar developer with simple
choices for each phenomenon, and then automati-
cally generates a working starter-grammar, incor-
porating both the cross-linguistic Matrix core and
language-specific constraints. The prototype ad-
dresses basic word order, sentential negation, yes-
no questions, and a small range of lexical entries.
2 The Grammar Matrix
Wide-coverage grammars representing deep lin-
guistic analysis exist in several frameworks, in-
cluding Head-Driven Phrase Structure Grammar
(HPSG), Lexical-Functional Grammar, and Lex-
icalized Tree Adjoining Grammar. In HPSG (P.
and Sag, 1994), the most extensive grammars
are those of English (Flickinger, 2000), German
(Hinrichs et al, 1997; Mu?ller and Kasper, 2000;
Crysmann, ip), and Japanese (Siegel, 2000; Siegel
and Bender, 2002). The Grammar Matrix is an at-
tempt to distill the wisdom of existing grammars
and document it in a form that can be used as the
basis for new grammars. The main goals of the
project are: (i) to develop in detail semantic rep-
resentations and the syntax-semantics interface,
consistent with other work in HPSG; (ii) to repre-
sent generalizations across linguistic objects and
across languages; and (iii) to allow for very quick
start-up as the Matrix is applied to new languages.
The original Grammar Matrix consisted of
types defining the basic feature geometry, types
associated with Minimal Recursion Semantics
(e.g., (Copestake et al, 2001)), types for lex-
203
ical and syntactic rules, and configuration files
for the LKB grammar development environment
(Copestake, 2002) and the PET system (Callmeier,
2000). Subsequent releases have refined the orig-
inal types and developed a lexical hierarchy. The
constraints in this ?core? Matrix are intended to be
language-independent and monotonically exten-
sible in any given grammar. With the typology-
based modules presented here, we extend the con-
straint definitions which can be supplied to gram-
mar developers to those that capture generaliza-
tions holding only for subsets of languages.
3 Typology-based modules
In general, we find two kinds of typological vari-
ation across languages. On the one hand, there
are systems (formal or functional) which must be
represented in every language. For example, ev-
ery language has some set of permissible word or-
ders (formal) and a means of expressing sentential
negation (functional). On the other hand, there
are linguistic phenomena which appear in only
some languages, and are not typically conceptual-
ized as alternative realizations of some universal
function, phenomena such as noun incorporation,
numeral classifiers, and auxiliary verbs. Each of
these phenomena are found in recurring varieties
that can be subjected to typological analysis (see,
e.g., (Mithun, 1984)). Our approach is designed
to handle both kinds of typological variation.
As with earlier versions of the Matrix, we aim
to support rapid prototyping of precision gram-
mars that can scale up to broad-coverage (as have
the NorSource (Hellan and Haugereid, 2003) and
Modern Greek (Kordoni and Neu, 2003) gram-
mars, based on early versions of the Matrix). This
sets a high bar for the modules themselves, requir-
ing them to be good early approximations which
may need to be refined but not thrown out. It also
requires that the automatically generated gram-
mar files maintain a high degree of readability so
that they may be effectively modified. In future
work, we intend to extend the system to allow the
linguist to revise decisions in the face of new in-
formation or improved linguistic analyses.
The core Matrix and modular extensions to it
may appear analogous to the Principles and Pa-
rameters proposed by Chomsky (1981) and oth-
ers. However, whereas Parameters are meant to
be abstract ?switches? which simultaneously con-
trol multiple different, apparently unrelated phe-
nomena, the modules in the Matrix each encode
the constraints necessary to handle one particu-
lar phenomenon. Nonetheless, this does not make
the modules trivial: they need to be carefully de-
signed in order to be mutually consistent, ide-
ally across all possible combinations. Our strat-
egy is thus consistent with a bottom-up, data-
driven investigation of linguistic universals and
constraints on cross-linguistic variation. As the
number and breadth of implemented grammars
grows, we expect linguistic predictions to emerge
and become part of improved modules, particu-
larly with respect to interactions among the dis-
tinct phenomena covered. Our approach should in
time be instrumental in assisting large-scale typo-
logical investigations (covering hundreds of lan-
guages), making use of the linguistically precise
constraints encoded in these modules to uncover
deeper and more subtle facts about languages.
4 Implementations of prototype system
We have implemented a prototype system with
a small set of modules targeting basic word or-
der, main-clause yes-no questions, and senten-
tial negation.1 The corresponding choices and
a questionnaire for creating a small lexicon are
presented to the user through an html form inter-
face. A perl/cgi back-end produces a starter gram-
mar from the user input and an internal knowl-
edge base. The resulting grammars can be used
immediately to parse and generate a fragment of
the target language. The system can be accessed
at http://www.delph-in.net/matrix/modules.html.
This section describes its linguistic coverage.
4.1 Word order
The word order module addresses the so-called
basic word order in a language: the relative or-
der of subjects, verbs, and verbal complements.
Languages vary in their rigidity in this respect,
and the question of how to determine the basic
word-order of a language is notoriously complex.
Nonetheless, we believe that most linguists work-
ing on linguistic description analyze some orders
as primary and others as derived. Thus the word
1Drellishak and Bender (ta) present a module for coordi-
nation which is integrated with those described here.
204
order module is meant to capture the relative or-
dering of major constituents in clauses without
word-order changing phenomena such as topical-
ization, extraposition, subject-verb inversion, etc.
Modules for such phenomena will need to interact
appropriately with the basic word-order module.
The Matrix core grammar provides defini-
tions of basic head-complement and head-subject
schemata which are consistent with our imple-
mentation of compositional semantics (Flickinger
and Bender, 2003), as well as definitions of head-
initial and head-final phrase types. The word
order module creates subtypes joining the head-
complement and head-subject schemata with the
types specifying head/dependent order, creates in-
stances of those types as required by the LKB
parser, and constrains the rules to eliminate spu-
rious ambiguity in the case of free word order. It
currently handles SOV, SVO, VSO, VOS, OVS,
OSV, V-final, V-initial, and free word order. We
leave to future work variations such as V2 or-
der, differing word order in main v. subordinate
clauses, and flexible ordering among comple-
ments in otherwise strict word order languages.
4.2 Yes-no questions
For yes-no questions, we implement four alterna-
tives: inversion of the subject and a main or aux-
iliary verb relative to declarative word order and
sentence-initial or final question particles.
Inversion of the subject and the main verb is
implemented with a lexical rule which relocates
the subject (the value of SUBJ in the valence spec-
ifications) to be the first on the COMPS list, and
further assigns a positive value for an additional
feature INV (inverted) on verbs. This feature may
well have independent syntactic motivation in the
language, but is in any case used here so the
declarative/interrogative distinction can be made
in the semantics once the clause is constructed.
Subject-aux inversion is a minor extension of the
basic inversion type, constraining the lexical rule
to only apply to auxiliary verbs. This module han-
dles ?support? verbs like do in English in not li-
censing inversion with main verbs, while licens-
ing similar strings with a semantically empty sup-
port verb (if it is in the lexicon). The third type of
mechanism employs a distinct question particle,
here treated as a pre- or post-modifying sentence
adverb. The grammar developer is prompted for
this positional distinction, and for the spelling of
the particle; the code for the relevant lexical en-
try is then autogenerated, instantiating a question
particle type which supplies the remaining syn-
tactic and semantic constraints needed.
Future work on this module includes support
for ?intonation questions?, where the same string
can be associated with either proposition or ques-
tion semantics, as well as the integration of
declarative/interrogative punctuation contrasts.
4.3 Sentential negation
The sentential negation module handles two gen-
eral negation strategies, several variants on each,
and allows for both to coexist in a single grammar.
The first strategy is negation via verbal inflec-
tion. For this strategy, the grammar developer
specifies whether the inflection attaches to main
verbs, auxiliaries, or either; whether it is a prefix
or a suffix; and the form of the affix. We cur-
rently only allow for strictly concatenative mor-
phology. In a more fully developed system, the
syntax-semantics modules here would be inter-
faced with a separate means of specifying mor-
phophonology (cf. (Bender and Good, ip)).
The second strategy is negation via a negative
adverb, with two sub-cases: The negative adverb
may be an independent modifier (of V, VP, or S
and pre- or post-head) or it may be a selected
complement of the verb (main verbs only, aux-
iliaries only, or both) (Kim, 2000). The grammar
developer specifies the form of the adverb.
Neither, either or both of these strategies may
be selected. If neither, the grammar produced will
not contain an analysis of negation. If both, the
grammar developer must specify how the strate-
gies interact, from among five choices: (i) the two
strategies are in complementary distribution, (ii)
the two strategies can appear independently or to-
gether, (iii) both inflection and an adverb are re-
quired to express sentential negation, (iv) the ad-
verb is obligatory, but it may appear with or with-
out the inflection, and (v) the inflection is obliga-
tory, but it may appear with or without the adverb.
In the generated grammars, independent ad-
verbs are implemented by adding appropriate lex-
ical types and lexical entries. Selected adverbs
and inflection are handled via lexical rules similar
205
to those presented in (Sag et al, 2003). For exam-
ple, in a language where sentential negation can
be expressed by inflection alone or inflection in
combination with a (selected) adverb, we gener-
ate two lexical rules. One changes the form of the
verb and adds the negative semantics. The other
changes the form of the verb and adds the nega-
tive adverb to its complements list.
4.4 Lexicon
As HPSG is a strongly lexicalist theory, words
tend to carry quite a bit of information. This
information is encoded in lexical types; lexical
entries merely specify the type they instantiate,
their orthographic form, and their semantic predi-
cate. Many of the constraints required (e.g., for
the linking of syntactic to semantic arguments)
are already provided by the core Matrix. How-
ever, there is also cross-linguistic variation.
We ask the grammar developer to specify two
nouns and two verbs (one transitive and one in-
transitive), as well as an auxiliary, two deter-
miners, two case-marking adpositions, a nega-
tive adverb and a question particle, if appropriate.
Nouns are specified as to whether they require,
allow, or disallow determiners. Verbs are speci-
fied as to whether each argument is expressed as
an NP or a PP, and optionally for an additional
(non-finite) form. Auxiliaries are specified as to
whether they introduce independent predicates or
only carry tense/aspect; take S, VP or V com-
plements; appear to the left, right or either side
of their complements; and take NP or PP sub-
jects. Case-marking adpositions must be specified
as either prepositions or postpositions. Finally,
the questionnaire requires orthographic forms and
predicate names. Note that the forms are assumed
to be fully inflected (modulo negation), support
morphological processes awaiting future work.
We use this information and the knowledge
base to produce a set of lexical types inherit-
ing from the types defined in the core Matrix
and specifying appropriate language-specific con-
straints, and a set of lexical entries.
5 Limits of modularity
Recent computational work in HPSG has asked
whether different parts of a single grammar can
be abstracted into separate, independent mod-
ules, either for processing (Kasper and Krieger,
1996; Theofilidis et al, 1997) or grammar devel-
opment (Kes?elj, 2001). Our work is most simi-
lar to Kes?elj?s though we are pursuing different
goals: Kes?elj is looking to support a division of
labor among multiple individuals working on the
same grammar and to support variants of a single
grammar for different domains. His modules each
have private and public features and types, and he
illustrates the approach with a small-scale ques-
tion answering system. In contrast, we are ap-
proaching this issue from the perspective of reuse
of grammar code in the context of multilingual
grammar engineering (a possibility suggested, but
not developed, by Theofilidis et al.
Our notion of modularity is influenced by the
following constraints: (i) The questions in the
customization interface must be sensible to the
working linguist; (ii) The resulting starter gram-
mars must be highly readable so that they can
be extended by the grammar developer (typically
only one per grammar); and (iii) HPSG prac-
tice values capturing linguistic generalizations by
having single types encode many different con-
straints and, ideally, single constraints contribute
to the analysis of many different phenomena.
Even with the modest linguistic coverage of
the existing system, we have found many cases
of non-trivial interaction between the modules:
Our phrase structure rules, following HPSG prac-
tice, capture cross-categorial generalizations: if
both verbs and adpositions follow their comple-
ments, then a single complement-head rule serves
for both. However, few languages (if any) are
completely consistent in their ordering of heads
and dependents. Thus, before defining the types
and instances for these rules, we must determine
whether the fragment requires auxiliaries (for
negation or yes-no questions) or case-marking ad-
positions, and whether their order with respect to
their complements is consistent with that of main
verbs. A second example is the lexical type for
main verbs, whose definition depends on whether
the language has auxiliaries (requiring a feature
AUX distinguishing the two kinds of verbs and
a feature FORM governing the distribution of fi-
nite and non-finite verbs). As a third example, the
negation and question modules each have options
requiring auxiliaries, but we must posit the asso-
206
ciated types and constraints at most once.
Thus we find that, for our purposes, the relevant
notion of modularity is modularity from the point
of view of the linguist who uses the system to cre-
ate a starter grammar. To support this, we strive to
make the questions we ask of the linguist be as in-
dependent of each other as possible, and to make
it clear when one particular choice (e.g., negation
as inflection) requires further information (suffix
v. prefix). The fact that the questions we present
to the linguist don?t correspond to neatly isolated
parts of the underlying knowledge base is not a
failure of the approach, but rather a reflection of
the complexity of language. The very intercon-
nectedness of grammatical phenomena is at the
heart of research in theoretical syntax. We in-
tend our system to provide a data-driven cross-
linguistic exploration of that interconnection.
6 Validation of prototype system
To verify the mutual consistency of the mod-
ules developed so far and to illustrate their ap-
plicability to a interesting range of languages,
we developed abstract test suites for seven lan-
guages. This convenience sample of languages is
not representative, either typologically or geneti-
cally. The grammatical and ungrammatical exam-
ples in each test suite use a small, artificial lexi-
con, and reflect the typological properties of each
language along the dimensions of basic word or-
der, sentential negation, and yes-no questions (Ta-
ble 1). Table 2 presents the performance of each
grammar (as generated by our prototype system
with appropriate input) on its associated test suite.
Language2 Order Negation Yes-no Q3
English SVO aux-selected adv aux inv
Hindi SOV pre-V adv S-init part.
Japanese V-final verbal suffix S-final part
Mandarin SVO pre-V adv S-final part,
A-not-A
Polish free pre-V adv S-init part
Slave SOV post-V adv S-init part
Spanish SVO pre-V adv main V inv
Table 1: Languages used in testing
While these test suites are quite modest, we be-
lieve they show that the prototype system is able
2Sources: Hindi: Snell and Weightman, 2000, Mandarin:
Li and Thompson, 1981, Polish: Adam Przepio?rkowski, p.c.,
Slave (Athabaskan): Rice, 1989
3In addition to intonation questions, if any.
Language Pos. Coverage Neg. Overgen.
English 5 100% 10 10%
Hindi 5 100% 10 0%
Japanese 6 100% 10 0%
Mandarin 4 75% 9 0%
Polish 14 100% 8 0%
Slave 3 100% 6 0%
Spanish 5 100% 7 0%
Table 2: Parsing evaluation results
to produce good first-pass grammar fragments for
an interesting variety of languages. More study is
needed to develop a means of testing the cross-
compatibility of all choices on all modules, to
evaluate the coverage against a typologically jus-
tified sample, and to gauge the success of this
strategy in producing grammars which are com-
prehensible to beginning grammar developers.
7 Conclusion and outlook
We have described a method for extending
a language-independent core grammar like the
Grammar Matrix with modules handling cross-
linguistically variable but still recurring patterns.
This method allows for extremely rapid prototyp-
ing of deep precision grammars in such a way
that the prototypes themselves can serve as the
basis for sustained development. We envision at
least four potential uses for this kind of grammar
prototyping: (i) in pedagogical contexts, where
it would allow grammar engineering students to
more quickly work on cutting-edge problems, (ii)
in language documentation, where a documen-
tary linguist in the field might be collaborating
remotely with a grammar engineer to propose and
test hypotheses, (iii) in leveraging the results from
economically powerful languages to reduce the
cost of creating resources for minority languages,
and (iv) in supporting typological or comparative
studies of linguistic phenomena or interactions
between phenomena across languages.
Acknowledgments
We thank Scott Drellishak, Stephan Oepen, Lau-
rie Poulson, and the 2004 and 2005 multilingual
grammar engineering classes at the University of
Washington for valuable input and NTT Com-
munication Science Laboratories for their support
through a grant to CSLI (Stanford). All remaining
errors are our own.
207
References
J.A. Bateman, I. Kruijff-Korbayova?, and G.-J. Krui-
jff. ip. Multilingual resource sharing across both
related and unrelated languages: an implemented,
open-source framework for practical natural lan-
guage generation. Res. on Lang. and Computation.
E.M. Bender and J. Good. ip. Implementation for
discovery: A bipartite lexicon to support morpho-
logical and syntactic analysis. In CLS 41.
E.M. Bender, D. Flickinger, and S. Oepen. 2002.
The grammar matrix. COLING 2002 Workshop on
Grammar Engineering and Evaluation.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
COLING 2002 Workshop on Grammar Engineering
and Evaluation.
U. Callmeier. 2000. PET ? A platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Lang. Engineering, 6 (1):99 ? 108.
N. Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
A. Copestake, A. Lascarides, and D. Flickinger. 2001.
An algebra for semantic construction in constraint-
based grammars. In ACL 2001.
A. Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford, CA.
B. Crysmann. ip. Relative clause extraposition in ger-
man: An efficient and portable implementation. Re-
search on Lang. and Computation.
S. Drellishak and E.M. Bender. ta. Coordination
modules for a crosslinguistic grammar resource. In
Proc. of HPSG 2005.
D. Flickinger and E.M. Bender. 2003. Compositional
semantics in a multilingual grammar resource. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 33?42.
D. Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Lang. En-
gineering, 6 (1):15 ? 28.
L. Hellan and P. Haugereid. 2003. Norsource: An
exercise in matrix grammar-building design. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 41?48.
W.D. Hinrichs, E.and Meurers, F. Richter, M. Sailer,
and H. Winhart. 1997. Ein HPSG-Fragement des
Deutschen. Arbeitspapiere des Sonderforschungs-
bereichs 340, Bericht Nr. 95.
W. Kasper and H.-U. Krieger. 1996. Modularizing
codescriptive grammars for efficient parsing. In
COLING 1996, pages 628?633.
V. Kes?elj. 2001. Modular HPSG. Technical Report
CS-2001-05, Department of Computer Science,
University of Waterloo, Waterloo, Ont., Canada.
R. Kim, M. Dalrymple, R.M. Kaplan, T.H. King,
H. Masuichi, and T. Ohkuma. 2003. Multlingual
grammar development via grammar porting. In
Proc. of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 49?56.
J. Kim. 2000. The Grammar of Negation: A
Constraint-Based Approach. CSLI, Stanford, CA.
V. Kordoni and J. Neu. 2003. Deep gramamr develop-
ment for Modern Greek. In Proc. of the Workshop
on Ideas and Strategies for Multilingual Grammar
Development, ESSLLI 2003, pages 65?72.
C.N. Li and S.A. Thompson. 1981. Mandarin Chi-
nese: A Functional Reference Grammar. Univer-
sity of California Press, Berkeley, CA.
M. Mithun. 1984. The evolution of noun incorpora-
tion. Language, 60(4):847?894.
S. Mu?ller and W. Kasper. 2000. HPSG analy-
sis of German. In W. Wahlster, editor, Verbmo-
bil. Foundations of Speech-to-Speech Translation,
pages 238 ? 253. Springer, Berlin, Germany.
Carl P. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The Univeristy of Chicago
Press, Chicago, IL.
K. Rice. 1989. A Grammar of Slave. Mouton de
Gruyter, Berlin.
I.A. Sag, T. Wasow, and E.M. Bender. 2003. Synactic
Theory: A Formal Introduction. CSLI, Stanford,
CA, 2nd edition.
M. Siegel and E.M. Bender. 2002. Efficient deep
processing of Japanese. In Proc. of the 3rd Work-
shop on Asian Language Resources and Interna-
tional Standardization at COLING 2002.
M. Siegel. 2000. HPSG analysis of Japanese.
In W. Wahlster, editor, Verbmobil. Foundations
of Speech-to-Speech Translation, pages 265 ? 280.
Springer, Berlin, Germany.
R. Snell and S. Weightman. 2000. Hindi. Teach Your-
self Books.
A. Theofilidis, P. Schmidt, and T. Declerck. 1997.
Grammar modularization for efficient processing:
Language engineering devices and their instantia-
tions. In Proc. of the DGFS/CL.
208
Proceedings of ACL-08: HLT, pages 977?985,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating a Crosslinguistic Grammar Resource:
A Case Study of Wambaya
Emily M. Bender
University of Washington
Department of Linguistics
Box 354340
Seattle WA 98195-4340
ebender@u.washington.edu
Abstract
This paper evaluates the LinGO Grammar Ma-
trix, a cross-linguistic resource for the de-
velopment of precision broad coverage gram-
mars, by applying it to the Australian language
Wambaya. Despite large typological differ-
ences between Wambaya and the languages
on which the development of the resource was
based, the Grammar Matrix is found to pro-
vide a significant jump-start in the creation of
the grammar for Wambaya: With less than 5.5
person-weeks of development, the Wambaya
grammar was able to assign correct seman-
tic representations to 76% of the sentences in
a naturally occurring text. While the work
on Wambaya identified some areas of refine-
ment for the Grammar Matrix, 59% of the
Matrix-provided types were invoked in the fi-
nal Wambaya grammar, and only 4% of the
Matrix-provided types required modification.
1 Introduction
Hand-built grammars are often dismissed as too ex-
pensive to build on the one hand, and too brittle
on the other. Nevertheless, they are key to various
NLP applications, including those benefiting from
deep natural language understanding (e.g., textual
inference (Bobrow et al, 2007)), generation of well-
formed output (e.g., natural language weather alert
systems (Lareau and Wanner, 2007)) or both (as in
machine translation (Oepen et al, 2007)). Of par-
ticular interest here are applications concerning en-
dangered languages: Endangered languages repre-
sent a case of minimal linguistic resources, typically
lacking even moderately-sized corpora, let alne
treebanks. In the best case, one finds well-crafted
descriptive grammars, bilingual dictionaries, and a
handful of translated texts. The methods of pre-
cision grammar engineering are well-suited to tak-
ing advantage of such resources. At the same time,
the applications of interest in the context of endan-
gered languages emphasize linguistic precision: im-
plemented grammars can be used to enrich existing
linguistic documentation, to build grammar check-
ers in the context of language standardization, and
to create software language tutors in the context of
language preservation efforts.
The LinGO Grammar Matrix (Bender et al, 2002;
Bender and Flickinger, 2005; Drellishak and Ben-
der, 2005) is a toolkit for reducing the cost of creat-
ing broad-coverage precision grammars by prepack-
aging both a cross-linguistic core grammar and a
series of libraries of analyses of cross-linguistically
variable phenomena, such as major-constituent word
order or question formation. The Grammar Ma-
trix was developed initially on the basis of broad-
coverage grammars for English (Flickinger, 2000)
and Japanese (Siegel and Bender, 2002), and has
since been extended and refined as it has been used
in the development of broad-coverage grammars for
Norwegian (Hellan and Haugereid, 2003), Modern
Greek (Kordoni and Neu, 2005), and Spanish (Ma-
rimon et al, 2007), as well as being applied to 42
other languages from a variety of language families
in a classroom context (Bender, 2007).
This paper aims to evaluate both the utility of the
Grammar Matrix in jump-starting precision gram-
mar development and the current state of its cross-
linguistic hypotheses through a case study of a
977
language typologically very different from any of
the languages above: the non-Pama-Nyungan Aus-
tralian language Wambaya (Nordlinger, 1998).
The remainder of this paper is structured as fol-
lows: ?2 provides background on the Grammar Ma-
trix and Wambaya, and situates the project with re-
spect to related work. ?3 presents the implemented
grammar of Wambaya, describes its development,
and evaluates it against unseen, naturally occurring
text. ?4 uses the Wambaya grammar and its devel-
opment as one point of reference to measure the use-
fulness and cross-linguistic validity of the Grammar
Matrix. ?5 provides further discussion.
2 Background
2.1 The LinGO Grammar Matrix
The LinGO Grammar Matrix is situated theoreti-
cally within Head-Driven Phrase Structure Gram-
mar (HPSG; Pollard and Sag, 1994), a lexicalist,
constraint-based framework. Grammars in HPSG
are expressed as a collection of typed feature struc-
tures which are arranged into a hierarchy such that
information shared across multiple lexical entries or
construction types is represented only on a single su-
pertype. The Matrix is written in the TDL (type de-
scription language) formalism, which is interpreted
by the LKB parser, generator, and grammar develop-
ment environment (Copestake, 2002). It is compati-
ble with the broader range of DELPH-IN tools, e.g.,
for machine translation (L?nning and Oepen, 2006),
treebanking (Oepen et al, 2004) and parse selection
(Toutanova et al, 2005).
The Grammar Matrix consists of a cross-
linguistic core type hierarchy and a collection of
phenomenon-specific libraries. The core type hierar-
chy defines the basic feature geometry, the ways that
heads combine with arguments and adjuncts, linking
types for relating syntactic to semantic arguments,
and the constraints required to compositionally build
up semantic representations in the format of Min-
imal Recursion Semantics (Copestake et al, 2005;
Flickinger and Bender, 2003). The libraries provide
collections of analyses for cross-linguistically vari-
able phenomena. The current libraries include anal-
yses of major constituent word order (SOV, SVO,
etc), sentential negation, coordination, and yes-no
question formation. The Matrix is accessed through
a web-based configuration system1 which elicits ty-
pological information from the user-linguist through
a questionnaire and then outputs a grammar consist-
ing of the Matrix core plus selected types and con-
straints from the libraries according to the specifica-
tions in the questionnaire.
2.2 Wambaya
Wambaya is a recently extinct language of the West
Barkly family from the Northern Territory in Aus-
tralia (Nordlinger, 1998). Wambaya was selected
for this project because of its typological properties
and because it is extraordinarily well-documented
by Nordlinger in her 1998 descriptive grammar.
Perhaps the most striking feature of Wambaya is
its word order: it is a radically non-configurational
language with a second position auxiliary/clitic clus-
ter. That is, aside from the constraint that verbal
clauses require a clitic cluster (marking subject and
object agreement and tense, aspect and mood) in
second position, the word order is otherwise free, to
the point that noun phrases can be non-contiguous,
with head nouns and their modifiers separated by un-
related words. Furthermore, head nouns are gener-
ally not required: argument positions can be instan-
tiated by modifiers only, or, if the referent is clear
from the context, by no nominal constituent of any
kind. It has a rich system of case marking, and ad-
nominal modifiers agree with the heads they modify
in case, number, and four genders. An example is
given in (1) (Nordlinger, 1998, 223).2
(1) Ngaragana-nguja
grog-PROP.IV.ACC
ngiy-a
3.SG.NM.A-PST
gujinganjanga-ni
mother.II.ERG
jiyawu
give
ngabulu.
milk.IV.ACC
?(His) mother gave (him) milk with grog in
it.? [wmb]
In (1), ngaragana-nguja (?grog-proprietive?, or
?having grog?) is a modifier of ngabulu milk. They
agree in case (accusative) and gender (class IV), but
they are not contiguous within the sentence.
To relate such discontinuous noun phrases to ap-
propriate semantic representations where ?having-
1http://www.delph-in.net/matrix/customize/matrix.cgi
2In this example, the glosses II, IV, and NM indicate gender
and ACC and ERG indicate case. A stands for ?agent?, PST for
?past?, and PROP for ?proprietive?.
978
grog? and ?milk? are predicated of the same entity re-
quires a departure from the ordinary way that heads
are combined with arguments and modifiers com-
bined with heads in HPSG in general and in the
Matrix in particular.3 In the Grammar Matrix, as
in most work in HPSG, lexical heads record the de-
pendents they require in valence lists (SUBJ, COMPS,
SPR). When a head combines with one of its ar-
guments, the result is a phrase with the same va-
lence requirements as the head daughter, minus the
one corresponding to the argument that was just sat-
isfied. In contrast, the project described here has
explored a non-cancellation analysis for Wambaya:
even after a head combines with one of its argu-
ments, that argument remains on the appropriate va-
lence list of the mother, so that it is visible for further
combination with modifiers. In addition, heads can
combine directly with modifiers of their arguments
(as opposed to just modifiers of themselves).
Argument realization and the combination of
heads and modifiers are fairly fundamental aspects
of the system implemented in the Matrix. In light
of the departure described above, it is interesting to
see to what extent the Matrix can still support rapid
development of a precision grammar for Wambaya.
2.3 Related Work
There are currently many multilingual grammar en-
gineering projects under active development, in-
cluding ParGram, (Butt et al, 2002; King et al,
2005), the MetaGrammar project (Kinyon et al,
2006), KPML (Bateman et al, 2005), Grammix
(Mu?ller, 2007) and OpenCCG (Baldridge et al,
2007). Among approaches to multilingual grammar
engineering, the Grammar Matrix?s distinguishing
characteristics include the deployment of a shared
core grammar for crosslinguistically consistent con-
straints and a series of libraries modeling vary-
ing linguistic properties. Thus while other work
has successfully exploited grammar porting between
typologically related languages (e.g., Kim et al,
2003), to my knowledge, no other grammar port-
ing project has covered the same typological dis-
3A linearization-based analysis as suggested by Donohue
and Sag (1999) for discontinuous constituents in Warlpiri (an-
other Australian language), is not available, because it relies on
disassociating the constituent structure from the surface order of
words in a way that is not compatible with the TDL formalism.
tance attempted here. The current project is also
situated within a broader trend of using computa-
tional linguistics in the service of endangered lan-
guage documentation (e.g., Robinson et al, 2007,
see also www.emeld.org).
3 Wambaya grammar
3.1 Development
The Wambaya grammar was developed on the basis
of the grammatical description in Nordlinger 1998,
including the Wambaya-English translation lexicon
and glosses of individual example sentences. The
development test suite consisted of all 794 distinct
positive examples from Ch. 3?8 of the descriptive
grammar. This includes elicited examples as well
as (sometimes simplified) naturally occurring exam-
ples. They range in length from one to thirteen
words (mean: 3.65). The test suite was extracted
from the descriptive grammar at the beginning of the
project and used throughout with only minor refine-
ments as errors in formatting were discovered. The
regression testing facilities of [incr tsdb()] allowed
for rapid experimentation with alternative analyses
as new phenomena were brought into the grammar
(cf. Oepen et al, 2002).
With no prior knowledge of this language beyond
its most general typological properties, we were able
to develop in under 5.5 person-weeks of develop-
ment time (210 hours) a grammar able to assign ap-
propriate analyses to 91% of the examples in the de-
velopment set.4 The 210 hours include 25 hours of
an RA?s time entering lexical entries, 7 hours spent
preparing the development test suite, and 15 hours
treebanking (using the LinGO Redwoods software
(Oepen et al, 2004) to annotate the intended parse
for each item). The remainder of the time was ordi-
nary grammar development work.5
In addition, this grammar has relatively low am-
biguity, assigning on average 11.89 parses per item
in the development set. This reflects the fact that the
grammar is modeling grammaticality: the rules are
4An additional 6% received some analysis, but not one that
matched the translation given in the reference grammar.
5These numbers do not include the time put into the origi-
nal field work and descriptive grammar work. Nordlinger (p.c.)
estimates that as roughly 28 linguist-months, plus the native
speaker consultants? time.
979
meant to exclude ungrammatical strings as well as
are unwarranted analyses of grammatical strings.
3.2 Scope
The grammar encodes mutually interoperable anal-
yses of a wide variety of linguistic phenomena, in-
cluding:
? Word order: second position clitic cluster, other-
wise free word order, discontinuous noun phrases
? Argument optionality: argument positions with no
overt head
? Linking of syntactic to semantic arguments
? Case: case assignment by verbs to dependents
? Agreement: subject and object agreement in per-
son and number (and to some extent gender) marked
in the clitic cluster, agreement between nouns and
adnominal modifiers in case, number and gender
? Lexical adverbs, including manner, time, and loca-
tion, and adverbs of negation, which vary by clause
type (declarative, imperative, or interrogative)
? Derived event modifiers: nominals (nouns, adjec-
tives, noun phrases) used as event modifiers with
meaning dependent on their case marking
? Lexical adjectives, including demonstratives ad-
verbs, numerals, and possessive adjectives, as well
as ordinary intersective adjectives
? Derived nominal modifiers: modifiers of nouns de-
rived from nouns, adjectives and verbs, including the
proprietive, privative, and ?origin? constructions
? Subordinate clauses: clausal complements of
verbs like ?tell? and ?remember?, non-finite subor-
dinate clauses such as purposives (?in order to?) and
clauses expressing prior or simultaneous events
? Verbless clauses: nouns, adjectives, and adverbs,
lexical or derived, functioning as predicates
? Illocutionary force: imperatives, declaratives, and
interrogatives (including wh questions)
? Coordination: of clauses and noun phrases
? Other: inalienable possession, secondary predi-
cates, causatives of verbs and adjectives
3.3 Sample Analysis
This section provides a brief description of the anal-
ysis of radical non-configurationality in order to
give a sense of the linguistic detail encoded in the
Wambaya grammar and give context for the evalu-
ation of the Wambaya grammar and the Grammar
Matrix in later sections.
The linguistic analyses encoded in the grammar
serve to map the surface strings to semantic repre-
sentations (in Minimal Recursion Semantics (MRS)
format (Copestake et al, 2005)). The MRS in Fig-
ure 1 is assigned to the example in (1).6 It in-
cludes the basic propositional structure: a situation
of ?giving? in which the first argument, or agent, is
?mother?, the second (recipient) is some third-person
entity, and the third (patient), is ?milk? which is also
related to ?grog? through the proprietive relation. It
is marked as past tense, and as potentially a state-
ment or a question, depending on the intonation.7,8
A simple tree display of the parse giving rise to
this MRS is given in Figure 2. The non-branching
nodes at the bottom of the tree represent the lexical
rules which associate morphosyntactic information
with a word according to its suffixes. The general
left-branching structure of the tree is a result of the
analysis of the second-position clitic cluster: The
clitic clusters are treated as argument-composition
auxiliaries, which combine with a lexical verb and
?inherit? all of the verb?s arguments. The auxiliaries
first pick up all dependents to the right, and then
combine with exactly one constituent to the left.
The grammar is able to connect x7 (the index of
?milk?) to both the ARG3 position of the ?give? rela-
tion and the ARG1 position of the proprietive rela-
tion, despite the separation between ngaraganaguja
(?grog-PROP.IV.ACC?) and ngabulu (?milk.IV.ACC?)
in the surface structure, as follows: The auxiliary
ngiya is subject to the constraints in (2), meaning
that it combines with a verb as its first complement
and then the verb?s complements as its remaining
complements.9 The auxiliary can combine with its
complements in any order, thanks to a series of head-
complement rules which realize the nth element of
6The grammar in fact finds 42 parses for this example. The
one associated with the MRS in Figure 1 best matches the in-
tended interpretation as indicated by the gloss of the example.
7The relations are given English predicate names for the
convenience of the grammar developer, and these are not in-
tended as any kind of interlingua.
8This MRS is ?fragmented? in the sense that the labels of
several of the elementary predications (eps) are not related to
any argument position of any other ep. This is related to the
fact that the grammar doesn?t yet introduce quantifiers for any
of the nominal arguments.
9In this and other attribute value matrices displayed, feature
paths are abbreviated and detail not relevant to the current point
is suppressed.
980
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
LTOP h1
INDEX e2 (prop-or-ques, past)
RELS
?
?
?
?
grog n rel
LBL h3
ARG0 x4 (3, iv)
?
?
?
,
?
?
?
?
?
?
?
proprietive a rel
LBL h5
ARG0 e6
ARG1 x7 (3, iv)
ARG2 x4
?
?
?
?
?
?
?
,
?
?
?
mother n rel
LBL h8
ARG0 x9 (3sg, ii)
?
?
?
,
?
?
?
?
?
?
?
?
?
?
give v rel
LBL h1
ARG0 e2
ARG1 x9
ARG2 x10 (3)
ARG3 x7
?
?
?
?
?
?
?
?
?
?
,
?
?
?
milk n rel
LBL h5
ARG0 x7
?
?
?
?
HCONS ? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: MRS for (1)
V
V
ADJ
ADJ
ADJ
N
N
Ngaraganaguja
V
V
V
V
V
V
V
ngiya
N
N
N
gujinganjangani
V
V
jiyawu
N
N
N
ngabulu
Figure 2: Phrase structure tree for (1)
the COMPS list. It this example, it first picks up
the subject gujinganjangani (?mother-ERG?), then
the main verb jiyawu (?give?), and then the object
ngabulu (?milk-ACC?).
(2) ?
?
?
?
?
?
?
?
?
?
lexeme
HEAD verb [AUX +]
SUBJ ? 1 ?
COMPS
?
?
?
HEAD verb [AUX ?]
SUBJ ? 1 ?
COMPS 2
?
?
?
? 2
?
?
?
?
?
?
?
?
?
?
The resulting V node over ngiya gujinganjangani
jiyawu ngabulu is associated with the constraints
sketched in (3).
(3) ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
phrase
HEAD verb [AUX +]
SUBJ
?
?
?
?
?
1 N:?mother?
INDEX x9
CASE erg
INST +
?
?
?
?
?
COMPS
?
?
?
?
?
V:?give?
SUBJ ? 1 ?
COMPS ? 2 , 3 ?
INST +
?
?
?
?
,
?
?
?
?
2 N
INDEX x10
CASE acc
INST ?
?
?
?
?
,
?
?
?
?
3 N:?milk?
INDEX x7
CASE acc
INST +
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Unlike in typical HPSG approaches, the informa-
tion about the realized arguments is still exposed
in the COMPS and SUBJ lists of this constituent.10
This makes the necessary information available
to separately-attaching modifiers (such as ngara-
ganaguja (?grog-PROP.IV.ACC?)) so that they can
check for case and number/gender compatibility and
connect the semantic index of the argument they
modify to a role in their own semantic contribution
(in this case, the ARG1 of the ?proprietive? relation).
3.4 Evaluation
The grammar was evaluated against a sample of nat-
urally occurring data taken from one of the texts
transcribed and translated by Nordlinger (1998)
(?The two Eaglehawks?, told by Molly Nurlanyma
Grueman). Of the 92 sentences in this text, 20 over-
lapped with items in the development set, so the
10The feature INST, newly proposed for this analysis, records
the fact that they have been instantiated by lexical heads.
981
correct parsed unparsed average
incorrect ambiguity
Existing 50% 8% 42% 10.62
vocab
w/added 76% 8% 14% 12.56
vocab
Table 1: Grammar performance on held-out data
evaluation was carried out only on the remaining
72 sentences. The evaluation was run twice: once
with the grammar exactly as is, including the exist-
ing lexicon, and a second time after new lexical en-
tries were added, using only existing lexical types.
In some cases, the orthographic components of the
lexical rules were also adjusted to accommodate the
new lexical entries. In both test runs, the analyses of
each test item were hand-checked against the trans-
lation provided by Nordlinger (1998). An item is
counted as correctly analyzed if the set of analyses
returned by the parser includes at least one with an
MRS that matches the dependency structure, illocu-
tionary force, tense, aspect, mood, person, number,
and gender information indicated.
The results are shown in Table 1: With only lexi-
cal additions, the grammar was able to assign a cor-
rect parse to 55 (76%) of the test sentences, with
an average ambiguity over these sentences of 12.56
parses/item.
3.5 Parse selection
The parsed portion of the development set (732
items) constitutes a sufficiently large corpus to train
a parse selection model using the Redwoods disam-
biguation technology (Toutanova et al, 2005). As
part of the grammar development process, the parses
were annotated using the Redwoods parse selection
tool (Oepen et al, 2004). The resulting treebank
was used to select appropriate parameters by 10-fold
cross-validation, applying the experimentation envi-
ronment and feature templates of (Velldal, 2007).
The optimal feature set included 2-level grandpar-
enting, 3-grams of lexical entry types, and both con-
stituent weight features. In the cross-validation tri-
als on the development set, this model achieved a
parse selection accuracy of 80.2% (random choice
baseline: 23.9%). A model with the same features
was then trained on all 544 ambiguous examples
from the development set and used to rank the parses
of the test set. It ranked the correct parse (exact
match) highest in 75.0% of the test sentences. This
is well above the random-choice baseline of 18.4%,
and affirms the cross-linguistic validity of the parse-
selection techniques.
3.6 Summary
This section has presented the Matrix-derived gram-
mar of Wambaya, illustrating its semantic represen-
tations and analyses and measuring its performance
against held-out data. I hope to have shown the
grammar to be reasonably substantial, and thus an
interesting case study with which to evaluate the
Grammar Matrix itself.
4 Evaluation of Grammar Matrix
It is not possible to directly compare the develop-
ment of a grammar for the same language, by the
same grammar engineer, with and without the assis-
tance of the Grammar Matrix. Therefore, in this sec-
tion, I evaluate the usefulness of the Grammar Ma-
trix by measuring the extent to which the Wambaya
grammar as developed makes use of types defined in
Matrix as well as the extent to which Matrix-defined
types had to be modified. The former is in some
sense a measure of the usefulness of the Matrix, and
the latter is a measure of its correctness.
While the libraries and customization system
were used in the initial grammar development, this
evaluation primarily concerns itself with the Matrix
core type hierarchy. The customization-provided
Wambaya-specific type definitions for word order,
lexical types, and coordination constructions were
used for inspiration, but most needed fairly exten-
sive modification. This is particularly unsurprising
for basic word order, where the closest available op-
tion (?free word order?) was taken, in the absence
of a pre-packaged analysis of non-configurationality
and second-position phenomena. The other changes
to the library output were largely side-effects of this
fundamental difference.
Table 2 presents some measurements of the over-
all size of the Wambaya grammar. Since HPSG
grammars consist of types organized into a hierarchy
and instances of those types, the unit of measure for
these evaluations will be types and/or instances. The
982
N
Matrix types 891
ordinary 390
pos disjunctions 591
Wambaya-specific types 911
Phrase structure rules 83
Lexical rules 161
Lexical entries 1528
Table 2: Size of Wambaya grammar
Matrix core types w/ POS types
Directly used 132 34% 136 15%
Indirectly used 98 25% 584 66%
Total types used 230 59% 720 81%
Types unused 160 41% 171 19%
Types modified 16 4% 16 2%
Total 390 100% 891 100%
Table 3: Matrix core types used in Wambaya grammar
Wambaya grammar includes 891 types defined in
the Matrix core type hierarchy. These in turn include
390 ordinary types, and 591 ?disjunctive? types, the
powerset of 9 part of speech types. These are pro-
vided in the Matrix so that Matrix users can easily
refer to classes of, say, ?nouns and verbs? or ?nouns
and verbs and adjectives?. The Wambaya-specific
portion of the grammar includes 911 types. These
types are invoked in the definitions of the phrase
structure rules, lexical rules, and lexical entries.
Including the disjunctive part-of-speech types,
just under half (49%) of the types in the grammar are
provided by the Matrix. However, it is necessary to
look more closely; just because a type is provided in
the Matrix core hierarchy doesn?t mean that it is in-
voked by any rules or lexical entries of the Wambaya
grammar. The breakdown of types used is given in
Table 3. Types that are used directly are either called
as supertypes for types defined in the Wambaya-
specific portion of the grammar, or used as the value
of some feature in a type constraint in the Wambaya-
specific portion of the grammar. Types that are used
indirectly are either ancestor types to types that are
used directly, or types that are used as the value of
a feature in a constraint in the Matrix core types
on a type that is used (directly or indirectly) by the
Wambaya-specific portion of the grammar.
Relatively few (16) of the Matrix-provided types
needed to be modified. These were types that
were useful, but somehow unsuitable, and typically
deeply interwoven into the type system, such that
not using and them and defining parallel types in
their place would be inconvenient.
Setting aside the types for part of speech disjunc-
tions, 59% of the Matrix-provided types are invoked
by the Wambaya-specific portion of the grammar.
While further development of the Wambaya gram-
mar might make use of some of the remaining 41%
of the types, this work suggests that there is a sub-
stantial amount of information in the Matrix core
type hierarchy which would better be stored as part
of the typological libraries. In particular, the analy-
ses of argument realization implemented in the Ma-
trix were not used for this grammar. The types
associated with argument realization in configura-
tional languages should be moved into the word-
order library, which should also be extended to in-
clude an analysis of Wambaya-style radical non-
configurationality. At the same time, the lexical
amalgamation analysis of the features used in long-
distance dependencies (Sag, 1997) was found to be
incompatible with the approach to argument realiza-
tion in Wambaya, and a phrasal amalgamation anal-
ysis was implemented instead. This again suggests
that lexical v. phrasal amalgamation should be en-
coded in the libraries, and selected according to the
word order pattern of the language.
As for parts of speech, of the nine types provided
by the Matrix, five were used in the Wambaya gram-
mar (verb, noun, adj, adv, and det) and four were not
(num, conj, comp, and adp(osition)). Four disjunc-
tive types were directly invoked, to describe phe-
nomena applying to nouns and adjectives, verbs and
adverbs, anything but nouns, and anything but de-
terminers. While it was convenient to have the dis-
junctive types predefined, it also seems that a much
smaller set of types would suffice in this case. Since
the nine proposed part of speech types have varying
crosslinguistic validity (e.g., not all languages have
conjunctions), it might be better to provide software
support for creating the disjunctive types as the need
arises, rather than predefining them.
Even though the number of Matrix-provided types
is small compared to the grammar as a whole, the
relatively short development time indicates that the
types that were incorporated were quite useful. In
providing the fundamental organization of the gram-
983
mar, to the extent that that organization is consistent
with the language modeled, these types significantly
ease the path to creating a working grammar.
The short development time required to create the
Wambaya grammar presents a qualitative evaluation
of the Grammar Matrix as a crosslinguistic resource,
as one goal of the Grammar Matrix is to reduce the
cost of developing precision grammars. The fact
that a grammar capable of assigning valid analy-
ses to an interesting portion of sentences from natu-
rally occurring text could be developed in less than
5.5 person-weeks of effort suggests that this goal
is indeed met. This is particularly encouraging in
the case of endangered and other resource-poor lan-
guages. A grammar such as the one described here
could be a significant aide in analyzing additional
texts as they are collected, and in identifying con-
structions that have not yet been analyzed (cf. Bald-
win et al 2005).
5 Conclusion
This paper has presented a precision, hand-built
grammar for the Australian language Wambaya, and
through that grammar a case study evaluation of
the LinGO Grammar Matrix. True validation of
the Matrix qua hypothesized linguistic universals re-
quires many more such case studies, but this first
test is promising. Even though Wambaya is in some
respects very different from the well-studied lan-
guages on which the Matrix is based, the existing
machinery otherwise worked quite well, providing a
significant jump-start to the grammar development
process. While the Wambaya grammar has a long
way to go to reach the complexity and range of
linguistic phenomena handled by, for example, the
LinGO English Resource Grammar, it was shown to
provide analyses of an interesting portion of a natu-
rally occurring text. This suggests that the method-
ology of building such grammars could be profitably
incorporated into language documentation efforts.
The Grammar Matrix allows new grammars to di-
rectly leverage the expertise in grammar engineering
gained in extensive work on previous grammars of
better-studied languages. Furthermore, the design
of the Matrix is such that it is not a static object,
but intended to evolve and be refined as more lan-
guages are brought into its purview. Generalizing
the core hierarchy and libraries of the Matrix to sup-
port languages like Wambaya can extend its typo-
logical reach and further its development as an in-
vestigation in computational linguistic typology.
Acknowledgments
I would like to thank Rachel Nordlinger for pro-
viding access to the data used in this work in elec-
tronic form, as well as for answering questions about
Wambaya; Russ Hugo for data entry of the lexicon;
Stephan Oepen for assistance with the parse ranking
experiments; and Scott Drellishak, Stephan Oepen,
and Laurie Poulson for general discussion. This ma-
terial is based upon work supported by the National
Science Foundation under Grant No. BCS-0644097.
References
J. Baldridge, S. Chatterjee, A. Palmer, and B. Wing.
2007. DotCCG and VisCCG: Wiki and programming
paradigms for improved grammar engineering with
OpenCCG. In T.H. King and E.M. Bender, editors,
GEAF 2007, Stanford, CA. CSLI.
T. Baldwin, J. Beavers, E.M. Bender, D. Flickinger, Ara
Kim, and S. Oepen. 2005. Beauty and the beast: What
running a broad-coverage precision grammar over the
BNC taught us about the grammar ? and the corpus.
In S. Kepser and M. Reis, editors, Linguistic Evidence:
Empirical, Theoretical, and Computational Perspec-
tives, pages 49?70. Mouton de Gruyter, Berlin.
J.A. Bateman, I. Kruijff-Korbayova?, and G.-J. Kruijff.
2005. Multilingual resource sharing across both re-
lated and unrelated languages: An implemented, open-
source framework for practical natural language gen-
eration. Research on Language and Computation,
3(2):191?219.
E.M. Bender and D. Flickinger. 2005. Rapid prototyping
of scalable grammars: Towards modularity in exten-
sions to a language-independent core. In IJCNLP-05
(Posters/Demos), Jeju Island, Korea.
E.M. Bender, D. Flickinger, and S. Oepen. 2002. The
grammar matrix: An open-source starter-kit for the
rapid development of cross-linguistically consistent
broad-coverage precision grammars. In J. Carroll,
N. Oostdijk, and R. Sutcliffe, editors, Proceedings of
the Workshop on Grammar Engineering and Evalua-
tion, COLING 19, pages 8?14, Taipei, Taiwan.
E.M. Bender. 2007. Combining research and pedagogy
in the development of a crosslinguistic grammar re-
source. In T.H. King and E.M. Bender, editors, GEAF
2007, Stanford, CA. CSLI.
984
D.G. Bobrow, C. Condoravdi, R.S. Crouch, V. de Paiva,
L. Karttunen, T.H. King, R. Nairn, L. Price, and A Za-
enen. 2007. Precision-focused textual inference. In
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, Prague, Czech Republic.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
J. Carroll, N. Oostdijk, and R. Sutcliffe, editors, Pro-
ceedings of the Workshop on Grammar Engineering
and Evaluation at COLING 19, pages 1?7.
A. Copestake, D. Flickinger, C. Pollard, and I.A. Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(2?3):281?
332.
A. Copestake. 2002. Implementing Typed Feature Struc-
ture Grammars. CSLI, Stanford, CA.
C. Donohue and I.A. Sag. 1999. Domains in Warlpiri.
Paper presented at HPSG 99, University of Edinburgh.
S. Drellishak and E.M. Bender. 2005. A coordination
module for a crosslinguistic grammar resource. In Ste-
fan Mu?ller, editor, HPSG 2005, pages 108?128, Stan-
ford. CSLI.
D. Flickinger and E.M. Bender. 2003. Compositional se-
mantics in a multilingual grammar resource. In E.M.
Bender, D. Flickinger, F. Fouvry, and M. Siegel, edi-
tors, Proceedings of the Workshop on Ideas and Strate-
gies for Multilingual Grammar Development, ESSLLI
2003, pages 33?42, Vienna, Austria.
D. Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineer-
ing, 6 (1):15 ? 28.
L. Hellan and P. Haugereid. 2003. NorSource: An ex-
ercise in Matrix grammar-building design. In E.M.
Bender, D. Flickinger, F. Fouvry, and M. Siegel, edi-
tors, Proceedings of the Workshop on Ideas and Strate-
gies for Multilingual Grammar Development, ESSLLI
2003, pages 41?48, Vienna, Austria.
R. Kim, M. Dalrymple, R.M. Kaplan, T.H. King, H. Ma-
suichi, and T. Ohkuma. 2003. Multilingual grammar
development via grammar porting. In E.M. Bender,
D. Flickinger, F. Fouvry, and M. Siegel, editors, Pro-
ceedings of the Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 2003,
pages 49?56, Vienna, Austria.
T.H. King, M. Forst, J. Kuhn, and M. Butt. 2005. The
feature space in parallel grammar writing. Research
on Language and Computation, 3(2):139?163.
A. Kinyon, O. Rambow, T. Scheffler, S.W. Yoon, and
A.K. Joshi. 2006. The metagrammar goes multilin-
gual: A cross-linguistic look at the V2-phenomenon.
In TAG+8, Sydney, Australia.
V. Kordoni and J. Neu. 2005. Deep analysis of Modern
Greek. In K-Y Su, J. Tsujii, and J-H Lee, editors, Lec-
ture Notes in Computer Science, volume 3248, pages
674?683. Springer-Verlag, Berlin.
F. Lareau and L. Wanner. 2007. Towards a generic
multilingual dependency grammar for text generation.
In T.H. King and E.M. Bender, editors, GEAF 2007,
pages 203?223, Stanford, CA. CSLI.
J.T. L?nning and S. Oepen. 2006. Re-usable tools for
precision machine translation. In COLING|ACL 2006
Interactive Presentation Sessions, pages 53 ? 56, Syd-
ney, Australia.
M. Marimon, N. Bel, and N. Seghezzi. 2007. Test-suite
construction for a Spanish grammar. In T.H. King
and E.M. Bender, editors, GEAF 2007, Stanford, CA.
CSLI.
Stefan Mu?ller. 2007. The Grammix CD-ROM: A soft-
ware collection for developing typed feature structure
grammars. In T.H. King and E.M. Bender, editors,
GEAF 2007, Stanford, CA. CSLI.
R. Nordlinger. 1998. A Grammar of Wambaya, Northern
Australia. Research School of Pacific and Asian Stud-
ies, The Australian National University, Canberra.
S. Oepen, E.M. Bender, U. Callmeier, D. Flickinger, and
M. Siegel. 2002. Parallel distributed grammar engi-
neering for practical applications. In Proceedings of
the Workshop on Grammar Engineering and Evalua-
tion, COLING 19, Taipei, Taiwan.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods. A rich and dynamic
treebank for HPSG. Journal of Research on Language
and Computation, 2(4):575 ? 596.
Stephan Oepen, Erik Velldal, Jan Tore Lnning, Paul
Meurer, Victoria Rosn, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine translation.
On linguistics and probabilities in MT. In TMI 2007,
Skvde, Sweden.
C. Pollard and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI, Stanford, CA.
S. Robinson, G. Aumann, and S. Bird. 2007. Managing
fieldwork data with Toolbox and the Natural Language
Toolkit. Language Documentation and Conservation,
1:44?57.
I.A. Sag. 1997. English relative clause constructions.
Journal of Linguistics, 33(2):431 ? 484.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Work-
shop on Asian Language Resources and International
Standardization, COLING 19, Taipei, Taiwan.
K. Toutanova, C.D. Manning, D. Flickinger, and
S. Oepen. 2005. Stochastic HPSG parse selection
using the Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83 ? 105.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo, Department of Informatics.
985
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 136?143,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Validation and Regression Testing for a Cross-linguisic Grammar Resource
Emily M. Bender, Laurie Poulson, Scott Drellishak, Chris Evans
University of Washington
Department of Linguistics
Seattle WA 98195-4340 USA
{ebender,lpoulson,sfd,chrisev@u.washington.edu}
Abstract
We present a validation methodology for
a cross-linguistic grammar resource which
produces output in the form of small gram-
mars based on elicited typological descrip-
tions. Evaluating the resource entails sam-
pling from a very large space of language
types, the type and range of which preclude
the use of standard test suites development
techniques. We produce a database from
which gold standard test suites for these
grammars can be generated on demand, in-
cluding well-formed strings paired with all
of their valid semantic representations as
well as a sample of ill-formed strings. These
string-semantics pairs are selected from a
set of candidates by a system of regular-
expression based filters. The filters amount
to an alternative grammar building system,
whose generative capacity is limited com-
pared to the actual grammars. We perform
error analysis of the discrepancies between
the test suites and grammars for a range of
language types, and update both systems ap-
propriately. The resulting resource serves as
a point of comparison for regression testing
in future development.
1 Introduction
The development and maintenance of test suites is
integral to the process of writing deep linguistic
grammars (Oepen and Flickinger, 1998; Butt and
King, 2003). Such test suites typically contain hand-
constructed examples illustrating the grammatical
phenomena treated by the grammar as well as rep-
resentative examples taken from texts from the tar-
get domain. In combination with test suite manage-
ment software such as [incr tsdb()] (Oepen, 2002),
they are used for validation and regression testing of
precision (deep linguistic) grammars as well as the
exploration of potential changes to the grammar.
In this paper, we consider what happens when the
precision grammar resource being developed isn?t a
grammar of a particular language, but rather a cross-
linguistic grammar resource. In particular, we con-
sider the LinGO Grammar Matrix (Bender et al,
2002; Bender and Flickinger, 2005). There are sev-
eral (related) obstacles to making effective use of
test suites in this scenario: (1) The Matrix core
grammar isn?t itself a grammar, and therefore can?t
parse any strings. (2) There is no single language
modeled by the cross-linguistic resource from which
to draw test strings. (3) The space of possible gram-
mars (alternatively, language types) modeled by the
resource is enormous, well beyond the scope of what
can be thoroughly explored.
We present a methodology for the validation and
regression testing of the Grammar Matrix that ad-
dresses these obstacles, developing the ideas origi-
nally proposed in (Poulson, 2006). In its broad out-
lines, our methodology looks like this:
? Define an abstract vocabulary to be used for test
suite purposes.
? Define an initial small set of string-semantics
pairs.
? Construct a large set of variations on the string-
semantics pairs.
136
? Define a set of filters that can delineate the le-
gitimate string-semantics pairs for a particular
language type
The filters in effect constitute a parallel grammar
definition system, albeit one that creates ?grammars?
of very limited generative capacity. As such, the out-
put of the filters cannot be taken as ground truth.
Rather, it serves as a point of comparison that al-
lows us to find discrepancies between the filters and
the Grammar Matrix which in turn can lead us to
errors in the Grammar Matrix.
2 Background
The Grammar Matrix is an open-source starter kit
designed to jump-start the development of broad-
coverage precision grammars, capable of both pars-
ing and generation and suitable for use in a vari-
ety of NLP applications. The Grammar Matrix is
written within the HPSG framework (Pollard and
Sag, 1994), using Minimal Recursion Semantics
(Copestake et al, 2005) for the semantic represen-
tations. The particular formalism we use is TDL
(type description language) as interpreted by the
LKB (Copestake, 2002) grammar development en-
vironment. Initial work on the Matrix (Bender et
al., 2002; Flickinger and Bender, 2003) focused on
the development of a cross-linguistic core grammar.
The core grammar provides a solid foundation for
sustained development of linguistically-motivated
yet computationally tractable grammars (e.g., (Hel-
lan and Haugereid, 2003; Kordoni and Neu, 2005)).
However, the core grammar alone cannot parse
and generate sentences: it needs to be specialized
with language-specific information such as the or-
der of daughters in its rules (e.g., head-subject or
subject-head), and it needs a lexicon. Although
word order and many other phenomena vary across
languages, there are still recurring patterns. To al-
low reuse of grammar code across languages and to
increase the size of the jump-start provided by the
Matrix, in more recent work (Bender and Flickinger,
2005; Drellishak and Bender, 2005), we have been
developing ?libraries? implementing realizations of
various linguistic phenomena. Through a web in-
terface, grammar developers can configure an initial
starter grammar by filling out a typological question-
naire about their language, which in turn calls a CGI
script to ?compile? a grammar (including language-
specific rule types, lexical entry types, rule entries,
and lexical entries) by making appropriate selections
from the libraries. These little grammars describe
very small fragments of the languages they model,
but they are not toys. Their purpose is to be good
starting points for further development.
The initial set of libraries includes: basic word or-
der of major constituents in matrix clauses (SOV et
al), optionality/obligatoriness of determiners, noun-
determiner order, NP v. PP arguments of intransitive
and transitive verbs, strategies for expressing senten-
tial negation and yes-no questions, and strategies for
constituent coordination. Even with this small set of
phenomena covered (and limiting ourselves for test-
ing purposes to maximally two coordination strate-
gies per language), we have already defined a space
of hundreds of thousands of possible grammars.1
3 The Non-modularity of Linguistic
Phenomena
In this section we discuss our findings so far about
the non-modularity of linguistic phenomena, and ar-
gue that this makes the testing of a broad sample of
grammars even more pressing.
The Grammar Matrix customization system reads
in the user?s language specification and then outputs
language-specific definitions of types (rule types,
lexical entry types and ancillary structures) that in-
herit from types defined in the crosslinguistic core
of the Matrix but add constraints appropriate for the
language at hand. Usability considerations put two
important constraints on this system: (1) The ques-
tions must be ones that are sensible to linguists, who
tend to consider phenomena one at a time. (2) The
output grammar code must be both readable and
maintainable. To achieve readable grammar code
in the output TDL, among other things, we follow
the guideline that any given constraint is stated only
once. If multiple types require the same constraint,
they should all inherit from some supertype bearing
that constraint. In addition, all constraints pertaining
to a particular type are stated in one place.
In light of the these usability considerations, we
1If all of the choices in the customization system were in-
dependent, we would have more than 2 x 1027 grammars. In
actuality, constraints on possible combinations of choices limit
this space considerably.
137
comp-head-phrase := basic-head-1st-comp-phrase & head-final.
subj-head-phrase := basic-head-subj-phrase & head-final &
[ HEAD-DTR.SYNSEM.LOCAL.CAT.VAL.COMPS < > ].
Figure 1: Specialized phrase structure rule types for SOV language
have found that it is not possible to treat the li-
braries as black-box modules with respect to each
other. The libraries are interdependent, and the por-
tions of the script that interpret one part of the input
questionnaire frequently need to make reference to
information elicited by other parts of the question-
naire. For example, the customization system imple-
ments major constituent word order by specializing
the head-complement and head-subject rule types
provided in the core grammar. In an SOV language,
these would both be cross-classified with the type
head-final, and the head-subject rule would further
be constrained to take only complement-saturated
phrases as its head daughter. The TDL encoding of
these constraints is shown in Figure 1.
Following standard practice in HPSG, we use the
head-complement phrase not only for ordinary VPs,
but also for PPs, CPs, and auxiliary-headed VPs,
etc. Consider Polish, a free word order language that
nonetheless has prepositions. To allow complements
on either side of the head, we instantiate both head-
comp and comp-head rules, inheriting from head-
initial and head-final respectively. Yet the preposi-
tions must be barred from the head-final version lest
the grammar license postpositional phrases by mis-
take. We do this by constraining the HEAD value of
the comp-head phrase. Similarly, question particles
(such as est-ce que in French or ma in Mandarin)
are treated as complementizers: heads that select for
an S complement. Since these, too, may differ in
their word order properties from verbs (and preposi-
tions), we need information about the question par-
ticles (elicited with the rest of the information about
yes-no questions) before we have complete informa-
tion about the head-complement rule. Furthermore,
it is not simply a question of adding constraints to
existing types. Consider the case of an SOV lan-
guage with prepositions and sentence-initial ques-
tion particles. This language would need a head-
initial head-comp rule that can take only preposi-
tions and complementizers as its head. To express
the disjunction, we must use the supertype to prep
and comp. This, in turn, means that we can?t decide
what constraint to put on the head value of the head-
comp rule until we?ve considered questions as well
as the basic word order facts.
We expect to study the issue of (non-)modularity
as we add additional libraries to the resource and to
investigate whether the grammar code can be refac-
tored in such a way as to make the libraries into true
modules. We suspect it might be possible to reduce
the degree of interdependence, but not to achieve
completely independent libraries, because syntactic
phenomena are inherently interdependent. Agree-
ment in NP coordination provides an example. In
English and many other languages, coordinated NPs
are always plural and the person of the coordinated
NP is the minimal person value of the coordinands.
(1) a. A cat and a dog are/*is chasing a mouse.
b. Kim and I should handle this ourselves.
c. You and Kim should handle this yourselves.
Gender systems often display a similar hierarchy of
values, as with French coordinated NPs, where the
whole NP is feminine iff all coordinands are femi-
nine and masculine otherwise. Thus it appears that
it is not possible to define all of the necessary con-
straints on the coordination rules without having ac-
cess to information about the agreement system.
Even if we were able to make our analyses of
different linguistic phenomena completely modular,
however, we would still need to test their interaction
in the analysis of particular sentences. Any sentence
that illustrates sentential negation, a matrix yes-no
question, or coordination also necessarily illustrates
at least some aspects of word order, the presence
v. absence of determiners and case-marking adpo-
sitions, and the subcategorization of the verb that
heads the sentence. Furthermore, broad-coverage
grammars need to allow negation, questions, coor-
dination etc. all to appear in the same sentence.
Given this non-modularity, we would ideally like
to be able to validate (and do regression testing on)
the full set of grammars generable by the customiza-
138
Form Description Options
det determiner
n1, n2 nouns det is optional, obligatory, impossible
iv, tv intransitive, transitive verb subj, obj are NP or PP
p-nom, p-acc case-marking adpositions preposition or postposition
neg negative element adverb, prefix, suffix
co1, co2 coordination marks word, prefix, suffix
qpart question particle
Table 1: Standardized lexicon
tion system. To approximate such thoroughness, we
instead sample from the grammar space.
4 Methodology
This section describes in some detail our methodol-
ogy for creating test suites on the basis of language-
type descriptions. A language type is a collection
of feature-value pairs representing a possible set
of answers to the Matrix customization question-
naire. We refer to these as language types rather
than languages, because the grammars produced by
the customization system are underspecified with re-
spect to actual languages, i.e., one and the same
starter grammar might be extended into multiple
models corresponding to multiple actual human lan-
guages. Accordingly, when we talk about the pre-
dicted (well)formedness, or (un)grammaticality, of a
candidate string, we are referring to its predicted sta-
tus with respect to a language type definition, not its
grammaticality in any particular (human) language.
4.1 Implementation: Python and MySQL
The test suite generation system includes a MySQL
database, a collection of Python scripts that interact
with the database, and some stored SQL queries. As
the set of items we are manipulating is quite large
(and will grow as new items are added to test ad-
ditional libraries), using a database is essential for
rapid retrieval of relevant items. Furthermore, the
database facilitates the separation of procedural and
declarative knowledge in the definition of the filters.
4.2 Abstract vocabulary for abstract strings
A grammar needs not just syntactic constructions
and lexical types, but also an actual lexicon. Since
we are working at the level of language types, we
are free to define this lexicon in whatever way is
most convenient. Much of the idiosyncrasy in lan-
guage resides in the lexicon, both in the form of mor-
phemes and in the particular grammatical and collo-
cational constraints associated with them. Of these
three, only the grammatical constraints are manip-
ulated in any interesting way within the Grammar
Matrix customization system. Therefore, for the test
suite, we define all of the language types to draw the
forms of their lexical items from a shared, standard-
ized vocabulary. Table 1 illustrates the vocabulary
along with the options that are currently available
for varying the grammatical constraints on the lex-
ical entries. Using the same word forms for each
grammar contributes substantially to building a sin-
gle resource that can be adapted for the testing of
each language type.
4.3 Constructing master item set
We use string to refer to a sequence of words to
be input to a grammar and result as the (expected)
semantic representation. An item is a particular
pair of string and result. Among strings, we have
seed strings provided by the Matrix developers to
seed the test suite, and constructed strings derived
from those seed strings. The constructor function
is the algorithm for deriving new strings from the
seed strings. Seed strings are arranged into seman-
tic equivalence classes, from which one representa-
tive is designated the harvester string. We parse the
harvester string with some appropriate grammar (de-
rived from the Matrix customization system) to ex-
tract the semantic representation (result) to be paired
with each member of the equivalence class.
The seed strings, when looked at as bags of words,
should cover all possible realizations of the phe-
nomenon treated by the library. For example, the
negation library allows both inflectional and adver-
bial negation, as well as negation expressed through
both inflection and an adverb together. To illustrate
139
negation of transitive sentences (allowing for lan-
guages with and without determiners2), we require
the seed strings in (2):
(2) Semtag: neg1 Semtag: neg2
n1 n2 neg tv det n1 det n2 neg tv
n1 n2 neg-tv det n1 det n2 neg-tv
n1 n2 tv-neg det n1 det n2 tv-neg
n1 n2 neg neg-tv det n1 det n2 neg neg-tv
n1 n2 neg tv-neg det n1 det n2 neg tv-neg
Sentential negation has the same semantic reflex
across all of its realizations, but the presence v. ab-
sence of overt determiners does have a semantic ef-
fect. Accordingly, the seed strings shown in (2) can
be grouped into two semantic equivalence classes,
shown as the first and second columns in the table,
and associated with the semantic tags ?neg1? and
?neg2?, respectively. The two strings in the first row
could be designated as the harvester strings, associ-
ated with a grammar for an SOV language with op-
tional determiners preceding the noun and sentential
negation expressed as a pre-head modifier of V.
We use the LKB in conjunction with [incr tsdb()]
to parse the harvester strings from all of the equiva-
lence classes with the appropriate grammars. Then
the seed strings and the parsing results from the har-
vester strings, as well as their semantic tags, are
stored and linked in our relational database. We use
the constructor function to create new strings from
these seed strings. This produces the master item set
that provides the basis for the test suites.
Currently, we have only one constructor function
(?permute?) which takes in a seed string and returns
all unique permutations of the morphemes in that
seed string.3 This constructor function is effective
in producing test items that cover the range of word
order variations currently permitted by the Grammar
Matrix customization system. Currently, most of the
other kinds of variation countenanced (e.g., adver-
bial v. inflectional negation or presence v. absence
of determiners) is handled through the initial seed
string construction. As the range of phenomena han-
dled by the customization system expands, we will
develop more sophisticated constructor functions to
2We require additional seed strings to account for languages
with and without case-marking adpositions
3
?permute? strips off any affixes, permutes the stems, and
then attaches the affixes to the stems in all possible ways.
handle, for example, the addition of all possible case
suffixes to each noun in the sentence.
4.4 Filters
The master item set provides us with an inventory
from which we can find positive (grammatical) ex-
amples for any language type generated by the sys-
tem as well as interesting negative examples for any
language type. To do so, we filter the master item
set, in two steps.
4.4.1 Universal Filters
The first step is the application of ?universal? fil-
ters, which mark any item known to be ungrammat-
ical across all language types currently produced by
the system. For example, the word order library does
not currently provide an analysis of radically non-
configurational languages with discontinuous NPs
(e.g., Warlpiri (Hale, 1981)). Accordingly, (3) will
be ungrammatical across all language types:
(3) det det n1 n2 tv
The universal filter definitions (provided by the
developers) each comprise one or more regular ex-
pressions, a filter type that specifies how the regular
expressions are to be applied, and a list of seman-
tic tags specifying which equivalence classes they
apply to. For example, the filter that would catch
example (3) above is defined as in (4):
(4) Filter Type: reject-unless-match
Regexp: (det (n1|n2).*det (n1|n2))|
(det (n1|n2).*(n1|n2) det)|
((n1|n2) det.*det (n1|n2))|
((n1|n2) det.*(n1|n2) det)
Sem-class: [semantic classes for all transitive
sentences with two determiners.]
We apply each filter to every item in the database.
For each filter whose semantic-class value includes
the semantic class of the item at hand, we store the
result (pass or fail) of the filter on that item. We can
then query the database to produce a list of all of the
potentially well-formed items.
4.4.2 Specific Filters
The next step is to run the filters that find the
grammatical examples for a particular language
type. In order to facilitate sampling of the entire
language space, we define these filters to be sensi-
tive not to complete language type definitions, but
140
rather to particular features (or small sets of fea-
tures) of a language type. Thus in addition to the
filter type, regular expression, and semantic class
fields, the language-specific filters also encode par-
tial descriptions of the language types to which they
apply, in the form of feature-value declarations. For
example, the filter in (5) plays a role in selecting
the correct form of negated sentences for language
types with both inflectional and adverbial negation
in complementary distribution (like English n?t and
sentential not). The first regular expression checks
for neg surrounded by white space (i.e., the negative
adverb) and the second for the negative affixes.
(5) Filter Type: reject-if-both-match
Regexp1: (\s|?)neg(\s|$)
Regexp2: -neg|neg-
Sem-class: [sem. classes for all neg. sent.]
Lg-feat: and(infl neg:on,adv neg:on,
multineg:comp)
This filter uses a conjunctive language feature spec-
ification (three feature-value pairs that must apply),
but disjunctions are also possible. These specifica-
tions are converted to disjunctive normal form be-
fore further processing.
As with the universal filters, the results of the spe-
cific filters are stored in the database. We process
each item that passed all of the universal filters with
each specific filter. Whenever a filter?s semantic-
class value matches the semantic-class of the item
at hand, we store the value assigned by the filter
(pass or fail). We also store the feature-value pairs
required by each filter, so that we can look up the
relevant filters for a language-type definition.
4.4.3 Recursive Linguistic Phenomena
Making the filters relative to particular semantic
classes allows us to use information about the lexi-
cal items in the sentences in the definition of the fil-
ters. This makes it easier to write regular-expression
based filters that can work across many different
complete language types. Complications arise, how-
ever, in examples illustrating recursive phenomena
To handle such phenomena with our finite-state sys-
tem, we do multiple passes with the filters. All items
with coordination are first processed with the co-
ordination filters, and then rewritten to replace any
well-formed coordinations with single constituents.
These rewritten strings are then processed with the
rest of the filters, and we store the results as the re-
sults for those filters on the original strings.
4.5 Language types
The final kind of information we store in the
database is definitions of language types. Even
though our system allows us to create test suites for
new language types on demand, we still store the
language-type definitions of language types we have
tested, for future regression testing purposes. When
a language type is read in, the list of feature-value
pairs defining it is compared to the list of feature-
groups declared by the filters. For each group of
feature-value pairs present in the language-type def-
inition, we find all of the filters that use that group.
We then query the database for all items that pass
the filters relevant to the language type. This list
of items represents all those in the master item set
predicted to be well-formed for this language type.
From the complement of this set, we also take a ran-
dom selection of items to test for overgeneration.
4.6 Validation of grammars
Once we have created the test suite for a partic-
ular language type, the developer runs the Matrix
customization system to get a starter grammar for
the same language type. The test suite is loaded
into [incr tsdb()] and processed with the grammar.
[incr tsdb()] allows the developer to compare the
grammar?s output with the test suite at varying lev-
els of detail: Do all and only the items predicted to
be well-formed parse? Do they get the same number
of readings as predicted? Do they get the semantic
representations predicted? A discrepancy at any of
these levels points to an error in either the Grammar
Matrix or the test suite generation system. The de-
veloper can query the database to find which filters
passed or failed a particular example as well as to
discover the provenance of the example and which
phenomena it is meant to test.
This methodology provides the ability to gener-
ate test suites for any arbitrary language type on de-
mand. Although this appears to eliminate the need to
store the test suites we do, in fact, store information
about previous test suites. This allows us to track the
evolution of the Grammar Matrix in relation to those
particular language types over time.
141
4.7 Investment and Return
The input required from the developer in order to test
any new library is as follows: (1) Seed strings illus-
trating the range of expressions handled by the new
library, organized into equivalence classes. (2) Des-
ignated harvester strings for each equivalence class
and a grammar or grammars that can parse them to
get the target semantic representation. (3) Universal
filters specific to the phenomenon and seed strings.
(4) Specific filters picking out the right items for
each language type. (5) Analysis of discrepancies
between the test suite and the generated grammars.
This is a substantial investment on the part of the de-
veloper but we believe the investment is worth it for
the return of being able to validate a library addition
and test for any loss of coverage going forward.
Arnold et al (1994) note that writing grammars
to generate test suites is impractical if the test suite
generating grammars aren?t substantially simpler to
write than the ?actual? grammars being tested. Even
though this system requires some effort to maintain,
we believe the methodology remains practical for
two reasons. First, the input required from the de-
veloper enumerated above is closely related to the
knowledge discovered in the course of building the
libraries in the first place. Second, the fact that the
filters are sensitive to only particular features of lan-
guage types means that a relatively small number of
filters can create test suites for a very large number
of language types.
5 Related Work
Kinyon and Rambow (2003) present an approach to
generating test suites on the basis of descriptions
of languages. The language descriptions are Meta-
Grammar (MG) hierarchies. Their approach appears
to be more flexible than the one presented here in
some ways, and more constrained in others. It does
not need any input strings, but rather produces test
items from the language description. In addition,
it annotates the output in multiple ways, including
phrase structure, dependency structure, and LFG F-
structure. On the other hand, there is no apparent
provision for creating negative (ungrammatical) test
data and it is does not appear possible to compose
new MG descriptions on the fly. Furthermore, the
focus of the MG test suite work appears to be the
generation of test suites for other grammar develop-
ment projects, but the MGs themselves are crosslin-
guistic resources in need of validation and testing.
An interesting area for future work would be the
comparison between the test suites generated by the
system described here and the MG test suites.
The key to the test-suite development process pro-
posed here is to leverage the work already being
done by the Matrix developers into a largely auto-
mated process for creating test-suite items. The in-
formation required from the developers is essentially
a structured and systematic version of the knowledge
that is required for the creation of libraries in the first
place. This basic approach, is also the basis for the
approach taken in (Bro?ker, 2000); the specific forms
of knowledge leveraged, and the test-suite develop-
ment strategies used, however, are quite different.
6 Future Work
The addition of the next library to the Grammar Ma-
trix will provide us with an opportunity to try to
quantify the effect of this methodology. With the
Grammar Matrix and the filters stabilized, the vali-
dation of a new library can be carefully tracked. We
can try to quantify the number of errors obtained and
the source of the errors, e.g., library or filters.
In addition to this kind of quantification and error
analysis as a means of validating this methodology,
we also intend to undertake a comparison of the test
suites created from our database to hand built cre-
ated for Matrix-derived grammars by students in the
multilingual grammar engineering course at the Uni-
versity of Washington.4 Students in this class each
develop grammars for a different language, and cre-
ate test suites of positive and negative examples as
part of their development process. We plan to use
the lexical types in the grammars to define a map-
ping from the surface lexical items used in the test
suites to our abstract vocabulary. We can then com-
pare the hand built and autogenerated test suites in
order to gauge the thoroughness of the system pre-
sented here.
7 Conclusion
The methodology outlined in this paper addresses
the three obstacles noted in the introduction: Al-
4http://courses.washington.edu/ling567
142
though the Grammar Matrix core itself isn?t a gram-
mar (1), we test it by deriving grammars from it.
Since we are testing the derived grammars, we are
simultaneously testing both the Matrix core gram-
mar, the libraries, and the customization script. Al-
though there is no single language being modeled
from which to draw strings (2), we can nonethe-
less find a relevant set of strings and associate
these strings with annotations of expected well-
formedness. The lexical formatives of the strings
are drawn from a standardized set of abstract forms.
The well-formedness predictions are made on the
basis of the system of filters. The system of filters
doesn?t represent ground truth, but rather a second
pathway to the judgments in addition to the direct
use of the Matrix-derived starter grammars. These
pathways are independent enough that the one can
serve as an error check on the other. The space of
possible language types remains too large for thor-
ough testing (3). However, since our system allows
for the efficient derivation of a test suite for any arbi-
trary language type, it is inexpensive to sample that
language-type space in many different ways.
Acknowledgments
This work has been supported by NSF grant BCS-
0644097.
References
Doug Arnold, Martin Rondell, and Frederik Fouvry.
1994. Design and implementation of test suite tools.
Technical Report LRE 62-089 D-WP5, University of
Essex, UK.
Emily M. Bender and Dan Flickinger. 2005. Rapid pro-
totyping of scalable grammars: Towards modularity in
extensions to a language-independent core. In Proc.
IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. the Workshop on Grammar Engineering and
Evaluation COLING 2002, pages 8?14.
Norbert Bro?ker. 2000. The use of instrumentation in
grammar engineering. In Proc. COLING 2000, pages
118?124.
Miriam Butt and Tracy Holloway King. 2003. Gram-
mar writing, testing, and evaluation. In Handbook for
Language Engineers, pages 129?179. CSLI.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language & Computation, 3(2?
3):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI.
Scott Drellishak and Emily M. Bender. 2005. A coordi-
nation module for a crosslinguistic grammar resource.
In Stefan Mu?ller, editor, The Proc. HPSG 2005, pages
108?128. CSLI.
Dan Flickinger and Emily M. Bender. 2003. Compo-
sitional semantics in a multilingual grammar resource.
In Proc. the Workshop on Ideas and Strategies for Mul-
tilingual Grammar Development, ESSLLI 2003, pages
33?42.
Kenneth Hale. 1981. On the position of Warlpiri in the
typology of the base. Distributed by Indiana Univer-
sity Linguistics Club, Bloomington.
Lars Hellan and Petter Haugereid. 2003. NorSource: An
exercise in Matrix grammar-building design. In Proc.
the Workshop on Ideas and Strategies for Multilingual
Grammar Development, ESSLLI 2003, pages 41?48.
Alexandra Kinyon and Owen Rambow. 2003. The meta-
grammar: A cross-framework and cross-language test-
suite generation tool. In Proc. 4th International Work-
shop on Linguistically Interpreted Corpora.
Valia Kordoni and Julia Neu. 2005. Deep analysis
of Modern Greek. In Keh-Yih Su, Jun?ichi Tsujii,
and Jong-Hyeok Lee, editors, Lecture Notes in Com-
puter Science, volume 3248, pages 674?683. Springer-
Verlag.
Stephan Oepen and Daniel P. Flickinger. 1998. Towards
systematic grammar profiling. Test suite technology
ten years after. Journal of Computer Speech and Lan-
guage, 12 (4) (Special Issue on Evaluation):411 ? 436.
Stephan Oepen. 2002. Competence and Performance
Profiling for Constraint-based Grammars: A New
Methodology, Toolkit, and Applications. Ph.D. thesis,
Universita?t des Saarlandes.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press.
Laurie Poulson. 2006. Evaluating a cross-linguistic
grammar model: Methodology and gold-standard re-
source development. Master?s thesis, University of
Washington.
143
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building a Flexible, Collaborative, Intensive Master?s
Program in Computational Linguistics
Emily M. Bender
University of Washington
Department of Linguistics
Box 354340
Seattle WA 98195-4340
ebender@u.washington.edu
Fei Xia
University of Washington
Department of Linguistics
Box 354340
Seattle WA 98195-4340
fxia@u.washington.edu
Erik Bansleben
University of Washington
Educational Outreach
UW Tower, Box 359485
4333 Brooklyn Ave., NE
Seattle, WA 98105-9485
ebansleben@extn.washington.edu
Abstract
We present the design of a professional mas-
ter?s program in Computational Linguistics.
This program can be completed in one-year of
full-time study, or two-three years of part-time
study. Originally designed for CS profession-
als looking for additional training, the pro-
gram has evolved in flexibility to accommo-
date students from more diverse backgrounds
and with more diverse goals.
1 Introduction
In the past two decades, there has been tremendous
progress in natural language processing and various
undergraduate/graduate programs in language tech-
nology have been established around the world (Koit
et al, 2002; Frederking et al, 2002; Dale et al,
2002; Uszkoreit et al, 2005; Pilon et al, 2005).
This paper introduces the University of Wash-
ington?s Professional Masters Program in Computa-
tional Linguistics (CLMA)?one of the largest pro-
grams of its kind in the United States?and high-
lights unique features that are key to its success. The
CLMA program is currently operating in its third
year as a fee-based degree program managed jointly
by the Department of Linguistics and the Educa-
tional Outreach arm of the University. The program
is distinguished by its programmatic focus, its flexi-
bility, its format and delivery as well as in the part-
nerships that are an integral part of this degree.
This paper highlights how these features of our
program contribute to effective teaching in our in-
terdisciplinary field as well as making the program
relevant to both working professionals and students
on the research track. We provide a brief program
overview highlighting the people and partnerships
involved, course design, practicum and research op-
tions, and dealing with student diversity. We then
reflect on how we have approached the challenges
of setting up the program and our future plans.
2 Program Overview
With working professionals who wanted to return
to school to retool for a career change in mind, we
designed a curriculum that can be completed in 12
months of intensive full-time study. In this way, stu-
dents can complete the degree without leaving the
working world for too long. In addition, the com-
pactness of the program means that even with part-
time study (one-two courses per quarter), the pro-
gram can be completed within a reasonable time
horizon (two-three years). Once the program got
underway, we found that we also had strong interest
from Linguistics students. The flexibility of the part-
time option has allowed us to develop a two-year
schedule which accommodates students who need
time to get up to speed with key CS concepts.
The curriculum is designed around hands-on and
collaborative work which prepares students for in-
dustry jobs. At the same time, the courses are struc-
tured around fundamental building blocks rather
than applications in order to teach students to think
like computational linguists, and to provide them
with an educational foundation which will remain
relevant for years to come.
This section gives an overview of the CLMA pro-
gram, including its structure and participants.
10
2.1 Program Structure
The CLMA program comprises three quarters (nine
courses) of coursework and a summer master?s
project, which can take the form of an internship
or a master?s thesis (?3.5). The courses, described
in more detail in ?3 below, include three courses in
Linguistics and six in Computational Linguistics.
The program also offers a series of talks by com-
putational linguists working at local companies, an
informal seminar of the computational Linguistics
lab group (which includes PhD students and focuses
on research methodology), and career development
services as we work with students to place them in
internships and post-degree jobs.
2.2 The new certificate program
This summer, the program is branching out with
a new Certificate in Natural Language Technol-
ogy. This three-course Certificate includes two NLP
courses from the Masters degree and an introduc-
tory course titled ?Computational Linguistics Foun-
dations? which serves both this Certificate audience
and acts as a refresher course for some degree stu-
dents. It reinforces the concepts from Linguistics,
CS and statistics that students rely on heavily in the
core course sequence (see ?4). The Certificate is an
alternate course of study for those students wanting
to study a single topic in depth but who are not yet
ready to commit to the entire degree.
2.3 Synchronous online and in-person courses
As part of the certificate summer launch, we will
be offering a selection of courses in a synchronous
online and in-person format, streaming the content
from the classroom to a live remote audience. This
will allow us to extend the reach of the program and
make study in Computational Linguistics available
to students who otherwise would not have access to
instruction in this field without relocating.
In the context of current globalization trends, the
need for online and distance education is grow-
ing (Zondiros, 2008), and indeed we hope that our
audience will extend beyond North America. At
the same time, we agree with Neal and Miller?s
(2004) position that even with remote participants,
the classroom remains a key part of the educational
experience. We have thus adopted an approach that
allows students to be part of a virtual classroom
where they can engage with other students while still
working from a remote location. This surmounts the
hurdle of more traditional distance or online educa-
tion that is primarily text-based and asynchronous.
In a pilot of an online course offering in Autumn
2007 (with Intro to Syntax), we found that most
of the pieces were already in place for taking our
courses online: Course materials are already dissem-
inated through websites, student programming work
is done on a server cluster that is always accessed
remotely, and most of the discussion outside of class
happens on electronic discussion boards.
2.4 Faculty and Staff
The CLMA program is taught by a group of instruc-
tors who combine academic knowledge and practi-
cal expertise in the field. The program budget sup-
ports two faculty positions, one tenure-track (and
guaranteed by the College of Arts and Sciences), and
one two-year visiting position.1 Each of these fac-
ulty teach two of the core NLP courses described in
?3.1 below and one seminar each year. In addition,
they share the work of supervising MA theses and
internships over the summer. In recognition of this
supervising load, their schedules are arranged so that
they each have one non-summer quarter off from
teaching. A third faculty member in Computational
Linguistics teaches three graduate-level courses in
Computational Linguistics per year, and takes on
one-two MA students from each CLMA cohort.
The program also includes a part-time administra-
tor and a technical specialist within the Department
of Linguistics. In addition, the program includes af-
filiated instructors and guest lecturers, ranging from
faculty members of other departments such as CS
and Statistics to researchers from industry.
2.5 Students
A strength of the program is its emphasis on stu-
dent diversity and allowance for individualized stu-
dent needs. The program allows for both part-
time and full-time enrollment and includes both re-
cent college graduates as well older, more non-
traditional students who work in industry. We have
students from throughout the US, as well as from
1To be converted to tenure-track in the future, once the pro-
gram has a longer track-record.
11
Canada, China, Germany, India, Japan, and Russia.
Two-year course schedules allow students to begin
CLMA course work while simultaneously taking CS
and statistics courses in the first year, increasing the
diversity of backgrounds of our students. The pro-
gram is starting to deliver several courses online (see
?2.3) which provides additional flexibility to local
students while also reaching a wider national and
international audience. Lastly, the program seeks to
foster both research and industry interests by provid-
ing both thesis and internship options.
2.6 Advisory board
The program maintains an advisory board composed
of significant industry researchers and practition-
ers, including representatives from AT&T, Boeing,
Google, IBM, Microsoft, Nuance, PARC, and Ya-
hoo!, and faculty from several departments at the
University. This board was instrumental in develop-
ing the original program focus and curriculum out-
line, as well as providing input from the perspective
of employers as the University decided whether or
not to launch the program. It continues to be en-
gaged in guiding the program?s content, providing
internship opportunities for students, and keeping
the content relevant to current industry trends.
2.7 Support from Educational Outreach
Another element of success is a centralized in-
frastructure of administration and support available
through the University?s Educational Outreach divi-
sion (UWEO) which manages the CLMA program,
among more than 30 degree offerings.
UWEO provides many benefits, including con-
siderable institutional knowledge in starting a new
degree program, providing methods of separating
fee-based revenue from that of state-run programs,
marketing expertise, fiscal management, registration
services and more. As the outreach arm of the Uni-
versity, UWEO works closely with non-traditional
students and is able to leverage its industry contacts
to assist serving this community most effectively.
Lastly, partnering with UWEO also serves as a
method of risk management for all new degree pro-
grams. As a state school, the University may have
difficulty in getting state approval and funding for
new degree programs unless initial need and demand
can be demonstrated persuasively. UWEO can as-
sume a certain degree of risk during the start-up
phase of new programs allowing for additional flex-
ibility and time to reach financial viability.
3 Curriculum Design
The program curriculum was designed according to
the following principles: (1) we should provide stu-
dents with an educational foundation that is relevant
in the long term; (2) we should emphasize hands-
on coursework to provide depth of understanding as
well as practical experience students can point to
when looking for a job; and (3) we should highlight
unifying themes in a diverse set of subject matter.
The courses were designed by taking an inven-
tory of the applications and research areas that com-
prise Computational Linguistics, and then breaking
them down into subtasks. These subtasks were then
grouped by similarity into coherent courses, and the
courses into core and elective sets. Three topics re-
sisted classification into any particular course: am-
biguity resolution, evaluation, and multilingualism.
These became our cross-cutting themes which are
highlighted throughout all of the courses. In addi-
tion to understanding each subtask, working compu-
tational linguists need to know how to combine the
stages of linguistic processing into end-to-end sys-
tems. For this reason, we include a capstone ?Sys-
tems and Applications? course in which students
work in groups to create an NLP application.
Key to making the Computational Linguistics cur-
riculum fit into one calendar year was deciding not
to include the course ?Intro to Computational Lin-
guistics.? Such a course serves to expose students to
a broad range of topics in the field and get them in-
terested in exploring further. CLMA students are al-
ready interested in further studies in Computational
Linguistics, and will be exposed to a broad range
of topics throughout the curriculum. However, we
did still want to give the students an overview of
the field so that they could see how the rest of their
studies will fit in. This is done through a two-day
orientation at the start of each year. The orienta-
tion also introduces the three cross-cutting themes
mentioned above, gives the students a chance to get
to know each other and the CLMA faculty, and pro-
vides practical information about the university such
as libraries, computing lab facilities, etc.
12
3.1 Required courses
There are six required courses: The first two are Lin-
guistics courses, and the remaining four form the
NLP core courses. Among the four NLP courses,
Ling 572 should be taken after Ling 570, and Ling
573 should be taken after Ling 570, 571, and 572.
Ling 450 (Intro to Linguistic Phonetics): Intro-
duction to the articulatory and acoustic correlates
of phonological features. Issues covered include
the mapping of dynamic events to static represen-
tations, phonetic evidence for phonological descrip-
tion, universal constraints on phonological structure,
and implications of psychological speech-sound cat-
egorization for phonological theory.
Ling 566 (Intro to Syntax for Computational
Linguistics): Introduction to syntactic analysis and
concepts (e.g., constituent structure, the syntax-
semantics interface, and long-distance dependen-
cies). Emphasis is placed on formally precise en-
coding of linguistic hypotheses and designing gram-
mars so that they can be scaled up for practical appli-
cations. Through the course we progressively build
up a consistent grammar for a fragment of English.
Problem sets introduce data and phenomena from
other languages.
Ling 570 (Shallow Processing Techniques for
NLP): Techniques and algorithms for associating
relatively surface-level structures and information
with natural language data, including tasks such as
tokenization, POS tagging, morphological analysis,
language modeling, named entity recognition, shal-
low parsing, and word sense disambiguation.
Ling 571 (Deep Processing Techniques for NLP):
Techniques and algorithms for associating deep or
elaborated linguistic structures with natural lan-
guage data (e.g., parsing, semantics, and discourse)
and for associating natural language strings with in-
put semantic representations (generation).
Ling 572 (Advanced Statistical Methods in NLP):
Major machine learning algorithms for NLP, includ-
ing Decision Tree, Naive Bayes, kNN, Maximum
Entropy, Support Vector Machine, Transformation-
Based Learning, and the like. Students implement
many of these algorithms and use them to solve clas-
sification and sequence labeling problems.
Ling 573 (NLP Systems and Applications): De-
sign and implementation of coherent systems for
practical applications, with topics varying year to
year. Sample topics: machine translation, ques-
tion answering, information retrieval, information
extraction, dialogue systems, and spell/grammar
checking. In 2006, the students collectively built a
question answering system, which was further de-
veloped into a submission to the TREC competition
(Jinguji et al, 2006). This year?s class is developing
a chatbot to submit to the Loebner Prize competi-
tion, an implementation of the Turing Test.
Among the required courses, Ling 566 was cre-
ated a year before the CLMA program started, and
has been taught four times. Ling 450 is an estab-
lished course from our Linguistics curriculum. Ling
570-573 were newly created for this program, and
have each been taught three times now. We have
put much effort in improving course design, as dis-
cussed in (Xia, 2008).
3.2 The prerequisites for the required courses
In order to cover the range of methodologies and
tasks that our program does in its core sequence, we
need to set as a prerequisite the ability to program,
including knowledge of data structures and algo-
rithms, and expertise in C++ or Java.2 Another pre-
requisite is a college-level course in probability and
statistics. Without such knowledge, it is all but im-
possible to discuss the sophisticated statistical mod-
els covered in the core NLP courses. For the two
Linguistics required courses, the only prerequisite is
a college-level introductory course in Linguistics or
equivalent. Because our students have very diverse
backgrounds, we have tried several approaches to
help the students meet al these prerequisites, which
will be discussed in ?4.
3.3 Elective courses
All students must take three electives, including
one in Linguistics, one in Computational Linguis-
tics, and one more in Computational Linguistics or
a related field. The Linguistics electives are drawn
from the broad range of graduate-level Linguistics
courses offered in the department. The related fields
2Knowing Perl or Python is recommended but not required,
as we believe that good C++ or Java programmers can learn Perl
or Python quickly.
13
electives include courses in CS and Electrical Engi-
neering on topics such as Machine Learning, Graph-
ical Models, Artificial Intelligence, and Human-
Computer Interaction as well as courses in the In-
formation School on topics such as Information Re-
trieval. We maintain a list of pre-approved courses,
which grows as students find additional courses of
interest and petition to have them approved.
The annual elective offerings in Computational
Linguistics include Multilingual Grammar Engi-
neering, as well as seminars taught by the Com-
putational Linguistics faculty and by guest experts
(including researchers in local industry), covering
new topics each year. Recent topics include: Cor-
pus Management, Development and Use, Text-to-
Speech, Multimodal Interfaces, Lexical Acquisition
for Precision Grammars, Semi-supervised and Un-
supervised Learning for NLP, and Information Ex-
traction from Heterogeneous Resources. There are
four-five such seminars per year, three from the
Computational Linguistics faculty and one-two from
guest experts.
We ask students to present their choices of elec-
tives for approval, and require that they articulate
reasons why their range of choices constitutes a co-
herent course of study.
3.4 Hands on, interactive courses
All of the courses in the curriculum are hands-on,
emphasizing learning through doing as well as col-
laboration between the students. Theoretical con-
cepts introduced in lecture are put into practice
with problem sets (e.g., in Intro to Syntax), pro-
gramming assignments (in the core sequence) and
opened-ended projects (in the Systems and Applica-
tions course and the seminars). Student collabora-
tion is promoted through group projects as well as
active online discussion boards where students and
faculty together solve problems as they arise.
3.5 The master?s project
In addition to nine courses, the students need to
complete a master?s project, either through an in-
ternship or through completing a master?s thesis.
The internship option: Internships counting to-
wards the MA degree must be relevant to Compu-
tational Linguistics or human language technology
more broadly. Students develop a pre-internship
proposal, including a statement of the area of inter-
est and proposed contributions, a discussion of why
the company targeted is a relevant place to do this
work, and a list of relevant references. Once the stu-
dents have been offered and accepted an internship,
they write a literature review on existing approaches
to the task in question.
At the end of the internship, students write a self-
evaluation which they present to the internship su-
pervisor for approval and then to the faculty advisor.
In addition, we require a confidential, written evalu-
ation from the intern?s supervisor which references
the self-evaluation. If this evaluation does not indi-
cate satisfactory work, the internship will not count.
Students also write a post-internship report, in-
cluding a description of the activities undertaken
during the internship and their results, a discussion
of how the program course work related to and/or
prepared the student for the internship work, and a
second version of the literature review. We expect
the second review to be different from the initial
version in incorporating the additional perspective
gained in the course of the internship as well as any
additional key papers that the student discovered in
the course of internship work.
The thesis option: This option is recommended for
students who wish to petition for admission to the
Department?s PhD program, and encouraged for stu-
dents who wish to apply to other PhD programs in
the near future. An MA thesis typically involves
the implementation of working systems (or exten-
sions or experimental evaluations thereof). In some
cases, they may provide theoretical contributions in-
stead. MA theses require a thorough literature re-
view, are typically longer (30-50 pages), and repre-
sent the kind of research which could be presented
at major conferences in our field.
The milestones: While the internship and a sig-
nificant portion of the thesis work are conducted in
the summer for full-time students, we start monthly
graduation planning meetings as early as the pre-
ceding October to help students decide which op-
tion they should take. For those seeking internships,
we will help them identify the companies that match
their interests and make the contact if possible.
14
Students who choose the thesis option are re-
quired to turn in an initial thesis proposal that in-
cludes a thesis topic, a summary of major existing
approaches, the students? proposed approach, and a
plan for evaluation. With the feedback from the fac-
ulty, the students will revise their proposals several
times before finalizing the thesis topic. Students are
encouraged to take elective courses relevant to their
topic. Because the amount of research is required for
a good master?s thesis, we expect students with this
option to take one or two additional quarters to finish
than the ones who choose the internship option.
4 Challenges
In this section, we address several challenges that we
encountered while establishing the new program.
Students enrolling in our program have varied
backgrounds in Linguistics, CS and other under-
graduate majors. In addition, some students come
to us straight from undergraduate studies, while oth-
ers are re-entry students. To better prepare students
for the program, starting this year we offer an eight-
week summer refresher course, which reinforces the
most important skills from contributing disciplines
to prepare students for the CLMA core courses. The
course covers the following topics: (1) formal gram-
mars and formal languages, (2) finite-state automata
and transducers, (3) review of main concepts from
probability and statistics, (4) review of major data
structures and algorithms, and (5) using Unix and
computer clusters.
After students are admitted to the program, they
are asked to take an online placement test to identify
the areas that they need to strengthen before enter-
ing the program. They can then choose to take the
summer course or study on their own.
While some of our students, typically fresh out
of college or stepping out of the workforce for re-
training to switch careers, are eager to complete the
degree in one year, others wish to complete the pro-
gram while continuing their current employment or
simply need more time. We offer various options to
accommodate different needs:
Part-time vs. full-time Students can complete the
program in one year, taking three classes each quar-
ter and completing the master?s project in the sum-
mer. At this pace, the program is very intense.
The program also offers part-time options, allow-
ing students to take courses one or two at a time.
This works well for students who are currently em-
ployed and also leaves time for students coming
from a Linguistics background to take CS and Statis-
tics courses before approaching the Computational
Linguistics core sequence. While full-time students
must start in Autumn quarter, part-time students can
start in any academic quarter.
Curriculum flexibility Students who come to us
with an extensive background in Linguistics (e.g., a
prior MA), can waive one or more of the Linguistics
requirements, giving them more time in their sched-
ule for Computational Linguistics or related fields
courses, such as CS.
Program options Our courses are open to qual-
ified students for single-course enrollment, allow-
ing people who don?t have the time or financial re-
sources to commit to the whole master?s program
to benefit from the course offerings. In addition,
the three-course certificate provides more continuity
than single-course enrollment (though less than the
full master?s program) as well as the recognition of a
certificate. In either case, graduate non-matriculated
status allows such students to apply their coursework
to the master?s program at a later date.
Master?s project options In providing for both in-
ternships and master?s theses, the program can ac-
commodate students seeking training for positions
in industry as well as those seeking to continue grad-
uate studies. In the former case, the practical ex-
perience of an internship together with the indus-
try connections it can provide are most valuable. In
the latter case, a chance to do independent research
is more appropriate. Students who spread the pro-
gram out over more than one year can do internships
in the summer between years one and two in addi-
tion to the master?s project (internship or thesis) in
the second summer. Finally, the ?internship option?
can also be fulfilled by ordinary full-time employ-
ment: when students begin full-time positions in the
summer after they complete coursework or apply the
knowledge gained in the master?s program to new
projects at their current places of employment.
In class or online By offering our courses in a hy-
brid, synchronous in-person and online format, we
15
add the flexibility to attend our program from any-
where in the world while still benefiting from the
same courses, lectures, online discussions and col-
laborative work. The online option is also benefi-
cial to local students, allowing them to tune in, for
example, when home caring for a sick child, to re-
view lectures previously attended, to attend online
on days without other on-campus obligations and to
avoid the commute. In the 2008-2009 school year,
three of our courses will be offered in this format,
and we plan to extend the offerings going forward.
5 Outcomes
5.1 Enrollment, graduation rate and placement
In years 1-3, 70 students have enrolled, and about
30 of them enrolled as full-time students.3 To data
13 have completed the program, and at least nine
of them are currently in jobs related to Computa-
tional Linguistics. Another 12 are projected to grad-
uate this year. Out of these 25 students, 15 chose
the internship option and 10 chose the thesis option.
We have placed students in internships with com-
panies such as Amazon.com, Google, Microsoft,
PARC, Tegic (Nuance), and VoiceBox, and have
graduates working at companies such as Cataphora,
Cisco, Google, InXight, Tegic (Nuance), and Voice-
Box. Among the 10 students who took the thesis op-
tion, four received RAships from CLMA faculty?s
research grants, and at least two will enroll in our
Ph.D. program after receiving their MAs.
Recent internal research completed by UWEO
identified a total of 34 CL programs, 23 in the US
and 11 in Western Europe. These programs vary
from named degrees in Computational Linguistics or
a similar variant, to concentrations in other degrees
and to loose courses of study. It appears that there is
one other university in the US that has enrollment as
high or higher than our own, but all other programs
typically have at least 50% fewer students enrolling
as of 2007. Given that this program is only in its
third year, we consider this level of high compara-
tive enrollment a strong measure of success. Addi-
tionally, during this 3 year period, there has been an
upward trend in applications which may be a reflec-
3Some of them later switched to part-time status due to vari-
ous reasons (e.g., the intensity of the program, financial consid-
eration).
tion on the growth and awareness of the discipline,
but may also be a reflection on the growing reputa-
tion of the program.
5.2 Student feedback
We sent an anonymous survey to all alumni and
current students (N=70) asking them about the ef-
fectiveness of the overall CLMA program, individ-
ual courses, the curriculum, success in getting a job
as well as for some qualitative feedback about the
strengths and weaknesses of the program. We re-
ceived 31 responses (44% response rate). For the
sake of brevity, we will provide a selection of ques-
tions and categorize the results as follows: positive
(1=very well, 2=fairly well), neutral (3=so so); neg-
ative (4=not very well, 5=poorly).
0%10%
20%30%
40%50%
60%
Q1 Q2 Q3 Q4 Q5
Percentage of r
esponses very wellfairly wellso sonot very wellpoorly
Figure 1: Student and alumni responses
As shown in Figure 1, the responses were over-
whelmingly positive. The first four questions ask
how well the program as a whole helped the students
achieve the goals of learning to think like a computa-
tional linguist (Q1), understanding the state of the art
in Computational Linguistics (Q2), understanding
the potential contributions of both machine learning
and knowledge engineering (Q3), and preparation
for a job in industry (Q4). The fifth question asks
how helpful the CLMA experience has been/will be
in finding a job.4 There were a number of other ques-
tions, but the results are all very similar to the ones
above. These same questions were also asked with
respect to individual courses. The results were again
similar, although slightly lower. Positive responses
were in the range of 80%-95%, neutral in the range
4Each of these questions was answered by 24-27 students.
16
of 5-20% and negative responses were usually lim-
ited to no more than 5%. For the question of how
well the program has prepared students for their cur-
rent job (alumni only, N=5), 100% answered posi-
tively. For the question about how important the pro-
gram was in attaining their current job, again 100%
felt that the program was crucial.
We also received valuable qualitative feedback on
this survey. The majority of students (67%) felt that
the program was very intense, but very much worth-
while. The faculty consistently receives high praise;
students enjoy the small hard-working community;
and comments indicate that the coursework is rele-
vant for their future career. When asked about sug-
gestions for improvement, students provided a num-
ber of logistical suggestions, would like to see some
degree of student mentoring, and seek to find ways
to reduce the intensity of the program, especially
for part-time students who are working. It is clear,
though, from the overall survey results, that students
feel very positive about the program as a whole, and
its relevance for their professional future.
While we at first thought the program to be pri-
marily a one-year program, the intensity of the cur-
riculum has resulted in a number students taking
longer than one year to complete the program which
has impacted the number of students who have thus
far completed. Consequently, we will consider stu-
dent feedback from the survey which?in conjunc-
tion with the new preparatory course?should lead
us to find methods of reducing the intensity but
maintaining the high quality.
6 Conclusion and future directions
6.1 Lessons learned
In starting this program, we had the privilege of de-
signing the curriculum first and then hiring faculty to
teach the required courses. We worked closely with
our advisory board to develop a course of study well-
suited to training students for industry jobs, while
also striving to design a program that will remain
relevant to graduates many years down the line.
Financial support from UWEO allowed us to
jump in with both feet, offering the full curriculum
from year one. This was critical in attracting a strong
and reasonably large student body. It also provided
the freedom to design a curriculum that goes in-
depth into Computational Linguistics.
Other facets of our curriculumwhich contribute to
its success include: (1) We combine in-depth explo-
ration of particular topics with cross-cutting themes
that tie the courses together. (2) The courses em-
phasize hands-on work, providing immediate moti-
vation to delve deeper into the theoretical concepts
presented. (3) The program combines high inten-
sity with high levels of support: We ask the stu-
dents to attempt real-world scale projects and then
assist them in achieving these goals through provid-
ing software to work from, offering high levels of
online interaction to answer questions, and facili-
tating collaboration. By working together, the stu-
dents can build more interesting systems than any-
one could alone, and therefore explore a broader ter-
ritory. In addition, collaborative projects help stu-
dents benefit from each other?s diverse backgrounds.
At the same time, we?ve found providing multi-
ple ways of completing program requirements to be
key to allowing students from different backgrounds
to succeed. Exceptional students coming from Lin-
guistics can get up to speed quickly enough to com-
plete the program on a full-time schedule (and some
have), but many others benefit from being able to
take it more slowly, as do some students from a CS
background. We also find that having expertise in
Linguistics among the students significantly benefits
the overall cohort.
6.2 Future directions
In the near future, we plan to expand our online of-
ferings, which directly expands our audience and
benefits local students as described above. We have
found connecting course work to faculty research
and/or external competitions such as TREC and the
Loebner Prize to be extremely motivating and re-
warding for students, and plan to seek more opportu-
nities for doing so. We are also expanding out inter-
disciplinary reach within the university. The TREC
submission was done jointly with faculty from the
Information School. This year?s guest faculty course
will be offered jointly with a course in the School
of Art on interface design. In pursuing all of these
directions, we will benefit from input from our advi-
sory board as well as feedback from current students
and alumni.
17
References
Robert Dale, Diego Molla Aliod, and Rolf Schwit-
ter. 2002. Evangelising Language Technology: A
Practically-Focussed Undergraduate Program. In Pro-
ceedings of the First ACL Workshop on Effective Tools
and Methodologies for Teaching NLP and CL.
Robert Frederking, Eric H. Nyberg, Teruko Mitamura,
and Jaime G. Carbonell. 2002. Design and Evolution
of a Language Technologies Curriculum. In Proceed-
ings of the First ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL.
Dan Jinguji, William D. Lewis, Efthimis N. Efthimiadis,
Joshua Minor, Albert Bertram, Shauna Eggers, Joshua
Johanson, Brian Nisonger, Ping Yu, and Zhengbo
Zhou. 2006. The University of Washington?s UW-
CLMAQA system. In Proceedings of the Text Re-
trieval Conference (TREC) 2006, Gaithersburg, Mary-
land.
Mare Koit, Tiit Roosmaa, and Haldur Oim. 2002. Teach-
ing Computational Linguistics at the University of
Tartu: Experience, Perspectives and Challenges. In
Proceedings of the First ACL Workshop on Effective
Tools and Methodologies for Teaching NLP and CL.
Diane Neal, Lisa nad Miller. 2004. Distance educa-
tion. In Robert W. Proctor and Kim-Phuong L. Vu,
editors, Handbook of Human Factors in Web Design.
Lawrence Erlbaum Associates.
Sule?ne Pilon, Gerhard B Van Huyssteen, and Bertus
Van Rooy. 2005. Teaching language technology at the
North-West University. In Proceedings of the Second
ACL Workshop on Effective Tools and Methodologies
for Teaching NLP and CL.
Hans Uszkoreit, Valia Kordoni, Vladislav Kubon,
Michael Rosner, and Sabine Kirchmeier-Andersen.
2005. Language technology from a European perspec-
tive. In Proceedings of the Second ACL Workshop on
Effective Tools and Methodologies for Teaching NLP
and CL.
Fei Xia. 2008. The evolution of a statistical nlp course.
In Proceedings of the Third ACL Workshop on Effec-
tive Tools and Methodologies for Teaching NLP and
CL, Columbus, Ohio, June.
Dimitris Zondiros. 2008. Online, distance education
and globalization: Its impact on educational access,
inequality and exclusion. European Journal of Open,
Distance and E-Learning, Volume I.
18
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 26?32,
Athens, Greece, 30 March, 2009. c?2009 Association for Computational Linguistics
Linguistically Na??ve != Language Independent:
Why NLP Needs Linguistic Typology
Emily M. Bender
University of Washington
Seattle, WA, USA
ebender@u.washington.edu
Abstract
In this position paper, I argue that in order
to create truly language-independent NLP
systems, we need to incorporate linguis-
tic knowledge. The linguistic knowledge
in question is not intricate rule systems,
but generalizations from linguistic typol-
ogy about the range of variation in linguis-
tic structures across languages.
1 Introduction
Language independence is commonly presented
as one of the advantages of modern, machine-
learning approaches to NLP. Once an algorithm is
developed, the argument goes, it can trivially be
extended to another language; ?all? that is needed
is a suitably large amount of training data for the
new language.1 This is indeed a virtue. How-
ever, the typical approach to developing language-
independent systems is to eschew using any lin-
guistic knowledge in their production. In this po-
sition paper, I argue that, on the contrary, the pro-
duction of language-independent NLP technology
requires linguistic knowledge, and that the rele-
vant kind of linguistic knowledge is in fact rela-
tively inexpensive.
The rest of this paper is structured as follows:
In Section 2, I discuss how linguistically na??ve
systems can end up tuned to the languages they
were originally developed for. In Section 3, I
survey the long papers from ACL2008:HLT to
give a snapshot of how linguistic diversity is cur-
rently handled in our field. In Section 4, I give
1This of course abstracts away from the production of
such data, which may require both significant pre-processing
and annotation work. For the purposes of the present argu-
ment, however, we can assume that all language-independent
NLP systems are unicode-enabled, assume a definition of
?word? that is cross-linguistically applicable, and require the
type of annotations that are likely to have already been de-
ployed for another purpose.
a brief overview of Linguistic Typology, and sug-
gest how knowledge derived from this field can be
profitably incorporated into language-independent
NLP systems.
2 Hidden Language Dependence
A simple example of subtle language dependence
is the way in which n-gram models work better for
languages that share important typological proper-
ties with English. On the face of it, n-gram mod-
els code in no linguistic knowledge. They treat
natural language text as simple sequences of sym-
bols and automatically reflect the ?hidden? struc-
ture through the way it affects the distributions
of words in various (flat, unstructured) contexts.
However, the effectiveness of n-gram models in
English (and similar languages) is partially pred-
icated on two properties of those languages: rel-
atively low levels of inflectional morphology, and
relatively fixed word order.
As is well-known by now, languages with
more elaborate morphology (more morphemes per
word, more distinctions within the same number
of morphological slots, and/or fewer uninflected
words) present greater data sparsity problems for
language models. This data sparsity limits the
ability of n-gram models to capture the depen-
dencies between open-class morphemes, but also
closed class morphemes. The information ex-
pressed by short function words in English is typ-
ically expressed by the inflectional morphology in
languages with more elaborate morphological sys-
tems. Word-based n-gram models have no way
of representing the function morphemes in such
a language. In addition, for n-gram models to
capture inter-word dependencies, both words have
to appear in the n-gram window. This will hap-
pen more consistently in languages with relatively
fixed word order, as compared to languages with
relatively free word order.
Thus even though n-grams models can be built
26
without any hand-coding of linguistic knowledge,
they are not truly language independent. Rather,
their success depends on typological properties
of the languages they were first developed for.
A more linguistically-informed (and thus more
language independent) approach to n-gram mod-
els is the factored language model approach of
Bilmes and Kirchhoff (2003). Factored language
models address the problems of data-sparsity in
morphologically complex languages by represent-
ing words as bundles of features, thus capturing
dependencies between subword parts of adjacent
words.
A second example of subtle language depen-
dence comes from Dasgupta and Ng (2007), who
present an unsupervised morphological segmenta-
tion algorithm meant to be language-independent.
Indeed, this work goes much further towards lan-
guage independence than is the norm (see Section
3). It is tested against data from English, Bengali,
Finnish and Turkish, a particularly good selection
of languages in that it includes diversity along a
key dimension (degree of morphological complex-
ity), as well as representatives of three language
families (Indo-European, Uralic, and Altaic). Fur-
thermore, the algorithm is designed to detect more
than one prefix or suffix per word, which is impor-
tant for analyzing morphologically complex lan-
guages. However, it seems unrealistic to expect a
one-size-fits-all approach to be achieve uniformly
high performance across varied languages, and,
in fact, it doesn?t. Though the system presented
in (Dasgupta and Ng, 2007) outperforms the best
systems in the 2006 PASCAL challenge for Turk-
ish and Finnish, it still does significantly worse on
these languages than English (F-scores of 66.2 and
66.5, compared to 79.4).
This seems to be due to an interesting interac-
tion of at least two properties of the languages
in question. First, the initial algorithm for dis-
covering candidate roots and affixes relies on the
presence of bare, uninflected roots in the train-
ing vocabulary, extracting a string as a candidate
affix (or sequence of affixes) when it appears at
the end (or beginning) of another string that also
appears independently. In Turkish and Finnish,
verbs appear as bare roots in many fewer con-
texts than in English.2 This is also true in Ben-
2In Finnish, depending on the verb class, the bare
root may appear in negated present tense sentences, in
second-person singular imperatives, and third-person singu-
lar present tense, or not at all (Karlsson and Chesterman,
gali, and the authors note that their technique for
detecting allomorphs is critical to finding ?out-of-
vocabulary? roots (those unattested as stand-alone
words) in that language. However, the technique
for finding allomorphs assumes that ?roots exhibit
the character changes during attachment, not suf-
fixes? (p.160), and this is where another property
of Finnish and Turkish becomes relevant: Both of
these languages exhibit vowel harmony, where the
vowels in many suffixes vary depending on the
vowels of the root, even if consonants intervene.
Thus I speculate that at least some of the reduced
performance in Turkish and Finnish is due to the
system not being able to recognize variants of the
same suffixes as the same, and, in addition, not be-
ing able to isolate all of the roots.
Of course, in some cases, one language may
represent, in some objective sense, a harder prob-
lem than another. A clear example of this is En-
glish letter-to-phoneme conversion, which, as a re-
sult of the lack of transparency in English orthog-
raphy, is a harder problem that letter-to-phoneme
conversion in other languages. Not surprisingly,
the letter-to-phoneme systems described in e.g.
(Jiampojamarn et al, 2008) and (Bartlett et al,
2008) do worse on the English test data than they
do on German, Dutch, or French. On the other
hand, just because one language may present a
harder problem than the other doesn?t mean that
system developers can assume that any perfor-
mance differences can be explained in such a way.
If one aims to create a language-independent sys-
tem, then one must explore the possibility that
the system includes assumptions about linguis-
tic structure which do not hold up across all lan-
guages.
The conclusions I would like to draw from
these examples are as follows: A truly language-
independent system works equally well across lan-
guages. When a system that is meant to be lan-
guage independent does not in fact work equally
well across languages, it is likely because some-
thing about the system design is making implicit
assumptions about language structure. These as-
sumptions are typically the result of ?overfitting?
to the original development language(s).3 In Sec-
1999). In Turkish, the bare root can function as a familiar
imperative, but other forms are inflected (Lewis, 1967; Un-
derhill, 1976).
3Here I use the term ?overfitting? metaphorically, to call
out the way in which, as the developers of NLP methodol-
ogy, we rely on our intuitions about the structure of the lan-
guage(s) we?re working with and the feedback we get by test-
27
tion 4, I will argue that the best way to achieve lan-
guage independence is by including, rather than
eschewing, linguistic knowledge.
3 Language Independence and Language
Representation at ACL
This section reports on a survey of the 119 long
papers from ACL2008:HLT. Of these 119 papers,
18 explicitly claimed (16) or suggested (2) that
the methods described could be applied to other
languages. Another 13 could be read as implic-
itly claiming that. Still others present the kind
of methodology that often is claimed to be cross-
linguistically applicable, such as statistical ma-
chine translation. Of the 16 explicitly claiming
language independence, 7 evaluated their systems
on multiple languages. Since many of the tech-
niques are meant to be cross-linguistically appli-
cable, I collected information about the languages
studied in all 119 papers. Table 1 groups the pa-
pers by how many languages (or language pairs)
they study. The three papers studying zero lan-
guages involved abstract, formal proofs regarding,
e.g., grammar formalisms. 95 of the papers stud-
ied just one language or language pair.
Languages or language Number of papers
pairs considered
0 3
1 95
2 13
3 3
4 2
5 1
12 1
13 1
Total 119
Table 1: Number of languages/language pairs con-
sidered
The two papers looking at the widest variety of
languages were (Ganchev et al, 2008) and (Nivre
and McDonald, 2008). Ganchev et al (2008)
explore whether better alignments lead to better
translations, across 6 language pairs, in each di-
rection (12 MT systems), collecting data from a
variety of sources. Nivre and McDonald (2008)
present an approach to dependency parsing which
integrates graph-based and transition-based meth-
ods, and evaluate the result against the 13 datasets
ing our ideas against particular languages.
provided in the CoNLL-X shared task (Nivre et al,
2007).
It is encouraging to see such use of multilingual
datasets; the field as a whole will be in a better
position to test (and improve) the cross-linguistic
applicability of various methods to the extent that
more such datasets are produced. It is worth not-
ing, however, that the sheer number of languages
tested is not the only important factor: Because
related languages tend to share typological prop-
erties, it is also important to sample across the
known language families.
Tables 2 and 3 list the languages and language
pairs studied in the papers in the survey. Table
2 presents the data on methodologies that involve
producing results for one language at a time, and
groups the languages by genus and family (accord-
ing to the classification used by the World Atlas of
Language Structures Online4). Table 3 presents
the data on methodologies that involve symmetri-
cal (e.g., bilingual lexicon extraction) or asymmet-
rical (e.g., MT) language pairs.5
The first thing to note in these tables is the con-
centration of work on English: 63% of the single-
language studies involved English, and all of the
language pairs studied included English as one
member. In many cases, the authors did not ex-
plicitly state which language they were working
on. That it was in fact English could be inferred
from the data sources cited, in some cases, or from
the examples used, in others. The common prac-
tice of not explicitly stating the language when it is
English would seem to follow from a general sense
that the methods should be crosslinguistically ap-
plicable.
The next thing to note about these tables is that
many of the languages included are close relatives
of each other. Ethnologue6 lists 94 language fami-
lies; ACL2008:HLT papers studied six. Of course,
the distribution of languages (and perhaps more
to the point, speakers) is not uniform across lan-
4http://wals.info (Haspelmath et al, 2008); Note that
Japanese is treated as a language isolate and Chinese is the
name for the genus including (among others) Mandarin and
Cantonese.
5The very interesting study by Snyder and Barzilay (2008)
on multilingual approaches to morphological segmentation
was difficult to classify. Their methodology involved jointly
analyzing two languages at a time in order to produce mor-
phological segmenters for each. Since the resulting systems
were monolingual, the data from these studies are included in
Table 2.
6http://www.ethnologue.com/ethno docs/distribution.asp,
accessed on 6 February 2009.
28
Language Studies Genus Studies Family Studies
N % N % N %
English 81 63.28 Germanic 91 71.09 Indo-European 109 85.16
German 5 3.91
Dutch 3 2.34
Danish 1 0.78
Swedish 1 0.78
Czech 3 2.34 Slavic 8 6.25
Russian 2 1.56
Bulgarian 1 0.78
Slovene 1 0.78
Ukranian 1 0.78
Portuguese 3 2.34 Romance 8 6.25
Spanish 3 2.34
French 2 1.56
Hindi 2 1.56 Indic 2 1.56
Arabic 4 3.13 Semitic 9 7.03 Afro-Asiatic 9 7.03
Hebrew 4 3.13
Aramaic 1 0.78
Chinese 5 3.91 Chinese 5 3.91 Sino-Tibetan 5 3.91
Japanese 3 2.34 Japanese 3 3.24 Japanese 3 3.24
Turkish 1 0.78 Turkic 1 0.78 Altaic 1 0.78
Wambaya 1 0.78 West Barkly 1 0.78 Australian 1 0.78
Total 128 100.00 128 100.00 128 100.00
Table 2: Languages studied in ACL 2008 papers, by language genus and family
Source Target N Source Target N Symmetrical pair N
Chinese English 9 English Chinese 2 English, Chinese 3
Arabic English 5 English Arabic 2 English, Arabic 1
French English 2 English French 2 English, French 1
Czech English 1 English Czech 2 English, Spanish 1
Finnish English 1 English Finnish 1
German English 1 English German 1
Italian English 1 English Italian 1
Spanish English 1 English Spanish 1
English Greek 1
English Russian 1
Table 3: Language pairs studied in ACL 2008 papers
29
Language family Living Examples % pop.
lgs.
Indo-European 430 Welsh 44.78
Pashto
Bengali
Sino-Tibetan 399 Mandarin 22.28
Sherpa
Burmese
Niger-Congo 1,495 Swahili 6.26
Wolof
Bissa
Afro-Asiatic 353 Arabic 5.93
Coptic
Somali
Austronesian 1,246 Bali 5.45
Tagalog
Malay
Total 3,923 84.7
Table 4: Six most populous language families,
from Ethnologue
guage families. Table 4 gives the five most pop-
ulous language families, again from Ethnologue.7
These language families together account for al-
most 85% of the world?s population.
Of course, language independence is not the
only motivation for machine-learning approaches
to NLP. Others include scaling to different genres
within a language, robustness in the face of noisy
input, the argument (in some cases) that creating
or obtaining training data is cheaper than creating
a rule-based system, and the difficulty in certain
tasks of creating rule-based systems. Nonetheless,
to the extent that language independence is an im-
portant goal, the field needs to improve both its
testing of language independence and its sampling
of languages to test against.
4 Linguistic Knowledge
Typically, when we think of linguistic knowledge-
based NLP systems, what comes to mind are com-
plicated, intricate sets of language-specific rules.
While I would be the last to deny that such sys-
tems can be both linguistically interesting and the
best approach to certain tasks, my purpose here is
7Ibid. Example languages are included to give the reader
a sense of where these language families are spoken, and are
deliberately chosen to represent the breadth of each language
family while still being relatively recognizable to the EACL
audience.
to point out that there are other kinds of linguis-
tic knowledge that can be fruitfully incorporated
into NLP systems. In particular, the results of lan-
guage typology represent a rich source of knowl-
edge that, by virtue of being already produced by
the typologists, can be relatively inexpensively in-
corporated into NLP systems.
Linguistic typology is an approach to the sci-
entific study of language which was pioneered in
its modern form by Joseph Greenberg in the 1950s
and 1960s (see e.g. Greenberg, 1963).8 In the in-
tervening decades, it has evolved from a search
for language universals and the limits of language
variation to what Bickel (2007) characterizes as
the study of ?what?s where why?. That is, typol-
ogists are interested in how variations on particu-
lar linguistic phenomena are distributed through-
out the world?s languages, both in terms of lan-
guage families and geography, and how those dis-
tributions came to be the way they are.
For the purposes of improving language-
independent NLP systems, we are primarily con-
cerned with ?what? and ?where?: Knowing
?what? (how languages can vary) allows us to both
broaden and parameterize our systems. Know-
ing ?where? also helps with parameterizing, as
well as with selecting appropriate samples of lan-
guages to test the systems against. We can broaden
them by studying what typologists have to say
about our initial development languages, and iden-
tifying those characteristics we might be implic-
itly relying on. This is effectively what Bilmes
and Kirchhoff (2003) did in generalizing n-gram
language models to factored language models.
We can parameterize our systems by identifying
and specifically accommodating relevant language
types (?what?) and then using databases produced
by typologists to map specific input languages to
types (?where?).
The practical point of language independence is
not to be able to handle in principle any possi-
ble language in the universe (human or extrater-
restrial!), but to improve the scalability of NLP
technology across the existing set of human lan-
guages. There are approximately 7,000 languages
spoken today, of which 347 have more than 1 mil-
lion speakers.9 An NLP system that uses differ-
ent parameters or algorithms for each one of a set
8See (Ramat, to appear) for discussion of much earlier
approaches.
9http://wwww.ethnologue.com/ethno docs/distribution.asp;
accessed 6 February 2009
30
of known languages is not language independent.
One that uses different parameters or even algo-
rithms for different language types, and includes as
a first step the classification of the input language,
either automatically or with reference to some ex-
ternal typological database, is language indepen-
dent, at least on the relevant, practical sense.
The preeminent typological database among
those which are currently publicly available is
WALS: The World Atlas of Linguistic Structures
Online (Haspelmath et al, 2008). WALS currently
includes studies of 142 chapters studying linguis-
tic features, each of which defines a dimension of
classification, describes values along that dimen-
sion, and then classifies a large sample of lan-
guages. It is also possible to view the data on a
language-by-language basis. These chapters rep-
resent concise summaries, as well as providing
pointers into the relevant literature for more infor-
mation.
To give a sense of how this information might
be of relevance to NLP or speech systems, here is
a brief overview of three chapters:
Maddieson (2008) studies tone, or the use of
pitch to differentiate words or inflectional cate-
gories. He classifies languages into those with no
tone systems, those with simple tone systems (a
binary contrast between high and low tone), and
those with more complex tone systems (more than
two tone types). Nearly half of the languages in
the sample have some tone, and Maddieson points
out that the sample in fact underestimates the num-
ber of languages with tone.
Dryer (2008b) investigates prefixing and suffix-
ing in inflectional morphology, looking at 10 com-
mon types of affixes (from case affixes on nouns to
adverbial subordinator affixes on verbs), and us-
ing them to classify languages in terms of tenden-
cies towards prefixing or suffixing.10 His result-
ing categories are: little affixation, strongly suf-
fixing, weakly suffixing, equal prefixing and suf-
fixing, weakly prefixing, and strongly prefixing.
The most common category (382/894 languages)
is predominantly suffixing.
Dryer (2008a) investigates the expression of
clausal negation. One finding of note is that all
languages studied use dedicated morphemes to ex-
press negation. This contrasts with the expression
of yes-no questions which can be handled with
10For the purposes of this study, he sets aside less com-
mon inflectional strategies such as infixing, tone changes, and
stem changes.
word order changes, intonation, or no overt mark
at all. The types of expression of clausal negation
that Dryer identifies are: negative affix, negative
auxiliary verb, and negative particle. In addition,
some languages are classified as using a negative
word that may be a verb or may be a particle, as
having variation between negative affixes and neg-
ative words, and as having double (or two-part)
negation, where each negative clause requires two
markers, one before the verb, and one after it.
These examples illustrate several useful aspects
of the knowledge systematized by linguistic typol-
ogy: First, languages show variation beyond that
which one might imagine looking only at a few
familiar (and possibly closely related) languages.
Second, however, that variation is still bounded:
Though typologists are always interested in find-
ing new categories that stretch the current classifi-
cation, for the purposes of computational linguis-
tics, we can get very far by assuming the known
types exhaust the possibilities. Finally, because of
the work done by field linguists and typologists,
this knowledge is available as high-level gener-
alizations about languages, of the sort that can
inform the design of linguistically-sophisticated,
language-independent NLP systems.
5 Conclusion
This paper has briefly argued that the best way
to create language-independent systems is to in-
clude linguistic knowledge, specifically knowl-
edge about the ways in which languages vary in
their structure. Only by doing so can we ensure
that our systems are not overfitted to the devel-
opment languages. Furthermore, this knowledge
is relatively inexpensive to incorporate, as it does
not require building or maintaining intricate rule
systems. Finally, if the field as a whole values
language independence as a property of NLP sys-
tems, then we should ensure that the languages we
select to use in evaluations are representative of
both the language types and language families we
are interested in.
Acknowledgments
I am grateful to Stephan Oepen and Timothy Bald-
win for helpful discussion. Any remaining in-
felicities are my own. This material is based in
part upon work supported by the National Science
Foundation under Grant No. 0644097. Any opin-
ions, findings, and conclusions or recommenda-
31
tions expressed in this material are those of the au-
thor and do not necessarily reflect the views of the
National Science Foundation.
References
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
SVMs for letter-to-phoneme conversion. In Pro-
ceedings of ACL-08: HLT, pages 568?576, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Balthasar Bickel. 2007. Typology in the 21st centure:
Major current developments. Linguistic Typology,
pages 239?251.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In in Proceedings of HLT/NACCL, 2003, pages 4?6.
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 155?
163, Rochester, New York, April. Association for
Computational Linguistics.
Matthew S. Dryer. 2008a. Negative morphemes.
In Martin Haspelmath, Matthew S. Dryer, David
Gil, and Bernard Comrie, editors, The World At-
las of Language Structures Online. Max Planck
Digital Library, Munich. Available online at
http://wals.info/feature/112. Accessed on 2009-02-
07.
Matthew S. Dryer. 2008b. Prefixing vs. suffixing
in inflectional morphology. In Martin Haspelmath,
Matthew S. Dryer, David Gil, and Bernard Comrie,
editors, The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich. Avail-
able online at http://wals.info/feature/26. Accessed
on 2009-02-07.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Joseph Greenberg. 1963. Some universals of grammar
with particular reference to the order of meaningful
elements. In Univerals of Language, pages 73?113.
MIT Press, Cambridge.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2008. The World Atlas
of Language Structures Online. Max Planck Digital
Library, Munich. http://wals.info.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Pro-
ceedings of ACL-08: HLT, pages 905?913, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Fred Karlsson and Andrew Chesterman. 1999.
Finnish: An Essential Grammar. Routledge, Lon-
don.
Geoffrey Lewis. 1967. Turkish Grammar. Clarendon
Press, Oxford.
Ian Maddieson. 2008. Tone. In Martin Haspelmath,
Matthew S. Dryer, David Gil, and Bernard Comrie,
editors, The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich. Avail-
able online at http://wals.info/feature/13. Accessed
on 2009-02-07.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Paolo Ramat. to appear. The (early) history of linguis-
tic typology. In The Oxford Handbook of Linguistic
Typology. Oxford University Press, Oxford.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of ACL-08: HLT,
pages 737?745, Columbus, Ohio, June. Association
for Computational Linguistics.
Robert Underhill. 1976. Turkish Grammar. MIT
Press, Cambridge, MA.
32
Coling 2010: Poster Volume, pages 1068?1076,
Beijing, August 2010
Argument Optionality in the LinGO Grammar Matrix
Safiyyah Saleem
University of Washington
ssaleem@u.washington.edu
Emily M. Bender
University of Washington
ebender@u.washington.edu
Abstract
We present a library of implemented
HPSG analyses for argument optional-
ity based on typological studies of this
phenomenon in the world?s languages,
developed in the context of a grammar
customization system that pairs a cross-
linguistic core grammar with extensions
for non-universal phenomena on the ba-
sis of user input of typological proper-
ties. Our analyses are compatible with
multiple intersecting phenomena, includ-
ing person, number, gender, tense, aspect
and morphological rule formulation. We
achieve 80-100% coverage on test suites
from 10 natural languages.
1 Introduction
The LinGO Grammar Matrix customization sys-
tem (Bender et al, 2002; 2010) is a web-based
tool that creates starter grammars based on users?
input to a questionnaire. The system comprises a
core grammar covering linguistic phenomena that
are posited to be universal (e.g. semantic compo-
sitionality) and a set of libraries providing anal-
yses for phenomena that vary across languages
(e.g. case). These resources are developed in the
context of HPSG (Pollard and Sag, 1994), Mini-
mal Recursion Semantics (Copestake et al, 2005),
and the LKB grammar development environment
(Copestake, 2002).
Previous to the work reported here, the Gram-
mar Matrix customization system did not handle
argument optionality?the possibility of leaving
arguments unexpressed in lieu of overt pronouns.
This phenomenon, also called pro-drop, argument
drop, or null instantiation, is extremely common:
according to Dryer (2008), 79% of the 674 lan-
guages sampled cannot or do not normally use in-
dependent pronouns in subject position. Accord-
ingly, adding it to the customization system im-
proves the system?s ability to handle a large class
of core sentences in many languages.
For example, in Modern Standard Arabic [arb]
(Semitic), overt pronominal subjects are dropped
in non-emphatic contexts. Previously, the system
was able to model only the longer variant of (1).
(1) (hiyya)
(3.FEM.SG)
naama-t
sleep.PAST-3.FEM.SG
She slept. [arb]
Furthermore, there was no way to adequately ac-
count for languages such as Hausa [hau] (Chadic)
which do not allow overt simple pronominal sub-
jects and prohibit overt objects after certain verb
forms. The grammar would predict the opposite
grammaticality for the examples in (2).
(2) (*n??)
(*1.SG)
na?-san
1.SG.COMP-know
amsa?
answer
I know the answer. [hau]
It might seem that these facts could be han-
dled by adding a rule that allows arguments to
be dropped if an appropriate option is checked in
the customization system. However, the data from
Arabic and Hausa suggest that such an approach
would be insufficient, as languages place different
constraints on the contexts in which overt argu-
ments are required or prohibited.
In ?2 we discuss the broad range of typological
variation in argument optionality in the world?s
languages. In ?3 we offer a set of HPSG analy-
ses for these patterns. ?4 explains how these anal-
yses were incorporated into the Grammar Matrix
1068
customization system and integrated with the ex-
isting libraries. We then present the results of a
three-tiered evaluation of the implemented system
in ?5. The results demonstrate that the system is
capable of accurately modeling the attested syn-
tactic argument optionality patterns exhibited by
a typologically diverse group of languages as well
as the currently unattested but logically possible
co-occurrence restrictions on affixes and overt ar-
guments. To our knowledge, this is the first such
system. The paper closes with a brief look at how
the library could be extended even further to cap-
ture the range of semantic distinctions.
2 Typological Patterns
The typological literature shows that argument
optionality is extremely common: Dryer (2008)
found that of 674 geographically and genetically
diverse languages, only 141 normally or obli-
gatorily used independent pronominal subjects.
Dryer distinguishes 4 categories in the remaining
533 languages, corresponding to how information
about the person, number, and gender (PNG) of
the subject is encoded: affixation on the verb, cl-
itics on variable hosts, no encoding, or a mixed
strategy. In addition, there are other dimensions
in which languages vary, e.g., constraints on con-
texts in which dropping is done (see (1)?(2)).
Although we were unable to find a similar com-
prehensive survey of unexpressed objects, there
is evidence to suggest that it too may be very
widespread. In particular, lexically-licensed ob-
ject dropping seems to be very common. Even En-
glish, which has a very strong preference for overt
subjects, can be analyzed as licensing lexically-
based object dropping (Fillmore, 1986). As with
subject dropping, we also found a number of dif-
ferent co-occurrence restrictions on the presence
of verbal affixes and overt objects. Some lan-
guages always encode the PNG of an object on the
verb, others optionally do so if an overt object is
present and obligatorily do so if one is not, while
still others do not encode this information at all.
Drawing on work by Dryer and others, Table 1
summarizes the 6 major dimensions along which
the rules licensing argument dropping differ. The
first constraint is syntactic context. Most lan-
guages that license argument dropping do so re-
gardless of tense/aspect, mood, or person. Finnish
[fin] and Hebrew [heb] are two notable exceptions
(Vainikka and Levy, 1999).
The second constraint, lexically-based licens-
ing, is most commonly found in object dropping.
For example, while English usually prohibits ar-
gument dropping, it arguably licenses it with
verbs such as ?found out?, ?agree?, and ?promise?
(Fillmore, 1986). Lexically-based subject drop
is found in Tamil [tam], which generally licenses
subject dropping aside from some weather related
verbs (Asher, 1985).
The third constraint, noun phrase type, captures
the difference between a language such as Hausa
which generally prohibits independent pronouns
from appearing as subjects and other languages,
which allow pronouns in subject position (possi-
bly with emphatic interpretations).
The fourth constraint concerns the position of
PNG markers. Of the languages with subject PNG
markers and subject dropping, many encode sub-
ject PNG as a verbal affix. This pattern is ex-
hibited by such geographically and genetically di-
verse languages as Spanish [spa], Arabic [arb],
West Greenlandic [kal], Tamil [tam], and Nkore-
Kiga [nyn]. Other languages such as Chemehuevi
[ute], Polish [pol], and Warlpiri [wbp] make use
of a clitic which can attach to different types of
hosts (Dryer, 2008).
The final two constraints concern co-
occurrence restrictions between PNG markers
and overt objects. In some Bantu languages such
as Nkore-Kiga, a verbal affix is not used unless
the object precedes the verb or is pronominal.
Object markers are not used when a full NP
follows the verb (Taylor, 1985). In written French
[fra], verbal affixes1 are required if an object is
dropped and not permitted if it is overt. In Arabic,
for most transitive verbs, an object marker is
required if an object is dropped and is optional
if it is present. Hausa exhibits a more complex
pattern: for tenses in which the verbal affix
denoting PNG is morphologically separable from
the tense marker, the PNG affix is optional if an
overt noun phrase is present and required if it is
not (Newman, 2000).
1See (Miller and Sag, 1997) for convincing arguments
that so-called ?clitics? in French are actually affixes.
1069
Constraint (GF) Possible Values
Syntactic context (SUBJ) { All, select } tenses/aspects/moods/persons
Lexically-based (SUBJ, OBJ) { All, select } verbs
Noun phrase type (SUBJ, OBJ) Independent pronouns { allowed, prohibited }
Placement of PNG marker (SUBJ) { Verb, variable host }
PNG marking w/ dropped argument (OBJ) { Required, optional, not permitted }
PNG marking w/ overt argument (OBJ) { Required, optional, not permitted }
Table 1: Typological variation in licensing argument dropping
Noting these differences led us to posit that
when an argument is dropped, there are three pos-
sibilities. A verbal affix can be: not permitted,
optional, or required. The same three possibilities
exist for overt objects as well. Combining what
happens when an argument is dropped with what
happens when it is present, gives us nine logically
possible co-occurrence patterns.
Our review of the typological literature has
shown that languages place different constraints
on argument dropping. These constraints can
be lexical, syntactic, or related to affixation and
affix/overt-argument co-occurrence restrictions.
3 Analysis
This section presents HPSG analyses modeling
the six dimensions of variation described in ?2.
HPSG models natural language by positing lex-
ical entries, lexical rules, and phrase structure
rules, all described in terms of feature struc-
tures. A central idea, inspired by earlier work
in Categorial Grammar (Ajdukiewicz, 1935; Bar-
Hillel, 1953), is the notion of valence features.
These list-valued features (including SUBJ and
COMPS) contain information about the dependents
required by a head. The valence lists are projected
up the tree within the domain of each head, but
shortened as the dependents are realized. A sen-
tence is thus a verbal projection with empty SUBJ
and COMPS lists.
In this context, argument dropping is the short-
ening of a valence list without the overt realiza-
tion of the argument. Formally, this can be ac-
complished in at least three different ways: (1) In
the mapping of arguments from the ARG-ST (ar-
gument structure) feature to the valence lists, one
or more arguments can be suppressed, (2) lexical
rules can operate on the valence lists, shortening
them, or (3) unary (non-branching) phrase struc-
ture rules can cancel off valence elements. In this
work, we take the third approach, as we find it
affords us the most flexibility to deal with varia-
tions across languages in constraints on argument
optionality, while promoting similarity of analy-
ses across languages.
We control the applicability of the unary-
branching rules with the boolean feature OPT,
marked on elements of valence lists.2 For lan-
guages which allow subject/object dropping, we
instantiate new phrase structure rules: head-opt-
subj-phrase and/or head-opt-comp-phrase. These
rules allow the head verb to satisfy a valence re-
quirement without combining with another ex-
pression. To undergo these rules, the head daugh-
ter (the verb) must specify that the argument that
is to be dropped is compatible with [OPT +]. This
is sufficient to account for many languages. How-
ever, to ensure that languages which have lexical,
syntactic context, and affix co-occurrence restric-
tions do not overgenerate, further additions to the
grammar are necessary.
For lexical and affix-co-occurrence restrictions,
we prevent overgeneration by manipulating the
OPT feature. In languages which only license
argument dropping for certain lexical items, we
force those verbs which do not allow argument
dropping to have arguments that are constrained
to be [OPT ?]. This prevents them from under-
going the subject/object dropping rules. Verbs
are then classified into four different types based
on whether or not they allow subject and/or ob-
ject dropping. Individual lexical items instantiate
these types. For those verbs which do not allow
a particular argument to be dropped, the only way
to satisfy the valence requirement is to combine
with an overt argument.
2This feature was provided by the core Matrix but was not
previously used in the customization system. To our knowl-
edge it is not commonly used within HPSG analyses aside
from in grammars that were derived from the Matrix.
1070
Dropped/Overt Argument Affix Overt Arg Rule No-Marker-Rule Marker-Rule Transitive Verb Lex
required/required underspecified none underspecified needs lex rule
optional/optional underspecified none underspecified underspecified
not permitted/not permitted underspecified none none underspecified
required/optional OPT ? OPT ? underspecified needs lex rule
optional/not permitted OPT ? none OPT + underspecified
not permitted/required OPT ? OPT + OPT ? needs lex rule
required/not permitted OPT ? OPT ? OPT + needs lex rule
optional/required OPT ? OPT + underspecified needs lex rule
not permitted/optional OPT ? none OPT ? underspecified
Table 2: Constraints associated with logically possible affix co-occurrence
Languages with complex affix co-occurrence
restrictions are modeled by manipulating the OPT
feature in a different way: Constraints are placed
on lexical and phrase structure rules, as well as
on lexical types. In particular, we constrain the
rules which combine verbs with overt arguments
to check that that argument position is compati-
ble with [OPT ?]. This allows the lexical rules
attaching the affixes to constrain the optionality
of the corresponding argument position. In some
of the nine logical possibilities, enforcing these
constraints requires sending the verb through ?no-
marker? lexical rules so that constraints associ-
ated with markerless verbs can be enforced. Ta-
ble 2 summarizes the constraints on the OPT fea-
ture on lexical and phrase structure rules, as well
as the constraints on lexical types. The first col-
umn of this table lists the nine logically possible
combinations described in ?2. For example, the
row labeled ?required/required? gives the analysis
for a language like West Greenlandic, which al-
lows object dropping and always requires an ob-
ject marker on the verb regardless of whether or
not an overt object is present. In such a language,
neither the lexical rules nor the overt-complement
phrase structure rule constrain OPT, but the tran-
sitive verb lex type is required to undergo some
object marking lexical rule.
For licensing that is based on syntactic context
(subject dropping only) such as the Finnish and
Hebrew examples presented in ?2, we place con-
straints on the daughter of the unary subject drop
rule which restrict its application to the right con-
texts. For example, to account for the argument
optionality pattern present in Finnish, we con-
strain the head-opt-subj-phrase rule to require that
the item on the head daughter?s SUBJ list be spec-
?
?
head-opt-subj
subj ? ?
obj ? ?
?
?
?
?
head-comp
subj ? NP[3sg.m] ?
obj ? ?
?
?
?
?
no-marker-lex-rule
subj ? NP[3sg.m] ?
obj ? NP[OPT ?] ?
?
?
?
?
3sgm-subj-lex-rule
subj ? NP[3sg.m] ?
obj ? NP ?
?
?
?
?
trans-verb-lex
subj ? NP ?
obj ? NP ?
?
?
ishtaraa
acc-lex-rule
noun-lex
kitaab-an
Figure 1: Parse structure for (3)
ified as non-third-person ([PER non-third]). Verbs
not meeting this constraint are only allowed to
empty their SUBJ lists by combining with an overt
subject via the standard, binary head-subj-phrase
rule. We have not seen a language which licenses
subject dropping in syntactic contexts which do
not form a natural class according to our feature
system. However, our analysis easily lends it-
self to modeling this type of pattern if it exists by
creating multiple different subtypes of the subject
drop rule.
We close this section by illustrating our analysis
with an example from Arabic. The sentence in (3)
involves subject drop and an overt object. Since
the object is overt, the verb bears only marking of
subject PNG. The grammar that our system gener-
ates for Arabic assigns (3) the structure sketched
in Figure 1.
(3) ishtaraa
3ms.buy.past
kitaab-an
book-acc
He bought a book
1071
4 Customized Grammar Creation
Before the addition of the argument optionality
library, the phenomena covered in the Grammar
Matrix customization system included word order,
person, number, gender, case, tense/aspect, coor-
dination, matrix yes-no questions, and sentential
negation. The user is also allowed to specify lex-
ical items and the morphological rules associated
with each of them. Each of the phenomena corre-
spond to a page of the questionnaire.
As the user answers questions, the choices are
saved in a ?choices? file. The questionnaire is dy-
namic and the answers contained in the ?choices?
file affect the types of features that the user is
able to choose from on subsequent pages. For
example, if the user describes the language as
having 1st, 2nd, and 3rd persons on the Person
page, then on the lexicon page, the user can cre-
ate separate noun types for each person. Once
the ?choices? file contains responses to required
sections, the user is able to create the customized
starter grammar by clicking on the ?create gram-
mar? button. This invokes the customization script
which uses the responses contained in the file to
create a grammar that is compatible with the LKB
grammar development environment.
Our implementation entailed additions to two
major components of the system: the web-based
questionnaire and the customization script. To de-
termine which, if any, of the analyses presented
in ?3 should be included in the customized gram-
mar, we needed to elicit the type of argument op-
tionality pattern the language exhibited. Thus, we
added an Argument Optionality page to the ques-
tionnaire. The page is divided into two sections?
one for subject dropping and one for object drop-
ping. In the section on subject dropping, the user
is asked whether subject dropping exists and if
so, whether it is context-dependent. For context-
dependent subject dropping, the user is allowed
to specify the syntactic contexts in which subject
dropping is licensed by choosing from a multi-
select list of features. There is the option to create
multiple contexts. The features that appear in the
list are drawn from those that the user chose on
previous pages in the questionnaire. The user is
also directed to select whether subject dropping
is lexically-based, whether affixes are required,
optional or not permitted with overt arguments
and whether affixes are required, optional or not
permitted with dropped arguments. The ques-
tions presented in the object dropping section are
identical to those in the subject dropping section
with the exception that there is no question about
context-dependent object dropping.
Since some of the constraints must be placed on
individual lexical items and morphological rules,
the page also includes instructions to the user on
additional steps that need to be taken when com-
pleting the Lexicon page. For example, when de-
scribing a language where affixes are optional if
an argument is dropped and not permitted if an
overt argument is present, users are instructed to
select ?overt-arg-not-permitted? for those affixes
on the Lexicon page.
The changes to the customization script in-
cluded adding each of the analyses described in
?3 along with a mechanism for determining which
of the analyses should be included in the gram-
mar depending on the choices related to argu-
ment optionality, lexical items, and morphological
rules contained in the ?choices? file. The result-
ing customized grammars include the rules and
constraints necessary to allow and prohibit strings
that do not contain overt arguments based upon
the facts of a particular language as described by
the user in the questionnaire.
5 Evaluation
The evaluation was conducted in a three stage pro-
cess. Each stage involves constructing a set of test
suites containing grammatical and ungrammatical
strings representing the argument optionality pat-
tern of a set of languages, generating grammars
for the languages by answering the Grammar Ma-
trix questionnaire, using the grammars to parse the
sentences in the test suite, and hand-verifying the
results. The three stages differed in the nature of
the languages, the method by which the languages
were selected, and the breadth of the customized
grammars. The test suites are small, as they are
specifically targeted at the phenomenon of argu-
ment optionality, but representative in the sense
that they cover the space of relevant contrasts in
each language.
1072
5.1 Set 1: Pseudo-Languages
In the first stage, we tested the analyses presented
in ?3 by creating and then using the Grammar Ma-
trix customization system to generate grammars
for 38 pseudo-languages (sets of strings with as-
sociated grammaticality assignments) which col-
lectively exhaustively exhibit each of the lexical,
syntactic context or affix co-occurrence restric-
tion patterns described in Table 1 (?2). All of
the possible values identified for these given pat-
terns are present in at least one language, as well
as cross-classifications of different dimensions of
constraints where appropriate. For example, there
are pseudo-languages which share the property
of always requiring object markers but differ in
that one has lexically licensed object dropping and
the other general object dropping. These pseudo-
languages test the argument optionality analyses
in isolation in that argument optionality is not con-
strained by other phenomena such as word order.
The customized grammars were able to accu-
rately parse grammatical strings and rule out un-
grammatical ones. Coverage on this set of 38
pseudo-languages was 100% with 0% overgener-
ation and no spurious ambiguity, thus validating
the functioning of our analyses across the known
typological space.
5.2 Set 2: Illustrative Languages
Next, we tested the system?s performance in mod-
eling part of a natural language. For this stage
we deliberately chose several languages which ex-
emplified interesting licensing and co-occurrence
restriction patterns, including some which were
considered during the development of the system.
Each test suite included examples of grammatical
and ungrammatical strings that were constructed
based on the descriptions of the language given
in the following sources: Suleiman 1990 (Ara-
bic), Sulkala and Merja 1992 (Finnish), Newman
2000 (Hausa), and Asher 1985 (Tamil). As the
test suites were designed to evaluate argument op-
tionality, we restricted the test items to this phe-
nomenon only. Other syntactic phenomena were
only included if they affected the argument op-
tionality pattern in the language. For example,
gender distinctions were considered only for lan-
guages in which this was relevant to affix mark-
ing. A brief description of the argument optional-
ity patterns found in these languages follows.
Arabic [arb] (Semitic) Pronominal subjects
and objects are generally dropped. Subject affixes
are always required whether or not an overt noun
phrases is present. Affixes marking object per-
son, number, and gender are required for strictly
transitive verbs when an overt noun phrase is not
present. Other transitive verbs appear to allow ob-
ject drop without the object affix.
Finnish [fin] (Uralic) First and second person
subjects are freely dropped and markers appear
on the verb whether or not an overt noun phrase
is present. Third person subjects are not allowed
to be dropped with a referential interpretation;
however, third person pronouns are obligatorily
dropped for what Sulkala and Merja (1992) de-
scribe as a generic impersonal meaning. This de-
scription fits into what some linguists refer to as
the fourth person?a non-referential impersonal
syntactic/semantic distinction that is often real-
ized in English as the impersonal pronoun one.
Since Finnish shows evidence of further syntac-
tic distinctions between generic and referential
use of the third person marker, we have analyzed
this marker as actually corresponding to two ho-
mophonous morphemes. One requires an overt
subject and the other requires a dropped subject.
There are no verbal affixes for PNG of the object.
Hausa [hau] (Chadic) Hausa generally re-
quires pronominal subjects to be dropped. Simple,
unmodified, uncoordinated independent pronouns
are ungrammatical in subject position. Subject
PNG is marked in a person aspect complex (PAC)
along with tense and aspect information. The
PAC precedes the lexical verb. When the PNG
marker is morphologically segmentable from the
tense/aspect, the PNG marker can be omitted if an
overt noun phrase is present and is required if the
noun phrase is not present. PNG is not marked for
objects; however the verb form changes depend-
ing on whether a full noun phrase, pronoun, or no
object immediately follows the verb.
Tamil [tam] (Dravidian) Subjects and objects
can be freely dropped aside from a special class
of weather verbs requiring overt subjects. Subject
1073
PNG markers are always required whether a sub-
ject is overt or not. PNG is not marked for objects.
Lg. Items Gram- Ungram- Coverage/
matical matical Over-
generation (%)
Arabic 13 10 3 90/0
Finnish 11 9 3 100/0
Hausa 20 8 12 100/0
Tamil 7 5 2 100/0
Table 3: Illustrative Languages Results
As shown in Table 3 we achieved 100% cov-
erage over every test suite in this set except for
Arabic. In addition, there was no overgeneration
or spurious ambiguity. One Arabic item did not
parse because the current implementation of our
analyses does not elegantly account for obligatory
object marking (with object drop) on some tran-
sitive verbs and optional object marking on oth-
ers. We could have customized a grammar that
included another, parallel set of lexical rules that
would account for this item. Improvements to this
aspect of the argument optionality library depend
on upgrades to the morphotactic system.
5.3 Set 3: Held-out Languages
Finally, we tested a set of ?held out? languages
not considered during development and chosen for
their geographic and genetic diversity without re-
gard for argument dropping patterns. We had pre-
viously created the non-argument optionality por-
tions of these test suites and choices files to test
the coverage of other libraries in the customiza-
tion system and thus they include a wider variety
of linguistic phenomena than Sets 1 or 2. As be-
fore, the construction and grammaticality judge-
ments of the strings were based on descriptive
grammars: Chirikba 2003 (Abkhaz), Press 1979
(Chemehuevi), Smirnova 1982 and Newman 2000
(Hausa), Pensalfini 2003 (Jingulu), Asher and
Kumari 1997 (Malayalam), Taylor 1985 (Nkore-
Kiga), and Fortescue 2003 (W. Greenlandic).
Due to space constraints, we provide only a
summary of the argument optionality patterns in
these languages (Table 4). All the languages li-
censed both subject and object dropping and in
two of the six, dropping pronominal arguments
was strongly preferred. Three languages have
word order constraints on how argument option-
ality is realized: Abkhaz restricts the appearance
of one of the third person affixes depending on
verb-object order. Nkore-Kiga requires and pro-
hibits the appearance of an object marker depend-
ing on where the overt object occurs. Chemehuevi
requires that the clitic which is used to mark the
subject appear in second position. It is also the
only language that has lexical constraints on ob-
ject dropping. Malayalam was the only language
which did not mark person, number, and gender
information for the subject.
The customized grammars were able to account
for the majority of the patterns demonstrated in
these languages (Table 5). We achieved 100%
coverage on four languages with zero (Jingulu,
Malayalam, West Greenlandic) or moderate (Abk-
haz) overgeneration. The main source of errors
found in the results is the handling of word or-
der constraints: The grammars were unable to li-
cense (Chemehuevi) or restrict (Nkore-Kiga and
Abkhaz) argument optionality based on the verb?s
and argument?s positions in the sentence. Once
the Grammar Matrix word order library has been
improved and is able to account for second po-
sition clitics and fine-grained head-complement
word order constraints, it will be a simple pro-
cess to add the new feature(s) to existing lexical
rules to account for these patterns. Incorporating
the new functionality will not require any major
changes to the argument optionality library aside
from modifying the questionnaire to elicit the new
information from the user.
Language Items Gram- Un- Coverage/
mat- gram- Overgen-
ical matical eration (%)
Abkhaz 10 6 4 100/10
Chemehuevi 8 6 2 83.3/0
Jingulu 9 6 3 100/0
Malayalam 4 4 0 100/0
Nkore-Kiga 10 4 6 100/83.3
W. Greenlandic 5 3 2 100/0
Table 5: Held-out Language Results
In addition, we verified that the addition of ar-
gument optionality didn?t reduce coverage on any
other portion of these testsuites. This indicates
that the new argument optionality library is inter-
acting properly with existing libraries. Additional
interactions will be tested as we add new libraries
to the customization system.
1074
Object Dropping Subject Dropping Word Order Constraints Lexical Constraints
Abkhaz opt opt yes none
Chemehuevi opt opt yes yes
Jingulu opt opt none none
Malayalam opt opt no none
Nkore-Kiga pref pref yes none
W. Greenlandic pref pref none none
Table 4: Existence of and constraints on argument optionality in six languages
6 Related Work
Subject dropping has been studied extensively
within theoretical linguistics under many differ-
ent frameworks (Rizzi, 1986; Bresnan, 2001;
Ackema et al, 2006; Ginzburg and Sag, 2000).
Within the context of HPSG, our analysis is simi-
lar to the one in the Grammar Matrix-derived Por-
tuguese grammar (Branco and Costa, 2008) and to
Mu?ller?s (2009) treatment of subject dropping in
Maltese. These analyses differ from Ginzburg and
Sag?s (2000) HPSG analysis which uses language
specific variations on the Argument Realization
Principle to control whether the subject/object is
placed onto the COMPS and/or SUBJ lists.
Language specific analyses have been imple-
mented in deep, broad-coverage grammars for
languages such as Japanese (Masuichi et al
(2003), Siegel and Bender (2002)) and Portuguese
(Branco and Costa (2008)). Within the ParGram
project (Butt et al, 2002), Kim et al (2003) were
able to directly port the argument optionality re-
lated rules from a Japanese grammar to Korean.
However, to our knowledge, no one has imple-
mented an analysis that has been applied to a large
number of typologically, geographically, and ge-
netically diverse languages.
7 Conclusion
Our current work has focused on modeling the
variation in syntactic constraints on the licens-
ing and restriction of argument dropping. To our
knowledge, this is the first analysis of argument
optionality that combines typological breadth
with precision analyses that have been imple-
mented and tested on a number of geographically
and genetically diverse languages. Although we
have tried to account for the patterns found in the
typological literature, there may be variants that
we are unaware of. We hope to learn of more pat-
terns as the Grammar Matrix customization sys-
tem is applied to an ever wider set languages.
While the current work focuses on syntactic
variation, we intend to expand the argument op-
tionality library to include semantic distinctions
as well. A likely starting point would be the pro-
posal given by Bender and Goss-Grubbs (2008)
who present a way to model the discourse status
(Prince, 1981) of an NP taking into account the
differences between definite and indefinite null in-
stantiation described by Fillmore (1986). In addi-
tion, ongoing work to improve the word order li-
brary may eventually allow us to more accurately
model word-order based constraints.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0644097. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Ackema, Peter, Patrick Brandt, Maaike Schoorlemmer,
and Fred Weerman, editors. 2006. Arguments and
Agreement. Oxford University Press, Oxford.
Ajdukiewicz, Kazimierz. 1935. Die syntaktische kon-
nexita?t. Studia Philosophica, 1:1?27.
Asher, R.E. and T.C. Kumari. 1997. Malayalam.
Routledge, NY.
Asher, R.E. 1985. Tamil. Croom Helm, London.
Bar-Hillel, Yehoshua. 1953. A quasi-arithmetical no-
tation for syntactic description. Language, 29:47?
58.
Bender, Emily M. and David Goss-Grubbs. 2008. Se-
mantic representations of syntactically marked dis-
course status in crosslinguistic perspective. In Proc.
2008 Conference on Semantics in Text Processing,
pages 17?29.
1075
Bender, Emily M., Dan Flickinger, and Stephan
Oepen. 2002. The grammar matrix: An open-
source starter-kit for the rapid development of cross-
linguistically consistent broad-coverage precision
grammars. In Proc. Workshop on Grammar Engi-
neering and Evaluation at COLING 2002, pages 8?
14.
Bender, Emily M., Scott Drellishak, Antske Fokkens,
Michael Wayne Goodman, Daniel P. Mills, Laurie
Poulson, and Safiyyah Saleem. 2010. Grammar
prototyping and testing with the LinGO Grammar
Matrix customization system. In Proc. ACL 2010
Software Demonstrations.
Branco, Anto?nio and Francisco Costa. 2008. A com-
putational grammar for deep linguistic processing of
Portuguese: LXGram, version a.4.1. Technical re-
port, University of Lisbon, Dept. of Informatics.
Bresnan, Joan. 2001. Lexical Functional Syntax.
Blackwell, Boston.
Butt, Miriam, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proc. Workshop on
Grammar Engineering and Evaluation at COLING
2002, pages 1?7.
Chirikba, Viachesiav. 2003. Abkhaz. LINCOM, Mu-
nich.
Copestake, Ann, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
Copestake, Ann. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford.
Dryer, Matthew. 2008. Expression of proniminal
subjects. In Haspelmath, Martin, Matthew Dryer,
David Gil, and Bernard Comrie, editors, The World
Atlas of Language Structures Online, chapter 101.
Max Planck Digital Library.
Fillmore, Charles. 1986. Pragmatically controlled
zero anaphora. In Proc. 12th annual meeting of the
Berkeley Linguistics Society, pages 95?107.
Fortescue, Michael. 2003. West Greenlandic. Croom
Helm, London.
Ginzburg, Johnathan and Ivan Sag. 2000. Interroga-
tive Investigations. CSLI, Stanford.
Kim, Roger, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, Hiroshi Masuichi, and
Tomoko Ohkuma. 2003. Multilingual grammar de-
velopment via grammar porting. In ESSLLI 2003
Workshop on Ideas and Strategies for Multilingual
Grammar Development, pages 49?56.
Masuichi, Hiroshi, Tomoko Ohkuma, Hiroki
Yoshimura, and Yasunari Harada. 2003. Japanese
parser on the basis of the lexical-functional gram-
mar formalism and its evaluation. In Dong Hong Ji,
Kim Teng Lua, editor, Proc. PACLIC17, pages
298?309.
Miller, Philip H. and Ivan A. Sag. 1997. French clitic
movement without clitics or movement. Natural
Language & Linguistic Theory, 15(3):573?639.
Mu?ller, Stefan. 2009. Towards an HPSG analysis of
Maltese. In et al Bernard Comrie, editor, Introduc-
ing Maltese linguistics. Papers from the 1st Inter-
national Conference on Maltese Linguistics, pages
83?112. Benjamins, Amsterdam.
Newman, Paul. 2000. The Hausa Language: An ency-
clopedic reference grammar. Yale University Press,
New Haven.
Pensalfini, Rob. 2003. A Grammar of Jingulu: An
Aboriginal language of the Northern Territory. Pa-
cific Linguistics, Canberra.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
Press, Margaret. 1979. Chemehuevi: A grammar and
lexicon. University of California Press, Berkeley.
Prince, Ellen. 1981. Toward a taxonomy of given-new
information. In Cole, P., editor, Radical Pragmat-
ics, pages 223?255. Academic Press, NY.
Rizzi, Luigi. 1986. Null objects in Italian and the the-
ory of pro. Linguistic Inquiry, 17(3):501?557.
Siegel, Melanie and Emily M. Bender. 2002. Ef-
ficient deep processing of Japanese. In Proc. 3rd
Workshop on Asian Language Resources and Inter-
national Standardization at COLING 2002.
Smirnova, Mirra A. 1982. The Hausa Language: A
Descriptive Grammar. Routledge, Boston.
Suleiman, Saleh M. 1990. The semantic functions of
object deletion in classical arabic. Language Sci-
ences, 12(2-3):255 ? 266.
Sulkala, Helena and Karjalaninen Merja. 1992.
Finnish. Routledge, NY.
Taylor, Charles. 1985. Nkore-Kiga. Croom Helm,
London.
Vainikka, Anne and Yonata Levy. 1999. Empty sub-
jects in Finnish and Hebrew. Natural Language and
Linguistic Theory, 17:613?671.
1076
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397?408,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parser Evaluation over
Local and Non-Local Deep Dependencies in a Large Corpus
Emily M. Bender?, Dan Flickinger?, Stephan Oepen?, Yi Zhang?
?Dept of Linguistics, University of Washington, ?CSLI, Stanford University
?Dept of Informatics, Universitetet i Oslo, ?Dept of Computational Linguistics, Saarland University
ebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.de
Abstract
In order to obtain a fine-grained evaluation of
parser accuracy over naturally occurring text,
we study 100 examples each of ten reason-
ably frequent linguistic phenomena, randomly
selected from a parsed version of the En-
glish Wikipedia. We construct a correspond-
ing set of gold-standard target dependencies
for these 1000 sentences, operationalize map-
pings to these targets from seven state-of-the-
art parsers, and evaluate the parsers against
this data to measure their level of success in
identifying these dependencies.
1 Introduction
The terms ?deep? and ?shallow? are frequently used
to characterize or contrast different approaches to
parsing. Inevitably, such informal notions lack a
clear definition, and there is little evidence of com-
munity consensus on the relevant dimension(s) of
depth, let alne agreement on applicable metrics. At
its core, the implied dichotomy of approaches al-
ludes to differences in the interpretation of the pars-
ing task. Its abstract goal, on the one hand, could
be pre-processing of the linguistic signal, to enable
subsequent stages of analysis. On the other hand, it
could be making explicit the (complete) contribution
that the grammatical form of the linguistic signal
makes to interpretation, working out who did what
to whom. Stereotypically, one expects correspond-
ing differences in the choice of interface representa-
tions, ranging from various levels of syntactic anal-
ysis to logical-form representations of semantics.
In this paper, we seek to probe aspects of variation
in automated linguistic analysis. We make the as-
sumption that an integral part of many (albeit not all)
applications of parsing technology is the recovery of
structural relations, i.e. dependencies at the level of
interpretation. We suggest a selection of ten linguis-
tic phenomena that we believe (a) occur with reason-
ably high frequency in running text and (b) have the
potential to shed some light on the depths of linguis-
tic analysis. We quantify the frequency of these con-
structions in the English Wikipedia, then annotate
100 example sentences for each phenomenon with
gold-standard dependencies reflecting core proper-
ties of the phenomena of interest. This gold standard
is then used to estimate the recall of these dependen-
cies by seven commonly used parsers, providing the
basis for a qualitative discussion of the state of the
art in parsing for English.
In this work, we answer the call by Rimell et
al. (2009) for ?construction-focused parser evalua-
tion?, extending and complementing their work in
several respects: (i) we investigate both local and
non-local dependencies which prove to be challeng-
ing for many existing state-of-the-art parsers; (ii) we
investigate a wider range of linguistic phenomena,
each accompanied with an in-depth discussion of
relevant properties; and (iii) we draw our data from
the 50-million sentence English Wikipedia, which
is more varied and a thousand times larger than the
venerable WSJ corpus, to explore a more level and
ambitious playing field for parser comparison.
2 Background
All parsing systems embody knowledge about possi-
ble and probable pairings of strings and correspond-
ing linguistic structure. Such linguistic and proba-
bilistic knowledge can be hand-coded (e.g., as gram-
mar rules) or automatically acquired from labeled or
397
unlabeled training data. A related dimension of vari-
ation is the type of representations manipulated by
the parser. We briefly review some representative
examples along these dimensions, as these help to
position the parsers we subsequently evaluate.1
2.1 Approaches to parsing
Source of linguistic knowledge At one end of this
dimension, we find systems whose linguistic knowl-
edge is encoded in hand-crafted rules and lexical en-
tries; for English, the ParGram XLE system (Rie-
zler et al, 2002) and DELPH-IN English Resource
Grammar (ERG; Flickinger (2000))?each reflect-
ing decades of continuous development?achieve
broad coverage of open-domain running text, for ex-
ample. At the other end of this dimension, we find
fully unsupervised approaches (Clark, 2001; Klein
and Manning, 2004), where the primary source of
linguistic knowledge is co-occurrence patterns of
words in unannotated text. As Haghighi and Klein
(2006) show, augmenting this knowledge with hand-
crafted prototype ?seeds? can bring strong improve-
ments. Somewhere between these poles, a broad
class of parsers take some or all of their linguistic
knowledge from annotated treebanks, e.g. the Penn
Treebank (PTB), which encodes ?surface grammati-
cal analysis? (Marcus et al, 1993). Such approaches
include those that directly (and exclusively) use the
information in the treebank (e.g. Charniak (1997),
Collins (1999), Petrov et al (2006), inter alios) as
well as those that complement treebank structures
with some amount of hand-coded linguistic knowl-
edge (e.g. O?Donovan et al (2004), Miyao et al
(2004), Hockenmaier and Steedman (2007), inter
alios). Another hybrid in terms of its acquisition of
linguistic knowledge is the RASP system of Briscoe
et al (2006), combining a hand-coded grammar over
PoS tag sequences with a probabilistic tagger and
statistical syntactic disambiguation.
Design of representations Approaches to parsing
also differ fundamentally in the style of represen-
tation assigned to strings. These vary both in their
1Additional sources of variation among extant parsing tech-
nologies include (a) the behavior with respect to ungrammatical
inputs and (b) the relationship between probabilistic and sym-
bolic knowledge in the parser, where parsers with a hand-coded
grammar at their core typically also incorporate an automati-
cally trained probabilistic disambiguation component.
formal nature and the ?granularity? of linguistic in-
formation (i.e. the number of distinctions assumed),
encompassing variants of constituent structure, syn-
tactic dependencies, or logical-form representations
of semantics. Parser interface representations range
between the relatively simple (e.g. phrase structure
trees with a limited vocabulary of node labels as in
the PTB, or syntactic dependency structures with a
limited vocabulary of relation labels as in Johansson
and Nugues (2007)) and the relatively complex, as
for example elaborate syntactico-semantic analyses
produced by the ParGram or DELPH-IN grammars.
There tends to be a correlation between the
methodology used in the acquisition of linguistic
knowledge and the complexity of representations: in
the creation of a mostly hand-crafted treebank like
the PTB, representations have to be simple enough
for human annotators to reliably manipulate. Deriv-
ing more complex representations typically presup-
poses further computational support, often involv-
ing some hand-crafted linguistic knowledge?which
can take the form of mappings from PTB-like repre-
sentations to ?richer? grammatical frameworks (as
in the line of work by O?Donovan et al (2004), and
others; see above), or can be rules for creating the
parse structures in the first place (i.e. a computa-
tional grammar), as for example in the treebanks of
van der Beek et al (2002) or Oepen et al (2004).2
In principle, one might expect that richer repre-
sentations allow parsers to capture complex syntac-
tic or semantic dependencies more explicitly. At the
same time, such ?deeper? relations may still be re-
coverable (to some degree) from comparatively sim-
ple parser outputs, as demonstrated for unbounded
dependency extraction from strictly local syntactic
dependency trees by Nivre et al (2010).
2.2 An armada of parsers
Stanford Parser (Klein and Manning, 2003) is a
probabilistic parser which can produce both phrase
structure trees and grammatical relations (syntactic
dependencies). The parsing model we evaluate is the
2A noteworthy exception to this correlation is the annotated
corpus of Zettlemoyer and Collins (2005), which pairs sur-
face strings from the realm of natural language database inter-
faces directly with semantic representations in lambda calculus.
These were hand-written on the basis of database query state-
ments distributed with the original datasets.
398
English factored model which combines the prefer-
ences of unlexicalized PCFG phrase structures and
of lexical dependencies, trained on sections 02?21
of the WSJ portion of the PTB. We chose Stanford
Parser from among the state-of-the-art PTB-derived
parsers for its support for grammatical relations as
an alternate interface representation.
Charniak&Johnson Reranking Parser (Char-
niak and Johnson, 2005) is a two-stage PCFG parser
with a lexicalized generative model for the first-
stage, and a discriminative MaxEnt reranker for the
second-stage. The models we evaluate are also
trained on sections 02?21 of the WSJ. Top-50 read-
ings were used for the reranking stage. The output
constituent trees were then converted into Stanford
Dependencies. According to Cer et al (2010), this
combination gives the best parsing accuracy in terms
of Stanford dependencies on the PTB.
Enju (Miyao et al, 2004) is a probabilistic HPSG
parser, combining a hand-crafted core grammar with
automatically acquired lexical types from the PTB.3
The model we evaluate is trained on the same ma-
terial from the WSJ sections of the PTB, but the
treebank is first semi-automatically converted into
HPSG derivations, and the annotation is enriched
with typed feature structures for each constituent.
In addition to HPSG derivation trees, Enju also pro-
duces predicate argument structures.
C&C (Clark and Curran, 2007) is a statistical
CCG parser. Abstractly similar to the approach of
Enju, the grammar and lexicon are automatically
induced from CCGBank (Hockenmaier and Steed-
man, 2007), a largely automatic projection of (the
WSJ portion of) PTB trees into the CCG framework.
In addition to CCG derivations, the C&C parser can
directly output a variant of grammatical relations.
RASP (Briscoe et al, 2006) is an unlexicalized
robust parsing system, with a hand-crafted ?tag se-
quence? grammar at its core. The parser thus anal-
yses a lattice of PoS tags, building a parse forest
from which the most probable syntactic trees and
sets of corresponding grammatical relations can be
extracted. Unlike other parsers in our mix, RASP
did not build on PTB data in either its PoS tagging
3This hand-crafted grammar is distinct from the ERG, de-
spite sharing the general framework of HPSG. The ERG is not
included in our evaluation, since it was used in the extraction of
the original examples and thus cannot be fairly evaluated.
or syntactic disambiguation components.
MSTParser (McDonald et al, 2005) is a data-
driven dependency parser. The parser uses an edge-
factored model and searches for a maximal span-
ning tree that connects all the words in a sentence
into a dependency tree. The model we evaluate
is the second-order projective model trained on the
same WSJ corpus, where the original PTB phrase
structure annotations were first converted into de-
pendencies, as established in the CoNLL shared task
2009 (Johansson and Nugues, 2007).
XLE/ParGram (Riezler et al, 2002, see also
Cahill et al, 2008) applies a hand-built Lexical
Functional Grammar for English and a stochastic
parse selection model. For our evaluation, we used
the Nov 4, 2010 release of XLE and the Nov 25,
2009 release of the ParGram English grammar, with
c-structure pruning turned off and resource limita-
tions set to the maximum possible to allow for ex-
haustive search. In particular, we are evaluating the
f-structures output by the system.
Each parser, of course, has its own requirements
regarding preprocessing of text, especially tokeniza-
tion. We customized the tokenization to each parser,
by using the parser?s own internal tokenization or
pre-tokenizing to match the parser?s desired input.
The evaluation script is robust to variations in tok-
enization across parsers.
3 Phenomena
In this section we summarize the ten phenomena we
explore and our motivations for choosing them. Our
goal was to find phenomena where the relevant de-
pendencies are relatively subtle, such that more lin-
guistic knowledge is beneficial in order to retrieve
them. Though this set is of course only a sampling,
these phenomena illustrate the richness of structure,
both local and non-local, involved in the mapping
from English strings to their meanings. We discuss
the phenomena in four sets and then briefly review
their representation in the Penn Treebank.
3.1 Long distance dependencies
Three of our phenomena can be classified as involv-
ing long-distance dependencies: finite that-less rel-
atives clauses (?barerel?), tough adjectives (?tough?)
and right node raising (?rnr?). These are illustrated
399
in the following examples:4
(1) barerel: This is the second time in a row Aus-
tralia has lost their home tri-nations? series.
(2) tough: Original copies are very hard to find.
(3) rnr: Ilu?vatar, as his names imply, exists before
and independently of all else.
While the majority of our phenomena involve lo-
cal dependencies, we include these long-distance
dependency types because they are challenging for
parsers and enable more direct comparison with the
work of Rimell et al (2009), who also address right
node raising and bare relatives. Our barerel category
corresponds to their ?object reduced relative? cate-
gory with the difference that we also include adverb
relatives, where the head noun functions as a modi-
fier within the relative clause, as does time in (1). In
contrast, our rnr category is somewhat narrower than
Rimell et al (2009)?s ?right node raising? category:
where they include raised modifiers, we restrict our
category to raised complements.
Part of the difficulty in retrieving long-distance
dependencies is that the so-called extraction site is
not overtly marked in the string. In addition to this
baseline level of complication, these three construc-
tion types present further difficulties: Bare relatives,
unlike other relative clauses, do not carry any lexi-
cal cues to their presence (i.e., no relative pronouns).
Tough adjective constructions require the presence
of specific lexical items which form a subset of a
larger open class. They are rendered more difficult
by two sources of ambiguity: alternative subcatego-
rization frames for the adjectives and the purposive
adjunct analysis (akin to in order to) for the infiniti-
val VP. Finally, right node raising often involves co-
ordination where one of the conjuncts is in fact not
a well-formed phrase (e.g., independently of in (3)),
making it potentially difficult to construct the correct
coordination structure, let alne associate the raised
element with the correct position in each conjunct.
3.2 Non-dependencies
Two of our phenomena crucially look for the lack of
dependencies. These are it expletives (?itexpl?) and
verb-particle constructions (?vpart?):
4All examples are from our data. Words involved in the rel-
evant dependencies are highlighted in italics (dependents) and
boldface (heads).
(4) itexpl: Crew negligence is blamed, and it is sug-
gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners at
home in the same inning.
The English pronoun it can be used as an ordi-
nary personal pronoun or as an expletive: a place-
holder for when the language demands a subject (or
occasionally object) NP but there is no semantic role
for that NP. The expletive it only appears when it
is licensed by a specific construction (such as ex-
traposition, (4)) or selecting head. If the goal of
parsing is to recover from the surface string the de-
pendencies capturing who did what to whom, exple-
tive it should not feature in any of those dependen-
cies. Likewise, instances of expletive it should be
detected and discarded in reference resolution. We
hypothesize that detecting expletive it requires en-
coding linguistic knowledge about its licensers.
The other non-dependency we explore is between
the particle in verb-particle constructions and the
direct object. Since English particles are almost
always homophonous with prepositions, when the
object of the verb-particle pair follows the par-
ticle, there will always be a competing analysis
which analyses the sequence as V+PP rather than
V+particle+NP. Furthermore, since verb-particle
pairs often have non-compositional semantics (Sag
et al, 2002), misanalyzing these constructions could
be costly to downstream components.
3.3 Phrasal modifiers
Our next category concerns modifier phrases:
(6) ned: Light colored glazes also have softening
effects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, each
team facing the other teams twice.
The first, (?ned?), is a pattern which to our knowl-
edge has not been named in the literature, where a
noun takes the typically verbal -ed ending, is modi-
fied by another noun or adjective, and functions as a
modifier or a predicate. We believe this phenomenon
to be interesting because its unusual morphology is
likely to lead PoS-taggers astray, and because the
often-hyphenated Adj+N-ed constituent has produc-
tive internal structure constraining its interpretation.
The second phrasal modifier we investigate is the
absolutive construction. An absolutive consists of an
400
NP followed by a non-finite predicate (such as could
appear after the copula be). The whole phrase mod-
ifies a verbal projection that it attaches to. Absolu-
tives may be marked with with or unmarked. Here,
we focus on the unmarked type as this lack of lexical
cue can make the construction harder to detect.
3.4 Subtle arguments
Our final three phenomena involve ways in which
verbal arguments can be more difficult to identify
than in ordinary finite clauses. These include de-
tecting the arguments of verbal gerunds (?vger?), the
interleaving of arguments and adjuncts (?argadj?) and
raising/control (?control?) constructions.
(8) vger: Accessing the website without the ?www?
subdomain returned a copy of the main site for
?EP.net?.
(9) argadj: The story shows, through flashbacks, the
different histories of the characters.
(10) control: Alfred ?retired? in 1957 at age 60 but
continued to paint full time.
In a verbal gerund, the -ing form a verb retains
verbal properties (e.g., being able to take NP com-
plements, rather than only PP complements) but
heads a phrase that fills an NP position in the syn-
tax (Malouf, 2000). Since gerunds have the same
morphology as present participle VPs, their role in
the larger clause is susceptible to misanalysis.
The argadj examples are of interest because En-
glish typically prefers to have direct objects directly
adjacent to the selecting verb. Nonetheless, phe-
nomena such as parentheticals and heavy-NP shift
(Arnold et al, 2000), in which ?heavy? constituents
appear further to the right in the string, allow for
adjunct-argument order in a minority of cases. We
hypothesize that the relative infrequency of this con-
struction will lead parsers to prefer incorrect analy-
ses (wherein the adjunct is picked up as a comple-
ment, the complement as an adjunct or the structure
differs entirely) unless they have access to linguis-
tic knowledge providing constraints on possible and
probable complementation patterns for the head.
Finally, we turn to raising and control verbs (?con-
trol?) (e.g., Huddleston and Pullum (2002, ch. 14)).
These verbs select for an infinitival VP complement
and stipulate that another of their arguments (sub-
ject or direct object in the examples we explore) is
identified with the unrealized subject position of the
infinitival VP. Here it is the dependency between
the infinitval VP and the NP argument of the ?up-
stairs? verb which we expect to be particularly sub-
tle. Getting this right requires specific lexical knowl-
edge about which verbs take these complementation
patterns. This lexical knowledge needs to be repre-
sented in such a way that it can be used robustly even
in the case of passives, relative clauses, etc.5
3.5 Penn Treebank representations
We investigated the representation of these 10 phe-
nomena in the PTB (Marcus et al, 1993) in two
steps: First we explored the PTB?s annotation guide-
lines (Bies et al, 1995) to determine how the rele-
vant dependencies were intended to be represented.
We then used Ghodke and Bird?s (2010) Treebank
Search to find examples of the intended annotations
as well as potential examples of the phenomena an-
notated differently, to get a sense of the consistency
of the annotation from both precision and recall per-
spectives. In this study, we take the phrase structure
trees of the PTB to represent dependencies based on
reasonable identification of heads.
The barerel, vpart, and absol phenomena are com-
pletely unproblematic, with their relevant dependen-
cies explicitly and reliably represented. In addition,
the tough construction is reliably annotated, though
one of the dependencies we take to be central is not
directly represented: The missing argument is linked
to a null wh head at the left edge of the comple-
ment of the tough predicate, rather than to its sub-
ject. Two further phenomena (rnr and vger) are es-
sentially correctly represented: the representations
of the dependencies are explicit and mostly but not
entirely consistently applied. Two out of a sample of
20 examples annotated as containing rnr did not, and
two out of a sample of 35 non-rnr-annotated coordi-
nations actually contained rnr. For vger the primary
problem is with the PoS tagging, where the gerund
is sometimes given a nominal tag, contrary to PTB
guidelines, though the structure above it conforms.
The remaining four constructions are more prob-
lematic. In the case of object control, while the guide-
5Distinguishing between raising and control requires fur-
ther lexical knowledge and is another example of a ?non-
dependency? (in the raising examples). We do not draw that
distinction in our annotations.
401
lines specify an analysis in which the shared NP is
attached as the object of the higher verb, the PTB
includes not only structures conforming to that anal-
ysis but also ?small clause? structures, with the latter
obscuring the relationship of the shared argument to
the higher verb. In the case of itexpl, the adjoined
(S(-NONE- *EXP*)) indicating an expletive use of
it is applied consistently for extraposition (as pre-
scribed in the guidelines). However, the set of lex-
ical licensers of the expletive is incomplete. For ar-
gadj we run into the problem that the PTB does not
explicitly distinguish between post-verbal modifiers
and verbal complements in the way that they are at-
tached. The guidelines suggest that the function tags
(e.g., PP-LOC, etc.) should allow one to distinguish
these, but examination of the PTB itself suggests
that they are not consistently applied. Finally, the
ned construction is not mentioned in the PTB guide-
lines nor is its internal structure represented in the
treebank. Rather, strings like gritty-eyed are left un-
segmented and tagged as JJ.
We note that the PTB representations of many of
these phenomena (barerel, tough, rnr, argadj, control,
itexpl) involve empty elements and/or function tags.
Systems that strip these out before training, as is
common practice, will not benefit from the informa-
tion that is in the PTB.
Our purpose here is not to criticize the PTB,
which has been a tremendously important resource
to the field. Rather, we have two aims: The first is
to provide context for the evaluation of PTB-derived
parsers on these phenomena. The second is to high-
light the difficulty of producing consistent annota-
tions of any complexity as well as the hurdles faced
by a hand-annotation approach when attempting to
scale a resource to more complex representations
and/or additional phenomena (though cf. Vadas and
Curran (2008) on improving PTB representations).
4 Methodology
4.1 Data extraction
We processed 900 million tokens of Wikipedia text
using the October 2010 release of the ERG, follow-
ing the work of the WikiWoods project (Flickinger
et al, 2010). Using the top-ranked ERG deriva-
tion trees as annotations over this corpus and sim-
ple patterns using names of ERG-specific construc-
Phenomenon Frequency Candidates
barerel 2.12% 546
tough 0.07% 175
rnr 0.69% 1263
itexpl 0.13% 402
vpart 4.07% 765
ned 1.18% 349
absol 0.51% 963
vger 5.16% 679
argadj 3.60% 1346
control 3.78% 124
Table 1: Relative frequencies of phenomena matches in
Wikipedia, and number of candidate strings vetted.
tions or lexical types, we randomly selected a set
of candidate sentences for each of our ten phenom-
ena. These candidates were then hand-vetted in se-
quence by two annotators to identify, for each phe-
nomenon, 100 examples that do in fact involve the
phenomenon in question and which are both gram-
matical and free of typos. Examples that were ei-
ther deemed overly basic (e.g. plain V+V coordi-
nation, which the ERG treats as rnr) or inappropri-
ately complex (e.g. non-constituent coordination ob-
scuring the interleaving of arguments and adjuncts)
were also discarded at this step. Table 1 summarizes
relative frequencies of each phenomenon in about
47 million parsed Wikipedia sentences, as well as
the total size of the candidate sets inspected. For
the control and tough phenomena hardly any filtering
for complexity was applied, hence these can serve
as indicators of the rate of genuine false positives.
For phenomena that partially overlap with those of
Rimell et al (2009), it appears our frequency es-
timates are comparable to what they report for the
Brown Corpus (but not the WSJ portion of the PTB).
4.2 Annotation format
We annotated up to two dependency triples per phe-
nomenon instance, identifying the heads and depen-
dents by the surface form of the head words in the
sentence suffixed with a number indicating word po-
sition (see Table 2).6 Some strings contain more
than one instance of the phenomenon they illustrate;
in these cases, multiple sets of dependencies are
6As the parsers differ in tokenization strategies, our evalua-
tion script treats these position IDs as approximate indicators.
402
Item ID Phenomenon Polarity Dependency
1011079100200 absol 1 having-2|been-3|passed-4 ARG act-1
1011079100200 absol 1 withdrew-9 MOD having-2|been-3|passed-4
1011079100200 absol 1 carried+on-12 MOD having-2|been-3|passed-4
Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7
Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.
Phenomenon Head Type Dependent Distance
Bare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)
(barerel) modified noun MOD top predicate of relative 3.3 (8)
Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)
(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)
Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)
(rnr) verb/prep1 ARG2 shared noun 6.1 (12)
Expletive It it-subject taking verb !ARG1 it 1.2 (3)
(itexpl) raising-to-object verb !ARG2 it ?
Verb+particle constructions particle !ARG2 complement 2.7 (9)
(vpart) verb+particle ARG2 complement 3.7 (10)
Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)
(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)
Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)
(absol) main clause predicate MOD absolutive predicate 9.8 (26)
Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)
(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)
Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)
(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)
Control ?upstairs? verb ARG[2,3] ?downstairs? verb 2.4 (23)
(control) ?downstairs? verb ARG1 shared argument 4.8 (17)
Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.
recorded. In addition, some strings evince more than
one of the phenomena we are studying. However,
we only annotate the dependencies associated with
the phenomenon the string was selected to repre-
sent. Finally, in examples with coordinated heads or
dependents, we recorded separate dependencies for
each conjunct. In total, we annotated 2127 depen-
dency triples for the 1000 sentences, including 253
negative dependencies (see below). Table 3 outlines
the dependencies annotated for each phenomenon.
To allow for multiple plausible attachment sites,
we give disjunctive values for heads or dependents
in several cases: (i) with auxiliaries, (ii) with com-
plementizers (that or to, as in Table 2), (iii) in cases
of measure or classifier nouns or partitives, (iv) with
multi-word proper names and (v) where there is
genuine attachment ambiguity for modifiers. As
these sets of targets are disjunctive, these conven-
tions should have the effect of increasing measured
parser performance. 580 (27%) of the annotated de-
pendencies had at least one disjunction.
4.3 Annotation and reconciliation process
The entire data set was annotated independently by
two annotators. Both annotators were familiar with
the ERG, used to identify these sentences in the
WikiWoods corpus, but the annotation was done
without reference to the ERG parses. Before begin-
ning annotation on each phenomenon, we agreed on
which dependencies to annotate. We also communi-
cated with each other about annotation conventions
as the need for each convention became clear. The
annotation conventions address how to handle co-
ordination, semantically empty auxiliaries, passives
and similar orthogonal phenomena.
Once the entire data set was dual-annotated, we
compared annotations, identifying the following
sources of mismatch: typographical errors, incom-
pletely specified annotation conventions, inconsis-
tent application of conventions (101 items, dropping
in frequency as the annotation proceeded), and gen-
uine disagreement about what to annotate, either dif-
ferent numbers of dependencies of interest identified
403
in an item (59 items) or conflicting elements in a de-
pendency (54 items).7 Overall, our initial annotation
pass led to agreement on 79% of the items, and a
higher per-dependency level of agreement. Agree-
ment could be expected to approach 90% with more
experience in applying annotation conventions.
We then reconciled the annotations, using the
comparison to address all sources of difference. In
most cases, we readily agreed which annotation was
correct and which was in error. In a few cases, we
decided that both annotations were plausible alter-
natives (e.g., in terms of alternative attachment sites
for modifiers) and so created a single merged anno-
tation expressing the disjunction of both (cf. ? 4.2).
5 Evaluation
With the test data consisting of 100 items for each of
our ten selected phenomena, we ran all seven pars-
ing systems and recorded their dependency-style
outputs for each sentence. While these outputs
are not directly comparable with each other, we
were able to associate our manually-annotated tar-
get dependencies with parser-specific dependencies,
by defining sets of phenomenon-specific regular ex-
pressions for each parser. In principle, we allow this
mapping to be somewhat complex (and forgiving to
non-contentful variation), though we require that it
work deterministically and not involve specific lexi-
cal information. An example set is given in Fig. 2.
"absol" =>
{?ARG1? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W2}\W*_(\d+) \W*{W1}\W*_(\d+)\)?],
?ARG? => [
?\(ncsubj \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+) _\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?],
?MOD? => [
?\(xmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(ncmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?,
?\(cmod _ \W*{W1}\W*_(\d+) \W*{W2}\W*_(\d+)\)?]}
Figure 2: Regexp set to evaluate C&C for absol.
These expressions fit the output that we got from the
C&C parser, illustrated in Fig. 3 with a relevant por-
tion of the dependencies produced for the example
in Table 2. Here the C&C dependency (ncsubj
passed 4 Act 1 ) matches the first target in the
7We do not count typographical errors or incompletely spec-
ified conventions as failures of inter-annotator agreement.
gold-standard (Table 2), but no matching C&C de-
pendency is found for the other two targets.
(xmod _ Act_1 passed_4)
(ncsubj passed_4 Act_1 _)
(ncmod _ withdrew,_9 Jessop_8)
(dobj year,_7 withdrew,_9)
Figure 3: Excerpts of C&C output for item in Table 2.
The regular expressions operate solely on the de-
pendency labels and are not lexically-specific. They
are specific to each phenomenon, as we did not at-
tempt to write a general dependency converter, but
rather to discover what patterns of dependency rela-
tions describe the phenomenon when it is correctly
identified by each parser. Thus, though we did not
hold out a test set, we believe that they would gener-
alize to additional gold standard material annotated
in the same way for the same phenomena.8
In total, we wrote 364 regular expressions to han-
dle the output of the seven parsers, allowing some
leeway in the role labels used by a parser for any
given target dependency. The supplementary mate-
rials for this paper include the test data, parser out-
puts, target annotations, and evaluation script.
Fig. 1 provides a visualization of the results of our
evaluation. Each column of points represents one
dependency type. Dependency types for the same
phenomenon are represented by adjacent columns.
The order of the columns within a phenomenon fol-
lows the order of the dependency descriptions in
Table 3: For each pair, the dependency type with
the higher score for the majority of the parsers is
shown first (to the left). The phenomena them-
selves are also arranged according to increasing (av-
erage) difficulty. itexpl only has one column, as we
annotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,
mutually-incompatible instance types.) Since ex-
pletive it should not be the semantic dependent of
any head, the targets are generalized for this phe-
nomenon and the evaluation script counts as incor-
8In the case of the XLE, our simplistic regular-expression
approach to the interpretation of parser outputs calls for much
more complex patterns than for the other parsers. This is owed
to the rich internal structure of LFG f-structures and higher
granularity of linguistic analysis, where feature annotations on
nodes as well as reentrancies need to be taken into account.
Therefore, our current results for the XLE admit small amounts
of both over- and under-counting.
404
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
vger
vpart
control
argadj
barerel
rnr tough
ned itexpl
absol
enju
xle
c&j
c&c
stanford
mst
rasp
Figure 1: Individual dependency recall for seven parsers over ten phenomena.
rect any dependency involving referential it.
We observe fairly high recall of the dependencies
for vpart and vger (with the exception of RASP), and
high recall for both dependencies representing con-
trol for five systems. While Enju, Stanford, MST,
and RASP all found between 70 and 85% of the de-
pendency between the adjective and its complement
in the tough construction, only Enju and XLE rep-
resented the dependency between the subject of the
adjective and the gap inside the adjective?s comple-
ment. For the remaining phenomena, each parser
performed markedly worse on one dependency type,
compared to the other. The only exceptions here
are XLE and C&C?s (and to a lesser extent, C&J?s)
scores for barerel. No system scored higher than
33% on the harder of the two dependencies in rnror
absol, and Stanford, MST, and RASP all scored be-
low 25% on the harder dependency in barerel. Only
XLE scored higher than 10% on the second depen-
dency for ned and higher than 50% for itexpl.
6 Discussion
From the results in Fig. 1, it is clear that even the best
of these parsers fail to correctly identify a large num-
ber of relevant dependencies associated with linguis-
tic phenomena that occur with reasonable frequency
in the Wikipedia. Each of the parsers attempts
with some success to analyze each of these phe-
nomena, reinforcing the claim of relevance, but they
vary widely across phenomena. For the two long-
distance phenomena that overlap with those studied
in Rimell et al (2009), our results are comparable.9
Our evaluation over Wikipedia examples thus shows
the same relative lack of success in recovering long-
distance dependencies that they found for WSJ sen-
tences. The systems did better on relatively well-
studied phenomena including control, vger, and vpart,
but had less success with the rest, even though all but
two of those remaining phenomena involve syntac-
tically local dependencies (as indicated in Table 3).
Successful identification of the dependencies in
these phenomena would, we hypothesize, benefit
from richer (or deeper) linguistic information when
parsing, whether it is lexical (tough, control, itexpl,
and vpart), or structural (rnr, absol, vger, argadj, and
barerel), or somewhere in between, as with ned. In
the case of treebank-trained parsers, for the informa-
tion to be available, it must be consistently encoded
in the treebank and attended to during training. As
9Other than Enju, which scores 16 points higher in the eval-
uation of Rimell et al, our average scores for each parser across
the dependencies for these phenomena are within 12 points of
those reported by Rimell et al (2009) and Nivre et al (2010).
405
noted in Sections 2.1 and 3.5, there is tension be-
tween developing sufficiently complex representa-
tions to capture linguistic phenomena and keeping
an annotation scheme simple enough that it can be
reliably produced by humans, in the case of hand-
annotation.
7 Related Work
This paper builds on a growing body of work which
goes beyond (un)labeled bracketing in parser evalua-
tion, including Lin (1995), Carroll et al (1998), Ka-
plan et al (2004), Rimell et al (2009), and Nivre et
al. (2010). Most closely related are the latter two of
the above, as we adopt their ?construction-focused
parser evaluation methodology?.
There are several methodological differences be-
tween our work and that of Rimell et al First, we
draw our evaluation data from a much larger and
more varied corpus. Second, we automate the com-
parison of parser output to the gold standard, and we
distribute the evaluation scripts along with the anno-
tated corpus, enhancing replicability. Third, where
Rimell et al extract evaluation targets on the basis
of PTB annotations, we make use of a linguistically
precise broad-coverage grammar to identify candi-
date examples. This allows us to include both local
and non-local dependencies not represented or not
reliably encoded in the PTB, enabling us to evalu-
ate parser performance with more precision over a
wider range of linguistic phenomena.
These methodological innovations bring two em-
pirical results. The first is qualitative: Where previ-
ous work showed that overall Parseval numbers hide
difficulties with long-distance dependencies, our re-
sults show that there are multiple kinds of reason-
ably frequent local dependencies which are also dif-
ficult for the current standard approaches to pars-
ing. The second is quantitative: Where Rimell et
al. found two phenomena which were virtually un-
analyzed (recall below 10%) for one or two parsers
each, we found eight phenomena which were vir-
tually unanalyzed by at least one system, includ-
ing two unanalyzed by five and one by six. Every
system had at least one virtually unanalyzed phe-
nomenon. Thus we have shown that the dependen-
cies being missed by typical modern approaches to
parsing are more varied and more numerous than
previously thought.
8 Conclusion
We have presented a detailed construction-focused
evaluation of seven parsers over 10 phenomena,
with 1000 examples drawn from English Wikipedia.
Gauging recall of such ?deep? dependencies, in our
view, can serve as a proxy for downstream pro-
cessing involving semantic interpretation of parser
outputs. Our annotations and automated evaluation
script are provided in the supplementary materials,
for full replicability. Our results demonstrate that
significant opportunities remain for parser improve-
ment, and highlight specific challenges that remain
invisible in aggregate parser evaluation (e.g. Parse-
val or overall dependency accuracy). These results
suggest that further progress will depend on train-
ing data that is both more extensive and more richly
annotated than what is typically used today (seeing,
for example, that a large part of more detailed PTB
annotation remains ignored in much parsing work).
There are obvious reasons calling for diversity in
approaches to parsing and for different trade-offs
in, for example, the granularity of linguistic analy-
sis, average accuracy, cost of computation, or ease
of adaptation. Our proposal is not to substitute
construction-focused evaluation on Wikipedia data
for widely used aggregate metrics and reference cor-
pora, but rather to augment such best practices in
the spirit of Rimell et al (2009) and expand the
range of phenomena considered in such evaluations.
Across frameworks and traditions (and in principle
languages), it is of vital importance to be able to
evaluate the quality of parsing (and grammar induc-
tion) algorithms in a maximally informative manner.
Acknowledgments
We are grateful to Tracy King for her assistance in
setting up the XLE system and to three anonymous
reviewers for helpful comments. The fourth author
thanks DFKI and the DFG funded Excellence Clus-
ter on MMCI for their support of the work. Data
preparation on the scale of Wikipedia was made pos-
sible through access to large-scale HPC facilities,
and we are grateful to the Scientific Computing staff
at UiO and the Norwegian Metacenter for Computa-
tional Science.
406
References
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76(1):28?55.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style Penn treebank project. Technical report, Univer-
sity of Pennsylvania.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, pages 77?80, Sydney, Australia.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2008. Speeding
up LFG parsing using c-structure pruning. In Coling
2008: Proceedings of the workshop on Grammar En-
gineering Across Frameworks, pages 33?40, Manch-
ester, England, August. Coling 2008 Organizing Com-
mittee.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447?454,
Granada.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Malta.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 598 ? 603, Providence, RI.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In Proceedings of the 5th Confer-
ence on Natural Language Learning, pages 105?112.
Toulouse, France.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.
2010. WikiWoods. Syntacto-semantic annotation for
English Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Processing
with HPSG):15 ? 28.
Sumukh Ghodke and Steven Bird. 2010. Fast query for
large treebanks. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 267?275, Los Angeles, California, June.
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881?888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
In Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 97?104, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3?10, Cambridge, MA. MIT
Press.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 478?485, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, pages 1420?1425, Montreal, Canada.
Robert Malouf. 2000. Verbal gerunds as mixed cate-
gories in HPSG. In Robert Borsley, editor, The Nature
407
and Function of Syntactic Categories, pages 133?166.
Academic Press, New York.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancouver,
Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
the Penn Treebank. In Proceedings of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing, pages 684?693, Hainan Island, China.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 833?841, Beijing, China.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
Van Genabith, and Andy Way. 2004. Large-scale in-
duction and evaluation of lexical resources from the
penn-ii treebank. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 367?374, Barcelona, Spain.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575 ? 596.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 813?821, Singapore. Association for Computa-
tional Linguistics.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions. A pain in the neck for NLP. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 2276 of Lecture Notes in
Computer Science, pages 189?206. Springer, Berlin,
Germany.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335?343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
L. van der Beek, Gosse Bouma, Robert Malouf, and Gert-
jan van Noord. 2002. The Alpino Dependency Tree-
bank. In Mariet Theune, Anton Nijholt, and Hen-
dri Hondorp, editors, Computational Linguistics in the
Netherlands, Amsterdam, The Netherlands. Rodopi.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Twenty-First Annual Conference on
Uncertainty in Artificial Intelligence, pages 658?666,
Arlington, Virginia. AUAI Press.
408
Obituary
Ivan A. Sag
Emily M. Bender
University of Washington
Ivan Sag died on September 10, 2013, after a long illness. He is survived by his wife,
Stanford sociolinguistics professor Penny Eckert. In a career spanning four decades, he
published over 100 articles and 10 books, centered on the theme of developing precise,
implementable, testable, psychologically plausible, and scalable models of natural lan-
guage, especially syntax and semantics.
Sag earned a B.A. from the University of Rochester (1971), an M.A. from the
University of Pennsylvania (1973) and a Ph.D. from MIT (1976), all in Linguistics. He
held teaching positions at the University of Pennsylvania (1976?1979) and Stanford
University (1979?2013) where he was the Sadie Dunham Patek Professor in Humanities
since 2008. In addition, he taught at ten LSA Linguistic Institutes (most recently as the
Edward Sapir Professor at the Linguistic Institute at the University of Colorado, Boul-
der, in 2011), at five ESSLLIs, as well as in summer schools or other visiting positions
at NTNU (Trondheim), Universite? de Paris 7, Rijksuniversiteit Utrecht, the University
of Rochester, the University of Chicago, and Harvard University. At Stanford he was
a founding member of and active participant in the Center for the Study of Language
and Information (CSLI), which housed the HPSG and LinGO projects. He was also a
key member of the group of faculty that developed and launched the Symbolic Systems
program (in 1985) and was director of Symbolic Systems from 2000?2001 and 2005?2009.
Among other honors, he was elected to the American Academy of Arts and Sciences in
2007 and named a Fellow of the Linguistic Society of America in 2008.
Sag?s advisor was Noam Chomsky; throughout his career, he saw himself as fur-
thering what he understood to be the original Chomskyan enterprise. However, in the
late 1970s, he broke with the Chomskyan mainstream because he felt it had abandoned
central aspects of that original enterprise. Here?s how Sag told the story to Ta! magazine
in 1993:
Well, it has always been hard for me to reconcile the original Chomskyan research goals
and methods with most of the modern work that goes on. Though Chomsky has denied
this. Current work in so-called Government and Binding Theory is basically
formulating ideas in ways that are so loose that they do not add up to precisely
constructing hypotheses about the nature of language.
All too often, people throw around formalisms that have no precise interpretation
and the consequences of particular proposals are absolutely impossible to assess. In my
opinion that is just not the way to do science. I think that the original goals of
generative grammar do constitute a set of desiderata for the science of language that
one can try to execute with much greater success than current work in GB has achieved
or is likely to, given the directions it seems to be going in.
doi:10.1162/COLI a 00179
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
The result of Sag holding to the initial goals of generative grammar, even when
mainstream syntax did not, has been an enormous boon for the field of computational
linguistics. Whereas much mainstream work in theoretical syntax is neither explicitly
formalized nor concerned with broad coverage, the frameworks that Sag was
instrumental in helping to create (Generalized Phrase Structure Grammar, Head-
driven Phrase Structure Grammar, and Sign-Based Construction Grammar [Boas and
Sag 2012], but especially HPSG) are implementable and in fact implemented, and
demonstrably scalable.
Sag first encountered the community working on what would come to be called
Generalized Phrase Structure Grammar (GPSG, a term coined by Sag), and in particular
Gerald Gazdar and Geoff Pullum, at the 1978 LSA Linguistic Institute. Gazdar and
colleagues set out to show that English (and other natural languages) could in fact be
described with context-free models, as Pullum and Gazdar (1982) had debunked all
previous arguments against that claim. But more importantly Sag and his colleagues
developing GPSG strove to be formally precise, in order to support valid scientific
investigation.1 The GPSG book (Gazdar et al. 1985) begins by throwing down the
gauntlet:
This book contains a fairly complete exposition of a general theory of grammar that we
have worked out in detail over the past four years. Unlike much theoretical linguistics,
it lays considerable stress on detailed specifications both of the theory and of the
descriptions of parts of English grammar that we use to illustrate the theory. We do not
believe that the working out of such details can be dismissed as ?a matter of execution?,
to be left to lab assistants. In serious work, one cannot ?assume some version of the
X-bar theory? or conjecture that a ?suitable? set of interpretive rules will do something as
desired, any more than one can evade the entire enterprise of generative grammar by
announcing: ?We assume some recursive function that assigns to each grammatical and
meaningful sentence of English an appropriate structure and interpretation.? One must
set about constructing such a function, or one is not in the business of theoretical
linguistics. (p. ix)
The computational benefits of that precision were quickly apparent. In 1981, Sag
taught a course on GPSG at Stanford with Gazdar and Pullum. One of the students
attending that course, Anne Paulson, was working at Hewlett-Packard Labs and saw the
potential for using GPSG as the basis of a question answering system (with a database
back-end). Paulson arranged a meeting between her boss, Egon Loebner, and Sag,
Gazdar, Pullum, and Tom Wasow, which led to a nearly decade-long project implement-
ing a grammar for English and processing tools to work with it. The project included HP
staff as well as Sag, Pullum, and Wasow as consultants, and Stanford and UC Berkeley
students, including Mark Gawron, Carl Pollard, and Dan Flickinger.
The work initially set out to implement GPSG (Gawron et al. 1982), but in the
context of developing and implementing analyses, Sag and colleagues added inno-
vations to the underlying theory until its formal basis was so different it warranted
a new name. The new theory, laid out in Pollard and Sag 1987 and Pollard and Sag
1994, among others, came to be called Head-driven Phrase Structure Grammar (HPSG).
1 The formal precision had much more lasting impact than the attempt at a context-free model: Eventually
Bresnan et al. (1982), Culy (1985), and Shieber (1985) found more solid counterexamples to the claim that
natural languages can be modeled with CF-PSGs.
2
Bender Obituary
HPSG synthesizes ideas from GPSG, Dependency Grammar (Hudson 1984), Categorial
Grammar (Wood 1993), Lexical Functional Grammar (Bresnan and Kaplan 1982), and
even Government and Binding Theory (Chomsky 1981).
Importantly, rather than encoding theoretical results as constraints on the formal-
ism, HPSG defines a flexible formalism (typed feature structures) in which different
theories can be defined. This flexibility facilitates testing and synthesis of theoretical
ideas developed in other frameworks. The stability of the formalism has been critical to
the success of HPSG in computational linguistics, as it has allowed for the development
of a variety of processing engines that interpret the formalism and thus can apply
grammars to the tasks of parsing and generation (Uszkoreit et al. 1994; Carpenter and
Penn 1994; Makino et al. 1998; Copestake 2002; Callmeier 2002; Penn 2004; Crysmann
and Packard 2012; Slayden 2012, inter alios).
HPSG-based parsing was deployed in Verbmobil (Wahlster 2000), a large-scale,
multi-site machine translation project funded by the German government, for which
Sag headed up the English Grammar Project (1994?2000), later redubbed LinGO (Lin-
guistic Grammars Online), at CSLI. The English grammar developed in that project
(beginning actually in 1993) came to be known as the English Resource Grammar
(Flickinger 2000, 2011).2 The resource grammar idea builds on insights articulated in
Gawron et al. 1982, namely, that detailed syntactic and semantic analysis are a crucial
component of natural language understanding (in addition to discourse and world
knowledge processing) and that the grammar which does that analysis can and should
be portable across domains.
The English Resource Grammar has been refined and extended as it has been
developed in the context of applications ranging from machine translation of dialogues
regarding travel and appointment scheduling (Verbmobil; Wahlster 2000), automated
customer service response (YY Technologies), machine translation of Norwegian hiking
brochures (LOGON; Oepen et al. 2007), and grammar checking as part of a language
arts instructional application (EPGY/Redbird; Suppes et al. 2012). The work of building
a grammar such as this involves identifying phenomena in sentences from the domain
of interest that the grammar does not yet account for, delimiting the phenomena, and
developing and implementing analyses. Throughout the mid and late 1990s and into
the early 2000s, the LinGO project at CSLI featured weekly project meetings led by
Flickinger, who would bring phenomena in need of analysis for discussion by the group,
including Sag and Wasow as well as Ann Copestake, Rob Malouf, Stanford linguistics
graduate students, and visitors to CSLI.
In these always lively discussions, Sag could be counted on to share his encyclo-
pedic knowledge of theoretical literature pertaining to the phenomenon in question
and key examples that had been identified and analyzed in that literature, to suggest
analyses, as well as to invent on the spot further examples to illustrate differences in
predictions of competing candidate analyses. Supporting his ability to do this was an
unsurpassed command of the theory and the workings of the grammar written in it.3
These meetings not only fed the development of the English Resource Grammar, but
2 The initial grammar was developed by Rob Malouf, then a grad student at Stanford. Since 1994 Dan
Flickinger has been its lead developer.
3 GPSG and HPSG were developed on the premise that precise formalization is critical to the testing of
linguistic hypotheses: Without the formalization, it is not possible to say for certain what the predictions
of a theory are. In fact, formalization alone isn?t enough: Grammars of the scale supported by the HPSG
framework are too complex for humans to reliably do those calculations without the aid of a machine.
The one possible exception to this generalization was Ivan Sag.
3
Computational Linguistics Volume 40, Number 1
also formed an important point of contact between computational and theoretical work
in HPSG, such that the ?pen and paper? theory remained responsive to computational
concerns.
Another key result of the LinGO project during the Verbmobil days was the de-
velopment of Minimal Recursion Semantics (MRS) (Copestake et al. 1995, 2005). Sag
and colleagues designed MRS to meet the competing demands of expressive adequacy,
grammatical compatibility, computational tractability, and underspecifiability. In other
words, it is a computational semantic formalism that allows grammars like the En-
glish Resource Grammar to make explicit exactly as much information about semantic
predicate argument structure and quantifier and operator scope as is determined by
sentence structure, leaving further ambiguity represented via underspecification rather
than enumeration of, for example, the full set of possible quantifier scopings for each
item.
The experience of the Verbmobil project highlighted the value of collaborative
work on natural language processing between grammarians and those working on
the software required to develop and deploy grammars in practical applications. In
the late 1990s, Sag and others, notably Dan Flickinger, Stephan Oepen, Jun?ichi Tsu-
jii, and Hans Uszkoreit, began exploring ways to continue the collaboration past the
end of the Verbmobil project. The result was the DELPH-IN consortium,4 which has
continued to facilitate further exchange of ideas and joint development of software
and grammars. As of 2013, DELPH-IN spans four continents and has developed open-
source grammars (including some large scale ones) for many languages, all of which
are interoperable with the same set of open-source processing (analysis and generation)
tools.
A hallmark of Sag?s work has been his openness to collaboration across frame-
works, subfields, and disciplines. In addition to the interactions he facilitated
between linguistics and computational linguistics, Sag also built bridges between
formal linguistics and psycholinguistics. Towards the end of his career, together
with graduate students at Stanford, Sag turned his attention to developing a clearer
understanding of the roles of processing and grammar in determining (un)acceptability.
Specifically, he argued that the unacceptability associated with phenomena such as
?island constraints? derives not from ungrammaticality but in fact from more general
processing pressures (Staum and Sag 2008; Hofmeister and Sag 2010; Staum Casasanto,
Hofmeister, and Sag 2010; Hofmeister et al. 2013). More generally, Sag?s work
on grammatical theory has highlighted the importance of ?performance-plausible
competence grammar,? namely, models of linguistic knowledge that can in fact be
embedded in models of human language processing (Sag 1992; Sag and Fodor 1994;
Sag, Wasow, and Bender 2003; Sag and Wasow 2011). A key feature of HPSG that makes
it ?performance plausible? is representation of grammatical information in a form
that allows it to be combined flexibly with other grammatical information as well as
extragrammatical information relevant to processing (e.g., general world knowledge,
or information about the present situation perceived by the hearer) as that information
becomes available. This form of grammatical description has been very influential
in computational work on incremental processing models which explicitly account
for the generation of upcoming grammatical structure (Jurafsky 1996; Hale 2001;
Levy 2008).
4 Deep Linguistic Processing in HPSG Initiative; http://www.delph-in.net.
4
Bender Obituary
Where many saw opposition between stochastic and symbolic methods, Sag saw
opportunity for hybridization, as early as 1993. When asked by Ta! magazine (in the
same interview cited above) about connectionism, he replied:
If you think of grammar as a transformational grammar, and you look at the
connectionist models that have evolved, then it is like looking at apples and oranges.
However, if you look at a theory of typed feature structures and a connectionist model,
it is more like apples and applesauce....
I am not just interested in computer natural language processing. I am interested
in understanding how in the world communication is possible at all. To me, it is
astonishing that the huge space of ambiguity and uncertain information that language
presents somehow gives rise to accurate and efficient communication. What we do
cognitively, in language processing, is nothing short of miraculous. We bring together
knowledge of the language, knowledge of the world, knowledge of the subject matter,
knowledge of the situation, in such a way as to never even see the landscape of what
the linguistic possibilities are....
Now is not the time to stop exploring discrete methodologies, but rather to look
for hybrid methodologies that exploit the complementary strengths of discrete and
statistical methodologies. That is the only way that language technology will ever
develop to play the role it must in the technology of tomorrow.
Finally, no appreciation of Sag?s career and its impact on the fields of computational
linguistics and especially linguistics would be complete without remarking on the
energy and verve he brought to the social aspects of doing science: He had a keen
understanding of the importance of community and communication in the doing of
science, and worked tirelessly to promote both. He organized the first International
Conference on Head-Driven Phrase Structure Grammar in Columbus, Ohio, in 1993 in
order to provide a forum for HPSG researchers scattered across the globe to exchange
ideas; the 20th iteration of that conference was held in Berlin in 2013. But his notion
of community extended beyond staid academic discourse: He also started the band
Dead Tongues with Geoffrey Nunberg in the early 1980s, and played keyboards in
every iteration of that band (with ever-changing, but always linguistically motivated,
membership) through the roof-raising performance at Ivan Fest at CSLI in April 2013.
He also organized the rental of sorority houses and the hiring of gourmet chefs for
shared housing at every Linguistic Institute he attended. These opportunities for social
engagement helped build the community, not just of HPSG researchers, but of linguists
more generally, which in turn supports the lively exchange of ideas on which Sag
thrived and to which he contributed so much.
References
Boas, Hans Christian and Ivan A Sag. 2012.
Sign-Based Construction Grammar. CSLI
Publications, Stanford, CA.
Bresnan, Joan and Ronald M. Kaplan.
1982. Lexical-Functional Grammar:
A formal system for grammatical
representation. In Joan Bresnan,
editor, The Mental Representation of
Grammatical Relations, pp. 29?130,
MIT Press, Cambridge, MA.
Bresnan, Joan, Ronald M. Kaplan, Stanley
Peters, and Annie Zaenen. 1982.
Cross-serial dependencies in Dutch.
Linguistic Inquiry, 13(4):613?635.
Callmeier, Ulrich. 2002. Preprocessing and
encoding techniques in PET. In Stephan
Oepen, Daniel Flickinger, J. Tsujii, and Hans
Uszkoreit, editors, Collaborative Language
Engineering. A Case Study in Efficient
Grammar-based Processing, pp. 127?140,
CSLI Publications, Stanford, CA.
Carpenter, Bob and Gerald Penn. 1994.
ALE: The attribute logic engine user?s
guide, version 2.0.1. Carnegie Mellon
University, Department of Philosophy,
Paper 526.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Foris Publications,
Dordrecht, Holland.
5
Computational Linguistics Volume 40, Number 1
Copestake, Ann. 2002. Implementing Typed
Feature Structure Grammars. CSLI
Publications, Stanford, CA.
Copestake, Ann, Dan Flickinger, Rob Malouf,
Susanne Riehemann, and Ivan Sag. 1995.
Translation using minimal recursion
semantics. In Proceedings of the Sixth
International Conference on Theoretical and
Methodological Issues in Machine Translation,
pp. 15?32, Leuven.
Copestake, Ann, Dan Flickinger, Carl
Pollard, and Ivan A. Sag. 2005. Minimal
recursion semantics: An introduction.
Research on Language & Computation,
3(4):281?332.
Crysmann, Berthold and Woodley Packard.
2012. Towards efficient HPSG generation
for German, a non-configurational
language. In Proceedings of COLING 2012,
pages 695?710, Mumbai.
Culy, Christopher. 1985. The complexity of
the vocabulary of bambara. Linguistics and
Philosophy, 8(3):345?351.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6 (1)
(Special Issue on Efficient Processing with
HPSG):15 ? 28.
Flickinger, Dan. 2011. Accuracy v. robustness
in grammar engineering. In Emily M.
Bender and Jennifer E. Arnold, editors,
Language from a Cognitive Perspective:
Grammar, Usage and Processing. CSLI
Publications, Stanford, CA, pages 31?50.
Gawron, Jean Mark, Jonathan King, John
Lamping, Egon Loebner, E. Anne Paulson,
Geoffrey K. Pullum, Ivan A. Sag, and
Thomas Wasow. 1982. Processing English
with a Generalized Phrase Structure
Grammar. In Proceedings of the 20th Annual
Meeting of the Association for Computational
Linguistics, pages 74?81, Toronto.
Gazdar, Gerald, Ewan Klein, Geoffrey
Pullum, and Ivan Sag. 1985. Generalized
Phrase Structure Grammar. Harvard
University Press, Cambridge, MA.
Hale, John. 2001. A probabilistic Earley
parser as a psycholinguistic model.
In Proceedings of the Second Meeting of the
North American Chapter of the Association
for Computational Linguistics on Language
Technologies, pages 1?8, Pittsburgh, PA.
Hofmeister, Philip, T Florian Jaeger, Inbal
Arnon, Ivan A. Sag, and Neal Snider.
2013. The source ambiguity problem:
Distinguishing the effects of grammar and
processing on acceptability judgments.
Language and Cognitive Processes,
28(1-2):48?87.
Hofmeister, Philip and Ivan A. Sag. 2010.
Cognitive constraints and island effects.
Language, 86(2):366?415.
Hudson, Richard. 1984. Word Grammar.
Blackwell, Oxford.
Jurafsky, Daniel. 1996. A probabilistic model
of lexical and syntactic access and
disambiguation. Cognitive Science,
20:137?194.
Levy, Roger. 2008. Expectation-based
syntactic comprehension. Cognition,
106(3):1126?1177.
Makino, Takaki, Minoru Yoshida, Kentaro
Torisawa, and J. Tsujii. 1998. LiLFeS ?
towards a practical HPSG parser. In
Proceedings of the 17th International
Conference on Computational Linguistics and
the 36th Annual Meeting of the Association for
Computational Linguistics, pages 807?811,
Montreal.
Oepen, Stephan, Erik Velldal, Jan Tore
L?nning, Paul Meurer, Victoria Rose?n,
and Dan Flickinger. 2007. Towards hybrid
quality-oriented machine translation. On
linguistics and probabilities in MT. In the
11th International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI-07), pages 144?153,
Sko?vde.
Penn, Gerald. 2004. Balancing clarity and
efficiency in typed feature logic through
delaying. In Proceedings of the 42nd Meeting
of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 239?246, Barcelona.
Pollard, Carl and Ivan A. Sag. 1987.
Information-Based Syntax and Semantics.
Volume 1: Fundamentals. CSLI Lecture
Notes # 13. Center for the Study of
Language and Information, Chicago,
IL and Stanford, CA. Distributed by
the University of Chicago Press.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
Studies in Contemporary Linguistics.
The University of Chicago Press and
CSLI Publications, Chicago, IL, and
Stanford, CA.
Pullum, Geoffrey K. and Gerald Gazdar.
1982. Natural languages and context-free
languages. Linguistics and Philosophy,
4:471?504.
Sag, Ivan A. 1992. Taking performance
seriously. VII Congresso de Languajes
Naturales y Lenguajes Formales,
pages 61?74.
Sag, Ivan A. 1993. Interview with
Anne-Marie Mineur and Gerrit Rentier.
Ta!, 2(2).
6
Bender Obituary
Sag, Ivan A. and Janet D. Fodor. 1994.
Extraction without traces. In West Coast
Conference on Formal Linguistics,
volume 13, pages 365?384,
Los Angeles, CA.
Sag, Ivan A. and Thomas Wasow. 2011.
Performance-compatible competence
grammar. Non-transformational Syntax:
Formal and Explicit Models of Grammar,
pages 359?377.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Syntactic Theory: A Formal
Introduction. CSLI, Stanford, CA,
second edition.
Shieber, Stuart M. 1985. Evidence against the
context-freeness of natural language.
Linguistics and Philosophy, 8(3):333?343.
Slayden, Glenn C. 2012. Array TFS storage
for unification grammars. Master?s thesis,
University of Washington.
Staum, Laura and Ivan A. Sag. 2008.
The advantage of the ungrammatical.
In Proceedings of the 30th Annual Meeting of
the Cognitive Science Society, pages 601?606,
Washington, DC.
Staum Casasanto, Laura, Philip Hofmeister,
and Ivan A. Sag. 2010. Understanding
acceptability judgments: Distinguishing
the effects of grammar and processing on
acceptability judgments. In Proceedings
of the 32nd Annual Conference of the
Cognitive Science Society, pages 224?229,
Portland, OR.
Suppes, P., D. Flickinger, B. Macken, J. Cook,
and T. Liang. 2012. Description of the
EPGY Stanford University online courses
for mathematics and language arts.
In International Society for Technology in
Education (ISTE) Annual 2012 Conference,
pages 1?9, San Diego, CA.
Uszkoreit, Hans, Rolf Backofen, Stephan
Busemann, Abdel Kader Diagne,
Elizabeth A. Hinkelman, Walter Kasper,
Bernd Kiefer, Hans-Ulrich Krieger, Klaus
Netter, Gu?nter Neumann, Stephan Oepen,
and Stephen P. Spackman. 1994. DISCO?
an HPSG-based NLP system and its
application for appointment scheduling.
In Proceedings of the 15th International
Conference on Computational Linguistics,
pages 436?440, Kyoto.
Wahlster, Wolfgang, editor. 2000. Verbmobil.
Foundations of Speech-to-Speech Translation.
Springer, Berlin, Germany.
Wood, Mary McGee. 1993. Categorial
Grammars. Routledge.
7

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315?326,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Getting More from Morphology in Multilingual Dependency Parsing
Matt Hohensee and Emily M. Bender
University of Washington
Department of Linguistics
Box 354340
Seattle WA 98195-4340, USA
{hohensee, ebender}@uw.edu
Abstract
We propose a linguistically motivated set of
features to capture morphological agreement
and add them to the MSTParser dependency
parser. Compared to the built-in morphologi-
cal feature set, ours is both much smaller and
more accurate across a sample of 20 morpho-
logically annotated treebanks. We find in-
creases in accuracy of up to 5.3% absolute.
While some of this results from the feature set
capturing information unrelated to morphol-
ogy, there is still significant improvement, up
to 4.6% absolute, due to the agreement model.
1 Introduction
Most data-driven dependency parsers are meant to
be language-independent. They do not use any
information that is specific to the language being
parsed, and they often rely heavily on n-grams, or
sequences of words and POS tags, to make parsing
decisions. However, designing a parser without in-
corporating any specific linguistic details does not
guarantee its language-independence; even linguis-
tically na??ve systems can involve design decisions
which in fact bias the system towards languages with
certain properties (Bender, 2011).
It is often taken for granted that using linguistic
information necessarily makes a system language-
dependent. But it is possible to design a linguisti-
cally intelligent parser without tuning it to a specific
language, by modeling at a high level phenomena
which appear cross-linguistically. Such a system is
still language-independent; it does not require any
knowledge or modeling of specific languages, but
it does use linguistic knowledge to make the most
of the available data. We present modifications to
an existing system, MSTParser (McDonald et al,
2006), to incorporate a very simple model of mor-
phological agreement. These modifications improve
parsing performance across a variety of languages
by making better use of morphological annotations.
2 Background and related work
2.1 Morphological marking of agreement
Most languages show some morphological agree-
ment via inflected noun, adjective, verb, and deter-
miner forms, although the degree to which this hap-
pens varies. At one end of the spectrum are analytic,
or ?morphologically impoverished?, languages. An
extreme example is Chinese, which shows no inflec-
tion at all; words do not take different forms de-
pending on features such as person or gender. En-
glish has some inflection, but is relatively morpho-
logically poor.
At the other end are synthetic or ?morphologi-
cally rich? languages such as Czech, which has, inter
alia, four genders and seven cases. In synthetic lan-
guages, words which are syntactically related in cer-
tain ways must agree: e.g., subject-verb agreement
for gender or determiner-noun agreement for case
(Corbett, 2006). Words participating in agreement
may be marked explicitly for the property in ques-
tion (via affixing or other morphological changes),
or may possess it inherently (with no specific affix
encoding the property). Treebanks are often anno-
tated to reflect some or all of these properties; the
level of detail depends on the annotation guidelines.
315
zahranic?n?? investice rostou
foreign investment grow
.F.PL.NOM .F.3RD.PL.NOM .3RD.PL.PRES
foreign investments grow
foreign investment grow
.3RD.PL .PL
Table 1: Sentence in Czech (Hajic?, 1998) and English
A sample sentence in English and Czech (Table 1)
demonstrates this contrast. In Czech, the adjective
and noun agree for gender, number, and case, and
the noun and verb agree for person and number. In
the English version, only the noun and verb agree.
Agreement can be very useful for data-driven de-
pendency parsing. A statistical parser can learn from
training data that, for example, a third-person singu-
lar noun is a likely dependent of a verb marked as
third-person singular. Similarly, it can learn that a
determiner showing genitive case and a noun show-
ing dative case are often not syntactically related.
It is often assumed that morphological complex-
ity correlates with degree of variation in word order.
This is because synthetic languages use inflection to
mark the roles of constituents, while analytic lan-
guages generally assign these roles to specific phrase
structural locations. Siewierska (1998) investigated
this empirically and found that it holds to a certain
extent: the absence of agreement and/or case mark-
ing predicts rigid word order, though their presence
is not particularly predictive of flexible word order.
Many parsers rely on word order to establish de-
pendencies, so they often perform best on languages
with more rigid word order. Making use of mor-
phological agreement could compensate for greater
variation in word order and help to bring parsing per-
formance on flexible-word-order languages up to par
with that on rigid-word-order languages.
2.2 MSTParser
The CoNLL-X (Buchholz and Marsi, 2006) and
CoNLL 2007 (Nivre et al, 2007) shared tasks fo-
cused on multilingual dependency parsing. Each
system was trained on treebanks in a variety of lan-
guages and predicted dependency arcs and labels for
POS-tagged data. The best performers in 2006 were
MSTParser (McDonald et al, 2006), which we use
here, and MaltParser (Nivre et al, 2006a).
MSTParser is a data-driven, graph-based parser
which creates a model from training data by learn-
ing weights for arc-level features. The feature set in-
cludes combinations of the word and POS tag of the
parent and child of each dependency arc; POS tags
of words between the parent and child; and POS tags
of the parent and child along with those of the pre-
ceding and following words. A similar feature set is
conjoined with arc labels in order to perform label-
ing, and an optional set of ?second-order? features
includes analogous information about siblings.
Morphological features for an arc are generated
by iterating over each pair in the cross product of
the parent and child tokens? lists of attributes. For
every such pair, thirteen groups of four features each
are generated. The thirteen groups represent combi-
nations of the head and child word forms/lemmas
and attributes. Each group contains subgroups dis-
tinguished by whether they use word forms or lem-
mas and by whether or not they encode the direc-
tion and distance of the dependency. These features
are summarized in Table 2. At run time, MSTParser
finds the highest-scoring parse for each sentence ac-
cording to the learned feature weights.
Decoding can be performed in projective or non-
projective mode, depending on the type of trees de-
sired. Projective trees are those in which every con-
stituent (head plus all dependents) forms a complete
subtree; non-projective parsing lacks this limitation.
2.3 Related work
The organizers of the CoNLL 2007 shared task
noted that languages with free word order and high
morphological complexity are the most difficult for
dependency parsing (Nivre et al, 2007). Most of the
participants took language-independent approaches
toward leveraging this complexity into better perfor-
mance: generating machine learning features based
on each item in a token?s list of morphological at-
tributes (Nivre et al, 2006b; Carreras et al, 2006);
using the entire list as an atomic feature (Chang et
al., 2006; Titov and Henderson, 2007); or generat-
ing features based on each pair of attributes in the
cross-product of the lists of a potential head and de-
pendent (McDonald et al, 2006; Nakagawa, 2007).
Language-specific uses of morphological infor-
mation have included using it to disambiguate func-
tion words (Bick, 2006) or to pick out finite verbs
316
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)
Table 2: Original MSTParser feature templates. hdForm and dpForm are the head and dependent word forms;
hdLemma and dpLemma are the lemmas. hdAtt and dpAtt are the morphological attributes; hdIdx and dpIdx
are their indices. dir+dist is a string encoding the direction and length of the arc. Each line represents one feature.
Unlabeled
<attr>_agrees,head=<headPOS>,dep=<depPOS>
<attr>_disagrees,head=<headPOS>,dep=<depPOS>
head_<attr=value>,head=<headPOS>,dep=<depPOS>
dep_<attr=value>,head=<headPOS>,dep=<depPOS>
Labeled
<attr>_agrees&label=<label>,head=<headPOS>,dep=<depPOS>
<attr>_disagrees&label=<label>,head=<headPOS>,dep=<depPOS>
head_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>
dep_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>
Table 3: Agreement feature templates. headPOS and depPOS are the head and dependent coarse POS tags.
(Carreras et al, 2006). Schiehlen and Spranger
(2007) used language-specific rules to add detail to
other features, such as fine-grained POS tags or lem-
mas. Attardi et al (2007) modeled agreement ex-
plicitly, generating a morphological agreement fea-
ture whenever two tokens possess the same value
for the same linguistic attribute. The authors note
accuracy improvements of up to 0.5% for Italian
and 0.8% for Catalan using a transition-based parser.
A similar approach was used by Goldberg and El-
hadad (2010), who improved the accuracy of their
transition-based Hebrew parser by adding features
for gender and number agreement in noun phrases.
The potential of morphological information to im-
prove parsing performance has been documented in
numerous experiments using MaltParser and with
various morphological attributes as machine learn-
ing features, on several morphologically rich lan-
guages, including: Russian (Nivre et al, 2008);
Swedish (?vrelid and Nivre, 2007); Bangla, Tel-
ugu, and Hindi (Nivre, 2009); Turkish (Eryig?it et
al., 2008); and Basque (Bengoetxea and Gojenola,
2010). These experiments, however, did not include
any higher-level features such as agreement.
Goldberg and Elhadad (2009) found that using
morphological features increased the accuracy of
MSTParser on Hebrew only when the morpholog-
ical annotations were gold-standard; automatic an-
notations decreased accuracy, although MaltParser
showed improvement with both gold and automatic
annotations. The accuracy of MaltParser on Arabic
was improved by different types of morphological
features depending on whether gold or automatic an-
notations were used (Marton et al, 2010).
As far as we can tell, no language-independent
approaches to utilizing morphological data thus far
have taken advantage of agreement specifically. We
take a linguistically informed approach, maintain-
ing language-independence, by explicitly modeling
agreement between head and dependent morphol-
ogy.
3 Methodology
3.1 Modifications to parser
Our approach builds on the observation that there
are two kinds of information marked in morphol-
ogy: symmetric, recorded on both head and depen-
317
ID TOKEN CPOS MORPH HEAD REL Gloss
1 Vznikaj?? VERB num=PL|per=3 0 ROOT arise.3RD.PL
2 zbytec?ne? ADJ num=PL|gen=I|case=NOM 3 ATR unnecessary.PL.INAN.NOM
3 konflikty NOUN num=PL|gen=I|case=NOM 1 SBJ conflicts.PL.INAN.NOM
num_agrees,head=NOUN,dep=ADJ num_agrees,head=VERB,dep=NOUN
num_agrees&label=ATR,head=NOUN,dep=ADJ num_agrees&label=SBJ,head=VERB,dep=NOUN
gen_agrees,head=NOUN,dep=ADJ head_per=3,head=VERB,dep=NOUN
gen_agrees&label=ATR,head=NOUN,dep=ADJ head_per=3&label=SBJ,head=VERB,dep=NOUN
case_agrees,head=NOUN,dep=ADJ dep_gen=I,head=VERB,dep=NOUN
case_agrees&label=ATR,head=NOUN,dep=ADJ dep_gen=I&label=SBJ,head=VERB,dep=NOUN
dep_case=NOM,head=VERB,dep=NOUN
dep_case=NOM&label=SBJ,head=VERB,dep=NOUN
Table 4: Sample sentence (Hajic?, 1998) and agreement features generated
dent, and asymmetric, marked on only one or the
other. Symmetric information provides a natural,
effectively non-lossy type of back-off that parsers
can take advantage of; all that matters is whether
the information on the head and dependent match.1
Furthermore, we don?t need to know ahead of time
which types of morphological information are sym-
metric. This is extracted from the annotations.
In order to take advantage of this property of nat-
ural language, we devised a set of features which
model agreement. These allow the learner to op-
erate at a higher level, using agreement itself as a
feature rather than having to discover agreement and
forming generalizations about whether tokens which
agree (or disagree) in various ways are related. Since
agreement appears cross-linguistically, such features
are applicable to a diverse set of languages.
Since MSTParser breaks down every parse into a
set of arcs, our features are defined at the arc level.
Each arc is a head and dependent pair, and each of
those tokens has a list of morphological features in
the normalized form attribute=value. We com-
pare these lists and add, for every attribute which
is present in both, either an agreement or a disagree-
ment feature, depending on whether the head and de-
pendent have the same value for that attribute. This
feature encapsulates the attribute, but not the value,
as well as the coarse POS tags of the head and the
dependent. If an attribute is present in only one of
1If an attribute is marked on both head and dependent and
the value matches, the specific value should not affect the prob-
ability or possibility of the dependency relationship. If the same
attribute is marked on both elements but is independent (not a
matter of agreement) we risk losing information, but we hypoth-
esize that such information is unlikely to be very predictive.
the lists, we add a feature encapsulating whether the
token is the head or the dependent, the single mor-
phological feature (attribute and value), and the two
coarse POS tags. We also generate both types of fea-
tures conjoined with the arc label. Like the original
feature set, we include only first-order morphologi-
cal features. See Table 3 for a summary. A sample
sentence in a simplified CoNLL format and the fea-
tures it would trigger are shown in Table 4.2
We hypothesize that these agreement features will
function as a type of back-off, allowing the parser
to extract more information from the morphological
marking. For instance, they can capture case agree-
ment between a determiner and noun. We expect
that this would lead to higher parsing accuracy, espe-
cially when training on smaller datasets, where mor-
phological data might be sparse.
We made a slight modification to the parser so that
underscores used in the treebanks to indicate the ab-
sence of morphological annotation for a token were
not themselves treated as morphological informa-
tion. This was necessary to ensure that all feature
configurations performed identically on treebanks
with no morphological information. Depending on
the treebank, this increased or decreased the perfor-
mance of the system slightly (by less than 0.5%).
3.2 Data collection and preparation
We gathered a range of dependency treebanks, rep-
resenting as many language families as possible (Ta-
ble 5). Many of these used the CoNLL shared task
treebank format, so we adopted it as well, and con-
2A more complete description of the system, as well as
source code, can be found in (Hohensee, 2012).
318
Language ISO Treebank Num.
sents.
Ref.
size
Avg.
atts.
Reference
Hindi-Urdu hin HUTB 3,855 2,800 3.6 (Bhatt et al, 2009)
Hungarian hun Szeged DTB 92,176 9,000 3.3 (Vincze et al, 2010)
Czech ces PDT 1.0 73,068 9,000 2.8 (Hajic?, 1998)
Tamil tam TamilTB v0.1 600 600 2.8 (Ramasamy and Z?abokrtsky?, 2011)
Slovene slv SDT 1,998 1,500 2.6 (Dz?eroski et al, 2006)
Danish dan DDT 5,512 5,500 2.4 (Kromann, 2003)
Basque eus 3LB* 3,175 2,800 2.4 (Aduriz et al, 2003)
Dutch nld Alpino 13,735 9,000 2.4 (Van der Beek et al, 2002)
Latin lat LDT 3,423 2,800 2.4 (Bamman and Crane, 2006)
Bulgarian bul BulTreeBank 13,221 9,000 2.1 (Simov et al, 2004)
Greek (ancient) grc AGDT 21,104 9,000 2.1 (Bamman et al, 2009)
Finnish fin Turku 4,307 2,800 2.0 (Haverinen et al, 2010)
German deu NEGRA 3,427 2,800 2.0 (Brants et al, 1999)
Turkish tur METU-Sabanci 5,620 5,500 1.6 (Oflazer et al, 2003)
Catalan cat CESS-ECE* 3,512 2,800 1.5 (Mart? et al, 2007)
Arabic ara PADT 1.0 2,367 2,300 1.2 (Hajic et al, 2004)
Italian ita TUT 2,858 2,800 1.1 (Bosco et al, 2000)
Portuguese por Floresta 9,359 9,000 1.0 (Afonso et al, 2002)
Hebrew (modern) heb DepTB 6,214 5,500 0.9 (Goldberg, 2011)
English eng Penn* 49,208 9,000 0.4 (Marcus et al, 1993)
Chinese cmn Penn Chinese 28,035 9,000 0.0 (Xue et al, 2005)
*Acquired as part of NLTK (Bird et al, 2009)
Table 5: Language, ISO 639-2 code, treebank name, total number of sentences, reference size, average number of
morphological attributes per token, and reference for each treebank used, ordered by average number of attributes.
verted the other treebanks to the same. It includes
for each token: position in the sentence; the token
itself; a lemma (not present in all datasets); a coarse
POS tag; a fine POS tag; a list of morphological fea-
tures; the token?s head; and the label for the depen-
dency relation to that head.3 We retained all punctu-
ation and other tokens in the treebanks.
The POS tagsets used in the treebanks varied
widely. We normalized the coarse tags to the univer-
sal twelve-tag set suggested by Petrov et al (2011),
in order to ensure that every treebank had coarse tags
for use in the agreement features, and to make the
features easier to interpret. It is unlikely that infor-
mation was lost in this process: for treebanks with
one set of tags, information was added, and for those
with two, the universal tags aligned closely with the
coarse tags already in the data.
Two of the treebanks we used included no mor-
phological information. We included the Penn Chi-
nese Treebank as a representative of analytic lan-
guages.4 We also included part of the (English) Penn
3The original format also included two more fields, projec-
tive head and label; neither is used by MSTParser.
4Dependency trees were generated from the Penn Chinese
Treebank, converted to dependency trees. For this
data we generated morphological annotations based
on fine POS tags, consisting of person and number
information for nouns and verbs, and person, num-
ber, and case information for pronouns. The German
NEGRA corpus includes detailed morphological an-
notations for about 3,400 sentences (of 20,600), and
we used only that portion.
Note that the amount of morphological informa-
tion present in any given treebank is a function of the
morphological properties of the language as well as
the annotation guidelines: annotations do not nec-
essarily encode all of the morphological informa-
tion which is actually marked in a language. Fur-
thermore, the presence of a morphological feature
does not imply that it participates in an agreement
relationship; it merely encodes some piece of mor-
phological information about the token. Finally, an-
notation guidelines vary as to whether they provide
for the explicit marking of morphological proper-
ties which are inherent to a lemma (e.g., gender on
nouns) and not marked by separate affixes.
Treebank using the Penn2Malt converter: http://w3.msi.
vxu.se/?nivre/research/Penn2Malt.html.
319
We normalized all morphological annotations to
the form attribute=value (e.g., case=NOM). For
treebanks that provided values only, this involved
adding attribute names, obtained from the annota-
tion guidelines. The attributes person, number, gen-
der, and case appeared often; also included in some
data were verb tense, adjective degree, and pronoun
type (e.g., personal, possessive, or reflexive). We
normalized all features in the data, regardless of
whether they participate in any agreement relations.
Many of the treebanks include data from multiple
domains; to minimize the effects of this, we random-
ized the order of sentences in each treebank.
3.3 Experimental setup
All experiments were performed using 5-fold cross-
validation. Reported accuracies, run times, and fea-
ture counts are averages over all five folds. We
ran experiments on multiple cross-validation dataset
sizes in order to assess the performance of our model
when trained on different amounts of data. For each
treebank, we report results on a ?reference size?:
9,000 sentences or the largest size available (for tree-
banks of less than 9,000 sentences).
For evaluation, we used the module built into
MSTParser. We focused on the unlabeled accu-
racy score (percentage of tokens with correctly as-
signed heads, ignoring labels). We also looked at
labeled accuracies, but found they displayed trends
very similar, if not identical, to the unlabeled scores.
4 Results
We ran the system on each treebank at all dataset
sizes in projective and non-projective modes, using
no morphological features. For each language, sub-
sequent tests used the algorithm which performed
better (or non-projective in the case of a tie).
4.1 Overall results
We ran the parser on each treebank with each of
four feature configurations: one with no morpho-
logical features (no-morph); one with the original
morphological features (orig; Table 2); one using
the agreement features (agr; Table 3); and one us-
ing both feature sets (agr+orig).
Table 6 displays the unlabeled accuracy, run time,
and feature counts when parsing each treebank using
each feature configuration at the reference size, with
the highest accuracy highlighted. Excluding Chi-
nese, agr generated the best performance in all but
two cases, outperforming orig by margins ranging
from 0.8% (Arabic) to 5.3% (Latin) absolute. In the
other cases, agr+orig outperformed agr slightly.
In all cases, the total number of machine learning
features was approximately the same for no-morph
and agr, and for orig and agr+orig, because
the number of morphological features generated by
orig is very large compared to the number gener-
ated by agr. Performance was noticeably faster for
the two smaller feature configurations.
Figure 1 shows the error reduction of orig, agr,
and agr+orig relative to no-morph, at the refer-
ence size. Despite its relative lack of morphological
inflection, English shows a fairly high error reduc-
tion, because parsing performance on English was
already high. Similarly, error reduction on some of
the morphologically rich languages is lower because
baseline performance was low. Calculating the cor-
relation coefficient (Pearson?s r) between average
morphological attributes per token and error reduc-
tion gives r = 0.608 for orig, r = 0.560 for agr,
and r = 0.428 for agr+orig, with p < 0.01 for
the first two and p < 0.10 for the last, indicating
moderate correlations for all feature sets.
The strength of these correlations depends on sev-
eral factors. Languages differ in what information is
marked morphologically, and in number of agree-
ment relationships. Annotation schemes vary in
what morphological information they encode, and in
how relevant that information is to agreement. Some
morphologically complex languages have rigid word
order, leading to better performance with no mor-
phological features at all, and limiting the amount
of improvement that is possible. Finally, it is pos-
sible that a stronger correlation is obscured by other
effects due to feature set design, as we will find later.
4.2 Performance vs. dataset size
Figures 2 presents unlabeled accuracy when parsing
Czech with the orig and agr configurations. Im-
provement with agr is roughly uniform across all
dataset sizes; this was the general trend for all tree-
banks. This is somewhat unexpected; we had pre-
dicted that the agreement features would be more
helpful at smaller dataset sizes.
320
no-morph orig agr agr+orig
Lang. UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?feats
hin 90.0 1.4k 1.6m 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%
hun 87.9 4.6k 5.3m 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%
ces 80.9 3.3k 4.8m 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%
tam 79.0 0.1k 0.5m 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%
slv 80.8 0.8k 1.0m 80.4 103% 352% 81.9 21% 1% 80.8 129% 353%
dan 87.7 2.0k 1.6m 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%
lat 61.7 1.8k 1.6m 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%
nld 88.2 2.0k 3.6m 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%
eus 78.7 0.7k 1.7m 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%
bul 89.9 1.7k 2.6m 90.1 60% 221% 93.0 14% 0% 92.5 54% 222%
grc 74.9 8.6k 3.8m 76.9 36% 314% 80.7 45% 0% 79.5 70% 314%
deu 90.0 0.9k 1.3m 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%
fin 73.3 0.7k 2.4m 76.3 74% 244% 79.1 23% 1% 78.7 84% 245%
tur 80.2 1.2k 2.1m 81.5 13% 178% 81.6 ?2% 0% 81.7 29% 178%
cat 81.8 3.0k 2.5m 81.9 2% 142% 84.9 -9% 0% 84.0 ?2% 143%
ara 78.0 3.2k 2.0m 78.1 65% 94% 78.9 23% 0% 78.7 20% 94%
ita 88.3 4.2k 1.8m 88.9 ?3% 59% 90.2 9% 0% 90.3 6% 59%
por 88.1 6.4k 5.0m 88.1 18% 46% 89.0 ?3% 0% 88.9 27% 46%
heb 87.4 4.3k 3.1m 87.4 ?18% 31% 89.2 ?16% 0% 89.1 ?5% 31%
eng 88.1 5.2k 3.1m 88.0 5% 7% 90.6 3% 0% 90.6 ?9% 8%
cmn 82.4 7.5k 6.0m 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%
Table 6: Unlabeled accuracy, run time in seconds, and number of features for all treebanks and feature configurations.
Run time and number of features for orig, agr, and agr+orig are given as percent change relative to no-morph
4.3 Gold vs. automatic tags
The Hebrew treebank includes both automatically
generated and gold standard POS and morphological
annotations. In order to test how sensitive the agree-
ment features are to automatically predicted mor-
phological information, tests were run on both ver-
sions at the reference size. These results are not di-
rectly comparable to those of Goldberg and Elhadad
(2009), because of the parser modifications, POS tag
normalization, and cross-validation described ear-
lier. Comparing results qualitatively, we find less
sensitivity to the automatic tags overall, and that the
orig features improve accuracy even when using
automatic tags.
Results appear in Table 7. Using the automatic
data affects all feature sets negatively by 2.1% to
2.9%. Since the no-morph parser was affected the
most, it appears that this decrease is due largely
to errors in the POS tags, rather than the morpho-
logical annotations. The orig features compensate
for this slightly (0.2%), and the agr features more
(0.8%); this indicates that including even automatic
morphological information can compensate for in-
correct POS tags, and that the agr feature configu-
ration is the most robust when given predicted tags.
Feature
configuration
Acc. on
gold data
Acc. on
auto data
Difference
no-morph 87.4 84.5 ?2.9
orig 87.4 84.7 ?2.7
agr 89.3 87.2 ?2.1
agr+orig 89.1 86.9 ?2.2
Table 7: Unlabeled accuracy on Hebrew dataset, with
gold and automatic POS and morphological annotations
4.4 PPL feature
Examining the feature weights from the first cross-
validation fold when running the agr feature config-
uration on the Czech dataset indicated that 323 of the
1,000 highest-weighted features are agreement fea-
tures. Of these, 79 are symmetric (?agrees? or ?dis-
agrees?) agr features, and 244 asymmetric. This
was unexpected, as the symmetric features would
seem to be more useful, and it suggested that the la-
beled asymmetric agr features might be important
for reasons other than their modeling of morpholog-
ical information. Careful analysis of the MSTParser
feature set revealed that it does not include a fea-
ture which incorporates head POS, dependent POS,
and dependency label. We hypothesized that the la-
beled asymmetric agr features were highly ranked
321
Figure 1: Error reduction relative to no-morph vs. language
Figure 2: Unlabeled accuracy vs. num. sentences, Czech
because they capture these three arc features, not be-
cause they include with morphological information.
To test this, we added a single feature template
to MSTParser which encapsulates head POS, de-
pendent POS, and dependency label (the POS-POS-
label, or PPL, feature). Running a subsequent ex-
periment on the Czech data and looking at feature
weights from the same cross-validation fold, 278 of
the 1,000 highest-weighted features were PPL fea-
tures, and 187 were asymmetric agr features. This
indicated that the improvement seen with agr fea-
tures was indeed due partly to their inclusion of fea-
tures combining label and head and dependent POS.
All feature configurations were run on all tree-
banks with the PPL feature included; results appear
in Table 8. Performance increases from orig to
agr are generally smaller, with a maximum of 4.6%
absolute. This is seen especially on languages with
less morphological information, such as English and
Hebrew; this indicates that for those languages, most
of the previous improvement was due not to agree-
ment modeling, but to the PPL effect.
Calculating Pearson?s r between morphological
features per token and the new error reduction data
gives a stronger correlation coefficient of 0.748 for
agr, with p < 0.01, demonstrating that improve-
ment due solely to agreement modeling correlates
strongly with quantity of morphological informa-
tion. The earlier error reduction data were likely
polluted by improvement due to capturing the PPL
information. Correlation for the other feature con-
figurations is still moderate (0.506 with p < 0.02
for orig and 0.621 with p < 0.01 for agr+orig).
5 Future work
In future work, we plan to experiment with more
careful normalization of treebanks. For instance,
if an adjective can agree with either a masculine
or a feminine noun, annotating it with both gen=M
322
no-morph orig agr agr+orig
Lang. UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?feats
hin 90.0 1.4k 1.6 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%
hun 87.9 4.6k 5.3 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%
ces 80.9 3.3k 4.8 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%
tam 79.0 0.1k 0.5 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%
slv 80.8 0.8k 1.0 80.4 102% 352% 81.8 21% 0% 80.8 129% 353%
dan 87.8 2.0k 1.6 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%
lat 61.7 1.8k 1.6 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%
nld 88.2 2.0k 3.6 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%
eus 78.7 0.7k 1.7 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%
bul 89.9 1.7k 2.6 90.2 60% 221% 93.0 14% 0% 92.5 54% 222%
grc 74.9 8.6k 3.8 77.0 36% 314% 80.7 45% 0% 79.5 70% 314%
deu 90.0 0.9k 1.3 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%
fin 73.3 0.7k 2.4 76.3 74% 244% 79.1 23% 0% 78.7 84% 245%
tur 80.2 1.2k 2.1 81.5 13% 178% 81.6 -2% 0% 81.7 29% 178%
cat 81.8 3.0k 2.5 81.9 1% 142% 84.9 -9% 0% 84.0 -2% 143%
ara 77.6 5.4k 1.8 77.7 20% 100% 78.2 -8% 0% 78.0 4% 100%
ita 88.4 4.2k 1.8 88.9 -2% 59% 90.2 9% 0% 90.3 6% 59%
por 88.1 6.4k 5.0 88.2 18% 46% 89.0 -3% 0% 88.9 27% 46%
heb 87.4 4.3k 3.1 87.4 -18% 31% 89.2 -16% 0% 89.1 -5% 31%
eng 88.1 5.2k 3.1 88.0 5% 7% 90.6 3% 0% 90.6 -9% 7%
cmn 82.4 7.5k 6.0 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%
Table 8: Unlabeled accuracy, run time in seconds, and number of features with PPL feature included. Run time and
number of features for orig, agr, and agr+orig are given as percent change relative to no-morph.
and gen=F (rather than gen=X) would ensure that
agreement with a noun of either gender would be
captured by our features. Furthermore, we may ex-
periment with filtering morphological information
based on part-of-speech, on attribute, or on whether
the attribute participates in any agreement relation-
ships. We also intend to perform feature selection on
the original feature set, and investigate the impor-
tance of labeled morphological features, which are
included in agr but not in orig. Finally, we plan to
develop metrics to measure the degree of word or-
der flexibility in a treebank, in order to explore the
extent to which it correlates with the degree of im-
provement achieved by our system.
6 Conclusions
We developed a simple, language-independent
model of agreement to better leverage morphologi-
cal data in dependency parsing. Testing on treebanks
containing varying amounts of morphological infor-
mation resulted in substantial improvements in pars-
ing accuracy while reducing feature counts and run
times significantly. Although originally intended to
compensate for lower accuracy on morphologically
rich languages, the model improved performance on
all treebanks with any morphological information.
We acknowledge that because our model was
tested on treebanks which differ widely in annota-
tion guidelines, variables such as the amount of mor-
phological information included and the treatment
of non-projective parses and coordination could af-
fect parsing performance. We did not delve into
these factors. However, we believe this is part of the
strength of the approach: we were able to achieve
performance gains without any detailed knowledge
of the languages and treebanks used.
We hope these results will encourage similarly
linguistically motivated design in future systems.
This case study provides strong evidence that in-
corporating linguistic knowledge into NLP systems
does not preclude language independence, and in-
deed may enhance it, by leveling performance across
typologically differing languages.
Acknowledgments
We would like to thank everyone who assisted us in
gathering treebanks, particularly Maite Oronoz and
her colleagues at the University of the Basque Coun-
try and Yoav Goldberg, as well as three anonymous
reviewers for their comments.
323
References
I. Aduriz, M.J. Aranzabe, J.M. Arriola, A. Atutxa, A.D.
de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In
Proc. of the Second Workshop on Treebanks and Lin-
guistic Theories (TLT 2003), pages 201?204.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta?(c)tica: A treebank for Portuguese. In
Proc. of the Third International Conference on Lan-
guage Resources and Evaluation (LREC 2002), page
1698.
G. Attardi, F. DellOrletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using DeSR. In Proc. of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL 2007), pages
1112?1118.
D. Bamman and G. Crane. 2006. The design and use
of a Latin dependency treebank. In Proc. of the Fifth
International Workshop on Treebanks and Linguistic
Theories (TLT 2006), pages 67?78.
D. Bamman, F. Mambrini, and G. Crane. 2009. An own-
ership model of annotation: The Ancient Greek De-
pendency Treebank. In Proc. of the Eighth Interna-
tional Workshop on Treebanks and Linguistic Theories
(TLT8), pages 5?15.
E.M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology: Special Issue on Interaction of
Linguistics and Computational Linguistics, 6(3):1?26.
K. Bengoetxea and K. Gojenola. 2010. Application of
different techniques to dependency parsing of Basque.
In Proc. of the First Workshop on Statistical Parsing
of Morphologically Rich Languages (SPMRL 2010),
pages 31?39. Association for Computational Linguis-
tics.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for Hindi/Urdu. In Proc. of
the Third Linguistic Annotation Workshop (LAW III),
pages 186?189. Association for Computational Lin-
guistics.
E. Bick. 2006. LingPars, a linguistically inspired,
language-independent machine learner for depen-
dency treebanks. In Proc. of the Tenth Conference on
Computational Natural Language Learning (CoNLL-
X), pages 171?175. Association for Computational
Linguistics.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
C. Bosco, V. Lombardo, D. Vassallo, and L. Lesmo.
2000. Building a treebank for Italian: a data-driven
annotation schema. In Proc. of the Second Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2000), pages 99?106.
T. Brants, W. Skut, and H. Uszkoreit. 1999. Syntactic
annotation of a German newspaper corpus. Treebanks:
Building and using parsed corpora, 20:73.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 149?164. Associa-
tion for Computational Linguistics.
X. Carreras, M. Surdeanu, and L. Marquez. 2006. Pro-
jective dependency parsing with perceptron. In Proc.
of the Tenth Conference on Computational Natural
Language Learning (CoNLL-X), pages 181?185. As-
sociation for Computational Linguistics.
M.W. Chang, Q. Do, and D. Roth. 2006. A pipeline
model for bottom-up dependency parsing. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 186?190. Associa-
tion for Computational Linguistics.
G.G. Corbett. 2006. Agreement. Cambridge University
Press.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. Z?abokrtsky, and A. Z?ele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006).
G. Eryig?it, J. Nivre, and K. Oflazer. 2008. Depen-
dency parsing of Turkish. Computational Linguistics,
34(3):357?389.
Y. Goldberg and M. Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proc. of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 129?133. Association for Com-
putational Linguistics.
Y. Goldberg and M. Elhadad. 2010. Easy-first depen-
dency parsing of Modern Hebrew. In Proc. of the First
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL 2010), pages 103?107. As-
sociation for Computational Linguistics.
Yoav Goldberg. 2011. Automatic Syntactic Processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University.
J. Hajic, O. Smrz, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR Inter-
national Conference on Arabic Language Resources
and Tools, pages 110?117.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning:
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
324
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proc. of the Ninth
International Workshop on Treebanks and Linguistic
Theories (TLT9, volume 9, pages 79?90.
M. Hohensee. 2012. It?s only morpho-logical: Model-
ing agreement in cross-linguistic dependency parsing.
Master?s thesis, University of Washington.
M.T. Kromann. 2003. The Danish Dependency Tree-
bank and the DTAG treebank tool. In Proc. of the Sec-
ond Workshop on Treebanks and Linguistic Theories
(TLT 2003), pages 217?220.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
M.A. Mart?, M. Taule?, L. Ma?rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel an-
notated corpus.
Y. Marton, N. Habash, and O. Rambow. 2010. Improv-
ing Arabic dependency parsing with lexical and inflec-
tional morphological features. In Proc. of the First
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL 2010), pages 13?21. Asso-
ciation for Computational Linguistics.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of the Tenth Conference on
Computational Natural Language Learning (CoNLL-
X), pages 216?220. Association for Computational
Linguistics.
T. Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), pages 952?
956.
J. Nivre, J. Hall, and J. Nilsson. 2006a. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC 2006), vol-
ume 6, pages 2216?2219.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 221?225. Associa-
tion for Computational Linguistics.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. CoNLL 2007 shared
task on dependency parsing. In Proc. of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007). Association
for Computational Linguistics.
J. Nivre, I.M. Boguslavsky, and L.L. Iomdin. 2008. Pars-
ing the SynTagRus treebank of Russian. In Proc. of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), volume 1, pages 641?648.
Association for Computational Linguistics.
J. Nivre. 2009. Parsing Indian languages with Malt-
Parser. In Proc. of the Seventh International Confer-
ence on Natural Language Processing (ICON 2009)
NLP Tools Contest, pages 12?18.
K. Oflazer, B. Say, D.Z. Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. Text, Speech, and Lan-
guage Technology, pages 261?277.
L. ?vrelid and J. Nivre. 2007. When word order and
part-of-speech tags are not enough?Swedish depen-
dency parsing with rich linguistic features. In Proc. of
the International Conference on Recent Advances in
Natural Language Processing (RANLP), pages 447?
451.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2011.
Tamil dependency parsing: Results using rule based
and corpus based approaches. In Proc. of the 12th
International Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLING 2011),
volume 1, pages 82?95, Berlin, Heidelberg. Springer-
Verlag.
M. Schiehlen and K. Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL 2007), pages
1156?1160.
Anna Siewierska. 1998. Variation in major constituent
order: A global and a European perspective. In
Anna Siewierska, editor, Constituent Order in the
Languages of Europe, pages 475?551. Mouton De
Gruyter.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2004. Design and implementation of the Bulgar-
ian HPSG-based treebank. Research on Language &
Computation, 2(4):495?522.
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 947?951.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers, 45(1):8?22.
325
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian dependency treebank.
In Proc. of the Seventh Conference on Language Re-
sources and Evaluation (LREC 2010).
N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
326
T1: 100 Things You Always Wanted to Know 
about Linguistics But Were Afraid to Ask* 
*... for fear of being told 1000 more
Emily M. Bender 
ABSTRACT
Many NLP tasks have at their core a subtask of extracting the dependencies---who did 
what to whom---from natural language sentences. This task can be understood as the 
inverse of the problem solved in different ways by diverse human languages, namely, 
how to indicate the relationship between different parts of a sentence. Understanding 
how languages solve the problem can be extremely useful in both feature design and 
error analysis in the application of machine learning to NLP. Likewise, understanding 
cross-linguistic variation can be important for the design of MT systems and other 
multilingual applications. The purpose of this tutorial is to present in a succinct and 
accessible fashion information about the structure of human languages that can be 
useful in creating more linguistically sophisticated, more language independent, and 
thus more successful NLP systems. 
While many kinds of linguistic structure can be relevant to different NLP tasks, the focus 
of this tutorial will be on morphosyntax. The tutorial will take an explicitly typological 
perspective as an understanding of cross-linguistic variation can facilitate the design of 
more portable (language-independent) NLP systems. In order to help participants retain 
the information better, the tutorial will be structured interactively. I will ask participants 
for examples of tasks and data sets they work with, and then as a group we will 
brainstorm ways in which each of the linguistic properties discussed can related to 
feature design and/or error analysis for those tasks. 
OUTLINE
1. Introduction: 
Overview of tutorial, elicitation of examples of tasks and data sets 
2. Morphology: 
Morphophonology and morphosyntax, the range of morphological processes 
found in the world's languages, what can be expressed through morphology 
3. Basic Syntax: 
Part of speech and grammatical functions in cross-linguistic perspective, 
syntactic phenomena which can obscure the relationship between syntactic and 
semantic roles 
4. Syntactic Complications: 
Phenomena beyond simple clauses, including long-distance dependencies, 
clausal modification, semantically empty function words, argument drop 
5. Resources: 
Where to go to find out more, what to expect from morphological analyzers, 
dependency parsers and precision grammars 
BIO
Emily M. Bender
Department of Linguistics 
University of Washington 
Email: ebender@uw.edu 
Emily M. Bender is an Associate Professor in the Department of Linguistics and Adjunct 
Associate Professor in the Department of Computer Science & Engineering at the 
University of Washington. Her primary research interests lie in multilingual grammar 
engineering and the incorporation of linguistic knowledge, especially from linguistic 
typology, in NLP. She is the PI of the Grammar Matrix project, which is developed in the 
context of the DELPH-IN Consortium (Deep Linguistic Processing with HPSG Initiative). 
More generally, she is interested in the intersection of linguistics and computational 
linguistics, from both directions: bringing computational methodologies to linguistic 
science and linguistic science to natural language processing. 
Her PhD (in Linguistics) is from Stanford University. She has authored or co-authored 
papers in Linguistic Issues in Language Technology, the Journal of Research on 
Language and Computation, English Language and Linguistics, the Encyclopedia of 
Language and Linguistics, and the proceedings of ACL, COLING, IJCNLP and 
associated workshops. 
Proceedings of the ACL 2010 System Demonstrations, pages 1?6,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Grammar Prototyping and Testing with the
LinGO Grammar Matrix Customization System
Emily M. Bender, Scott Drellishak, Antske Fokkens, Michael Wayne Goodman,
Daniel P. Mills, Laurie Poulson, and Safiyyah Saleem
University of Washington, Seattle, Washington, USA
{ebender,sfd,goodmami,dpmills,lpoulson,ssaleem}@uw.edu,
afokkens@coli.uni-saarland.de
Abstract
This demonstration presents the LinGO
Grammar Matrix grammar customization
system: a repository of distilled linguis-
tic knowledge and a web-based service
which elicits a typological description of
a language from the user and yields a cus-
tomized grammar fragment ready for sus-
tained development into a broad-coverage
grammar. We describe the implementation
of this repository with an emphasis on how
the information is made available to users,
including in-browser testing capabilities.
1 Introduction
This demonstration presents the LinGO Gram-
mar Matrix grammar customization system1 and
its functionality for rapidly prototyping grammars.
The LinGO Grammar Matrix project (Bender et
al., 2002) is situated within the DELPH-IN2 col-
laboration and is both a repository of reusable
linguistic knowledge and a method of delivering
this knowledge to a user in the form of an ex-
tensible precision implemented grammar. The
stored knowledge includes both a cross-linguistic
core grammar and a series of ?libraries? contain-
ing analyses of cross-linguistically variable phe-
nomena. The core grammar handles basic phrase
types, semantic compositionality, and general in-
frastructure such as the feature geometry, while
the current set of libraries includes analyses of
word order, person/number/gender, tense/aspect,
case, coordination, pro-drop, sentential negation,
yes/no questions, and direct-inverse marking, as
well as facilities for defining classes (types) of lex-
ical entries and lexical rules which apply to those
types. The grammars produced are compatible
with both the grammar development tools and the
1
http://www.delph-in.net/matrix/customize/
2
http://www.delph-in.net
grammar-based applications produced by DELPH-
IN. The grammar framework used is Head-driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) and the grammars map bidirectionally
between surface strings and semantic representa-
tions in the format of Minimal Recursion Seman-
tics (Copestake et al, 2005).
The Grammar Matrix project has three goals?
one engineering and two scientific. The engineer-
ing goal is to reduce the cost of creating gram-
mars by distilling the solutions developed in exist-
ing DELPH-IN grammars and making them easily
available for new projects. The first scientific goal
is to support grammar engineering for linguistic
hypothesis testing, allowing users to quickly cus-
tomize a basic grammar and use it as a medium in
which to develop and test analyses of more inter-
esting phenomena.3 The second scientific goal is
to use computational methods to combine the re-
sults of typological research and formal syntactic
analysis into a single resource that achieves both
typological breadth (handling the known range of
realizations of the phenomena analyzed) and ana-
lytical depth (producing analyses which work to-
gether to map surface strings to semantic represen-
tations) (Drellishak, 2009).
2 System Overview
Grammar customization with the LinGO Gram-
mar Matrix consists of three primary activities:
filling out the questionnaire, preliminary testing of
the grammar fragment, and grammar creation.
2.1 Questionnaire
Most of the linguistic phenomena supported by the
questionnaire vary across languages along multi-
ple dimensions. It is not enough, for example,
3Research of this type based on the Grammar Matrix
includes (Crysmann, 2009) (tone change in Hausa) and
(Fokkens et al, 2009) (Turkish suspended affixation).
1
simply to know that the target language has coor-
dination. It is also necessary to know, among other
things, what types of phrases can be coordinated,
how those phrases are marked, and what patterns
of marking appear in the language. Supporting a
linguistic phenomenon, therefore, requires elicit-
ing the answers to such questions from the user.
The customization system elicits these answers us-
ing a detailed, web-based, typological question-
naire, then interprets the answers without human
intervention and produces a grammar in the format
expected by the LKB (Copestake, 2002), namely
TDL (type description language).
The questionnaire is designed for linguists who
want to create computational grammars of natu-
ral languages, and therefore it freely uses techni-
cal linguistic terminology, but avoids, when possi-
ble, mentioning the internals of the grammar that
will be produced, although a user who intends to
extend the grammar will need to become familiar
with HPSG and TDL before doing so.
The questionnaire is presented to the user as a
series of connected web pages. The first page the
user sees (the ?main page?) contains some intro-
ductory text and hyperlinks to direct the user to
other sections of the questionnaire (?subpages?).
Each subpage contains a set of related questions
that (with some exceptions) covers the range of
a single Matrix library. The actual questions in
the questionnaire are represented by HTML form
fields, including: text fields, check boxes, ra-
dio buttons, drop-downs, and multi-select drop-
downs. The values of these form fields are stored
in a ?choices file?, which is the object passed on
to the grammar customization stage.
2.1.1 Unbounded Content
Early versions of the customization system (Ben-
der and Flickinger, 2005; Drellishak and Bender,
2005) only allowed a finite (and small) number
of entries for things like lexical types. For in-
stance, users were required to provide exactly one
transitive verb type and one intransitive verb type.
The current system has an iterator mechanism in
the questionnaire that allows for repeated sections,
and thus unlimited entries. These repeated sec-
tions can also be nested, which allows for much
more richly structured information.
The utility of the iterator mechanism is most
apparent when filling out the Lexicon subpage.
Users can create an arbitrary number of lexical
rule ?slots?, each with an arbitrary number of
morphemes which each in turn bear any num-
ber of feature constraints. For example, the
user could create a tense-agreement morpholog-
ical slot, which contains multiple portmanteau
morphemes each expressing some combination of
tense, subject person and subject number values
(e.g., French -ez expresses 2nd person plural sub-
ject agreement together with present tense).
The ability provided by the iterators to create
unbounded content facilitates the creation of sub-
stantial grammars through the customization sys-
tem. Furthermore, the system allows users to ex-
pand on some iterators while leaving others un-
specified, thus modeling complex rule interactions
even when it cannot cover features provided by
these rules. A user can correctly model the mor-
photactic framework of the language using ?skele-
tal? lexical rules?those that specify morphemes?
forms and their co-occurrence restrictions, but per-
haps not their morphosyntactic features. The user
can then, post-customization, augment these rules
with the missing information.
2.1.2 Dynamic Content
In earlier versions of the customization system, the
questionnaire was static. Not only was the num-
ber of form fields static, but the questions were
the same, regardless of user input. The current
questionnaire is more dynamic. When the user
loads the customization system?s main page or
subpages, appropriate HTML is created on the fly
on the basis of the information already collected
from the user as well as language-independent in-
formation provided by the system.
The questionnaire has two kinds of dynamic
content: expandable lists for unbounded entry
fields, and the population of drop-down selec-
tors. The lists in an iterated section can be ex-
panded or shortened with ?Add? and ?Delete? but-
tons near the items in question. Drop-down selec-
tors can be automatically populated in several dif-
ferent ways.4 These dynamic drop-downs greatly
lessen the amount of information the user must
remember while filling out the questionnaire and
can prevent the user from trying to enter an invalid
value. Both of these operations occur without re-
freshing the page, saving time for the user.
4These include: the names of currently-defined features,
the currently-defined values of a feature, or the values of vari-
ables that match a particular regular expression.
2
2.2 Validation
It makes no sense to attempt to create a consis-
tent grammar from an empty questionnaire, an in-
complete questionnaire, or a questionnaire con-
taining contradictory answers, so the customiza-
tion system first sends a user?s answers through
?form validation?. This component places a set
of arbitrarily complex constraints on the answers
provided. The system insists, for example, that
the user not state the language contains no deter-
miners but then provide one in the Lexicon sub-
page. When a question fails form validation, it
is marked with a red asterisk in the questionnaire,
and if the user hovers the mouse cursor over the as-
terisk, a pop-up message appears describing how
form validation failed. The validation component
can also produce warnings (marked with red ques-
tion marks) in cases where the system can gen-
erate a grammar from the user?s answers, but we
have reason to believe the grammar won?t behave
as expected. This occurs, for example, when there
are no verbal lexical entries provided, yielding a
grammar that cannot parse any sentences.
2.3 Creating a Grammar
After the questionnaire has passed validation, the
system enables two more buttons on the main
page: ?Test by Generation? and ?Create Gram-
mar?. ?Test by Generation? allows the user to test
the performance of the current state of the gram-
mar without leaving the browser, and is described
in ?3. ?Create Grammar? causes the customiza-
tion system to output an LKB-compatible grammar
that includes all the types in the core Matrix, along
with the types from each library, tailored appropri-
ately, according to the specific answers provided
for the language described in the questionnaire.
2.4 Summary
This section has briefly presented the structure
of the customization system. While we antici-
pate some future improvements (e.g., visualiza-
tion tools to assist with designing type hierarchies
and morphotactic dependencies), we believe that
this system is sufficiently general to support the
addition of analyses of many different linguistic
phenomena. The system has been used to create
starter grammars for more than 40 languages in the
context of a graduate grammar engineering course.
To give sense of the size of the grammars
produced by the customization system, Table 1
compares the English Resource Grammar (ERG)
(Flickinger, 2000), a broad-coverage precision
grammar in the same framework under develop-
ment since 1994, to 11 grammars produced with
the customization system by graduate students in
a grammar engineering class at the University of
Washington. The students developed these gram-
mars over three weeks using reference materials
and the customization system. We compare the
grammars in terms of the number types they de-
fine, as well as the number of lexical rule and
phrase structure rule instances.5 We separate
types defined in the Matrix core grammar from
language-specific types defined by the customiza-
tion system. Not all of the Matrix-provided types
are used in the definition of the language-specific
rules, but they are nonetheless an important part of
the grammar, serving as the foundation for further
hand-development. The Matrix core grammar in-
cludes a larger number of types whose function is
to provide disjunctions of parts of speech. These
are given in Table 1, as ?head types?. The final col-
umn in the table gives the number of ?choices? or
specifications that the users gave to the customiza-
tion system in order to derive these grammars.
3 Test-by-generation
The purpose of the test-by-generation feature is to
provide a quick method for testing the grammar
compiled from a choices file. It accomplishes this
by generating sentences the grammar deems gram-
matical. This is useful to the user in two main
ways: it quickly shows whether any ungrammat-
ical sentences are being licensed by the grammar
and, by providing an exhaustive list of licensed
sentences for an input template, allows users to see
if an expected sentence is not being produced.
It is worth emphasizing that this feature of the
customization system relies on the bidirectional-
ity of the grammars; that is, the fact that the same
grammar can be used for both parsing and genera-
tion. Our experience has shown that grammar de-
velopers quickly find generation provides a more
stringent test than parsing, especially for the abil-
ity of a grammar to model ungrammaticality.
3.1 Underspecified MRS
Testing by generation takes advantage of the gen-
eration algorithm include in the LKB (Carroll et al,
5Serious lexicon development is taken as a separate task
and thus lexicon size is not included in the table.
3
Language Family Lg-specific types Matrix types Head types Lex rules Phrasal rules Choices
ERG Germanic 3654 N/A N/A 71 226 N/A
Breton Celtic 220 413 510 57 49 1692
Cherokee Iroquoian 182 413 510 95 27 985
French Romance 137 413 510 29 22 740
Jamamad?? Arauan 188 413 510 87 11 1151
Lushootseed Salish 95 413 510 20 8 391
Nishnaabemwin Algonquian 289 413 510 124 50 1754
Pashto Iranian 234 413 510 86 19 1839
Pali Indo-Aryan 237 413 510 92 55 1310
Russian Slavic 190 413 510 56 35 993
Shona Bantu 136 413 510 51 9 591
Vietnamese Austro-Asiatic 105 413 510 2 26 362
Average 182.9 413 510 63.5 28.3 1073.5
Table 1: Grammar sizes in comparison to ERG
1999). This algorithm takes input in the form of
Minimal Recursion Semantics (MRS) (Copestake
et al, 2005): a bag of elementary predications,
each bearing features encoding a predicate string,
a label, and one or more argument positions that
can be filled with variables or with labels of other
elementary predications.6 Each variable can fur-
ther bear features encoding ?variable properties?
such as tense, aspect, mood, sentential force, per-
son, number or gender.
In order to test our starter grammars by gen-
eration, therefore, we must provide input MRSs.
The shared core grammar ensures that all of
the grammars produce and interpret valid MRSs,
but there are still language-specific properties in
these semantic representations. Most notably, the
predicate strings are user-defined (and language-
specific), as are the variable properties. In addi-
tion, some coarser-grained typological properties
(such as the presence or absence of determiners)
lead to differences in the semantic representations.
Therefore, we cannot simply store a set of MRSs
from one grammar to use as input to the generator.
Instead, we take a set of stored template MRSs
and generalize them by removing all variable
properties (allowing the generator to explore all
possible values), leaving only the predicate strings
and links between the elementary predications.
We then replace the stored predicate strings with
ones selected from among those provided by the
user. Figure 1a shows an MRS produced by a
grammar fragment for English. Figure 1b shows
the MRS with the variable properties removed
and the predicate strings replaced with generic
place-holders. One such template is needed for
every sentence type (e.g., intransitive, transitive,
6This latter type of argument encodes scopal dependen-
cies. We abstract away here from the MRS approach to scope
underspecification which is nonetheless critical for its com-
putational tractability.
a. ? h1,e2, {h7: cat n rel(x4:SG:THIRD),
h3:exist q rel(x4, h5, h6),
h1: sleep v rel(e2:PRES, x4)},
{h5 qeq h7} ?
b. ? h1,e2, {h7:#NOUN1#(x4),
h3:#DET1#(x4, h5, h6),
h1:#VERB#(e2, x4)},
{h5 qeq h7} ?
Figure 1: Original and underspecified MRS
negated-intransitive, etc.). In order to ensure that
the generated strings are maximally informative to
the user testing a grammar, we take advantage of
the lexical type system. Because words in lexical
types as defined by the customization system dif-
fer only in orthography and predicate string, and
not in syntactic behavior, we need only consider
one word of each type. This allows us to focus the
range of variation produced by the generator on
(a) the differences between lexical types and (b)
the variable properties.
3.2 Test by generation process
The first step of the test-by-generation process is
to compile the choices file into a grammar. Next,
a copy of the LKB is initialized on the web server
that is hosting the Matrix system, and the newly-
created grammar is loaded into this LKB session.
We then construct the underspecified MRSs in
order to generate from them. To do this, the pro-
cess needs to find the proper predicates to use for
verbs, nouns, determiners, and any other parts of
speech that a given MRS template may require. For
nouns and determiners, the choices file is searched
for the predicate for one noun of each lexical noun
type, all of the determiner predicates, and whether
or not each noun type needs a determiner or not.
For verbs, the process is more complicated, re-
quiring valence information as well as predicate
strings in order to select the correct MRS template.
In order to get this information, the process tra-
verses the type hierarchy above the verbal lexical
4
types until it finds a type that gives valence infor-
mation about the verb. Once the process has all
of this information, it matches verbs to MRS tem-
plates and fills in appropriate predicates.
The test-by-generation process then sends these
constructed MRSs to the LKB process and displays
the generation results, along with a brief explana-
tion of the input semantics that gave rise to them,
in HTML for the user.7
4 Related Work
As stated above, the engineering goal of the Gram-
mar Matrix is to facilitate the rapid development
of large-scale precision grammars. The starter
grammars output by the customization system are
compatible in format and semantic representations
with existing DELPH-IN tools, including software
for grammar development and for applications in-
cluding machine translation (Oepen et al, 2007)
and robust textual entailment (Bergmair, 2008).
More broadly, the Grammar Matrix is situated
in the field of multilingual grammar engineer-
ing, or the practice of developing linguistically-
motivated grammars for multiple languages within
a consistent framework. Other projects in this
field include ParGram (Butt et al, 2002; King
et al, 2005) (LFG), the CoreGram project8 (e.g.,
(Mu?ller, 2009)) (HPSG), and the MetaGrammar
project (de la Clergerie, 2005) (TAG).
To our knowledge, however, there is only one
other system that elicits typological information
about a language and outputs an appropriately cus-
tomized implemented grammar. The system, de-
scribed in (Black, 2004) and (Black and Black,
2009), is called PAWS (Parser And Writer for
Syntax) and is available for download online.9
PAWS is being developed by SIL in the context
of both descriptive (prose) grammar writing and
?computer-assisted related language adaptation?,
the practice of writing a text in a target language
by starting with a translation of that text in a
related source language and mapping the words
from target to source. Accordingly, the output of
PAWS consists of both a prose descriptive grammar
7This set-up scales well to multiple users, as the user?s in-
teraction with the LKB is done once per customized grammar,
providing output for the user to peruse as his or her leisure.
The LKB process does not persist, but can be started again
by reinvoking test-by-generation, such as when the user has
updated the grammar definition.
8
http://hpsg.fu-berlin.de/Projects/core.html
9
http://www.sil.org/computing/catalog/show_
software.asp?id=85
and an implemented grammar. The latter is in the
format required by PC-PATR (McConnel, 1995),
and is used primarily to disambiguate morpholog-
ical analyses of lexical items in the input string.
Other systems that attempt to elicit linguistic in-
formation from a user include the Expedition (Mc-
Shane and Nirenburg, 2003) and Avenue projects
(Monson et al, 2008), which are specifically tar-
geted at developing machine translation for low-
density languages. These projects differ from the
Grammar Matrix customization system in elic-
iting information from native speakers (such as
paradigms or translations of specifically tailored
corpora), rather than linguists. Further, unlike the
Grammar Matrix customization system, they do
not produce resources meant to sustain further de-
velopment by a linguist.
5 Demonstration Plan
Our demonstration illustrates how the customiza-
tion system can be used to create starter gram-
mars and test them by invoking test-by-generation.
We first walk through the questionnaire to illus-
trate the functionality of libraries and the way that
the user interacts with the system to enter infor-
mation. Then, using a sample grammar for En-
glish, we demonstrate how test-by-generation can
expose both overgeneration (ungrammatical gen-
erated strings) and undergeneration (gaps in gen-
erated paradigms). Finally, we return to the ques-
tionnaire to address the bugs in the sample gram-
mar and retest to show the result.
6 Conclusion
This paper has presented an overview of the
LinGO Grammar Matrix Customization System,
highlighting the ways in which it provides ac-
cess to its repository of linguistic knowledge. The
current customization system covers a sufficiently
wide range of phenomena that the grammars it
produces are non-trivial. In addition, it is not al-
ways apparent to a user what the implications will
be of selecting various options in the question-
naire, nor how analyses of different phenomena
will interact. The test-by-generation methodology
allows users to interactively explore the conse-
quences of different linguistic analyses within the
platform. We anticipate that it will, as a result, en-
courage users to develop more complex grammars
within the customization system (before moving
on to hand-editing) and thereby gain more benefit.
5
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0644097. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Emily M. Bender and Dan Flickinger. 2005. Rapid
prototyping of scalable grammars: Towards modu-
larity in extensions to a language-independent core.
In Proc. of IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. of the Workshop on Grammar Engineering
and Evaluation at COLING 2002, pages 8?14.
Richard Bergmair. 2008. Monte Carlo semantics:
McPIET at RTE4. In Text Analysis Conference (TAC
2008) Workshop-RTE-4 Track. National Institute of
Standards and Technology, pages 17?19.
Cheryl A. Black and H. Andrew Black. 2009. PAWS:
Parser and writer for syntax: Drafting syntactic
grammars in the third wave. In SIL Forum for Lan-
guage Fieldwork, volume 2.
Cheryl A. Black. 2004. Parser and writer for syn-
tax. Paper presented at the International Confer-
ence on Translation with Computer-Assisted Tech-
nology: Changes in Research, Teaching, Evaluation,
and Practice, University of Rome ?La Sapienza?,
April 2004.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proc. of the Workshop
on Grammar Engineering and Evaluation at COL-
ING 2002, pages 1?7.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proc. of the 7th
European workshop on natural language generation
(EWNLG99), pages 86?95.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford.
Berthold Crysmann. 2009. Autosegmental representa-
tions in an HPSG for Hausa. In Proc. of the Work-
shop on Grammar Engineering Across Frameworks
2009.
E?ric Villemonte de la Clergerie. 2005. From meta-
grammars to factorized TAG/TIG parsers. In Proc.
of IWPT?05, pages 190?191.
Scott Drellishak and Emily M. Bender. 2005. A co-
ordination module for a crosslinguistic grammar re-
source. In Stefan Mu?ller, editor, Proc. of HPSG
2005, pages 108?128, Stanford. CSLI.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6:15 ? 28.
Antske Fokkens, Laurie Poulson, and Emily M. Ben-
der. 2009. Inflectional morphology in Turkish VP-
coordination. In Stefan Mu?ller, editor, Proc. of
HPSG 2009, pages 110?130, Stanford. CSLI.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The feature space in parallel
grammar writing. Research on Language & Com-
putation, 3(2):139?163.
Stephen McConnel. 1995. PC-PATR Refer-
ence Manual. Summer Institute for Linguistics.
http://www.sil.org/pcpatr/manual/pcpatr.html.
Marjorie McShane and Sergei Nirenburg. 2003. Pa-
rameterizing and eliciting text elements across lan-
guages for use in natural language processing sys-
tems. Machine Translation, 18:129?165.
Christian Monson, Ariadna Font Llitjs, Vamshi Am-
bati, Lori Levin, Alon Lavie, Alison Alvarez,
Roberto Aranovich, Jaime Carbonell, Robert Fred-
erking, Erik Peterson, and Katharina Probst. 2008.
Linguistic structure and bilingual informants help
induce machine translation of lesser-resourced lan-
guages. In LREC?08.
Stefan Mu?ller. 2009. Towards an HPSG analysis of
Maltese. In Bernard Comrie, Ray Fabri, Beth Hume,
Manwel Mifsud, Thomas Stolz, and Martine Van-
hove, editors, Introducing Maltese linguistics. Pa-
pers from the 1st International Conference on Mal-
tese Linguistics, pages 83?112. Benjamins, Amster-
dam.
Stephan Oepen, Erik Velldal, Jan Tore Lnning, Paul
Meurer, Victoria Rosn, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine transla-
tion. On linguistics and probabilities in MT. In
11th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
6
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Simple Negation Scope Resolution through Deep Parsing:
A Semantic Solution to a Semantic Problem
Woodley Packard
?
, Emily M. Bender
?
, Jonathon Read
?
, Stephan Oepen
??
, and Rebecca Dridan
?
?
University of Washington, Department of Linguistics
?
Teesside University, School of Computing
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
ebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.no
Abstract
In this work, we revisit Shared Task 1
from the 2012
*
SEM Conference: the au-
tomated analysis of negation. Unlike the
vast majority of participating systems in
2012, our approach works over explicit
and formal representations of proposi-
tional semantics, i.e. derives the notion of
negation scope assumed in this task from
the structure of logical-form meaning rep-
resentations. We relate the task-specific
interpretation of (negation) scope to the
concept of (quantifier and operator) scope
in mainstream underspecified semantics.
With reference to an explicit encoding
of semantic predicate-argument structure,
we can operationalize the annotation deci-
sions made for the 2012
*
SEM task, and
demonstrate how a comparatively simple
system for negation scope resolution can
be built from an off-the-shelf deep parsing
system. In a system combination setting,
our approach improves over the best pub-
lished results on this task to date.
1 Introduction
Recently, there has been increased community in-
terest in the theoretical and practical analysis of
what Morante and Sporleder (2012) call modality
and negation, i.e. linguistic expressions that mod-
ulate the certainty or factuality of propositions.
Automated analysis of such aspects of meaning
is important for natural language processing tasks
which need to consider the truth value of state-
ments, such as for example text mining (Vincze
et al, 2008) or sentiment analysis (Lapponi et al,
2012). Owing to its immediate utility in the cura-
tion of scholarly results, the analysis of negation
and so-called hedges in bio-medical research liter-
ature has been the focus of several workshops, as
well as the Shared Task at the 2011 Conference on
Computational Language Learning (CoNLL).
Task 1 at the First Joint Conference on Lex-
ical and Computational Semantics (
*
SEM 2012;
Morante and Blanco, 2012) provided a fresh, prin-
cipled annotation of negation and called for sys-
tems to analyze negation?detecting cues (affixes,
words, or phrases that express negation), resolv-
ing their scopes (which parts of a sentence are ac-
tually negated), and identifying the negated event
or property. The task organizers designed and
documented an annotation scheme (Morante and
Daelemans, 2012) and applied it to a little more
than 100,000 tokens of running text by the nov-
elist Sir Arthur Conan Doyle. While the task and
annotations were framed from a semantic perspec-
tive, only one participating system actually em-
ployed explicit compositional semantics (Basile et
al., 2012), with results ranking in the middle of
the 12 participating systems. Conversely, the best-
performing systems approached the task through
machine learning or heuristic processing over syn-
tactic and linguistically relatively coarse-grained
representations; see ? 2 below.
Example (1), where ?? marks the cue and {}
the in-scope elements, illustrates the annotations,
including how negation inside a noun phrase can
scope over discontinuous parts of the sentence.
1
(1) {The German} was sent for but professed to
{know} ?nothing? {of the matter}.
In this work, we return to the 2012
*
SEM
task from a deliberately semantics-centered point
of view, focusing on the hardest of the three
sub-problems: scope resolution.
2
Where Morante
and Daelemans (2012) characterize negation as an
?extra-propositional aspect of meaning? (p. 1563),
1
Our running example is a truncated variant of an item
from the Shared Task training data. The remainder of the
original sentence does not form part of the scope of this cue.
2
Resolving negation scope is a more difficult sub-problem
at least in part because (unlike cue and event identification) it
is concerned with much larger, non-local and often discontin-
uous parts of each utterance. This intuition is confirmed by
Read et al (2012), who report results for each sub-problem
using gold-standard inputs; in this setup, scope resolution
showed by far the lowest performance levels.
69
we in fact see it as a core piece of composi-
tionally constructed logical-form representations.
Though the task-specific concept of scope of
negation is not the same as the notion of quan-
tifier and operator scope in mainstream under-
specified semantics, we nonetheless find that re-
viewing the 2012
*
SEM Shared Task annotations
with reference to an explicit encoding of seman-
tic predicate-argument structure suggests a sim-
ple and straightforward operationalization of their
concept of negation scope. Our system imple-
ments these findings through a notion of functor-
argument ?crawling?, using as our starting point
the underspecified logical-form meaning represen-
tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically,
we correlate the structures at play in the Morante
and Daelemans (2012) view on negation with
formal semantic analyses; methodologically, we
demonstrate how to approach the task in terms of
underspecified, logical-form semantics; and prac-
tically, our combined system retroactively ?wins?
the 2012
*
SEM Shared Task. In the following
sections, we review related work (? 2), detail our
own setup (? 3), and present and discuss our ex-
perimental results (? 4 and ? 5, respectively).
2 Related Work
Read et al (2012) describe the best-performing
submission to Task 1 of the 2012
*
SEM Confer-
ence. They investigated two approaches for scope
resolution, both of which were based on syntac-
tic constituents. Firstly, they created a set of 11
heuristics that describe the path from the preter-
minal of a cue to the constituent whose projec-
tion is predicted to match the scope. Secondly
they trained an SVM ranker over candidate con-
stituents, generated by following the path from a
cue to the root of the tree and describing each
candidate in terms of syntactic properties along
the path and various surface features. Both ap-
proaches attempted to handle discontinuous in-
stances by applying two heuristics to the predicted
scope: (a) removing preceding conjuncts from the
scope when the cue is in a conjoined phrase and
(b) removing sentential adverbs from the scope.
The ranking approach showed a modest advan-
tage over the heuristics (with F
1
equal to 77.9
and 76.7, respectively, when resolving the scope
of gold-standard cues in evaluation data). Read et
al. (2012) noted however that the annotated scopes
did not align with the Shared Task?provided con-
stituents for 14% of the instances in the training
data, giving an F
1
upper-bound of around 86.0 for
systems that depend on those constituents.
Basile et al (2012) present the only submission
to Task 1 of the 2012
*
SEM Conference which
employed compositional semantics. Their scope
resolution pipeline consisted primarily of the C&C
parser and Boxer (Curran et al, 2007), which pro-
duce Discourse Representation Structures (DRSs).
The DRSs represent negation explicitly, including
representing other predications as being within the
scope of negation. Basile et al (2012) describe
some amount of tailoring of the Boxer lexicon to
include more of the Shared Task scope cues among
those that produce the negation operator in the
DRSs, but otherwise the system appears to directly
take the notion of scope of negation from the DRS
and project it out to the string, with one caveat: As
with the logical-forms representations we use, the
DRS logical forms do not include function words
as predicates in the semantics. Since the Shared
Task gold standard annotations included such ar-
guably semantically vacuous (see Bender, 2013,
p. 107) words in the scope, further heuristics are
needed to repair the string-based annotations com-
ing from the DRS-based system. Basile et al re-
sort to counting any words between in-scope to-
kens which are not themselves cues as in-scope.
This simple heuristic raises their F
1
for full scopes
from 20.1 to 53.3 on system-predicted cues.
3 System Description
The new system described here is what we call
the MRS Crawler. This system operates over
the normalized semantic representations provided
by the LinGO English Resource Grammar (ERG;
Flickinger, 2000).
3
The ERG maps surface strings
to meaning representations in the format of Mini-
mal Recursion Semantics (MRS; Copestake et al,
2005). MRS makes explicit predicate-argument
relations, as well as partial information about
scope (see below). We used the grammar together
with one of its pre-packaged conditional Maxi-
mum Entropy models for parse ranking, trained
on a combination of encyclopedia articles and
tourism brochures. Thus, the deep parsing front-
end system to our MRS Crawler has not been
3
In our experiments, we use the 1212 release of the ERG,
in combination with the ACE parser (http://sweaglesw
.org/linguistics/ace/). The ERG and ACE are DELPH-
IN resources; see http://www.delph-in.net.
70
? h
1
,
h
4
:_the_q?0:3?(ARG0 x
6
, RSTR h
7
, BODY h
5
), h
8
:_german_n_1?4:10?(ARG0 x
6
),
h
9
:_send_v_for?15:19?(ARG0 e
10
, ARG1 , ARG2 x
6
), h
2
:_but_c?24:27?(ARG0 e
3
, L-HNDL h
9
, R-HNDL h
14
),
h
14
:_profess_v_to?28:37?(ARG0 e
13
, ARG1 x
6
, ARG2 h
15
), h
16
:_know_v_1?41:45?(ARG0 e
17
, ARG1 x
6
, ARG2 x
18
),
h
20
:_no_q?46:53?(ARG0 x
18
, RSTR h
21
, BODY h
22
), h
19
:thing?46:53?(ARG0 x
18
),
h
19
:_of_p?54:56?(ARG0 e
23
, ARG1 x
18
, ARG2 x
24
),
h
25
:_the_q?57:60?(ARG0 x
24
, RSTR h
27
, BODY h
26
), h
28
:_matter_n_of?61:68?(ARG0 x
24
, ARG1 )
{ h
27
=
q
h
28
, h
21
=
q
h
19
, h
15
=
q
h
16
, h
7
=
q
h
8
, h
1
=
q
h
2
} ?
Figure 1: MRS analysis of our running example (1).
adapted to the task or its text type; it is applied
in an ?off the shelf? setting. We combine our
system with the outputs from the best-performing
2012 submission, the system of Read et al (2012),
firstly by relying on the latter for system negation
cue detection,
4
and secondly as a fall-back in sys-
tem combination as described in ? 3.4 below.
Scopal information in MRS analyses delivered
by the ERG fixes the scope of operators?such as
negation, modals, scopal adverbs (including sub-
ordinating conjunctions like while), and clause-
embedding verbs (e.g. believe)?based on their
position in the constituent structure, while leaving
the scope of quantifiers (e.g. a or every, but also
other determiners) free. From these underspec-
ified representations of possible scopal configu-
rations, a scope resolution component can spell
out the full range of fully-connected logical forms
(Koller and Thater, 2005), but it turns out that such
enumeration is not relevant here: the notion of
scope encoded in the Shared Task annotations is
not concerned with the relative scope of quantifiers
and negation, such as the two possible readings of
(2) represented informally below:
5
(2) Everyone didn?t leave.
a. ?(x)?leave(x) ? Everyone stayed.
b. ??(x)leave(x) ? At least some stayed.
However, as shown below, the information about
fixed scopal elements in an underspecified MRS is
sufficient to model the Shared Task annotations.
3.1 MRS Crawling
Fig. 1 shows the ERG semantic analysis for our
running example. The heart of the MRS is a mul-
tiset of elementary predications (EPs). Each ele-
4
Read et al (2012) predicted cues using a closed vocabu-
lary assumption with a supervised classifier to disambiguate
instances of cues.
5
In other words, a possible semantic interpretation of the
(string-based) Shared Task annotation guidelines and data is
in terms of a quantifier-free approach to meaning representa-
tion, or in terms of one where quantifier scope need not be
made explicit (as once suggested by, among others, Alshawi,
1992). From this interpretation, it follows that the notion of
scope assumed in the Shared Task does not encompass inter-
actions of negation operators and quantifiers.
mentary prediction includes a predicate symbol,
a label (or ?handle?, prefixed to predicates with
a colon in Fig. 1), and one or more argument
positions, whose values are semantic variables.
Eventualities (e
i
) in MRS denote states or activ-
ities, while instance variables (x
j
) typically corre-
spond to (referential or abstract) entities. All EPs
have the argument position ARG0, called the dis-
tinguished variable (Oepen and L?nning, 2006),
and no variable is the ARG0 of more than one non-
quantifier EP.
The arguments of one EP are linked to the argu-
ments of others either directly (sharing the same
variable as their value), or indirectly (through so-
called ?handle constraints?, where =
q
in Fig. 1 de-
notes equality modulo quantifier insertion). Thus
a well-formed MRS forms a connected graph. In
addition, the grammar links the EPs to the ele-
ments of the surface string that give rise to them,
via character offsets recorded in each EP (shown
in angle brackets in Fig. 1). For the purposes of
the present task, we take a negation cue as our en-
try point into the MRS graph (as our initial active
EP), and then move through the graph according
to the following simple operations to add EPs to
the active set:
Argument Crawling Add to the scope all EPs
whose distinguished variable or label is an argu-
ment of the active EP; for arguments of type h
k
,
treat any =
q
constraints as label equality.
Label Crawling Add all EPs whose label is iden-
tical to that of the active EP.
Functor Crawling Add all EPs that take the dis-
tinguished variable or label of the active EP as an
argument (directly or via =
q
constraints).
Our MRS crawling algorithm is sketched in
Fig. 2. To illustrate how the rules work, we will
trace their operation in the analysis of example (1),
i.e. traverse the EP graph in Fig. 1.
The negation cue is nothing, from character po-
sition 46 to 53. This leads us to _no_q as our en-
try point into the graph. Our algorithm states that
for this type of cue (a quantifier) the first step is
71
1: Activate the cue EP
2: if the cue EP is a quantifier then
3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP
4: end if
5: repeat
6: for each active EP X do
7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.
a
8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctions
reached by ARG1: whether, when, because, to, with, although, unless, until, or as.
9: end for
10: until a fixpoint is reached (no additional EPs were activated)
11: Deactivate zero-pronoun EPs (from imperative constructions)
12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)
13: Apply punctuation heuristics
Figure 2: Algorithm for scope detection by MRS crawling
a
Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =
q
equated with the
label of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negation
cue is one of its own arguments. See ? 3.3 for elaboration.
functor crawling (see ? 3.3 below), which brings
_know_v_1 into the scope. We proceed with ar-
gument crawling and label crawling, which pick
up _the_q?0:3? and _german_n_1 as the ARG1.
Further, as the ARG2 of _know_v_1, we reach
thing and through recursive invocation we acti-
vate _of_p and, in yet another level of recursion,
_the_q?57:60? and _matter_n_of. At this point,
crawling has no more links to follow. Thus, the
MRS crawling operations ?paint? a subset of the
MRS graph as in-scope for a given negation cue.
3.2 Semantically Empty Word Handling
Our crawling rules operate on semantic represen-
tations, but the annotations are with reference to
the surface string. Accordingly, we need projec-
tion rules to map from the ?painted? MRS to the
string. We can use the character offsets recorded
in each EP to project the scope to the string. How-
ever, the string-based annotations also include
words which the ERG treats as semantically vacu-
ous. Thus in order to match the gold annotations,
we define a set of heuristics for when to count vac-
uous words as in scope. In (1), there are no se-
mantically empty words in-scope, so we illustrate
these heuristics with another example:
(3) ?I trust that {there is} ?nothing? {of consequence
which I have overlooked}??
The MRS crawling operations discussed above
paint the EPs corresponding to is, thing, of, conse-
quence, I, and overlooked as in-scope (underlined
in (3)). Conversely, the ERG treats the words that,
there, which, and have as semantically empty. Of
these, we need to add all except that to the scope.
Our vacuous word handling rules use the syntac-
tic structure provided by the ERG as scaffolding to
help link the scope information gleaned from con-
tentful words to vacuous words. Each node in the
syntax tree is initially colored either in-scope or
out-of-scope in agreement with the decision made
by the crawler about the lexical head of the corre-
sponding subtree. A semantically empty word is
determined to be in-scope if there is an in-scope
syntax tree node in the right position relative to it,
as governed by a short list of templates organized
by the type of the semantically empty word (par-
ticles, complementizers, non-referential pronouns,
relative pronouns, and auxiliary verbs).
As an example, the rule for auxiliary verbs like
have in our example (3) is that they are in scope
when their verb phrase complement is in scope.
Since overlooked is marked as in-scope by the
crawler, the semantically empty have becomes in-
scope as well. Sometimes the rules need to be
iterated. For example, the main rule for relative
pronouns is that they are in-scope when they fill
a gap in an in-scope constituent; which fills a gap
in the constituent have overlooked, but since have
is the (syntactic) lexical head of that constituent,
the verb phrase is not considered in-scope the first
time the rules are tried.
Similar rules deal with that (complementizers
are in-scope when the complement phrase is an ar-
gument of an in-scope verb, which is not the case
here) and there (non-referential pronouns are in-
scope when they are the subject of an in-scope VP,
which is true here).
72
3.3 Re-Reading the Annotation Guidelines
Our MRS crawling algorithm was defined by look-
ing at the annotated data rather than the annota-
tion guidelines for the Shared Task (Morante et al,
2011). Nonetheless, our algorithm can be seen as
a first pass formalization of the guidelines. In this
section, we briefly sketch how our algorithm cor-
responds to different aspects of the guidelines.
For negated verbs, the guidelines state that ?If
the negated verb is the main verb in the sen-
tence, the entire sentence is in scope.? (Morante
et al, 2011, 17). In terms of our operations de-
fined over semantic representations, this is ren-
dered as follows: all arguments of the negated
verb are selected by argument crawling, all in-
tersective modifiers by label crawling, and func-
tor crawling (Fig. 2, line 8) captures modal auxil-
iaries and non-intersective modifiers. The guide-
lines treat predicative adjectives under a separate
heading from verbs, but describe the same desired
annotations (scope over the whole clause; ibid.,
p. 20). Since these structures are analogous in the
semantic representations, the same operations that
handle negated verbs also handle negated predica-
tive adjectives correctly.
For negated subjects and objects, the guidelines
state that the negation scopes over ?all the clause?
and ?the clause headed by the verb? (Morante et
al., 2011, 19), respectively. The examples given in
the annotation guidelines suggest that these are in
fact meant to refer to the same thing. The negation
cue for a negated nominal argument will appear
as a quantifier EP in the MRS, triggering line 3 of
our algorithm. This functor crawling step will get
to the verb?s EP, and from there, the process is the
same as the last two cases.
In contrast to subjects and objects, negation of
a clausal argument is not treated as negation of the
verb (ibid., p. 18). Since in this case, the negation
cue will not be a quantifier in the MRS, there will
be no functor crawling to the verb?s EP.
For negated modifiers, the situation is somewhat
more complex, and this is a case where our crawl-
ing algorithm, developed on the basis of the anno-
tated data, does not align directly with the guide-
lines as given. The guidelines state that negated at-
tributive adjectives have scope over the entire NP
(including the determiner) (ibid., p. 20) and anal-
ogously negated adverbs have scope over the en-
tire clause (ibid., p. 21). However, the annotations
are not consistent, especially with respect to the
treatment of negated adjectives: while the head
noun and determiner (if present) are typically an-
notated as in scope, other co-modifiers, especially
long, post-nominal modifiers (including relative
clauses) are not necessarily included:
(4) ?A dabbler in science, Mr. Holmes, a picker up
of shells on the shores of {the} great ?un?{known
ocean}.
(5) Our client looked down with a rueful face at {his}
own ?un?{conventional appearance}.
(6) Here was {this} ?ir?{reproachable Englishman}
ready to swear in any court of law that the accused
was in the house all the time.
(7) {There is}, on the face of it, {something}
?un?{natural about this strange and sudden friend-
ship between the young Spaniard and Scott Eccles}.
Furthermore, the guidelines treat relative clauses
as subordinate clauses and thus negation inside a
relative clause is treated as bound to that clause
only, and includes neither the head noun of the
relative clause nor any of its other dependents in
its scope. However, from the perspective of MRS,
a negated relative clause is indistinguishable from
any other negated modifier of a noun. This treat-
ment of relative clauses (as well as the inconsis-
tencies in other forms of co-modification) is the
reason for the exception noted at line 7 of Fig. 2.
By disallowing the addition of EPs to the scope if
they share the label of the negation cue but are not
one of its arguments, we block the head noun?s EP
(and any EPs only reachable from it) in cases of
relative clauses where the head verb inside the rel-
ative clause is negated. It also blocks co-modifiers
like great, own, and the phrases headed by ready
and about in (4)?(7). As illustrated in these exam-
ples, this is correct some but not all of the time.
Having been unable to find a generalization cap-
turing when comodifiers are annotated as in scope,
we stuck with this approximation.
For negation within clausal modifiers of verbs,
the annotation guidelines have further informa-
tion, but again, our existing algorithm has the cor-
rect behavior: The guidelines state that a negation
cue inside of the complement of a subordinating
conjunction (e.g. if ) has scope only over the sub-
ordinate clause (ibid., p. 18 and p. 26). The ERG
treats all subordinating conjunctions as two-place
predicates taking two scopal arguments. Thus,
as with clausal complements of clause-embedding
verbs, the embedding subordinating conjunction
and any other arguments it might have are inac-
cessible, since functor crawling is restricted to a
handful of specific configurations.
73
As is usually the case with exercises in for-
malization, our crawling algorithm generalizes be-
yond what is given explicitly in the annotation
guidelines. For example, all arguments that are
treated as semantically nominal (including PP ar-
guments where the preposition is semantically
null) are treated in the same way as subjects and
objects; similarly, all arguments which are seman-
tically clausal (including certain PP arguments)
are handled the same way as clausal complements.
This is possible because we take advantage of the
high degree of normalization that the ERG accom-
plishes in mapping to the MRS representation.
There are also cases where we are more spe-
cific. The guidelines do not handle coordination in
detail, except to state that in coordinated clauses
negation is restricted to the clause it appears in
(ibid., p. 17?18) and to include a few examples of
coordination under the heading ?ellipsis?. In the
case of VP coordination, our existing algorithm
does not need any further elaboration to pick up
the subject of the coordinated VP but not the non-
negated conjunct, as shown in discussion of (1) in
? 3.1 above. In the case of coordination of negated
NPs, recall that to reach the main portion of the
negated scope we must first apply functor crawl-
ing. The functor crawling procedure has a general
mechanism to transparently continue crawling up
through coordinated structures while blocking fu-
ture crawling from traversing them again.
6
On the other hand, there are some cases in the
annotation guidelines which our algorithm does
not yet handle. We have not yet provided any anal-
ysis of the special cases for save and expect dis-
cussed in Morante et al, 2011, pp. 22?23, and also
do not have a means of picking out the overt verb
in gapping constructions (p. 24).
Finally, we note that even carefully worked out
annotation guidelines such as these are never fol-
lowed perfectly consistently by the human annota-
tors who apply them. Because our crawling algo-
rithm so closely models the guidelines, this puts
our system in an interesting position to provide
feedback to the Shared Task organizers.
3.4 Fall-Back Configurations
The close match between our crawling algorithm
and the annotation guidelines supported by the
mapping to MRS provides for very high precision
6
This allows ate to be reached in We ate bread but no fish.,
while preventing but and bread from being reached, which
they otherwise would via argument crawling from ate.
and recall when the analysis engine produces the
desired MRS.
7
However, the analysis engine does
not always provide the desired analysis, largely
because of idiosyncrasies of the genre (e.g. voca-
tives appearing mid-sentence) that are either not
handled by the grammar or not well modeled in the
parse selection component. In addition, as noted
above, there are a handful of negation cues we do
not yet handle. Thus, we also tested fall-back con-
figurations which use scope predictions based on
MRS in some cases, and scope predictions from
the system of Read et al (2012) in others.
Our first fall-back configuration (Crawler
N
in
Table 1) uses MRS-based predictions whenever
there is a parse available and the cue is one that
our system handles. Sometimes, the analysis
picked by the ERG?s statistical model is not the
correct analysis for the given context. To com-
bat such suboptimal parse selection performance,
we investigated using the probability of the top
ranked analysis (as determined by the parse selec-
tion model and conditioned on the sentence) as a
confidence metric. Our second fall-back configu-
ration (Crawler
P
in Table 1) uses MRS-based pre-
dictions when there is a parse available whose con-
ditional probability is at least 0.5.
8
4 Experiments
We evaluated the performance of our system using
the Shared Task development and evaluation data
(respectively CDD and CDE in Table 1). Since we
do not attempt to perform cue detection, we report
performance using gold cues and also using the
system cues predicted by Read et al (2012). We
used the official Shared Task evaluation script to
compute all scores.
4.1 Data Sets
The Shared Task data consists of chapters from
the Adventures of Sherlock Holmes mystery nov-
els and short stories. As such, the text is carefully
edited turn-of-the-20th-century British English,
9
7
And in fact, the task is somewhat noise-tolerant: some
parse selection decisions are independent of each other, and
a mistake in a part of the analysis far enough away from the
negation cue does not harm performance.
8
This threshold was determined empirically on the devel-
opment data. We also experimented with other confidence
metrics?the probability ratio of the top-ranked and second
parse or the entropy over the probability distribution of the
top 10 parses?but found no substantive differences.
9
In contrast, the ERG was engineered for the analysis of
contemporary American English, and an anecdotal analysis
of parse failures and imperfect top-ranked parses suggests
74
Gold Cues System Cues
Scopes Tokens Scopes Tokens
Set Method Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
Prec Rec F
1
C
D
D
Ranker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1
Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1
Crawler
N
100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3
Crawler
P
100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4
Oracle 100.0 76.8 86.9 91.5 89.1 90.3
C
D
E
Ranker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3
Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
N
98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9
Crawler
P
98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4
Oracle 100.0 70.3 82.6 89.5 93.1 91.3
Table 1: Scope resolution performance of various configurations over each subset of the Shared Task
data. Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,
or falling back to the Ranker prediction either when the sentence is not covered by the parser (Crawler
N
),
or when the parse probability is predicted to be less than 0.5 (Crawler
P
); finally, Oracle simulates best
possible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).
annotated with token-level information about the
cues and scopes in every negated sentence. The
training set contains 848 negated sentences, the
development set 144, and the evaluation set 235.
As there can be multiple usages of negation in one
sentence, this corresponds to 984, 173, and 264
instances, respectively.
Being rule-based, our system does not require
any training data per se. However, the majority of
our rule development and error analysis were per-
formed against the designated training data. We
used the designated development data for a single
final round of error analysis and corrections. The
system was declared frozen before running with
the formal evaluation data. All numbers reported
here reflect this frozen system.
10
4.2 Results
Table 1 presents the results of our various config-
urations in terms of both (a) whole scopes (i.e. a
true positive is only generated when the predicted
scope matches the gold scope exactly) and (b) in-
scope tokens (i.e. a true positive for every token
the system correctly predicts to be in scope). The
table also details the performance upper-bound for
system combination, in which an oracle selects the
system prediction which scores the greater token-
wise F
1
for each gold cue.
The low recall levels for Crawler can be mostly
that the archaic style in the 2012
*
SEM Shared Task texts
has a strong adverse effect on the parser.
10
The code and data are available from http://www
.delph-in.net/crawler/, for replicability (Fokkens et al,
2013).
attributed to imperfect parser coverage. Crawler
N
,
which falls back just for parse failure brings the
recall back up, and results in F
1
levels closer to
the system of Read et al (2012), albeit still not
quite advancing the state of the art (except over
the development set). Our best results are from
Crawler
P
, which outperforms all other configura-
tions on the development and evaluation sets.
The Oracle results are interesting because they
show that there is much more to be gained in com-
bining our semantics-based system with the Read
et al (2012) syntactically-focused system. Further
analysis of these results to draw out the patterns of
complementary errors and strengths is a promising
avenue for future work.
4.3 Error Analysis
To shed more light on specific strengths and weak-
nesses of our approach, we performed a manual er-
ror analysis of scope predictions by Crawler, start-
ing from gold cues so as to focus in-depth analy-
sis on properties specific to scope resolution over
MRSs. This analysis was performed on CDD, in
order to not bar future work on this task. Of the
173 negation cue instances in CDD, Crawler by it-
self makes 94 scope predictions that exactly match
the gold standard. In comparison, the system of
Read et al (2012) accomplishes 119 exact scope
matches, of which 80 are shared with Crawler; in
other words, there are 14 cue instances (or 8%
of all cues) in which our approach can improve
over the best-performing syntax-based submission
to the original Shared Task.
75
We reviewed the 79 negation instances where
Crawler made a wrong prediction in terms of ex-
act scope match, categorizing the source of failure
into five broad error types:
(1) Annotation Error In 11% of all instances, we
consider the annotations erroneous or inconsistent.
These judgments were made by two of the authors,
who both were familiar with the annotation guide-
lines and conventions observable in the data. For
example, Morante et al (2011) unambiguously
state that subordinating conjunctions shall not be
in-scope (8), whereas relative pronouns should be
(9), and a negated predicative argument to the cop-
ula must scope over the full clause (10):
(8) It was after nine this morning {when we} reached
his house and {found} ?neither? {you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flight
something precious, something which {he could}
?not? {bear to part with}, had been left behind.
(10) He said little about the case, but from that little we
gathered that he also was not ?dis?{satisfied} at the
course of events.
(2) Parser Failure Close to 30% of Crawler fail-
ures reflect lacking coverage in the ERG parser,
i.e. inputs for which the parser does not make
available an analysis (within certain bounds on
time and memory usage).
11
In this work, we have
treated the ERG as an off-the-shelf system, but
coverage could certainly be straightforwardly im-
proved by adding analyses for phenomena partic-
ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our false
scope predictions are Crawler-external, viz. owing
to erroneous input MRSs due to imperfect disam-
biguation by the parser or other inadequacies in
the parser output. Again, these judgments (assign-
ing blame outside our own work) were double-
checked by two authors, and we only counted
MRS imperfections that actually involve the cue
or in-scope elements. Here, we could anticipate
improvements by training the parse ranker on in-
domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,
there is a valid MRS, but Crawler fails to pick out
an initial EP that corresponds to the negation cue.
This first type of genuine crawling failure often re-
lates to cues expressed as affixation (11), as well
11
Overall parsing coverage on this data is about 86%, but
of course all parser failures on sentences containing negation
surface in our error analysis of Crawler in isolation.
Scopes Tokens
Method Prec Rec F
1
Prec Rec F
1
C
D
E
Boxer 76.1 41.0 53.3 69.2 82.3 75.2
Crawler 87.8 43.4 58.1 78.8 66.7 72.2
Crawler
P
87.6 62.7 73.1 82.6 88.5 85.4
Table 2: Comparison to Basile et al (2012).
as to rare usages of cue expressions that predomi-
nantly occur with different categories, e.g. neither
as a generalized quantifier (12):
(11) Please arrange your thoughts and let me know, in
their due sequence, exactly what those events are
{which have sent you out} ?un?{brushed} and un-
kempt, with dress boots and waistcoat buttoned
awry, in search of advice and assistance.
(12) You saw yourself {how} ?neither? {of the inspec-
tors dreamed of questioning his statement}, extraor-
dinary as it was.
(5) Crawler Deficiency Finally, a little more
than 16% of incorrect predictions we attribute to
our crawling rules proper, where we see many
instances of under-coverage of MRS elements
(13, 14) and a few cases of extending the scope too
wide (15). In the examples below, erroneous scope
predictions by Crawler are indicated through un-
derlining. Hardly any of the errors in this category,
however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoor
servants who unite in {their} fear and
?dis?{like of their master}.
(14) He said little about the case, but from that
little we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.
5 Discussion and Comparison
The example in (1) nicely illustrates the strengths
of the MRS Crawler and of the abstraction pro-
vided by the deep linguistic analysis made pos-
sible by the ERG. The negated verb in that sen-
tence is know, and its first semantic argument is
The German. This semantic dependency is di-
rectly and explicitly represented in the MRS, but
the phrase expressing the dependent is not adja-
cent to the head in the string. Furthermore, even
a system using syntactic structure to model scope
would be faced with a more complicated task than
our crawling rules: At the level of syntax the de-
pendency is mediated by both verb phrase coordi-
nation and the control verb profess, as well as by
the semantically empty infinitival marker to.
76
The system we propose is very similar in spirit
to that of Basile et al (2012). Both systems map
from logical forms with explicit representations of
scope of negation out to string-based annotations
in the format provided by the Shared Task gold
standard. The main points of difference are in the
robustness of the system and in the degree of tai-
loring of both the rules for determining scope on
the logical form level and the rules for handling se-
mantically vacuous elements. The system descrip-
tion in Basile et al (2012) suggests relatively little
tailoring at either level: aside from adjustments to
the Boxer lexicon to make more negation cues take
the form of the negation operator in the DRS, the
notion of scope is directly that given in the DRS.
Similarly, their heuristic for picking up semanti-
cally vacuous words is string-based and straight-
forward. Our system, on the other hand, models
the annotation guidelines more closely in the def-
inition of the MRS crawling rules, and has more
elaborated rules for handling semantically empty
words. The Crawler alone is less robust than the
Boxer-based system, returning no output for 29%
of the cues in CDE. These factors all point to
higher precision and lower recall for the Crawler
compared to the Boxer-based system. At the to-
ken level, that is what we see. Since full-scope re-
call depends on token-level precision, the Crawler
does better across the board at the full-scope level.
A comparison of the results is shown in Table 2.
A final key difference between our results and
those of Basile et al (2012) is the cascading with
a fall-back system. Presumably a similar system
combination strategy could be pursued with the
Boxer-based system in place of the Crawler.
6 Conclusion and Outlook
Our motivation in this work was to take the design
of the 2012
*
SEM Shared Task on negation analy-
sis at face value?as an overtly semantic problem
that takes a central role in our long-term pursuit of
language understanding. Through both theoreti-
cal and practical reflection on the nature of repre-
sentations at play in this task, we believe we have
demonstrated that explicit semantic structure will
be a key driver of further progress in the analy-
sis of negation. We were able to closely align
two independently developed semantic analyses?
the negation-specific annotations of Morante et al
(2011), on the one hand, and the broad-coverage,
MRS meaning representations of the ERG, on the
other hand. In our view, the conceptual correla-
tion between these two semantic views on nega-
tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-
tems from the original 2012 competition, our MRS
Crawler is defined by a small set of general rules
that operate over general-purpose, explicit mean-
ing representations. Thus, our approach scores
high on transparency, adaptability, and replicabil-
ity. In isolation, the Crawler provides premium
precision but comparatively low recall. Its limi-
tations, we conjecture, reflect primarily on ERG
parsing challenges and inconsistencies in the tar-
get data. In a sense, our approach pushes a
larger proportion of the task into the parser, mean-
ing (a) there should be good opportunities for
parser adaptation to this somewhat idiosyncratic
text type; (b) our results can serve to offer feed-
back on ERG semantic analyses and parse rank-
ing; and (c) there is a much smaller proportion
of very task-specific engineering. When embed-
ded in a confidence-thresholded cascading archi-
tecture, our system advances the state of the art
on this task, and oracle combination scores sug-
gest there is much remaining room to better ex-
ploit the complementarity of approaches in our
study. In future work, we will seek to better un-
derstand the division of labor between the systems
involved through contrastive error analysis and
possibly another oracle experiment, constructing
gold-standard MRSs for part of the data. It would
also be interesting to try a task-specific adaptation
of the ERG parse ranking model, for example re-
training on the pre-existing treebanks but giving
preference to analyses that lead to correct Crawler
results downstream.
Acknowledgments
We are grateful to Dan Flickinger, the main devel-
oper of the ERG, for many enlightening discus-
sions and continuous assistance in working with
the analyses available from the grammar. This
work grew out of a discussion with colleagues of
the Language Technology Group at the University
of Oslo, notably Elisabeth Lien and Jan Tore L?n-
ning, to whom we are indebted for stimulating co-
operation. Furthermore, we have benefited from
comments by participants of the 2013 DELPH-
IN Summit, in particular Joshua Crowgey, Guy
Emerson, Glenn Slayden, Sanghoun Song, and
Rui Wang.
77
References
Alshawi, H. (Ed.). 1992. The Core Language Engine.
Cambridge, MA, USA: MIT Press.
Basile, V., Bos, J., Evang, K., and Venhuizen, N.
2012. UGroningen. Negation detection with Dis-
course Representation Structures. In Proceedings of
the 1st Joint Conference on Lexical and Computa-
tional Semantics (p. 301 ? 309). Montr?al, Canada.
Bender, E. M. 2013. Linguistic fundamentals for nat-
ural language processing: 100 essentials from mor-
phology and syntax. San Rafael, CA, USA: Morgan
& Claypool Publishers.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. 2005. Minimal Recursion Semantics. An intro-
duction. Research on Language and Computation,
3(4), 281 ? 332.
Curran, J., Clark, S., and Bos, J. 2007. Linguistically
motivated large-scale NLP with C&C and Boxer.
In Proceedings of the 45th Meeting of the Associa-
tion for Computational Linguistics Demo and Poster
Sessions (p. 33 ? 36). Prague, Czech Republic.
Flickinger, D. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engi-
neering, 6 (1), 15 ? 28.
Fokkens, A., van Erp, M., Postma, M., Pedersen,
T., Vossen, P., and Freire, N. 2013. Offspring
from reproduction problems. What replication fail-
ure teaches us. In Proceedings of the 51th Meet-
ing of the Association for Computational Linguistics
(p. 1691 ? 1701). Sofia, Bulgaria.
Koller, A., and Thater, S. 2005. Efficient solving and
exploration of scope ambiguities. In Proceedings of
the 43rd Meeting of the Association for Computa-
tional Linguistics: Interactive Poster and Demon-
stration Sessions (p. 9 ? 12). Ann Arbor, MI, USA.
Lapponi, E., Read, J., and ?vrelid, L. 2012. Repre-
senting and resolving negation for sentiment analy-
sis. In Proceedings of the 2012 ICDM workshop on
sentiment elicitation from natural text for informa-
tion retrieval and extraction. Brussels, Belgium.
Morante, R., and Blanco, E. 2012. *SEM 2012 Shared
Task. Resolving the scope and focus of negation. In
Proceedings of the 1st Joint Conference on Lexical
and Computational Semantics (p. 265 ? 274). Mon-
tr?al, Canada.
Morante, R., and Daelemans, W. 2012. ConanDoyle-
neg. Annotation of negation in Conan Doyle stories.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation. Istanbul,
Turkey.
Morante, R., Schrauwen, S., and Daelemans, W. 2011.
Annotation of negation cues and their scope guide-
lines v1.0 (Tech. Rep. # CTRS-003). Antwerp, Bel-
gium: Computational Linguistics & Psycholinguis-
tics Research Center, Universiteit Antwerpen.
Morante, R., and Sporleder, C. 2012. Modality and
negation. An introduction to the special issue. Com-
putational Linguistics, 38(2), 223 ? 260.
Oepen, S., and L?nning, J. T. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.
UiO1. Constituent-based discriminative ranking for
negation resolution. In Proceedings of the 1st Joint
Conference on Lexical and Computational Seman-
tics (p. 310 ? 318). Montr?al, Canada.
Vincze, V., Szarvas, G., Farkas, R., M?ra, G., and
Csirik, J. 2008. The BioScope corpus. Biomedical
texts annotated for uncertainty, negation and their
scopes. BMC Bioinformatics, 9(Suppl 11).
78
Semantic Representations of
Syntactically Marked Discourse
Status in Crosslinguistic
Perspective
Emily M. Bender
David Goss-Grubbs
University of Washington (USA)
email: ebender@u.washington.edu
Abstract
This paper presents suggested semantic representations for different types
of referring expressions in the format of Minimal Recursion Semantics
and sketches syntactic analyses which can create them compositionally.
We explore cross-linguistic harmonization of these representations, to
promote interoperability and reusability of linguistic analyses. We follow
Borthen and Haugereid (2005) in positing COG-ST (?cognitive status?)
as a feature on the syntax-semantics interface to handle phenomena as-
sociated with definiteness. Our proposal helps to unify the treatments of
definiteness markers, demonstratives, overt pronouns and null anaphora
across languages. In languages with articles, they contribute an existen-
tial quantifier and the appropriate value for COG-ST. In other languages,
the COG-ST value is determined by an affix. The contribution of demon-
strative determiners is decomposed into a COG-ST value, a quantifier,
and proximity information, each of which can be contributed by a dif-
ferent kind of grammatical construction in a given language. Along with
COG-ST, we posit a feature that distinguishes between pronouns (and
null anaphora) that are sensitive to the identity of the referent of their
antecedent and those that are sensitive to its type.
17
18 Bender and Goss-Grubbs
1 Introduction
In this paper, we discuss the compositional construction of semantic representations
reflecting discourse status across a range of phenomena. Borthen and Haugereid
(2005) propose COG-ST (?cognitive-status?)1 as a feature on the syntax-semantics
interface to handle phenomena associated with definiteness. We explore how their
approach leads to cross-linguistically unified treatments of demonstratives, overt pro-
nouns and null anaphora as well. We find that cross-linguistic studies motivate differ-
ent representations than we might have arrived at from just one language.
Our work grows out of the Grammar Matrix, a multilingual grammar engineering
project (Bender et al, 2002; Bender and Flickinger, 2005) which strives to harmonize
semantic representations across diverse languages. The Grammar Matrix is couched
within the Head-driven Phrase Structure Grammar (HPSG) framework (Pollard and
Sag, 1994). We use Minimal Recursion Semantics (Copestake et al, 2001, 2005) as
our semantic representation system.
2 Background
2.1 Minimal Recursion Semantics
Grammar Matrix-derived grammars associate surface strings with MRS representa-
tions (or MRSs), in a bidirectional mapping that allows both parsing and generation.
An MRS consists of a multiset of elementary predications (eps), each of which is a
single relation with its associated arguments, labeled by a handle; a set of handle con-
straints relating the labels of eps to argument positions within other eps; and a top
handle indicating which of the labels has outermost scope (Copestake et al, 2001,
2005). The MRSs produced by these grammars are underspecified for scope, allowing
multiple different fully-scoped variants, according to the handle constraints.
Each ep has a predicate (PRED) value and one or more argument positions, usu-
ally labeled ARG0 through ARGn. By convention, we refer to elementary predications
by their PRED values. For scope-taking eps (including quantifiers as well as clause-
embedding predicates such as _believe_v_rel and scopal modifiers such as negation),
at least one argument position is handle-valued, and related (in a well-formed struc-
ture) to the label of another ep. For non-scopal predications, the values of the argument
positions are variables (also called indices) which may themselves be associated with
?variable properties?, such as person, number and gender on individual variables, or
tense, aspect, sentential force and mood on event variables.
One benefit of MRS is that it is designed to be compatible with feature-structure
grammars. We build up MRSs through an HPSG implementation of the MRS algebra
in Copestake et al (2001), in which each constituent bears features recording the eps
and handle constraints contributed within the constituent, as well as a set of properties
exposed through the feature HOOK to facilitate further composition. These proper-
ties include pointers to the local top handle (LTOP), the constituent?s primary index
(INDEX), and the external argument, if any (XARG).
Eps are canonically contributed by lexical entries, with one ep per lexical entry.
Lexical entries can, however, contribute more than one ep or no eps at all. In addition,
syntactic constructions can also contribute eps of their own.
1Original feature name: COGN-ST.
Semantic Representations of Syntactically Marked Discourse Status 19
cog-st
activ-or-less uniq-or-more
uniq+fam+activ
fam-or-less fam-or-more
uniq+fam activ+fam
uniq-or-less activ-or-more
type-id uniq-id familiar activated in-foc
Figure 1: Cognitive status hierarchy
2.2 Harmonization of Representations
The semantic representations used in the Grammar Matrix were originally derived
from those used in the English Resource Grammar (Flickinger, 2000), a wide-coverage
grammar of English. In this paper, we propose to refine the semantic representations
for phenomena connected to discourse status in light of the constraints on the syntax-
semantics interface we find in a range of languages. This is not to say that we are
promoting working towards an interlingua: indeed, even if it were possible to define a
suitable interlingual set of representations, we believe it wouldn?t be possible to map
from surface strings to such representations in one compositional step.
Nonetheless, it is useful to harmonize representations across languages while still
allowing for necessary differences, for at least two reasons. First, when semantic
representations are as similar as they practically can be, this simplifies both the transfer
component in transfer-basedmachine translation systems (e.g., Oepen et al, 2007) and
the design of downstream components that make use of semantic representations in
multilingual NLP systems in general. Second, harmonized semantic representations
facilitate the creation of libraries in a resource like the GrammarMatrix, which in turn
promotes both the reuse of analyses within implemented grammars and the exploration
of computational linguistic typology.
2.3 Discourse/Cognitive Status
This paper builds on a tradition of work investigating the way the discourse status
of referents influences the form of the referring expressions used to refer to them, or
alternatively, the way that speakers use contrasts in form to signal to their interlocu-
tors the discourse (or cognitive) status of their intended referents (Chafe, 1976, 1994;
Prince, 1981; Gundel et al, 1993; Borthen and Haugereid, 2005; Arnold, 2008).
Borthen and Haugereid (2005) (henceforth B&H) present arguments from a range
of languages that the discourse status associated with referring expressions can be
constrained by multiple intersecting syntactic factors. They use this to motivate em-
bedding the discourse status information within the semantic features of a sign, rather
20 Bender and Goss-Grubbs
than on the contextual features. They adapt the implicational scale proposed by Gun-
del et al (1993) and Prince (1981), representing discourse referents as having a range
of values from ?type identifiable? through ?in focus?. In Gundel et al and Prince?s
work, this is an implicational scale, where a discourse status of ?in focus?, for example,
also entails a discourse status of ?activated?. B&H argue that it needs to be represented
within the syntax by a type hierarchy that makes each discourse status type incompat-
ible with the others, while also creating supertypes that represent ranges of discourse
status values. Their intuition is that the syntactic constraints restrict the distribution
of certain forms based on the highest discourse status they are compatible with, rather
than on the actual discourse status of the referent they are used to evoke in a given
context. The cognitive status hierarchy, as we adopt it from Borthen and Haugereid
(2005) is shown in Fig 1.
3 Markers of Definiteness
The first phenomenon we consider is markers of definiteness. In English, these are
syntactically identified with determiners, and thus the English Resource Grammar
represents the semantic contrast between the and a with the PRED value of the ep
contributed by the determiner: _the_q_rel vs. _a_q_rel (where ?q? stands for ?quan-
tifier?). Crosslinguistically, however, definiteness is not always marked in lexical de-
terminers which might plausibly contribute quantifier relations. For example, in Nor-
wegian, definiteness is signaled in part by an affix on the noun:
(1) Jeg
I
s?
saw
bilen.
car.DEF
?I saw the car.? [nob]
This does not lend itself to the analysis of definiteness in English provided by the
ERG: First, the definite suffix can co-occur with something else in the determiner
role, as in (2).2 Second, even if the affix did contribute a _def_q_rel, this would lead
to ill-formed MRSs as soon as there were any intersective modifiers: Eps introduced
by intersective modifiers (such as nye in (2)) should be labeled with the same handle
as the ep introduced by the noun. But according to the MRS model of semantic com-
positionality, the label of the noun?s relation is not available for further composition
once the quantifier has attached.
(2) Jeg
I
s?
saw
den
the
nye
new.DEF
bilen
car.DEF
?I saw the new car.? [nob]
Third, adjectives can also take definite forms. We would like to enforce the com-
patibility of this information, rather than having each instance of the definite suffix
contribute an additional ep. Per B&H, this supports treating definiteness in terms of a
feature rather than through eps.
2Note that the determiner is required when there is an adjective in a definite NP, and pragmatically very
restricted when there is not.
Semantic Representations of Syntactically Marked Discourse Status 21
Following B&H, we note that the apparently binary distinction between definites
and indefinites is better assimilated to the cognitive status hierarchy. There are mor-
phosyntactic phenomena in various languages which divide the cognitive status hier-
archy into two separate ranges, though the division point may vary across languages
and within languages across phenomena. Using a single feature for cognitive status
that takes its values from the type hierarchy in Fig 1 allows these various distinctions
to be modeled elegantly.
B&H propose wrapping semantic indices in a new structure ref-prop, which con-
tains COG-ST as well as other features related to the referential properties of a noun
phrase. In this paper, we focus on COG-ST and leave the other dimensions to future
work. However, we differ from B&H in proposing that COG-ST, at least, should be a
feature of semantic indices, rather than inside a parallel structure (i.e., their ref-prop).
This has the benefit of causing the COG-ST information from particular words or af-
fixes to be included in the compositionally created semantic representations of phrases
and sentences without any further effort: wherever the index so marked appears, it will
carry its COG-ST value with it. It also makes the (correct, we believe) prediction that
whenever an index appears in multiple places in the semantic representation, it should
bear the same cognitive status information in all of them. For example, the MRS for
(3) is as in (4), where the variable ?x5? represents the cat, and appears as a value in four
separate elementary predications: ARG0 of _cat_n_rel, ARG0 of _exist_q_rel, ARG1
of _want_v_rel, and ARG1 of _go+out_v_rel. We claim that in all of these guises,
the cognitive status of the referent is the same; there is only one mental representation
of the referent involved.
(3) The cat wanted to go out.
(4) ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
LTOP h0
INDEX e1
RELS
?
?
?
?
?
?
?
?
_exist_q_rel
LBL h3
ARG0 x5[uniq-id]
RSTR h6
BODY h4
?
?
?
?
?
?
?
,
?
?
?
_cat_n_rel
LBL h7
ARG0 x5
?
?
?
,
?
?
?
?
?
?
?
_want_v_rel
LBL h8
ARG0 e2
ARG1 x5
ARG2 h9
?
?
?
?
?
?
?
,
?
?
?
?
?
_go+out_v_rel
LBL h10
ARG0 e11
ARG1 x5
?
?
?
?
?
?
HCONS
?
h6 =q h7, h9 =q h10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
B&H consider this possibility and dismiss it on the grounds that coreferential noun
phrases don?t necessarily share the same cognitive status. However, placing the COG-
ST value on the index does not necessarily entail that the expressions The cat, herself,
and her impute the same cognitive status to their discourse referent in (5). As far as
the syntactic processing is concerned, these expressions introduce distinct indices. It is
up to a separate reference resolution component to identify them, and that component
could merge their COG-ST values or not, as appropriate.
(5) The cat opened the door herself with her paw.
Thus rather than having English the and similar elements introduce a specialized
quantifier relation, we instead do a small amount of semantic decomposition: the
introduces just an existential quantifier (_exist_q_rel), but constrains the variable it
22 Bender and Goss-Grubbs
a. def-noun-lex-rule := inflecting-lexeme-to-word-rule &
%prefix (ha- *)
[ SYNSEM.LOCAL.CONT.HOOK.INDEX.COG-ST uniq-id,
DTR noun-lex ].
b. def-adj-lex-rule := inflecting-lexeme-to-word-rule &
%prefix (ha- *)
[ SYNSEM.LOCAL.CAT.HEAD.MOD.LOCAL.CONT.HOOK.INDEX.COG-ST
uniq-or-more, DTR adj-lex ].
c. indef-noun-lex-rule := constant-lexeme-to-word-rule &
[ SYNSEM.LOCAL.CAT.HEAD.MOD.LOCAL.CONT.HOOK.INDEX.COG-ST
type-id, DTR adj-lex ].
Figure 2: Sample lexical rules for definiteness affixes
binds to be [COG-ST uniq-id]. This signals to the hearer that s/he should be able to
assign a unique representation to the referent (but not that the referent itself is unique
in the world or in the previous discourse, cf. Gundel et al, 2001).
In other languages affixes can also constrain COG-ST to uniq-or-more or uniq-id.
We illustrate here with the Hebrew definite prefix ha-, shown in (6) (from Wintner,
2000:322).
(6) koll
all
?e??
six
ha-smalot
DEF-dresses
ha-yapot
DEF-nice
ha-?elle
DEF-these
?elli
mine
mi-?rhb
from-US
?all these six nice dresses of mine from the US? [heb]
ha- is added by a lexical rule (sketched in Fig 2a) which adds information about
the COG-ST to the noun?s own INDEX value.3 When ha- attaches to an adjective
in Hebrew, it instead adds the information that the noun the adjective is modifying
must have the COG-ST value uniq-or-more, as sketched in Fig 2b. This rule is paired
with a non-inflecting lexical rule Fig 2c which produces adjectives which can only
modify nouns that are [COG-ST type-id], i.e., indefinite. This will enforce definiteness
agreement across the noun phrase.4
This section has briefly outlined an adaptation of B&H?s proposal for definiteness
marking. The main difference to their proposal is in the location of COG-ST in the
feature geometry. In the following two sections, we extend the approach to demon-
stratives and a variety of null anaphora.
4 Demonstratives
Demonstratives can stand alone as noun phrases (demonstrative pronouns) or func-
tion as nominal dependents. Starting again with English, we find that demonstra-
tives in their nominal-dependent guise, like the markers of definiteness, fill the spec-
3The lexical rules in Fig 2 are non-branching productions that apply at the bottom of the parse tree,
before any syntactic rules can apply. The SYNSEM value represents the mother and the DTR value the
daughter. The types they inherit from (e.g., inflecting-lexeme-to-word-rule) enforce identity of most of
the information between mother and daughter. The rules add information about COG-ST, which must be
compatible with what?s provided by the lexical entries for the rules to apply.
4For the rule for unmarked nouns, see ?4 below.
Semantic Representations of Syntactically Marked Discourse Status 23
ifier slot of the noun phrase and function as determiners. Accordingly, the ERG
represents their semantic contribution through the PRED value of the quantifier rela-
tion: _this_q_dem_rel and _that_q_dem_rel. Crosslinguistically, however, demon-
stratives functioning as nominal dependents can also appear as adjectives or affixes
(Dryer, 2008). In such languages, within the general constraints of composition of
MRS, it is not elegant or natural-seeming to have an adjective contribute a quantifier
relation or constrain the PRED value of a relation contributed by a separate determiner
or non-branching NP construction.
Instead, it seems more appropriate to decompose the semantic representation of de-
terminers into a quantifier relation (_exist_q_rel) and a separate one-place modifier
relation (e.g., _distal+dem_a_rel, for ?that?). In languages with demonstrative adjec-
tives, the demonstrative form contributes only the modifier relation. In languages with
demonstrative determiners, the demonstrative forms contribute both.
Demonstratives also constrain the COG-ST value of the nouns the modify, typically
to activ-or-fam. In some languages, (e.g., Irish Gaelic), the demonstratives require ad-
ditional marking of discourse status. Typically this takes the form of a definite article
(see (7) from McCloskey (2004)), but demonstratives can also attach to pronouns and
proper nouns (McCloskey, 2004).
(7) an
the
fear
man
m?r
big
t?agartha
stocky
gro?
cheerful
seo
DEM
?this big stocky cheerful man? [gle]
(8) *fear m?r t?agartha gro? seo
Such languages are straightforwardly countenanced within this system: the definite
article and article-less NPs have incompatible COG-ST values, and only the former is
compatible with the COG-ST constraints contributed by the demonstrative adjective.5
The situation in Hebrew is slightly more complex: Demonstratives can occur with
or without the ha- prefix, so long as they agree with the noun they modify. Conversely,
nouns without the ha- prefix are interpreted as indefinite, unless they are modified by
a demonstrative adjective. It is unclear at this point whether there is a difference
in interpretation between (9) and (10) (from Wintner, 2000:334), but it seems likely
that type-id is not the correct cognitive status for (9); that is, it is most likely not an
indefinite.
(9) sepr
book
ze
this
nimkar
is.sold
heiteb
well
?This book sells well.? [heb]
(10) ha-sepr
DEF-book
ha-ze
DEF-this
nimkar
is.sold
heiteb
well
?This book sells well.? [heb]
5McCloskey points out that the demonstratives can attach to coordinated NPs, each with their own
article. This raises difficulties for treating the demonstratives as adjectives, as it would require the demon-
strative adjectives to attach outside the determiner (cf. Bender et al, 2005). We leave this issue to future
work.
24 Bender and Goss-Grubbs
Here, we postpone the assignment of a COG-ST value to an unmarked noun until the
NP level, filling in type-id in case no demonstrative has attached. This requires an
additional syntactic feature to control the application of the NPs rules, but this seems
motivated: As Wintner notes, ha- is functioning as an agreement marker; its distri-
bution has become grammaticized and drifted somewhat from what purely semantic
constraints would predict.
To provide complete representations for demonstratives, we also need to address the
additional information they carry in many languages, such as the relative proximity
of the referent to the speaker and/or the hearer, its visibility or elevation (Diessel,
1999). These distinctions appear to be at least partially independent of the COG-
ST dimension. In addition, in the absence of any evidence for syntactically-mediated
agreement between elements of a sentence along this dimension, for now we represent
this part of the meaning of demonstratives as an elementary predication rather than as
a feature.
Some languages (e.g., Lithuanian) have a demonstrative element which does not
express any distance contrast, in addition to ones that do (Diessel, 2008). In this
case, it might make sense to reduce the contribution of the former sort of element to
the constraints it places on the noun?s COG-ST value. However, in the interests of
uniformity within the system, we continue to assign it an elementary predication.
Other languages (e.g., French and German) don?t mark any distance contrast on
the primary demonstrative element. In all such languages, there are optional, deictic
adverbials which can be added to mark the contrast (Diessel, 2008).
(11) Das
DEM
Bild
picture
hier
here
gef?llt
like
mir
me
besser
better
als
than
das
DEM
da.
there.
?I like this picture better than that one (over there).? [deu]
In light of such data, we could decompose demonstratives with distance contrasts in
all languages into separate demonstrative and deictic/distance relations. Alternatively,
we could do that decomposition only in languages like German and French. To the
extent that the deictic elements (e.g., German hier and da) have other uses as ordinary
adverbs which can be syntactically assimilated to the same lexical entry, we would
want to at least make sure that the ep they contribute is the same in both cases.
5 Overt pronouns and zero anaphora
Pronouns in the ERG are currently represented by an index which is bound by the
quantifier _pronoun_q_rel andmodified by _pronoun_n_rel. The quantifier ep marks
the pronoun as definite, and the modifier ep serves as the restriction for the quantifier
as well as identifying the index as a pronoun.
Following the treatment of other nominals presented here, however, we do away
with the quantifier ep in favor of the COG-ST feature. Similarly, we replace the mod-
ifier ep with a feature PRON-TYPE, which indicates whether an index is to be inter-
preted as pronominal, and if so, the type of the pronoun (as discussed below). Not
only is this representation simpler, there is no prediction that pronouns participate in
quantifier scope relations, as there is when using _pronoun_q_rel.
Semantic Representations of Syntactically Marked Discourse Status 25
Overt pronouns, clitics and zero pronominals are generally assumed to take a COG-
ST value of in-focus (Gundel et al, 1993; Borthen and Haugereid, 2005). In general,
we agree. We assume that most overt pronouns and many forms of zero anaphora do
take that value. However, there are forms which require us to make exceptions to this.
First let us consider the English indefinite pronoun one, as in (12). Clearly in this
case the referent of one is not in focus. Rather, such a pronoun should bear the COG-ST
value type-id.
(12) Kim bought a computer and Sandy borrowed one.
B&H make a distinction between what they call token pronouns and type pronouns,
where the former are the standard pronouns, which corefer with their antecedents, and
the latter are like English one, which refer to a new token whose type is taken from
its antecedent. We propose that the PRON-TYPE feature take a value of type pron-
type, with subtypes not-pron for non-pronouns and type-or-token for pronouns. The
latter will have two further subtypes, token-pron and type-pron. English one will be
lexically specified as [PRON-TYPE type-pron].
Certain cases of zero anaphora similarly get their type information from their an-
tecedents. A couple of instances of the Italian null subject construction appear in (13)
and (14).
(13) John
John
ha
has
fatto
make.PPRT
la
the
torta.
cake.
La-ha
it-has
mangiata
eat.PPRT
?John baked the cake. (He) ate it.? [ita]
(14) Se
if
uno
a
bambino
child
vuole
wants
un
a
biscotto,
cookie
gli-arriva
to.him-arrives
?If a child wants a cookie, he gets one.? [ita]
In (13), the referent of the null subject is indeed an entity which is in focus, namely
John. On the other hand, in (14) the referent of the null subject is a new token of a
type which is in focus, namely the type ?cookie?.
To handle this situation, we propose that Italian null subjects are associated with
COG-ST in-focus, and with PRON-TYPE type-or-token. The grammar for Italian con-
tains a ?subject drop? construction which discharges the subject requirement of the
verb without realizing any overt dependent. Because the verb will have linked the ap-
propriate argument position of its own ep to the HOOK.INDEX value inside the feature
recording its subject requirement, the subject drop construction can constrain the prop-
erties of this index. In particular, it will specify that its PRON-TYPE is type-or-token
(i.e., it is a pronominal), and that its COG-ST is in-focus. The subject-drop construction
is sketched in Fig 3. When further processing determines the nature of the antecedent,
the PRON-TYPE value will get further specified. If it is a non-specific indefinite, e.g. it
is an indefinite in an intensional context, the pronominal will be specified type-pron,
otherwise it will be specified token-pron.
The next type of zero pronominal we consider are Japanese dropped arguments,
which present a counterexample to Gundel et al (1993)?s claim that all zero pronom-
inals are COG-ST in-focus. To be sure, Japanese zero anaphora can be understood
26 Bender and Goss-Grubbs
head-opt-subj-phrase := head-valence-phrase & head-only &
[ SYNSEM.LOCAL.CAT.VAL.SUBJ < >,
HEAD-DTR.SYNSEM.LOCAL.CAT [ HEAD verb & [ FORM fin ],
VAL.SUBJ < [ LOCAL.CONT.HOOK.INDEX
[ COG-ST in-focus,
PRON-TYPE type-or-token ]] > ]].
Figure 3: Subject drop construction for Italian
similarly to overt token pronouns, as in (15). However, there are also examples where
it can be understood like an overt type pronoun, like English one, as in (16). Note
that (16) is different from (14) in that the antecedent of the null anaphor is not in an
intensional context.
(15) Mi-ta.
see.PAST
?(He/she) saw (it).? [jpn]
(16) Zyon-wa
John.TOP
konpyuutaa-o
computer.ACC
kat-ta.
buy.PAST
Mearii-wa
Mary.TOP
kari-ta.
borrow.PAST
?John bought a computer. Mary borrowed one.? [jpn]
We propose that Japanese dropped arguments are underspecified with respect to
cognitive status and pronoun type. They are associated with indices specified as COG-
ST cog-st and PRON-TYPE type-or-token.
Finally, we turn to lexically licensed null instantiation in English, beginning with
definite null instantiation. Fillmore et al (2003) define definite null instantiation as
a phenomenon whereby some conceptually necessary participant in a situation is left
unexpressed, but its identity is derivable from context. In lexically licensed null instan-
tiation, the possibility of argument drop and the interpretation of the dropped argument
are dependent on the selecting head. In English, lexically licensed DNI is typically a
kind of token pronominal, as in (17). But some items can also license type-pronominal
DNI, as in (18). In (17), the thing that was won is the previously mentioned game.
In (18), there is no particular job that is being sought, although we do know from the
context that it is a job.
(17) Kim played a game with Sandy, and Sandy won.
(18) I can?t find a job, but I?m still looking.
We model lexical licensing of null instantiation through a feature called OPT which
allows selecting heads to record whether or not their arguments are ?optional?. Since
the interpretation of dropped arguments is also constrained by the lexical heads, we
propose two additional features OPT-CS and OPT-PT which encode the cognitive status
and pronoun type to assign to that argument in case it is dropped. The complement-
drop construction and the lexical constraints on look are sketched in Fig 4a-b.
In this figure, strings prefixed with # indicate reentrancy in the feature structure.
The feature KEYREL in lexical entries is a pointer to the main ep they contribute. The
Semantic Representations of Syntactically Marked Discourse Status 27
a. head-opt-comp-phrase := head-valence-phrase & head-only &
[ SYNSEM.LOCAL.CAT.VAL.COMPS #comps, HEAD-DTR.SYNSEM.LOCAL.CAT
[ VAL.COMPS [ FIRST [ OPT +,
OPT-CS #cog-st,
OPT-PT #pron-type,
LOCAL.CONT.HOOK.INDEX [ COG-ST #cog-st,
PRON-TYPE #pron-type ]],
REST #comps ]]].
b. look := pp-transitive-verb-lex &
[ STEM < "look" >,
SYNSEM [ LOCAL.CAT.VAL.COMPS < [ OPT-CS in-focus,
OPT-PT type-or-token ] >,
LKEYS.KEYREL.PRED "_look_v_rel" ]].
c. read := transitive-verb-lex &
[ STEM < "read" >,
SYNSEM [ LOCAL.CAT.VAL.COMPS < [ OPT-CS type-id,
OPT-PT non-pron ] >,
LKEYS.KEYREL.PRED "_read_v_rel" ]].
d. devour := transitive-verb-lex &
[ STEM < "devour" >,
SYNSEM [ LOCAL.CAT.VAL.COMPS < [ OPT - ] >,
LKEYS.KEYREL.PRED "_devour_v_rel" ]].
Figure 4: Lexically licensed complement drop for English
type transitive-verb-lex inherits from its supertypes the linking constraints which iden-
tify the HOOK.INDEX values of the syntactic arguments with the appropriate ARGn
values in the ep contributed by the verb.6
Indefinite null instantiation is similar, except that the identity of the missing element
is either unknown or immaterial. An example of this is (19). INI differs from other
null nominals in that it is not a kind of anaphor. There is nothing in the context that
helps to identify its referent.
(19) Kim is reading.
We propose that indices in INI constructions are specified as COG-ST type-id and
PRON-TYPE non-pron. In English, these constructions are also lexically licensed, and
can be handled with the same features described for DNI. The lexical constraints on
read are illustrated in Fig 4c. For completeness, we also include in Fig 4d an example
of a lexical item which does not license missing complements.
6 Summary and Future Work
In this paper we have explored the construction of semantic representations for a va-
riety of forms of referring expressions. Building on Borthen and Haugereid (2005)?s
proposal to treat cognitive status as a semantic feature within HPSG, we have devel-
oped representations for definite, demonstrative and null NPs, and sketched means of
arriving at them compositionally.
6The constraints shown on the COMPS value of lexical entries would actually be implemented as con-
straints on types that the lexical entries inherit from, allowing the grammar to capture generalizations across
lexical entries. They are shown as constraints on the lexical entries here for ease of exposition only.
28 Bender and Goss-Grubbs
In future work, we plan to expand the range of these analyses to cover phenomena
such as Irish demonstratives taking scope over coordinated noun phrases and cross-
linguistic variation in the marking of generics as definite or indefinite.
On the basis of these analyses, we plan to develop libraries for the Grammar Ma-
trix customization system covering the topics discussed here. The Grammar Matrix
customization system (Bender and Flickinger, 2005; Drellishak and Bender, 2005)
presents the linguist-user with a typological questionnaire which elicits information
about the language to be described. On the basis of the user?s responses to the ques-
tionnaire, the customization system compiles a working starter grammar out of the
Matrix core grammar and analyses stored in libraries. The new libraries will cover ar-
gument optionality (both general pro-drop and lexically-licensed), as well as demon-
stratives of different syntactic types (pronouns, determiners, adjectives and affixes),
the marking of definiteness, and definiteness agreement.
Acknowledgments
We would like to thank Toshiyuki Ogihara, Laurie Poulson, Jeanette Gundel, Jennifer
Arnold, Francesca Gola, and the reviewers for STEP 2008 for helpful comments and
discussion. Any remaining errors are our own. This material is based upon work
supported by the National Science Foundation under Grant No. BCS-0644097.
References
Arnold, J. E. (2008). Reference production: Production-internal and addressee-
oriented processes. Language and Cognitive Processes 23(4), 495?527.
Bender, E. M., M. Egg, and M. Tepper (2005). Semantic construction for nominal
expressions in cross-linguistic perspective. In IWCS-6.
Bender, E. M. and D. Flickinger (2005). Rapid prototyping of scalable grammars:
Towards modularity in extensions to a language-independent core. In Proceed-
ings of the 2nd International Joint Conference on Natural Language Processing
IJCNLP-05 (Posters/Demos), Jeju Island, Korea.
Bender, E. M., D. Flickinger, and S. Oepen (2002). The Grammar Matrix: An
open-source starter-kit for the rapid development of cross-linguistically consistent
broad-coverage precision grammars. In J. Carroll, N. Oostdijk, and R. Sutcliffe
(Eds.), Proceedings of the Workshop on Grammar Engineering and Evaluation at
the COLING19, Taipei, Taiwan, pp. 8?14.
Borthen, K. and P. Haugereid (2005). Representing referential properties of nominals.
Research on Language and Computation 3(2), 221?246.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics, and point
of view. In C. Li (Ed.), Subject and Topic, pp. 25?56. New York: Academic Press.
Chafe, W. (1994). Discourse, Consciousness, and Time. Chicago: Chicago University
Press.
Semantic Representations of Syntactically Marked Discourse Status 29
Copestake, A., D. Flickinger, C. Pollard, and I. A. Sag (2005). Minimal recursion
semantics: An introduction. Research on Language and Computation 3(4), 281?
332.
Copestake, A., A. Lascarides, and D. Flickinger (2001). An algebra for semantic
construction in constraint-based grammars. In Proc. ACL.
Diessel, H. (1999). Demonstratives: Form, Function, and Grammaticalization. Am-
sterdam: John Benjamins.
Diessel, H. (2008). Distance contrasts in demonstratives. In M. Haspelmath,
M. Dryer, D. Gil, and B. Comrie (Eds.), The World Atlas of Linguistic Structures
Online, Chapter 41. Munich: Max Planck Digital Library.
Drellishak, S. and E. M. Bender (2005). A coordination module for a crosslinguistic
grammar resource. In S. M?ller (Ed.), Proc. HPSG, Stanford, pp. 108?128. CSLI
Publications.
Dryer, M. S. (2008). Order of demonstrative and noun. In M. Haspelmath, M. Dryer,
D. Gil, and B. Comrie (Eds.), The World Atlas of Linguistic Structures Online,
Chapter 88. Munich: Max Planck Digital Library.
Fillmore, C., C. Johnson, and M. Petruck (2003). Background to FrameNet. Interna-
tional Journal of Lexicography 16, 235?250.
Flickinger, D. (2000). On building a more efficient grammar by exploiting types.
Natural Language Engineering 6 (1) (Special Issue on Efficient Processing with
HPSG), 15?28.
Gundel, J., N. Hedberg, and R. Zacharski (1993). Cognitive status and the from of
referring expressions in discourse. Language 69, 274?307.
Gundel, J., N. Hedberg, and R. Zacharski (2001). Definite descriptions and cognitive
status in English: Why accommodation is unnecessary. English Language and
Linguistics 5, 273?295.
McCloskey, J. (2004). Irish nominal syntax I: Demonstratives. UC Santa Cruz.
Oepen, S., E. Velldal, J. T. L?nning, P. Meurer, V. Ros?n, and D. Flickinger (2007).
Towards hybrid quality-oriented machine translation. On linguistics and probabili-
ties in MT. In TMI 2007, Sk?vde, Sweden.
Pollard, C. and I. A. Sag (1994). Head-Driven Phrase Structure Grammar. Studies
in Contemporary Linguistics. Chicago: University of Chicago Press.
Prince, E. (1981). Toward a taxonomy of given-new information. In P. Cole (Ed.),
Radical Pragmatics, pp. 223?255. New York: Academic Press.
Wintner, S. (2000). Definiteness in the Hebrew noun phrase. Journal of Linguis-
tics 36, 319?363.
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48?57,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Annotating Social Acts: Authority Claims and Alignment Moves
in Wikipedia Talk Pages
Emily M. Bender?, Jonathan T. Morgan?, Meghan Oxley?, Mark Zachry?,
Brian Hutchinson?, Alex Marin?, Bin Zhang?, Mari Ostendorf?
?Department of Linguistics, ?Department of Human Centered Design and Engineering
?Department of Electrical Engineering
University of Washington
{ebender,jmo25,what,zachry}@uw.edu, {brianhutchinson,iskander,binz,mo}@ee.washington.edu
Abstract
We present the AAWD corpus, a collection
of 365 discussions drawn from Wikipedia talk
pages and annotated with labels capturing two
kinds of social acts: alignment moves and au-
thority claims. We describe these social acts
and our annotation process, and analyze the
resulting data set for interactions between par-
ticipant status and social acts and between the
social acts themselves.
1 Introduction
This paper presents a new annotated resource: the
Authority and Alignment in Wikipedia Discussions
(AAWD) corpus (available from http://ssli.
ee.washington.edu/projects/SCIL.html).
The AAWD corpus contains discussions from
English-language Wikipedia talk pages extracted
from the 2008 Wikipedia data dump and annotated
for two types of social acts: authority claims and
positive/negative alignment moves. In brief, an
authority claim is a statement made by a discussion
participant aimed at bolstering their credibility in
the discussion. An alignment move is a statement
by a participant which explicitly positions them as
agreeing or disagreeing with another participant or
participants regarding a particular topic.
These annotations are intended to make acces-
sible for automated processing two interesting and
characteristic aspects of interaction in online discus-
sion forums. As a dataset for computational and
sociolinguistic analysis, the discussion pages within
Wikipedia are valuable for several reasons. First, the
interaction among the participants is nearly entirely
captured within the dataset, and all of the ?identity-
work? (Bucholtz and Hall, 2010) done by Wikipedia
discussion participants needs to be done directly in
the text of their comments. Furthermore, the discus-
sions tend to be task-driven, focused on the shared
goal of improving the associated article. This leads
the data to be a particularly rich source of linguistic
expressions of authority and alignment.
Our annotations represent a kind of information
which is rather different from that involved in NLP
tasks such as POS tagging, morphological analysis,
parsing and semantic role labeling. Such tasks in-
volve recognizing information that is implicit in the
linguistic signal but nonetheless part of its struc-
ture. Tasks such as named-entity recognition and
word sense disambiguation are also close to the lin-
guistic structure of the signal. Authority claims and
alignment moves, on the other hand, are examples
of communicative moves aimed at social position-
ing of a discussant within a group of participants,
which may be specialized dialog acts but are referred
to here as ?social acts.? We distinguish social acts
from ?social events? as described in (Agarwal and
Rambow, 2010): social events correspond to types
of interactions among people, whereas a social act
is associated with a fine-grained social goal and re-
flected in the specific choices of words and ortho-
graphic or prosodic cues at the level of a turn.
The primary value of this new data set is in facil-
itating computational modeling of a new task type,
i.e. the identification of fine-grained social acts in
linguistic interaction. While there has been some
prior work on detecting agreements and disagree-
48
ments in multiparty discussions (Hillard et al, 2003;
Galley et al, 2004), which is related to detecting
positive/negative alignment moves, most previous
work on authority bids has involved descriptive stud-
ies, e.g. (Galegher et al, 1998). Computational
modeling of these phenomena and automatic detec-
tion will help with understanding effective argumen-
tation strategies in online discussions and automatic
identification of divisive or controversial discussions
and online trolls. We believe that these tasks also
provide an interesting arena in which to study lin-
guistic feature engineering and feature selection. As
with tasks such as sentiment analysis, a simple ?bag-
of-words? model with word or even n-gram-based
features is not sufficiently powerful to detect many
instances of these social acts, where combinations of
positive and negative words must be interpreted in
context, e.g. absolutely is positive alone but ampli-
fies a negative in absolutely not, and yeah in yeah,
I want to correct something John said of course
doesn?t necessarily indicate agreement. The typical
scenario where hand-annotated training data is lim-
ited presents a challenge for learning phrase patterns
that discriminate social acts.
In the remainder of this paper, we further describe
the social acts and annotation schemata (Section 2),
provide details of the AAWD corpus (Section 3),
and analyze the distribution of the social acts (Sec-
tion 4). This analysis describes the distribution of
the social acts and tests hypotheses about their inter-
actions with each other and with user status.
2 Annotation Schemata
2.1 Authority Claims
The ability to persuade others to believe in one?s
statements or the soundness of one?s judgments is
a necessary component of human social interac-
tion. In order to establish the necessary credibil-
ity to secure the belief or assent of others, commu-
nicators will often couch their statements in some
broadly-recognized basis for authority. These ?ar-
guments from authority? have been recognized as
an important component of informal logic by many
language philosophers (Liu, 1997), including John
Locke (1959 [1690]). In recent decades the self-
presentation of authority has been studied in a va-
riety of spoken and written contexts by scholars
from disciplines such as communication, rhetoric,
health studies, sociolinguistics, linguistic pragmat-
ics and political science in order to understand the
strategies that communicators operating in differ-
ent genres and media employ to establish them-
selves as credible discursive participants. Studies
of online product reviews (Mackiewicz, 2010), on-
line political deliberation (Jensen, 2003), scientific
publications (Thompson, 1993), online forum posts
(Galegher et al, 1998; Richardson, 2003) and radio
talk-shows (Thornborrow, 2001) have revealed that
considerations of genre, medium and social context
all shape the ways interactants attempt to claim the
authority to be listened to and taken seriously.
From the perspective of discourse analysis, au-
thority claims provide an interesting lens through
which to view a text, as the overall frequency of
claims can reflect the nature or purpose of the dis-
course (e.g. task-oriented collaboration vs. undi-
rected conversation) and the distribution of claim
types can reveal features of the social context in
which they are made, such as shared norms, prac-
tices and community values. For example, since cer-
tain bases for authority may be seen as more credi-
ble than others in certain contexts (such as citation
of peer-reviewed publications in academic scholar-
ship, or references to personal experience in online
support groups), the prevalence and distribution of
different types of claims in a written text or a con-
versation transcript can illuminate the shared values
of speakers and audiences in a given genre (Galegher
et al, 1998). Although the linguistic construction of
authority claims can vary greatly according to the
genre of the communication, within a single genre
there is often great regularity in the ways claims
are made, such as the common I?m a long-time
listener introduction used by radio talk-show call-
in guests. Even across genres, recognizable types
emerge: references to personal credentials (such as
education or profession) are found to be important
in newsgroup messages (Richardson, 2003), product
reviews (Mackiewicz, 2010) and online scientific ar-
ticle comments (Shanahan, 2010).
Our taxonomy of authority claims was itera-
tively developed based on our empirical analysis
of conversational interaction in two different gen-
res: political talk shows and Wikipedia discus-
sion pages (Oxley et al, 2010), with reference to
49
the literature cited above. Our codebook (avail-
able from http://ssli.ee.washington.edu/
projects/SCIL.html) includes detailed defini-
tions as well as positive and negative examples for
each claim type.
We classify authority claims into the following
types (examples are drawn from our data):
Credentials: Credentials claims involve refer-
ence to education, training, or a history of work in
an area. (Ex: Speaking as a native born Midwest-
erner who is also a professional writer. . . )
Experiential: Experiential claims are based on
an individual?s involvement in or witnessing of an
event. (Ex: If I recall correctly, God is mentioned in
civil ceremonies in Snohomish County, Washington,
the only place I?ve witnessed one.)
Institutional: Institutional claims are based on
an individual?s position within an organization struc-
ture that governs the current discussion forum or has
power to affect the topic or direction of the discus-
sion. (Not attested in our corpus.)
Forum: Forum claims are based on policy,
norms, or contextual rules of behavior in the in-
teraction. (Ex: Do any of these meet wikipedia?s
[[WP:RS | Reliable Sources ]] criteria?)
External: External claims are based on an out-
side authority or source of expertise, such as a book,
magazine article, website, written law, press release,
or court decision. (Ex: The treaty of international
law which states that wars have to begin with a
declaration is the Hague Convention relative to the
Opening of Hostilities from 1907.)
Social Expectations: Social Expectations claims
are based on the intentions or expectations (what
they think, feel or believe) of groups or communities
that exist beyond the current conversational context.
(Ex: I think in the minds of most people, including
the government, the word ?war? and a formal dec-
laration of war have come apart.)
2.2 Alignment Moves
In multiparty discourse, relationships among par-
ticipants manifest themselves in social moves that
participants make to demonstrate alignment with or
against other participants. Expressing alignment
with another participant functions as a means of
enhancing solidarity with that participant while ex-
pressing alignment against another participant main-
tains social distance between conversational partic-
ipants, particularly in situations where participants
may be previously unacquainted with each other
(Svennevig, 1999). Changes in the alignment of par-
ticipants toward one another or ?shifts in footing?
may reflect changes in interpersonal relationships or
may be more transitory, demonstrating minor con-
cessions and critiques embedded within larger, more
stable patterns of participant agreement and dis-
agreement (Goffman, 1981; Wine, 2008).
As Wikipedia editors negotiate about article con-
tent, they make statements that support or oppose
propositions suggested by other editors and thereby
publicly align either with or against other editors in
the discussion. Although ways of expressing agree-
ment and disagreement vary according to power re-
lations between participants, participant goals, and
conversational context (Rees-Miller, 2000), pre-
vious research has suggested that expressions of
agreement and disagreement in written language are
more explicit than oral expressions of agreement and
disagreement (Mulkay, 1985; Mulkay, 1986) and
that statements of agreement are particularly explicit
in online discussions (Baym, 1996).
We classify alignment moves into positive and
negative types, according to whether the participant
is agreeing or disagreeing with the target:
Positive alignment moves express agreement
with the opinions of another participant. Positive
alignment is annotated in cases of explicit agree-
ment, praise/thanking, positive reference to another
participant?s point (e.g. As Joe pointed out. . . ), or
where other clear indicators of positive alignment
are present.
Negative alignment moves express disagreement
with the opinions of another participant. Negative
alignment is annotated in cases of explicit disagree-
ment, doubting, sarcastic praise, criticism/insult,
dismissing, or where other clear indicators of neg-
ative alignment (such as typographical cues) are
present.
Based on our experience using the types of au-
thority claims to diagnose and correct sources of
inter-annotator disagreement (see ?3.3 below), we
developed subtypes of positive and negative align-
ment. While these do not have the same theoretical
grounding as the types of authority claims, they did
serve the same purpose of improving our annotation
50
over time.
We annotate a target for each alignment move,
which may be one or more specific other parties in
the conversation, the group as the whole, or some-
one outside the conversation. In addition, we in-
clude a category labeled ?unclear? for cases where
there is an alignment move, but the annotators are
not able to discern its target. Again, the codebook
includes example subtypes as part of detailed defini-
tions as well as positive and negative examples for
each alignment type.
3 The Corpus
3.1 Source Data
Wikipedia talk pages (also called discussion pages)
are editable pages on which editors can take part in
threaded, asynchronous discussions about the con-
tent of other pages. All editors potentially interested
in a given article can join the conversation on that
article?s talk page. Sometimes these conversations
take the form of a deliberative exchange or even a
heated argument as editors advocate different ideas
about such things as the content or form of an ar-
ticle. Each edit to the talk pages is recorded as a
unique revision in the system and thus becomes part
of the permanent record of system activity.
Wikipedia constitutes a particularly valuable nat-
ural laboratory for studies such as this one, for
several reasons. First, the interaction among the
participants is almost entirely captured within the
Wikipedia database: while some Wikipedians might
interact with each other in person or in other online
fora (such as IRC or mailing lists), this is the excep-
tion rather than the rule. Furthermore, while partici-
pants often maintain persistent identities (usernames
for registered users; IP addresses for unregistered
ones) there are no cues to social identities available
to the participants beyond what is captured in the
digital record. Therefore all of the effort that partic-
ipants put into constructing their online identities is
in the record for analysis. Second, the discussions
on Wikipedia talk pages tend to be goal-oriented, as
the discussion topic is the Wikipedia article that the
participants are collaboratively editing. This goal-
orientation motivates participants to explicitly align
with each other in the course of discussions and but-
tress their arguments with authority claims. Finally,
the Wikipedia dataset contains rich metadata, such
as the date and time of each edit (identified by re-
vision id) to every article or talk page; the editor
responsible for the edit (identified by username or
IP address, depending on registration status); and
markup such as hyperlinks and formatting used in
the textual content of each edit. These metadata al-
low for sophisticated data analysis at the editor level
(e.g. how many edits made by one editor in a given
span of time) and the page level (e.g. how many ed-
itors have participated in a talk page discussion).
The Wikimedia Foundation frequently releases
the database dump of the Wikipedia pages in the
form of XML (available at http://download.
wikimedia.org). The database dumps are cate-
gorized into languages, and for each language, there
are XML files corresponding to different levels of
detail in terms of the information they contain. To
get the information on all revisions, we used the
largest database dump, which contains all Wikipedia
pages and complete edit history. The XML file was
parsed and a database created locally with all the
revision information for both main pages and talk
pages. We then constructed queries to retrieve the
main pages and corresponding talk pages based on
a list of topics for which extensive discussions are
likely to occur.
Our data is drawn from a set of 365 discussions
from 47 talk pages. The discussions were selected
to contain at least 5 turns and at least 4 human par-
ticipants.1 The earliest edit in our data set is from
January 29, 2002 and the latest is from January 6,
2008. A total of 1,509 editors collectively make
6,066 turns in this data. Of the 365 discussions,
185 were annotated for both alignment moves and
authority claims. An additional 26 were annotated
for alignment only and an additional 154 were an-
notated for authority only. The numbers of editors
and turns in these sets are shown in Table 1.
3.2 Annotation Units
A Wikipedia talk page is in itself a wiki-style docu-
ment. Thus, each modification to a talk page by an
editor can modify multiple sections of the page. We
define a ?turn? as a contiguous body of text on the
1Wikipedia discussions may also include contributions by
automated ?bots?.
51
Annotated for
authority alignment both
pages 47 36 36
discussions 339 211 185
editors 1,417 988 896
turns 5,636 3,390 2,960
Table 1: Pages, discussions, editors and turns in anno-
tated data
corresponding page that was modified as part of a
single revision. Thus, a single revision may result in
multiple turns being added. Each turn may include
one or more paragraphs of text, either existing but
modified, or new additions. We annotated authority
claims at the paragraph level and alignment moves at
the turn level. The larger unit is used for alignment
moves because the phenomenon as defined can span
a larger section of text.
The annotation tool (a modified version of LDC?s
XTrans (Glenn et al, 2009)) allowed annotators to
indicate the presence and type of claims or moves in
each annotation unit, in addition to selecting spans
of text corresponding to each social act. For align-
ment moves, within a turn, alignment of the same
type (positive or negative) with the same target was
annotated as a single alignment move, even across
multiple sentences. Where the type or target dif-
fered, we annotated up to three separate alignment
moves per annotation unit. For authority claims, we
also annotated up to three claims per annotation unit,
with each claim identified by a single span of text.
Claims in separate sentences of an annotation unit
counted as separate even if they were of the same
type. Figure 1 gives an example from our codebook
of a turn with multiple alignment moves.
3.3 Annotation Process
Each discussion thread was annotated independently
by two or more annotators. Inter-annotator agree-
ment was calculated at weekly intervals to assess
annotation progress and identify areas of disagree-
ment. Adjudicators also performed ?spot checks? of
annotated data weekly and provided feedback when
there were disagreements among annotators or when
codes seemed to be inconsistently or erroneously ap-
plied. The codebooks for authority claims and align-
ment moves were also iteratively refined with the ad-
dition of positive and negative examples and specific
linguistic cues commonly associated with particular
move or claim types based on spot-check results and
annotator feedback.
Two strategies that proved useful in maintaining
consistency in the frequency and reliability of cod-
ing across annotators were the computation of av-
erage agreement and comparison of overall counts
of each codable unit on a weekly basis. Comput-
ing average agreement allowed adjudicators to iden-
tify particular categories that were proving espe-
cially difficult to code consistently, and to better fo-
cus their efforts on re-training annotators and up-
dating the relevant sections of the annotation guide-
lines. Comparing counts of the number of times two
annotators had coded a particular category over the
same number of discussions also proved useful for
identifying potential problems with under- or over-
coding of a category by a particular annotator.
3.4 Reconciliation
The manual annotation process was completed in-
dependently by each annotator, resulting in multiple
sets of labels. To create a single copy of the data that
can be used in learning experiments, an algorithm
was designed to merge the annotations into a single,
?master? version. The algorithm balances annota-
tion consistency and simplicity of the merging pro-
cess. We treat the annotations for each unit in a file
as a set with respect to type: Multiple labels of the
same type are treated as a single label for purposes
of reconciliation, with only one label of each type
allowed for each annotation unit.
We mark each social act which had been identi-
fied by at least two annotators as having ?high con-
fidence.? If a social act was identified by only one
annotator in that annotation unit, it is marked as hav-
ing ?low confidence.? This procedure yields two sets
of social act types found in each annotation unit, one
consisting of the high confidence labels, and another
of the low confidence labels. The labels from each
set are kept distinct, i.e. for each label in the high
confidence set, the corresponding label in the low
confidence set has the suffix ? single? appended to
the high confidence label.
Aggregated social act labels are propagated to
the sentence level by using a dynamic program-
ming algorithm to match sentences (determined by
automatic segmentation) with the keyword spans
52
speaker turn transcript alignment1 alignment2 alignment3
S1 3 <k1>S2, I think you?re right</k1>. <k2>S3?s idea
is way off base </k2>, but <k1> you seem to have a
good solution</k1>. <k3>But I disagree with your
name for the section</k3> ? Iraq War is used in the
United States media and should be used here as well.
positive:S2:
:explicit
agreement
negative:S3:
:explicit
disagreement
negative:S2:
:explicit
disagreement
Figure 1: Example from alignment codebook
based on overlap. A sentence could have multi-
ple positive labels if one or more annotators la-
beled it for different types in the high or low con-
fidence set. Sentences in turns with a marked
social act but not aligned to text spans are la-
beled as ?unused? due to the ambiguity associated
with a limit on the number of social acts anno-
tated per unit. All sentences in an annotation unit
for which no annotator found any positive labels
are labeled with the negative class. The data dis-
tributed at http://ssli.ee.washington.edu/
projects/SCIL.html include both the underly-
ing per-annotator files as well as the files output by
the reconciliation process.
3.5 Annotation Quality
In complicated annotation tasks, such as those con-
ducted in this work, establishing reliable ground
truth is a fundamental challenge. The most popular
approach to measuring annotation quality is via the
surrogate of annotation consistency. This assumes
that when annotators working independently arrive
at the same decisions they have correctly carried out
the task specified by the annotation guidelines. Sev-
eral quantitative measures of annotator consistency
have been proposed and debated over the years (Art-
stein and Poesio, 2008). We use the well-known
Cohen?s kappa coefficient ?, which accounts for un-
even class priors, so one may obtain a low agreement
score even when a high percentage of tokens have
the same label. We also report the percentage of in-
stances on which the annotators agreed, A, which
includes agreement on the absence of a particular
label. When a set of instances have been labeled by
more than two annotators, we compute the average
of pairwise agreement.
Scores for authority claim and alignment move
agreement are presented in Tables 2 and 3.2 For
2Institutional claims are exceedingly rare in our data, ap-
pearing in only three labels. This is not sufficient for proper ?
Claim Type N ? A
forum 451 0.52 0.92
external 715 0.63 0.91
experiential 185 0.33 0.96
social expectations 78 0.13 0.98
credentials 6 0.57 0.99
Overall 1157 0.59 0.86
Table 2: Agreement summary for authority claims. N
denotes the number of turns of the given type that at least
one annotator marked.
Move Type N ? A
explicit agreement 379 0.62 0.94
praise/thanking 117 0.60 0.98
positive reference 86 0.20 0.98
explicit disagreement 453 0.29 0.92
doubting 198 0.23 0.96
sarcastic praise 38 0.30 0.99
criticism/insult 556 0.32 0.91
dismissing 396 0.16 0.91
All positive 509 0.66 0.94
All negative 1092 0.45 0.85
Overall 1378 0.50 0.80
Table 3: Agreement summary for alignment moves. N
denotes the number of turns of the given type that at least
one annotator marked.
authority, the most common types of claims, forum
and external, are also two of the most reliably identi-
fied. For alignment, the positive type has much bet-
ter agreement scores than the negative type. Inter-
estingly, it appears that the fine distinctions between
the types of negative alignment move are a large fac-
tor in the low agreement scores. When all of the
negative categories are merged, agreement is higher,
although still less than for positive alignment moves.
Our ? values generally fall within the range that
Landis and Koch (1977) deem ?moderate agree-
ment?, but below the .8 cut-off tentatively suggested
computation, and so we do not include them in Table 3.
53
by Artstein and Poesio (2008).3 One possible rea-
son is that the negative class is not as discrete as it
might be in other tasks: both alignment moves and
authority claims can be more or less subtle or ex-
plicit. We have designed our annotation guidelines
to emphasize the more explicit variants of each, but
the same guidelines can sometimes lead annotators
to pick up more subtle examples that other annota-
tors might not feel meet the strict definitions in the
guidelines. Thus we expect our ?high-confidence?
labels to correspond to the more blatant examples
and the ?low-confidence? labels, while sometimes
being genuine noise, to pick out more subtle exam-
ples.
4 Analysis
While the main goal of this paper is to document
the AAWD corpus, we also performed several sta-
tistical analyses of authority and alignment, in or-
der to demonstrate the relevance of these social acts
as markers of user identity and social dynamics
within our corpus. In this section we present the
overall distribution of authority claims and align-
ment moves, compare the prevalence of authority
claims across user types, and show how a partici-
pant?s claim-making behavior may affect how others
subsequently align with them. In doing so, we con-
sider only high-confidence labels from files which
were annotated by at least two annotators. This sub-
set includes 186 discussions annotated for alignment
moves and 200 discussions annotated for authority
claims. Of those, 149 discussions were annotated
for both types of social acts.
4.1 Distribution of Social Acts
We find that 25% of the turns in our alignment data
contain alignment moves and 21% of the turns in our
authority data contain authority claims. In addition,
35% and 32% of the editors in each set make align-
ment moves and authority claims, respectively. The
breakdown by alignment move and authority claim
type is given in Table 4. Note that any given turn
might contain both positive and negative alignment
moves or multiple types of authority claims.
3Artstein and Poesio also note that it may not make sense to
have only one threshold for the field.
N %
Alignment data
total turns 2,890 100
turns w/positive alignment 330 11.4
turns w/negative alignment 467 16.2
turns w/any alignment 710 24.6
total editors 905 100
editors w/alignment moves 315 34.8
Authority data
total turns 3,361 100
turns w/external claim 459 13.7
turns w/forum claim 260 7.7
turns w/experiential claim 77 2.3
turns w/soc. exp. claim 21 0.6
turns w/credentials claim 3 0.1
turns w/institutional claim 0 0
turns w/any claim 703 20.9
total editors 930 100
editors w/authority claims 297 31.9
Table 4: Summary of high-confidence alignment moves
and authority claims
4.2 Authority Claim Types by User Status
Wikipedia distinguishes three different statuses: un-
registered users (able to perform most editing activ-
ities, identified only by IP address), registered users
(able to perform more editing activities, edits at-
tributed to a consistent user name) and administra-
tors (registered users with additional ?sysop? privi-
leges). Participants of different statuses tend to do
different kinds of work on Wikipedia, with admin-
istrators in particular being more likely to take on
moderator work (Burke and Kraut, 2008), such as
mediating and diffusing disputes among editors. Be-
cause conflict mediation requires a different kind of
credibility than collaborative writing work, and be-
cause unregistered users are likely to be newer and
therefore less likely to be incorporating references
to Wikipedia-specific rules and norms into their pro-
jected identities (and, therefore, their conversation),
we hypothesized that editors of different statuses
would use different kinds of authority claims.
Indeed, this is borne out. While no user group
was significantly more or less likely than any other
to include authority claims overall in their posts
(chi square test for independence, n=3164, df=2,
?2=2.367, p=.306) users of different statuses did use
significantly different proportions of each type of
claim (chi square test for independence, n=973, df=8
54
Participant # % % % claim-
type users forum external bearing
turns
admin 44 47.1 45.1 19.6
reg 192 29.1 63.6 22.3
unreg 55 18.3 70.6 19.8
all 291 29.8 62.5 21.6
Table 5: Percentage of authority claims of forum and
external types, and percentage of total turns which con-
tained claims, across user statuses
?2=38.301, p<.001). As illustrated in Table 5, ad-
ministrators are more likely than the other groups to
make forum claims and less likely to make exter-
nal claims, unregistered users make more external
claims and fewer forum claims, and registered users
exhibit a claim distribution that more closely reflects
the overall distribution of claim types.
4.3 Authority Claim Prevalence by V-Index
Given the few visible markers of status on Wikipedia
and the fact that editors are constantly interact-
ing with new collaborators, Wikipedians perform
authority by adopting insider language and norms
of interaction. Supporting arguments with specific
references is one such norm. Thus we hypothe-
sized that as editors become more integrated into
Wikipedia, they will make more authority claims.
In order to test this hypothesis, we developed ?v-
index? as a proxy measure of degree of integration or
?veteran status? within the community. Inspired by
Ball?s (2005) ?h-index? of scholarly productivity, v-
index balances frequency of interaction with length
of interaction. Specifically, an editor?s v-index at the
time of a particular revision is the greatest v such
that the editor has made at least v edits within the
past v months (28-day periods).
We measured the v-index for each revision in
our dataset, using all edits to Wikipedia in order
to calculate v (not just edits to the discussions we
have annotated). The v-index values for edits within
our dataset range from 1 to 46.4 We measured the
proportion of turns with authority claims (of any
type) for each v-index. The proportion of turns
with authority claims is in fact positively correlated
4The data becomes very sparse for v-indices above 29, with
every v-index in this range represented by < 10 turns, so the
v-indices of 30-46 were not included in this analysis.
Initial turn Alignment in next 10 turns
no auth. claim 0.52
any auth. claim 0.63
Table 6: Average prevalence of alignment moves targeted
at participant in 10 following turns
with v-index, confirming our hypothesis (one-sided
Pearson?s correlation coefficient, n=29 v-indices,
r=0.371, p=0.024).
4.4 Interaction of Social Phenomena
Thus far, we have been addressing our social acts
independently, but of course no social act occurs in
a vacuum. Alignment moves and authority claims
are only two types of social acts; many other types
of social acts are present (and could be annotated) in
this same data set. Even with only these two types
(and their subtypes), however, we find interactions.
We hypothesized that authority claims would be
likely to provoke alignment moves. That is, al-
though participants may make alignment moves
whenever someone else has expressed an opinion or
taken action (e.g. edited the article attached to the
discussion), we hypothesized that by making an au-
thority claim, a participant becomes more likely to
become a focal point in the debate. To test this,
we calculated, for every turn, the number of align-
ment moves targeted at the author of that turn within
the next 10 turns. We then divided the turns into
those that contained authority claims and those that
did not. Making an authority claim in a given turn
made the participant significantly more likely to be
the target of an alignment move within the subse-
quent 10 turns compared to turns that did not contain
any claims (t=-2.086, df=772, p=.037; Table 6))
Furthermore, we find that different types of au-
thority claims elicit different numbers of subsequent
alignment moves. Specifically, turns that contain ei-
ther external claims or forum claims (the two most
prevalent claim types in our sample) interact dif-
ferently with alignment. External claims elicited
more alignment overall (t=3.189, df=411, p=.002)
and more negative alignment moves than did forum
claims (t=3.839, df=415, p<.001). However, ex-
ternal claims did not elicit significantly more pos-
itive alignment moves than forum claims (t=0.695,
df=309, p=.488). This is illustrated in Table 7.
55
Initial turn Alignment in next 10 turns
positive negative overall
external claim 0.26 0.49 0.74
forum claim 0.22 0.20 0.42
Table 7: Average prevalence of alignment moves targeted
at participant in 10 following turns
5 Conclusion
We have presented the Authority and Alignment
in Wikipedia Discussions (AAWD) corpus, a col-
lection of 365 discussions drawn from Wikipedia
talk pages and annotated for two broad types of so-
cial acts: authority claims and alignment moves.
These annotations make explicit important discur-
sive strategies that discussion participants use to
construct their identities in this online forum. That
?identity work? is being done with these social acts
is confirmed by the correlations we find between
proportions of turns with authority claims and ex-
ternal variables such as user status and v-index, on
the one hand, and the interaction between authority
claims and alignment moves on the other.
As an example of a social medium, Wikipedia is
characterized by its task-orientation and by the fact
that all of the interactants? ?identity work? with re-
spect to their identity in the medium is captured in
the database. This, in turn, causes the data set to be
rich in the type of social acts we are investigating.
The dataset was used for research in automatic de-
tection of forum claims, as presented in a compan-
ion paper (Marin et al, 2011). That work focused
on using lexical features, filtered through word lists
obtained from domain experts and through data-
driven methods, and extended with parse tree infor-
mation. Automatic detection of other types of au-
thority claims and of alignment moves is left for fu-
ture research.
We believe that, as social acts, authority claims
and alignment moves are broadly recognized com-
munication behaviors that play an important role in
human interaction across a variety of contexts. How-
ever, because Wikipedia discussions are shaped by
a set of well-defined, local communication norms
which are closely tied to the task of distributed,
collaborative writing, we expect authority claims
and alignment moves will manifest differently in
other genres. Future work could explore the range
of variation among the linguistic cues associated
with authority and alignment categories across gen-
res, cultures and communication media, as well
as the possible role of additional categories or so-
cial acts not discussed here. We believe that the
communicative ecology of Wikipedia discussions,
combined with the rich metadata of the Wikipedia
database, presents a highly valuable natural labora-
tory in which to explore social scientific analyses of
communication behaviors as well as a resource for
the development of NLP systems which can auto-
matically identify these social acts, in Wikipedia and
beyond.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA). All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
The original Wikipedia discussion page data for
this study was made available from a research
project supported by NSF award IIS-0811210. We
thank Travis Kriplean for his initial assistance with
scripts to process this data dump.
We also gratefully acknowledge the contribution
of the annotators: Wendy Kempsell, Kelley Kilan-
ski, Robert Sykes and Lisa Tittle.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic
detection and classification of social events. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1024?
1034, Cambridge, MA. Association for Computational
Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Philip Ball. 2005. Index aims for fair ranking of scien-
tists. Nature, 436:900?900.
Nancy Baym. 1996. Agreements and disagreements in
a computer-mediated discussion. Research on Lan-
guage and Social Interaction, 29:315?345.
Mary Bucholtz and Kira Hall. 2010. Locating identity
in language. In C. Llamas and D. Watt, editors, Lan-
56
guage and Identities. Edinburgh University Press, Ed-
inburgh.
Moira Burke and Robert Kraut. 2008. Mopping up:
Modeling wikipedia promotion decisions. In Proceed-
ings of the 2008 ACM Conference on Computer Sup-
ported Cooperative Work, pages 27?36. Association of
Computing Machinery.
Jolene Galegher, Lee Sproull, and Sara Kiesler. 1998.
Legitimacy, authority, and community in electronic
support groups. Written Communication, 15(4):493?
530.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech: Use
of Bayesian networks to model pragmatic dependen-
cies. In Proceedings of the 42nd Meeting of the Asso-
ciation for Computational Linguistics (ACL?04), Main
Volume, pages 669?676, Barcelona, Spain.
Meghan Lammie Glenn, Stephanie M. Strassel, and Hae-
joong Lee. 2009. XTrans: A speech annotation and
transcription tool. In INTERSPEECH-2009, pages
2855?2858.
Erving Goffman. 1981. Forms of Talk. University of
Pennsylvania Press, Philadelphia.
Dustin Hillard, Mari Ostendorf, and Elizabeth Shriberg.
2003. Detection of agreement vs. disagreement in
meetings: training with unlabeled data. In Proceed-
ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL),
pages 34?36.
Jakob L. Jensen. 2003. Public spheres on the inter-
net: Anarchic or government sponsored; a compari-
son. Scandinavian Political Studies, 26:349?374.
J. Richard Landis and Gary G. Koch. 1977. Measure-
ment of observer agreement for categorical data. Bio-
metrics, 33:159?174.
Yameng Liu. 1997. Authority, presumption and inven-
tion. Philosophy and Rhetoric, 30(4):413?427.
John Locke. 1959 [1690]. An Essay Concerning Human
Understanding. Dover Publications, New York.
Jo Mackiewicz. 2010. Assertions of expertise in online
product reviews. Journal of Business and Technical
Communication, 24(1):3?28.
Alex Marin, Bin Zhang, and Mari Ostendorf. 2011. De-
tecting forum authority claims in online discussions.
Proceedings of the Workshop on Language in Social
Media (LSM 2011).
Michael Mulkay. 1985. Agreement and disagreement in
conversations and letters. Text, 5(3):201?227.
Michael Mulkay. 1986. Conversations and texts. Human
Studies, 9(2-3):303?321.
Meghan Oxley, Jonathan T. Morgan, Mark Zachry, and
Brian Hutchinson. 2010. ?What I know is...?: Estab-
lishing credibility on Wikipedia talk pages. In Pro-
ceedings of the 6th International Symposium on Wikis
and Open Collaboration, Gdansk, Poland. Association
for Computing Machinery.
Janie Rees-Miller. 2000. Power, severity, and context in
disagreeement. Journal of Pragmatics, 32(8):1087?
1111.
Kay Richardson. 2003. Health risks on the internet: Es-
tablishing credibility on line. Health, Risk and Society,
5(2):171?184.
Marie-Claire Shanahan. 2010. Changing the meaning
of peer-to-peer? Exploring online comment spaces as
sites of negotiated expertise. Journal of Science Com-
munication, 9(1):1?13.
Jan Svennevig. 1999. Getting Acquainted in Conversa-
tion: A Study of Initial Interactions. John Benjamins
Publishing Company, Amsterdam.
Dorothea K. Thompson. 1993. Arguing for experimental
?facts? in science. Written Communication, 10:106.
Joanna Thornborrow. 2001. Authenticating talk: Build-
ing public identities in audience participation broad-
casting. Discourse Studies, 3(4):459?479.
Linda Wine. 2008. Towards a deeper understanding of
framing, footing, and alignment. Teachers College,
Columbia University Working Papers in TESOL and
Applied Linguistics, 8(3):1?3.
57
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 74?83,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards Creating Precision Grammars from Interlinear Glossed Text:
Inferring Large-Scale Typological Properties
Emily M. Bender Michael Wayne Goodman Joshua Crowgey Fei Xia
Department of Linguistics
University of Washington
Seattle WA 98195-4340
{ebender,goodmami,jcrowgey,fxia}@uw.edu
Abstract
We propose to bring together two kinds of
linguistic resources?interlinear glossed
text (IGT) and a language-independent
precision grammar resource?to automat-
ically create precision grammars in the
context of language documentation. This
paper takes the first steps in that direction
by extracting major-constituent word or-
der and case system properties from IGT
for a diverse sample of languages.
1 Introduction
Hale et al (1992) predicted that more than 90%
of the world?s approximately 7,000 languages will
become extinct by the year 2100. This is a crisis
not only for the field of linguistics?on track to
lose the majority of its primary data?but also a
crisis for the social sciences more broadly as lan-
guages are a key piece of cultural heritage. The
field of linguistics has responded with increased
efforts to document endangered languages. Lan-
guage documentation not only captures key lin-
guistic data (both primary data and analytical
facts) but also supports language revitalization ef-
forts. It must include both primary data collec-
tion (as in Abney and Bird?s (2010) universal cor-
pus) and analytical work elucidating the linguistic
structures of each language. As such, the outputs
of documentary linguistics are dictionaries, de-
scriptive (prose) grammars as well as transcribed
and translated texts (Woodbury, 2003).
Traditionally, these outputs were printed ar-
tifacts, but the field of documentary linguistics
has increasingly realized the benefits of producing
digital artifacts as well (Nordhoff and Poggeman,
2012). Bender et al (2012a) argue that the docu-
mentary value of electronic descriptive grammars
can be significantly enhanced by pairing them with
implemented (machine-readable) precision gram-
mars and grammar-derived treebanks. However,
the creation of such precision grammars is time
consuming, and the cost of developing them must
be brought down if they are to be effectively inte-
grated into language documentation projects.
In this work, we are interested in leveraging
existing linguistic resources of two distinct types
in order to facilitate the development of precision
grammars for language documentation. The first
type of linguistic resource is collections of inter-
linear glossed text (IGT), a typical format for dis-
playing linguistic examples. A sample of IGT
from Shona is shown in (1).
(1) Ndakanga
ndi-aka-nga
SBJ.1SG-RP-AUX
ndakatenga
ndi-aka-teng-a
SBJ.1SG-RP-buy-FV
muchero
mu-chero
CL3-fruit
?I had bought fruit.? [sna] (Toews, 2009:34)
The annotations in IGT result from deep linguistic
analysis and represent much effort on the part of
field linguists. These rich annotations include the
segmentation of the source line into morphemes,
the glossing of those individual morphemes, and
the translation into a language of broader commu-
nication. The IGT format was developed to com-
pactly display this information to other linguists.
Here, we propose to repurpose such data in the au-
tomatic development of further resources.
The second resource we will be working with
is the LinGO Grammar Matrix (Bender et al,
2002; 2010), an open source repository of imple-
mented linguistic analyses. The Grammar Matrix
pairs a core grammar, shared across all grammars
it creates, with a series of libraries of analyses
of cross-linguistically variable phenomena. Users
access the system through a web-based question-
naire which elicits linguistic descriptions of lan-
guages and then outputs working HPSG (Pol-
lard and Sag, 1994) grammar fragments compat-
ible with DELPH-IN (www.delph-in.net) tools
based on those descriptions. For present purposes,
this system can be viewed as a function which
maps simple descriptions of languages to preci-
74
sion grammar fragments. These fragments are rel-
atively modest, yet they relate linguistic strings to
semantic representations (and vice versa) and are
ready to be built out to broad coverage.
Thus we ask whether the information encoded
by documentary linguists in IGT can be lever-
aged to answer the Grammar Matrix?s question-
naire and create a precision grammar fragment
automatically. The information required by the
Grammar Matrix questionnaire concerns five dif-
ferent aspects of linguistic systems: (i) constituent
ordering (including the presence/absence of con-
stituent types), (ii) morphosyntactic systems, (iii)
morphosyntactic features, (iv) lexical types and
their instances and (v) morphological rules. In this
initial work, we target examples of types (i) and
(ii): the major constituent word order and the gen-
eral type of case system in a language. The Gram-
mar Matrix and other related work are described
in further in ?2. In ?3 we present our test data and
experimental set-up. ??4?5 describe our method-
ology and results for the two tasks, respectively,
with further discussion and outlook in ??6?7.
2 Background and Related Work
2.1 The Grammar Matrix
The Grammar Matrix produces precision gram-
mars on the basis of description of languages
that include both high-level typological informa-
tion and more specific detail. Among the for-
mer are aspects (i)?(iii) listed in ?1. The third
of these (morphosyntactic features) concerns the
type and range of grammaticized information that
a language marks in its morphology and/or syn-
tax. This includes person/number systems (e.g.,
is there an inclusive/exclusive distinction in non-
singular first person forms?), the range of aspec-
tual distinctions a language marks, and the range
of cases (if any) in a language, inter alia. The an-
swers to these questions in turn cause the system
to provide relevant features that the user can ref-
erence in providing the more specific information
elicited by the questionnaire ((iv) and (v) above),
viz., the definition of both lexical types (e.g., first
person dual exclusive pronouns) and morphologi-
cal rules (e.g., nominative case marking on nouns).
The information input by the user to the Gram-
mar Matrix questionnaire is stored in a file called
a ?choices file?. The choices file is used both in
the dynamic definition of the html pages (so that
the features available for lexical definitions de-
pend on earlier choices) and as the input to the cus-
tomization script that actually produces the gram-
mar fragments to spec. The customization sys-
tem distinguishes between choices files which are
complete and consistent (and can be used to cre-
ate working grammar fragments) and those which
do not yet have answers to required questions or
give answers which are inconsistent according to
the underlying grammatical theory. The ultimate
goal of the present project is to be able to automat-
ically create complete and consistent choices files
on the basis of IGT, and in fact to create complete
and consistent choices files which take maximal
advantage of the analyses stored in the Grammar
Matrix customization system, answering not only
the minimal set of questions required but in fact all
which are relevant and possible to answer based on
the information in the IGT.
Creating such complete and consistent choices
files is a long-term project, with different ap-
proches required for the different types of ques-
tions outlined in ?1. Bender et al (2012b) take
some initial steps towards answering the questions
which define lexical rules. We envision answering
the questions regarding morphosyntactic features
through an analysis of the grams that appear on the
gloss line, with reference to the GOLD ontology
(Farrar and Langendoen, 2003). The implementa-
tion of such systems in such a way that they are
robust to potentially noisy data will undoubtedly
be non-trivial. The contribution of this paper is
the development of systems to handle one example
each of the questions of types (i) and (ii), namely
detecting major constituent word order and the un-
derlying case system. For the first, we build di-
rectly on the work of Lewis and Xia (2008) (see
?2.2). Our experiment can be viewed as an at-
tempt to reproduce their results in the context of
the specific view of word order possibilities devel-
oped in the Grammar Matrix. The second question
(that of case systems) is in some ways more sub-
tle, requiring not only analysis of IGT instances in
isolation and aggregation of the results, but also
identification of particular kinds of IGT instances
and comparison across them.
2.2 RiPLes
The RiPLes project has two intertwined goals.
The first goal is to create a framework that allows
the rapid development of resources for resource-
poor languages (RPLs), which is accomplished by
75
Figure 1: Welsh IGT with alignment and projected
syntactic structure
bootstrapping NLP tools with initial seeds created
by projecting syntactic information from resource-
rich languages to RPLs through IGT. Projecting
syntactic structures has two steps. First, the words
in the language line and the translation line are
aligned via the gloss line. Second, the transla-
tion line is parsed by a parser for the resource-rich
language and the parse tree is then projected to
the language line using word alignment and some
heuristics as illustrated in Figure 1 (adapted from
Xia and Lewis (2009)).1 Previous work has ap-
plied these projected trees to enhance the perfor-
mance of statistical parsers (Georgi et al, 2012).
Though the projected trees are noisy, they contain
enough information for those tasks.
The second goal of RiPLes is to use the au-
tomatically created resources to perform cross-
lingual study on a large number of languages
to discover linguistic knowledge. For instance,
Lewis and Xia (2008) showed that IGT data en-
riched with the projected syntactic structure could
be used to determine the word order property of a
language with a high accuracy (see ?4). Naseem
et al (2012) use this type of information (in their
case, drawn from the WALS database (Haspel-
math et al, 2008)) to improve multilingual depen-
dency parsing. Here, we build on this aspect of
RiPLes and begin to extend it towards the wider
range of linguistic phenomena and more detailed
classification within phenomena required by the
Grammar Matrix questionnaire.
2.3 Other Related Work
Our work is also situated with respect to attempts
to automatically characterize typological proper-
1The details of the algorithm and experimental results
were reported in (Xia and Lewis, 2007).
ties of languages, including Daume? III and Camp-
bell?s (2007) Bayesian approach to discovering ty-
pological implications and Georgi et al?s (2010)
work on predicting (unknown) typological proper-
ties by clustering languages based on known prop-
erties. Both projects use the typological database
WALS (Haspelmath et al, 2008), which has in-
formation about 192 different typological proper-
ties and about 2,678 different languages (though
the matrix is very sparse). This approach is com-
plementary to ours, and it remains an interesting
question whether our results could be improved
by bringing in information about other typological
properties of the language (either extracted from
the IGT or looked up in a typological database).
Another strand of related work concerns the col-
lection and curation of IGT, including the ODIN
project (Lewis, 2006; Xia and Lewis, 2008),
which harvests IGT from linguistics publications
available over the web and TypeCraft (Beermann
and Mihaylov, 2009), which facilitates the collab-
orative development of IGT annotations. TerraL-
ing/SSWL2 (Syntactic Structures of the World?s
Languages) has begun a database which combines
both typological properties and IGT illustrating
those properties, contributed by linguists.
Finally, Beerman and Hellan (2011) represents
another approach to inducing grammars from IGT,
by bringing the hand-built linguistic knowledge
sources closer together: On the one hand, their
cross-linguistic grammar resource (TypeGram) in-
cludes a mechanism for mapping from strings
specifying verb valence and valence-altering lex-
ical rules to sets of grammar constraints. On
the other hand, their IGT authoring environment
(TypeCraft) provides support for annotating exam-
ples with those strings. The approach advocated
here attempts to bridge the gap between IGT and
grammar specification algorithmically, instead.
3 Development and Test Data
Our long-term goal is to produce working gram-
mar fragments from IGT produced in documen-
tary linguistics projects. However, in order to
evaluate the performance of approaches to answer-
ing the high-level questions in the Grammar Ma-
trix questionnaire, we need both IGT and gold-
standard answers for a reasonably-sized sample of
languages. We have constructed development and
test data for this purpose on the basis of work done
2
http://sswl.railsplayground.net/, accessed 4/25/13
76
Sets of languages DEV1 (n=10) DEV2 (n=10) TEST (n=11)
Range of testsuite sizes 16?359 11?229 48?216
Median testsuite size 91 87 76
Language families Indo-European (4), Niger- Indo-European (3), Indo-European (2), Afro-Asiatic,
Congo (2), Afro-Asiatic, Dravidian (2), Algic, Austro-Asiatic, Austronesian,
Japanese, Nadahup, Creole, Niger-Congo, Arauan, Carib, Karvelian,
Sino-Tibetan Quechuan, Salishan N. Caucasian, Tai-Kadai, Isolate
Table 1: Language families and testsuites sizes (in number of grammatical examples)
by students in a class that uses the Grammar Ma-
trix (Bender, 2007). In this class, students work
with descriptive resources for languages they are
typically not familiar with to create testsuites (cu-
rated collections of grammatical and ungrammat-
ical examples) and Grammar Matrix choices files.
Later on in the class, the students extend the gram-
mar fragments output by the customization system
to handle a broader fragment of the language. Ac-
cordingly, the testsuites cover phenomena which
go beyond the customization system.
Testsuites for grammars, especially in their
early stages of development, require examples that
are simple (isolating the phenomena illustrated by
the examples to the extent possible), built out of
a small vocabulary, and include both grammati-
cal and ungrammatical examples (Lehmann et al,
1996). The examples included in descriptive re-
sources often don?t fit these requirements exactly.
As a result, the data we are working with include
examples invented by the students on the basis of
the descriptive statements in their resources.3
In total, we have testsuites and associated
choices files for 31 languages, spanning 17 lan-
guage families (plus one creole and one language
isolate). The most well-represented family is
Indo-European, with nine languages. We used 20
languages, in two dev sets, for algorithm develop-
ment (including manual error analysis), and saved
11 languages as a held-out test set to verify the
generalizability of our approach. Table 1 lists the
language families and the range of testsuite sizes
for each of these sets of languages.
4 Inferring Word Order
Lewis and Xia (2008) show how IGT from ODIN
(Lewis, 2006) can be used to determine, with high
accuracy, the word order properties of a language.
They identify 14 typological parameters related to
word order for which WALS (Haspelmath et al,
2008) or other typological resources provide in-
3Such examples are flagged in the testsuites? meta-data.
formation. The parameter most closely relevant to
the present work is Order of Words in a Sentence
(Dryer, 2011). For this parameter, Lewis and Xia
tested their method in 97 languages and found that
their system had 99% accuracy provided the IGT
collections had at least 40 instances per language.
The Grammar Matrix?s word order questions
differ somewhat from the typological classifi-
cation that Lewis and Xia (2008) were using.
Answering the Grammar Matrix questionnaire
amounts to more than making a descriptive state-
ment about a language. The Grammar Matrix cus-
tomization system translates collections of such
descriptive statements into working grammar frag-
ments. In the case of word order, this most di-
rectly effects the number and nature of phrase
structure rules included in the output grammar, but
can also interact with other aspects of the gram-
mar (e.g., the treatment of argument optionality).
More broadly, specifying the word order system
of a grammar determines both grammaticality (ac-
cepting some strings, ruling out others) and, for
the fixed word orders at least, aspects of the map-
ping of syntactic to semantic arguments.
Lewis and Xia (2008), like Dryer (2011), gave
the six fixed orders of S, O and V plus ?no dom-
inant order?. In contrast, the Grammar Matrix
distinguishes Free (pragmatically constrained), V-
final, V-initial, and V2 orders, in addition to the
six fixed orders. It is important to note that the
relationship between the word order type of a lan-
guage and the actual orders attested in sentences
can be somewhat indirect. For a fixed word order
language, we would expect the order declared as
its type to be the most common in running text,
but not the only type available. English, for exam-
ple, is an SVO language, but several constructions
allow for other orders, including subject-auxiliary
inversion, so-called topicalization, and others:
(2) Did Kim leave?
(3) The book, Kim forgot.
In a language with more word order flexibility in
general, there may still be a preferred word order
77
which is the most common due to pragmatic or
other constraints. Users of the Grammar Matrix
are advised to choose one of the fixed word orders
if the deviations from that order can generally be
accounted for by specific syntactic constructions,
and a freer word order otherwise.
The relationship between the correct word or-
der choice for the Grammar Matrix customization
system and the distribution of actual token word
orders in our development and test data is affected
by another factor, related to Lewis and Xia?s ?IGT
bias? which we dub ?testsuite bias?. The collec-
tions of IGT we are using were constructed as test-
suites for grammar engineering projects and thus
comprise examples selected or constructed to il-
lustrate specific grammatical properties in a test-
ing regime where one example is enough to repre-
sent each sentence type of interest. Therefore, they
do not represent a natural distribution of word or-
der types. For example, the testsuite authors may
show the full range of possible word orders in the
word order section of the testsuite and then default
to one particular choice for other portions (those
illustrating e.g., case systems or negation).
4.1 Methodology
Our first stpes mirror the RiPLes approach, pars-
ing parse the English translation of each sentence
and projecting the parsed structure onto the source
language line. Functional tags, such as SBJ and
OBJ, are added to the NP nodes on the English
side based on our knowledge of English word or-
der and then carried over to the source language
side during the projection of parse trees. The trees
are then searched for any of ten patterns: SOV,
SVO, OSV, OVS, VSO, VOS, SV, VS, OV, and
VO. The six ternary patterns match when both ver-
bal arguments are present in the same clause. The
four binary patterns are for intransitive sentences
or those with dropped arguments. These ten pat-
terns make up the observed word orders.
Given our relatively limited data set (each lan-
guage is one data point), we present an initial
approach to determining underlying word order
based on heuristics informed by general linguis-
tic knowledge. We compare the distribution of ob-
served word orders to distributions we expect to
see for canonical examples of underlying word or-
ders. We accomplish this by first deconstructing
the ternary observed-word-orders into binary pat-
terns (the four above plus SO and OS). This gives
us three axes: one for the tendency to exhibit VS
or SV order, another for VO or OV order, and an-
other for OS or SO order. By counting the ob-
served word orders in the IGT examples, we can
place the language in this three-dimensional space.
Figure 4.1 depicts this space with the positions of
canonical word orders.4 The canonical word order
positions are those found under homogeneous ob-
servations. For example, the canonical position for
SOV order is when 100% of the sentences exhibit
SO, OV, and SV orders; and the canonical position
for Free word order is when each observed order
occurs with equal frequency to its opposite order
(on the same axis; e.g. VO and OV). We select the
underlying word order by finding which canoni-
cal word order position has the shortest Euclidean
distance to the observed word order position.
When a language is selected as Free word or-
der, we employ a secondary heuristic to decide if
it is actually V2 word order. The V2 order cannot
be easily recognized only with the binary word or-
ders, so it is not given a unique point in the three-
dimensional space. Rather, we try to recognize it
by comparing the ternary orders. A Free-order lan-
guage is reclassified as V2 if SVO and OVS occur
more frequently than SOV and OSV.5
OVS
SOV
V-finalVS
OV
OSV
SVV-initial
VOS
OS
SO
SVO
VSO
VO
Free/V2
Figure 2: Three axes of basic word order and the
positions of canonical word orders.
4.2 Results
Table 2 shows the results we obtained for our dev
and test sets. For comparison, we use a most-
4Of the eight vertices of this cube, six represent canoni-
cal word orders the other two impossible combinations: The
vertex for (SV, VO, OS) (e.g.) has S both before and after O.
5The VOS and VSO patterns are excluded from this com-
parison, since they can go either way?there may be un-
aligned constituents (i.e. not a S, O, or V) before the verb
which are ignored by our system.
78
frequent-type baseline, selecting SOV for all lan-
guages, based on Dryer?s (2011) survey. We get
high accuracy for DEV1, low accuracy for DEV2,
and moderate accuracy for TEST, but all are sig-
nificantly higher than the baseline.
Dataset Inferred WO Baseline
DEV1 0.900 0.200
DEV2 0.500 0.100
TEST 0.727 0.091
Table 2: Accuracy of word-order inference
Hand analysis of the errors in the dev sets
show that some languages fall victim to the test-
suite bias, such as Russian, Quechua, and Tamil.
All of these languages have Free word order, but
our system infers SVO for Russian and SOV for
Quechua and Tamil, because the authors of the
testsuites used one order significantly more than
the others. Similarly, the Free word order lan-
guage Nishnaabemwin is inferred as V2 because
there are more SVO and OVS patterns given than
others. We also see errors due to misalignment
from RiPLes? syntactic projection. The VSO lan-
guage Welsh is inferred as SVO because the near-
ubiquitous sentence-initial auxiliary doesn?t align
to the main verb of the English translation.
5 Inferring Case Systems
Case refers to linguistic phenomena in which the
form of a noun phrase (NP) varies depending on
the function of the NP in a sentence (Blake, 2001).
The Grammar Matrix?s case library (Drellishak,
2009) focuses on case marking of core arguments
of verbs. Specifying a grammar for case involves
both choosing the high-level case system to be
modeled as well as associating verb types with
case frames and defining the lexical items or lex-
ical rules which mark the case on the NPs. Here,
we focus on the high-level case system question
as it is logically prior, and in some ways more in-
teresting than the lexical details: Answering this
question requires identifying case frames of verbs
in particular examples and then comparing across
those examples, as described below.
The high-level case system of a language con-
cerns the alignment of case marking between tran-
sitive and intransitive clauses. The three ele-
ments in question are the subjects of intransi-
tives (dubbed S), the subjects (or agent-like ar-
guments) of transitives (dubbed A) and the ob-
jects (or patient-like arguments) of intransitives
Case Case grams present
system NOM ? ACC ERG ? ABS
none
nom-acc X
erg-abs X
split-erg X X
(conditioned on V)
Table 3: GRAM case system assignment rules
(O). Among languages which make use of case,
the most common alignment type is a nominative-
accusative system (Comrie 2011a,b). In this
type, S takes the same kind of marking as A.6
The Grammar Matrix case library provides nine
options, including none, nominative-accusative,
ergative-absolutive (S marked like O), tripartite (S,
A and O all distinct) and several more intricate
types. For example, in a language with one type
of split case system the alignment is nominative-
accusative in non-past tense clauses, but ergative-
absolutive in past tense ones.
As with major constituent word order, the con-
straints implementing a case system in a grammar
serve to model both grammaticality and the map-
ping between syntactic and semantic arguments.
Here too, the distribution of tokens may be some-
thing other than a pure expression of the case
alignment type. Sources of noise in the distri-
bution include: argument optionality (e.g., tran-
sitives with one or more covert arguments), ar-
gument frames other than simple intransitives or
transitives, and quirky case (verbs that use a non-
standard case frame for their arguments, such as
the German verb helfen which selects a dative ar-
gument, though the language?s general system is
nominative-accusative (Drellishak, 2009)).
5.1 Methodology
We explore two possible methodologies for infer-
ring case systems, one relatively na??ve and one
more elaborate, and compare them to a most-
frequent-type baseline. Method 1, called GRAM,
considers only the gloss line of the IGT and as-
sumes that it complies with the Leipzig Glossing
Rules (Bickel et al, 2008). These rules not only
prescribe formatting aspects of IGT but also pro-
vide a set of licensed ?grams?, or tags for grammat-
ical properties that appear in the gloss line. GRAM
scans for the grams associated with case, and as-
signs case systems according to Table 3.
This methodology is simple to implement and
6English?s residual case system is of this type.
79
expected to work well given Leipzig-compliant
IGT. However, since it does not model the func-
tion of case, it is dependent on the IGT authors?
choice of gram symbols, and may be confused by
either alternative case names (e.g., SBJ and OBJ for
nominative and accusative or LOC for ergative in
languages where it is homophonous with the loca-
tive case) or by other grams which collide with the
case name-space (such as NOM for nominalizer).
It also only handles four of the nine case systems
(albeit the most frequent ones).
Method 2, called SAO, is more theoretically
motivated, builds on the RiPLes approach used
in inferring word order, and is designed to be
robust to idiosyncratic glossing conventions. In
this methodology, we first identify the S, A and
O arguments by projecting the information from
the parse of the English translation (including
the function tags) to the source sentence (and its
glosses). We discard all items which do not appear
to be simple transitive or intransitive clauses with
all arguments overt, and then collect all grams for
each argument type (from all words within in the
NP, including head nouns as well as determiners
and adjectives). While there are many grammati-
cal features that can be marked on NPs (such as
number, definiteness, honorifics, etc.), the only
ones that should correlate strongly with grammat-
ical function are case-marking grams. Further-
more, in any given NP, while case may be multi-
ply marked, we only expect one type of case gram
to appear. We thus assume that the most frequent
gram for each argument type is a case marker (if
there are any) and assign the case system accord-
ing to the following rules, where Sg, Og and Ag de-
note the most frequent grams associated with these
argument positions, respectively:
? Nominative-accusative: Sg=Ag, Sg 6=Og
? Ergative-absolutive: Sg=Og, Sg 6=Ag
? No case: Sg=Ag=Og, or Sg 6=Ag 6=Og and Sg,
Ag, Og also present on each of the other ar-
gument types
? Tripartite: Sg 6=Ag 6=Og, and Sg, Ag, Og (vir-
tually) absent from the other argument types
? Split-S: Sg 6=Ag 6=Og, and Ag and Og are both
present in the list for the S argument type
Here, we?re using Split-S to stand in for both
Split-S and Fluid-S. These are both systems where
some S arguments are marked like A, and some
like O. In Split-S, which is taken depends on the
verb. In Fluid-S, it depends on the interpretation of
the verb. These could be distinguished by looking
for intransitive verbs that appear more than once in
the data and checking whether their S arguments
all have consistently A or O marking.
This system is agnostic as to the spelling of the
case grams. By relying on more analysis of the
IGT than GRAM, it also introduces new kinds of
brittleness. Recognizing the difference between
grams being present and (virtually) absent makes
the system susceptible to noise.
5.2 Results
Table 4 shows the results for the inference of case-
marking systems. Currently GRAM performs best,
but both methods generally perform better than
the baseline. The better performance of GRAM
is expected, given the small size and generally
Leipzig-compliant glossing of our data sets. In
future work, we plan to incorporate data from
ODIN, which is likely less consistently annotated
but more voluminous, and we expect SAO to be
more robust than GRAM to this kind of data.
Dataset GRAM SAO Baseline
DEV1 0.900 0.700 0.400
DEV2 0.900 0.500 0.500
TEST 0.545 0.545 0.455
Table 4: Accuracy of case-marking inference
We find that GRAM is sometimes able to do well
when RiPLes gives alignment errors. For exam-
ple, Old Japanese is a NOM-ACC language, but the
case-marking grams (associated to postpositions)
are not aligned to the NP arguments, so SAO is not
able to judge their distribution. On the other hand,
SAO prevails when non-standard grams are used,
such as the NOM-ACC language Hupdeh, which is
annotated with SUBJ and OBJ grams. This comple-
mentarity suggests scope for system combination,
which we leave to future work.
6 Discussion and Future Work
Our initial results are promising, but also show
remaining room for improvement. Error analysis
suggests two main directions to pursue:
Overcoming testsuite bias In both the word or-
der and case system tasks, we see the effect of
testsuite bias on our system results. The testsuites
for freer word order languages can be artificially
dominated by a particular word order that the test-
suite author found convenient. Further, the re-
stricted vocabulary used in testsuites, combined
80
with a general preference for animates as subjects,
leads to stems and certain grams potentially being
misidentified as case markers.
We believe that these aspects of testsuite bias
are not typical of our true target input data, viz.,
the larger collections of IGT created by field
projects. On the other hand, there may be other as-
pects of testsuites which are simplifying the prob-
lem and to which our current methods are over-
fitted. To address these issues, we intend to look
to larger datasets in future work, both IGT collec-
tions from field projects and IGT from ODIN. For
the field projects, we will need to construct choices
files. For ODIN, we can search for data from the
languages we already have choices files for.
As we move from testsuites to test corpora
(e.g., narratives collected in documentary linguis-
tics projects), we expect to find different distribu-
tions of word order types. Our current methodol-
ogy for extracting word order is based on idealized
locations in our word order space for each strict
word order type. Working with naturally occurring
corpora it should be possible to gain a more em-
pirically based understanding of the relationship
between underlying word order and sentence type
distributions. It will be particularly interesting to
see how stable these relationships are across lan-
guages with the same underlying word order type
but from different language families and/or with
differences in other typological characteristics.
Better handling of unaligned words The other
main source of error is words that remain un-
aligned in the projected syntactic structure and
thus only loosely incorporated into the syntax
trees. This includes items like case marking adpo-
sitions in Japanese, which are unaligned because
there is no corresponding word in English, and
auxiliaries in Welsh, which are unaligned when
the English translation doesn?t happen to use an
auxiliary. In the former case, our SAO method
for case system extraction doesn?t include the case
grams in the set of grams for each NP. In the latter,
the word order inference system is unable to pick
up on the VSO order represented as Aux+S+[VP].
Simply fixing the attachment of the auxiliaries will
not be enough in this case, as the word order infer-
ence algorithm will need to be extended to han-
dle auxiliaries, but fixing the alignment is the first
step. Alignment problems are also the main reason
our initial attempts to extract information about
the order of determiners and nouns haven?t yet
been able to beat the most-frequent-type baseline.
Better handling of these unaligned words is
a non-trivial task, and will require bringing in
sources of knowledge other than the structure of
the English translation. The information we have
to leverage in this regard comes mainly from the
gloss line and from general linguistic/typological
knowledge which can be added to the algorithm.
That is, there are types of grams which are canon-
ically associated with verbal projections and types
of grams canonically associated with nominal pro-
jections. When these grams occur on unaligned
elements, we can hypothesize that the elements
are auxiliaries and case-marking adpositions re-
spectively. Further typological considerations will
motivate heuristics for modifying tree structures
based on these classifications.
Other directions for future work include extend-
ing this methodology to other aspects of grammat-
ical description, including additional high-level
systems (e.g., argument optionality), discovering
the range of morphosyntactic features active in a
language, and describing and populating lexical
types (e.g., common nouns with a particular gen-
der). Once we are able to answer enough of the
questionnaire that the customization system is able
to output a grammar, interesting options for de-
tailed evaluation will become available. In par-
ticular, we will be able to parse the IGT (includ-
ing held-out examples) with the resulting gram-
mar, and then compare the resulting semantic rep-
resentations to those produced by parsing the En-
glish translations with tools that produce compara-
ble semantic representations for English (using the
English Resource Grammar (Flickinger, 2000)).
7 Conclusions and Future Work
In this paper we have presented an approach to
combining two types of linguistic resources?IGT,
as produced by documentary linguists and a cross-
linguistic grammar resource supporting preci-
sion parsing and generation?to create language-
specific resources which can help enrich language
documentation and support language revitaliza-
tion efforts. In addition to presenting the broad vi-
sion of the project, we have reported initial results
in two case studies as a proof-of-concept. Though
there is still a ways to go, we find these initial re-
sults a promising indication of the approach?s abil-
ity to assist in the preservation of the key type of
cultural heritage that is linguistic systems.
81
Acknowledgments
We are grateful to the students in Ling 567 at the
University of Washington who created the test-
suites and choices files used as development and
test data in this work and to the three anonymous
reviewers for helpful comments and discussion.
This material is based upon work supported
by the National Science Foundation under Grant
No. BCS-1160274. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 88?97, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Dorothee Beerman and Lars Hellan. 2011. Induc-
ing grammar from IGT. In Proceedings of the 5th
Language and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics (LTC 2011).
Dorothee Beermann and Pavel Mihaylov. 2009. Type-
Craft: Linguistic data and knowledge sharing, open
access and linguistic methodology. Paper presented
at the Workshop on Small Tools in Cross-linguistic
Research, University of Utrecht. The Netherlands.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, David Wax, and Michael Wayne
Goodman. 2012b. From IGT to precision grammar:
French verbal morphology. In LSA Annual Meeting
Extended Abstracts 2012.
Emily M. Bender. 2007. Combining research and
pedagogy in the development of a crosslinguistic
grammar resource. In Tracy Holloway King and
Emily M. Bender, editors, Proceedings of the GEAF
2007 Workshop, Stanford, CA. CSLI Publications.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, second edition.
Bernard Comrie. 2011a. Alignment of case marking of
full noun phrases. In Matthew S. Dryer and Martin
Haspelmath, editors, The World Atlas of Language
Structures Online. Max Planck Digital Library, Mu-
nich.
Bernard Comrie. 2011b. Alignment of case marking of
pronouns. In Matthew S. Dryer and Martin Haspel-
math, editors, The World Atlas of Language Struc-
tures Online. Max Planck Digital Library, Munich.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 65?
72, Prague, Czech Republic, June. Association for
Computational Linguistics.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Matthew S. Dryer. 2011. Order of subject, object and
verb. In Matthew S. Dryer and Martin Haspelmath,
editors, The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich.
Scott Farrar and Terry Langendoen. 2003. A linguistic
ontology for the semantic web. Glot International,
7:97?100.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1) (Special Issue on Efficient Pro-
cessing with HPSG):15 ? 28.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 385?393,
Beijing, China, August. Coling 2010 Organizing
Committee.
82
Ryan Georgi, Fei Xia, and William Lewis. 2012. Im-
proving dependency parsing with interlinear glossed
text and syntactic projection. In Proceedings of
COLING 2012: Posters, pages 371?380, Mumbai,
India, December.
Ken Hale, Michael Krauss, Lucille J. Wata-
homigie, Akira Y. Yamamoto, Colette Craig,
LaVerne Masayesva Jeanne, and Nora C. England.
1992. Endangered languages. Language, 68(1):pp.
1?42.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2008. The World Atlas
of Language Structures Online. Max Planck Digital
Library, Munich. http://wals.info.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve` Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP:
Test suites for natural language processing. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 2, COLING ?96, pages 711?
716, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
William D. Lewis. 2006. ODIN: A model for adapting
and enriching legacy infrastructure. In Proceedings
of the e-Humanities Workshop, Held in cooperation
with e-Science, Amsterdam.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 629?637, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Sebastian Nordhoff and Karl-Ludwig G. Poggeman,
editors. 2012. Electronic Grammaticography. Uni-
versity of Hawaii Press, Honolulu.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Carmela Toews. 2009. The expression of tense and as-
pect in Shona. Selected Proceedings of the 39th An-
nual Converence on African Linguistics, pages 32?
41.
Tony Woodbury. 2003. Defining documentary lin-
guistics. Language documentation and description,
1(1):35.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia and William D. Lewis. 2008. Repurposing
theoretical linguistic data for tool development and
search. In Proceedings of the Third International
Joint Conference on Natural Language Processing,
pages 529?536, Hyderabad, India.
Fei Xia and William Lewis. 2009. Applying NLP
technologies to the collection and enrichment of lan-
guage data on the web to aid linguistic research. In
Proceedings of the EACL 2009 Workshop on Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education
(LaTeCH ? SHELT&R 2009), pages 51?59, Athens,
Greece, March. Association for Computational Lin-
guistics.
83
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43?53,
Baltimore, Maryland, USA, 26 June 2014.
c
?2014 Association for Computational Linguistics
Learning Grammar Specifications from IGT: A Case Study of Chintang
Emily M. Bender Joshua Crowgey Michael Wayne Goodman Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195-4340 USA
{ebender,jcrowgey,goodmami,fxia}@uw.edu
Abstract
We present a case study of the methodol-
ogy of using information extracted from
interlinear glossed text (IGT) to create of
actual working HPSG grammar fragments
using the Grammar Matrix focusing on
one language: Chintang. Though the re-
sults are barely measurable in terms of
coverage over running text, they nonethe-
less provide a proof of concept. Our expe-
rience report reflects on the ways in which
this task is non-trivial and on mismatches
between the assumptions of the methodol-
ogy and the realities of IGT as produced in
a large-scale field project.
1 Introduction
We explore the possibility of learning precision
grammar fragments from existing products of doc-
umentary linguistic work. A precision grammar is
a grammar which encodes a sharp notion of gram-
maticality and furthermore relates strings to elabo-
rate semantic representations. Such objects are of
interest in the context of documentary linguistics
because: (1) they are valuable tools in the explo-
ration of linguistic hypotheses (especially regard-
ing the interaction of various phenomena); (2) they
facilitate the search for examples in corpora which
are not yet understood; and (3) they can support
the development of treebanks (see Bender et al.,
2012a). However, they are expensive to build.
The present work is carried out in the context of
the AGGREGATION project,
1
which is exploring
whether such grammars can be learned on the ba-
sis of data already collected and enriched through
the work of descriptive linguists, specifically, col-
lections of IGT (interlinear glossed text).
The grammars themselves are not likely targets
for machine learning, especially in the absence of
1
http://depts.washington.edu/uwcl/aggregation/
treebanks, which are not generally available for
languages that are the focus of descriptive and
documentary linguistics. Instead, we take advan-
tage of the LinGO Grammar Matrix customiza-
tion system (Bender et al., 2002; Bender et al.,
2010) which maps from collections of statements
of linguistic properties (encoded in choices files)
to HPSG (Pollard and Sag, 1994) grammar frag-
ments which in turn can be used to parse strings
into semantic representations in the format of Min-
imal Recursion Semantics (MRS; Copestake et al.,
2005) and conversely, to generate strings from
MRS representations. The choices files are a
much simpler representation than the grammars
derived from them and therefore a more approach-
able learning target. Furthermore, using the Gram-
mar Matrix customization system to produce the
grammars results in much less noise in the auto-
matically derived grammar code than would arise
in a system learning grammars directly.
Here, we focus on a case study of Chintang, a
Kiranti language of Nepal, described by the Chin-
tang Language Research Project (CLRP) (Bickel
et al., 2009). Where Lewis and Xia (2008) and
Bender et al. (2013) apply similar methodologies
to extract large scale properties for many lan-
guages, we focus on a case study of a single lan-
guage, looking at both the large scale properties
and the lexical details. This is important for two
reasons: First, it gives us a chance to look in-
depth at the possible sources of difficulty in ex-
tracting the large scale properties. Second, while
large-scale properties are undoubtedly important,
the bulk of the information specified in a preci-
sion grammar is far more fine-grained. In this
case study we apply the methodology of Bender et
al. (2013) to extract general word order and case
properties and examine the sources of error affect-
ing those results. We also explore extensions of
those methodologies and that of Wax (2014) to ex-
tract lexical entries and specifications for morpho-
43
logical rules. Together with a few default spec-
ifications, this information is enough to allow us
to define grammars through the Grammar Matrix
customization system and thus evaluate the results
in terms of parsing coverage, accuracy and am-
biguity over running text. Chintang is particu-
larly well-suited for this case study because it is
an actual endangered language subject to active
descriptive research, making the evaluation of our
techniques realistic. Furthermore, the descriptive
research on Chintang is fairly advanced, having
produced both large corpora of high-quality IGT
and sophisticated linguistic descriptions, making
the evaluation and error analysis possible.
2 Related Work
This work can be understood as a task related to
both grammar induction and grammar extraction,
though it is distinct from both. It also connects
with and extends previous work using interlinear
glossed text to extract grammatical properties.
Grammar induction (Clark, 2001; Klein and
Manning, 2002; Klein and Manning, 2004;
Haghighi and Klein, 2006; Smith and Eisner,
2006; Snyder et al., 2009, inter alios) involves the
learning of grammars from unlabeled sentences.
Here, unlabeled means that the sentences are of-
ten POS tagged, but no syntactic structures for
the sentences are available. Most of those stud-
ies choose probabilistic context-free grammars
(PCFGs) or dependency grammars as the gram-
mar framework, and estimate the probability of
the context-free rules or dependency arcs from the
data. These studies improve parsing performance
significantly over some baselines such as the EM
algorithm, but the induced grammars are very dif-
ferent from precision grammars with respect to
content, quality, and grammar framework.
Grammar extraction, on the other hand, learns
grammars (sets of rules) from treebanks. Here the
idea is to use heuristics to convert the syntactic
structures in a treebank into derivation trees con-
forming to a particular framework, and then ex-
tract grammars from those trees. This has been
done in a wide range of grammar frameworks, in-
cluding PCFG (e.g. Krotov et al., 1998), LTAG
(e.g. Xia, 1999; Chen and Vijay-Shanker, 2000),
LFG (e.g. Cahill et al., 2004), CCG (e.g. Hock-
enmaier and Steedman, 2002, 2007), and HPSG
(e.g. Miyao et al., 2004; Cramer and Zhang, 2009).
However, this approach is not applicable to work
word-order=v-final
has-dets=yes
noun-det-order=det-noun
...
case-marking=erg-abs
erg-abs-erg-case-name=erg
erg-abs-abs-case-name=abs
...
verb4_valence=erg-abs
verb4_stem1_orth=sams-i-ne
verb4_stem1_pred=_sams-i-ne_v_re
...
verb-pc3_inputs=verb-pc2
verb-pc3_lrt1_name=2nd-person-subj
verb-pc3_lrt1_feat1_name=pernum
verb-pc3_lrt1_feat1_value=2nd
verb-pc3_lrt1_feat1_head=subj
verb-pc3_lrt1_lri1_inflecting=yes
verb-pc3_lrt1_lri1_orth=a-
Figure 1: Excerpts from a choices file
on endangered language documentation, as tree-
banks are not available for such languages.
A third line of research attempts to bootstrap
NLP tools for resource-poor languages by taking
advantage of IGT data and resources for resource-
rich languages. The canonical form of an IGT in-
stance includes a language line, a word-to-word
or morpheme-to-morpheme gloss line, and a trans-
lation line (typically in a resource-rich language).
The bootstrapping process starts with word align-
ment of the language line and translation line with
the help of the gloss line. Then the translation line
is parsed and the parse tree is projected to the lan-
guage line using the alignments (Xia and Lewis,
2007). The projected trees can be used to answer
linguistic questions such as word order (Lewis
and Xia, 2008) or bootstrap parsers (Georgi et al.,
2013). Our work extends this methodology to the
construction of precision grammars.
3 Methodology
Our goal in this work is to automatically create
choices files on the basis of IGT data. The choices
files encode both general properties about the lan-
guage we are trying to model as well as more spe-
cific information including lexical classes, lexical
items within lexical classes and definitions of lexi-
cal rules. Lexical rule definitions can include both
morphotactic information (ordering of affixes) as
well as morphosyntactic information, though here
our focus is on the former. Sample excerpts from
a choices file are given in Fig 1. These choices
files are then input into the Grammar Matrix cus-
tomization system
2
which produces HPSG gram-
2
SVN revision (for reproducibility): 27678.
44
mar fragments that meet the specifications in the
choices files. The Grammar Matrix customization
system provides analyses of a range of linguistic
phenomena. Here, we focus on a few that we con-
sider the most basic: major constituent word or-
der, the general case system, case frames for spe-
cific verbs, case marking on nouns, and morpho-
tactics for verbs. In ?3.1 we describe the dataset
we are working with. ?3.2 describes the different
approaches we take to building choices files on the
basis of this dataset. ?3.3 explains the metrics we
will use to evaluate the resulting grammars in ?4.
3.1 The Chintang Dataset
Chintang (ISO639-3: ctn) is a language spoken by
about 5000 people in Nepal and believed to be-
long to the Eastern subgroup of the Kiranti lan-
guages, which in turn are argued to belong to the
larger Tibeto-Burman family (Bickel et al., 2007;
Schikowski et al., in press). Here we briefly sum-
marize properties of the language that relate to
the information we are attempting to automatically
detect in the IGT, and in many cases make the
problem interestingly difficult.
Schikowski et al. (in press) describe Chintang
as exhibiting information-structurally constrained
word order: All permutations of the major senten-
tial constituents are expected to be valid, with the
different orders subject to different felicity condi-
tions. They state, however, that no detailed analy-
sis of word order has yet been carried out, and so
this description should be taken as preliminary.
In contrast, much detailed work has been done
on the marking of arguments, both via agree-
ment on the verb and via case marking of depen-
dents (Bickel et al., 2010; Stoll and Bickel, 2012;
Schikowski et al., in press). The case marking sys-
tem can be understood as following an ergative-
absolutive pattern, but with several variations from
that theme. In an ergative-absolutive pattern, the
sole argument of an intransitive verb (here called
S) is marked the same as the most patient-like ar-
gument of a transitive verb (here called O) and
differentiated from the most agent-like argument
of a transitive verb (here called A). Most A ar-
guments are marked with an overt case marker
called ergative, while S and O arguments appear
without a case marker. In most writing about the
language, this unmarked case is called nomina-
tive; here we will use the term absolutive. Simi-
larly, verbs agree with up to two arguments, and
the agreement markers for S and O are generally
shared and distinguished from those for A.
Divergences from the ergative-absolutive pat-
tern include variable marking of ergative case on
first and second person pronouns as well as va-
lence alternations such as one that licenses oc-
currences of transitive verbs with two absolutive
arguments (and S-style agreement with the A ar-
gument) when the O argument is of an indefinite
quantity (Schikowski et al., in press). Further-
more, the language allows dropping of arguments
(A, S, and O). Finally, there are of course valences
beyond simple intransitive and transitive, as well
as case frames even for two-argument verbs other
than { ERG, ABS }. As a result of the combination of
these facts, the actual occurrence of ergative-case-
marked arguments in speech is relatively low: Ex-
amining a corpus of speech spoken to and around
children, Stoll and Bickel (2012) find that only
11% of (semantically) transitive verb tokens have
an overt, ergative-marked NP A argument. As dis-
cussed below, these properties make it difficult for
automated methods to detect both the overall case
system of the language and accurate information
regarding the case frames of individual verbs.
The dataset we are using contains 9793 (8863
train, 930 test) IGT instances which come from
the corpus of narratives and other speech col-
lected, transcribed, translated and glossed by the
CLRP.
3
An example is shown in Fig. 2. As can
be seen in Fig. 2, the glossing in this dataset is ex-
tremely thorough. It is also supported by a detailed
Toolbox lexicon that encodes not only alternative
forms for each lemma as well as glosses in English
and Nepali, but also valence frames for most verb
entries which list the expected case marking on
the arguments. Finally, note that morphosyntactic
properties without a morphological reflex are sys-
tematically unglossed in the data, so that ABS never
appears (nor does SG for singular nouns, etc.).
In our experiments, we abstract away from the
problem of morphophonological analysis in order
to focus on morphosyntax and lexical acquisition.
Accordingly, our grammars target the second line
of the IGT, which represents each form as a se-
quence of phonologically regularized morphemes.
3.2 Grammars
In this section, we describe the different means we
use for extracting the different kinds of informa-
3
http://www.spw.uzh.ch/clrp
45
unisaNa
u-nisa-Na
3sPOSS-younger.brother-ERG.A
khatte
khatt-e
take-IND.PST
mo
mo
DEM.DOWN
kosi
kosi-i
river-LOC
moba
mo-pe
DEM.DOWN-LOC
?The younger brother took it to the river.? [ctn] (Bickel et al., 2013c)
Figure 2: Sample IGT
tion required to build the choices files (see Fig 1
above). We first describe our points of comparison
(oracle, ?3.2.1 and baseline, ?3.2.2), and then con-
sider different ways of detecting the large-scale
properties (word order, ?3.2.3; overall case sys-
tem, ?3.2.4). Next we turn to different ways of ex-
tracting two kinds of lexical information: the con-
straints on case (i.e. case frames of verbs and the
case marking on nouns, ?3.2.5) and verbal mor-
photactics (?3.2.6). Finally, we describe a small
set of hand-coded ?choices? which are added to all
choices files (except the oracle one) in order to cre-
ate working grammars (?3.2.7).
The alternative approaches to extracting the var-
ious kinds of information can be cross-classified
with each other, giving the set of choices files de-
scribed in Table 1. The first column gives iden-
tifiers for the choices files. The second specifies
how the lexicon was created, the third how the
value for major constituent word order was deter-
mined, and the fourth how the values for case were
determined, including the overall case system, the
case frames, and the case values for nouns. These
options are all described in more detail below.
3.2.1 Oracle choices file
As an upper-bound, we use the choices file de-
veloped in Bender et al., 2012b. This file in-
cludes hand-specified definitions of lexical rules
for nouns and verbs as well as lexical entries cre-
ated by importing lexical entries from the Tool-
box lexicon developed by the CLRP. This lex-
icon, as noted above, lists valence frames for
most verbal entries. As the Grammar Matrix
customization system currently only provides for
simple transitive and intransitive verbs, only two
verb classes were defined: intransitives with the
case frame { ABS } and transitives with the case
frame { ERG, ABS }. In addition, there is one class
of nouns. Finally, the choices file includes hand-
coded lexical entries for pronouns. As an upper-
bound, this choices file can be expected to repre-
sent high precision and moderate recall: verbs that
don?t fit the two classes defined aren?t imported.
Note that the Grammar Matrix customization
system does not currently support the definition of
adjectives, adverbs, or other parts of speech out-
side of verb, noun, determiner, (certain) adposi-
tions, conjunctions and auxiliaries. Thus while we
expect each grammar to be able to parse at least
some sentences in the corpus, to the extent that
sentences tend to include words outside the classes
noun, verb and determiner, we expect relatively
low coverage, even from our upper-bound.
3.2.2 Baseline choices file
Our baseline choices file is designed to create a
working grammar, without particular high-level
information about Chintang, that focuses on cov-
erage at the expense of precision. We hand-
specified the (counter-factual) assertion that there
is no case marking in Chintang, and in addi-
tion that Chintang allows free word order (on the
grounds that this is the least constrained word or-
der possibility). It also defines bare-bones classes
of nouns, determiners and transitive verbs, and
then populates the lexicon by using a variant of the
methodology in Xia and Lewis 2007. In particu-
lar, we parse the translation line using the Char-
niak parser (Charniak, 1997) and then use the cor-
respondences inherent in IGT to create a projected
tree structure for the language line, following Xia
and Lewis. An example of the result for Chintang
is shown in Fig 3. The projected trees include part
of speech tags for each word that can be aligned.
For each such word tagged as noun, verb, or deter-
miner, we create an instance in the corresponding
lexical type. In this baseline grammar, all verbs
are assumed to be transitive, but since all argu-
ments can (optionally) be dropped, the grammar is
expected to be able to cover intransitive sentences,
even if the semantic representation is wrong.
Since this baseline choices file models Chintang
as if it had no case marking, we expect it the re-
sulting grammar to have relatively high recall in
terms of the combination of nominal and verbal
constituents. On the other hand, since it is build-
ing a full-form lexicon and Chintang is a morpho-
logically complex language, we expect it to have
relatively low lexical coverage on held-out data.
46
Choices file Lexicon Word order Case
ORACLE Manual Manual Manual
BASELINE Fullform Default None
FF-AUTO-NONE Fullform Auto None
FF-DEFAULT-GRAM Fullform Default Auto (GRAM)
FF-AUTO-GRAM Fullform Auto Auto (GRAM)
FF-DEFAULT-SAO* Fullform Default Auto (SAO)
FF-AUTO-SAO* Fullform Auto Auto (SAO)
MOM-DEFAULT-NONE MOM Default None
MOM-AUTO-NONE MOM Auto None
MOM-DEFAULT-GRAM* MOM Default Auto (GRAM)
MOM-AUTO-GRAM* MOM Auto Auto (GRAM)
MOM-DEFAULT-SAO* MOM Default Auto (SAO)
MOM-AUTO-SAO* MOM Auto Auto (SAO)
Table 1: Choices files generated
s
vp
np-obj
pp
np
nn vbd
jutta khet-a-N-e
shoe buy-PST-1sS/P-IND.PST
I bought a pair of shoe .
prp vbd dt nn in nn
np-subj-prp np np
pp .
np-obj
vp
s
Figure 3: Projected tree structure (ex. from (Bickel
et al., 2013d))
3.2.3 Word order
We applied the methodology of Bender et al.
(2013) for determining major constituent order.
For our dataset, the algorithm chose ?v-final?,
which matches what is in the ORACLE choices file,
but is not necessarily correct. We created two ver-
sions of each of the other choices files, one with
the default (baseline) answer of ?free word order?
and one with this automatically supplied answer.
3.2.4 Case system
Similarly, we applied extended versions of the two
methods for automatically discovering case sys-
tems from Bender et al. 2013: GRAM which looks
for known case grams in glosses (not using pro-
jected trees) and SAO which extends the structure-
projection methodology of Xia and Lewis (2007)
to detect S, A and O arguments and then looks
for the most frequent gram associated with each
of these.
4
The GRAM method determines the
case system of Chintang to be ergative-absolutive,
while the SAO method indicates ?none? (no case).
Specifying a case system in a choices file has no
effect on the coverage or precision of the resulting
grammar if the lexical items don?t constrain case.
Thus the case system choices only make sense in
combination with the case frames choices (?3.2.5).
3.2.5 Case frames and case values
The HPSG analysis of case involves a feature CASE
which is constrained by both verbs and nouns:
Nouns constrain their own CASE value, while verbs
constrain the CASE value of the arguments they se-
lect for.
5
In order to constrain verbs and nouns
appropriately, we first need a range of possible
case values. For choices files built based on the
GRAM system, we consider case markers to be any
of those included in the set of grams defined by
the Leipzig Glossing Rules (Bickel et al., 2008):
ABL, ABS, ACC, ALL, COM, DAT, ERG, GEN, INS, LOC,
and OBL. For choices files built based on the SAO
system, we consider as case markers only those
grams (automatically) identified as marking S, A,
or O. In the present study, that should only be erga-
tive; as there is no marked case for absolutive, all
other nouns were treated as absolutive (regardless
of their actual case marking, since the SAO system
has no way to detect other case grams).
4
Our extensions involved making the system able to han-
dle the situation where one or more of S, A and O are morpho-
logically unmarked and therefore unreflected in the glosses.
5
For the details of the analyses of case systems provided
by the Grammar Matrix, see Drellishak 2009.
47
In choices files which specify case systems, we
constrain the case value for nouns by creating one
noun class for every case value, and then assigning
the lexical entries for nouns to those lexical classes
based on the grams in the gloss of the noun.
6
Similarly, we create lexical classes for each
case frame identified for transitive and intransitive
verbs: We look for case grams on each argument
of the verb, as determined by the function tags in
the projected tree (e.g. NP-SUBJ-PRP in Fig 3).
7
For
each case frame we identify, we create a lexical
class, and we create lexical entries for verbs based
on the case frames we extract for them. When
the system identifies both an overt subject and an
overt object, it considers the verb to be transitive
and constrains the case of its two arguments based
on the observed case values. If either argument
is overt but not marked for case, the verb is con-
strained to select for the default case on that argu-
ment, according to the detected case marking sys-
tem (i.e. ergative for transitive subjects and absolu-
tive for transitive objects, in this instance). When
there is an overt subject but no overt object, the
verb is treated as intransitive and is constrained to
select for a subject of the observed case (or the
default case, here absolutive, if the overt subject
bears no case marker). When there is an overt ob-
ject but no subject, the verb is assumed to be tran-
sitive and the object?s case assigned as with other
transitives but the subject?s case is constrained to
the default (i.e. ergative, in this instance). Verbs
with no overt arguments are not matched.
3.2.6 MOM choices file: Automatically
extracted lemmas and lexical rules
The final refinement we try on our baseline is
to apply the ?Matrix-ODIN Morphology? (MOM)
methodology of Wax 2014. This methodology at-
tempts to automatically identify affixes and cre-
ate appropriate descriptions of lexical rules in a
choices file to model those affixes. As a result,
it also identifies stems. Thus we use the same ba-
sic choices as in the baseline choices file, but now
populate the lexicon with stems rather than full-
forms. Compared to BASELINE, this one should re-
sult in a grammar with better lexical coverage on
held-out data, to the extent that the MOM system
6
In future work, we plan to extend the MOM approach
(?3.2.6) from verbs to nouns, but for now, the nouns are
treated as full-form lexical entries across all choices files.
7
While the GRAM method doesn?t require the projected
trees to determine the overall case system, we do need them
here to find case frames for particular verbs.
is able to correctly extract both stems and inflec-
tional rules. We note that while the MOM system
uses the same conceptual approach to alignment as
that in the BASELINE, GRAM and SAO approaches, the
implementation is separate, and so does not find
exactly the same set of verbs.
3.2.7 Shared choices
The ORACLE choices file ran as-is. For the re-
maining choices files, we also needed to answer
the questions about determiners (whether there are
any, position with respect to the noun). Based on
initial experiments, we chose ?yes? for the pres-
ence of determiners and ?det-noun? order. In an
attempt to boost coverage generally, we also coded
the choices that allow any argument to be dropped.
While the determiner-related choices are specific
to Chintang, the latter set of choices could be ex-
pected to boost coverage (at the cost of some pre-
cision) for any language.
3.2.8 Summary
Table 1 shows the 10 logical possibilities that arise
from combining the methods discussed in this sec-
tion, in addition to the ORACLE grammar and the
BASELINE grammar. However, we test only a subset
of these possibilities for the following reasons:
8
The SAO system chose no case as the case system
for Chintang. As a result, this makes FF-DEFAULT-
SAO and FF-AUTO-SAO the same as BASELINE and FF-
AUTO-NONE, respectively. In future work, we aim
to improve the SAO system but until it is effec-
tive enough to pick some case system for Chin-
tang, these options do not require further testing.
Secondly, while it is possible in principle to com-
bine the output of the MOM system (which classi-
fies verbs based on their morphological combina-
toric potential) with the output of the system be-
hind the GRAM choices files (which classifies verbs
based on their case frames), doing so is non-trivial
because these classifications are orthogonal, yet
each verb must inherit from each dimension. We
thus leave the exploration of MOM-DEFAULT-GRAM
and MOM-AUTO-GRAM (and likewise MOM-DEFAULT-
SAO and MOM-AUTO-SAO) for future work.
3.3 Evaluation
We evaluate the grammars generated by the
choices files over both the data used to develop
them (?training?; 8863 items) as well as data not
included in the development process (held-out
8
Untested choices files are marked with an * in the table.
48
?test? data; 930 items). We run both of these eval-
uations because we are actually testing two sepa-
rate questions. The first is whether the grammars
generated in this way can provide useful analyti-
cal tools to linguists. In this primary use-case, we
expect a linguist to provide the system with all of
their IGT and then use the generated grammars in
order to gain insights into that same data. This
does not amount to a case of testing on the train-
ing data because the annotations provided to the
system (IGT) are not the same as those produced
by the system (full parses, including semantic rep-
resentations). However, we are still interested in
also testing on held-out data in order to answer the
second question: whether grammars generated in
this way can also generalize to further texts.
We evaluate the grammars generated by the
choices files we create in terms of lexical cov-
erage, parse coverage, parse accuracy and am-
biguity. Lexical coverage measures how many
items consist only of word forms recognized by
the grammar. Any item with unknown lexical
items won?t parse.
9
Parse coverage is the num-
ber of items that receive any analysis at all, where
ambiguity is the number of different analyses each
item receives. To measure parse accuracy, we
examined the items that parse and determined
which parses had semantic representations whose
predicate-argument structures plausibly matched
what was indicated in the gloss.
4 Results
Table 2 compares the lexical information encoded
in each of the choices files in a quantitative fash-
ion. The first thing to note is that the grammars
vary widely in the size of their lexicons. The BASE-
LINE/FF lexicons are expected to be larger than the
others because they take each fully inflected form
encountered as a separate lexical entry. On the
other hand, the ORACLE choices file was built on the
basis of the Toolbox lexicon (dictionary) from the
CLRP and thus is effectively created on the basis
of a much larger dataset. The GRAM choices files
only contain verbs for which a case frame could
be identified. If the projected tree was not inter-
pretable by our extraction heuristics or if the ex-
ample had no overt arguments, then the verb will
not be extracted. The MOM choices files, on the
9
There are methods for handling unknown lexical items
(e.g. Adolphs et al., 2008) in more mature grammars of this
type, but these are not applicable at this stage.
other hand, only need to identify verbs in the string
to be able to extract them, and should be able to
generalize across different inflected forms of the
same verb. This gives a number of verb entries
intermediate between that for BASELINE/FF and the
GRAM files. For nouns, there is less variation: the
MOM files use the same data as the BASELINE, while
the GRAM method faces as simpler problem than
for verbs: it only needs to identify the case gram
(if any) in a noun?s gloss. The slightly larger num-
bers of nouns in the GRAM files v. the others can be
explained by the same form being glossed in two
different ways in the training data.
The remaining differences can be briefly ex-
plained as follows: The ORACLE choices file does
not contain any entries for determiners. The oth-
ers all contain the same 240 entries; one for any
word aligned by the algorithm to a determiner in
the English translation. Only the ORACLE and MOM
choices files attempt to handle morphology, and so
far MOM only does verbal morphology.
Table 3 presents the results of parsing training
and test data with the various grammars, in abso-
lute numbers and in percentages of the entire data
set. The ?lexical coverage? columns indicate for
how many items the grammars were able to rec-
ognize each constituent word form. The ?items
parsed? columns show the number of items that
received any analysis at all, while ?items correct?
show the number of items that were judged (by
one of the authors) to have a predicate-argument
structure that plausibly reflects the gloss given in
the IGT. The final column shows the average num-
ber of distinct analyses the grammars find for the
items they parse at all.
The results are in fact barely measurable with
these metrics (especially on the test data), but
nonetheless speak to the differences between the
grammars. Regarding lexical coverage, the ORA-
CLE grammar does best on the test data set. This
is because it is the only choices file not derived
from the training data. Not surprisingly, the BASE-
LINE grammar has the highest number of readings
per item parsed, followed closely by FF-AUTO-NONE
which adds only a minor constraint on word or-
der.
10
On the other hand, comparing the number
of items parsed to the number judged correct, ex-
cept for the MOM choices files, the ?survival rate?
was over 50% for all other tests.
11
This suggests
10
It is in this relative lack of constraint that BASELINE
mostly clearly forms a baseline to improve upon.
11
The vast majority of the incorrect parses for the MOM
49
Choices file # verb entries # noun entries # det entries # verb affixes # noun affixes
ORACLE 900 4751 0 160 24
BASELINE 3005 1719 240 0 0
FF-AUTO-NONE 3005 1719 240 0 0
FF-DEFAULT-GRAM 739 1724 240 0 0
FF-AUTO-GRAM 739 1724 240 0 0
MOM-DEFAULT-NONE 1177 1719 240 262 0
MOM-AUTO-NONE 1177 1719 240 262 0
Table 2: Amount of lexical information in each choices file
Training Data (N = 8863) Test Data (N = 930)
lexical items items average lexical items items average
choices file coverage (%) parsed (%) correct (%) readings coverage (%) parsed (%) correct (%) readings
ORACLE 1165 (13) 174 (3.5) 132 (1.5) 2.17 116 (12.5) 20 (2.2) 10 (1.1) 1.35
BASELINE 1276 (14) 398 (7.9) 216 (2.4) 8.30 41 (4.4) 15 (1.6) 8 (0.9) 28.87
FF-AUTO-NONE 1276 (14) 354 (4.0) 196 (2.2) 7.12 41 (4.4) 13 (1.4) 7 (0.8) 13.92
FF-DEFAULT-GRAM 911 (10) 126 (1.4) 84 (0.9) 4.08 18 (1.9) 4 (0.4) 2 (0.2) 5.00
FF-AUTO-GRAM 911 (10) 120 (1.4) 82 (0.9) 3.84 18 (1.9) 4 (0.4) 2 (0.2) 5.00
MOM-DEFAULT-NONE 1102 (12) 814 (9.2) 52 (0.6) 6.04 39 (4.2) 16 (1.7) 3 (0.3) 10.81
MOM-AUTO-NONE 1102 (12) 753 (8.5) 49 (0.6) 4.20 39 (4.2) 10 (1.1) 3 (0.3) 9.20
Table 3: Results
that, despite the noise introduced by the automatic
methods of lexical extraction, the precision gram-
mar backbone provided by the Grammar Matrix
can still provide high-quality parses.
For example, the BASELINE grammar produces
six parses of the string in (1):
(1) din
din
day
khiptukum
khipt-u-kV-m
count-3P-IND.NPST-1/2nsA
?(We) count days.? [ctn] (Bickel et al., 2013b)
Among these six is one which produces the se-
mantic representation in (2). While this grammar
does not yet capture any of the agreement mor-
phology that indicates that the subject is first per-
son plural, it does correctly link the ?day? to the
semantic ARG2 of ?count?.
(2)
? h
1
,
h
3
: din n day(x
4
),
h
5
: exist q rel(x
4
, h
6
, h
7
),
h
6
: khipt-u-kv-m v count(e
2
, x
9
, x
4
)
{ h
6
=
q
h
3
} ?
Finally, we note that the longest items we are
able to parse consist of one verb and two NPs, each
of which can have only up to two words (a deter-
miner and a noun). Most of the examples that do
parse consist of only one or two words, while the
full data set ranges from items of length 1 to items
of length 25 (average 4.5 words/item in training,
choices files involved analyses of words for ?yes?, ?well?,
?what? and the like as verbs. Note that one form of ?yes? is the
copula, and such examples were accepted. Another source of
incorrect parses for many grammars involves homophony be-
tween the focus particle and a verb meaning ?come?.
5 words/item in test). The Grammar Matrix al-
ready supports some longer sentences in the form
of coordination, so one avenue for future work is
to explore the automatic detection of coordination
strategies. Otherwise, branching out to longer sen-
tences will require additions to the Grammar Ma-
trix allowing the specification of modifiers and a
wider range of valence types for verbs.
5 Error Analysis
The opportunity to work closely with one lan-
guage has allowed us to observe several ways
in which the assumptions of the systems we are
building on do not match what we find in the data.
Here we briefly review some of those mismatches
and reflects on what could be done to handle them.
The first observation concerns the non-glossing
of zero-marked morphosyntactic features, such as
absolutive case in Chintang. From the point of
view of a consumer of IGT it is certainly desirable
to have as much information as possible made ex-
plicit in the glossing. From the point of view of
a project creating IGT in the context of on-going
fieldwork, however, it is likely often difficult to
reliably gloss zero morphemes and thus the de-
cision to leave them systematically unglossed is
quite sensible. Both the GRAM method and espe-
cially the SAO method for detecting case systems,
which we extended to extracting case frames for
particular verbs, are not yet fully robust to the
possibility that certain case values are unmarked
morphologically and thus not glossed in the data.
50
While we extended them to a certain extent in this
work, there is still more to be done on this front.
A second observation concerns the glossing of
proper names, as in (3):
(3) pailego
paile-ko
first-GEN
ubhiyauti
u-bhiya
3A-marriage
paphuma
paphu-ma
a.clan.of.Rai.people-F
?His first marriage was with a Phuphu woman.?
[ctn] (Bickel et al., 2013a)
We use statistical alignment between the trans-
lation line and the gloss line and between the
gloss line and the language line in order to project
information from the analysis of the translation
line onto the language line. Glosses such as
?a.clan.of.Rai.people? tend to confuse this align-
ment process, though they are very informative to
a human reader of the IGT. Error analysis of sen-
tences for which we were unable to extract subject
and object arguments at all suggested that many
of the errors were caused by misalignments likely
due to the aligner not being able to cope with this
kind of glossing. Future work will explore how to
train the aligner to function better in such cases.
In addition to properties of the glossing conven-
tions, there are also properties of the language that
proved challenging for our system. The first is the
intricate nature of the case-marking system as dis-
cussed in ?3.1. In particular, our system does not
model any distinction between 1st and 2nd per-
son pronouns and other nouns, such that when the
pronouns appear without a case marker, they are
taken to be in the unmarked case (i.e. absolutive),
though this is not necessarily so. The second prop-
erty of the language that our system found diffi-
cult is the optionality of arguments. We were able
to adapt our case frame extraction strategy to han-
dle dropped subjects, but dropped objects are more
confounding: our system is unable so far to distin-
guish such verbs from intransitives. One possible
way forward in this case is to draw more informa-
tion from the English translation in the IGT: En-
glish tends not to drop arguments, and so when we
find an object (especially a pronominal object) in
the English translation that is not aligned to any-
thing in the language line, we would have evidence
that the verb in question may be transitive.
Finally, we looked closely at the items in the test
data for which we had complete lexical analysis,
but which still failed to parse. We did this both for
the fullform and MOM-based lexicons. The goal
here was to evaluate whether (a) our assignment of
items to lexical categories was correct (and there
was some other issue standing in the way of an-
alyzing the item) or (b) we should have parsed a
given item, but our system had misidentified the
words in question in such a way that no syntactic
analysis could be found. For the baseline system,
we found that although some items had misidenti-
fied categories (specifically, pronouns and adverbs
were sometimes misidentified as determiners), the
two major obstacles to parsing came from multi-
verb constructions or sentential fragments. Of the
26 unparsed items with lexical coverage, 10 con-
tained multiple verbs and 12 were NP or interjec-
tory fragments (eg: ?Yes, yes, yes.?). We observed
a similar pattern among 23 unparsed items from
the MOM-based lexicon. We can take two lessons
from this assessment: (1) since much of our data
comes from naturally occurring speech, it may be
useful to rerun our tests with an NP fragment as
a valid root symbol in our grammars; (2) proper
identification of auxiliary verbs is an important
next step for improving our system.
6 Conclusion
In this paper we have taken the first steps towards
creating actual precision grammars by creating
Grammar Matrix customization system choices
files on the basis of automated analysis of IGT.
Measured in terms of coverage over held-out data,
the results are hardly impressive and might seem
discouraging. However, we see in these initial for-
ays rather a proof-of-concept. Moreover, the pro-
cess of digging into the details of getting an IGT-
to-grammar system working for one particular lan-
guage has been a very rich source of information
on the mismatches between the assumptions of
systems built to handle high-level properties and
the linguistic facts and glossing conventions of the
kind of data they are meant to handle.
7 Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
BCS-1160274. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the NSF.
We would like to thank David Wax for his as-
sistance in setting up the MOM system, Olga Za-
maraeva for general discussion, and especially the
CRLP for providing access to the Chintang data.
51
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural
language parsing. Marrakech, Morocco, May.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, Robert Schikowski, and Balthasar
Bickel. 2012b. Deriving a lexicon for a precision
grammar from language documentation resources:
A case study of Chintang. In Proceedings of COL-
ING 2012, pages 247?262, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Emily M. Bender, Michael Wayne Goodman, Joshua
Crowgey, and Fei Xia. 2013. Towards creating pre-
cision grammars from interlinear glossed text: Infer-
ring large-scale typological properties. In Proceed-
ings of the 7th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 74?83, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Balthasar Bickel, Goma Banjade, Martin Gaenszle,
Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj
Rai, Novel Kishor Rai, and Sabine Stoll. 2007. Free
prefix ordering in Chintang. Language, 83(1):43?
73.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai,
Elena Lieven, Goma Banjade, Toya Nath Bhatta,
Netra Paudyal, Judith Pettigrew, Ichchha P. Rai,
Manoj Rai, Robert Schikowski, and Sabine Stoll.
2009. Audiovisual corpus of the chintang lan-
guage, including a longitudinal corpus of language
acquisition by six children, plus a trilingual dic-
tionary, paradigm sets, grammar sketches, ethno-
graphic descriptions, and photographs. DOBES
Archive, http://www.mpi.nl/DOBES.
Balthasar Bickel, Manoj Rai, Netra P. Paudyal, Goma
Banjade, Toya N. Bhatta, Martin Gaenszle, Elena
Lieven, Ichchha Purna Rai, Novel Kishore Rai, and
Sabine Stoll. 2010. The syntax of three-argument
verbs in Chintang and Belhare (Southeastern Ki-
ranti). In Studies in Ditransitive Constructions: A
Comparative Handbook, pages 382?408. Mouton de
Gruyter, Berlin.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013a. Hatuwa. Accessed:
15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013b. Khadak?s daily life.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013c. Tale of a poor guy.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013d. Talk of kazi?s trip.
Accessed: 15 January 2013.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 319?326, Barcelona, Spain, July.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of AAAI-1997.
John Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proc. of the 6th International Workshop on Parsing
Technologies (IWPT-2000), Italy.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distri-
butional clustering. In Proc. of the 5th Confer-
ence on Computational Natural Language Learning
(CoNLL-2001).
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
52
Bart Cramer and Yi Zhang. 2009. Construction of a
german hpsg grammar from a detailed treebank. In
Proceedings of the 2009 Workshop on Grammar En-
gineering Across Frameworks (GEAF 2009), pages
37?45, Suntec, Singapore.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Ryan Georgi, Fei Xia, and William D. Lewis. 2013.
Enhanced and portable dependency projection algo-
rithms using interlinear glossed text. In Proceedings
of ACL 2013 (Volume 2: Short Papers), pages 306?
311, Sofia, Bulgaria, August.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881?888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proc. of LREC-2002, pages
1974?1981.
Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355?396.
Dan Klein and Christopher Manning. 2002. A gen-
eral constituent context model for improved gram-
mar induction. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), Philadelphia, PA.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2004), Barcelona, Spain.
Alexander Krotov, Mark Hepple, Robert Gaizauskas,
and Yorick Wilks. 1998. Compacting the Penn
Treebank Grammar. In Proc. of the 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-1998), Montreal, Quebec, Canada.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proc. of the First In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-2004), Hainan, China.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Robert Schikowski, Balthasar Bickel, and Netra
Paudyal. in press. Flexible valency in Chintang.
In B. Comrie and A. Malchukov, editors, Valency
Classes: A Comparative Handbook. Mouton de
Gruyter, Berlin.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL/COLING 2006), pages 569?
576, Sydney, Australia, July. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 73?81, August.
Sabine Stoll and Balthasar Bickel. 2012. How to
measure frequency? Different ways of counting
ergatives in Chintang (Tibeto-Burman, Nepal) and
their implications. In Frank Seifart, Geoffrey Haig,
Nikolaus P. Himmelmann, Dagmar Jung, Anna Mar-
getts, and Paul Trilsbeek, editors, Potentials of Lan-
guage Documentation: Methods, Analyses, and Uti-
lization, pages 83?89. University of Hawai?i Press,
Manoa.
David Wax. 2014. Automated grammar engineering
for verbal morphology. Master?s thesis, University
of Washington.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed Corpora. In Proc. of 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-1999), Beijing, China.
53
