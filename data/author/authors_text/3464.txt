PCFG Parsing for Restricted Classical Chinese Texts 
 
Liang HUANG 
Department of Computer Science,  
Shanghai Jiaotong University 
No. 1954 Huashan Road, Shanghai 
P.R. China 200030 
lhuang@sjtu.edu.cn 
Yinan PENG 
 Department of Computer Science,  
Shanghai Jiaotong University 
No. 1954 Huashan Road, Shanghai 
P.R. China 200030 
ynpeng@sjtu.edu.cn 
  
Huan WANG 
Department of Chinese Literature and Linguistics,  
East China Normal University 
No. 3663 North Zhongshan Road, Shanghai,  
P.R. China 200062 
Zhenyu WU 
Department of Computer Science,  
Shanghai Jiaotong University 
No. 1954 Huashan Road, Shanghai 
P.R. China 200030 
neochinese@sjtu.edu.cn 
  
Abstract 
The Probabilistic Context-Free Grammar 
(PCFG) model is widely used for parsing 
natural languages, including Modern 
Chinese. But for Classical Chinese, the 
computer processing is just commencing. 
Our previous study on the part-of-speech 
(POS) tagging of Classical Chinese is a 
pioneering work in this area. Now in this 
paper, we move on to the PCFG parsing of 
Classical Chinese texts. We continue to 
use the same tagset and corpus as our 
previous study, and apply the 
bigram-based forward-backward algorithm 
to obtain the context-dependent 
probabilities. Then for the PCFG model, 
we restrict the rewriting rules to be 
binary/unary rules, which will simplify our 
programming. A small-sized rule-set was 
developed that could account for the 
grammatical phenomena occurred in the 
corpus. The restriction of texts lies in the 
limitation on the amount of proper nouns 
and difficult characters.  In our 
preliminary experiments, the parser gives 
a promising accuracy of 82.3%. 
Introduction 
Classical Chinese is an essentially different 
language from Modern Chinese, especially in 
syntax and morphology. While there has been 
a number of works on Modern Chinese 
Processing over the past decade (Yao and Lua, 
1998a), Classical Chinese is largely neglected, 
mainly because of its obsolete and difficult 
grammar patterns. In our previous work (2002), 
however, we have stated that in terms of 
computer processing, Classical Chinese is 
even easier as there is no need of word 
segmentation, an inevitable obstacle in the 
processing of Modern Chinese texts. Now in 
this paper, we move on to the parsing of 
Classical Chinese by PCFG model. In this 
section, we will first briefly review related 
works, then provide the background of 
Classical Chinese processing, and finally give 
the outline of the rest of the paper. 
A number of parsing methods have been 
developed in the past few decades.  They can 
be roughly classified into two categories: 
rule-based approaches and statistical 
approaches. Typical rule-based approaches as 
described in James (1995) are driven by 
grammar rules. Statistical approaches such as 
Yao and Lua (1998a), Klein and Manning 
(2001) and Johnson, M. (2001), on the other 
hand, learn the parameters the distributional 
regularities from a usually large-sized corpus. 
In recent years, the statistical approaches have 
been more successful both in part-of-speech 
tagging and parsing. In this paper, we apply 
the PCFG parsing with context-dependent 
probabilities. 
A special difficulty lies in the word 
segmentation for Modern Chinese processing. 
Unlike Indo-European languages, Modern 
Chinese words are written without white 
spaces indicating the gaps between two 
adjacent words. And different possible 
segmentations may cause consistently 
different meanings. In this sense, Modern 
Chinese is much more ambiguous than those 
Indo-European Languages and thus more 
difficult to process automatically (Huang et al, 
2002).  
For Classical Chinese processing, such 
segmentation is largely unnecessary, since 
most Classical Chinese words are 
single-syllable and single-character formed. 
To this end, it is easier than Modern Chinese 
but actually Classical Chinese is even more 
ambiguous because more than half of the 
words have two or more possible lexical 
categories and dynamic shifts of lexical 
categories are the most common grammatical 
phenomena in Classical Chinese. Despite of 
these difficulties, our work (2002) on 
part-of-speech tagging has shown an 
encouraging result. 
The rest of the paper is organized as 
follows. In Section 1, a tagset designed 
specially for Classical Chinese is introduced 
and the forward-backward algorithm for 
obtaining the context-dependent probabilities 
briefly discussed. We will briefly present the 
traditional two-level PCFG model, the 
syntactic tagset and CFG rule-set for Classical 
Chinese in Section 2. Features of the Classical 
Chinese grammar will also be covered in this 
section. In Section 3 we will present our 
experimental results. A summary of the paper 
is given in the conclusion section. 
1 Tagset and Context-Dependent 
Probabilities 
Generally speaking, the design of tagset is 
very crucial to the accuracy and efficiency of 
tagging and parsing, and this was commonly 
neglected in the literature where many 
researchers use those famous corpora and their 
tagset as the standard test-beds. Still there 
should be a tradeoff between accuracy and 
efficiency. In our previous work (2002), a 
small-sized tagset for Classical Chinese is 
presented that is shown to be accurate in their 
POS tagging experiments. We will continue to 
use their tagset in this paper. We will also use 
a forward-backward algorithm to obtain the 
context-dependent probabilities. 
1.1 Tagset 
The tagset was designed with special interest 
not only to the lexical categories, but also the 
categories of components, namely 
subcategories a word may belong. For 
example, it discriminates adjectives into 4 
subcategories like Adjective as attributive, etc. 
(See table 1). And several grammatical 
features should be reflected in the tagset. 
These discriminations and features turn out to 
be an important contributing factor of the 
accuracy in our parsing experiments. 
Table 1.  The tagset for Classical Chinese 
 
1.2 Tagging Algorithms 
We apply the Hidden Markov Model (HMM) 
(Viterbi, 1967) and the forward-backward 
algorithm (James, 1995) to obtain the 
context-dependent probabilities. 
Generally there are 2 types of HMM taggers 
for parsers, the trigram model and the bigram 
forward-backward model. Charniak (1996) 
suggested that the former is better for parsers. 
But the former only result in a deterministic 
sequence of most probable POS, in other 
words, it assigns only one POS tag for each 
word. Although the accuracy of trigram by our 
previous work (2002) is as high as 97.6%, for 
a sentence of 10 words long, the possibility of 
all-correctness is as low as low as 
78.4%(97.6%)10 = , and the single-tag scheme 
does not allow parsers to re-call the correct 
tags, as is often done if we apply the 
forward-backward model. So in this paper we 
still apply the traditional bigram 
forward-backward algorithm. We suggest that 
a combination of trigram and 
forward-backward model would be the best 
choice, although no such attempt exists in the 
literature. 
2 PCFG Model and Classical Chinese 
Grammar 
In this section we will cover the PCFG model 
and context-sensitive rules designed for 
Classical Chinese. Features of the rule-set will 
be also discussed. 
2.1 PCFG Model and Rule Restriction 
CFG: A context-free grammar (CFG) is a 
quadruple ),,,( RSVV TN  where TV  is a set of 
terminals (POS tags), NV  is a set of 
non-terminals (syntactic tags), NVS ?  is the 
start non-terminal, and R is the finite set of 
rules, which are pairs from +?VVN , where V 
denotes TN VV  . A rule >< ?,A  is written in the 
form ??A , A is called the left hand side 
(LHS) and ?  the right hand side (RHS). 
PCFG: A probabilistic context-free grammar 
(PCFG) is a quintuple ),,,,( PRSVV TN , where 
),,,( RSVV TN  is a CFG and ]1,0(: RP  is a 
probability function such 
that
NVN ?? :? ?? =?RN NP?? ?: 1)(  
Rule Restriction:
 We restrict the CFG rules to 
be binary or unary rules, but NOT as strict as 
the Chomsky Normal Form (CNF). Each 
RRi ? could be in the following two forms 
only: 
1.  ABNR ji ?:  
2.  ANR ji ?:  
where Nj VN ?  and VBA ?,  
The advantage of binary/unary rules lies in the 
simplicity of parsing algorithm, and will be 
discussed in Section 4. 
The major difference between our model and 
CNF is that for unary rules, we do not require 
the right-hand-side to be terminals. And this 
enables us easier representation of the 
Classical Chinese language. 
2.2 Rule-Set for Classical Chinese 
An important advantage of PCFG is that it 
needs fewer rules and parameters. According 
to our corpus, which is representative of 
Classical Chinese classics, only 100-150 rules 
would be sufficient. This is mainly because 
our rule set is linguistically sound. A summary 
of the set of rules is presented as follows. 
Table 2. Our non-terminals (also called syntactic tagset, 
or constituent set) 
 
A subset of most frequently used rules is 
shown in the following table. 
Table 3. A simple subset of PCFG Rules for 
Classical Chinese 
1. S ->   NP VP ; simple S/V 
2. S ->   VP ; S omitted 
3. S ->   VP NP ; S/V inversion 
4. S ->  ad S 
5. VP -> vi 
6. VP -> vt NP ; simple V/O 
7. VP -> NP vt ; V/O inversion 
8. VP -> ad VP 
9. VP -> PP VP ; prepositioned PP 
10. VP -> VP PP ; postpositioned PP 
11. VP -> NP ; NP as VP 
12. VP -> VP yq  
13. NP -> n 
14. NP -> npron 
15. NP -> ADJP NP 
16. NP -> POSTADJP 
17. NP -> VP ; V/O as NP 
18. NP -> fy NP 
19. ADJP -> aa 
20. ADJP -> apron 
21. ADJP -> NP zd 
22. PP -> prep NP ; P+NP 
23. PP -> NP prep ; inversion 
24. PP -> prepb ; object omitted 
25. PP -> NP ; prep. omitted 
26. POSTADJP-> VP zj 
 
 Examples of parse trees are shown in the 
following figure. 
 
 
(a)      (b) 
Fig. 1. the parse trees of 2 sentences  
2.3 Features of Classical Chinese 
Grammar Rules 
As an aside, it is worthwhile to point out here 
some peculiarities of the Classical Chinese 
grammar used in our work. Readers not 
interested in grammar modeling may simply 
skip this subsection. As mentioned before, the 
grammar of Classical Chinese is entirely 
different from that of English, so a few special 
features must be studied. Although these 
features bring many difficulties to the parser, 
we have developed successful programming 
techniques to solve them. 
From the rule-set, the reader might find that 
two special grammatical structures is very 
common in Classical Chinese: 
1. Inversion: subject/verb inversion (rule 3), 
preposition/object inversion (rule 23). 
2. Omission: Subject omitted (rule 2), 
preposition?s object omitted (rule 24), 
preposition omitted (rule 25). 
Maybe the strangest feature is the structure of 
PP. English PP is always P+NP. But here in 
Classical Chinese, by inversion and omission, 
the PP may have up to 4 forms, as shown in 
rule 22-25. 
Table 4. The 4 rules from PP. The object of the 
preposition is in brackets, and [] indicate an omission. 
  
Another feature that must be pointed out here 
is the cycle. In our rule-set, there are 2 rules 
(rule 11 and rule 17) forming a cycle: 
 
Fig. 2. A cycle in the rule-set. Rule 11: NP-> VP, Rule 17: 
VP-> NP. 
It will ease our parsing because Classical 
Chinese is lexically and syntactically very 
ambiguous. An NP can act as a VP (a main 
verb), while a VP can act as a NP (subject or 
object). These two features are exemplified in 
figure 3. There are actually more cycles in the 
rule-set. Helpful as they are, the cycles bring 
great difficulty to the memory-based top-down 
parser. In practice, we develop a 
closure?based method to solve this problem, 
as shown in the following pseudo-code: 
better_results_found=true; 
while (better_results_found) 
{ 
 better_results_found=false; 
 memory_based_top_down_parse();  
 // if better results found, the variable will be set true 
} 
  Another point is the use of preferences for 
ambiguity resolution. While the ambiguities in 
our rule-set greatly ease our modeling 
Classical Chinese grammar, it causes the 
parser to make a lot of ridiculous errors. So we 
here apply some predefined preferences such 
as ?an fy must be at the first of an NP? and ?a 
yq must be at the end of a VP?. This 
consideration results in a significant increase 
in the parsing accuracies. 
3 Evaluations 
In our preliminary experiments, we 
constructed a treebank of 1000 manually 
parsed sentences (quite large for Classical 
Chinese treebank), in which 100 sentences are 
selected as the test set using the 
cross-validation scheme, while the others as 
the learning set. The majority of these 
sentences are extracted from classics of 
pre-Tsin Classical Chinese such as Hanfeizi 
and Xunzi because in these texts there are 
fewer proper nouns and difficult words. That 
is the restriction we put on the selection of 
Classical Chinese texts. It must be pointed out 
here that compared from other languages, 
Classical Chinese sentences are so short that 
the average length is only about 4-6 words 
long. 
 
 
 
Fig. 3.
 Sentence Distributions and Parsing 
Accuracies 
 
Figure 3 shows the distribution of sentences 
and parsing accuracies for different sentence 
lengths. For distribution, we can see that those 
4-word, 5-word, and 6-word sentences 
constitute for the majority of the corpus, while 
those 1-word and 2-word sentences are very 
few. For accuracy, the parser is more effective 
for shorter sentences than for longer sentences. 
And for 1-word and 2-word sentences, there is 
no error report from the parse results. 
Conclusion 
Computer processing of Classical 
Chinese has just been commencing. While 
Classical Chinese is generally considered too 
difficult to process, our previous work on 
part-of-speech tagging has been largely 
successful because there is almost no need to 
segment Classical Chinese words. And we 
continue to use the tagset and corpus into this 
work. We first apply the forward-backward 
algorithm to obtain the context-dependent 
probabilities. The PCFG model is then 
presented where we restrict the rules into 
binary/unary rules, which greatly simplifies 
our parsing programming. According to the 
model, we developed a CFG rule-set of 
Classical Chinese. Some special features of the 
set are also studied. Classical Chinese 
processing is generally considered too difficult 
and thus neglected, while our works have 
shown that by good modelling and proper 
techniques, we can still get encouraging results. 
Although Classical Chinese is currently a dead 
language, our work still has applications in 
those areas as Classical-Modern Chinese 
Translation. 
For future work of this paper, we expect 
to incorporate trigram model into the 
forward-backward algorithm, which will 
increase the tagging accuracy. And most 
important of all, it is obvious that the 
state-of-the-art PCFG model is still 
two-leveled, we expect to devise a three-level 
model, just like trigram versus bigram. 
Acknowledgements 
Our special thanks go to Prof. Lu Ruzhan of 
Shanghai Jiaotong University for his sincere 
guidance. 
References 
Allen, J. (1995) Natural Language Understanding, 
The Benjamin/Cummings Publishing Company, 
Inc.  
Viterbi, A. (1967) Error bounds for convolution 
codes and an asymptotically optimal decoding 
algorithm. IEEE Trans. on Information Theory 
13:260-269.  
Yao Y., Lua K. (1998a) A Probabilistic 
Context-Free Grammar Parser for Chinese, 
Computer Processing of Oriental Languages, Vol. 
11, No. 4,  pp. 393-407 
Huang L., Peng Y., Wang H. (2002) Statistical 
Part-of-Speech Tagging for Classical Chinese, 
Proceedings of the 5th International Conference 
on Text, Speech, and Dialog (TSD), Brno, in 
press 
Klein, D., and Manning C. (2001) Natural 
Language Grammar Induction using a 
Constituent-Context Model, Proceedings of 
Neural Information Processing Systems, 
Vancouver. 
Yao Y., Lua K. (1998b) Mutual Information and 
Trigram Based Merging for Grammar Rule 
Induction and Sentence Parsing, Computer 
Processing of Oriental Languages, Vol. 11, No. 4, 
pp. 393-407 
Johnson, M. (2001) Joint and conditional 
estimation of tagging and parsing models, 
Proceedings of International computational 
linguistics conference, Toulouse 
Charniak, E. (1996) Taggers for Parsers, Artificial 
Intelligence, Vol. 85, No. 1-2, pp. 45-47. 
Proceedings of ACL-08: HLT, pages 897?904,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Cascaded Linear Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? Liang Huang ? Qun Liu ? Yajuan Lu? ?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
jiangwenbin@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
We propose a cascaded linear model for
joint Chinese word segmentation and part-
of-speech tagging. With a character-based
perceptron as the core, combined with real-
valued features such as language models, the
cascaded model is able to efficiently uti-
lize knowledge sources that are inconvenient
to incorporate into the perceptron directly.
Experiments show that the cascaded model
achieves improved accuracies on both seg-
mentation only and joint segmentation and
part-of-speech tagging. On the Penn Chinese
Treebank 5.0, we obtain an error reduction of
18.5% on segmentation and 12% on joint seg-
mentation and part-of-speech tagging over the
perceptron-only baseline.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are important tasks in computer processing of
Chinese and other Asian languages. Several mod-
els were introduced for these problems, for example,
the Hidden Markov Model (HMM) (Rabiner, 1989),
Maximum Entropy Model (ME) (Ratnaparkhi and
Adwait, 1996), and Conditional Random Fields
(CRFs) (Lafferty et al, 2001). CRFs have the ad-
vantage of flexibility in representing features com-
pared to generative ones such as HMM, and usually
behaves the best in the two tasks. Another widely
used discriminative method is the perceptron algo-
rithm (Collins, 2002), which achieves comparable
performance to CRFs with much faster training, so
we base this work on the perceptron.
To segment and tag a character sequence, there
are two strategies to choose: performing POS tag-
ging following segmentation; or joint segmentation
and POS tagging (Joint S&T). Since the typical ap-
proach of discriminative models treats segmentation
as a labelling problem by assigning each character
a boundary tag (Xue and Shen, 2003), Joint S&T
can be conducted in a labelling fashion by expand-
ing boundary tags to include POS information (Ng
and Low, 2004). Compared to performing segmen-
tation and POS tagging one at a time, Joint S&T can
achieve higher accuracy not only on segmentation
but also on POS tagging (Ng and Low, 2004). Be-
sides the usual character-based features, additional
features dependent on POS?s or words can also be
employed to improve the performance. However, as
such features are generated dynamically during the
decoding procedure, two limitation arise: on the one
hand, the amount of parameters increases rapidly,
which is apt to overfit on training corpus; on the
other hand, exact inference by dynamic program-
ming is intractable because the current predication
relies on the results of prior predications. As a result,
many theoretically useful features such as higher-
order word or POS n-grams are difficult to be in-
corporated in the model efficiently.
To cope with this problem, we propose a cascaded
linear model inspired by the log-linear model (Och
and Ney, 2004) widely used in statistical machine
translation to incorporate different kinds of knowl-
edge sources. Shown in Figure 1, the cascaded
model has a two-layer architecture, with a character-
based perceptron as the core combined with other
real-valued features such as language models. We
897
Core
Linear Model
(Perceptron)
g1 =
?
i ?i ? fi
~?
Outside-layer
Linear Model
S = ?j wj ? gj
~w
f1
f2
f|R|
g1
Word LM: g2 = Pwlm(W ) g2
POS LM: g3 = Ptlm(T ) g3
Labelling: g4 = P (T |W ) g4
Generating: g5 = P (W |T ) g5
Length: g6 = |W | g6
S
Figure 1: Structure of Cascaded Linear Model. |R| denotes the scale of the feature space of the core perceptron.
will describe it in detail in Section 4. In this ar-
chitecture, knowledge sources that are intractable to
incorporate into the perceptron, can be easily incor-
porated into the outside linear model. In addition,
as these knowledge sources are regarded as separate
features, we can train their corresponding models in-
dependently with each other. This is an interesting
approach when the training corpus is large as it re-
duces the time and space consumption. Experiments
show that our cascaded model can utilize different
knowledge sources effectively and obtain accuracy
improvements on both segmentation and Joint S&T.
2 Segmentation and POS Tagging
Given a Chinese character sequence:
C1:n = C1 C2 .. Cn
the segmentation result can be depicted as:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
while the segmentation and POS tagging result can
be depicted as:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
Here, Ci (i = 1..n) denotes Chinese character,
ti (i = 1..m) denotes POS tag, and Cl:r (l ? r)
denotes character sequence ranges from Cl to Cr.
We can see that segmentation and POS tagging task
is to divide a character sequence into several subse-
quences and label each of them a POS tag.
It is a better idea to perform segmentation and
POS tagging jointly in a uniform framework. Ac-
cording to Ng and Low (2004), the segmentation
task can be transformed to a tagging problem by as-
signing each character a boundary tag of the follow-
ing four types:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
We can extract segmentation result by splitting
the labelled result into subsequences of pattern s or
bm?e which denote single-character word and multi-
character word respectively. In order to perform
POS tagging at the same time, we expand boundary
tags to include POS information by attaching a POS
to the tail of a boundary tag as a postfix following
Ng and Low (2004). As each tag is now composed
of a boundary part and a POS part, the joint S&T
problem is transformed to a uniform boundary-POS
labelling problem. A subsequence of boundary-POS
labelling result indicates a word with POS t only if
the boundary tag sequence composed of its bound-
ary part conforms to s or bm?e style, and all POS
tags in its POS part equal to t. For example, a tag
sequence b NN m NN e NN represents a three-
character word with POS tag NN .
3 The Perceptron
The perceptron algorithm introduced into NLP by
Collins (2002), is a simple but effective discrimina-
tive training method. It has comparable performance
898
Non-lexical-target Instances
Cn (n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?
CnCn+1 (n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?
C?1C1 C?1C1=?/
Lexical-target Instances
C0Cn (n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?
C0CnCn+1 (n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?
C0C?1C1 C0C?1C1 =U?/
Table 1: Feature templates and instances. Suppose we are considering the third character ?U? in ?e? U /??.
to CRFs, while with much faster training. The per-
ceptron has been used in many NLP tasks, such as
POS tagging (Collins, 2002), Chinese word seg-
mentation (Ng and Low, 2004; Zhang and Clark,
2007) and so on. We trained a character-based per-
ceptron for Chinese Joint S&T, and found that the
perceptron itself could achieve considerably high ac-
curacy on segmentation and Joint S&T. In following
subsections, we describe the feature templates and
the perceptron training algorithm.
3.1 Feature Templates
The feature templates we adopted are selected from
those of Ng and Low (2004). To compare with oth-
ers conveniently, we excluded the ones forbidden by
the close test regulation of SIGHAN, for example,
Pu(C0), indicating whether character C0 is a punc-
tuation.
All feature templates and their instances are
shown in Table 1. C represents a Chinese char-
acter while the subscript of C indicates its posi-
tion in the sentence relative to the current charac-
ter (it has the subscript 0). Templates immediately
borrowed from Ng and Low (2004) are listed in
the upper column named non-lexical-target. We
called them non-lexical-target because predications
derived from them can predicate without consider-
ing the current character C0. Templates in the col-
umn below are expanded from the upper ones. We
add a field C0 to each template in the upper col-
umn, so that it can carry out predication according
to not only the context but also the current char-
acter itself. As predications generated from such
templates depend on the current character, we name
these templates lexical-target. Note that the tem-
plates of Ng and Low (2004) have already con-
tained some lexical-target ones. With the two kinds
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi)?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? +?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
of predications, the perceptron model will do exact
predicating to the best of its ability, and can back
off to approximately predicating if exact predicating
fails.
3.2 Training Algorithm
We adopt the perceptron training algorithm of
Collins (2002) to learn a discriminative model map-
ping from inputs x ? X to outputs y ? Y , where X
is the set of sentences in the training corpus and Y
is the set of corresponding labelled results. Follow-
ing Collins, we use a function GEN(x) generating
all candidate results of an input x , a representation
? mapping each training example (x, y) ? X ? Y
to a feature vector ?(x, y) ? Rd, and a parameter
vector ~? ? Rd corresponding to the feature vector.
d means the dimension of the vector space, it equals
to the amount of features in the model. For an input
character sequence x, we aim to find an output F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
?(x, y) ? ~? represents the inner product of feature
vector ?(x, y) and the parameter vector ~?. We used
the algorithm depicted in Algorithm 1 to tune the
parameter vector ~?.
899
To alleviate overfitting on the training examples,
we use the refinement strategy called ?averaged pa-
rameters? (Collins, 2002) to the algorithm in Algo-
rithm 1.
4 Cascaded Linear Model
In theory, any useful knowledge can be incorporated
into the perceptron directly, besides the character-
based features already adopted. Additional features
most widely used are related to word or POS n-
grams. However, such features are generated dy-
namically during the decoding procedure so that
the feature space enlarges much more rapidly. Fig-
ure 2 shows the growing tendency of feature space
with the introduction of these features as well as the
character-based ones. We noticed that the templates
related to word unigrams and bigrams bring to the
feature space an enlargement much rapider than the
character-base ones, not to mention the higher-order
grams such as trigrams or 4-grams. In addition, even
though these higher grams were managed to be used,
there still remains another problem: as the current
predication relies on the results of prior ones, the
decoding procedure has to resort to approximate in-
ference by maintaining a list of N -best candidates at
each predication position, which evokes a potential
risk to depress the training.
To alleviate the drawbacks, we propose a cas-
caded linear model. It has a two-layer architec-
ture, with a perceptron as the core and another linear
model as the outside-layer. Instead of incorporat-
ing all features into the perceptron directly, we first
trained the perceptron using character-based fea-
tures, and several other sub-models using additional
ones such as word or POS n-grams, then trained the
outside-layer linear model using the outputs of these
sub-models, including the perceptron. Since the per-
ceptron is fixed during the second training step, the
whole training procedure need relative small time
and memory cost.
The outside-layer linear model, similar to those
in SMT, can synthetically utilize different knowl-
edge sources to conduct more accurate comparison
between candidates. In this layer, each knowledge
source is treated as a feature with a corresponding
weight denoting its relative importance. Suppose we
have n features gj (j = 1..n) coupled with n corre-
 0
 300000
 600000
 900000
 1.2e+006
 1.5e+006
 1.8e+006
 2.1e+006
 2.4e+006
 2.7e+006
 3e+006
 3.3e+006
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
Fe
atu
re 
sp
ac
e
Introduction of features
growing curve
Figure 2: Feature space growing curve. The horizontal
scope X[i:j] denotes the introduction of different tem-
plates. X[0:5]: Cn (n = ?2..2); X[5:9]: CnCn+1 (n =
?2..1); X[9:10]: C?1C1; X[10:15]: C0Cn (n =
?2..2); X[15:19]: C0CnCn+1 (n = ?2..1); X[19:20]:
C0C?1C1; X[20:21]: W0; X[21:22]: W?1W0. W0 de-
notes the current considering word, while W?1 denotes
the word in front of W0. All the data are collected from
the training procedure on MSR corpus of SIGHAN bake-
off 2.
sponding weights wj (j = 1..n), each feature gj
gives a score gj(r) to a candidate r, then the total
score of r is given by:
S(r) =
?
j=1..n
wj ? gj(r) (2)
The decoding procedure aims to find the candidate
r? with the highest score:
r? = argmax
r
S(r) (3)
While the mission of the training procedure is to
tune the weights wj(j = 1..n) to guarantee that the
candidate r with the highest score happens to be the
best result with a high probability.
As all the sub-models, including the perceptron,
are regarded as separate features of the outside-layer
linear model, we can train them respectively with
special algorithms. In our experiments we trained
a 3-gram word language model measuring the flu-
ency of the segmentation result, a 4-gram POS lan-
guage model functioning as the product of state-
transition probabilities in HMM, and a word-POS
co-occurrence model describing how much probably
a word sequence coexists with a POS sequence. As
shown in Figure 1, the character-based perceptron is
used as the inside-layer linear model and sends its
output to the outside-layer. Besides the output of the
perceptron, the outside-layer also receive the outputs
900
of the word LM, the POS LM, the co-occurrence
model and a word count penalty which is similar to
the translation length penalty in SMT.
4.1 Language Model
Language model (LM) provides linguistic probabil-
ities of a word sequence. It is an important measure
of fluency of the translation in SMT. Formally, an
n-gram word LM approximates the probability of a
word sequence W = w1:m with the following prod-
uct:
Pwlm(W ) =
m
?
i=1
Pr(wi|wmax(0,i?n+1):i?1) (4)
Similarly, the n-gram POS LM of a POS sequence
T = t1:m is:
Ptlm(T ) =
m
?
i=1
Pr(ti|tmax(0,i?n+1):i?1) (5)
Notice that a bi-gram POS LM functions as the prod-
uct of transition probabilities in HMM.
4.2 Word-POS Co-occurrence Model
Given a training corpus with POS tags, we can train
a word-POS co-occurrence model to approximate
the probability that the word sequence of the la-
belled result co-exists with its corresponding POS
sequence. Using W = w1:m to denote the word se-
quence, T = t1:m to denote the corresponding POS
sequence, P (T |W ) to denote the probability that W
is labelled as T , and P (W |T ) to denote the prob-
ability that T generates W , we can define the co-
occurrence model as follows:
Co(W,T ) = P (T |W )?wt ? P (W |T )?tw (6)
?wt and ?tw denote the corresponding weights of the
two components.
Suppose the conditional probability Pr(t|w) de-
scribes the probability that the word w is labelled as
the POS t, while Pr(w|t) describes the probability
that the POS t generates the word w, then P (T |W )
can be approximated by:
P (T |W ) ?
m
?
k=1
Pr(tk|wk) (7)
And P (W |T ) can be approximated by:
P (W |T ) ?
m
?
k=1
Pr(wk|tk) (8)
Pr(w|t) and Pr(t|w) can be easily acquired by
Maximum Likelihood Estimates (MLE) over the
corpus. For instance, if the word w appears N times
in training corpus and is labelled as POS t for n
times, the probability Pr(t|w) can be estimated by
the formula below:
Pr(t|w) ? nN (9)
The probability Pr(w|t) could be estimated through
the same approach.
To facilitate tuning the weights, we use two com-
ponents of the co-occurrence model Co(W,T ) to
represent the co-occurrence probability of W and T ,
rather than use Co(W,T ) itself. In the rest of the
paper, we will call them labelling model and gener-
ating model respectively.
5 Decoder
Sequence segmentation and labelling problem can
be solved through a viterbi style decoding proce-
dure. In Chinese Joint S&T, the mission of the de-
coder is to find the boundary-POS labelled sequence
with the highest score. Given a Chinese character
sequence C1:n, the decoding procedure can proceed
in a left-right fashion with a dynamic programming
approach. By maintaining a stack of size N at each
position i of the sequence, we can preserve the top N
best candidate labelled results of subsequence C1:i
during decoding. At each position i, we enumer-
ate all possible word-POS pairs by assigning each
POS to each possible word formed from the charac-
ter subsequence spanning length l = 1..min(i,K)
(K is assigned 20 in all our experiments) and ending
at position i, then we derive all candidate results by
attaching each word-POS pair p (of length l) to the
tail of each candidate result at the prior position of p
(position i? l), and select for position i a N -best list
of candidate results from all these candidates. When
we derive a candidate result from a word-POS pair
p and a candidate q at prior position of p, we cal-
culate the scores of the word LM, the POS LM, the
labelling probability and the generating probability,
901
Algorithm 2 Decoding algorithm.
1: Input: character sequence C1:n
2: for i? 1 .. n do
3: L ? ?
4: for l? 1 .. min(i, K) do
5: w ? Ci?l+1:i
6: for t ? POS do
7: p? label w as t
8: for q ? V[i? l] do
9: append D(q, p) to L
10: sort L
11: V[i]? L[1 : N ]
12: Output: n-best results V[n]
as well as the score of the perceptron model. In ad-
dition, we add the score of the word count penalty as
another feature to alleviate the tendency of LMs to
favor shorter candidates. By equation 2, we can syn-
thetically evaluate all these scores to perform more
accurately comparing between candidates.
Algorithm 2 shows the decoding algorithm.
Lines 3 ? 11 generate a N -best list for each char-
acter position i. Line 4 scans words of all possible
lengths l (l = 1..min(i,K), where i points to the
current considering character). Line 6 enumerates
all POS?s for the word w spanning length l and end-
ing at position i. Line 8 considers each candidate
result in N -best list at prior position of the current
word. Function D derives the candidate result from
the word-POS pair p and the candidate q at prior po-
sition of p.
6 Experiments
We reported results from two set of experiments.
The first was conducted to test the performance of
the perceptron on segmentation on the corpus from
SIGHAN Bakeoff 2, including the Academia Sinica
Corpus (AS), the Hong Kong City University Cor-
pus (CityU), the Peking University Corpus (PKU)
and the Microsoft Research Corpus (MSR). The sec-
ond was conducted on the Penn Chinese Treebank
5.0 (CTB5.0) to test the performance of the cascaded
model on segmentation and Joint S&T. In all ex-
periments, we use the averaged parameters for the
perceptrons, and F-measure as the accuracy mea-
sure. With precision P and recall R, the balance
F-measure is defined as: F = 2PR/(P + R).
 0.966
 0.968
 0.97
 0.972
 0.974
 0.976
 0.978
 0.98
 0.982
 0.984
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
su
re
number of iterations
Perceptron Learning Curve
Non-lex + avg
Lex + avg
Figure 3: Averaged perceptron learning curves with Non-
lexical-target and Lexical-target feature templates.
AS CityU PKU MSR
SIGHAN best 0.952 0.943 0.950 0.964
Zhang & Clark 0.946 0.951 0.945 0.972
our model 0.954 0.958 0.940 0.975
Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN
best: best scores SIGHAN reported on the four corpus,
cited from Zhang and Clark (2007).
6.1 Experiments on SIGHAN Bakeoff
For convenience of comparing with others, we focus
only on the close test, which means that any extra
resource is forbidden except the designated train-
ing corpus. In order to test the performance of the
lexical-target templates and meanwhile determine
the best iterations over the training corpus, we ran-
domly chosen 2, 000 shorter sentences (less than 50
words) as the development set and the rest as the
training set (84, 294 sentences), then trained a per-
ceptron model named NON-LEX using only non-
lexical-target features and another named LEX us-
ing both the two kinds of features. Figure 3 shows
their learning curves depicting the F-measure on the
development set after 1 to 10 training iterations. We
found that LEX outperforms NON-LEX with a mar-
gin of about 0.002 at each iteration, and its learn-
ing curve reaches a tableland at iteration 7. Then
we trained LEX on each of the four corpora for 7
iterations. Test results listed in Table 2 shows that
this model obtains higher accuracy than the best of
SIGHAN Bakeoff 2 in three corpora (AS, CityU
and MSR). On the three corpora, it also outper-
formed the word-based perceptron model of Zhang
and Clark (2007). However, the accuracy on PKU
corpus is obvious lower than the best score SIGHAN
902
Training setting Test task F-measure
POS- Segmentation 0.971
POS+ Segmentation 0.973
POS+ Joint S&T 0.925
Table 3: F-measure on segmentation and Joint S&T of
perceptrons. POS-: perceptron trained without POS,
POS+: perceptron trained with POS.
reported, we need to conduct further research on this
problem.
6.2 Experiments on CTB5.0
We turned to experiments on CTB 5.0 to test the per-
formance of the cascaded model. According to the
usual practice in syntactic analysis, we choose chap-
ters 1? 260 (18074 sentences) as training set, chap-
ter 271? 300 (348 sentences) as test set and chapter
301? 325 (350 sentences) as development set.
At the first step, we conducted a group of contrast-
ing experiments on the core perceptron, the first con-
centrated on the segmentation regardless of the POS
information and reported the F-measure on segmen-
tation only, while the second performed Joint S&T
using POS information and reported the F-measure
both on segmentation and on Joint S&T. Note that
the accuracy of Joint S&T means that a word-POS
pair is recognized only if both the boundary tags and
the POS?s are correctly labelled.
The evaluation results are shown in Table 3. We
find that Joint S&T can also improve the segmen-
tation accuracy. However, the F-measure on Joint
S&T is obvious lower, about a rate of 95% to the
F-measure on segmentation. Similar trend appeared
in experiments of Ng and Low (2004), where they
conducted experiments on CTB 3.0 and achieved F-
measure 0.919 on Joint S&T, a ratio of 96% to the
F-measure 0.952 on segmentation.
As the next step, a group of experiments were
conducted to investigate how well the cascaded lin-
ear model performs. Here the core perceptron was
just the POS+ model in experiments above. Be-
sides this perceptron, other sub-models are trained
and used as additional features of the outside-layer
linear model. We used SRI Language Modelling
Toolkit (Stolcke and Andreas, 2002) to train a 3-
gram word LM with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), and a 4-gram POS
Features Segmentation F1 Joint S&T F1
All 0.9785 0.9341
All - PER 0.9049 0.8432
All - WLM 0.9785 0.9340
All - PLM 0.9752 0.9270
All - GPR 0.9774 0.9329
All - LPR 0.9765 0.9321
All - LEN 0.9772 0.9325
Table 4: Contribution of each feture. ALL: all features,
PER: perceptron model, WLM: word language model,
PLM: POS language model, GPR: generating model,
LPR: labelling model, LEN: word count penalty.
LM with Witten-Bell smoothing, and we trained
a word-POS co-occurrence model simply by MLE
without smoothing. To obtain their corresponding
weights, we adapted the minimum-error-rate train-
ing algorithm (Och, 2003) to train the outside-layer
model. In order to inspect how much improvement
each feature brings into the cascaded model, every
time we removed a feature while retaining others,
then retrained the model and tested its performance
on the test set.
Table 4 shows experiments results. We find that
the cascaded model achieves a F-measure increment
of about 0.5 points on segmentation and about 0.9
points on Joint S&T, over the perceptron-only model
POS+. We also find that the perceptron model func-
tions as the kernel of the outside-layer linear model.
Without the perceptron, the cascaded model (if we
can still call it ?cascaded?) performs poorly on both
segmentation and Joint S&T. Among other features,
the 4-gram POS LM plays the most important role,
removing this feature causes F-measure decrement
of 0.33 points on segmentation and 0.71 points on
Joint S&T. Another important feature is the labelling
model. Without it, the F-measure on segmentation
and Joint S&T both suffer a decrement of 0.2 points.
The generating model, which functions as that in
HMM, brings an improvement of about 0.1 points
to each test item. However unlike the three fea-
tures, the word LM brings very tiny improvement.
We suppose that the character-based features used
in the perceptron play a similar role as the lower-
order word LM, and it would be helpful if we train
a higher-order word LM on a larger scale corpus.
Finally, the word count penalty gives improvement
to the cascaded model, 0.13 points on segmentation
903
and 0.16 points on Joint S&T.
In summary, the cascaded model can utilize these
knowledge sources effectively, without causing the
feature space of the percptron becoming even larger.
Experimental results show that, it achieves obvious
improvement over the perceptron-only model, about
from 0.973 to 0.978 on segmentation, and from
0.925 to 0.934 on Joint S&T, with error reductions
of 18.5% and 12% respectively.
7 Conclusions
We proposed a cascaded linear model for Chinese
Joint S&T. Under this model, many knowledge
sources that may be intractable to be incorporated
into the perceptron directly, can be utilized effec-
tively in the outside-layer linear model. This is a
substitute method to use both local and non-local
features, and it would be especially useful when the
training corpus is very large.
However, can the perceptron incorporate all the
knowledge used in the outside-layer linear model?
If this cascaded linear model were chosen, could
more accurate generative models (LMs, word-POS
co-occurrence model) be obtained by training on
large scale corpus even if the corpus is not correctly
labelled entirely, or by self-training on raw corpus in
a similar approach to that of McClosky (2006)? In
addition, all knowledge sources we used in the core
perceptron and the outside-layer linear model come
from the training corpus, whereas many open knowl-
edge sources (lexicon etc.) can be used to improve
performance (Ng and Low, 2004). How can we uti-
lize these knowledge sources effectively? We will
investigate these problems in the following work.
Acknowledgement
This work was done while L. H. was visiting
CAS/ICT. The authors were supported by National
Natural Science Foundation of China, Contracts
60736014 and 60573188, and 863 State Key Project
No. 2006AA010108 (W. J., Q. L., and Y. L.), and by
NSF ITR EIA-0205456 (L. H.). We would also like
to Hwee-Tou Ng for sharing his code, and Yang Liu
and Yun Huang for suggestions.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th ICML, pages 282?289, Mas-
sachusetts, USA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of ACL 2006.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Lawrence. R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 311?318.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
904
Coling 2008: Advanced Dynamic Programming in Computational Linguistics ? Tutorial notes
Manchester, August 2008
Advanced Dynamic Programming in
Semiring and Hypergraph Frameworks
?
Liang Huang
Department of Computer and Information Science
University of Pennsylvania
lhuang3@cis.upenn.edu
July 15, 2008
Abstract
Dynamic Programming (DP) is an important class of algorithms
widely used in many areas of speech and language processing. Recently
there have been a series of work trying to formalize many instances of
DP algorithms under algebraic and graph-theoretic frameworks. This
tutorial surveys two such frameworks, namely semirings and directed
hypergraphs, and draws connections between them. We formalize two
particular types of DP algorithms under each of these frameworks: the
Viterbi-style topological algorithms and the Dijkstra-style best-first
algorithms. Wherever relevant, we also discuss typical applications of
these algorithms in Natural Language Processing.
1 Introduction
Many algorithms in speech and language processing can be viewed as in-
stances of dynamic programming (DP) (Bellman, 1957). The basic idea of
DP is to solve a bigger problem by divide-and-conquer, but also reuses the
solutions of overlapping subproblems to avoid recalculation. The simplest
such example is a Fibonacci series, where each F (n) is used twice (if cached).
The correctness of a DP algorithm is ensured by the optimal substructure
property, which informally says that an optimal solution must contain op-
timal subsolutions for subproblems. We will formalize this property as an
algebraic concept of monotonicity in Section 2.
?
Survey paper to accompany the COLING 2008 tutorial on dynamic programming. The
material presented here is based on the author?s candidacy exam report at the University
of Pennsylvania. I would like to thank Fernando Pereira for detailed comments on an
earlier version of this survey. This work was supported by NSF ITR EIA-0205456.
1
1
search space \ ordering topological-order best-first
graph + semirings (2) Viterbi (3.1) Dijkstra/A* (3.2)
hypergraph + weight functions (4) Gen. Viterbi (5.1) Knuth/A* (5.2)
Table 1: The structure of this paper: a two dimensional classification of dy-
namic programming algorithms, based on search space (rows) and propoga-
tion ordering (columns). Corresponding section numbers are in parentheses.
This report surveys a two-dimensional classification of DP algorithms
(see Table 1): we first study two types of search spaces (rows): the semir-
ing framework (Mohri, 2002) when the underlying representation is a di-
rected graph as in finite-state machines, and the hypergraph framework
(Gallo et al, 1993) when the search space is hierarchically branching as in
context-free grammars; then, under each of these frameworks, we study two
important types of DP algorithms (columns) with contrasting order of vis-
iting nodes: the Viterbi style topological-order algorithms (Viterbi, 1967),
and the Dijkstra-Knuth style best-first algorithms (Dijkstra, 1959; Knuth,
1977). This survey focuses on optimization problems where one aims to find
the best solution of a problem (e.g. shortest path or highest probability
derivation) but other problems will also be discussed.
2 Semirings
The definitions in this section follow Kuich and Salomaa (1986) and Mohri
(2002).
Definition 1. A monoid is a triple (A,?, 1) where ? is a closed associative
binary operator on the set A, and 1 is the identity element for ?, i.e., for all
a ? A, a ? 1 = 1 ? a = a. A monoid is commutative if ? is commutative.
Definition 2. A semiring is a 5-tuple R = (A,?,?, 0, 1) such that
1. (A,?, 0) is a commutative monoid.
2. (A,?, 1) is a monoid.
3. ? distributes over ?: for all a, b, c in A,
(a ? b) ? c = (a ? c) ? (b ? c),
c ? (a ? b) = (c ? a) ? (c ? b).
4. 0 is an annihilator for ?: for all a in A, 0 ? a = a ? 0 = 0.
2
2
Semiring Set ? ? 0 1 intuition/application
Boolean {0, 1} ? ? 0 1 logical deduction, recognition
Viterbi [0, 1] max ? 0 1 prob. of the best derivation
Inside R
+
? {+?} + ? 0 1 prob. of a string
Real R ? {+?} min + +? 0 shortest-distance
Tropical R
+
? {+?} min + +? 0 with non-negative weights
Counting N + ? 0 1 number of paths
Table 2: Examples of semirings
Table 2 shows some widely used examples of semirings and their appli-
cations.
Definition 3. A semiring (A,?,?, 0, 1) is commutative if its multiplicative
operator ? is commutative.
For example, all the semirings in Table 2 are commutative.
Definition 4. A semiring (A,?,?, 0, 1) is idempotent if for all a in A,
a ? a = a.
Idempotence leads to a comparison between elements of the semiring.
Lemma 1. Let (A,?,?, 0, 1) be an idempotent semiring, then the relation
? defined by
(a ? b) ? (a ? b = a)
is a partial ordering over A, called the natural order over A.
However, for optimization problems, a partial order is often not enough
since we need to compare arbitrary pair of values, which requires a total
ordering over A.
Definition 5. An idempotent semiring (A,?,?, 0, 1) is totally-ordered if its
natural order is a total ordering.
An important property of semirings when dealing with optimization
problems is monotonicity, which justifies the optimal subproblem property
in dynamic programming (Cormen et al, 2001) that the computation can
be factored (into smaller problems).
Definition 6. Let K = (A,?,?, 0, 1) be a semiring, and ? a partial order-
ing over A. We say K is monotonic if for all a, b, c ? A
(a ? b) ? (a ? c ? b ? c)
(a ? b) ? (c ? a ? c ? b)
3
3
Lemma 2. Let (A,?,?, 0, 1) be an idempotent semiring, then its natural
order is monotonic.
In the following section, we mainly focus on totally-ordered semirings
(whose natural order is monotonic).
Another (optional) property is superiority which corresponds to the non-
negative weights restriction in shortest-path problems. When superiority
holds, we can explore the vertices in a best-first order as in the Dijkstra
algorithm (see Section 3.2).
Definition 7. Let K = (A,?,?, 0, 1) be a semiring, and ? a partial order-
ing over A. We say K is superior if for all a, b ? A
a ? a ? b, b ? a ? b.
Intuitively speaking, superiority means the combination of two elements
always gets worse (than each of the two inputs). In shortest-path problems,
if you traverse an edge, you always get worse cost (longer path). In Table 2,
the Boolean, Viterbi, and Tropical semirings are superior while the Real
semiring is not.
Lemma 3. Let (A,?,?, 0, 1) be a superior semiring with a partial order ?
over A, then for all a ? A
1 ? a ? 0.
Proof. For all a ? A, we have 1 ? 1 ? a = a by superiority and 1 being the
identity of ?; on the other hand, we have a ? 0 ? a = 0 by superiority and
0 being the annihilator of ?.
This property, called negative boundedness in (Mohri, 2002), intuitively
illustrates the direction of optimization from 0, the initial value, towards as
close as possible to 1, the best possible value.
3 Dynamic Programming on Graphs
Following Mohri (2002), we next identify the common part shared between
these two algorithms as the generic shortest-path problem in graphs.
Definition 8. A (directed) graph is a pair G = (V, E) where V is the set
of vertices and E the set of edges. A weighted (directed) graph is a graph
G = (V, E) with a mapping w : E 7? A that assigns each edge a weight from
the semiring (A,?,?, 0, 1).
Definition 9. The backward-star BS (v) of a vertex v is the set of incoming
edges and the forward-star FS (v) the set of outgoing edges.
4
4
Definition 10. A path ? in a graph G is a sequence of consecutive edges,
i.e. ? = e
1
e
2
? ? ? e
k
where e
i
and e
i+1
are connected with a vertex. We define
the weight (or cost) of path ? to be
w(?) =
k
?
i=1
w(e
i
) (1)
We denote P (v) to be the set of all paths from a given source vertex s
to vertex v. In the remainder of the section we only consider single-source
shortest-path problems.
Definition 11. The best weight ?(v) of a vertex v is the weight of the best
path from the source s to v:
1
?(v) =
{
1 v = s
?
??P (v)
w(?) v 6= s
(2)
For each vertex v, the current estimate of the best weight is denoted
by d(v), which is initialized in the following procedure:
procedure Initialize(G, s)
for each vertex v 6= s do
d(v) ? 0
d(s) ? 1
The goal of a shortest-path algorithm is to repeatedly update d(v) for
each vertex v to some better value (based on the comparison ?) so that
eventually d(v) will converge to ?(v), a state we call fixed. For example, the
generic update along an incoming edge e = (u, v) for vertex v is
2
d(v) ? = d(u) ? w(e) (3)
Notice that we are using the current estimate of u to update v, so if
later on d(u) is updated we have to update d(v) as well. This introduces the
problem of cyclic updates, which might cause great inefficiency. To alleviate
this problem, in the algorithms presented below, we will not trigger the
update until u is fixed, so that the u ? v update happens at most once.
3.1 Viterbi Algorithm for DAGs
In many NLP applications, the underlying graph exhibits some special struc-
tural properties which lead to faster algorithms. Perhaps the most common
1
By convention, if P (v) = ?, we have ?(v) = 0.
2
Here we adopt the C notation where a ? = b means the assignment a ? a ? b.
5
5
of such properties is acyclicity, as in Hidden Markov Models (HMMs). For
acyclic graphs, we can use the Viterbi (1967) Algorithm
3
which simply
consists of two steps:
1. topological sort
2. visit each vertex in the topological ordering and do updates
The pseudo-code of the Viterbi algorithm is presented in Algorithm 1.
Algorithm 1 Viterbi Algorithm.
1: procedure Viterbi(G, w, s)
2: topologically sort the vertices of G
3: Initialize(G, s)
4: for each vertex v in topological order do
5: for each edge e = (u, v) in BS (v) do
6: d(v)? = d(u) ? w(e)
The correctness of this algorithm (that d(v) = ?(v) for all v after ex-
ecution) can be easily proved by an induction on the topologically sorted
sequence of vertices. Basically, at the end of the outer-loop, d(v) is fixed to
be ?(v).
This algorithm is widely used in the literature and there have been some
alternative implementions.
Variant 1. If we replace the backward-star BS (v) in line 5 by the forward-
star FS (v) and modify the update accordingly, this procedure still works
(see Algorithm 2 for pseudo-code). We refer to this variant the forward-
update version of Algorithm 1.
4
The correctness can be proved by a similar
induction (that at the beginning of the outer-loop, d(v) is fixed to be ?(v)).
Algorithm 2 Forward update version of Algorithm 1.
1: procedure Viterbi-Forward(G, w, s)
2: topologically sort the vertices of G
3: Initialize(G, s)
4: for each vertex v in topological order do
5: for each edge e = (v, u) in FS (v) do
6: d(u)? = d(v) ? w(e)
3
Also known as the Lawler (1976) algorithm in the theory community, but he considers
it as part of the folklore.
4
This is not to be confused with the forward-backward algorithm (Baum, 1972). In
fact both forward and backward updates here are instances of the forward phase of a
forward-backward algorithm.
6
6
Variant 2. Another popular implemention is memoized recursion (Cormen
et al, 2001), which starts from a target vertex t and invokes recursion on
sub-problems in a top-down fashion. Solved sub-problems are memoized to
avoid duplicate calculation.
The running time of the Viterbi algorithm, regardless of which imple-
mention, is O(V + E) because each edge is visited exactly once.
It is important to notice that this algorithm works for all semirings as
long as the graph is a DAG, although for non-total-order semirings the
semantics of ?(v) is no longer ?best? weight since there is no comparison.
See Mohri (2002) for details.
Example 1 (Counting). Count the number of paths between the source
vertex s and the target vertex t in a DAG.
Solution Use the counting semiring (Table 2).
Example 2 (Longest Path). Compute the longest (worst cost) paths from
the source vertex s in a DAG.
Solution Use the semiring (R ? {??}, max, +,??, 0).
Example 3 (HMM Tagging). See Manning and Schu?tze (1999, Chap. 10).
3.2 Dijkstra Algorithm
The well-known Dijkstra (1959) algorithm can also be viewed as dynamic
programming, since it is based on optimal substructure property, and also
utilizes the overlapping of sub-problems. Unlike Viterbi, this algorithm does
not require the structural property of acyclicity; instead, it requires the
algebraic property of superiority of the semiring to ensure the correctness of
best-first exploration.
Algorithm 3 Dijkstra Algorithm.
1: procedure Dijkstra(G, w, s)
2: Initialize(G, s)
3: Q ? V [G]  prioritized by d-values
4: while Q 6= ? do
5: v ? Extract-Min(Q)
6: for each edge e = (v, u) in FS (v) do
7: d(u)? = d(v) ? w(e)
8: Decrease-Key(Q, u)
The time complexity of Dijkstra Algorithm is O((E + V ) log V ) with a
binary heap, or O(E+V log V ) with a Fibonacci heap (Cormen et al, 2001).
7
7
Since Fibonacci heap has an excessively high constant overhead, it is rarely
used in real applications and we will focus on the more popular binary heap
case below.
For problems that satisfy both acyclicity and superiority, which include
many applications in NLP such as HMM tagging, both Dijkstra and Viterbi
can apply (Nederhof, 2003). So which one is better in this case?
From the above analysis, the complexity O((V + E) log V ) of Dijkstra
look inferior to Viterbi?s O(V +E) (due to the overhead for maintaining the
priority queue), but keep in mind that we can quit as long as the solution
for the target vertex t is found, at which time we can ensure the current
solution for the target vertex is already optimal. So the real running time of
Dijkstra depends on how early the target vertex is popped from the queue,
or how good is the solution of the target vertex compared to those of other
vertices, and whether this early termination is worthwhile with respect to
the priority queue overhead. More formally, suppose the complete solution
is ranked rth among V vertices, and we prefer Dijkstra to be faster, i.e.,
r
V
(V + E) log r < (V + E),
then we have
r log r < V (4)
as the condition to favor Dijkstra to Viterbi. However, in many real-world
applications (especially AI search, NLP parsing, etc.), often times the com-
plete solution (a full parse tree, or a source-sink path) ranks very low among
all vertices (Eq. 4 does not hold), so normally the direct use of Dijkstra does
not bring speed up as opposed to Viterbi. To alleviate this problem, there
is a popular technique named A* (Hart et al, 1968) described below.
3.2.1 A* Algorithm for State-Space Search
We prioritize the queue using a combination
d(v) ?
?
h(v)
of the known cost d(v) from the source vertex, and an estimate
?
h(v) of the
(future) cost from v to the target t:
h(v) =
{
1 v = t
?
??P (v,t)
w(?) v 6= t
(5)
where P (v, t) is the set of paths from v to t. In case where the estimate
?
h(v)
is admissible, namely, no worse than the true future cost h(v),
?
h(v) ? h(v) for all v,
8
8
we can prove that the optimality of d(t) when t is extracted still holds. Our
hope is that
d(t) ?
?
h(t) = d(t) ? 1 = d(t)
ranks higher among d(v) ?
?
h(v) and can be popped sooner. The Dijkstra
Algorithm is a special case of the A* Algorithm where
?
h(v) = 1 for all v.
4 Hypergraphs
Hypergraphs, as a generalization of graphs, have been extensively stud-
ied since 1970s as a powerful tool for modeling many problems in Discrete
Mathematics. In this report, we use directed hypergraphs (Gallo et al, 1993)
to abstract a hierarchically branching search space for dynamic program-
ming, where we solve a big problem by dividing it into (more than one)
sub-problems. Classical examples of these problems include matrix-chain
multiplication, optimal polygon triangulation, and optimal binary search
tree (Cormen et al, 2001).
Definition 12. A (directed) hypergraph is a pair H = ?V, E? with a set
R, where V is the set of vertices, E is the set of hyperedges, and R is the
set of weights. Each hyperedge e ? E is a triple e = ?T (e), h(e), f
e
?, where
h(e) ? V is its head vertex and T (e) ? V
?
is an ordered list of tail vertices.
f
e
is a weight function from R
|T (e)|
to R.
Note that our definition differs slightly from the classical definitions of
Gallo et al (1993) and Nielsen et al (2005) where the tails are sets rather
than ordered lists. In other words, we allow multiple occurrences of the same
vertex in a tail and there is an ordering among the components. We also
allow the head vertex to appear in the tail creating a self-loop which is ruled
out in (Nielsen et al, 2005).
Definition 13. We denote |e| = |T (e)| to be the arity of the hyperedge
5
.
If |e| = 0, then f
e
() ? R is a constant (f
e
is a nullary function) and we
call h(e) a source vertex. We define the arity of a hypergraph to be the
maximum arity of its hyperedges.
A hyperedge of arity one degenerates into an edge, and a hypergraph of
arity one is standard graph.
Similar to the case of graphs, in many applications presented below,
there is also a distinguished vertex t ? V called target vertex.
We can adapt the notions of backward- and forward-star to hypergraphs.
5
The arity of e is different from its cardinality defined in (Gallo et al, 1993; Nielsen et
al., 2005) which is |T (e)| + 1.
9
9
Definition 14. The backward-star BS (v) of a vertex v is the set of incoming
hyperedges {e ? E | h(e) = v}. The in-degree of v is |BS(v)|. The forward-
star FS (v) of a vertex v is the set of outgoing hyperedges {e ? E | v ? T (e)}.
The out-degree of v is |FS(v)|.
Definition 15. The graph projection of a hypergraph H = ?V, E, t,R? is a
directed graph G = ?V, E
?
? where
E
?
= {(u, v) | ?e ? BS(v), s.t. u ? T (e)}.
A hypergraph H is acyclic if its graph projection G is acyclic; then a topo-
logical ordering of H is an ordering of V that is a topological ordering in G.
4.1 Weight Functions and Semirings
We also extend the concepts of monotonicity and superiority from semirings
to hypergraphs.
Definition 16. A function f : R
m
7? R is monotonic with regarding to ,
if for all i ? 1..m
(a
i
 a
?
i
) ? f(a
1
, ? ? ? , a
i
, ? ? ? , a
m
)  f(a
1
, ? ? ? , a
?
i
, ? ? ? , a
m
).
Definition 17. A hypergraph H is monotonic if there is a total ordering 
on R such that every weight function f in H is monotonic with regarding
to . We can borrow the additive operator ? from semiring to define a
comparison operator
a ? b =
{
a a  b,
b otherwise.
In this paper we will assume this monotonicity, which corresponds to
the optimal substructure property in dynamic programming (Cormen et al,
2001).
Definition 18. A function f : R
m
7? R is superior if the result of function
application is worse than each of its argument:
?i ? 1..m, a
i
 f(a
1
, ? ? ? , a
i
, ? ? ? , a
m
).
A hypergraph H is superior if every weight function f in H is superior.
10
10
4.2 Derivations
To do optimization we need to extend the notion of paths in graphs to hy-
pergraphs. This is, however, not straightforward due to the assymmetry of
the head and the tail in a hyperedge and there have been multiple propos-
als in the literature. Here we follow the recursive definition of derivations
in (Huang and Chiang, 2005). See Section 6 for the alternative notion of
hyperpaths.
Definition 19. A derivation D of a vertex v in a hypergraph H, its size
|D| and its weight w(D) are recursively defined as follows:
? If e ? BS(v) with |e| = 0, then D = ?e, ? is a derivation of v, its size
|D| = 1, and its weight w(D) = f
e
().
? If e ? BS(v) where |e| > 0 and D
i
is a derivation of T
i
(e) for 1 ?
i ? |e|, then D = ?e, D
1
? ? ?D
|e|
? is a derivation of v, its size |D| =
1 +
?
|e|
i=1
|D
i
| and its weight w(D) = f
e
(w(D
1
), . . . , w(D
|e|
)).
The ordering on weights in R induces an ordering on derivations: D  D
?
iff w(D)  w(D
?
).
We denote D(v) to be the set of derivations of v and extend the best
weight in definition 11 to hypergraph:
Definition 20. The best weight ?(v) of a vertex v is the weight of the best
derivation of v:
?(v) =
{
1 v is a source vertex
?
D?D(v)
w(D) otherwise
(6)
4.3 Related Formalisms
Hypergraphs are closely related to other formalisms like AND/OR graphs,
context-free grammars, and deductive systems (Shieber et al, 1995; Neder-
hof, 2003).
In an AND/OR graph, the OR-nodes correspond to vertices in a hy-
pergraph and the AND-nodes, which links several OR-nodes to another
OR-node, correspond to a hyperedge. Similarly, in context-free grammars,
nonterminals are vertices and productions are hyperedges; in deductive sys-
tems, items are vertices and instantied deductions are hyperedges. Table 3
summarizes these correspondences. Obviously one can construct a corre-
sponding hypergraph for any given AND/OR graph, context-free grammar,
or deductive system. However, the hypergraph formulation provides greater
11
11
hypergraph AND/OR graph context-free grammar deductive system
vertex OR-node symbol item
source-vertex leaf OR-node terminal axiom
target-vertex root OR-node start symbol goal item
hyperedge AND-node production instantiated deduction
({u
1
, u
2
}, v, f) v
f
? u
1
u
2
u
1
: a u
2
: b
v : f(a, b)
Table 3: Correspondence between hypergraphs and related formalisms.
modeling flexibility than the weighted deductive systems of Nederhof (2003):
in the former we can have a separate weight function for each hyperedge,
where as in the latter, the weight function is defined for a deductive (tem-
plate) rule which corresponds to many hyperedges.
5 Dynamic Programming on Hypergraphs
Since hypergraphs with weight functions are generalizations of graphs with
semirings, we can extend the algorithms in Section 3 to the hypergraph case.
5.1 Generalized Viterbi Algorithm
The Viterbi Algorithm (Section 3.1) can be adapted to acyclic hypergraphs
almost without modification (see Algorithm 4 for pseudo-code).
Algorithm 4 Generalized Viterbi Algorithm.
1: procedure General-Viterbi(H)
2: topologically sort the vertices of H
3: Initialize(H)
4: for each vertex v in topological order do
5: for each hyperedge e in BS (v) do
6: e is ({u
1
, u
2
, ? ? ? , u
|e|
}, v, f
e
)
7: d(v)? = f
e
(d(u
1
), d(u
2
), ? ? ? , d(u
|e|
))
The correctness of this algorithm can be proved by a similar induction.
Its time complexity is O(V + E) since every hyperedge is visited exactly
once (assuming the arity of the hypergraph is a constant).
The forward-update version of this algorithm, however, is not as trivial
as the graph case. This is because the tail of a hyperedge now contains
several vertices and thus the forward- and backward-stars are no longer
symmetric. The naive adaption would end up visiting a hyperedge many
12
12
times. To ensure that a hyperedge e is fired only when all of its tail vertices
have been fixed to their best weights, we maintain a counter r[e] of the
remaining vertices yet to be fixed (line 5) and fires the update rule for e
when r[e] = 0 (line 9). This method is also used in the Knuth algorithm
(Section 5.2).
Algorithm 5 Forward update version of Algorithm 4.
1: procedure General-Viterbi-Forward(H)
2: topologically sort the vertices of H
3: Initialize(H)
4: for each hyperedge e do
5: r[e] ? |e|  counter of remaining tails to be fixed
6: for each vertex v in topological order do
7: for each hyperedge e in FS (v) do
8: r[e] ? r[e] ? 1
9: if r[e] == 0 then  all tails have been fixed
10: e is ({u
1
, u
2
, ? ? ? , u
|e|
}, h(e), f
e
)
11: d(h(e))? = f
e
(d(u
1
), d(u
2
), ? ? ? , d(u
|e|
))
5.1.1 CKY Algorithm
The most widely used algorithm for parsing in NLP, the CKY algorithm
(Kasami, 1965), is a specific instance of the Viterbi algorithm for hyper-
graphs. The CKY algorithm takes a context-free grammar G in Chomsky
Normal Form (CNF) and essentially intersects G with a DFA D representing
the input sentence to be parsed. The resulting search space by this intersec-
tion is an acyclic hypergraph whose vertices are items like (X, i, j) and whose
hyperedges are instantiated deductive steps like ({(Y, i, k)(Z, k, j)}, (X, i, j), f)
for all i < k < j if there is a production X ? Y Z. The weight function f is
simply
f(a, b) = a ? b ? w(X ? Y Z).
The Chomsky Normal Form ensures acyclicity of the hypergraph but
there are multiple topological orderings which result in different variants of
the CKY algorithm, e.g., bottom-up CKY, left-to-right CKY, and right-to-
left CKY, etc.
5.2 Knuth Algorithm
Knuth (1977) generalizes the Dijkstra algorithm to what he calls the gram-
mar problem, which essentially corresponds to the search problem in a mono-
tonic superior hypergraph (see Table 3). However, he does not provide
13
13
an efficient implementation nor analysis of complexity. Graehl and Knight
(2004) present an implementation that runs in time O(V log V + E) using
the method described in Algorithm 5 to ensure that every hyperedge is vis-
ited only once (assuming the priority queue is implemented as a Fibonaaci
heap; for binary heap, it runs in O((V + E) log V )).
Algorithm 6 Knuth Algorithm.
1: procedure Knuth(H)
2: Initialize(H)
3: Q ? V [H]  prioritized by d-values
4: for each hyperedge e do
5: r[e] ? |e|
6: while Q 6= ? do
7: v ? Extract-Min(Q)
8: for each edge e in FS (v) do
9: e is ({u
1
, u
2
, ? ? ? , u
|e|
}, h(e), f
e
)
10: r[e] ? r[e] ? 1
11: if r[e] == 0 then
12: d(h(e))? = f
e
(d(u
1
), d(u
2
), ? ? ? , d(u
|e|
))
13: Decrease-Key(Q, h(e))
5.2.1 A* Algorithm on Hypergraphs
We can also extend the A* idea to hypergraphs to speed up the Knuth
Algorithm. A specific case of this algorithm is the A* parsing of Klein
and Manning (2003) where they achieve significant speed up using carefully
designed heuristic functions. More formally, we first need to extend the
concept of (exact) outside cost from Eq. 5:
?(v) =
{
1 v = t
?
D?D(v,t)
w(D) v 6= t
(7)
where D(v, t) is the set of (partial) derivations using v as a leaf node.
This outside cost can be computed from top-down following the inverse
topological order: for each vertex v, for each incoming hyperedge e =
({u
1
, . . . , u
|e|
}, v, f
e
) ? BS (v), we update
?(u
i
) ? = f
e
(d(u
1
) . . . d(u
i?1
), ?(v), d(u
i+1
) . . . d(u
|e|
)) for each i.
Basically we replace d(u
i
) by ?(v) for each i. In case weight functions are
composed of semiring operations, as in shortest paths (+) or probabilistic
14
14
grammars (?), this definition makes sense, but for general weight functions
there should be some formal requirements to make the definition sound.
However, this topic is beyond the scope of this paper.
6 Extensions and Discussions
In most of the above we focus on optimization problems where one aims
to find the best solution. Here we consider two extensions of this scheme:
non-optimization problems where the goal is often to compute the summa-
tion or closure, and k-best problems where one also searches for the 2nd,
3rd, through kth-best solutions. Both extensions have many applications
in NLP. For the former, algorithms based on the Inside semiring (Table 1),
including the forward-backward algorithm (Baum, 1972) and Inside-Outside
algorithm (Baker, 1979; Lari and Young, 1990) are widely used for unsu-
pervised training with the EM algorithm (Dempster et al, 1977). For the
latter, since NLP is often a pipeline of several modules, where the 1-best
solution from one module might not be the best input for the next module,
and one prefers to postpone disambiguation by propogating a k-best list of
candidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and John-
son, 2005; Huang and Chiang, 2005). The k-best list is also frequently used
in discriminative learning to approximate the whole set of candidates which
is usually exponentially large (Och, 2003; McDonald et al, 2005).
6.1 Beyond Optimization Problems
We know that in optimization problems, the criteria for using dynamic pro-
gramming is monotonicity (definitions 6 and 16). But in non-optimization
problems, since there is no comparison, this criteria is no longer applica-
ble. Then when can we apply dynamic programming to a non-optimization
problem?
Cormen et al (1990) develop a more general criteria of closed semir-
ing where ? is idempotent and infinite sums are well-defined and present a
more sophisticated algorithm that can be proved to work for all closed semir-
ings. This definition is still not general enough since many non-optimization
semirings including the Inside semiring are not even idempotent. Mohri
(2002) solves this problem by a slightly different definition of closedness
which does not assume idempotence. His generic single-source algorithm
subsumes many classical algorithms like Dijkstra, Bellman-Ford (Bellman,
1958), and Viterbi as specific instances.
It remains an open problem how to extend the closedness definition to
the case of weight functions in hypergraphs.
15
15
6.2 k-best Extensions
The straightforward extension from 1-best to k-best is to simply replace the
old semiring (A,?,?, 0, 1) by its k-best version (A
k
,?
k
,?
k
, 0
k
, 1
k
) where
each element is now a vector of length k, with the ith component represent
the ith-best value. Since ? is a comparison, we can define ?
k
to be the
top-k elements of the 2k elements from the two vectors, and ?
k
the top-k
elements of the k
2
elements from the cross-product of two vectors:
a ?
k
b = ?
?
k
({a
i
| 1 ? i ? k} ? {b
j
| 1 ? j ? k})
a ?
k
b = ?
?
k
{a
i
? b
j
| 1 ? i, j ? k}
where ?
?
k
returns the ordered list of the top-k elements in a set. A similar
construction is obvious for the weight functions in hypergraphs.
Now we can re-use the 1-best Viterbi Algorithm to solve the k-best
problem in a generic way, as is done in (Mohri, 2002). However, some more
sophisticated techniques that breaks the modularity of semirings results in
much faster k-best algorithms. For example, the Recursive Enumeration Al-
gorithm (REA) (Jime?nez and Marzal, 1999) uses a lazy computation method
on top of the Viterbi algorithm to efficiently compute the ith-best solution
based on the 1st, 2nd, ..., (i ? 1)th solutions. A simple k-best Dijkstra
algorithm is described in (Mohri and Riley, 2002).
For the hypergraph case, the REA algorithm has been adapted for k-best
derivations (Jime?nez and Marzal, 2000; Huang and Chiang, 2005). Applica-
tions of this algorithm include k-best parsing (McDonald et al, 2005; Mohri
and Roark, 2006) and machine translation (Chiang, 2007). It is also imple-
mented as part of Dyna (Eisner et al, 2005), a generic langauge for dynamic
programming. The k-best extension of the Knuth Algorithm is studied by
Huang (2005). A separate problem, k-shortest hyperpaths, has been studied
by Nielsen et al (2005).
Eppstein (2001) compiles an annotated bibliography for k-shortest-path
and other related k-best problems.
7 Conclusion
This report surveys two frameworks for formalizing dynamic programming
and presents two important classes of DP algorithms under these frame-
works. We focused on 1-best optimization problems but also discussed other
scenarios like non-optimization problems and k-best solutions. We believe
that a better understanding of the theoretical foundations of DP is benefitial
for NLP researchers.
16
16
References
Baker, James K. 1979. Trainable grammars for speech recognition. In Proceedings
of the Spring Conference of the Acoustical Society of America, pages 547?550.
Baum, L. E. 1972. An inequality and associated maximization technique in
statistical estimation of probabilistic functions of a markov process.
Inequalities, (3).
Bellman, Richard. 1957. Dynamic Programming. Princeton University Press.
Bellman, Richard. 1958. On a routing problem. Quarterly of Applied
Mathematics, (16).
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine-grained n-best parsing
and discriminative reranking. In Proceedings of the 43rd ACL.
Chiang, David. 2007. Hierarchical phrase-based translation. In Computational
Linguistics. To appear.
Collins, Michael. 2000. Discriminative reranking for natural language parsing. In
Proceedings of ICML, pages 175?182.
Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. 1990.
Introduction to Algorithms. MIT Press, first edition.
Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
2001. Introduction to Algorithms. MIT Press, second edition.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society,
Series B, 39:1?38.
Dijkstra, Edsger W. 1959. A note on two problems in connexion with graphs.
Numerische Mathematik, (1):267?271.
Eisner, Jason, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling:
Weighted dynamic programming and the dyna language. In Proceedings of
HLT-EMNLP.
Eppstein, David. 2001. Bibliography on k shortest paths and other ?k best
solutions? problems. http://www.ics.uci.edu/?eppstein/bibs/kpath.bib.
Gallo, Giorgio, Giustino Longo, and Stefano Pallottino. 1993. Directed
hypergraphs and applications. Discrete Applied Mathematics, 42(2):177?201.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Graehl, Jonathan and Kevin Knight. 2004. Training tree transducers. In
HLT-NAACL, pages 105?112.
Hart, P. E., N. J. Nilsson, and B. Raphael. 1968. A formal basis for the heuristic
determination of minimum cost paths. IEEE Transactions on Systems Science
and Cybernetics, 4(2):100?107.
Huang, Liang. 2005. k-best Knuth algorithm and k-best A* parsing. Unpublished
manuscript.
Huang, Liang and David Chiang. 2005. Better k-best Parsing. In Proceedings of
the Ninth International Workshop on Parsing Technologies (IWPT-2005).
17
17
Jime?nez, V??ctor and Andre?s Marzal. 1999. Computing the k shortest paths: A
new algorithm and an experimental comparison. In Algorithm Engineering,
pages 15?29.
Jime?nez, V??ctor M. and Andre?s Marzal. 2000. Computation of the n best parse
trees for weighted and stochastic context-free grammars. In Proc. of the Joint
IAPR International Workshops on Advances in Pattern Recognition.
Kasami, T. 1965. An efficient recognition and syntax analysis algorithm for
context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge
Research Laboratory, Bedford, MA?.
Klein, Dan and Chris Manning. 2003. A* parsing: Fast exact Viterbi parse
selection. In Proceedings of HLT-NAACL.
Knuth, Donald. 1977. A generalization of Dijkstra?s algorithm. Information
Processing Letters, 6(1).
Kuich, W. and A. Salomaa. 1986. Semirings, Automata, Languages. Number 5 in
EATCS Monographs on Theoretical Computer Science. Springer-Verlag, Berlin,
Germany.
Lari, K. and S. J. Young. 1990. The estimation of stochastic context-free
grammars using the inside-outside algorithm. Computer Speech and Language,
4:35?56.
Lawler, E. L. 1976. Combinatorial Optimization: Networks and Matroids. Holt,
Rinehart, and Winston.
Manning, Chris and Hinrich Schu?tze. 1999. Foundations of Statistical Natural
Language Processing. MIT Press.
McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online
large-margin training of dependency parsers. In Proceedings of the 43rd ACL.
Mohri, Mehryar. 2002. Semiring frameworks and algorithms for shortest-distance
problems. Journal of Automata, Languages and Combinatorics, 7(3):321?350.
Mohri, Mehryar and Michael Riley. 2002. An efficient algorithm for the
n-best-strings problem. In Proceedings of the International Conference on
Spoken Language Processing 2002 (ICSLP ?02), Denver, Colorado, September.
Mohri, Mehryar and Brian Roark. 2006. Probabilistic context-free grammar
induction based on structural zeros. In Proceedings of HLT-NAACL.
Nederhof, Mark-Jan. 2003. Weighted deductive parsing and Knuth?s algorithm.
29(1):135?143.
Nielsen, Lars Relund, Kim Allan Andersen, and Daniele Pretolani. 2005. Finding
the k shortest hyperpaths. Computers and Operations Research.
Och, Franz Joseph. 2003. Minimum error rate training in statistical machine
translation. In Proceedings of ACL, pages 160?167.
Shieber, Stuart, Yves Schabes, and Fernando Pereira. 1995. Principles and
implementation of deductive parsing. Journal of Logic Programming, 24:3?36.
Viterbi, Andrew J. 1967. Error bounds for convolutional codes and an
asymptotically optimum decoding algorithm. IEEE Transactions on
Information Theory, IT-13(2):260?269, April.
18
18
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 256?263,
New York, June 2006. c?2006 Association for Computational Linguistics
Synchronous Binarization for Machine Translation
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Kevin Knight
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Abstract
Systems based on synchronous grammars
and tree transducers promise to improve
the quality of statistical machine transla-
tion output, but are often very computa-
tionally intensive. The complexity is ex-
ponential in the size of individual gram-
mar rules due to arbitrary re-orderings be-
tween the two languages, and rules ex-
tracted from parallel corpora can be quite
large. We devise a linear-time algorithm
for factoring syntactic re-orderings by bi-
narizing synchronous rules when possible
and show that the resulting rule set signif-
icantly improves the speed and accuracy
of a state-of-the-art syntax-based machine
translation system.
1 Introduction
Several recent syntax-based models for machine
translation (Chiang, 2005; Galley et al, 2004) can
be seen as instances of the general framework of
synchronous grammars and tree transducers. In this
framework, both alignment (synchronous parsing)
and decoding can be thought of as parsing problems,
whose complexity is in general exponential in the
number of nonterminals on the right hand side of a
grammar rule. To alleviate this problem, we investi-
gate bilingual binarization to factor the synchronous
grammar to a smaller branching factor, although it is
not guaranteed to be successful for any synchronous
rule with arbitrary permutation. In particular:
? We develop a technique called synchronous bi-
narization and devise a fast binarization algo-
rithm such that the resulting rule set alows ef-
ficient algorithms for both synchronous parsing
and decoding with integrated n-gram language
models.
? We examine the effect of this binarization
method on end-to-end machine translation
quality, compared to a more typical baseline
method.
? We examine cases of non-binarizable rules in a
large, empirically-derived rule set, and we in-
vestigate the effect on translation quality when
excluding such rules.
Melamed (2003) discusses binarization of multi-
text grammars on a theoretical level, showing the
importance and difficulty of binarization for efficient
synchronous parsing. One way around this diffi-
culty is to stipulate that all rules must be binary
from the outset, as in inversion-transduction gram-
mar (ITG) (Wu, 1997) and the binary synchronous
context-free grammar (SCFG) employed by the Hi-
ero system (Chiang, 2005) to model the hierarchical
phrases. In contrast, the rule extraction method of
Galley et al (2004) aims to incorporate more syn-
tactic information by providing parse trees for the
target language and extracting tree transducer rules
that apply to the parses. This approach results in
rules with many nonterminals, making good bina-
rization techniques critical.
Suppose we have the following SCFG, where su-
perscripts indicate reorderings (formal definitions of
256
S
NP
Baoweier
PP
yu
Shalong
VP
juxing le
huitan
S
NP
Powell
VP
held
a meeting
PP
with
Sharon
Figure 1: A pair of synchronous parse trees in the
SCFG (1). The dashed curves indicate pairs of syn-
chronous nonterminals (and sub trees).
SCFGs can be found in Section 2):
(1)
S? NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
NP? Powell, Baoweier
VP? held a meeting, juxing le huitan
PP? with Sharon, yu Shalong
Decoding can be cast as a (monolingual) parsing
problem since we only need to parse the source-
language side of the SCFG, as if we were construct-
ing a CFG projected on Chinese out of the SCFG.
The only extra work we need to do for decoding
is to build corresponding target-language (English)
subtrees in parallel. In other words, we build syn-
chronous trees when parsing the source-language in-
put, as shown in Figure 1.
To efficiently decode with CKY, we need to bi-
narize the projected CFG grammar.1 Rules can be
binarized in different ways. For example, we could
binarize the first rule left to right or right to left:
S? VNP-PP VP
VNP-PP? NP PP or
S? NP VPP-VP
VPP-VP ? PP VP
We call those intermediate symbols (e.g. VPP-VP) vir-
tual nonterminals and corresponding rules virtual
rules, whose probabilities are all set to 1.
These two binarizations are no different in the
translation-model-only decoding described above,
just as in monolingual parsing. However, in the
source-channel approach to machine translation, we
need to combine probabilities from the translation
model (an SCFG) with the language model (an n-
gram), which has been shown to be very impor-
tant for translation quality (Chiang, 2005). To do
bigram-integrated decoding, we need to augment
each chart item (X, i, j) with two target-language
1Other parsing strategies like the Earley algorithm use an
internal binary representation (e.g. dotted-rules) of the original
grammar to ensure cubic time complexity.
boundary words u and v to produce a bigram-item
like
( u ??? vX
i j
)
, following the dynamic program-
ming algorithm of Wu (1996).
Now the two binarizations have very different ef-
fects. In the first case, we first combine NP with PP:
( Powell ??? PowellNP
1 2
)
: p
( with ??? SharonPP
2 4
)
: q
( Powell ??? Powell ??? with ??? Sharon
VNP-PP
1 4
)
: pq
where p and q are the scores of antecedent items.
This situation is unpleasant because in the target-
language NP and PP are not contiguous so we can-
not apply language model scoring when we build the
VNP-PP item. Instead, we have to maintain all fourboundary words (rather than two) and postpone the
language model scoring till the next step where VNP-PP
is combined with ( held ??? meetingVP
2 4
) to form an S item.
We call this binarization method monolingual bina-
rization since it works only on the source-language
projection of the rule without respecting the con-
straints from the other side.
This scheme generalizes to the case where we
have n nonterminals in a SCFG rule, and the decoder
conservatively assumes nothing can be done on lan-
guage model scoring (because target-language spans
are non-contiguous in general) until the real nonter-
minal has been recognized. In other words, target-
language boundary words from each child nonter-
minal of the rule will be cached in all virtual non-
terminals derived from this rule. In the case of
m-gram integrated decoding, we have to maintain
2(m ? 1) boundary words for each child nontermi-
nal, which leads to a prohibitive overall complex-
ity of O(|w|3+2n(m?1)), which is exponential in rule
size (Huang et al, 2005). Aggressive pruning must
be used to make it tractable in practice, which in
general introduces many search errors and adversely
affects translation quality.
In the second case, however:
( with ??? SharonPP
2 4
)
: r
( held ??? meetingVP
4 7
)
: s
( held ??? Sharon
VPP-VP
2 7
)
: rs ? Pr(with | meeting)
Here since PP and VP are contiguous (but
swapped) in the target-language, we can include the
257
NP
NP
PP
VP
VP
PP
target (English)
source (Chinese)
VPP-VP
NP
PP
VP
Chinese indices
English
boundary
w
o
rds 1 2 4 7Powell
Powellheld
meetingwith
Sharon
VPP-VP
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the synchronous production.
language model score by adding Pr(with | meeting),
and the resulting item again has two boundary
words. Later we add Pr(held | Powell) when the
resulting item is combined with ( Powell ??? PowellNP
1 2
) to
form an S item. As illustrated in Figure 2, VPP-VP hascontiguous spans on both source and target sides, so
that we can generate a binary-branching SCFG:
(2) S? NP(1) VPP-VP(2), NP(1) VPP-VP(2)VPP-VP ? VP(1) PP(2), PP(2) VP(1)
In this case m-gram integrated decoding can be
done in O(|w|3+4(m?1)) time which is much lower-
order polynomial and no longer depends on rule size
(Wu, 1996), allowing the search to be much faster
and more accurate facing pruning, as is evidenced in
the Hiero system of Chiang (2005) where he restricts
the hierarchical phrases to be a binary SCFG. The
benefit of binary grammars also lies in synchronous
parsing (alignment). Wu (1997) shows that parsing
a binary SCFG is in O(|w|6) while parsing SCFG is
NP-hard in general (Satta and Peserico, 2005).
The same reasoning applies to tree transducer
rules. Suppose we have the following tree-to-string
rules, following Galley et al (2004):
(3)
S(x0:NP, VP(x2:VP, x1:PP))? x0 x1 x2NP(NNP(Powell))? Baoweier
VP(VBD(held), NP(DT(a) NPS(meeting)))
? juxing le huitan
PP(TO(with), NP(NNP(Sharon)))? yu Shalong
where the reorderings of nonterminals are denoted
by variables xi.Notice that the first rule has a multi-level left-
hand side subtree. This system can model non-
isomorphic transformations on English parse trees
to ?fit? another language, for example, learning that
the (S (V O)) structure in English should be trans-
formed into a (V S O) structure in Arabic, by look-
ing at two-level tree fragments (Knight and Graehl,
2005). From a synchronous rewriting point of view,
this is more akin to synchronous tree substitution
grammar (STSG) (Eisner, 2003). This larger locality
is linguistically motivated and leads to a better pa-
rameter estimation. By imagining the left-hand-side
trees as special nonterminals, we can virtually cre-
ate an SCFG with the same generative capacity. The
technical details will be explained in Section 3.2.
In general, if we are given an arbitrary syn-
chronous rule with many nonterminals, what are the
good decompositions that lead to a binary grammar?
Figure 2 suggests that a binarization is good if ev-
ery virtual nonterminal has contiguous spans on both
sides. We formalize this idea in the next section.
2 Synchronous Binarization
A synchronous CFG (SCFG) is a context-free
rewriting system for generating string pairs. Each
rule (synchronous production) rewrites a nontermi-
nal in two dimensions subject to the constraint that
the sequence of nonterminal children on one side is
a permutation of the nonterminal sequence on the
other side. Each co-indexed child nonterminal pair
will be further rewritten as a unit.2 We define the
language L(G) produced by an SCFG G as the pairs
of terminal strings produced by rewriting exhaus-
tively from the start symbol.
As shown in Section 3.2, terminals do not play
an important role in binarization. So we now write
rules in the following notation:
X ? X(1)1 ...X(n)n , X
(pi(1))
pi(1) ...X
(pi(n))
pi(n)
where each Xi is a variable which ranges over non-terminals in the grammar and pi is the permutation
of the rule. We also define an SCFG rule as n-ary
if its permutation is of n and call an SCFG n-ary if
its longest rule is n-ary. Our goal is to produce an
equivalent binary SCFG for an input n-ary SCFG.
2In making one nonterminal play dual roles, we follow the
definitions in (Aho and Ullman, 1972; Chiang, 2005), origi-
nally known as Syntax Directed Translation Schema (SDTS).
An alternative definition by Satta and Peserico (2005) allows
co-indexed nonterminals taking different symbols in two di-
mensions. Formally speaking, we can construct an equivalent
SDTS by creating a cross-product of nonterminals from two
sides. See (Satta and Peserico, 2005, Sec. 4) for other details.
258
(2,3,5,4)
(2,3)
2 3
(5,4)
5 4
(2,3,5,4)
2 (3,5,4)
3 (5,4)
5 4
(a) (b) (c)
Figure 3: (a) and (b): two binarization patterns
for (2, 3, 5, 4). (c): alignment matrix for the non-
binarizable permuted sequence (2, 4, 1, 3)
However, not every SCFG can be binarized. In
fact, the binarizability of an n-ary rule is determined
by the structure of its permutation, which can some-
times be resistant to factorization (Aho and Ullman,
1972). So we now start to rigorously define the bi-
narizability of permutations.
2.1 Binarizable Permutations
A permuted sequence is a permutation of consec-
utive integers. For example, (3, 5, 4) is a permuted
sequence while (2, 5) is not. As special cases, single
numbers are permuted sequences as well.
A sequence a is said to be binarizable if it is a
permuted sequence and either
1. a is a singleton, i.e. a = (a), or
2. a can be split into two sub sequences, i.e.
a = (b; c), where b and c are both binarizable
permuted sequences. We call such a division
(b; c) a binarizable split of a.
This is a recursive definition. Each binarizable
permuted sequence has at least one hierarchical bi-
narization pattern. For instance, the permuted se-
quence (2, 3, 5, 4) is binarizable (with two possible
binarization patterns) while (2, 4, 1, 3) is not (see
Figure 3).
2.2 Binarizable SCFG
An SCFG is said to be binarizable if the permu-
tation of each synchronous production is binariz-
able. We denote the class of binarizable SCFGs as
bSCFG. This set represents an important subclass
of SCFG that is easy to handle (parsable in O(|w|6))
and covers many interesting longer-than-two rules.3
3Although we factor the SCFG rules individually and de-
fine bSCFG accordingly, there are some grammars (the dashed
SCFG bSCFG SCFG-2
O(|w|6) parsable
Figure 4: Subclasses of SCFG. The thick arrow de-
notes the direction of synchronous binarization. For
clarity reasons, binary SCFG is coded as SCFG-2.
Theorem 1. For each grammar G in bSCFG, there
exists a binary SCFG G?, such that L(G?) = L(G).
Proof. Once we decompose the permutation of n
in the original rule into binary permutations, all
that remains is to decorate the skeleton binary parse
with nonterminal symbols and attach terminals to
the skeleton appropriately. We explain the technical
details in the next section.
3 Binarization Algorithms
We have reduced the problem of binarizing an SCFG
rule into the problem of binarizing its permutation.
This problem can be cast as an instance of syn-
chronous ITG parsing (Wu, 1997). Here the parallel
string pair that we are parsing is the integer sequence
(1...n) and its permutation (pi(1)...pi(n)). The goal
of the ITG parsing is to find a synchronous tree that
agrees with the alignment indicated by the permu-
tation. In fact, as demonstrated previously, some
permutations may have more than one binarization
patterns among which we only need one. Wu (1997,
Sec. 7) introduces a non-ambiguous ITG that prefers
left-heavy binary trees so that for each permutation
there is a unique synchronous derivation (binariza-
tion pattern).
However, this problem has more efficient solu-
tions. Shapiro and Stephens (1991, p. 277) infor-
mally present an iterative procedure where in each
pass it scans the permuted sequence from left to right
and combines two adjacent sub sequences whenever
possible. This procedure produces a left-heavy bi-
narization tree consistent with the unambiguous ITG
and runs in O(n2) time since we need n passes in the
worst case. We modify this procedure and improve
circle in Figure 4), which can be binarized only by analyzing
interactions between rules. Below is a simple example:
S? X(1) X(2) X(3) X(4), X(2) X(4) X(1) X(3)
X? a , a
259
iteration stack input action
1 5 3 4 2
1 5 3 4 2 shift
1 1 5 3 4 2 shift
2 1 5 3 4 2 shift
3 1 5 3 4 2 shift
1 5 3-4 2 reduce [3, 4]
1 3-5 2 reduce ?5, [3, 4]?
4 1 3-5 2 shift
1 2-5 reduce ?2, ?5, [3, 4]??
1-5 reduce [1, ?2, ?5, [3, 4]??]
Figure 5: Example of Algorithm 1 on the input
(1, 5, 3, 4, 2). The rightmost column shows the
binarization-trees generated at each reduction step.
it into a linear-time shift-reduce algorithm that only
needs one pass through the sequence.
3.1 The linear-time skeleton algorithm
The (unique) binarization tree bi(a) for a binariz-
able permuted sequence a is recursively defined as
follows:
? if a = (a), then bi(a) = a;
? otherwise let a = (b; c) to be the rightmost
binarizable split of a. then
bi(a) =
{
[bi(b), bi(c)] b1 < c1
?bi(b), bi(c)? b1 > c1.
For example, the binarization tree for (2, 3, 5, 4)
is [[2, 3], ?5, 4?], which corresponds to the binariza-
tion pattern in Figure 3(a). We use [] and ?? for
straight and inverted combinations respectively, fol-
lowing the ITG notation (Wu, 1997). The rightmost
split ensures left-heavy binary trees.
The skeleton binarization algorithm is an instance
of the widely used left-to-right shift-reduce algo-
rithm. It maintains a stack for contiguous subse-
quences discovered so far, like 2-5, 1. In each it-
eration, it shifts the next number from the input and
repeatedly tries to reduce the top two elements on
the stack if they are consecutive. See Algorithm 1
for details and Figure 5 for an example.
Theorem 2. Algorithm 1 succeeds if and only if the
input permuted sequence a is binarizable, and in
case of success, the binarization pattern recovered
is the binarization tree of a.
Proof. ?: it is obvious that if the algorithm suc-
ceeds then a is binarizable using the binarization
pattern recovered.
?: by a complete induction on n, the length of a.
Base case: n = 1, trivial.
Assume it holds for all n? < n.
If a is binarizable, then let a = (b; c) be its right-
most binarizable split. By the induction hypothesis,
the algorithm succeeds on the partial input b, reduc-
ing it to the single element s[0] on the stack and re-
covering its binarization tree bi(b).
Let c = (c1; c2). If c1 is binarizable and trig-gers our binarizer to make a straight combination
of (b; c1), based on the property of permutations, itmust be true that (c1; c2) is a valid straight concate-nation. We claim that c2 must be binarizable in thissituation. So, (b, c1; c2) is a binarizable split to theright of the rightmost binarizable split (b; c), which
is a contradiction. A similar contradiction will arise
if b and c1 can make an inverted concatenation.
Therefore, the algorithm will scan through the
whole c as if from the empty stack. By the in-
duction hypothesis again, it will reduce c into s[1]
on the stack and recover its binarization tree bi(c).
Since b and c are combinable, the algorithm re-
duces s[0] and s[1] in the last step, forming the bi-
narization tree for a, which is either [bi(b), bi(c)] or
?bi(b), bi(c)?.
The running time of Algorithm 1 is linear in n, the
length of the input sequence. This is because there
are exactly n shifts and at most n?1 reductions, and
each shift or reduction takes O(1) time.
3.2 Binarizing tree-to-string transducers
Without loss of generality, we have discussed how
to binarize synchronous productions involving only
nonterminals through binarizing the corresponding
skeleton permutations. We still need to tackle a few
technical problems in the actual system.
First, we are dealing with tree-to-string trans-
ducer rules. We view each left-hand side subtree
as a monolithic nonterminal symbol and factor each
transducer rule into two SCFG rules: one from
the root nonterminal to the subtree, and the other
from the subtree to the leaves. In this way we can
uniquely reconstruct the tree-to-string derivation us-
ing the two-step SCFG derivation. For example,
260
Algorithm 1 The Linear-time Binarization Algorithm
1: function BINARIZABLE(a)
2: top? 0 . stack top pointer
3: PUSH(a1, a1) . initial shift4: for i? 2 to |a| do . for each remaining element
5: PUSH(ai, ai) . shift6: while top > 1 and CONSECUTIVE(s[top], s[top? 1]) do . keep reducing if possible
7: (p, q)? COMBINE(s[top], s[top? 1])
8: top? top? 2
9: PUSH(p, q)
10: return (top = 1) . if reduced to a single element then the input is binarizable, otherwise not
11: function CONSECUTIVE((a, b), (c, d))
12: return (b = c? 1) or (d = a? 1) . either straight or inverted
13: function COMBINE((a, b), (c, d))
14: return (min(a, c), max(b, d))
consider the following tree-to-string rule:
ADJP
x0:RB JJ
responsible
PP
IN
for
NP-C
NPB
DT
the
x2:NN
x1:PP
? x0 fuze x1 de x2
We create a specific nonterminal, say, T859, whichis a unique identifier for the left-hand side subtree
and generate the following two SCFG rules:
ADJP ? T859 (1), T859 (1)
T859 ? RB
(1) resp. for the NN(2) PP(3),
RB(1) fuze PP(3) de NN(2)
Second, besides synchronous nonterminals, ter-
minals in the two languages can also be present, as
in the above example. It turns out we can attach the
terminals to the skeleton parse for the synchronous
nonterminal strings quite freely as long as we can
uniquely reconstruct the original rule from its binary
parse tree. In order to do so we need to keep track of
sub-alignments including both aligned nonterminals
and neighboring terminals.
When binarizing the second rule above, we first
run the skeleton algorithm to binarize the under-
lying permutation (1, 3, 2) to its binarization tree
[1, ?3, 2?]. Then we do a post-order traversal to the
skeleton tree, combining Chinese terminals (one at
a time) at the leaf nodes and merging English termi-
nals greedily at internal nodes:
[1, ?3, 2?]
1 ?3, 2?
3 2
?
T859 [1,?3,2?]
V[RB, fuze]1
RB fuze
V?V[PP, de], resp. for the NN??3,2?
V[PP, de]3
PP de
NN2
A pre-order traversal of the decorated binarization
tree gives us the following binary SCFG rules:
T859 ? V1(1) V2(2), V1(1) V2(2)
V1 ? RB(1), RB(1) fuze
V2 ? resp. for the NN(1) V(2)3 , V(2)3 NN(1)V3 ? PP(1), PP(1) de
where the virtual nonterminals are:
V1: V[RB, fuze]V2: V?V[PP, de], resp. for the NN?V3: V[PP, de]
Analogous to the ?dotted rules? in Earley pars-
ing for monolingual CFGs, the names we create
for the virtual nonterminals reflect the underlying
sub-alignments, ensuring intermediate states can be
shared across different tree-to-string rules without
causing ambiguity.
The whole binarization algorithm still runs in time
linear in the number of symbols in the rule (includ-
ing both terminals and nonterminals).
4 Experiments
In this section, we answer two empirical questions.
261
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
 0  5  10  15  20  25  30  35  40
 0
 20
 40
 60
 80
 100
# 
of
 ru
le
s
pe
rc
en
ta
ge
 (%
)
length
Figure 6: The solid-line curve represents the distribution of all rules against permutation lengths. The
dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set while the dotted-line
denotes that percentage among all permutations.
4.1 How many rules are binarizable?
It has been shown by Shapiro and Stephens (1991)
and Wu (1997, Sec. 4) that the percentage of binariz-
able cases over all permutations of length n quickly
approaches 0 as n grows (see Figure 6). However,
for machine translation, it is more meaningful to
compute the ratio of binarizable rules extracted from
real text. Our rule set is obtained by first doing word
alignment using GIZA++ on a Chinese-English par-
allel corpus containing 50 million words in English,
then parsing the English sentences using a variant
of Collins parser, and finally extracting rules using
the graph-theoretic algorithm of Galley et al (2004).
We did a ?spectrum analysis? on the resulting rule
set with 50,879,242 rules. Figure 6 shows how the
rules are distributed against their lengths (number
of nonterminals). We can see that the percentage
of non-binarizable rules in each bucket of the same
length does not exceed 25%. Overall, 99.7% of
the rules are binarizable. Even for the 0.3% non-
binarizable rules, human evaluations show that the
majority of them are due to alignment errors. It is
also interesting to know that 86.8% of the rules have
monotonic permutations, i.e. either taking identical
or totally inverted order.
4.2 Does synchronous binarizer help decoding?
We did experiments on our CKY-based decoder with
two binarization methods. It is the responsibility of
the binarizer to instruct the decoder how to compute
the language model scores from children nontermi-
nals in each rule. The baseline method is mono-
lingual left-to-right binarization. As shown in Sec-
tion 1, decoding complexity with this method is ex-
ponential in the size of the longest rule and since we
postpone all the language model scorings, pruning
in this case is also biased.
system bleu
monolingual binarization 36.25
synchronous binarization 38.44
alignment-template system 37.00
Table 1: Syntax-based systems vs. ATS
To move on to synchronous binarization, we first
did an experiment using the above baseline system
without the 0.3% non-binarizable rules and did not
observe any difference in BLEU scores. So we
safely move a step further, focusing on the binariz-
able rules only.
The decoder now works on the binary translation
rules supplied by an external synchronous binarizer.
As shown in Section 1, this results in a simplified de-
coder with a polynomial time complexity, allowing
less aggressive and more effective pruning based on
both translation model and language model scores.
We compare the two binarization schemes in
terms of translation quality with various pruning
thresholds. The rule set is that of the previous sec-
tion. The test set has 116 Chinese sentences of no
longer than 15 words. Both systems use trigram as
the integrated language model. Figure 7 demon-
strates that decoding accuracy is significantly im-
proved after synchronous binarization. The number
of edges proposed during decoding is used as a mea-
sure of the size of search space, or time efficiency.
Our system is consistently faster and more accurate
than the baseline system.
We also compare the top result of our syn-
chronous binarization system with the state-of-the-
art alignment-template approach (ATS) (Och and
Ney, 2004). The results are shown in Table 1. Our
system has a promising improvement over the ATS
262
 33.5
 34.5
 35.5
 36.5
 37.5
 38.5
 3e+09  4e+09  5e+09  6e+09  7e+09
bl
eu
 s
co
re
s
# of edges proposed during decoding
synchronous binarization
monolingual binarization
Figure 7: Comparing the two binarization methods
in terms of translation quality against search effort.
system which is trained on a larger data-set but tuned
independently.
5 Conclusion
Modeling reorderings between languages has been a
major challenge for machine translation. This work
shows that the majority of syntactic reorderings, at
least between languages like English and Chinese,
can be efficiently decomposed into hierarchical bi-
nary reorderings. From a modeling perspective, on
the other hand, it is beneficial to start with a richer
representation that has more transformational power
than ITG or binary SCFG. Our work shows how to
convert it back to a computationally friendly form
without harming much of its expressiveness. As a
result, decoding with n-gram models can be fast and
accurate, making it possible for our syntax-based
system to overtake a comparable phrase-based sys-
tem in BLEU score. We believe that extensions of
our technique to more powerful models such as syn-
chronous tree-adjoining grammar (Shieber and Sch-
abes, 1990) is an interesting area for further work.
Acknowledgments Much of this work was done
when H. Zhang and L. Huang were visiting
USC/ISI. The authors wish to thank Wei Wang,
Jonathan Graehl and Steven DeNeefe for help with
the experiments. We are also grateful to Daniel
Marcu, Giorgio Satta, and Aravind Joshi for discus-
sions. This work was partially supported by NSF
ITR IIS-09325646 and NSF ITR IIS-0428020.
References
Albert V. Aho and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Prentice-Hall, Englewood Cliffs, NJ.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270, Ann Arbor, Michigan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL-
03, companion volume, Sapporo, Japan.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL-04.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proceedings of IWPT-05, Vancouver, BC.
Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing). LNCS.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of NAACL-03, Ed-
monton.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proceedings of HLT/EMNLP-05, pages
803?810, Vancouver, Canada, October.
L. Shapiro and A. B. Stephens. 1991. Bootstrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In COLING-90, volume III,
pages 253?258.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
263
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223?226,
New York, June 2006. c?2006 Association for Computational Linguistics
Efficient Algorithms for Richer Formalisms:
Parsing and Machine Translation
Liang Huang
Department of Computer and Information Science
University of Pennsylvania
lhuang3@cis.upenn.edu
My PhD research has been on the algorithmic and
formal aspects of computational linguistics, esp. in
the areas of parsing and machine translation. I am
interested in developing efficient algorithms for for-
malisms with rich expressive power, so that we can
have a better modeling of human languages without
sacrificing efficiency. In doing so, I hope to help in-
tegrating more linguistic and structural knowledge
with modern statistical techniques, and in particular,
for syntax-based machine translation (MT) systems.
Among other projects, I have been working on k-
best parsing, synchronous binarization, and syntax-
directed translation.
1 k-best Parsing and Hypergraphs
NLP systems are often cascades of several modules,
e.g., part-of-speech tagging, then syntactic parsing,
and finally semantic interpretation. It is often the
case that the 1-best output from one module is not
always optimal for the next module. So one might
want to postpone some disambiguation by propa-
gating k-best lists (instead of 1-best solutions) to
subsequent phases, as in joint parsing and seman-
tic role-labeling (Gildea and Jurafsky, 2002). This
is also true for reranking and discriminative train-
ing, where the k-best list of candidates serves as an
approximation of the full set (Collins, 2000; Och,
2003; McDonald et al, 2005). In this way we can
optimize some complicated objective function on
the k-best set, rather than on the full search space
which is usually exponentially large.
Previous algorithms for k-best parsing (Collins,
2000; Charniak and Johnson, 2005) are either sub-
optimal or slow and rely significantly on prun-
ing techniques to make them tractable. So I co-
developed several fast and exact algorithms for k-
best parsing in the general framework of directed
monotonic hypergraphs (Huang and Chiang, 2005).
This formulation extends and refines Klein and
Manning?s work (2001) by introducing monotonic
 1.5
 2.5
 3.5
 4.5
 5.5
 6.5
 7.5
 1  10  100  1000  10000
Av
er
ag
e 
Pa
rs
in
g 
Ti
m
e 
(se
co
nd
s)
k
Algorithm 0
Algorithm 1
Algorithm 3
Figure 1: Average parsing speed on the Section 23
of Penn Treebank (Algorithms 0, 1, and 3, log-log).
weight functions, which is closely related to the opti-
mal subproblem property in dynamic programming.
We first generalize the classical 1-best Viterbi al-
gorithm to hypergraphs, and then present four k-best
algorithms, each improving its predessor by delay-
ing more work until necessary. The final one, Al-
gorithm 3, starts with a normal 1-best search for
each vertex (or item, as in deductive frameworks),
and then works backwards from the target vertex (fi-
nal item) for its 2nd, 3rd, . . ., kth best derivations,
calling itself recursively only on demand, being the
laziest of the four algorithms. When tested on top
of two state-of-the-art systems, the Collins/Bikel
parser (Bikel, 2004) and Chiang?s CKY-based Hiero
decoder (Chiang, 2005), this algorithm is shown to
have very little overhead even for quite large k (say,
106) (See Fig. 1 for experiments on Bikel parser).
These algorithms have been re-implemented by
other researchers in the field, including Eugene
Charniak for his n-best parser, Ryan McDonald for
his dependency parser (McDonald et al, 2005), Mi-
crosoft Research NLP group (Simon Corston-Oliver
and Kevin Duh, p.c.) for a similar model, Jonathan
Graehl for the ISI syntax-based MT decoder, David
A. Smith for the Dyna language (Eisner et al, 2005),
223
and Jonathan May for ISI?s tree automata package
Tiburon. All of these experiments confirmed the
findings in our work.
2 Synchronous Binarization for MT
Machine Translation has made very good progress
in recent times, especially, the so-called ?phrase-
based? statistical systems (Och and Ney, 2004). In
order to take a substantial next-step it will be neces-
sary to incorporate several aspects of syntax. Many
researchers have explored syntax-based methods,
for instance, Wu (1996) and Chiang (2005) both uses
binary-branching synchronous context-free gram-
mars (SCFGs). However, to be more expressive
and flexible, it is often easier to start with a gen-
eral SCFG or tree-transducer (Galley et al, 2004).
In this case, binarization of the input grammar is
required for the use of the CKY algorithm (in or-
der to get cubic-time complexity), just as we convert
a CFG into the Chomsky Normal Form (CNF) for
monolingual parsing. For synchronous grammars,
however, different binarization schemes may result
in very different-looking chart items that greatly af-
fect decoding efficiency. For example, consider the
following SCFG rule:
(1) S ? NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
We can binarize it either left-to-right or right-to-left:
S ? VNP-PP VP
VNP-PP? NP PP
or
S ? NP VPP-VP
VPP-VP ? PP VP
The intermediate symbols (e.g. VPP-VP) are called vir-
tual nonterminals. We would certainly prefer the
right-to-left binarization because the virtual nonter-
minal has consecutive span (see Fig. 2). The left-to-
right binarization causes discontinuities on the target
side, which results in an exponential time complex-
ity when decoding with an integrated n-gram model.
We develop this intuition into a technique called
synchronous binarization (Zhang et al, 2006)
which binarizes a synchronous production or tree-
tranduction rule on both source and target sides si-
multaneously. It essentially converts an SCFG into
an equivalent ITG (the synchronous extension of
CNF) if possible. We reduce this problem to the
binarization of the permutation of nonterminal sym-
bols between the source and target sides of a syn-
chronous rule and devise a linear-time algorithm
NP
NP
PP
VP
VP
PP
target (English)
source (Chinese)
VPP-VP
NP
PP
VP
Chinese indices
English
boundary
w
o
rds 1 2 4 7
Powell
Powell
held
meeting
with
Sharon
VPP-VP
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the SCFG rule.
system BLEU
monolingual binarization 36.25
synchronous binarization 38.44
Table 1: Synchronous vs. monolingual binarization
in terms of translation quality (BLEU score).
for it. Experiments show that the resulting rule set
significantly improves the speed and accuracy over
monolingual binarization (see Table 1) in a state-
of-the-art syntax-based machine translation system
(Galley et al, 2004). We also propose another trick
(hook) for further speeding up the decoding with in-
tegrated n-gram models (Huang et al, 2005).
3 Syntax-Directed Translation
Syntax-directed translation was originally proposed
for compiling programming languages (Irons, 1961;
Lewis and Stearns, 1968), where the source pro-
gram is parsed into a syntax-tree that guides the
generation of the object code. These translations
have been formalized as a synchronous context-free
grammar (SCFG) that generates two languages si-
multaneously (Aho and Ullman, 1972), and equiv-
alently, as a top-down tree-to-string transducer
(Ge?cseg and Steinby, 1984). We adapt this syntax-
directed transduction process to statistical MT by
applying stochastic operations at each node of the
source-language parse-tree and searching for the
best derivation (a sequence of translation steps) that
converts the whole tree into some target-language
string (Huang et al, 2006).
3.1 Extended Domain of Locality
From a modeling perspective, however, the struc-
tural divergence across languages results in non-
isomorphic parse-trees that are not captured by
224
SCFGs. For example, the S(VO) structure in En-
glish is translated into a VSO order in Arabic, an
instance of complex re-ordering (Fig. 4).
To alleviate this problem, grammars with richer
expressive power have been proposed which can
grab larger fragments of the tree. Following Galley
et al (2004), we use an extended tree-to-string trans-
ducer (xRs) with multi-level left-hand-side (LHS)
trees.1 Since the right-hand-side (RHS) string can
be viewed as a flat one-level tree with the same non-
terminal root from LHS (Fig. 4), this framework is
closely related to STSGs in having extended domain
of locality on the source-side except for remain-
ing a CFG on the target-side. These rules can be
learned from a parallel corpus using English parse-
trees, Chinese strings, and word alignment (Galley
et al, 2004).
3.2 A Running Example
Consider the following English sentence and its Chi-
nese translation (note the reordering in the passive
construction):
(2) the gunman was killed by the police .
qiangshou
[gunman]
bei
[passive]
jingfang
[police]
jibi
[killed]
?
.
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
(r1) S (x1:NP-C x2:VP PUNC (.) ) ? x1 x2 ?
Then, the rule r2 grabs the whole sub-tree for ?the
gunman? and translates it as a phrase:
(r2) NP-C ( DT (the) NN (gunman) ) ? qiangshou
Now we get a ?partial Chinese, partial English? sen-
tence ?qiangshou VP ?? as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
1we will use LHS and source-side interchangeably (so are
RHS and target-side). In accordance with our experiments, we
also use English and Chinese as the source and target languages,
opposite to the Foreign-to-English convention of Brown et al
(1993).
(a) the gunman was killed by the police .
parser ?
(b)
S
NP-C
DT
the
NN
gunman
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
PUNC
.
r1, r2 ?
(c) qiangshou
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
?
r3 ?
(d) qiangshou bei
NP-C
DT
the
NN
police
VBN
killed
?
r5 ? r4 ?
(e) qiangshou bei jingfang jibi ?
Figure 3: A synatx-directed translation process.
S
NP(1)? VP
VB(2)? NP
(3)
?
, S
VB(2)? NP
(1)
? NP
(3)
?
Figure 4: An example of complex re-ordering.
225
(r3)
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? bei x2 x1
which captures the fact that the agent (NP-C, ?the
police?) and the verb (VBN, ?killed?) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r4 and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
3.3 Translation Algorithm
Given a fixed parse-tree ? ?, the search for the best
derivation (as a sequence of conversion steps) can
be done by a simple top-down traversal (or depth-
first search) from the root of the tree. With memo-
izationm, we get a dynamic programming algorithm
that is guaranteed to run in O(n) time where n is the
length of the input string, since the size of the parse-
tree is proportional to n. Similar algorithms have
also been proposed for dependency-based transla-
tion (Lin, 2004; Ding and Palmer, 2005).
I am currently performing large-scale experi-
ments on English-to-Chinese translation using the
xRs rules. We are not doing the usual direction of
Chinese-to-English partly due to the lack of a suf-
ficiently good Chinese parser. Initial results show
promising translation quality (in terms of BLEU
scores) and fast translation speed.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of
Parsing, Translation, and Compiling, volume I: Parsing of
Series in Automatic Computation. Prentice Hall, Englewood
Cliffs, New Jersey.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511, December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine-
grained n-best parsing and discriminative reranking. In Pro-
ceedings of the 43rd ACL.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd ACL.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML, pages 175?182.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probablisitic synchronous dependency insertion gram-
mars. In Proceedings of the 43rd ACL.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Com-
piling comp ling: Weighted dynamic programming and the
dyna language. In Proceedings of HLT-EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
F. Ge?cseg and M. Steinby. 1984. Tree Automata. Akade?miai
Kiado?, Budapest.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. In Proceedings of 9th International Workshop of Pars-
ing Technologies (IWPT).
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine
translation as lexicalized parsing with hooks. In Proceed-
ings of 9th International Workshop of Parsing Technologies
(IWPT).
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Syntax-
directed translation with extended domain of locality. In
submission.
E. T. Irons. 1961. A syntax-directed compiler for ALGOL 60.
Comm. ACM, 4(1):51?55.
Dan Klein and Christopher D. Manning. 2001. Parsing and
Hypergraphs. In Proceedings of the Seventh International
Workshop on Parsing Technologies (IWPT-2001), 17-19 Oc-
tober 2001, Beijing, China.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduc-
tion. Journal of the ACM, 15(3):465?488.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of the 20th COLING.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of the 43rd ACL.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30:417?449.
Franz Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. of ACL.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. In Proceedings of the 34th ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of HLT-NAACL.
226
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Forest Rescoring: Faster Decoding with Integrated Language Models ?
Liang Huang
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
David Chiang
USC Information Sciences Institute
Marina del Rey, CA 90292
chiang@isi.edu
Abstract
Efficient decoding has been a fundamental
problem in machine translation, especially
with an integrated language model which
is essential for achieving good translation
quality. We develop faster approaches for
this problem based on k-best parsing algo-
rithms and demonstrate their effectiveness
on both phrase-based and syntax-based MT
systems. In both cases, our methods achieve
significant speed improvements, often by
more than a factor of ten, over the conven-
tional beam-search method at the same lev-
els of search error and translation accuracy.
1 Introduction
Recent efforts in statistical machine translation
(MT) have seen promising improvements in out-
put quality, especially the phrase-based models (Och
and Ney, 2004) and syntax-based models (Chiang,
2005; Galley et al, 2006). However, efficient de-
coding under these paradigms, especially with inte-
grated language models (LMs), remains a difficult
problem. Part of the complexity arises from the ex-
pressive power of the translation model: for exam-
ple, a phrase- or word-based model with full reorder-
ing has exponential complexity (Knight, 1999). The
language model also, if fully integrated into the de-
coder, introduces an expensive overhead for main-
taining target-language boundary words for dynamic
? The authors would like to thank Dan Gildea, Jonathan
Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob
Moore and Hao Zhang. L. H. was partially supported by
NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA-
0205456 at UPenn. D. C. was partially supported under the
GALE/DARPA program, contract HR0011-06-C-0022.
programming (Wu, 1996; Och and Ney, 2004). In
practice, one must prune the search space aggres-
sively to reduce it to a reasonable size.
A much simpler alternative method to incorporate
the LM is rescoring: we first decode without the LM
(henceforth ?LM decoding) to produce a k-best list
of candidate translations, and then rerank the k-best
list using the LM. This method runs much faster in
practice but often produces a considerable number
of search errors since the true best translation (taking
LM into account) is often outside of the k-best list.
Cube pruning (Chiang, 2007) is a compromise be-
tween rescoring and full-integration: it rescores k
subtranslations at each node of the forest, rather than
only at the root node as in pure rescoring. By adapt-
ing the k-best parsing Algorithm 2 of Huang and
Chiang (2005), it achieves significant speed-up over
full-integration on Chiang?s Hiero system.
We push the idea behind this method further and
make the following contributions in this paper:
? We generalize cube pruning and adapt it to two
systems very different from Hiero: a phrase-
based system similar to Pharaoh (Koehn, 2004)
and a tree-to-string system (Huang et al, 2006).
? We also devise a faster variant of cube pruning,
called cube growing, which uses a lazy version
of k-best parsing (Huang and Chiang, 2005)
that tries to reduce k to the minimum needed
at each node to obtain the desired number of
hypotheses at the root.
Cube pruning and cube growing are collectively
called forest rescoring since they both approxi-
mately rescore the packed forest of derivations from
?LM decoding. In practice they run an order of
144
magnitude faster than full-integration with beam
search, at the same level of search errors and trans-
lation accuracy as measured by BLEU.
2 Preliminaries
We establish in this section a unified framework
for translation with an integrated n-gram language
model in both phrase-based systems and syntax-
based systems based on synchronous context-free
grammars (SCFGs). An SCFG (Lewis and Stearns,
1968) is a context-free rewriting system for generat-
ing string pairs. Each rule A ? ?, ? rewrites a pair
of nonterminals in both languages, where ? and ?
are the source and target side components, and there
is a one-to-one correspondence between the nonter-
minal occurrences in ? and the nonterminal occur-
rences in ?. For example, the following rule
VP ? PP (1) VP (2), VP (2) PP (1)
captures the swapping of VP and PP between Chi-
nese (source) and English (target).
2.1 Translation as Deduction
We will use the following example from Chinese to
English for both systems described in this section:
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
[past]
hu?`ta?n
meeting
?held a meeting with Sharon?
A typical phrase-based decoder generates partial
target-language outputs in left-to-right order in the
form of hypotheses (Koehn, 2004). Each hypothesis
has a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment.
This process can be formalized as a deduc-
tive system. For example, the following deduc-
tion step grows a hypothesis by the phrase-pair
?yu? Sha?lo?ng, with Sharon?:
( ???) : (w, ?held a talk?)
(?????) : (w + c, ?held a talk with Sharon?) (1)
where a ? in the coverage vector indicates the source
word at this position is ?covered? (for simplicity
we omit here the ending position of the last phrase
which is needed for distortion costs), and where w
and w + c are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair.
Similarly, the decoding problem with SCFGs can
also be cast as a deductive (parsing) system (Shieber
et al, 1995). Basically, we parse the input string us-
ing the source projection of the SCFG while build-
ing the corresponding subtranslations in parallel. A
possible deduction of the above example is notated:
(PP1,3) : (w1, t1) (VP3,6) : (w2, t2)
(VP1,6) : (w1 + w2 + c?, t2t1) (2)
where the subscripts denote indices in the input sen-
tence just as in CKY parsing, w1, w2 are the scores
of the two antecedent items, and t1 and t2 are the
corresponding subtranslations. The resulting trans-
lation t2t1 is the inverted concatenation as specified
by the target-side of the SCFG rule with the addi-
tional cost c? being the cost of this rule.
These two deductive systems represent the search
space of decoding without a language model. When
one is instantiated for a particular input string, it de-
fines a set of derivations, called a forest, represented
in a compact structure that has a structure of a graph
in the phrase-based case, or more generally, a hyper-
graph in both cases. Accordingly we call items like
(?????) and (VP1,6) nodes in the forest, and instan-
tiated deductions like
(?????) ? ( ???) with Sharon,
(VP1,6) ? (VP3,6) (PP1,3)
we call hyperedges that connect one or more an-
tecedent nodes to a consequent node.
2.2 Adding a Language Model
To integrate with a bigram language model, we can
use the dynamic-programming algorithms of Och
and Ney (2004) and Wu (1996) for phrase-based
and SCFG-based systems, respectively, which we
may think of as doing a finer-grained version of the
deductions above. Each node v in the forest will
be split into a set of augmented items, which we
call +LM items. For phrase-based decoding, a +LM
item has the form (v a) where a is the last word
of the hypothesis. Thus a +LM version of Deduc-
tion (1) might be:
( ??? talk) : (w, ?held a talk?)
(????? Sharon) : (w?, ?held a talk with Sharon?)
145
1.0
1.1
3.5
1.0 4.0 7.0
2.5 8.3 8.5
2.4 9.5 8.4
9.2 17.0 15.2
(VP held ? meeting3,6 )
(VP held ? talk3,6 )
(VP hold ? conference3,6 )
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
2.5
2.4
8.3
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
2.5
2.4
8.3
9.5
9.2
(PP
w
ith
? S
har
on
1,3
)
(PP
alo
ng
? S
har
on
1,3
)
(PP
w
ith
? S
hal
on
g
1,3
)
1.0 4.0 7.0
2.5
2.4
8.3
9.2
9.5
8.5
(a) (b) (c) (d)
Figure 1: Cube pruning along one hyperedge. (a): the numbers in the grid denote the score of the resulting
+LM item, including the combination cost; (b)-(d): the best-first enumeration of the top three items. Notice
that the items popped in (b) and (c) are out of order due to the non-monotonicity of the combination cost.
where the score of the resulting +LM item
w? = w + c? logPlm(with | talk)
now includes a combination cost due to the bigrams
formed when applying the phrase-pair.
Similarly, a +LM item in SCFG-based models
has the form (va?b), where a and b are boundary
words of the hypothesis string, and ? is a placeholder
symbol for an elided part of that string, indicating
that a possible translation of the part of the input
spanned by v starts with a and ends with b. An ex-
ample +LM version of Deduction (2) is:
(PP with ? Sharon1,3 ): (w1, t1) (VP held ? talk3,6 ): (w2, t2)
(VP held ? Sharon1,6 ): (w, t2t1)
where w = w1 +w2 +c?? logPlm(with | talk) with
a similar combination cost formed in combining ad-
jacent boundary words of antecedents. This scheme
can be easily extended to work with a general n-
gram model (Chiang, 2007). The experiments in this
paper use trigram models.
The conventional full-integration approach tra-
verses the forest bottom-up and explores all pos-
sible +LM deductions along each hyperedge.
The theoretical running time of this algorithm
is O(|F ||T |(m?1)) for phrase-based models, and
O(|F ||T |4(m?1)) for binary-branching SCFG-based
models, where |F | is the size of the forest, and |T |
is the number of possible target-side words. Even
if we assume a constant number of translations for
each word in the input, with a trigram model, this
still amounts to O(n11) for SCFG-based models and
O(2nn2) for phrase-based models.
3 Cube Pruning
Cube pruning (Chiang, 2007) reduces the search
space significantly based on the observation that
when the above method is combined with beam
search, only a small fraction of the possible +LM
items at a node will escape being pruned, and more-
over we can select with reasonable accuracy those
top-k items without computing all possible items
first. In a nutshell, cube pruning works on the ?LM
forest, keeping at most k +LM items at each node,
and uses the k-best parsing Algorithm 2 of Huang
and Chiang (2005) to speed up the computation.
For simplicity of presentation, we will use concrete
SCFG-based examples, but the method applies to the
general hypergraph framework in Section 2.
Consider Figure 1(a). Here k = 3 and we use
D(v) to denote the top-k +LM items (in sorted or-
der) of node v. Suppose we have computed D(u1)
and D(u2) for the two antecedent nodes u1 =
(VP3,6) and u2 = (PP1,3) respectively. Then for
the consequent node v = (VP1,6) we just need
to derive the top-3 from the 9 combinations of
(Di(u1), Dj(u2)) with i, j ? [1, 3]. Since the an-
tecedent items are sorted, it is very likely that the
best consequent items in this grid lie towards the
upper-left corner. This situation is very similar to k-
best parsing and we can adapt the Algorithm 2 of
Huang and Chiang (2005) here to explore this grid
in a best-first order.
Suppose that the combination costs are negligible,
and therefore the weight of a consequent item is just
the product of the weights of the antecedent items.
146
1: function CUBE(F ) ? the input is a forest F
2: for v ? F in (bottom-up) topological order do
3: KBEST(v)
4: return D1(TOP)
5: procedure KBEST(v)
6: cand ? {?e,1? | e ? IN (v)} ? for each incoming e
7: HEAPIFY(cand) ? a priority queue of candidates
8: buf ? ?
9: while |cand | > 0 and |buf | < k do
10: item? POP-MIN(cand)
11: append item to buf
12: PUSHSUCC(item, cand)
13: sort buf to D(v)
14: procedure PUSHSUCC(?e, j?, cand )
15: e is v ? u1 . . . u|e|
16: for i in 1 . . . |e| do
17: j? ? j + bi
18: if |D(ui)| ? j?i then
19: PUSH(?e, j??, cand)
Figure 2: Pseudocode for cube pruning.
Then we know that D1(v) = (D1(u1), D1(u2)),
the upper-left corner of the grid. Moreover, we
know that D2(v) is the better of (D1(u1), D2(u2))
and (D2(u1), D1(u2)), the two neighbors of the
upper-left corner. We continue in this way (see Fig-
ure 1(b)?(d)), enumerating the consequent items
best-first while keeping track of a relatively small
number of candidates (shaded cells in Figure 1(b),
cand in Figure 2) for the next-best item.
However, when we take into account the combi-
nation costs, this grid is no longer monotonic in gen-
eral, and the above algorithm will not always enu-
merate items in best-first order. We can see this in
the first iteration in Figure 1(b), where an item with
score 2.5 has been enumerated even though there is
an item with score 2.4 still to come. Thus we risk
making more search errors than the full-integration
method, but in practice the loss is much less signif-
icant than the speedup. Because of this disordering,
we do not put the enumerated items directly into
D(v); instead, we collect items in a buffer (buf in
Figure 2) and re-sort the buffer into D(v) after it has
accumulated k items.1
In general the grammar may have multiple rules
that share the same source side but have different
target sides, which we have treated here as separate
1Notice that different combinations might have the same re-
sulting item, in which case we only keep the one with the better
score (sometimes called hypothesis recombination in MT liter-
ature), so the number of items in D(v) might be less than k.
method k-best +LM rescoring. . .
rescoring Alg. 3 only at the root node
cube pruning Alg. 2 on-the-fly at each node
cube growing Alg. 3 on-the-fly at each node
Table 1: Comparison of the three methods.
hyperedges in the ?LM forest. In Hiero, these hy-
peredges are processed as a single unit which we
call a hyperedge bundle. The different target sides
then constitute a third dimension of the grid, form-
ing a cube of possible combinations (Chiang, 2007).
Now consider that there are many hyperedges that
derive v, and we are only interested the top +LM
items of v over all incoming hyperedges. Following
Algorithm 2, we initialize the priority queue cand
with the upper-left corner item from each hyper-
edge, and proceed as above. See Figure 2 for the
pseudocode for cube pruning. We use the notation
?e, j? to identify the derivation of v via the hyper-
edge e and the jith best subderivation of antecedent
ui (1 ? i ? |j|). Also, we let 1 stand for a vec-
tor whose elements are all 1, and bi for the vector
whose members are all 0 except for the ith whose
value is 1 (the dimensionality of either should be ev-
ident from the context). The heart of the algorithm
is lines 10?12. Lines 10?11 move the best deriva-
tion ?e, j? from cand to buf , and then line 12 pushes
its successors {?e, j + bi? | i ? 1 . . . |e|} into cand .
4 Cube Growing
Although much faster than full-integration, cube
pruning still computes a fixed amount of +LM items
at each node, many of which will not be useful for
arriving at the 1-best hypothesis at the root. It would
be more efficient to compute as few +LM items at
each node as are needed to obtain the 1-best hypoth-
esis at the root. This new method, called cube grow-
ing, is a lazy version of cube pruning just as Algo-
rithm 3 of Huang and Chiang (2005), is a lazy ver-
sion of Algorithm 2 (see Table 1).
Instead of traversing the forest bottom-up, cube
growing visits nodes recursively in depth-first or-
der from the root node (Figure 4). First we call
LAZYJTHBEST(TOP, 1), which uses the same al-
gorithm as cube pruning to find the 1-best +LM
item of the root node using the best +LM items of
147
1.0
1.1
3.5
1.0 4.0 7.0
2.1 5.1 8.1
2.2 5.2 8.2
4.6 7.6 10.6
1.0 4.0 7.0
2.5
2.4
8.3
(a) h-values (b) true costs
Figure 3: Example of cube growing along one hyper-
edge. (a): the h(x) scores for the grid in Figure 1(a),
assuming hcombo(e) = 0.1 for this hyperedge; (b)
cube growing prevents early ranking of the top-left
cell (2.5) as the best item in this grid.
the antecedent nodes. However, in this case the best
+LM items of the antecedent nodes are not known,
because we have not visited them yet. So we re-
cursively invoke LAZYJTHBEST on the antecedent
nodes to obtain them as needed. Each invocation of
LAZYJTHBEST(v, j) will recursively call itself on
the antecedents of v until it is confident that the jth
best +LM item for node v has been found.
Consider again the case of one hyperedge e. Be-
cause of the nonmonotonicity caused by combina-
tion costs, the first +LM item (?e,1?) popped from
cand is not guaranteed to be the best of all combina-
tions along this hyperedge (for example, the top-left
cell of 2.5 in Figure 1 is not the best in the grid). So
we cannot simply enumerate items just as they come
off of cand .2 Instead, we need to store up popped
items in a buffer buf , just as in cube pruning, and
enumerate an item only when we are confident that it
will never be surpassed in the future. In other words,
we would like to have an estimate of the best item
not explored yet (analogous to the heuristic func-
tion in A* search). If we can establish a lower bound
hcombo(e) on the combination cost of any +LM de-
duction via hyperedge e, then we can form a mono-
tonic grid (see Figure 3(a)) of lower bounds on the
grid of combinations, by using hcombo(e) in place of
the true combination cost for each +LM item x in
the grid; call this lower bound h(x).
Now suppose that the gray-shaded cells in Fig-
ure 3(a) are the members of cand . Then the min-
imum of h(x) over the items in cand , in this ex-
2If we did, then the out-of-order enumeration of +LM items
at an antecedent node would cause an entire row or column in
the grid to be disordered at the consequent node, potentially
leading to a multiplication of search errors.
1: procedure LAZYJTHBEST(v, j)
2: if cand [v] is undefined then
3: cand [v]? ?
4: FIRE(e,1, cand) foreach e ? IN (v)
5: buf [v]? ?
6: while |D(v)| < j and |buf [v]| + |D(v)| < k and
|cand [v]| > 0 do
7: item? POP-MIN(cand [v])
8: PUSH(item, buf [v])
9: PUSHSUCC(item, cand [v])
10: bound ? min{h(x) | x ? cand [v]}
11: ENUM(buf [v],D(v), bound)
12: ENUM(buf [v],D(v), +?)
13: procedure FIRE(e, j, cand )
14: e is v ? u1 . . . u|e|
15: for i in 1 . . . |e| do
16: LAZYJTHBEST(ui, ji)
17: if |D(ui)| < ji then return
18: PUSH(?e, j?, cand)
19: procedure PUSHSUCC(?e, j?, cand )
20: FIRE(e, j + bi, cand) foreach i in 1 . . . |e|
21: procedure ENUM(buf ,D, bound )
22: while |buf | > 0 and MIN(buf ) < bound do
23: append POP-MIN(buf ) to D
Figure 4: Pseudocode of cube growing.
ample, min{2.2, 5.1} = 2.2 is a lower bound on
the cost of any item in the future for the hyperedge
e. Indeed, if cand contains items from multiple hy-
peredges for a single consequent node, this is still a
valid lower bound. More formally:
Lemma 1. For each node v in the forest, the term
bound = min
x?cand [v]
h(x) (3)
is a lower bound on the true cost of any future item
that is yet to be explored for v.
Proof. For any item x that is not explored yet, the
true cost c(x) ? h(x), by the definition of h. And
there exists an item y ? cand[v] along the same hy-
peredge such that h(x) ? h(y), due to the mono-
tonicity of h within the grid along one hyperedge.
We also have h(y) ? bound by the definition of
bound. Therefore c(x) ? bound .
Now we can safely pop the best item from buf if
its true cost MIN(buf ) is better than bound and pass
it up to the consequent node (lines 21?23); but other-
wise, we have to wait for more items to accumulate
in buf to prevent a potential search error, for exam-
ple, in the case of Figure 3(b), where the top-left cell
148
(a)
1 2 3 4 5
(b)
1 2 3 4 5
Figure 5: (a) Pharaoh expands the hypotheses in the
current bin (#2) into longer ones. (b) In Cubit, hy-
potheses in previous bins are fed via hyperedge bun-
dles (solid arrows) into a priority queue (shaded tri-
angle), which empties into the current bin (#5).
(2.5) is worse than the current bound of 2.2. The up-
date of bound in each iteration (line 10) can be effi-
ciently implemented by using another heap with the
same contents as cand but prioritized by h instead.
In practice this is a negligible overhead on top of
cube pruning.
We now turn to the problem of estimating the
heuristic function hcombo . In practice, computing
true lower bounds of the combination costs is too
slow and would compromise the speed up gained
from cube growing. So we instead use a much sim-
pler method that just calculates the minimum com-
bination cost of each hyperedge in the top-i deriva-
tions of the root node in ?LM decoding. This is
just an approximation of the true lower bound, and
bad estimates can lead to search errors. However, the
hope is that by choosing the right value of i, these es-
timates will be accurate enough to affect the search
quality only slightly, which is analogous to ?almost
admissible? heuristics in A* search (Soricut, 2006).
5 Experiments
We test our methods on two large-scale English-to-
Chinese translation systems: a phrase-based system
and our tree-to-string system (Huang et al, 2006).
1.0
1.1
3.5
1.0 4.0 7.0
2.5 8.3 8.5
2.4 9.5 8.4
9.2 17.0 15.2
( ??? meeting)
( ??? talk)
( ??? conference)
with
Sha
ron
an
d Sh
aro
n
with
Arie
l Sh
aro
n
.
.
.
Figure 6: A hyperedge bundle represents all +LM
deductions that derives an item in the current bin
from the same coverage vector (see Figure 5). The
phrases on the top denote the target-sides of appli-
cable phrase-pairs sharing the same source-side.
5.1 Phrase-based Decoding
We implemented Cubit, a Python clone of the
Pharaoh decoder (Koehn, 2004),3 and adapted cube
pruning to it as follows. As in Pharaoh, each bin
i contains hypotheses (i.e., +LM items) covering i
words on the source-side. But at each bin (see Fig-
ure 5), all +LM items from previous bins are first
partitioned into ?LM items; then the hyperedges
leading from those ?LM items are further grouped
into hyperedge bundles (Figure 6), which are placed
into the priority queue of the current bin.
Our data preparation follows Huang et al (2006):
the training data is a parallel corpus of 28.3M words
on the English side, and a trigram language model is
trained on the Chinese side. We use the same test set
as (Huang et al, 2006), which is a 140-sentence sub-
set of the NIST 2003 test set with 9?36 words on the
English side. The weights for the log-linear model
are tuned on a separate development set. We set the
decoder phrase-table limit to 100 as suggested in
(Koehn, 2004) and the distortion limit to 4.
Figure 7(a) compares cube pruning against full-
integration in terms of search quality vs. search ef-
ficiency, under various pruning settings (threshold
beam set to 0.0001, stack size varying from 1 to
200). Search quality is measured by average model
cost per sentence (lower is better), and search effi-
ciency is measured by the average number of hy-
potheses generated (smaller is faster). At each level
3In our tests, Cubit always obtains a BLEU score within
0.004 of Pharaoh?s (Figure 7(b)). Source code available at
http://www.cis.upenn.edu/
?
lhuang3/cubit/
149
76
80
84
88
92
102 103 104 105 106
a
ve
ra
ge
 m
od
el
 c
os
t
average number of hypotheses per sentence
full-integration (Cubit)
cube pruning (Cubit)
0.200
0.205
0.210
0.215
0.220
0.225
0.230
0.235
0.240
0.245
102 103 104 105 106
BL
EU
 s
co
re
average number of hypotheses per sentence
Pharaoh
full-integration (Cubit)
cube pruning (Cubit)
(a) (b)
Figure 7: Cube pruning vs. full-integration (with beam search) on phrase-based decoding.
of search quality, the speed-up is always better than
a factor of 10. The speed-up at the lowest search-
error level is a factor of 32. Figure 7(b) makes a
similar comparison but measures search quality by
BLEU, which shows an even larger relative speed-up
for a given BLEU score, because translations with
very different model costs might have similar BLEU
scores. It also shows that our full-integration imple-
mentation in Cubit faithfully reproduces Pharaoh?s
performance. Fixing the stack size to 100 and vary-
ing the threshold yielded a similar result.
5.2 Tree-to-string Decoding
In tree-to-string (also called syntax-directed) decod-
ing (Huang et al, 2006; Liu et al, 2006), the source
string is first parsed into a tree, which is then re-
cursively converted into a target string according to
transfer rules in a synchronous grammar (Galley et
al., 2006). For instance, the following rule translates
an English passive construction into Chinese:
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? be`i x2 x1
Our tree-to-string system performs slightly bet-
ter than the state-of-the-art phrase-based system
Pharaoh on the above data set. Although differ-
ent from the SCFG-based systems in Section 2, its
derivation trees remain context-free and the search
space is still a hypergraph, where we can adapt the
methods presented in Sections 3 and 4.
The data set is same as in Section 5.1, except that
we also parsed the English-side using a variant of
the Collins (1997) parser, and then extracted 24.7M
tree-to-string rules using the algorithm of (Galley et
al., 2006). Since our tree-to-string rules may have
many variables, we first binarize each hyperedge in
the forest on the target projection (Huang, 2007).
All the three +LM decoding methods to be com-
pared below take these binarized forests as input. For
cube growing, we use a non-duplicate k-best method
(Huang et al, 2006) to get 100-best unique transla-
tions according to ?LM to estimate the lower-bound
heuristics.4 This preprocessing step takes on aver-
age 0.12 seconds per sentence, which is negligible
in comparison to the +LM decoding time.
Figure 8(a) compares cube growing and cube
pruning against full-integration under various beam
settings in the same fashion of Figure 7(a). At the
lowest level of search error, the relative speed-up
from cube growing and cube pruning compared with
full-integration is by a factor of 9.8 and 4.1, respec-
tively. Figure 8(b) is a similar comparison in terms
of BLEU scores and shows an even bigger advantage
of cube growing and cube pruning over the baseline.
4If a hyperedge is not represented at all in the 100-best?LM
derivations at the root node, we use the 1-best ?LM derivation
of this hyperedge instead. Here, rules that share the same source
side but have different target sides are treated as separate hy-
peredges, not collected into hyperedge bundles, since grouping
becomes difficult after binarization.
150
218.2
218.4
218.6
218.8
219.0
103 104 105
a
ve
ra
ge
 m
od
el
 c
os
t
average number of +LM items explored per sentence
full-integration
cube pruning
cube growing
0.254
0.256
0.258
0.260
0.262
103 104 105
BL
EU
 s
co
re
average number of +LM items explored per sentence
full-integration
cube pruning
cube growing
(a) (b)
Figure 8: Cube growing vs. cube pruning vs. full-integration (with beam search) on tree-to-string decoding.
6 Conclusions and Future Work
We have presented a novel extension of cube prun-
ing called cube growing, and shown how both can be
seen as general forest rescoring techniques applica-
ble to both phrase-based and syntax-based decoding.
We evaluated these methods on large-scale transla-
tion tasks and observed considerable speed improve-
ments, often by more than a factor of ten. We plan
to investigate how to adapt cube growing to phrase-
based and hierarchical phrase-based systems.
These forest rescoring algorithms have potential
applications to other computationally intensive tasks
involving combinations of different models, for
example, head-lexicalized parsing (Collins, 1997);
joint parsing and semantic role labeling (Sutton and
McCallum, 2005); or tagging and parsing with non-
local features. Thus we envision forest rescoring as
being of general applicability for reducing compli-
cated search spaces, as an alternative to simulated
annealing methods (Kirkpatrick et al, 1983).
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2). To appear.
Michael Collins. 1997. Three generative lexicalised models for
statistical parsing. In Proc. ACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models. In
Proc. COLING-ACL.
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proc. IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proc. NAACL Workshop on
Syntax and Structure in Statistical Translation.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220(4598):671?680.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proc. AMTA, pages 115?124.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduc-
tion. J. ACM, 15:465?488.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proc. COLING-ACL, pages 609?616.
Franz Joseph Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417?449.
Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995.
Principles and implementation of deductive parsing. J. Logic
Programming, 24:3?36.
Radu Soricut. 2006. Natural Language Generation using an
Information-Slim Representation. Ph.D. thesis, University
of Southern California.
Charles Sutton and Andrew McCallum. 2005. Joint parsing
and semantic role labeling. In Proc. CoNLL 2005.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. In Proc. ACL.
151
Proceedings of ACL-08: HLT, pages 192?199,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Forest-Based Translation
Haitao Mi? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
{htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
Among syntax-based translation models, the
tree-based approach, which takes as input a
parse tree of the source sentence, is a promis-
ing direction being faster and simpler than
its string-based counterpart. However, current
tree-based systems suffer from a major draw-
back: they only use the 1-best parse to direct
the translation, which potentially introduces
translation mistakes due to parsing errors. We
propose a forest-based approach that trans-
lates a packed forest of exponentially many
parses, which encodes many more alternatives
than standard n-best lists. Large-scale exper-
iments show an absolute improvement of 1.7
BLEU points over the 1-best baseline. This
result is also 0.8 points higher than decoding
with 30-best parses, and takes even less time.
1 Introduction
Syntax-based machine translation has witnessed
promising improvements in recent years. Depend-
ing on the type of input, these efforts can be di-
vided into two broad categories: the string-based
systems whose input is a string to be simultane-
ously parsed and translated by a synchronous gram-
mar (Wu, 1997; Chiang, 2005; Galley et al, 2006),
and the tree-based systems whose input is already a
parse tree to be directly converted into a target tree
or string (Lin, 2004; Ding and Palmer, 2005; Quirk
et al, 2005; Liu et al, 2006; Huang et al, 2006).
Compared with their string-based counterparts, tree-
based systems offer some attractive features: they
are much faster in decoding (linear time vs. cubic
time, see (Huang et al, 2006)), do not require a
binary-branching grammar as in string-based mod-
els (Zhang et al, 2006), and can have separate gram-
mars for parsing and translation, say, a context-free
grammar for the former and a tree substitution gram-
mar for the latter (Huang et al, 2006). However, de-
spite these advantages, current tree-based systems
suffer from a major drawback: they only use the 1-
best parse tree to direct the translation, which po-
tentially introduces translation mistakes due to pars-
ing errors (Quirk and Corston-Oliver, 2006). This
situation becomes worse with resource-poor source
languages without enough Treebank data to train a
high-accuracy parser.
One obvious solution to this problem is to take as
input k-best parses, instead of a single tree. This k-
best list postpones some disambiguation to the de-
coder, which may recover from parsing errors by
getting a better translation from a non 1-best parse.
However, a k-best list, with its limited scope, of-
ten has too few variations and too many redundan-
cies; for example, a 50-best list typically encodes
a combination of 5 or 6 binary ambiguities (since
25 < 50 < 26), and many subtrees are repeated
across different parses (Huang, 2008). It is thus inef-
ficient either to decode separately with each of these
very similar trees. Longer sentences will also aggra-
vate this situation as the number of parses grows ex-
ponentially with the sentence length.
We instead propose a new approach, forest-based
translation (Section 3), where the decoder trans-
lates a packed forest of exponentially many parses,1
1There has been some confusion in the MT literature regard-
ing the term forest: the word ?forest? in ?forest-to-string rules?
192
VP
PP
P
yu?
x1:NPB
VPB
VV
ju?x??ng
AS
le
x2:NPB
? held x2 with x1
Figure 1: An example translation rule (r3 in Fig. 2).
which compactly encodes many more alternatives
than k-best parses. This scheme can be seen as
a compromise between the string-based and tree-
based methods, while combining the advantages of
both: decoding is still fast, yet does not commit to
a single parse. Large-scale experiments (Section 4)
show an improvement of 1.7 BLEU points over the
1-best baseline, which is also 0.8 points higher than
decoding with 30-best trees, and takes even less time
thanks to the sharing of common subtrees.
2 Tree-based systems
Current tree-based systems perform translation in
two separate steps: parsing and decoding. A parser
first parses the source language input into a 1-best
tree T , and the decoder then searches for the best
derivation (a sequence of translation steps) d? that
converts source tree T into a target-language string
among all possible derivations D:
d? = argmax
d?D
P(d|T ). (1)
We will now proceed with a running example
translating from Chinese to English:
(2) ?
Bu`sh??
Bush
?
yu?
with/and
??
Sha?lo?ng
Sharon1
>L
ju?x??ng
hold
?
le
pass.
Proceedings of ACL-08: HLT, pages 586?594,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Forest Reranking: Discriminative Parsing with Non-Local Features?
Liang Huang
University of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Abstract
Conventional n-best reranking techniques of-
ten suffer from the limited scope of the n-
best list, which rules out many potentially
good alternatives. We instead propose forest
reranking, a method that reranks a packed for-
est of exponentially many parses. Since ex-
act inference is intractable with non-local fea-
tures, we present an approximate algorithm in-
spired by forest rescoring that makes discrim-
inative training practical over the whole Tree-
bank. Our final result, an F-score of 91.7, out-
performs both 50-best and 100-best reranking
baselines, and is better than any previously re-
ported systems trained on the Treebank.
1 Introduction
Discriminative reranking has become a popular
technique for many NLP problems, in particular,
parsing (Collins, 2000) and machine translation
(Shen et al, 2005). Typically, this method first gen-
erates a list of top-n candidates from a baseline sys-
tem, and then reranks this n-best list with arbitrary
features that are not computable or intractable to
compute within the baseline system. But despite its
apparent success, there remains a major drawback:
this method suffers from the limited scope of the n-
best list, which rules out many potentially good al-
ternatives. For example 41% of the correct parses
were not in the candidates of ?30-best parses in
(Collins, 2000). This situation becomes worse with
longer sentences because the number of possible in-
terpretations usually grows exponentially with the
? Part of this work was done while I was visiting Institute
of Computing Technology, Beijing, and I thank Prof. Qun Liu
and his lab for hosting me. I am also grateful to Dan Gildea and
Mark Johnson for inspirations, Eugene Charniak for help with
his parser, and Wenbin Jiang for guidance on perceptron aver-
aging. This project was supported by NSF ITR EIA-0205456.
local non-local
conventional reranking only at the root
DP-based discrim. parsing exact N/A
this work: forest-reranking exact on-the-fly
Table 1: Comparison of various approaches for in-
corporating local and non-local features.
sentence length. As a result, we often see very few
variations among the n-best trees, for example, 50-
best trees typically just represent a combination of 5
to 6 binary ambiguities (since 25 < 50 < 26).
Alternatively, discriminative parsing is tractable
with exact and efficient search based on dynamic
programming (DP) if all features are restricted to be
local, that is, only looking at a local window within
the factored search space (Taskar et al, 2004; Mc-
Donald et al, 2005). However, we miss the benefits
of non-local features that are not representable here.
Ideally, we would wish to combine the merits of
both approaches, where an efficient inference algo-
rithm could integrate both local and non-local fea-
tures. Unfortunately, exact search is intractable (at
least in theory) for features with unbounded scope.
So we propose forest reranking, a technique inspired
by forest rescoring (Huang and Chiang, 2007) that
approximately reranks the packed forest of expo-
nentially many parses. The key idea is to compute
non-local features incrementally from bottom up, so
that we can rerank the n-best subtrees at all internal
nodes, instead of only at the root node as in conven-
tional reranking (see Table 1). This method can thus
be viewed as a step towards the integration of dis-
criminative reranking with traditional chart parsing.
Although previous work on discriminative pars-
ing has mainly focused on short sentences (? 15
words) (Taskar et al, 2004; Turian and Melamed,
2007), our work scales to the whole Treebank, where
586
VP1,6
VBD1,2 blah NP2,6
NP2,3 blah PP3,6
be2 e1
Figure 1: A partial forest of the example sentence.
we achieved an F-score of 91.7, which is a 19% er-
ror reduction from the 1-best baseline, and outper-
forms both 50-best and 100-best reranking. This re-
sult is also better than any previously reported sys-
tems trained on the Treebank.
2 Packed Forests as Hypergraphs
Informally, a packed parse forest, or forest in short,
is a compact representation of all the derivations
(i.e., parse trees) for a given sentence under a
context-free grammar (Billot and Lang, 1989). For
example, consider the following sentence
0 I 1 saw 2 him 3 with 4 a 5 mirror 6
where the numbers between words denote string po-
sitions. Shown in Figure 1, this sentence has (at
least) two derivations depending on the attachment
of the prep. phrase PP3,6 ?with a mirror?: it can ei-
ther be attached to the verb ?saw?,
VBD1,2 NP2,3 PP3,6
VP1,6 , (*)
or be attached to ?him?, which will be further com-
bined with the verb to form the same VP as above.
These two derivations can be represented as a sin-
gle forest by sharing common sub-derivations. Such
a forest has a structure of a hypergraph (Klein and
Manning, 2001; Huang and Chiang, 2005), where
items like PP3,6 are called nodes, and deductive
steps like (*) correspond to hyperedges.
More formally, a forest is a pair ?V,E?, where V
is the set of nodes, and E the set of hyperedges. For
a given sentence w1:l = w1 . . . wl, each node v ? V
is in the form of X i,j , which denotes the recogni-
tion of nonterminal X spanning the substring from
positions i through j (that is, wi+1 . . . wj). Each hy-
peredge e ? E is a pair ?tails(e), head(e)?, where
head(e) ? V is the consequent node in the deduc-
tive step, and tails(e) ? V ? is the list of antecedent
nodes. For example, the hyperedge for deduction (*)
is notated:
e1 = ?(VBD1,2, NP2,3, PP3,6), VP1,6?
We also denote IN (v) to be the set of incom-
ing hyperedges of node v, which represent the dif-
ferent ways of deriving v. For example, in the for-
est in Figure 1, IN (VP1,6) is {e1, e2}, with e2 =
?(VBD1,2, NP2,6), VP1,6?. We call |e| the arity of
hyperedge e, which counts the number of tail nodes
in e. The arity of a hypergraph is the maximum ar-
ity over all hyperedges. A CKY forest has an arity
of 2, since the input grammar is required to be bi-
nary branching (cf. Chomsky Normal Form) to en-
sure cubic time parsing complexity. However, in this
work, we use forests from a Treebank parser (Char-
niak, 2000) whose grammar is often flat in many
productions. For example, the arity of the forest in
Figure 1 is 3. Such a Treebank-style forest is eas-
ier to work with for reranking, since many features
can be directly expressed in it. There is also a distin-
guished root node TOP in each forest, denoting the
goal item in parsing, which is simply S0,l where S is
the start symbol and l is the sentence length.
3 Forest Reranking
3.1 Generic Reranking with the Perceptron
We first establish a unified framework for parse
reranking with both n-best lists and packed forests.
For a given sentence s, a generic reranker selects
the best parse y? among the set of candidates cand(s)
according to some scoring function:
y? = argmax
y?cand(s)
score(y) (1)
In n-best reranking, cand(s) is simply a set of
n-best parses from the baseline parser, that is,
cand(s) = {y1, y2, . . . , yn}. Whereas in forest
reranking, cand(s) is a forest implicitly represent-
ing the set of exponentially many parses.
As usual, we define the score of a parse y to be
the dot product between a high dimensional feature
representation and a weight vector w:
score(y) = w ? f(y) (2)
587
where the feature extractor f is a vector of d func-
tions f = (f1, . . . , fd), and each feature fj maps
a parse y to a real number fj(y). Following (Char-
niak and Johnson, 2005), the first feature f1(y) =
log Pr(y) is the log probability of a parse from the
baseline generative parser, while the remaining fea-
tures are all integer valued, and each of them counts
the number of times that a particular configuration
occurs in parse y. For example, one such feature
f2000 might be a question
?how many times is a VP of length 5 surrounded
by the word ?has? and the period? ?
which is an instance of the WordEdges feature (see
Figure 2(c) and Section 3.2 for details).
Using a machine learning algorithm, the weight
vector w can be estimated from the training data
where each sentence si is labelled with its cor-
rect (?gold-standard?) parse y?i . As for the learner,
Collins (2000) uses the boosting algorithm and
Charniak and Johnson (2005) use the maximum en-
tropy estimator. In this work we use the averaged
perceptron algorithm (Collins, 2002) since it is an
online algorithm much simpler and orders of magni-
tude faster than Boosting and MaxEnt methods.
Shown in Pseudocode 1, the perceptron algo-
rithm makes several passes over the whole train-
ing data, and in each iteration, for each sentence si,
it tries to predict a best parse y?i among the candi-
dates cand(si) using the current weight setting. In-
tuitively, we want the gold parse y?i to be picked, but
in general it is not guaranteed to be within cand(si),
because the grammar may fail to cover the gold
parse, and because the gold parse may be pruned
away due to the limited scope of cand(si). So we
define an oracle parse y+i to be the candidate that
has the highest Parseval F-score with respect to the
gold tree y?i :1
y+i , argmax
y?cand(si)
F (y, y?i ) (3)
where function F returns the F-score. Now we train
the reranker to pick the oracle parses as often as pos-
sible, and in case an error is made (line 6), perform
an update on the weight vector (line 7), by adding
the difference between two feature representations.
1If one uses the gold y?i for oracle y+i , the perceptron will
continue to make updates towards something unreachable even
when the decoder has picked the best possible candidate.
Pseudocode 1 Perceptron for Generic Reranking
1: Input: Training examples {cand(si), y+i }Ni=1 ? y+i is the
oracle tree for si among cand(si)
2: w? 0 ? initial weights
3: for t? 1 . . . T do ? T iterations
4: for i? 1 . . . N do
5: y? = argmaxy?cand(si) w ? f(y)
6: if y? 6= y+i then
7: w? w + f(y+i )? f(y?)
8: return w
In n-best reranking, since all parses are explicitly
enumerated, it is trivial to compute the oracle tree.2
However, it remains widely open how to identify the
forest oracle. We will present a dynamic program-
ming algorithm for this problem in Sec. 4.1.
We also use a refinement called ?averaged param-
eters? where the final weight vector is the average of
weight vectors after each sentence in each iteration
over the training data. This averaging effect has been
shown to reduce overfitting and produce much more
stable results (Collins, 2002).
3.2 Factorizing Local and Non-Local Features
A key difference between n-best and forest rerank-
ing is the handling of features. In n-best reranking,
all features are treated equivalently by the decoder,
which simply computes the value of each one on
each candidate parse. However, for forest reranking,
since the trees are not explicitly enumerated, many
features can not be directly computed. So we first
classify features into local and non-local, which the
decoder will process in very different fashions.
We define a feature f to be local if and only if
it can be factored among the local productions in a
tree, and non-local if otherwise. For example, the
Rule feature in Fig. 2(a) is local, while the Paren-
tRule feature in Fig. 2(b) is non-local. It is worth
noting that some features which seem complicated
at the first sight are indeed local. For example, the
WordEdges feature in Fig. 2(c), which classifies
a node by its label, span length, and surrounding
words, is still local since all these information are
encoded either in the node itself or in the input sen-
tence. In contrast, it would become non-local if we
replace the surrounding words by surrounding POS
2In case multiple candidates get the same highest F-score,
we choose the parse with the highest log probability from the
baseline parser to be the oracle parse (Collins, 2000).
588
VP
VBD NP PP
S
VP
VBD NP PP
VP
VBZ
has
NP
|? 5 words?|
.
.
VP
VBD
saw
NP
DT
the
...
(a) Rule (local) (b) ParentRule (non-local) (c) WordEdges (local) (d) NGramTree (non-local)
? VP? VBD NP PP ? ? VP? VBD NP PP | S ? ? NP 5 has . ? ? VP (VBD saw) (NP (DT the)) ?
Figure 2: Illustration of some example features. Shaded nodes denote information included in the feature.
tags, which are generated dynamically.
More formally, we split the feature extractor f =
(f1, . . . , fd) into f = (fL; fN ) where fL and fN are
the local and non-local features, respectively. For the
former, we extend their domains from parses to hy-
peredges, where f(e) returns the value of a local fea-
ture f ? fL on hyperedge e, and its value on a parsey
factors across the hyperedges (local productions),
fL(y) =
?
e?y
fL(e) (4)
and we can pre-compute fL(e) for each e in a forest.
Non-local features, however, can not be pre-
computed, but we still prefer to compute them as
early as possible, which we call ?on-the-fly? com-
putation, so that our decoder can be sensitive to them
at internal nodes. For instance, the NGramTree fea-
ture in Fig. 2 (d) returns the minimum tree fragement
spanning a bigram, in this case ?saw? and ?the?, and
should thus be computed at the smallest common an-
cestor of the two, which is the VP node in this ex-
ample. Similarly, the ParentRule feature in Fig. 2
(b) can be computed when the S subtree is formed.
In doing so, we essentially factor non-local features
across subtrees, where for each subtree y? in a parse
y, we define a unit feature f?(y?) to be the part of
f(y) that are computable within y?, but not com-
putable in any (proper) subtree of y?. Then we have:
fN (y) =
?
y??y
f?N (y?) (5)
Intuitively, we compute the unit non-local fea-
tures at each subtree from bottom-up. For example,
for the binary-branching node Ai,k in Fig. 3, the
Ai,k
Bi,j
wi . . . wj?1
Cj,k
wj . . . wk?1
Figure 3: Example of the unit NGramTree feature
at node Ai,k: ? A (B . . . wj?1) (C . . . wj) ?.
unit NGramTree instance is for the pair ?wj?1, wj?
on the boundary between the two subtrees, whose
smallest common ancestor is the current node. Other
unit NGramTree instances within this span have al-
ready been computed in the subtrees, except those
for the boundary words of the whole node, wi and
wk?1, which will be computed when this node is fur-
ther combined with other nodes in the future.
3.3 Approximate Decoding via Cube Pruning
Before moving on to approximate decoding with
non-local features, we first describe the algorithm
for exact decoding when only local features are
present, where many concepts and notations will be
re-used later. We will use D(v) to denote the top
derivations of node v, where D1(v) is its 1-best
derivation. We also use the notation ?e, j? to denote
the derivation along hyperedge e, using the jith sub-
derivation for tail ui, so ?e,1? is the best deriva-
tion along e. The exact decoding algorithm, shown
in Pseudocode 2, is an instance of the bottom-up
Viterbi algorithm, which traverses the hypergraph in
a topological order, and at each node v, calculates
its 1-best derivation using each incoming hyperedge
e ? IN (v). The cost of e, c(e), is the score of its
589
Pseudocode 2 Exact Decoding with Local Features
1: function VITERBI(?V, E?)
2: for v ? V in topological order do
3: for e ? IN (v) do
4: c(e)? w ? fL(e) +
P
ui?tails(e) c(D1(ui))
5: if c(e) > c(D1(v)) then ? better derivation?
6: D1(v)? ?e,1?
7: c(D1(v))? c(e)
8: return D1(TOP)
Pseudocode 3 Cube Pruning for Non-local Features
1: function CUBE(?V, E?)
2: for v ? V in topological order do
3: KBEST(v)
4: return D1(TOP)
5: procedure KBEST(v)
6: heap ? ?; buf ? ?
7: for e ? IN (v) do
8: c(?e,1?)? EVAL(e,1) ? extract unit features
9: append ?e,1? to heap
10: HEAPIFY(heap) ? prioritized frontier
11: while |heap| > 0 and |buf | < k do
12: item? POP-MAX(heap) ? extract next-best
13: append item to buf
14: PUSHSUCC(item, heap)
15: sort buf to D(v)
16: procedure PUSHSUCC(?e, j?, heap)
17: e is v ? u1 . . . u|e|
18: for i in 1 . . . |e| do
19: j? ? j + bi ? bi is 1 only on the ith dim.
20: if |D(ui)| ? j?i then ? enough sub-derivations?
21: c(?e, j??)? EVAL(e, j?) ? unit features
22: PUSH(?e, j??, heap)
23: function EVAL(e, j)
24: e is v ? u1 . . . u|e|
25: return w ? fL(e) + w ? f?N (?e, j?) +
P
i c(Dji(ui))
(pre-computed) local features w ? fL(e). This algo-
rithm has a time complexity of O(E), and is almost
identical to traditional chart parsing, except that the
forest might be more than binary-branching.
For non-local features, we adapt cube pruning
from forest rescoring (Chiang, 2007; Huang and
Chiang, 2007), since the situation here is analogous
to machine translation decoding with integrated lan-
guage models: we can view the scores of unit non-
local features as the language model cost, computed
on-the-fly when combining sub-constituents.
Shown in Pseudocode 3, cube pruning works
bottom-up on the forest, keeping a beam of at most k
derivations at each node, and uses the k-best pars-
ing Algorithm 2 of Huang and Chiang (2005) to
speed up the computation. When combining the sub-
derivations along a hyperedge e to form a new sub-
tree y? = ?e, j?, we also compute its unit non-local
feature values f?N (?e, j?) (line 25). A priority queue
(heap in Pseudocode 3) is used to hold the candi-
dates for the next-best derivation, which is initial-
ized to the set of best derivations along each hyper-
edge (lines 7 to 9). Then at each iteration, we pop
the best derivation (lines 12), and push its succes-
sors back into the priority queue (line 14). Analo-
gous to the language model cost in forest rescoring,
the unit feature cost here is a non-monotonic score in
the dynamic programming backbone, and the deriva-
tions may thus be extracted out-of-order. So a buffer
buf is used to hold extracted derivations, which is
sorted at the end (line 15) to form the list of top-k
derivations D(v) of node v. The complexity of this
algorithm is O(E + V k log kN ) (Huang and Chi-
ang, 2005), where O(N ) is the time for on-the-fly
feature extraction for each subtree, which becomes
the bottleneck in practice.
4 Supporting Forest Algorithms
4.1 Forest Oracle
Recall that the Parseval F-score is the harmonic
mean of labelled precision P and labelled recall R:
F (y, y?) , 2PR
P + R
=
2|y ? y?|
|y|+ |y?| (6)
where |y| and |y?| are the numbers of brackets in the
test parse and gold parse, respectively, and |y ? y?|
is the number of matched brackets. Since the har-
monic mean is a non-linear combination, we can not
optimize the F-scores on sub-forests independently
with a greedy algorithm. In other words, the optimal
F-score tree in a forest is not guaranteed to be com-
posed of two optimal F-score subtrees.
We instead propose a dynamic programming al-
gorithm which optimizes the number of matched
brackets for a given number of test brackets. For ex-
ample, our algorithm will ask questions like,
?when a test parse has 5 brackets, what is the
maximum number of matched brackets??
More formally, at each node v, we compute an ora-
cle function ora[v] : N 7? N, which maps an integer
t to ora[v](t), the max. number of matched brackets
590
Pseudocode 4 Forest Oracle Algorithm
1: function ORACLE(?V, E?, y?)
2: for v ? V in topological order do
3: for e ? BS(v) do
4: e is v ? u1u2 . . . u|e|
5: ora[v]? ora[v]? (?iora[ui])
6: ora[v]? ora[v] ? (1,1v?y?)
7: return F (y+, y?) = maxt 2?ora[TOP](t)t+|y?| ? oracle F1
for all parses yv of node v with exactly t brackets:
ora[v](t) , max
yv :|yv |=t
|yv ? y?| (7)
When node v is combined with another node u
along a hyperedge e = ?(v, u), w?, we need to com-
bine the two oracle functions ora[v] and ora[u] by
distributing the test brackets of w between v and u,
and optimize the number of matched bracktes. To
do this we define a convolution operator ? between
two functions f and g:
(f ? g)(t) , max
t1+t2=t
f(t1) + g(t2) (8)
For instance:
t f(t)
2 1
3 2
?
t g(t)
4 4
5 4
=
t (f ? g)(t)
6 5
7 6
8 6
The oracle function for the head node w is then
ora[w](t) = (ora[v]? ora[u])(t? 1)+1w?y? (9)
where 1 is the indicator function, returning 1 if node
w is found in the gold tree y?, in which case we
increment the number of matched brackets. We can
also express Eq. 9 in a purely functional form
ora[w] = (ora[v]? ora[u]) ? (1,1w?y?) (10)
where ? is a translation operator which shifts a
function along the axes:
(f ? (a, b))(t) , f(t? a) + b (11)
Above we discussed the case of one hyperedge. If
there is another hyperedge e? deriving node w, we
also need to combine the resulting oracle functions
from both hyperedges, for which we define a point-
wise addition operator ?:
(f ? g)(t) , max{f(t), g(t)} (12)
Shown in Pseudocode 4, we perform these com-
putations in a bottom-up topological order, and fi-
nally at the root node TOP, we can compute the best
global F-score by maximizing over different num-
bers of test brackets (line 7). The oracle tree y+ can
be recursively restored by keeping backpointers for
each ora[v](t), which we omit in the pseudocode.
The time complexity of this algorithm for a sen-
tence of l words is O(|E| ? l2(a?1)) where a is the
arity of the forest. For a CKY forest, this amounts
to O(l3 ? l2) = O(l5), but for general forests like
those in our experiments the complexities are much
higher. In practice it takes on average 0.05 seconds
for forests pruned by p = 10 (see Section 4.2), but
we can pre-compute and store the oracle for each
forest before training starts.
4.2 Forest Pruning
Our forest pruning algorithm (Jonathan Graehl, p.c.)
is very similar to the method based on marginal
probability (Charniak and Johnson, 2005), except
that ours prunes hyperedges as well as nodes. Ba-
sically, we use an Inside-Outside algorithm to com-
pute the Viterbi inside cost ?(v) and the Viterbi out-
side cost ?(v) for each node v, and then compute the
merit ??(e) for each hyperedge:
??(e) = ?(head(e)) +
?
ui?tails(e)
?(ui) (13)
Intuitively, this merit is the cost of the best deriva-
tion that traverses e, and the difference ?(e) =
??(e) ? ?(TOP) can be seen as the distance away
from the globally best derivation. We prune away
all hyperedges that have ?(e) > p for a thresh-
old p. Nodes with all incoming hyperedges pruned
are also pruned. The key difference from (Charniak
and Johnson, 2005) is that in this algorithm, a node
can ?partially? survive the beam, with a subset of its
hyperedges pruned. In practice, this method prunes
on average 15% more hyperedges than their method.
5 Experiments
We compare the performance of our forest reranker
against n-best reranking on the Penn English Tree-
bank (Marcus et al, 1993). The baseline parser is
the Charniak parser, which we modified to output a
591
Local instances Non-Local instances
Rule 10, 851 ParentRule 18, 019
Word 20, 328 WProj 27, 417
WordEdges 454, 101 Heads 70, 013
CoLenPar 22 HeadTree 67, 836
Bigram? 10, 292 Heavy 1, 401
Trigram? 24, 677 NGramTree 67, 559
HeadMod? 12, 047 RightBranch 2
DistMod? 16, 017
Total Feature Instances: 800, 582
Table 2: Features used in this work. Those with a ?
are from (Collins, 2000), and others are from (Char-
niak and Johnson, 2005), with simplifications.
packed forest for each sentence.3
5.1 Data Preparation
We use the standard split of the Treebank: sections
02-21 as the training data (39832 sentences), sec-
tion 22 as the development set (1700 sentences), and
section 23 as the test set (2416 sentences). Follow-
ing (Charniak and Johnson, 2005), the training set is
split into 20 folds, each containing about 1992 sen-
tences, and is parsed by the Charniak parser with a
model trained on sentences from the remaining 19
folds. The development set and the test set are parsed
with a model trained on all 39832 training sentences.
We implemented both n-best and forest reranking
systems in Python and ran our experiments on a 64-
bit Dual-Core Intel Xeon with 3.0GHz CPUs. Our
feature set is summarized in Table 2, which closely
follows Charniak and Johnson (2005), except that
we excluded the non-local features Edges, NGram,
and CoPar, and simplified Rule and NGramTree
features, since they were too complicated to com-
pute.4 We also added four unlexicalized local fea-
tures from Collins (2000) to cope with data-sparsity.
Following Charniak and Johnson (2005), we ex-
tracted the features from the 50-best parses on the
training set (sec. 02-21), and used a cut-off of 5 to
prune away low-count features. There are 0.8M fea-
tures in our final set, considerably fewer than that
of Charniak and Johnson which has about 1.3M fea-
3This is a relatively minor change to the Charniak parser,
since it implements Algorithm 3 of Huang and Chiang (2005)
for efficient enumeration of n-best parses, which requires stor-
ing the forest. The modified parser and related scripts for han-
dling forests (e.g. oracles) will be available on my homepage.
4In fact, our Rule and ParentRule features are two special
cases of the original Rule feature in (Charniak and Johnson,
2005). We also restricted NGramTree to be on bigrams only.
89.0
91.0
93.0
95.0
97.0
99.0
 0  500  1000  1500  2000
Pa
rs
ev
al
 F
-s
co
re
 (
%
)
average # of hyperedges or brackets per sentence
p=10 p=20
n=10
n=50 n=100
1-best
forest oracle
n-best oracle
Figure 4: Forests (shown with various pruning
thresholds) enjoy higher oracle scores and more
compact sizes than n-best lists (on sec 23).
tures in the updated version.5 However, our initial
experiments show that, even with this much simpler
feature set, our 50-best reranker performed equally
well as theirs (both with an F-score of 91.4, see Ta-
bles 3 and 4). This result confirms that our feature
set design is appropriate, and the averaged percep-
tron learner is a reasonable candidate for reranking.
The forests dumped from the Charniak parser are
huge in size, so we use the forest pruning algorithm
in Section 4.2 to prune them down to a reasonable
size. In the following experiments we use a thresh-
old of p = 10, which results in forests with an av-
erage number of 123.1 hyperedges per forest. Then
for each forest, we annotate its forest oracle, and
on each hyperedge, pre-compute its local features.6
Shown in Figure 4, these forests have an forest or-
acle of 97.8, which is 1.1% higher than the 50-best
oracle (96.7), and are 8 times smaller in size.
5.2 Results and Analysis
Table 3 compares the performance of forest rerank-
ing against standard n-best reranking. For both sys-
tems, we first use only the local features, and then
all the features. We use the development set to deter-
mine the optimal number of iterations for averaged
perceptron, and report the F1 score on the test set.
With only local features, our forest reranker achieves
an F-score of 91.25, and with the addition of non-
5http://www.cog.brown.edu/?mj/software.htm. We follow
this version as it corrects some bugs from their 2005 paper
which leads to a 0.4% increase in performance (see Table 4).
6A subset of local features, e.g. WordEdges, is independent
of which hyperedge the node takes in a derivation, and can thus
be annotated on nodes rather than hyperedges. We call these
features node-local, which also include part of Word features.
592
baseline: 1-best Charniak parser 89.72
n-best reranking
features n pre-comp. training F1%
local 50 1.7G / 16h 3 ? 0.1h 91.28
all 50 2.4G / 19h 4 ? 0.3h 91.43
all 100 5.3G / 44h 4 ? 0.7h 91.49
forest reranking (p = 10)
features k pre-comp. training F1%
local - 1.2G / 2.9h 3 ? 0.8h 91.25
all 15 4 ? 6.1h 91.69
Table 3: Forest reranking compared to n-best rerank-
ing on sec. 23. The pre-comp. column is for feature
extraction, and training column shows the number
of perceptron iterations that achieved best results on
the dev set, and average time per iteration.
local features, the accuracy rises to 91.69 (with beam
size k = 15), which is a 0.26% absolute improve-
ment over 50-best reranking.7
This improvement might look relatively small, but
it is much harder to make a similar progress with
n-best reranking. For example, even if we double
the size of the n-best list to 100, the performance
only goes up by 0.06% (Table 3). In fact, the 100-
best oracle is only 0.5% higher than the 50-best one
(see Fig. 4). In addition, the feature extraction step
in 100-best reranking produces huge data files and
takes 44 hours in total, though this part can be paral-
lelized.8 On two CPUs, 100-best reranking takes 25
hours, while our forest-reranker can also finish in 26
hours, with a much smaller disk space. Indeed, this
demonstrates the severe redundancies as another dis-
advantage of n-best lists, where many subtrees are
repeated across different parses, while the packed
forest reduces space dramatically by sharing com-
mon sub-derivations (see Fig. 4).
To put our results in perspective, we also compare
them with other best-performing systems in Table 4.
Our final result (91.7) is better than any previously
reported system trained on the Treebank, although
7It is surprising that 50-best reranking with local features
achieves an even higher F-score of 91.28, and we suspect this is
due to the aggressive updates and instability of the perceptron,
as we do observe the learning curves to be non-monotonic. We
leave the use of more stable learning algorithms to future work.
8The n-best feature extraction already uses relative counts
(Johnson, 2006), which reduced file sizes by at least a factor 4.
type system F1%
D
Collins (2000) 89.7
Henderson (2004) 90.1
Charniak and Johnson (2005) 91.0
updated (Johnson, 2006) 91.4
this work 91.7
G Bod (2003) 90.7Petrov and Klein (2007) 90.1
S McClosky et al (2006) 92.1
Table 4: Comparison of our final results with other
best-performing systems on the whole Section 23.
Types D, G, and S denote discriminative, generative,
and semi-supervised approaches, respectively.
McClosky et al (2006) achieved an even higher ac-
cuarcy (92.1) by leveraging on much larger unla-
belled data. Moreover, their technique is orthogonal
to ours, and we suspect that replacing their n-best
reranker by our forest reranker might get an even
better performance. Plus, except for n-best rerank-
ing, most discriminative methods require repeated
parsing of the training set, which is generally im-
pratical (Petrov and Klein, 2008). Therefore, pre-
vious work often resorts to extremely short sen-
tences (? 15 words) or only looked at local fea-
tures (Taskar et al, 2004; Henderson, 2004; Turian
and Melamed, 2007). In comparison, thanks to the
efficient decoding, our work not only scaled to the
whole Treebank, but also successfully incorporated
non-local features, which showed an absolute im-
provement of 0.44% over that of local features alone.
6 Conclusion
We have presented a framework for reranking on
packed forests which compactly encodes many more
candidates than n-best lists. With efficient approx-
imate decoding, perceptron training on the whole
Treebank becomes practical, which can be done in
about a day even with a Python implementation. Our
final result outperforms both 50-best and 100-best
reranking baselines, and is better than any previ-
ously reported systems trained on the Treebank. We
also devised a dynamic programming algorithm for
forest oracles, an interesting problem by itself. We
believe this general framework could also be applied
to other problems involving forests or lattices, such
as sequence labeling and machine translation.
593
References
Sylvie Billot and Bernard Lang. 1989. The struc-
ture of shared forests in ambiguous parsing. In
Proceedings of ACL ?89, pages 143?151.
Rens Bod. 2003. An efficient implementation of a
new DOP model. In Proceedings of EACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACL.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?208.
Michael Collins. 2000. Discriminative reranking
for natural language parsing. In Proceedings of
ICML, pages 175?182.
Michael Collins. 2002. Discriminative training
methods for hidden markov models: Theory and
experiments with perceptron algorithms. In Pro-
ceedings of EMNLP.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proceedings
of ACL.
Liang Huang and David Chiang. 2005. Better k-
best Parsing. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technologies (IWPT-
2005).
Liang Huang and David Chiang. 2007. Forest
rescoring: Fast decoding with integrated language
models. In Proceedings of ACL.
Mark Johnson. 2006. Features of statisti-
cal parsers. Talk given at the Joint Mi-
crosoft Research and Univ. of Washing-
ton Computational Linguistics Colloquium.
http://www.cog.brown.edu/?mj/papers/ms-
uw06talk.pdf.
Dan Klein and Christopher D. Manning. 2001.
Parsing and Hypergraphs. In Proceedings of the
Seventh International Workshop on Parsing Tech-
nologies (IWPT-2001), 17-19 October 2001, Bei-
jing, China.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn Tree-
bank. Computational Linguistics, 19:313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT-NAACL, New York City,
USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT-NAACL.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Pro-
ceedings of NIPS 20.
Libin Shen, Anoop Sarkar, and Franz Josef Och.
2005. Discriminative reranking for machine
translation. In Proceedings of HLT-NAACL.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Chris Manning. 2004. Max-margin
parsing. In Proceedings of EMNLP.
Joseph Turian and I. Dan Melamed. 2007. Scalable
discriminative learning for natural language pars-
ing and translation. In Proceedings of NIPS 19.
594
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 53?64,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Better k-best Parsing
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
David Chiang
Inst. for Advanced Computer Studies
University of Maryland
3161 AV Williams
College Park, MD 20742
dchiang@umiacs.umd.edu
Abstract
We discuss the relevance of k-best parsing to
recent applications in natural language pro-
cessing, and develop efficient algorithms for
k-best trees in the framework of hypergraph
parsing. To demonstrate the efficiency, scal-
ability and accuracy of these algorithms, we
present experiments on Bikel?s implementation
of Collins? lexicalized PCFG model, and on
Chiang?s CFG-based decoder for hierarchical
phrase-based translation. We show in particu-
lar how the improved output of our algorithms
has the potential to improve results from parse
reranking systems and other applications.
1 Introduction
Many problems in natural language processing (NLP) in-
volve optimizing some objective function over a set of
possible analyses of an input string. This set is often
exponential-sized but can be compactly represented by
merging equivalent subanalyses. If the objective function
is compatible with a packed representation, then it can be
optimized efficiently by dynamic programming. For ex-
ample, the distribution of parse trees for a given sentence
under a PCFG can be represented as a packed forest from
which the highest-probability tree can be easily extracted.
However, when the objective function f has no com-
patible packed representation, exact inference would be
intractable. To alleviate this problem, one common ap-
proach from machine learning is loopy belief propaga-
tion (Pearl, 1988). Another solution (which is popular
in NLP) is to split the computation into two phases: in
the first phase, use some compatible objective function
f ? to produce a k-best list (the top k candidates under
f ?), which serves as an approximation to the full set.
Then, in the second phase, optimize f over all the anal-
yses in the k-best list. A typical example is discrimina-
tive reranking on k-best lists from a generative module,
such as (Collins, 2000) for parsing and (Shen et al, 2004)
for translation, where the reranking model has nonlocal
features that cannot be computed during parsing proper.
Another example is minimum-Bayes-risk decoding (Ku-
mar and Byrne, 2004; Goodman, 1998),where, assum-
ing f ? defines a probability distribution over all candi-
dates, one seeks the candidate with the highest expected
score according to an arbitrary metric (e.g., PARSEVAL
or BLEU); since in general the metric will not be com-
patible with the parsing algorithm, the k-best lists can
be used to approximate the full distribution f ?. A simi-
lar situation occurs when the parser can produce multiple
derivations that are regarded as equivalent (e.g., multiple
lexicalized parse trees corresponding to the same unlexi-
calized parse tree); if we want the maximum a posteriori
parse, we have to sum over equivalent derivations. Again,
the equivalence relation will in general not be compati-
ble with the parsing algorithm, so the k-best lists can be
used to approximate f ?, as in Data Oriented Parsing (Bod,
2000) and in speech recognition (Mohri and Riley, 2002).
Another instance of this k-best approach is cascaded
optimization. NLP systems are often cascades of mod-
ules, where we want to optimize the modules? objective
functions jointly. However, often a module is incompati-
ble with the packed representation of the previous module
due to factors like non-local dependencies. So we might
want to postpone some disambiguation by propagating
k-best lists to subsequent phases, as in joint parsing and
semantic role labeling (Gildea and Jurafsky, 2002; Sutton
and McCallum, 2005), information extraction and coref-
erence resolution (Wellner et al, 2004), and formal se-
mantics of TAG (Joshi and Vijay-Shanker, 1999).
Moreover, much recent work on discriminative train-
ing uses k-best lists; they are sometimes used to ap-
proximate the normalization constant or partition func-
tion (which would otherwise be intractable), or to train a
model by optimizing some metric incompatible with the
packed representation. For example, Och (2003) shows
how to train a log-linear translation model not by max-
imizing the likelihood of training data, but maximizing
the BLEU score (among other metrics) of the model on
53
the data. Similarly, Chiang (2005) uses the k-best pars-
ing algorithm described below in a CFG-based log-linear
translation model in order to learn feature weights which
maximize BLEU.
For algorithms whose packed representations are
graphs, such as Hidden Markov Models and other finite-
state methods, Ratnaparkhi?s MXPARSE parser (Ratna-
parkhi, 1997), and many stack-based machine transla-
tion decoders (Brown et al, 1995; Och and Ney, 2004),
the k-best paths problem is well-studied in both pure
algorithmic context (see (Eppstein, 2001) and (Brander
and Sinclair, 1995) for surveys) and NLP/Speech com-
munity (Mohri, 2002; Mohri and Riley, 2002). This pa-
per, however, aims at the k-best tree algorithms whose
packed representations are hypergraphs (Gallo et al,
1993; Klein and Manning, 2001) (equivalently, and/or
graphs or packed forests), which includes most parsers
and parsing-based MT decoders. Any algorithm express-
ible as a weighted deductive system (Shieber et al, 1995;
Goodman, 1999; Nederhof, 2003) falls into this class. In
our experiments, we apply the algorithms to the lexical-
ized PCFG parser of Bikel (2004), which is very similar
to Collins? Model 2 (Collins, 2003), and to a synchronous
CFG based machine translation system (Chiang, 2005).
2 Previous Work
As pointed out by Charniak and Johnson (2005), the ma-
jor difficulty in k-best parsing is dynamic programming.
The simplest method is to abandon dynamic program-
ming and rely on aggressive pruning to maintain tractabil-
ity, as is used in (Collins, 2000; Bikel, 2004). But this
approach is prohibitively slow, and produces rather low-
quality k-best lists (see Sec. 5.1.2). Gildea and Juraf-
sky (2002) described an O(k2)-overhead extension for the
CKY algorithm and reimplemented Collins? Model 1 to
obtain k-best parses with an average of 14.9 parses per
sentence. Their algorithm turns out to be a special case
of our Algorithm 0 (Sec. 4.1), and is reported to also be
prohibitively slow.
Since the original design of the algorithm described
below, we have become aware of two efforts that are
very closely related to ours, one by Jime?nez and Marzal
(2000) and another done in parallel to ours by Charniak
and Johnson (2005). Jime?nez and Marzal present an al-
gorithm very similar to our Algorithm 3 (Sec. 4.4) while
Charniak and Johnson propose using an algorithm similar
to our Algorithm 0, but with multiple passes to improve
efficiency. They apply this method to the Charniak (2000)
parser to get 50-best lists for reranking, yielding an im-
provement in parsing accuracy.
Our work differs from Jime?nez and Marzal?s in the
following three respects. First, we formulate the pars-
ing problem in the more general framework of hyper-
graphs (Klein and Manning, 2001), making it applica-
ble to a very wide variety of parsing algorithms, whereas
Jime?nez and Marzal define their algorithm as an exten-
sion of CKY, for CFGs in Chomsky Normal Form (CNF)
only. This generalization is not only of theoretical impor-
tance, but also critical in the application to state-of-the-
art parsers such as (Collins, 2003) and (Charniak, 2000).
In Collins? parsing model, for instance, the rules are dy-
namically generated and include unary productions, mak-
ing it very hard to convert to CNF by preprocessing,
whereas our algorithms can be applied directly to these
parsers. Second, our Algorithm 3 has an improvement
over Jime?nez and Marzal which leads to a slight theoret-
ical and empirical speedup. Third, we have implemented
our algorithms on top of state-of-the-art, large-scale sta-
tistical parser/decoders and report extensive experimental
results while Jime?nez and Marzal?s was tested on rela-
tively small grammars.
On the other hand, our algorithms are more scalable
and much more general than the coarse-to-fine approach
of Charniak and Johnson. In our experiments, we can ob-
tain 10000-best lists nearly as fast as 1-best parsing, with
very modest use of memory. Indeed, Charniak (p.c.) has
adopted our Algorithm 3 into his own parser implemen-
tation and confirmed our findings.
In the literature of k shortest-path problems, Minieka
(1974) generalized the Floyd algorithm in a way very
similar to our Algorithm 0 and Lawler (1977) improved
it using an idea similar to but a little slower than the bi-
nary branching case of our Algorithm 1. For hypergraphs,
Gallo et al (1993) study the shortest hyperpath problem
and Nielsen et al (2005) extend it to k shortest hyper-
path. Our work differes from (Nielsen et al, 2005) in two
aspects. First, we solve the problem of k-best derivations
(i.e., trees), not the k-best hyperpaths, although in many
cases they coincide (see Sec. 3 for further discussions).
Second, their work assumes non-negative costs (or prob-
abilities ? 1) so that they can apply Dijkstra-like algo-
rithms. Although generative models, being probability-
based, do not suffer from this problem, more general
models (e.g., log-linear models) may require negative
edge costs (McDonald et al, 2005; Taskar et al, 2004).
Our work, based on the Viterbi algorithm, is still appli-
cable as long as the hypergraph is acyclic, and is used by
McDonald et al (2005) to get the k-best parses.
3 Formulation
Following Klein and Manning (2001), we use weighted
directed hypergraphs (Gallo et al, 1993) as an abstraction
of the probabilistic parsing problem.
Definition 1. An ordered hypergraph (henceforth hy-
pergraph) H is a tuple ?V, E, t,R?, where V is a finite
set of vertices, E is a finite set of hyperarcs, and R
is the set of weights. Each hyperarc e ? E is a triple
54
e = ?T (e), h(e), f (e)?, where h(e) ? V is its head and
T (e) ? V? is a vector of tail nodes. f (e) is a weight func-
tion from R|T (e)| to R. t ? V is a distinguished vertex
called target vertex.
Note that our definition is different from those in previ-
ous work in the sense that the tails are now vectors rather
than sets, so that we can allow multiple occurrences of
the same vertex in a tail and there is an ordering among
the components of a tail.
Definition 2. A hypergraph H is said to be monotonic if
there is a total ordering ? on R such that every weight
function f in H is monotonic in each of its arguments ac-
cording to ?, i.e., if f : Rm 7? R, then ?1 ? i ? m, if ai ?
a?i , then f (a1, ? ? ? , ai, ? ? ? , am) ? f (a1, ? ? ? , a?i , ? ? ? , am).We also define the comparison function min?(a, b) to out-
put a if a ? b, or b if otherwise.
In this paper we will assume this monotonicity, which
corresponds to the optimal substructure property in dy-
namic programming (Cormen et al, 2001).
Definition 3. We denote |e| = |T (e)| to be the arity of the
hyperarc. If |e| = 0, then f (e) ? R is a constant and we
call h(e) a source vertex. We define the arity of a hyper-
graph to be the maximum arity of its hyperarcs.
Definition 4. The backward-star BS(v) of a vertex v is
the set of incoming hyperarcs {e ? E | h(e) = v}. The
in-degree of v is |BS (v)|.
Definition 5. A derivation D of a vertex v in a hyper-
graph H, its size |D| and its weight w(D) are recursively
defined as follows:
? If e ? BS (v) with |e| = 0, then D = ?e, ?? is
a derivation of v, its size |D| = 1, and its weight
w(D) = f (e)().
? If e ? BS (v) where |e| > 0 and Di is a derivation
of Ti(e) for 1 ? i ? |e|, then D = ?e,D1 ? ? ?D|e|? is
a derivation of v, its size |D| = 1 + ?|e|i=1 |Di| and itsweight w(D) = f (e)(w(D1), . . . ,w(D|e|)).
The ordering on weights in R induces an ordering on
derivations: D ? D? iff w(D) ? w(D?).
Definition 6. Define Di(v) to be the ith-best derivation of
v. We can think of D1(v), . . . ,Dk(v) as the components of
a vector we shall denote by D(v). The k-best derivations
problem for hypergraphs, then, is to find D(t) given a hy-
pergraph ?V, E, t,R?.
With the derivations thus ranked, we can introduce a
nonrecursive representation for derivations that is analo-
gous to the use of back-pointers in parser implementa-
tion.
Definition 7. A derivation with back-pointers (dbp) D?
of v is a tuple ?e, j? such that e ? BS(v), and j ?
{1, 2, . . . , k}|e|. There is a one-to-one correspondence ?
between dbps of v and derivations of v:
?e, ( j1 ? ? ? j|e|)? ? ?e,D j1 (T1(e)) ? ? ?D j|e| (T |e|(e))?
Accordingly, we extend the weight function w to dbps:
w(D?) = w(D) if D? ? D. This in turn induces an ordering
on dbps: D? ? D?? iff w(D?) ? w(D??). Let D?i(v) denote the
ith-best dbp of v.
Where no confusion will arise, we use the terms ?deriva-
tion? and ?dbp? interchangeably.
Computationally, then, the k-best problem can be
stated as follows: given a hypergraph H with arity a, com-
pute D?1(t), . . . , D?k(t).1
As shown by Klein and Manning (2001), hypergraphs
can be used to represent the search space of most parsers
(just as graphs, also known as trellises or lattices, can
represent the search space of finite-state automata or
HMMs). More generally, hypergraphs can be used to rep-
resent the search space of most weighted deductive sys-
tem (Nederhof, 2003). For example, the weighted CKY
algorithm given a context-free grammar G = ?N,T, P, S ?
in Chomsky Normal Form (CNF) and an input string w
can be represented as a hypergraph of arity 2 as follows.
Each item [X, i, j] is represented as a vertex v, corre-
sponding to the recognition of nonterminal X spanning
w from positions i+1 through j. For each production rule
X ? YZ in P and three free indices i < j < k, we have a
hyperarc ?((Y, i, k), (Z, k, j)), (X, i, k), f ? corresponding to
the instantiation of the inference rule C??????? in the de-
ductive system of (Shieber et al, 1995), and the weight
function f is defined as f (a, b) = ab ?Pr(X ? YZ), which
is the same as in (Nederhof, 2003). In this sense, hyper-
graphs can be thought of as compiled or instantiated ver-
sions of weighted deductive systems.
A parser does nothing more than traverse this hyper-
graph. In order that derivation values be computed cor-
rectly, however, we need to traverse the hypergraph in a
particular order:
Definition 8. The graph projection of a hypergraph H =
?V, E, t,R? is a directed graph G = ?V, E?? where E? =
{(u, v) | ?e ? BS (v), u ? T (e)}. A hypergraph H is said to
be acyclic if its graph projection G is a directed acyclic
graph; then a topological ordering of H is an ordering
of V that is a topological ordering in G (from sources to
target).
We assume the input hypergraph is acyclic so that we
can use its topological ordering to traverse it. In practice
the hypergraph is typically not known in advance, but the
1Note that although we have defined the weight of a deriva-
tion as a function on derivations, in practice one would store a
derivation?s weight inside the dbp itself, to avoid recomputing
it over and over.
55
p v
u t
q w
(a)
p v
u t
w
(b)
p u v
t
q u w
(c)
Figure 1: Examples of hypergraph, hyperpath, and derivation: (a) a hypergraph H, with t as the target vertex and p, q as
source vertices, (b) a hyperpath pit in H, and (c) a derivation of t in H, where vertex u appears twice with two different
(sub-)derivations. This would be impossible in a hyperpath.
topological ordering often is, so that the (dynamic) hy-
pergraph can be generated in that order. For example, for
CKY it is sufficient to generate all items [X, i, j] before all
items [Y, i?, j?] when j? ? i? > j ? i (X and Y are arbitrary
nonterminals).
Excursus: Derivations and Hyperpaths
The work of Klein and Manning (2001) introduces a cor-
respondence between hyperpaths and derivations. When
extended to the k-best case, however, that correspondence
no longer holds.
Definition 9. (Nielsen et al, 2005) Given a hypergraph
H = ?V, E, t,R?, a hyperpath piv of destination v ? V is an
acyclic minimal hypergraph Hpi = ?Vpi, Epi, v,R? such that
1. Epi ? E
2. v ? Vpi = ?e?Epi (T (e) ? {h(e)})
3. ?u ? Vpi, u is either a source vertex or connected to
a source vertex in Hpi.
As illustrated by Figure 1, derivations (as trees) are dif-
ferent from hyperpaths (as minimal hypergraphs) in the
sense that in a derivation the same vertex can appear more
than once with possibly different sub-derivations while it
is represented at most once in a hyperpath. Thus, the k-
best derivations problem we solve in this paper is very
different in nature from the k-shortest hyperpaths prob-
lem in (Nielsen et al, 2005).
However, the two problems do coincide when k = 1
(since all the sub-derivations must be optimal) and for
this reason the 1-best hyperpath algorithm in (Klein and
Manning, 2001) is very similar to the 1-best tree algo-
rithm in (Knuth, 1977). For k-best case (k > 1), they also
coincide when the hypergraph is isomorphic to a Case-
Factor Diagram (CFD) (McAllester et al, 2004) (proof
omitted). The derivation forest of CFG parsing under the
CKY algorithm, for instance, can be represented as a
CFD while the forest of Earley algorithm can not. An
(A? ?.B?, i, j)
(A? ?.B?, i, j)
(B? .?, j, j)
? ? ? ? ? ?
(B? ?., j, k)
(A? ?B.?, i, k)
Figure 2: An Earley derivation. Note that item (A ?
?.B?, i, j) appears twice (predict and complete).
1: procedure V??????(k)
2: for v ? V in topological order do
3: for e ? BS(v) do . for all incoming hyperarcs
4: D?1(v)? min?(D?1(v), ?e, 1?) . update
Figure 3: The generic 1-best Viterbi algorithm
item (or equivalently, a vertex in hypergraph) can appear
twice in an Earley derivation because of the prediction
rule (see Figure 2 for an example).
The k-best derivations problem has potentially more
applications in tree generation (Knight and Graehl,
2005), which can not be modeled by hyperpaths. But de-
tailed discussions along this line are out of the scope of
this paper.
4 Algorithms
The traditional 1-best Viterbi algorithm traverses the hy-
pergraph in topological order and for each vertex v, cal-
culates its 1-best derivation D1(v) using all incoming hy-
perarcs e ? BS(v) (see Figure 3). If we take the arity of
the hypergraph to be constant, then the overall time com-
plexity of this algorithm is O(|E|).
4.1 Algorithm 0: na??ve
Following (Goodman, 1999; Mohri, 2002), we isolate
two basic operations in line 4 of the 1-best algorithm that
56
can be generalized in order to extend the algorithm: first,
the formation of the derivation ?e, 1? out of |e| best sub-
derivations (this is a generalization of the binary operator
? in a semiring); second, min?, which chooses the better
of two derivations (same as the ? operator in an idem-
potent semiring (Mohri, 2002)). We now generalize these
two operations to operate on k-best lists.
Let r = |e|. The new multiplication operation,
mult?k(e), is performed in three steps:
1. enumerate the kr derivations {?e, j1 ? ? ? jr? | ?i, 1 ?
ji ? k}. Time: O(kr).
2. sort these kr derivations (according to weight).
Time: O(kr log(kr)) = O(rkr log k).
3. select the first k elements from the sorted list of kr
elements. Time: O(k).
So the overall time complexity of mult?k is O(rkr log k).
We also have to extend min? to merge?k, which takes
two vectors of length k (or fewer) as input and outputs the
top k (in sorted order) of the 2k elements. This is similar
to merge-sort (Cormen et al, 2001) and can be done in
linear time O(k). Then, we only need to rewrite line 4 of
the Viterbi algorithm (Figure 3) to extend it to the k-best
case:
4: D?(v) ? merge?k(D?(v),mult?k(e))
and the time complexity for this line is O(|e|k|e| log k),
making the overall complexity O(|E|ka log k) if we con-
sider the arity a of the hypergraph to be constant.2 The
overall space complexity is O(|V |k) since for each vertex
we need to store a vector of length k.
In the context of CKY parsing for CFG, the 1-best
Viterbi algorithm has complexity O(n3|P|) while the k-
best version is O(n3|P|k2 log k), which is slower by a fac-
tor of O(k2 log k).
4.2 Algorithm 1: speed up mult?k
First we seek to exploit the fact that input vectors are all
sorted and the function f is monotonic; moreover, we are
only interested in the top k elements of the k|e| possibili-
ties.
Define 1 to be the vector whose elements are all 1; de-
fine bi to be the vector whose elements are all 0 except
bii = 1.As we compute pe = mult?k(e), we maintain a candi-
date set C of derivations that have the potential to be the
next best derivation in the list. If we picture the input as an
|e|-dimensional space, C contains those derivations that
2Actually, we do not need to sort all k|e| elements in order
to extract the top k among them; there is an efficient algorithm
(Cormen et al, 2001) that can select the kth best element from
the k|e| elements in time O(k|e|). So we can improve the overhead
to O(ka).
have not yet been included in pe, but are on the bound-
ary with those which have. It is initialized to {?e, 1?}. At
each step, we extract the best derivation from C?call it
?e, j??and append it to pe. Then ?e, j? must be replaced
in C by its neighbors,
{?e, j + bl? | 1 ? l ? |e|}
(see Figure 4.2 for an illustration). We implement C as a
priority queue (Cormen et al, 2001) to make the extrac-
tion of its best derivation efficient. At each iteration, there
are one E??????-M?? and |e| I????? operations. If we use
a binary-heap implementation for priority queues, we get
O(|e| log k|e|) time complexity for each iteration.3 Since
we are only interested in the top k elements, there are
k iterations and the time complexity for a single mult?k
is O(k|e| log k|e|), yielding an overall time complexity of
O(|E|k log k) and reducing the multiplicative overhead by
a factor of O(ka?1) (again, assuming a is constant). In
the context of CKY parsing, this reduces the overhead
to O(k log k). Figure 5 shows the additional pseudocode
needed for this algorithm. It is integrated into the Viterbi
algorithm (Figure 3) simply by rewriting line 4 of to in-
voke the function M???(e, k):
4: D?(v) ? merge?k(D?(v),M???(e, k))
4.3 Algorithm 2: combine merge?k into mult?k
We can further speed up both merge?k and mult?k by a
similar idea. Instead of letting each mult?k generate a full
k derivations for each hyperarc e and only then applying
merge?k to the results, we can combine the candidate sets
for all the hyperarcs into a single candidate set. That is,
we initialize C to {?e, 1? | e ? BS (v)}, the set of all the
top parses from each incoming hyperarc (cf. Algorithm
1). Indeed, it suffices to keep only the top k out of the
|BS (v)| candidates in C, which would lead to a significant
speedup in the case where |BS (v)| ? k. 4 Now the top
derivation in C is the top derivation for v. Then, whenever
we remove an element ?e, j? from C, we replace it with
the |e| elements {?e, j + bl? | 1 ? l ? |e|} (again, as in
Algorithm 1). The full pseudocode for this algorithm is
shown in Figure 6.
4.4 Algorithm 3: compute mult?k lazily
Algorithm 2 exploited the idea of lazy computation: per-
forming mult?k only as many times as necessary. But this
algorithm still calculates a full k-best list for every ver-
tex in the hypergraph, whereas we are only interested in
3If we maintain a Min-Heap along with the Min-Heap, we
can reduce the per-iteration cost to O(|e| log k), and with Fi-
bonacci heap we can further improve it to be O(|e| + log k). But
these techniques do not change the overall complexity when a
is constant, as we will see.
4This can be implemented by a linear-time randomized-
selection algorithm (a.k.a. quick-select) (Cormen et al, 2001).
57
2
2 ?
0
?
?
?
?
1 ?
1 2 4
(a)
2
2 ?
?
?
?
3 ?
0 1
?
?
?
?
2 ?
1 2 4
(b)
2
2
?
?
?
?
3 ?
?
?
?
4
0 1 2 ?
?
?
?
4
1 2 4
(c)
Figure 4: An illustration of Algorithm 1 in |e| = 2 dimensions. Here k = 3, ? is the numerical ?, and the monotonic
function f is defined as f (a, b) = a + b. Italic numbers on the x and y axes are ai?s and b j?s, respectively. We want
to compute the top 3 results from f (ai, b j) with 1 ? i, j ? 3. In each iteration the current frontier is shown in oval
boxes, with the bold-face denoting the best element among them. That element will be extracted and replaced by its
two neighbors (? and?) in the next iteration.
1: function M???(e, k)
2: cand ? {?e, 1?} . initialize the heap
3: p? empty list . the result of mult?k
4: while |p| < k and |cand| > 0 do
5: A?????N???(cand,p, k)
6: return p
7:
8: procedure A?????N???(cand, p)
9: ?e, j? ? E??????-M??(cand)
10: append ?e, j? to p
11: for i? 1 . . . |e| do . add the |e| neighbors
12: j? ? j + bi
13: if j?i ? |D?(Ti(e))| and ?e, j?? < cand then
14: I?????(cand, ?e, j??) . add to heap
Figure 5: Part of Algorithm 1.
1: procedure F???A??KB???(k)
2: for v ? V in topological order do
3: F???KB???(v, k)
4:
5: procedure F???KB???(v, k)
6: G??C?????????(v, k) . initialize the heap
7: while |D?(v)| < k and |cand[v]| > 0 do
8: A?????N???(cand[v], D?(v))
9:
10: procedure G??C?????????(v, k)
11: temp? {?e, 1? | e ? BS (v)}
12: cand[v]? the top k elements in temp . prune
away useless candidates
13: H??????(cand[v])
Figure 6: Algorithm 2
1: procedure L???K??B???(v, k, k?) . k? is the global k
2: if |D?(v)| ? k then . kth derivation already computed?
3: return
4: if cand[v] is not defined then . first visit of vertex v?
5: G??C?????????(v, k?) . initialize the heap
6: append E??????-M??(cand[v]) to D?(v) . 1-best
7: while |D?(v)| < k and |cand[v]| > 0 do
8: ?e, j? ? D?|D?(v)|(v) . last derivation
9: L???N???(cand[v], e, j, k?) . update the heap, adding the successors of last derivation
10: append E??????-M??(cand[v]) to D?(v) . get the next best derivation and delete it from the heap
11:
12: procedure L???N???(cand, e, j, k?)
13: for i? 1 . . . |e| do . add the |e| neighbors
14: j? ? j + bi
15: L???K??B???(Ti(e), j?i , k?) . recursively solve a sub-problem
16: if j?i ? |D?(Ti(e))| and ?e, j?? < cand then . if it exists and is not in heap yet
17: I?????(cand, ?e, j??) . add to heap
Figure 7: Algorithm 3
58
Algorithm Time Complexity
1-best Viterbi O(E)
Algorithm 0 O(Eka log k)
Algorithm 1 O(Ek log k)
Algorithm 2 O(E + Vk log k)
Algorithm 3 O(E + |Dmax|k log k)
generalized J&M O(E + |Dmax|k log(d + k))
Table 1: Summary of Algorithms.
the k-best derivations of the target vertex (goal item). We
can therefore take laziness to an extreme by delaying the
whole k-best calculation until after parsing. Algorithm 3
assumes an initial parsing phase that generates the hyper-
graph and finds the 1-best derivation of each item; then
in the second phase, it proceeds as in Algorithm 2, but
starts at the goal item and calls itself recursively only as
necessary. The pseudocode for this algorithm is shown in
Figure 7. As a side note, this second phase should be ap-
plicable also to a cyclic hypergraph as long as its deriva-
tion weights are bounded.
Algorithm 2 has an overall complexity of O(|E| +
|V |k log k) and Algorithm 3 is O(|E|+ |Dmax|k log k) where
|Dmax| is the size of the longest among all top k deriva-
tions (for CFG in CNF, |D| = 2n?1 for all D, so |Dmax| is
O(n)). These are significant improvements against Algo-
rithms 0 and 1 since it turns the multiplicative overhead
into an additive overhead. In practice, |E| usually dom-
inates, as in CKY parsing of CFG. So theoretically the
running times grow very slowly as k increases, which is
exactly demonstrated by our experiments below.
4.5 Summary and Discussion of Algorithms
The four algorithms, along with the 1-best Viterbi algo-
rithm and the generalized Jime?nez and Marzal algorithm,
are compared in Table 1.
The key difference between our Algorithm 3 and
Jime?nez and Marzal?s algorithm is the restriction of top
k candidates before making heaps (line 11 in Figure 6,
see also Sec. 4.3). Without this line Algorithm 3 could
be considered as a generalization of the Jime?nez and
Marzal algorithm to the case of acyclic monotonic hy-
pergraphs. This line is also responsible for improving
the time complexity from O(|E| + |Dmax|k log(d + k))
(generalized Jime?nez and Marzal algorithm) to O(|E| +
|Dmax|k log k), where d = maxv |BS (v)| is the maximum
in-degree among all vertices. So in case k < d, our algo-
rithm outperforms Jime?nez and Marzal?s.
5 Experiments
We report results from two sets of experiments. For prob-
abilistic parsing, we implemented Algorithms 0, 1, and
3 on top of a widely-used parser (Bikel, 2004) and con-
ducted experiments on parsing efficiency and the qual-
ity of the k-best-lists. We also implemented Algorithms 2
and 3 in a parsing-based MT decoder (Chiang, 2005) and
report results on decoding speed.
5.1 Experiment 1: Bikel Parser
Bikel?s parser (2004) is a state-of-the-art multilingual
parser based on lexicalized context-free models (Collins,
2003; Eisner, 2000). It does support k-best parsing, but,
following Collins? parse-reranking work (Collins, 2000)
(see also Section 5.1.2), it accomplishes this by sim-
ply abandoning dynamic programming, i.e., no items
are considered equivalent (Charniak and Johnson, 2005).
Theoretically, the time complexity is exponential in n (the
input sentence length) and constant in k, since, without
merging of equivalent items, there is no limit on the num-
ber of items in the chart. In practice, beam search is used
to reduce the observed time.5 But with the standard beam
width of 10?4, this method becomes prohibitively expen-
sive for n ? 25 on Bikel?s parser. Collins (2000) used
a narrower 10?3 beam and further applied a cell limit of
100,6 but, as we will show below, this has a detrimental
effect on the quality of the output. We therefore omit this
method from our speed comparisons, and use our imple-
mentation of Algorithm 0 (na??ve) as the baseline.
We implemented our k-best Algorithms 0, 1, and 3 on
top of Bikel?s parser and conducted experiments on a 2.4
GHz 64-bit AMD Opteron with 32 GB memory. The pro-
gram is written in Java 1.5 running on the Sun JVM in
server mode with a maximum heap size of 5 GB. For this
experiment, we used sections 02?21 of the Penn Tree-
bank (PTB) (Marcus et al, 1993) as the training data and
section 23 (2416 sentences) for evaluation, as is now stan-
dard. We ran Bikel?s parser using its settings to emulate
Model 2 of (Collins, 2003).
5.1.1 Efficiency
We tested our algorithms under various conditions. We
first did a comparison of the average parsing time per
sentence of Algorithms 0, 1, and 3 on section 23, with
k ? 10000 for the standard beam of width 10?4. Fig-
ure 8(a) shows that the parsing speed of Algorithm 3 im-
proved dramatically against the other algorithms and is
nearly constant in k, which exactly matches the complex-
ity analysis. Algorithm 1 (k log k) also significantly out-
performs the baseline na??ve algorithm (k2 log k).
We also did a comparison between our Algorithm 3
and the Jime?nez and Marzal algorithm in terms of average
5In beam search, or threshold pruning, each cell in the chart
(typically containing all the items corresponding to a span [i, j])
is reduced by discarding all items that are worse than ? times the
score of the best item in the cell. This ? is known as the beam
width.
6In this type of pruning, also known as histogram pruning,
only the ? best items are kept in each cell. This ? is called the
cell limit.
59
 1.5
 2.5
 3.5
 4.5
 5.5
 6.5
 7.5
 1  10  100  1000  10000
Av
er
ag
e 
Pa
rs
in
g 
Ti
m
e 
(se
co
nd
s)
k
Algorithm 0
Algorithm 1
Algorithm 3
(a) Average parsing speed (Algs. 0 vs. 1 vs. 3, log-log)
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
 2.6
 2  4  8  16  32  64
Av
er
ag
e 
He
ap
 S
ize
k
JM Algorithm with 10-5 beam
Algorithm 3 with 10-5 beam 
JM Algorithm with 10-4 beam
Algorithm 3 with 10-4 beam 
(b) Average heap size (Alg. 3 vs. Jime?nez and Marzal)
Figure 8: Efficiency results of the k-best Algorithms, compared to Jime?nez and Marzal?s algorithm
heap size. Figure 8(b) shows that for larger k, the two al-
gorithms have the same average heap size, but for smaller
k, our Algorithm 3 has a considerably smaller average
heap size. This difference is useful in applications where
only short k-best lists are needed. For example, McDon-
ald et al (2005) find that k = 5 gives optimal parsing
accuracy.
5.1.2 Accuracy
Our efficient k-best algorithms enable us to search over
a larger portion of the whole search space (e.g. by less
aggressive pruning), thus producing k-best lists with bet-
ter quality than previous methods. We demonstrate this
by comparing our k-best lists to those in (Ratnaparkhi,
1997), (Collins, 2000) and the parallel work by Char-
niak and Johnson (2005) in several ways, including oracle
reranking and average number of found parses.
Ratnaparkhi (1997) introduced the idea of oracle
reranking: suppose there exists a perfect reranking
scheme that magically picks the best parse that has the
highest F-score among the top k parses for each sentence.
Then the performance of this oracle reranking scheme
is the upper bound of any actual reranking system like
(Collins, 2000).As k increases, the F-score is nondecreas-
ing, and there is some k (which might be very large) at
which the F-score converges.
Ratnaparkhi reports experiments using oracle rerank-
ing with his statistical parser MXPARSE, which can
compute its k-best parses (in his experiments, k = 20).
Collins (2000), in his parse-reranking experiments, used
his Model 2 parser (Collins, 2003) with a beam width of
10?3 together with a cell limit of 100 to obtain k-best lists;
the average number of parses obtained per sentence was
29.2, the maximum, 101.7 Charniak and Johnson (2005)
use coarse-to-fine parsing on top of the Charniak (2000)
parser and get 50-best lists for section 23.
Figure 9(a) compares the results of oracle reranking.
Collins? curve converges at around k = 50 while ours
continues to increase. With a beam width of 10?4 and
k = 100, our parser plus oracle reaches an F-score of
96.4%, compared to Collins? 94.9%. Charniak and John-
son?s work, however, is based on a completely different
parser whose 1-best F-score is 1.5 points higher than the
1-bests of ours and Collins?, making it difficult to com-
pare in absolute numbers. So we instead compared the
relative improvement over 1-best. Figure 9(b) shows that
our work has the largest percentage of improvement in
terms of F-score when k > 20.
To further explore the impact of Collins? cell limit on
the quality of k-best lists, we plotted average number of
parses for a given sentence length (Figure 10). Generally
speaking, as input sentences get longer, the number of
parses grows (exponentially). But we see that the curve
for Collins? k-best list goes down for large k (> 40). We
suspect this is due to the cell limit of 100 pruning away
potentially good parses too early in the chart. As sen-
tences get longer, it is more likely that a lower-probability
parse might contribute eventually to the k-best parses. So
we infer that Collins? k-best lists have limited quality for
large k, and this is demonstrated by the early convergence
of its oracle-reranking score. By comparison, our curves
of both beam widths continue to grow with k = 100.
All these experiments suggest that our k-best parses are
of better quality than those from previous k-best parsers,
7The reason the maximum is 101 and not 100 is that Collins
merged the 100-best list using a beam of 10?3 with the 1-best
list using a beam of 10?4 (Collins, p.c.).
60
 86
 88
 90
 92
 94
 96
 98
 1  2  5  10  20  30  50  70  100
O
ra
cle
 F
-s
co
re
k
(Charniak and Johnson, 2005)
This work with beam width 10-4(Collins, 2000)
(Ratnaparkhi, 1997)
(a) Oracle Reranking
 0
 2
 4
 6
 8
 10
 1  2  5  10  20  30  50  70  100
Pe
rc
en
ta
ge
 o
f I
m
pr
ov
em
en
t o
ve
r 1
-b
es
t
k
(Charniak and Johnson, 2005)
This work with beam width 10-4(Collins, 2000)
(Ratnaparkhi, 1997)
(b) Relative Improvement
Figure 9: Absolutive and Relative F-scores of oracle reranking for the top k (? 100) parses for section 23, compared
to (Charniak and Johnson, 2005), (Collins, 2000) and (Ratnaparkhi, 1997).
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70
Av
er
ag
e 
Nu
m
be
r o
f P
ar
se
s
Sentence Length
This work with beam width 10-4
This work with beam width 10-3
(Collins, 2000) with beam width 10-3
Figure 10: Average number of parses for each sentence length in section 23, using k=100, with beam width 10?4 and
10?3, compared to (Collins, 2000).
61
 0.001
 0.01
 0.1
 1
 10
 10  100  1000  10000  100000  1e+06
s
e
c
o
n
ds
k
Algorithm 2
Algorithm 3
Figure 11: Algorithm 2 compared with Algorithm 3 (of-
fline) on MT decoding task. Average time (both exclud-
ing initial 1-best phase) vs. k (log-log).
and similar quality to those from (Charniak and Johnson,
2005) which has so far the highest F-score after rerank-
ing, and this might lead to better results in real parse
reranking.
5.2 Experiment 2: MT decoder
Our second experiment was on a CKY-based decoder
for a machine translation system (Chiang, 2005), imple-
mented in Python 2.4 accelerated with Psyco 1.3 (Rigo,
2004). We implemented Algorithms 2 and 3 to compute
k-best English translations of Mandarin sentences. Be-
cause the CFG used in this system is large to begin with
(millions of rules), and then effectively intersected with
a finite-state machine on the English side (the language
model), the grammar constant for this system is quite
large. The decoder uses a relatively narrow beam search
for efficiency.
We ran the decoder on a 2.8 GHz Xeon with 4 GB of
memory, on 331 sentences from the 2002 NIST MTEval
test set. We tested Algorithm 2 for k = 2i, 3 ? i ? 10, and
Algorithm 3 (offline algorithm) for k = 2i, 3 ? i ? 20.
For each sentence, we measured the time to calculate the
k-best list, not including the initial 1-best parsing phase.
We then averaged the times over our test set to produce
the graph of Figure 11, which shows that Algorithm 3
runs an average of about 300 times faster than Algorithm
2. Furthermore, we were able to test Algorithm 3 up to
k = 106 in a reasonable amount of time.8
8The curvature in the plot for Algorithm 3 for k < 1000
may be due to lack of resolution in the timing function for short
times.
6 Conclusion
The problem of k-best parsing and the effect of k-best list
size and quality on applications are subjects of increas-
ing interest for NLP research. We have presented here
a general-purpose algorithm for k-best parsing and ap-
plied it to two state-of-the-art, large-scale NLP systems:
Bikel?s implementation of Collins? lexicalized PCFG
model (Bikel, 2004; Collins, 2003) and Chiang?s syn-
chronous CFG based decoder (Chiang, 2005) for machine
translation. We hope that this work will encourage further
investigation into whether larger and better k-best lists
will improve performance in NLP applications, questions
which we ourselves intend to pursue as well.
Acknowledgements
We would like to thank one of the anonymous reviewers
of a previous version of this paper for pointing out the
work by Jime?nez and Marzal, and Eugene Charniak and
Mark Johnson for providing an early draft of their paper
and very useful comments. We are also extremely grate-
ful to Dan Bikel for the help in experiments, and Michael
Collins for providing the data in his paper. Our thanks
also go to Dan Gildea, Jonathan Graehl, Julia Hock-
enmaier, Aravind Joshi, Kevin Knight, Daniel Marcu,
Mitch Marcus, Ryan McDonald, Fernando Pereira, Gior-
gio Satta, Libin Shen, and Hao Zhang.
References
Bikel, D. M. (2004). Intricacies of Collins? parsing
model. Computational Linguistics, 30, 479?511.
Bod, R. (2000). Parsing with the shortest derivation. In
Proc. Eighteenth International Conference on Compu-
tational Linguistics (COLING), pages 69?75.
Brander, A. and Sinclair, M. (1995). A comparative
study of k-shortest path algorithms. In Proc. 11th UK
Performance Engineering Workshop for Computer and
Telecommunications Systems.
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra,
V. J., Jelinek, F., Lai, J. C., and Mercer, R. L. (1995).
Method and system for natural language translation.
U. S. Patent 5,477,451.
Charniak, E. (2000). A maximum-entropy-inspired
parser. In Proc. First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL), pages 132?139.
Charniak, E. and Johnson, M. (2005). Coarse-to-fine-
grained n-best parsing and discriminative reranking. In
Proc. ACL 2005.
Chiang, D. (2005). A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
62
Collins, M. (2000). Discriminative reranking for natural
language parsing. In Proc. Seventeenth International
Conference on Machine Learning (ICML), pages 175?
182. Morgan Kaufmann.
Collins, M. (2003). Head-driven statistical models for
natural language parsing. Computational Linguistics,
29, 589?637.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein,
C. (2001). Introduction to Algorithms. MIT Press, sec-
ond edition.
Eisner, J. (2000). Bilexical grammars and their cubic-
time parsing algorithms. In H. Bunt and A. Nijholt,
editors, Advances in Probabilistic and Other Parsing
Technologies, pages 29?62. Kluwer Academic Pub-
lishers.
Eppstein, D. (2001). Bibliography on k short-
est paths and other ?k best solutions? problems.
http://www.ics.uci.edu/?eppstein/bibs/kpath.bib.
Gallo, G., Longo, G., and Pallottino, S. (1993). Directed
hypergraphs and applications. Discrete Applied Math-
ematics, 42(2), 177?201.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28(3),
245?288.
Goodman, J. (1998). Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Goodman, J. (1999). Semiring parsing. Computational
Linguistics, 25, 573?605.
Jime?nez, V. M. and Marzal, A. (2000). Computation
of the n best parse trees for weighted and stochastic
context-free grammars. In Proc. of the Joint IAPR In-
ternational Workshops on Advances in Pattern Recog-
nition.
Joshi, A. K. and Vijay-Shanker, K. (1999). Composi-
tional semantics with lexicalized tree-adjoining gram-
mar (LTAG): How much underspecification is neces-
sary? In H. C. Bunt and E. G. C. Thijsse, editors, Proc.
IWCS-3, pages 131?145.
Klein, D. and Manning, C. D. (2001). Parsing and hy-
pergraphs. In Proceedings of the Seventh International
Workshop on Parsing Technologies (IWPT-2001), 17-
19 October 2001, Beijing, China. Tsinghua University
Press.
Knight, K. and Graehl, J. (2005). An overview of proba-
bilistic tree transducers for natural language process-
ing. In Proc. of the Sixth International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), LNCS.
Knuth, D. (1977). A generalization of Dijkstra?s algo-
rithm. Information Processing Letters, 6(1).
Kumar, S. and Byrne, W. (2004). Minimum bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
Lawler, E. L. (1977). Comment on computing the k short-
est paths in a graph. Comm. of the ACM, 20(8), 603?
604.
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
(1993). Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19,
313?330.
McAllester, D., Collins, M., and Pereira, F. (2004). Case-
factor diagrams for structured probabilistic modeling.
In Proc. UAI 2004.
McDonald, R., Crammer, K., and Pereira, F. (2005). On-
line large-margin training of dependency parsers. In
Proc. ACL 2005.
Minieka, E. (1974). On computing sets of shortest paths
in a graph. Comm. of the ACM, 17(6), 351?353.
Mohri, M. (2002). Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3), 321?350.
Mohri, M. and Riley, M. (2002). An efficient algorithm
for the n-best-strings problem. In Proceedings of the
International Conference on Spoken Language Pro-
cessing 2002 (ICSLP ?02), Denver, Colorado.
Nederhof, M.-J. (2003). Weighted deductive parsing and
Knuth?s algorithm. Computational Linguistics, pages
135?143.
Nielsen, L. R., Andersen, K. A., and Pretolani, D. (2005).
Finding the k shortest hyperpaths. Computers and Op-
erations Research.
Och, F. J. (2003). Minimum error rate training in statis-
tical machine translation. In Proc. ACL 2003, pages
160?167.
Och, F. J. and Ney, H. (2004). The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30, 417?449.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
Ratnaparkhi, A. (1997). A linear observed time statistical
parser based on maximum entropy models. In Proc.
EMNLP 1997, pages 1?10.
Rigo, A. (2004). Representation-based just-in-time spe-
cialization and the Psyco prototype for Python. In
N. Heintze and P. Sestoft, editors, Proceedings of the
2004 ACM SIGPLAN Workshop on Partial Evaluation
and Semantics-based Program Manipulation, pages
15?26.
63
Shen, L., Sarkar, A., and Och, F. J. (2004). Discrimina-
tive reranking for machine translation. In Proc. HLT-
NAACL 2004.
Shieber, S., Schabes, Y., and Pereira, F. (1995). Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24, 3?36.
Sutton, C. and McCallum, A. (2005). Joint parsing and
semantic role labeling. In Proc. CoNLL 2005.
Taskar, B., Klein, D., Collins, M., Koller, D., and Man-
ning, C. (2004). Max-margin parsing. In Proc. EMNLP
2004.
Wellner, B., McCallum, A., Peng, F., and Hay, M. (2004).
An integrated, conditional model of information ex-
traction and coreference with application to citation
matching. In Proc. UAI 2004.
64
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 65?73,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Machine Translation as Lexicalized Parsing with Hooks
Liang Huang
Dept. of Computer & Information Science
University of Pennsylvania
Philadelphia, PA 19104
Hao Zhang and Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
Abstract
We adapt the ?hook? trick for speeding up
bilexical parsing to the decoding problem
for machine translation models that are
based on combining a synchronous con-
text free grammar as the translation model
with an n-gram language model. This
dynamic programming technique yields
lower complexity algorithms than have
previously been described for an impor-
tant class of translation models.
1 Introduction
In a number of recently proposed synchronous
grammar formalisms, machine translation of new
sentences can be thought of as a form of parsing on
the input sentence. The parsing process, however,
is complicated by the interaction of the context-free
translation model with an m-gram1 language model
in the output language. While such formalisms ad-
mit dynamic programming solutions having poly-
nomial complexity, the degree of the polynomial is
prohibitively high.
In this paper we explore parallels between transla-
tion and monolingual parsing with lexicalized gram-
mars. Chart items in translation must be augmented
with words from the output language in order to cap-
ture language model state. This can be thought of as
a form of lexicalization with some similarity to that
of head-driven lexicalized grammars, despite being
unrelated to any notion of syntactic head. We show
1We speak of m-gram language models to avoid confusion
with n, which here is the length of the input sentence for trans-
lation.
that techniques for parsing with lexicalized gram-
mars can be adapted to the translation problem, re-
ducing the complexity of decoding with an inversion
transduction grammar and a bigram language model
from O(n7) to O(n6). We present background on
this translation model as well as the use of the tech-
nique in bilexicalized parsing before describing the
new algorithm in detail. We then extend the al-
gorithm to general m-gram language models, and
to general synchronous context-free grammars for
translation.
2 Machine Translation using Inversion
Transduction Grammar
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a type of context-free grammar (CFG) for
generating two languages synchronously. To model
the translational equivalence within a sentence pair,
ITG employs a synchronous rewriting mechanism to
relate two sentences recursively. To deal with the
syntactic divergence between two languages, ITG
allows the inversion of rewriting order going from
one language to another at any recursive level. ITG
in Chomsky normal form consists of unary produc-
tion rules that are responsible for generating word
pairs:
X ? e/f
X ? e/
X ? /f
where e is a source language word, f is a foreign lan-
guage word, and  means the null token, and binary
production rules in two forms that are responsible
for generating syntactic subtree pairs:
X ? [Y Z]
65
and
X ? ?Y Z?
The rules with square brackets enclosing the
right-hand side expand the left-hand side symbol
into the two symbols on the right-hand side in the
same order in the two languages, whereas the rules
with angled brackets expand the left hand side sym-
bol into the two right-hand side symbols in reverse
order in the two languages. The first class of rules
is called straight rule. The second class of rules is
called inverted rule.
One special case of 2-normal ITG is the so-called
Bracketing Transduction Grammar (BTG) which
has only one nonterminal A and two binary rules
A ? [AA]
and
A ? ?AA?
By mixing instances of the inverted rule with
those of the straight rule hierarchically, BTG can
meet the alignment requirements of different lan-
guage pairs. There exists a more elaborate version
of BTG that has 4 nonterminals working together
to guarantee the property of one-to-one correspon-
dence between alignments and synchronous parse
trees. Table 1 lists the rules of this BTG. In the
discussion of this paper, we will consider ITG in 2-
normal form.
By associating probabilities or weights with the
bitext production rules, ITG becomes suitable for
weighted deduction over bitext. Given a sentence
pair, searching for the Viterbi synchronous parse
tree, of which the alignment is a byproduct, turns out
to be a two-dimensional extension of PCFG parsing,
having time complexity of O(n6), where n is the
length of the English string and the foreign language
string. A more interesting variant of parsing over bi-
text space is the asymmetrical case in which only the
foreign language string is given so that Viterbi pars-
ing involves finding the English string ?on the fly?.
The process of finding the source string given its tar-
get counterpart is decoding. Using ITG, decoding is
a form of parsing.
2.1 ITG Decoding
Wu (1996) presented a polynomial-time algorithm
for decoding ITG combined with an m-gram lan-
guage model. Such language models are commonly
used in noisy channel models of translation, which
find the best English translation e of a foreign sen-
tence f by finding the sentence e that maximizes the
product of the translation model P (f |e) and the lan-
guage model P (e).
It is worth noting that since we have specified ITG
as a joint model generating both e and f , a language
model is not theoretically necessary. Given a foreign
sentence f , one can find the best translation e?:
e? = argmax
e
P (e, f)
= argmax
e
?
q
P (e, f, q)
by approximating the sum over parses q with the
probability of the Viterbi parse:
e? = argmax
e
max
q
P (e, f, q)
This optimal translation can be computed in using
standard CKY parsing over f by initializing the
chart with an item for each possible translation of
each foreign word in f , and then applying ITG rules
from the bottom up.
However, ITG?s independence assumptions are
too strong to use the ITG probability alone for ma-
chine translation. In particular, the context-free as-
sumption that each foreign word?s translation is cho-
sen independently will lead to simply choosing each
foreign word?s single most probable English trans-
lation with no reordering. In practice it is beneficial
to combine the probability given by ITG with a local
m-gram language model for English:
e? = argmax
e
max
q
P (e, f, q)Plm(e)?
with some constant language model weight ?. The
language model will lead to more fluent output by
influencing both the choice of English words and the
reordering, through the choice of straight or inverted
rules. While the use of a language model compli-
cates the CKY-based algorithm for finding the best
translation, a dynamic programming solution is still
possible. We extend the algorithm by storing in each
chart item the English boundary words that will af-
fect the m-gram probabilities as the item?s English
string is concatenated with the string from an adja-
cent item. Due to the locality of m-gram language
66
Structural Rules Lexical Rules
S ? A
S ? B
S ? C
A ? [AB]
A ? [BB]
A ? [CB]
A ? [AC]
A ? [BC]
A ? [CC]
B ? ?AA?
B ? ?BA?
B ? ?CA?
B ? ?AC?
B ? ?BC?
B ? ?CC?
C ? ei/fj
C ? /fj
C ? ei/
Table 1: Unambiguous BTG
model, only m?1 boundary words need to be stored
to compute the new m-grams produced by combin-
ing two substrings. Figure 1 illustrates the combi-
nation of two substrings into a larger one in straight
order and inverted order.
3 Hook Trick for Bilexical Parsing
A traditional CFG generates words at the bottom of
a parse tree and uses nonterminals as abstract rep-
resentations of substrings to build higher level tree
nodes. Nonterminals can be made more specific to
the actual substrings they are covering by associ-
ating a representative word from the nonterminal?s
yield. When the maximum number of lexicalized
nonterminals in any rule is two, a CFG is bilexical.
A typical bilexical CFG in Chomsky normal form
has two types of rule templates:
A[h] ? B[h]C[h?]
or
A[h] ? B[h?]C[h]
depending on which child is the head child that
agrees with the parent on head word selection.
Bilexical CFG is at the heart of most modern statisti-
cal parsers (Collins, 1997; Charniak, 1997), because
the statistics associated with word-specific rules are
more informative for disambiguation purposes. If
we use A[i, j, h] to represent a lexicalized con-
stituent, ?(?) to represent the Viterbi score function
applicable to any constituent, and P (?) to represent
the rule probability function applicable to any rule,
Figure 2 shows the equation for the dynamic pro-
gramming computation of the Viterbi parse. The two
terms of the outermost max operator are symmetric
cases for heads coming from left and right. Contain-
ing five free variables i,j,k,h?,h, ranging over 1 to
n, the length of input sentence, both terms can be
instantiated in n5 possible ways, implying that the
complexity of the parsing algorithm is O(n5).
Eisner and Satta (1999) pointed out we don?t have
to enumerate k and h? simultaneously. The trick,
shown in mathematical form in Figure 2 (bottom) is
very simple. When maximizing over h?, j is irrele-
vant. After getting the intermediate result of maxi-
mizing over h?, we have one less free variable than
before. Throughout the two steps, the maximum
number of interacting variables is 4, implying that
the algorithmic complexity is O(n4) after binarizing
the factors cleverly. The intermediate result
max
h?,B
[?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])]
can be represented pictorially as
C[h]
A
i k . The
same trick works for the second max term in
Equation 1. The intermediate result coming from
binarizing the second term can be visualized as
A
k
B[h]
j
. The shape of the intermediate re-
sults gave rise to the nickname of ?hook?. Melamed
(2003) discussed the applicability of the hook trick
for parsing bilexical multitext grammars. The anal-
ysis of the hook trick in this section shows that it is
essentially an algebraic manipulation. We will for-
mulate the ITG Viterbi decoding algorithm in a dy-
namic programming equation in the following sec-
tion and apply the same algebraic manipulation to
produce hooks that are suitable for ITG decoding.
4 Hook Trick for ITG Decoding
We start from the bigram case, in which each de-
coding constituent keeps a left boundary word and
67
tu11 u12 v12v11 u21 u22 v22v21
X
Y Z[ ]
Ss
u21
X
Y Z
Ss t
< >
v21 v22 u11 u12 v11 v12u22
(a) (b)
Figure 1: ITG decoding using 3-gram language model. Two boundary words need to be kept on the left (u)
and right (v) of each constituent. In (a), two constituents Y and Z spanning substrings s, S and S, t of the
input are combined using a straight rule X ? [Y Z]. In (b), two constituents are combined using a inverted
rule X ? ?Y Z?. The dashed line boxes enclosing three words are the trigrams produced from combining
two substrings.
?(A[i, j, h]) = max
?
?
?
?
?
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
,
max
k,h?,B,C
[
?(B[i, k, h]) ? ?(C[k, j, h?]) ? P (A[h] ? B[h]C[h?])
]
?
?
?
?
?
(1)
max
k,h?,B,C
[
?(B[i, k, h?]) ? ?(C[k, j, h]) ? P (A[h] ? B[h?]C[h])
]
= max
k,C
[
max
h?,B
[
?(B[i, k, h?]) ? P (A[h] ? B[h?]C[h])
]
? ?(C[k, j, h])
]
Figure 2: Equation for bilexical parsing (top), with an efficient factorization (bottom)
a right boundary word. The dynamic programming
equation is shown in Figure 3 (top) where i,j,k range
over 1 to n, the length of input foreign sentence, and
u,v,v1,u2 (or u,v,v2,u1) range over 1 to V , the size
of English vocabulary. Usually we will constrain the
vocabulary to be a subset of words that are probable
translations of the foreign words in the input sen-
tence. So V is proportional to n. There are seven
free variables related to input size for doing the max-
imization computation. Hence the algorithmic com-
plexity is O(n7).
The two terms in Figure 3 (top) within the first
level of the max operator, corresponding to straight
rules and inverted rules, are analogous to the two
terms in Equation 1. Figure 3 (bottom) shows how to
decompose the first term; the same method applies
to the second term. Counting the free variables en-
closed in the innermost max operator, we get five: i,
k, u, v1, and u2. The decomposition eliminates one
free variable, v1. In the outermost level, there are
six free variables left. The maximum number of in-
teracting variables is six overall. So, we reduced the
complexity of ITG decoding using bigram language
model from O(n7) to O(n6).
The hooks k
X
Zu u2
i that we have built for de-
coding with a bigram language model turn out to be
similar to the hooks for bilexical parsing if we focus
on the two boundary words v1 and u2 (or v2 and u1)
68
?(X[i, j, u, v]) = max
?
?
?
?
?
?
?
?
?
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v])
? P (X ? [Y Z]) ? bigram(v1, u2)
]
,
max
k,v2,u1,Y,Z
[
?(Y [i, k, u1, v]) ? ?(Z[k, j, u, v2])
? P (X ? ?Y Z?) ? bigram(v2, u1)
]
?
?
?
?
?
?
?
?
?
(2)
max
k,v1,u2,Y,Z
[
?(Y [i, k, u, v1]) ? ?(Z[k, j, u2, v]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
= max
k,u2,Z
[
max
v1,Y
[
?(Y [i, k, u, v1]) ? P (X ? [Y Z]) ? bigram(v1, u2)
]
? ?(Z[k, j, u2, v])
]
Figure 3: Equation for ITG decoding (top), with an efficient factorization (bottom)
that are interacting between two adjacent decoding
constituents and relate them with the h? and h that
are interacting in bilexical parsing. In terms of al-
gebraic manipulation, we are also rearranging three
factors (ignoring the non-lexical rules), trying to re-
duce the maximum number of interacting variables
in any computation step.
4.1 Generalization to m-gram Cases
In this section, we will demonstrate how to use the
hook trick for trigram decoding which leads us to a
general hook trick for any m-gram decoding case.
We will work only on straight rules and use icons
of constituents and hooks to make the equations eas-
ier to interpret.
The straightforward dynamic programming equa-
tion is:
i
X
u1u2 v1v2
j = maxv11,v12,u21,u22,
k,Y,Z
u22
i k j
X
Y Z
u1u2 v2v1
][
v11v12 u21
(3)
By counting the variables that are dependent
on input sentence length on the right hand side
of the equation, we know that the straightfor-
ward algorithm?s complexity is O(n11). The max-
imization computation is over four factors that
are dependent on n: ?(Y [i, k, u1, u2, v11, v12]),
?(Z[k, j, u21, u22, v1, v2]), trigram(v11, v12, u21),
and trigram(v12, u21, u22). As before, our goal is
to cleverly bracket the factors.
By bracketing trigram(v11, v12, u21) and
?(Y [i, k, u1, u2, v11, v12]) together and maximizing
over v11 and Y , we can build the the level-1 hook:
u21
i k
X
Z
u1u2
][
v12
= max
v11,Y
u21
i k
X
Y Z
u1u2
][
v11v12
The complexity is O(n7).
Grouping the level-1 hook and
trigram(v12, u21, u22), maximizing over v12,
we can build the level-2 hook:
u21
i k
X
Z
u1u2
][
u22
= max
v12
u21
i k
X
Z
u1u2
][
v12 u22
The complexity is O(n7). Finally,
we can use the level-2 hook to com-
bine with Z[k, j, u21, u22, v1, v2] to build
X[i, j, u1, u2, v1, v2]. The complexity is O(n9)
after reducing v11 and v12 in the first two steps.
i
X
u1u2 v1v2
j = max
u21,u22,k,Z
u22
i k j
X
Z
u1u2 v2v1
][
u21
(4)
Using the hook trick, we have reduced the com-
plexity of ITG decoding using bigrams from O(n7)
to O(n6), and from O(n11) to O(n9) for trigram
69
case. We conclude that for m-gram decoding of
ITG, the hook trick can change the the time com-
plexity from O(n3+4(m?1)) to O(n3+3(m?1)). To
get an intuition of the reduction, we can compare
Equation 3 with Equation 4. The variables v11 and
v12 in Equation 3, which are independent of v1 and
v2 for maximizing the product have been concealed
under the level-2 hook in Equation 4. In general,
by building m ? 1 intermediate hooks, we can re-
duce m ? 1 free variables in the final combination
step, hence having the reduction from 4(m ? 1) to
3(m ? 1).
5 Generalization to Non-binary Bitext
Grammars
Although we have presented our algorithm as a de-
coder for the binary-branching case of Inversion
Transduction Grammar, the same factorization tech-
nique can be applied to more complex synchronous
grammars. In this general case, items in the dy-
namic programming chart may need to represent
non-contiguous span in either the input or output
language. Because synchronous grammars with in-
creasing numbers of children on the right hand side
of each production form an infinite, non-collapsing
hierarchy, there is no upper bound on the number
of discontinuous spans that may need to be repre-
sented (Aho and Ullman, 1972). One can, however,
choose to factor the grammar into binary branching
rules in one of the two languages, meaning that dis-
continuous spans will only be necessary in the other
language.
If we assume m is larger than 2, it is likely that
the language model combinations dominate com-
putation. In this case, it is advantageous to factor
the grammar in order to make it binary in the out-
put language, meaning that the subrules will only
need to represent adjacent spans in the output lan-
guage. Then the hook technique will work in the
same way, yielding O(n2(m?1)) distinct types of
items with respect to language model state, and
3(m?1) free indices to enumerate when combining
a hook with a complete constituent to build a new
item. However, a larger number of indices point-
ing into the input language will be needed now that
items can cover discontinuous spans. If the gram-
mar factorization yields rules with at most R spans
in the input language, there may be O(n2R) dis-
tinct types of chart items with respect to the input
language, because each span has an index for its
beginning and ending points in the input sentence.
Now the upper bound of the number of free in-
dices with respect to the input language is 2R + 1,
because otherwise if one rule needs 2R + 2 in-
dices, say i1, ? ? ? , i2R+2, then there are R + 1 spans
(i1, i2), ? ? ? , (i2R+1, i2R+2), which contradicts the
above assumption. Thus the time complexity at the
input language side is O(n2R+1), yielding a total al-
gorithmic complexity of O(n3(m?1)+(2R+1)).
To be more concrete, we will work through a 4-
ary translation rule, using a bigram language model.
The standard DP equation is:
i
u v
j
A
= maxv3,u1,v1,u4,v4,u2,
k1,k2,k3,
B,C,D,E
B C D E
A
v3u u1 v1 u4 v4 u2 v
i k1 k2 k3 j (5)
This 4-ary rule is a representative difficult case.
The underlying alignment pattern for this rule is as
follows:
D
C
E
B
A
It is a rule that cannot be binarized in the bitext
space using ITG rules. We can only binarize it in
one dimension and leave the other dimension having
discontinuous spans. Without applying binarization
and hook trick, decoding parsing with it according
to Equation 5 requires time complexity of O(n13).
However, we can build the following partial con-
stituents and hooks to do the combination gradually.
The first step finishes a hook by consuming one
bigram. Its time complexity is O(n5):
C D E
A
u1u
k2 k3 = max
v3,B
B C D E
A
u v3 u1
k2 k3
The second step utilizes the hook we just built and
builds a partial constituent. The time complexity is
O(n7):
70
D E
A
u v1
i k1 k2 k3 = max
u1,C
C D E
A
u u1 v1
i k1 k2 k3
By ?eating? another bigram, we build the second
hook using O(n7):
D E
A
u u4
i k1 k2 k3 = max
v1
D E
A
u v1 u4
i k1 k2 k3
We use the last hook. This step has higher com-
plexity: O(n8):
E
A
u v4
i k1 k2 j = max
u4,k3,D
v4u4
k2 k3
D E
A
jk1i
u
The last bigram involved in the 4-ary rule is com-
pleted and leads to the third hook, with time com-
plexity of O(n7):
E
A
jk2k1i
u u2
= max
v4
E
A
u v4 u2
i k1 k2 j
The final combination is O(n7):
i
u v
j
A
= max
u2,k1,k2,E
u
i k1 k2
E
A
u2
j
v
The overall complexity has been reduced to
O(n8) after using binarization on the output side and
using the hook trick all the way to the end. The result
is one instance of our general analysis: here R = 2,
m = 2, and 3(m ? 1) + (2R + 1) = 8.
6 Implementation
The implementation of the hook trick in a practi-
cal decoder is complicated by the interaction with
pruning. If we build hooks looking for all words
in the vocabulary whenever a complete constituent
is added to the chart, we will build many hooks
that are never used, because partial hypotheses with
many of the boundary words specified by the hooks
may never be constructed due to pruning. In-
stead of actively building hooks, which are inter-
mediate results, we can build them only when we
need them and then cache them for future use. To
make this idea concrete, we sketch the code for bi-
gram integrated decoding using ITG as in Algo-
rithm 1. It is worthy of noting that for clarity we
are building hooks in shape of
v
k j
v?
Z
, instead
of
X
Y v
k j
v?
as we have been showing in the
previous sections. That is, the probability for the
grammar rule is multiplied in when a complete con-
stituent is built, rather than when a hook is created.
If we choose the original representation, we would
have to create both straight hooks and inverted hooks
because the straight rules and inverted rules are to be
merged with the ?core? hooks, creating more speci-
fied hooks.
7 Conclusion
By showing the parallels between lexicalization for
language model state and lexicalization for syntac-
tic heads, we have demonstrated more efficient al-
gorithms for previously described models of ma-
chine translation. Decoding for Inversion Transduc-
tion Grammar with a bigram language model can be
done in O(n6) time. This is the same complexity
as the ITG alignment algorithm used by Wu (1997)
and others, meaning complete Viterbi decoding is
possible without pruning for realistic-length sen-
tences. More generally, ITG with an m-gram lan-
guage model is O(n3+3(m?1)), and a synchronous
context-free grammar with at most R spans in the
input language is O(n3(m?1)+(2R+1)). While this
improves on previous algorithms, the degree in n
is probably still too high for complete search to
be practical with such models. The interaction of
the hook technique with pruning is an interesting
71
Algorithm 1 ITGDecode(Nt)
for all s, t such that 0 ? s < t ? Nt do
for all S such that s < S < t do
 straight rule
for all rules X ? [Y Z] ? G do
for all (Y, u1, v1) possible for the span of (s, S) do
 a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required
if not exist hooks(S, t, Z, v1) then
build hooks(S, t, Z, v1)
end if
for all v2 possible for the hooks in (S, t, Z, v1) do
 combining a hook and a hypothesis, using straight rule
?(s, t, X, u1, v2) =
max
{
?(s, t, X, u1, v2), ?(s, S, Y, u1, v1) ? ?+(S, t, Z, v1, v2) ? P (X ? [Y Z])
}
end for
end for
end for
 inverted rule
for all rules X ? ?Y Z? ? G do
for all (Z, u2, v2) possible for the span of (S, t) do
 a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is required
if not exist hooks(s, S, Y, v2) then
build hooks(s, S, Y, v2)
end if
for all v1 possible for the hooks in (s, S, Y, v2) do
 combining a hook and a hypothesis, using inverted rule
?(s, t, X, u2, v1) =
max
{
?(s, t, X, u2, v1), ?(S, t, Z, u2, v2) ? ?+(s, S, Y, v2, v1) ? P (X ? ?Y Z?)
}
end for
end for
end for
end for
end for
routine build hooks(s, t, X, v?)
for all (X, u, v) possible for the span of (s, t) do
 combining a bigram with a hypothesis
?+(s, t, X, v?, v) =
max
{
?+(s, t, X, v?, v), bigram(v?, u) ? ?(s, t, X, u, v)
}
end for
72
area for future work. Building the chart items with
hooks may take more time than it saves if many of
the hooks are never combined with complete con-
stituents due to aggressive pruning. However, it may
be possible to look at the contents of the chart in or-
der to build only those hooks which are likely to be
useful.
References
Aho, Albert V. and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Englewood Cliffs, NJ: Prentice-Hall.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence (AAAI-97), pages 598?603, Menlo
Park, August. AAAI Press.
Collins, Michael. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Conference of the Association for Compu-
tational Linguistics (ACL-97), pages 16?23, Madrid,
Spain.
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics.
Melamed, I. Dan. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Wu, Dekai. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
73
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1?8,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
A Syntax-Directed Translator with Extended Domain of Locality
Liang Huang
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Kevin Knight
Info. Sci. Inst.
Univ. of Southern California
Marina del Rey, CA 90292
knight@isi.edu
Aravind Joshi
Dept. of Comp. & Info. Sci.
Univ. of Pennsylvania
Philadelphia, PA 19104
joshi@linc.cis.upenn.edu
Abstract
A syntax-directed translator first parses
the source-language input into a parse-
tree, and then recursively converts the tree
into a string in the target-language. We
model this conversion by an extended tree-
to-string transducer that have multi-level
trees on the source-side, which gives our
system more expressive power and flexi-
bility. We also define a direct probabil-
ity model and use a linear-time dynamic
programming algorithm to search for the
best derivation. The model is then ex-
tended to the general log-linear frame-
work in order to rescore with other fea-
tures like n-gram language models. We
devise a simple-yet-effective algorithm to
generate non-duplicate k-best translations
for n-gram rescoring. Initial experimen-
tal results on English-to-Chinese transla-
tion are presented.
1 Introduction
The concept of syntax-directed (SD) translation
was originally proposed in compiling (Irons, 1961;
Lewis and Stearns, 1968), where the source program
is parsed into a tree representation that guides the
generation of the object code. Following Aho and
Ullman (1972), a translation, as a set of string pairs,
can be specified by a syntax-directed translation
schema (SDTS), which is essentially a synchronous
context-free grammar (SCFG) that generates two
languages simultaneously. An SDTS also induces a
translator, a device that performs the transformation
induces implements
SD translator
(source parser + recursive converter)
specifies translation
(string relation)
SD translation schema
(synchronous grammar)
Figure 1: The relationship among SD concepts,
adapted from (Aho and Ullman, 1972).
?
?
?
?
?
?
S
NP(1)? VP
VB(2)? NP
(3)
?
,
S
VB(2)? NP
(1)
? NP
(3)
?
?
?
?
?
?
?
Figure 2: An example of complex reordering repre-
sented as an STSG rule, which is beyond any SCFG.
from input string to output string. In this context, an
SD translator consists of two components, a source-
language parser and a recursive converter which is
usually modeled as a top-down tree-to-string trans-
ducer (Ge?cseg and Steinby, 1984). The relationship
among these concepts is illustrated in Fig. 1.
This paper adapts the idea of syntax-directed
translator to statistical machine translation (MT).
We apply stochastic operations at each node of the
source-language parse-tree and search for the best
derivation (a sequence of translation steps) that con-
verts the whole tree into some target-language string
with the highest probability. However, the structural
divergence across languages often results in non-
isomorphic parse-trees that is beyond the power of
SCFGs. For example, the S(VO) structure in English
is translated into a VSO word-order in Arabic, an in-
stance of complex reordering not captured by any
1
SCFG (Fig. 2).
To alleviate the non-isomorphism problem, (syn-
chronous) grammars with richer expressive power
have been proposed whose rules apply to larger frag-
ments of the tree. For example, Shieber and Sch-
abes (1990) introduce synchronous tree-adjoining
grammar (STAG) and Eisner (2003) uses a syn-
chronous tree-substitution grammar (STSG), which
is a restricted version of STAG with no adjunctions.
STSGs and STAGs generate more tree relations than
SCFGs, e.g. the non-isomorphic tree pair in Fig. 2.
This extra expressive power lies in the extended do-
main of locality (EDL) (Joshi and Schabes, 1997),
i.e., elementary structures beyond the scope of one-
level context-free productions. Besides being lin-
guistically motivated, the need for EDL is also sup-
ported by empirical findings in MT that one-level
rules are often inadequate (Fox, 2002; Galley et al,
2004). Similarly, in the tree-transducer terminology,
Graehl and Knight (2004) define extended tree trans-
ducers that have multi-level trees on the source-side.
Since an SD translator separates the source-
language analysis from the recursive transformation,
the domains of locality in these two modules are or-
thogonal to each other: in this work, we use a CFG-
based Treebank parser but focuses on the extended
domain in the recursive converter. Following Gal-
ley et al (2004), we use a special class of extended
tree-to-string transducer (xRs for short) with multi-
level left-hand-side (LHS) trees.1 Since the right-
hand-side (RHS) string can be viewed as a flat one-
level tree with the same nonterminal root from LHS
(Fig. 2), this framework is closely related to STSGs:
they both have extended domain of locality on the
source-side, while our framework remains as a CFG
on the target-side. For instance, an equivalent xRs
rule for the complex reordering in Fig. 2 would be
S(x1:NP, VP(x2:VB, x3:NP))? x2 x1 x3
While Section 3 will define the model formally,
we first proceed with an example translation from
English to Chinese (note in particular that the in-
verted phrases between source and target):
1Throughout this paper, we will use LHS and source-side
interchangeably (so are RHS and target-side). In accordance
with our experiments, we also use English and Chinese as the
source and target languages, opposite to the Foreign-to-English
convention of Brown et al (1993).
(a) the gunman was [killed]1 by [the police]2 .
parser ?
(b)
S
NP-C
DT
the
NN
gunman
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
PUNC
.
r1, r2 ?
(c) qiangshou
VP
VBD
was
VP-C
VBN
killed
PP
IN
by
NP-C
DT
the
NN
police
?
r3 ?
(d) qiangshou bei
NP-C
DT
the
NN
police
VBN
killed
?
r5 ? r4 ?
(e) qiangshou bei [jingfang]2 [jibi]1 ?
Figure 3: A synatx-directed translation process for
Example (1).
(1) the gunman was killed by the police .
qiangshou
[gunman]
bei
[passive]
jingfang
[police]
jibi
[killed]
?
.
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
(r1) S (x1:NP-C x2:VP PUNC (.) ) ? x1 x2 ?
2
Then, the rule r2 grabs the whole sub-tree for ?the
gunman? and translates it as a phrase:
(r2) NP-C ( DT (the) NN (gunman) )? qiangshou
Now we get a ?partial Chinese, partial English? sen-
tence ?qiangshou VP ?? as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
(r3)
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? bei x2 x1
which captures the fact that the agent (NP-C, ?the
police?) and the verb (VBN, ?killed?) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r4 and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
2 Previous Work
It is helpful to compare this approach with recent ef-
forts in statistical MT. Phrase-based models (Koehn
et al, 2003; Och and Ney, 2004) are good at learn-
ing local translations that are pairs of (consecutive)
sub-strings, but often insufficient in modeling the re-
orderings of phrases themselves, especially between
language pairs with very different word-order. This
is because the generative capacity of these models
lies within the realm of finite-state machinery (Ku-
mar and Byrne, 2003), which is unable to process
nested structures and long-distance dependencies in
natural languages.
Syntax-based models aim to alleviate this prob-
lem by exploiting the power of synchronous rewrit-
ing systems. Both Yamada and Knight (2001) and
Chiang (2005) use SCFGs as the underlying model,
so their translation schemata are syntax-directed as
in Fig. 1, but their translators are not: both systems
do parsing and transformation in a joint search, es-
sentially over a packed forest of parse-trees. To this
end, their translators are not directed by a syntac-
tic tree. Although their method potentially consid-
ers more than one single parse-tree as in our case,
the packed representation of the forest restricts the
scope of each transfer step to a one-level context-
free rule, while our approach decouples the source-
language analyzer and the recursive converter, so
that the latter can have an extended domain of local-
ity. In addition, our translator also enjoys a speed-
up by this decoupling, with each of the two stages
having a smaller search space. In fact, the recursive
transfer step can be done by a a linear-time algo-
rithm (see Section 5), and the parsing step is also
fast with the modern Treebank parsers, for instance
(Collins, 1999; Charniak, 2000). In contrast, their
decodings are reported to be computationally expen-
sive and Chiang (2005) uses aggressive pruning to
make it tractable. There also exists a compromise
between these two approaches, which uses a k-best
list of parse trees (for a relatively small k) to approx-
imate the full forest (see future work).
Besides, our model, as being linguistically mo-
tivated, is also more expressive than the formally
syntax-based models of Chiang (2005) and Wu
(1997). Consider, again, the passive example in rule
r3. In Chiang?s SCFG, there is only one nonterminal
X, so a corresponding rule would be
? was X(1) by X(2), bei X(2) X(1) ?
which can also pattern-match the English sentence:
I was [asleep]1 by [sunset]2 .
and translate it into Chinese as a passive voice. This
produces very odd Chinese translation, because here
?was A by B? in the English sentence is not a pas-
sive construction. By contrast, our model applies
rule r3 only if A is a past participle (VBN) and B
is a noun phrase (NP-C). This example also shows
that, one-level SCFG rule, even if informed by the
Treebank as in (Yamada and Knight, 2001), is not
enough to capture a common construction like this
which is five levels deep (from VP to ?by?).
There are also some variations of syntax-directed
translators where dependency structures are used
in place of constituent trees (Lin, 2004; Ding and
Palmer, 2005; Quirk et al, 2005). Although they
share with this work the basic motivations and simi-
lar speed-up, it is difficult to specify re-ordering in-
formation within dependency elementary structures,
so they either resort to heuristics (Lin) or a sepa-
rate ordering model for linearization (the other two
3
works).2 Our approach, in contrast, explicitly mod-
els the re-ordering of sub-trees within individual
transfer rules.
3 Extended Tree-to-String Tranducers
In this section, we define the formal machinery of
our recursive transformation model as a special case
of xRs transducers (Graehl and Knight, 2004) that
has only one state, and each rule is linear (L) and
non-deleting (N) with regarding to variables in the
source and target sides (henth the name 1-xRLNs).
Definition 1. A 1-xRLNs transducer is a tuple
(N,?,?,R) where N is the set of nonterminals, ?
is the input alphabet, ? is the output alphabet, and
R is a set of rules. A rule in R is a tuple (t, s, ?)
where:
1. t is the LHS tree, whose internal nodes are la-
beled by nonterminal symbols, and whose fron-
tier nodes are labeled terminals from ? or vari-
ables from a set X = {x1, x2, . . .};
2. s ? (X ??)? is the RHS string;
3. ? is a mapping from X to nonterminals N .
We require each variable xi ? X occurs exactly once
in t and exactly once in s (linear and non-deleting).
We denote ?(t) to be the root symbol of tree t.
When writing these rules, we avoid notational over-
head by introducing a short-hand form from Galley
et al (2004) that integrates the mapping into the tree,
which is used throughout Section 1. Following TSG
terminology (see Figure 2), we call these ?variable
nodes? such as x2:NP-C substitution nodes, since
when applying a rule to a tree, these nodes will be
matched with a sub-tree with the same root symbol.
We also define |X | to be the rank of the rule, i.e.,
the number of variables in it. For example, rules r1
and r3 in Section 1 are both of rank 2. If a rule has
no variable, i.e., it is of rank zero, then it is called a
purely lexical rule, which performs a phrasal trans-
lation as in phrase-based models. Rule r2, for in-
stance, can be thought of as a phrase pair ?the gun-
man, qiangshou?.
Informally speaking, a derivation in a transducer
is a sequence of steps converting a source-language
2Although hybrid approaches, such as dependency gram-
mars augmented with phrase-structure information (Alshawi et
al., 2000), can do re-ordering easily.
r1
r2 r3
r4 r5
r1
r2 r6
r4 r7
r5
(a) (b)
Figure 4: (a) the derivation in Figure 3; (b) another
derviation producing the same output by replacing
r3 with r6 and r7, which provides another way of
translating the passive construction:
(r6) VP ( VBD (was) VP-C (x1:VBN x2:PP ) )? x2 x1
(r7) PP ( IN (by) x1:NP-C )? bei x1
tree into a target-language string, with each step ap-
plying one tranduction rule. However, it can also
be formalized as a tree, following the notion of
derivation-tree in TAG (Joshi and Schabes, 1997):
Definition 2. A derivation d, its source and target
projections, noted E(d) and C(d) respectively, are
recursively defined as follows:
1. If r = (t, s, ?) is a purely lexical rule (? = ?),
then d = r is a derivation, where E(d) = t and
C(d) = s;
2. If r = (t, s, ?) is a rule, and di is a (sub-)
derivation with the root symbol of its source
projection matches the corresponding substitu-
tion node in r, i.e., ?(E(di)) = ?(xi), then
d = r(d1, . . . , dm) is also a derivation, where
E(d) = [xi 7? E(di)]t and C(d) = [xi 7?
C(di)]s.
Note that we use a short-hand notation [xi 7? yi]t
to denote the result of substituting each xi with yi
in t, where xi ranges over all variables in t.
For example, Figure 4 shows two derivations for
the sentence pair in Example (1). In both cases, the
source projection is the English tree in Figure 3 (b),
and the target projection is the Chinese translation.
Galley et al (2004) presents a linear-time algo-
rithm for automatic extraction of these xRs rules
from a parallel corpora with word-alignment and
parse-trees on the source-side, which will be used
in our experiments in Section 6.
4
4 Probability Models
4.1 Direct Model
Departing from the conventional noisy-channel ap-
proach of Brown et al (1993), our basic model is a
direct one:
c? = argmax
c
Pr(c | e) (2)
where e is the English input string and c? is the
best Chinese translation according to the translation
model Pr(c | e). We now marginalize over all En-
glish parse trees T (e) that yield the sentence e:
Pr(c | e) =
?
??T (e)
Pr(?, c | e)
=
?
??T (e)
Pr(? | e) Pr(c | ?) (3)
Rather than taking the sum, we pick the best tree ??
and factors the search into two separate steps: pars-
ing (4) (a well-studied problem) and tree-to-string
translation (5) (Section 5):
?? = argmax
??T (e)
Pr(? | e) (4)
c? = argmax
c
Pr(c | ??) (5)
In this sense, our approach can be considered as
a Viterbi approximation of the computationally ex-
pensive joint search using (3) directly. Similarly, we
now marginalize over all derivations
D(??) = {d | E(d) = ??}
that translates English tree ? into some Chinese
string and apply the Viterbi approximation again to
search for the best derivation d?:
c? = C(d?) = C(argmax
d?D(??)
Pr(d)) (6)
Assuming different rules in a derivation are ap-
plied independently, we approximate Pr(d) as
Pr(d) =
?
r?d
Pr(r) (7)
where the probability Pr(r) of the rule r is estimated
by conditioning on the root symbol ?(t(r)):
Pr(r) = Pr(t(r), s(r) | ?(t(r)))
= c(r)?
r?:?(t(r?))=?(t(r)) c(r?)
(8)
where c(r) is the count (or frequency) of rule r in
the training data.
4.2 Log-Linear Model
Following Och and Ney (2002), we extend the direct
model into a general log-linear framework in order
to incorporate other features:
c? = argmax
c
Pr(c | e)? ? Pr(c)? ? e??|c| (9)
where Pr(c) is the language model and e??|c| is the
length penalty term based on |c|, the length of the
translation. Parameters ?, ?, and ? are the weights
of relevant features. Note that positive ? prefers
longer translations. We use a standard trigram model
for Pr(c).
5 Search Algorithms
We first present a linear-time algorithm for searching
the best derivation under the direct model, and then
extend it to the log-linear case by a new variant of
k-best parsing.
5.1 Direct Model: Memoized Recursion
Since our probability model is not based on the noisy
channel, we do not call our search module a ?de-
coder? as in most statistical MT work. Instead, read-
ers who speak English but not Chinese can view it as
an ?encoder? (or encryptor), which corresponds ex-
actly to our direct model.
Given a fixed parse-tree ??, we are to search
for the best derivation with the highest probability.
This can be done by a simple top-down traversal
(or depth-first search) from the root of ??: at each
node ? in ??, try each possible rule r whose English-
side pattern t(r) matches the subtree ??? rooted at ?,
and recursively visit each descendant node ?i in ???
that corresponds to a variable in t(r). We then col-
lect the resulting target-language strings and plug
them into the Chinese-side s(r) of rule r, getting
a translation for the subtree ??? . We finally take the
best of all translations.
With the extended LHS of our transducer, there
may be many different rules applicable at one tree
node. For example, consider the VP subtree in
Fig. 3 (c), where both r3 and r6 can apply. As a re-
sult, the number of derivations is exponential in the
size of the tree, since there are exponentially many
5
decompositions of the tree for a given set of rules.
This problem can be solved by memoization (Cor-
men et al, 2001): we cache each subtree that has
been visited before, so that every tree node is visited
at most once. This results in a dynamic program-
ming algorithm that is guaranteed to run in O(npq)
time where n is the size of the parse tree, p is the
maximum number of rules applicable to one tree
node, and q is the maximum size of an applicable
rule. For a given rule-set, this algorithm runs in time
linear to the length of the input sentence, since p
and q are considered grammar constants, and n is
proportional to the input length. The full pseudo-
code is worked out in Algorithm 1. A restricted
version of this algorithm first appears in compiling
for optimal code generation from expression-trees
(Aho and Johnson, 1976). In computational linguis-
tics, the bottom-up version of this algorithm resem-
bles the tree parsing algorithm for TSG by Eisner
(2003). Similar algorithms have also been proposed
for dependency-based translation (Lin, 2004; Ding
and Palmer, 2005).
5.2 Log-linear Model: k-best Search
Under the log-linear model, one still prefers to
search for the globally best derivation d?:
d? = argmax
d?D(??)
Pr(d)? Pr(C(d))?e??|C(d)| (10)
However, integrating the n-gram model with the
translation model in the search is computationally
very expensive. As a standard alternative, rather
than aiming at the exact best derivation, we search
for top-k derivations under the direct model using
Algorithm 1, and then rerank the k-best list with the
language model and length penalty.
Like other instances of dynamic programming,
Algorithm 1 can be viewed as a hypergraph search
problem. To this end, we use an efficient algo-
rithm by Huang and Chiang (2005, Algorithm 3)
that solves the general k-best derivations problem
in monotonic hypergraphs. It consists of a normal
forward phase for the 1-best derivation and a recur-
sive backward phase for the 2nd, 3rd, . . . , kth deriva-
tions.
Unfortunately, different derivations may have the
same yield (a problem called spurious ambiguity),
due to multi-level LHS of our rules. In practice, this
results in a very small ratio of unique strings among
top-k derivations. To alleviate this problem, deter-
minization techniques have been proposed by Mohri
and Riley (2002) for finite-state automata and ex-
tended to tree automata by May and Knight (2006).
These methods eliminate spurious ambiguity by ef-
fectively transforming the grammar into an equiva-
lent deterministic form. However, this transforma-
tion often leads to a blow-up in forest size, which is
exponential to the original size in the worst-case.
So instead of determinization, here we present a
simple-yet-effective extension to the Algorithm 3 of
Huang and Chiang (2005) that guarantees to output
unique translated strings:
? keep a hash-table of unique strings at each vertex
in the hypergraph
? when asking for the next-best derivation of a ver-
tex, keep asking until we get a new string, and
then add it into the hash-table
This method should work in general for any
equivalence relation (say, same derived tree) that can
be defined on derivations.
6 Experiments
Our experiments are on English-to-Chinese trans-
lation, the opposite direction to most of the recent
work in SMT. We are not doing the reverse direction
at this time partly due to the lack of a sufficiently
good parser for Chinese.
6.1 Data Preparation
Our training set is a Chinese-English parallel corpus
with 1.95M aligned sentences (28.3M words on the
English side). We first word-align them by GIZA++,
then parse the English side by a variant of Collins
(1999) parser, and finally apply the rule-extraction
algorithm of Galley et al (2004). The resulting rule
set has 24.7M xRs rules. We also use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
Chinese trigram model with Knesser-Ney smooth-
ing on the Chinese side of the parallel corpus.
Our evaluation data consists of 140 short sen-
tences (< 25 Chinese words) of the Xinhua portion
of the NIST 2003 Chinese-to-English evaluation set.
Since we are translating in the other direction, we
use the first English reference as the source input
and the Chinese as the single reference.
6
Algorithm 1 Top-down Memoized Recursion
1: function TRANSLATE(?)
2: if cache[?] defined then . this sub-tree visited before?
3: return cache[?]
4: best? 0
5: for r ? R do . try each rule r
6: matched, sublist? PATTERNMATCH(t(r), ?) . tree pattern matching
7: if matched then . if matched, sublist contains a list of matched subtrees
8: prob? Pr(r) . the probability of rule r
9: for ?i ? sublist do
10: pi, si ? TRANSLATE(?i) . recursively solve each sub-problem
11: prob? prob ? pi
12: if prob > best then
13: best? prob
14: str ? [xi 7? si]s(r) . plug in the results
15: cache[?]? best, str . caching the best solution for future use
16: return cache[?] . returns the best string with its prob.
6.2 Initial Results
We implemented our system as follows: for each in-
put sentence, we first run Algorithm 1, which returns
the 1-best translation and also builds the derivation
forest of all translations for this sentence. Then we
extract the top 5000 non-duplicate translated strings
from this forest and rescore them with the trigram
model and the length penalty.
We compared our system with a state-of-the-art
phrase-based system Pharaoh (Koehn, 2004) on the
evaluation data. Since the target language is Chi-
nese, we report character-based BLEU score instead
of word-based to ensure our results are indepen-
dent of Chinese tokenizations (although our lan-
guage models are word-based). The BLEU scores
are based on single reference and up to 4-gram pre-
cisions (r1n4). Feature weights of both systems are
tuned on the same data set.3 For Pharaoh, we use the
standard minimum error-rate training (Och, 2003);
and for our system, since there are only two in-
dependent features (as we always fix ? = 1), we
use a simple grid-based line-optimization along the
language-model weight axis. For a given language-
model weight ?, we use binary search to find the best
length penalty ? that leads to a length-ratio closest
3In this sense, we are only reporting performances on the
development set at this point. We will report results tuned and
tested on separate data sets in the final version of this paper.
Table 1: BLEU (r1n4) score results
system BLEU
Pharaoh 25.5
direct model (1-best) 20.3
log-linear model (rescored 5000-best) 23.8
to 1 against the reference. The results are summa-
rized in Table 1. The rescored translations are better
than the 1-best results from the direct model, but still
slightly worse than Pharaoh.
7 Conclusion and On-going Work
This paper presents an adaptation of the clas-
sic syntax-directed translation with linguistically-
motivated formalisms for statistical MT. Currently
we are doing larger-scale experiments. We are also
investigating more principled algorithms for inte-
grating n-gram language models during the search,
rather than k-best rescoring. Besides, we will extend
this work to translating the top k parse trees, instead
of committing to the 1-best tree, as parsing errors
certainly affect translation quality.
7
References
A. V. Aho and S. C. Johnson. 1976. Optimal code gen-
eration for expression trees. J. ACM, 23(3):488?501.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing. Prentice Hall, Englewood Cliffs, New Jersey.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL, pages 132?139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
ACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. MIT Press, second edition.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
(companion volume), pages 205?208.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
F. Ge?cseg and M. Steinby. 1984. Tree Automata.
Akade?miai Kiado?, Budapest.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL, pages 105?112.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Nineth International
Workshop on Parsing Technologies (IWPT-2005), 9-10
October 2005, Vancouver, Canada.
E. T. Irons. 1961. A syntax-directed compiler for AL-
GOL 60. Comm. ACM, 4(1):51?55.
Aravind Joshi and Yves Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
? 124. Springer, Berlin.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. of AMTA, pages 115?124.
Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the alignment
template model for statistical machine translation. In
Proc. of HLT-NAACL, pages 142?149.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed
transduction. Journal of the ACM, 15(3):465?488.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th COLING.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. Submitted to HLT-NAACL 2006.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002 (ICSLP ?02), Denver, Col-
orado, September.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417?449.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING, pages
253?258.
Andrea Stolcke. 2002. Srilm: an extensible language
modeling toolkit. In Proc. of ICSLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL.
8
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 33?40,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Binarization, Synchronous Binarization, and Target-side Binarization?
Liang Huang
University of Pennsylvania
3330 Walnut Street, Levine Hall
Philadelphia, PA 19104
lhuang3@cis.upenn.edu
Abstract
Binarization is essential for achieving
polynomial time complexities in pars-
ing and syntax-based machine transla-
tion. This paper presents a new binariza-
tion scheme, target-side binarization, and
compares it with source-side and syn-
chronous binarizations on both string-
based and tree-based systems using syn-
chronous grammars. In particular, we
demonstrate the effectiveness of target-
side binarization on a large-scale tree-to-
string translation system.
1 Introduction
Several recent syntax-based models for machine
translation (Chiang, 2005; Galley et al, 2006) can
be seen as instances of the general framework of
synchronous grammars and tree transducers. In this
framework, decoding can be thought of as pars-
ing problems, whose complexity is in general expo-
nential in the number of nonterminals on the right
hand side of a grammar rule. To alleviate this prob-
lem, one can borrow from parsing the technique
of binarizing context-free grammars (into Chomsky
Normal Form) to reduce the complexity. With syn-
chronous context-free grammars (SCFG), however,
this problem becomes more complicated with the
additional dimension of target-side permutation.
The simplest method of binarizing an SCFG is
to binarize (left-to-right) on the source-side as if
treating it as a monolingual CFG for the source-
langauge. However, this approach does not guaran-
?This work is partially supported by NSF ITR grants IIS-
0428020 (while I was visiting USC/ISI) and EIA-0205456. I
also wish to thank Jonathan Graehl, Giorgio Satta, Hao Zhang,
and the three anonymous reviewers for helpful comments.
tee contiguous spans on the target-side, due to the ar-
bitrary re-ordering of nonterminals between the two
languages. As a result, decoding with an integrated
language model still has an exponential complexity.
Synchronous binarization (Zhang et al, 2006)
solves this problem by simultaneously binarizing
both source and target-sides of a synchronous rule,
making sure of contiguous spans on both sides
whenever possible. Neglecting the small amount
of non-binarizable rules, the decoding complexity
with an integrated language model becomes polyno-
mial and translation quality is significantly improved
thanks to the better search. However, this method is
more sophisticated to implement than the previous
method and binarizability ratio decreases on freer
word-order languages (Wellington et al, 2006).
This paper presents a third alternative, target-
side binarization, which is the symmetric version of
the simple source-side variant mentioned above. We
compare it with the other two schemes in two pop-
ular instantiations of MT systems based on SCFGs:
the string-based systems (Chiang, 2005; Galley et
al., 2006) where the input is a string to be parsed
using the source-side of the SCFG; and the tree-
based systems (Liu et al, 2006; Huang et al, 2006)
where the input is a parse tree and is recursively
converted into a target string using the SCFG as a
tree-transducer. While synchronous binarization is
the best strategy for string-based systems, we show
that target-side binarization can achieve the same
performance of synchronous binarization for tree-
based systems, with much simpler implementation
and 100% binarizability.
2 Synchronous Grammars and
Binarization Schemes
In this section, we define synchronous context-
free grammars and present the three binarization
33
NP
PP
VP
Chinese ??
En
gl
ish
??
NP-PP
NP-PP
VP
contiguous
ga
p
NP
PP-VP
contiguous
co
n
tig
uo
us
PP
N
P-
V
P
N
P-
V
P
gap
co
n
tig
uo
us
(a) example rule (b) source-side (c) synchronous (d) target-side
Figure 1: Illustration of the three binarization schemes, with virtual nonterminals in gray.
schemes through a motivational example.
A synchronous CFG (SCFG) is a context-free
rewriting system for generating string pairs. Each
rule (synchronous production) rewrites a nontermi-
nal in two dimensions subject to the constraint that
the sequence of nonterminal children on one side is
a permutation of the nonterminal sequence on the
other side. Each co-indexed child nonterminal pair
will be further rewritten as a unit. The rank of a rule
is defined as the number of its synchronous nonter-
minals. We also define the source and target projec-
tions of an SCFG to be the CFGs for the source and
target languages, respectively.
For example, the following SCFG1
(1)
S ? NP 1 PP 2 VP 3 , NP 1 VP 3 PP 2
NP ? Baoweier, Powell
VP ? juxing le huitan, held a meeting
PP ? yu Shalong, with Sharon
captures the re-ordering of PP and VP between
Chinese (source) and English (target). The source-
projection of the first rule, for example, is
S ? NP PP VP.
Decoding with an SCFG (e.g., translating from
Chinese to English using the above grammar) can be
cast as a parsing problem (see Section 3 for details),
in which case we need to binarize a synchronous rule
with more than two nonterminals to achieve polyno-
mial time algorithms (Zhang et al, 2006). We will
next present the three different binarization schemes
using Example 1.
1An alternative notation, used by Satta and Peserico (2005),
allows co-indexed nonterminals to take different symbols across
languages, which is convenient in describing syntactic diver-
gences (see Figure 2).
2.1 Source-side Binarization
The first and simplest scheme, source-side binariza-
tion, works left-to-right on the source projection of
the SCFG without respecting the re-orderings on the
target-side. So it will binarize the first rule as:
(2) S ? NP-PP VPNP-PP ? NP PP
which corresponds to Figure 1 (b). Notice that the
virtual nonterminal NP-PP representing the inter-
mediate symbol is discontinuous with two spans on
the target (English) side, because this binarization
scheme completely ignores the reorderings of non-
terminals. As a result, the binarized grammar, with
a gap on the target-side, is no longer an SCFG, but
can be represented in the more general formalism of
Multi-Text Grammars (MTG) (Melamed, 2003):
(3)
(
S
S
)
??? [1, 2][1, 2, 1]
(
NP-PP VP
NP-PP (2) VP
)
here [1, 2, 1] denotes that on that target-side, the first
nonterminal NP-PP has two discontinuous spans,
with the second nonterminal VP in the gap.
Intuitively speaking, the gaps on the target-side
will lead to exponential complexity in decoding with
integrated language models (see Section 3), as well
as synchronous parsing (Zhang et al, 2006).
2.2 Synchronous Binarization
A more principled method is synchronous binariza-
tion, which simultaneously binarizes both source
and target sides, with the constraint that virtual non-
terminals always have contiguous spans on both
sides. The resulting grammar is thus another SCFG,
the binary branching equivalent of the original gram-
mar, which can be thought of as an extension of the
34
[jinyibu]1
further
[ jiu
on
zhongdong
Mideast
weiji
crisis
]2 [juxing]3
hold
[huitan]4
talk
?[hold]3 [further]1 [talks]4 [on the Mideast crisis]2?
1
2
3
4
Chinese ??
En
gl
ish
??
Figure 2: An example of non-binarizable rule from the hand-aligned Chinese-English data in Liu et al
(2005). The SCFG rule is VP ? ADVP 1 PP 2 VB 3 NN 4 , VP ? VB 3 JJ 1 NNS 4 PP 2 in the notatoin
of Satta and Peserico (2005).
Chomsky Normal Form in synchronous grammars.
The example rule is now binarized into:
(4) S ? NP
1 PP-VP 2 , NP 1 PP-VP 2
PP-VP ? PP 1 VP 2 , VP 2 PP 1
which corresponds to Figure 1 (c). This represen-
tation, being contiguous on both sides, successfully
reduces the decoding complexity to a low polyno-
mial and significantly improved the search quality
(Zhang et al, 2006).
However, this scheme has the following draw-
backs. First, synchronous binarization is not always
possible with an arbitrary SCFG. Some reorder-
ings, for example, the permutation (2, 4, 1, 3), is
non-binarizable. Although according to Zhang et al
(2006), the vast majority (99.7%) of rules in their
Chinese-English dataset are binarizable, there do ex-
ist some interesting cases that are not (see Figure 2
for a real-data example). More importantly, the ra-
tio of binarizability, as expected, decreases on freer
word-order languages (Wellington et al, 2006). Sec-
ond, synchronous binarization is significantly more
complicated to implement than the straightforward
source-side binarization.
2.3 Target-side Binarization
We now introduce a novel scheme, target-side bi-
narization, which is the symmetric version of the
source-side variant. Under this method, the target-
side is always contiguous, while leaving some gaps
on the source-side. The example rule is binarized
into the following MTG form:
(5)
(
S
S
)
??? [1, 2, 1][1, 2]
(
NP-VP (2) PP
NP-VP PP
)
which corresponds to Figure 1 (d).
scheme s(b) t(b)
source-side 1 ? n/2
synchronous 1 1
target-side ? n/2 1
Table 1: Source and target arities of the three bina-
rization schemes of an SCFG rule of rank n.
Although the discontinuity on the source-side in
this new scheme causes exponential complexity in
string-based systems (Section 3.1), the continuous
spans on the target-side will ensure polynomial com-
plexity in tree-based systems (Section 3.2).
Before we move on to study the effects of vari-
ous binarization schemes in decoding, we need some
formal machineries of discontinuities.
We define the source and target arities of a
virtual nonterminal V , denoted s(V ) and t(V ), to
be the number of (consecutive) spans of V on the
source and target sides, respectively. This definition
extends to a binarization b of an SCFG rule of rank
n, where arities s(b) and t(b) are defined as the
maximum source and target arities over all virtual
nonterminals in b, respectively. For example, the
source and target arities of the three binarizations in
Figure 1 are 1 and 2 for (b), 1 and 1 for (c), and
2 and 1 for (d). In general, the arities for the three
binarization schemes are summarized in Table 1.
3 Theoretical Analysis
We now compare the algorithmic complexities of the
three binarization schemes in a central problem of
machine translation: decoding with an integrated n-
gram language model. Depending on the input be-
ing a string or a parse-tree, we divide MT systems
based on synchronous grammars into two broad cat-
egories: string-based and tree-based.
35
3.1 String-based Approaches
String-based approaches include both string-to-
string (Chiang, 2005) and string-to-tree systems
(Galley et al, 2006).2 To simplify the presentation
we will just focus on the former but the analysis also
applies to the latter. We will first discuss decoding
with a pure SCFG as the translation model (hence-
forth ?LM decoding), and then extend it to include
an n-gram model (+LM decoding).
3.1.1 Translation as Parsing
The ?LM decoder can be cast as a (monolin-
gual) parser on the source language: it takes the
source-language string as input and parses it using
the source-projection of the SCFG while building
the corresponding target-language sub-translations
in parallel. For source-side and synchronous bina-
rizations, since the resulting grammar has contigu-
ous source spans, we can apply the CKY algorithm
which guarantees cubic time complexity.
For example, a deduction along the virtual rule in
the synchronously binarized grammar (4) is notated
(PPj,k) : (w1, t1) (VPk,l) : (w2, t2)
(PP-VPj,l) : (w1 + w2, t2t1) (6)
where i, j, k are free indices in the source string,
w1, w2 are the scores of the two antecedent items,
and t1, t2 are the corresponding sub-translations.3
The resulting translation t2t1 is the inverted concate-
nation as specified by the target-side of the SCFG
rule.
The case for a source-side binarized grammar (3)
is slightly more complicated than the above, because
we have to keep track of gaps on the target side. For
example, we first combine NP with PP
(NPi,j) : (w1, t1) (PPj,k) : (w2, t2)
(NP-PPi,k) : (w1 + w2, t1 ? t2) (7)
2Our notation of X-to-Y systems is defined as follows: X de-
notes the input, either a string or a tree; while Y represents the
RHS structure of an individual rule: Y is string if the RHS is
a flat one-level tree (as in SCFGs), and Y is tree if the RHS
is multi-level as in (Galley et al, 2006). This convention also
applies to tree-based approaches.
3The actual system does not need to store the translations
since they can be recovered from backpointers and they are not
considered part of the state. We keep them here only for presen-
tation reasons.
NP-PP
NP-PP
VP
three Chinese indices
i k l
En
gl
ish
tr
an
sla
tio
ns
t 1
t 3
t 2 PP
N
P-
V
P
N
P-
V
P
four Chinese indices
i j k l
t 1
t 2
(a): Deduction (8) (b): Deduction (10)
Figure 3: Illustrations of two deductions with gaps.
leaving a gap (?) on the target-side resulting item,
because NP and PP are not contiguous in the En-
glish ordering. This gap is later filled in by the sub-
translation t3 of VP (see also Figure 3 (a)):
(NP-PPi,k) : (w1, t1 ? t2) (VPk,l) : (w2, t3)
(Si,l) : (w1 + w2, t1t3t2)
(8)
In both cases, there are still only three free indices
on the source-side, so the complexity remains cubic.
The gaps on the target-side do not require any ex-
tra computation in the current ?LM setting, but as
we shall see shortly below, will lead to exponential
complexity when integrating a language model.
For a target-side binarized grammar as in (5),
however, the source-side spans are discontinuous
where CKY can not apply, and we have to enumerate
more free indices on the source side. For example,
the first deduction
(NPi,j) : (w1, t1) (VPk,l) : (w2, t2)
(NP-VPi,j?k,l) : (w1 + w2, t1t2) (9)
leaves a gap in the source-side span of the resulting
item, which is later filled in when the item is com-
bined with a PP (see also Figure 3 (b)):
(NP-VPi,j?k,l) : (w1, t1) (PPj,k) : (w2, t2)
(Si,l) : (w1 + w2, t1t2)
(10)
Both of the above deductions have four free in-
dices, and thus of complexity O(|w|4) instead of cu-
bic in the length of the input string w.
More generally, the complexity of a binarization
scheme depends on its source arity. In the worst-
case, a binarized grammar with a source arity of s
will require at most (2s+1) free indices in a deduc-
tion, because otherwise if one rule needs (2s + 2)
36
indices, then there are s+1 spans, which contradicts
the definition of arity (Huang et al, 2005).4
These deductive systems represent the search
space of decoding without a language model. When
one is instantiated for a particular input string, it de-
fines a set of derivations, called a forest, represented
in a compact structure that has a structure of a hyper-
graph. Accordingly we call items like (PP1,3) nodes
in the forest, and an instantiated deduction like
(PP-VP1,6) ? (PP1,3)(VP3,6)
we call a hyperedge that connects one or more an-
tecedent nodes to a consequent node. In this rep-
resentation, the time complexity of ?LM decoding,
which we refer to as source-side complexity, is pro-
portional to the size of the forest F , i.e., the num-
ber of hyperedges (instantiated deductions) in F . To
summarize, the source-side complexity for a bina-
rized grammar of source arity s is
|F | = O(|w|2s+1).
3.1.2 Adding a Language Model
To integrate with a bigram language model, we
can use the dynamic-programming algorithm of Wu
(1996), which we may think of as proceeding in
two passes. The first pass is as above, and the sec-
ond pass traverses the first-pass forest, assigning to
each node v a set of augmented items, which we call
+LM items, of the form (va?b), where a and b are
target words and ? is a placeholder symbol for an
elided part of a target-language string. This item in-
dicates that a possible translation of the part of the
input spanned by v is a target string that starts with
a and ends with b.
Here is an example deduction in the syn-
chronously binarized grammar (4), for a +LM item
for the node (PP-VP1,6) based on the ?LM Deduc-
tion (6):
(PP with ? Sharon1,3 ): (w1, t1) (VP held ? talk3,6 ): (w2, t2)
(PP-VP held ? Sharon1,6 ): (w?, t2t1)
(11)
4Actually this is true only if in any binarization scheme,
a non-contiguous item is always combined with a contiguous
item. We define both source and target binarizations to be in-
cremental (i.e., left-to-right or right-to-left), so this assumption
trivially holds. More general binarization schemes are possible
to have even higher complexities, but also possible to achieve
better complexities. Full discussion is left for a separate paper.
where w? = w1 + w2 ? logPlm(with | talk) is
the score of the resulting +LM item: the sum of
the scores of the antecedent items, plus a combi-
nation cost which is the negative log probability of
the bigrams formed in combining adjacent boundary
words of antecedents.
Now that we keep track of target-side boundary
words, an additional complexity, called target-side
complexity, is introduced. In Deduction (11), four
target words are enumerated, and each +LM item
stores two boundary words; this is also true in gen-
eral for synchronous and target-side binarized gram-
mars where we always combine two consecutive
target strings in a deduction. More generally, this
scheme can be easily extended to work with an m-
gram model (Chiang, 2007) where m is usually ? 3
(trigram or higher) in practice. The target-side com-
plexity for this case is thus
O(|V |4(m?1))
where V is the target language vocabulary. This is
because each constituent must store its initial and
final (m ? 1)-grams, which yields four (m ? 1)-
grams in a binary combination. In practice, it is often
assumed that there are only a constant number of
translations for each input word, which reduces this
complexity into O(|w|4(m?1)).
However, for source-side binarization which
leaves gaps on the target-side, the situation becomes
more complicated. Consider Deduction (8), where
the sub-translation for the virtual node NP-PP is
gapped (t1?t2). Now if we integrate a bigram model
based on that deduction, we have to maintain the
boundary words of both t1 and t2 in the +LM node
of NP-PP. Together with the boundary words in node
VP, there are a total of six target words to enumerate
for this +LM deduction:
(NP-PPa?b?e?fi,k ) : (w1, t1 ? t2) (VPc?dk,l ) : (w2, t3)
(Sa?fi,l ) : (w?, t1t3t2)
(12)
where w? = w1 + w2 ? logPlm(c | b)Plm(e | d).
With an analysis similar to that of the source-side,
we state that, for a binarized grammar with target
arity t, the target-side complexity, denoted T , is
T = O(|w|2(t+1)(m?1))
37
scheme string-based tree-based
source-side |w|3+2(t+1)(m?1) |w|1+2(t+1)(m?1)
synchronous |w|3+4(m?1) |w|1+4(m?1)
target-side |w|(2s+1)+4(m?1) |w|1+4(m?1)
Table 2: Worst-case decoding complexities of the
three binarization schemes in the two approaches
(excluding the O(|w|3) time for source-side parsing
in tree-based approaches).
because in the worst-case, there are t + 1 spans in-
volved in a +LM deduction (t of them from one vir-
tual antecedent and the other one non-virtual), and
for each span, there are m ? 1 target words to enu-
merate at both left and right boundaries, giving a
total of 2(t + 1)(m ? 1) words in this deduction.
We now conclude that, in a string-based system,
the combined complexities for a binarized grammar
with source arity s and target arity t is
O(|F |T ) = O(|w|(2s+1)+2(t+1)(m?1)).
The results for the three specific binarization
schemes are summarized in Table 2. Although both
source-side and target-side binarizations lead to ex-
ponential complexities, it is likely that language
model combinations (target-side complexity) dom-
inate the computation, since m is larger than 2 in
practice. In this sense, target-side binarization is still
preferable to source-side binarization.
It is also worth noting that with the hook trick
of Huang et al (2005), the target-side complex-
ity can be reduced to O(|w|(2t+1)(m?1)), making
it more analogous to its source-side counterpart:
if we consider the decoding problem as intersect-
ing the SCFG with a source-side DFA which has
|S| = |w|+1 states, and a target-side DFA which has
|T | = O(|w|m?1) states, then the intersected gram-
mar has a parsing complexity of O(|S|2s+1|T |2t+1),
which is symmetric from both sides.
3.2 Tree-based Approaches
The tree-based approaches include the tree-to-string
(also called syntax-directed) systems (Liu et al,
2006; Huang et al, 2006). This approach takes
a source-language parse tree, instead of the plain
string, as input, and tries to find the best derivation
that recursively rewrites the input tree into a target
...
S? : t1t3t2
NP??1 : t1
...
PP??2 : t2
...
VP??3 : t3
...
Figure 4: Illustration of tree-to-string deduction.
string, using the SCFG as a tree-transducer. In this
setting, the ?LM decoding phase is a tree-parsing
problem (Eisner, 2003) which aims to cover the en-
tire tree by a set of rules. For example, a deduction
of the first rule in Example 1 would be:
(NP??1) : (w1, t1) (PP??2) : (w2, t2) (VP??3) : (w3, t3)
(S?) : (w1 + w2 + w3, t1t3t2)
(13)
where ? and ? ? i(i = 1, 2, 3) are tree addresses
(Shieber et al, 1995), with ? ? i being the ith child
of ? (the address of the root node is ?). The nonter-
minal labels at these tree nodes must match those in
the SCFG rule, e.g., the input tree must have a PP at
node ? ? 2.
The semantics of this deduction is the following:
if the label of the current node in the input tree is
S, and its three children are labeled NP, PP, and VP,
with corresponding sub-translations t1, t2, and t3,
then a possible translation for the current node S is
t1t3t2 (see Figure 4). An alternative, top-down ver-
sion of this bottom-up deductive system is, at each
node, try all SCFG rules that pattern-match the cur-
rent subtree, and recursively solve sub-problems in-
dicated by the variables, i.e., synchronous nontermi-
nals, of the matching rule (Huang et al, 2006).
With the input tree completely given, this setting
has some fundamental differences from its string-
based counterpart. First, we do not need to bina-
rize the SCFG grammar before ?LM decoding. In
fact, it will be much harder to do the tree-parsing
(pattern-matching) with a binarized grammar. Sec-
ond, regardless of the number of nonterminals in a
rule, building the ?LM forest always costs time lin-
ear in the size of the input tree (times a grammar
constant, see (Huang et al, 2006, Sec. 5.1) for de-
tails), which is in turn linear in the length of the input
string. So we have:
O(|F |) = O(|w|).
38
This fast ?LM decoding is a major advantage of
tree-based approaches.
Now in +LM decoding, we still need binariza-
tion of the hyperedges, as opposed to rules, in the
forest, but the analysis is almost identical to that of
string-based approach. For example, the tree-based
version of Deduction (12) for source-side binariza-
tion is now notated
(NP??1-PP??2a?b?e?f ) : (w1, t1 ? t2) (VP??3c?d) : (w2, t3)
(S?a?f ) : (w?, t1t3t2)
(14)
In general, the target-side complexity of a bina-
rized grammar with target arity t is still T =
O(|w|2(t+1)(m?1)) and the combined decoding com-
plexity of the tree-based approach is
O(|F |T ) = O(|w|1+2(t+1)(m?1)).
Table 2 shows that in this tree-based setting,
target-side binarization has exactly the same perfor-
mance with synchronous binarization while being
much simpler to implement and does not have the
problem of non-binarizability. The fact that simple
binarization works (at least) equally well, which is
not possible in string-based systems, is another ad-
vantage of the tree-based approaches.
4 Experiments
Section 3 shows that target-side binarization
achieves the same polynomial decoding complexity
as the more sophisticated synchronous binarization
in the tree-based systems. We now empirically com-
pare target-side binarization with an even simpler
variant, on-the-fly generation, where the only dif-
ference is that the latter does target-side left-to-right
binarization during +LM decoding on a hyperedge-
per-hyperedge basis, without sharing common vir-
tual nonterminals across hyperedges, while the for-
mer binarizes the whole ?LM forest before the
+LM decoding.
Our experiments are on English-to-Chinese trans-
lation in the tree-to-string system of Huang et al
(2006), which takes a source-language parse tree as
input and tries to recursively convert it to a target-
language string according to transfer rules in a syn-
chronous grammar (Galley et al, 2006). For in-
stance, the following rule
 0
 100
 200
 300
 400
 500
 600
 5  10  15  20  25  30  35  40
n
u
m
be
r o
f n
od
es
 in
 th
e 
fo
re
st
length of the input sentence
original forest
target-side binarization
on-the-fly generation
Figure 5: Number of nodes in the forests. Input
sentences are grouped into bins according to their
lengths (5-9, 10-14, 15-20, etc.).
VP
VBD
was
VP-C
x1:VBN PP
IN
by
x2:NP-C
? bei x2 x1
translates an English passive construction into Chi-
nese. Although the rules are actually in a syn-
chronous tree-substitution grammar (STSG) instead
of an SCFG, its derivation structure is still a hy-
pergraph and all the analysis in Section 3.2 still
applies. This system performs slightly better than
the state-of-the-art phrase-based system Pharaoh
(Koehn, 2004) on English to Chinese translation. A
very similar system for the reverse direction is de-
scribed in (Liu et al, 2006).
Our data preparation follows (Huang et al, 2006):
the training data is a parallel corpus of 28.3M words
on the English side, from which we extracted 24.7M
tree-to-string rules using the algorithm of (Galley et
al., 2006), and trained a Chinese trigram model on
the Chinese side. We test our methods on the same
test-set as in (Huang et al, 2006) which is a 140 sen-
tence subset of NIST 2003 MT evaluation with 9?36
words on the English side. The weights for the log-
linear model is tuned on a separate development set.
Figure 5 compares the number of nodes in the bi-
narized forests against the original forest. On-the-fly
generation essentially works on a larger forest with
39
 25
 25.2
 25.4
 25.6
 25.8
 26
 26.2
 5  10  15  20
 0
 20000
 40000
 60000
 80000
 100000
 120000
BL
EU
 s
co
re
a
ve
ra
ge
 #
 o
f +
LM
 it
em
s 
pe
r s
en
te
nc
e
beam size
BLEU score
on-the-fly generation
target-side binarization
Figure 6: Decoding speed and BLEU scores under
beam search.
duplicate nodes due to the lack of sharing, which is
on average 1.85 times bigger than the target-side bi-
narized forest. This difference is also reflected in the
decoding speed, which is illustrated in Figure 6 un-
der various beam settings and where the amount of
computation is measured by the number of +LM
items generated. At each individual beam setting,
the two methods produce exactly the same set of
translations (i.e., there is no relative search error),
but the target-side binarization is consistently 1.3
times faster thanks to the sharing. In terms of transla-
tion quality, the final BLEU score at the largest beam
setting is 0.2614, significantly higher than Pharaoh?s
0.2354 as reported in (Huang et al, 2006).
5 Conclusion
This paper introduces a simple binarization scheme,
target-side binarization, and presents a systematic
study of the theoretical properties of the three bina-
rization schemes in both string-based and tree-based
systems using syncrhonous grammars. In particular,
we show that target-side binarization achieves the
same polynomial complexity as synchronous bina-
rization while being much simpler to implement and
universally applicable to arbitrary SCFGs. We also
demonstrate the empirical effectiveness of this new
scheme on a large-scale tree-to-string system.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33. To
appear.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
(poster), pages 205?208.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
Proceedings of the Ninth International Workshop on
Parsing Technologies (IWPT-2005).
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of NAACL.
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proc. of HLT-EMNLP 2005.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In Proceedings of
COLING-ACL.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT-NAACL.
40
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222?1231,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Liang Huang
Google Research
1350 Charleston Rd.
Mountain View, CA 94043, USA
lianghuang@google.com
liang.huang.sh@gmail.com
Wenbin Jiang and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
jiangwenbin@ict.ac.cn
Abstract
Jointly parsing two languages has been
shown to improve accuracies on either or
both sides. However, its search space is
much bigger than the monolingual case,
forcing existing approaches to employ
complicated modeling and crude approxi-
mations. Here we propose a much simpler
alternative, bilingually-constrained mono-
lingual parsing, where a source-language
parser learns to exploit reorderings as ad-
ditional observation, but not bothering to
build the target-side tree as well. We show
specifically how to enhance a shift-reduce
dependency parser with alignment fea-
tures to resolve shift-reduce conflicts. Ex-
periments on the bilingual portion of Chi-
nese Treebank show that, with just 3 bilin-
gual features, we can improve parsing ac-
curacies by 0.6% (absolute) for both En-
glish and Chinese over a state-of-the-art
baseline, with negligible (?6%) efficiency
overhead, thus much faster than biparsing.
1 Introduction
Ambiguity resolution is a central task in Natu-
ral Language Processing. Interestingly, not all lan-
guages are ambiguous in the same way. For exam-
ple, prepositional phrase (PP) attachment is (no-
toriously) ambiguous in English (and related Eu-
ropean languages), but is strictly unambiguous in
Chinese and largely unambiguous Japanese; see
(1a) I [ saw Bill ] [ with a telescope ].
wo [ yong wangyuanjin] [kandao le Bi?er].
?I used a telescope to see Bill.?
(1b) I saw [ Bill [ with a telescope ] ].
wo kandao le [ [ na wangyuanjin ] de Bi?er].
?I saw Bill who had a telescope at hand.?
Figure 1: PP-attachment is unambiguous in Chi-
nese, which can help English parsing.
Figure 1 for an example.1 It is thus intuitive to use
two languages for better disambiguation, which
has been applied not only to this PP-attachment
problem (Fossum and Knight, 2008; Schwartz et
al., 2003), but also to the more fundamental prob-
lem of syntactic parsing which subsumes the for-
mer as a subproblem. For example, Smith and
Smith (2004) and Burkett and Klein (2008) show
that joint parsing (or reranking) on a bitext im-
proves accuracies on either or both sides by lever-
aging bilingual constraints, which is very promis-
ing for syntax-based machine translation which re-
quires (good-quality) parse trees for rule extrac-
tion (Galley et al, 2004; Mi and Huang, 2008).
However, the search space of joint parsing is in-
evitably much bigger than the monolingual case,
1Chinese uses word-order to disambiguate the attachment
(see below). By contrast, Japanese resorts to case-markers
and the unambiguity is limited: it works for the ?V or N?
attachment ambiguities like in Figure 1 (see (Schwartz et al,
2003)) but not for the ?N
1
or N
2
? case (Mitch Marcus, p.c.).
1222
forcing existing approaches to employ compli-
cated modeling and crude approximations. Joint
parsing with a simplest synchronous context-free
grammar (Wu, 1997) is O(n6) as opposed to the
monolingual O(n3) time. To make things worse,
languages are non-isomorphic, i.e., there is no 1-
to-1 mapping between tree nodes, thus in practice
one has to use more expressive formalisms such
as synchronous tree-substitution grammars (Eis-
ner, 2003; Galley et al, 2004). In fact, rather than
joint parsing per se, Burkett and Klein (2008) re-
sort to separate monolingual parsing and bilingual
reranking over k2 tree pairs, which covers a tiny
fraction of the whole space (Huang, 2008).
We instead propose a much simpler alterna-
tive, bilingually-constrained monolingual parsing,
where a source-language parser is extended to ex-
ploit the reorderings between languages as addi-
tional observation, but not bothering to build a tree
for the target side simultaneously. To illustrate the
idea, suppose we are parsing the sentence
(1) I saw Bill [PP with a telescope ].
which has 2 parses based on the attachment of PP:
(1a) I [ saw Bill ] [PP with a telescope ].
(1b) I saw [ Bill [PP with a telescope ]].
Both are possible, but with a Chinese translation
the choice becomes clear (see Figure 1), because
a Chinese PP always immediately precedes the
phrase it is modifying, thus making PP-attachment
strictly unambiguous.2 We can thus use Chinese to
help parse English, i.e., whenever we have a PP-
attachment ambiguity, we will consult the Chinese
translation (from a bitext), and based on the align-
ment information, decide where to attach the En-
glish PP. On the other hand, English can help Chi-
nese parsing as well, for example in deciding the
scope of relative clauses which is unambiguous in
English but ambiguous in Chinese.
This method is much simpler than joint pars-
ing because it remains monolingual in the back-
bone, with alignment information merely as soft
evidence, rather than hard constraints since auto-
matic word alignment is far from perfect. It is thus
2to be precise, in Fig. 1(b), the English PP is translated
into a Chinese relative clause, but nevertheless all phrasal
modifiers attach to the immediate right in Mandarin Chinese.
straightforward to implement within a monolin-
gual parsing algorithm. In this work we choose
shift-reduce dependency parsing for its simplicity
and efficiency. Specifically, we make the following
contributions:
? we develop a baseline shift-reduce depen-
dency parser using the less popular, but clas-
sical, ?arc-standard? style (Section 2), and
achieve similar state-of-the-art performance
with the the dominant but complicated ?arc-
eager? style of Nivre and Scholz (2004);
? we propose bilingual features based on word-
alignment information to prefer ?target-side
contiguity? in resolving shift-reduce conflicts
(Section 3);
? we verify empirically that shift-reduce con-
flicts are the major source of errors, and cor-
rect shift-reduce decisions strongly correlate
with the above bilingual contiguity condi-
tions even with automatic alignments (Sec-
tion 5.3);
? finally, with just three bilingual features,
we improve dependency parsing accuracy
by 0.6% for both English and Chinese over
the state-of-the-art baseline with negligible
(?6%) efficiency overhead (Section 5.4).
2 Simpler Shift-Reduce Dependency
Parsing with Three Actions
The basic idea of classical shift-reduce parsing
from compiler theory (Aho and Ullman, 1972) is
to perform a left-to-right scan of the input sen-
tence, and at each step, choose one of the two ac-
tions: either shift the current word onto the stack,
or reduce the top two (or more) items on the stack,
replacing them with their combination. This idea
has been applied to constituency parsing, for ex-
ample in Sagae and Lavie (2006), and we describe
below a simple variant for dependency parsing
similar to Yamada and Matsumoto (2003) and the
?arc-standard? version of Nivre (2004).
2.1 The Three Actions
Basically, we just need to split the reduce ac-
tion into two symmetric (sub-)actions, reduceL
and reduceR, depending on which one of the two
1223
stack queue arcs
previous S w
i
|Q A
shift S|w
i
Q A
previous S|s
t?1
|s
t
Q A
reduceL S|st Q A ? {(st, st?1)}
reduceR S|st?1 Q A ? {(st?1, st)}
Table 1: Formal description of the three actions.
Note that shift requires non-empty queue while
reduce requires at least two elements on the stack.
items becomes the head after reduction. More for-
mally, we describe a parser configuration by a tu-
ple ?S,Q,A? where S is the stack, Q is the queue
of remaining words of the input, and A is the set
of dependency arcs accumulated so far.3 At each
step, we can choose one of the three actions:
1. shift: move the head of (a non-empty) queue
Q onto stack S;
2. reduceL: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace
them with s
t
(as the head), and add a left arc
(s
t
, s
t?1
) to A;
3. reduceR: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace them
with s
t?1
(as the head), and add a right arc
(s
t?1
, s
t
) to A.
These actions are summarized in Table 1. The
initial configuration is always ??, w
1
. . . w
n
, ??
with empty stack and no arcs, and the final con-
figuration is ?w
j
, ?, A? where w
j
is recognized as
the root of the whole sentence, and A encodes a
spanning tree rooted at w
j
. For a sentence of n
words, there are exactly 2n ? 1 actions: n shifts
and n ? 1 reductions, since every word must be
pushed onto stack once, and every word except the
root will eventually be popped in a reduction. The
time complexity, as other shift-reduce instances, is
clearly O(n).
2.2 Example of Shift-Reduce Conflict
Figure 2 shows the trace of this paradigm on the
example sentence. For the first two configurations
3a ?configuration? is sometimes called a ?state? (Zhang
and Clark, 2008), but that term is confusing with the states in
shift-reduce LR/LL parsing, which are quite different.
0 - I saw Bill with a ...
1 shift I saw Bill with a ...
2 shift I saw Bill with a ...
3 reduceL saw Bill with a ...
I
4 shift saw Bill with a ...
I
5a reduceR saw with a ...
I Bill
5b shift saw Bill with a ...
I
Figure 2: A trace of 3-action shift-reduce on the
example sentence. Shaded words are on stack,
while gray words have been popped from stack.
After step (4), the process can take either (5a)
or (5b), which correspond to the two attachments
(1a) and (1b) in Figure 1, respectively.
(0) and (1), only shift is possible since there are
not enough items on the stack for reduction. At
step (3), we perform a reduceL, making word ?I?
a modifier of ?saw?; after that the stack contains
a single word and we have to shift the next word
?Bill? (step 4). Now we face a shift-reduce con-
flict: we can either combine ?saw? and ?Bill? in
a reduceR action (5a), or shift ?Bill? (5b). We will
use features extracted from the configuration to re-
solve the conflict. For example, one such feature
could be a bigram s
t
? s
t?1
, capturing how likely
these two words are combined; see Table 2 for the
complete list of feature templates we use in this
baseline parser.
We argue that this kind of shift-reduce conflicts
are the major source of parsing errors, since the
other type of conflict, reduce-reduce conflict (i.e.,
whether left or right) is relatively easier to resolve
given the part-of-speech information. For exam-
ple, between a noun and an adjective, the former
is much more likely to be the head (and so is a
verb vs. a preposition or an adverb). Shift-reduce
resolution, however, is more non-local, and often
involves a triple, for example, (saw, Bill, with) for
a typical PP-attachment. On the other hand, if we
indeed make a wrong decision, a reduce-reduce
mistake just flips the head and the modifier, and
often has a more local effect on the shape of the
tree, whereas a shift-reduce mistake always leads
1224
Type Features
Unigram s
t
T (s
t
) s
t
? T (s
t
)
s
t?1
T (s
t?1
) s
t?1
? T (s
t?1
)
w
i
T (w
i
) w
i
? T (w
i
)
Bigram s
t
? s
t?1
T (s
t
) ? T (s
t?1
) T (s
t
) ? T (w
i
)
T (s
t
) ? s
t?1
? T (s
t?1
) s
t
? s
t?1
? T (s
t?1
) s
t
? T (s
t
) ? T (s
t?1
)
s
t
? T (s
t
) ? s
t?1
s
t
? T (s
t
) ? s
t?1
? T (s
t?1
)
Trigram T (s
t
) ? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? T (s
t
) ? T (w
i
) T (s
t?2
) ? T (s
t?1
) ? T (s
t
)
s
t
? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? s
t
? T (w
i
)
Modifier T (s
t?1
) ? T (lc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (rc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (s
t
) ? T (lc(s
t
))
T (s
t?1
) ? T (s
t
) ? T (rc(s
t
)) T (s
t?1
) ? T (lc(s
t?1
)) ? s
t
T (s
t?1
) ? T (rc(s
t?1
)) ? s
t
T (s
t?1
) ? s
t
? T (lc(s
t
))
Table 2: Feature templates of the baseline parser. s
t
, s
t?1
denote the top and next to top words on the
stack; w
i
and w
i+1
denote the current and next words on the queue. T (?) denotes the POS tag of a
given word, and lc(?) and rc(?) represent the leftmost and rightmost child. Symbol ? denotes feature
conjunction. Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.
to vastly incompatible tree shapes with crossing
brackets (for example, [saw Bill] vs. [Bill with a
telescope]). We will see in Section 5.3 that this
is indeed the case in practice, thus suggesting us
to focus on shift-reduce resolution, which we will
return to with the help of bilingual constraints in
Section 3.
2.3 Comparison with Arc-Eager
The three action system was originally described
by Yamada and Matsumoto (2003) (although their
methods require multiple passes over the input),
and then appeared as ?arc-standard? in Nivre
(2004), but was argued against in comparison to
the four-action ?arc-eager? variant. Most subse-
quent works on shift-reduce or ?transition-based?
dependency parsing followed ?arc-eager? (Nivre
and Scholz, 2004; Zhang and Clark, 2008), which
now becomes the dominant style. But we argue
that ?arc-standard? is preferable because:
1. in the three action ?arc-standard? system, the
stack always contains a list of unrelated sub-
trees recognized so far, with no arcs between
any of them, e.g. (I? saw) and (Bill) in step
4 of Figure 2), whereas the four action ?arc-
eager? style can have left or right arrows be-
tween items on the stack;
2. the semantics of the three actions are atomic
and disjoint, whereas the semantics of 4 ac-
tions are not completely disjoint. For exam-
ple, their Left action assumes an implicit Re-
duce of the left item, and their Right ac-
tion assumes an implicit Shift. Furthermore,
these two actions have non-trivial precondi-
tions which also causes the next problem (see
below). We argue that this is rather compli-
cated to implement.
3. the ?arc-standard? scan always succeeds,
since at the end we can always reduce with
empty queue, whereas the ?arc-eager? style
sometimes goes into deadends where no ac-
tion can perform (prevented by precondi-
tions, otherwise the result will not be a well-
formed tree). This becomes parsing failures
in practice (Nivre and Scholz, 2004), leaving
more than one fragments on stack.
As we will see in Section 5.1, this simpler
arc-standard system performs equally well with
a state-of-the-art arc-eager system (Zhang and
Clark, 2008) on standard English Treebank pars-
ing (which is never shown before). We argue
that all things being equal, this simpler paradigm
should be preferred in practice. 4
2.4 Beam Search Extension
We also enhance deterministic shift-reduce pars-
ing with beam search, similar to Zhang and Clark
(2008), where k configurations develop in paral-
lel. Pseudocode 1 illustrates the algorithm, where
we keep an agenda V of the current active con-
figurations, and at each step try to extend them by
applying one of the three actions. We then dump
the best k new configurations from the buffer back
4On the other hand, there are also arguments for ?arc-
eager?, e.g., ?incrementality?; see (Nivre, 2004; Nivre, 2008).
1225
Pseudocode 1 beam-search shift-reduce parsing.
1: Input: POS-tagged word sequence w
1
. . . w
n
2: start ? ??, w
1
. . . w
n
, ?? ? initial config: empty stack,
no arcs
3: V? {start} ? initial agenda
4: for step ? 1 . . . 2n? 1 do
5: BUF? ? ? buffer for new configs
6: for each config in agenda V do
7: for act ? {shift, reduceL, reduceR} do
8: if act is applicable to config then
9: next ? apply act to config
10: insert next into buffer BUF
11: V? top k configurations of BUF
12: Output: the tree of the best config in V
into the agenda for the next step. The complexity
of this algorithm is O(nk), which subsumes the
determinstic mode as a special case (k = 1).
2.5 Online Training
To train the parser we need an ?oracle? or gold-
standard action sequence for gold-standard depen-
dency trees. This oracle turns out to be non-unique
for the three-action system (also non-unique for
the four-action system), because left dependents
of a head can be reduced either before or after all
right dependents are reduced. For example, in Fig-
ure 2, ?I? is a left dependent of ?saw?, and can in
principle wait until ?Bill? and ?with? are reduced,
and then finally combine with ?saw?. We choose
to use the heuristic of ?shortest stack? that always
prefers reduceL over shift, which has the effect that
all left dependents are first recognized inside-out,
followed by all right dependents, also inside-out,
which coincides with the head-driven constituency
parsing model of Collins (1999).
We use the popular online learning algorithm
of structured perceptron with parameter averag-
ing (Collins, 2002). Following Collins and Roark
(2004) we also use the ?early-update? strategy,
where an update happens whenever the gold-
standard action-sequence falls off the beam, with
the rest of the sequence neglected. As a special
case, for the deterministic mode, updates always
co-occur with the first mistake made. The intuition
behind this strategy is that future mistakes are of-
ten caused by previous ones, so with the parser on
the wrong track, future actions become irrelevant
for learning. See Section 5.3 for more discussions.
(a) I
:::::::::
saw Bill with a telescope .
wo yong wangyuanjin kandao le Bi?er.
c(s
t?1
, s
t
) =+; reduce is correct
(b) I
:::::::::
saw Bill with a telescope .
wo kandao le na wangyuanjin de Bi?er.
c(s
t?1
, s
t
) =?; reduce is wrong
(c) I saw
:::::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo kandao le na wangyuanjin de Bi?er.
cR(st, wi) =+; shift is correct
(d) I saw
:::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo yong wangyuanjin kandao le Bi?er.
cR(st, wi) =?; shift is wrong
Figure 3: Bilingual contiguity features c(s
t?1
, s
t
)
and cR(st, wi) at step (4) in Fig. 2 (facing a shift-
reduce decision). Bold words are currently on
stack while gray ones have been popped. Here the
stack tops are s
t
= Bill, s
t?1
= saw, and the queue
head is w
i
= with; underlined texts mark the source
and target spans being considered, and wavy un-
derlines mark the allowed spans (Tab. 3). Red bold
alignment links violate contiguity constraints.
3 Soft Bilingual Constraints as Features
As suggested in Section 2.2, shift-reduce con-
flicts are the central problem we need to address
here. Our intuition is, whenever we face a deci-
sion whether to combine the stack tops s
t?1
and
s
t
or to shift the current word w
i
, we will consult
the other language, where the word-alignment in-
formation would hopefully provide a preference,
as in the running example of PP-attachment (see
Figure 1). We now develop this idea into bilingual
contiguity features.
1226
3.1 A Pro-Reduce Feature c(s
t?1
, s
t
)
Informally, if the correct decision is a reduction,
then it is likely that the corresponding words of
s
t?1
and s
t
on the target-side should also form a
contiguous span. For example, in Figure 3(a), the
source span of a reduction is [saw .. Bill], which
maps onto [kandao . . . Bi?er] on the Chinese side.
This target span is contiguous, because no word
within this span is aligned to a source word out-
side of the source span. In this case we say feature
c(s
t?1
, s
t
) =+, which encourages ?reduce?.
However, in Figure 3(b), the source span is still
[saw .. Bill], but this time maps onto a much
longer span on the Chinese side. This target span
is discontiguous, since the Chinese words na and
wangyuanjin are alinged to English ?with? and
?telescope?, both of which fall outside of the
source span. In this case we say feature c(s
t?1
, s
t
)
=?, which discourages ?reduce? .
3.2 A Pro-Shift Feature cR(st, wi)
Similarly, we can develop another feature
cR(st, wi) for the shift action. In Figure 3(c),
when considering shifting ?with?, the source
span becomes [Bill .. with] which maps to [na
.. Bi?er] on the Chinese side. This target span
looks like discontiguous in the above definition
with wangyuanjin aligned to ?telescope?, but we
tolerate this case for the following reasons. There
is a crucial difference between shift and reduce:
in a shift, we do not know yet the subtree spans
(unlike in a reduce we are always combining two
well-formed subtrees). The only thing we are
sure of in a shift action is that s
t
and w
i
will be
combined before s
t?1
and s
t
are combined (Aho
and Ullman, 1972), so we can tolerate any target
word aligned to source word still in the queue,
but do not allow any target word aligned to an
already recognized source word. This explains
the notational difference between cR(st, wi) and
c(s
t?1
, s
t
), where subscript ?R? means ?right
contiguity?.
As a final example, in Figure 3(d), Chinese
word kandao aligns to ?saw?, which is already
recognized, and this violates the right contiguity.
So cR(st, wi) =?, suggesting that shift is probably
wrong. To be more precise, Table 3 shows the for-
mal definitions of the two features. We basically
source target alowed
feature f span sp span tp span ap
c(s
t?1
, s
t
) [s
t?1
..s
t
] M(sp) [s
t?1
..s
t
]
cR(st, wi) [st..wi] M(sp) [st..wn]
f = + iff. M?1(M(sp)) ? ap
Table 3: Formal definition of bilingual features.
M(?) is maps a source span to the target language,
and M?1(?) is the reverse operation mapping back
to the source language.
map a source span sp to its target span M(sp),
and check whether its reverse image back onto the
source language M?1(M(sp)) falls inside the al-
lowed span ap. For cR(st, wi), the allowed span
extends to the right end of the sentence.5
3.3 Variations and Implementation
To conclude so far, we have got two alignment-
based features, c(s
t?1
, s
t
) correlating with reduce,
and cR(st, wi) correlating with shift. In fact, the
conjunction of these two features,
c(s
t?1
, s
t
) ? cR(st, wi)
is another feature with even stronger discrimina-
tion power. If
c(s
t?1
, s
t
) ? cR(st, wi) = + ? ?
it is strongly recommending reduce, while
c(s
t?1
, s
t
) ? cR(st, wi) = ? ?+
is a very strong signal for shift. So in total we got
three bilingual feature (templates), which in prac-
tice amounts to 24 instances (after cross-product
with {?,+} and the three actions). We show in
Section 5.3 that these features do correlate with
the correct shift/reduce actions in practice.
The naive implemention of bilingual feature
computation would be of O(kn2) complexity
in the worse case because when combining the
largest spans one has to scan over the whole sen-
tence. We envision the use of a clever datastructure
would reduce the complexity, but leave this to fu-
ture work, as the experiments (Table 8) show that
5Our definition implies that we only consider faithful
spans to be contiguous (Galley et al, 2004). Also note that
source spans include all dependents of s
t
and s
t?1
.
1227
the parser is only marginally (?6%) slower with
the new bilingual features. This is because the ex-
tra work, with just 3 bilingual features, is not the
bottleneck in practice, since the extraction of the
vast amount of other features in Table 2 dominates
the computation.
4 Related Work in Grammar Induction
Besides those cited in Section 1, there are some
other related work on using bilingual constraints
for grammar induction (rather than parsing). For
example, Hwa et al (2005) use simple heuris-
tics to project English trees to Spanish and Chi-
nese, but get discouraging accuracy results learned
from those projected trees. Following this idea,
Ganchev et al (2009) and Smith and Eisner (2009)
use constrained EM and parser adaptation tech-
niques, respectively, to perform more principled
projection, and both achieve encouraging results.
Our work, by constrast, never uses bilingual
tree pairs not tree projections, and only uses word
alignment alone to enhance a monolingual gram-
mar, which learns to prefer target-side contiguity.
5 Experiments
5.1 Baseline Parser
We implement our baseline monolingual parser (in
C++) based on the shift-reduce algorithm in Sec-
tion 2, with feature templates from Table 2. We
evaluate its performance on the standard Penn En-
glish Treebank (PTB) dependency parsing task,
i.e., train on sections 02-21 and test on section 23
with automatically assigned POS tags (at 97.2%
accuracy) using a tagger similar to Collins (2002),
and using the headrules of Yamada and Mat-
sumoto (2003) for conversion into dependency
trees. We use section 22 as dev set to deter-
mine the optimal number of iterations in per-
ceptron training. Table 4 compares our baseline
against the state-of-the-art graph-based (McDon-
ald et al, 2005) and transition-based (Zhang and
Clark, 2008) approaches, and confirms that our
system performs at the same level with those state-
of-the-art, and runs extremely fast in the determin-
istic mode (k=1), and still quite fast in the beam-
search mode (k=16).
parser accuracy secs/sent
McDonald et al (2005) 90.7 0.150
Zhang and Clark (2008) 91.4 0.195
our baseline at k=1 90.2 0.009
our baseline at k=16 91.3 0.125
Table 4: Baseline parser performance on standard
Penn English Treebank dependency parsing task.
The speed numbers are not exactly comparable
since they are reported on different machines.
Training Dev Test
CTB Articles 1-270 301-325 271-300
Bilingual Paris 2745 273 290
Table 5: Training, dev, and test sets from bilingual
Chinese Treebank a` la Burkett and Klein (2008).
5.2 Bilingual Data
The bilingual data we use is the translated por-
tion of the Penn Chinese Treebank (CTB) (Xue
et al, 2002), corresponding to articles 1-325 of
PTB, which have English translations with gold-
standard parse trees (Bies et al, 2007). Table 5
shows the split of this data into training, devel-
opment, and test subsets according to Burkett and
Klein (2008). Note that not all sentence pairs could
be included, since many of them are not one-
to-one aligned at the sentence level. Our word-
alignments are generated from the HMM aligner
of Liang et al (2006) trained on approximately
1.7M sentence pairs (provided to us by David Bur-
kett, p.c.). This aligner outputs ?soft alignments?,
i.e., posterior probabilities for each source-target
word pair. We use a pruning threshold of 0.535 to
remove low-confidence alignment links,6 and use
the remaining links as hard alignments; we leave
the use of alignment probabilities to future work.
For simplicity reasons, in the following exper-
iments we always supply gold-standard POS tags
as part of the input to the parser.
5.3 Testing our Hypotheses
Before evaluating our bilingual approach, we need
to verify empirically the two assumptions we
made about the parser in Sections 2 and 3:
6and also removing notoriously bad links in {the, a, an}?
{de, le} following Fossum and Knight (2008).
1228
sh ? re re ? sh sh-re re-re
# 92 98 190 7
% 46.7% 49.7% 96.4% 3.6%
Table 6: [Hypothesis 1] Error distribution in the
baseline model (k = 1) on English dev set.
?sh ? re? means ?should shift, but reduced?. Shift-
reduce conflicts overwhelmingly dominate.
1. (monolingual) shift-reduce conflict is the ma-
jor source of errors while reduce-reduce con-
flict is a minor issue;
2. (bilingual) the gold-standard decisions of
shift or reduce should correlate with contigu-
ities of c(s
t?1
, s
t
), and of cR(st, wi).
Hypothesis 1 is verified in Table 6, where we
count all the first mistakes the baseline parser
makes (in the deterministic mode) on the En-
glish dev set (273 sentences). In shift-reduce pars-
ing, further mistakes are often caused by previ-
ous ones, so only the first mistake in each sen-
tence (if there is one) is easily identifiable;7 this
is also the argument for ?early update? in apply-
ing perceptron learning to these incremental pars-
ing algorithms (Collins and Roark, 2004) (see also
Section 2). Among the 197 first mistakes (other
76 sentences have perfect output), the vast ma-
jority, 190 of them (96.4%), are shift-reduce er-
rors (equally distributed between shift-becomes-
reduce and reduce-becomes-shift), and only 7
(3.6%) are due to reduce-reduce conflicts.8 These
statistics confirm our intuition that shift-reduce de-
cisions are much harder to make during parsing,
and contribute to the overwhelming majority of er-
rors, which is studied in the next hypothesis.
Hypothesis 2 is verified in Table 7. We take
the gold-standard shift-reduce sequence on the En-
glish dev set, and classify them into the four cat-
egories based on bilingual contiguity features: (a)
c(s
t?1
, s
t
), i.e. whether the top 2 spans on stack
is contiguous, and (b) cR(st, wi), i.e. whether the
7to be really precise one can define ?independent mis-
takes? as those not affected by previous ones, i.e., errors
made after the parser recovers from previous mistakes; but
this is much more involved and we leave it to future work.
8Note that shift-reduce errors include those due to the
non-uniqueness of oracle, i.e., between some reduceL and
shift. Currently we are unable to identify ?genuine? errors
that would result in an incorrect parse. See also Section 2.5.
c(s
t?1
, s
t
) cR(st, wi) shift reduce
+ ? 172 ? 1,209
? + 1,432 > 805
+ + 4,430 ? 3,696
? ? 525 ? 576
total 6,559 = 6,286
Table 7: [Hyp. 2] Correlation of gold-standard
shift/reduce decisions with bilingual contiguity
conditions (on English dev set). Note there is al-
ways one more shift than reduce in each sentence.
stack top is contiguous with the current word w
i
.
According to discussions in Section 3, when (a) is
contiguous and (b) is not, it is a clear signal for
reduce (to combine the top two elements on the
stack) rather than shift, and is strongly supported
by the data (first line: 1209 reduces vs. 172 shifts);
and while when (b) is contiguous and (a) is not,
it should suggest shift (combining s
t
and w
i
be-
fore s
t?1
and s
t
are combined) rather than reduce,
and is mildly supported by the data (second line:
1432 shifts vs. 805 reduces). When (a) and (b) are
both contiguous or both discontiguous, it should
be considered a neutral signal, and is also consis-
tent with the data (next two lines). So to conclude,
this bilingual hypothesis is empirically justified.
On the other hand, we would like to note that
these correlations are done with automatic word
alignments (in our case, from the Berkeley aligner)
which can be quite noisy. We suspect (and will fin-
ish in the future work) that using manual align-
ments would result in a better correlation, though
for the main parsing results (see below) we can
only afford automatic alignments in order for our
approach to be widely applicable to any bitext.
5.4 Results
We incorporate the three bilingual features (again,
with automatic alignments) into the baseline
parser, retrain it, and test its performance on the
English dev set, with varying beam size. Table 8
shows that bilingual constraints help more with
larger beams, from almost no improvement with
the deterministic mode (k=1) to +0.5% better with
the largest beam (k=16). This could be explained
by the fact that beam-search is more robust than
the deterministic mode, where in the latter, if our
1229
baseline +bilingual
k accuracy time (s) accuracy time (s)
1 84.58 0.011 84.67 0.012
2 85.30 0.025 85.62 0.028
4 85.42 0.040 85.81 0.044
8 85.50 0.081 85.95 0.085
16 85.57 0.158 86.07 0.168
Table 8: Effects of beam size k on efficiency and
accuracy (on English dev set). Time is average
per sentence (in secs). Bilingual constraints show
more improvement with larger beams, with a frac-
tional efficiency overhead over the baseline.
English Chinese
monolingual baseline 86.9 85.7
+bilingual features 87.5 86.3
improvement +0.6 +0.6
signficance level p < 0.05 p < 0.08
Berkeley parser 86.1 87.9
Table 9: Final results of dependency accuracy (%)
on the test set (290 sentences, beam size k=16).
bilingual features misled the parser into a mistake,
there is no chance of getting back, while in the
former multiple configurations are being pursued
in parallel. In terms of speed, both parsers run pro-
portionally slower with larger beams, as the time
complexity is linear to the beam-size. Computing
the bilingual features further slows it down, but
only fractionally so (just 1.06 times as slow as the
baseline at k=16), which is appealing in practice.
By contrast, Burkett and Klein (2008) reported
their approach of ?monolingual k-best parsing fol-
lowed by bilingual k2-best reranking? to be ?3.8
times slower? than monolingual parsing.
Our final results on the test set (290 sentences)
are summarized in Table 9. On both English
and Chinese, the addition of bilingual features
improves dependency arc accuracies by +0.6%,
which is mildly significant using the Z-test of
Collins et al (2005). We also compare our results
against the Berkeley parser (Petrov and Klein,
2007) as a reference system, with the exact same
setting (i.e., trained on the bilingual data, and test-
ing using gold-standard POS tags), and the result-
ing trees are converted into dependency via the
same headrules. We use 5 iterations of split-merge
grammar induction as the 6th iteration overfits the
small training set. The result is worse than our
baseline on English, but better than our bilingual
parser on Chinese. The discrepancy between En-
glish and Chinese is probably due to the fact that
our baseline feature templates (Table 2) are engi-
neered on English not Chinese.
6 Conclusion and Future Work
We have presented a novel parsing paradigm,
bilingually-constrained monolingual parsing,
which is much simpler than joint (bi-)parsing, yet
still yields mild improvements in parsing accuracy
in our preliminary experiments. Specifically,
we showed a simple method of incorporating
alignment features as soft evidence on top of a
state-of-the-art shift-reduce dependency parser,
which helped better resolve shift-reduce conflicts
with fractional efficiency overhead.
The fact that we managed to do this with only
three alignment features is on one hand encour-
aging, but on the other hand leaving the bilingual
feature space largely unexplored. So we will en-
gineer more such features, especially with lexical-
ization and soft alignments (Liang et al, 2006),
and study the impact of alignment quality on pars-
ing improvement. From a linguistics point of view,
we would like to see how linguistics distance
affects this approach, e.g., we suspect English-
French would not help each other as much as
English-Chinese do; and it would be very interest-
ing to see what types of syntactic ambiguities can
be resolved across different language pairs. Fur-
thermore, we believe this bilingual-monolingual
approach can easily transfer to shift-reduce con-
stituency parsing (Sagae and Lavie, 2006).
Acknowledgments
We thank the anonymous reviewers for pointing to
us references about ?arc-standard?. We also thank
Aravind Joshi and Mitch Marcus for insights on
PP attachment, Joakim Nivre for discussions on
arc-eager, Yang Liu for suggestion to look at man-
ual alignments, and David A. Smith for sending
us his paper. The second and third authors were
supported by National Natural Science Foundation
of China, Contracts 60603095 and 60736014, and
863 State Key Project No. 2006AA010108.
1230
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v1.0. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL (poster), pages 205?208.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual chinese-english word alignments to resolve pp-
attachment ambiguity in english. In Proceedings of
AMTA Student Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL, pages 273?280.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP,
Honolulu, Haiwaii.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of ACL
(poster).
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of english pp attachment using mul-
tilingual aligned data. In Proceedings of MT Summit
IX.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous features.
In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
1231
Proceedings of NAACL HLT 2009: Tutorials, pages 5?6,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Dynamic Programming-based Search Algorithms in NLP
Liang Huang, Google Research
Dynamic Programming (DP) is an important class of algorithms widely used in many ar-
eas of speech and language processing. It provides efficient solutions to seemingly intractable
inference over exponentially-large spaces by sharing overlapping subproblems. Well-known
examples of DP in our field include Viterbi and Forward-Backward Algorithms for finite-
state models, CKY and Earley Algorithms for context-free parsing, and A* Algorithm for
both. These algorithms are widely used to solve problems ranging from sequence labeling
to word alignment to machine translation decoding.
With this overwhelming popularity, this tutorial aims to provide a better understanding
of DP from both theoretical and practical perspectives. In the theory part, we try to unify
various DP algorithms under a generic algebraic framework, where the above mentioned
examples are merely special cases, and we can easily analyze their correctness and complex-
ities. However, exact DP algorithms are often infeasible in practice due to time and space
constraints. So in the practice part, we will survey several widely used tricks to reduce the
size of the search space, including beam search, histogram pruning, coarse-to-fine search,
and cube pruning. We will discuss these methods within the context of state-of-the-art
large-scale NLP systems.
1 Outline
? Part A: Dynamic Programming on Lattices/Graphs under Semiring Framework
? theory:
? Motivations and Examples
? Semirings
? Viterbi Algorithm
? Dijkstra and A* Algorithms
? Comparison between Viterbi and Dijkstra/A* Algorithms
? practice:
? Beam Search and Histogram Pruning; E.g.: Pharaoh (phrase-based MT)
? Part B: Dynamic in Programming on Packed Forests under Hypergraph Framework
? theory:
5
? Hypergraphs; Examples in Parsing and Machine Translation
? Generalized Viterbi Algorithm; CKY Parsing
? Knuth and A* Algorithms
? practice:
? A* in Practice: A* Parsing; beam A*; inadmissible heuristics
? Coarse-to-Fine Search; Example: Charniak and Berkeley parsers
? Cube Pruning; Example: Hiero (syntax-based decoding)
2 Target Audience
This tutorial is intended for researchers with any level of familiarity with dynamic program-
ming. A basic understanding of the CKY Algorithm is recommended, but not required.
3 Brief Bio of the Presenter
Liang Huang is a Research Scientist at Google Research (Mountain View). He recently
obtained his PhD in 2008 from the University of Pennsylvania under Aravind Joshi and
Kevin Knight (USC/ISI). His research interests include algorithms in parsing and transla-
tion, generic dynamic programming, and syntax-based machine translation. His work on
?forest-based algorithms? received an Outstanding Paper Award at ACL 2008, as well as
Best Paper Nominations at ACL 2007 and EMNLP 2008. He also loves teaching and was a
recipient of the University Teaching Prize at Penn.
6
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522?530,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging ? A Case Study
Wenbin Jiang ? Liang Huang ? Qun Liu ?
?Key Lab. of Intelligent Information Processing ?Google Research
Institute of Computing Technology 1350 Charleston Rd.
Chinese Academy of Sciences Mountain View, CA 94043, USA
P.O. Box 2704, Beijing 100190, China lianghuang@google.com
{jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com
Abstract
Manually annotated corpora are valuable
but scarce resources, yet for many anno-
tation tasks such as treebanking and se-
quence labeling there exist multiple cor-
pora with different and incompatible anno-
tation guidelines or standards. This seems
to be a great waste of human efforts, and
it would be nice to automatically adapt
one annotation standard to another. We
present a simple yet effective strategy that
transfers knowledge from a differently an-
notated corpus to the corpus with desired
annotation. We test the efficacy of this
method in the context of Chinese word
segmentation and part-of-speech tagging,
where no segmentation and POS tagging
standards are widely accepted due to the
lack of morphology in Chinese. Experi-
ments show that adaptation from the much
larger People?s Daily corpus to the smaller
but more popular Penn Chinese Treebank
results in significant improvements in both
segmentation and tagging accuracies (with
error reductions of 30.2% and 14%, re-
spectively), which in turn helps improve
Chinese parsing accuracy.
1 Introduction
Much of statistical NLP research relies on some
sort of manually annotated corpora to train their
models, but these resources are extremely expen-
sive to build, especially at a large scale, for ex-
ample in treebanking (Marcus et al, 1993). How-
ever the linguistic theories underlying these anno-
tation efforts are often heavily debated, and as a re-
sult there often exist multiple corpora for the same
task with vastly different and incompatible anno-
tation philosophies. For example just for English
treebanking there have been the Chomskian-style
{1 B2 o3 ?4 ?5 u6
NR NN VV NR
U.S. Vice-President visited China
{1 B2 o3 ?4 ?5 u6
ns b n v
U.S. Vice President visited-China
Figure 1: Incompatible word segmentation and
POS tagging standards between CTB (upper) and
People?s Daily (below).
Penn Treebank (Marcus et al, 1993) the HPSG
LinGo Redwoods Treebank (Oepen et al, 2002),
and a smaller dependency treebank (Buchholz and
Marsi, 2006). A second, related problem is that
the raw texts are also drawn from different do-
mains, which for the above example range from
financial news (PTB/WSJ) to transcribed dialog
(LinGo). These two problems seem be a great
waste in human efforts, and it would be nice if
one could automatically adapt from one annota-
tion standard and/or domain to another in order
to exploit much larger datasets for better train-
ing. The second problem, domain adaptation, is
very well-studied, e.g. by Blitzer et al (2006)
and Daume? III (2007) (and see below for discus-
sions), so in this paper we focus on the less stud-
ied, but equally important problem of annotation-
style adaptation.
We present a very simple yet effective strategy
that enables us to utilize knowledge from a differ-
ently annotated corpora for the training of a model
on a corpus with desired annotation. The basic
idea is very simple: we first train on a source cor-
pus, resulting in a source classifier, which is used
to label the target corpus and results in a ?source-
style? annotation of the target corpus. We then
522
train a second model on the target corpus with the
first classifier?s prediction as additional features
for guided learning.
This method is very similar to some ideas in
domain adaptation (Daume? III and Marcu, 2006;
Daume? III, 2007), but we argue that the underly-
ing problems are quite different. Domain adapta-
tion assumes the labeling guidelines are preserved
between the two domains, e.g., an adjective is al-
ways labeled as JJ regardless of from Wall Street
Journal (WSJ) or Biomedical texts, and only the
distributions are different, e.g., the word ?control?
is most likely a verb in WSJ but often a noun
in Biomedical texts (as in ?control experiment?).
Annotation-style adaptation, however, tackles the
problem where the guideline itself is changed, for
example, one treebank might distinguish between
transitive and intransitive verbs, while merging the
different noun types (NN, NNS, etc.), and for ex-
ample one treebank (PTB) might be much flatter
than the other (LinGo), not to mention the fun-
damental disparities between their underlying lin-
guistic representations (CFG vs. HPSG). In this
sense, the problem we study in this paper seems
much harder and more motivated from a linguistic
(rather than statistical) point of view. More inter-
estingly, our method, without any assumption on
the distributions, can be simultaneously applied to
both domain and annotation standards adaptation
problems, which is very appealing in practice be-
cause the latter problem often implies the former,
as in our case study.
To test the efficacy of our method we choose
Chinese word segmentation and part-of-speech
tagging, where the problem of incompatible an-
notation standards is one of the most evident: so
far no segmentation standard is widely accepted
due to the lack of a clear definition of Chinese
words, and the (almost complete) lack of mor-
phology results in much bigger ambiguities and
heavy debates in tagging philosophies for Chi-
nese parts-of-speech. The two corpora used in
this study are the much larger People?s Daily (PD)
(5.86M words) corpus (Yu et al, 2001) and the
smaller but more popular Penn Chinese Treebank
(CTB) (0.47M words) (Xue et al, 2005). They
used very different segmentation standards as well
as different POS tagsets and tagging guidelines.
For example, in Figure 1, People?s Daily breaks
?Vice-President? into two words while combines
the phrase ?visited-China? as a compound. Also
CTB has four verbal categories (VV for normal
verbs, and VC for copulas, etc.) while PD has only
one verbal tag (v) (Xia, 2000). It is preferable to
transfer knowledge from PD to CTB because the
latter also annotates tree structures which is very
useful for downstream applications like parsing,
summarization, and machine translation, yet it is
much smaller in size. Indeed, many recent efforts
on Chinese-English translation and Chinese pars-
ing use the CTB as the de facto segmentation and
tagging standards, but suffers from the limited size
of training data (Chiang, 2007; Bikel and Chiang,
2000). We believe this is also a reason why state-
of-the-art accuracy for Chinese parsing is much
lower than that of English (CTB is only half the
size of PTB).
Our experiments show that adaptation from PD
to CTB results in a significant improvement in seg-
mentation and POS tagging, with error reductions
of 30.2% and 14%, respectively. In addition, the
improved accuracies from segmentation and tag-
ging also lead to an improved parsing accuracy on
CTB, reducing 38% of the error propagation from
word segmentation to parsing. We envision this
technique to be general and widely applicable to
many other sequence labeling tasks.
In the rest of the paper we first briefly review
the popular classification-based method for word
segmentation and tagging (Section 2), and then
describe our idea of annotation adaptation (Sec-
tion 3). We then discuss other relevant previous
work including co-training and classifier combina-
tion (Section 4) before presenting our experimen-
tal results (Section 5).
2 Segmentation and Tagging as
Character Classification
Before describing the adaptation algorithm, we
give a brief introduction of the baseline character
classification strategy for segmentation, as well as
joint segmenation and tagging (henceforth ?Joint
S&T?). following our previous work (Jiang et al,
2008). Given a Chinese sentence as sequence of n
characters:
C1 C2 .. Cn
where Ci is a character, word segmentation aims
to split the sequence into m(? n) words:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
where each subsequence Ci:j indicates a Chinese
word spanning from characters Ci to Cj (both in-
523
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
clusive). While in Joint S&T, each word is further
annotated with a POS tag:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
where tk(k = 1..m) denotes the POS tag for the
word Cek?1+1:ek .
2.1 Character Classification Method
Xue and Shen (2003) describe for the first time
the character classification approach for Chinese
word segmentation, where each character is given
a boundary tag denoting its relative position in a
word. In Ng and Low (2004), Joint S&T can also
be treated as a character classification problem,
where a boundary tag is combined with a POS tag
in order to give the POS information of the word
containing these characters. In addition, Ng and
Low (2004) find that, compared with POS tagging
after word segmentation, Joint S&T can achieve
higher accuracy on both segmentation and POS
tagging. This paper adopts the tag representation
of Ng and Low (2004). For word segmentation
only, there are four boundary tags:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
while for Joint S&T, a POS tag is attached to the
tail of a boundary tag, to incorporate the word
boundary information and POS information to-
gether. For example, b-NN indicates that the char-
acter is the begin of a noun. After all charac-
ters of a sentence are assigned boundary tags (or
with POS postfix) by a classifier, the correspond-
ing word sequence (or with POS) can be directly
derived. Take segmentation for example, a char-
acter assigned a tag s or a subsequence of words
assigned a tag sequence bm?e indicates a word.
2.2 Training Algorithm and Features
Now we will show the training algorithm of the
classifier and the features used. Several classi-
fication models can be adopted here, however,
we choose the averaged perceptron algorithm
(Collins, 2002) because of its simplicity and high
accuracy. It is an online training algorithm and
has been successfully used in many NLP tasks,
such as POS tagging (Collins, 2002), parsing
(Collins and Roark, 2004), Chinese word segmen-
tation (Zhang and Clark, 2007; Jiang et al, 2008),
and so on.
Similar to the situation in other sequence label-
ing problems, the training procedure is to learn a
discriminative model mapping from inputs x ? X
to outputs y ? Y , where X is the set of sentences
in the training corpus and Y is the set of corre-
sponding labelled results. Following Collins, we
use a function GEN(x) enumerating the candi-
date results of an input x , a representation?map-
ping each training example (x, y) ? X ? Y to a
feature vector?(x, y) ? Rd, and a parameter vec-
tor ~? ? Rd corresponding to the feature vector.
For an input character sequence x, we aim to find
an output F (x) that satisfies:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
where?(x, y) ?~? denotes the inner product of fea-
ture vector ?(x, y) and the parameter vector ~?.
Algorithm 1 depicts the pseudo code to tune the
parameter vector ~?. In addition, the ?averaged pa-
rameters? technology (Collins, 2002) is used to al-
leviate overfitting and achieve stable performance.
Table 1 lists the feature template and correspond-
ing instances. Following Ng and Low (2004),
the current considering character is denoted as C0,
while the ith character to the left of C0 as C?i,
and to the right as Ci. There are additional two
functions of which each returns some property of a
character. Pu(?) is a boolean function that checks
whether a character is a punctuation symbol (re-
turns 1 for a punctuation, 0 for not). T (?) is a
multi-valued function, it classifies a character into
four classifications: number, date, English letter
and others (returns 1, 2, 3 and 4, respectively).
3 Automatic Annotation Adaptation
From this section, several shortened forms are
adopted for representation inconvenience. We use
source corpus to denote the corpus with the anno-
tation standard that we don?t require, which is of
524
Feature Template Instances
Ci (i = ?2..2) C?2 =?, C?1 =, C0 =c, C1 =?, C2 = R
CiCi+1 (i = ?2..1) C?2C?1 =?, C?1C0 =c, C0C1 =c?, C1C2 =?R
C?1C1 C?1C1 =?
Pu(C0) Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) T (C?2)T (C?1)T (C0)T (C1)T (C2) = 11243
Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004). Suppose we are
considering the third character ?c? in ?? c ?R?.
course the source of the adaptation, while target
corpus denoting the corpus with the desired stan-
dard. And correspondingly, the two annotation
standards are naturally denoted as source standard
and target standard, while the classifiers follow-
ing the two annotation standards are respectively
named as source classifier and target classifier, if
needed.
Considering that word segmentation and Joint
S&T can be conducted in the same character clas-
sification manner, we can design an unified stan-
dard adaptation framework for the two tasks, by
taking the source classifier?s classification result
as the guide information for the target classifier?s
classification decision. The following section de-
picts this adaptation strategy in detail.
3.1 General Adaptation Strategy
In detail, in order to adapt knowledge from the
source corpus, first, a source classifier is trained
on it and therefore captures the knowledge it con-
tains; then, the source classifier is used to clas-
sify the characters in the target corpus, although
the classification result follows a standard that we
don?t desire; finally, a target classifier is trained
on the target corpus, with the source classifier?s
classification result as additional guide informa-
tion. The training procedure of the target clas-
sifier automatically learns the regularity to trans-
fer the source classifier?s predication result from
source standard to target standard. This regular-
ity is incorporated together with the knowledge
learnt from the target corpus itself, so as to ob-
tain enhanced predication accuracy. For a given
un-classified character sequence, the decoding is
analogous to the training. First, the character se-
quence is input into the source classifier to ob-
tain an source standard annotated classification
result, then it is input into the target classifier
with this classification result as additional infor-
mation to get the final result. This coincides with
the stacking method for combining dependency
parsers (Martins et al, 2008; Nivre and McDon-
source corpus
train with
normal features
source classifier
train with
additional features
target classifier
target corpus source annotation
classification result
Figure 2: The pipeline for training.
raw sentence source classifier source annotation
classification result
target classifier
target annotation
classification result
Figure 3: The pipeline for decoding.
ald, 2008), and is also similar to the Pred baseline
for domain adaptation in (Daume? III and Marcu,
2006; Daume? III, 2007). Figures 2 and 3 show
the flow charts for training and decoding.
The utilization of the source classifier?s classi-
fication result as additional guide information re-
sorts to the introduction of new features. For the
current considering character waiting for classi-
fication, the most intuitive guide features is the
source classifier?s classification result itself. How-
ever, our effort isn?t limited to this, and more spe-
cial features are introduced: the source classifier?s
classification result is attached to every feature
listed in Table 1 to get combined guide features.
This is similar to feature design in discriminative
dependency parsing (McDonald et al, 2005; Mc-
525
Donald and Pereira, 2006), where the basic fea-
tures, composed of words and POSs in the context,
are also conjoined with link direction and distance
in order to obtain more special features. Table 2
shows an example of guide features and basic fea-
tures, where ?? = b ? represents that the source
classifier classifies the current character as b, the
beginning of a word.
Such combination method derives a series of
specific features, which helps the target classifier
to make more precise classifications. The parame-
ter tuning procedure of the target classifier will au-
tomatically learn the regularity of using the source
classifier?s classification result to guide its deci-
sion making. For example, if a current consid-
ering character shares some basic features in Ta-
ble 2 and it is classified as b, then the target clas-
sifier will probably classify it as m. In addition,
the training procedure of the target classifier also
learns the relative weights between the guide fea-
tures and the basic features, so that the knowledge
from both the source corpus and the target corpus
are automatically integrated together.
In fact, more complicated features can be
adopted as guide information. For error tolerance,
guide features can be extracted from n-best re-
sults or compacted lattices of the source classifier;
while for the best use of the source classifier?s out-
put, guide features can also be the classification
results of several successive characters. We leave
them as future research.
4 Related Works
Co-training (Sarkar, 2001) and classifier com-
bination (Nivre and McDonald, 2008) are two
technologies for training improved dependency
parsers. The co-training technology lets two dif-
ferent parsing models learn from each other dur-
ing parsing an unlabelled corpus: one model
selects some unlabelled sentences it can confi-
dently parse, and provide them to the other model
as additional training corpus in order to train
more powerful parsers. The classifier combina-
tion lets graph-based and transition-based depen-
dency parsers to utilize the features extracted from
each other?s parsing results, to obtain combined,
enhanced parsers. The two technologies aim to
let two models learn from each other on the same
corpora with the same distribution and annota-
tion standard, while our strategy aims to integrate
the knowledge in multiple corpora with different
Baseline Features
C?2 ={
C?1 =B
C0 =o
C1 =?
C2 =?
C?2C?1 ={B
C?1C0 =Bo
C0C1 =o?
C1C2 =??
C?1C1 =B?
Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444
Guide Features
? = b
C?2 ={ ? ? = b
C?1 =B ? ? = b
C0 =o ? ? = b
C1 =? ? ? = b
C2 =? ? ? = b
C?2C?1 ={B ? ? = b
C?1C0 =Bo ? ? = b
C0C1 =o? ? ? = b
C1C2 =?? ? ? = b
C?1C1 =B? ? ? = b
Pu(C0) = 0 ? ? = b
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444 ? ? = b
Table 2: An example of basic features and guide
features of standard-adaptation for word segmen-
tation. Suppose we are considering the third char-
acter ?o? in ?{B o ??u?.
annotation-styles.
Gao et al (2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
They design some class-type transformation tem-
plates and use the transformation-based error-
driven learning method of Brill (1995) to learn
what word delimiters should be modified. How-
ever, this converter need human designed transfor-
mation templates, and is hard to be generalized to
POS tagging, not to mention other structure label-
ing tasks. Moreover, the processing procedure is
divided into two isolated steps, conversion after
segmentation, which suffers from error propaga-
tion and wastes the knowledge in the corpora. On
the contrary, our strategy is automatic, generaliz-
able and effective.
In addition, many efforts have been devoted
to manual treebank adaptation, where they adapt
PTB to other grammar formalisms, such as such
as CCG and LFG (Hockenmaier and Steedman,
2008; Cahill and Mccarthy, 2007). However, they
are heuristics-based and involve heavy human en-
gineering.
526
5 Experiments
Our adaptation experiments are conducted from
People?s Daily (PD) to Penn Chinese Treebank 5.0
(CTB). These two corpora are segmented follow-
ing different segmentation standards and labeled
with different POS sets (see for example Figure 1).
PD is much bigger in size, with about 100K sen-
tences, while CTB is much smaller, with only
about 18K sentences. Thus a classifier trained on
CTB usually falls behind that trained on PD, but
CTB is preferable because it also annotates tree
structures, which is very useful for downstream
applications like parsing and translation. For ex-
ample, currently, most Chinese constituency and
dependency parsers are trained on some version
of CTB, using its segmentation and POS tagging
as the de facto standards. Therefore, we expect the
knowledge adapted from PD will lead to more pre-
cise CTB-style segmenter and POS tagger, which
would in turn reduce the error propagation to pars-
ing (and translation).
Experiments adapting from PD to CTB are con-
ducted for two tasks: word segmentation alone,
and joint segmentation and POS tagging (Joint
S&T). The performance measurement indicators
for word segmentation and Joint S&T are bal-
anced F-measure, F = 2PR/(P +R), a function
of Precision P and Recall R. For word segmen-
tation, P indicates the percentage of words in seg-
mentation result that are segmented correctly, and
R indicates the percentage of correctly segmented
words in gold standard words. For Joint S&T, P
and R mean nearly the same except that a word
is correctly segmented only if its POS is also cor-
rectly labelled.
5.1 Baseline Perceptron Classifier
We first report experimental results of the single
perceptron classifier on CTB 5.0. The original
corpus is split according to former works: chap-
ters 271 ? 300 for testing, chapters 301 ? 325 for
development, and others for training. Figure 4
shows the learning curves for segmentation only
and Joint S&T, we find all curves tend to moder-
ate after 7 iterations. The data splitting conven-
tion of other two corpora, People?s Daily doesn?t
reserve the development sets, so in the following
experiments, we simply choose the model after 7
iterations when training on this corpus.
The first 3 rows in each sub-table of Table 3
show the performance of the single perceptron
0.880
0.890
0.900
0.910
0.920
0.930
0.940
0.950
0.960
0.970
0.980
 1  2  3  4  5  6  7  8  9  10
F 
m
ea
su
re
number of iterations
segmentation only
segmentation in Joint S&T
Joint S&T
Figure 4: Averaged perceptron learning curves for
segmentation and Joint S&T.
Train on Test on Seg F1% JST F1%
Word Segmentation
PD PD 97.45 ?
PD CTB 91.71 ?
CTB CTB 97.35 ?
PD ? CTB CTB 98.15 ?
Joint S&T
PD PD 97.57 94.54
PD CTB 91.68 ?
CTB CTB 97.58 93.06
PD ? CTB CTB 98.23 94.03
Table 3: Experimental results for both baseline
models and final systems with annotation adap-
tation. PD ? CTB means annotation adaptation
from PD to CTB. For the upper sub-table, items of
JST F1 are undefined since only segmentation is
performs. While in the sub-table below, JST F1
is also undefined since the model trained on PD
gives a POS set different from that of CTB.
models. Comparing row 1 and 3 in the sub-table
below with the corresponding rows in the upper
sub-table, we validate that when word segmenta-
tion and POS tagging are conducted jointly, the
performance for segmentation improves since the
POS tags provide additional information to word
segmentation (Ng and Low, 2004). We also see
that for both segmentation and Joint S&T, the per-
formance sharply declines when a model trained
on PD is tested on CTB (row 2 in each sub-table).
In each task, only about 92% F1 is achieved. This
obviously fall behind those of the models trained
on CTB itself (row 3 in each sub-table), about 97%
F1, which are used as the baselines of the follow-
ing annotation adaptation experiments.
527
POS #Word #BaseErr #AdaErr ErrDec%
AD 305 30 19 36.67 ?
AS 76 0 0
BA 4 1 1
CC 135 8 8
CD 356 21 14 33.33 ?
CS 6 0 0
DEC 137 31 23 25.81 ?
DEG 197 32 37 ?
DEV 10 0 0
DT 94 3 1 66.67 ?
ETC 12 0 0
FW 1 1 1
JJ 127 41 44 ?
LB 2 1 1
LC 106 3 2 33.33 ?
M 349 18 4 77.78 ?
MSP 8 2 1 50.00 ?
NN 1715 151 126 16.56 ?
NR 713 59 50 15.25 ?
NT 178 1 2 ?
OD 84 0 0
P 251 10 6 40.00 ?
PN 81 1 1
PU 997 0 1 ?
SB 2 0 0
SP 2 2 2
VA 98 23 21 08.70 ?
VC 61 0 0
VE 25 1 0 100.00 ?
VV 689 64 40 37.50 ?
SUM 6821 213 169 20.66 ?
Table 4: Error analysis for Joint S&T on the devel-
oping set of CTB. #BaseErr and #AdaErr denote
the count of words that can?t be recalled by the
baseline model and adapted model, respectively.
ErrDec denotes the error reduction of Recall.
5.2 Adaptation for Segmentation and
Tagging
Table 3 also lists the results of annotation adap-
tation experiments. For word segmentation, the
model after annotation adaptation (row 4 in upper
sub-table) achieves an F-measure increment of 0.8
points over the baseline model, corresponding to
an error reduction of 30.2%; while for Joint S&T,
the F-measure increment of the adapted model
(row 4 in sub-table below) is 1 point, which cor-
responds to an error reduction of 14%. In addi-
tion, the performance of the adapted model for
Joint S&T obviously surpass that of (Jiang et al,
2008), which achieves an F1 of 93.41% for Joint
S&T, although with more complicated models and
features.
Due to the obvious improvement brought by an-
notation adaptation to both word segmentation and
Joint S&T, we can safely conclude that the knowl-
edge can be effectively transferred from on an-
Input Type Parsing F1%
gold-standard segmentation 82.35
baseline segmentation 80.28
adapted segmentation 81.07
Table 5: Chinese parsing results with different
word segmentation results as input.
notation standard to another, although using such
a simple strategy. To obtain further information
about what kind of errors be alleviated by annota-
tion adaptation, we conduct an initial error analy-
sis for Joint S&T on the developing set of CTB. It
is reasonable to investigate the error reduction of
Recall for each word cluster grouped together ac-
cording to their POS tags. From Table 4 we find
that out of 30 word clusters appeared in the devel-
oping set of CTB, 13 clusters benefit from the an-
notation adaptation strategy, while 4 clusters suf-
fer from it. However, the compositive error rate of
Recall for all word clusters is reduced by 20.66%,
such a fact invalidates the effectivity of annotation
adaptation.
5.3 Contribution to Chinese Parsing
We adopt the Chinese parser of Xiong et al
(2005), and train it on the training set of CTB 5.0
as described before. To sketch the error propaga-
tion to parsing from word segmentation, we rede-
fine the constituent span as a constituent subtree
from a start character to a end character, rather
than from a start word to a end word. Note that if
we input the gold-standard segmented test set into
the parser, the F-measure under the two definitions
are the same.
Table 5 shows the parsing accuracies with dif-
ferent word segmentation results as the parser?s
input. The parsing F-measure corresponding to
the gold-standard segmentation, 82.35, represents
the ?oracle? accuracy (i.e., upperbound) of pars-
ing on top of automatic word segmention. After
integrating the knowledge from PD, the enhanced
word segmenter gains an F-measure increment of
0.8 points, which indicates that 38% of the error
propagation from word segmentation to parsing is
reduced by our annotation adaptation strategy.
6 Conclusion and Future Works
This paper presents an automatic annotation adap-
tation strategy, and conducts experiments on a
classic problem: word segmentation and Joint
528
S&T. To adapt knowledge from a corpus with an
annotation standard that we don?t require, a clas-
sifier trained on this corpus is used to pre-process
the corpus with the desired annotated standard, on
which a second classifier is trained with the first
classifier?s predication results as additional guide
information. Experiments of annotation adapta-
tion from PD to CTB 5.0 for word segmentation
and POS tagging show that, this strategy can make
effective use of the knowledge from the corpus
with different annotations. It obtains considerable
F-measure increment, about 0.8 point for word
segmentation and 1 point for Joint S&T, with cor-
responding error reductions of 30.2% and 14%.
The final result outperforms the latest work on the
same corpus which uses more complicated tech-
nologies, and achieves the state-of-the-art. More-
over, such improvement further brings striking F-
measure increment for Chinese parsing, about 0.8
points, corresponding to an error propagation re-
duction of 38%.
In the future, we will continue to research on
annotation adaptation for other NLP tasks which
have different annotation-style corpora. Espe-
cially, we will pay efforts to the annotation stan-
dard adaptation between different treebanks, for
example, from HPSG LinGo Redwoods Treebank
to PTB, or even from a dependency treebank
to PTB, in order to obtain more powerful PTB
annotation-style parsers.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are especially grateful to
Fernando Pereira and the anonymous reviewers
for pointing us to relevant domain adaption refer-
ences. We also thank Yang Liu and Haitao Mi for
helpful discussions.
References
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. In Computational
Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Aoife Cahill and Mairead Mccarthy. 2007. Auto-
matic annotation of the penn treebank with lfg f-
structure information. In in Proceedings of the
LREC Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated
Language Data.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201?228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42th Annual Meeting of the Association
for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artifi-
cial Intelligence Research.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Julia Hockenmaier and Mark Steedman. 2008. Ccg-
bank: a corpus of ccg derivations and dependency
structures extracted from the penn treebank. In
Computational Linguistics, volume 33(3), pages
355?396.
Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
529
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics (COLING 2002).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0). In Technical
Reports.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
530
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206?214,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Forest-based Translation Rule Extraction
Haitao Mi1
1Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
htmi@ict.ac.cn
Liang Huang2,1
2Dept. of Computer & Information Science
University of Pennsylvania
3330 Walnut St., Levine Hall
Philadelphia, PA 19104, USA
lhuang3@cis.upenn.edu
Abstract
Translation rule extraction is a fundamental
problem in machine translation, especially for
linguistically syntax-based systems that need
parse trees from either or both sides of the bi-
text. The current dominant practice only uses
1-best trees, which adversely affects the rule
set quality due to parsing errors. So we pro-
pose a novel approach which extracts rules
from a packed forest that compactly encodes
exponentially many parses. Experiments show
that this method improves translation quality
by over 1 BLEU point on a state-of-the-art
tree-to-string system, and is 0.5 points better
than (and twice as fast as) extracting on 30-
best parses. When combined with our previous
work on forest-based decoding, it achieves a
2.5 BLEU points improvement over the base-
line, and even outperforms the hierarchical
system of Hiero by 0.7 points.
1 Introduction
Automatic extraction of translation rules is a funda-
mental problem in statistical machine translation, es-
pecially for many syntax-based models where trans-
lation rules directly encode linguistic knowledge.
Typically, these models extract rules using parse
trees from both or either side(s) of the bitext. The
former case, with trees on both sides, is often called
tree-to-tree models; while the latter case, with trees
on either source or target side, include both tree-
to-string and string-to-tree models (see Table 1).
Leveraging from structural and linguistic informa-
tion from parse trees, these models are believed
to be better than their phrase-based counterparts in
source target examples (partial)
tree-to-tree Ding and Palmer (2005)
tree-to-string Liu et al (2006); Huang et al (2006)
string-to-tree Galley et al (2006)
string-to-string Chiang (2005)
Table 1: A classification of syntax-based MT. The first
three use linguistic syntax, while the last one only formal
syntax. Our experiments cover the second type using a
packed forest in place of the tree for rule-extraction.
handling non-local reorderings, and have achieved
promising translation results.1
However, these systems suffer from a major limi-
tation, that the rule extractor only uses 1-best parse
tree(s), which adversely affects the rule set quality
due to parsing errors. To make things worse, mod-
ern statistical parsers are often trained on domains
quite different from those used in MT. By contrast,
formally syntax-based models (Chiang, 2005) do not
rely on parse trees, yet usually perform better than
these linguistically sophisticated counterparts.
To alleviate this problem, an obvious idea is to
extract rules from k-best parses instead. However, a
k-best list, with its limited scope, has too few vari-
ations and too many redundancies (Huang, 2008).
This situation worsens with longer sentences as the
number of possible parses grows exponentially with
the sentence length and a k-best list will only capture
a tiny fraction of the whole space. In addition, many
subtrees are repeated across different parses, so it is
1For example, in recent NIST Evaluations, some of these
models (Galley et al, 2006; Quirk et al, 2005; Liu et al, 2006)
ranked among top 10. See http://www.nist.gov/speech/tests/mt/.
206
IP
NP
x1:NPB CC
yu?
x2:NPB
x3:VPB ? x1 x3 with x2
Figure 1: Example translation rule r1. The Chinese con-
junction yu? ?and? is translated into English prep. ?with?.
also inefficient to extract rules separately from each
of these very similar trees (or from the cross-product
of k2 similar tree-pairs in tree-to-tree models).
We instead propose a novel approach that ex-
tracts rules from packed forests (Section 3), which
compactly encodes many more alternatives than k-
best lists. Experiments (Section 5) show that forest-
based extraction improves BLEU score by over 1
point on a state-of-the-art tree-to-string system (Liu
et al, 2006; Mi et al, 2008), which is also 0.5
points better than (and twice as fast as) extracting
on 30-best parses. When combined with our previ-
ous orthogonal work on forest-based decoding (Mi
et al, 2008), the forest-forest approach achieves a
2.5 BLEU points improvement over the baseline,
and even outperforms the hierarchical system of Hi-
ero, one of the best-performing systems to date.
Besides tree-to-string systems, our method is also
applicable to other paradigms such as the string-to-
tree models (Galley et al, 2006) where the rules are
in the reverse order, and easily generalizable to pairs
of forests in tree-to-tree models.
2 Tree-based Translation
We review in this section the tree-based approach to
machine translation (Liu et al, 2006; Huang et al,
2006), and its rule extraction algorithm (Galley et
al., 2004; Galley et al, 2006).
2.1 Tree-to-String System
Current tree-based systems perform translation in
two separate steps: parsing and decoding. The input
string is first parsed by a parser into a 1-best tree,
which will then be converted to a target language
string by applying a set of tree-to-string transforma-
tion rules. For example, consider the following ex-
ample translating from Chinese to English:
(a) Bu`sh?? yu? Sha?lo?ng ju?x??ng le hu?`ta?n
? 1-best parser(b) IP
NP
NPB
Bu`sh??
CC
yu?
NPB
Sha?lo?ng
VPB
VV
ju?x??ng
AS
le
NPB
hu?`ta?n
r1?
(c) NPB
Bu`sh??
VPB
VV
ju?x??ng
AS
le
NPB
hu?`ta?n
with NPB
Sha?lo?ng
r2 ? r3 ?
(d) Bush held NPB
hu?`ta?n
with NPB
Sha?lo?ng
r4 ? r5 ?
(e) Bush held a meeting with Sharon
r2 NPB(Bu`sh??)? Bush
r3 VPB(VV(ju?x??ng) AS(le) x1:NPB)? held x1
r4 NPB(Sha?lo?ng)? Sharon
r5 NPB(hu?`ta?n)? a meeting
Figure 2: Example derivation of tree-to-string translation,
with rules used. Each shaded region denotes a tree frag-
ment that is pattern-matched with the rule being applied.
(1) Bu`sh??
Bush
yu?
and/with
Sha?lo?ng
Sharon1
ju?x??ng
hold
le
past.
hu?`ta?n
meeting2
?Bush held a meeting2 with Sharon1?
Figure 2 shows how this process works. The Chi-
nese sentence (a) is first parsed into a parse tree (b),
which will be converted into an English string in 5
steps. First, at the root node, we apply rule r1 shown
in Figure 1, which translates the Chinese coordina-
tion construction (?... and ...?) into an English prepo-
sitional phrase. Then, from step (c) we continue ap-
plying rules to untranslated Chinese subtrees, until
we get the complete English translation in (e).2
2We swap the 1-best and 2-best parses of the example sen-
tence from our earlier paper (Mi et al, 2008), since the current
1-best parse is easier to illustrate the rule extraction algorithm.
207
IP
?Bush .. Sharon?
NP
?Bush ? with Sharon?
NPB
?Bush?
Bu`sh??
CC
?with?
yu?
NPB
?Sharon?
Sha?lo?ng
VPB
?held .. meeting?
VV
?held?
ju?x??ng
AS
?held?
le
NPB
?a meeting?
hu?`ta?n
(minimal) rules extracted
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? x1 x4 x2 x3
CC (yu?)? with
NPB (Bu`sh??)? Bush
NPB (Sha?lo?ng)? Sharon
VPB (VV(ju?x??ng) AS(le) x1:NPB)
? held x1
NPB (hu?`ta?n)? a meeting
Bush held a meeting with Sharon
Figure 3: Tree-based rule extraction (Galley et al, 2004). Each non-leaf node in the tree is annotated with its target
span (below the node), where ? denotes a gap, and non-faithful spans are crossed out. Shadowed nodes are admissible,
with contiguous and faithful spans. The first two rules can be ?composed? to form rule r1 in Figure 1.
IP0, 6
?Bush .. Sharon?
e2
NP0, 3
?Bush ? with Sharon?
e3
NPB0, 1
?Bush?
Bu`sh??
CC1, 2
?with?
yu?
VP1, 6
?held .. Sharon?
PP1, 3
?with Sharon?
P1, 2
?with?
NPB2, 3
?Sharon?
Sha?lo?ng
VPB3, 6
?held .. meeting?
VV3, 4
?held?
ju?x??ng
AS4, 5
?held?
le
NPB5, 6
?a meeting?
hu?`ta?n
e1
extra (minimal) rules extracted
IP (x1:NPB x2:VP)? x1 x2
VP (x1:PP x2:VPB)? x2 x1
PP (x1:P x2:NPB)? x1 x2
P (yu?)? with
Bush held a meeting with Sharon
Figure 4: Forest-based rule extraction. Solid hyperedges correspond to the 1-best tree in Figure 3, while dashed hyper-
edges denote the alternative parse interpreting yu? as a preposition in Figure 5.
More formally, a (tree-to-string) translation rule
(Galley et al, 2004; Huang et al, 2006) is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the source-
side tree fragment, whose internal nodes are la-
beled by nonterminal symbols (like NP and VP),
and whose frontier nodes are labeled by source-
language words (like ?yu??) or variables from a set
X = {x1, x2, . . .}; rhs(r) is the target-side string
expressed in target-language words (like ?with?) and
variables; and ?(r) is a mapping from X to nonter-
minals. Each variable xi ? X occurs exactly once in
lhs(r) and exactly once in rhs(r). For example, for
rule r1 in Figure 1,
lhs(r1) = IP ( NP(x1 CC(yu?) x2) x3),
rhs(r1) = x1 x3 with x2,
?(r1) = {x1: NPB, x2: NPB, x3: VPB}.
These rules are being used in the reverse direction of
the string-to-tree transducers in Galley et al (2004).
208
2.2 Tree-to-String Rule Extraction
We now briefly explain the algorithm of Galley et al
(2004) that can extract these translation rules from a
word-aligned bitext with source-side parses.
Consider the example in Figure 3. The basic idea
is to decompose the source (Chinese) parse into a se-
ries of tree fragments, each of which will form a rule
with its corresponding English translation. However,
not every fragmentation can be used for rule extrac-
tion, since it may or may not respect the alignment
and reordering between the two languages. So we
say a fragmentation is well-formed with respect to
an alignment if the root node of every tree fragment
corresponds to a contiguous span on the target side;
the intuition is that there is a ?translational equiva-
lence? between the subtree rooted at the node and
the corresponding target span. For example, in Fig-
ure 3, each node is annotated with its corresponding
English span, where the NP node maps to a non-
contiguous one ?Bush ? with Sharon?.
More formally, we need a precise formulation
to handle the cases of one-to-many, many-to-one,
and many-to-many alignment links. Given a source-
target sentence pair (?, ?) with alignment a, the (tar-
get) span of node v is the set of target words aligned
to leaf nodes yield(v) under node v:
span(v) , {?i ? ? | ??j ? yield(v), (?j , ?i) ? a}.
For example, in Figure 3, every node in the parse tree
is annotated with its corresponding span below the
node, where most nodes have contiguous spans ex-
cept for the NP node which maps to a gapped phrase
?Bush ? with Sharon?. But contiguity alone is not
enough to ensure well-formedness, since there might
be words within the span aligned to source words
uncovered by the node. So we also define a span s
to be faithful to node v if every word in it is only
aligned to nodes dominated by v, i.e.:
??i ? s, (?j , ?i) ? a? ?j ? yield(v).
For example, sibling nodes VV and AS in the tree
have non-faithful spans (crossed out in the Figure),
because they both map to ?held?, thus neither of
them can be translated to ?held? alone. In this case,
a larger tree fragment rooted at VPB has to be
extracted. Nodes with non-empty, contiguous, and
faithful spans form the admissible set (shaded nodes
IP0,6
NPB0,1
Bu`sh??
VP1,6
PP1,3
P1,2
yu?
NPB2,3
Sha?lo?ng
VPB3,6
ju?x??ng le hu?`ta?n
Figure 5: An alternative parse of the Chinese sentence,
with yu? as a preposition instead of a conjunction; com-
mon parts shared with 1-best parse in Fig. 3 are elided.
in the figure), which serve as potential cut-points for
rule extraction.3
With the admissible set computed, rule extraction
is as simple as a depth-first traversal from the root:
we ?cut? the tree at all admissible nodes to form tree
fragments and extract a rule for each fragment, with
variables matching the admissible descendant nodes.
For example, the tree in Figure 3 is cut into 6 pieces,
each of which corresponds to a rule on the right.
These extracted rules are called minimal rules,
which can be glued together to form composed rules
with larger tree fragments (e.g. r1 in Fig. 1) (Galley
et al, 2006). Our experiments use composed rules.
3 Forest-based Rule Extraction
We now extend tree-based extraction algorithm from
the previous section to work with a packed forest
representing exponentially many parse trees.
3.1 Packed Forest
Informally, a packed parse forest, or forest in
short, is a compact representation of all the deriva-
tions (i.e., parse trees) for a given sentence under
a context-free grammar (Earley, 1970; Billot and
Lang, 1989). For example, consider again the Chi-
nese sentence in Example (1) above, which has
(at least) two readings depending on the part-of-
speech of the word yu?: it can be either a conjunction
(CC ?and?) as shown in Figure 3, or a preposition
(P ?with?) as shown in Figure 5, with only PP and
VPB swapped from the English word order.
3Admissible set (Wang et al, 2007) is also known as ?fron-
tier set? (Galley et al, 2004). For simplicity of presentation, we
assume every target word is aligned to at least one source word;
see Galley et al (2006) for handling unaligned target words.
209
These two parse trees can be represented as a
single forest by sharing common subtrees such as
NPB0, 1 and VPB3, 6, as shown in Figure 4. Such a
forest has a structure of a hypergraph (Huang and
Chiang, 2005), where items like NP0, 3 are called
nodes, whose indices denote the source span, and
combinations like
e1 : IP0, 6 ? NPB0, 3 VP3, 6
we call hyperedges. We denote head(e) and tails(e)
to be the consequent and antecedant items of hyper-
edge e, respectively. For example,
head(e1) = IP0, 6, tails(e1) = {NPB0, 3,VP3, 6}.
We also denote BS (v) to be the set of incoming hy-
peredges of node v, being different ways of deriving
it. For example, in Figure 4, BS (IP0, 6) = {e1, e2}.
3.2 Forest-based Rule Extraction Algorithm
Like in tree-based extraction, we extract rules from
a packed forest F in two steps:
(1) admissible set computation (where to cut), and
(2) fragmentation (how to cut).
It turns out that the exact formulation developed
for admissible set in the tree-based case can be ap-
plied to a forest without any change. The fragmen-
tation step, however, becomes much more involved
since we now face a choice of multiple parse hyper-
edges at each node. In other words, it becomes non-
deterministic how to ?cut? a forest into tree frag-
ments, which is analogous to the non-deterministic
pattern-match in forest-based decoding (Mi et al,
2008). For example there are two parse hyperedges
e1 and e2 at the root node in Figure 4. When we fol-
low one of them to grow a fragment, there again will
be multiple choices at each of its tail nodes. Like in
tree-based case, a fragment is said to be complete
if all its leaf nodes are admissible. Otherwise, an in-
complete fragment can grow at any non-admissible
frontier node v, where following each parse hyper-
edge at v will split off a new fragment. For example,
following e2 at the root node will immediately lead
us to two admissible nodes, NPB0, 1 and VP1, 6
(we will highlight admissible nodes by gray shades
Algorithm 1 Forest-based Rule Extraction.
Input: forest F , target sentence ? , and alignment a
Output: minimal rule setR
1: admset ? ADMISSIBLE(F, ?, a) ? admissible set
2: for each v ? admset do
3: open ? ? ? queue of active fragments
4: for each e ? BS (v) do ? incoming hyperedges
5: front ? tails(e) \ admset ? initial frontier
6: open .append(?{e}, front?)
7: while open 6= ? do
8: ?frag , front? ? open .pop() ? active fragment
9: if front = ? then
10: generate a rule r using fragment frag
11: R.append(r)
12: else ? incomplete: further expand
13: u? front .pop() ? a frontier node
14: for each e ? BS (u) do
15: front ? ? front ? (tails(e) \ admset)
16: open .append(?frag ? {e}, front ??)
in this section like in Figures 3 and 4). So this frag-
ment, frag1 = {e2}, is now complete and we can
extract a rule,
IP (x1:NPB x2:VP)? x1 x2.
However, following the other hyperedge e1
IP0, 6 ? NP0, 3 VPB3, 6
will leave the new fragment frag2 = {e1} incom-
plete with one non-admissible node NP0, 3. We then
grow frag2 at this node by choosing hyperedge e3
NP0, 3 ? NPB0, 1 CC1, 2 NPB2, 3 ,
and spin off a new fragment frag3 = {e1, e3}, which
is now complete since all its four leaf nodes are ad-
missible. We then extract a rule with four variables:
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? x1 x4 x2 x3.
This procedure is formalized by a breadth-first
search (BFS) in Pseudocode 1. The basic idea is to
visit each frontier node v, and keep a queue open
of actively growing fragments rooted at v. We keep
expanding incomplete fragments from open , and ex-
tract a rule if a complete fragment is found (line 10).
Each fragment is associated with a frontier (variable
210
front in the Pseudocode), being the subset of non-
admissible leaf nodes (recall that expansion stops at
admissible nodes). So each initial fragment along
hyperedge e is associated with an initial frontier
(line 5), front = tails(e) \ admset .
A fragment is complete if its frontier is empty
(line 9), otherwise we pop one frontier node u to
expand, spin off new fragments by following hyper-
edges of u, and update the frontier (lines 14-16), un-
til all active fragments are complete and open queue
is empty (line 7).
A single parse tree can also be viewed as a triv-
ial forest, where each node has only one incoming
hyperedge. So the Galley et al (2004) algorithm for
tree-based rule extraction (Sec. 2.2) can be consid-
ered a special case of our algorithm, where the queue
open always contains one single active fragment.
3.3 Fractional Counts and Rule Probabilities
In tree-based extraction, for each sentence pair, each
rule extracted naturally has a count of one, which
will be used in maximum-likelihood estimation of
rule probabilities. However, a forest is an implicit
collection of many more trees, each of which, when
enumerated, has its own probability accumulated
from of the parse hyperedges involved. In other
words, a forest can be viewed as a virtual weighted
k-best list with a huge k. So a rule extracted from a
non 1-best parse, i.e., using non 1-best hyperedges,
should be penalized accordingly and should have a
fractional count instead of a unit one, similar to the
E-step in EM algorithms.
Inspired by the parsing literature on pruning
(Charniak and Johnson, 2005; Huang, 2008) we pe-
nalize a rule r by the posterior probability of its tree
fragment frag = lhs(r). This posterior probability,
notated ??(frag), can be computed in an Inside-
Outside fashion as the product of three parts: the out-
side probability of its root node, the probabilities of
parse hyperedges involved in the fragment, and the
inside probabilities of its leaf nodes,
??(frag) =?(root(frag))
?
?
e ? frag
P(e)
?
?
v ? yield(frag)
?(v)
(2)
where ?(?) and ?(?) denote the outside and inside
probabilities of tree nodes, respectively. For example
in Figure 4,
??({e2, e3}) = ?(IP0, 6) ? P(e2) ? P(e3)
? ?(NPB0, 1)?(CC1, 2)?(NPB2, 3)?(VPB3, 6).
Now the fractional count of rule r is simply
c(r) = ??(lhs(r))??(TOP) (3)
where TOP denotes the root node of the forest.
Like in the M-step in EM algorithm, we now
extend the maximum likelihood estimation to frac-
tional counts for three conditional probabilities re-
garding a rule, which will be used in the experi-
ments:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
, (4)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
, (5)
P(r |root(lhs(r)))
=
c(r)
?
r?:root(lhs(r?))=root(lhs(r)) c(r?)
. (6)
4 Related Work
The concept of packed forest has been previously
used in translation rule extraction, for example in
rule composition (Galley et al, 2006) and tree bina-
rization (Wang et al, 2007). However, both of these
efforts only use 1-best parses, with the second one
packing different binarizations of the same tree in a
forest. Nevertheless we suspect that their extraction
algorithm is in principle similar to ours, although
they do not provide details of forest-based fragmen-
tation (Algorithm 1) which we think is non-trivial.
The forest concept is also used in machine transla-
tion decoding, for example to characterize the search
space of decoding with integrated language models
(Huang and Chiang, 2007). The first direct appli-
cation of parse forest in translation is our previous
work (Mi et al, 2008) which translates a packed for-
est from a parser; it is also the base system in our
experiments (see below). This work, on the other
hand, is in the orthogonal direction, where we uti-
lize forests in rule extraction instead of decoding.
211
Our experiments will use both default 1-best decod-
ing and forest-based decoding. As we will see in the
next section, the best result comes when we combine
the merits of both, i.e., using forests in both rule ex-
traction and decoding.
There is also a parallel work on extracting rules
from k-best parses and k-best alignments (Venu-
gopal et al, 2008), but both their experiments and
our own below confirm that extraction on k-best
parses is neither efficient nor effective.
5 Experiments
5.1 System
Our experiments are on Chinese-to-English trans-
lation based on a tree-to-string system similar to
(Huang et al, 2006; Liu et al, 2006). Given a 1-
best tree T , the decoder searches for the best deriva-
tion d? among the set of all possible derivations D:
d? = arg max
d?D
?0 log P(d | T ) + ?1 log Plm(?(d))
+ ?2|d|+ ?3|?(d)|
(7)
where the first two terms are translation and lan-
guage model probabilities, ?(d) is the target string
(English sentence) for derivation d, and the last two
terms are derivation and translation length penalties,
respectively. The conditional probability P(d | T )
decomposes into the product of rule probabilities:
P(d | T ) =
?
r?d
P(r). (8)
Each P(r) is in turn a product of five probabilities:
P(r) =P(r | lhs(r))?4 ? P(r | rhs(r))?5
? P(r | root(lhs(r)))?6
? Plex(lhs(r) | rhs(r))?7
? Plex(rhs(r) | lhs(r))?8
(9)
where the first three are conditional probabilities
based on fractional counts of rules defined in Sec-
tion 3.3, and the last two are lexical probabilities.
These parameters ?1 . . . ?8 are tuned by minimum
error rate training (Och, 2003) on the dev sets. We
refer readers to Mi et al (2008) for details of the
decoding algorithm.
0.240
0.242
0.244
0.246
0.248
0.250
0.252
0.254
 0  1  2  3  4  5  6
B
L
E
U
 s
co
re
average extracting time (secs/1000 sentences)
1-best
pe=2
pe=5
pe=8
k=30
forest extraction
k-best extraction
Figure 6: Comparison of extraction time and BLEU
score: forest-based vs.1-best and 30-best.
rules from... extraction decoding BLEU
1-best trees 0.24 1.74 0.2430
30-best trees 5.56 3.31 0.2488
forest: pe=8 2.36 3.40 0.2533
Pharaoh - - 0.2297
Table 2: Results with different rule extraction methods.
Extraction and decoding columns are running times in
secs per 1000 sentences and per sentence, respectively.
We use the Chinese parser of Xiong et al (2005)
to parse the source side of the bitext. Following
Huang (2008), we also modify this parser to out-
put a packed forest for each sentence, which can
be pruned by the marginal probability-based inside-
outside algorithm (Charniak and Johnson, 2005;
Huang, 2008). We will first report results trained
on a small-scaled dataset with detailed analysis, and
then scale to a larger one, where we also combine the
technique of forest-based decoding (Mi et al, 2008).
5.2 Results and Analysis on Small Data
To test the effect of forest-based rule extraction, we
parse the training set into parse forests and use three
levels of pruning thresholds: pe = 2, 5, 8.
Figure 6 plots the extraction speed and transla-
tion quality of forest-based extraction with various
pruning thresholds, compared to 1-best and 30-best
baselines. Using more than one parse tree apparently
improves the BLEU score, but at the cost of much
slower extraction, since each of the top-k trees has to
be processed individually although they share many
212
rules from ... total # on dev new rules used
1-best trees 440k 90k -
30-best trees 1.2M 130k 8.71%
forest: pe=8 3.3M 188k 16.3%
Table 3: Statistics of rules extracted from small data. The
last column shows the ratio of new rules introduced by
non 1-best parses being used in 1-best derivations.
common subtrees. Forest extraction, by contrast, is
much faster thanks to packing and produces consis-
tently better BLEU scores. With pruning threshold
pe = 8, forest-based extraction achieves a (case in-
sensitive) BLEU score of 0.2533, which is an ab-
solute improvement of 1.0% points over the 1-best
baseline, and is statistically significant using the
sign-test of Collins et al (2005) (p < 0.01). This
is also 0.5 points better than (and twice as fast as)
extracting on 30-best parses. These BLEU score re-
sults are summarized in Table 2, which also shows
that decoding with forest-extracted rules is less than
twice as slow as with 1-best rules, and only fraction-
ally slower than with 30-best rules.
We also investigate the question of how often
rules extracted from non 1-best parses are used by
the decoder. Table 3 shows the numbers of rules
extracted from 1-best, 30-best and forest-based ex-
tractions, and the numbers that survive after filter-
ing on the dev set. Basically in the forest-based case
we can use about twice as many rules as in the 1-
best case, or about 1.5 times of 30-best extraction.
But the real question is, are these extra rules really
useful in generating the final (1-best) translation?
The last row shows that 16.3% of the rules used
in 1-best derivations are indeed only extracted from
non 1-best parses in the forests. Note that this is a
stronger condition than changing the distribution of
rules by considering more parses; here we introduce
new rules never seen on any 1-best parses.
5.3 Final Results on Large Data
We also conduct experiments on a larger training
dataset, FBIS, which contains 239K sentence pairs
with about 6.9M/8.9M words in Chinese/English,
respectively. We also use a bigger trigram model
trained on the first 1/3 of the Xinhua portion of Gi-
gaword corpus. To integrate with forest-based de-
coding, we use both 1-best trees and packed forests
extract. \ decoding 1-best tree forest: pd=10
1-best trees 0.2560 0.2674
30-best trees 0.2634 0.2767
forest: pe=5 0.2679 0.2816
Hiero 0.2738
Table 4: BLEU score results trained on large data.
during both rule extraction and decoding phases.
Since the data scale is larger than the small data, we
are forced to use harsher pruning thresholds, with
pe = 5 for extraction and pd = 10 for decoding.
The final BLEU score results are shown in Ta-
ble 4. With both tree-based and forest-based decod-
ing, rules extracted from forests significantly outper-
form those extracted from 1-best trees (p < 0.01).
The final result with both forest-based extraction
and forest-based decoding reaches a BLEU score of
0.2816, outperforming that of Hiero (Chiang, 2005),
one of the best performing systems to date. These re-
sults confirm that our novel forest-based rule extrac-
tion approach is a promising direction for syntax-
based machine translation.
6 Conclusion and Future Work
In this paper, we have presented a novel approach
that extracts translation rules from a packed forest
encoding exponentially many trees, rather than from
1-best or k-best parses. Experiments on a state-of-
the-art tree-to-string system show that this method
improves BLEU score significantly, with reasonable
extraction speed. When combined with our previ-
ous work on forest-based decoding, the final result
is even better than the hierarchical system Hiero.
For future work we would like to apply this ap-
proach to other types of syntax-based translation
systems, namely the string-to-tree systems (Galley
et al, 2006) and tree-to-tree systems.
Acknowledgement
This work was funded by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108 (H. M.), and by NSF ITR EIA-
0205456 (L. H.). We would also like to thank Qun
Liu for supporting this work, and the three anony-
mous reviewers for improving the earlier version.
213
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
of ACL ?89, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the 43rd ACL, Ann Arbor, MI.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd ACL, Ann Arbor, MI.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL,
Ann Arbor, MI.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA, August.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
Columbus, OH.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Ar-
bor, MI.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
of AMTA, Honolulu, Hawaii.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of EMNLP,
Prague, Czech Rep., July.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings of IJCNLP 2005, pages
70?81.
214
Coling 2010: Poster Volume, pages 837?845,
Beijing, August 2010
Machine Translation with Lattices and Forests
Haitao Mi?? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Information Sciences Institute
Institute of Computing Technology Viterbi School of Engineering
Chinese Academy of Sciences University of Southern California
{htmi,liuqun}@ict.ac.cn {lhuang,haitaomi}@isi.edu
Abstract
Traditional 1-best translation pipelines
suffer a major drawback: the errors of 1-
best outputs, inevitably introduced by each
module, will propagate and accumulate
along the pipeline. In order to alleviate
this problem, we use compact structures,
lattice and forest, in each module instead
of 1-best results. We integrate both lat-
tice and forest into a single tree-to-string
system, and explore the algorithms of lat-
tice parsing, lattice-forest-based rule ex-
traction and decoding. More importantly,
our model takes into account all the proba-
bilities of different steps, such as segmen-
tation, parsing, and translation. The main
advantage of our model is that we can
make global decision to search for the best
segmentation, parse-tree and translation in
one step. Medium-scale experiments show
an improvement of +0.9 BLEU points over
a state-of-the-art forest-based baseline.
1 Introduction
Statistical machine translation (SMT) has wit-
nessed promising progress in recent years. Typi-
cally, conventional SMT is characterized as a 1-
best pipeline system (Figure 1(a)), whose mod-
ules are independent of each other and only take
as input 1-best results from the previous module.
Though this assumption is convenient to reduce
the complexity of SMT systems. It also bring a
major drawback of error propagation. The errors
of 1-best outputs, introduced inevitably in each
phase, will propagate and accumulate along the
pipeline. Not recoverable in the final decoding
(b)source segmentation lattice
parse forest target
source 1-best segmentation
1-best tree target
(a)
Figure 1: The pipeline of tree-based system: (a) 1-
best (b) lattice-forest.
step. These errors will severely hurt the translation
quality. For example, if the accuracy of each mod-
ule is 90%, the final accuracy will drop to 73%
after three separate phases.
To alleviate this problem, an obvious solution
is to widen the pipeline with k-best lists rather
than 1-best results. For example Venugopal et
al. (2008) use k-best alignments and parses in the
training phase. However, with limited scope and
too many redundancies, it is inefficient to search
separately on each of these similar lists (Huang,
2008).
Another efficient method is to use compact data
structures instead of k-best lists. A lattice or forest,
compactly encoded exponentially many deriva-
tions, have proven to be a promising technique.
For example, Mi and Huang (2008), Mi et al
(2008), Liu et al (2009) and Zhang et al (2009)
use forests in rule extraction and decoding phases
to extract more general rules and weaken the influ-
ence of parsing errors; Dyer et al (2008) use word
lattice in Chinese word segmentation and Arabic
morphological variation phases to weaken the in-
fluence of segmentation errors; Huang (2008) and
837
0 1 2 3 4 5 6 7 8 9c0:Bu` c1:sh?? c2:yu? c3:Sha? c4:lo?ng c5:ju? c6:x??ng c7:ta?o c8:lu`n
(0, 2, NR) (2, 3, CC) (3, 5, NR) (5, 6, VV) (6, 8, NN) (8, 9, NN)
(5, 7, VV) (7, 9, NN)(2, 3, P)
Figure 2: The lattice of the example:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n.? The solid lines show the 1-best
result, which is wrong.
Jiang et al (2008b) stress the problems in re-
ranking phase. Both lattices and forests have be-
come popular in machine translation literature.
However, to the best of our knowledge, previous
work only focused on one module at a time. In this
paper, we investigate the combination of lattice
and forest (Section 2), as shown in Figure 1(b).
We explore the algorithms of lattice parsing (Sec-
tion 3.2), rule extraction (Section 4) and decod-
ing (Section 5). More importantly, in the decoding
step, our model can search among not only more
parse-trees but also more segmentations encoded
in the lattice-forests and can take into account all
the probabilities of segmentations and parse-trees.
In other words, our model postpones the disambi-
guition of segmentation and parsing into the final
translation step, so that we can do global search
for the best segmentation, parse-tree and transla-
tion in one step. When we integrate a lattice into
a forest system, medium-scale experiments (Sec-
tion 6) show another improvement of +0.9 BLEU
points over a state-of-the-art forest-based system.
2 Compact Structures
A word lattice (Figure 2) is a compact representa-
tion of all the possible of segmentations and POS
tags, while a parse forest (Figure 5) is a compact
representation of all parse trees.
2.1 Word Lattice
For a given input sentence C = c0..cn?1, where
ci denotes a character at position i, and n is the
length of the sentence.
A word lattice (Figure 2), or lattice in short, is
a set of edges L, where each edge is in the form
of (i, j, X), which denotes a word of tag X , cov-
ering characters ci through cj?1. For example, in
Figure 2, (7, 9, NN) is a noun ?ta?olu`n? of two char-
acters.
The lattice in Figure 2 shows result of the ex-
ample:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n ?.
One ambiguity comes from the POS tag of word
?yu?? (preposition (P) or conjunction (CC)). The
other one is the segmentation ambiguity of the last
four characters, we can segment into either ?ju?
x??ngta?o lu`n? (solid lines), which means lift, beg-
ging and argument separately for each word or
?ju?x??ng ta?olu`n? (dashed lines), which means hold
a discussion.
lift begging argument
5 ju? 6 x??ng 7 ta?o 8 lu`n 9
hold a discussion
The solid lines above (and also in Figure 2)
show the 1-best result, which is obviously wrong.
If we feed it into the next modules in the SMT
pipeline, parsing and translation will be become
much more difficult, since the segmentation is not
recoverable. So it is necessary to postpone er-
ror segmentation decisions to the final translation
step.
2.2 Parse Forest
In parsing scenario, a parse forest (Figrure 5), or
forest for short, can be formalized as a hyper-
graph H , a pair ?V, E?, where node v ? V is in
the form of Xi,j , which denotes the recognition of
nonterminal X spanning the substring ci:j?1 from
positions ci through cj?1. Each hyperedge e ? E
is a pair ?tails(e), head(e)?, where head(e) ? V
is the consequent node in an instantiated deduc-
tive step, and tails(e) ? (V )? is the list of an-
tecedent nodes.
For the following deduction:
NR0,2 CC2,3 NR3,5
NP0,5 (*)
838
its hyperedge e? is notated:
?(NR0,2, CC2,3, NR3,5), NP0,5?.
where
head(e?) = {NP0,5}, and
tails(e?) = {NR0,2,CC2,3,NR3,5}.
We also denote IN (v) to be the set of incoming
hyperedges of node v, which represents the dif-
ferent ways of deriving v. For simplicity, we only
show a tree in Figure 5(a) over 1-best segmenta-
tion and POS tagging result in Figure 2. So the
IN (NP0,5) is {e?}.
3 Lattice Parsing
In this section, we first briefly review the con-
ventional CYK parsing, and then extend to lattice
parsing. More importantly, we propose a more ef-
ficient parsing paradigm in Section 3.3.
3.1 Conventional Parsing
The conventional CYK parsing algorithm in Fig-
ure 3(a) usually takes as input a single sequence of
words, so the CYK cells are organized over words.
This algorithm consists of two steps: initialization
and parsing. The first step is to initialize the CYK
cells, whose span size is one, with POS tags pro-
duced by a POS tagger or defined by the input
string1. For example, the top line in Figure 3(a)
is initialized with a series of POS tags in 1-best
segmentation. The second step is to search for the
best syntactic tree under a context-free grammar.
For example, the tree composed by the solid lines
in Figure 5(a) shows the parsing tree for the 1-best
segmentation and POS tagging results.
3.2 Lattice Parsing
The main differences of our lattice parsing in Fig-
ure 3(b) from conventional approach are listed in
following: First, the CYK cells are organized over
characters rather than words. Second, in the ini-
tialization step, we only initialize the cells with
all edges L in the lattice. Take the edge (7, 9,
NN) in Figure 2 for example, the corresponding
cell should be (7, 9), then we add a leaf node
v = NN7,9 with a word ta?olu`n. The final initial-
ization is shown in Figure 3(b), which shows that
1For simplicity, we assume the input of a parser is a seg-
mentation and POS tagging result
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC NR VV NN NN
NP VPB
IP
O(n3w)
(a): Parsing over 1-best segmentation
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR
CC,P
NR
VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3)
(b): Parsing over characters
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC,P NR VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3r)
(c): Parsing over most-refined segmentation
Figure 3: CKY parsing charts (a): Conventional
parsing over 1-best segmentation. (b): Lattice
parsing over characters of input sentence. (c): Lat-
tice parsing over most-refined segmentation of lat-
tice. nw and nr denotes the number of tokens over
the 1-best segmentation and the most-refined seg-
menation respectively, and nw ? nr ? n.
lattice parsing can initialize the cells, whose span
size is larger than one. Third, in the deduction step
of the parsing algorithm i, j, k are the indexes be-
tween characters rather than words.
We formalize our lattice parser as a deductive
proof system (Shieber et al, 1994) in Figure 4.
Following the definitions of the previous Sec-
839
tion, given a set of edges L of a lattice for an in-
put sentence C = c0..cn?1 and a PCFG grammar:
a 4-tuple ?N, ?, P, S?, where N is a set of non-
terminals, ? is a set of terminal symbols, P is a
set of inference rules, each of which is in the form
of X ? ? : p for X ? N , ? ? (N ? ?)? and p is
the probability, and S ? N is the start symbol. The
deductive proof system (Figure 4) consists of ax-
ioms, goals and inference rules. The axioms are
converted by edges in L. Take the (5, 7, NN) as-
sociated with a weight p1 for example, the corre-
sponding axiom is NN ? ta?olu`n : p1. All axioms
converted from the lattice are shown in Figure 3(b)
exclude the italic non-terminals. Please note that
all the probabilities of the edges L in a lattice are
taken into account in the parsing step. The goals
are the recognition X0,n ? S of the whole sen-
tence. The inference rules are the deductions in
parsing. Take the deduction (*) for example, it will
prove a new item NP0,5 (italic NP in Figure 3(b))
and generate a new hyper-edge e? (in Figure 5(b)).
So the parsing algorithm starts with the axioms,
and then applies the inference rules to prove new
items until a goal item is proved. The final whole
forest for the input lattice (Figure 2) is shown in
Figure 5(b). The extra hyper-edges of lattice-forest
are highlighted with dashed lines, which can in-
ference the input sentence correctly. For example:
?yu?? is tagged into P rather than CC.
3.3 Faster Parsing with Most-refined Lattice
However, our statistics show that the average num-
ber of characters n in a sentence is 1.6 times than
the number of words nw in its 1-best segmenta-
tion. As a result, the parsing time over the charac-
ters will grow more than 4 times than parsing over
the 1-best segmentation, since the time complexity
is O(n3). In order to alleviate this problem, we re-
duce the parsing time by using most-refined seg-
mentation for a lattice, whose number of tokens
is nr and has the property nw ? nr ? n.
Given a lattice with its edges L over indexes
(0, .., n), a index i is a split point, if and only if
there exists some edge (i, j, X) ? L or (k, i, X) ?
L. The most-refined segmentation, or ms for
short, is the segmentation result by using all split
points in a lattice. For example, the corresponding
ms of the example is ?Bu`sh?? yu? Sha?lo?ng ju? x??ng
ta?o lu`n? since points 1 and 4 are not split points.
Item form: Xi,j
Axioms: Xi,j : p(i, j, X)
(i, j, X) ? L
Infer. rules:
Xi,k : p1 Yk,j : p2
Zi,j : pp1p2
Z ? XY : p ? P
Goals: X0,n
Figure 4: Lattice parsing as deductive proof sys-
tem. The i, j, k are the indexes between characters.
Figure 3(c) shows the CKY parsing cells over
most-refined segmentation, the average number
of tokens nr is reduced by combining columns,
which are shown with red dashed boxes. As a re-
sult, the search space is reduced without losing any
derivations. Theoretically, the parsing over fs will
speed up in O((n/nr)3). And our experiments in
Section 6 show the efficiency of our new approach.
It turns out that the parsing algorithm developed
in lattice-parsing Section 3.2 can be used here
without any change. The non-terminals inducted
are also shown in Figure 3(c) in italic style.
4 Rule Extraction with Lattice & Forest
We now explore the extraction algorithm from
aligned source lattice-forest and target string2,
which is a tuple ?F, ?, a? in Figure 5(b). Following
Mi and Huang (2008), we extract minimal rules
from a lattice-forest also in two steps:
(1) frontier set computation
(2) fragmentation
Following the algorithms developed by Mi and
Huang (2008) in Algorithm 1, all the nodes in
frontier set (fs) are highlighted with gray in Fig-
ure 5(b).
Our process of fragmentation (lines 1- 13) is
to visit each frontier node v and initial a queue
(open) of growing fragments with a pair of empty
fragment and node v (line 3). Each fragment is as-
sociated with a list of expansion sites (front) being
2For simplicity and consistency, we use character-based
lattice-forest for the running example. The ?Bu`? and ?sh???
are aligned to the same word ?Bush?. In our experiment,
we use most-refined segmentation to run lattice-parsing and
word alignment.
840
IP0,9
NP0,5 VPB5,9
(a)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
.NR0,2 .CC2,3 .NR3,5 .VV5,6 .NN6,8 .NN8,9
e?
IP0,9
NP0,5 VP2,9
PP2,5 VPB5,9
(b)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
. NR0,2 . CC2,3 . NR3,5 .VV5,6 .NN6,8 .NN8,9. VV5,7 . NN7,9. P2,3
e?
Bush held a discussion with Sharon
Forest only (Minimal rules) Lattice & forest (Extra minimal rules)
(c)
IP(NP(x1:NR x2:CC x3:NR) x4:VPB) IP(x1:NR x2:VP) ? x1 x2
? x1 x4 x2 x3 VP(x1:PP x2:VPB) ? x2 x1
CC(yu?) ?with PP(x1:P x2:NR) ? x1 x2
NR(Sha?lo?ng) ?Sharon P(yu?) ?with
NR(Bu`sh??) ?Bush VPB(x1:VV x2:NN) ? x1 x2
VPB(VV(ju?) NN(x??ngta?o) NN(lu`n)) VV(ju?x??ng) ?held
?held a discussion NN(ta?olu`n) ?a discussion
Figure 5: (a): The parse forest over the 1-best segmentation and POS tagging result. (b): Word-aligned
tuple ?F, ?, a?: the lattice-forest F , the target string ? and the word alingment a. The solid hyperedges
form the forest in (a). The dashed hyperedges are the extra hyperedges introduced by the lattice-forest.
(c): The minimal rules extracted on forest-only (left column), and the extra minimal rules extracted on
lattice-forest (right column).
the subset of leaf nodes of the current fragment
that are not in the fs except for the initial node
v. Then we keep expanding fragments in open in
following way. If current fragment is complete,
whose expansion sites is empty, we extract rule
corresponding to the fragment and its target string
841
Code 1 Rule Extraction (Mi and Huang, 2008).
Input: lattice-forest F , target sentence ? , and
alignment a
Output: minimal rule set R
1: fs ? FROSET(F, ?, a)  frontier set
2: for each v ? fs do
3: open ? {??, {v}?}  initial queue
4: while open 6= ? do
5: ?frag , front? ? open.pop()
6: if front = ? then  finished?
7: generate a rule r using frag
8: R.append(r)
9: else  incomplete: further expand
10: u ? front .pop()  expand frontier
11: for each e ? IN (u) do
12: f ? front ? (tails(e) \ fs)
13: open .append(?frag ? {e}, f ?)
(line 7) . Otherwise we pop one expansion node
u to grow and spin-off new fragments by IN (u),
adding new expansion sites (lines 11- 13), until all
active fragments are complete and open queue is
empty.
The extra minimal rules extracted on lattice-
forest are listed at the right bottom of Figure 5(c).
Compared with the forest-only approach, we can
extract smaller and more general rules.
After we get al the minimal rules, we com-
pose two or more minimal rules into composed
rules (Galley et al, 2006), which will be used in
our experiments.
For each rule r extracted, we also assign a frac-
tional count which is computed by using inside-
outside probabilities:
c(r) =
?(root(r)) ? P(lhs(r)) ? Qv?yield(root(r)) ?(v)
?(TOP) ,
(1)
where root(r) is the root of the rule, lhs(r) is
the left-hand-side of rule, rhs(r) is the right-
hand-side of rule, P(lhs(r)) is the product of
all probabilities of hyperedges involved in lhs(r),
yield(root(r)) is the leave nodes, TOP is the root
node of the forest, ?(v) and ?(v) are outside and
inside probabilities, respectively.
Then we compute three conditional probabili-
ties for each rule:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(2)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(3)
P(r | root(r)) = c(r)?
r?:root(r?)=root(r) c(r?)
. (4)
All these probabilities are used in decoding step
(Section 5). For more detail, we refer to the algo-
rithms of Mi and Huang (2008).
5 Decoding with Lattice & Forest
Given a source-side lattice-forest F , our decoder
searches for the best derivation d? among the set of
all possible derivation D, each of which converts
a tree in lattice-forest into a target string ? :
d? = argmax
d?D,T?F
P (d|T )?0 ? e?1|d|
? LM(?(d))?2 ? e?3|?(d)|,
(5)
where |d| is the penalty term on the number of
rules in a derivation, LM(?(d)) is the language
model and e?3|?(d)| is the length penalty term on
target translation. The P (d|T ) decomposes into
the product of rule probabilities P (r), each of
which is decomposed further into
P (d|T ) =
?
r?d
P (r). (6)
Each P (r) in Equation 6 is decomposed further
into the production of five probabilities:
P(r) = P(r|lhs(r))?4
? P(r|rhs(r))?5
? P(r|root(lhs(r))?6
? Plex(lhs(r)|rhs(r))?7
? Plex(rhs(r)|lhs(r))?8 ,
(7)
where the last two are the lexical probabilities be-
tween the terminals of lhs(r) and rhs(r). All the
weights of those features are tuned by using Min-
imal Error Rate Training (Och, 2003).
Following Mi et al (2008), we first convert the
lattice-forest into lattice translation forest with the
conversion algorithm proposed byMi et al (2008),
842
and then the decoder finds the best derivation on
the lattice translation forest. For 1-best search, we
use the cube pruning technique (Chiang, 2007;
Huang and Chiang, 2007) which approximately
intersects the translation forest with the LM. For
k-best search after getting 1-best derivation, we
use the lazy Algorithm 3 of Huang and Chiang
(2005) to incrementally compute the second, third,
through the kth best alternatives.
For more detail, we refer to the algorithms of
Mi et al (2008).
6 Experiments
6.1 Data Preparation
Our experiments are on Chinese-to-English trans-
lation. Our training corpus is FBIS corpus with
about 6.9M/8.9M words in Chinese/English re-
spectively.
We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
Kneser-Ney smoothing on the first 1/3 of the Xin-
hua portion of Gigaword corpus.
We use the 2002 NIST MT Evaluation test set
as development set and the 2005 NIST MT Eval-
uation test set as test set. We evaluate the trans-
lation quality using the case-insensitive BLEU-4
metric (Papineni et al, 2002). We use the standard
MERT (Och, 2003) to tune the weights.
6.1.1 Baseline Forest-based System
We first segment the Chinese sentences into the
1-best segmentations using a state-of-the-art sys-
tem (Jiang et al, 2008a), since it is not necessary
for a conventional parser to take as input the POS
tagging results. Then we parse the segmentation
results into forest by using the parser of Xiong et
al. (2005). Actually, the parser will assign multiple
POS tags to each word rather than one. As a result,
our baseline system has already postponed the
POS tagging disambiguition to the decoding step.
Forest is pruned by using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold are pf = 5 and pf = 10 at
rule extraction and decoding steps respectively.
We word-align the strings of 1-best segmenta-
tions and target strings with GIZA++ (Och and
Ney, 2000) and apply the refinement method
?grow-diag-final-and? (Koehn et al, 2003) to get
the final alignments. Following Mi and Huang
(2008) and Mi et al (2008), we also extract rules
from forest-string pairs and translate forest to
string.
6.1.2 Lattice-forest System
We first segment and POS tag the Chinese sen-
tences into word lattices using the same sys-
tem (Jiang et al, 2008a), and prune each lat-
tice into a reasonable size using the marginal
probability-based pruning algorithm.
Then, as current GIZA++ (Och and Ney, 2000)
can only handle alignment between string-string
pairs, and word-alingment with the pairs of Chi-
nese characters and target-string will obviously re-
sult in worse alignment quality. So a much better
way to utilize GIZA++ is to use the most-refined
segmentation for each lattice instead of the char-
acter sequence. This approach can be viewed as a
compromise between character-string and lattice-
string word-alignment paradigms. In our exper-
iments, we construct the most-refined segmen-
tations for lattices and word-align them against
the English sentences. We again apply the refine-
ment method ?grow-diag-final-and? (Koehn et al,
2003) to get the final alignments.
In order to get the lattice-forests, we modi-
fied Xiong et al (2005)?s parser into a lattice
parser, which produces the pruned lattice forests
for both training, dev and test sentences. Finally,
we apply the rule extraction algorithm proposed in
this paper to obtain the rule set. Both lattices and
forests are pruned using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold of lattice is pl = 20 at both
the rule extraction and decoding steps, the thresh-
olds for the latice-forests are pf = 5 and pf = 10
at rule extraction and decoding steps respectively.
6.2 Results and Analysis
Table 1 shows results of two systems. Our lattice-
forest (LF) system achieves a BLEU score of
29.65, which is an absolute improvement of 0.9
points over the forest (F) baseline system, and the
improvement is statistically significant at p < 0.01
using the sign-test of Collins et al (2005).
The average number of tokens for the 1-best
and most-refined segmentations are shown in sec-
ond column. The average number of characters
is 46.7, which is not shown in Table 1. Com-
843
Sys Avg # of Rules BLEU
tokens links All dev&tst
F 28.7 35.1 29.6M 3.3M 28.75
LF 37.1 37.1 23.5M 3.4M 29.65
Table 1: Results of forest (F) and lattice-forest
(LF) systems. Please note that lattice-forest system
only extracts 23.5M rules, which is only 79.4% of
the rules extracted by forest system. However, in
decoding step, lattice-forest system can use more
rules after filtered on dev and test sets.
pared with the characters-based lattice parsing, our
most-refined lattice parsing speeds up parsing by
(37.1/46.7)3 ? 2 times, since parsing complexity
is O(n3).
More interestingly, our lattice-forest model only
extracts 23.5M rules, which is 79.4% percent of
the rules extracted by the baseline system. The
main reason lies in the larger average number
of words for most-refined segmentations over lat-
tices being 37.1 words vs 28.7 words over 1-best
segmentations. With much finer granularity, more
word aligned links and restrictions are introduced
during the rule extraction step by GIZA++. How-
ever, more rules can be used in the decoding step
for the lattice-forest system, since the lattice-forest
is larger than the forest over 1-best segmentation.
We also investigate the question of how often
the non 1-best segmentations are picked in the fi-
nal translation. The statistic on our dev set sug-
gests 33% of sentences choose non 1-best segmen-
tations. So our lattice-forest model can do global
search for the best segmentation and parse-tree to
direct the final translation. More importantly, we
can use more translation rules in the translation
step.
7 Related Works
Compactly encoding exponentially many deriva-
tions, lattice and forest have been used in some
previous works on SMT. To alleviate the prob-
lem of parsing error in 1-best tree-to-string trans-
lation model, Mi et al (2008) first use forest to
direct translation. Then Mi and Huang (2008) use
forest in rule extraction step. Following the same
direction, Liu et al (2009) use forest in tree-
to-tree model, and improve 1-best system by 3
BLEU points. Zhang et al (2009) use forest in
tree-sequence-to-string model and also achieve a
promising improvement. Dyer et al (2008) com-
bine multiple segmentations into word lattice and
then use lattice to direct a phrase-based transla-
tion decoder. Then Dyer (2009) employ a single
Maximum Entropy segmentation model to gen-
erate more diverse lattice, they test their model
on the hierarchical phrase-based system. Lattices
and forests can also be used in Minimal Error
Rate Training and Minimum Bayes Risk Decod-
ing phases (Macherey et al, 2008; Tromble et al,
2008; DeNero et al, 2009; Kumar et al, 2009; Li
and Eisner, 2009). Different from the works listed
above, we mainly focus on how to combine lattice
and forest into a single tree-to-string system.
8 Conclusion and Future Work
In this paper, we have proposed a lattice-forest
based model to alleviate the problem of error prop-
agation in traditional single-best pipeline frame-
work. Unlike previous works, which only focus on
one module at a time, our model successfully in-
tegrates lattice into a state-of-the-art forest tree-to-
string system. We have explored the algorithms of
lattice parsing, rule extraction and decoding. Our
model postpones the disambiguition of segmenta-
tion and parsing into the final translation step, so
that we can make a more global decision to search
for the best segmentation, parse-tree and transla-
tion in one step. The experimental results show
that our lattice-forest approach achieves an abso-
lute improvement of +0.9 points in term of BLEU
score over a state-of-the-art forest-based model.
For future work, we would like to pay more
attention to word alignment between lattice pairs
and forest pairs, which would be more principled
than our current method of word alignment be-
tween most-refined segmentation and string.
Acknowledgement
We thank Steve DeNeefe and the three anony-
mous reviewers for comments. The work is sup-
ported by National Natural Science Foundation
of China, Contracts 90920004 and 60736014,
and 863 State Key Project No. 2006AA010108
(H. M and Q. L.), and in part by DARPA GALE
Contract No. HR0011-06-C-0022, and DARPA
under DOI-NBC Grant N10AP20031 (L. H and
H. M).
844
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL/IJCNLP.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, June.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney,
Australia, July.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling
2008.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133, Edmon-
ton, Canada, May.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the ACL/IJCNLP 2009.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of EMNLP, pages 40?51, Singapore,
August. Association for Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of EMNLP 2008.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206?214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311?318, Philadephia, USA, July.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of de-
ductive parsing.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP 2008.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in MT training. In Proceed-
ings of AMTA.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
845
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1133?1143, Dublin, Ireland, August 23-29 2014.
A Structured Language Model for Incremental Tree-to-String Translation
Heng Yu1
1Institute of Computing Technology. CAS
University of Chinese Academy of Sciences
yuheng@ict.ac.cn
Haitao Mi
T.J. Watson Research Center
IBM
hmi@us.ibm.com
Liang Huang
Queens College & Grad. Center
City University of New York
huang@cs.qc.cuny.edu
Qun Liu1,2
2Centre for Next Generation Localisation.
Faculty of Engineering and Computing
Dublin City University
qliu@computing.dcu.ie
Abstract
Tree-to-string systems have gained significant popularity thanks to their simplicity and efficien-
cy by exploring the source syntax information, but they lack in the target syntax to guarantee
the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate
a structured language model, a left-to-right shift-reduce parser in specific, into an incremental
tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integra-
tion. Large-scale experiments on various Chinese-English test sets show that with a reasonable
speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
1 Introduction
Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained
significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et
al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster
by proposing an incremental tree-to-string model, which generates the target translation exactly in a left-
to-right manner. Although, tree-to-string models have made those progresses, they can not utilize the
target syntax information to guarantee the grammaticality of the output, as they only generate strings on
the target side.
One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree
models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches
still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al.,
2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010).
Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Char-
niak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved
better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is inde-
pendent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the
problems that tree-to-tree models have. However, integration is not easy, as the following two questions
arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures.
Second, hypotheses in the same bin may not be comparable, since their syntactic structures may not be
comparable, and the future costs are hard to estimate. Hassan et al. (2009) skip those problems by only
keeping the best parsing structure for each hypothesis.
In this paper, we integrate a shift-reduce parser into an incremental tree-to-string model, and intro-
duce an efficient grouping and pruning method to handle the growing search space and incomparable
hypotheses problems. Large-scale experiments on various Chinese-English test sets show that with a rea-
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1133
sonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
2 Linear-time Shift-reduce Parsing
parsing
action signature dependency structure
s1 s0 q0
Bush S 0
sh Bush held S 1: Bush
sh Bush held a S 2: Bush held
re
x
held
Bush
a S 3: Bush held
sh held
Bush
a meeting S 4: Bush held a
sh a meeting with S 5: Bush held a meeting
re
x
held
Bush
meeting
a
with S 6: Bush held a meeting
re
y
held
Bush meeting
with S 7: Bush held a meeting
sh held
Bush meeting
with Sharon S 8: Bush held a meeting with
sh with Sharon S 9: Bush held a meeting with Sharon
re
y
held
Bush meeting
with
Sharon
S 10: Bush held a meeting with Sharon
re
y
held
Bush meeting with
S 11: Bush held a meeting with Sharon
Figure 1: Linear-time left-to-right dependency parsing.
A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step,
chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re)
the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing
scenario, the reduce action is further divided into two cases: left-reduce (re
x
) and right-reduce (re
y
),
depending on which one of the two items becomes the head after reduction. Each parsing derivation can
be represented by a sequence of parsing actions.
1134
2.1 Shift-reduce Dependency Parsing
We will use the following sentence as the running example:
Bush held a meeting with Sharon
Given an input sentence e, where ei is the ith token, ei...e j is the substring of e from i to j, a shift-reduce
parser searches for a dependency tree with a sequence of shift-reduce moves (see Figure 1). Starting
from an initial structure S 0, we first shift (sh) a word e1, ?Bush?, onto the parsing stack s0, and form a
structure S 1 with a singleton tree. Then e2, ?held?, is shifted, and there are two or more structures in the
parsing stack, we can use re
x
or re
y
step to combine the top two trees on the stack, replace them with
dependency structure e1 x e0 or e1 y e0 (shown as S 3), and add one more dependency edge between
e0 and e1.
Note that the shade nodes are exposed heads on which re
x
or re
y
parsing actions can be performed.
The middle columns in Figure 1 are the parsing signatures: q0 (parsing queue), s0 and s1 (parsing stack),
where s0 and s1 only have one level dependency. Take the line of S 11 for example, ?a? is not in the
signature. As each action results in an update of cost, we can pick the best one (or few, with beam) after
each action. Costs are accumulated in each step by extracting contextual features from the structure and
the action. As the sentence gets longer, the number of partial structures generated at each steps grows
exponentially, which makes it impossible to search all of the hypothesis. In practice, we usually use beam
search instead.
(a) atomic features
s0.w s0.t
s1.w s1.t
s0.lc.t s0.rc.t
q0.w q0.t
(b) feature templates
unigram
s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
bigram
s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.w ? s1.w ? s1.t s0.t ? s1.w ? s1.t
s0.w ? s0.t ? s1.w
trigram s0.t ? s1.t ? q0.t s1.t ? s0.t ? s0.lc.t
s1.t ? s0.t ? q0.t s1.t ? s0.t ? s0.rc.t
(c) ?? parsing stack parsing queue ??
. . . s1 s0
s0.lc ? ? ? s0.rc
q0
Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from
Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree,
x.lc and x.rc denote x?s leftmost and rightmost child respectively. (c) the feature window.
2.2 Features
We view features as ?abstractions? or (partial) observations of the current structure. Feature templates f
are functions that draw information from the feature window, consisting of current partial tree and first
word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic
1135
IP
NP
Bu`sh??
VP
PP
P
yu?
NP
Sha?lo?ng
VP
VV
ju?x??ng
AS
le
NP
hu?`ta?n
Figure 2: A parse tree
features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way
classification based on f, and conjoin these feature instances with each action:
[f ? (action=sh/re
x
/re
y
)]
We extract all the feature templates from training data, and use the average perceptron algorithm and
early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model.
3 Incremental Tree-to-string Translation with Slm
The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps:
parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and
the linear incremental decoder then searches for the best derivation that generates a target-language string
in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the
following section.
3.1 Decoding with Slm
Since the incremental tree-to-string model generates translation in strictly left-to-right fashion, and the
shift-reduce dependency parser also processes an input sentence in left-to-right order, it is intuitive to
combine them together. The last two columns in Figure 3 show the dependency structures for the corre-
sponding hypotheses. Start at the root translation stack with a dot  before the root node IP:
[ IP ],
we first predict (pr) with rule r1,
(r1) IP (x1:NP x2:VP)? x1 x2,
and push its English-side to the translation stack, with variables replaced by matched tree nodes, here
x1 for NP and x2 for VP. Since this translation action does not generate any translation string, we don?t
perform any dependency parsing actions. So we have the following translation stack
[ IP ][ NP VP],
where the dot  indicates the next symbol to process in the English word-order. Since node NP is the next
symbol, we then predict with rule r2,
(r2) NP(Bu`sh??)? Bush,
and add it to the translation stack:
[ IP ] [ NP VP ] [ Bush]
Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current
translation, which results in the new translation stack
[ IP ] [ NP VP ] [Bush  ]
1136
translation parsing
stack string dependency structure Slm
[  IP ] S 0
1 pr [  IP ] [  NP VP] S 0
2 pr [  IP ] [ NP VP ] [  Bush ] S 0
3 sc [  IP ] [ NP VP] [Bush  ] Bush S 1: Bush P(Bush | S 0)
4 co [  IP ] [NP  VP] S 1:
5 pr [  IP ] [NP  VP] [ held NP with NP] S 1:
6 sc [  IP ] [NP  VP] [held  NP with NP] held S 3: Bush held P(held | S 1)
7 pr [ IP] [NP VP] [held NP with NP] [ a meeting] S 3
8 sc [ IP] [NP VP] [held  NP with NP] [a meeting  ] a meeting S 7: Bush held a meeting P(a meeting | S 3)
9 co [ IP ] [NP VP] [held NP  with NP] S 7
10 sc [ IP] [NP VP] [held NP with  NP] with S 8: Bush held a meeting with P(with | S 7)
S ?8: Bush held a meeting with P
? (with | S 7)
11 pr [ IP] [NP VP] [held NP with  NP] [ Sharon] S 8
S 8?
12 sc [ IP ] [NP  VP] [held NP with  NP] [Sharon ] Sharon S 11: Bush held a meeting with Sharon P(Sharon | S 8)
S ?11? : Bush held a meeting with Sharon P
? (Sharon | S ?8)
13 co [  IP ] [NP  VP] [held NP with NP ] S 11
14 co [  IP ] [NP VP ] S 11
15 co [ IP  ] S 11
Figure 3: Simulation of the integraton of an Slm into an incremental tree-to-string decoding. The first
column is the line number. The second column shows the translation actions: predict (pr), scan (sc), and
complete (co). S i denotes a dependency parsing structure. The shaded nodes are exposed roots of S i.
Immediately after each sc translation action, our shift-reduce parser is triggered. Here, our parser applies
the parsing action sh, and shift ?Bush? into a partial dependency structure S 1 as a root ?Bush? (shaded
node) in Figure 3. Now the top rule on the translation stack has finished (dot is at the end), so we complete
(co) it, pop the top rule and advance the dot in the second-to-top rule, denoting that NP is completed:
[ IP ] [NP  VP].
Following this procedure, we have a dependency structure S 3 after we scan (sc) the word ?held? and
take a shift (sh) and a left reduce (re
x
) parsing actions. The shaded node ?held? means exposed roots,
that the shift-reduce parser takes actions on.
Following Huang and Mi (2010), the hypotheses with same translation step1 fall into the same bin.
Thus, only the prediction (pr) actions actually make a jump from a bin to another. Here line 2 to 4 fall
into one bin (translation step = 4, as there are 4 nodes, IP, NP, VP and Bu`sh??, in the source tree are
covered). Similarly, lines from 7 to 10 fall into another bin (translation step = 15).
1The step number is defined by the number of tree nodes covered in the source tree, and it is not equal to the number of
translation actions taken so far.
1137
Noted that as we number the bins by the translation step, only pr actions make progress, the sc and
co actions are treated as ?closure? operators in practice. Thus we always do as many sc/co actions as
possible immediately after a pr step until the symbol after the dot is another non-terminal. The total
number of bins is equal to the size of the parse tree, and each hypothesis has a constant number of
outgoing hyper-edges to predict, so the time complexity is linear in the sentence length.
After adding our Slm to this translation, an interesting branch occurs after we scan the word ?with?,
we have two different partial dependency structures S 8 and S
?
8 for the same translation. If we denote
N(S i) as the number of re actions that S i takes, N(S 8) is 3, while N(S ?8) is 4. Here N(S i) does not take
into account the number of sh parsing actions, since all partial structures with same translations should
shift the same number of translations. Thus, N(S i) determines the score of dependency structures, and
only the hypotheses with same N(S i) are comparable to each other. In this case, we should distinguish
S 8 with S
?
8, and if we make a prediction over the hypothesis of S 8, we can reach the correct parsing state
S 11 (shown in the red dashed line in Figure 3).
So the key problem of our integration is that, after each translation step, we will apply different se-
quences of parsing actions, which result in different and incomparable dependency structures with the
same translation. In the following two Sections, we introduce three ways for this integration.
3.2 Na??ve: Adding Parsing Signatures into Translation Signatures
One straightforward approach is to add the parsing signatures (in Figure 1) of each dependency structure
(in Figure 1 and Figure 3) to translation signatures. Here, we only take into account of the s0 and s1 in
the parsing stack, as the q0 is the future word that is not available in translation strings. For example, the
dependency structure S 8 has parsing signatures:
held
Bush meeting
with
We add those information to its translation signature, and only the hypothesis that have same translation
and parsing signatures can be recombined.
So, in each translation bin, different dependency structures with same translation strings are treated as
different hypothesis, and all the hypothesis are sorted and ranked in the same way. For example, S 8 and
S ?8 are compared in the bin, and we only keep top b (the beam size) hypothesis for each bin.
Obviously, this simple approach suffers from the incomparable problem for those hypothesis that have
different number of parsing actions (e.g. S 8 and S ?8). Moreover, it may result in very low translation
variance in each beam.
3.3 Best-parse: Keeping the Best Dependency Structure for Each Translation
Following Hassan et al. (2009), we only keep the best parsing tree for each translation. That means after
a consecutive translation sc actions, our shift-reduce parser applies all the possible parsing actions, and
generates a set of new partial dependency structures. Then we only choose the best one with the highest
Slm score, and only use this dependency structure for future predictions.
For example, for the translation in line 10 in Figure 3, we only keep S 8, if the parsing score of S 8 is
higher than S ?8, although they are not comparable. Another complicate example is shown in Figure 4,
within the translation step 15, there are many alternatives with different parsing structures for the same
translation (?a meeting with?) in the third column, but we can only choose the top one in the final.
3.4 Grouping: Regrouping Hypothesis by N(S ) in Each Bin
In order to do comparable sorting and pruning, our basic idea is to regroup those hypotheses in a same
bin into small groups by N(S ). For each translation, we first apply all the possible parsing actions,
and generate all dependency structures. Then we regroup all the hypothesis with different dependency
structures based on the size of N(S ).
1138
Bush held al Bush held a meetingl i
sh
Bush held al
re
Bush held a meetingl i
Bush held a meetingl i
re
sh
Bush held a meetingl i
re
Bush held a meeting withl i i
sh
Bush held a meeting withl i i
sh
sh
Bush held a meeting withl i i
Bush held a meeting withl i i
sh
re
re
Bush held a meeting withl i i
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
Bush held a meeting with Sharonl i i
re
sh
Bush held a meeting with Sharonl i i
sh
......
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
re
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i ish
......
Bush held a meeting with Sharonl i i
re
Step 15 Step 16
G1: N(S)=1
......
Bush held a meeting withl i i
G2: N(S)=2
G3: N(S)=3
G4: N(S)=4
Figure 4: Multi-beam structures of two bins with different translation steps (15 and 16). The first three
columns show the parsing movements in bin 15. Each dashed box is a group based on the number of
reduce actions over the new translation strings (?a meeting with? for bin 15, and ?Sharon? for bin 16).
G2 means two reduce actions have been applied. After this regrouping, we perform the pruning in two
phases: 1) keep top b states in each group, and labeled each group with the state with the highest parsing
score in this group; 2) sort the different groups, and keep top g groups.
For example, Figure 4 shows two bins with two different translation steps (15 and 16). In bin 15, the
graph shows the parsing movements after we scan three new words (?a?, ?meeting?, and ?with?). The
parsing sh action happens from a parsing state in one column to another state in the next column, while
re happens from a state to another state in the same column. The third column in bin 15 lists some partial
dependency structures that have all new words parsed. Here each dashed box is a group of hypothesis
with a same N(S ), e.g. the G2 contains all the dependency structures that have two reduce actions after
parsed all the new words. Then, we sort and prune each group by the beam size b, and each group labeled
as the highest hypothesis in this group. Finally, we sort those groups and only keep top g groups for the
future predictions. Again, in Figure 4, we can keep the whole group G3 and partial group of G2 if b = 2.
In our experiments, we set the group size g to 5.
3.5 Log-linear Model
We integrate our dependency parser into the log-linear model as an additional feature. So the decoder
searches for the best translation e? with a latent tree structure (evaluated by our Slm) according to the
following equation:
e? = argmax
e?E
exp(Slm(e) ? ws +
?
i
fi ? wi) (1)
where Slm(e) is the dependency parsing score calculated by our parser, ws is the weight of Slm(e), fi are
the features in the baseline model and wi are the weights.
1139
4 Experiments
4.1 Data Preparation
The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respec-
tively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08
(newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement
option ?grow-diag-and? (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley
parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string
translation rules. Our trigram word language model was trained on the target side of the training corpus
using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we
again parse the input sentences using the Berkeley parser, and convert them into translation forests using
rule pattern-matching (Mi et al., 2008).
Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the
same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate
training (Och, 2003) to maximize the Bleu score on the development set.
Our dependency parser is an implementation of the ?arc-standard? shift-reduce parser (Nivre, 2004),
and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training
set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and
Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We
add the structured language model as an additional feature into the baseline system.
We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the scrip-
t mteval-v13a.pl. We also report the Ter scores.
4.2 Complete Comparisons on MT08
To explore the soundness of our approach, we carry out some experiments in Table 2. With a beam size
100, the baseline decoder achieves a Bleu score of 21.06 with a speed of 1.7 seconds per sentence.
Since our dependency parser is trained on the English PTB, which is not included in the MT training
set, there is a chance that the gain of Bleu score is due to the increase of new n-grams in the PTB data.
In order to rule out this possibility, we use the tool SRILM to train another tri-gram language model on
English PTB and use it as a secondary language model for the decoder. The Bleu score is 21.10, which
is similar to the baseline result. Thus we can conclude that any gain of the following +Slm experiments
is not because of the using of the additional English PTB.
Our second experiment re-ranks the 100-best translations of the baseline with our structured language
model trained on PTB. The improvement is less than 0.2 Bleu, which is not statistically significant, as
the search space for re-ranking is relatively small compared with the decoding space.
As shown in Section 3, we have three different ways to integrate an Slm to the baseline system:
? na??ve: adding the parsing signature to the translation signature;
? best-parse: keeping the best dependency structure for each translation;
? grouping: regrouping the hypothesis by N(S ) in each bin.
The na??ve approach achieves a Bleu score of 19.12, which is significantly lower than the baseline. The
main reason is that adding parsing signatures leads to very restricted translation variance in each beam.
We also tried to increase the beam size to 1000, but we do not see any improvement.
The fourth line in Table 2 shows the result of the best-parse (Hassan et al., 2009). This approach only
slows the speed by a factor of two, but the improvement is not statistically significant. We manually
looked into some dependency trees this approach generates, and found this approach always introduce
local parsing errors.
The last line shows our efficient beam grouping scheme with a grouping size 5, it achieves a significant
improvement with an acceptable speed, which is about 6 times slower than the baseline system.
1140
System Bleu Speed
baseline 21.06 1.7
+Slm
re-ranking 21.23 1.73
na??ve 19.12 2.6
best-parse 21.30 3.4
grouping (g=5) 21.64 10.6
Table 2: Results on MT08. The bold score is significantly better than the baseline result at level p < 0.05.
System MT03 MT04 MT05 MT08 Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
baseline 19.94 10.73 22.03 18.63 19.92 11.45 21.06 10.37 12.80
+Slm 21.49 9.44 22.33 18.38 20.51 10.71 21.64 9.88 12.10
Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p < 0.5).
4.3 Final Results on All Test Sets
Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points
in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p < 0.05, using
bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times
slower than the baseline.
5 Related Work
The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways.
First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to
a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can
only tune their system on short sentences less than 20 words. Furthermore, their results are from a much
bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the
larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-the-
art tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions
beyond their work.
Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant
improvement. Different from their work, we pay more attention to the dependency parser, and we also
test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they
are suffering from the local parsing errors.
Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005)
to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental
parser remains in quadratic time.
Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008)
and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and
tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in
translation rules. Both papers integrate dependency structures into translation model, we instead model
the dependency structures with a monolingual parsing model over translation strings.
6 Conclusion
In this paper, we presented an efficient algorithm to integrate a structured language model (an incremen-
tal shift-reduce parser in specific) into an incremental tree-to-string system. We calculate the structured
language model scores incrementally at the decoding step, rather than re-scoring a complete transla-
tion. Our experiments suggest that it is important to design efficient pruning strategies, which have been
1141
overlooked in previous work. Experimental results on large-scale data set show that our approach signif-
icantly improves the translation quality at a reasonable slower speed than a state-of-the-art tree-to-string
system.
The structured language model introduced in our work only takes into account the target string, and
ignores the reordering information in the source side. Thus, our future work seeks to incorporate more
source side syntax information to guide the parsing of the target side, and tune a structured language
model for both Bleu and paring accuracy. Another potential work lies in the more efficient searching and
pruning algorithms for integration.
Acknowledgments
We thank the three anonymous reviewers for helpful suggestions, and Dan Gildea and Licheng Fang for
discussions. Yu and Liu were supported in part by CAS Action Plan for the Development of Western
China (No. KGZD-EW-501) and a grant from Huawei Noah?s Ark Lab, Hong Kong. Liu was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin C-
ity University. Huang was supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty Research
Award, and a PSC-CUNY Award, and Mi by DARPA HR0011-12-C-0015. The views and findings in
this paper are those of the authors and are not endorsed by the US or Chinese governments.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. Parsing of series in automatic computation. In The Theory of Parsing,
Translation, and Compiling, page Volume I.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine
translation. In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. volume 14, pages 283 ? 332.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of
ACL.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of ACL 2009 and AFNLP, pages 773?781, Suntec, Singapore, August.
Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In Proceed-
ings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-
ACL, pages 961?968.
Hany Hassan, Khalil Sima?an, and Andy Way. 2009. A syntactified direct translation model with linear-time de-
coding. In Proceedings of EMNLP 2009, pages 1182?1191, Singapore, August. Association for Computational
Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of ACL 2010, pages 1077?1086, Uppsala, Sweden, July. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings
of NAACL 2012, Montreal, Quebec.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of NAACL, pages 127?133.
1142
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP,
pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of COLING-ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of ACL/IJCNLP, pages 558?566, Suntec, Singapore, August.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523?530, Vancouver, British Columbia,
Canada, October.
Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of ACL, pages
1433?1442, Uppsala, Sweden, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL: HLT, pages
192?199.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational Linguis-
tics.
Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411.
Matt Post and Daniel Gildea. 2008. Language modeling with tree substitution grammars. In Proceedings of
AMTA.
Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In Proceedings of NIPS
workshop on Grammar Induction, Representation of Language, and Language Learning.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Arbor, MI, June.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of ACL 2011, pages 620?631, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
1143
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273?283,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Efficient Incremental Decoding for Tree-to-String Translation
Liang Huang 1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
{lhuang,haitaomi}@isi.edu
Haitao Mi 2,1
2Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
htmi@ict.ac.cn
Abstract
Syntax-based translation models should in
principle be efficient with polynomially-sized
search space, but in practice they are often
embarassingly slow, partly due to the cost
of language model integration. In this paper
we borrow from phrase-based decoding the
idea to generate a translation incrementally
left-to-right, and show that for tree-to-string
models, with a clever encoding of deriva-
tion history, this method runs in average-
case polynomial-time in theory, and linear-
time with beam search in practice (whereas
phrase-based decoding is exponential-time in
theory and quadratic-time in practice). Exper-
iments show that, with comparable translation
quality, our tree-to-string system (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++).
1 Introduction
Most efforts in statistical machine translation so far
are variants of either phrase-based or syntax-based
models. From a theoretical point of view, phrase-
based models are neither expressive nor efficient:
they typically allow arbitrary permutations and re-
sort to language models to decide the best order. In
theory, this process can be reduced to the Traveling
Salesman Problem and thus requires an exponential-
time algorithm (Knight, 1999). In practice, the de-
coder has to employ beam search to make it tractable
(Koehn, 2004). However, even beam search runs in
quadratic-time in general (see Sec. 2), unless a small
distortion limit (say, d=5) further restricts the possi-
ble set of reorderings to those local ones by ruling
out any long-distance reorderings that have a ?jump?
in theory in practice
phrase-based exponential quadratic
tree-to-string polynomial linear
Table 1: [main result] Time complexity of our incremen-
tal tree-to-string decoding compared with phrase-based.
In practice means ?approximate search with beams.?
longer than d. This has been the standard prac-
tice with phrase-based models (Koehn et al, 2007),
which fails to capture important long-distance re-
orderings like SVO-to-SOV.
Syntax-based models, on the other hand, use
syntactic information to restrict reorderings to
a computationally-tractable and linguistically-
motivated subset, for example those generated by
synchronous context-free grammars (Wu, 1997;
Chiang, 2007). In theory the advantage seems quite
obvious: we can now express global reorderings
(like SVO-to-VSO) in polynomial-time (as opposed
to exponential in phrase-based). But unfortunately,
this polynomial complexity is super-linear (being
generally cubic-time or worse), which is slow in
practice. Furthermore, language model integration
becomes more expensive here since the decoder now
has to maintain target-language boundary words at
both ends of a subtranslation (Huang and Chiang,
2007), whereas a phrase-based decoder only needs
to do this at one end since the translation is always
growing left-to-right. As a result, syntax-based
models are often embarassingly slower than their
phrase-based counterparts, preventing them from
becoming widely useful.
Can we combine the merits of both approaches?
While other authors have explored the possibilities
273
of enhancing phrase-based decoding with syntax-
aware reordering (Galley and Manning, 2008), we
are more interested in the other direction, i.e., can
syntax-based models learn from phrase-based de-
coding, so that they still model global reordering, but
in an efficient (preferably linear-time) fashion?
Watanabe et al (2006) is an early attempt in
this direction: they design a phrase-based-style de-
coder for the hierarchical phrase-based model (Chi-
ang, 2007). However, this algorithm even with the
beam search still runs in quadratic-time in prac-
tice. Furthermore, their approach requires grammar
transformation that converts the original grammar
into an equivalent binary-branching Greibach Nor-
mal Form, which is not always feasible in practice.
We take a fresh look on this problem and turn our
focus to one particular syntax-based paradigm, tree-
to-string translation (Liu et al, 2006; Huang et al,
2006), since this is the simplest and fastest among
syntax-based approaches. We develop an incremen-
tal dynamic programming algorithm and make the
following contributions:
? we show that, unlike previous work, our in-
cremental decoding algorithm runs in average-
case polynomial-time in theory for tree-to-
string models, and the beam search version runs
in linear-time in practice (see Table 1);
? large-scale experiments on a tree-to-string sys-
tem confirm that, with comparable translation
quality, our incremental decoder (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++) (Koehn
et al, 2007);
? furthermore, on the same tree-to-string system,
incremental decoding is slightly faster than the
standard cube pruning method at the same level
of translation quality;
? this is also the first linear-time incremental de-
coder that performs global reordering.
We will first briefly review phrase-based decod-
ing in this section, which inspires our incremental
algorithm in the next section.
2 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English to explain both phrase-based and
syntax-based decoding throughout this paper:
0 Bu`sh?? 1
Bush
yu? 2
with
Sha?lo?ng 3
Sharon
ju?x??ng 4
hold
le
-ed
5 hu?`ta?n 6
meeting
?Bush held talks with Sharon?
2.1 Basic Dynamic Programming Algorithm
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (Koehn, 2004). Each hypothesis has
a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment. This process can be formalized as
a deductive system. For example, the following de-
duction step grows a hypothesis by the phrase-pair
?yu? Sha?lo?ng, with Sharon? covering Chinese span
[1-3]:
(? ???6) : (w, ?Bush held talks?)
(???3???) : (w?, ?Bush held talks with Sharon?) (1)
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where w and
w? = w+c+d are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair,
and d being the distortion cost. To compute d we
also need to maintain the ending position of the last
phrase (the 3 and 6 in the coverage vector).
To add a bigram model, we split each ?LM item
above into a series of +LM items; each +LM item
has the form (v,a ) where a is the last word of the
hypothesis. Thus a +LM version of (1) might be:
(? ???6,talks ) : (w, ?Bush held talks?)
(???3???,Sharon ) : (w?, ?Bush held talks with Sharon?)
where the score of the resulting +LM item
w? = w + c + d? logPlm(with | talk)
now includes a combination cost due to the bigrams
formed when applying the phrase-pair. The com-
plexity of this dynamic programming algorithm for
g-gram decoding is O(2nn2|V |g?1) where n is the
sentence length and |V | is the English vocabulary
size (Huang and Chiang, 2007).
274
1 2 3 4 5
Figure 1: Beam search in phrase-based decoding expands
the hypotheses in the current bin (#2) into longer ones.
VP
PP
P
yu?
x1:NP
VP
VV
ju?x??ng
AS
le
x2:NP
? held x2 with x1
Figure 2: Tree-to-string rule r3 for reordering.
2.2 Beam Search in Practice
To make the exponential algorithm practical, beam
search is the standard approximate search method
(Koehn, 2004). Here we group +LM items into n
bins, with each bin Bi hosting at most b items that
cover exactly i Chinese words (see Figure 1). The
complexity becomes O(n2b) because there are a to-
tal of O(nb) items in all bins, and to expand each
item we need to scan the whole coverage vector,
which costs O(n). This quadratic complexity is still
too slow in practice and we often set a small distor-
tion limit of dmax (say, 5) so that no jumps longer
than dmax are allowed. This method reduces the
complexity to O(nbdmax) but fails to capture long-
distance reorderings (Galley and Manning, 2008).
3 Incremental Decoding for Tree-to-String
Translation
We will first briefly review tree-to-string translation
paradigm and then develop an incremental decoding
algorithm for it inspired by phrase-based decoding.
3.1 Tree-to-string Translation
A typical tree-to-string system (Liu et al, 2006;
Huang et al, 2006) performs translation in two
steps: parsing and decoding. A parser first parses the
source language input into a 1-best tree T , and the
decoder then searches for the best derivation (a se-
(a) Bu`sh?? [yu? Sha?lo?ng ]1 [ju?x??ng le hu?`ta?n ]2
? 1-best parser
(b) IP@?
NP@1
Bu`sh??
VP@2
PP@2.1
P
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP@2.2.3
hu?`ta?n
r1 ?
(c) NP@1
Bu`sh??
VP@2
PP@2.1
P
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP@2.2.3
hu?`ta?n
r2 ? r3 ?
(d) Bush held NP@2.2.3
hu?`ta?n
with NP@2.1.2
Sha?lo?ng
r4 ? r5 ?
(e) Bush [held talks]2 [with Sharon]1
Figure 3: An example derivation of tree-to-string trans-
lation (much simplified from Mi et al (2008)). Shaded
regions denote parts of the tree that matches the rule.
quence of translation steps) d? that converts source
tree T into a target-language string.
Figure 3 shows how this process works. The Chi-
nese sentence (a) is first parsed into tree (b), which
will be converted into an English string in 5 steps.
First, at the root node, we apply rule r1 preserving
the top-level word-order
(r1) IP (x1:NP x2:VP) ? x1 x2
which results in two unfinished subtrees, NP@1 and
VP@2 in (c). Here X@? denotes a tree node of la-
bel X at tree address ? (Shieber et al, 1995). (The
root node has address ?, and the first child of node ?
has address ?.1, etc.) Then rule r2 grabs the Bu`sh??
subtree and transliterate it into the English word
275
in theory in practice
phrase* O(2nn2 ? |V |g?1) O(n2b)
tree-to-str O(nc ? |V |4(g?1)) O(ncb2)
this work* O(nk log2(cr) ? |V |g?1) O(ncb)
Table 2: Summary of time complexities of various algo-
rithms. b is the beam width, V is the English vocabulary,
and c is the number of translation rules per node. As a
special case, phrase-based decoding with distortion limit
dmax is O(nbdmax). *: incremental decoding algorithms.
?Bush?. Similarly, rule r3 shown in Figure 2 is ap-
plied to the VP subtree, which swaps the two NPs,
yielding the situation in (d). Finally two phrasal
rules r4 and r5 translate the two remaining NPs and
finish the translation.
In this framework, decoding without language
model (?LM decoding) is simply a linear-time
depth-first search with memoization (Huang et al,
2006), since a tree of n words is also of size
O(n) and we visit every node only once. Adding
a language model, however, slows it down signifi-
cantly because we now have to keep track of target-
language boundary words, but unlike the phrase-
based case in Section 2, here we have to remember
both sides the leftmost and the rightmost boundary
words: each node is now split into +LM items like
(? a ? b) where ? is a tree node, and a and b are left
and right English boundary words. For example, a
bigram +LM item for node VP@2 might be
(VP@2 held ? Sharon).
This is also the case with other syntax-based models
like Hiero or GHKM: language model integration
overhead is the most significant factor that causes
syntax-based decoding to be slow (Chiang, 2007). In
theory +LM decoding is O(nc|V |4(g?1)), where V
denotes English vocabulary (Huang, 2007). In prac-
tice we have to resort to beam search again: at each
node we would only allow top-b +LM items. With
beam search, tree-to-string decoding with an inte-
grated language model runs in time O(ncb2), where
b is the size of the beam at each node, and c is (max-
imum) number of translation rules matched at each
node (Huang, 2007). See Table 2 for a summary.
3.2 Incremental Decoding
Can we borrow the idea of phrase-based decoding,
so that we also grow the hypothesis strictly left-
to-right, and only need to maintain the rightmost
boundary words?
The key intuition is to adapt the coverage-vector
idea from phrase-based decoding to tree-to-string
decoding. Basically, a coverage-vector keeps track
of which Chinese spans have already been translated
and which have not. Similarly, here we might need
a ?tree coverage-vector? that indicates which sub-
trees have already been translated and which have
not. But unlike in phrase-based decoding, we can
not simply choose any arbitrary uncovered subtree
for the next step, since rules already dictate which
subtree to visit next. In other words what we need
here is not really a tree coverage vector, but more of
a derivation history.
We develop this intuition into an agenda repre-
sented as a stack. Since tree-to-string decoding is a
top-down depth-first search, we can simulate this re-
cursion with a stack of active rules, i.e., rules that are
not completed yet. For example we can simulate the
derivation in Figure 3 as follows. At the root node
IP@?, we choose rule r1, and push its English-side
to the stack, with variables replaced by matched tree
nodes, here x1 for NP@1 and x2 for VP@2. So we
have the following stack
s = [ NP@1 VP@2],
where the dot  indicates the next symbol to process
in the English word-order. Since node NP@1 is the
first in the English word-order, we expand it first,
and push rule r2 rooted at NP to the stack:
[ NP@1 VP@2 ] [ Bush].
Since the symbol right after the dot in the top rule is
a word, we immediately grab it, and append it to the
current hypothesis, which results in the new stack
[ NP@1 VP@2 ] [Bush  ].
Now the top rule on the stack has finished (dot is at
the end), so we trigger a ?pop? operation which pops
the top rule and advances the dot in the second-to-
top rule, denoting that NP@1 is now completed:
[NP@1  VP@2].
276
stack hypothesis
[<s>  IP@? </s>] <s>
p [<s>  IP@? </s>] [ NP@1 VP@2] <s>
p [<s>  IP@? </s>] [ NP@1 VP@2] [ Bush] <s>
s [<s>  IP@? </s>] [ NP@1 VP@2] [Bush  ] <s> Bush
c [<s>  IP@? </s>] [NP@1  VP@2] <s> Bush
p [<s>  IP@? </s>] [NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2] <s> Bush
s [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] <s> Bush held
p [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [ talks] <s> Bush held
s [<s>  IP@? </s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [talks  ] <s> Bush held talks
c [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3  with NP@2.1.2] <s> Bush held talks
s [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] <s> Bush held talks with
p [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [ Sharon] <s> Bush held talks with
s [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [Sharon ] <s> Bush held talks with Sharon
c [<s>  IP@? </s>] [NP@1  VP@2] [held NP@2.2.3 with NP@2.1.2 ] <s> Bush held talks with Sharon
c [<s>  IP@? </s>] [NP@1 VP@2 ] <s> Bush held talks with Sharon
c [<s> IP@?  </s>] <s> Bush held talks with Sharon
s [<s> IP@? </s> ] <s> Bush held talks with Sharon </s>
Figure 4: Simulation of tree-to-string derivation in Figure 3 in the incremental decoding algorithm. Actions: p, predict;
s, scan; c, complete (see Figure 5).
Item ? : ?s, ?? : w; ?: step, s: stack, ?: hypothesis, w: weight
Equivalence ? : ?s, ?? ? ? : ?s?, ??? iff. s = s? and lastg?1(?) = lastg?1(??)
Axiom 0 : ?[<s>g?1  ? </s>], <s>g?1? : 0
Predict
? : ?... [?  ? ?], ?? : w
? + |C(r)| : ?... [?  ? ?] [ f(?,E(r))], ?? : w + c(r) match(?, C(r))
Scan
? : ?... [?  e ?], ?? : w
? : ?... [? e  ?], ?e? : w ? log Pr(e | lastg?1(?))
Complete
? : ?... [?  ? ?] [?], ?? : w
? : ?... [? ?  ?], ?? : w
Goal |T | : ?[<s>g?1 ? </s>], ?</s>? : w
Figure 5: Deductive system for the incremental tree-to-string decoding algorithm. Function lastg?1(?) returns the
rightmost g ? 1 words (for g-gram LM), and match(?, C(r)) tests matching of rule r against the subtree rooted at
node ?. C(r) and E(r) are the Chinese and English sides of rule r, and function f(?,E(r)) = [xi 7? ?.var(i)]E(r)
replaces each variable xi on the English side of the rule with the descendant node ?.var(i) under ? that matches xi.
277
The next step is to expand VP@2, and we use rule r3
and push its English-side ?VP ? held x2 with x1?
onto the stack, again with variables replaced by
matched nodes:
[NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2]
Note that this is a reordering rule, and the stack al-
ways follows the English word order because we
generate hypothesis incrementally left-to-right. Fig-
ure 4 works out the full example.
We formalize this algorithm in Figure 5. Each
item ?s, ?? consists of a stack s and a hypothesis
?. Similar to phrase-based dynamic programming,
only the last g?1 words of ? are part of the signature
for decoding with g-gram LM. Each stack is a list of
dotted rules, i.e., rules with dot positions indicting
progress, in the style of Earley (1970). We call the
last (rightmost) rule on the stack the top rule, which
is the rule being processed currently. The symbol af-
ter the dot in the top rule is called the next symbol,
since it is the symbol to expand or process next. De-
pending on the next symbol a, we can perform one
of the three actions:
? if a is a node ?, we perform a Predict action
which expands ? using a rule r that can pattern-
match the subtree rooted at ?; we push r is to
the stack, with the dot at the beginning;
? if a is an English word, we perform a Scan ac-
tion which immediately adds it to the current
hypothesis, advancing the dot by one position;
? if the dot is at the end of the top rule, we
perform a Complete action which simply pops
stack and advance the dot in the new top rule.
3.3 Polynomial Time Complexity
Unlike phrase-based models, we show here
that incremental decoding runs in average-case
polynomial-time for tree-to-string systems.
Lemma 1. For an input sentence of n words and
its parse tree of depth d, the worst-case complex-
ity of our algorithm is f(n, d) = c(cr)d|V |g?1 =
O((cr)dng?1), assuming relevant English vocabu-
lary |V | = O(n), and where constants c, r and g are
the maximum number of rules matching each tree
node, the maximum arity of a rule, and the language-
model order, respectively.
Proof. The time complexity depends (in part) on the
number of all possible stacks for a tree of depth d. A
stack is a list of rules covering a path from the root
node to one of the leaf nodes in the following form:
R1
? ?? ?
[... ?1...]
R2
? ?? ?
[... ?2...] ...
Rs
? ?? ?
[... ?s...],
where ?1 = ? is the root node and ?s is a leaf node,
with stack depth s ? d. Each rule Ri(i > 1) ex-
pands node ?i?1, and thus has c choices by the defi-
nition of grammar constant c. Furthermore, each rule
in the stack is actually a dotted-rule, i.e., it is associ-
ated with a dot position ranging from 0 to r, where r
is the arity of the rule (length of English side of the
rule). So the total number of stacks is O((cr)d).
Besides the stack, each state also maintains (g?1)
rightmost words of the hypothesis as the language
model signature, which amounts to O(|V |g?1). So
the total number of states is O((cr)d|V |g?1). Fol-
lowing previous work (Chiang, 2007), we assume
a constant number of English translations for each
foreign word in the input sentence, so |V | = O(n).
And as mentioned above, for each state, there are c
possible expansions, so the overall time complexity
is f(n, d) = c(cr)d|V |g?1 = O((cr)dng?1).
We do average-case analysis below because the
tree depth (height) for a sentence of n words is a
random variable: in the worst-case it can be linear in
n (degenerated into a linear-chain), but we assume
this adversarial situation does not happen frequently,
and the average tree depth is O(log n).
Theorem 1. Assume for each n, the depth of a
parse tree of n words, notated dn, distributes nor-
mally with logarithmic mean and variance, i.e.,
dn ? N (?n, ?2n), where ?n = O(logn) and ?2n =
O(logn), then the average-case complexity of the
algorithm is h(n) = O(nk log2(cr)+g?1) for constant
k, thus polynomial in n.
Proof. From Lemma 1 and the definition of average-
case complexity, we have
h(n) = Edn?N (?n,?2n)[f(n, dn)],
where Ex?D[?] denotes the expectation with respect
278
to the random variable x in distribution D.
h(n) = Edn?N (?n,?2n)[f(n, dn)]
= Edn?N (?n,?2n)[O((cr)
dnng?1)],
= O(ng?1Edn?N (?n,?2n)[(cr)
dn ]),
= O(ng?1Edn?N (?n,?2n)[exp(dn log(cr))]) (2)
Since dn ? N (?n, ?2n) is a normal distribution,
dn log(cr) ? N (??, ??2) is also a normal distribu-
tion, where ?? = ?n log(cr) and ?? = ?n log(cr).
Therefore exp(dn log(cr)) is a log-normal distribu-
tion, and by the property of log-normal distribution,
its expectation is exp (?? + ??2/2). So we have
Edn?N (?n,?2/2)[exp(dn log(cr))]
= exp (?? + ??2/2)
= exp (?n log(cr) + ?2n log2(cr)/2)
= exp (O(log n) log(cr) + O(log n) log2(cr)/2)
= exp (O(log n) log2(cr))
? exp (k(log n) log2(cr)), for some constant k
= exp (log nk log2(cr))
= nk log2(cr). (3)
Plug it back to Equation (2), and we have the
average-case complexity
Edn [f(n, dn)] ? O(ng?1nk log
2(cr))
= O(nk log2(cr)+g?1). (4)
Since k, c, r and g are constants, the average-case
complexity is polynomial in sentence length n.
The assumption dn ? N (O(logn), O(logn))
will be empirically verified in Section 5.
3.4 Linear-time Beam Search
Though polynomial complexity is a desirable prop-
erty in theory, the degree of the polynomial,
O(log cr) might still be too high in practice, depend-
ing on the translation grammar. To make it linear-
time, we apply the beam search idea from phrase-
based again. And once again, the only question to
decide is the choice of ?binning?: how to assign each
item to a particular bin, depending on their progress?
While the number of Chinese words covered is a
natural progress indicator for phrase-based, it does
not work for tree-to-string because, among the three
actions, only scanning grows the hypothesis. The
prediction and completion actions do not make real
progress in terms of words, though they do make
progress on the tree. So we devise a novel progress
indicator natural for tree-to-string translation: the
number of tree nodes covered so far. Initially that
number is zero, and in a prediction step which ex-
pands node ? using rule r, the number increments by
|C(r)|, the size of the Chinese-side treelet of r. For
example, a prediction step using rule r3 in Figure 2
to expand VP@2 will increase the tree-node count by
|C(r3)| = 6, since there are six tree nodes in that
rule (not counting leaf nodes or variables).
Scanning and completion do not make progress
in this definition since there is no new tree node
covered. In fact, since both of them are determin-
istic operations, they are treated as ?closure? op-
erators in the real implementation, which means
that after a prediction, we always do as many scan-
ning/completion steps as possible until the symbol
after the dot is another node, where we have to wait
for the next prediction step.
This method has |T | = O(n) bins where |T | is
the size of the parse tree, and each bin holds b items.
Each item can expand to c new items, so the overall
complexity of this beam search is O(ncb), which is
linear in sentence length.
4 Related Work
The work of Watanabe et al (2006) is closest in
spirit to ours: they also design an incremental decod-
ing algorithm, but for the hierarchical phrase-based
system (Chiang, 2007) instead. While we leave de-
tailed comparison and theoretical analysis to a future
work, here we point out some obvious differences:
1. due to the difference in the underlying trans-
lation models, their algorithm runs in O(n2b)
time with beam search in practice while ours
is linear. This is because each prediction step
now has O(n) choices, since they need to ex-
pand nodes like VP[1, 6] as:
VP[1,6] ? PP[1, i] VP[i, 6],
where the midpoint i in general has O(n)
choices (just like in CKY). In other words, their
grammar constant c becomes O(n).
2. different binning criteria: we use the number of
tree nodes covered, while they stick to the orig-
279
inal phrase-based idea of number of Chinese
words translated;
3. as a result, their framework requires gram-
mar transformation into the binary-branching
Greibach Normal Form (which is not always
possible) so that the resulting grammar always
contain at least one Chinese word in each rule
in order for a prediction step to always make
progress. Our framework, by contrast, works
with any grammar.
Besides, there are some other efforts less closely
related to ours. As mentioned in Section 1, while
we focus on enhancing syntax-based decoding with
phrase-based ideas, other authors have explored the
reverse, but also interesting, direction of enhancing
phrase-based decoding with syntax-aware reorder-
ing. For example Galley and Manning (2008) pro-
pose a shift-reduce style method to allow hiearar-
chical non-local reorderings in a phrase-based de-
coder. While this approach is certainly better than
pure phrase-based reordering, it remains quadratic
in run-time with beam search.
Within syntax-based paradigms, cube pruning
(Chiang, 2007; Huang and Chiang, 2007) has be-
come the standard method to speed up +LM de-
coding, which has been shown by many authors to
be highly effective; we will be comparing our incre-
mental decoder with a baseline decoder using cube
pruning in Section 5. It is also important to note
that cube pruning and incremental decoding are not
mutually exclusive, rather, they could potentially be
combined to further speed up decoding. We leave
this point to future work.
Multipass coarse-to-fine decoding is another pop-
ular idea (Venugopal et al, 2007; Zhang and Gildea,
2008; Dyer and Resnik, 2010). In particular, Dyer
and Resnik (2010) uses a two-pass approach, where
their first-pass, ?LM decoding is also incremental
and polynomial-time (in the style of Earley (1970)
algorithm), but their second-pass, +LM decoding is
still bottom-up CKY with cube pruning.
5 Experiments
To test the merits of our incremental decoder we
conduct large-scale experiments on a state-of-the-art
tree-to-string system, and compare it with the stan-
dard phrase-based system of Moses. Furturemore we
also compare our incremental decoder with the stan-
dard cube pruning approach on the same tree-to-
string decoder.
5.1 Data and System Preparation
Our training corpus consists of 1.5M sentence pairs
with about 38M/32M words in Chinese/English, re-
spectively. We first word-align them by GIZA++ and
then parse the Chinese sentences using the Berke-
ley parser (Petrov and Klein, 2007), then apply
the GHKM algorithm (Galley et al, 2004) to ex-
tract tree-to-string translation rules. We use SRILM
Toolkit (Stolcke, 2002) to train a trigram language
model with modified Kneser-Ney smoothing on the
target side of training corpus. At decoding time,
we again parse the input sentences into trees, and
convert them into translation forest by rule pattern-
matching (Mi et al, 2008).
We use the newswire portion of 2006 NIST MT
Evaluation test set (616 sentences) as our develop-
ment set and the newswire portion of 2008 NIST
MT Evaluation test set (691 sentences) as our test
set. We evaluate the translation quality using the
BLEU-4 metric, which is calculated by the script
mteval-v13a.pl with its default setting which is case-
insensitive matching of n-grams. We use the stan-
dard minimum error-rate training (Och, 2003) to
tune the feature weights to maximize the system?s
BLEU score on development set.
We first verify the assumptions we made in Sec-
tion 3.3 in order to prove the theorem that tree depth
(as a random variable) is normally-distributed with
O(logn) mean and variance. Qualitatively, we veri-
fied that for most n, tree depth d(n) does look like a
normal distribution. Quantitatively, Figure 6 shows
that average tree height correlates extremely well
with 3.5 log n, while tree height variance is bounded
by 5.5 log n.
5.2 Comparison with Cube pruning
We implemented our incremental decoding algo-
rithm in Python, and test its performance on the de-
velopment set. We first compare it with the stan-
dard cube pruning approach (also implemented in
Python) on the same tree-to-string system.1 Fig-
1Our implementation of cube pruning follows (Chiang,
2007; Huang and Chiang, 2007) where besides a beam size b
of unique +LM items, there is also a hard limit (of 1000) on the
280
 0
 1
 2
 3
 4
 5
 0  10  20  30  40  50  60  70
Av
er
ag
e 
De
co
di
ng
 T
im
e 
(S
ec
s)
Sentence Length
incremental
cube pruning
 29.5
 29.6
 29.7
 29.8
 29.9
 30
 30.1
 0  0.2  0.4  0.6  0.8  1  1.2  1.4
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
incremental
cube pruning
(a) decoding time against sentence length (b) BLEU score against decoding time
Figure 7: Comparison with cube pruning. The scatter plot in (a) confirms that our incremental decoding scales linearly
with sentence length, while cube pruning super-linearly (b = 50 for both). The comparison in (b) shows that at the
same level of translation quality, incremental decoding is slightly faster than cube pruning, especially at smaller beams.
 0
 5
 10
 15
 20
 25
 0  10  20  30  40  50
Tr
ee
 D
ep
th
 d
(n)
Sentence Length (n)
Avg Depth
Variance
3.5 log n
Figure 6: Mean and variance of tree depth vs. sentence
length. The mean depth clearly scales with 3.5 log n, and
the variance is bounded by 5.5 log n.
ure 7(a) is a scatter plot of decoding times versus
sentence length (using beam b = 50 for both sys-
tems), where we confirm that our incremental de-
coder scales linearly, while cube pruning has a slight
tendency of superlinearity. Figure 7(b) is a side-by-
side comparison of decoding speed versus transla-
tion quality (in BLEU scores), using various beam
sizes for both systems (b=10?70 for cube pruning,
and b=10?110 for incremental). We can see that in-
cremental decoding is slightly faster than cube prun-
ing at the same levels of translation quality, and the
difference is more pronounced at smaller beams: for
number of (non-unique) pops from priority queues.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60  70
Av
er
ag
e 
De
co
di
ng
 T
im
e 
(S
ec
s)
Sentence Length
M +?
 M 10
M 6
M 0
t2s
Figure 8: Comparison of our incremental tree-to-string
decoder with Moses in terms of speed. Moses is shown
with various distortion limits (0, 6, 10, +?; optimal: 10).
example, at the lowest levels of translation quality
(BLEU scores around 29.5), incremental decoding
takes only 0.12 seconds, which is about 4 times as
fast as cube pruning. We stress again that cube prun-
ing and incremental decoding are not mutually ex-
clusive, and rather they could potentially be com-
bined to further speed up decoding.
5.3 Comparison with Moses
We also compare with the standard phrase-based
system of Moses (Koehn et al, 2007), with stan-
dard settings except for the ttable limit, which we set
to 100. Figure 8 compares our incremental decoder
281
system/decoder BLEU time
Moses (optimal dmax=10) 29.41 10.8
tree-to-str: cube pruning (b=10) 29.51 0.65
tree-to-str: cube pruning (b=20) 29.96 0.96
tree-to-str: incremental (b=10) 29.54 0.32
tree-to-str: incremental (b=50) 29.96 0.77
Table 3: Final BLEU score and speed results on the test
data (691 sentences), compared with Moses and cube
pruning. Time is in seconds per sentence, including pars-
ing time (0.21s) for the two tree-to-string decoders.
with Moses at various distortion limits (dmax=0, 6,
10, and +?). Consistent with the theoretical anal-
ysis in Section 2, Moses with no distortion limit
(dmax = +?) scale quadratically, and monotone
decoding (dmax = 0) scale linearly. We use MERT
to tune the best weights for each distortion limit, and
dmax = 10 performs the best on our dev set.
Table 3 reports the final results in terms of BLEU
score and speed on the test set. Our linear-time
incremental decoder with the small beam of size
b = 10 achieves a BLEU score of 29.54, compara-
ble to Moses with the optimal distortion limit of 10
(BLEU score 29.41). But our decoding (including
source-language parsing) only takes 0.32 seconds a
sentences, which is more than 30 times faster than
Moses. With a larger beam of b = 50 our BLEU
score increases to 29.96, which is a half BLEU point
better than Moses, but still about 15 times faster.
6 Conclusion
We have presented an incremental dynamic pro-
gramming algorithm for tree-to-string translation
which resembles phrase-based based decoding. This
algorithm is the first incremental algorithm that runs
in polynomial-time in theory, and linear-time in
practice with beam search. Large-scale experiments
on a state-of-the-art tree-to-string decoder confirmed
that, with a comparable (or better) translation qual-
ity, it can run more than 30 times faster than the
phrase-based system of Moses, even though ours is
in Python while Moses in C++. We also showed that
it is slightly faster (and scale better) than the popular
cube pruning technique. For future work we would
like to apply this algorithm to forest-based transla-
tion and hierarchical system by pruning the first-pass
?LM forest. We would also combine cube pruning
with our incremental algorithm, and study its perfor-
mance with higher-order language models.
Acknowledgements
We would like to thank David Chiang, Kevin
Knight, and Jonanthan Graehl for discussions and
the anonymous reviewers for comments. In partic-
ular, we are indebted to the reviewer who pointed
out a crucial mistake in Theorem 1 and its proof
in the submission. This research was supported in
part by DARPA, under contract HR0011-06-C-0022
under subcontract to BBN Technologies, and under
DOI-NBC Grant N10AP20031, and in part by the
National Natural Science Foundation of China, Con-
tracts 60736014 and 90920004.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?208.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proceedings of
NAACL.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of EMNLP 2008.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA, August.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proc. NAACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL:
demonstration sesion.
282
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
Columbus, OH.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Ashish Venugopal, Andreas Zollmann, and Stephen Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL.
Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of COLING-
ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In Proceedings of ACL.
283
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 758?768,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Optimal Incremental Parsing via Best-First Dynamic Programming?
Kai Zhao1 James Cross1
1Graduate Center
City University of New York
365 Fifth Avenue, New York, NY 10016
{kzhao,jcross}@gc.cuny.edu
Liang Huang1,2
2Queens College
City University of New York
6530 Kissena Blvd, Queens, NY 11367
huang@cs.qc.cuny.edu
Abstract
We present the first provably optimal polyno-
mial time dynamic programming (DP) algo-
rithm for best-first shift-reduce parsing, which
applies the DP idea of Huang and Sagae
(2010) to the best-first parser of Sagae and
Lavie (2006) in a non-trivial way, reducing
the complexity of the latter from exponential
to polynomial. We prove the correctness of
our algorithm rigorously. Experiments con-
firm that DP leads to a significant speedup
on a probablistic best-first shift-reduce parser,
and makes exact search under such a model
tractable for the first time.
1 Introduction
Best-first parsing, such as A* parsing, makes con-
stituent parsing efficient, especially for bottom-up
CKY style parsing (Caraballo and Charniak, 1998;
Klein and Manning, 2003; Pauls and Klein, 2009).
Traditional CKY parsing performs cubic time exact
search over an exponentially large space. Best-first
parsing significantly speeds up by always preferring
to explore states with higher probabilities.
In terms of incremental parsing, Sagae and Lavie
(2006) is the first work to extend best-first search to
shift-reduce constituent parsing. Unlike other very
fast greedy parsers that produce suboptimal results,
this best-first parser still guarantees optimality but
requires exponential time for very long sentences
in the worst case, which is intractable in practice.
Because it needs to explore an exponentially large
space in the worst case, a bounded priority queue
becomes necessary to ensure limited parsing time.
?This work is mainly supported by DARPA FA8750-13-2-
0041 (DEFT), a Google Faculty Research Award, and a PSC-
CUNY Award. In addition, we thank Kenji Sagae and the
anonymous reviewers for their constructive comments.
On the other hand, Huang and Sagae (2010) ex-
plore the idea of dynamic programming, which is
originated in bottom-up constituent parsing algo-
rithms like Earley (1970), but in a beam-based non
best-first parser. In each beam step, they enable
state merging in a style similar to the dynamic pro-
gramming in bottom-up constituent parsing, based
on an equivalence relation defined upon feature val-
ues. Although in theory they successfully reduced
the underlying deductive system to polynomial time
complexity, their merging method is limited in that
the state merging is only between two states in the
same beam step. This significantly reduces the num-
ber of possible merges, because: 1) there are only
a very limited number of states in the beam at the
same time; 2) a lot of states in the beam with differ-
ent steps cannot be merged.
We instead propose to combine the idea of dy-
namic programming with the best-first search frame-
work, and apply it in shift-reduce dependency pars-
ing. We merge states with the same features set
globally to further reduce the number of possible
states in the search graph. Thus, our DP best-first al-
gorithm is significantly faster than non-DP best-first
parsing, and, more importantly, it has a polynomial
time complexity even in the worst case.
We make the following contributions:
? theoretically, we formally prove that our DP
best-first parsing reaches optimality with poly-
nomial time complexity. This is the first time
that exact search under such a probabilistic
model becomes tractable.
? more interestingly, we reveal that our dynamic
programming over shift-reduce parsing is in
parallel with the bottom-up parsers, except that
we have an extra order constraint given by the
shift action to enforce left to right generation of
758
input w0 . . . wn?1
axiom 0 : ?0, ?: 0
sh
` : ?j, S? : c
`+ 1 : ?j + 1, S|wj? : c+ scsh(j, S)
j < n
rex
` : ?j, S|s1|s0? : c
`+ 1 : ?j, S|s1
xs0? : c+ screx(j, S|s1|s0)
rey
` : ?j, S|s1|s0? : c
`+ 1 : ?j, S|s1
ys0? : c+ screy(j, S|s1|s0)
Figure 1: Deductive system of basic non-DP shift-reduce
parsing. Here ` is the step index (for beam search), S is
the stack, c is the score of the precedent, and sca(x) is
the score of action a from derivation x. See Figure 2 for
the DP version.
partial trees, which is analogous to Earley.
? practically, our DP best-first parser is only ?2
times slower than a pure greedy parser, but is
guaranteed to reach optimality. In particular,
it is ?20 times faster than a non-DP best-first
parser. With inexact search of bounded prior-
ity queue size, DP best-first search can reach
optimality with a significantly smaller priority
queue size bound, compared to non-DP best-
first parser.
Our system is based on a MaxEnt model to meet
the requirement from best-first search. We observe
that this locally trained model is not as strong as
global models like structured perceptron. With that
being said, our algorithm shows its own merits in
both theory and practice. To find a better model for
best-first search would be an interesting topic for fu-
ture work.
2 Shift-Reduce and Best-First Parsing
In this section we review the basics of shift-reduce
parsing, beam search, and the best-first shift-reduce
parsing algorithm of Sagae and Lavie (2006).
2.1 Shift-Reduce Parsing and Beam Search
Due to space constraints we will assume some ba-
sic familiarity with shift-reduce parsing; see Nivre
(2008) for details. Basically, shift-reduce parsing
(Aho and Ullman, 1972) performs a left-to-right
scan of the input sentence, and at each step, chooses
either to shift the next word onto the stack, or to re-
duce, i.e., combine the top two trees on stack, ei-
ther with left as the root or right as the root. This
scheme is often called ?arc-standard? in the litera-
ture (Nivre, 2008), and is the basis of several state-
of-the-art parsers, e.g. Huang and Sagae (2010). See
Figure 1 for the deductive system of shift-reduce de-
pendency parsing.
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search (Zhang
and Clark, 2008), where b derivations develop in
parallel. At each step we extend the derivations in
the current beam by applying each of the three ac-
tions, and then choose the best b resulting deriva-
tions for the next step.
2.2 Best-First Shift-Reduce Parsing
Sagae and Lavie (2006) present the parsing prob-
lem as a search problem over a DAG, in which each
parser derivation is denoted as a node, and an edge
from node x to node y exists if and only if the corre-
sponding derivation y can be generated from deriva-
tion x by applying one action.
The best-first parsing algorithm is an applica-
tion of the Dijkstra algorithm over the DAG above,
where the score of each derivation is the priority.
Dijkstra algorithm requires the priority to satisfy
the superiority property, which means a descendant
derivation should never have a higher score than its
ancestors. This requirement can be easily satisfied if
we use a generative scoring model like PCFG. How-
ever, in practice we use a MaxEnt model. And we
use the negative log probability as the score to sat-
isfy the superiority:
x ? y ? x.score < y.score,
where the order x ? y means derivation x has a
higher priority than y.1
The vanilla best-first parsing algorithm inher-
its the optimality directly from Dijkstra algorithm.
However, it explores exponentially many derivations
to reach the goal configuration in the worst case.
We propose a new method that has polynomial time
complexity even in the worst case.
1For simplicity we ignore the case when two derivations
have the same score. In practice we can choose either one of
the two derivations when they have the same score.
759
3 Dynamic Programming for Best-First
Shift-Reduce Parsing
3.1 Dynamic Programming Notations
The key innovation of this paper is to extend best-
first parsing with the ?state-merging? method of dy-
namic programming described in Huang and Sagae
(2010). We start with describing a parsing configu-
ration as a non-DP derivation:
?i, j, ...s2s1s0?,
where ...s2s1s0 is the stack of partial trees, [i..j] is
the span of the top tree s0, and s1s2... are the re-
mainder of the trees on the stack.
The notation fk(sk) is used to indicate the features
used by the parser from the tree sk on the stack. Note
that the parser only extracts features from the top
d+1 trees on the stack.
Following Huang and Sagae (2010), f?(x) of a
derivation x is called atomic features, defined as the
smallest set of features s.t.
f?(i, j, ...s2s1s0) = f?(i, j, ...s?2s
?
1s
?
0)
? fk(sk) = fk(s
?
k), ?k ? [0, d].
The atomic feature function f?(?) defines an equiv-
alence relation ? in the space of derivations D:
?i, j, ...s2s1s0? ? ?i, j, ...s
?
2s
?
1s
?
0?
? f?(i, j, ...s2s1s0) = f?(i, j, ...s?2s
?
1s
?
0)
This implies that any derivations with the same
atomic features are in the same equivalence class,
and their behaviors are similar in shift and reduce.
We call each equivalence class a DP state. More
formally we define the space of all states S as:
S
?
= D/?.
Since only the top d+1 trees on the stack are used
in atomic features, we only need to remember the
necessary information and write the state as:
?i, j, sd...s0?.
We denote a derivation x?s state as [x]?. In the rest
of this paper, we always denote derivations with let-
ters x, y, and z, and denote states with letters p, q,
and r.
The deductive system for dynamic programming
best-first parsing is adapted from Huang and Sagae
(2010). (See the left of Figure 2.) The difference is
that we do not distinguish the step index of a state.
This deductive system describes transitions be-
tween states. However, in practice we use one state?s
best derivation found so far to represent the state.
For each state p, we calculate the prefix score, p.pre,
which is the score of the derivation to reach this
state, and the inside score, p.ins , which is the score
of p?s top tree p.s0. In addition we denote the shift
score of state p as p.sh
?
= scsh(p), and the reduce
score of state p as p.re
?
= scre(p). Similarly we
have the prefix score, inside score, shift score, and
reduce score for a derivation.
With this deductive system we extend the concept
of reducible states with the following definitions:
The set of all states with which a state p can
legally reduce from the right is denoted L(p), or left
states. (see Figure 3 (a)) We call any state q ? L(p)
a left state of p. Thus each element of this set would
have the following form:
L(?i, j, sd...s0?)
?
={?h, i, s?d...s
?
0? |
fk(s
?
k?1)=fk(sk), ?k ? [1, d]} (1)
in which the span of the ?left? state?s top tree ends
where that of the ?right? state?s top tree begins, and
fk(sk) = fk(s?k?1) for all k ? [1, d].
Similarly, the set of all states with which a state p
can legally reduce from the left is denoted R(p), or
right states. (see Figure 3 (a)) For two states p, q,
p ? L(q)? q ? R(p)
3.2 Algorithm 1
We constrain the searching time with a polynomial
bound by transforming the original search graph
with exponentially many derivations into a graph
with polynomial number of states.
In Algorithm 1, we maintain a chart C and a prior-
ity queue Q , both of which are based on hash tables.
Chart C can be formally defined as a function
mapping from the space of states to the space of
derivations:
C : S ? D.
In practice, we use the atomic features f?(p) as the
signature of state p, since all derivations in the same
state share the same atomic features.
760
sh
state p:
? , j, sd...s0?: (c, )
?j, j + 1, sd?1...s0, wj? : (c+ ?, 0)
j < n PRED
? , j, A? ?.B?? : (c, )
?j, j, B ? .?? : (c+s, s)
(B ? ?) ? G
rex
state q:
?k, i, s?d...s
?
0?: (c
?, v?)
state p:
?i, j, sd...s0?: ( , v)
?k, j, s?d...s
?
1, s
?
0
xs0? : (c
?+v+?, v?+v+?)
q ? L(p) COMP
?k, i, A??.B?? : (c?, v?) ?i, j, B? : ( , v)
?k, j, A? ?B.?? : (c?+v, v?+v)
Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting
rey case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ? = scsh(p), ? = scsh(q) + screx(p),
s = sc(B ? ?), G is the set of CFG rules, ?i, j, B? is a surrogate for any ?i, j, B ? ?.?, and is a wildcard that
matches anything.
. . .
L(p)
sh sh
. . .
R(p)p
. . .
L(p)
sh
. . .
T (p)p
(a) L(p) andR(p) (b) T (p) = R(L(p))
Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set
of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states
R(p) is the set of states that can be reduced with p so that p.s0 will be the left child of the top tree of the result state.
(b) Left corner states T (p) is the set of states that have the same reducibility as shifted state p, i.e., ?p? ? L(p), we
have ?q ? T (p), q ? R(p?). In both (a) and (b), thick sh arrow means shifts from multiple states; thin sh arrow means
shift from a single state.
We use C [p] to retrieve the derivation in C that
is associated with state p. We sometimes abuse this
notation to say C [x] to retrieve the derivation asso-
ciated with signature f?(x) for derivation x. This is
fine since we know derivation x?s state immediately
from the signature. We say state p ? C if f?(p) is
associated with some derivation in C . A derivation
x ? C if C [x] = x. Chart C supports operation
PUSH, denoted as C [x]? x, which associate a sig-
nature f?(x) with derivation x.
Priority queue Q is defined similarly as C , except
that it supports the operation POP that pops the high-
est priority item.
Following Stolcke (1995) and Nederhof (2003),
we use the prefix score and the inside score as the
priority in Q :
x ? y ? x.pre < y.pre or
(x.pre = y.pre and x.ins < y.ins), (2)
Note that, for simplicity, we again ignore the spe-
cial case when two derivations have the same prefix
score and inside score. In practice for this case we
can pick either one of them. This will not affect the
correctness of our optimality proof in Section 5.1.
In the DP best-first parsing algorithm, once a
derivation x is popped from the priority queue Q ,
as usual we try to expand it with shift and reduce.
Note that both left and right reduces are between
the derivation x of state p = [x]? and an in-chart
derivation y of left state q = [y]? ? L(p) (Line 10
of Algorithm 1), as shown in the deductive system
(Figure 2). We call this kind of reduction left expan-
sion.
We further expand derivation x of state p with
some in-chart derivation z of state r s.t. p ? L(r),
i.e., r ? R(p) as in Figure 3 (a). (see Line 11 of
Algorithm 1.) Derivation z is in the chart because it
is the descendant of some other derivation that has
been explored before x. We call this kind of reduc-
tion right expansion.
Our reduction with L andR is inspired by Neder-
hof (2003) and Knuth (1977) algorithm, which will
be discussed in Section 4.
761
Algorithm 1 Best-First DP Shift-Reduce Parsing.
Let LC (x)
?
= C [L([x]?)] be in-chart derivations of
[x]??s left states
Let RC (x)
?
= C [R(p)] be in-chart derivations of
[x]??s right states
1: function PARSE(w0 . . . wn?1)
2: C ? ? . empty chart
3: Q ? {INIT} . initial priority queue
4: while Q 6= ? do
5: x? POP(Q)
6: if GOAL(x) then return x . found best parse
7: if [x]? 6? C then
8: C [x]? x . add x to chart
9: SHIFT(x,Q)
10: REDUCE(LC (x), {x},Q) . left expansion
11: REDUCE({x},RC (x),Q) . right expansion
12: procedure SHIFT(x,Q)
13: TRYADD(sh(x),Q) . shift
14: procedure REDUCE(A,B,Q)
15: for (x, y) ? A?B do . try all possible pairs
16: TRYADD(rex(x, y),Q) . left reduce
17: TRYADD(rey(x, y),Q) . right reduce
18: function TRYADD(x, Q)
19: if [x]? 6? Q or x ? Q[x] then
20: Q[x]? x . insert x into Q or update Q[x]
3.3 Algorithm 2: Lazy Expansion
We further improve DP best-first parsing with lazy
expansion.
In Algorithm 2 we only show the parts that are
different from Algorithm 1.
Assume a shifted derivation x of state p is a direct
descendant from derivation x? of state p?, then p ?
R(p?), and we have:
?ys.t . [y]? = q ? REDUCE({p
?},R(p?)), x ? y
which is proved in Section 5.1.
More formally, we can conclude that
?ys.t . [y]? = q ? REDUCE(L(p), T (p)), x ? y
where T (p) is the left corner states of shifted state
p, defined as
T (?i, i+1, sd...s0?)
?
={?i, h, s?d...s
?
0? |
fk(s
?
k)=fk(sk), ?k ? [1, d]}
which represents the set of all states that have the
same reducibility as a shifted state p. In other words,
T (p) = R(L(p)),
Algorithm 2 Lazy Expansion of Algorithm 1.
Let TC (x)
?
= C [T ([x]?)] be in-chart derivations of
[x]??s left-corner states
1: function PARSE(w0 . . . wn?1)
2: C ? ? . empty chart
3: Q ? {INIT} . initial priority queue
4: while Q 6= ? do
5: x? POP(Q)
6: if GOAL(x) then return x . found best parse
7: if [x]? 6? C then
8: C [x]? x . add x to chart
9: SHIFT(x,Q)
10: REDUCE(x.lefts, {x},Q) . left expansion
11: else if x.action is sh then
12: REDUCE(x.lefts, TC (x),Q) . right expan.
13: procedure SHIFT(x,Q)
14: y ? sh(x)
15: y.lefts ? {x} . initialize lefts
16: TRYADD(y,Q)
17: function TRYADD(x, Q)
18: if [x]? ? Q then
19: if x.action is sh then . maintain lefts
20: y ? Q[x]
21: if x ? y then Q[x]? x
22: Q[x].lefts ? y.lefts ? x.lefts
23: else if x ? Q[x] then
24: Q[x]? x
25: else . x 6? Q
26: Q[x]? x
which is illustrated in Figure 3 (a). Intuitively, T (p)
is the set of states that have p?s top tree, p.s0, which
contains only one node, as the left corner.
Based on this observation, we can safely delay the
REDUCE({x},RC (x)) operation (Line 11 in Algo-
rithm 1), until the derivation x of a shifted state is
popped out from Q . This helps us eliminate unnec-
essary right expansion.
We can delay even more derivations by extending
the concept of left corner states to reduced states.
Note that for any two states p, q, if q?s top tree q.s0
has p?s top tree p.s0 as left corner, and p, q share the
same left states, then derivations of p should always
have higher priority than derivations of q. We can
further delay the generation of q?s derivations until
p?s derivations are popped out.2
2We did not implement this idea in experiments due to its
complexity.
762
4 Comparison with Best-First CKY and
Best-First Earley
4.1 Best-First CKY and Knuth Algorithm
Vanilla CKY parsing can be viewed as searching
over a hypergraph(Klein and Manning, 2005), where
a hyperedge points from two nodes x, y to one node
z, if x, y can form a new partial tree represented by
z. Best-first CKY performs best-first search over
the hypergraph, which is a special application of the
Knuth Algorithm (Knuth, 1977).
Non-DP best-first shift-reduce parsing can be
viewed as searching over a graph. In this graph, a
node represents a derivation. A node points to all its
possible descendants generated from shift and left
and right reduces. This graph is actually a tree with
exponentially many nodes.
DP best-first parsing enables state merging on
the previous graph. Now the nodes in the hyper-
graph are not derivations, but equivalence classes of
derivations, i.e., states. The number of nodes in the
hypergraph is no longer always exponentially many,
but depends on the equivalence function, which is
the atomic feature function f?(?) in our algorithms.
DP best-first shift-reduce parsing is still a special
case of the Knuth algorithm. However, it is more dif-
ficult than best-first CKY parsing, because of the ex-
tra topological order constraints from shift actions.
4.2 Best-First Earley
DP best-first shift-reduce parsing is analogous to
weighted Earley (Earley, 1970; Stolcke, 1995), be-
cause: 1) in Earley the PRED rule generates states
similar to shifted states in shift-reduce parsing; and,
2) a newly completed state also needs to check all
possible left expansions and right expansions, simi-
lar to a state popped from the priority queue in Al-
gorithm 1. (see Figure 2)
Our Algorithm 2 exploits lazy expansion, which
reduces unnecessary expansions, and should be
more efficient than pure Earley.
5 Optimality and Polynomial Complexity
5.1 Proof of Optimality
We define a best derivation of state [x]? as a deriva-
tion x such that ?y ? [x]?, x  y.
Note that each state has a unique feature signa-
ture. We want to prove that Algorithm 1 actually fills
the chart by assigning a best derivation to its state.
Without loss of generality, we assume Algorithm 1
fills C with derivations in the following order:
x0, x1, x2, . . . , xm
where x0 is the initial derivation, xm is the first goal
derivation in the sequence, and C [xi] = xi, 0 ? i ?
m. Denote the status of chart right after xk being
filled as Ck. Specially, we define C?1 = ?
However, we do not have superiority as in non-DP
best-first parsing. Because we use a pair of prefix
score and inside score, (pre, ins), as priority (Equa-
tion 2) in the deductive system (Figure 2). We have
the following property as an alternative for superior-
ity:
Lemma 1. After derivation xk has been filled into
chart, ?x s.t. x ? Q , and x is a best derivation
of state [x]?, then x?s descendants can not have a
higher priority than xk.
Proof. Note that when xk pops out, x is still in Q ,
so xk  x. Assume z is x?s direct descendant.
? If z = sh(x) or z = re(x, ), based on the de-
ductive system, x ? z, so xk  x ? z.
? If z = re(y, x), y ? L(x), assume z ? xk.
z.pre = y.pre + y.sh + x.ins + x.re
We can construct a new derivation x? ? x by
appending x?s top tree, x.s0 to y?s stack, and
x?.pre = y.pre + y.sh + x.ins < z.pre
So x? ? z ? xk  x, which contradicts that x
is a best derivation of its state.
With induction we can easily show that any descen-
dants of x can not have a higher priority than xk.
We can now derive:
Theorem 1 (Stepwise Completeness and Optimal-
ity). For any k, 0 ? k ? m, we have the following
two properties:
?x ? xk, [x]? ? Ck?1 (Stepwise Completeness)
?x ? xk, xk  x (Stepwise Optimality)
763
Proof. We prove by induction on k.
1. For k = 0, these two properties trivially hold.
2. Assume this theorem holds for k = 2, ..., i?1.
For k = i, we have:
a) [Proof for Stepwise Completeness]
(Proof by Contradiction) Assume ?x ? xi
s.t. [x]? 6? Ci?1.Without loss of generality we
take a best derivation of state [x]? as x. x must
be derived from other best derivations only.
Consider this derivation transition hypergraph,
which starts at initial derivation x0 ? Ci?1, and
ends at x 6? Ci?1.
There must be a best derivation x? in this tran-
sition hypergraph, s.t. all best parent deriva-
tion(s) of x? are in Ci?1, but not x?.
If x? is a reduced derivation, assume x??s best
parent derivations are y ? Ci?1, z ? Ci?1.
Because y and z are best derivations, and they
are in Ci?1, from Stepwise Optimality on k =
1, ..., i? 1, y, z ? {x0, x1, . . . , xi?1}. From
Line 7-11 in Algorithm 1, x? must have been
pushed into Q when the latter of y, z is popped.
If x? is a shifted derivation, similarly x? must
have been pushed into Q .
As x? 6? Ci?1, x? must still be in Q when xi is
popped. However, from Lemma 1, none of x??s
descendants can have a higher priority than xi,
which contradicts x ? xi.
b) [Proof for Stepwise Optimality]
(Proof by Contradiction) Assume ?x ? xi
s.t. x ? xi. From Stepwise Completeness on
k = 1, ..., i, x ? Ci?1, which means the state
[xi]? has already been assigned to x, contra-
dicting the premise that xi is pushed into chart.
Both of the two properties have very intuitive
meanings. Stepwise Optimality means Algorithm 1
only fills chart with a best derivation for each state.
Stepwise Completeness means every state that has
its best derivation better than best derivation pi must
have been filled before pi, this guarantees that the
rex
?h??, h
?
k...i
? : (c?, v?) ?h?, h
i...j
? : ( , v)
?h??, h
k...j
? : (c? + v + ?, v? + v + ?)
Figure 4: Example of shift-reduce with dynamic pro-
gramming: simulating an edge-factored model. GSS
is implicit here, and rey case omitted. Here ? =
scsh(h??, h?) + screx(h
?, h).
global best goal derivation is captured by Algo-
rithm 1.
More formally we have:
Theorem 2 (Optimality of Algorithm 1). The first
goal derivation popped off the priority queue is the
optimal parse.
Proof. (Proof by Contradiction.) Assume ?x, x is
the a goal derivation and x ? xm. Based on Step-
wise Completeness of Theorem 1, x ? Cm?1, thus x
has already been popped out, which contradicts that
xm is the first popped out goal derivation.
Furthermore, we can see our lazy expansion ver-
sion, i.e., Algorithm 2, is also optimal. The key ob-
servation is that we delay the reduction of derivation
x? and a derivation of right states R([x?]?) (Line 11
of Algorithm 1), until shifted derivation, x = sh(x?),
is popped out (Line 11 of Algorithm 2). However,
this delayed reduction will not generate any deriva-
tion y, s.t. y ? x, because, based on our deduc-
tive system (Figure 2), for any such kind of reduced
derivations y, y.pre = x?.pre+x?.sh+y.re+y.ins ,
while x.pre = x?.pre + x?.sh .
5.2 Analysis of Time and Space Complexity
Following Huang and Sagae (2010) we present the
complexity analysis for our DP best-first parsing.
Theorem 3. Dynamic programming best-first pars-
ing runs in worst-case polynomial time and space,
as long as the atomic features function satisfies:
? bounded: ? derivation x, |?f(x)| is bounded by
a constant.
? monotonic:
764
? horizontal: ?k, fk(s) = fk(t) ?
fk+1(s) = fk+1(t), for all possible trees
s, t.
? vertical: ?k, fk(sys?) = fk(tyt?) ?
fk(s) = fk(t) and fk(sxs?) = fk(txt?)?
fk(s?) = fk(t?), for all possible trees s, s?,
t, t?.
In the above theorem, boundness means we can
only extract finite information from a derivation, so
that the atomic feature function f?(?) can only dis-
tinguish a finite number of different states. Mono-
tonicity requires the feature representation fk sub-
sumes fk+1. This is necessary because we use the
features as signature to match all possible left states
and right states (Equation 1). Note that we add the
vertical monotonicity condition following the sug-
gestion from Kuhlmann et al (2011), which fixes
a flaw in the original theorem of Huang and Sagae
(2010).
We use the edge-factored model (Eisner, 1996;
McDonald et al, 2005) with dynamic programming
described in Figure 4 as a concrete example for com-
plexity analysis. In the edge-factored model the fea-
ture set consists of only combinations of informa-
tion from the roots of the two top trees s1, s0, and
the queue. So the atomic feature function is
f?(p) = (i, j, h(p.s1), h(p.s0))
where h(s) returns the head word index of tree s.
The deductive system for the edge-factored model
is in Figure 4. The time complexity for this deduc-
tive system is O(n6), because we have three head
indexes and three span indexes as free variables in
the exploration. Compared to the work of Huang
and Sagae (2010), we reduce the time complexity
from O(n7) to O(n6) because we do not need to
keep track of the number of the steps for a state.
6 Experiments
In experiments we compare our DP best-first parsing
with non-DP best-first parsing, pure greedy parsing,
and beam parser of Huang and Sagae (2010).
Our underlying MaxEnt model is trained on the
Penn Treebank (PTB) following the standard split:
Sections 02-21 as the training set and Section 22 as
the held-out set. We collect gold actions at differ-
ent parsing configurations as positive examples from
model score accuracy # states time
greedy ?1.4303 90.08% 125.8 0.0055
beam? ?1.3302 90.60% 869.6 0.0331
non-DP ?1.3269 90.70% 4, 194.4 0.2622
DP ?1.3269 90.70% 243.2 0.0132
Table 1: Dynamic programming best-first parsing reach
optimality faster. *: for beam search we use beam size of
8. (All above results are averaged over the held-out set.)
gold parses in PTB to train the MaxEnt model. We
use the feature set of Huang and Sagae (2010).
Furthermore, we reimplemented the beam parser
with DP of Huang and Sagae (2010) for compari-
son. The result of our implementation is consistent
with theirs. We reach 92.39% accuracy with struc-
tured perceptron. However, in experiments we still
use MaxEnt to make the comparison fair.
To compare the performance we measure two sets
of criteria: 1) the internal criteria consist of the
model score of the parsing result, and the number
of states explored; 2) the external criteria consist of
the unlabeled accuracy of the parsing result, and the
parsing time.
We perform our experiments on a computer with
two 3.1GHz 8-core CPUs (16 processors in total)
and 64GB RAM. Our implementation is in Python.
6.1 Search Quality & Speed
We first compare DP best-first parsing algorithm
with pure greedy parsing and non-DP best-first pars-
ing without any extra constraints.
The results are shown in Table 1. Best-first pars-
ing reaches an accuracy of 90.70% in the held-out
set. Since that the MaxEnt model is locally trained,
this accuracy is not as high as the best shift-reduce
parsers available now. However, this is sufficient for
our comparison, because we aim at improving the
search quality and efficiency of parsing.
Compared to greedy parsing, DP best-first pars-
ing reaches a significantly higher accuracy, with ?2
times more parsing time. Given the extra time in
maintaining priority queue, this is consistent with
the internal criteria: DP best-first parsing reaches a
significantly higher model score, which is actually
optimal, exploring twice as many as states.
On the other hand, non-DP best-first parsing also
achieves the optimal model score and accuracy.
765
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0  10  20  30  40  50  60  70
a
vg
. p
ar
sin
g 
tim
e 
(se
cs
)
sentence length
non-DP
DP
beam
Figure 5: DP best-first significantly reduces parsing time.
Beam parser (beam size 8) guarantees linear parsing time.
Non-DP best-first parser is fast for short sentences, but
the time grows exponentially with sentence length. DP
best-first parser is as fast as non-DP for short sentences,
but the time grows significantly slower.
However, it explores?17 times more states than DP,
with an unbearable average time.
Furthermore, on average our DP best-first parsing
is significantly faster than the beam parser, because
most sentences are short.
Figure 5 explains the inefficiency of non-DP best-
first parsing. As the time complexity grows expo-
nentially with the sentence length, non-DP best-first
parsing takes an extremely long time for long sen-
tences. DP best-first search has a polynomial time
bound, which grows significantly slower.
In general DP best-first parsing manages to reach
optimality in tractable time with exact search. To
further investigate the potential of this DP best-
first parsing, we perform inexact search experiments
with bounded priority queue.
6.2 Parsing with Bounded Priority Queue
Bounded priority queue is a very practical choice
when we want to parse with only limited memory.
We bound the priority queue size at 1, 2, 5, 10,
20, 50, 100, 500, and 1000, and once the priority
queue size exceeds the bound, we discard the worst
one in the priority queue. The performances of non-
DP best-first parsing and DP best-first parsing are
illustrated in Figure 6 (a) (b).
Firstly, in Figure 6 (a), our DP best-first pars-
ing reaches the optimal model score with bound
50, while non-DP best-first parsing fails even with
bound 1000. Also, in average with bound 1000,
compared to non-DP, DP best-first only needs to ex-
plore less than half of the number of states.
Secondly, for external criteria in Figure 6 (b), both
algorithms reach accuracy of 90.70% in the end. In
speed, with bound 1000, DP best-first takes ?1/3
time of non-DP to parse a sentence in average.
Lastly, we also compare to beam parser with beam
size 1, 2, 4, 8. Figure 6 (a) shows that beam parser
fails to reach the optimality, while exploring signif-
icantly more states. On the other hand, beam parser
also fails to reach an accuracy as high as best-first
parsers. (see Figure 6 (b))
6.3 Simulating the Edge-Factored Model
We further explore the potential of DP best-first
parsing with the edge-factored model.
The simplified feature set of the edge-factored
model reduces the number of possible states, which
means more state-merging in the search graph. We
expect more significant improvement from our DP
best-first parsing in speed and number of explored
states.
Experiment results confirms this. In Figure 6 (c)
(d), curves of DP best-first diverge from non-DP
faster than standard model (Figure 6 (a) (b)).
7 Conclusions and Future Work
We have presented a dynamic programming algo-
rithm for best-first shift-reduce parsing which is
guaranteed to return the optimal solution in poly-
nomial time. This algorithm is related to best-first
Earley parsing, and is more sophisticated than best-
first CKY. Experiments have shown convincingly
that our algorithm leads to significant speedup over
the non-dynamic programming baseline, and makes
exact search tractable for the first-time under this
model.
For future work we would like to improve the per-
formance of the probabilistic models that is required
by the best-first search. We are also interested in
exploring A* heuristics to further speed up our DP
best-first parsing.
766
-1.45
-1.4
-1.35
-1.3
 0  100 200 300 400 500 600 700 800 900
a
v
g.
 m
od
el
 s
co
re
 o
n 
he
ld
-o
ut
# of states
bound=50 bound=1000
beam=8
non-DP
DP
beam
-1.3269
 90
 90.2
 90.4
 90.6
 90.8
 0  0.01  0.02  0.03  0.04  0.05
a
v
g.
 a
cc
ur
ac
y 
(%
) o
n 
he
ld
-o
ut
parsing time (secs)
bound=50
bound=1000
beam=8
non-DP
DP
beam
90.70
(a) search quality vs. # of states (b) parsing accuracy vs. time
-1.4
-1.36
-1.32
-1.28
-1.24
 0  100 200 300 400 500 600 700 800 900
a
v
g.
 m
od
el
 s
co
re
 o
n 
he
ld
-o
ut
# of states
bound=20 bound=500 beam=8
non-DP
DP
beam
-1.2565
 89.8
 90
 90.2
 0  0.01  0.02  0.03  0.04  0.05
a
v
g.
 a
cc
ur
ac
y 
(%
) o
n 
he
ld
-o
ut
parsing time (secs)
bound=20 bound=500
beam=8
non-DP
DP
beam
90.25
(c) search quality vs. # of states (edge-factored) (d) parsing accuracy vs. time (edge-factored)
Figure 6: Parsing performance comparison between DP and non-DP. (a) (b) Standard model with features of Huang
and Sagae (2010). (c) (d) Simulating edge-factored model with reduced feature set based on McDonald et al (2005).
Note that to implement bounded priority queue we use two priority queues to keep track of the worst elements, which
introduces extra overhead, so that our bounded parser is slower than the unbounded version for large priority queue
size bound.
767
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing of Series in Automatic Computation. Prentice
Hall, Englewood Cliffs, New Jersey.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24(2):275?298.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Dan Klein and Christopher D Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of HLT-NAACL.
Dan Klein and Christopher D Manning. 2005. Pars-
ing and hypergraphs. In New developments in parsing
technology, pages 351?372. Springer.
Donald Knuth. 1977. A generalization of Dijkstra?s al-
gorithm. Information Processing Letters, 6(1).
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, pages 135?143.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 557?565. Association for
Computational Linguistics.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of ACL.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165?
201.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP.
768
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 908?913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Online Learning for Inexact Hypergraph Search
Hao Zhang
Google
haozhang@google.com
Liang Huang Kai Zhao
City University of New York
{lhuang@cs.qc,kzhao@gc}.cuny.edu
Ryan McDonald
Google
ryanmcd@google.com
Abstract
Online learning algorithms like the percep-
tron are widely used for structured predic-
tion tasks. For sequential search problems,
like left-to-right tagging and parsing, beam
search has been successfully combined with
perceptron variants that accommodate search
errors (Collins and Roark, 2004; Huang et
al., 2012). However, perceptron training with
inexact search is less studied for bottom-up
parsing and, more generally, inference over
hypergraphs. In this paper, we generalize
the violation-fixing perceptron of Huang et
al. (2012) to hypergraphs and apply it to the
cube-pruning parser of Zhang and McDonald
(2012). This results in the highest reported
scores on WSJ evaluation set (UAS 93.50%
and LAS 92.41% respectively) without the aid
of additional resources.
1 Introduction
Structured prediction problems generally deal with
exponentially many outputs, often making exact
search infeasible. For sequential search problems,
such as tagging and incremental parsing, beam
search coupled with perceptron algorithms that ac-
count for potential search errors have been shown
to be a powerful combination (Collins and Roark,
2004; Daume? and Marcu, 2005; Zhang and Clark,
2008; Huang et al, 2012). However, sequen-
tial search algorithms, and in particular left-to-right
beam search (Collins and Roark, 2004; Zhang and
Clark, 2008), squeeze inference into a very narrow
space. To address this, Huang (2008) formulated
constituency parsing as approximate bottom-up in-
ference in order to compactly represent an exponen-
tial number of outputs while scoring features of ar-
bitrary scope. This idea was adapted to graph-based
dependency parsers by Zhang and McDonald (2012)
and shown to outperform left-to-right beam search.
Both these examples, bottom-up approximate de-
pendency and constituency parsing, can be viewed
as specific instances of inexact hypergraph search.
Typically, the approximation is accomplished by
cube-pruning throughout the hypergraph (Chiang,
2007). Unfortunately, as the scope of features at
each node increases, the inexactness of search and
its negative impact on learning can potentially be ex-
acerbated. Unlike sequential search, the impact on
learning of approximate hypergraph search ? as well
as methods to mitigate any ill effects ? has not been
studied. Motivated by this, we develop online learn-
ing algorithms for inexact hypergraph search by gen-
eralizing the violation-fixing percepron of Huang et
al. (2012). We empirically validate the benefit of
this approach within the cube-pruning dependency
parser of Zhang and McDonald (2012).
2 Structured Perceptron for Inexact
Hypergraph Search
The structured perceptron algorithm (Collins, 2002)
is a general learning algorithm. Given training in-
stances (x, y?), the algorithm first solves the decod-
ing problem y? = argmaxy?Y(x)w ? f(x, y) given
the weight vector w for the high-dimensional fea-
ture representation f of the mapping (x, y), where
y? is the prediction under the current model, y? is the
gold output and Y(x) is the space of all valid outputs
for input x. The perceptron update rule is simply:
w? = w + f(x, y?) ? f(x, y?).
The convergence of original perceptron algorithm
relies on the argmax function being exact so that
the conditionw ?f(x, y?) > w ?f(x, y?) (modulo ties)
always holds. This condition is called a violation
because the prediction y? scores higher than the cor-
rect label y?. Each perceptron update moves weights
908
A B C D E F
G H I J
K L
M
N
Figure 1: A hypergraph showing the union of the gold
and Viterbi subtrees. The hyperedges in bold and dashed
are from the gold and Viterbi trees, respectively.
away from y? and towards y? to fix such violations.
But when search is inexact, y? could be suboptimal
so that sometimes w ? f(x, y?) < w ? f(x, y?). Huang
et al (2012) named such instances non-violations
and showed that perceptron model updates for non-
violations nullify guarantees of convergence. To ac-
count for this, they generalized the original update
rule to select an output y? within the pruned search
space that scores higher than y?, but is not necessar-
ily the highest among all possibilities, which repre-
sents a true violation of the model on that training
instance. This violation fixing perceptron thus re-
laxes the argmax function to accommodate inexact
search and becomes provably convergent as a result.
In the sequential cases where y? has a linear struc-
ture such as tagging and incremental parsing, the
violation fixing perceptron boils down to finding
and updating along a certain prefix of y?. Collins
and Roark (2004) locate the earliest position in a
chain structure where y?pref is worse than y?pref by
a margin large enough to cause y? to be dropped
from the beam. Huang et al (2012) locate the po-
sition where the violation is largest among all pre-
fixes of y?, where size of a violation is defined as
w ? f(x, y?pref) ? w ? f(x, y?pref).
For hypergraphs, the notion of prefix must be gen-
eralized to subtrees. Figure 1 shows the packed-
forest representation of the union of gold subtrees
and highest-scoring (Viterbi) subtrees at every gold
node for an input. At each gold node, there are
two incoming hyperedges: one for the gold subtree
and the other for the Viterbi subtree. After bottom-
up parsing, we can compute the scores for the gold
subtrees as well as extract the corresponding Viterbi
subtrees by following backpointers. These Viterbi
subtrees need not necessarily to belong to the full
Viterbi path (i.e., the Viterbi tree rooted at node N ).
An update strategy must choose a subtree or a set of
subtrees at gold nodes. This is to ensure that the
model is updating its weights relative to the inter-
section of the search space and the gold path.
Our first update strategy is called single-node
max-violation (s-max). Given a gold tree y?, it tra-
verses the gold tree and finds the node n on which
the violation between the Viterbi subtree and the
gold subtree is the largest over all gold nodes. The
violation is guaranteed to be greater than or equal to
zero because the lower bound for the max-violation
on any hypergraph is 0 which happens at the leaf
nodes. Then we choose the subtree pair (y?n, y?n) and
do the update similar to the prefix update for the se-
quential case. For example, in Figure 1, suppose the
max-violation happens at node K , which covers the
left half of the input x, then the perceptron update
would move parameters to the subtree represented
by nodes B , C , H and K and away from A ,
B , G and K .
Our second update strategy is called parallel max-
violation (p-max). It is based on the observation that
violations on non-overlapping nodes can be fixed
in parallel. We define a set of frontiers as a set
of nodes that are non-overlapping and the union of
which covers the entire input string x. The frontier
set can include up to |x| nodes, in the case where the
frontier is equivalent to the set of leaves. We traverse
y? bottom-up to compute the set of frontiers such
that each has the max-violation in the span it cov-
ers. Concretely, for each node n, the max-violation
frontier set can be defined recursively,
ft(n) =
{
n, if n = maxv(n)
?
ni?children(n) ft(ni), otherwise
where maxv(n) is the function that returns the node
with the absolute maximum violation in the subtree
rooted at n and can easily be computed recursively
over the hypergraph. To make a perceptron update,
we generate the max-violation frontier set for the en-
tire hypergraph and use it to choose subtree pairs
?
n?ft(root(x))(y?n, y
?
n), where root(x) is the root of
the hypergraph for input x. For example, in Figure 1,
if the union of K and L satisfies the definition of
ft, then the perceptron update would move feature
909
weights away from the union of the two Viterbi sub-
trees and towards their gold counterparts.
In our experiments, we compare the performance
of the two violation-fixing update strategies against
two baselines. The first baseline is the standard up-
date, where updates always happen at the root node
of a gold tree, even if the Viterbi tree at the root node
leads to a non-violation update. The second baseline
is the skip update, which also always updates at the
root nodes but skips any non-violations. This is the
strategy used by Zhang and McDonald (2012).
3 Experiments
We ran a number of experiments on the cube-
pruning dependency parser of Zhang and McDonald
(2012), whose search space can be represented as a
hypergraph in which the nodes are the complete and
incomplete states and the hyperedges are the instan-
tiations of the two parsing rules in the Eisner algo-
rithm (Eisner, 1996).
The feature templates we used are a superset of
Zhang and McDonald (2012). These features in-
clude first-, second-, and third-order features and
their labeled counterparts, as well as valency fea-
tures. In addition, we also included a feature tem-
plate from Bohnet and Kuhn (2012). This tem-
plate examines the leftmost child and the rightmost
child of a modifier simultaneously. All other high-
order features of Zhang and McDonald (2012) only
look at arcs on the same side of their head. We
trained the parser with hamming-loss-augmented
MIRA (Crammer et al, 2006), following Martins et
al. (2010). Based on results on the English valida-
tion data, in all the experiments, we trained MIRA
with 8 epochs and used a beam of size 6 per node.
To speed up the parser, we used an unlabeled
first-order model to prune unlikely dependency arcs
at both training and testing time (Koo and Collins,
2010; Martins et al, 2013). We followed Rush and
Petrov (2012) to train the first-order model to min-
imize filter loss with respect to max-marginal filter-
ing. On the English validation corpus, the filtering
model pruned 80% of arcs while keeping the oracle
unlabeled attachment score above 99.50%. During
training only, we insert the gold tree into the hy-
pergraph if it was mistakenly pruned. This ensures
that the gold nodes are always available, which is
required for model updates.
3.1 English and Chinese Results
We report dependency parsing results on the Penn
WSJ Treebank and the Chinese CTB-5 Treebank.
Both treebanks are constituency treebanks. We gen-
erated two versions of dependency treebanks by ap-
plying commonly-used conversion procedures. For
the first English version (PTB-YM), we used the
Penn2Malt1 software to apply the head rules of Ya-
mada and Matsumoto and the Malt label set. For
the second English version (PTB-S), we used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. We split the data in the standard way:
sections 2-21 for training; section 22 for validation;
and section 23 for evaluation. We utilized a linear
chain CRF tagger which has an accuracy of 96.9%
on the validation data and 97.3% on the evaluation
data2. For Chinese, we use the Chinese Penn Tree-
bank converted to dependencies and split into train/-
validation/evaluation according to Zhang and Nivre
(2011). We report both unlabeled attachment scores
(UAS) and labeled attachment scores (LAS), ignor-
ing punctuations (Buchholz and Marsi, 2006).
Table 1 displays the results. Our improved
cube-pruned parser represents a significant improve-
ment over the feature-rich transition-based parser of
Zhang and Nivre (2011) with a large beam size. It
also improves over the baseline cube-pruning parser
without max-violation update strategies (Zhang and
McDonald, 2012), showing the importance of up-
date strategies in inexact hypergraph search. The
UAS score on Penn-YM is slightly higher than the
best result known in the literature which was re-
ported by the fourth-order unlabeled dependency
parser of Ma and Zhao (2012), although we did
not utilize fourth-order features. The LAS score on
Penn-YM is on par with the best reported by Bohnet
and Kuhn (2012). On Penn-S, there are not many
existing results to compare with, due to the tradition
of reporting results on Penn-YM in the past. Never-
theless, our result is higher than the second best by
a large margin. Our Chinese parsing scores are the
highest reported results.
1http://stp.lingfil.uu.se//?nivre/research/Penn2Malt.html
2The data was prepared by Andre? F. T. Martins as was done
in Martins et al (2013).
910
Penn-YM Penn-S CTB-5
Parser UAS LAS Toks/Sec UAS LAS Toks/Sec UAS LAS Toks/Sec
Zhang and Nivre (2011) 92.9- 91.8- ?680 - - - 86.0- 84.4- -
Zhang and Nivre (reimpl.) (beam=64) 93.00 91.98 800 92.96 90.74 500 85.93 84.42 700
Zhang and Nivre (reimpl.) (beam=128) 92.94 91.91 400 93.11 90.84 250 86.05 84.50 360
Koo and Collins (2010) 93.04 - - - - - - - -
Zhang and McDonald (2012) 93.06 91.86 220 - - - 86.87 85.19 -
Rush and Petrov (2012) - - - 92.7- - 4460 - - -
Martins et al (2013) 93.07 - 740 92.82 - 600 - - -
Qian and Liu (2013) 93.17 - 180 - - - 87.25 - 100
Bohnet and Kuhn (2012) 93.39 92.38 ?120 - - - 87.5- 85.9- -
Ma and Zhao (2012) 93.4- - - - - - 87.4- - -
cube-pruning w/ skip 93.21 92.07 300 92.92 90.35 200 86.95 85.23 200
w/ s-max 93.50 92.41 300 93.59 91.17 200 87.78 86.13 200
w/ p-max 93.44 92.33 300 93.64 91.28 200 87.87 86.24 200
Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except
punctuations. We also include the tokens per second numbers for different parsers whenever available, although the
numbers from other papers were obtained on different machines. Speed numbers marked with ? were converted from
sentences per second.
The speed of our parser is around 200-300 tokens
per second for English. This is faster than the parser
of Bohnet and Kuhn (2012) which has roughly the
same level of accuracy, but is slower than the parser
of Martins et al (2013) and Rush and Petrov (2012),
both of which only do unlabeled dependency pars-
ing and are less accurate. Given that predicting la-
bels on arcs can slow down a parser by a constant
factor proportional to the size of the label set, the
speed of our parser is competitive. We also tried to
prune away arc labels based on observed labels for
each POS tag pair in the training data. By doing so,
we could speed up our parser to 500-600 tokens per
second with less than a 0.2% drop in both UAS and
LAS.
3.2 Importance of Update Strategies
The lower portion of Table 1 compares cube-pruning
parsing with different online update strategies in or-
der to show the importance of choosing an update
strategy that accommodates search errors. The max-
violation update strategies (s-max and p-max) im-
proved results on both versions of the Penn Treebank
as well as the CTB-5 Chinese treebank. It made
a larger difference on Penn-S relative to Penn-YM,
improving as much as 0.93% in LAS against the skip
update strategy. Additionally, we measured the per-
centage of non-violation updates at root nodes. In
the last epoch of training, on Penn-YM, there was
24% non-violations if we used the skip update strat-
egy; on Penn-S, there was 36% non-violations. The
portion of non-violations indicates the inexactness
 92
 92.2
 92.4
 92.6
 92.8
 93
 93.2
 93.4
 93.6
 93.8
 94
 1  2  3  4  5  6  7  8
UA
S
epochs
UAS on Penn-YM dev
s-max
p-max
skip
standard
Figure 2: Constrast of different update strategies on the
validation data set of Penn-YM. The x-axis is the number
of training epochs. The y-axis is the UAS score. s-max
stands for single-node max-violation. p-max stands for
parallel max-violation.
of the underlying search. Search is harder on Penn-S
due to the larger label set. Thus, as expected, max-
violation update strategies improve most where the
search is the hardest and least exact.
Figure 2 shows accuracy per training epoch on the
validation data. It can be seen that bad update strate-
gies are not simply slow learners. More iterations
of training cannot close the gap between strategies.
Forcing invalid updates on non-violations (standard
update) or simply ignoring them (skip update) pro-
duces less accurate models overall.
911
ZN 2011 (reimpl.) skip s-max p-max Best Published?
Language UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
SPANISH 86.76 83.81 87.34 84.15 87.96 84.95 87.68 84.75 87.48 84.05
CATALAN 94.00 88.65 94.54 89.14 94.58 89.05 94.98 89.56 94.07 89.09
JAPANESE 93.10 91.57 93.40 91.65 93.26 91.67 93.20 91.49 93.72 91.7-
BULGARIAN 93.08 89.23 93.52 89.25 94.02 89.87 93.80 89.65 93.50 88.23
ITALIAN 87.31 82.88 87.75 83.41 87.57 83.22 87.79 83.59 87.47 83.50
SWEDISH 90.98 85.66 90.64 83.89 91.62 85.08 91.62 85.00 91.44 85.42
ARABIC 78.26 67.09 80.42 69.46 80.48 69.68 80.60 70.12 81.12 66.9-
TURKISH 76.62 66.00 76.18 65.90 76.94 66.80 76.86 66.56 77.55 65.7-
DANISH 90.84 86.65 91.40 86.59 91.88 86.95 92.00 87.07 91.86 84.8-
PORTUGUESE 91.18 87.66 91.69 88.04 92.07 88.30 92.19 88.40 93.03 87.70
GREEK 85.63 78.41 86.37 78.29 86.14 78.20 86.46 78.55 86.05 77.87
SLOVENE 84.63 76.06 85.01 75.92 86.01 77.14 85.77 76.62 86.95 73.4-
CZECH 87.78 82.38 86.92 80.36 88.36 82.16 88.48 82.38 90.32 80.2-
BASQUE 79.65 71.03 79.57 71.43 79.59 71.52 79.61 71.65 80.23 73.18
HUNGARIAN 84.71 80.16 85.67 80.84 85.85 81.02 86.49 81.67 86.81 81.86
GERMAN 91.57 89.48 91.23 88.34 92.03 89.44 91.79 89.28 92.41 88.42
DUTCH 82.49 79.71 83.01 79.79 83.57 80.29 83.35 80.09 86.19 79.2-
AVG 86.98 81.55 87.33 81.56 87.76 82.08 87.80 82.14
Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years,
we use the 2006 data set. The best results with ? are the maximum in the following papers: Buchholz and Marsi
(2006), Nivre et al (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al (2013), For
consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is
our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011
reimplementation and different update strategies from this paper.
3.3 CoNLL Results
We also report parsing results for 17 languages from
the CoNLL 2006/2007 shared-task (Buchholz and
Marsi, 2006; Nivre et al, 2007). The parser in
our experiments can only produce projective depen-
dency trees as it uses an Eisner algorithm backbone
to generate the hypergraph (Eisner, 1996). So, at
training time, we convert non-projective trees ? of
which there are many in the CoNLL data ? to projec-
tive ones through flattening, i.e., attaching words to
the lowest ancestor that results in projective trees. At
testing time, our parser can only predict projective
trees, though we evaluate on the true non-projective
trees.
Table 2 shows the full results. We sort the
languages according to the percentage of non-
projective trees in increasing order. The Spanish
treebank is 98% projective, while the Dutch tree-
bank is only 64% projective. With respect to the
Zhang and Nivre (2011) baseline, we improved UAS
in 16 languages and LAS in 15 languages. The im-
provements are stronger for the projective languages
in the top rows. We achieved the best published
UAS results for 7 languages: Spanish, Catalan, Bul-
garain, Italian, Swedish, Danish, and Greek. As
these languages are typically from the more projec-
tive data sets, we speculate that extending the parser
used in this study to handle non-projectivity will
lead to state-of-the-art models for the majority of
languages.
4 Conclusions
We proposed perceptron update strategies for in-
exact hypergraph search and experimented with
a cube-pruning dependency parser. Both single-
node max-violation and parallel max-violation up-
date strategies signficantly improved parsing results
over the strategy that ignores any invalid udpates
caused by inexactness of search. The update strate-
gies are applicable to any bottom-up parsing prob-
lems such as constituent parsing (Huang, 2008) and
syntax-based machine translation with online learn-
ing (Chiang et al, 2008).
Acknowledgments: We thank Andre? F. T. Martins
for the dependency converted Penn Treebank with
automatic POS tags from his experiments; the re-
viewers for their useful suggestions; the NLP team
at Google for numerous discussions and comments;
Liang Huang and Kai Zhao are supported in part by
DARPA FA8750-13-2-0041 (DEFT), PSC-CUNY,
and a Google Faculty Research Award.
912
References
B. Bohnet and J. Kuhn. 2012. The best of bothworlds
- a graph-based completion model for transition-based
parsers. In Proc. of EACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
H. Daume? and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proc. of ICML.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
L. Huang, S. Fayong, and G. Yang. 2012. Structured
perceptron with inexact search. In Proc. of NAACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. B. Almeida, and N. A. Smith. 2013.
Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
X. Qian and Y. Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, Vol 1.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
913
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112?1123,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Violation Perceptron and Forced Decoding for Scalable MT Training
Heng Yu1?
1Institute of Computing Tech.
Chinese Academy of Sciences
yuheng@ict.ac.cn
Liang Huang2? Haitao Mi3
2Queens College & Grad. Center
City University of New York
{huang@cs.qc,kzhao@gc}.cuny.edu
Kai Zhao2
3T.J. Watson Research Center
IBM
hmi@us.ibm.com
Abstract
While large-scale discriminative training has
triumphed in many NLP problems, its defi-
nite success on machine translation has been
largely elusive. Most recent efforts along this
line are not scalable (training on the small
dev set with features from top ?100 most fre-
quent words) and overly complicated. We in-
stead present a very simple yet theoretically
motivated approach by extending the recent
framework of ?violation-fixing perceptron?,
using forced decoding to compute the target
derivations. Extensive phrase-based transla-
tion experiments on both Chinese-to-English
and Spanish-to-English tasks show substantial
gains in BLEU by up to +2.3/+2.0 on dev/test
over MERT, thanks to 20M+ sparse features.
This is the first successful effort of large-scale
online discriminative training for MT.
1 Introduction
Large-scale discriminative training has witnessed
great success in many NLP problems such as pars-
ing (McDonald et al, 2005) and tagging (Collins,
2002), but not yet for machine translation (MT) de-
spite numerous recent efforts. Due to scalability is-
sues, most of these recent methods can only train
on a small dev set of about a thousand sentences
rather than on the full training set, and only with
2,000?10,000 rather ?dense-like? features (either
unlexicalized or only considering highest-frequency
words), as in MIRA (Watanabe et al, 2007; Chiang
et al, 2008; Chiang, 2012), PRO (Hopkins and May,
2011), and RAMP (Gimpel and Smith, 2012). How-
ever, it is well-known that the most important fea-
tures for NLP are lexicalized, most of which can not
?Work done while visiting City University of New York.
?Corresponding author.
be seen on a small dataset. Furthermore, these meth-
ods often involve complicated loss functions and
intricate choices of the ?target? derivations to up-
date towards or against (e.g. k-best/forest oracles, or
hope/fear derivations), and are thus hard to replicate.
As a result, the classical method of MERT (Och,
2003) remains the default training algorithm for MT
even though it can only tune a handful of dense fea-
tures. See also Section 6 for other related work.
As a notable exception, Liang et al (2006) do
train a structured perceptron model on the train-
ing data with sparse features, but fail to outperform
MERT. We argue this is because structured percep-
tron, like many structured learning algorithms such
as CRF and MIRA, assumes exact search, and search
errors inevitably break theoretical properties such as
convergence (Huang et al, 2012). Empirically, it
is now well accepted that standard perceptron per-
forms poorly when search error is severe (Collins
and Roark, 2004; Zhang et al, 2013).
To address the search error problem we propose a
very simple approach based on the recent framework
of ?violation-fixing perceptron? (Huang et al, 2012)
which is designed specifically for inexact search,
with a theoretical convergence guarantee and excel-
lent empirical performance on beam search pars-
ing and tagging. The basic idea is to update when
search error happens, rather than at the end of the
search. To adapt it to MT, we extend this framework
to handle latent variables corresponding to the hid-
den derivations. We update towards ?gold-standard?
derivations computed by forced decoding so that
each derivation leads to the exact reference transla-
tion. Forced decoding is also used as a way of data
selection, since those reachable sentence pairs are
generally more literal and of higher quality, which
the training should focus on. When the reachable
subset is small for some language pairs, we augment
1112
it by including reachable prefix-pairs when the full
sentence pair is not.
We make the following contributions:
1. Our work is the first successful effort to scale
online structured learning to a large portion of
the training data (as opposed to the dev set).
2. Our work is the first to use a principled learning
method customized for inexact search which
updates on partial derivations rather than full
ones in order to fix search errors. We adapt it
to MT using latent variables for derivations.
3. Contrary to the common wisdom, we show that
simply updating towards the exact reference
translation is helpful, which is much simpler
than k-best/forest oracles or loss-augmented
(e.g. hope/fear) derivations, avoiding sentence-
level BLEU scores or other loss functions.
4. We present a convincing analysis that it is the
search errors and standard perceptron?s inabil-
ity to deal with them that prevent previous
work, esp. Liang et al (2006), from succeed-
ing.
5. Scaling to the training data enables us to engi-
neer a very rich feature set of sparse, lexical-
ized, and non-local features, and we propose
various ways to alleviate overfitting.
For simplicity and efficiency reasons, in this paper
we use phrase-based translation, but our method has
the potential to be applicable to other translation
paradigms. Extensive experiments on both Chinese-
to-English and Spanish-to-English tasks show statis-
tically significant gains in BLEU by up to +2.3/+2.0
on dev/test over MERT, and up to +1.5/+1.5 over
PRO, thanks to 20M+ sparse features.
2 Phrase-Based MT and Forced Decoding
We first review the basic phrase-based decoding al-
gorithm (Koehn, 2004), which will be adapted for
forced decoding.
2.1 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English from Mi et al (2008):
0 1 2 3 4 5 6
Figure 1: Standard beam-search phrase-based decoding.
Bu`sh??
Bush
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
-ed
hu?`ta?n
meeting
?Bush held a meeting with Sharon?
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (or states) (Koehn, 2004). Each hy-
pothesis has a coverage vector capturing the source-
language words translated so far, and can be ex-
tended into a longer hypothesis by a phrase-pair
translating an uncovered segment. For example, the
following is one possible derivation:
(0 ) : (0, ??)
(?1 ) : (s1, ?Bush?)
r1
(? ???6) : (s2, ?Bush held talks?)
r2
(???3???) : (s3, ?Bush held talks with Sharon?)
r3
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where each
si is the score of each state, each adding the rule
score and the distortion cost (dc) to the score of the
previous state. To compute the distortion cost we
also need to maintain the ending position of the last
phrase (e.g., the 3 and 6 in the coverage vectors).
In phrase-based translation there is also a distortion-
limit which prohibits long-distance reorderings.
The above states are called?LM states since they
do not involve language model costs. To add a bi-
gram model, we split each ?LM state into a series
of +LM states; each +LM state has the form (v,a)
where a is the last word of the hypothesis. Thus a
+LM version of the above derivation might be:
(0 ,<s>) : (0, ?<s>?)
(?1 ,Bush) : (s?1, ?<s> Bush?)
r1
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
(???3???,Sharon) : (s?3, ?<s> Bush held ... with Sharon?)
r3
1113
0 1 2 3 4 5 6
Bush held
held talks
talks with
with Sharon
Sharon
Figure 2: Forced decoding and y-good derivation lattice.
where the score of applying each rule now also in-
cludes a combination cost due to the bigrams formed
when applying the phrase-pair, e.g.
s?3 = s?2 + s(r3) +dc(|6?3|)? logPlm(with | talk)
To make this exponential-time algorithm practi-
cal, beam search is the standard approximate search
method (Koehn, 2004). Here we group +LM states
into n bins, with each bin Bi hosting at most b states
that cover exactly i Chinese words (see Figure 1).
2.2 Forced Decoding
The idea of forced decoding is to consider only those
(partial) derivations that can produce (a prefix of)
the exact reference translation (assuming single ref-
erence). We call these partial derivations ?y-good?
derivations (Daume?, III and Marcu, 2005), and those
that deviate from the reference translation ?y-bad?
derivations. The forced decoding algorithm is very
similar to +LM decoding introduced above, with the
new ?forced decoding LM? to be defined as only
accepting two consecutive words on the reference
translation, ruling out any y-bad hypothesis:
Pforced (b | a) =
{
1 if ?j, s.t. a = yj and b = yj+1
0 otherwise
In the +LM state, we can simply replace the
boundary word by the index on the reference trans-
lation:
(0 ,0) : (0, ?<s>?)
(?1 ,1) : (w?1, ?<s> Bush?)
r1
(? ???6,3) : (w?2, ?<s> Bush held talks?)
r2
(???3???,5) : (w?3, ?<s> Bush held talks with Sharon?)
r3
The complexity of this forced decoding algorithm
is reduced to O(2nn3) where n is the source sen-
tence length, without the expensive bookkeeping for
English boundary words.
Lia?
nhe?
guo?
pa`i
qia?
n
50 gua?
nch
a?iy
ua?n
jia?n
du?
Bo?l
?`we?
iya`
hu??
fu`
m??n
zhu?
zhe`
ngz
h?`
y??la?
i
sho?
uc?`
qua?
ngu?
o
da`x
ua?n
P            
U.N.
 P           
sent
  P          
50
   P         
observers
             
to
    P        
monitor
          P  
the
          P  
1st
           PP
election
         P   
since
     P       
Bolivia
      P      
restored
       PP    
democracy
5
33
4
1
Figure 3: Example of unreachable sentence pair and
reachable prefix-pair. The first big jump is disallowed for
a distortion limit of 4, but we can still extract the top-left
box as a reachable prefix-pair. Note that this example is
perfectly reachable in syntax-based MT.
2.3 Reachable Prefix-Pairs
In practice, many sentence pairs in the parallel text
fail in forced decoding due to two reasons:
1. distortion limit: long-distance reorderings are
disallowed but are very common between lan-
guages with very different word orders such as
English and Chinese.
2. noisy alignment and phrase limit: the word-
alignment quality (typically from GIZA++) are
usually very noisy, which leads to unnecessar-
ily big chunks of rules beyond the phrase limit.
If we only rely on the reachable whole sentence
pairs, we will not be able to use much of the training
set for Chinese-English. So we propose to augment
the set of reachable examples by considering reach-
able prefix-pairs (see Figure 3 for an example).
3 Violation-Fixing Perceptron for MT
Huang et al (2012) establish a theoretical frame-
work called ?violation-fixing perceptron? which is
tailored for structured learning with inexact search
and has provable convergence properties. The high-
level idea is that standard full update does not fix
search errors; to do that we should instead up-
date when search error occurs, e.g., when the gold-
1114
standard derivation falls below the beam. Huang et
al. (2012) show dramatic improvements in the qual-
ity of the learned model using violation-fixing per-
ceptron (compared to standard perceptron) on incre-
mental parsing and part-of-speech tagging.
Since phrase-based decoding is also an incremen-
tal search problem which closely resembles beam-
search incremental parsing, it is very natural to em-
ploy violation-fixing perceptron here for MT train-
ing. Our goal is to produce the exact reference trans-
lation, or in other words, we want at least one y-good
derivation to survive in the beam search.
To adapt the violation-fixing perceptron frame-
work to MT we need to extend the framework
to handle latent variables since the gold-standard
derivation is not observed. This is done in a way
similar to the latent variable structured perceptron
(Zettlemoyer and Collins, 2005; Liang et al, 2006;
Sun et al, 2009) where each update is from the best
(y-bad) derivation towards the best y-good deriva-
tion in the current model; the latter is a constrained
search which is exactly forced decoding in MT.
3.1 Notations
We first establish some necessary notations. Let
?x, y? be a sentence pair in the training data, and
d = r1 ? r2 ? . . . ? r|d|
be a (partial) derivation, where each ri =
?c(ri), e(ri)? is a rule, i.e., a Chinese-English
phrase-pair. Let |c(d)| ?= ?i |c(ri)| be the num-
ber of Chinese words covered by this derivation, and
e(d) ?= e(r1) ? e(r2) . . . ? e(r|d|) be the English pre-
fix generated so far. Let D(x) be the set of all pos-
sible partial derivations translating part of the input
sentence x. Let pre(y) ?= {y[0:j] | 0 ? j ? |y|}
be the set of prefixes of the reference translation y,
and good i(x, y) be the set of partial y-good deriva-
tions whose English side is a prefix of the reference
translation y, and whose Chinese projection covers
exactly i words on the input sentence x, i.e.,
good i(x, y)
?
= {d ? D(x) | e(d)?pre(y), |c(d)|= i}.
Conversely, we define the set of y-bad partial deriva-
tions covering i Chinese words to be:
bad i(x, y) ?= {d ? D(x) | e(d) /?pre(y), |c(d)|= i}.
Basically, at each bin Bi, y-good derivations
good i(x, y) and y-bad ones bad i(x, y) compete for
the b slots in the bin:
B0 = {} (1)
Bi = topb
?
j=1..l
{d ? r | d ? Bi?j , |c(r)| = j} (2)
where r is a rule covering j Chinese words, l is
the phrase-limit, and topb S is a shorthand for
argtopbd?S w ? ?(x, d) which selects the top b
derivations according to the current model w.
3.2 Algorithm 1: Early Update
As a special case of violation-fixing perceptron,
early update (Collins and Roark, 2004) stops decod-
ing whenever the gold derivation falls off the beam,
makes an update on the prefix so far and move on
to the next example. We adapt it to MT as fol-
lows: if at a certain bin Bi, all y-good derivations
in good i(x, y) have fallen off the bin, then we stop
and update, rewarding the best y-good derivation in
good i(x, y) (with respect to current model w), and
penalizing the best y-bad derivation in the same step:
d+i (x, y)
?
= argmax
d?goodi(x,y)
w ??(x, d) (3)
d?i (x, y)
?
= argmax
d?badi(x,y)?Bi
w ??(x, d) (4)
w? w + ??(x, d+i (x, y), d?i (x, y)) (5)
where ??(x, d, d?) ?= ?(x, d)??(x, d?) is a short-
hand notation for the difference of feature vectors.
Note that the set good i(x, y) is independent of the
beam search and current model and is instead pre-
computed in the forced decoding phase, whereas the
negative signal d?i (x, y) depends on the beam.
In practice, however, there are exponentially
many y-good derivations for each reachable sen-
tence pair, and our goal is just to make sure (at least)
one y-good derivation triumphs at the end. So it
is possible that at a certain bin, all y-good partial
derivations fall off the bin, but the search can still
continue and produce the exact reference translation
through some other y-good path that avoids that bin.
For example, in Figure 1, the y-good states in steps
3 and 5 are not critical; it is totally fine to miss them
in the search as long as we save the y-good states
1115
ea
r
l
y
m
a
x
-
v
i
o
l
a
t
i
o
n
best in the beam
worst in the beam
d i
d+i d+i?
d i?
d+|x|
dy|x|
s
t
d
l
o
c
a
l
standard update 
is invalid
m
o
d
e
l
w
d |x|
Figure 4: Illustration of four update methods. The blue
paths denote (possibly lots of) gold-standard derivations
from forced decoding. Standard update in this case is
invalid as it reinforces the error of w (Huang et al, 2012).
in bins 1, 4 and 6. So we actually use a ?softer?
version of the early update algorithm: only stop and
update when there is no hope to continue. To be
more concrete, let l denote the phrase-limit then we
stop where there are l consecutive bins without any
y-good states, and update on the first among them.
3.3 Algorithm 2: Max-Violation Update
While early update learns substantially better mod-
els than standard perceptron in the midst of inex-
act search, it is also well-known to be converging
much slower than the latter, since each update is
on a (short) prefix. Huang et al (2012) propose an
improved method ?max-violation? which updates at
the worst mistake instead of the first, and converges
much faster than early update with similar or better
accuracy. We adopt this idea here as follows: decode
the whole sentence, and find the step i? where the
difference between the best y-good derivation and
the best y-bad one is the biggest. This amount of dif-
ference is called the amount of ?violation? in Huang
et al (2012), and the place of maximum violation is
intuitively the site of the biggest mistake during the
search. More formally, the update rule is:
i? ?= argmin
i
w ???(x, d+i (x, y), d?i (x, y)) (6)
w? w + ??(x, d+i?(x, y), d?i?(x, y)) (7)
3.4 Previous Work: Standard and Local Updates
We compare the above new update methods with the
two existing ones from Liang et al (2006).
Standard update (also known as ?bold update?
in Liang et al (2006)) simply updates at the very
end, from the best derivation in the beam towards the
best gold-standard derivation (regardless of whether
it survives the beam search):
w? w + ??(x, d+|x|(x, y), d
?
|x|(x, y)) (8)
Local update, however, updates towards the
derivation in the final bin that is most similar to the
reference y, denoted dy|x|(x, y):
dy|x|(x, y) = argmax
d?B|x|
Bleu+1(y, e(d)) (9)
w? w + ??(x, dy|x|(x, y), d
?
|x|(x, y))
(10)
where Bleu+1(?, ?) returns the sentence-level BLEU.
Liang et al (2006) observe that standard update
performs worse than local update, which they at-
tribute to the fact that the former often update to-
wards a gold derivation made up of ?unreasonable?
rules. Here we give a very different but theoreti-
cally more reasonable explanation based on the the-
ory of Huang et al (2012), who define an update
??(x, d+, d?) to be invalid if d+ scores higher
than d? (i.e., w ? ??(x, d+, d?) > 0, or update
?w points to the same direction as w in Fig. 4), in
which case there is no ?violation? or mistake to fix.
Perceptron is guaranteed to converge if all updates
are valid. Clearly, early and max-violation updates
are valid. But standard update is not: it is possible
that at the end of search, the best y-good derivation
d+|x|(x, y), though pruned earlier in the search, rankseven higher in the current model than anything in the
final bin (see Figure 4). In other words, there is no
mistake at the final step, while there must be some
search error in earlier steps which expels the y-good
subderivation. We will see in Section 5.3 that invalid
updates due to search errors are indeed the main rea-
son why standard update fails. Local update, how-
ever, is always valid in that definition.
Finally, it is worth noting that in terms of imple-
mentation, standard and max-violation are the easi-
est, while early update is more involved.
4 Feature Design
Our feature set includes the following 11 dense fea-
tures: LM, four conditional and lexical translation
probabilities (pc(e|f), pc(f |e), pl(e|f), pl(f |e)),
length and phrase penalties, distortion cost, and
three lexicalized reordering features. All these fea-
tures are inherited from Moses (Koehn et al, 2007).
1116
(?1 ,Bush) : (s?1, ?<s> Bush?)
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
</s>ju?x??ng le hu?`ta?nyu? Sha?lo?ngBu`sh??<s>
held talksBush<s>
r1 r2
features for applying r2 on span x[3:6]
WordEdges
c(r2)[0] = ju?x??ng, c(r2)[?1] = hu?`ta?n
e(r2)[0] = held, e(r2)[?1] = talks
x[2:3] = Sha?lo?ng, x[6:7] = </s>, |c(r2)| = 3
... (combos of the above atomic features) ...
non-local e(r0 ? r1)[?2:] ? id(r2)id(r1) ? id(r2)
Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax.
4.1 Local Sparse Features: Ruleid & WordEdges
We first add the rule identification feature for each
rule: id(ri). We also introduce lexicalized Word-
Edges features, which are shown to be very effec-
tive in parsing (Charniak and Johnson, 2005) and
MT (Liu et al, 2008; He et al, 2008) literatures.
We use the following atomic features when apply-
ing a rule ri = ?c(ri), e(ri)?: the source-side length
|c(ri)|, the boundary words of both c(ri) and e(ri),
and the surrounding words of c(ri) on the input sen-
tence x. See Figure 5 for examples. These atomic
features are concatenated to generate all kinds of
combo features.
Chinese English class size budget
word 52.9k 64.2k 5
characters - 3.7k - 3
Brown cluster, full string 200 3
Brown cluster, prefix 6 6 8 2
Brown cluster, prefix 4 4 4 2
POS tag 52 36 2
word type - 4 - 1
Table 1: Various levels of backoff for WordEdges fea-
tures. Class size is estimated on the small Chinese-
English dataset (Sec. 5.3). The POS tagsets are ICT-
CLAS for Chinese (Zhang et al, 2003) and Penn Tree-
bank for English (Marcus et al, 1993).
4.2 Addressing Overfitting
With large numbers of lexicalized combo features
we will face the overfitting problem, where some
combo features found in the training data are too
rare to be seen in the test data. Thus we propose
three ways to alleviate this problem.
First, we introduce various levels of backoffs for
each word w (see Table 1). We include w?s Brown
cluster and its prefixes of lengths 4 and 6 (Brown et
al., 1992), and w?s part-of-speech tag. If w is Chi-
nese we also include its word type (punctuations,
digits, alpha, or otherwise) and (leftmost or right-
most) character. In such a way, we significantly in-
crease the feature coverage on unseen data.
However, if we allow arbitrary combinations, we
can extract a hexalexical feature (4 Chinese + 2 En-
glish words) for a local window in Figure 5, which
is unlikely to be seen at test time. To control model
complexity we introduce a feature budget for each
level of backoffs, shown in the last column in Ta-
ble 1. The total budget for a combo feature is the
sum of the budgets of all atomic features. In our ex-
periments, we only use the combo features with a
total budget of 10 or less, i.e., we can only include
bilexical but not trilexical features, and we can in-
clude for example combo features with one Chinese
word plus two English tags (total budget: 9).
Finally, we use two methods to alleviate overfit-
ting due to one-count rules: for large datasets, we
simply remove all one-count rules, but for small
datasets where out-of-vocabulary words (OOVs)
abound, we use a simple leave-one-out method:
when training on a sentence pair (x, y), do not use
the one-count rules extracted from (x, y) itself.
4.3 Non-Local Features
Following the success of non-local features in pars-
ing (Huang, 2008) and MT (Vaswani et al, 2011),
we also introduce them to capture the contextual in-
formation in MT. Our non-local features, shown in
Figure 5, include bigram rule-ids and the concatena-
tion of a rule id with the translation history, i.e. the
last two English words. Note that we also use back-
offs (Table 1) for the words included. Experiments
(Section 5.3) show that although the set of non-local
features is just a tiny fraction of all features, it con-
tributes substantially to the improvement in BLEU.
1117
Scale Language Training Data Reachability ?BLEU SectionsPair # sent. # words sent. words # feats # refs dev/test
small CH-EN 30K 0.8M/1.0M 21.4% 8.8% 7M 4 +2.2/2.0 5.2, 5.3large 230K 6.9M/8.9M 32.1% 12.7% 23M +2.3/2.0 5.2, 5.4
large SP-EN 174K 4.9M/4.3M 55.0% 43.9% 21M 1 +1.3/1.1 5.5
Table 2: Overview of all experiments. The ?BLEU column shows the absolute improvements of our method MAX-
FORCE on dev/test sets over MERT. The Chinese datasets also use prefix-pairs in training (see Table 3).
5 Experiments
In order to test our approach in different language
pairs, we conduct three experiments, shown in Ta-
ble 2, on two significantly different language pairs
(long vs. short distance reorderings), Chinese-to-
English (CH-EN) and Spanish-to-English (SP-EN).
5.1 System Preparation and Data
We base our experiments on Cubit, a state-of-art
phrase-based system in Python (Huang and Chiang,
2007).1 We set phrase-limit to 7 in rule extraction,
and beam size to 30 and distortion limit 6 in de-
coding. We compare our violation-fixing percep-
tron with two popular tuning methods: MERT (Och,
2003) and PRO (Hopkins and May, 2011).
For word alignments we use GIZA++-`0
(Vaswani et al, 2012) which produces sparser align-
ments, alleviating the garbage collection problem.
We use the SRILM toolkit (Stolcke, 2002) to train a
trigram language model with modified Kneser-Ney
smoothing on 1.5M English sentences.
Our dev and test sets for CH-EN task are from the
newswire portion of 2006 and 2008 NIST MT Eval-
uations (616/691 sentences, 18575/18875 words),
with four references.2 The dev and test sets for SP-
EN task are from newstest2012 and newstest2013,
with only one reference. Below both MERT and PRO
tune weights on the dev set, while our method on the
training set. Specifically, our method only uses the
dev set to know when to stop training.
5.2 Forced Decoding Reachability on Chinese
As mentioned in Section 2.2, we perform forced de-
coding to select reachable sentences from the train-
1http://www.cis.upenn.edu/?lhuang3/cubit/. We
will release the new version at http://acl.cs.qc.edu.
2We use the ?average? reference length to compute the
brevity penalty factor, which does not decrease with more ref-
erences unlike the ?shortest? heuristic.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 10  20  30  40  50  60  70
R
at
io
 o
f c
om
pl
et
e 
co
ve
ra
ge
Sentence length
dist-unlimited
dist-6
dist-4
dist-2
dist-0
Figure 6: Reachability ratio vs. sentence length on the
small CH-EN training set.
small large
sent. words sent. words
full 21.4% 8.8% 32.1% 12.7%
+prefix 61.3% 24.6% 67.3% 32.8%
Table 3: Ratio of sentence reachability and word cover-
age on the two CH-EN training data (distortion limit: 6).
ing data; this part is done with exact search with-
out any beam pruning. Figure 6 shows the reacha-
bility ratio vs. sentence length on the small CH-EN
training data, where the ratio decreases sharply with
sentence length, and increases with distortion limit.
We can see that there are a lot of long distance re-
orderings beyond small distortion limits. In the ex-
treme case of unlimited distortion, a large amount of
sentences will be reachable, but at the cost of much
slower decoding (O(n2V 2) in beam search decod-
ing, andO(2nn3) in forced decoding). In fact forced
decoding is too slow in the unlimited mode that we
only plot reachability for sentences up to 30 words.
Table 3 shows the statistics of forced decoding on
both small and large CH-EN training sets. In the
1118
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
 5  10  15  20  25  30  35  40  45  50
A
ve
ra
ge
 n
um
be
r 
of
 d
er
iv
at
io
ns
Sentence length
dist-6
dist-4
dist-2
dist-0
Figure 7: Average number of derivations in gold lattices.
small data-set, 21.4% sentences are fully reachable
which only contains 8.8% words (since shorter sen-
tences are more likely to be reachable). Larger data
improves reachable ratios significantly thanks to bet-
ter alignment quality, but still only 12.7% words can
be used. In order to add more examples for per-
ceptron training, we pick all non-trivial reachable
prefix-pairs (with 5 or more Chinese words) as addi-
tional training examples (see Section 2.2). As shown
in Table 3, with prefix-pairs we can use about 1/4 of
small data and 1/3 of large data for training, which is
10x and 120x bigger than the 616-sentence dev set.
After running forced decoding, we obtain gold
translation lattice for each reachable sentence (or
prefix) pair. Figure 7 shows, as expected, the av-
erage number of gold derivations in these lattices
grows exponentially with sentence length.
5.3 Analysis on Small Chinese-English Data
Figure 8 shows the BLEU scores of different learn-
ing algorithms on the dev set. MAXFORCE3 per-
forms the best, peaking at iteration 13 while early
update learns much slower (the first few iterations
are faster than other methods due to early stopping
but this difference is immaterial later). The local and
standard updates, however, underperform MERT; in
particular, the latter gets worse as training goes on.
As analysized in Section 3.4, the reason why stan-
dard update (or ?bold update? in Liang et al (2006))
fails is that inexact search leads to many invalid up-
dates. This is confirmed by Figure 9, where more
3Stands for Max-Violation Perceptron w/ Forced Decoding
17
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16  18  20
B
LE
U
Number of iteration
Max
Forc
e
MERT
early
local
standard
Figure 8: BLEU scores on the heldout dev set for different
update methods (trained on small CH-EN data).
50%
60%
70%
80%
90%
 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
R
at
io
beam size
+non-local features
standard perceptron
Figure 9: Ratio of invalid updates in standard update.
than half of the updates remain invalid even at a
beam of 30. These analyses provide an alternative
but theoretically more reasonable explanation to the
findings of Liang et al (2006): while they blame
?unreasonable? gold derivations for the failure of
standard update, we observe that it is the search er-
rors that make the real difference, and that an up-
date that respects search errors towards a gold sub-
derivation is indeed helpful, even if that subderiva-
tion might be ?unreasonable?.
In order to speedup training, we use mini-batch
parallelization of Zhao and Huang (2013) which has
been shown to be much faster than previous paral-
lelization methods. We set the mini-batch size to
24 and train MAXFORCE with 1, 6, and 24 cores
on a small subset of the our original reachable sen-
1119
 22
 23
 24
 0  0.5  1  1.5  2  2.5  3  3.5  4
B
LE
U
Time
MERT PRO-dense
minibatch(24-core)
minibatch(6-core)
minibatch(1 core)
single processor
Figure 10: Minibatch parallelization speeds up learning.
10
12
14
16
18
20
22
24
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MaxForce
MERT
PRO-dense
PRO-medium
PRO-large
Figure 11: Comparison between different training meth-
ods. Ours trains the training set while others on dev set.
tences. The number of sentence pairs in this subset
is 1,032, which contains similar number of words to
our 616-sentence dev set (since reachable sentences
are much shorter). Thus, it is reasonable to compare
different learning algorithms in terms of speed and
performance. Figure 10 shows that first of all, mini-
batch improves BLEU even in the serial setting, and
when run on 24 cores, it leads to a speedup of about
7x. It is also interesting to know that on 1 CPU,
minibatch perceptron takes similar amount of time
to reach the same performance as MERT and PRO.
Figure 11 compares the learning curves of MAX-
FORCE, MERT, and PRO. We test PRO in three
different ways: PRO-dense (dense features only),
PRO-medium (dense features plus top 3K most fre-
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MERT
+non-local
+word-edges
+ruleid
dense
Figure 12: Incremental contributions of different feature
sets (dense features, ruleid, WordEdges, and non-local).
type count % BLEU
dense 11 - 22.3
+ruleid +9,264 +0.1% +0.8
+WordEdges +7,046,238 +99.5% +2.0
+non-local +22,536 +0.3% +0.7
all 7,074,049 100% 25.8
Table 4: Feature counts and incremental BLEU improve-
ments. MAXFORCE with all features is +2.2 over MERT.
quent sparse features4), and PRO-large (dense fea-
tures plus all sparse features). The results show that
PRO-dense performs almost the same as MERT but
with a stabler learning curve while PRO-medium im-
proves by +0.6. However, PRO-large decreases the
performance significantly, which indicates PRO is
not scalable to truly sparse features. By contrast,
our method handles large-scale sparse features well
and outperforms all other methods by a large margin
and with a stable learning curve.
We also investigate the individual contribution
from each group of features (ruleid, WordEdges, and
non-local features). So we perform experiments by
adding each group incrementally. Figure 12 shows
the learning curves and Table 4 lists the counts and
incremental contributions of different feature sets.
With dense features alone MAXFORCE does not do
4To prevent overfitting we remove all lexicalized features
and only use Brown clusters. It is difficult to engineer the right
feature set for PRO, whereas MAXFORCE is much more robust.
1120
system algorithm # feat. dev test
Moses MERT 11 25.5 22.5
Cubit
MERT 11 25.4 22.5
PRO
11 25.6 22.6
3K 26.3 23.0
36K 17.7 14.3
MAXFORCE 23M 27.8 24.5
Table 5: BLEU scores (with four references) using the
large CH-EN data. Our approach is +2.3/2.0 over MERT.
well because perceptron is known to suffer from fea-
tures of vastly different scales. Adding ruleid helps,
but still not enough. WordEdges (which is the vast
majority of features) improves BLEU by +2.0 points
and outperforms MERT, when sparse features totally
dominate dense features. Finally, the 0.3% non-local
features contribute a final +0.7 in BLEU.
5.4 Results on Large Chinese-English Data
Table 5 shows all BLEU scores for different learn-
ing algorithms on the large CH-EN data. The MERT
baseline on Cubit is essentially the same as Moses.
Our MAXFORCE activates 23M features on reach-
able sentences and prefixes in the training data, and
takes 35 hours to finish 15 iterations on 24 cores,
peaking at iteration 13. It achieves significant im-
provements over other approaches: +2.3/+2.0 points
over MERT and +1.5/+1.5 over PRO-medium on de-
v/test sets, respectively.
5.5 Results on Large Spanish-English Data
In SP-EN translation, we first run forced decod-
ing on the training set, and achieve a very high
reachability of 55% (with the same distortion limit
of 6), which is expected since the word order be-
tween Spanish and English are more similar than
than between Chinese and English, and most SP-
EN reorderings are local. Table 6 shows that MAX-
FORCE improves the translation quality over MERT
by +1.3/+1.1 BLEU on dev/test. These gains are
comparable to the improvements on the CH-EN task,
since it is well accepted in MT literature that a
change of ? in 1-reference BLEU is roughly equiva-
lent to a change of 2? with 4 references.
system algorithm # feat. dev test
Moses MERT 11 27.4 24.4
Cubit MAXFORCE 21M 28.7 25.5
Table 6: BLEU scores (with one reference) on SP-EN.
6 Related Work
Besides those discussed in Section 1, there are also
some research on tuning sparse features on the train-
ing data, but they integrate those sparse features into
the MT log-linear model as a single feature weight,
and tune its weight on the dev set (e.g. (Liu et al,
2008; He et al, 2008; Wuebker et al, 2010; Simi-
aner et al, 2012; Flanigan et al, 2013; Setiawan
and Zhou, 2013; He and Deng, 2012; Gao and He,
2013)). By contrast, our approach learns sparse fea-
tures only on the training set, and use dev set as held-
out to know when to stop.
Forced decoding has been used in the MT litera-
ture. For example, open source MT systems Moses
and cdec have implemented it. Liang et al (2012)
also use the it to boost the MERT tuning by adding
more y-good derivations to the standard k-best list.
7 Conclusions and Future Work
We have presented a simple yet effective approach
of structured learning for machine translation which
scales, for the first time, to a large portion of the
whole training data, and enables us to tune a rich set
of sparse, lexical, and non-local features. Our ap-
proach results in very significant BLEU gains over
MERT and PRO baselines. For future work, we will
consider other translation paradigms such as hierar-
chical phrase-based or syntax-based MT.
Acknowledgement
We thank the three anonymous reviewers for helpful sug-
gestions. We are also grateful to David Chiang, Dan
Gildea, Yoav Goldberg, Yifan He, Abe Ittycheriah, and
Hao Zhang for discussions, and Chris Callison-Burch,
Philipp Koehn, Lemao Liu, and Taro Watanabe for help
with datasets. Huang, Yu, and Zhao are supported by
DARPA FA8750-13-2-0041 (DEFT), a Google Faculty
Research Award, and a PSC-CUNY Award, and Mi by
DARPA HR0011-12-C-0015. Yu is also supported by the
China 863 State Key Project (No. 2011AA01A207). The
views and findings in this paper are those of the authors
and are not endorsed by the US or Chinese governments.
1121
References
Peter Brown, Peter Desouza, Robert Mercer, Vincent
Pietra, and Jenifer Lai. 1992. Class-based n-gram
models of natural language. Computational linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of NAACL 2013.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient ascent.
In Proceedings of NAACL:HLT, pages 450?459, At-
lanta, Georgia, June.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proceedings of ACL.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of COLING, pages
321?328, Manchester, UK, August.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of ACL:
Demonstrations.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
COLING-ACL, Sydney, Australia, July.
Huashen Liang, Min Zhang, and Tiejun Zhao. 2012.
Forced decoding for minimum error rate training in
statistical machine translation. Journal of Computa-
tional Information Systems, (8):861868.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of EMNLP, pages 89?97, Honolulu,
Hawaii, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Hendra Setiawan and Bowen Zhou. 2013. Discrimi-
native training of 150 million translation parameters
and its application to pruning. In Proceedings of
NAACL:HLT, pages 335?341, Atlanta, Georgia, June.
ACL.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proceedings of ACL, Jeju Island, Korea.
1122
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Xu Sun, Takuya Matsuzaki, and Daisuke Okanohara.
2009. Latent variable perceptron algorithm for struc-
tured classification. In Proceedings of IJCAI.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of ACL 2011, Port-
land, OR.
Ashish Vaswani, Liang Huang, and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of ACL.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of EMNLP-
CoNLL.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of ACL, pages 475?484, Uppsala,
Sweden, July.
Luke Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In Pro-
ceedings of UAI.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. In Proceedings of the second SIGHAN workshop
on Chinese language processing, pages 184?187.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-
ald. 2013. Online learning with inexact hypergraph
search. In Proceedings of EMNLP 2013.
Kai Zhao and Liang Huang. 2013. Minibatch and paral-
lelization for online large margin structured learning.
In Proceedings of NAACL 2013.
1123
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1942?1952,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Search-Aware Tuning for Machine Translation
Lemao Liu
Queens College
City University of New York
lemaoliu@gmail.com
Liang Huang
Queens College and Graduate Center
City University of New York
liang.huang.sh@gmail.com
Abstract
Parameter tuning is an important problem in
statistical machine translation, but surpris-
ingly, most existing methods such as MERT,
MIRA and PRO are agnostic about search,
while search errors could severely degrade
translation quality. We propose a search-
aware framework to promote promising par-
tial translations, preventing them from be-
ing pruned. To do so we develop two met-
rics to evaluate partial derivations. Our tech-
nique can be applied to all of the three
above-mentioned tuning methods, and ex-
tensive experiments on Chinese-to-English
and English-to-Chinese translation show up
to +2.6 BLEU gains over search-agnostic
baselines.
1 Introduction
Parameter tuning has been a key problem for ma-
chine translation since the statistical revolution.
However, most existing tuning algorithms treat the
decoder as a black box (Och, 2003; Hopkins and
May, 2011; Chiang, 2012), ignoring the fact that
many potentially promising partial translations are
pruned by the decoder due to the prohibitively
large search space. For example, the popular
beam-search decoding algorithm for phrase-based
MT (Koehn, 2004) only explores O(nb) items for
a sentence of n words (with a beam width of b),
while the full search space is O(2
n
n
2
) or worse
(Knight, 1999).
As one of the very few exceptions to the
?search-agnostic? majority, Yu et al. (2013) and
Zhao et al. (2014) propose a variant of the per-
ceptron algorithm that learns to keep the refer-
ence translations in the beam or chart. How-
ever, there are several obstacles that prevent their
method from becoming popular: First of all, they
rely on ?forced decoding? to track gold derivations
that lead to the reference translation, but in practice
only a small portion of (mostly very short) sen-
(a)
0 1 2 3 4
(b)
Figure 1: (a) Some potentially promising partial trans-
lations (in red) fall out of the beam (bin 2); (b) We
identify such partial translations and assign them higher
model scores so that they are more likely to survive the
search.
tence pairs have at least one such derivation. Sec-
ondly, they learn the model on the training set, and
while this does enable a sparse feature set, it is or-
ders of magnitude slower compared to MERT and
PRO.
We instead propose a very simple framework,
search-aware tuning, which does not depend on
forced decoding, and thus can be trained on all sen-
tence pairs of any dataset. The key idea is that,
besides caring about the rankings of the complete
translations, we also promote potentially promis-
ing partial translations so that they are more likely
to survive throughout the search, see Figure 1 for
illustration. We make the following contributions:
? Our idea of search-aware tuning can be ap-
plied (as a patch) to all of the three most
popular tuning methods (MERT, PRO, and
MIRA) by defining a modified objective func-
tion (Section 4).
? To measure the ?promise? or ?potential? of a
partial translation, we define a new concept
?potential BLEU? inspired by future cost in
MT decoding (Koehn, 2004) and heuristics in
A* search (Hart et al., 1968) (Section 3.2).
This work is the first study of evaluating met-
rics for partial translations.
? Our method obtains substantial and consistent
1942
improvements on both the large-scale NIST
Chinese-to-English and English-to-Chinese
translation tasks on top of MERT, MIRA, and
PRO baselines. This is the first time that con-
sistent improvements can be achieved with a
new learning algorithm under dense feature
settings (Section 5).
For simplicity reasons, in this paper we use
phrase-based translation, but our work has the po-
tential to be applied to other translation paradigms.
2 Review: Beam Search for PBMT
Decoding
We review beam search for phrase-based decoding
in our notations which will facilitate the discussion
of search-aware tuning in Section 4. Following Yu
et al. (2013), let ?x, y? be a Chinese-English sen-
tence pair in the tuning set D, and
d = r
1
? r
2
? . . . ? r
|d|
be a (partial) derivation, where each r
i
=
?c(r
i
), e(r
i
)? is a rule, i.e., a phrase-pair. Let |c(r)|
be the number of Chinese words in rule r, and
e(d)
?
= e(r
1
) ? e(r
2
) . . . ? e(r
|d|
) be the English
prefix (i.e., partial translation) generated so far.
In beam search, each binB
i
(x) contains the best
k derivations covering exactly i Chinese words,
based on items in previous bins (see Figures 1
and 2):
B
0
(x) = {}
B
i
(x) = top
k
w
0
(
?
j=1..i
{d ? r | d?B
i?j
(x), |c(r)|=j})
where r is a rule covering j Chinese words, and
top
k
w
0
(?) returns the top k derivations according
to the current model w
0
. As a special case, note
that top
1
w
0
(S) = argmax
d?S
w
0
? ?(d), so
top
1
w
0
(B
|x|
(x)) is the final 1-best result.
1
See Fig-
ure 2 for an illustration.
3 Challenge: Evaluating Partial
Derivations
As mentioned in Section 1, the current mainstream
tuning methods such as MERT, MIRA, and PRO are
1
Actually B
|x|
(x) is an approximation to the k-best list
since some derivations are merged by dynamic programming;
to recover those we can use Alg. 3 of Huang and Chiang
(2005).
0 1 2 3 4
B
0
(x) B
1
(x) B
2
(x) B
3
(x) B
4
(x)
Figure 2: Beam search for phrase-based decoding. The
item in red is top
1
w
0
(B
4
(x)), i.e., the 1-best result.
Traditional tuning only uses the final bin B
4
(x) while
search-aware tuning considers all binsB
i
(x) (i = 1..4).
all search-agnostic: they only care about the com-
plete translations from the last bin, B
|x|
(x), ignor-
ing all partial ones, i.e., B
i
(x) for all i < |x|. As
a result, many potentially promising partial deriva-
tions never reach the final bin (See Figure 1).
To address this problem, our new ?search-aware
tuning? aims to promote not only the accurate
translations in the final bin, but more importantly
those potentially promising partial derivations in
non-final bins. The key challenge, however, is
how to evaluate the ?promise? or ?potential? of
a partial derivation. In this Section, we develop
two such measures, a simple ?partial BLEU? (Sec-
tion 3.1) and a more principled ?potential BLEU?
(Section 3.2). In Section 4, we will then adapt tra-
ditional tuning methods to their search-aware ver-
sions using these partial evaluation metrics.
3.1 Solution 1: Simple and Naive Partial BLEU
Inspired by a trick in (Li and Khudanpur, 2009)
and (Chiang, 2012) for oracle or hope extraction,
we use a very simple metric to evaluate partial
translations for tuning. For a given derivation d,
the basic idea is to evaluate the (short) partial trans-
lation e(d) against the (full) reference y, but using
a ?prorated? reference length proportional to c(d)
which is the number of Chinese words covered so
far in d:
|y| ? |c(d)|/|x|
For example, if d has covered 2 words on a 8-
word Chinese sentence with a 12-word English
reference, then the ?effective reference length? is
12?2/8 = 3. We call this method ?partial BLEU?
since it does not complete the translation, and de-
note it by
?
?
|x|
y
(d) = ??
(
y, e(d); reflen = |y| ? |c(d)|/|x|
)
.
(1)
1943
?(y, y
?
) = ?Bleu
+1
(y, y
?
) string distance metric
?
y
(d) = ?(y, e(d)) full derivations eval
?
x
y
(d) =
{
?
?
|x|
y
(d) partial bleu (Sec. 3.1)
?(y, e?
x
(d)) potential bleu (Sec. 3.2)
Table 1: Notations for evaluating full and partial deriva-
tions. Functions
?
?
|x|
y
(?) and e?
x
(?) are defined by Equa-
tions 1 and 3, respectively.
where reflen is the effective length of reference
translations, see (Papineni et al., 2002) for details.
3.1.1 Problem with Partial BLEU
Simple as it is, this method does not work well in
practice because comparison of partial derivations
might be unfair for different derivations covering
different set of Chinese words, as it will naturally
favor those covering ?easier? portions of the in-
put sentence (which we do observe empirically).
For instance, consider the following Chinese-to-
English example which involves a reordering of
the Chinese PP:
(2) w?o
I
c?ong
from
Sh`angh?ai
Shanghai
f?ei
fly
d`ao
to
B?eij??ng
Beijing
?I flew from Shanghai to Beijing?
Partial BLEU will prefer subtranslation ?I from? to
?I fly? in bin 2 (covering 2 Chinese words) because
the former has 2 unigram mathces while the latter
only 1, even though the latter is almost identical
to the reference and will eventually lead to a com-
plete translation with substantially higher Bleu
+1
score (matching a 4-gram ?from Shanghai to Bei-
jing?). Similarly, it will prefer ?I from Shanghai?
to ?I fly from? in bin 3, without knowing that the
former will eventually pay the price of word-order
difference. This example suggests that we need a
more ?global? or less greedy metric (see below).
3.2 Solution 2: Potential BLEU via Extension
Inspired by future cost computation in MT decod-
ing (Koehn, 2004), we define a very simple fu-
ture string by simply concatenating the best model-
score translation (with no reorderings) in each un-
covered span. Let best
w
(x
[i:j]
) denote the best
monotonic derivation for span [i : j], then
future(d, x) = ?
[i:j]?uncov(d,x)
e(best
w
(x
[i:j]
))
where ? is the concatenation operator and
uncov(d, x) returns an ordered list of uncovered
e(d) future(d, x)
x =
e?
x
(d) =
monotonicreordering
 
Figure 3: Example of the extension function e?
x
(?) (and
future string) on an incomplete derivation d.
spans of x. See Figure 3 for an example. This fu-
ture string resembles (inadmissible) heuristic func-
tion (Hart et al., 1968). Now the ?extended trans-
lation? is simply a concatenation of the exist-
ing partial translation e(d) and the future string
future(d, x):
e?
x
(d) = e(d) ? future(d, x). (3)
Instead of calculating best
w
(x
[i:j]
) on-the-fly
for each derivation d, we can precompute it for
each span [i : j] during future-cost computa-
tion, since the score of best
w
(x
[i:j]
) is context-
free (Koehn, 2004). Algorithm 1 shows the
pseudo-code of computing best
w
(x
[i:j]
). In prac-
tice, since future-cost precomputation already
solves the best (monotonic) model-score for each
span, is the only extra work for potential BLEU
is to record (for each span) the subtranslation that
achieves that best score. Therefore, the extra time
for potential BLEU is negligible (the time com-
plexity is O(n
2
), but just as in future cost, the con-
stant is much smaller than real decoding). The im-
plementation should require minimal hacking on a
phrase-based decoder (such as Moses).
To summarize the notation, we use ?
x
y
(d) to
denote a generic evaluation function for par-
tial derivation d, which could be instantiated in
two ways, partial bleu (
?
?
|x|
y
(d)) or potential bleu
(?(y, e?
x
(d))). See Table 1 for details. The next
Section will only use the generic notation ?
x
y
(d).
Finally, it is important to note that although
both partial and potential metrics are not BLEU-
specific, the latter is much easier to adapt to other
metrics such as TER since it does not change the
original Bleu
+1
definition. By contrast, it is not
clear to us at all how to generalize partial BLEU to
partial TER.
4 Search-Aware MERT, MIRA, and PRO
Parameter tuning aims to optimize the weight vec-
tor w so that the rankings based on model score de-
fined by w is positively correlated with those based
1944
Algorithm 1 Computation of best Translations for Potential BLEU.
Input: Source sentence x, a rule set < for x, and w.
Output: Best translations e(best
w
(x[i : j])) for all spans [i : j].
1: for l in (0..|x|) do
2: for i in (0..|x| ? l) do
3: j = i+ l + 1
4: best score = ??
5: if <[i : j] 6= ? then . <[i : j] is a subset of rules < for span [i : j].
6: best
w
(x[i : j]) = argmax
r?<[i:j]
w ??({r}) . {r} is a derivation consisting of one rule r.
7: best score = w ??(best
w
(x[i : j]))
8: for k in (i+ 1 .. i+ p) do . p is the phrase length limit
9: if best score < w ??
(
best
w
(x[i : k]) ? best
w
(x[k : j])
)
then
10: best
w
(x[i : j]) = best
w
(x[i : k]) ? best
w
(x[k : j])
11: best score = w ??(best
w
(x[i : j]))
on some translation metric (such as BLEU (Pap-
ineni et al., 2002)). In other words, for a train-
ing sentence pair ?x, y?, if a pair of its trans-
lations y
1
= e(d
1
) and y
2
= e(d
2
) satisfies
BLEU(y, y
1
) > BLEU(y, y
2
), then we expect w ?
?(d
1
) > w ??(d
2
) to hold after tuning.
4.1 From MERT to Search-Aware MERT
Suppose D is a tuning set of ?x, y? pairs. Tra-
ditional MERT learns the weight by iteratively
reranking the complete translations towards those
with higher BLEU in the final bin B
|x|
(x) for
each x in D. Formally, it tries to minimize the
document-level error of 1-best translations:
`
MERT
(D,w) =
?
?x,y??D
?
y
(
top
1
w
(B
|x|
(x))
)
,
(4)
where top
1
w
(S) is the best derivation in S under
model w, and ?
?
(?) is the full derivation metric as
defined in Table 1; in this paper we use ?
y
(y
?
) =
?BLEU(y, y
?
). Here we follow Och (2003) and
Lopez (2008) to simplify the notations, where the
? operator (similar to
?
) is an over-simplification
for BLEU which, as a document-level metric, is ac-
tually not factorizable across sentences.
Besides reranking the complete translations as
traditional MERT, our search-aware MERT (SA-
MERT) also reranks the partial translations such
that potential translations may survive in the mid-
dle bins during search. Formally, its objective
function is defined as follows:
`
SA-MERT
(D,w)=
?
?x,y??D
?
i=1..|x|
?
x
y
(
top
1
w
(B
i
(x))
)
(5)
where top
1
w
(?) is defined in Eq. (4), and ?
x
y
(d),
defined in Table 1, is the generic metric for eval-
uating a partial derivation d which has two imple-
mentations (partial bleu or potential bleu). In or-
der words we can obtain two implementations of
search-aware MERT methods, SA-MERT
par
and
SA-MERT
pot
.
Notice that the traditional MERT is a special
case of SA-MERT where i is fixed to |x|.
4.2 From MIRA to Search-Aware MIRA
MIRA is another popular tuning method for SMT.
It firstly introduced in (Watanabe et al., 2007), and
then was improved in (Chiang et al., 2008; Chiang,
2012; Cherry and Foster, 2012). Its main idea is to
optimize a weight such that the model score dif-
ference of a pair of derivations is greater than their
loss difference.
In this paper, we follow the objective function
in (Chiang, 2012; Cherry and Foster, 2012), where
only the violation between hope and fear deriva-
tions is concerned. Formally, we define d
+
(x, y)
and d
?
(x, y) as the hope and fear derivations in
the final bin (i.e., complete derivations):
d
+
(x, y) = argmax
d?B
|x|
(x)
w
0
??(d)? ?
y
(d) (10)
d
?
(x, y) = argmax
d?B
|x|
(x)
w
0
??(d) + ?
y
(d) (11)
where w
0
is the current model. The loss function
of MIRA is in Figure 4. The update will be be-
tween d
+
(x, y) and d
?
(x, y).
To adapt MIRA to search-aware MIRA (SA-
MIRA), we need to extend the definitions of hope
1945
`MIRA
(D,w) =
1
2C
?w?w
0
?
2
+
?
?x,y??D
[
??
y
(
d
+
(x, y), d
?
(x, y)
)
?w???
(
d
+
(x, y), d
?
(x, y)
)]
+
(6)
`
SA-MIRA
(D,w)=
1
2C
?w?w
0
?
2
+
?
?x,y??D
|x|
?
i=1
[
??
x
y
(
d
+
i
(x, y), d
?
i
(x, y)
)
?w???
(
d
+
i
(x, y), d
?
i
(x, y)
)]
+
(7)
`
PRO
(D,w) =
?
?x,y??D
?
d
1
,d
2
?B
|x|
(x), ??
y
(d
1
,d
2
)>0
log
(
1 + exp(?w???(d
1
, d
2
))
)
(8)
`
SA-PRO
(D,w) =
?
?x,y??D
|x|
?
i=1
?
d
1
,d
2
?B
i
(x), ??
x
y
(d
1
,d
2
)>0
log
(
1 + exp(?w???(d
1
, d
2
))
)
(9)
Figure 4: Loss functions of MIRA, SA-MIRA, PRO, and SA-PRO. The differences between traditional and search-
aware versions are highlighted in gray. The hope and fear derivations are defined in Equations 10?13, and we
define ??
y
(d
1
, d
2
) = ?
y
(d
1
)? ?
y
(d
2
), and ??
x
y
(d
1
, d
2
) = ?
x
y
(d
1
)? ?
x
y
(d
2
). In addition, [?]
+
= max{?, 0}.
and fear derivations from the final bin to all bins:
d
+
i
(x, y) = argmax
d?B
i
(x)
w
0
??(d)? ?
y
(d) (12)
d
?
i
(x, y) = argmax
d?B
i
(x)
w
0
??(d) + ?
y
(d) (13)
The new loss function for SA-MIRA is Eq. 7 in
Figure 4. Now instead of one update per sentence,
we will perform |x| updates, each based on a pair
d
+
i
(x, y) and d
?
i
(x, y).
4.3 From PRO to Search-Aware PRO
Finally, the PRO algorithm (Hopkins and May,
2011; Green et al., 2013) aims to correlate the
ranking under model score and the ranking un-
der BLEU score, among all complete derivations
in the final bin. For each preference-pair d
1
, d
2
?
B
|x|
(x) such that d
1
has a higher BLEU score than
d
2
(i.e., ?
y
(d
1
) < ?
y
(d
2
)), we add one positive ex-
ample ?(d
1
) ? ?(d
2
) and one negative example
?(d
2
)??(d
1
).
Now to adapt it to search-aware PRO (SA-
PRO), we will have many more examples to con-
sider: besides the final bin, we will include all
preference-pairs in the non-final bins as well. For
each bin B
i
(x), for each preference-pairs d
1
, d
2
?
B
i
(x) such that d
1
has a higher partial or potential
BLEU score than d
2
(i.e., ?
x
y
(d
1
) < ?
x
y
(d
2
)), we
add one positive example ?(d
1
)??(d
2
) and one
negative example ?(d
2
)??(d
1
). In sum, search-
aware PRO has |x| times more examples than tradi-
tional PRO. The loss functions of PRO and search-
aware PRO are defined in Figure 4.
5 Experiments
We evaluate our new tuning methods on two large
scale NIST translation tasks: Chinese-to-English
(CH-EN) and English-to-Chinese (EN-CH) tasks.
5.1 System Preparation and Data
We base our experiments on Cubit
2
(Huang and
Chiang, 2007), a state-of-art phrase-based system
in Python. We set phrase-limit to 7, beam size to
30 and distortion limit 6. We use the 11 dense
features from Moses (Koehn et al., 2007), which
can lead to good performance and are widely used
in almost all SMT systems. The baseline tuning
methods MERT (Och, 2003), MIRA (Cherry and
Foster, 2012), and PRO (Hopkins and May, 2011)
are from the Moses toolkit, which are batch tuning
methods based on k-best translations. The search-
aware tuning methods are called SA-MERT, SA-
MIRA, and SA-PRO, respectively. Their partial
BLEU versions are marked with superscript
1
and
their potential BLEU versions are marked with su-
perscript
2
, as explained in Section 3. All these
search-aware tuning methods are implemented on
the basis of Moses toolkit. They employ the de-
2
http://www.cis.upenn.edu/
?
lhuang3/cubit/
1946
Methods nist03 nist04 nist05 nist06 nist08 avg
MERT 33.6 35.1 33.4 31.6 27.9 ?
SA-MERT
par
-0.2 +0.0 +0.1 -0.1 -0.1 ?
SA-MERT
pot
+0.8 +1.1 +0.9 +1.7 +1.5 +1.2
MIRA 33.5 35.2 33.5 31.6 27.6 ?
SA-MIRA
par
+0.3 +0.3 +0.4 +0.4 +0.6 ?
SA-MIRA
pot
+1.3 +1.6 +1.4 +2.2 +2.6 +1.8
PRO 33.3 35.1 33.3 31.1 27.5 ?
?
SA-PRO
par
-2.0 -2.7 -2.2 -1.0 -1.7 ?
?
SA-PRO
pot
+0.8 +0.5 +1.0 +1.6 +1.6 +1.1
Table 2: CH-EN task: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08).
par
: partial BLEU;
pot
:
potential BLEU.
?
: SA-PRO tunes on only 109 short sentences (with less than 10 words) from nist02.
Final bin All bins
MERT 35.5 28.2
SA-MERT -0.1 +3.1
Table 3: Evaluation on nist02 tuning set using two
methods: BLEU is used to evaluate 1-best complete
translations in the final bin; while potential BLEU is
used to evaluate 1-best partial translations in all bins.
The search-aware objective cares about (the potential
of) all bins, not just the final bin, which can explain this
result.
fault settings following Moses toolkit: for MERT
and SA-MERT, the stop condition is defined by the
weight difference threshold; for MIRA, SA-MIRA,
PRO and SA-PRO, their stop condition is defined
by max iteration set to 25; for all tuning methods,
we use the final weight for testing.
The training data for both CH-EN and EN-CH
tasks is the same, and it is collected from the
NIST2008 Open Machine Translation Campaign.
It consists of about 1.8M sentence pairs, including
about 40M/48M words in Chinese/English sides.
For CH-EN task, the tuning set is nist02 (878
sents), and test sets are nist03 (919 sents), nist04
(1788 sents), nist05 (1082 sents), nist06 (616 sents
from news portion) and nist08 (691 from news por-
tion). For EN-CH task, the tuning set is ssmt07
(995 sents)
3
, and the test set is nist08 (1859 sents).
For both tasks, all the tuning and test sets contain
4 references.
We use GIZA++ (Och and Ney, 2003) for word
alignment, and SRILM (Stolcke, 2002) for 4-gram
language models with the Kneser-Ney smoothing
3
On EN-CH task, there is only one test set available for us,
and thus we use ssmt07 as the tuning set, which is provided
at the Third Symposium on Statistical Machine Translation
(http://mitlab.hit.edu.cn/ssmt2007.html).
option. The LM for EN-CH is trained on its target
side; and that for CH-EN is trained on the Xin-
hua portion of Gigaword. We use BLEU-4 (Pap-
ineni et al., 2002) with ?average ref-len? to evalu-
ate the translation performance for all experiments.
In particular, the character-based BLEU-4 is em-
ployed for EN-CH task. Since all tuning meth-
ods involve randomness, all scores reported are av-
erage of three runs, as suggested by Clark et al.
(2011) for fairer comparisons.
5.2 Main Results on CH-EN Task
Table 2 depicts the main results of our methods on
CH-EN translation task. On all five test sets, our
methods consistently achieve substantial improve-
ments with two pruning options: SA-MERT
pot
gains +1.2 BLEU points over MERT on average;
and SA-MIRA
pot
gains +1.8 BLEU points over
MIRA on average as well. SA-PRO
pot
, however,
does not work out of the box when we use the en-
tire nist02 as the tuning set, which might be at-
tributed to the ?Monster? behavior (Nakov et al.,
2013). To alleviate this problem, we only use the
109 short sentences with less than 10 words from
nist02 as our new tuning data. To our supurise,
this trick works really well (despite using much
less data), and also made SA-PRO
pot
an order of
magnitude faster. This further confirms that our
search-aware tuning is consistent across all tuning
methods and datasets.
As discussed in Section 3, evaluation metrics
of partial derivations are crucial for search-aware
tuning. Besides the principled ?potential BLEU?
version of search-aware tuning (i.e. SA-MERT
pot
,
SA-MIRA
pot
, and SA-PRO
pot
), we also run the
simple ?partial BLEU? version of search-aware
tuning (i.e. SA-MERT
par
, SA-MIRA
par
, and SA-
1947
30
31
32
33
34
35
 1  2  4  8  16  32  64
B
LE
U
Beam Size
Traditional MERT Tuning
Search-aware MERT Tuning
Figure 5: BLEU scores against beam size on nist05.
Our search-aware tuning can achieve (almost) the same
BLEU scores with much smaller beam size (beam of 4
vs. 16).
methods nist02 nist05
1-best
MERT 35.5 33.4
SA-MERT -0.1 +0.9
Oracle
MERT 44.3 41.1
SA-MERT +0.5 +1.6
Table 4: The k-best oracle BLEU comparison between
MERT and SA-MERT.
PRO
par
). In Table 2, we can see that they may
achieve slight improvements over tradition tuning
on some datasets, but SA-MERT
pot
, SA-MIRA
pot
,
and SA-PRO
pot
using potential BLEU consistently
outperform them on all the datasets.
Even though our search-aware tuning gains sub-
stantially on all test sets, it does not gain signif-
icantly on nist02 tuning set. The main reason is
that, search-aware tuning optimizes an objective
(i.e. BLEU for all bins) which is different from
the objective for evaluation (i.e. BLEU for the final
bin), and thus it is not quite fair to evaluate the
complete translations for search-aware tuning as
the same done for traditional tuning on the tuning
set. Actally, if we evaluate the potential BLEU for
all partial translations, we find that search-aware
tuning gains about 3.0 BLEU on nist02 tuning set,
as shown in Table 3.
5.3 Analysis on CH-EN Task
Different beam size. Since our search-aware tun-
ing considers the rankings of partial derivations
in the middle bins besides complete ones in the
last bin, ideally, if the weight learned by search-
aware tuning can exactly evaluate partial deriva-
Diversity nist02 nist05
MERT 0.216 0.204
SA-MERT 0.227 0.213
Table 5: The diversity comparison based on the k-best
list in the final bin on both tuning and nist05 test sets
by tuning methods. The higher the metric is, the more
diverse the k-best list is.
tions, then accurate partial derivations will rank
higher according to model score. In this way, even
with small beam size, these accurate partial deriva-
tions may still survive in the bins. Therefore, it
is expected that search-aware tuning can achieve
good performance with smaller beam size. To
justify our conjecture, we run SA-MERT
pot
with
different beam size (2,4,8,16,30,100), its testing
results on nist05 are depicted in Figure 5. our
mehtods achieve better trade-off between perfor-
mance and efficiey. Figure 5 shows that search-
aware tuning is consistent with all beam sizes, and
as a by-product, search-aware MERT with a beam
of 4 can achieve almost identical BLEU scores to
MERT with beam of 16.
Oracle BLEU. In addition, we examine the BLEU
ponits of oracle for MERT and SA-MERT. We
use the weight tuned by MERT and SA-MERT for
k-best decoding on nist05 test set, and calculate
the oracle over these two k-best lists. The oracle
BLEU comparison is shown in Table 4. On nist05
test set, for MERT the oracle BLEU is 41.1; while
for SA-MERT its oracle BLEU is 42.7, i.e. with 1.6
BLEU improvements. Although search-aware tun-
ing employs the objective different from the objec-
tive of evaluation on nist02 tuning set, it still gains
0.5 BLEU improvements.
Diversity. A k-best list with higher diversity can
better represent the entire decoding space, and thus
tuning on such a k-best list may lead to better
tesing performance (Gimpel et al., 2013). Intu-
itively, tuning with all bins will encourage the di-
versity in prefix, infix and suffix of complete trans-
lations in the final bin. To testify this, we need a
diversity metric.
Indeed, Gimpel et al. (2013) define a diversity
metric based on the n-gram matches between two
sentences y and y
?
as follows:
d(y, y
?
) = ?
|y|?q
?
i=1
|y
?
|?q
?
j=1
[[y
i:i+q
= y
?
j:j+q
]]
1948
Methods
tuning set test sets (4-refs)
set # refs # sents # words nist03 nist04 nist05 nist06 nist08
MERT nist02 4 878 23181 33.6 35.1 33.4 31.6 27.9
SA-MERT
pot
nist02 4 878 23181 34.4 36.2 34.3 33.3 29.4
MAXFORCE nist02-px 1 434 6227 29.0 30.3 28.7 26.8 24.1
MAXFORCE train-r-part 1 1225 22684 31.7 33.5 31.5 30.3 26.7
MERT nist02-r 1 92 1173 31.6 32.7 31.3 29.3 25.9
SA-MERT
pot
nist02-r 1 92 1173 33.5 35.0 33.4 31.5 28.0
Table 6: Comparisons with MAXFORCE in terms of BLEU. nist02-px is the non-trivial reachable prefix-data from
nist02 via forced decoding; nist02-r is a subset of nist02-px consisting of the fully reachable data; train-r is a
subset of fully reachable data from training data that is comparable in size to nist02. All experiments use only
dense features.
where q = n? 1, and [[x]] equals to 1 if x is true, 0
otherwise. This metric, however, has the following
critical problems:
? it is not length-normalized: longer strings will
look as if they are more different.
? it suffers from duplicates in n-grams. Af-
ter normalization, d(y, y) will exceed -1 for
any y. In the extreme case, consider y
1
=
?the the the the? and y
2
= ?the ... the? with
10 the?s then will be considered identical af-
ter normalization by length.
So we define a balanced metric based on their met-
ric
d
?
(y, y
?
) = 1?
2? d(y, y
?
)
d(y, y) + d(y
?
, y
?
)
which satisfies the following nice properties:
? d
?
(y, y) = 0 for all y;
? 0 ? d
?
(y, y
?
) ? 1 for all y, y
?
;
? d
?
(y, y
?
) = 1 if y and y
?
is completely dis-
joint.
? it does not suffer from duplicates, and can dif-
ferentiate y
1
and y
2
defined above.
With this new metric, we evaluate the diversity
of k-best lists for both MERT and SA-MERT. As
shown in Table 5, on both tuning and test sets the
k-best list generated by SA-MERT is more diverse.
5.4 Comparison with Max-Violation
Perceptron
Our method considers the rankings of partial
derivations, which is simlar to MAXFORCE
B`ush?? y?u Sh?al?ong j?ux??ng hu`?t?an
Bush and Sharon held a meeting
Bush held talks with Sharon
qi?angsh?ou b`ei j??ngf?ang j??b`?
police killed the gunman
the gunman was shot dead
?
B`ush?? y?u Sh?al?ong j?ux??ng hu`?t?an Bush and Sharon held a meeting
B`ush?? y?u Sh?al?ong j?ux??ng hu`?t?an Bush held talks with Sharon
qi?angsh?ou b`ei j??ngf?ang j??b`? police killed the gunman
qi?angsh?ou b`ei j??ngf?ang j??b`? the gunman was shot dead
Figure 6: Transformation of a tuning set in forced de-
coding for MAXFORCE: the original tuning set (on the
top) contains 2 source sentences with 2 references for
each; while the transformed set (on the bottom) con-
tains 4 source sentences with one reference for each.
method (Yu et al., 2013), and thus we re-
implement MAXFORCE method. Since the nist02
tuning set contains 4 references and forced decod-
ing is performed for only one reference, we enlarge
the nist02 set to a variant set following the trans-
formation in Figure 6, and obtain a variant tun-
ing set denoted as nist02-px, which consists of 4-
times sentence-pairs. On nist02-px, the non-trivial
reachable prefix-data only accounts for 12% sen-
tences and 7% words. Both these sentence-level
and the word-level percentages are much lower
than those on the training data as shown in Ta-
ble 3 from (Yu et al., 2013). This is because there
are many OOV words on a tuning set. We run the
MAXFORCE with dense feature setting on nist02-
px and its testing results are shown in Table 6. We
can see that on all the test sets, its testing perfor-
mance is lower than that of SA-MERT
pot
tuning on
nist02 with about 5 BLEU points.
For more direct comparisons, we run MERT and
SA-MERT
pot
on a data set similar to nist02-px. We
pick up the fully reachable sentences from nist02-
px, remove the sentence pairs with the same source
side, and get a new tuning set denoted as nist02-r.
When tuning on nist02-r, we find that MERT is bet-
1949
Methods tuning-set nist08
MERT ssmt07 31.3
MAXFORCE train-r-part 29.9
SA-MERT
par
ssmt07 31.3
SA-MERT
pot
ssmt07 31.7
Table 7: EN-CH task: BLEU scores on nist08 test set for
MERT, SA-MERT, and MAXFORCE on different tun-
ing sets. train-r-part is a part of fully reachable data
from training data via forced decoding. All the tuning
methods run with dense feature set.
ter than MAXFORCE,
4
and SA-MERT
pot
are much
better than MERT on all the test sets. In addition,
we select about 1.2k fully reachable sentence pairs
from training data, and run the forced decoding
on this new tuning data (denoted as train-r-part),
which is with similar size to nist02.
5
With more
tuning data, the performance of max-violation is
improved largely, but it is still underperformed by
SA-MERT
pot
.
5.5 Results on EN-CH Translation Task
We also run our search-aware tuning method on
EN-CH task. We use SA-MERT as the representa-
tive of search-aware tuning methods, and compare
its two versions with other tuning methods MERT,
MAXFORCE. For MAXFORCE, we first run forced
decoding on the training data and then select about
1.2k fully reachable sentence pairs as its tuning
set (denoted as train-r-part). For MERT, SA-
MERT
pot
, and SA-MERT
par
, their tuning set is
ssmt07. Table 7 shows that SA-MERT
pot
is much
better than MAXFORCE, i.e. it achieves 0.4 BLEU
improvements over MERT. Finally, comparison
between SA-MERT
pot
and SA-MERT
par
shows
that the potential BLEU is better for evaluation of
partial derivations.
5.6 Discussions on Tuning Efficiency
As shown in Figure 2, search-aware tuning consid-
ers all partial translations in the middle bins beside
all complete translations in the last bin, and thus its
total number of training examples is much greater
than that of the traditional tuning. In details, sup-
4
Under the dense feature setting, MAXFORCE is worse
than standard MERT. This result is consistent with that in
Figure 12 of (Yu et al., 2013).
5
We run MAXFORCE on train-r-part, i.e. a part of reach-
able data instead of the entire reachable data, as we found
that more tuning data does not necessarily lead to better test-
ing performance under dense feature setting in our internal
experiments.
Optimization time MERT MIRA PRO
basline 3 2 2
search-aware 50 7 6
Table 8: Search-aware tuning slows down MERT sig-
nificantly, and MIRA and PRO moderately. The time (in
minutes) is for optimization only (excluding decoding)
and measured at the last iteration during the entire tun-
ing (search aware tuning does not increase the number
of iterations in our experiments). The decoding time is
20 min. on a single CPU but can be parallelized.
pose the tuning data consists of two sentences with
length 10 and 30, respectively. Then, for tradi-
tional tuning its number of training examples is 2;
but for search-aware tuning, the total number is 40.
More training examples makes our search-aware
tuning slower than the traditional tuning.
Table 8 shows the training time comparisons
between search-aware tuning and the traditional
tuning. From this Table, one can see that both
SA-MIRA and SA-PRO are with the same order
of magtitude as MIRA and PRO; but SA-MERT
is much slower than MERT. The main reason is
that, as the training examples increase dramati-
cally, the envelope calculation for exact line search
(see (Och, 2003)) in MERT is less efficient than the
update based on (sub-)gradient with inexact line
search in MIRA and PRO.
One possible solution to speed up SA-MERT is
the parallelization but we leave it for future work.
6 Related Work
Many tuning methods have been proposed for
SMT so far. These methods differ by the ob-
jective function or training mode: their objective
functions are based on either evaluation-directed
loss (Och, 2003; Galley and Quirk, 2011; Gal-
ley et al., 2013) or surrogate loss (Hopkins and
May, 2011; Gimpel and Smith, 2012; Eidelman
et al., 2013); they are either batch (Och, 2003;
Hopkins and May, 2011; Cherry and Foster, 2012)
or online mode (Watanabe, 2012; Simianer et al.,
2012; Flanigan et al., 2013; Green et al., 2013).
These methods share a common characteristic:
they learn a weight by iteratively reranking a set of
complete translations represented by k-best (Och,
2003; Watanabe et al., 2007; Chiang et al., 2008)
or lattice (hypergraph) (Tromble et al., 2008; Ku-
mar et al., 2009), and they do not care about search
errors that potential partial translations may be
pruned during decoding, even if they agree with
1950
that their decoders are built on the beam pruning
based search.
On the other hand, it is well-known that search
errors can undermine the standard training for
many beam search based NLP systems (Huang et
al., 2012). As a result, Collins and Roark (2004)
and Huang et al. (2012) propose the early-update
and max-violation update to deal with the search
errors. Their idea is to update on prefix or par-
tial hypotheses when the correct solution falls out
of the beam. This idea has been successfully
used in many NLP tasks and improves the perfor-
mance over the state-of-art NLP systems (Huang
and Sagae, 2010; Huang et al., 2012; Zhang et al.,
2013).
Goldberg and Nivre (2012) propose the concept
of ?dynamic oracle? which is the absolute best po-
tential of a partial derivation, and is more akin to
a strictly admissible heuristic. This idea inspired
and is closely related to our potential BLEU, except
that in our case, computing an admissible heuristic
is too costly, so our potential BLEU is more like an
average potential.
Gesmundo and Henderson (2014) also consider
the rankings between partial translation pairs as
well. However, they evaluate a partial translation
through extending it to a complete translation by
re-decoding, and thus they need many passes of
decoding for many partial translations, while ours
only need one pass of decoding for all partial trans-
lations and thus is much more efficient. In sum-
mary, our tuning framework is more general and
has potential to be employed over all the state-of-
art tuning methods mentioned above, even though
ours is only tested on three popular methods.
7 Conclusions and Future Work
We have presented a simple yet powerful approach
of ?search-aware tuning? by promoting promising
partial derivations, and this idea can be applied to
all three popular tuning methods. To solve the key
challenge of evaluating partial derivations, we de-
velop a concept of ?potential BLEU? inspired by
future cost in MT decoding. Extensive experi-
ments confirmed substantial BLEU gains with only
dense features. We believe our framework can be
applied to sparse feature settings and other transla-
tion paradigms, and potentially to other structured
prediction problems (such as incremental parsing)
as well.
Acknowledgements
We thank the three anonymous reviewers for sug-
gestions, and Kai Zhao and Feifei Zhai for dis-
cussions. In particular, we thank reviewer #3 and
Chin-Yew Lin for pushing us to think about di-
versity. This project was supported by DARPA
FA8750-13-2-0041 (DEFT), NSF IIS-1449278, a
Google Faculty Research Award, and a PSC-
CUNY Award.
References
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montr?eal,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL 2011.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Vladimir Eidelman, Yuval Marton, and Philip Resnik.
2013. Online relative margin maximization for sta-
tistical machine translation. In Proceedings of ACL,
pages 1116?1126, Sofia, Bulgaria, August.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of NAACL-HLT, pages 248?258, At-
lanta, Georgia, June.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
EMNLP, pages 38?49, Edinburgh, Scotland, UK.,
July.
Michel Galley, Chris Quirk, Colin Cherry, and Kristina
Toutanova. 2013. Regularized minimum error rate
training. In Proceedings of EMNLP, pages 1948?
1959, Seattle, Washington, USA, October.
Andrea Gesmundo and James Henderson. 2014. Undi-
rected machine translation with discriminative rein-
forcement learning. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, April.
1951
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of NAACL-HLT, pages 221?
231, Montr?eal, Canada, June.
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory
Shakhnarovich. 2013. A systematic exploration of
diversity in machine translation. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, October.
Yoav Goldberg and Joakim Nivre. 2012. Training
deterministic parsers with non-deterministic oracles.
In Proceedings of COLING 2012.
Spence Green, Sida Wang, Daniel Cer, and Christopher
Manning. 2013. Fast and adaptive online training
of feature-rich translation models. In Proc. of ACL
2013.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A for-
mal basis for the heuristic determination of minimum
cost paths. IEEE Transactions on Systems Science
and Cybernetics, 4(2):100?107.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings of
ACL: Demonstrations.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
ACL-IJCNLP, Suntec, Singapore, August.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of HLT-NAACL Short Pa-
pers.
Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv., 40(3).
Preslav Nakov, Francisco Guzmn, and Stephan Voge.
2013. A tale about pro and monsters. In Proceedings
of ACL Short Papers.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318, Philadephia, USA, July.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of ACL, pages 11?21, Jeju Island, Ko-
rea, July.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, pages 620?629, Honolulu,
Hawaii, October.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Taro Watanabe. 2012. Optimized online rank learning
for machine translation. In Proceedings of NAACL-
HLT, pages 253?262, Montr?eal, Canada, June.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In Proceedings of
EMNLP 2013.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-
ald. 2013. Online learning with inexact hypergraph
search. In Proceedings of EMNLP 2013.
Kai Zhao, Liang Huang, Haitao Mi, and Abe Itty-
cheriah. 2014. Hierarchical mt training using max-
violation perceptron. In Proceedings of ACL, Balti-
more, Maryland, June.
1952
Binarization of Synchronous
Context-Free Grammars
Liang Huang?
USC/Information Science Institute
Hao Zhang??
Google Inc.
Daniel Gildea?
University of Rochester
Kevin Knight?
USC/Information Science Institute
Systems based on synchronous grammars and tree transducers promise to improve the quality
of statistical machine translation output, but are often very computationally intensive. The
complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings
between the two languages. We develop a theory of binarization for synchronous context-free
grammars and present a linear-time algorithm for binarizing synchronous rules when possible.
In our large-scale experiments, we found that almost all rules are binarizable and the resulting
binarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-
based machine translation system. We also discuss the more general, and computationally more
difficult, problem of finding good parsing strategies for non-binarizable rules, and present an
approximate polynomial-time algorithm for this problem.
1. Introduction
Several recent syntax-based models for machine translation (Chiang 2005; Galley et al
2004) can be seen as instances of the general framework of synchronous grammars
and tree transducers. In this framework, both alignment (synchronous parsing) and
decoding can be thought of as parsing problems, whose complexity is in general ex-
ponential in the number of nonterminals on the right-hand side of a grammar rule.
To alleviate this problem, we investigate bilingual binarization as a technique to fac-
tor each synchronous grammar rule into a series of binary rules. Although mono-
lingual context-free grammars (CFGs) can always be binarized, this is not the case
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: lhuang@isi.edu,
liang.huang.sh@gmail.com.
?? 1600 Amphitheatre Parkway, Mountain View, CA 94303. E-mail: haozhang@google.com.
? Computer Science Dept., University of Rochester, Rochester NY 14627. E-mail: gildea@cs.rochester.edu.
? Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292. E-mail: knight@isi.edu.
Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication:
25 March 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
for all synchronous rules; we investigate algorithms for non-binarizable rules as well.
In particular:
r We develop a technique called synchronous binarization and devise a
linear-time binarization algorithm such that the resulting rule set alows
efficient algorithms for both synchronous parsing and decoding with
integrated n-gram language models.
r We examine the effect of this binarization method on end-to-end
translation quality on a large-scale Chinese-to-English syntax-based
system, compared to a more typical baseline method, and a state-of-the-art
phrase-based system.
r We examine the ratio of binarizability in large, empirically derived rule
sets, and show that the vast majority is binarizable. However, we also
provide, for the first time, real examples of non-binarizable cases verified
by native speakers.
r In the final, theoretical, sections of this article, we investigate the general
problem of finding the most efficient synchronous parsing or decoding
strategy for arbitrary synchronous context-free grammar (SCFG) rules,
including non-binarizable cases. Although this problem is believed to be
NP-complete, we prove two results that substantially reduce the search
space over strategies. We also present an optimal algorithm that runs
tractably in practice and a polynomial-time algorithm that is a good
approximation of the former.
Melamed (2003) discusses binarization of multi-text grammars on a theoretical
level, showing the importance and difficulty of binarization for efficient synchronous
parsing. One way around this difficulty is to stipulate that all rules must be binary
from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary
SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.
In contrast, the rule extraction method of Galley et al (2004) aims to incorporate more
syntactic information by providing parse trees for the target language and extracting
tree transducer rules that apply to the parses. This approach results in rules with many
nonterminals, making good binarization techniques critical.
We explain how synchronous rule binarization interacts with n-gram language
models and affects decoding for machine translation in Section 2. We define binarization
formally in Section 3, and present an efficient algorithm for the problem in Section 4.
Experiments described in Section 51 show that binarization improves machine trans-
lation speed and quality. Some rules cannot be binarized, and we present a decoding
strategy for these rules in Section 6. Section 7 gives a solution to the general theo-
retical problem of finding optimal decoding and synchronous parsing strategies for
arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our
Chinese?English data. These final two sections are of primarily theoretical interest, as
nonbinarizable rules have not been shown to benefit real-world machine translation sys-
tems. However, the algorithms presented may become relevant as machine translation
systems improve.
1 A preliminary version of Section 1?5 appeared in Zhang et al (2006).
560
Huang et al Binarization of Synchronous Context-Free Grammars
2. Motivation
Consider the following Chinese sentence and its English translation:
(1) ?
Ba`owe?ier
Powell

yu?
with
??
Sha?lo?ng
Sharon
>L
ju?x??ng
hold
?
le
[past.]
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142?151,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Perceptron with Inexact Search
Liang Huang
Information Sciences Institute
University of Southern California
liang.huang.sh@gmail.com
Suphan Fayong
Dept. of Computer Science
University of Southern California
suphan.ying@gmail.com
Yang Guo
Bloomberg L.P.
New York, NY
yangg86@gmail.com
Abstract
Most existing theory of structured prediction
assumes exact inference, which is often in-
tractable in many practical problems. This
leads to the routine use of approximate infer-
ence such as beam search but there is not much
theory behind it. Based on the structured
perceptron, we propose a general framework
of ?violation-fixing? perceptrons for inexact
search with a theoretical guarantee for conver-
gence under new separability conditions. This
framework subsumes and justifies the pop-
ular heuristic ?early-update? for perceptron
with beam search (Collins and Roark, 2004).
We also propose several new update meth-
ods within this framework, among which the
?max-violation? method dramatically reduces
training time (by 3 fold as compared to early-
update) on state-of-the-art part-of-speech tag-
ging and incremental parsing systems.
1 Introduction
Discriminative structured prediction algorithms
such as conditional random fields (Lafferty et al,
2001), structured perceptron (Collins, 2002), max-
margin markov networks (Taskar et al, 2003), and
structural SVMs (Tsochantaridis et al, 2005) lead
to state-of-the-art performance on many structured
prediction problems such as part-of-speech tagging,
sequence labeling, and parsing. But despite their
success, there remains a major problem: these learn-
ing algorithms all assume exact inference (over an
exponentially-large search space), which is needed
to ensure their theoretical properties such as conver-
gence. This exactness assumption, however, rarely
holds in practice since exact inference is often in-
tractable in many important problems such as ma-
chine translation (Liang et al, 2006), incremen-
tal parsing (Collins and Roark, 2004; Huang and
Sagae, 2010), and bottom-up parsing (McDonald
and Pereira, 2006; Huang, 2008). This leads to
routine use of approximate inference such as beam
search as evidenced in the above-cited papers, but
the inexactness unfortunately abandons existing the-
oretical guarantees of the learning algorithms, and
besides notable exceptions discussed below and in
Section 7, little is known for theoretical properties
of structured prediction under inexact search.
Among these notable exceptions, many exam-
ine how and which approximations break theoretical
guarantees of existing learning algorithms (Kulesza
and Pereira, 2007; Finley and Joachims, 2008), but
we ask a deeper and practically more useful ques-
tion: can we modify existing learning algorithms to
accommodate the inexactness in inference, so that
the theoretical properties are still maintained?
For the structured perceptron, Collins and Roark
(2004) provides a partial answer: they suggest vari-
ant called ?early update? for beam search, which up-
dates on partial hypotheses when the correct solution
falls out of the beam. This method works signif-
icantly better than standard perceptron, and is fol-
lowed by later incremental parsers, for instance in
(Zhang and Clark, 2008; Huang and Sagae, 2010).
However, two problems remain: first, up till now
there has been no theoretical justification for early
update; and secondly, it makes learning extremely
slow as witnessed by the above-cited papers because
it only learns on partial examples and often requires
15?40 iterations to converge while normal percep-
tron converges in 5?10 iterations (Collins, 2002).
We develop a theoretical framework of ?violation-
fixing? perceptron that addresses these challenges.
In particular, we make the following contributions:
? We show that, somewhat surprisingly, exact
142
search is not required by perceptron conver-
gence. All we need is that each update involves
a ?violation?, i.e., the 1-best sequence has a
higher model score than the correct sequence.
Such an update is considered a ?valid update?,
and any perceptron variant that maintains this
is bound to converge. We call these variants
?violation-fixing perceptrons? (Section 3.1).
? This theory explains why standard perceptron
update may fail to work with inexact search,
because violation is no longer guaranteed: the
correct structure might indeed be preferred by
the model, but was pruned during the search
process (Sec. 3.2). Such an update is thus con-
sidered invalid, and experiments show that in-
valid updates lead to bad learning (Sec. 6.2).
? We show that the early update is always valid
and is thus a special case in our framework; this
is the first theoretical justification for early up-
date (Section 4). We also show that (a variant
of) LaSO (Daume? and Marcu, 2005) is another
special case (Section 7).
? We then propose several other update meth-
ods within this framework (Section 5). Experi-
ments in Section 6 confirm that among them,
the max-violation method can learn equal or
better models with dramatically reduced learn-
ing times (by 3 fold as compared to early
update) on state-of-the-art part-of-speech tag-
ging (Collins, 2002)1 and incremental parsing
(Huang and Sagae, 2010) systems. We also
found strong correlation between search error
and invalid updates, suggesting that the ad-
vantage of valid update methods is more pro-
nounced with harder inference problems.
Our techniques are widely applicable to other str-
cutured prediction problems which require inexact
search like machine translation and protein folding.
2 Structured Perceptron
We review the convergence properties of the stan-
dard structured perceptron (Collins, 2002) in our
1Incidentally, we achieve the best POS tagging accuracy to
date (97.35%) on English Treebank by early update (Sec. 6.1).
Algorithm 1 Structured Perceptron (Collins, 2002).
Input: data D = {(x(t), y(t))}nt=1 and feature map ?
Output: weight vector w
Let: EXACT(x,w) ?= argmaxs?Y(x) w ??(x, s)
Let: ??(x, y, z) ?= ?(x, y)??(x, z)
1: repeat
2: for each example (x, y) in D do
3: z ? EXACT(x,w)
4: if z 6= y then
5: w? w + ??(x, y, z)
6: until converged
own notations that will be reused in later sections
for non-exact search. We first define a new concept:
Definition 1. The standard confusion set Cs(D)
for training data D = {(x(t), y(t))}nt=1 is the set of
triples (x, y, z) where z is a wrong label for input x:
Cs(D) ?= {(x, y, z) | (x, y) ? D, z ? Y(x)? {y}}.
The rest of the theory, including separation and
violation, all builds upon this concept. We call such
a triple S = ?D,?, C? a training scenario, and
in the remainder of this section, we assume C =
Cs(D), though later we will define other confusion
sets to accommodate other update methods.
Definition 2. The training scenario S = ?D,?, C?
is said to be linearly separable (i.e., dataset D is
linearly separable in C by representation ?) with
margin ? > 0 if there exists an oracle vector u
with ?u? = 1 such that it can correctly classify
all examples in D (with a gap of at least ?), i.e.,
?(x, y, z) ? C,u ? ??(x, y, z) ? ?. We define
the maximal margin ?(S) to be the maximal such
margin over all unit oracle vectors:
?(S) ?= max
?u?=1
min
(x,y,z)?C
u ???(x, y, z).
Definition 3. A triple (x, y, z) is said to be a vi-
olation in training scenario S = ?D,?, C? with
respect to weight vector w if (x, y, z) ? C and
w ???(x, y, z) ? 0.
Intuitively, this means model w is possible to mis-
label example x (though not necessarily to z) since
y is not its single highest scoring label under w.
Lemma 1. Each update triple (x, y, z) in Algo-
rithm 1 (line 5) is a violation in S = ?D,?, Cs(D)?.
143
Proof. z = EXACT(x,w), thus for all z? ? Y(x),
w ??(x, z) ? w ??(x, z?), i.e., w ???(x, y, z) ? 0.
On the other hand, z ? Y(x) and z 6= y, so
(x, y, z) ? Cs(D).
This lemma basically says exact search guaran-
tees violation in each update, but as we will see in
the convergence proof, violation itself is more fun-
damental than search exactness.
Definition 4. The diameter R(S) for scenario S =
?D,?, C? is max(x,y,z)?C???(x, y, z)?.
Theorem 1 (convergence, Collins). For a separable
training scenario S = ?D,?, Cs(D)? with ?(S) >
0, the perceptron algorithm in Algorithm 1 will make
finite number of updates (before convergence):
err(S) ? R2(S)/?2(S).
Proof. Let w(k) be the weight vector before the kth
update; w(0) = 0. Suppose the kth update happens
on the triple (x, y, z). We will bound ?w(k+1)? from
two directions:
1. w(k+1) = w(k) + ??(x, y, z). Since scenario
S is separable with max margin ?(S), there ex-
ists a unit oracle vector u that achieves this
margin. Dot product both sides with u, we have
u ?w(k+1) = u ?w(k) + u ???(x, y, z)
? u ?w(k) + ?(S)
by Lemma 1 that (x, y, z) ? Cs(D) and by the
definition of margin. By induction, we have
u ? w(k+1) ? k?(S). Since for any two vec-
tors a and b we have ?a??b? ? a ? b, thus
?u??w(k+1)? ? u ?w(k+1) ? k?(S). As u is
a unit vector, we have ?w(k+1)? ? k?(S).
2. On the other hand, since ?a + b?2 = ?a?2 +
?b?2 +2 a ?b for any vectors a and b,we have
?w(k+1)?2 = ?w(k) + ??(x, y, z)?2
= ?w(k)?2 + ???(x, y, z)?2
+ 2 w(k) ???(x, y, z)
? ?w(k)?2 + R2(S) + 0 .
By Lemma 1, the update triple is a violation so
that w(k) ???(x, y, z) ? 0, and that (x, y, z) ?
Cs(D) thus ???(x, y, z)?2 ? R2(S) by the
definition of diameter. By induction, we have
?w(k+1)?2 ? kR2(S).
Algorithm 2 Local Violation-Fixing Perceptron.
Input: training scenario S = ?D,?, C?
1: repeat
2: for each example (x, y) in D do
3: (x, y?, z) = FINDVIOLATION(x, y,w)
4: if z 6= y then ? (x, y?, z) is a violation
5: w? w + ??(x, y?, z)
6: until converged
Combining the two bounds, we have k2?2(S) ?
?w(k+1)?2 ? kR2(S), thus k ? R2(S)/?2(S).
3 Violation is All We Need
We now draw the central observation of this work
from part 2 of the above proof: note that exact search
(argmax) is not required in the proof, instead, it
just needs a violation, which is a much weaker con-
dition.2 Exact search is simply used to ensure viola-
tion. In other words, if we can guarantee violation in
each update (which we call ?valid update?), it does
not matter whether or how exact the search is.
3.1 Violation-Fixing Perceptron
This observation leads us to two generalized vari-
ants of perceptron which we call ?violation-fixing
perceptrons?. The local version, Algorithm 2 still
works on one example at a time, and searches for
one violation (if any) in that example to update with.
The global version, Algorithm 3, can update on any
violation in the dataset at any time. We state the fol-
lowing generalized theorem:
Theorem 2. For a separable training scenario S
the perceptrons in Algorithms 2 and 3 both con-
verge with the same update bounds of R2(S)/?2(S)
as long as the FINDVIOLATION and FINDVIO-
LATIONINDATA functions always return violation
triples if there are any.
Proof. Same as the proof to Theorem 1, except for
replacing Lemma 1 in part 2 by the fact that the up-
date triples are guaranteed to be violations. (Note a
violation triple is by definition in the confusion set,
thus we can still use separation and diameter).
These generic violation-fixing perceptron algo-
rithms can been seen as ?interfaces? (or APIs),
2Crammer and Singer (2003) further demonstrates that a
convex combination of violations can also be used for update.
144
Algorithm 3 Global Violation-Fixing Perceptron.
Input: training scenario S = ?D,?, C?
1: repeat
2: (x, y, z)? FINDVIOLATIONINDATA(C,w)
3: if x = ? then break ? no more violation?
4: w? w + ??(x, y, z)
5: until converged
data D = {(x, y)}:
x fruit flies fly .
y N N V .
search space: Y(x) = {N} ? {N, V} ? {N, V} ? {.}.
feature map: ?(x, y) = (#N?N(y), #V?.(y)).
iter label z ??(x, y, z) w??? new w
0 (0, 0)
1 N N N . (?1,+1) 0 ? (?1, 1)
2 N V N . (+1,+1) 0 ? (0, 2)
3 N N N . (?1,+1) 2 ? (?1, 3)
4 N V N . (+1,+1) 2 ? (0, 4)
... infinite loop ...
Figure 1: Example that standard perceptron does not
converge with greedy search on a separable scenario
(e.g. u = (1, 2) can separate D with exact search).
where later sections will supply different implemen-
tations of the FINDVIOLATION and FINDVIOLA-
TIONINDATA subroutines, thus establishing alterna-
tive update methods for inexact search as special
cases in this general framework.
3.2 Non-Convergence with Inexact Search
What if we can not guarantee valid updates? Well,
the convergence proof in Theorems 1 and 2 would
break in part 2. This is exactly why standard struc-
tured perceptron may not work well with inexact
search: with search errors it is no longer possible
to guarantee violation in each update. For example,
an inexact search method explores a (proper) subset
of the search space Y ?w(x) ( Y(x), and finds a label
z = argmax
s?Y ?w(x)
w ??(x, s).
It is possible that the correct label y is outside of
the explored subspace, and yet has a higher score:
??(x, y, z) > 0 but y /? Y ?w(x). In this case,
(x, y, z) is not a violation, which breaks the proof.
We show below that this situation actually exists.
Algorithm 4 Greedy Search.
Let: NEXT(x, z) ?= {z ? a | a ? Y|z|+1(x)} ? set of
possible one-step extensions (successors)
BEST(x, z,w) ?= argmaxz??NEXT(x,z) w ? ?(x, z?)
? best one-step extension based on history
1: function GREEDYSEARCH(x,w)
2: z ? ? ? empty sequence
3: for i ? 1 . . . |x| do
4: z ? BEST(x, z,w)
5: return z
Theorem 3. For a separable training scenario S =
?D,?, Cs(D)?, if the argmax in Algorithm 1 is not
exact, the perceptron might not converge.
Proof. See the example in Figure 1.
This situation happens very often in NLP: of-
ten the search space Y(x) is too big either because
it does not factor locally, or because it is still too
big after factorization, which requires some approxi-
mate search. In either case, updating the model w on
a non-violation (i.e., ?invalid?) triple (x, y, z) does
not make sense: it is not the model?s problem: w
does score the correct label y higher than the incor-
rect z; rather, it is a problem of the search, or its
interaction with the model that prunes away the cor-
rect (sub)sequence during the search.
What shall we do in this case? Collins and
Roark (2004) suggest that instead of the standard
full update, we should only update on the prefix
(x, y[1:i], z[1:i]) up to the point i where the correct
sequence falls off the beam. This intuitively makes
a lot of sense, since up to i we can still guarantee
violation, but after i we may not. The next section
formalizes this intuition.
4 Early Update is Violation-Fixing
We now proceed to show that early update is always
valid and it is thus a special case of the violation-
fixing perceptron framework. First, let us study the
simplest special case, greedy search (beam=1).
4.1 Greedy Search and Early Update
Greedy search is the simplest form of inexact search.
Shown in Algorithm 4, at each position, we com-
mit to the single best action (e.g., tag for the current
word) given the previous actions and continue to the
145
? ? ? ? ? ? ?
?? update ?? skip ??
Figure 2: Early update at the first error in greedy search.
Algorithm 5 Early update for greedy search adapted
from Collins and Roark (2004).
Input: training scenario S = ?D,?, Cg(D)?
1: repeat
2: for each example (x, y) in D do
3: z ? ? ? empty sequence
4: for i ? 1 . . . |x| do
5: z ? BEST(x, z,w)
6: if zi 6= yi then ? first wrong action
7: w? w + ??(x, y[1:i], z) ? early update
8: break ? skip the rest of this example
9: until converged
next position. The notation Yi(x) denotes the set of
possible actions at position i for example x (for in-
stance, the set of possible tags for a word).
The early update heuristic, originally proposed for
beam search (Collins and Roark, 2004), now simpli-
fies into ?update at the first wrong action?, since this
is exactly the place where the correct sequence falls
off the singleton beam (see Algorithm 5 for pseu-
docode and Fig. 2). Informally, it is easy to show
(below) that this kind of update is always a valid vi-
olation, but we need to redefine confusion set.
Definition 5. The greedy confusion set Cg(D) for
training data D = {(x(t), y(t))}nt=1 is the set of
triples (x, y[1:i], z[1:i]) where y[1:i] is a i-prefix of the
correct label y, and z[1:i] is an incorrect i-prefix that
agrees with the correct prefix on all decisions except
the last one:
Cg(D) ?= {(x, y[1:i], z[1:i]) | (x, y, z) ? Cs(D),
1 ? i ? |y|, z[1:i?1] = y[1:i?1], zi 6= yi}.
We can see intuitively that this new defintion
is specially tailored to the early updates in greedy
search. The concepts of separation/margin, viola-
tion, and diameter all change accordingly with this
new confusion set. In particular, we say that a
dataset D is greedily separable in representation
? if and only if ?D,?, Cg(D)? is linearly separa-
ble, and we say (x, y?, z?) is a greedy violation if
(x, y?, z?) ? Cg(D) and w ???(x, y?, z?) ? 0.
Algorithm 6 Alternative presentation of Alg. 5 as a
Local Violation-Fixing Perceptron (Alg. 2).
1: function FINDVIOLATION(x, y,w)
2: z ? ? ? empty sequence
3: for i ? 1 . . . |x| do
4: z ? BEST(x, z,w)
5: if zi 6= yi then ? first wrong action
6: return (x, y[1:i], z) ? return for early update
7: return (x, y, y) ? success (z = y), no violation
We now express early update for greedy search
(Algorithm 5) in terms of violation-fixing percep-
tron. Algorithm 6 implements the FINDVIOLATION
function for the generic Local Violation-Fixing Per-
ceptron in Algorithm 2. Thus Algorithm 5 is equiv-
alent to Algorithm 6 plugged into Algorithm 2.
Lemma 2. Each triple (x, y[1:i], z) returned at line 6
in Algorithm 6 is a greedy violation.
Proof. Let y? = y[1:i]. Clearly at line 6, |y?| = i =
|z| and y?i 6= zi. But y?j = zj for all j < i otherwise it
would have returned before position i, so (x, y?, z) ?
Cg(D). Also z = BEST(x, z,w), so w ??(x, z) ?
w ??(x, y?), thus w ???(x, y?, z) ? 0.
Theorem 4 (convergence of greedy search with
early update). For a separable training scenario
S = ?D,?, Cg(D)?, the early update perceptron
by plugging Algorithm 6 into Algorithm 2 will make
finite number of updates (before convergence):
err(S) < R2(S)/?2(S).
Proof. By Lemma 2 and Theorem 2.
4.2 Beam Search and Early Update
To formalize beam search, we need some notations:
Definition 6 (k-best). We denote argtopkz?Z f(z)
to return (a sorted list of) the top k unique z in terms
of f(z), i.e., it returns a list B = [z(1), z(2), . . . , z(k)]
where z(i) ? Z and f(z(1)) ? f(z(2)) ? . . . ?
f(z(k)) ? f(z?) for all z? ? Z ? B.
By unique we mean that no two elements are
equivalent with respect to some equivalence relation,
i.e., z(i) ? z(j) ? i = j. This equivalence rela-
tion is useful for dynamic programming (when used
with beam search). For example, in trigram tagging,
two tag sequences are equivalent if they are of the
146
Algorithm 7 Beam-Search.
BESTk(x,B,w) ?= argtopkz???z?BNEXT(z) w ??(x, z
?)
? top k (unique) extensions from the beam
1: function BEAMSEARCH(x,w, k) ? k is beam width
2: B0 ? [?] ? initial beam
3: for i ? 1 . . . |x| do
4: Bi ? BESTk(x,Bi?1,w)
5: return B|x|[0] ? best sequence in the final beam
Algorithm 8 Early update for beam search (Collins
and Roark 04) as Local Violation-Fixing Perceptron.
1: function FINDVIOLATION(x, y,w)
2: B0 ? [?]
3: for i ? 1 . . . |x| do
4: Bi ? BESTk(x,Bi?1,w)
5: if y[1:i] /? Bi then ? correct seq. falls off beam
6: return (x, y[1:i],Bi[0]) ? update on prefix
7: return (x, y,B|x|[0]) ? full update if wrong final
same length and if they agree on the last two tags,
i.e. z ? z? iff. |z| = |z?| and z|z|?1:|z| = z?|z|?1:|z|. In
incremental parsing this equivalence relation could
be relevant bits of information on the last few trees
on the stack (depending on feature templates), as
suggested in (Huang and Sagae, 2010). 3
Algorithm 7 shows the pseudocode for beam
search. It is trivial to verify that greedy search is
a special case of beam search with k = 1. However,
the definition of confusion set changes considerably:
Definition 7. The beam confusion set Cb(D) for
training data D = {(x(t), y(t))}nt=1 is the set of
triples (x, y[1:i], z[1:i]) where y[1:i] is a i-prefix of the
correct label y, and z[1:i] is an incorrect i-prefix that
differs from the correct prefix (in at least one place):
Cb(D) ?= {(x, y[1:i], z[1:i]) | (x, y, z) ? Cs(D),
1 ? i ? |y|, z[1:i] 6= y[1:i]}.
Similarly, we say that a dataset D is beam
separable in representation ? if and only if
3Note that when checking whether the correct sequence
falls off the beam (line 5), we could either store the whole
(sub)sequence for each candidate in the beam (which is what
we do for non-DP anyway), or check if the equivalence class of
the correct sequence is in the beam, i.e. Jy[1:i]K? ? Bi, and if
its backpointer points to Jy[1:i?1]K?. For example, in trigram
tagging, we just check if ?yi?1, yi? ? Bi and if its backpointer
points to ?yi?2, yi?1?.
ea
rly
m
ax
-
vi
ol
at
io
n
la
te
st
fu
ll 
(s
ta
nd
ar
d)
best in the beam
worst in the beam
falls off 
the beam biggest
violation
last valid 
update
correct sequence
invalid
update!
Figure 3: Illustration of various update methods: early,
max-violation, latest, and standard (full) update, in the
case when standard update is invalid (shown in red). The
rectangle denotes the beam and the blue line segments
denote the trajectory of the correct sequence.
?D,?, Cb(D)? is linearly separable, and we say
(x, y?, z?) is a beam violation if (x, y?, z?) ? Cb(D)
and w ???(x, y?, z?) ? 0.
It is easy to verify that beam confusion set is su-
perset of both greedy and standard confusion sets:
for all dataset D, Cg(D) ( Cb(D), and Cs(D) (
Cb(D). This means that beam separability is the
strongest condition among the three separabilities:
Theorem 5. If a dataset D is beam separable, then
it is also greedily and (standard) linear separable.
We now present early update for beam search as
a Local Violation Fixing Perceptron in Algorithm 8.
See Figure 3 for an illustration.
Lemma 3. Each update (lines 6 or 7 in Algorithm 8)
involves a beam violation.
Proof. Case 1: early update (line 6): Let z? = Bi[0]
and y? = y[1:i]. Case 2: full update (line 8): Let z? =
B|x|[0] and y? = y. In both cases we have z? 6= y?
and |z?| = |y?|, thus (x, y?, z?) ? Cb(D). Also we
have w ? ?(x, z?) ? w ? ?(x, y?) by defintion of
argtop, so w ???(x, y?, z?) ? 0.
Theorem 6 (convergence of beam search with early
update). For a separable training scenario S =
?D,?, Cb(D)?, the early update perceptron by
plugging Algorithm 8 into Algorithm 2 will make fi-
nite number of updates (before convergence):
err(S) < R2(S)/?2(S).
Proof. By Lemma 3 and Theorem 2.
147
5 New Update Methods for Inexact Search
We now propose three novel update methods for
inexact search within the framework of violation-
fixing perceptron. These methods are inspired by
early update but addresses its very limitation of slow
learning. See Figure 3 for an illustration.
1. ?hybrid? update. When the standard update
is valid (i.e., a violation), we perform it, other-
wise we perform the early update.
2. ?max-violation? update. While there are
more than one possible violations on one exam-
ple x, we choose the triple that is most violated:
(x, y?, z?) = argmin
(x,y?,z?)?C,z???i{Bi[0]}
w ???(x, y?, z?).
3. ?latest? update. Contrary to early update, we
can also choose the latest point where the up-
date is still a violation:
(x, y?, z?) = argmax
(x,y?,z?)?C,z???i{Bi[0]},w???(x,y?,z?)>0
|z?|.
All these three methods go beyond early update
but can be represented in the Local Violation Fixing
Perceptron (Algorithm 2), and are thus all guaran-
teed to converge. As we will see in the experiments,
these new methods are motivated to address the ma-
jor limitation of early update, that is, it learns too
slowly since it only updates on prefixes and neglect
the rest of the examples. Hybrid update is trying
to do as much standard (?full?) updates as possible,
and latest update further addresses the case when
standard update is invalid: instead of backing-off to
early update, it uses the longest possible update.
6 Experiments
We conduct experiments on two typical structured
learning tasks: part-of-speech tagging with a trigram
model where exact search is possible, and incremen-
tal dependency parsing with arbitrary non-local fea-
tures where exact search is intractable. We run both
experiments on state-of-the-art implementations.
6.1 Part-of-Speech Tagging
Following the standard split for part-of-speech tag-
ging introduced by Collins (2002), we use sec-
tions 00?18 of the Penn Treebank (Marcus et al,
 96.4
 96.6
 96.8
 97
 97.2
 1  2  3  4  5  6  7  8  9  10b
es
t t
ag
gi
ng
 a
cc
ur
ac
y 
on
 h
el
d-
ou
t
beam size
max-violation
early
standard
Figure 4: POS tagging using beam search with various
update methods (hybrid/latest similar to early; omitted).
b = 1 b = 2 b = 7
method it dev it dev it dev
standard 12 96.27 6 97.07 4 97.17
early 13 96.97 6 97.15 7 97.19
max-viol. 7 96.97 3 97.20 4 97.20
Table 1: Convergence rate of part-of-speech tagging. In
general, max-violation converges faster and better than
early and standard updates, esp. in smallest beams.
1993) for training, sections 19?21 as a held-out
development set, and sections 22?24 for testing.
Our baseline system is a faithful implementation of
the perceptron tagger in Collins (2002), i.e., a tri-
gram model with spelling features from Ratnaparkhi
(1996), except that we replace one-count words as
<unk>. With standard perceptron and exact search,
our baseline system performs slightly better than
Collins (2002) with a beam of 20 (M. Collins, p.c.).
We then implemented beam search on top of dy-
namic programming and experimented with stan-
dard, early, hybrid, and max-violation update meth-
ods with various beam settings (b = 1, 2, 4, 7, 10).
Figure 4(a) summarizes these experiments. We ob-
serve that, first of all, the standard update performs
poorly with the smallest beams, esp. at b = 1
(greedy search), when search error is the most se-
vere causing lots of invalid updates (see Figure 5).
Secondly,max-violation is almost consistently the
best-performing method (except for b = 4). Table 1
shows convergence rates, where max-violation up-
date also converges faster than early and standard
methods. In particular, at b = 1, it achieves a 19%
error reduction over standard update, while converg-
148
 0
 25
 50
 75
 100
 2  4  6  8  10  12  14  16
%
 o
f i
nv
al
id
 u
pd
at
es
beam size
parsing
tagging
Figure 5: Percentages of invalid updates for standard up-
date. In tagging it quickly drops to 0% while in parsing it
converges to ? 50%. This means search-wise, parsing is
much harder than tagging, which explains why standard
update does OK in tagging but terribly in parsing. The
harder the search is, the more needed valid updates are.
method b it time dev test
standard* ? 6 162 m 97.17 97.28
early 4 6 37 m 97.22 97.35
hybrid 5 5 30 m 97.18 97.19
latest 7 5 45 m 97.17 97.13
max-viol. 2 3 26 m 97.20 97.33
standard 20 Collins (2002) 97.11
guided 3 Shen et al (2007) 97.33
Table 2: Final test results on POS tagging. *:baseline.
ing twice as fast as early update.4 This agrees with
our intuition that by choosing the ?most-violated?
triple for update, the perceptron should learn faster.
Table 2 presents the final tagging results on the
test set. For each of the five update methods, we
choose the beam size at which it achieves the high-
est accuracy on the held-out. For standard update, its
best held-out accuracy 97.17 is indeed achieved by
exact search (i.e., b = +?) since it does not work
well with beam search, but it costs 2.7 hours (162
minutes) to train. By contrast, the four valid up-
date methods handle beam search better. The max-
violation method achieves its highest held-out/test
accuracies of 97.20 / 97.33 with a beam size of
only 2, and only 26 minutes to train. Early up-
date achieves the highest held-out/test accuracies of
97.22 / 97.35 across all update methods at the beam
size of 4. This test accuracy is even better than Shen
4for tagging (but not parsing) the difference in per-iteration
speed between early update and max-violation update is small.
method b it time dev test
early*
8
38 15.4 h 92.24 92.09
standard 1 0.4 h 78.99 79.14
hybrid 11 5.6 h 92.26 91.95
latest 9 4.5 h 92.06 91.96
max-viol. 12 5.5 h 92.32 92.18
early 8 Huang & Sagae 2010 92.1
Table 3: Final results on incremental parsing. *: baseline.
 91
 91.25
 91.5
 91.75
 92
 92.25
 0  2  4  6  8  10  12  14  16  18
pa
rs
in
g 
ac
cu
ra
cy
 o
n 
he
ld
-o
ut
training time (hours)
max-violation
early
Figure 6: Training progress curves for incremental pars-
ing (b = 8). Max-violation learns faster and better: it
takes 4.6 hours (10 iterations) to reach 92.25 on held-out,
compared with early update?s 15.4 hours (38 iterations),
even though the latter is faster in each iteration due to
early stopping (esp. at the first few iterations).
et al (2007), the best tagging accuracy reported on
the Penn Treebank to date.5,6 To conclude, with
valid update methods, we can learn a better tagging
model with 5 times faster training than exact search.
6.2 Incremental Parsing
While part-of-speech tagging is mainly a proof of
concept, incremental parsing is much harder since
non-local features rules out exact inference.
We use the standard split for parsing: secs 02?
21 for training, 22 as held-out, and 23 for testing.
Our baseline system is a faithful reimplementation
of the beam-search dynamic programming parser of
Huang and Sagae (2010). Like most incremental
parsers, it used early update as search error is severe.
5according to ACL Wiki: http://aclweb.org/aclwiki/.
6Note that Shen et al (2007) employ contextual features
up to 5-gram which go beyond our local trigram window. We
suspect that adding genuinely non-local features would demon-
strate even better the advantages of valid update methods with
beam search, since exact inference will no longer be tractable.
149
We first confirm that, as reported by Huang and
Sagae, early update learns very slowly, reaching
92.24 on held-out with 38 iterations (15.4 hours).
We then experimented with the other update
methods: standard, hybrid, latest, and max-
violation, with beam size b = 1, 2, 4, 8. We found
that, first of all, the standard update performs horri-
bly on this task: at b = 1 it only achieves 60.04%
on held-out, while at b = 8 it improves to 78.99%
but is still vastly below all other methods. This is
because search error is much more severe in incre-
mental parsing (than in part-of-speech tagging), thus
standard update produces an enormous amount of
invalid updates even at b = 8 (see Figure 5). This
suggests that the advantage of valid update meth-
ods is more pronounced with tougher search prob-
lems. Secondly, max-violation learns much faster
(and better) than early update: it takes only 10 it-
erations (4.6 hours) to reach 92.25, compared with
early update?s 15.4 hours (see Fig. 6). At its peak,
max-violation achieves 92.18 on test which is bet-
ter than (Huang and Sagae, 2010). To conclude, we
can train a parser with only 1/3 of training time with
max-violation update, and the harder the search is,
the more needed the valid update methods are.
7 Related Work and Discussions
Besides the early update method of Collins and
Roark (2004) which inspired us, this work is also
related to the LaSO method of Daume? and Marcu
(2005). LaSO is similar to early update, except that
after each update, instead of skipping the rest of the
example, LaSO continues on the same example with
the correct hypothesis. For example, in the greedy
case LaSO is just replacing the break statement in
Algorithm 5 by
8?: zi = yi
and in beam search it is replacing it with
8?: Bi = [y[1:i]].
This is beyond our Local Violation-Fixing Per-
ceptron since it makes more than one updates on one
example, but can be easily represented as a Global
Violation-Fixing Perceptron (Algorithm 3), since we
can prove any further updates on this example is a vi-
olation (under the new weights). We thus establish
LaSO as a special case within our framework.7
More interestingly, it is easy to verify that the
greedy case of LaSO update is equivalent to training
a local unstructured perceptron which indepen-
dently classifies at each position based on history,
which is related to SEARN (Daume? et al, 2009).
Kulesza and Pereira (2007) study perceptron
learning with approximate inference that overgen-
erates instead of undergenerates as in our work,
but the underlying idea is similar: by learning in a
harder setting (LP-relaxed version in their case and
prefix-augmented version in our case) we can learn
the simpler original setting. Our ?beam separabil-
ity? can be viewed as an instance of their ?algorith-
mic separability?. Finley and Joachims (2008) study
similar approximate inference for structural SVMs.
Our max-violation update is also related to other
training methods for large-margin structured predic-
tion, in particular the cutting-plane (Joachims et al,
2009) and subgradient (Ratliff et al, 2007) methods,
but detailed exploration is left to future work.
8 Conclusions
We have presented a unifying framework of
?violation-fixing? perceptron which guarantees con-
vergence with inexact search. This theory satisfin-
gly explained why standard perceptron might not
work well with inexact search, and why the early
update works. We also proposed some new vari-
ants within this framework, among which the max-
violation method performs the best on state-of-the-
art tagging and parsing systems, leading to better
models with greatly reduced training times. Lastly,
the advantage of valid update methods is more pro-
nounced when search error is severe.
Acknowledgements
We are grateful to the four anonymous reviewers, es-
pecially the one who wrote the comprehensive re-
view. We also thank David Chiang, Kevin Knight,
Ben Taskar, Alex Kulesza, Joseph Keshet, David
McAllester, Mike Collins, Sasha Rush, and Fei Sha
for discussions. This work is supported in part by a
Google Faculty Research Award to the first author.
7It turns out the original theorem in the LaSO paper (Daume?
and Marcu, 2005) contains a bug; see (Xu et al, 2009) for cor-
rections. Thanks to a reviewer for pointing it out.
150
References
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Collins, Michael and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Crammer, Koby and Yoram Singer. 2003. Ultra-
conservative online algorithms for multiclass prob-
lems. Journal of Machine Learning Research (JMLR),
3:951?991.
Daume?, Hal, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction.
Daume?, Hal and Daniel Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of ICML.
Finley, Thomas and Thorsten Joachims. 2008. Training
structural SVMs when exact inference is intractable.
In Proceedings of ICML.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
Huang, Liang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of ACL 2010.
Joachims, T., T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning, 77(1):27?59.
Kulesza, Alex and Fernando Pereira. 2007. Structured
learning with approximate inference. In NIPS.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML.
Liang, Percy, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
COLING-ACL, Sydney, Australia, July.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
McDonald, Ryan and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ratliff, Nathan, J. Andrew Bagnell, and Martin Zinke-
vich. 2007. (online) subgradient methods for struc-
tured prediction. In Proceedings of AIStats.
Ratnaparkhi, Adwait. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
Shen, Libin, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL.
Taskar, Ben, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin markov networks. In Proceedings of
NIPS. MIT Press.
Tsochantaridis, I., T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
Xu, Yuehua, Alan Fern, and Sungwook Yoon. 2009.
Learning linear ranking functions for beam search with
application to planning. Journal of Machine Learning
Research (JMLR), 10:1349?1388.
Zhang, Yue and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP.
151
Proceedings of NAACL-HLT 2013, pages 370?379,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Minibatch and Parallelization for Online Large Margin Structured Learning
Kai Zhao1
1Computer Science Program, Graduate Center
City University of New York
kzhao@gc.cuny.edu
Liang Huang2,1
2Computer Science Dept, Queens College
City University of New York
huang@cs.qc.cuny.edu
Abstract
Online learning algorithms such as perceptron
and MIRA have become popular for many
NLP tasks thanks to their simpler architec-
ture and faster convergence over batch learn-
ing methods. However, while batch learning
such as CRF is easily parallelizable, online
learning is much harder to parallelize: previ-
ous efforts often witness a decrease in the con-
verged accuracy, and the speedup is typically
very small (?3) even with many (10+) pro-
cessors. We instead present a much simpler
architecture based on ?mini-batches?, which
is trivially parallelizable. We show that, un-
like previous methods, minibatch learning (in
serial mode) actually improves the converged
accuracy for both perceptron and MIRA learn-
ing, and when combined with simple paral-
lelization, minibatch leads to very significant
speedups (up to 9x on 12 processors) on state-
of-the-art parsing and tagging systems.
1 Introduction
Online structured learning algorithms such as the
structured perceptron (Collins, 2002) and k-best
MIRA (McDonald et al, 2005) have become more
and more popular for many NLP tasks such as de-
pendency parsing and part-of-speech tagging. This
is because, compared to their batch learning counter-
parts, online learning methods offer faster conver-
gence rates and better scalability to large datasets,
while using much less memory and a much simpler
architecture which only needs 1-best or k-best de-
coding. However, online learning for NLP typically
involves expensive inference on each example for 10
or more passes over millions of examples, which of-
ten makes training too slow in practice; for example
systems such as the popular (2nd-order) MST parser
(McDonald and Pereira, 2006) usually require the
order of days to train on the Treebank on a com-
modity machine (McDonald et al, 2010).
There are mainly two ways to address this scala-
bility problem. On one hand, researchers have been
developing modified learning algorithms that allow
inexact search (Collins and Roark, 2004; Huang et
al., 2012). However, the learner still needs to loop
over the whole training data (on the order of mil-
lions of sentences) many times. For example the
best-performing method in Huang et al (2012) still
requires 5-6 hours to train a very fast parser.
On the other hand, with the increasing popularity
of multicore and cluster computers, there is a grow-
ing interest in speeding up training via paralleliza-
tion. While batch learning such as CRF (Lafferty
et al, 2001) is often trivially parallelizable (Chu et
al., 2007) since each update is a batch-aggregate of
the update from each (independent) example, online
learning is much harder to parallelize due to the de-
pendency between examples, i.e., the update on the
first example should in principle influence the de-
coding of all remaining examples. Thus if we de-
code and update the first and the 1000th examples
in parallel, we lose their interactions which is one
of the reasons for online learners? fast convergence.
This explains why previous work such as the itera-
tive parameter mixing (IPM) method of McDonald
et al (2010) witnesses a decrease in the accuracies
of parallelly-learned models, and the speedup is typ-
ically very small (about 3 in their experiments) even
with 10+ processors.
We instead explore the idea of ?minibatch? for on-
line large-margin structured learning such as percep-
tron and MIRA. We argue that minibatch is advan-
tageous in both serial and parallel settings.
First, for minibatch perceptron in the serial set-
370
ting, our intuition is that, although decoding is done
independently within one minibatch, updates are
done by averaging update vectors in batch, provid-
ing a ?mixing effect? similar to ?averaged parame-
ters? of Collins (2002) which is also found in IPM
(McDonald et al, 2010), and online EM (Liang and
Klein, 2009).
Secondly, minibatch MIRA in the serial setting
has an advantage that, different from previous meth-
ods such as SGD which simply sum up the up-
dates from all examples in a minibatch, a minibatch
MIRA update tries to simultaneously satisfy an ag-
gregated set of constraints that are collected from
multiple examples in the minibatch. Thus each mini-
batch MIRA update involves an optimization over
many more constraints than in pure online MIRA,
which could potentially lead to a better margin. In
other words we can view MIRA as an online version
or stepwise approximation of SVM, and minibatch
MIRA can be seen as a better approximation as well
as a middleground between pure MIRA and SVM.1
More interestingly, the minibatch architecture is
trivially parallelizable since the examples within
each minibatch could be decoded in parallel on mul-
tiple processors (while the update is still done in se-
rial). This is known as ?synchronous minibatch?
and has been explored by many researchers (Gim-
pel et al, 2010; Finkel et al, 2008), but all previ-
ous works focus on probabilistic models along with
SGD or EM learning methods while our work is the
first effort on large-margin methods.
We make the following contributions:
? Theoretically, we present a serial minibatch
framework (Section 3) for online large-margin
learning and prove the convergence theorems
for minibatch perceptron and minibatch MIRA.
? Empirically, we show that serial minibatch
could speed up convergence and improve the
converged accuracy for both MIRA and percep-
tron on state-of-the-art dependency parsing and
part-of-speech tagging systems.
? In addition, when combined with simple (syn-
chronous) parallelization, minibatch MIRA
1This is similar to Pegasos (Shalev-Shwartz et al, 2007) that
applies subgradient descent over a minibatch. Pegasos becomes
pure online when the minibatch size is 1.
Algorithm 1 Generic Online Learning.
Input: dataD = {(x(t), y(t))}nt=1 and feature map ?
Output: weight vector w
1: repeat
2: for each example (x, y) in D do
3: C ? FINDCONSTRAINTS(x, y,w) . decoding
4: if C 6= ? then UPDATE(w, C)
5: until converged
leads to very significant speedups (up to 9x on
12 processors) that are much higher than that of
IPM (McDonald et al, 2010) on state-of-the-art
parsing and tagging systems.
2 Online Learning: Perceptron and MIRA
We first present a unified framework for online
large-margin learning, where perceptron and MIRA
are two special cases. Shown in Algorithm 1, the
online learner considers each input example (x, y)
sequentially and performs two steps:
1. find the set C of violating constraints, and
2. update the weight vector w according to C.
Here a triple ?x, y, z? is said to be a ?violating con-
straint? with respect to model w if the incorrect la-
bel z scores higher than (or equal to) the correct
label y in w, i.e., w ? ??(?x, y, z?) ? 0, where
??(?x, y, z?) is a short-hand notation for the up-
date vector ?(x, y) ? ?(x, z) and ? is the feature
map (see Huang et al (2012) for details). The sub-
routines FINDCONSTRAINTS and UPDATE are anal-
ogous to ?APIs?, to be specified by specific instances
of this online learning framework. For example, the
structured perceptron algorithm of Collins (2002)
is implemented in Algorithm 2 where FINDCON-
STRAINTS returns a singleton constraint if the 1-best
decoding result z (the highest scoring label accord-
ing to the current model) is different from the true
label y. Note that in the UPDATE function, C is al-
ways a singleton constraint for the perceptron, but
we make it more general (as a set) to handle the
batch update in the minibatch version in Section 3.
On the other hand, Algorith 3 presents the k-best
MIRA Algorithm of McDonald et al (2005) which
generalizes multiclass MIRA (Crammer and Singer,
2003) for structured prediction. The decoder now
371
Algorithm 2 Perceptron (Collins, 2002).
1: function FINDCONSTRAINTS(x, y,w)
2: z ? argmaxs?Y(x) w ??(x, s) . decoding
3: if z 6= y then return {?x, y, z?}
4: else return ?
5: procedure UPDATE(w, C)
6: w? w + 1|C|
?
c?C ??(c) . (batch) update
Algorithm 3 k-best MIRA (McDonald et al, 2005).
1: function FINDCONSTRAINTS(x, y,w)
2: Z ? k-bestz?Y(x)w ??(x, z)
3: Z ? {z ? Z | z 6= y,w ???(?x, y, z?) ? 0}
4: return {(?x, y, z?, `(y, z)) | z ? Z}
5: procedure UPDATE(w, C)
6: w? argmin
w?:?(c,`)?C, w????(c)?`
?w? ?w?2
finds the k-best solutions Z first, and returns a set
of violating constraints in Z, The update in MIRA
is more interesting: it searches for the new model
w? with minimum change from the current model
w so that w? corrects each violating constraint by
a margin at least as large as the loss `(y, z) of the
incorrect label z.
Although not mentioned in the pseudocode, we
also employ ?averaged parameters? (Collins, 2002)
for both perceptron and MIRA in all experiments.
3 Serial Minibatch
The idea of serial minibatch learning is extremely
simple: divide the data into dn/me minibatches
of size m, and do batch updates after decoding
each minibatch (see Algorithm 4). The FIND-
CONSTRAINTS and UPDATE subroutines remain un-
changed for both perceptron and MIRA, although
it is important to note that a perceptron batch up-
date uses the average of update vectors, not the sum,
which simplifies the proof. This architecture is of-
ten called ?synchronous minibatch? in the literature
(Gimpel et al, 2010; Liang and Klein, 2009; Finkel
et al, 2008). It could be viewed as a middleground
between pure online learning and batch learning.
3.1 Convergence of Minibatch Perceptron
We denote C(D) to be the set of all possible violat-
ing constraints in data D (cf. Huang et al (2012)):
C(D) = {?x, y, z? | (x, y) ? D, z ? Y(x)? {y}}.
Algorithm 4 Serial Minibatch Online Learning.
Input: data D, feature map ?, and minibatch size m
Output: weight vector w
1: Split D into dn/me minibatches D1 . . . Ddn/me
2: repeat
3: for i? 1 . . . dn/me do . for each minibatch
4: C ? ?(x,y)?DiFINDCONSTRAINTS(x, y,w)
5: if C 6= ? then UPDATE(w, C) . batch update
6: until converged
A training set D is separable by feature map ?
with margin ? > 0 if there exists a unit oracle vec-
tor u with ?u? = 1 such that u ???(?x, y, z?) ? ?,
for all ?x, y, z? ? C(D). Furthermore, let radius
R ? ???(?x, y, z?)? for all ?x, y, z? ? C(D).
Theorem 1. For a separable datasetD with margin
? and radius R, the minibatch perceptron algorithm
(Algorithms 4 and 2) will terminate after tminibatch
updates where t ? R2/?2.
Proof. Let wt be the weight vector before the tth
update; w0 = 0. Suppose the tth update happens
on the constraint set Ct = {c1, c2, . . . , ca} where
a = |Ct|, and each ci = ?xi, yi, zi?. We convert
them to the set of update vectors vi = ??(ci) =
??(?xi, yi, zi?) for all i. We know that:
1. u ? vi ? ? (margin on unit oracle vector)
2. wt ? vi ? 0 (violation: zi dominates yi)
3. ?vi?2 ? R2 (radius)
Now the update looks like
wt+1 = wt +
1
|Ct|
?
c?Ct
??(c) = wt +
1
a
?
i vi.
(1)
We will bound ?wt+1? from two directions:
1. Dot product both sides of the update equa-
tion (1) with the unit oracle vector u, we have
u ?wt+1 = u ?wt +
1
a
?
i u ? vi
? u ?wt +
1
a
?
i ? (margin)
= u ?wt + ? (
?
i = a)
? t? (by induction)
372
Since for any two vectors a and b we have
?a??b? ? a?b, thus ?u??wt+1? ? u?wt+1 ?
t?. As u is a unit vector, we have ?wt+1? ? t?.
2. On the other hand, take the norm of both sides
of Eq. (1):
?wt+1?2 = ?wt +
1
a
?
i vi?
2
=?wt?2 + ?
?
i
1
avi?
2 +
2
a
wt ?
?
i vi
??wt?2 + ?
?
i
1
avi?
2 + 0 (violation)
??wt?2 +
?
i
1
a?vi?
2 (Jensen?s)
??wt?2 +
?
i
1
aR
2 (radius)
=?wt?2 +R2 (
?
i = a)
?tR2 (by induction)
Combining the two bounds, we have
t2?2 ? ?wt+1?2 ? tR2
thus the number of minibatch updates t ? R2/?2.
Note that this bound is identical to that of pure
online perceptron (Collins, 2002, Theorem 1) and is
irrelevant to minibatch size m. The use of Jensen?s
inequality is inspired by McDonald et al (2010).
3.2 Convergence of Minibatch MIRA
We also give a proof of convergence for MIRA with
relaxation.2 We present the optimization problem in
the UPDATE function of Algorithm 3 as a quadratic
program (QP) with slack variable ?:
wt+1 ?argmin
wt+1
?wt+1 ?wt?2 + ?
s.t. wt+1 ? vi ? `i ? ?, for all(ci, `i) ? Ct
where vi = ??(ci) is the update vector for con-
straint ci. Consider the Lagrangian:
L =?wt+1 ?wt?2 + ? +
|Ct|?
i=1
?i(`i ?w? ? vi ? ?)
?i ? 0, for 1 ? i ? |Ct|.
2Actually this relaxation is not necessary for the conver-
gence proof. We employ it here solely to make the proof shorter.
It is not used in the experiments either.
Set the partial derivatives to 0 with respect to w? and
? we have:
w? = w +
?
i ?ivi (2)
?
i ?i = 1 (3)
This result suggests that the weight change can al-
ways be represnted by a linear combination of the
update vectors (i.e. normal vectors of the constraint
hyperplanes), with the linear coefficencies sum to 1.
Theorem 2 (convergence of minibatch MIRA). For
a separable dataset D with margin ? and radius R,
the minibatch MIRA algorithm (Algorithm 4 and 3)
will make t updates where t ? R2/?2.
Proof. 1. Dot product both sides of Equation 2
with unit oracle vector u:
u ?wt+1 = u ?wt +
?
i ?iu ? vi
?u ?wt +
?
i ?i? (margin)
=u ?wt + ? (Eq. 3)
=t? (by induction)
2. On the other hand
?wt+1?2 = ?wt +
?
i ?ivi?
2
=?wt?2 + ?
?
i ?ivi?
2 + 2 wt ?
?
i ?ivi
??wt?2 + ?
?
i ?ivi?
2 + 0 (violation)
??wt?2 +
?
i ?iv
2
i (Jensen?s)
??wt?2 +
?
i ?iR
2 (radius)
=?wt?2 +R2 (Eq. 3)
?tR2 (by induction)
From the two bounds we have:
t2?2 ? ?wt+1?2 ? tR2
thus within at most t ? R2/?2 minibatch up-
dates MIRA will converge.
4 Parallelized Minibatch
The key insight into parallelization is that the calcu-
lation of constraints (i.e. decoding) for each exam-
ple within a minibatch is completely independent of
373
update
1
3
4
2
update
update
update
6
5
8
7
update
update
update
update
12
9
10
11
update
update
update
update
15
14
13
16
update
update
update
update
?
update
3
1
4
6
5
8
7
2
12
15
14
9
13
16
10
11
?
update
?
3
1
4
6
5
8
7
2
12
15
14
9
13
16
10
11
update
?
update
?
update
1
2
3
4
6
5
8
7
update
update
9
10
update
12
11
14
13
15
16
update
update
update
update
(a) IPM (b) unbalanced (c) balanced (d) asynchronous
Figure 1: Comparison of various methods for parallelizing online learning (number of processors p = 4). (a) iterative
parameter mixing (McDonald et al, 2010). (b) unbalanced minibatch parallelization (minibatch size m = 8). (c)
minibatch parallelization after load-balancing (within each minibatch). (d) asynchronous minibatch parallelization
(Gimpel et al, 2010) (not implemented here). Each numbered box denotes the decoding of one example, and ?
denotes an aggregate operation, i.e., the merging of constraints after each minibatch or the mixing of weights after
each iteration in IPM. Each gray shaded box denotes time wasted due to synchronization in (a)-(c) or blocking in (d).
Note that in (d) at most one update can happen concurrently, making it substantially harder to implement than (a)-(c).
Algorithm 5 Parallized Minibatch Online Learning.
Input: D, ?, minibatch sizem, and # of processors p
Output: weight vector w
Split D into dn/me minibatches D1 . . . Ddn/me
Split each Di into m/p groups Di,1 . . . Di,m/p
repeat
for i? 1 . . . dn/me do . for each minibatch
for j ? 1 . . .m/p in parallel do
Cj ? ?(x,y)?Di,j FINDCONSTRAINTS(x, y,w)
C ? ?jCj . in serial
if C 6= ? then UPDATE(w, C) . in serial
until converged
other examples in the same batch. Thus we can eas-
ily distribute decoding for different examples in the
same minibatch to different processors.
Shown in Algorithm 5, for each minibatchDi, we
split Di into groups of equal size, and assign each
group to a processor to decode. After all processors
finish, we collect all constraints and do an update
based on the union of all constraints. Figure 1 (b) il-
lustrates minibatch parallelization, with comparison
to iterative parameter mixing (IPM) of McDonald et
al. (2010) (see Figure 1 (a)).
This synchronous parallelization framework
should provide significant speedups over the serial
mode. However, in each minibatch, inevitably,
some processors will end up waiting for others to
finish, especially when the lengths of sentences vary
substantially (see the shaded area in Figure 1 (b)).
To alleviate this problem, we propose ?per-
minibatch load-balancing?, which rearranges the
sentences within each minibatch based on their
lengths (which correlate with their decoding times)
so that the total workload on each processor is bal-
anced (Figure 1c). It is important to note that this
shuffling does not affect learning at all thanks to the
independence of each example within a minibatch.
Basically, we put the shortest and longest sentences
into the first thread, the second shortest and second
longest into the second thread, etc. Although this is
not necessary optimal scheduling, it works well in
practice. As long as decoding time is linear in the
length of sentence (as in incremental parsing or tag-
ging), we expect a much smaller variance in process-
ing time on each processor in one minibatch, which
is confirmed in the experiments (see Figure 8).3
3In IPM, however, the waiting time is negligible, since the
workload on each processor is almost balanced, analogous to
a huge minibatch (Fig. 1a). Furthermore, shuffling does affect
learning here since each thread in IPM is a pure online learner.
So our IPM implementation does not use load-balancing.
374
5 Experiments
We conduct experiments on two typical structured
prediction problems: incremental dependency pars-
ing and part-of-speech tagging; both are done on
state-of-the-art baseline. We also compare our
parallelized minibatch algorithm with the iterative
parameter mixing (IPM) method of McDonald et
al. (2010). We perform our experiments on a
commodity 64-bit Dell Precision T7600 worksta-
tion with two 3.1GHz 8-core CPUs (16 processors
in total) and 64GB RAM. We use Python 2.7?s
multiprocessing module in all experiments.4
5.1 Dependency Parsing with MIRA
We base our experiments on our dynamic program-
ming incremental dependency parser (Huang and
Sagae, 2010).5 Following Huang et al (2012), we
use max-violation update and beam size b = 8. We
evaluate on the standard Penn Treebank (PTB) us-
ing the standard split: Sections 02-21 for training,
and Section 22 as the held-out set (which is indeed
the test-set in this setting, following McDonald et
al. (2010) and Gimpel et al (2010)). We then ex-
tend it to employ 1-best MIRA learning. As stated
in Section 2, MIRA separates the gold label y from
the incorrect label z with a margin at least as large
as the loss `(y, z). Here in incremental dependency
parsing we define the loss function between a gold
tree y and an incorrect partial tree z as the number
of incorrect edges in z, plus the number of correct
edges in y which are already ruled out by z. This
MIRA extension results in slightly higher accuracy
of 92.36, which we will use as the pure online learn-
ing baseline in the comparisons below.
5.1.1 Serial Minibatch
We first run minibatch in the serial mode with
varying minibatch size of 4, 16, 24, 32, and 48 (see
Figure 2). We can make the following observations.
First, except for the largest minibatch size of 48,
minibatch learning generally improves the accuracy
4We turn off garbage-collection in worker processes oth-
erwise their running times will be highly unbalanced. We also
admit that Python is not the best choice for parallelization, e.g.,
asychronous minibatch (Gimpel et al, 2010) requires ?shared
memory? not found in the current Python (see also Sec. 6).
5Available at http://acl.cs.qc.edu/. The version
with minibatch parallelization will be available there soon.
 90.75
 91
 91.25
 91.5
 91.75
 92
 92.25
 92.5
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y 
on
 h
el
d-
ou
t
wall-clock time (hours)
m=1
m=4
m=16
m=24
m=32
m=48
Figure 2: Minibatch with various minibatch sizes (m =
4, 16, 24, 32, 48) for parsing with MIRA, compared to
pure MIRA (m = 1). All curves are on a single CPU.
of the converged model, which is explained by our
intuition that optimization with a larger constraint
set could improve the margin. In particular, m = 16
achieves the highest accuracy of 92.53, which is a
0.27 improvement over the baseline.
Secondly, minibatch learning can reach high lev-
els of accuracy faster than the baseline can. For ex-
ample, minibatch of size 4 can reach 92.35 in 3.5
hours, and minibatch of size 24 in 3.7 hours, while
the pure online baseline needs 6.9 hours. In other
words, just minibatch alone in serial mode can al-
ready speed up learning. This is also explained by
the intuition of better optimization above, and con-
tributes significantly to the final speedup of paral-
lelized minibatch.
Lastly, larger minibatch sizes slow down the con-
vergence, with m = 4 converging the fastest and
m = 48 the slowest. This can be explained by the
trade-off between the relative strengths from online
learning and batch update: with larger batch sizes,
we lose the dependencies between examples within
the same minibatch.
Although larger minibatches slow down conver-
gence, they actually offer better potential for paral-
lelization since the number of processors p has to be
smaller than minibatch size m (in fact, p should di-
vide m). For example, m = 24 can work with 2, 3,
4, 6, 8, or 12 processors while m = 4 can only work
with 2 or 4 and the speed up of 12 processors could
easily make up for the slightly slower convergence
375
 91.4
 91.6
 91.8
 92
 92.2
 92.4
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
baseline
m=24,p=1
m=24,p=4
m=24,p=12
 91.4
 91.6
 91.8
 92
 92.2
 92.4
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
wall-clock time (hours)
baseline
IPM,p=4
IPM,p=12
Figure 3: Parallelized minibatch is much faster than iter-
ative parameter mixing. Top: minibatch of size 24 using
4 and 12 processors offers significant speedups over the
serial minibatch and pure online baselines. Bottom: IPM
with the same processors offers very small speedups.
rate. So there seems to be a ?sweetspot? of mini-
batch sizes, similar to the tipping point observed in
McDonald et al (2010) when adding more proces-
sors starts to hurt convergence.
5.1.2 Parallelized Minibatch vs. IPM
In the following experiments we use minibatch
size of m = 24 and run it in parallel mode on vari-
ous numbers of processors (p = 2 ? 12). Figure 3
(top) shows that 4 and 12 processors lead to very
significant speedups over the serial minibatch and
pure online baselines. For example, it takes the 12
processors only 0.66 hours to reach an accuracy of
92.35, which takes the pure online MIRA 6.9 hours,
amounting to an impressive speedup of 10.5.
We compare our minibatch parallelization with
the iterative parameter mixing (IPM) of McDonald
et al (2010). Figure 3 (bottom) shows that IPM not
only offers much smaller speedups, but also con-
verges lower, and this drop in accuracy worsens with
more processors.
Figure 4 gives a detailed analysis of speedups.
Here we perform both extrinsic and intrinsic com-
parisons. In the former, we care about the time to
reach a given accuracy; in this plot we use 92.27
which is the converged accuracy of IPM on 12 pro-
cessors. We choose it since it is the lowest accu-
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 2  4  6  8  10  12
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
sp
ee
du
ps
number of processors
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
Figure 4: Speedups of minibatch parallelization vs. IPM
on 1 to 12 processors (parsing with MIRA). Extrinsic
comparisons use ?the time to reach an accuracy of 92.27?
for speed calculations, 92.27 being the converged accu-
racy of IPM using 12 processors. Intrinsic comparisons
use average time per iteration regardless of accuracy.
racy among all converged models; choosing a higher
accuracy would reveal even larger speedups for our
methods. This figure shows that our method offers
superlinear speedups with small number of proces-
sors (1 to 6), and almost linear speedups with large
number of processors (8 and 12). Note that even
p = 1 offers a speedup of 1.5 thanks to serial mini-
batch?s faster convergence; in other words, within
the 9 fold speed-up at p = 12, parallelization con-
tributes about 6 and minibatch about 1.5. By con-
trast, IPM only offers an almost constant speedup of
around 3, which is consistent with the findings of
McDonald et al (2010) (both of their experiments
show a speedup of around 3).
We also try to understand where the speedup
comes from. For that purpose we study intrinsic
speedup, which is about the speed regardless of ac-
curacy (see Figure 4). For our minibatch method,
intrinsic speedup is the average time per iteration
of a parallel run over the serial minibatch base-
line. This answers the questions such as ?how CPU-
efficient is our parallelization? or ?how much CPU
time is wasted?. We can see that with small num-
ber of processors (2 to 4), the efficiency, defined as
Sp/p where Sp is the intrinsic speedup for p pro-
cessors, is almost 100% (ideal linear speedup), but
with more processors it decreases to around 50%
with p = 12, meaning about half of CPU time is
376
 96.8
 96.85
 96.9
 96.95
 97
 97.05
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y 
on
 h
el
d-
ou
t
wall-clock time (hours)
m=1
m=16
m=24
m=48
Figure 5: Minibatch learning for tagging with perceptron
(m = 16, 24, 32) compared with baseline (m = 1) for
tagging with perceptron. All curves are on single CPU.
wasted. This wasting is due to two sources: first, the
load-balancing problem worsens with more proces-
sors, and secondly, the update procedure still runs in
serial mode with p? 1 processors sleeping.
5.2 Part-of-Speech Tagging with Perceptron
Part-of-speech tagging is usually considered as a
simpler task compared to dependency parsing. Here
we show that using minibatch can also bring better
accuracies and speedups for part-of-speech tagging.
We implement a part-of-speech tagger with aver-
aged perceptron. Following the standard splitting of
Penn Treebank (Collins, 2002), we use Sections 00-
18 for training and Sections 19-21 as held-out. Our
implementation provides an accuracy of 96.98 with
beam size 8.
First we run the tagger on a single processor with
minibatch sizes 8, 16, 24, and 32. As in Figure 5, we
observe similar convergence acceleration and higher
accuracies with minibatch. In particular, minibatch
of size m = 16 provides the highest accuracy of
97.04, giving an improvement of 0.06. This im-
provement is smaller than what we observe in MIRA
learning for dependency parsing experiments, which
can be partly explained by the fast convergence of
the tagger, and that perceptron does not involve op-
timization in the updates.
Then we choose minibatch of size 24 to investi-
gate the parallelization performance. As Figure 6
(top) shows, with 12 processors our method takes
only 0.10 hours to converge to an accuracy of 97.00,
compared to the baseline of 96.98 with 0.45 hours.
We also compare our method with IPM as in Fig-
 96.8
 96.85
 96.9
 96.95
 97
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y baseline
m=24,p=1
m=24,p=4
m=24,p=12
 96.8
 96.85
 96.9
 96.95
 97
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y
wall-clock time (hours)
baseline
IPM,p=4
IPM,p=12
Figure 6: Parallelized minibatch is faster than iterative
parameter mixing (on tagging with perceptron). Top:
minibatch of size 24 using 4 and 12 processors offers
significant speedups over the baselines. Bottom: IPM
with the same 4 and 12 processors offers slightly smaller
speedups. Note that IPM with 4 processors converges
lower than other parallelization curves.
ure 6 (bottom). Again, our method converges faster
and better than IPM, but this time the differences are
much smaller than those in parsing.
Figure 7 uses 96.97 as a criteria to evaluate the
extrinsic speedups given by our method and IPM.
Again we choose this number because it is the lowest
accuracy all learners can reach. As the figure sug-
gests, although our method does not have a higher
pure parallelization speedup (intrinsic speedup), it
still outperforms IPM.
We are interested in the reason why tagging ben-
efits less from minibatch and parallelization com-
pared to parsing. Further investigation reveals that
in tagging the working load of different processors
are more unbalanced than in parsing. Figure 8 shows
that, when p is small, waiting time is negligible, but
when p = 12, tagging wastes about 40% of CPU
cycles and parser about 30%. By contrast, there
is almost no waiting time in IPM and the intrinsic
speedup for IPM is almost linear. The communica-
tion overhead is not included in this figure, but by
comparing it to the speedups (Figures 4 and 7), we
conclude that the communication overhead is about
10% for both parsing and tagging at p = 12.
377
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 2  4  6  8  10  12
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
sp
ee
du
p 
ra
tio
number of processors
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
Figure 7: Speedups of minibatch parallelization and IPM
on 1 to 12 processors (tagging with perceptron). Extrin-
sic speedup uses ?the time to reach an accuracy of 96.97?
as the criterion to measure speed. Intrinsic speedup mea-
sures the pure parallelization speedup. IPM has an al-
most linear intrinsic speedup but a near constant extrinsic
speedup of about 3 to 4.
 0
 10
 20
 30
 40
 50
 60
 2  4  6  8  10  12
%
 o
f w
ai
tin
g 
tim
e
number of processors
parser(balanced)
tagger(balanced)
parser(unbalanced)
tagger(unbalanced)
Figure 8: Percentage of time wasted due to synchroniza-
tion (waiting for other processors to finish) (minibatch
m = 24), which corresponds to the gray blocks in Fig-
ure 1 (b-c). The number of sentences assigned to each
processor decreases with more processors, which wors-
ens the unbalance. Our load-balancing strategy (Figure 1
(c)) alleviates this problem effectively. The communica-
tion overhead and update time are not included.
6 Related Work and Discussions
Besides synchronous minibatch and iterative param-
eter mixing (IPM) discussed above, there is another
method of asychronous minibatch parallelization
(Zinkevich et al, 2009; Gimpel et al, 2010; Chiang,
2012), as in Figure 1. The key advantage of asyn-
chronous over synchronous minibatch is that the for-
mer allows processors to remain near-constant use,
while the latter wastes a significant amount of time
when some processors finish earlier than others in a
minibatch, as found in our experiments. Gimpel et
al. (2010) show significant speedups of asychronous
parallelization over synchronous minibatch on SGD
and EM methods, and Chiang (2012) finds asyn-
chronous parallelization to be much faster than IPM
on MIRA for machine translation. However, asyn-
chronous is significantly more complicated to imple-
ment, which involves locking when one processor
makes an update (see Fig. 1 (d)), and (in languages
like Python) message-passing to other processors af-
ter update. Whether this added complexity is worth-
while on large-margin learning is an open question.
7 Conclusions and Future Work
We have presented a simple minibatch paralleliza-
tion paradigm to speed up large-margin structured
learning algorithms such as (averaged) perceptron
and MIRA. Minibatch has an advantage in both se-
rial and parallel settings, and our experiments con-
firmed that a minibatch size of around 16 or 24 leads
to a significant speedups over the pure online base-
line, and when combined with parallelization, leads
to almost linear speedups for MIRA, and very signif-
icant speedups for perceptron. These speedups are
significantly higher than those of iterative parame-
ter mixing of McDonald et al (2010) which were
almost constant (3?4) in both our and their own ex-
periments regardless of the number of processors.
One of the limitations of this work is that although
decoding is done in parallel, update is still done in
serial and in MIRA the quadratic optimization step
(Hildreth algorithm (Hildreth, 1957)) scales super-
linearly with the number of constraints. This pre-
vents us from using very large minibatches. For
future work, we would like to explore parallelized
quadratic optimization and larger minibatch sizes,
and eventually apply it to machine translation.
Acknowledgement
We thank Ryan McDonald, Yoav Goldberg, and Hal
Daume?, III for helpful discussions, and the anony-
mous reviewers for suggestions. This work was
partially supported by DARPA FA8750-13-2-0041
?Deep Exploration and Filtering of Text? (DEFT)
Program and by Queens College for equipment.
378
References
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
C.-T. Chu, S.-K. Kim, Y.-A. Lin, Y.-Y. Yu, G. Bradski,
A. Ng, and K. Olukotun. 2007. Map-reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems 19.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Kevin Gimpel, Dipanjan Das, and Noah Smith. 2010.
Distributed asynchronous online learning for natural
language processing. In Proceedings of CoNLL.
Clifford Hildreth. 1957. A quadratic programming pro-
cedure. Naval Research Logistics Quarterly, 4(1):79?
85.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML.
Percy Liang and Dan Klein. 2009. Online em for unsu-
pervised models. In Proceedings of NAACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of NAACL, June.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML.
M. Zinkevich, A. J. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems 22.
379
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Dynamic Programming for Linear-Time Incremental Parsing
Liang Huang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
lhuang@isi.edu
Kenji Sagae
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
sagae@ict.usc.edu
Abstract
Incremental parsing techniques such as
shift-reduce have gained popularity thanks
to their efficiency, but there remains a
major problem: the search is greedy and
only explores a tiny fraction of the whole
space (even with beam search) as op-
posed to dynamic programming. We show
that, surprisingly, dynamic programming
is in fact possible for many shift-reduce
parsers, by merging ?equivalent? stacks
based on feature values. Empirically, our
algorithm yields up to a five-fold speedup
over a state-of-the-art shift-reduce depen-
dency parser with no loss in accuracy. Bet-
ter search also leads to better learning, and
our final parser outperforms all previously
reported dependency parsers for English
and Chinese, yet is much faster.
1 Introduction
In terms of search strategy, most parsing al-
gorithms in current use for data-driven parsing
can be divided into two broad categories: dy-
namic programming which includes the domi-
nant CKY algorithm, and greedy search which in-
cludes most incremental parsing methods such as
shift-reduce.1 Both have pros and cons: the for-
mer performs an exact search (in cubic time) over
an exponentially large space, while the latter is
much faster (in linear-time) and is psycholinguis-
tically motivated (Frazier and Rayner, 1982), but
its greedy nature may suffer from severe search er-
rors, as it only explores a tiny fraction of the whole
space even with a beam.
Can we combine the advantages of both ap-
proaches, that is, construct an incremental parser
1McDonald et al (2005b) is a notable exception: the MST
algorithm is exact search but not dynamic programming.
that runs in (almost) linear-time, yet searches over
a huge space with dynamic programming?
Theoretically, the answer is negative, as Lee
(2002) shows that context-free parsing can be used
to compute matrix multiplication, where sub-cubic
algorithms are largely impractical.
We instead propose a dynamic programming al-
ogorithm for shift-reduce parsing which runs in
polynomial time in theory, but linear-time (with
beam search) in practice. The key idea is to merge
equivalent stacks according to feature functions,
inspired by Earley parsing (Earley, 1970; Stolcke,
1995) and generalized LR parsing (Tomita, 1991).
However, our formalism is more flexible and our
algorithm more practical. Specifically, we make
the following contributions:
? theoretically, we show that for a large class
of modern shift-reduce parsers, dynamic pro-
gramming is in fact possible and runs in poly-
nomial time as long as the feature functions
are bounded and monotonic (which almost al-
ways holds in practice);
? practically, dynamic programming is up to
five times faster (with the same accuracy) as
conventional beam-search on top of a state-
of-the-art shift-reduce dependency parser;
? as a by-product, dynamic programming can
output a forest encoding exponentially many
trees, out of which we can draw better and
longer k-best lists than beam search can;
? finally, better and faster search also leads to
better and faster learning. Our final parser
achieves the best (unlabeled) accuracies that
we are aware of in both English and Chi-
nese among dependency parsers trained on
the Penn Treebanks. Being linear-time, it is
also much faster than most other parsers,
even with a pure Python implementation.
1077
input: w0 . . . wn?1
axiom 0 : ?0, ??: 0
sh
? : ?j, S? : c
? + 1 : ?j + 1, S|wj? : c + ?
j < n
rex
? : ?j, S|s1|s0? : c
? + 1 : ?j, S|s1xs0? : c + ?
rey
? : ?j, S|s1|s0? : c
? + 1 : ?j, S|s1ys0? : c + ?
goal 2n? 1 : ?n, s0?: c
where ? is the step, c is the cost, and the shift cost ?
and reduce costs ? and ? are:
? = w ? fsh(j, S) (1)
? = w ? frex (j, S|s1|s0) (2)
? = w ? frey (j, S|s1|s0) (3)
Figure 1: Deductive system of vanilla shift-reduce.
For convenience of presentation and experimen-
tation, we will focus on shift-reduce parsing for
dependency structures in the remainder of this pa-
per, though our formalism and algorithm can also
be applied to phrase-structure parsing.
2 Shift-Reduce Parsing
2.1 Vanilla Shift-Reduce
Shift-reduce parsing performs a left-to-right scan
of the input sentence, and at each step, choose one
of the two actions: either shift the current word
onto the stack, or reduce the top two (or more)
items at the end of the stack (Aho and Ullman,
1972). To adapt it to dependency parsing, we split
the reduce action into two cases, rex and rey, de-
pending on which one of the two items becomes
the head after reduction. This procedure is known
as ?arc-standard? (Nivre, 2004), and has been en-
gineered to achieve state-of-the-art parsing accu-
racy in Huang et al (2009), which is also the ref-
erence parser in our experiments.2
More formally, we describe a parser configura-
tion by a state ?j, S? where S is a stack of trees
s0, s1, ... where s0 is the top tree, and j is the
2There is another popular variant, ?arc-eager? (Nivre,
2004; Zhang and Clark, 2008), which is more complicated
and less similar to the classical shift-reduce algorithm.
input: ?I saw Al with Joe?
step action stack queue
0 - I ...
1 sh I saw ...
2 sh I saw Al ...
3 rex Ixsaw Al ...
4 sh Ixsaw Al with ...
5a rey IxsawyAl with ...
5b sh Ixsaw Al with Joe
Figure 2: A trace of vanilla shift-reduce. After
step (4), the parser branches off into (5a) or (5b).
queue head position (current word q0 is wj). At
each step, we choose one of the three actions:
1. sh: move the head of queue, wj , onto stack S
as a singleton tree;
2. rex: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1xs0.
3. rey: combine the top two trees on the stack,
s0 and s1, and replace them with tree s1ys0.
Note that the shorthand notation txt? denotes a
new tree by ?attaching tree t? as the leftmost child
of the root of tree t?. This procedure can be sum-
marized as a deductive system in Figure 1. States
are organized according to step ?, which denotes
the number of actions accumulated. The parser
runs in linear-time as there are exactly 2n?1 steps
for a sentence of n words.
As an example, consider the sentence ?I saw Al
with Joe? in Figure 2. At step (4), we face a shift-
reduce conflict: either combine ?saw? and ?Al? in
a rey action (5a), or shift ?with? (5b). To resolve
this conflict, there is a cost c associated with each
state so that we can pick the best one (or few, with
a beam) at each step. Costs are accumulated in
each step: as shown in Figure 1, actions sh, rex,
and rey have their respective costs ?, ?, and ?,
which are dot-products of the weights w and fea-
tures extracted from the state and the action.
2.2 Features
We view features as ?abstractions? or (partial) ob-
servations of the current state, which is an im-
portant intuition for the development of dynamic
programming in Section 3. Feature templates
are functions that draw information from the fea-
ture window (see Tab. 1(b)), consisting of the
top few trees on the stack and the first few
words on the queue. For example, one such fea-
ture templatef100 = s0.w ? q0.t is a conjunction
1078
of two atomic features s0.w and q0.t, capturing
the root word of the top tree s0 on the stack, and
the part-of-speech tag of the current head word q0
on the queue. See Tab. 1(a) for the list of feature
templates used in the full model. Feature templates
are instantiated for a specific state. For example, at
step (4) in Fig. 2, the above template f100 will gen-
erate a feature instance
(s0.w = Al) ? (q0.t = IN).
More formally, we denote f to be the feature func-
tion, such that f(j, S) returns a vector of feature
instances for state ?j, S?. To decide which action
is the best for the current state, we perform a three-
way classification based on f(j, S), and to do so,
we further conjoin these feature instances with the
action, producing action-conjoined instances like
(s0.w = Al) ? (q0.t = IN) ? (action = sh).
We denote fsh(j, S), frex (j, S), and frey (j, S) to
be the conjoined feature instances, whose dot-
products with the weight vector decide the best ac-
tion (see Eqs. (1-3) in Fig. 1).
2.3 Beam Search and Early Update
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search
(Zhang and Clark, 2008), where b states develop
in parallel. At each step we extend the states in
the current beam by applying one of the three ac-
tions, and then choose the best b resulting states
for the next step. Our dynamic programming algo-
rithm also runs on top of beam search in practice.
To train the model, we use the averaged percep-
tron algorithm (Collins, 2002). Following Collins
and Roark (2004) we also use the ?early-update?
strategy, where an update happens whenever the
gold-standard action-sequence falls off the beam,
with the rest of the sequence neglected.3 The intu-
ition behind this strategy is that later mistakes are
often caused by previous ones, and are irrelevant
when the parser is on the wrong track. Dynamic
programming turns out to be a great fit for early
updating (see Section 4.3 for details).
3 Dynamic Programming (DP)
3.1 Merging Equivalent States
The key observation for dynamic programming
is to merge ?equivalent states? in the same beam
3As a special case, for the deterministic mode (b=1), up-
dates always co-occur with the first mistake made.
(a) Features Templates f(j, S) qi = wj+i
(1) s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
(2) s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.t ? s1.w ? s1.t s0.w ? s1.w ? s1.t
s0.w ? s0.t ? s1.w s0.w ? s0.t ? s1 ? s1.t
(3) s0.t ? q0.t ? q1.t s1.t ? s0.t ? q0.t
s0.w ? q0.t ? q1.t s1.t ? s0.w ? q0.t
(4) s1.t ? s1.lc.t ? s0.t s1.t ? s1.rc.t ? s0.t
s1.t ? s0.t ? s0.rc.t s1.t ? s1.lc.t ? s0
s1.t ? s1.rc.t ? s0.w s1.t ? s0.w ? s0.lc.t
(5) s2.t ? s1.t ? s0.t
(b) ? stack queue?
... s2
...
s1
s1.lc
...
... s1.rc
...
s0
s0.lc
...
... s0.rc
...
q0 q1 ...
(c) Kernel features for DP
ef(j, S) = (j, f2(s2), f1(s1), f0(s0))
f2(s2) s2.t
f1(s1) s1.w s1.t s1.lc.t s1.rc.t
f0(s0) s0.w s0.t s0.lc.t s0.rc.t
j q0.w q0.t q1.t
Table 1: (a) feature templates used in this work,
adapted from Huang et al (2009). x.w and x.t de-
notes the root word and POS tag of tree (or word)
x. and x.lc and x.rc denote x?s left- and rightmost
child. (b) feature window. (c) kernel features.
(i.e., same step) if they have the same feature
values, because they will have the same costs as
shown in the deductive system in Figure 1. Thus
we can define two states ?j, S? and ?j?, S?? to be
equivalent, notated ?j, S? ? ?j?, S??, iff.
j = j? and f(j, S) = f(j?, S?). (4)
Note that j = j? is also needed because the
queue head position j determines which word to
shift next. In practice, however, a small subset of
atomic features will be enough to determine the
whole feature vector, which we call kernel fea-
tures f?(j, S), defined as the smallest set of atomic
templates such that
f?(j, S) = f?(j?, S?) ? ?j, S? ? ?j?, S??.
For example, the full list of 28 feature templates
in Table 1(a) can be determined by just 12 atomic
features in Table 1(c), which just look at the root
words and tags of the top two trees on stack, as
well as the tags of their left- and rightmost chil-
dren, plus the root tag of the third tree s2, and fi-
nally the word and tag of the queue head q0 and the
1079
state form ? : ?i, j, sd...s0?: (c, v, ?) ?: step; c, v: prefix and inside costs; ?: predictor states
equivalence ? : ?i, j, sd...s0? ? ? : ?i, j, s?d...s?0? iff. f?(j, sd...s0) = f?(j, s?d...s?0)
ordering ? : : (c, v, ) ? ? : : (c?, v?, ) iff. c < c? or (c = c? and v < v?).
axiom (p0) 0 : ?0, 0, ??: (0, 0, ?)
sh
state p:
? : ? , j, sd...s0?: (c, , )
? + 1 : ?j, j + 1, sd?1...s0, wj? : (c + ?, 0, {p})
j < n
rex
state p:
: ?k, i, s?d...s?0?: (c?, v?, ??)
state q:
? : ?i, j, sd...s0?: ( , v, ?)
? + 1 : ?k, j, s?d...s?1, s?0
xs0? : (c? + v + ?, v? + v + ?, ??)
p ? ?
goal 2n? 1 : ?0, n, sd...s0?: (c, c, {p0})
where ? = w ? fsh(j, sd...s0), and ? = ?? + ?, with ?? = w ? fsh(i, s?d...s?0) and ? = w ? frex (j, sd...s0).
Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set ?
is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995).
The rey case is similar, replacing s?0
xs0 with s?0
ys0, and ? with ? = w ? frey (j, sd...s0). Irrelevant
information in a deduction step is marked as an underscore ( ) which means ?can match anything?.
tag of the next word q1. Since the queue is static
information to the parser (unlike the stack, which
changes dynamically), we can use j to replace fea-
tures from the queue. So in general we write
f?(j, S) = (j, fd(sd), . . . , f0(s0))
if the feature window looks at top d + 1 trees
on stack, and where fi(si) extracts kernel features
from tree si (0 ? i ? d). For example, for the full
model in Table 1(a) we have
f?(j, S) = (j, f2(s2), f1(s1), f0(s0)), (5)
where d = 2, f2(x) = x.t, and f1(x) = f0(x) =
(x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
3.2 Graph-Structured Stack and Deduction
Now that we have the kernel feature functions, it
is intuitive that we might only need to remember
the relevant bits of information from only the last
(d + 1) trees on stack instead of the whole stack,
because they provide all the relevant information
for the features, and thus determine the costs. For
shift, this suffices as the stack grows on the right;
but for reduce actions the stack shrinks, and in or-
der still to maintain d + 1 trees, we have to know
something about the history. This is exactly why
we needed the full stack for vanilla shift-reduce
parsing in the first place, and why dynamic pro-
gramming seems hard here.
To solve this problem we borrow the idea
of ?graph-structured stack? (GSS) from Tomita
(1991). Basically, each state p carries with it a set
?(p) of predictor states, each of which can be
combined with p in a reduction step. In a shift step,
if state p generates state q (we say ?p predicts q?
in Earley (1970) terms), then p is added onto ?(q).
When two equivalent shifted states get merged,
their predictor states get combined. In a reduction
step, state q tries to combine with every predictor
state p ? ?(q), and the resulting state r inherits
the predictor states set from p, i.e., ?(r) = ?(p).
Interestingly, when two equivalent reduced states
get merged, we can prove (by induction) that their
predictor states are identical (proof omitted).
Figure 3 shows the new deductive system with
dynamic programming and GSS. A new state has
the form
? : ?i, j, sd...s0?
where [i..j] is the span of the top tree s0, and
sd..s1 are merely ?left-contexts?. It can be com-
bined with some predictor state p spanning [k..i]
?? : ?k, i, s?d...s?0?
to form a larger state spanning [k..j], with the
resulting top tree being either s1xs0 or s1ys0.
1080
This style resembles CKY and Earley parsers. In
fact, the chart in Earley and other agenda-based
parsers is indeed a GSS when viewed left-to-right.
In these parsers, when a state is popped up from
the agenda, it looks for possible sibling states
that can combine with it; GSS, however, explicitly
maintains these predictor states so that the newly-
popped state does not need to look them up.4
3.3 Correctness and Polynomial Complexity
We state the main theoretical result with the proof
omitted due to space constraints:
Theorem 1. The deductive system is optimal and
runs in worst-case polynomial time as long as the
kernel feature function satisfies two properties:
? bounded: f?(j, S) = (j, fd(sd), . . . , f0(s0))
for some constant d, and each |ft(x)| also
bounded by a constant for all possible tree x.
? monotonic: ft(x) = ft(y) ? ft+1(x) =
ft+1(y), for all t and all possible trees x, y.
Intuitively, boundedness means features can
only look at a local window and can only extract
bounded information on each tree, which is always
the case in practice since we can not have infinite
models. Monotonicity, on the other hand, says that
features drawn from trees farther away from the
top should not be more refined than from those
closer to the top. This is also natural, since the in-
formation most relevant to the current decision is
always around the stack top. For example, the ker-
nel feature function in Eq. 5 is bounded and mono-
tonic, since f2 is less refined than f1 and f0.
These two requirements are related to grammar
refinement by annotation (Johnson, 1998), where
annotations must be bounded and monotonic: for
example, one cannot refine a grammar by only
remembering the grandparent but not the parent
symbol. The difference here is that the annotations
are not vertical ((grand-)parent), but rather hori-
zontal (left context). For instance, a context-free
rule A ? B C would become DA ? DB BC
for some D if there exists a rule E ? ?DA?.
This resembles the reduce step in Fig. 3.
The very high-level idea of the proof is that
boundedness is crucial for polynomial-time, while
monotonicity is used for the optimal substructure
property required by the correctness of DP.
4In this sense, GSS (Tomita, 1988) is really not a new in-
vention: an efficient implementation of Earley (1970) should
already have it implicitly, similar to what we have in Fig. 3.
3.4 Beam Search based on Prefix Cost
Though the DP algorithm runs in polynomial-
time, in practice the complexity is still too high,
esp. with a rich feature set like the one in Ta-
ble 1. So we apply the same beam search idea
from Sec. 2.3, where each step can accommodate
only the best b states. To decide the ordering of
states in each beam we borrow the concept of pre-
fix cost from Stolcke (1995), originally developed
for weighted Earley parsing. As shown in Fig. 3,
the prefix cost c is the total cost of the best action
sequence from the initial state to the end of state p,
i.e., it includes both the inside cost v (for Viterbi
inside derivation), and the cost of the (best) path
leading towards the beginning of state p. We say
that a state p with prefix cost c is better than a state
p? with prefix cost c?, notated p ? p? in Fig. 3, if
c < c?. We can also prove (by contradiction) that
optimizing for prefix cost implies optimal inside
cost (Nederhof, 2003, Sec. 4). 5
As shown in Fig. 3, when a state q with costs
(c, v) is combined with a predictor state p with
costs (c?, v?), the resulting state r will have costs
(c? + v + ?, v? + v + ?),
where the inside cost is intuitively the combined
inside costs plus an additional combo cost ? from
the combination, while the resulting prefix cost
c? + v + ? is the sum of the prefix cost of the pre-
dictor state q, the inside cost of the current state p,
and the combo cost. Note the prefix cost of q is ir-
relevant. The combo cost ? = ?? + ? consists of
shift cost ?? of p and reduction cost ? of q.
The cost in the non-DP shift-reduce algorithm
(Fig. 1) is indeed a prefix cost, and the DP algo-
rithm subsumes the non-DP one as a special case
where no two states are equivalent.
3.5 Example: Edge-Factored Model
As a concrete example, Figure 4 simulates an
edge-factored model (Eisner, 1996; McDonald et
al., 2005a) using shift-reduce with dynamic pro-
gramming, which is similar to bilexical PCFG
parsing using CKY (Eisner and Satta, 1999). Here
the kernel feature function is
f?(j, S) = (j, h(s1), h(s0))
5Note that using inside cost v for ordering would be a
bad idea, as it will always prefer shorter derivations like in
best-first parsing. As in A* search, we need some estimate
of ?outside cost? to predict which states are more promising,
and the prefix cost includes an exact cost for the left outside
context, but no right outside context.
1081
sh
? : ? , h
...j
? : (c, )
? + 1 : ?h, j? : (c, 0) j < n
rex
: ?h??, h?
k...i
? : (c?, v?) ? : ?h?, h
i...j
? : ( , v)
? + 1 : ?h??, h
h?
k...i
i...j
? : (c? + v + ?, v? + v + ?)
where rex cost ? = w ? frex(h?, h)
Figure 4: Example of shift-reduce with dynamic
programming: simulating an edge-factored model.
GSS is implicit here, and rey case omitted.
where h(x) returns the head word index of tree x,
because all features in this model are based on the
head and modifier indices in a dependency link.
This function is obviously bounded and mono-
tonic in our definitions. The theoretical complexity
of this algorithm is O(n7) because in a reduction
step we have three span indices and three head in-
dices, plus a step index ?. By contrast, the na??ve
CKY algorithm for this model is O(n5) which can
be improved to O(n3) (Eisner, 1996).6 The higher
complexity of our algorithm is due to two factors:
first, we have to maintain both h and h? in one
state, because the current shift-reduce model can
not draw features across different states (unlike
CKY); and more importantly, we group states by
step ? in order to achieve incrementality and lin-
ear runtime with beam search that is not (easily)
possible with CKY or MST.
4 Experiments
We first reimplemented the reference shift-reduce
parser of Huang et al (2009) in Python (hence-
forth ?non-DP?), and then extended it to do dy-
namic programing (henceforth ?DP?). We evalu-
ate their performances on the standard Penn Tree-
bank (PTB) English dependency parsing task7 us-
ing the standard split: secs 02-21 for training, 22
for development, and 23 for testing. Both DP and
non-DP parsers use the same feature templates in
Table 1. For Secs. 4.1-4.2, we use a baseline model
trained with non-DP for both DP and non-DP, so
that we can do a side-by-side comparison of search
6Or O(n2) with MST, but including non-projective trees.
7Using the head rules of Yamada and Matsumoto (2003).
quality; in Sec. 4.3 we will retrain the model with
DP and compare it against training with non-DP.
4.1 Speed Comparisons
To compare parsing speed between DP and non-
DP, we run each parser on the development set,
varying the beam width b from 2 to 16 (DP) or 64
(non-DP). Fig. 5a shows the relationship between
search quality (as measured by the average model
score per sentence, higher the better) and speed
(average parsing time per sentence), where DP
with a beam width of b=16 achieves the same
search quality with non-DP at b=64, while being 5
times faster. Fig. 5b shows a similar comparison
for dependency accuracy. We also test with an
edge-factored model (Sec. 3.5) using feature tem-
plates (1)-(3) in Tab. 1, which is a subset of those
in McDonald et al (2005b). As expected, this dif-
ference becomes more pronounced (8 times faster
in Fig. 5c), since the less expressive feature set
makes more states ?equivalent? and mergeable in
DP. Fig. 5d shows the (almost linear) correlation
between dependency accuracy and search quality,
confirming that better search yields better parsing.
4.2 Search Space, Forest, and Oracles
DP achieves better search quality because it ex-
pores an exponentially large search space rather
than only b trees allowed by the beam (see Fig. 6a).
As a by-product, DP can output a forest encoding
these exponentially many trees, out of which we
can draw longer and better (in terms of oracle) k-
best lists than those in the beam (see Fig. 6b). The
forest itself has an oracle of 98.15 (as if k ? ?),
computed a` la Huang (2008, Sec. 4.1). These can-
didate sets may be used for reranking (Charniak
and Johnson, 2005; Huang, 2008).8
4.3 Perceptron Training and Early Updates
Another interesting advantage of DP over non-DP
is the faster training with perceptron, even when
both parsers use the same beam width. This is due
to the use of early updates (see Sec. 2.3), which
happen much more often with DP, because a gold-
standard state p is often merged with an equivalent
(but incorrect) state that has a higher model score,
which triggers update immediately. By contrast, in
non-DP beam search, states such as p might still
8DP?s k-best lists are extracted from the forest using the
algorithm of Huang and Chiang (2005), rather than those in
the final beam as in the non-DP case, because many deriva-
tions have been merged during dynamic programming.
1082
 2370
 2373
 2376
 2379
 2382
 2385
 2388
 2391
 2394
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
av
g.
 m
od
el
 sc
or
e
b=16 b=64
DP
non-DP
 92.2
 92.3
 92.4
 92.5
 92.6
 92.7
 92.8
 92.9
 93
 93.1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
de
pe
nd
en
cy
 a
cc
ur
ac
y
b=16 b=64
DP
non-DP
(a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model)
 2290
 2295
 2300
 2305
 2310
 2315
 2320
 2325
 2330
 2335
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
av
g.
 m
od
el
 sc
or
e
b=16
b=64
DP
non-DP
 88.5
 89
 89.5
 90
 90.5
 91
 91.5
 92
 92.5
 93
 93.5
 2280  2300  2320  2340  2360  2380  2400
de
pe
nd
en
cy
 a
cc
ur
ac
y
full, DP
full, non-DP
edge-factor, DP
edge-factor, non-DP
(c) search quality vs. time (edge-factored model) (d) correlation b/w parsing (y) and search (x)
Figure 5: Speed comparisons between DP and non-DP, with beam size b ranging 2?16 for DP and 2?64
for non-DP. Speed is measured by avg. parsing time (secs) per sentence on x axis. With the same level
of search quality or parsing accuracy, DP (at b=16) is ?4.8 times faster than non-DP (at b=64) with the
full model in plots (a)-(b), or ?8 times faster with the simplified edge-factored model in plot (c). Plot (d)
shows the (roughly linear) correlation between parsing accuracy and search quality (avg. model score).
100
102
104
106
108
1010
1012
 0  10  20  30  40  50  60  70
nu
m
be
r o
f t
re
es
 e
xp
lo
re
d
sentence length
DP forest
non-DP (16)
 93
 94
 95
 96
 97
 98
 99
 64 32 16 8 4 1
or
ac
le
 p
re
ci
sio
n
 k
DP forest (98.15)
DP k-best in forest
non-DP k-best in beam
(a) sizes of search spaces (b) oracle precision on dev
Figure 6: DP searches over a forest of exponentially many trees, which also produces better and longer
k-best lists with higher oracles, while non-DP only explores b trees allowed in the beam (b = 16 here).
1083
 90.5
 91
 91.5
 92
 92.5
 93
 93.5
 0  4  8  12  16  20  24
ac
cu
ra
cy
 o
n 
de
v 
(e
ac
h 
ro
un
d)
hours
17th
18th
DP
non-DP
Figure 7: Learning curves (showing precision on
dev) of perceptron training for 25 iterations (b=8).
DP takes 18 hours, peaking at the 17th iteration
(93.27%) with 12 hours, while non-DP takes 23
hours, peaking at the 18th (93.04%) with 16 hours.
survive in the beam throughout, even though it is
no longer possible to rank the best in the beam.
The higher frequency of early updates results
in faster iterations of perceptron training. Table 2
shows the percentage of early updates and the time
per iteration during training. While the number of
updates is roughly comparable between DP and
non-DP, the rate of early updates is much higher
with DP, and the time per iteration is consequently
shorter. Figure 7 shows that training with DP is
about 1.2 times faster than non-DP, and achieves
+0.2% higher accuracy on the dev set (93.27%).
Besides training with gold POS tags, we also
trained on noisy tags, since they are closer to the
test setting (automatic tags on sec 23). In that
case, we tag the dev and test sets using an auto-
matic POS tagger (at 97.2% accuracy), and tag
the training set using four-way jackknifing sim-
ilar to Collins (2000), which contributes another
+0.1% improvement in accuracy on the test set.
Faster training also enables us to incorporate more
features, where we found more lookahead features
(q2) results in another +0.3% improvement.
4.4 Final Results on English and Chinese
Table 3 presents the final test results of our DP
parser on the Penn English Treebank, compared
with other state-of-the-art parsers. Our parser
achieves the highest (unlabeled) dependency ac-
curacy among dependency parsers trained on the
Treebank, and is also much faster than most other
parsers even with a pure Python implementation
it update early% time update early% time
1 31943 98.9 22 31189 87.7 29
5 20236 98.3 38 19027 70.3 47
17 8683 97.1 48 7434 49.5 60
25 5715 97.2 51 4676 41.2 65
Table 2: Perceptron iterations with DP (left) and
non-DP (right). Early updates happen much more
often with DP due to equivalent state merging,
which leads to faster training (time in minutes).
word L time comp.
McDonald 05b 90.2 Ja 0.12 O(n2)
McDonald 05a 90.9 Ja 0.15 O(n3)
Koo 08 base 92.0 ? ? O(n4)
Zhang 08 single 91.4 C 0.11 O(n)?
this work 92.1 Py 0.04 O(n)
?Charniak 00 92.5 C 0.49 O(n5)
?Petrov 07 92.4 Ja 0.21 O(n3)
Zhang 08 combo 92.1 C ? O(n2)?
Koo 08 semisup 93.2 ? ? O(n4)
Table 3: Final test results on English (PTB). Our
parser (in pure Python) has the highest accuracy
among dependency parsers trained on the Tree-
bank, and is also much faster than major parsers.
?converted from constituency trees. C=C/C++,
Py=Python, Ja=Java. Time is in seconds per sen-
tence. Search spaces: ?linear; others exponential.
(on a 3.2GHz Xeon CPU). Best-performing con-
stituency parsers like Charniak (2000) and Berke-
ley (Petrov and Klein, 2007) do outperform our
parser, since they consider more information dur-
ing parsing, but they are at least 5 times slower.
Figure 8 shows the parse time in seconds for each
test sentence. The observed time complexity of our
DP parser is in fact linear compared to the super-
linear complexity of Charniak, MST (McDonald
et al, 2005b), and Berkeley parsers. Additional
techniques such as semi-supervised learning (Koo
et al, 2008) and parser combination (Zhang and
Clark, 2008) do achieve accuracies equal to or
higher than ours, but their results are not directly
comparable to ours since they have access to ex-
tra information like unlabeled data. Our technique
is orthogonal to theirs, and combining these tech-
niques could potentially lead to even better results.
We also test our final parser on the Penn Chi-
nese Treebank (CTB5). Following the set-up of
Duan et al (2007) and Zhang and Clark (2008), we
split CTB5 into training (secs 001-815 and 1001-
1084
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 0  10  20  30  40  50  60  70
pa
rs
in
g 
tim
e 
(s
ec
s)
sentence length
Cha
Berk
MST
DP
Figure 8: Scatter plot of parsing time against sen-
tence length, comparing with Charniak, Berkeley,
and the O(n2) MST parsers.
word non-root root compl.
Duan 07 83.88 84.36 73.70 32.70
Zhang 08? 84.33 84.69 76.73 32.79
this work 85.20 85.52 78.32 33.72
Table 4: Final test results on Chinese (CTB5).
?The transition parser in Zhang and Clark (2008).
1136), development (secs 886-931 and 1148-
1151), and test (secs 816-885 and 1137-1147) sets,
assume gold-standard POS-tags for the input, and
use the head rules of Zhang and Clark (2008). Ta-
ble 4 summarizes the final test results, where our
work performs the best in all four types of (unla-
beled) accuracies: word, non-root, root, and com-
plete match (all excluding punctuations). 9,10
5 Related Work
This work was inspired in part by Generalized LR
parsing (Tomita, 1991) and the graph-structured
stack (GSS). Tomita uses GSS for exhaustive LR
parsing, where the GSS is equivalent to a dy-
namic programming chart in chart parsing (see
Footnote 4). In fact, Tomita?s GLR is an in-
stance of techniques for tabular simulation of non-
deterministic pushdown automata based on deduc-
tive systems (Lang, 1974), which allow for cubic-
time exhaustive shift-reduce parsing with context-
free grammars (Billot and Lang, 1989).
Our work advances this line of research in two
aspects. First, ours is more general than GLR in
9Duan et al (2007) and Zhang and Clark (2008) did not
report word accuracies, but those can be recovered given non-
root and root ones, and the number of non-punctuation words.
10Parser combination in Zhang and Clark (2008) achieves
a higher word accuracy of 85.77%, but again, it is not directly
comparable to our work.
that it is not restricted to LR (a special case of
shift-reduce), and thus does not require building an
LR table, which is impractical for modern gram-
mars with a large number of rules or features. In
contrast, we employ the ideas behind GSS more
flexibly to merge states based on features values,
which can be viewed as constructing an implicit
LR table on-the-fly. Second, unlike previous the-
oretical results about cubic-time complexity, we
achieved linear-time performance by smart beam
search with prefix cost inspired by Stolcke (1995),
allowing for state-of-the-art data-driven parsing.
To the best of our knowledge, our work is the
first linear-time incremental parser that performs
dynamic programming. The parser of Roark and
Hollingshead (2009) is also almost linear time, but
they achieved this by discarding parts of the CKY
chart, and thus do achieve incrementality.
6 Conclusion
We have presented a dynamic programming al-
gorithm for shift-reduce parsing, which runs in
linear-time in practice with beam search. This
framework is general and applicable to a large-
class of shift-reduce parsers, as long as the feature
functions satisfy boundedness and monotonicity.
Empirical results on a state-the-art dependency
parser confirm the advantage of DP in many as-
pects: faster speed, larger search space, higher ora-
cles, and better and faster learning. Our final parser
outperforms all previously reported dependency
parsers trained on the Penn Treebanks for both
English and Chinese, and is much faster in speed
(even with a Python implementation). For future
work we plan to extend it to constituency parsing.
Acknowledgments
We thank David Chiang, Yoav Goldberg, Jonathan
Graehl, Kevin Knight, and Roger Levy for help-
ful discussions and the three anonymous review-
ers for comments. Mark-Jan Nederhof inspired the
use of prefix cost. Yue Zhang helped with Chinese
datasets, and Wenbin Jiang with feature sets. This
work is supported in part by DARPA GALE Con-
tract No. HR0011-06-C-0022 under subcontract to
BBN Technologies, and by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). Statements and opinions expressed
do not necessarily reflect the position or the policy
of the United States Government, and no official
endorsement should be inferred.
1085
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
S. Billot and B. Lang. 1989. The structure of shared
forests in ambiguous parsing. In Proceedings of the
27th ACL, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACL, Ann Ar-
bor, MI.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of ICML,
pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/PKDD.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the ACM, 13(2):94?
102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of ACL.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of COLING.
Lyn Frazier and Keith Rayner. 1982. Making and cor-
recting errors during sentence comprehension: Eye
movements in the analysis of structurally ambigu-
ous sentences. Cognitive Psychology, 14(2):178 ?
210.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT-2005).
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24:613?632.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
B. Lang. 1974. Deterministic techniques for efficient
non-deterministic parsers. In Automata, Languages
and Programming, 2nd Colloquium, volume 14 of
Lecture Notes in Computer Science, pages 255?269,
Saarbru?cken. Springer-Verlag.
Lillian Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1?15.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, pages 135?143.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings of HLT-NAACL.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165?201.
Masaru Tomita. 1988. Graph-structured stack and nat-
ural language parsing. In Proceedings of the 26th
annual meeting on Association for Computational
Linguistics, pages 249?257, Morristown, NJ, USA.
Association for Computational Linguistics.
Masaru Tomita, editor. 1991. Generalized LR Parsing.
Kluwer Academic Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
1086
Tutorial Abstracts of ACL 2010, page 2,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Tree-based and Forest-based Translation
Yang Liu
Institute of Computing Technology
Chinese Academy of Sciences
yliu@ict.ac.cn
Liang Huang
Information Sciences Institute
University of Southern California
lhuang@isi.edu
1 Introduction
The past several years have witnessed rapid ad-
vances in syntax-based machine translation, which
exploits natural language syntax to guide transla-
tion. Depending on the type of input, most of these
efforts can be divided into two broad categories:
(a) string-based systems whose input is a string,
which is simultaneously parsed and translated by a
synchronous grammar (Wu, 1997; Chiang, 2005;
Galley et al, 2006), and (b) tree-based systems
whose input is already a parse tree to be directly
converted into a target tree or string (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Liu et
al., 2006; Huang et al, 2006).
Compared with their string-based counterparts,
tree-based systems offer many attractive features:
they are much faster in decoding (linear time vs.
cubic time), do not require sophisticated bina-
rization (Zhang et al, 2006), and can use sepa-
rate grammars for parsing and translation (e.g. a
context-free grammar for the former and a tree
substitution grammar for the latter).
However, despite these advantages, most tree-
based systems suffer from a major drawback: they
only use 1-best parse trees to direct translation,
which potentially introduces translation mistakes
due to parsing errors (Quirk and Corston-Oliver,
2006). This situation becomes worse for resource-
poor source languages without enough Treebank
data to train a high-accuracy parser.
This problem can be alleviated elegantly by us-
ing packed forests (Huang, 2008), which encodes
exponentially many parse trees in a polynomial
space. Forest-based systems (Mi et al, 2008; Mi
and Huang, 2008) thus take a packed forest instead
of a parse tree as an input. In addition, packed
forests could also be used for translation rule ex-
traction, which helps alleviate the propagation of
parsing errors into rule set. Forest-based transla-
tion can be regarded as a compromise between the
string-based and tree-based methods, while com-
bining the advantages of both: decoding is still
fast, yet does not commit to a single parse. Sur-
prisingly, translating a forest of millions of trees
is even faster than translating 30 individual trees,
and offers significantly better translation quality.
This approach has since become a popular topic.
2 Content Overview
This tutorial surveys tree-based and forest-based
translation methods. For each approach, we will
discuss the two fundamental tasks: decoding,
which performs the actual translation, and rule ex-
traction, which learns translation rules from real-
world data automatically. Finally, we will in-
troduce some more recent developments to tree-
based and forest-based translation, such as tree
sequence based models, tree-to-tree models, joint
parsing and translation, and faster decoding algo-
rithms. We will conclude our talk by pointing out
some directions for future work.
3 Tutorial Overview
1. Tree-based Translation
? Motivations and Overview
? Tree-to-String Model and Decoding
? Tree-to-String Rule Extraction
? Language Model-Integrated Decoding:
Cube Pruning
2. Forest-based Translation
? Packed Forest
? Forest-based Decoding
? Forest-based Rule Extraction
3. Extensions
? Tree-Sequence-to-String Models
? Tree-to-Tree Models
? Joint Parsing and Translation
? Faster Decoding Methods
4. Conclusion and Open Problems
2
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856?864,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Rule Markov Models for Fast Tree-to-String Translation
Ashish Vaswani
Information Sciences Institute
University of Southern California
avaswani@isi.edu
Haitao Mi
Institute of Computing Technology
Chinese Academy of Sciences
htmi@ict.ac.cn
Liang Huang and David Chiang
Information Sciences Institute
University of Southern California
{lhuang,chiang}@isi.edu
Abstract
Most statistical machine translation systems
rely on composed rules (rules that can be
formed out of smaller rules in the grammar).
Though this practice improves translation by
weakening independence assumptions in the
translation model, it nevertheless results in
huge, redundant grammars, making both train-
ing and decoding inefficient. Here, we take the
opposite approach, where we only use min-
imal rules (those that cannot be formed out
of other rules), and instead rely on a rule
Markov model of the derivation history to
capture dependencies between minimal rules.
Large-scale experiments on a state-of-the-art
tree-to-string translation system show that our
approach leads to a slimmer model, a faster
decoder, yet the same translation quality (mea-
sured using Bleu) as composed rules.
1 Introduction
Statistical machine translation systems typically
model the translation process as a sequence of trans-
lation steps, each of which uses a translation rule,
for example, a phrase pair in phrase-based transla-
tion or a tree-to-string rule in tree-to-string transla-
tion. These rules are usually applied independently
of each other, which violates the conventional wis-
dom that translation should be done in context.
To alleviate this problem, most state-of-the-art sys-
tems rely on composed rules, which are larger rules
that can be formed out of smaller rules (includ-
ing larger phrase pairs that can be formerd out of
smaller phrase pairs), as opposed to minimal rules,
which are rules that cannot be formed out of other
rules. Although this approach does improve trans-
lation quality dramatically by weakening the inde-
pendence assumptions in the translation model, they
suffer from two main problems. First, composition
can cause a combinatorial explosion in the number
of rules. To avoid this, ad-hoc limits are placed dur-
ing composition, like upper bounds on the number
of nodes in the composed rule, or the height of the
rule. Under such limits, the grammar size is man-
ageable, but still much larger than the minimal-rule
grammar. Second, due to large grammars, the de-
coder has to consider many more hypothesis transla-
tions, which slows it down. Nevertheless, the advan-
tages outweigh the disadvantages, and to our knowl-
edge, all top-performing systems, both phrase-based
and syntax-based, use composed rules. For exam-
ple, Galley et al (2004) initially built a syntax-based
system using only minimal rules, and subsequently
reported (Galley et al, 2006) that composing rules
improves Bleu by 3.6 points, while increasing gram-
mar size 60-fold and decoding time 15-fold.
The alternative we propose is to replace composed
rules with a rule Markov model that generates rules
conditioned on their context. In this work, we re-
strict a rule?s context to the vertical chain of ances-
tors of the rule. This ancestral context would play
the same role as the context formerly provided by
rule composition. The dependency treelet model de-
veloped by Quirk and Menezes (2006) takes such
an approach within the framework of dependency
translation. However, their study leaves unanswered
whether a rule Markov model can take the place
of composed rules. In this work, we investigate the
use of rule Markov models in the context of tree-
856
to-string translation (Liu et al, 2006; Huang et al,
2006). We make three new contributions.
First, we carry out a detailed comparison of rule
Markov models with composed rules. Our experi-
ments show that, using trigram rule Markov mod-
els, we achieve an improvement of 2.2 Bleu over
a baseline of minimal rules. When we compare
against vertically composed rules, we find that our
rule Markov model has the same accuracy, but our
model is much smaller and decoding with our model
is 30% faster. When we compare against full com-
posed rules, we find that our rule Markov model still
often reaches the same level of accuracy, again with
savings in space and time.
Second, we investigate methods for pruning rule
Markov models, finding that even very simple prun-
ing criteria actually improve the accuracy of the
model, while of course decreasing its size.
Third, we present a very fast decoder for tree-to-
string grammars with rule Markov models. Huang
and Mi (2010) have recently introduced an efficient
incremental decoding algorithm for tree-to-string
translation, which operates top-down and maintains
a derivation history of translation rules encountered.
This history is exactly the vertical chain of ancestors
corresponding to the contexts in our rule Markov
model, which makes it an ideal decoder for our
model.
We start by describing our rule Markov model
(Section 2) and then how to decode using the rule
Markov model (Section 3).
2 Rule Markov models
Our model which conditions the generation of a rule
on the vertical chain of its ancestors, which allows it
to capture interactions between rules.
Consider the example Chinese-English tree-to-
string grammar in Figure 1 and the example deriva-
tion in Figure 2. Each row is a derivation step; the
tree on the left is the derivation tree (in which each
node is a rule and its children are the rules that sub-
stitute into it) and the tree pair on the right is the
source and target derived tree. For any derivation
node r, let anc1(r) be the parent of r (or  if it has no
parent), anc2(r) be the grandparent of node r (or  if
it has no grandparent), and so on. Let ancn1(r) be the
chain of ancestors anc1(r) ? ? ? ancn(r).
The derivation tree is generated as follows. With
probability P(r1 | ), we generate the rule at the root
node, r1. We then generate rule r2 with probability
P(r2 | r1), and so on, always taking the leftmost open
substitution site on the English derived tree, and gen-
erating a rule ri conditioned on its chain of ancestors
with probability P(ri | ancn1(ri)). We carry on until
no more children can be generated. Thus the proba-
bility of a derivation tree T is
P(T ) =
?
r?T
P(r | ancn1(r)) (1)
For the minimal rule derivation tree in Figure 2, the
probability is:
P(T ) = P(r1 | ) ? P(r2 | r1) ? P(r3 | r1)
? P(r4 | r1, r3) ? P(r6 | r1, r3, r4)
? P(r7 | r1, r3, r4) ? P(r5 | r1, r3) (2)
Training We run the algorithm of Galley et al
(2004) on word-aligned parallel text to obtain a sin-
gle derivation of minimal rules for each sentence
pair. (Unaligned words are handled by attaching
them to the highest node possible in the parse tree.)
The rule Markov model
can then be trained on the path set of these deriva-
tion trees.
Smoothing We use interpolation with absolute
discounting (Ney et al, 1994):
Pabs(r | ancn1(r)) =
max
{
c(r | ancn1(r)) ? Dn, 0
}
?
r? c(r? | ancn1(r?))
+ (1 ? ?n)Pabs(r | ancn?11 (r)), (3)
where c(r | ancn1(r)) is the number of times we have
seen rule r after the vertical context ancn1(r), Dn is
the discount for a context of length n, and (1 ? ?n) is
set to the value that makes the smoothed probability
distribution sum to one.
We experiment with bigram and trigram rule
Markov models. For each, we try different values of
D1 and D2, the discount for bigrams and trigrams,
respectively. Ney et al (1994) suggest using the fol-
lowing value for the discount Dn:
Dn =
n1
n1 + n2
(4)
857
rule id translation rule
r1 IP(x1:NP x2:VP) ? x1 x2
r2 NP(Bu`sh??) ? Bush
r3 VP(x1:PP x2:VP) ? x2 x1
r4 PP(x1:P x2:NP) ? x1 x2
r5 VP(VV(ju?x??ng) AS(le) NPB(hu?`ta?n)) ? held talks
r6 P(yu?) ? with
r?6 P(yu?) ? and
r7 NP(Sha?lo?ng) ? Sharon
Figure 1: Example tree-to-string grammar.
derivation tree derived tree pair
 IP@ : IP@
r1
IP@
NP@1 VP@2 : NP
@1 VP@2
r1
r2 r3
IP@
NP@1
Bu`sh??
VP@2
PP@2.1 VP@2.2
: Bush VP@2.2 PP@2.1
r1
r2 r3
r4 r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1 NP@2.1.2
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks P@2.1.1 NP@2.1.2
r1
r2 r3
r4
r6 r7
r5
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV
ju?x??ng
AS
le
NP
hu?`ta?n
: Bush held talks with Sharon
Figure 2: Example tree-to-string derivation. Each row shows a rewriting step; at each step, the leftmost nonterminal
symbol is rewritten using one of the rules in Figure 1.
858
Here, n1 and n2 are the total number of n-grams with
exactly one and two counts, respectively. For our
corpus, D1 = 0.871 and D2 = 0.902. Additionally,
we experiment with 0.4 and 0.5 for Dn.
Pruning In addition to full n-gram Markov mod-
els, we experiment with three approaches to build
smaller models to investigate if pruning helps. Our
results will show that smaller models indeed give a
higher Bleu score than the full bigram and trigram
models. The approaches we use are:
? RM-A: We keep only those contexts in which
more than P unique rules were observed. By
optimizing on the development set, we set P =
12.
? RM-B: We keep only those contexts that were
observed more than P times. Note that this is a
superset of RM-A. Again, by optimizing on the
development set, we set P = 12.
? RM-C: We try a more principled approach
for learning variable-length Markov models in-
spired by that of Bejerano and Yona (1999),
who learn a Prediction Suffix Tree (PST). They
grow the PST in an iterative manner by start-
ing from the root node (no context), and then
add contexts to the tree. A context is added if
the KL divergence between its predictive distri-
bution and that of its parent is above a certain
threshold and the probability of observing the
context is above another threshold.
3 Tree-to-string decoding with rule
Markov models
In this paper, we use our rule Markov model frame-
work in the context of tree-to-string translation.
Tree-to-string translation systems (Liu et al, 2006;
Huang et al, 2006) have gained popularity in recent
years due to their speed and simplicity. The input to
the translation system is a source parse tree and the
output is the target string. Huang and Mi (2010) have
recently introduced an efficient incremental decod-
ing algorithm for tree-to-string translation. The de-
coder operates top-down and maintains a derivation
history of translation rules encountered. The history
is exactly the vertical chain of ancestors correspond-
ing to the contexts in our rule Markov model. This
IP@
NP@1
Bu`sh??
VP@2
PP@2.1
P@2.1.1
yu?
NP@2.1.2
Sha?lo?ng
VP@2.2
VV@2.2.1
ju?x??ng
AS@2.2.2
le
NP@2.2.3
hu?`ta?n
Figure 3: Example input parse tree with tree addresses.
makes incremental decoding a natural fit with our
generative story. In this section, we describe how
to integrate our rule Markov model into this in-
cremental decoding algorithm. Note that it is also
possible to integrate our rule Markov model with
other decoding algorithms, for example, the more
common non-incremental top-down/bottom-up ap-
proach (Huang et al, 2006), but it would involve
a non-trivial change to the decoding algorithms to
keep track of the vertical derivation history, which
would result in significant overhead.
Algorithm Given the input parse tree in Figure 3,
Figure 4 illustrates the search process of the incre-
mental decoder with the grammar of Figure 1. We
write X@? for a tree node with label X at tree address
? (Shieber et al, 1995). The root node has address ,
and the ith child of node ? has address ?.i. At each
step, the decoder maintains a stack of active rules,
which are rules that have not been completed yet,
and the rightmost (n ? 1) English words translated
thus far (the hypothesis), where n is the order of the
word language model (in Figure 4, n = 2). The stack
together with the translated English words comprise
a state of the decoder. The last column in the fig-
ure shows the rule Markov model probabilities with
the conditioning context. In this example, we use a
trigram rule Markov model.
After initialization, the process starts at step 1,
where we predict rule r1 (the shaded rule) with prob-
ability P(r1 | ) and push its English side onto the
stack, with variables replaced by the correspond-
ing tree nodes: x1 becomes NP@1 and x2 becomes
VP@2. This gives us the following stack:
s = [ NP@1 VP@2]
The dot () indicates the next symbol to process in
859
stack hyp. MR prob.
0 [<s>  IP@ </s>] <s>
1 [<s>  IP@ </s>] [ NP@1 VP@2] <s> P(r1 | )
2 [<s>  IP@ </s>] [ NP@1 VP@2] [ Bush] <s> P(r2 | r1)
3 [<s>  IP@ </s>] [ NP@1 VP@2] [Bush  ] . . . Bush
4 [<s>  IP@ </s>] [NP@1  VP@2] . . . Bush
5 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] . . . Bush P(r3 | r1)
6 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks] . . . Bush P(r5 | r1, r3)
7 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held  talks] . . . held
8 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks  ] . . . talks
9 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] . . . talks
10 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] . . . talks P(r4 | r1, r3)
11 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ with] . . . with P(r6 | r3, r4)
12 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [with  ] . . . with
13 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . with
14 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . with P(r7 | r3, r4)
11? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ and] . . . and P(r?6 | r3, r4)
12? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [and  ] . . . and
13? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] . . . and
14? [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] . . . and P(r7 | r3, r4)
15 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [Sharon  ] . . . Sharon
16 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1 NP@2.1.2  ] . . . Sharon
17 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2 PP@2.1  ] . . . Sharon
18 [<s>  IP@ </s>] [NP@1 VP@2  ] . . . Sharon
19 [<s> IP@  </s>] . . . Sharon
20 [<s> IP@ </s>  ] . . . </s>
Figure 4: Simulation of incremental decoding with rule Markov model. The solid arrows indicate one path and the
dashed arrows indicate an alternate path.
860
VP@2
VP@2.2 PP@2.1
P@2.1.1
yu?
NP@2.1.2
Figure 5: Vertical context r3 r4 which allows the model
to correctly translate yu? as with.
the English word order. We expand node NP@1 first
with English word order. We then predict lexical rule
r2 with probability P(r2 | r1) and push rule r2 onto
the stack:
[ NP@1 VP@2 ] [ Bush]
In step 3, we perform a scan operation, in which
we append the English word just after the dot to the
current hypothesis and move the dot after the word.
Since the dot is at the end of the top rule in the stack,
we perform a complete operation in step 4 where we
pop the finished rule at the top of the stack. In the
scan and complete steps, we don?t need to compute
rule probabilities.
An interesting branch occurs after step 10 with
two competing lexical rules, r6 and r?6. The Chinese
word yu? can be translated as either a preposition with
(leading to step 11) or a conjunction and (leading
to step 11?). The word n-gram model does not have
enough information to make the correct choice, with.
As a result, good translations might be pruned be-
cause of the beam. However, our rule Markov model
has the correct preference because of the condition-
ing ancestral sequence (r3, r4), shown in Figure 5.
Since VP@2.2 has a preference for yu? translating to
with, our corpus statistics will give a higher proba-
bility to P(r6 | r3, r4) than P(r?6 | r3, r4). This helps
the decoder to score the correct translation higher.
Complexity analysis With the incremental decod-
ing algorithm, adding rule Markov models does not
change the time complexity, which is O(nc|V |g?1),
where n is the sentence length, c is the maximum
number of incoming hyperedges for each node in the
translation forest, V is the target-language vocabu-
lary, and g is the order of the n-gram language model
(Huang and Mi, 2010). However, if one were to use
rule Markov models with a conventional CKY-style
bottom-up decoder (Liu et al, 2006), the complexity
would increase to O(nCm?1|V |4(g?1)), where C is the
maximum number of outgoing hyperedges for each
node in the translation forest, and m is the order of
the rule Markov model.
4 Experiments and results
4.1 Setup
The training corpus consists of 1.5M sentence pairs
with 38M/32M words of Chinese/English, respec-
tively. Our development set is the newswire portion
of the 2006 NIST MT Evaluation test set (616 sen-
tences), and our test set is the newswire portion of
the 2008 NIST MT Evaluation test set (691 sen-
tences).
We word-aligned the training data using GIZA++
followed by link deletion (Fossum et al, 2008),
and then parsed the Chinese sentences using the
Berkeley parser (Petrov and Klein, 2007). To extract
tree-to-string translation rules, we applied the algo-
rithm of Galley et al (2004). We trained our rule
Markov model on derivations of minimal rules as
described above. Our trigram word language model
was trained on the target side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing. The base feature
set for all systems is similar to the set used in Mi et
al. (2008). The features are combined into a standard
log-linear model, which we trained using minimum
error-rate training (Och, 2003) to maximize the Bleu
score on the development set.
At decoding time, we again parse the input
sentences using the Berkeley parser, and convert
them into translation forests using rule pattern-
matching (Mi et al, 2008). We evaluate translation
quality using case-insensitive IBM Bleu-4, calcu-
lated by the script mteval-v13a.pl.
4.2 Results
Table 1 presents the main results of our paper. We
used grammars of minimal rules and composed rules
of maximum height 3 as our baselines. For decod-
ing, we used a beam size of 50. Using the best
bigram rule Markov models and the minimal rule
grammar gives us an improvement of 1.5 Bleu over
the minimal rule baseline. Using the best trigram
rule Markov model brings our gain up to 2.3 Bleu.
861
grammar rule Markov max parameters (?10
6) Bleu time
model rule height full dev+test test (sec/sent)
minimal None 3 4.9 0.3 24.2 1.2
RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8
RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0
vertical composed None 7 176.8 1.3 26.5 2.9
composed None 3 17.5 1.6 26.4 2.2
None 7 448.7 3.3 27.5 6.8
RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2
Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same
level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for
both the full model and the model filtered for the concatenation of the development and test sets (dev+test).
These gains are statistically significant with p <
0.01, using bootstrap resampling with 1000 samples
(Koehn, 2004). We find that by just using bigram
context, we are able to get at least 1 Bleu point
higher than the minimal rule grammar. It is interest-
ing to see that using just bigram rule interactions can
give us a reasonable boost. We get our highest gains
from using trigram context where our best perform-
ing rule Markov model gives us 2.3 Bleu points over
minimal rules. This suggests that using longer con-
texts helps the decoder to find better translations.
We also compared rule Markov models against
composed rules. Since our models are currently lim-
ited to conditioning on vertical context, the closest
comparison is against vertically composed rules. We
find that our approach performs equally well using
much less time and space.
Comparing against full composed rules, we find
that our system matches the score of the base-
line composed rule grammar of maximum height 3,
while using many fewer parameters. (It should be
noted that a parameter in the rule Markov model is
just a floating-point number, whereas a parameter in
the composed-rule system is an entire rule; there-
fore the difference in memory usage would be even
greater.) Decoding with our model is 0.2 seconds
faster per sentence than with composed rules.
These experiments clearly show that rule Markov
models with minimal rules increase translation qual-
ity significantly and with lower memory require-
ments than composed rules. One might wonder if
the best performance can be obtained by combin-
ing composed rules with a rule Markov model. This
rule Markov D1
Bleu time
model dev (sec/sent)
RM-A 0.871 29.2 1.8
RM-B 0.4 29.9 1.8
RM-C 0.871 29.8 1.8
RM-Full 0.4 29.7 1.9
Table 2: For rule bigrams, RM-B with D1 = 0.4 gives the
best results on the development set.
rule Markov D1 D2
Bleu time
model dev (sec/sent)
RM-A 0.5 0.5 30.3 2.0
RM-B 0.5 0.5 29.9 2.0
RM-C 0.5 0.5 30.1 2.0
RM-Full 0.4 0.5 30.1 2.2
Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 gives
the best results on the development set.
is straightforward to implement: the rule Markov
model is still defined over derivations of minimal
rules, but in the decoder?s prediction step, the rule
Markov model?s value on a composed rule is cal-
culated by decomposing it into minimal rules and
computing the product of their probabilities. We find
that using our best trigram rule Markov model with
composed rules gives us a 0.5 Bleu gain on top of
the composed rule grammar, statistically significant
with p < 0.05, achieving our highest score of 28.0.1
4.3 Analysis
Tables 2 and 3 show how the various types of rule
Markov models compare, for bigrams and trigrams,
1For this experiment, a beam size of 100 was used.
862
parameters (?106) Bleu dev/test time (sec/sent)
dev/test without RMM with RMM without/with RMM
2.6 31.0/27.0 31.1/27.4 4.5/7.0
2.9 31.5/27.7 31.4/27.3 5.6/8.1
3.3 31.4/27.5 31.4/28.0 6.8/9.2
Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.
D2
D1
0.4 0.5 0.871
0.4 30.0 30.0
0.5 29.3 30.3
0.902 30.0
Table 4: RM-A is robust to different settings of Dn on the
development set.
parameters (?106) Bleu time
dev+test dev test (sec/sent)
1.2 30.2 26.1 2.8
1.3 30.1 26.5 2.9
1.3 30.1 26.2 3.2
Table 5: Comparison of vertically composed rules using
various settings (maximum rule height 7).
respectively. It is interesting that the full bigram and
trigram rule Markov models do not give our high-
est Bleu scores; pruning the models not only saves
space but improves their performance. We think that
this is probably due to overfitting.
Table 4 shows that the RM-A trigram model does
fairly well under all the settings of Dn we tried. Ta-
ble 5 shows the performance of vertically composed
rules at various settings. Here we have chosen the
setting that gives the best performance on the test
set for inclusion in Table 1.
Table 6 shows the performance of fully composed
rules and fully composed rules with a rule Markov
Model at various settings.2 In the second line (2.9
million rules), the drop in Bleu score resulting from
adding the rule Markov model is not statistically sig-
nificant.
5 Related Work
Besides the Quirk and Menezes (2006) work dis-
cussed in Section 1, there are two other previous
2For these experiments, a beam size of 100 was used.
efforts both using a rule bigram model in machine
translation, that is, the probability of the current rule
only depends on the immediate previous rule in the
vertical context, whereas our rule Markov model
can condition on longer and sparser derivation his-
tories. Among them, Ding and Palmer (2005) also
use a dependency treelet model similar to Quirk and
Menezes (2006), and Liu and Gildea (2008) use a
tree-to-string model more like ours. Neither com-
pared to the scenario with composed rules.
Outside of machine translation, the idea of weak-
ening independence assumptions by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. However, be-
sides the difference between parsing and translation,
there are still two major differences. First, our work
conditions rule probabilities on parent and grandpar-
ent rules, not just nonterminals. Second, we com-
pare against a composed-rule system, which is anal-
ogous to the Data Oriented Parsing (DOP) approach
in parsing (Bod, 2003). To our knowledge, there has
been no direct comparison between a history-based
PCFG approach and DOP approach in the parsing
literature.
6 Conclusion
In this paper, we have investigated whether we can
eliminate composed rules without any loss in trans-
lation quality. We have developed a rule Markov
model that captures vertical bigrams and trigrams of
minimal rules, and tested it in the framework of tree-
to-string translation. We draw three main conclu-
sions from our experiments. First, our rule Markov
models dramatically improve a grammar of minimal
rules, giving an improvement of 2.3 Bleu. Second,
when we compare against vertically composed rules
we are able to get about the same Bleu score, but
our model is much smaller and decoding with our
863
model is faster. Finally, when we compare against
full composed rules, we find that we can reach the
same level of performance under some conditions,
but in order to do so consistently, we believe we
need to extend our model to condition on horizon-
tal context in addition to vertical context. We hope
that by modeling context in both axes, we will be
able to completely replace composed-rule grammars
with smaller minimal-rule grammars.
Acknowledgments
We would like to thank Fernando Pereira, Yoav
Goldberg, Michael Pust, Steve DeNeefe, Daniel
Marcu and Kevin Knight for their comments.
Mi?s contribution was made while he was vis-
iting USC/ISI. This work was supported in part
by DARPA under contracts HR0011-06-C-0022
(subcontract to BBN Technologies), HR0011-09-1-
0028, and DOI-NBC N10AP20031, by a Google
Faculty Research Award to Huang, and by the Na-
tional Natural Science Foundation of China under
contracts 60736014 and 90920004.
References
Gill Bejerano and Golan Yona. 1999. Modeling pro-
tein families using probabilistic suffix trees. In Proc.
RECOMB, pages 15?24. ACM Press.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of EACL, pages 19?26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of ACL, pages 541?
548.
Victoria Fossum, Kevin Knight, and Steve Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66?73.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-
string transducer for machine translation. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 62?69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192?199.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404?411.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of NAACL
HLT, pages 9?16.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?36.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
864
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?319,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the `0-norm
Ashish Vaswani Liang Huang David Chiang
University of Southern California
Information Sciences Institute
{avaswani,lhuang,chiang}@isi.edu
Abstract
Two decades after their invention, the IBM
word-based translation models, widely avail-
able in the GIZA++ toolkit, remain the dom-
inant approach to word alignment and an in-
tegral part of many statistical translation sys-
tems. Although many models have surpassed
them in accuracy, none have supplanted them
in practice. In this paper, we propose a simple
extension to the IBM models: an `0 prior to en-
courage sparsity in the word-to-word transla-
tion model. We explain how to implement this
extension efficiently for large-scale data (also
released as a modification to GIZA++) and
demonstrate, in experiments on Czech, Ara-
bic, Chinese, and Urdu to English translation,
significant improvements over IBM Model 4
in both word alignment (up to +6.7 F1) and
translation quality (up to +1.4 Bleu).
1 Introduction
Automatic word alignment is a vital component of
nearly all current statistical translation pipelines. Al-
though state-of-the-art translation models use rules
that operate on units bigger than words (like phrases
or tree fragments), they nearly always use word
alignments to drive extraction of those translation
rules. The dominant approach to word alignment has
been the IBM models (Brown et al, 1993) together
with the HMM model (Vogel et al, 1996). These
models are unsupervised, making them applicable
to any language pair for which parallel text is avail-
able. Moreover, they are widely disseminated in the
open-source GIZA++ toolkit (Och and Ney, 2004).
These properties make them the default choice for
most statistical MT systems.
In the decades since their invention, many mod-
els have surpassed them in accuracy, but none has
supplanted them in practice. Some of these models
are partially supervised, combining unlabeled paral-
lel text with manually-aligned parallel text (Moore,
2005; Taskar et al, 2005; Riesa and Marcu, 2010).
Although manually-aligned data is very valuable, it
is only available for a small number of language
pairs. Other models are unsupervised like the IBM
models (Liang et al, 2006; Grac?a et al, 2010; Dyer
et al, 2011), but have not been as widely adopted as
GIZA++ has.
In this paper, we propose a simple extension to
the IBM/HMM models that is unsupervised like the
IBM models, is as scalable as GIZA++ because it is
implemented on top of GIZA++, and provides sig-
nificant improvements in both alignment and trans-
lation quality. It extends the IBM/HMM models by
incorporating an `0 prior, inspired by the princi-
ple of minimum description length (Barron et al,
1998), to encourage sparsity in the word-to-word
translation model (Section 2.2). This extension fol-
lows our previous work on unsupervised part-of-
speech tagging (Vaswani et al, 2010), but enables
it to scale to the large datasets typical in word
alignment, using an efficient training method based
on projected gradient descent (Section 2.3). Ex-
periments on Czech-, Arabic-, Chinese- and Urdu-
English translation (Section 3) demonstrate consis-
tent significant improvements over IBM Model 4 in
both word alignment (up to +6.7 F1) and transla-
tion quality (up to +1.4 Bleu). Our implementation
has been released as a simple modification to the
GIZA++ toolkit that can be used as a drop-in re-
placement for GIZA++ in any existing MT pipeline.
311
2 Method
We start with a brief review of the IBM and HMM
word alignment models, then describe how to extend
them with a smoothed `0 prior and how to efficiently
train them.
2.1 IBM Models and HMM
Given a French string f = f1 ? ? ? f j ? ? ? fm and an
English string e = e1 ? ? ? ei ? ? ? e`, these models de-
scribe the process by which the French string is
generated by the English string via the alignment
a = a1, . . . , a j, . . . , am. Each a j is a hidden vari-
ables, indicating which English word ea j the French
word f j is aligned to.
In IBM Model 1?2 and the HMM model, the joint
probability of the French sentence and alignment
given the English sentence is
P(f, a | e) =
m?
j=1
d(a j | a j?1, j)t( f j | ea j). (1)
The parameters of these models are the distortion
probabilities d(a j | a j?1, j) and the translation prob-
abilities t( f j | ea j). The three models differ in their
estimation of d, but the differences do not concern us
here. All three models, as well as IBM Models 3?5,
share the same t. For further details of these models,
the reader is referred to the original papers describ-
ing them (Brown et al, 1993; Vogel et al, 1996).
Let ? stand for all the parameters of the model.
The standard training procedure is to find the param-
eter values that maximize the likelihood, or, equiv-
alently, minimize the negative log-likelihood of the
observed data:
?? = arg min
?
(
? log P(f | e, ?)
)
(2)
= arg min
?
?
??????? log
?
a
P(f, a | e, ?)
?
?????? (3)
This is done using the Expectation-Maximization
(EM) algorithm (Dempster et al, 1977).
2.2 MAP-EM with the `0-norm
Maximum likelihood training is prone to overfitting,
especially in models with many parameters. In word
alignment, one well-known manifestation of overfit-
ting is that rare words can act as ?garbage collectors?
(Moore, 2004), aligning to many unrelated words.
This hurts alignment precision and rule-extraction
recall. Previous attempted remedies include early
stopping, smoothing (Moore, 2004), and posterior
regularization (Grac?a et al, 2010).
We have previously proposed another simple
remedy to overfitting in the context of unsuper-
vised part-of-speech tagging (Vaswani et al, 2010),
which is to minimize the size of the model using a
smoothed `0 prior. Applying this prior to an HMM
improves tagging accuracy for both Italian and En-
glish.
Here, our goal is to apply a similar prior in a
word-alignment model to the word-to-word transla-
tion probabilities t( f | e). We leave the distortion
models alone, since they are not very large, and there
is not much reason to believe that we can profit from
compacting them.
With the addition of the `0 prior, the MAP (maxi-
mum a posteriori) objective function is
?? = arg min
?
(
? log P(f | e, ?)P(?)
)
(4)
where
P(?) ? exp
(
??????0
)
(5)
and
????0 =
?
e, f
(
1 ? exp
?t( f | e)
?
)
(6)
is a smoothed approximation of the `0-norm. The
hyperparameter ? controls the tightness of the ap-
proximation, as illustrated in Figure 1. Substituting
back into (4) and dropping constant terms, we get
the following optimization problem: minimize
? log P(f | e, ?) ? ?
?
e, f
exp
?t( f | e)
?
(7)
subject to the constraints
?
f
t( f | e) = 1 for all e. (8)
We can carry out the optimization in (7) with the
MAP-EM algorithm (Bishop, 2006). EM and MAP-
EM share the same E-step; the difference lies in the
312
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
Figure 1: The `0-norm (top curve) and smoothed approx-
imations (below) for ? = 0.05, 0.1, 0.2.
M-step. For vanilla EM, the M-step is:
?? = arg min
?
?
????????
?
?
e, f
E[C(e, f )] log t( f | e)
?
????????
(9)
again subject to the constraints (8). The count
C(e, f ) is the number of times that f occurs aligned
to e. For MAP-EM, it is:
?? = arg min
?
(
?
?
e, f
E[C(e, f )] log t( f | e) ?
?
?
e, f
exp
?t( f | e)
?
) (10)
This optimization problem is non-convex, and we
do not know of a closed-form solution. Previously
(Vaswani et al, 2010), we used ALGENCAN, a non-
linear optimization toolkit, but this solution does not
scale well to the number of parameters involved in
word alignment models. Instead, we use a simpler
and more scalable method which we describe in the
next section.
2.3 Projected gradient descent
Following Schoenemann (2011b), we use projected
gradient descent (PGD) to solve the M-step (but
with the `0-norm instead of the `1-norm). Gradient
projection methods are attractive solutions to con-
strained optimization problems, particularly when
the constraints on the parameters are simple (Bert-
sekas, 1999). Let F(?) be the objective function in
(10); we seek to minimize this function. As in pre-
vious work (Vaswani et al, 2010), we optimize each
set of parameters {t(? | e)} separately for each En-
glish word type e. The inputs to the PGD are the
expected counts E[C(e, f )] and the current word-to-
word conditional probabilities ?. We run PGD for K
iterations, producing a sequence of intermediate pa-
rameter vectors ?1, . . . , ?k, . . . , ?K . Each iteration has
two steps, a projection step and a line search.
Projection step In this step, we compute:
?
k
=
[
?k ? s?F(?k)
]?
(11)
This moves ? in the direction of steepest descent
(?F) with step size s, and then the function [?]?
projects the resulting point onto the simplex; that
is, it finds the nearest point that satisfies the con-
straints (8).
The gradient ?F(?k) is
?F
?t( f | e)
= ?
E[C( f , e)]
t( f | e)
+
?
?
exp
?t( f | e)
?
(12)
In contrast to Schoenemann (2011b), we use an
O(n log n) algorithm for the projection step due to
Duchi et. al. (2008), shown in Pseudocode 1.
Pseudocode 1 Project input vector u ? Rn onto the
probability simplex.
v = u sorted in non-increasing order
? = 0
for i = 1 to n do
if vi ? 1i
(?i
r=1 vr ? 1
)
> 0 then
? = i
end if
end for
? = 1
?
(??
r=1 vr ? 1
)
wr = max{vr ? ?, 0} for 1 ? r ? n
return w
Line search Next, we move to a point between ?k
and ?
k
that satisfies the Armijo condition,
F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
(13)
where ?m = ?m(?
k
? ?k) and ? and ? are both con-
stants in (0, 1). We try values m = 1, 2, . . . until the
Armijo condition (13) is satisfied or the limit m = 20
313
Pseudocode 2 Find a point between ?k and ?
k
that
satisfies the Armijo condition.
Fmin = F(?k)
?min = ?
k
for m = 1 to 20 do
?m = ?
m
(
?
k
? ?k
)
if F(?k + ?m) < Fmin then
Fmin = F(?k + ?m)
?min = ?
k + ?m
end if
if F(?k + ?m) ? F(?k) + ?
(
?F(?k) ? ?m
)
then
break
end if
end for
?k+1 = ?min
return ?k+1
is reached. (Note that we don?t allow m = 0 because
this can cause ?k + ?m to land on the boundary of
the probability simplex, where the objective func-
tion is undefined.) Then we set ?k+1 to the point in
{?k} ? {?k + ?m | 1 ? m ? 20} that minimizes F.
The line search algorithm is summarized in Pseu-
docode 2.
In our implementation, we set ? = 0.5 and ? =
0.5. We keep s fixed for all PGD iterations; we ex-
perimented with s ? {0.1, 0.5} and did not observe
significant changes in F-score. We run the projection
step and line search alternately for at most K itera-
tions, terminating early if there is no change in ?k
from one iteration to the next. We set K = 35 for the
large Arabic-English experiment; for all other con-
ditions, we set K = 50. These choices were made to
balance efficiency and accuracy. We found that val-
ues of K between 30 and 75 were generally reason-
able.
3 Experiments
To demonstrate the effect of the `0-norm on the IBM
models, we performed experiments on four trans-
lation tasks: Arabic-English, Chinese-English, and
Urdu-English from the NIST Open MT Evaluation,
and the Czech-English translation from the Work-
shop on Machine Translation (WMT) shared task.
We measured the accuracy of word alignments gen-
erated by GIZA++ with and without the `0-norm,
and also translation accuracy of systems trained us-
ing the word alignments. Across all tests, we found
strong improvements from adding the `0-norm.
3.1 Training
We have implemented our algorithm as an open-
source extension to GIZA++.1 Usage of the exten-
sion is identical to standard GIZA++, except that the
user can switch the `0 prior on or off, and adjust the
hyperparameters ? and ?.
For vanilla EM, we ran five iterations of Model 1,
five iterations of HMM, and ten iterations of
Model 4. For our approach, we first ran one iter-
ation of Model 1, followed by four iterations of
Model 1 with smoothed `0, followed by five itera-
tions of HMM with smoothed `0. Finally, we ran ten
iterations of Model 4.2
We used the following parallel data:
? Chinese-English: selected data from the con-
strained task of the NIST 2009 Open MT Eval-
uation.3
? Arabic-English: all available data for the
constrained track of NIST 2009, excluding
United Nations proceedings (LDC2004E13),
ISI Automatically Extracted Parallel Text
(LDC2007E08), and Ummah newswire text
(LDC2004T18), for a total of 5.4+4.3 mil-
lion words. We also experimented on a larger
Arabic-English parallel text of 44+37 million
words from the DARPA GALE program.
? Urdu-English: all available data for the con-
strained track of NIST 2009.
1The code can be downloaded from the first author?s website
at http://www.isi.edu/?avaswani/giza-pp-l0.html.
2GIZA++ allows changing some heuristic parameters for
efficient training. Currently, we set two of these to zero:
mincountincrease and probcutoff. In the default setting,
both are set to 10?7. We set probcutoff to 0 because we would
like the optimization to learn the parameter values. For a fair
comparison, we applied the same setting to our vanilla EM
training as well. To test, we ran GIZA++ with the default set-
ting on the smaller of our two Arabic-English datasets with the
same number of iterations and found no change in F-score.
3LDC catalog numbers LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,
LDC2006E85, LDC2006E86, LDC2006E92, and
LDC2006E93.
314
pr
es
id
en
t
of th
e
fo
re
ig
n
af
fa
ir
s
in
st
it
ut
e
sh
uq
in
li
u
wa
s
al
so
pr
es
en
t
at th
e
me
et
in
g
.
   u u           
wa`ijia?o
     u          
xue?hu?`
u               
hu?`zha?ng
       u        
liu?
      u u        
shu?q??ng
             u  
hu?`jia`n
               
sh??
         u u u  u  
za`izuo`
              u 
.
ov
er
40
00
gu
es
ts
fr
om
ho
me
an
d
ab
ro
ad
at
te
nd
ed
th
e
op
en
in
g
ce
re
mo
ny
.
    u  u      
zho?ngwa`i
  u          
la?ib??n
 u           
s?`qia?n
u u           
duo?
            
re?n
       u     
chu?x??
       u     
le
         u u  
ka?imu`sh?`
           u 
.
(a) (b)
it ?s ex
tr
em
el
y
tr
ou
bl
es
om
e
to ge
t
th
er
e
vi
a
la
nd
.
u          
ru?guo?
    u      
ya`o
       u u  
lu`lu`
          
zhua?n
     u u    
qu`
          
dehua`
          
ne
         u 
,
  u        
he?n
  u        
he?n
  u        
he?n
  u        
he?n
   u       
ma?fan
          
de
         u 
,
af
te
r
th
is
wa
s
ta
ke
n
ca
re
of , fo
ur
bl
oc
kh
ou
se
s
we
re
bl
ow
n
up .
 u            
zhe`ge
    u         
chu`l??
             
wa?n
u             
y??ho`u
             
ne
      u       
,
             
ha?i
          u   
zha`
           u  
le
       u      
s?`ge
        u     
dia?oba?o
            u 
.
(c) (d)
Figure 2: Smoothed-`0 alignments (red circles) correct many errors in the baseline GIZA++ alignments (black
squares), as shown in four Chinese-English examples (the red circles are almost perfect for these examples, except
for minor mistakes such as liu-shu?q??ng and meeting-za`izuo` in (a) and .-, in (c)). In particular, the baseline system
demonstrates typical ?garbage-collection? phenomena in proper name ?shuqing? in both languages in (a), number
?4000? and word ?la?ib??n? (lit. ?guest?) in (b), word ?troublesome? and ?lu`lu`? (lit. ?land-route?) in (c), and ?block-
houses? and ?dia?oba?o? (lit. ?bunker?) in (d). We found this garbage-collection behavior to be especially common with
proper names, numbers, and uncommon words in both languages. Most interestingly, in (c), our smoothed-`0 system
correctly aligns ?extremely? to ?he?n he?n he?n he?n? (lit. ?very very very very?) which is rare in the bitext.
315
task data (M) system align F1 (%) word trans (M) ??sing. Bleu (%)
2008 2009 2010
Chi-Eng 9.6+12
baseline 73.2 3.5 6.2 28.7
`0-norm 76.5 2.0 3.3 29.5
difference +3.3 ?43% ?47% +0.8
Ara-Eng 5.4+4.3
baseline 65.0 3.1 4.5 39.8 42.5
`0-norm 70.8 1.8 1.8 41.1 43.7
difference +5.9 ?39% ?60% +1.3 +1.2
Ara-Eng 44+37
baseline 66.2 15 5.0 41.6 44.9
`0-norm 71.8 7.9 1.8 42.5 45.3
difference +5.6 ?47% ?64% +0.9 +0.4
Urd-Eng 1.7+1.5
baseline 1.7 4.5 25.3? 29.8
`0-norm 1.2 2.2 25.9? 31.2
difference ?29% ?51% +0.6? +1.4
Cze-Eng 2.1+2.3
baseline 65.6 1.5 3.0 17.3 18.0
`0-norm 72.3 1.0 1.4 17.9 18.4
difference +6.7 ?33% ?53% +0.6 +0.4
Table 1: Adding the `0-norm to the IBM models improves both alignment and translation accuracy across four different
language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the
lexical weighting table) is reduced. The ??sing. column shows the average fertility of once-seen source words. For
Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open
MT Evaluation. ?Half of this test set was also used for tuning feature weights.
? Czech-English: A corpus of 4 million words of
Czech-English data from the News Commen-
tary corpus.4
We set the hyperparameters ? and ? by tuning
on gold-standard word alignments (to maximize F1)
when possible. For Arabic-English and Chinese-
English, we used 346 and 184 hand-aligned sen-
tences from LDC2006E86 and LDC2006E93. Sim-
ilarly, for Czech-English, 515 hand-aligned sen-
tences were available (Bojar and Prokopova?, 2006).
But for Urdu-English, since we did not have any
gold alignments, we used ? = 10 and ? = 0.05. We
did not choose a large ?, as the dataset was small,
and we chose a conservative value for ?.
We ran word alignment in both directions and
symmetrized using grow-diag-final (Koehn et al,
2003). For models with the smoothed `0 prior, we
tuned ? and ? separately in each direction.
3.2 Alignment
First, we evaluated alignment accuracy directly by
comparing against gold-standard word alignments.
4This data is available at http://statmt.org/wmt10.
The results are shown in the alignment F1 col-
umn of Table 1. We used balanced F-measure rather
than alignment error rate as our metric (Fraser and
Marcu, 2007).
Following Dyer et al (2011), we also measured
the average fertility, ??sing., of once-seen source
words in the symmetrized alignments. Our align-
ments show smaller fertility for once-seen words,
suggesting that they suffer from ?garbage collec-
tion? effects less than the baseline alignments do.
The fact that we had to use hand-aligned data to
tune the hyperparameters ? and ? means that our
method is no longer completely unsupervised. How-
ever, our observation is that alignment accuracy is
actually fairly robust to the choice of these hyperpa-
rameters, as shown in Table 2. As we will see below,
we still obtained strong improvements in translation
quality when hand-aligned data was unavailable.
We also tried generating 50 word classes using
the tool provided in GIZA++. We found that adding
word classes improved alignment quality a little, but
more so for the baseline system (see Table 3). We
used the alignments generated by training with word
classes for our translation experiments.
316
? model
?
0 10 25 50 75 100 250 500 750
?
HMM 47.5
M4 52.1
0.5
HMM 46.3 48.4 52.8 55.7 57.5 61.5 62.6 62.7
M4 51.7 53.7 56.4 58.6 59.8 63.3 64.4 64.8
0.1
HMM 55.6 60.4 61.6 62.1 61.9 61.8 60.2 60.1
M4 58.2 62.4 64.0 64.4 64.8 65.5 65.6 65.9
0.05
HMM 59.1 61.4 62.4 62.5 62.3 60.8 58.7 57.7
M4 61.0 63.5 64.6 65.3 65.3 65.4 65.7 65.7
0.01
HMM 59.7 61.6 60.0 59.5 58.7 56.9 55.7 54.7
M4 62.9 65.0 65.1 65.2 65.1 65.4 65.3 65.4
0.005
HMM 58.1 59.0 58.3 57.6 57.0 55.9 53.9 51.7
M4 62.0 64.1 64.5 64.5 64.5 65.0 64.8 64.6
0.001
HMM 51.7 52.1 51.4 49.3 50.4 46.8 45.4 44.0
M4 59.8 61.3 61.5 61.0 61.8 61.2 61.0 61.2
Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model
for Arabic-English alignment (? = 0).
word classes?
direction system no yes
P( f | e)
baseline 49.0 52.1
`0-norm 63.9 65.9
difference +14.9 +13.8
P(e | f )
baseline 64.3 65.2
`0-norm 69.2 70.3
difference +4.9 +5.1
Table 3: Adding word classes improves the F-score in
both directions for Arabic-English alignment by a little,
for the baseline system more so than ours.
Figure 2 shows four examples of Chinese-
English alignment, comparing the baseline with our
smoothed-`0 method. In all four cases, the base-
line produces incorrect extra alignments that prevent
good translation rules from being extracted while
the smoothed-`0 results are correct. In particular, the
baseline system demonstrates typical ?garbage col-
lection? behavior (Moore, 2004) in all four exam-
ples.
3.3 Translation
We then tested the effect of word alignments on
translation quality using the hierarchical phrase-
based translation system Hiero (Chiang, 2007). We
used a fairly standard set of features: seven in-
herited from Pharaoh (Koehn et al, 2003), a sec-
setting align F1 (%) Bleu (%)
t( f | e) t(e | f ) 2008 2009
1st 1st 70.8 41.1 43.7
1st 2nd 70.7 41.1 43.8
2nd 1st 70.7 40.7 44.1
2nd 2nd 70.9 41.1 44.2
Table 4: Optimizing hyperparameters on alignment F1
score does not necessarily lead to optimal Bleu. The
first two columns indicate whether we used the first- or
second-best alignments in each direction (according to
F1); the third column shows the F1 of the symmetrized
alignments, whose corresponding Bleu scores are shown
in the last two columns.
ond language model, and penalties for the glue
rule, identity rules, unknown-word rules, and two
kinds of number/name rules. The feature weights
were discriminatively trained using MIRA (Chi-
ang et al, 2008). We used two 5-gram language
models, one on the combined English sides of
the NIST 2009 Arabic-English and Chinese-English
constrained tracks (385M words), and another on
2 billion words of English.
For each language pair, we extracted grammar
rules from the same data that were used for word
alignment. The development data that were used for
discriminative training were: for Chinese-English
and Arabic-English, data from the NIST 2004 and
NIST 2006 test sets, plus newsgroup data from the
317
GALE program (LDC2006E92); for Urdu-English,
half of the NIST 2008 test set; for Czech-English,
a training set of 2051 sentences provided by the
WMT10 translation workshop.
The results are shown in the Bleu column of Ta-
ble 1. We used case-insensitive IBM Bleu (closest
reference length) as our metric. Significance test-
ing was carried out using bootstrap resampling with
1000 samples (Koehn, 2004; Zhang et al, 2004).
All of the tests showed significant improvements
(p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu.
For Urdu, even though we didn?t have manual align-
ments to tune hyperparameters, we got significant
gains over a good baseline. This is promising for lan-
guages that do not have any manually aligned data.
Ideally, one would want to tune ? and ? to max-
imize Bleu. However, this is prohibitively expen-
sive, especially if we must tune them separately
in each alignment direction before symmetrization.
We ran some contrastive experiments to investi-
gate the impact of hyperparameter tuning on trans-
lation quality. For the smaller Arabic-English cor-
pus, we symmetrized all combinations of the two
top-scoring alignments (according to F1) in each di-
rection, yielding four sets of alignments. Table 4
shows Bleu scores for translation models learned
from these alignments. Unfortunately, we find that
optimizing F1 is not optimal for Bleu?using the
second-best alignments yields a further improve-
ment of 0.5 Bleu on the NIST 2009 data, which is
statistically significant (p < 0.05).
4 Related Work
Schoenemann (2011a), taking inspiration from Bo-
drumlu et al (2009), uses integer linear program-
ming to optimize IBM Model 1?2 and the HMM
with the `0-norm. This method, however, does not
outperform GIZA++. In later work, Schoenemann
(2011b) used projected gradient descent for the `1-
norm. Here, we have adopted his use of projected
gradient descent, but using a smoothed `0-norm.
Liang et al (2006) show how to train IBM mod-
els in both directions simultaneously by adding a
term to the log-likelihood that measures the agree-
ment between the two directions. Grac?a et al (2010)
explore modifications to the HMM model that en-
courage bijectivity and symmetry. The modifications
take the form of constraints on the posterior dis-
tribution over alignments that is computed during
the E-step. Mermer and Sarac?lar (2011) explore a
Bayesian version of IBM Model 1, applying sparse
Dirichlet priors to t. However, because this method
requires the use of Monte Carlo methods, it is not
clear how well it can scale to larger datasets.
5 Conclusion
We have extended the IBM models and HMM model
by the addition of an `0 prior to the word-to-word
translation model, which compacts the word-to-
word translation table, reducing overfitting, and, in
particular, the ?garbage collection? effect. We have
shown how to perform MAP-EM with this prior
efficiently, even for large datasets. The method is
implemented as a modification to the open-source
toolkit GIZA++, and we have shown that it signif-
icantly improves translation quality across four dif-
ferent language pairs. Even though we have used a
small set of gold-standard alignments to tune our
hyperparameters, we found that performance was
fairly robust to variation in the hyperparameters, and
translation performance was good even when gold-
standard alignments were unavailable. We hope that
our method, due to its simplicity, generality, and ef-
fectiveness, will find wide application for training
better statistical translation systems.
Acknowledgments
We are indebted to Thomas Schoenemann for ini-
tial discussions and pilot experiments that led to
this work, and to the anonymous reviewers for
their valuable comments. We thank Jason Riesa for
providing the Arabic-English and Chinese-English
hand-aligned data and the alignment visualization
tool, and Chris Dyer for the Czech-English hand-
aligned data. This research was supported in part
by DARPA under contract DOI-NBC D11AP00244
and a Google Faculty Research Award to L. H.
318
References
Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information Theory,
44(6):2743?2760.
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing.
Ondr?ej Bojar and Magdalena Prokopova?. 2006. Czech-
English word alignment. In Proceedings of LREC.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263?311.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?208.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Computational Linguistics, 39(4):1?38.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Joa?o V. Grac?a, Kuzman Ganchev, and Ben Taskar.
2010. Learning tractable word alignment models
with complex constraints. Computational Linguistics,
36(3):481?504.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Cos?kun Mermer and Murat Sarac?lar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of ACL HLT.
Robert C. Moore. 2004. Improving IBM word-
alignment Model 1. In Proceedings of ACL.
Robert Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of HLT-
EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of ACL.
Thomas Schoenemann. 2011a. Probabilistic word align-
ment under the L0-norm. In Proceedings of CoNLL.
Thomas Schoenemann. 2011b. Regularizing mono- and
bi-word models for word alignment. In Proceedings
of IJCNLP.
Ben Taskar, Lacoste-Julien Simon, and Klein Dan. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT-EMNLP.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC.
319
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?82,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Event Extraction via Structured Prediction with Global Features
Qi Li Heng Ji Liang Huang
Departments of Computer Science and Linguistics
The Graduate Center and Queens College
City University of New York
New York, NY 10016, USA
{liqiearth, hengjicuny, liang.huang.sh}@gmail.com
Abstract
Traditional approaches to the task of ACE
event extraction usually rely on sequential
pipelines with multiple stages, which suf-
fer from error propagation since event trig-
gers and arguments are predicted in isola-
tion by independent local classifiers. By
contrast, we propose a joint framework
based on structured prediction which ex-
tracts triggers and arguments together so
that the local predictions can be mutu-
ally improved. In addition, we propose
to incorporate global features which ex-
plicitly capture the dependencies of multi-
ple triggers and arguments. Experimental
results show that our joint approach with
local features outperforms the pipelined
baseline, and adding global features fur-
ther improves the performance signifi-
cantly. Our approach advances state-of-
the-art sentence-level event extraction, and
even outperforms previous argument la-
beling methods which use external knowl-
edge from other sentences and documents.
1 Introduction
Event extraction is an important and challeng-
ing task in Information Extraction (IE), which
aims to discover event triggers with specific types
and their arguments. Most state-of-the-art ap-
proaches (Ji and Grishman, 2008; Liao and Gr-
ishman, 2010; Hong et al, 2011) use sequential
pipelines as building blocks, which break down
the whole task into separate subtasks, such as
trigger identification/classification and argument
identification/classification. As a common draw-
back of the staged architecture, errors in upstream
component are often compounded and propagated
to the downstream classifiers. The downstream
components, however, cannot impact earlier deci-
sions. For example, consider the following sen-
tences with an ambiguous word ?fired?:
(1) In Baghdad, a cameraman died when an
American tank fired on the Palestine Hotel.
(2) He has fired his air defense chief .
In sentence (1), ?fired? is a trigger of type Attack.
Because of the ambiguity, a local classifier may
miss it or mislabel it as a trigger of End-Position.
However, knowing that ?tank? is very likely to be
an Instrument argument of Attack events, the cor-
rect event subtype assignment of ?fired? is obvi-
ously Attack. Likewise, in sentence (2), ?air de-
fense chief? is a job title, hence the argument clas-
sifier is likely to label it as an Entity argument for
End-Position trigger.
In addition, the local classifiers are incapable
of capturing inter-dependencies among multiple
event triggers and arguments. Consider sentence
(1) again. Figure 1 depicts the corresponding
event triggers and arguments. The dependency be-
tween ?fired? and ?died? cannot be captured by the
local classifiers, which may fail to attach ?camera-
man? to ?fired? as a Target argument. By using
global features, we can propagate the Victim ar-
gument of the Die event to the Target argument
of the Attack event. As another example, know-
ing that an Attack event usually only has one At-
tacker argument, we could penalize assignments
in which one trigger has more than one Attacker.
Such global features cannot be easily exploited by
a local classifier.
Therefore, we take a fresh look at this prob-
lem and formulate it, for the first time, as a struc-
tured learning problem. We propose a novel joint
event extraction algorithm to predict the triggers
and arguments simultaneously, and use the struc-
tured perceptron (Collins, 2002) to train the joint
model. This way we can capture the dependencies
between triggers and argument as well as explore
73
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
AttackDie
Instrument
Place
Victim
Target
Instrument
Target
Place
Figure 1: Event mentions of example (1). There are two event mentions that share three arguments,
namely the Die event mention triggered by ?died?, and the Attack event mention triggered by ?fired?.
arbitrary global features over multiple local pre-
dictions. However, different from easier tasks such
as part-of-speech tagging or noun phrase chunking
where efficient dynamic programming decoding is
feasible, here exact joint inference is intractable.
Therefore we employ beam search in decoding,
and train the model using the early-update percep-
tron variant tailored for beam search (Collins and
Roark, 2004; Huang et al, 2012).
We make the following contributions:
1. Different from traditional pipeline approach,
we present a novel framework for sentence-
level event extraction, which predicts triggers
and their arguments jointly (Section 3).
2. We develop a rich set of features for event
extraction which yield promising perfor-
mance even with the traditional pipeline
(Section 3.4.1). In this paper we refer to them
as local features.
3. We introduce various global features to ex-
ploit dependencies among multiple triggers
and arguments (Section 3.4.2). Experi-
ments show that our approach outperforms
the pipelined approach with the same set of
local features, and significantly advances the
state-of-the-art with the addition of global
features which brings a notable further im-
provement (Section 4).
2 Event Extraction Task
In this paper we focus on the event extraction task
defined in Automatic Content Extraction (ACE)
evaluation.1 The task defines 8 event types and
33 subtypes such as Attack, End-Position etc. We
introduce the terminology of the ACE event ex-
traction that we used in this paper:
1http://projects.ldc.upenn.edu/ace/
? Event mention: an occurrence of an event
with a particular type and subtype.
? Event trigger: the word most clearly ex-
presses the event mention.
? Event argument: an entity mention, tempo-
ral expression or value (e.g. Job-Title) that
serves as a participant or attribute with a spe-
cific role in an event mention.
? Event mention: an instance that includes one
event trigger and some arguments that appear
within the same sentence.
Given an English text document, an event ex-
traction system should predict event triggers with
specific subtypes and their arguments from each
sentence. Figure 1 depicts the event triggers and
their arguments of sentence (1) in Section 1. The
outcome of the entire sentence can be considered a
graph in which each argument role is represented
as a typed edge from a trigger to its argument.
In this work, we assume that argument candi-
dates such as entities are part of the input to the
event extraction, and can be from either gold stan-
dard or IE system output.
3 Joint Framework for Event Extraction
Based on the hypothesis that facts are inter-
dependent, we propose to use structured percep-
tron with inexact search to jointly extract triggers
and arguments that co-occur in the same sentence.
In this section, we will describe the training and
decoding algorithms for this model.
3.1 Structured perceptron with beam search
Structured perceptron is an extension to the stan-
dard linear perceptron for structured prediction,
which was proposed in (Collins, 2002). Given a
sentence instance x ? X , which in our case is a
sentence with argument candidates, the structured
perceptron involves the following decoding prob-
74
lem which finds the best configuration z ? Y ac-
cording to the current model w:
z = argmax
y??Y(x)
w ? f(x, y?) (1)
where f(x, y?) represents the feature vector for in-
stance x along with configuration y?.
The perceptron learns the model w in an on-
line fashion. Let D = {(x(j), y(j))}nj=1 be the set
of training instances (with j indexing the current
training instance). In each iteration, the algorithm
finds the best configuration z for x under the cur-
rent model (Eq. 1). If z is incorrect, the weights
are updated as follows:
w = w + f(x, y)? f(x, z) (2)
The key step of the training and test is the de-
coding procedure, which aims to search for the
best configuration under the current parameters. In
simpler tasks such as part-of-speech tagging and
noun phrase chunking, efficient dynamic program-
ming algorithms can be employed to perform ex-
act inference. Unfortunately, it is intractable to
perform the exact search in our framework be-
cause: (1) by jointly modeling the trigger labeling
and argument labeling, the search space becomes
much more complex. (2) we propose to make use
of arbitrary global features, which makes it infea-
sible to perform exact inference efficiently.
To address this problem, we apply beam-search
along with early-update strategy to perform inex-
act decoding. Collins and Roark (2004) proposed
the early-update idea, and Huang et al (2012) later
proved its convergence and formalized a general
framework which includes it as a special case. Fig-
ure 2 describes the skeleton of perceptron train-
ing algorithm with beam search. In each step of
the beam search, if the prefix of oracle assign-
ment y falls out from the beam, then the top re-
sult in the beam is returned for early update. One
could also use the standard-update for inference,
however, with highly inexact search the standard-
update generally does not work very well because
of ?invalid updates?, i.e., updates that do not fix a
violation (Huang et al, 2012). In Section 4.5 we
will show that the standard perceptron introduces
many invalid updates especially with smaller beam
sizes, also observed by Huang et al (2012).
To reduce overfitting, we used averaged param-
eters after training to decode test instances in our
experiments. The resulting model is called aver-
aged perceptron (Collins, 2002).
Input: Training set D = {(x(j), y(j))}ni=1,
maximum iteration number T
Output: Model parameters w
1 Initialization: Set w = 0;
2 for t? 1...T do
3 foreach (x, y) ? D do
4 z ? beamSearch (x, y,w)
5 if z 6= y then
6 w? w + f(x, y[1:|z|])? f(x, z)
Figure 2: Perceptron training with beam-
search (Huang et al, 2012). Here y[1:i] de-
notes the prefix of y that has length i, e.g.,
y[1:3] = (y1, y2, y3).
3.2 Label sets
Here we introduce the label sets for trigger and ar-
gument in the model. We use L ? {?} to denote
the trigger label alphabet, where L represents the
33 event subtypes, and ? indicates that the token
is not a trigger. Similarly, R ? {?} denotes the
argument label sets, whereR is the set of possible
argument roles, and ? means that the argument
candidate is not an argument for the current trig-
ger. It is worth to note that the set R of each par-
ticular event subtype is subject to the entity type
constraints defined in the official ACE annotation
guideline2. For example, the Attacker argument
for an Attack event can only be one of PER, ORG
and GPE (Geo-political Entity).
3.3 Decoding
Let x = ?(x1, x2, ..., xs), E? denote the sentence
instance, where xi represents the i-th token in the
sentence and E = {ek}mk=1 is the set of argument
candidates. We use
y = (t1, a1,1, . . . , a1,m, . . . , ts, as,1, . . . , as,m)
to denote the corresponding gold standard struc-
ture, where ti represents the trigger assignment for
the token xi, and ai,k represents the argument role
label for the edge between xi and argument candi-
date ek.
2http://projects.ldc.upenn.edu/ace/docs/English-Events-
Guidelines v5.4.3.pdf
75
y = (t1, a1,1, a1,2, t2, a2,1, a2,2,| {z }
arguments for x2
t3, a3,1, a3,2)
g(1) g(2) h(2, 1) h(3, 2)
Figure 3: Example notation with s = 3,m = 2.
For simplicity, throughout this paper we use
yg(i) and yh(i,k) to represent ti and ai,k, respec-
tively. Figure 3 demonstrates the notation with
s = 3 and m = 2. The variables for the toy sen-
tence ?Jobs founded Apple? are as follows:
x = ?(Jobs,
x2? ?? ?
founded, Apple),
E? ?? ?
{JobsPER,AppleORG}?
y = (?,?,?, Start Org? ?? ?
t2
, Agent, Org? ?? ?
args for founded
,?,?,?)
Figure 4 describes the beam-search procedure
with early-update for event extraction. During
each step with token i, there are two sub-steps:
? Trigger labeling We enumerate all possible
trigger labels for the current token. The linear
model defined in Eq. (1) is used to score each
partial configuration. Then the K-best par-
tial configurations are selected to the beam,
assuming the beam size is K.
? Argument labeling After the trigger label-
ing step, we traverse all configurations in the
beam. Once a trigger label for xi is found in
the beam, the decoder searches through the
argument candidates E to label the edges be-
tween each argument candidate and the trig-
ger. After labeling each argument candidate,
we again score each partial assignment and
select the K-best results to the beam.
After the second step, the rank of different trigger
assignments can be changed because of the argu-
ment edges. Likewise, the decision on later argu-
ment candidates may be affected by earlier argu-
ment assignments.
The overall time complexity for decoding is
O(K ? s ?m).
3.4 Features
In this framework, we define two types of fea-
tures, namely local features and global features.
We first introduce the definition of local and global
features in this paper, and then describe the im-
plementation details later. Recall that in the lin-
ear model defined in Eq. (1), f(x, y) denotes the
features extracted from the input instance x along
Input: Instance x = ?(x1, x2, ..., xs), E? and
the oracle output y if for training.
K: Beam size.
L ? {?}: trigger label alphabet.
R? {?}: argument label alphabet.
Output: 1-best prediction z for x
1 Set beam B ? [] /*empty configuration*/
2 for i? 1...s do
3 buf ? {z? ? l | z? ? B, l ? L ? {?}}
B ?K-best(buf )
4 if y[1:g(i)] 6? B then
5 return B[0] /*for early-update*/
6 for ek ? E do /*search for arguments*/
7 buf ? ?
8 for z? ? B do
9 buf ? buf ? {z? ? ?}
10 if z?g(i) 6= ? then /*xi is a trigger*/
11 buf ? buf ? {z? ? r | r ? R}
12 B ?K-best(buf )
13 if y[1:h(i,k)] 6? B then
14 return B[0] /*for early-update*/
15 return B[0]
Figure 4: Decoding algorithm for event extrac-
tion. z?l means appending label l to the end of
z. During test, lines 4-5 & 13-14 are omitted.
with configuration y. In general, each feature in-
stance f in f is a function f : X ? Y ? R, which
maps x and y to a feature value. Local features are
only related to predictions on individual trigger or
argument. In the case of unigram tagging for trig-
ger labeling, each local feature takes the form of
f(x, i, yg(i)), where i denotes the index of the cur-
rent token, and yg(i) is its trigger label. In practice,
it is convenient to define the local feature function
as an indicator function, for example:
f1(x, i, yg(i)) =
{
1 if yg(i) = Attack and xi = ?fire?
0 otherwise
The global features, by contrast, involve longer
range of the output structure. Formally,
each global feature function takes the form of
f(x, i, k, y), where i and k denote the indices
of the current token and argument candidate in
decoding, respectively. The following indicator
function is a simple example of global features:
f101(x, i, k, y) =
?
??
??
1 if yg(i) = Attack and
y has only one ?Attacker?
0 otherwise
76
Category Type Feature Description
Trigger
Lexical
1. unigrams/bigrams of the current and context words within the window of size 2
2. unigrams/bigrams of part-of-speech tags of the current and context words within the
window of size 2
3. lemma and synonyms of the current token
4. base form of the current token extracted from Nomlex (Macleod et al, 1998)
5. Brown clusters that are learned from ACE English corpus (Brown et al, 1992; Miller et
al., 2004; Sun et al, 2011). We used the clusters with prefixes of length 13, 16 and 20 for
each token.
Syntactic
6. dependent and governor words of the current token
7. dependency types associated the current token
8. whether the current token is a modifier of job title
9. whether the current token is a non-referential pronoun
Entity
Information
10. unigrams/bigrams normalized by entity types
11. dependency features normalized by entity types
12. nearest entity type and string in the sentence/clause
Argument
Basic
1. context words of the entity mention
2. trigger word and subtype
3. entity type, subtype and entity role if it is a geo-political entity mention
4. entity mention head, and head of any other name mention from co-reference chain
5. lexical distance between the argument candidate and the trigger
6. the relative position between the argument candidate and the trigger: {before, after,
overlap, or separated by punctuation}
7. whether it is the nearest argument candidate with the same type
8. whether it is the only mention of the same entity type in the sentence
Syntactic
9. dependency path between the argument candidate and the trigger
10. path from the argument candidate and the trigger in constituent parse tree
11. length of the path between the argument candidate and the trigger in dependency graph
12. common root node and its depth of the argument candidate and parse tree
13. whether the argument candidate and the trigger appear in the same clause
Table 1: Local features.
3.4.1 Local features
In general there are two kinds of local features:
Trigger features The local feature func-
tion for trigger labeling can be factorized as
f(x, i, yg(i)) = p(x, i) ? q(yg(i)), where p(x, i) is
a predicate about the input, which we call text fea-
ture, and q(yg(i)) is a predicate on the trigger label.
In practice, we define two versions of q(yg(i)):
q0(yg(i)) = yg(i) (event subtype)
q1(yg(i)) = event type of yg(i)
q1(yg(i)) is a backoff version of the standard un-
igram feature. Some text features for the same
event type may share a certain distributional sim-
ilarity regardless of the subtypes. For example,
if the nearest entity mention is ?Company?, the
current token is likely to be Personnel no matter
whether it is End-Postion or Start-Position.
Argument features Similarly, the local fea-
ture function for argument labeling can be rep-
resented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ?
q(yg(i), yh(i,k)), where yh(i,k) denotes the argu-
ment assignment for the edge between trigger
word i and argument candidate ek. We define two
versions of q(yg(i), yh(i,k)):
q0(yg(i), yh(i,k)) =
?
??
??
yh(i,k) if yh(i,k) is Place,
Time or None
yg(i) ? yh(i,k) otherwise
q1(yg(i), yh(i,k)) =
{
1 if yh(i,k) 6=None
0 otherwise
It is notable that Place and Time arguments are
applicable and behave similarly to all event sub-
types. Therefore features for these arguments are
not conjuncted with trigger labels. q1(yh(i,k)) can
be considered as a backoff version of q0(yh(i,k)),
which does not discriminate different argument
roles but only focuses on argument identification.
Table 1 summarizes the text features about the in-
put for trigger and argument labeling. In our ex-
periments, we used the Stanford parser (De Marn-
effe et al, 2006) to create dependency parses.
3.4.2 Global features
Table 2 summarizes the 8 types of global features
we developed in this work. They can be roughly
divided into the following two categories:
77
Category Feature Description
Trigger
1. bigram of trigger types occur in the same sentence or the same clause
2. binary feature indicating whether synonyms in the same sentence have the same trigger label
3. context and dependency paths between two triggers conjuncted with their types
Argument
4. context and dependency features about two argument candidates which share the same role within the
same event mention
5. features about one argument candidate which plays as arguments in two event mentions in the same
sentence
6. features about two arguments of an event mention which are overlapping
7. the number of arguments with each role type of an event mention conjuncted with the event subtype
8. the pairs of time arguments within an event mention conjuncted with the event subtype
Table 2: Global features.
Transport
(transport)
Entity
(women)
Entity
(children)
Art
ifac
t Artifact
conj and
(a)
Entity
(cameramen)
Die
(died)
Attack
(fired)
Vic
tim
Target
advcl
(b)
End-Position
(resigned)
Entity Entity
[co-chief executive of [Vivendi Universal Entertainment]]
Pos
itio
n Entity
Overlapping
(c)
Figure 5: Illustration of global features (4-6) in Table 2.
Event Probability
Attack 0.34
Die 0.14
Transport 0.08
Injure 0.04
Meet 0.02
Table 3: Top 5 event subtypes that co-occur with
Attack event in the same sentence.
Trigger global feature This type of feature
captures the dependencies between two triggers
within the same sentence. For instance: feature (1)
captures the co-occurrence of trigger types. This
kind of feature is motivated by the fact that two
event mentions in the same sentence tend to be se-
mantically coherent. As an example, from Table 3
we can see that Attack event often co-occur with
Die event in the same sentence, but rarely co-occur
with Start-Position event. Feature (2) encourages
synonyms or identical tokens to have the same la-
bel. Feature (3) exploits the lexical and syntactic
relation between two triggers. A simple example
is whether an Attack trigger and a Die trigger are
linked by the dependency relation conj and.
Argument global feature This type of feature
is defined over multiple arguments for the same
or different triggers. Consider the following sen-
tence:
(3) Trains running to southern Sudan were used
to transport abducted women and children.
The Transport event mention ?transport? has
two Artifact arguments, ?women? and ?chil-
dren?. The dependency edge conj and be-
tween ?women? and ?children? indicates that
they should play the same role in the event men-
tion. The triangle structure in Figure 5(a) is an ex-
ample of feature (4) for the above example. This
feature encourages entities that are linked by de-
pendency relation conj and to play the same role
Artifact in any Transport event.
Similarly, Figure 5(b) depicts an example of
feature (5) for sentence (1) in Section 1. In this ex-
ample, an entity mention is Victim argument to Die
event and Target argument to Attack event, and the
two event triggers are connected by the typed de-
pendency advcl. Here advcl means that the word
?fired? is an adverbial clause modier of ?died?.
Figure 5(c) shows an example of feature (6) for
the following sentence:
(4) Barry Diller resigned as co-chief executive of
Vivendi Universal Entertainment.
The job title ?co-chief executive of Vivendi Uni-
versal Entertainment? overlaps with the Orga-
nization mention ?Vivendi Universal Entertain-
ment?. The feature in the triangle shape can be
considered as a soft constraint such that if a Job-
Title mention is a Position argument to an End-
Position trigger, then the Organization mention
78
which appears at the end of it should be labeled
as Entity argument for the same trigger.
Feature (7-8) are based on the statistics about
different arguments for the same trigger. For in-
stance, in many cases, a trigger can only have one
Place argument. If a partial configuration mis-
takenly classifies more than one entity mention as
Place arguments for the same trigger, then it will
be penalized.
4 Experiments
4.1 Data set and evaluation metric
We utilized the ACE 2005 corpus as our testbed.
For comparison, we used the same test set with 40
newswire articles (672 sentences) as in (Ji and Gr-
ishman, 2008; Liao and Grishman, 2010) for the
experiments, and randomly selected 30 other doc-
uments (863 sentences) from different genres as
the development set. The rest 529 documents (14,
840 sentences) are used for training.
Following previous work (Ji and Grishman,
2008; Liao and Grishman, 2010; Hong et al,
2011), we use the following criteria to determine
the correctness of an predicted event mention:
? A trigger is correct if its event subtype and
offsets match those of a reference trigger.
? An argument is correctly identified if its event
subtype and offsets match those of any of the
reference argument mentions.
? An argument is correctly identified and clas-
sified if its event subtype, offsets and argu-
ment role match those of any of the reference
argument mentions.
Finally we use Precision (P), Recall (R) and F-
measure (F1) to evaluate the overall performance.
4.2 Baseline system
Chen and Ng (2012) have proven that perform-
ing identification and classification in one step is
better than two steps. To compare our proposed
method with the previous pipelined approaches,
we implemented two Maximum Entropy (Max-
Ent) classifiers for trigger labeling and argument
labeling respectively. To make a fair comparison,
the feature sets in the baseline are identical to the
local text features we developed in our framework
(see Figure 1).
4.3 Training curves
We use the harmonic mean of the trigger?s F1
measure and argument?s F1 measure to measure
the performance on the development set.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21# of training iteration0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
0.60
Harm
onic
 me
an
local+globallocal
Figure 6: Training curves on dev set.
Figure 6 shows the training curves of the aver-
aged perceptron with respect to the performance
on the development set when the beam size is 4.
As we can see both curves converge around itera-
tion 20 and the global features improve the over-
all performance, compared to its counterpart with
only local features. Therefore we set the number
of iterations as 20 in the remaining experiments.
4.4 Impact of beam size
The beam size is an important hyper parameter in
both training and test. Larger beam size will in-
crease the computational cost while smaller beam
size may reduce the performance. Table 4 shows
the performance on the development set with sev-
eral different beam sizes. When beam size = 4, the
algorithm achieved the highest performance on the
development set with trigger F1 = 67.9, argument
F1 = 51.5, and harmonic mean = 58.6. When
the size is increased to 32, the accuracy was not
improved. Based on this observation, we chose
beam size as 4 for the remaining experiments.
4.5 Early-update vs. standard-update
Huang et al (2012) define ?invalid update? to be
an update that does not fix a violation (and instead
reinforces the error), and show that it strongly
(anti-)correlates with search quality and learning
quality. Figure 7 depicts the percentage of in-
valid updates in standard-update with and with-
out global features, respectively. With global fea-
tures, there are numerous invalid updates when the
79
Beam size 1 2 4 8 16 32
Training time (sec) 993 2,034 3,982 8,036 15,878 33,026
Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8
Table 4: Comparison of training time and accuracy on the dev set.
1 2 4 8 16 32beam size0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
% of
 inva
lid u
pdat
es
local+globallocal
Figure 7: Percentage of the so-called ?invalid up-
dates? (Huang et al, 2012) in standard perceptron.
Strategy F1 on Dev F1 on TestTrigger Arg Trigger Arg
Standard (b = 1) 68.3 47.4 64.4 49.8
Early (b = 1) 68.9 49.5 65.2 52.1
Standard (b = 4) 68.4 50.5 67.1 51.4
Early (b = 4) 67.9 51.5 67.5 52.7
Table 5: Comparison between the performance
(%) of standard-update and early-update with
global features. Here b stands for beam size.
beam size is small. The ratio decreases mono-
tonically as beam size increases. The model with
only local features made much smaller numbers
of invalid updates, which suggests that the use of
global features makes the search problem much
harder. This observation justify the application of
early-update in this work. To further investigate
the difference between early-update and standard-
update, we tested the performance of both strate-
gies, which is summarized in Table 5. As we can
see the performance of standard-update is gener-
ally worse than early-update. When the beam size
is increased (b = 4), the gap becomes smaller as
the ratio of invalid updates is reduced.
4.6 Overall performance
Table 6 shows the overall performance on the blind
test set. In addition to our baseline, we compare
against the sentence-level system reported in Hong
et al (2011), which, to the best of our knowledge,
is the best-reported system in the literature based
on gold standard argument candidates. The pro-
posed joint framework with local features achieves
comparable performance for triggers and outper-
forms the staged baseline especially on arguments.
By adding global features, the overall performance
is further improved significantly. Compared to
the staged baseline, it gains 1.6% improvement
on trigger?s F-measure and 8.8% improvement on
argument?s F-measure. Remarkably, compared to
the cross-entity approach reported in (Hong et al,
2011), which attained 68.3% F1 for triggers and
48.3% for arguments, our approach with global
features achieves even better performance on ar-
gument labeling although we only used sentence-
level information.
We also tested the performance with argument
candidates automatically extracted by a high-
performing name tagger (Li et al, 2012b) and an
IE system (Grishman et al, 2005). The results
are summarized in Table 7. The joint approach
with global features significantly outperforms the
baseline and the model with only local features.
We also show that it outperforms the sentence-
level baseline reported in (Ji and Grishman, 2008;
Liao and Grishman, 2010), both of which at-
tained 59.7% F1 for triggers and 36.6% for argu-
ments. Our approach aims to tackle the problem of
sentence-level event extraction, thereby only used
intra-sentential evidence. Nevertheless, the perfor-
mance of our approach is still comparable with the
best-reported methods based on cross-document
and cross-event inference (Ji and Grishman, 2008;
Liao and Grishman, 2010).
5 Related Work
Most recent studies about ACE event extraction
rely on staged pipeline which consists of separate
local classifiers for trigger labeling and argument
labeling (Grishman et al, 2005; Ahn, 2006; Ji and
Grishman, 2008; Chen and Ji, 2009; Liao and Gr-
ishman, 2010; Hong et al, 2011; Li et al, 2012a;
Chen and Ng, 2012). To the best of our knowl-
edge, our work is the first attempt to jointly model
these two ACE event subtasks.
80
Methods
Trigger
Identification (%)
Trigger Identification
+ classification (%)
Argument
Identification (%) Argument Role (%)P R F1 P R F1 P R F1 P R F1
Sentence-level in Hong et al (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5
Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5
Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Cross-entity in Hong et al (2011)? N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Table 6: Overall performance with gold-standard entities, timex, and values. ?beyond sentence level.
Methods Trigger F1 Arg F1
Ji and Grishman (2008)
cross-doc Inference
67.3 42.6
Ji and Grishman (2008)
sentence-level
59.7 36.6
MaxEnt classifiers 64.7 (?1.2) 33.7 (?10.2)
Joint w/ local 63.7 (?2.0) 35.8 (?10.7)
Joint w/ local + global 65.6 (?1.9) 41.8 (?10.9)
Table 7: Overall performance (%) with predicted
entities, timex, and values. ? indicates the perfor-
mance drop from experiments with gold-standard
argument candidates (see Table 6).
For the Message Understanding Conference
(MUC) and FAS Program for Monitoring Emerg-
ing Diseases (ProMED) event extraction tasks,
Patwardhan and Riloff (2009) proposed a proba-
bilistic framework to extract event role fillers con-
ditioned on the sentential event occurrence. Be-
sides having different task definitions, the key
difference from our approach is that their role
filler recognizer and sentential event recognizer
are trained independently but combined in the test
stage. Our experiments, however, have demon-
strated that it is more advantageous to do both
training and testing with joint inference.
There has been some previous work on joint
modeling for biomedical events (Riedel and Mc-
Callum, 2011a; Riedel et al, 2009; McClosky et
al., 2011; Riedel and McCallum, 2011b). (Mc-
Closky et al, 2011) is most closely related to our
approach. They casted the problem of biomedi-
cal event extraction as a dependency parsing prob-
lem. The key assumption that event structure can
be considered as trees is incompatible with ACE
event extraction. In addition, they used a separate
classifier to predict the event triggers before ap-
plying the parser, while we extract the triggers and
argument jointly. Finally, the features in the parser
are edge-factorized. To exploit global features,
they applied a MaxEnt-based global re-ranker. In
comparison, our approach is a unified framework
based on beam search, which allows us to exploit
arbitrary global features efficiently.
6 Conclusions and Future Work
We presented a joint framework for ACE event ex-
traction based on structured perceptron with inex-
act search. As opposed to traditional pipelined
approaches, we re-defined the task as a struc-
tured prediction problem. The experiments proved
that the perceptron with local features outperforms
the staged baseline and the global features further
improve the performance significantly, surpassing
the current state-of-the-art by a large margin.
As shown in Table 7, the overall performance
drops substantially when using predicted argu-
ment candidates. To improve the accuracy of end-
to-end IE system, we plan to develop a complete
joint framework to recognize entities together with
event mentions for future work. Also we are inter-
ested in applying this framework to other IE tasks
such as relation extraction.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149,
U.S. NSF EAGER Award under Grant No. IIS-
1144111, U.S. DARPA Award No. FA8750-13-2-
0041 in the ?Deep Exploration and Filtering of
Text? (DEFT) Program, a CUNY Junior Faculty
Award, and Queens College equipment funds. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
81
References
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1?8.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Zheng Chen and Heng Ji. 2009. Language specific
issue and feature exploration in chinese event ex-
traction. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 209?212.
Chen Chen and Vincent Ng. 2012. Joint modeling for
chinese event extraction with rich linguistic features.
In COLING, pages 529?544.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In Proceedings of ACE 2005 Evaluation Workshop.
Washington.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL, pages 1127?1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012a. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1006?1016.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012b. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 1727?1731.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789?797.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX, volume 98, pages 187?193.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626?1635.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL,
volume 4, pages 337?342.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 151?
160.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1?
12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings
of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, pages
41?49.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521?529.
82
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628?633,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Efficient Implementation of Beam-Search Incremental Parsers?
Yoav Goldberg
Dept. of Computer Science
Bar-Ilan University
Ramat Gan, Tel Aviv, 5290002 Israel
yoav.goldberg@gmail.com
Kai Zhao Liang Huang
Graduate Center and Queens College
City University of New York
{kzhao@gc, lhuang@cs.qc}.cuny.edu
{kzhao.hf, liang.huang.sh}.gmail.com
Abstract
Beam search incremental parsers are ac-
curate, but not as fast as they could be.
We demonstrate that, contrary to popu-
lar belief, most current implementations
of beam parsers in fact run in O(n2),
rather than linear time, because each state-
transition is actually implemented as an
O(n) operation. We present an improved
implementation, based on Tree Structured
Stack (TSS), in which a transition is per-
formed in O(1), resulting in a real linear-
time algorithm, which is verified empiri-
cally. We further improve parsing speed
by sharing feature-extraction and dot-
product across beam items. Practically,
our methods combined offer a speedup of
?2x over strong baselines on Penn Tree-
bank sentences, and are orders of magni-
tude faster on much longer sentences.
1 Introduction
Beam search incremental parsers (Roark, 2001;
Collins and Roark, 2004; Zhang and Clark, 2008;
Huang et al, 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Zhang and Clark, 2011)
provide very competitive parsing accuracies for
various grammar formalisms (CFG, CCG, and de-
pendency grammars). In terms of purning strate-
gies, they can be broadly divided into two cat-
egories: the first group (Roark, 2001; Collins
and Roark, 2004) uses soft (aka probabilistic)
beams borrowed from bottom-up parsers (Char-
niak, 2000; Collins, 1999) which has no control
of complexity, while the second group (the rest
and many more recent ones) employs hard beams
borrowed from machine translation (Koehn, 2004)
which guarantee (as they claim) a linear runtime
O(kn) where k is the beam width. However, we
will demonstrate below that, contrary to popular
?Supported in part by DARPA FA8750-13-2-0041 (DEFT).
belief, in most standard implementations their ac-
tual runtime is in fact O(kn2) rather than linear.
Although this argument in general also applies to
dynamic programming (DP) parsers,1 in this pa-
per we only focus on the standard, non-dynamic
programming approach since it is arguably still the
dominant practice (e.g. it is easier with the popular
arc-eager parser with a rich feature set (Kuhlmann
et al, 2011; Zhang and Nivre, 2011)) and it bene-
fits more from our improved algorithms.
The dependence on the beam-size k is because
one needs to do k-times the number of basic opera-
tions (feature-extractions, dot-products, and state-
transitions) relative to a greedy parser (Nivre and
Scholz, 2004; Goldberg and Elhadad, 2010). Note
that in a beam setting, the same state can expand
to several new states in the next step, which is usu-
ally achieved by copying the state prior to making
a transition, whereas greedy search only stores one
state which is modified in-place.
Copying amounts to a large fraction of the
slowdown of beam-based with respect to greedy
parsers. Copying is expensive, because the state
keeps track of (a) a stack and (b) the set of
dependency-arcs added so far. Both the arc-set and
the stack can grow to O(n) size in the worst-case,
making the state-copy (and hence state-transition)
an O(n) operation. Thus, beam search imple-
mentations that copy the entire state are in fact
quadratic O(kn2) and not linear, with a slowdown
factor of O(kn) with respect to greedy parsers,
which is confirmed empirically in Figure 4.
We present a way of decreasing the O(n) tran-
sition cost to O(1) achieving strictly linear-time
parsing, using a data structure of Tree-Structured
Stack (TSS) that is inspired by but simpler than
the graph-structured stack (GSS) of Tomita (1985)
used in dynamic programming (Huang and Sagae,
2010).2 On average Treebank sentences, the TSS
1The Huang-Sagae DP parser (http://acl.cs.qc.edu)
does run in O(kn), which inspired this paper when we ex-
perimented with simulating non-DP beam search using GSS.
2Our notion of TSS is crucially different from the data
628
input: w0 . . . wn?1
axiom 0 : ?0, ?: ?
SHIFT
` : ?j, S? : A
`+ 1 : ?j + 1, S|wj? : A
j < n
REDUCEL
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s0? : A ? {s1xs0}
REDUCER
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s1? : A ? {s1ys0}
goal 2n? 1 : ?n, s0?: A
Figure 1: An abstraction of the arc-standard de-
ductive system Nivre (2008). The stack S is a list
of heads, j is the index of the token at the front of
the buffer, and ` is the step number (beam index).
A is the arc-set of dependency arcs accumulated
so far, which we will get rid of in Section 4.1.
version, being linear time, leads to a speedup of
2x?2.7x over the naive implementation, and about
1.3x?1.7x over the optimized baseline presented
in Section 5.
Having achieved efficient state-transitions, we
turn to feature extraction and dot products (Sec-
tion 6). We present a simple scheme of sharing
repeated scoring operations across different beam
items, resulting in an additional 7 to 25% speed in-
crease. On Treebank sentences, the methods com-
bined lead to a speedup of ?2x over strong base-
lines (?10x over naive ones), and on longer sen-
tences they are orders of magnitude faster.
2 Beam Search Incremental Parsing
We assume familiarity with transition-based de-
pendency parsing. The unfamiliar reader is re-
ferred to Nivre (2008). We briefly describe a
standard shift-reduce dependency parser (which is
called ?arc-standard? by Nivre) to establish nota-
tion. Parser states (sometimes called configura-
tions) are composed of a stack, a buffer, and an
arc-set. Parsing transitions are applied to states,
and result in new states. The arc-standard system
has three kinds of transitions: SHIFT, REDUCEL,
structure with the same name in an earlier work of Tomita
(1985). In fact, Tomita?s TSS merges the top portion of the
stacks (more like GSS) while ours merges the bottom por-
tion. We thank Yue Zhang for informing us that TSS was
already implemented for the CCG parser in zpar (http://
sourceforge.net/projects/zpar/) though it was not men-
tioned in his paper (Zhang and Clark, 2011).
and REDUCER, which are summarized in the de-
ductive system in Figure 1. The SHIFT transition
removes the first word from the buffer and pushes
it to the stack, and the REDUCEL and REDUCER
actions each add a dependency relation between
the two words on the top of the stack (which is
achieved by adding the arc s1xs0 or s1ys0 to the
arc-set A), and pops the new dependent from the
stack. When reaching the goal state the parser re-
turns a tree composed of the arcs in the arc-set.
At parsing time, transitions are chosen based on
a trained scoring model which looks at features
of the state. In a beam parser, k items (hypothe-
ses) are maintained. Items are composed of a state
and a score. At step i, each of the k items is ex-
tended by applying all possible transitions to the
given state, resulting in k ? a items, a being the
number of possible transitions. Of these, the top
scoring k items are kept and used in step i+1. Fi-
nally, the tree associated with the highest-scoring
item is returned.
3 The Common Implementation of State
The stack is usually represented as a list or an array
of token indices, and the arc-set as an array heads
of length n mapping the word at position m to the
index of its parent. In order to allow for fast fea-
ture extraction, additional arrays are used to map
each token to its left-most and right-most modi-
fier, which are used in most incremental parsers,
e.g. (Huang and Sagae, 2010; Zhang and Nivre,
2011). The buffer is usually implemented as a
pointer to a shared sentence object, and an index j
to the current front of the buffer. Finally, it is com-
mon to keep an additional array holding the tran-
sition sequence leading to the current state, which
can be represented compactly as a pointer to the
previous state and the current action. The state
structure is summarized below:
class state
stack[n] of token_ids
array[n] heads
array[n] leftmost_modifiers
array[n] rightmost_modifiers
int j
int last_action
state previous
In a greedy parser, state transition is performed in-
place. However, in a beam parser the states cannot
be modified in place, and a state transition oper-
ation needs to result in a new, independent state
object. The common practice is to copy the cur-
rent state, and then update the needed fields in the
copy. Copying a stack and arrays of size n is an
629
O(n) operation. In what follows, we present a way
to perform transitions in O(1).
4 Efficient State Transitions
4.1 Distributed Representation of Trees
The state needs to keep track of the set of arcs
added to the tree so far for two reasons:
(a) In order to return the complete tree at the end.
(b) In order to compute features when parsing.
Observe that we do not in fact need to store any
arc in order to achieve (a) ? we could reconstruct
the entire set by backtracking once we reach the
final configuration. Hence, the arc-set in Figure 1
is only needed for computing features. Instead of
storing the entire arc-set, we could keep only the
information needed for feature computation. In
the feature set we use (Huang and Sagae, 2010),
we need access to (1) items on the buffer, (2)
the 3 top-most elements of the stack, and (3) the
current left-most and right-most modifiers of the
two topmost stack elements. The left-most and
right-most modifiers are already kept in the state
representation, but store more information than
needed: we only need to keep track of the mod-
ifiers of current stack items. Once a token is re-
moved from the stack it will never return, and we
will not need access to its modifiers again. We
can therefore remove the left/rightmost modifier
arrays, and instead have the stack store triplets
(token, leftmost_mod, rightmost_mod). The
heads array is no longer needed. Our new state
representation becomes:
class state
stack[n] of (tok, left, right)
int j
int last_action
state previous
4.2 Tree Structured Stack: TSS
We now turn to handle the stack. Notice that the
buffer, which is also of size O(n), is represented
as a pointer to an immutable shared object, and is
therefore very efficient to copy. We would like to
treat the stack in a similar fashion.
An immutable stack can be implemented func-
tionally as a cons list, where the head is the top
of the stack and the tail is the rest of the stack.
Pushing an item to the stack amounts to adding a
new head link to the list and returning it. Popping
an item from the stack amounts to returning the
tail of the list. Notice that, crucially, a pop opera-
tion does not change the underlying list at all, and
a push operation only adds to the front of a list.
Thus, the stack operations are non-destructive, in
the sense that once you hold a reference to a stack,
the view of the stack through this reference does
not change regardless of future operations that are
applied to the stack. Moreover, push and pop op-
erations are very efficient. This stack implementa-
tion is an example of a persistent data structure ? a
data structure inspired by functional programming
which keeps the old versions of itself intact when
modified (Okasaki, 1999).
While each client sees the stack as a list, the un-
derlying representation is a tree, and clients hold
pointers to nodes in the tree. A push operation
adds a branch to the tree and returns the new
pointer, while a pop operation returns the pointer
of the parent, see Figure 3 for an example. We call
this representation a tree-structured stack (TSS).
Using this stack representation, we can replace
the O(n) stack by an integer holding the item at
the top of the stack (s0), and a pointer to the tail of
the stack (tail). As discussed above, in addition
to the top of the stack we also keep its leftmost and
rightmost modifiers s0L and s0R. The simplified
state representation becomes:
class state
int s0, s0L, s0R
state tail
int j
int last_action
state previous
State is now reduced to seven integers, and the
transitions can be implemented very efficiently as
we show in Figure 2. The parser state is trans-
formed into a compact object, and state transitions
are O(1) operations involving only a few pointer
lookups and integer assignments.
4.3 TSS vs. GSS; Space Complexity
TSS is inspired by the graph-structured stack
(GSS) used in the dynamic-programming parser of
Huang and Sagae (2010), but without reentrancy
(see also Footnote 2). More importantly, the state
signature in TSS is much slimmer than that in
GSS. Using the notation of Huang and Sagae, in-
stead of maintaining the full DP signature of
f?DP(j, S) = (j, fd(sd), . . . , f0(s0))
where sd denotes the dth tree on stack, in non-DP
TSS we only need to store the features f0(s0) for
the final tree on the stack,
f?noDP(j, S) = (j, f0(s0)),
630
def Shift(state)
newstate.s0 = state.j
newstate.s0L = None
newstate.s0R = None
newstate.tail = state
newstate.j = state.j + 1
return newstate
def ReduceL(state)
newstate.s0 = state.s0
newstate.s0L = state.tail.s0
newstate.s0R = state.s0R
newstate.tail = state.tail.tail
newstate.j = j
return newstate
def ReduceR(state)
newstate.s0 = state.tail.s0
newstate.s0L = state.tail.s0L
newstate.s0R = state.s0
newstate.tail = state.tail.tail
newstate.j = j
return newstate
Figure 2: State transitions implementation in the TSS representation (see Fig. 3 for the tail pointers).
The two lines on s0L and s0R are specific to feature set design, and can be expanded for richer feature
sets. To conserve space, we do not show the obvious assignments to last_action and previous.
b
1 2
 c
3
a
a d
4
b
c
0
b
c
c
L
R
L
R
sh sh sh sh
sh
sh
Figure 3: Example of tree-structured stack. The
forward arrows denote state transitions, and the
dotted backward arrows are the tail pointers to
the stack tail. The boxes denote the top-of-stack at
each state. Notice that for b = shift(a) we perform
a single push operation getting b.tail = a, while
for b = reduce(a) transition we perform two pops
and a push, resulting in b.tail = a.tail.tail.
thanks to the uniqueness of tail pointers (?left-
pointers? in Huang and Sagae).
In terms of space complexity, each state is re-
duced from O(n) in size to O(d) with GSS and
to O(1) with TSS,3 making it possible to store the
entire beam in O(kn) space. Moreover, the con-
stant state-size makes memory management easier
and reduces fragmentation, by making it possible
to pre-allocate the entire beam upfront. We did
not explore its empirical implications in this work,
as our implementation language, Python, does not
support low-level memory management.
4.4 Generality of the Approach
We presented a concrete implementation for the
arc-standard system with a relatively simple (yet
state-of-the-art) feature set. As in Kuhlmann et
al. (2011), our approach is also applicable to
other transitions systems and richer feature-sets
with some additional book-keeping. A well-
3For example, a GSS state in Huang and Sagae?s experi-
ments also stores s1, s1L, s1R, s2 besides the f0(s0) fea-
tures (s0, s0L, s0R) needed by TSS. d is treated as a con-
stant by Huang and Sagae but actually it could be a variable.
documented Python implementation for the la-
beled arc-eager system with the rich feature set
of Zhang and Nivre (2011) is available on the first
author?s homepage.
5 Fewer Transitions: Lazy Expansion
Another way of decreasing state-transition costs
is making less transitions to begin with: instead
of performing all possible transitions from each
beam item and then keeping only k of the re-
sulting states, we could perform only transitions
that are sure to end up on the next step in the
beam. This is done by first computing transition
scores from each beam item, then keeping the top
k highest scoring (state, action) pairs, perform-
ing only those k transitions. This technique is
especially important when the number of possi-
ble transitions is large, such as in labeled parsing.
The technique, though never mentioned in the lit-
erature, was employed in some implementations
(e.g., Yue Zhang?s zpar). We mention it here for
completeness since it?s not well-known yet.
6 (Partial) Feature Sharing
After making the state-transition efficient, we turn
to deal with the other major expensive operation:
feature-extractions and dot-products. While we
can?t speed up the process, we observe that some
computations are repeated in different parts of the
beam, and propose to share these computations.
Notice that relatively few token indices from a
state can determine the values of many features.
For example, knowing the buffer index j deter-
mines the words and tags of items after location
j on the buffer, as well as features composed of
combinations of these values.
Based on this observation we propose the no-
tion of a state signature, which is a set of token
indices. An example of a state signature would
be sig(state) = (s0, s0L, s1, s1L), indicating the
indices of the two tokens at the top of the stack to-
gether with their leftmost modifiers. Given a sig-
631
Figure 4: Non-linearity of the standard beam
search compared to the linearity of our TSS beam
search for labeled arc-eager and unlabeled arc-
standard parsers on long sentences (running times
vs. sentence length). All parsers use beam size 8.
nature, we decompose the feature function ?(x)
into two parts ?(x) = ?s(sig(x)) + ?o(x), where
?s(sig(x)) extracts all features that depend exclu-
sively on signature items, and ?o(x) extracts all
other features.4 The scoring function w ? ?(x) de-
composes into w ? ?s(sig(x)) + w ? ?o(x). Dur-
ing beam decoding, we maintain a cache map-
ping seen signatures sig(state) to (partial) tran-
sition scores w ? ?s(sig(state)). We now need
to calculate w ? ?o(x) for each beam item, but
w ? ?s(sig(x)) only for one of the items sharing
the signature. Defining the signature involves a
natural balance between signatures that repeat of-
ten and signatures that cover many features. In the
experiments in this paper, we chose the signature
function for the arc-standard parser to contain all
core elements participating in feature extraction5,
and for the arc-eager parser a signature containing
only a partial subset.6
7 Experiments
We implemented beam-based parsers using the
traditional approach as well as with our proposed
extension and compared their runtime.
The first experiment highlights the non-linear
behavior of the standard implementation, com-
pared to the linear behavior of the TSS method.
4One could extend the approach further to use several sig-
natures and further decompose the feature function. We did
not pursue this idea in this work.
5s0,s0L,s0R,s1,s1L,s1R,s2,j.
6s0, s0L, s0R,s0h,b0L,j, where s0h is the parent of
s0, and b0L is the leftmost modifier of j.
system plain plain plain plain +TSS+lazy
+TSS +lazy +TSS +feat-share
(sec 3) (sec 4) (sec 5) +lazy (sec 6)
ArcS-U 20.8 38.6 24.3 41.1 47.4
ArcE-U 25.4 48.3 38.2 58.2 72.3
ArcE-L 1.8 4.9 11.1 14.5 17.3
Table 1: Parsing speeds for the different tech-
niques measured in sentences/sec (beam size 8).
All parsers are implemented in Python, with dot-
products in C. ArcS/ArcE denotes arc-standard
vs. arc-eager, L/U labeled (stanford deps, 49 la-
bels) vs. unlabeled parsing. ArcS use feature set
of Huang and Sagae (2010) (50 templates), and ArcE
that of Zhang and Nivre (2011) (72 templates).
As parsing time is dominated by score computa-
tion, the effect is too small to be measured on
natural language sentences, but it is noticeable
for longer sentences. Figure 4 plots the runtime
for synthetic examples with lengths ranging from
50 to 1000 tokens, which are generated by con-
catenating sentences from Sections 22?24 of Penn
Treebank (PTB), and demonstrates the non-linear
behavior (dataset included). We argue parsing
longer sentences is by itself an interesting and
potentially important problem (e.g. for other lan-
guages such as Arabic and Chinese where word
or sentence boundaries are vague, and for pars-
ing beyond sentence-level, e.g. discourse parsing
or parsing with inter-sentence dependencies).
Our next set of experiments compares the actual
speedup observed on English sentences. Table 1
shows the speed of the parsers (sentences/sec-
ond) with the various proposed optimization tech-
niques. We first train our parsers on Sections 02?
21 of PTB, using Section 22 as the test set. The
accuracies of all our parsers are at the state-of-
the-art level. The final speedups are up to 10x
against naive baselines and ?2x against the lazy-
transitions baselines.
8 Conclusions
We demonstrated in both theory and experiments
that the standard implementation of beam search
parsers run in O(n2) time, and have presented im-
proved algorithms which run in O(n) time. Com-
bined with other techniques, our method offers
significant speedups (?2x) over strong baselines,
or 10x over naive ones, and is orders of magnitude
faster on much longer sentences. We have demon-
strated that our approach is general and we believe
it will benefit many other incremental parsers.
632
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of HLT-NAACL,
pages 742?750.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA, pages 115?
124.
Marco Kuhlmann, Carlos Gmez-Rodrguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of ACL.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Chris Okasaki. 1999. Purely functional data struc-
tures. Cambridge University Press.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Masaru Tomita. 1985. An efficient context-free pars-
ing algorithm for natural languages. In Proceedings
of the 9th international joint conference on Artificial
intelligence - Volume 2, pages 756?764.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
Yue Zhang and Stephen Clark. 2011. Shift-reduce ccg
parsing. In Proceedings of ACL.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
633
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 785?790,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical MT Training using Max-Violation Perceptron
Kai Zhao
?
Liang Huang
?
?
Graduate Center & Queens College
City University of New York
{kzhao@gc,huang@cs.qc}.cuny.edu
Haitao Mi
?
Abe Ittycheriah
?
?
T. J. Watson Research Center
IBM
{hmi,abei}@us.ibm.com
Abstract
Large-scale discriminative training has be-
come promising for statistical machine
translation by leveraging the huge train-
ing corpus; for example the recent effort
in phrase-based MT (Yu et al, 2013) sig-
nificantly outperforms mainstream meth-
ods that only train on small tuning sets.
However, phrase-based MT suffers from
limited reorderings, and thus its training
can only utilize a small portion of the bi-
text due to the distortion limit. To address
this problem, we extend Yu et al (2013)
to syntax-based MT by generalizing their
latent variable ?violation-fixing? percep-
tron from graphs to hypergraphs. Exper-
iments confirm that our method leads to
up to +1.2 BLEU improvement over main-
stream methods such as MERT and PRO.
1 Introduction
Many natural language processing problems in-
cluding part-of-speech tagging (Collins, 2002),
parsing (McDonald et al, 2005), and event extrac-
tion (Li et al, 2013) have enjoyed great success us-
ing large-scale discriminative training algorithms.
However, a similar success on machine translation
has been elusive, where the mainstream methods
still tune on small datasets.
What makes large-scale MT training so hard
then? After numerous attempts by various re-
searchers (Liang et al, 2006; Watanabe et al,
2007; Arun and Koehn, 2007; Blunsom et al,
2008; Chiang et al, 2008; Flanigan et al, 2013;
Green et al, 2013), the recent work of Yu et al
(2013) finally reveals a major reason: it is the vast
amount of (inevitable) search errors in MT decod-
ing that astray learning. To alleviate this prob-
lem, their work adopts the theoretically-motivated
framework of violation-fixing perceptron (Huang
et al, 2012) tailed for inexact search, yielding
great results on phrase-based MT (outperforming
Collins (02)
inexact
??
search
Huang et al (12)
latent
??
variable
Yu et al (13)
? hypergraph ?
Zhang et al (13) ??
variable
this work
Figure 1: Relationship with previous work.
small-scale MERT/PRO by a large margin for the
first time). However, the underlying phrase-based
model suffers from limited distortion and thus can
only employ a small portion (about 1/3 in their Ch-
En experiments) of the bitext in training.
To better utilize the large training set, we
propose to generalize from phrase-based MT to
syntax-based MT, in particular the hierarchical
phrase-based translation model (HIERO) (Chiang,
2005), in order to exploit sentence pairs beyond
the expressive capacity of phrase-based MT.
The key challenge here is to extend the latent
variable violation-fixing perceptron of Yu et al
(2013) to handle tree-structured derivations and
translation hypergraphs. Luckily, Zhang et al
(2013) have recently generalized the underlying
violation-fixing perceptron of Huang et al (2012)
from graphs to hypergraphs for bottom-up parsing,
which resembles syntax-based decoding. We just
need to further extend it to handle latent variables.
We make the following contributions:
1. We generalize the latent variable violation-
fixing perceptron framework to inexact
search over hypergraphs, which subsumes
previous algorithms for PBMT and bottom-
up parsing as special cases (see Fig. 1).
2. We show that syntax-based MT, with its bet-
ter handling of long-distance reordering, can
exploit a larger portion of the training set,
which facilitates sparse lexicalized features.
3. Experiments show that our training algo-
rithm outperforms mainstream tuning meth-
ods (which optimize on small devsets) by
+1.2 BLEU over MERT and PRO on FBIS.
785
id rule
r
0
S? ?X
1
,X
1
?
r
1
S? ?S
1
X
2
,S
1
X
2
?
r
2
X? ?B`ush??,Bush?
r
3
X? ?Sh?al?ong,Sharon?
r
4
X? ?hu`?t?an, talks?
r
5
X? ?y?u X
1
j?ux??ng X
2
,
held X
2
with X
1
?
r
6
X? ?y?u Sh?al?ong, with Sharon?
r
7
X? ?X
1
j?ux??ng X
2
,
X
1
held X
2
?
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[2:3]
Sh?al?ong
3
|
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
Sharon
5
with
4
X
talks
3
held
2
S
X
0
Bush
1
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[1:3]
Sh?al?ong
3
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
talks
5
held
4
X
Sharon
3
with
2
S
X
0
Bush
1
(a) HIERO rules (b) gold derivation (c) Viterbi derivation
Figure 2: An example of HIERO translation.
X[0:1] X[2:3] X[4:5]
X[1:5]
X[1:3]
S[0:1]
S[0:5]
Figure 3: A ?LM hypergraph with two deriva-
tions: the gold derivation (Fig. 2b) in solid lines,
and the Viterbi derivation (Fig. 2c) in dashed lines.
2 Review: Syntax-based MT Decoding
For clarity reasons we will describe HIERO decod-
ing as a two-pass process, first without a language
model, and then integrating the LM. This section
mostly follows Huang and Chiang (2007).
In the first, ?LM phase, the decoder parses the
source sentence using the source projection of the
synchronous grammar (see Fig. 2 (a) for an ex-
ample), producing a?LM hypergraph where each
node has a signature N
[i:j]
, where N is the nonter-
minal type (either X or S in HIERO) and [i : j] is
the span, and each hyperedge e is an application
of the translation rule r(e) (see Figure 3).
To incorporate the language model, each node
also needs to remember its target side boundary
words. Thus a ?LM node N
[i:j]
is split into mul-
tiple +LM nodes of signature N
a?b
[i:j]
, where a and
b are the boundary words. For example, with a bi-
gram LM, X
held?Sharon
[1:5]
is a node whose translation
starts with ?held? and ends with ?Sharon?.
More formally, the whole decoding process can
be cast as a deductive system. Take the partial
translation of ?held talks with Sharon? in Figure 2
(b) for example, the deduction is
X
Sharon?Sharon
[2:3]
: s
1
X
talks?talks
[4:5]
: s
2
X
held?Sharon
[1:5]
: s
1
+ s
2
+ s(r
5
) + ?
r
5
,
where s(r
5
) is the score of rule r
5
, and the LM
combo score ? is log P
lm
(talks | held)P
lm
(with |
talks)P
lm
(Sharon | with).
3 Violation-Fixing Perceptron for HIERO
As mentioned in Section 1, the key to the success
of Yu et al (2013) is the adoption of violation-
fixing perceptron of Huang et al (2012) which
is tailored for vastly inexact search. The general
idea is to update somewhere in the middle of the
search (where search error happens) rather than at
the very end (standard update is often invalid). To
adapt it to MT where many derivations can output
the same translation (i.e., spurious ambiguity), Yu
et al (2013) extends it to handle latent variables
which correspond to phrase-based derivations. On
the other hand, Zhang et al (2013) has generalized
Huang et al (2012) from graphs to hypergraphs
for bottom-up parsing, which resembles HIERO
decoding. So we just need to combine the two
generalizing directions (latent variable and hyper-
graph, see Fig. 1).
3.1 Latent Variable Hypergraph Search
The key difference between bottom-up parsing
and MT decoding is that in parsing the gold tree
for each input sentence is unique, while in MT
many derivations can generate the same reference
translation. In other words, the gold derivation to
update towards is a latent variable.
786
Here we formally define the latent variable
?max-violation? perceptron over a hypergraph for
MT training. For a given sentence pair ?x, y?, we
denote H(x) as the decoding hypergraph of HI-
ERO without any pruning. We say D ? H(x) if
D is a full derivation of decoding x, and D can be
derived from the hypergraph. Let good(x, y) be
the set of y-good derivations for ?x, y?:
good(x, y)
?
= {D ? H(x) | e(D) = y},
where e(D) is the translation from derivation D.
We then define the set of y-good partial derivations
that cover x
[i:j]
with root N
[i:j]
as
good
N
[i:j]
(x, y)
?
= {d ? D | D ? good(x, y),
root(d) = N
[i:j]
}
We further denote the real decoding hypergraph
with beam-pruning and cube-pruning as H
?
(x).
The set of y-bad derivations is defined as
bad
N
[i:j]
(x, y)
?
= {d ? D | D ? H
?
(x, y),
root(d) = N
[i:j]
, d 6? good
N
[i:j]
(x, y)}.
Note that the y-good derivations are defined over
the unpruned whole decoding hypergraph, while
the y-bad derivations are defined over the real de-
coding hypergraph with pruning.
The max-violation method performs the update
where the model score difference between the
incorrect Viterbi partial derivation and the best
y-good partial derivation is maximal, by penaliz-
ing the incorrect Viterbi partial derivation and re-
warding the y-good partial derivation.
More formally, we first find the Viterbi partial
derivation d
?
and the best y-good partial deriva-
tion d
+
for each N
[i:j]
group in the pruned +LM
hypergraph:
d
+
N
[i:j]
(x, y)
?
= argmax
d?good
N
[i:j]
(x,y)
w ??(x, d),
d
?
N
[i:j]
(x, y)
?
= argmax
d?bad
N
[i:j]
(x,y)
w ??(x, d),
where ?(x, d) is the feature vector for derivation
d. Then it finds the group N
?
[i
?
:j
?
]
with the max-
imal score difference between the Viterbi deriva-
tion and the best y-good derivation:
N
?
[i
?
:j
?
]
?
= argmax
N
[i:j]
w ???(x, d
+
N
[i:j]
(x, y), d
?
N
[i:j]
(x, y)),
and update as follows:
w? w + ??(x, d
+
N
?
[i
?
:j
?
]
(x, y), d
?
N
?
[i
?
:j
?
]
(x, y)),
where ??(x, d, d
?
)
?
= ?(x, d)??(x, d
?
).
3.2 Forced Decoding for HIERO
We now describe how to find the gold derivations.
1
Such derivations can be generated in way similar
to Yu et al (2013) by using a language model tai-
lored for forced decoding:
P
forced
(q | p) =
{
1 if q = p+ 1
0 otherwise
,
where p and q are the indices of the boundary
words in the reference translation. The +LM node
now has signature N
p?q
[i:j]
, where p and q are the in-
dexes of the boundary words. If a boundary word
does not occur in the reference, its index is set to
? so that its language model score will always be
??; if a boundary word occurs more than once in
the reference, its ?LM node is split into multiple
+LM nodes, one for each such index.
2
We have a similar deductive system for forced
decoding. For the previous example, rule r
5
in
Figure 2 (a) is rewritten as
X? ?y?u X
1
j?ux??ng X
2
, 1 X
2
4 X
1
?,
where 1 and 4 are the indexes for reference words
?held? and ?with? respectively. The deduction for
X
[1:5]
in Figure 2 (b) is
X
5?5
[2:3]
: s
1
X
2?3
[4:5]
: s
2
X
1?5
[1:5]
: s(r
5
) + ?+ s
1
+ s
2
r
5
,
where ? = log
?
i?{1,3,4}
P
forced
(i+ 1 | i) = 0.
4 Experiments
Following Yu et al (2013), we call our max-
violation method MAXFORCE. Our implemen-
tation is mostly in Python on top of the cdec
system (Dyer et al, 2010) via the pycdec in-
terface (Chahuneau et al, 2012). In addition, we
use minibatch parallelization of (Zhao and Huang,
1
We only consider single reference in this paper.
2
Our formulation of index-based language model fixes a
bug in the word-based LM of Yu et al (2013) when a sub-
string appears more than once in the reference (e.g. ?the
man...the man...?); thanks to Dan Gildea for pointing it out.
787
2013) to speedup perceptron training. We evalu-
ate MAXFORCE for HIERO over two CH-EN cor-
pora, IWSLT09 and FBIS, and compare the per-
formance with vanilla n-best MERT (Och, 2003)
from Moses (Koehn et al, 2007), Hypergraph
MERT (Kumar et al, 2009), and PRO (Hopkins
and May, 2011) from cdec.
4.1 Features Design
We use all the 18 dense features from cdec, in-
cluding language model, direct translation prob-
ability p(e|f), lexical translation probabilities
p
l
(e|f) and p
l
(f |e), length penalty, counts for the
source and target sides in the training corpus, and
flags for the glue rules and pass-through rules.
For sparse features we use Word-Edges fea-
tures (Charniak and Johnson, 2005; Huang, 2008)
which are shown to be extremely effective in
both parsing and phrase-based MT (Yu et al,
2013). We find that even simple Word-Edges
features boost the performance significantly, and
adding complex Word-Edges features from Yu et
al. (2013) brings limited improvement and slows
down the decoding. So in the following experi-
ments we only use Word-Edges features consisting
of combinations of English and Chinese words,
and Chinese characters, and do not use word clus-
ters nor word types. For simplicity and efficiency
reasons, we also exclude all non-local features.
4.2 Datasets and Preprocessing
Our first corpus, IWSLT09, contains ?30k
short sentences collected from spoken language.
IWSLT04 is used as development set in MAX-
FORCE training, and as tuning set for n-best
MERT, Hypergraph MERT, and PRO. IWSLT05
is used as test set. Both IWSLT04 and IWSLT05
contain 16 references.We mainly use this corpus
to investigate the properties of MAXFORCE.
The second corpus, FBIS, contains ?240k sen-
tences. NIST06 newswire is used as development
set for MAXFORCE training, and as tuning set
for all other tuning methods. NIST08 newswire
is used as test set. Both NIST06 newswire
and NIST08 newswire contain 4 references. We
mainly use this corpus to demonstrate the perfor-
mance of MAXFORCE in large-scale training.
For both corpora, we do standard tokeniza-
tion, alignment and rule extraction using the cdec
tools. In rule extraction, we remove all 1-count
rules but keep the rules mapping from one Chi-
nese word to one English word to help balancing
sent. words
phrase-based MT 32% 12%
HIERO 35% 30%
HIERO (all rules) 65% 55%
Table 1: Reachability comparison (on FBIS) be-
tween phrase-based MT reported in Yu et al
(2013) (without 1-count rules) and HIERO (with
and without 1-count rules).
 0
 0.2
 0.4
 0.6
 0.8
 1
 20  40  60  80  100
fo
rc
ed
 d
ec
od
ab
le
 ra
tio
sentence length
loose
tight
Figure 4: Reachability vs. sent. length on FBIS.
See text below for ?loose? and ?tight?.
between overfitting and coverage. We use a tri-
gram language model trained from the target sides
of the two corpora respectively.
4.3 Forced Decoding Reachability
We first report the forced decoding reachability for
HIERO on FBIS in Table 1. With the full rule set,
65% sentences and 55% words of the whole cor-
pus are forced decodable in HIERO. After pruning
1-count rules, our forced decoding covers signif-
icantly more words than phrase-based MT in Yu
et al (2013). Furthermore, in phrase-based MT,
most decodable sentences are very short, while
in HIERO the lengths of decodable sentences are
more evenly distributed.
However, in the following experiments, due to
efficiency considerations, we use the ?tight? rule
extraction in cdec that is more strict than the
standard ?loose? rule extraction, which generates
a reduced rule set and, thus, a reduced reachabil-
ity. We show the reachability distributions of both
tight and loose rule extraction in Figure 4.
4.4 Evaluation on IWSLT
For IWSLT, we first compare the performance
from various update methods in Figure 5. The
max-violation method is more than 15 BLEU
788
 30
 35
 40
 45
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
Max-Violation
local update
skip
standard update
Figure 5: Comparison of various update methods.
 42
 43
 44
 45
 46
 47
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
sparse features
dense features
Hypergraph MERT
PRO
n-best MERT
Figure 6: Sparse features (Word-Edges) contribute
?2 BLEU points, outperforming PRO and MERT.
points better than the standard perceptron (also
known as ?bold-update? in Liang et al (2006))
which updates at the root of the derivation tree.
3,4
This can be explained by the fact that in train-
ing ?58% of the standard updates are invalid (i.e.,
they do not fix any violation). We also use the
?skip? strategy of Zhang et al (2013) which up-
dates at the root of the derivation only when it fixes
a search error, avoiding all invalid updates. This
achieves ?10 BLEU better than the standard up-
date, but is still more than ?5 BLEU worse than
Max-Violation update. Finally we also try the
?local-update? method from Liang et al (2006)
which updates towards the derivation with the best
Bleu
+1
in the root group S
[0:|x|]
. This method is
about 2 BLEU points worse than max-violation.
We further investigate the contribution of sparse
features in Figure 6. On the development set,
max-violation update without Word-Edges fea-
tures achieves BLEU similar to n-best MERT and
3
We find that while MAXFORCE generates translations of
length ratio close to 1 during training, the length ratios on
dev/test sets are significantly lower, due to OOVs. So we
run a binary search for the length penalty weight after each
training iteration to tune the length ratio to ?0.97 on dev set.
4
We report BLEU with averaged reference lengths.
algorithm # feats dev test
n-best MERT 18 44.9 47.9
Hypergraph MERT 18 46.6 50.7
PRO 18 45.0 49.5
local update perc. 443K 45.6 49.1
MAXFORCE 529K 47.4 51.5
Table 2: BLEU scores (with 16 references) of var-
ious training algorithms on IWSLT09.
algorithm # feats dev test
Hypergraph MERT 18 27.3 23.0
PRO 18 26.4 22.7
MAXFORCE 4.5M 27.7 23.9
Table 3: BLEU scores (with 4 references) of vari-
ous training algorithms on FBIS.
PRO, but lower than Hypergraph MERT. Adding
simple Word-Edges features improves BLEU by
?2 points, outperforming the very strong Hyper-
graph MERT baseline by?1 point. See Table 2 for
details. The results of n-best MERT, Hypergraph
MERT, and PRO are averages from 3 runs.
4.5 Evaluation on FBIS
Table 3 shows BLEU scores of Hypergraph MERT,
PRO, and MAXFORCE on FBIS. MAXFORCE ac-
tives 4.5M features, and achieves +1.2 BLEU over
PRO and +0.9 BLEU over Hypergraph MERT. The
training time (on 32 cores) for Hypergraph MERT
and PRO is about 30 min. on the dev set, and is
about 5 hours for MAXFORCE on the training set.
5 Conclusions
We have presented a latent-variable violation-
fixing framework for general structured predic-
tion problems with inexact search over hyper-
graphs. Its application on HIERO brings signif-
icant improvement in BLEU, compared to algo-
rithms that are specially designed for MT tuning
such as MERT and PRO.
Acknowledgment
Part of this work was done during K. Z.?s intern-
ship at IBM. We thank Martin
?
Cmejrek and Lemao
Liu for discussions, David Chiang for pointing
us to pycdec, Dan Gildea for Footnote 2, and
the anonymous reviewers for comments. This
work is supported by DARPA FA8750-13-2-0041
(DEFT), DARPA HR0011-12-C-0015 (BOLT),
and a Google Faculty Research Award.
789
References
Abhishek Arun and Philipp Koehn. 2007. On-
line learning methods for discriminative training of
phrase based statistical machine translation. Proc.
of MT Summit XI, 2(5):29.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Victor Chahuneau, Noah Smith, and Chris Dyer. 2012.
pycdec: A python interface to cdec. Prague Bulletin
of Mathematical Linguistics, (98).
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of NAACL 2013.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D Manning. 2013. Fast and adaptive online
training of feature-rich translation models. to ap-
pear) ACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of ACL and AFNLP.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of ACL.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan Mc-
Donald. 2013. Online learning with inexact hyper-
graph search. In Proceedings of EMNLP.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL 2013.
790
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 4?5,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Scalable Large-Margin Structured Learning:
Theory and Algorithms
Liang Huang Kai Zhao Lemao Liu
Graduate Center and Queens College, City University of New York
{liang.huang.sh, kzhao.hf, lemaoliu}@gmail.com
1 Motivations
Much of NLP tries to map structured input (sen-
tences) to some form of structured output (tag se-
quences, parse trees, semantic graphs, or trans-
lated/paraphrased/compressed sentences). Thus
structured prediction and its learning algorithm
are of central importance to us NLP researchers.
However, when applying machine learning to
structured domains, we often face scalability is-
sues for two reasons:
1. Even the fastest exact search algorithms for
most NLP problems (such as parsing and
translation) is too slow for repeated use on the
training data, but approximate search (such
as beam search) unfortunately breaks down
the nice theoretical properties (such as con-
vergence) of existing machine learning algo-
rithms.
2. Even with inexact search, the scale of the
training data in NLP still makes pure online
learning (such as perceptron and MIRA) too
slow on a single CPU.
This tutorial reviews recent advances that ad-
dress these two challenges. In particular, we will
cover principled machine learning methods that
are designed to work under vastly inexact search,
and parallelization algorithms that speed up learn-
ing on multiple CPUs. We will also extend struc-
tured learning to the latent variable setting, where
in many NLP applications such as translation and
semantic parsing the gold-standard derivation is
hidden.
2 Contents
1. Overview of Structured Learning
(a) key challenge 1: search efficiency
(b) key challenge 2: interactions between
search and learning
2. Structured Perceptron
(a) the basic algorithm
(b) the geometry of convergence proof
(c) voted and averaged perceptrons, and ef-
ficient implementation tricks
(d) applications in tagging, parsing, etc.
3. Structured Perceptron under Inexact Search
(a) convergence theory breaks under inex-
act search
(b) early update
(c) violation-fixing perceptron
(d) applications in tagging, parsing, etc.
?coffee break?
4. From Perceptron to MIRA
(a) 1-best MIRA; geometric solution
(b) k-best MIRA; hildreth algorithm
(c) MIRA with all constraints; loss-
augmented decoding
(d) MIRA under inexact search
5. Large-Margin Structured Learning with La-
tent Variables
(a) examples: machine translation, seman-
tic parsing, transliteration
(b) separability condition and convergence
proof
(c) latent-variable perceptron under inexact
search
(d) applications in machine translation
6. Parallelizing Large-Margin Structured
Learning
(a) iterative parameter mixing (IPM)
(b) minibatch perceptron and MIRA
4
3 Instructor Biographies
Liang Huang is an Assistant Professor at the City
University of New York (CUNY). He received
his Ph.D. in 2008 from Penn and has worked
as a Research Scientist at Google and a Re-
search Assistant Professor at USC/ISI. His work
is mainly on the theoretical aspects (algorithms
and formalisms) of computational linguistics, as
well as theory and algorithms of structured learn-
ing. He has received a Best Paper Award at ACL
2008, several best paper nominations (ACL 2007,
EMNLP 2008, and ACL 2010), two Google Fac-
ulty Research Awards (2010 and 2013), and a Uni-
versity Graduate Teaching Prize at Penn (2005).
He has given two tutorials at COLING 2008 and
NAACL 2009, being the most popular tutorial at
both venues.
Kai Zhao is a Ph.D. candidate at the City Univer-
sity of New York (CUNY), working with Liang
Huang. He received his B.S. from the Univer-
sity of Science and Technology in China (USTC).
He has published on structured prediction, online
learning, machine translation, and parsing algo-
rithms. He was a summer intern with IBM TJWat-
son Research Center in 2013.
Lemao Liu is a postdoctoral research associate at
the City University of New York (CUNY), work-
ing with Liang Huang. He received his Ph.D. from
the Harbin Institute of Technology in 2013. Much
of his Ph.D. work was done while visiting NICT,
Japan, under Taro Watanabe. His research area is
machine translation and machine learning.
5
