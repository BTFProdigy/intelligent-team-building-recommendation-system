Using Maximum Entropy to Extract Biomedical Named Entities
without Dictionaries
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu
Institute of Information Science, Academia Sinica
Nankang, Taipei, Taiwan 115
{thtsai, cwwu, hsu}@iis.sinica.edu.tw
Abstract
Current NER approaches include:
dictionary-based, rule-based, or ma-
chine learning. Since there is no
consolidated nomenclature for most
biomedical NEs, most NER systems
relying on limited dictionaries or rules
do not perform satisfactorily. In this
paper, we apply Maximum Entropy
(ME) to construct our NER framework.
We represent shallow linguistic infor-
mation as linguistic features in our ME
model. On the GENIA 3.02 corpus, our
system achieves satisfactory F-scores
of 74.3% in protein and 70.0% overall
without using any dictionary. Our
system performs significantly better
than dictionary-based systems. Using
partial match criteria, our system
achieves an F-score of 81.3%. Using
appropriate domain knowledge to
modify the boundaries, our system has
the potential to achieve an F-score of
over 80%.
1 Introduction
Biomedical literature available on the web has ex-
perienced unprecedented growth in recent years.
Therefore, demand for efficiently processing
these documents is increasing rapidly. There has
been a surge of interest in mining biomedical
literature. Some possible applications for such
efforts include the reconstruction and prediction
of pathways, establishing connections between
genes and disease, finding the relationships be-
tween genes, and much more.
Critical tasks for biomedical literature min-
ing include named entity recognition (NER), to-
kenization, relation extraction, indexing and cate-
gorization/clustering (Cohen and Hunter, 2005).
Among these technologies, NER is most fun-
damental. It is defined as recognizing objects
of a particular class in plain text. Depending
on required application, NER can extract objects
ranging from protein/gene names to disease/virus
names.
In general, biomedical NEs do not follow any
nomenclature (Shatkay and Feldman, 2003) and
can comprise long compound words and short ab-
breviations (Pakhomov, 2002). Some NEs con-
tain various symbols and other spelling variations.
On average, any NE of interest has five synonyms.
Biomedical NER is a challenging problem. There
are many different aspects to deal with. For ex-
ample, one can have unknown acronyms, abbre-
viations, or words containing hyphens, digits, let-
ters, and Greek letters; Adjectives preceding an
NE may or may not be part of that NE depend-
ing on the context and applications; NEs with the
same orthographical features may fall into differ-
ent categories; An NE may also belong to mul-
tiple categories intrinsically; An NE of one cate-
gory may contain an NE of another category in-
side it.
To tackle these challenges, researchers use
three main approaches: dictionary-based, rule-
based, and machine learning. In biomedical do-
main, there are more and more well-curated re-
sources, including lexical resources such as Lo-
268
cusLink (Maglott, 2002) and ontologies such as
MeSH (NLM, 2003). One might think that
dictionary-based systems relying solely on these
resources could achieve satisfactory performance.
However, according to (Pakhomov, 2002), they
typically perform quite poorly, with average re-
call rates in the range of only 10-30%. Rule-based
approaches, on the other hand, are more accurate,
but less portable across domains. Therefore, we
chose the machine learning approach.
Various machine learning approaches such as
ME (Kazama et al, 2002), SVM (Kazama et al,
2002; Song et al, 2004), HMM (Zhao, 2004) are
applied to NER. In this paper, we chose ME as
our framework since it is much easier to represent
various features in such a framework. In addi-
tion, ME models are flexible enough to capture
many correlated features, including overlapping
and non-independent features. We can thus use
multiple features with more ease than on an HMM
system. ME-based tagger, in particular, excel at
solving sequence tagging problems such as POS
tagging (Ratnaparkhi, 1997), general English
NER (Borthwick, 1999), and Chunking (Koeling,
2000).
In this paper, we describe how to construct a
ME-based framework that can exploit shallow lin-
guistic information in the recognition of biomed-
ical named entities. Hopefully, our experience
in integrating these features may prove useful for
those interested in constructing machine learning
based NER system.
2 Maximum Entropy Based Tagger
2.1 Formulation
In the Biomedical NER problem, we regard each
word in a sentence as a token. Each token is asso-
ciated with a tag that indicates the category of the
NE and the location of the token within the NE,
for example, B c, I c where c is a category, and
the two tags denote respectively the beginning to-
ken and the following token of an NE in category
c. In addition, we use the tag O to indicate that a
token is not part of an NE. The NER problem can
then be phrased as the problem of assigning one
of 2n + 1 tags to each token, where n is the num-
ber of NE categories. For example, one way to
tag the phrase ?IL-2 gene expression, CD28, and
NF-kappa B? in a paper is [B-DNA, I-DNA, O, O,
B-protein, O, O, B-protein, I-protein].
2.2 Maximum Entropy Modeling
ME is a flexible statistical model which assigns
an outcome for each token based on its history
and features. ME computes the probability p(o|h)
for any o from the space of all possible outcomes
O, and for every h from the space of all possi-
ble histories H . A history is all the condition-
ing data that enables one to assign probabilities
to the space of outcomes. In NER, history can
be viewed as all information derivable from the
training corpus relative to the current token. The
computation of p(o|h) in ME depends on a set of
binary-valued features, which are helpful in mak-
ing predictions about the outcome. For instance,
one of our features is: when all alphabets of the
current token are capitalized, it is likely to be part
of a biomedical NE. Formally, we can represent
this feature as follows:
f(h, o) =
?
??
??
1 : if W0-AllCaps(h)=true
and o=B-protein
0 : otherwise
(1)
Here, W0-AllCaps(h) is a binary function that
returns the value true if all alphabets of the cur-
rent token in the history h are capitalized. Given a
set of features and a training corpus, the ME esti-
mation process produces a model in which every
feature fi has a weight ?i. From (Berger et al,
1996), we can compute the conditional probabil-
ity as:
p(o|h) =
1
Z(h)
?
i
?fi(h,o)i (2)
Z(h) =
?
o
?
i
?fi(h,o)i (3)
The probability is given by multiplying the
weights of active features (i.e., those fi(h, o) =
1). The weight ?i is estimated by a procedure
called Generalized Iterative Scaling (GIS) (Dar-
roch and Ratcliff, 1972). This method improves
the estimation of weights iteratively. The ME esti-
mation technique guarantees that, for every fea-
ture fi, the expected value of ?equals the empirical
expectation of ?in the training corpus.
269
As noted in (Borthwick, 1999), ME allows
users to focus on finding features that character-
izes the problem while leaving feature weight as-
signment to the ME estimation routine. When
new features, e.g., syntax features, are added to
ME, users do not need to reformulate the model as
in the HMM model. The ME estimation routine
can automatically calculate new weight assign-
ments. More complete discussions of ME includ-
ing a description of the MEs estimation proce-
dure and references to some of the many success-
ful computational linguistics systems using ME
can be found in the following introduction (Rat-
naparkhi, 1997).
2.3 Decoding
After having trained an ME model and assigned
the proper weights ?to each feature fi, decoding
(i.e., marking up) a new piece of text becomes
simple. First, the ME module tokenizes the text.
Then, for each token, we check which features are
active and combine ?i of the active features ac-
cording to Equation 2. Finally, the probability of
a tag sequence y1...yn given a sentence w1...wn
is approximated as follows:
p(o1...on|w1...wn) ?
n?
j=1
p(oj |hj) (4)
where hj is the context for word wj . The tag-
ger uses beam search to find the most probable
sequence given the sentence. Sequences contain-
ing invalid subsequences are filtered out. For in-
stance, the sequence [B-protein, I-DNA] is in-
valid because it does not contain an ending token
and these two tokens are not in the same cate-
gory. Further details on the beam search can be
found in http://www-jcsu.jesus.cam.
ac.uk/?tdk22/project/beam.html.
3 Linguistic Features
3.1 Orthographical Features
Table 1 lists some orthographical features used
in our system. In our experience, ALLCAPS,
CAPSMIX, and INITCAP are more useful than
others.
Table 1: Orthographical features
Feature name Regular Expression
INITCAP [A-Z].*
CAPITALIZED [A-Z][a-z]+
ALLCAPS [A-Z]+
CAPSMIX .*[A-Z][a-z].* |
.*[a-z][A-Z].*
ALPHANUMERIC .*[A-Za-z].*[0-9].* |
.*[0-9].*[A-Za-z].*
SINGLECHAR [A-Za-z]
SINGLEDIGIT [0-9]
DOUBLEDIGIT [0-9][0-9]
INTEGER -?[0-9]+
REAL -?[0-9][.,]+[0-9]+
ROMAN [IVX]+
HASDASH .*-.*
INITDASH -.*
ENDDASH .*-
PUNCTUATION [,.;:?!-+]
QUOTE [???]
3.2 Context Features
Words preceding or following the target word
may be useful for determining its category. Take
the sentence ?The IL-2 gene localizes to bands
BC on mouse Chromosome 3? for example. If the
target word is ?IL-2,? the following word ?gene?
will help ME to distinguish ?IL-2 gene? from the
protein of the same name. Obviously, the more
context words analyzed the better and more pre-
cise the results. However, widening the context
window quickly leads to an explosion of the num-
ber of possibilities to calculate. In our experience,
a suitable window size is five.
3.3 Part-of-speech Features
Part of speech information is quite useful for iden-
tifying NEs. Verbs and prepositions usually indi-
cate an NEs boundaries, whereas nouns not found
in the dictionary are usually good candidates for
named entities. Our experience indicates that five
is also a suitable window size. The MBT POS
tagger (Daelemans et al, 1996) is used to provide
POS information. We trained it on GENIA 3.02p
and achieves 97.85% accuracy.
3.4 Word Shape Features
NEs in the same category may look similar (e.g.,
IL-2 and IL-4). So we have come up with sim-
ple way to normalize all similar words. Accord-
ing to our method, capitalized characters are all
replaced by ?A?, digits are all replaced by ?0?,
270
Table 2: Basic statistics for the data set
Data # abs # sen # words
GENIA 3.02 2,000 18,546 472,006 (236.00/abs)
non-English characters are replaced by ? ? (un-
derscore), and non-capitalized characters are re-
placed by ?a?. For example, Kappa-B will be nor-
malized as ?Aaaaa A?. To further normalize these
words, we shorten consecutive strings of iden-
tical characters to one character. For example,
?Aaaaa A? is normalized to ?Aa A?.
3.5 Prefix and Suffix Features
Some prefixes and suffixes can provide good
clues for classifying named entities. For example,
words which end in ?ase? are usually proteins. In
our experience, the acceptable length for prefixes
and suffixes is 3-5 characters.
4 Experiment
4.1 Datasets
In our experiment, we use the GENIA version
3.02 corpus (Kim et al, 2003). Its basic statis-
tics is summarized in Table 2. Frequencies for all
NE classes in it are showed in Table 3.
4.2 Results
In Table 4, one can see that F-scores for protein
and cell-type are comparably high. We believe
this is because protein and cell type are among
the top three most frequent categories in the train-
ing set (as shown in Table 3). One notices, how-
ever, that although DNA is the second most fre-
quent category, it does not have a high F-score.
We think this discrepancy is due to the fact that
DNA names are commonly used in proteins, caus-
ing a substantial overlap between these two cate-
gories. RNAs performance is comparably low be-
cause its training set is much smaller than those
of other categories. Cell lines performance is the
lowest since it overlaps heavily with cell type and
its training set is also very small.
In Table 5, one can see that, using the par-
tial matching criterion, the precision rates, recall
rates, and F-scores of protein names are all over
85%. The overall F-Score is 81.3%. The table
also shows that 83.9% of our systems suggestions
Table 4: NER performance of each NE category
on the GENIA 3.02 data (10-fold CV)
NE category Precision Recall F-score
protein 74.1 74.5 74.3
DNA 65.9 54.4 59.6
RNA 75.3 48.0 58.6
cell line 65.4 51.4 57.6
cell type 72.3 69.1 70.7
Overall 72.0 67.9 70.0
Table 5: Partial matching performance on the GE-
NIA 3.02 corpus (10-fold CV)
NE category Precision Recall F-score
protein 85.3 85.5 85.4
DNA 80.3 66.3 72.7
RNA 84.0 53.0 65.0
cell line 80.9 63.3 71.1
cell type 83.1 79.4 81.2
Overall 83.9 78.9 81.3
correctly identify at least one part of an NE, and
that our system tags at least one part of 78.9%
of all NEs in the test corpus. The precision rate in
all categories is over 80%, showing that , by using
appropriate post-processing methods, our system
can achieve high precision in all NE categories.
In Table 6, we compare our system with two
dictionary-based systems. One exploits hand-
crafted rules based on heuristics and protein name
dictionaries (Seki and Mostafa, 2003). We de-
note this system as ?rule + dictionary?. The other
system (Tsuruoka and Tsujii, 2004) has two con-
figurations: the first one exploits patterns to de-
tect protein names and their fragments, which
is denoted as ?dictionary expansion?; the sec-
ond one further applies naive Bayes filters to ex-
clude erroneous detections, which is denoted as
?dictionary expansion + filters?. One can see
that our system performs better than these dic-
tionary/heuristic systems by a wide margin. The
basic ?rule + dictionary? system achieves only
54.4% recall. By expanding the original dic-
tionary (?dictionary expansion?), they improve
the recall rate to 68.1%. After applying post
processing filters (?dictionary expansion + fil-
ters?), the recall rate dropped slightly, but preci-
sion increased by 25.7%. Still, our system per-
forms better than the best dictionary-based system
by 7.6%.
271
Table 3: Frequencies for NEs in each data set
Data protein DNA RNA cell type cell line All
GENIA 3.02 30,269 9,533 951 6,718 3,830 51,301
Table 6: Performance comparison between sys-
tems with and w/o dictionaries in extracting pro-
tein names on the GENIA 3.02 data
System Precision Recall F-score
our system 74.1 74.5 74.3
rule + dictionary 42.6 54.4 47.8
dictionary expansion 46.0 68.1 54.8
dictionary expansion + filters 71.7 62.3 66.6
5 Analysis and discussion
Recognition disagreement between our system
and GENIA is caused by the following two fac-
tors: Annotation problems:
1. Preceding adjective problem
Some descriptive adjectives are annotated as
parts of the following NE, but some are not.
2. Nested NEs
In GENIA, we found that in some instances
only embedded NEs are annotated while in
other instances, only the outside NE is an-
notated. However, according to the GENIA
tagging guidelines, the outside NE should be
tagged. For example, in 59 instances of the
phrase ?IL-2 gene?, ?IL-2? is tagged as a
protein 13 times, while in the other 46 it is
tagged as a DNA. This irregularity can con-
fuse machine learning based systems.
3. Cell-line/cell-type confusion
NEs in the cell line class are from certain cell
types. It is difficult even for an expert to dis-
tinguish them.
System recognition errors:
1. Misclassification
Some protein molecules or regions are mis-
classified as DNA molecules or regions.
These errors may be solved by exploiting
more context information.
2. Coordinated phrases
In GENIA, most conjunction phrases are
tagged as single NEs. However, conjunc-
tion phrases are usually composed of several
NEs, punctuation, and conjunctions such as
?and?, ?or? and ?but not?. Therefore, our
system sometimes only tags one of these NE
components. For example, in the phrase ?c-
Fos and c-Jun family members?, only ?c-
Jun family members? is tagged as a protein
by our system, while in GENIA, the whole
phrase is tagged as a protein.
3. False positives
Some entities appeared without accompany-
ing a specific name, for example, only men-
tion about ?the epitopes? rather than which
kind of epitopes. The GENIA corpus tends
to ignore these entities, but their contexts are
similar to the entities with specific names,
therefore, our system sometimes incorrectly
recognizes them as an NE.
6 Conclusion
Our system successfully integrates linguistic fea-
tures into the ME framework. Without using
any biomedical dictionaries, our system achieves
a satisfactory F-score of 74.3% in protein and
70.0% overall. Our system performs significantly
better than dictionary-based systems. Using par-
tial match criteria, our system achieves an F-score
of 81.3%. That means, with appropriate bound-
ary modification algorithms (with domain knowl-
edge), our system has the potential to achieve an
F-score of over 80%.
It is still difficult to recognize long, compli-
cated NEs and to distinguish between two over-
lapping NE classes, such as cell-line and cell-
type. This is because biomedical texts have com-
plicated syntax and involve more expert knowl-
edge than general domain news articles. An-
other serious problem is annotation inconsistency,
which confuses machine learning models and
makes evaluation difficult. Certain errors, such as
those in boundary identification, are more tolera-
ble if the main purpose is to discover relationships
272
between NEs.
In the future, we will exploit more linguistic
features such as composite features and external
features. Finally, to reduce human annotation ef-
fort and to alleviate the scarcity of available anno-
tated corpora, we will develop machine learning
techniques to learn from Web corpora in different
biomedical domains.
Acknowledgements
We are grateful for the support of National Sci-
ence Council under GRANT NSC94-2752-E-
001-001.
References
A. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computer Linguistics, 22:39?
71.
A. Borthwick. 1999. A Maximum Entropy Approach
to Named Entity Recognition. Phd thesis, New York
University.
K. Bretonnel Cohen and Lawrence Hunter. 2005. Nat-
ural language processing and systems biology. In
W. Dubitzky and F. Azuaje, editors, Artificial In-
telligence and Systems Biology, Springer Series on
Computational Biology. Springer.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part of
speech tagger-generator. In E. Ejerhed and I. Da-
gan, editors, Fourth Workshop on Very Large Cor-
pora, pages 14?27.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematicl Statistics, 43:1470?1480.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning support vector machines for biomedical
named entity recognition. In ACL-02 Workshop on
Natural Language Processing in Biomedical Appli-
cations.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. Genia corpus - a semanti-
cally annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1).
Rob Koeling. 2000. Chunking with maximum en-
tropy models. In CoNLL-2000.
D. Maglott. 2002. Locuslink: a directory of genes. In
NCBI Handbook, pages 19?1 to 19?16.
NLM. 2003. Mesh: Medical subject headings.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical text. In the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
A. Ratnaparkhi. 1997. A simple introduction to maxi-
mum entropy models for natural language process-
ing. Technical Report Techical Report 97-08, Insti-
tute for Research in Cognitive Science University
of Pennsylvania.
Kazuhiro Seki and Javed Mostafa. 2003. An approach
to protein name extraction using heuristics and a
dictionary. In ASIST 2003.
Hagit Shatkay and Ronen Feldman. 2003. Min-
ing the biomedical literature in the genomic era:
an overview. Journal of Computational Biology,
10(6):821?855.
Yu Song, Eunju Kim, Gary Geunbae Lee, and
Byoung-kee Yi. 2004. Posbiotm-ner in the shared
task of bionlp/nlpba 2004. In the Joint Workshop on
Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), Geneva, Switzer-
land.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2004. Im-
proving the performance of dictionary-based ap-
proaches in protein name recognition. Journal of
Biomedical Informatics, 37(6):461?470.
Shaojun Zhao. 2004. Named entity recognition in
biomedical texts using an hmm model. In COL-
ING 2004 International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
273
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 233?236, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploiting Full Parsing Information to Label Semantic Roles Using an  
Ensemble of ME and SVM via Integer Linear Programming 
 
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica  
Taipei 115, Taiwan 
{thtsai, cwwu, sbb, hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we propose a method that 
exploits full parsing information by repre-
senting it as features of argument classifi-
cation models and as constraints in integer 
linear learning programs. In addition, to 
take advantage of SVM-based and Maxi-
mum Entropy-based argument classifica-
tion models, we incorporate their scoring 
matrices, and use the combined matrix in 
the above-mentioned integer linear pro-
grams. The experimental results show that 
full parsing information not only in-
creases the F-score of argument classifi-
cation models by 0.7%, but also 
effectively removes all labeling inconsis-
tencies, which increases the F-score by 
0.64%. The ensemble of SVM and ME 
also boosts the F-score by 0.77%. Our 
system achieves an F-score of 76.53% in 
the development set and 76.38% in Test 
WSJ. 
1 Introduction 
The Semantic Role Labeling problem can be for-
mulated as a sentence tagging problem. A sentence 
can be represented as a sequence of words, as 
phrases (chunks), or as a parsing tree. The basic 
units of a sentence are words, phrases, and con-
stituents in these representations, respectively.. 
Pradhan et al (2004) established that Constituent-
by-Constituent (C-by-C) is better than Phrase-by-
Phrase (P-by-P), which is better than Word-by-
Word (W-by-W).  This is probably because the 
boundaries of the constituents coincide with the 
arguments; therefore, C-by-C has the highest ar-
gument identification F-score among the three ap-
proaches.  
In addition, a full parsing tree also provides 
richer syntactic information than a sequence of 
chunks or words. Pradhan et al (2004) compared 
the seven most common features as well as several 
features related to the target constituent?s parent 
and sibling constituents. Their experimental results 
show that using other constituents? information 
increases the F-score by 6%. Punyakanok et al 
(2004) represent full parsing information as con-
straints in integer linear programs. Their experi-
mental results show that using such information 
increases the argument classification accuracy by 
1%. 
In this paper, we not only add more full parsing 
features to argument classification models, but also 
represent full parsing information as constraints in 
integer linear programs (ILP) to resolve label in-
consistencies. We also build an ensemble of two 
argument classification models: Maximum Entropy 
and SVM by combining their argument classifica-
tion results and applying them to the above-
mentioned ILPs. 
2 System Architecture 
Our SRL system is comprised of four stages: prun-
ing, argument classification, classification model 
incorporation, and integer linear programming. 
This section describes how we build these stages, 
including the features used in training the argu-
ment classification models. 
2.1 Pruning 
233
When the full parsing tree of a sentence is avail-
able, only the constituents in the tree are consid-
ered as argument candidates. In CoNLL-2005, full 
parsing trees are provided by two full parsers: the 
Collins parser (Collins, 1999)  and the Charniak 
parser (Charniak, 2000). According to Punyakanok 
et al (2005), the boundary agreement of Charniak 
is higher than that of Collins; therefore, we choose 
the Charniak parser?s results. However, there are 
two million nodes on the full parsing trees in the 
training corpus, which makes the training time of 
machine learning algorithms extremely long. Be-
sides, noisy information from unrelated parts of a 
sentence could also affect the training of machine 
learning models. Therefore, our system exploits the 
heuristic rules introduced by Xue and Palmer 
(2004) to filter out simple constituents that are 
unlikely to be arguments. Applying pruning heuris-
tics to the output of Charniak?s parser effectively 
eliminates 61% of the training data and 61.3% of 
the development data, while still achieves 93% and 
85.5% coverage of the correct arguments in the 
training and development sets, respectively. 
2.2 Argument Classification 
This stage assigns the final labels to the candidates 
derived in Section 2.1. A multi-class classifier is 
trained to classify the types of the arguments sup-
plied by the pruning stage. In addition, to reduce 
the number of excess candidates mistakenly output 
by the previous stage, these candidates can be la-
beled as null (meaning ?not an argument?). The 
features used in this stage are as follows. 
Basic Features 
? Predicate ? The predicate lemma. 
? Path ? The syntactic path through the 
parsing tree from the parse constituent be-
ing classified to the predicate. 
? Constituent Type 
? Position ? Whether the phrase is located 
before or after the predicate. 
? Voice ? passive: if the predicate has a POS 
tag VBN, and its chunk is not a VP, or it is 
preceded by a form of ?to be? or ?to get? 
within its chunk; otherwise, it is active. 
? Head Word ? calculated using the head 
word table described by Collins (1999). 
? Head POS ? The POS of the Head Word. 
? Sub-categorization ? The phrase structure 
rule that expands the predicate?s parent 
node in the parsing tree. 
? First and Last Word/POS 
? Named Entities ? LOC, ORG, PER, and 
MISC. 
? Level ? The level in the parsing tree. 
Combination Features 
? Predicate Distance Combination 
? Predicate Phrase Type Combination 
? Head Word and Predicate Combination 
? Voice Position Combination 
Context Features 
? Context Word/POS ? The two words pre-
ceding and the two words following the 
target phrase, as well as their correspond-
ing POSs.  
? Context Chunk Type ? The two chunks 
preceding and the two chunks following 
the target phrase. 
Full Parsing Features 
We believe that information from related constitu-
ents in the full parsing tree helps in labeling the 
target constituent. Denote the target constituent by 
t. The following features are the most common 
baseline features of t?s parent and sibling constitu-
ents. For example, Parent/ Left Sibling/ Right Sib-
ling Path denotes t?s parents?, left sibling?s, and 
right sibling?s Path features.  
? Parent / Left Sibling / Right Sibling 
Path 
? Parent / Left Sibling / Right Sibling 
Constituent Type 
? Parent / Left Sibling / Right Sibling Po-
sition 
? Parent / Left Sibling / Right Sibling 
Head Word 
? Parent / Left Sibling / Right Sibling 
Head POS 
? Head of PP parent ? If the parent is a PP, 
then the head of this PP is also used as a 
feature. 
Argument Classification Models 
234
We use all the features of the SVM-based and ME-
based argument classification models. All SVM 
classifiers are realized using SVM-Light with a 
polynomial kernel of degree 2. The ME-based 
model is implemented based on Zhang?s MaxEnt 
toolkit1 and L-BFGS (Nocedal and Wright, 1999) 
method to perform parameter estimation. 
2.3 Classification Model Incorporation  
We now explain how we incorporate the SVM-
based and ME-based argument classification mod-
els. After argument classification, we acquire two 
scoring matrices, PME and PSVM, respectively. In-
corporation of these two models is realized by 
weighted summation of PME and PSVM as follows: 
P? = wMEPME + wSVMPSVM
We use P? for the objective coefficients of the 
ILP described in Section 2.4. 
2.4 Integer Linear Programming (ILP) 
To represent full parsing information as features, 
there are still several syntactic constraints on a 
parsing tree in the SRL problem. For example, on a 
path of the parsing tree, there can be only one con-
stituent annotated as a non-null argument. How-
ever, it is difficult to encode this constraint in the 
argument classification models. Therefore, we ap-
ply integer linear programming to resolve inconsis-
tencies produced in the argument classification 
stage.  
According to Punyakanok et al (2004), given a 
set of constituents, S, and a set of semantic role 
labels, A, the SRL problem can be formulated as 
an ILP as follows: 
Let zia be the indicator variable that represents 
whether or not an argument,  a, is assigned to any 
Si ? S; and let pia = score(Si = a). The scoring ma-
trix P composed of all pia is calculated by the ar-
gument classification models. The goal of this ILP 
is to find a set of assignments for all zia that maxi-
mizes the following function: 
??
? ?S AiS a
iaia zp . 
Each Si?  S should have one of these argument 
types, or no type (null). Therefore, we have  
?
?
=
Aa
iaz 1 . 
Next, we show how to transform the constraints in 
                                                          
1 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 
the filter function into linear equalities or inequali-
ties, and use them in this ILP. 
Constraint I: No overlapping or embedding  
For arguments Sj1 , . . . , Sjk  on the same path in a 
full parsing tree, only one argument can be as-
signed to an argument type. Thus, at least k ? 1 
arguments will be null, which is represented by ?  
in the following linear equality: 
?
=
??
k
i
j ki
1
1z ? .                                                     
Constraint II: No duplicate argument classes 
Within the same sentence, A0-A5 cannot appear 
more than once. The inequality for A0 is therefore: 
?
=
?
k
i
iz
1
A0 1. 
Constraint III: R-XXX arguments  
The linear inequalities that represent A0 and its 
reference type R-A0 are: 
?
=
????
k
i
mi zzMm
1
A0RA0:},...,1{ . 
Constraint IV: C-XXX arguments  
The continued argument XXX has to occur before 
C-XXX. The linear inequalities for A0 are: 
??
=
????
1
1
A0CA0:},...,2{
m
i
mj zzMm i . 
Constraint V: Illegal arguments  
For each verb, we look up its allowed roles. This 
constraint is represented by summing all the corre-
sponding indicator variables to 0. 
3 Experiment Results  
3.1 Data and Evaluation Metrics 
The data, which is part of the PropBank corpus, 
consists of sections from the Wall Street Journal 
part of the Penn Treebank. All experiments were 
carried out using Section 2 to Section 21 for train-
ing, Section 24 for development, and Section 23 
for testing. Unlike CoNLL-2004, part of the Brown 
corpus is also included in the test set.  
3.2 Results 
Table 1 shows that our system makes little differ-
ence to the development set and Test WSJ. How-
ever, due to the intrinsic difference between the 
WSJ and Brown corpora, our system performs bet-
ter on Test WSJ than on Test Brown. 
235
Precision Recall F
?=1
Development 81.13% 72.42% 76.53
Test WSJ 82.77% 70.90% 76.38
Test Brown 73.21% 59.49% 65.64
Test WSJ+Brown 81.55% 69.37% 74.97
Test WSJ Precision Recall F
?=1
Overall 82.77% 70.90% 76.38
A0 88.25% 84.93% 86.56
A1 82.21% 72.21% 76.89
A2 74.68% 52.34% 61.55
A3 78.30% 47.98% 59.50
A4 84.29% 57.84% 68.60
A5 100.00% 60.00% 75.00
AM-ADV 64.19% 47.83% 54.81
AM-CAU 70.00% 38.36% 49.56
AM-DIR 38.20% 40.00% 39.08
AM-DIS 83.33% 71.88% 77.18
AM-EXT 86.67% 40.62% 55.32
AM-LOC 63.71% 41.60% 50.33
AM-MNR 63.36% 48.26% 54.79
AM-MOD 98.00% 97.64% 97.82
AM-NEG 99.53% 92.61% 95.95
AM-PNC 44.44% 17.39% 25.00
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.21% 61.09% 70.45
R-A0 91.08% 86.61% 88.79
R-A1 79.49% 79.49% 79.49
R-A2 87.50% 43.75% 58.33
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 72.73% 61.54% 66.67
V 97.32% 97.32% 97.32  
Table 1. Overall results (top) and detailed results 
on the WSJ test (bottom). 
Precision Recall F
?=1
ME w/o parsing 77.28% 70.55% 73.76%
ME 78.19% 71.08% 74.46%
ME with ILP 79.57% 71.11% 75.10%
SVM 79.88% 72.03% 75.76%
Hybrid 81.13% 72.42% 76.53%
 
Table 2. Results of all configurations on the devel-
opment set. 
From Table 2, we can see that the model with 
full parsing features outperforms the model with-
out the features in all three performance matrices. 
After applying ILP, the performance is improved 
further. We also observe that SVM slightly outper-
forms ME. However, the hybrid argument classifi-
cation model achieves the best results in all three 
metrics. 
4 Conclusion  
In this paper, we add more full parsing features to 
argument classification models, and represent full 
parsing information as constraints in ILPs to re-
solve labeling inconsistencies. We also integrate 
two argument classification models, ME and SVM, 
by combining their argument classification results 
and applying them to the above-mentioned ILPs. 
The results show full parsing information increases 
the total F-score by 1.34%. The ensemble of SVM 
and ME also boosts the F-score by 0.77%. Finally, 
our system achieves an F-score of 76.53% in the 
development set and 76.38% in Test WSJ. 
Acknowledgement 
We are indebted to Wen Shong Lin and Prof. Fu 
Chang for their invaluable advice in data pruning, 
which greatly speeds up the training of our ma-
chine learning models. 
References  
X. Carreras and L. M?rquez. 2005. Introduction to the 
CoNLL-2005 Shared Task: Semantic Role Labeling. 
In Proceedings of the CoNLL-2005. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proceedings of the NAACL-2000. 
M. J. Collins. 1999. Head-driven Statistical Models for 
Natural Language Parsing. Ph.D. thesis, University 
of Pennsylvania. 
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion, Springer. 
S. Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J. H. 
Martin, and D. Jurafsky. 2004. Support Vector 
Learning for Semantic Argument Classification. 
Journal of Machine Learning. 
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling. 
In Proceedings of the 19th International Joint Con-
ference on Artificial Intelligence (IJCAI-05). 
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. 
Semantic Role Labeling via Integer Linear Pro-
gramming Inference. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics 
(COLING-04). 
N. Xue and M. Palmer. 2004. Calibrating Features for 
Semantic Role Labeling. In Proceedings of the 
EMNLP 2004. 
236
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 142?145,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Using Ensemble Methods for Chinese Named Entity Recognition  
Chia-Wei Wu Shyh-Yi Jan Richard Tzong-Han 
Tsai 
Wen-Lian Hsu 
 
Institute of Information Science, Academia Sinica, Nankang, Taipei,115, Taiwan 
{cwwu,shihyi,thtsai,Hsu}@iis.sinica.edu.tw 
 
  
Abstract 
In sequence labeling tasks, applying dif-
ferent machine learning models and fea-
ture sets usually leads to different results. 
In this paper, we exploit two ensemble 
methods in order to integrate multiple 
results generated under different condi-
tions. One method is based on majority 
vote, while the other is a memory-based 
approach that integrates maximum en-
tropy and conditional random field clas-
sifiers. Our results indicate that the 
memory-based method can outperform 
the individual classifiers, but the major-
ity vote method cannot. 
1 Introduction 
Sequence labeling and segmentation tasks have 
been studied extensively in the fields of computa-
tional linguistics and information extraction. Sev-
eral tasks, including, word segmentation, and 
semantic role labeling, provide rich information 
for various applications, such as segmentation in 
Chinese information retrieval and named entity 
recognition in biomedical literature mining.  
Probabilistic state automata models, such as the 
Hidden Markov model  (HMM) [6] and condi-
tional random fields (CRF) [5] are some of best, 
and therefore most popular, approaches for se-
quence labeling tasks. Both HMM and CRF con-
sider that the state transition and the state 
prediction are conditional on the observation of 
data. The advantage of the CRF model is that 
richer feature sets can be considered, because, 
unlike HMM, it does not make a dependence as-
sumption. However, the obvious drawback of the 
CRF model is that it needs more computing re-
sources, so we can not apply all the features of 
the model. One possible way to resolve this prob-
lem is to effectively combine the results of vari-
ous individual classifiers trained with different 
feature sets. In this paper, we use two ensemble 
methods to combine the results of the classifiers. 
We also combine the results generated by two 
machine learning models: maximum entropy 
(ME) [1] and CRF. One ensemble method is 
based on the majority vote [3], and the other is 
the memory based learner [7]. Although the en-
semble methods have been applied in some se-
quence labeling tasks [2],[3], similar work in 
Chinese named entity recognition is scarce. 
Our Chinese named entity tagger uses a charac-
ter-based model. For English named entity tasks, 
a character-based NER model proposed by Dan 
Klein [4] proves the usefulness of substrings 
within words. In Chinese NER, the character-
based model is more straightforward, since there 
are no spaces between Chinese words and each 
Chinese character is actually meaningful.  An-
other reason for using a character-based model is 
that it can avoid the errors sometimes made by a 
Chinese word segmentor.  
The remainder of this paper is organized as fol-
lows. In the Section 2, we introduce the machine 
learning models, the features we apply in the ma-
chine learning models, and the ensemble methods. 
In Section 3, we briefly describe the experimental 
data and the experiment results. Then, in Section 
4, we present our conclusions.. 
2 Method 
2.1 Machine Learning Models 
In this section, we introduce ME and CRF.
Maximum Entropy 
ME[1] is a statistical modeling technique used 
for estimating the conditional probability of a 
target label based on given information. The 
technique computes the probability p(y|x), where 
y denotes all possible outcomes of the space, and 
x denotes all possible features of the space. The 
computation of p(y|x) depends on a set of fea-
142
tures in x; the features are helpful for making 
predictions about the outcomes, y. 
Given a set of features and a training set, the ME 
estimation process produces a model, in which 
every feature fi has a weight ?i. The ME model 
can be represented by the following formula: 
( ) ( ) ( )???
?
???
?= ?
i
ii yxfxz
xyp ,exp| ?1
, 
( ) ( )? ? ???
?
???
?=
y i
ii yxfxz ,exp ?
.  
The probability is derived by multiplying the 
weights of the active features (i.e., those fi (y,x) = 
1). 
Conditional Random Field 
A conditional random field (CRF)[5] can be seen 
as an undirected graph model in which the nodes 
corresponding to the label sequence y are condi-
tional on the observed sequence x. The goal of 
CRF is to find the label sequence y that has the 
maximized probability, given an observation se-
quence x. The formula for the CRF model can be 
written as: 
( ) ( ) ( )( )xyxxy ,exp1| jj j FZP ?= ? ,  
where ?j is the parameter of a corresponding fea-
ture Fj , Z(x) is an normalizing factor, and Fj can 
be written as:   
( ) (? = ?= ni iiij iyyfF 0 1 ,,,, xxy ), 
where i means the relative position in the se-
quence, and yi-1 and yi denote the label at position 
i-1 and i respectively. In this paper, we only con-
sider linear chain and first-order Markov assump-
tion CRFs. In NER applications, a feature 
function fj (yi-1, yi, x, i) can be set to check 
whether x is a specific character, and whether yi-1 
is a label (such as Location) and yi is a label (such 
as Others).   
2.2 Chinese Named Entity Recognition 
In this section, we present the features applied in 
our CRF and ME models, namely, characters, 
words, and chuck information. 
Character Features 
The character features we apply in the CRF 
model and the ME model are presented in Tables 
1 and 2 respectively. The numbers listed in the 
feature type column indicate the relative position 
of a character in the sliding window. For example, 
-1 means the previous character of the target 
character. Therefore, the characters in those posi-
tions are applied in the model. The numbers in 
parentheses mean that the feature includes a 
combination of the characters in those positions. 
The unigrams in Tables 1 and 2 indicate that the 
listed features only consider to their own labels, 
whereas the bigram model considers the combi-
nation of the current label and the previous label. 
Since ME does not consider multiple states in a 
single feature, there are only unigrams in Table 2. 
In addition, as ME can handle more features than 
CRF, we apply extra features in the ME model  
 
Table 1 Character features for CRF 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) 
bigram -2 -1 0 +1 +2, (0,1) 
 
Table 2 Character features for ME 
 Feature Types 
unigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1), 
(1,2), (-1,0,1) (-1,1) 
Word Information  
Because of the limitations of the closed task, we 
use the NER corpus to train the segmentors based 
on the CRF model. To simulate noisy word in-
formation in the test corpus, we use a ten-fold 
method for training segmentors to tag the training 
corpus. The word features we apply in our NER 
systems are presented in Tables 3 and 4. 
In addition to the word itself, chuck information, 
i.e., the relative position of a character in a word, 
is also valuable information. Hence, we also add 
chuck information to our models. As the diversity 
of Chinese words is greater than that of Chinese 
characters, the number of features that can be 
used in CRF is much lower than the number that 
can be used in ME.   
Table 3 Word features for CRF 
 Feature Types 
unigram 0 
bigram 0  
 
Table 4 Word features for ME 
 Feature Types 
unigram -1, 0, 1, (-2,-1), (-1,0), (0,1), (1,2) 
2.3 Ensemble Methods 
Majority vote 
We can not put all the features into the CRF 
model because of its limited resources. Therefore, 
we train several CRF classifiers with different 
feature sets so that we can use as many features 
143
as possible. Then, we use the following simple, 
equally weighted linear equation, called majority 
vote, to combine the results of the CRF classifi-
ers.   
( ) ( )? == Ti i xyCxyS 0 ,, , 
where S(y,x) is the score of a label y and a char-
acter x respectively; T denotes the total number 
of CRF models; and the value of Ci(y,x) is 1 if 
the decision of the result of the ith CRF model is 
y, otherwise it is zero. The highest score of y is 
chosen as the label of x. The results are incorpo-
rated into the Viterbi algorithm to search for the 
path with the maximum scores. 
In this paper, the first step in the majority vote 
experiment is to train three CRF classifiers with 
different feature sets. Then, in the second step, 
we use the results obtained in the first step to 
generate the voting scores for the Viterbi algo-
rithm. 
Memory Based learner 
The memory-based learning method memorizes 
all examples in a training corpus. If a word is 
unknown, the memory-based classifier uses the 
k-nearest neighbors to find the most similar ex-
ample as the answer. Instead of using the com-
plete algorithm of the memory-based learner, we 
do not handle unseen data. In our memory- based 
combination method, the learner remembers all 
named entities from the results of the various 
classifiers and then tags the characters that were 
originally tagged as ?Other?. For example, if a 
character x is tagged by one classifier as ?0? 
(?Others? tag) and if the memory-based classifier 
learns from another classifier that this character 
is tagged as PER, then x will be tagged as ?B-
PER? by the memory-based classifier. 
The obvious drawback of this method is that the 
precision rate might decrease as the recall rate 
increases. Therefore, we set the following three 
rules to filter out samples that are likely to have a 
high error rate.  
1. Named entities can not be tagged as differ-
ent named entity tags by different classifiers. 
2. We set an absolute frequency threshold to 
filter out examples that occur less than the 
threshold. 
3. We set a relative frequency threshold to 
filter out examples that occur less than the 
threshold. For example, if a word x appears 
10 times in the corpus, then half of the in-
stances of x have to be tagged as named en-
tities; otherwise, x will be filtered out of the 
memory classifier. 
In our experiment, we used the memory-based 
learner to memorize the named entities from the 
tagging results of an ME classifier and a CRF 
classifier, and then tagged the tagging results of 
the CRF classifier.   
3 Experiments 
3.1 Data 
We selected the corpora of City University of 
Hong Kong (CityU) and Microsoft Research 
(MSRA) corpora to evaluate our methods. CityU 
is a Traditional Chinese corpus, and MSRA is 
Simplified Chinese corpus. 
3.2 Results 
Table 5 shows the results of several methods ap-
plied to the MSRA corpus. The memory-based 
ensemble method, which combines the results of 
a maximum entropy model and those of a CRF 
classifier, achieves the best performance. The 
majority vote combined with the results of three 
CRF models based on different feature sets has 
the worst performance. 
 
Table 5 msra  
 Precision Recall FB1 
Memory based 86.21 78.14 81.98 
Majority Vote 85.83 76.06 80.65 
Only-Character 86.70 75.54 80.74 
CRF 86.23 77.40 81.58 
 
The results obtained on Cityu, presented in Table 
6, show that the single CRF classifier achieved 
the best performance. None of the ensemble 
methods can outperform the non-ensemble meth-
ods. 
 
Table 6 cityu 
 Precision Recall FB1 
Memory based  90.79 86.26 88.47 
Majority Vote 90.52 84.15 87.22 
Only-Character 91.32 84.55 87.80 
CRF 92.01 85.45 88.61 
 
Tables 7 and 8 show the results of the memory-
based ensemble methods under different rules. 
We set the frequency threshold as 2 and the rela-
tive frequency threshold as 0.5. The results show 
that the relative frequencies rule effectively re-
duces the loss of precision caused by more enti-
ties being tagged by the memory-based classifier. 
The memory-based ensemble method works well 
on the MSRA corpus, but not on the CityU cor-
pus. In the MSRA corpus, the memory-based 
144
ensemble method outperforms the individual 
CRF model by approximately 0.4 % in FB1. We 
found that the memory-based classifier can not 
achieve a better performance than the CRF model 
because it misclassifies many organizations? 
names. Therefore, we chose another strategy that 
restricts the memory-based classifier to tagging 
person names only. Under this restriction, the 
performance of the memory-based classifier im-
proves FB1 by approximately 0.2%. 
 
Table 7 msra- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 86.18 78.16 81.97 
Relative Frequency 
Threshold 
86.21 78.14 81.98 
Only Person 86.27 77.58 81.69 
 
Table 8 cityu- The performances of memory 
based ensemble methods under different rules. 
 Precision Recall FB1 
Frequency Threshold 90.69 86.55 88.57 
Relative Frequency 
Threshold 
90.87 86.29 88.52 
Only Person 92.00 85.66 88.72 
4 Conclusion  
In this paper, we use ME and CRF models to 
train a Chinese named entity tagger. Like previ-
ous researchers, we found that CRF models out-
perform ME models. We also apply two 
ensemble methods, namely, majority vote and 
memory-based approaches, to the closed NER 
shared task. Our results show that integrating 
individual classifiers as the majority vote ap-
proach does not outperform the individual classi-
fiers. Furthermore, a memory-based combination 
only seems to work when we restrict the mem-
ory-based classifier to handling person names. 
Acknowledgement 
We are grateful for the support of National Sci-
ence Council under Grant NSC 95-2752-E-001-
001-PAE. 
References  
1. Berger, A., Pietra, S.A.D. and Pietra, V.J.D. A 
Maximum Entropy Approach to Natural Language 
Processing. Computer Linguistic, 22. 1996 39-71. 
2. Florian, R., Ittycheriah, A., Jing, H. and Zhang, T., 
Named Entity Recognition through Classifier 
Combination. in Proceedings of Conference on 
Computational Natural Language Learning, 2003, 
168-171. 
3. Halteren, H.v., Zavrel, J. and Daelemans, W. Im-
proving accuracy in word class tagging through 
combination of machine learning systems. Compu-
tational Linguistics, 27 (2). 2001 199-230. 
4. Klein, D., Smarr, J., Nguyen, H. and Manning, 
C.D., Named Entity Recognition with Character-
Level Models. in Conference on Computational 
Natural Language Learning, 2003, 180-183. 
5. Lafferty, J., McCallum, A. and Pereira, F. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. International 
Conference on Machine Learning. 2001 282-289. 
6. Rabiner, L. A tutorial on hidden Markov models 
and selected applications in speech recognition. 
Proceedings of the IEEE, 77 (2). 1989 257-286. 
7. Sutton, C., Rohanimanesh, K. and McCallum, A., 
Dynamic Conditional Random Fields: Factorized 
Probabilistic Models for Labeling and Segmenting 
Sequence Data. in Proceedings of the Twenty-First 
International Conference on Machine Learning, 
2004, 99-107. 
8. Zavrel, J. and Daelemans, W. Memory-based learn-
ing: using similarity for smoothing. Proceedings of 
the eighth conference on European chapter of the 
Association for Computational Linguistics. 1997 
436 - 443. 
145
