N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of the ACL Student Research Workshop, pages 67?72,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Phrase Linguistic Classification and Generalization for Improving Statistical
Machine Translation
Adria` de Gispert
TALP Research Center
Universitat Polite`cnica de Catalunya (UPC)
Barcelona
agispert@gps.tsc.upc.es
Abstract
In this paper a method to incorporate lin-
guistic information regarding single-word
and compound verbs is proposed, as a
first step towards an SMT model based
on linguistically-classified phrases. By
substituting these verb structures by the
base form of the head verb, we achieve
a better statistical word alignment perfor-
mance, and are able to better estimate the
translation model and generalize to unseen
verb forms during translation. Preliminary
experiments for the English - Spanish lan-
guage pair are performed, and future re-
search lines are detailed.
1 Introduction
Since its revival in the beginning of the 1990s, statis-
tical machine translation (SMT) has shown promis-
ing results in several evaluation campaigns. From
original word-based models, results were further im-
proved by the appearance of phrase-based transla-
tion models.
However, many SMT systems still ignore any
morphological analysis and work at the surface level
of word forms. For highly-inflected languages, such
as German or Spanish (or any language of the Ro-
mance family) this poses severe limitations both in
training from parallel corpora, as well as in produc-
ing a correct translation of an input sentence.
This lack of linguistic knowledge in SMT forces
the translation model to learn different transla-
tion probability distributions for all inflected forms
of nouns, adjectives or verbs (?vengo?, ?vienes?,
?viene?, etc.), and this suffers from usual data sparse-
ness. Despite the recent efforts in the community to
provide models with this kind of information (see
Section 6 for details on related previous work), re-
sults are yet to be encouraging.
In this paper we address the incorporation of
morphological and shallow syntactic information re-
garding verbs and compound verbs, as a first step
towards an SMT model based on linguistically-
classified phrases. With the use of POS-tags and
lemmas, we detect verb structures (with or without
personal pronoun, single-word or compound with
auxiliaries) and substitute them by the base form1
of the head verb. This leads to an improved statisti-
cal word alignment performance, and has the advan-
tages of improving the translation model and gen-
eralizing to unseen verb forms, during translation.
Experiments for the English - Spanish language pair
are performed.
The organization of the paper is as follows. Sec-
tion 2 describes the rationale of this classification
strategy, discussing the advantages and difficulties
of such an approach. Section 3 gives details of
the implementation for verbs and compound verbs,
whereas section 4 shows the experimental setting
used to evaluate the quality of the alignments. Sec-
tion 5 explains the current point of our research, as
well as both our most-immediate to-do tasks and our
medium and long-term experimentation lines. Fi-
nally, sections 6 and 7 discuss related works that can
be found in literature and conclude, respectively.
1The terms ?base form? or ?lemma? will be used equivalently
in this text.
67
2 Morphosyntactic classification of
translation units
State-of-the-art SMT systems use a log-linear com-
bination of models to decide the best-scoring tar-
get sentence given a source sentence. Among
these models, the basic ones are a translation model
Pr(e|f) and a target language model Pr(e), which
can be complemented by reordering models (if the
language pairs presents very long alignments in
training), word penalty to avoid favoring short sen-
tences, class-based target-language models, etc (Och
and Ney, 2004).
The translation model is based on phrases; we
have a table of the probabilities of translating a cer-
tain source phrase f?j into a certain target phrase
e?k. Several strategies to compute these probabili-
ties have been proposed (Zens et al, 2004; Crego et
al., 2004), but none of them takes into account the
fact that, when it comes to translation, many differ-
ent inflected forms of words share the same transla-
tion. Furthermore, they try to model the probability
of translating certain phrases that contain just aux-
iliary words that are not directly relevant in trans-
lation, but play a secondary role. These words are
a consequence of the syntax of each language, and
should be dealt with accordingly.
For examples, consider the probability of translat-
ing ?in the? into a phrase in Spanish, which does not
make much sense in isolation (without knowing the
following meaning-bearing noun), or the modal verb
?will?, when Spanish future verb forms are written
without any auxiliary.
Given these two problems, we propose a classifi-
cation scheme based on the base form of the phrase
head, which is explained next.
2.1 Translation with classified phrases
Assuming we translate from f to e, and defining e?i,
f?j a certain source phrase and a target phrases (se-
quences of contiguous words), the phrase translation
model Pr(e?i|f?j) can be decomposed as:
?
T
Pr(e?i|T, f?j)Pr(E?i|F?j , f?j)Pr(F?j , f?j) (1)
where E?i, F?j are the generalized classes of the
source and target phrases, respectively, and T =
(E?i, F?j) is the pair of source and target classes used,
which we call Tuple. In our current implementation,
we consider a classification of phrases that is:
? Linguistic, ie. based on linguistic knowledge
? Unambiguous, ie. given a source phrase there
is only one class (if any)
? Incomplete, ie. not all phrases are classified,
but only the ones we are interested in
? Monolingual, ie. it runs for every language in-
dependently
The second condition implies Pr(F? |f?) = 1,
leading to the following expression:
Pr(e?i|f?j) = Pr(E?i|F?j)Pr(e?i|T, f?j) (2)
where we have just two terms, namely a standard
phrase translation model based on the classified
parallel data, and an instance model assigning a
probability to each target instance given the source
class and the source instance. The latter helps us
choose among target words in combination with the
language model.
2.2 Advantages
This strategy has three advantages:
Better alignment. By reducing the number of
words to be considered during first word alignment
(auxiliary words in the classes disappear and no
inflected forms used), we lessen the data sparseness
problem and can obtain a better word alignment.
In a secondary step, one can learn word alignment
relationships inside aligned classes by realigning
them as a separate corpus, if that is desired.
Improvement of translation probabilities. By
considering many different phrases as different
instances of a single phrase class, we reduce the size
of our phrase-based (now class-based) translation
model and increase the number of occurrences of
each unit, producing a model Pr(E?|F? ) with less
perplexity.
68
Generalizing power. Phrases not occurring in
the training data can still be classified into a class,
and therefore be assigned a probability in the trans-
lation model. The new difficulty that rises is how to
produce the target phrase from the target class and
the source phrase, if this was not seen in training.
2.3 Difficulties
Two main difficulties2 are associated with this
strategy, which will hopefully lead to improved
translation performance if tackled conveniently.
Instance probability. On the one hand, when a
phrase of the test sentence is classified to a class,
and then translated, how do we produce the instance
of the target class given the tuple T and the source
instance? This problem is mathematically expressed
by the need to model the term of the Pr(e?i|T, f?j) in
Equation 2.
At the moment, we learn this model from relative
frequency across all tuples that share the same
source phrase, dividing the times we see the pair
(f?j, e?i) in the training by the times we see f?j .
Unseen instances. To produce a target instance
f? given the tuple T and an unseen e?, our idea is to
combine both the information of verb forms seen in
training and off-the-shelf knowledge for generation.
A translation memory can be built with all the seen
pairs of instances with their inflectional affixes
separated from base forms.
For example, suppose we translate from English
to Spanish and see the tuple T=(V[go],V[ir]) in
training, with the following instances:
I will go ire?
PRP(1S) will VB VB 1S F
you will go ira?s
PRP(2S) will VB VB 2S F
you will go vas
PRP(2S) will VB VB 2S P
2A third difficulty is the classification task itself, but we take
it for granted that this is performed by an independent system
based on other knowledge sources, and therefore out of scope
here.
where the second row is the analyzed form in terms
of person (1S: 1st singular, 2S: 2nd singular and
so on) and tense (VB: infinitive and P: present, F:
future). From these we can build a generalized rule
independent of the person ? PRP(X) will VB ? that
would enable us to translate ?we will go? to two
different alternatives (present and future form):
we will go VB 1P F
we will go VB 1P P
These alternatives can be weighted according to
the times we have seen each case in training. An un-
ambiguous form generator produces the forms ?ire-
mos? and ?vamos? for the two Spanish translations.
3 Classifying Verb Forms
As mentioned above, our first and basic implemen-
tation deals with verbs, which are classified unam-
biguously before alignment in training and before
translating a test.
3.1 Rules used
We perform a knowledge-based detection of verbs
using deterministic automata that implement a few
simple rules based on word forms, POS-tags and
word lemmas, and map the resulting expression to
the lemma of the head verb (see Figure 1 for some
rules and examples of detected verbs). This is done
both in the English and the Spanish side, and before
word alignment.
Note that we detect verbs containing adverbs and
negations (underlined in Figure 1), which are or-
dered before the verb to improve word alignment
with Spanish, but once aligned they are reordered
back to their original position inside the detected
verb, representing the real instance of this verb.
4 Experiments
In this section we present experiments with the
Spanish-English parallel corpus developed in the
framework of the LC-STAR project. This corpus
consists of transcriptions of spontaneously spoken
dialogues in the tourist information, appointment
scheduling and travel planning domain. Therefore,
sentences often lack correct syntactic structure. Pre-
processing includes:
69
 PP + V(L=have) {+RB} {+been} +V{G}
 V(L=have) {+not} +PP {+RB} {+been} +V{G}
 PP +V(L=be) {+RB} +VG
 V(L=be) {+not} +PP {+RB} +VG
 PP + MD(L=will/would/...) {+RB} +V
 MD(L=will/would/...) {+not} +PP {+RB} +V
 PP {+RB} +V
 V(L=do) {+not} +PP {+RB} +V
 V(L=be) {+not} +PP
  PP: Personal Pronoun
  V / MD / VG / RB: Verb / Modal / Gerund / Adverb (PennTree Bank POS)
  L: Lemma (or base form)
  { } / ( ): optionality / instantiation
Examples:
  leaves
  do you have
  did you come
  he has not attended
  have you ever been
  I will have
  she is going to be
  we would arrive
Figure 1: Some verb phrase detection rules and detected forms in English.
? Normalization of contracted forms for English
(ie. wouldn?t = would not, we?ve = we have)
? English POS-tagging using freely-available
TnT tagger (Brants, 2000), and lemmatization
using wnmorph, included in the WordNet pack-
age (Miller et al, 1991).
? Spanish POS-tagging using FreeLing analysis
tool (Carreras et al, 2004). This software also
generates a lemma or base form for each input
word.
4.1 Parallel corpus statistics
Table 1 shows the statistics of the data used, where
each column shows number of sentences, number of
words, vocabulary, and mean length of a sentence,
respectively.
sent. words vocab. Lmean
Train set
English 419113 5940 14.0
Spanish 29998 388788 9791 13.0
Test set
English 7412 963 14.8
Spanish 500 6899 1280 13.8
Table 1: LC-Star English-Spanish Parallel corpus.
There are 116 unseen words in the Spanish test
set (1.7% of all words), and 48 unseen words in the
English set (0.7% of all words), an expected big dif-
ference given the much more inflectional nature of
the Spanish language.
4.2 Verb Phrase Detection/Classification
Table 2 shows the number of detected verbs using
the detection rules presented in section 3.1, and the
number of different lemmas they map to. For the test
set, the percentage of unseen verb forms and lemmas
are also shown.
verbs unseen lemmas unseen
Train set
English 56419 768
Spanish 54460 911
Test set
English 1076 5.2% 146 4.7%
Spanish 1061 5.6% 171 4.7%
Table 2: Detected verb forms in corpus.
In average, detected English verbs contain 1.81
words, whereas Spanish verbs contain 1.08 words.
This is explained by the fact that we are including
the personal pronouns in English and modals for fu-
ture, conditionals and other verb tenses.
4.3 Word alignment results
In order to assess the quality of the word alignment,
we randomly selected from the training corpus 350
sentences, and a manual gold standard alignment
has been done with the criterion of Sure and Pos-
sible links, in order to compute Alignment Error
Rate (AER) as described in (Och and Ney, 2000) and
widely used in literature, together with appropriately
redefined Recall and Precision measures. Mathe-
matically, they can be expressed thus:
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER = 1 ? |A ? S| + |A ? P ||A| + |S|
70
where A is the hypothesis alignment and S is the
set of Sure links in the gold standard reference, and
P includes the set of Possible and Sure links in the
gold standard reference.
We have aligned our data using GIZA++ (Och,
2003) from English to Spanish and vice versa (per-
forming 5 iterations of model IBM1 and HMM, and
3 iterations of models IBM3 and IBM4), and have
evaluated two symmetrization strategies, namely the
union and the intersection, the union always rating
the best. Table 3 compares the result when aligning
words (current baseline), and when aligning classi-
fied verb phrases. In this case, after the alignment
we substitute the class for the original verb form and
each new word gets the same links the class had. Of
course, adverbs and negations are kept apart from
the verb and have separate links.
Recall Precision AER
baseline 74.14 86.31 20.07
with class. verbs 76.45 89.06 17.37
Table 3: Results in statistical alignment.
Results show a significant improvement in AER,
which proves that verbal inflected forms and auxil-
iaries do harm alignment performance in absence of
the proposed classification.
4.4 Translation results
We have integrated our classification strategy in an
SMT system which implements:
? Pr(e?i|f?k) as a tuples language model (Ngram),
as done in (Crego et al, 2004)
? Pr(e) as a standard Ngram language model us-
ing SRILM toolkit (Stolcke, 2002)
Parameters have been optimised for BLEU score
in a 350 sentences development set. Three refer-
ences are available for both development and test
sets. Table 4 presents a comparison of English to
Spanish translation results of the baseline system
and the configuration with classification (without
dealing with unseen instances). Results are promis-
ing, as we achieve a significant mWER error re-
duction, while still leaving about 5.6 % of the verb
forms in the test without translation. Therefore, we
expect a further improvement with the treatment of
unseen instances.
mWER BLEU
baseline 23.16 0.671
with class. verbs 22.22 0.686
Table 4: Results in English to Spanish translation.
5 Ongoing and future research
Ongoing research is mainly focused on developing
an appropriate generalization technique for unseen
instances and evaluating its impact in translation
quality.
Later, we expect to run experiments with a much
bigger parallel corpus such as the European Parlia-
ment corpus, in order to evaluate the improvement
due to morphological information for different sizes
of the training data. Advanced methods to compute
Pr(e?i|T, f?j) should also be tested (based on source
and target contextual features).
The next step will be to extend the approach to
other potential classes such as:
? Nouns and adjectives. A straightforward strat-
egy would classify all nouns and adjectives to
their base form, reducing sparseness.
? Simple Noun phrases. Noun phrases with or
without article (determiner), and with or with-
out preposition, could also be classified to the
base form of the head noun, leading to a fur-
ther reduction of the data sparseness, in a sub-
sequent stage. In this case, expressions like at
night, the night, nights or during
the night would all be mapped to the class
?night?.
? Temporal and numeric expressions. As they are
usually tackled in a preprocessing stage in cur-
rent SMT systems, we did not deal with them
here.
More on a long-term basis, ambiguous linguistic
classification could also be allowed and included in
the translation model. For this, incorporating statis-
tical classification tools (chunkers, shallow parsers,
phrase detectors, etc.) should be considered, and
evaluated against the current implementation.
71
6 Related Work
The approach to deal with inflected forms presented
in (Ueffing and Ney, 2003) is similar in that it also
tackles verbs in an English ? Spanish task. How-
ever, whereas the authors join personal pronouns
and auxiliaries to form extended English units and
do not transform the Spanish side, leading to an in-
creased English vocabulary, our proposal aims at re-
ducing both vocabularies by mapping all different
verb forms to the base form of the head verb.
An improvement in translation using IBM model
1 in an Arabic ? English task can be found in (Lee,
2004). From a processed Arabic text with all pre-
fixes and suffixes separated, the author determines
which of them should be linked back to the word
and which should not. However, no mapping to base
forms is performed, and plurals are still different
words than singulars.
In (Nie?en and Ney, 2004) hierarchical lexicon
models including base form and POS information
for translation from German into English are intro-
duced, among other morphology-based data trans-
formations. Finally, the same pair of languages is
used in (Corston-Oliver and Gamon, 2004), where
the inflectional normalization leads to improvements
in the perplexity of IBM translation models and re-
duces alignment errors. However, compound verbs
are not mentioned.
7 Conclusion
A proposal of linguistically classifying translation
phrases to improve statistical machine translation
performance has been presented. This classification
allows for a better translation modeling and a gen-
eralization to unseen forms. A preliminary imple-
mentation detecting verbs in an English ? Spanish
task has been presented. Experiments show a sig-
nificant improvement in word alignment, and in pre-
liminary translation results. Ongoing and future re-
search lines are discussed.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
X. Carreras, I. Chao, L. Padr o?, and M. Padr o?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
S. Corston-Oliver and M. Gamon. 2004. Normalizing
german and english inflectional morphology to im-
prove statistical word alignment. Proc. of the 6th
Conf. of the Association for Machine Translation in
the Americas, pages 48?57, October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
Y.S. Lee. 2004. Morphological analysis for statistical
machine translation. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Short
Papers, pages 57?60, Boston, Massachusetts, USA,
May. Association for Computational Linguistics.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
K. Miller, and R. Tengi. 1991. Five papers on word-
net. Special Issue of International Journal of Lexicog-
raphy, 3(4):235?312.
S. Nie?en and H. Ney. 2004. Statistical machine trans-
lation with scarce resources using morpho-syntactic
information. Computational Linguistics, 30(2):181?
204, June.
F.J. Och and H. Ney. 2000. Improved statistical align-
ment models. 38th Annual Meeting of the Association
for Computational Linguistics, pages 440?447, Octo-
ber.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
N. Ueffing and H. Ney. 2003. Using pos information for
smt into morphologically rich languages. 10th Conf.
of the European Chapter of the Association for Com-
putational Linguistics, pages 347?354, April.
R. Zens, F.J. Och, and H. Ney. 2004. Improvements
in phrase-based statistical machine translation. Proc.
of the Human Language Technology Conference, HLT-
NAACL?2004, pages 257?264, May.
72
Low?cost Named Entity Classification for Catalan: Exploiting
Multilingual Resources and Unlabeled Data
Llu??s Ma`rquez, Adria` de Gispert, Xavier Carreras, and Llu??s Padro?
TALP Research Center
Universitat Polite`cnica de Catalunya
Jordi Girona, 1?3, E-08034, Barcelona
 
lluism,agispert,carreras,padro  @talp.upc.es
Abstract
This work studies Named Entity Classi-
fication (NEC) for Catalan without mak-
ing use of large annotated resources of
this language. Two views are explored
and compared, namely exploiting solely
the Catalan resources, and a direct training
of bilingual classification models (Span-
ish and Catalan), given that a large col-
lection of annotated examples is available
for Spanish. The empirical results ob-
tained on real data point out that multi-
lingual models clearly outperform mono-
lingual ones, and that the resulting Cata-
lan NEC models are easier to improve by
bootstrapping on unlabelled data.
1 Introduction
There is a wide consensus about that Named Entity
Recognition and Classification (NERC) are Natural
Language Processing tasks which may improve the
performance of many applications, such as Informa-
tion Extraction, Machine Translation, Question An-
swering, Topic Detection and Tracking, etc. Thus,
interest on detecting and classifying those units in a
text has kept on growing during the last years.
Previous work in this topic is mainly framed in the
Message Understanding Conferences (MUC), de-
voted to Information Extraction, which included a
NERC competition task. More recent approaches
can be found in the proceedings of the shared task
at the 2002 and 2003 editions of the Conference
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
where several machine?learning (ML) systems were
compared at the NERC task for several languages.
One remarkable aspect of most widely used ML
algorithms is that they are supervised, that is, they
require a set of labelled data to be trained on. This
may cause a severe bottleneck when such data is not
available or is expensive to obtain, which is usu-
ally the case for minority languages with few pre?
existing linguistic resources and/or limited funding
possibilities. This is one of the main causes for
the recent growing interest on developing language?
independent NERC systems, which may be trained
from small training sets by taking advantage of un-
labelled examples (Collins and Singer, 1999; Abney,
2002), and which are easy to adapt to changing do-
mains (being all these aspects closely related).
This work focuses on exploring the construc-
tion of a low?cost Named Entity classification
(NEC) module for Catalan without making use of
large/expensive resources of the language. In doing
so, the paper first explores the training of classifi-
cation models by using only Catalan resources and
then proposes a training scheme, in which a Cata-
lan/Spanish bilingual classifier is trained directly
from a training set including examples of the two
languages. In both cases, the bootstrapping of the
resulting classifiers is also explored by using a large
unannotated Catalan corpus. The strategy used for
training the bilingual NE classification models has
been also applied with good results to NE recogni-
tion in (Carreras et al, 2003), a work that can be
considered complementary to this one.
When considering the training of bilingual mod-
els, we take advantage of the facts that Spanish
and Catalan are two Romance languages with sim-
ilar syntactic structure, and that ?since Spanish
and Catalan social and cultural environments greatly
overlap? many Named Entities appear in both lan-
guages corpora. Relying on this structural and con-
tent similarity, we will build our Catalan NE classi-
fier on the following assumptions: (a) Named Enti-
ties appear in the same contexts in both languages,
and (b) Named Entities are composed by similar pat-
terns in both languages.
The paper presents an extensive experimental
evaluation, giving strong evidence about the advan-
tage of using multilingual models for training on a
language with scarce resources. Additionally, the
Catalan NEC models resulting from the bilingual
training are easier to improve by bootstrapping on
unlabelled data.
The paper is organized as follows. Section 2
describes the Catalan and Spanish resources avail-
able and the feature codification of examples. Sec-
tion 3 briefly describes the learning algorithms used
to train the classifiers. Section 4 is devoted to the
learning of NEC modules using only Catalan re-
sources, while section 5 presents and evaluates the
bilingual approach. Finally, the main conclusions of
the work are summarized in section 6.
2 Setting
2.1 Corpus and data resources
The experimentation of this work has been carried
on two corpora, one for each language. Both corpora
consist of sentences extracted from news articles of
the year 2,000. The Catalan data, extracted from the
Catalan edition of the daily newspaper El Perio?dico
de Catalunya, has been randomly divided into three
sets: a training set (to train a system) and a test set
(to perform evaluation) for manual annotation, and
a remaining set left as unlabelled. The Spanish data
corresponds to the CoNLL 2002 Shared Task Span-
ish data, the original source being the EFE Spanish
Newswire Agency. The training set has been used
to improve classification for Catalan, whereas the
test set has been used to evaluate the bilingual classi-
fier. The original development set has not been used.
Table 1 shows the number of sentences, words and
lang. set #sent. #words #NEs
es train. 8,322 264,715 18,797
es test 1,516 51,533 3,558
ca train. 817 23,177 1,232
ca test 844 23,595 1,338
ca unlab. 83,725 2,201,712 75,038 
Table 1: Sizes of Spanish and Catalan data sets
Named Entities in each set. Although a large amount
of Catalan unlabelled NEs is available, it must be ob-
served that these are automatically recognised with a
91.5% accurate NER module, introducing a certain
error that might undermine bootstrapping results.
Considered classes include MUC categories PER
LOC and ORG, plus a fourth category MIS, includ-
ing named entities such as documents, measures and
taxes, sport competitions, titles of art works and oth-
ers. For Catalan, we find 33.0% of PER, 17.1% of
LOC, 43.5% of ORG and 6.4% of MIS out of the
2,570 manually annotated NEs, whereas for Span-
ish, out of the 22,355 labelled NEs, 22.6% are PER,
26.8% are LOC, 39.4% are ORG and the remaining
11.2% are MIS.
Additionally, we used a Spanish 7,427 trigger?
word list typically accompanying persons, organiza-
tions, locations, etc., and an 11,951 entry gazetteer
containing geographical and person names. These
lists have been semi-automatically extracted from
lexical resources and manually enriched afterwards.
They have been used in some previous works allow-
ing significant improvements for the Spanish NERC
task (Carreras et al, 2002; Carreras et al, 2003).
Trigger?words are annotated with the correspond-
ing Spanish synsets in the EuroWordNet lexical
knowledge base. Since there are translation links
among Spanish and Catalan (and other languages)
for the majority of these words, an equivalent ver-
sion of the trigger?word list for Catalan has been
automatically derived. In this work, we consider
the gazetteer as a language independent resource and
is indistinctly used for training Catalan and Spanish
models.
2.2 Feature codification
The features that characterise the NE examples are
defined in a window  anchored at a word  , repre-
senting its local context used by a classifier to make
a decision. In the window, each word around  is
codified with a set of primitive features, requiring no
linguistic pre?processing, together with its relative
position to  . Each primitive feature with each rela-
tive position and each possible value forms a final bi-
nary feature for the classifier (e.g., ?the word form
at position(-2) is street?). The kind of information
coded in these features may be grouped in the fol-
lowing kinds:
 Lexical: Word forms and their position in the
window (e.g., 	
 =?bank?), as well as word
forms appearing in the named entity under con-
sideration, independent from their position.
 Orthographic: Word properties regarding
how it is capitalised (initial-caps, all-caps),
the kind of characters that form the word
(contains-digits, all-digits, alphanumeric,
roman-number), the presence of punctua-
tion marks (contains-dots, contains-hyphen,
acronym), single character patterns (lonely-
initial, punctuation-mark, single-char), or the
membership of the word to a predefined class
(functional-word1) or pattern (URL).
 Affixes: The prefixes and suffixes up to 4 char-
acters of the NE being classified and its internal
components.
 Word Type Patterns: Type pattern of consec-
utive words in the context. The type of a word
is either functional (f), capitalised (C), lower-
cased (l), punctuation mark (.), quote (?) or
other (x).
 Bag-of-Words: Form of the words in the
window, without considering positions (e.g.,
?bank?  ).
 Trigger Words: Triggering properties of win-
dow words, using an external list to deter-
mine whether a word may trigger a certain
Named Entity (NE) class (e.g., ?president? may
trigger class PER). Also context patterns to
the left of the NE are considered, where each
word is marked with its triggering properties,
or with a functional?word tag, if appropriate
(e.g., the phrase ?the president of United Na-
tions? produces pattern f ORG f for the NE
1Functional words are determiners and prepositions which
typically appear inside NEs.
?United Nations?, assuming that ?president? is
listed as a possible trigger for ORG).
 Gazetteer Features: Gazetteer information for
window words. A gazetteer entry consists of a
set of possible NE categories.
 Additionally, binary features encoding the
length in words of the NE being classified.
All features are computed for a  -3,+3  window
around the NE being classified, except for the Bag-
of-Words, for which a  -5,+5  window is used.
3 Learning Algorithms
As previously said, we compare two learning ap-
proaches when learning from Catalan examples: su-
pervised (using the AdaBoost algorithm), and unsu-
pervised (using the Greedy Agreement Algorithm).
Both of them are briefly described below.
3.1 Supervised Learning
We use the multilabel multiclass AdaBoost.MH
algorithm (with confidence?rated predictions) for
learning the classification models. The idea of this
algorithm is to learn an accurate strong classifier by
linearly combining, in a weighted voting scheme,
many simple and moderately?accurate base classi-
fiers or rules. Each base rule is sequentially learned
by presenting the base learning algorithm a weight-
ing over the examples (denoting importance of ex-
amples), which is dynamically adjusted depending
on the behaviour of the previously learned rules. We
refer the reader to (Schapire and Singer, 1999) for
details about the general algorithm, and to (Schapire,
2002) for successful applications to many areas, in-
cluding several NLP tasks. Additionally, a NERC
system based on the AdaBoost algorithm obtained
the best results in the CoNLL?02 Shared Task com-
petition (Carreras et al, 2002).
In our setting, the boosting algorithm combines
several small fixed?depth decision trees. Each
branch of a tree is, in fact, a conjunction of binary
features, allowing the strong boosting classifier to
work with complex and expressive rules.
3.2 Unsupervised Learning
We have implemented the Greedy Agreement Algo-
rithm (Abney, 2002) which, based on two indepen-
dent views of the data, is able to learn two binary
classifiers from a set of hand-typed seed rules. Each
classifier is a majority vote of several atomic rules,
which abstains when the voting ends in a tie. The
atomic rules are just mappings of a single feature
into a class (e.g., if suffix ?lez? then PER). When
learning, the atomic rule that maximally reduces the
disagreement on unlabelled data between both clas-
sifiers is added to one of the classifiers, and the
process is repeated alternating the classifiers. See
(Abney, 2002) for a formal proof that this algo-
rithm tends to gradually reduce the classification er-
ror given the adequate seed rules.
For its extreme simplicity and potentially good re-
sults, this algorithm is very appealing for the NEC
task. In fact, results are reported to be competitive
against more sophisticated methods (Co-DL, Co-
Boost, etc.) for this specific task in (Abney, 2002).
Three important questions arise from the algo-
rithm. First, what features compose each view. Sec-
ond, how seed rules should be selected or whether
this selection strongly affects the final classifiers.
Third, how the algorithm, presented in (Abney,
2002) for binary classification, can be extended to
a multiclass problem.
In order to answer these questions and gain some
knowledge on how the algorithm works empirically,
we performed initial experiments on the big labelled
portion of the Spanish data.
When it comes to view selection, we tried two
alternatives. The first, suggested in (Collins and
Singer, 1999; Abney, 2002), divides into one view
capturing internal features of the NE, and the other
capturing features of its left-right contexts (here-
after referred to as Greedy Agreement pure, or GA  ).
Since the contextual view turned out to be quite lim-
ited in performance, we interchanged some feature
groups between the views. Specifically, we moved
the Lexical features independent of their position to
the contextual view, and the the Bag-of-Words fea-
tures to the internal one (we will refer to this divi-
sion as Greedy Agreement mixed, or GA  ). The lat-
ter, containing redundant and conditionally depen-
dent features, yielded slightly better results in terms
of precision?coverage trade?off.
As for seed rules selection, we have tried two dif-
ferent strategies. On the one hand, blindly choos-
ing as many atomic rules as possible that decide at
least in 98% of the cases for a class in a small vali-
dation set of labelled data, and on the other, manu-
ally selecting from these atomic rules only those that
might be valid still for a bigger data set. This second
approach proved empirically better, as it provided a
much higher starting point in the test set (in terms
of precision), whereas a just slightly lower coverage
value, presenting a better learning curve.
Finally, we have approached the multiclass set-
ting by a one?vs?all binarization, that is, divid-
ing the classification problem into four binary de-
cisions (one per class), and combining the resul-
tant rules. Several techniques to combine them have
been tested, from making a prediction only when
one classifier assigns positive for the given instance
and all other classifiers assign negative (very high
precision, low coverage), to much unrestrictive ap-
proaches, such as combining all votes from each
classifier (lower precision, higher coverage). Re-
sults proved that the best approach is to sum all votes
from all non-abstaining binary classifiers, where a
vote of a concrete classifier for the negative class is
converted to one vote for each of the other classes.
The best results obtained in terms of cover-
age/precision and evaluated over the whole set of
training data (and thus more significant than over a
small test set) are 80.7/84.9. These results are com-
parable to the ones presented in (Abney, 2002), tak-
ing into account, apart from the language change,
that we have introduced a fourth class to be treated
the same as the other three. Results when using
Catalan data are presented in section 4.
4 Using only Catalan resources
This section describes the results obtained by using
only the Catalan resources and comparing the fully
unsupervised Greedy Agreement algorithm with the
AdaBoost supervised learning algorithm.
4.1 Unsupervised vs. supervised learning
In this experiment, we used the Catalan training set
for extracting seed rules of the GA algorithm and to
train an AdaBoost classifier. The whole unlabelled
Catalan corpus was used for bootstrapping the GA
algorithm. All the results were computed over the
Catalan test set.
Figure 1 shows a precision?coverage plot of
AdaBoost (noted as CA, for CAtalan training) and
20
30
40
50
60
70
80
90
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n
Coverage
CA
GA(p)
GA(m)
Figure 1: Precision?coverage plot of GA  , GA  , and
CA models trained on Catalan resources
the Greedy Agreement algorithm for the two views
selections (noted GA  and GA  , respectively). The
curve for CA has been computed by varying a confi-
dence threshold: CA abstains when the highest pre-
diction of AdaBoost is lower than this threshold.
On the one hand, it can be seen that GA  is more
precise than GA  for low values of coverage but their
asymptotic behaviour is quite similar. By stopping
at the best point in the validation set, the Greedy
Agreement algorithm (GA  ) achieves a precision of
76.53% with a coverage of 83.62% on the test set.
On the other hand, the AdaBoost classifier clearly
outperforms both GA models at all levels of cover-
age, indicating that the supervised training is prefer-
able even when using really small training sets (an
accuracy around 70% is obtained by training Ad-
aBoost only with the 20% of the learning examples,
i.e., 270 examples).
The first three rows of table 2 contain the accu-
racy of these systems (i.e., precision when coverage
is 100%), detailed at the NE type level (best results
printed in boldface)2. The fourth row (BTS) corre-
sponds to the best results obtained when additional
unlabelled Catalan examples are taken into account,
as explained below.
It can be observed that the GA models are highly
biased towards the most frequent NE types (ORG and
PER) and that the accuracy achieved on the less rep-
2In order to obtain a 100% coverage with the GA models we
have introduced a naive algorithm for breaking ties in favour of
the most frequent categories, in the cases in which the algorithm
abstains.
LOC ORG PER MIS avg.
GA  14.66 83.64 93.88 0.00 66.66
GA  20.67 95.30 76.94 4.00 68.28
CA 61.65 86.84 91.67 40.00 79.83
BTS 65.41 87.22 91.94 37.33 80.63
Table 2: Accuracy results of all models trained on
Catalan resources
resented categories is very low for LOC and negli-
gible for MIS. The MIS category is rather difficult
to learn (also for the supervised algorithm), proba-
bly because it does not account for any concrete NE
type and does not show many regularities. Consid-
ering this fact, we learned the models using only the
LOC, ORG, and PER categories and treated the MIS
as a default value (assigned whenever the classifier
does not have enough evidence for any of the cate-
gories). The results obtained were even worse.
4.2 Bootstrapping AdaBoost models using
unlabelled examples
Ideally, the supervised approach can be boosted by
using the unlabelled Catalan examples in a kind of
iterative bootstrapping procedure. We have tested
a quite simple strategy for bootstrapping. The
unlabelled data in Catalan has been randomly di-
vided into a number of equal?size disjoint subsets

. . .

, containing 1,000 sentences each. Given
the initial training set for Catalan, noted as  , the
process is as follows:
1. Learn the ffProceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Statistical Machine Translation of Euparl Data by using Bilingual N-grams
Rafael E. Banchs Josep M. Crego Adria` de Gispert
Department of Signal Theory and Communications
Universitat Polite`cnica de Catalunya, Barcelona 08034, Spain
{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu
Patrik Lambert Jose? B. Marin?o
Abstract
This work discusses translation results for
the four Euparl data sets which were made
available for the shared task ?Exploit-
ing Parallel Texts for Statistical Machine
Translation?. All results presented were
generated by using a statistical machine
translation system which implements a
log-linear combination of feature func-
tions along with a bilingual n-gram trans-
lation model.
1 Introduction
During the last decade, statistical machine transla-
tion (SMT) systems have evolved from the orig-
inal word-based approach (Brown et al, 1993)
into phrase-based translation systems (Koehn et al,
2003). Similarly, the noisy channel approach has
been expanded to a more general maximum entropy
approach in which a log-linear combination of mul-
tiple models is implemented (Och and Ney, 2002).
The SMT approach used in this work implements
a log-linear combination of feature functions along
with a translation model which is based on bilingual
n-grams. This translation model was developed by
de Gispert and Marin?o (2002), and it differs from the
well known phrase-based translation model in two
basic issues: first, training data is monotonously seg-
mented into bilingual units; and second, the model
considers n-gram probabilities instead of relative
frequencies. This model is described in section 2.
Translation results from the four source languages
made available for the shared task (es: Spanish, fr:
French, de: German, and fi: Finnish) into English
(en) are presented and discussed.
The paper is structured as follows. Section 2 de-
scribes the bilingual n-gram translation model. Sec-
tion 3 presents a brief overview of the whole SMT
procedure. Section 4 presents and discusses the
shared task results and other interesting experimen-
tation. Finally, section 5 presents some conclusions
and further work.
2 Bilingual N-gram Translation Model
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units which
are referred to as tuples (de Gispert and Marin?o,
2002). This model approximates the joint probabil-
ity between source and target languages by using 3-
grams as it is described in the following equation:
p(T, S) ?
N
?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to the
nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, the produced segmentation is maximal in the
sense that no smaller tuples can be extracted with-
out violating the previous constraint (Crego et al,
2004). According to this, tuple extraction provides a
unique segmentation for a given bilingual sentence
pair alignment. Figure 1 illustrates this idea with a
simple example.
133
We would like to achieve perfect translations
NULL quisieramos lograr traducciones perfectas
t1 t2 t3 t4
Figure 1: Example of tuple extraction from an
aligned sentence pair.
Two important issues regarding this translation
model must be mentioned. First, when extracting
tuples, some words always appear embedded into tu-
ples containing two or more words, so no translation
probability for an independent occurrence of such
words exists. To overcome this problem, the tuple
3-gram model is enhanced by incorporating 1-gram
translation probabilities for all the embedded words
(de Gispert et al, 2004).
Second, some words linked to NULL end up pro-
ducing tuples with NULL source sides. This cannot
be allowed since no NULL is expected to occur in a
translation input. This problem is solved by prepro-
cessing alignments before tuple extraction such that
any target word that is linked to NULL is attached
to either its precedent or its following word.
3 SMT Procedure Description
This section describes the procedure followed for
preprocessing the data, training the models and op-
timizing the translation system parameters.
3.1 Preprocessing and Alignment
The Euparl data provided for this shared task (Eu-
parl, 2003) was preprocessed for eliminating all sen-
tence pairs with a word ratio larger than 2.4. As a
result of this preprocessing, the number of sentences
in each training set was slightly reduced. However,
no significant reduction was produced.
In the case of French, a re-tokenizing procedure
was performed in which all apostrophes appearing
alone were attached to their corresponding words.
For example, pairs of tokens such as l ? and qu ?
were reduced to single tokens such as l? and qu?.
Once the training data was preprocessed, a word-
to-word alignment was performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2000). As an approxi-
mation to the most probable alignment, the Viterbi
alignment was considered. Then, the intersection
and union of alignment sets in both directions were
computed for each training set.
3.2 Feature Function Computation
The considered translation system implements a to-
tal of five feature functions. The first of these mod-
els is the tuple 3-gram model, which was already de-
scribed in section 2. Tuples for the translation model
were extracted from the union set of alignments as
shown in Figure 1. Once tuples had been extracted,
the tuple vocabulary was pruned by using histogram
pruning. The same pruning parameter, which was
actually estimated for Spanish-English, was used for
the other three language pairs. After pruning, the
tuple 3-gram model was trained by using the SRI
Language Modeling toolkit (Stolcke, 2002). Finally,
the obtained model was enhanced by incorporating
1-gram probabilities for the embedded word tuples,
which were extracted from the intersection set of
alignments.
Table 1 presents the total number of running
words, distinct tokens and tuples, for each of the four
training data sets.
Table 1: Total number of running words, distinct to-
kens and tuples in training.
source running distinct tuple
language words tokens vocabulary
Spanish 15670801 113570 1288770
French 14844465 78408 1173424
German 15207550 204949 1391425
Finnish 11228947 389223 1496417
The second feature function considered was a tar-
get language model. This feature actually consisted
of a word 3-gram model, which was trained from the
target side of the bilingual corpus by using the SRI
Language Modeling toolkit.
The third feature function was given by a word
penalty model. This function introduces a sentence
length penalization in order to compensate the sys-
134
tem preference for short output sentences. More
specifically, the penalization factor was given by the
total number of words contained in the translation
hypothesis.
Finally, the fourth and fifth feature functions cor-
responded to two lexicon models based on IBM
Model 1 lexical parameters p(t|s) (Brown et al,
1993). These lexicon models were calculated for
each tuple according to the following equation:
plexicon((t, s)n) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(tin|sjn) (2)
where sjn and tin are the jth and ith words in the
source and target sides of tuple (t, s)n, being J and
I the corresponding total number words in each side
of it.
The forward lexicon model uses IBM Model 1 pa-
rameters obtained from source-to-target algnments,
while the backward lexicon model uses parameters
obtained from target-to-source alignments.
3.3 Decoding and Optimization
The search engine for this translation system was
developed by Crego et al (2005). It implements
a beam-search strategy based on dynamic program-
ming and takes into account all the five feature func-
tions described above simultaneously. It also allows
for three different pruning methods: threshold prun-
ing, histogram pruning, and hypothesis recombina-
tion. For all the results presented in this work the
decoder?s monotonic search modality was used.
An optimization tool, which is based on a simplex
method (Press et al, 2002), was developed and used
for computing log-linear weights for each of the fea-
ture functions described above. This algorithm ad-
justs the log-linear weights so that BLEU (Papineni
et al, 2002) is maximized over a given development
set. One optimization for each language pair was
performed by using the 2000-sentence development
sets made available for the shared task.
4 Shared Task Results
Table 2 presents the BLEU scores obtained for the
shared task test data. Each test set consisted of 2000
sentences. The computed BLEU scores were case
insensitive and used one translation reference.
Table 2: BLEU scores (shared task test sets).
es - en fr - en de - en fi - en
0.3007 0.3020 0.2426 0.2031
As can be seen from Table 2 the best ranked trans-
lations were those obtained for French, followed by
Spanish, German and Finnish. A big difference is
observed between the best and the worst results.
Differences can be observed from translation out-
puts too. Consider, for example, the following seg-
ments taken from one of the test sentences:
es-en: We know very well that the present Treaties are not
enough and that , in the future , it will be necessary to develop
a structure better and different for the European Union...
fr-en: We know very well that the Treaties in their current
are not enough and that it will be necessary for the future to
develop a structure more effective and different for the Union...
de-en: We very much aware that the relevant treaties are
inadequate and , in future to another , more efficient structure
for the European Union that must be developed...
fi-en: We know full well that the current Treaties are not
sufficient and that , in the future , it is necessary to develop the
Union better and a different structure...
It is evident from these translation outputs that
translation quality decreases when moving from
Spanish and French to German and Finnish. A
detailed observation of translation outputs reveals
that there are basically two problems related to this
degradation in quality. The first has to do with re-
ordering, which seems to be affecting Finnish and,
specially, German translations.
The second problem has to do with vocabulary. It
is well known that large vocabularies produce data
sparseness problems (Koehn, 2002). As can be con-
firmed from Tables 1 and 2, translation quality de-
creases as vocabulary size increases. However, it is
not clear yet, in which degree such degradation is
due to monotonic decoding and/or vocabulary size.
Finally, we also evaluated how much the full fea-
ture function system differs from the baseline tu-
ple 3-gram model alone. In this way, BLEU scores
were computed for translation outputs obtained for
the baseline system and the full system. Since the
English reference for the test set was not available,
we computed translations and BLEU scores over de-
135
velopment sets. Table 3 presents the results for both
the full system and the baseline.1
Table 3: Baseline- and full-system BLEU scores
(computed over development sets).
language pair baseline full
es - en 0.2588 0.3004
fr - en 0.2547 0.2938
de - en 0.1844 0.2350
fi - en 0.1526 0.1989
From Table 3, it is evident that the four additional
feature functions produce important improvements
in translation quality.
5 Conclusions and Further Work
As can be concluded from the presented results, per-
formance of the translation system used is much bet-
ter for French and Spanish than for German and
Finnish. As some results suggest, reordering and
vocabulary size are the most important problems re-
lated to the low translation quality achieved for Ger-
man and Finnish.
It is also evident that the bilingual n-gram model
used requires the additional feature functions to pro-
duce better translations. However, more experimen-
tation is required in order to fully understand each
individual feature?s influence on the overall log-
linear model performance.
6 Acknowledgments
This work has been funded by the European Union
under the integrated project TC-STAR - Technology
and Corpora for Speech to Speech Translation -(IST-
2002-FP6-506738, http://www.tc-star.org).
The authors also want to thank Jose? A. R. Fonol-
losa and Marta Ruiz Costa-jussa` for their participa-
tion in discussions related to this work.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. ?The mathemat-
1Differently from BLEU scores presented in Table 2, which
are case insensitive, BLEU scores presented in Table 3 are case
sensitive.
ics of statistical machine translation: parameter esti-
mation?. Computational Linguistics, 19(2):263?311.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2004. ?Finite-state-based and phrase-based statistical
machine translation?. Proc. of the 8th Int. Conf. on
Spoken Language Processing, :37?40, October.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. ?A Ngram-based Statistical Machine Transla-
tion Decoder?. Submitted to INTERSPEECH 2005.
Adria` de Gispert, and Jose? B. Marin?o. 2002. ?Using X-
grams for speech-to-speech translation?. Proc. of the
7th Int. Conf. on Spoken Language Processing.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2004. ?TALP: Xgram-based spoken language transla-
tion system?. Proc. of the Int. Workshop on Spoken
Language Translation, :85?90. Kyoto, Japan, October.
EUPARL: European Parliament Proceedings Parallel
Corpus 1996-2003. Available on-line at: http://
people.csail.mit.edu/people/koehn/public
ations/europarl/
Philipp Koehn. 2002. ?Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation?. Avail-
able on-line at: http://people.csail.mit.edu/
people/koehn/publications/europarl/
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
?Statistical phrase-based translation?. Proc. of the
2003 Meeting of the North American chapter of the
ACL, Edmonton, Alberta.
Franz J. Och and Hermann Ney. 2000. ?Improved statis-
tical alignment models?. Proc. of the 38th Ann. Meet-
ing of the ACL, Hong Kong, China, October.
Franz J. Och and Hermann Ney. 2002. ?Discriminative
training and maximum entropy models for statistical
machine translation?. Proc. of the 40th Ann. Meeting
of the ACL, :295?302, Philadelphia, PA, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. ?Bleu: a method for automatic eval-
uation of machine translation?. Proc. of the 40th Ann.
Conf. of the ACL, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing, Cambridge
University Press.
Andreas Stolcke. 2002. ?SRLIM: an extensible language
modeling toolkit?. Proc. of the Int. Conf. on Spoken
Language Processing :901?904, Denver, CO, Septem-
ber. Available on line at: http://www.speech.sr
i.com/projects/srilm/
136
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Coling 2008: Companion volume ? Posters and Demonstrations, pages 19?22
Manchester, August 2008
Phrasal Segmentation Models for Statistical Machine Translation
Graeme Blackwood, Adri
`
a de Gispert, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
Phrasal segmentation models define a
mapping from the words of a sentence
to sequences of translatable phrases. We
discuss the estimation of these models
from large quantities of monolingual train-
ing text and describe their realization as
weighted finite state transducers for incor-
poration into phrase-based statistical ma-
chine translation systems. Results are re-
ported on the NIST Arabic-English trans-
lation tasks showing significant comple-
mentary gains in BLEU score with large
5-gram and 6-gram language models.
1 Introduction
In phrase-based statistical machine transla-
tion (Koehn et al, 2003) phrases extracted from
word-aligned parallel data are the fundamental
unit of translation. Each phrase is a sequence
of contiguous translatable words and there is no
explicit model of syntax or structure.
Our focus is the process by which a string of
words is segmented as a sequence of such phrases.
Ideally, the segmentation process captures two as-
pects of natural language. Firstly, segmentations
should reflect the underlying grammatical sentence
structure. Secondly, common sequences of words
should be grouped as phrases in order to preserve
context and respect collocations. Although these
aspects of translation are not evaluated explicitly,
phrases have been found very useful in transla-
tion. They have the advantage that, within phrases,
words appear as they were found in fluent text.
However, reordering of phrases in translation can
lead to disfluencies. By defining a distribution over
possible segmentations, we hope to address such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
disfluencies. A strength of our approach is that it
exploits abundantly available monolingual corpora
that are usually only used for training word lan-
guage models.
Most prior work on phrase-based statistical lan-
guage models concerns the problem of identifying
useful phrasal units. In (Ries et al, 1996) an iter-
ative algorithm selectively merges pairs of words
as phrases with the goal of minimising perplex-
ity. Several criteria including word pair frequen-
cies, unigram and bigram log likelihoods, and a
correlation coefficient related to mutual informa-
tion are compared in (Kuo and Reichl, 1999). The
main difference between these approaches and the
work described here is that we already have a defi-
nition of the phrases of interest (i.e. the phrases of
the phrase table extracted from parallel text) and
we focus instead on estimating a distribution over
the set of possible alternative segmentations of the
sentence.
2 Phrasal Segmentation Models
Under the generative model of phrase-based statis-
tical machine translation, a source sentence s
I
1
gen-
erates sequences u
K
1
= u
1
, . . . , u
K
of source lan-
guage phrases that are to be translated. Sentences
cannot be segmented into phrases arbitrarily: the
space of possible segmentations is constrained by
the contents of the phrase table which consists of
phrases found with translations in the parallel text.
We start initially with a distribution in which seg-
mentations assume the following dependencies:
P (u
K
1
,K|s
I
1
) = P (u
K
1
|K, s
I
1
)P (K|I). (1)
The distribution over the number of phrases K is
chosen to be uniform, i.e. P (K|I) = 1/I, K ?
{1, 2, . . . , I}, and all segmentations are considered
equally likely. The probability of a particular seg-
mentation is therefore
P (u
K
1
|K, s
I
1
) =
{
C(K, s
I
1
) if u
K
1
= s
I
1
0 otherwise
(2)
19
where C(K, s
I
1
) is chosen to ensure normalisation
and the phrases u
1
, . . . , u
K
are found in the phrase
table. This simple model of segmentation has been
found useful in practice (Kumar et al, 2006).
Our goal is to improve upon the uniform seg-
mentation of equation (2) by estimating the phrasal
segmentation model parameters from naturally oc-
curing phrase sequences in a large monolingual
training corpus. An order-n phrasal segmentation
model assigns a probability to a phrase sequence
u
K
1
according to
P (u
K
1
|K, s
I
1
) =
K
?
k=1
P (u
k
|u
k?1
1
,K, s
I
1
) ?
{
C(K, s
I
1
)
?
K
k=1
P (u
k
|u
k?1
k?n+1
) if u
K
1
= s
I
1
0 otherwise
(3)
where the approximation is due to the Markov as-
sumption that only the most recent n ? 1 phrases
are useful when predicting the next phrase. Again,
each u
k
must be a phrase with a known transla-
tion. For a fixed sentence s
I
1
, the normalisation
term C(K, s
I
1
) can be calculated. In translation,
however, calculating this quantity becomes harder
since the s
I
1
are not fixed. We therefore ignore
the normalisation and use the unnormalised like-
lihoods as scores.
2.1 Parameter Estimation
We focus on first-order phrasal segmentation mod-
els. Although we have experimented with higher-
order models we have not yet found them to yield
improved translation.
Let f(u
k?1
, u
k
) be the frequency of occurrence
of a string of words w
j
i
in a very large training
corpus that can be split at position x such that
i < x ? j and the substrings w
x?1
i
and w
j
x
match
precisely the words of two phrases u
k?1
and u
k
in
the phrase table. The maximum likelihood proba-
bility estimate for phrase bigrams is then their rel-
ative frequency:
?
P (u
k
|u
k?1
) =
f(u
k?1
, u
k
)
f(u
k?1
)
. (4)
These maximum likelihood estimates are dis-
counted and smoothed with context-dependent
backoff such that
P (u
k
|u
k?1
) =
{
?(u
k?1
, u
k
)
?
P (u
k
|u
k?1
) if f(u
k?1
, u
k
) > 0
?(u
k?1
)P (u
k
) otherwise
(5)
where ?(u
k?1
, u
k
) discounts the maximum like-
lihood estimates and the context-specific backoff
weights ?(u
k?1
) are chosen to ensure normalisa-
tion.
3 The Transducer Translation Model
The Transducer Translation Model (TTM) (Kumar
and Byrne, 2005; Kumar et al, 2006) is a gener-
ative model of translation that applies a series of
transformations specified by conditional probabil-
ity distributions and encoded as Weighted Finite
State Transducers (Mohri et al, 2002).
The generation of a target language sentence
t
J
1
starts with the generation of a source lan-
guage sentence s
I
1
by the source language model
P
G
(s
I
1
). Next, the source language sentence is
segmented according to the uniform phrasal seg-
mentation model distribution P
W
(u
K
1
,K|s
I
1
) of
equation (2). The phrase translation and reorder-
ing model P
?
(v
R
1
|u
K
1
) generates the reordered se-
quence of target language phrases v
R
1
. Finally,
the reordered target language phrases are trans-
formed to word sequences t
J
1
under the target
segmentation model P
?
(t
J
1
|v
R
1
). These compo-
nent distributions together form a joint distribu-
tion over the source and target language sentences
and their possible intermediate phrase sequences
as P (t
J
1
, v
R
1
, u
K
1
, s
I
1
).
In translation under the generative model, we
start with the target sentence t
J
1
in the foreign lan-
guage and search for the best source sentence s?
I
1
.
Encoding each distribution as a WFST leads to a
model of translation as a series of compositions
L = G ?W ? ? ? ? ? T (6)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations
obtained during decoding. The most likely trans-
lation s?
I
1
is the path in L with least cost.
The above approach generates a word lattice L
under the unweighted phrasal segmentation model
of equation (2). In the initial experiments reported
here, we apply the weighted phrasal segmentation
model via lattice rescoring. We take the word lat-
tice L and compose it with the unweighted trans-
ducer W to obtain a lattice of phrases L ?W ; this
lattice contains phrase sequences and translation
scores consistent with the initial translation. We
also extract the complete list of phrases relevant to
each translation.
20
We then wish to apply the phrasal segmentation
model distribution of equation (3) to this phrase
lattice. The conditional probabilities and backoff
structure defined in equation (5) can be encoded
as a weighted finite state acceptor (Allauzen et al,
2003). In this acceptor, ?, states encode histories
and arcs define the bigram and backed-off unigram
phrase probabilities. We note that the raw counts
of equation (4) are collected prior to translation
and the first-order probabilities are estimated only
for phrases found in the lattice.
The phrasal segmentation model is composed
with the phrase lattice and projected on the in-
put to obtain the rescored word lattice L
?
=
(L ?W ) ??. The most likely translation after ap-
plying the phrasal segmentation model is found as
the path in L
?
with least cost. Apart from likeli-
hood pruning when generating the original word
lattice, the model scores are included correctly in
translation search.
4 System Development
We describe experiments on the NIST Arabic-
English machine translation task and apply phrasal
segmentation models in lattice rescoring.
The development set mt02-05-tune is formed
from the odd numbered sentences of the NIST
MT02?MT05 evaluation sets; the even numbered
sentences form the validation set mt02-05-test.
Test performance is evaluated using the NIST sub-
sets from the MT06 evaluation: mt06-nist-nw for
newswire data and mt06-nist-ng for newsgroup
data. Results are also reported for the MT08 evalu-
ation. Each set contains four references and BLEU
scores are computed for lower-case translations.
The uniformly segmented TTM baseline system
is trained using all of the available Arabic-English
data for the NIST MT08 evaluation
1
. In first-pass
translation, decoding proceeds with a 4-gram lan-
guage model estimated over the parallel text and a
965 million word subset of monolingual data from
the English Gigaword Third Edition. Minimum
error training (Och, 2003) under BLEU optimises
the decoder feature weights using the development
set mt02-05-tune. In the second pass, 5-gram and
6-gram zero-cutoff stupid-backoff (Brants et al,
2007) language models estimated using 4.7 billion
words of English newswire text are used to gener-
ate lattices for phrasal segmentation model rescor-
ing. The phrasal segmentation model parameters
1
http://www.nist.gov/speech/tests/mt/2008/
mt02-05-tune mt02-05-test
TTM+MET 48.9 48.6
+6g 51.9 51.7
+6g+PSM 52.7 52.7
Table 2: BLEU scores for phrasal segmentation
model rescoring of 6-gram rescored lattices.
are trained using a 1.8 billion word subset of the
same monolingual training data used to build the
second-pass word language model. A phrasal seg-
mentation model scale factor and phrase insertion
penalty are tuned using the development set.
5 Results and Analysis
First-pass TTM translation lattices generated with
a uniform segmentation obtain baseline BLEU
scores of 48.9 for mt02-05-tune and 48.6 for
mt02-05-test. In our experiments we demon-
strate that phrasal segmentation models continue
to improve translation even for second-pass lat-
tices rescored with very large zero-cutoff higher-
order language models. Table 1 shows phrasal seg-
mentation model rescoring of 5-gram lattices. The
phrasal segmentation models consistently improve
the BLEU score: +1.1 for both the development
and validation sets, and +1.4 and +0.4 for the in-
domain newswire and out-of-domain newsgroup
test sets. Rescoring MT08 gives gains of +0.9 on
mt08-nist-nw and +0.3 on mt08-nist-ng.
For a limited quantity of training data it is not
always possible to improve translation quality sim-
ply by increasing the order of the language model.
Comparing tables 1 and 2 shows that the gains in
moving from a 5-gram to a 6-gram are small. Even
setting aside the practical difficulty of estimating
and applying such higher-order language models,
it is doubtful that further gains could be had simply
by increasing the order. That the phrasal segmenta-
tion models continue to improve upon the 6-gram
lattice scores suggests they capture more than just
a longer context and that they are complementary
to word-based language models.
The role of the phrase insertion penalty is to
encourage longer phrases in translation. Table 3
shows the effect of tuning this parameter. The
upper part of the table shows the BLEU score,
brevity penalty and individual n-gram precisions.
The lower part shows the total number of words
in the output, the number of words translated as
a phrase of the specified length, and the average
number of words per phrase. When the insertion
21
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08-nist-nw mt08-nist-ng
TTM+MET 48.9 48.6 46.1 35.2 48.4 33.7
+5g 51.5 51.5 48.4 36.7 49.1 36.4
+5g+PSM 52.6 52.6 49.8 37.1 50.0 36.7
Table 1: BLEU scores for phrasal segmentation model rescoring of 5-gram rescored lattices.
PIP -4.0 -2.0 0.0 2.0 4.0
BLEU 48.6 50.1 51.1 49.9 48.7
BP 0.000 0.000 0.000 -0.034 -0.072
1g 82.0 83.7 84.9 85.7 86.2
2g 57.3 58.9 59.9 60.5 61.1
3g 40.8 42.2 43.1 43.6 44.2
4g 29.1 30.3 31.1 31.5 32.0
words 70550 66964 63505 60847 58676
1 58840 46936 25040 15439 11744
2 7606 12388 18890 19978 18886
3 2691 4890 11532 13920 14295
4 860 1820 5016 6940 8008
5 240 450 1820 2860 3500
6+ 313 480 1207 1710 2243
w/p 1.10 1.21 1.58 1.86 2.02
Table 3: Effect of phrase insertion penalty (PIP)
on BLEU score, brevity penalty (BP), individual
n-gram precisions, phrase length distribution, and
average words per phrase (w/p) for mt02-05-tune.
penalty is too low, single word phrases dominate
the output and any benefits from longer context or
phrase-internal fluency are lost. As the phrase in-
sertion penalty increases, there are large gains in
precision at each order and many longer phrases
appear in the output. At the optimal phrase in-
sertion penalty, the average phrase length is 1.58
words and over 60% of the translation output is
generated from multi-word phrases.
6 Discussion
We have defined a simple model of the phrasal seg-
mentation process for phrase-based SMT and esti-
mated the model parameters from naturally occur-
ring phrase sequence examples in a large training
corpus. Applying first-order models to the NIST
Arabic-English machine translation task, we have
demonstrated complementary improved transla-
tion quality through exploitation of the same abun-
dantly available monolingual data used for training
regular word-based language models.
Comparing the in-domain newswire and out-
of-domain newsgroup test set performance shows
the importance of choosing appropriate data for
training the phrasal segmentation model param-
eters. When in-domain data is of limited avail-
ability, count mixing or other adaptation strategies
may lead to improved performance.
Acknowledgements
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on EMNLP and CoNLL,
pages 858?867.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference for Computational
Linguistics on Human Language Technology, pages
48?54, Morristown, NJ, USA.
Kumar, Shankar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of the conference on HLT
and EMNLP, pages 161?168.
Kumar, Shankar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Kuo, Hong-Kwang Jeff and Wolfgang Reichl. 1999.
Phrase-based language models for speech recogni-
tion. In Sixth European Conference on Speech Com-
munication and Technology, pages 1595?1598.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language,
volume 16, pages 69?88.
Och, Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Ries, Klaus, Finn Dag Bu, and Alex Waibel. 1996.
Class phrase models for language modeling. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing.
22
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380?388,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rule Filtering by Pattern for Efficient Hierarchical Translation
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
We describe refinements to hierarchical
translation search procedures intended to
reduce both search errors and memory us-
age through modifications to hypothesis
expansion in cube pruning and reductions
in the size of the rule sets used in transla-
tion. Rules are put into syntactic classes
based on the number of non-terminals and
the pattern, and various filtering strate-
gies are then applied to assess the impact
on translation speed and quality. Results
are reported on the 2008 NIST Arabic-to-
English evaluation task.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005) has emerged as one of the dominant cur-
rent approaches to statistical machine translation.
Hiero translation systems incorporate many of
the strengths of phrase-based translation systems,
such as feature-based translation and strong tar-
get language models, while also allowing flexi-
ble translation and movement based on hierarchi-
cal rules extracted from aligned parallel text. The
approach has been widely adopted and reported to
be competitive with other large-scale data driven
approaches, e.g. (Zollmann et al, 2008).
Large-scale hierarchical SMT involves auto-
matic rule extraction from aligned parallel text,
model parameter estimation, and the use of cube
pruning k-best list generation in hierarchical trans-
lation. The number of hierarchical rules extracted
far exceeds the number of phrase translations typ-
ically found in aligned text. While this may lead
to improved translation quality, there is also the
risk of lengthened translation times and increased
memory usage, along with possible search errors
due to the pruning procedures needed in search.
We describe several techniques to reduce mem-
ory usage and search errors in hierarchical trans-
lation. Memory usage can be reduced in cube
pruning (Chiang, 2007) through smart memoiza-
tion, and spreading neighborhood exploration can
be used to reduce search errors. However, search
errors can still remain even when implementing
simple phrase-based translation. We describe a
?shallow? search through hierarchical rules which
greatly speeds translation without any effect on
quality. We then describe techniques to analyze
and reduce the set of hierarchical rules. We do
this based on the structural properties of rules and
develop strategies to identify and remove redun-
dant or harmful rules. We identify groupings of
rules based on non-terminals and their patterns and
assess the impact on translation quality and com-
putational requirements for each given rule group.
We find that with appropriate filtering strategies
rule sets can be greatly reduced in size without im-
pact on translation performance.
1.1 Related Work
The search and rule pruning techniques described
in the following sections add to a growing lit-
erature of refinements to the hierarchical phrase-
based SMT systems originally described by Chi-
ang (2005; 2007). Subsequent work has addressed
improvements and extensions to the search proce-
dure itself, the extraction of the hierarchical rules
needed for translation, and has also reported con-
trastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning
to improve translation speed. Venugopal et al
(2007) introduce a Hiero variant with relaxed con-
straints for hypothesis recombination during pars-
ing; speed and results are comparable to those of
cube pruning, as described by Chiang (2007). Li
and Khudanpur (2008) report significant improve-
ments in translation speed by taking unseen n-
grams into account within cube pruning to mini-
mize language model requests. Dyer et al (2008)
380
extend the translation of source sentences to trans-
lation of input lattices following Chappelier et al
(1999).
Extensions to Hiero Blunsom et al (2008)
discuss procedures to combine discriminative la-
tent models with hierarchical SMT. The Syntax-
Augmented Machine Translation system (Zoll-
mann and Venugopal, 2006) incorporates target
language syntactic constituents in addition to the
synchronous grammars used in translation. Shen
at al. (2008) make use of target dependency trees
and a target dependency language model during
decoding. Marton and Resnik (2008) exploit shal-
low correspondences of hierarchical rules with
source syntactic constituents extracted from par-
allel text, an approach also investigated by Chiang
(2005). Zhang and Gildea (2006) propose bina-
rization for synchronous grammars as a means to
control search complexity arising from more com-
plex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al (2008)
describe a linear algorithm, a modified version of
shift-reduce, to extract phrase pairs organized into
a tree from which hierarchical rules can be directly
extracted. Lopez (2007) extracts rules on-the-fly
from the training bitext during decoding, search-
ing efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman
et al (2008) compare phrase-based, hierarchical
and syntax-augmented decoders for translation of
Arabic, Chinese, and Urdu into English, and they
find that attempts to expedite translation by simple
schemes which discard rules also degrade transla-
tion performance. Lopez (2008) explores whether
lexical reordering or the phrase discontiguity in-
herent in hierarchical rules explains improvements
over phrase-based systems. Hierarchical transla-
tion has also been used to great effect in combina-
tion with other translation architectures (e.g. (Sim
et al, 2007; Rosti et al, 2007)).
1.2 Outline
The paper proceeds as follows. Section 2 de-
scribes memoization and spreading neighborhood
exploration in cube pruning intended to reduce
memory usage and search errors, respectively. A
detailed comparison with a simple phrase-based
system is presented. Section 3 describes pattern-
based rule filtering and various procedures to se-
lect rule sets for use in translation with an aim
to improving translation quality while minimizing
rule set size. Finally, Section 4 concludes.
2 Two Refinements in Cube Pruning
Chiang (2007) introduced cube pruning to apply
language models in pruning during the generation
of k-best translation hypotheses via the application
of hierarchical rules in the CYK algorithm. In the
implementation of Hiero described here, there is
the parser itself, for which we use a variant of the
CYK algorithm closely related to CYK+ (Chap-
pelier and Rajman, 1998); it employs hypothesis
recombination, without pruning, while maintain-
ing back pointers. Before k-best list generation
with cube pruning, we apply a smart memoiza-
tion procedure intended to reduce memory con-
sumption during k-best list expansion. Within the
cube pruning algorithm we use spreading neigh-
borhood exploration to improve robustness in the
face of search errors.
2.1 Smart Memoization
Each cell in the chart built by the CYK algorithm
contains all possible derivations of a span of the
source sentence being translated. After the parsing
stage is completed, it is possible to make a very ef-
ficient sweep through the backpointers of the CYK
grid to count how many times each cell will be ac-
cessed by the k-best generation algorithm. When
k-best list generation is running, the number of
times each cell is visited is logged so that, as each
cell is visited for the last time, the k-best list as-
sociated with each cell is deleted. This continues
until the one k-best list remaining at the top of the
chart spans the entire sentence. Memory reduc-
tions are substantial for longer sentences: for the
longest sentence in the tuning set described later
(105 words in length), smart memoization reduces
memory usage during the cube pruning stage from
2.1GB to 0.7GB. For average length sentences of
approx. 30 words, memory reductions of 30% are
typical.
2.2 Spreading Neighborhood Exploration
In generation of a k-best list of translations for
a source sentence span, every derivation is trans-
formed into a cube containing the possible trans-
lations arising from that derivation, along with
their translation and language model scores (Chi-
ang, 2007). These derivations may contain non-
terminals which must be expanded based on hy-
potheses generated by lower cells, which them-
381
HIERO MJ1 HIERO HIERO SHALLOW
X ? ?V2V1,V1V2? X ? ??,?? X ? ??s,?s?
X ? ?V ,V ? ?, ? ? ({X} ?T)+ X ? ?V ,V ?
V ? ?s,t? V ? ?s,t?
s, t ? T+ s, t ? T+; ?s, ?s ? ({V } ? T)+
Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.
selves may contain non-terminals. For efficiency
each cube maintains a queue of hypotheses, called
here the frontier queue, ranked by translation and
language model score; it is from these frontier
queues that hypotheses are removed to create the
k-best list for each cell. When a hypothesis is ex-
tracted from a frontier queue, that queue is updated
by searching through the neighborhood of the ex-
tracted item to find novel hypotheses to add; if no
novel hypotheses are found, that queue necessar-
ily shrinks. This shrinkage can lead to search er-
rors. We therefore require that, when a hypothe-
sis is removed, new candidates must be added by
exploring a neighborhood which spreads from the
last extracted hypothesis. Each axis of the cube
is searched (here, to a depth of 20) until a novel
hypothesis is found. In this way, up to three new
candidates are added for each entry extracted from
a frontier queue.
Chiang (2007) describes an initialization pro-
cedure in which these frontier queues are seeded
with a single candidate per axis; we initialize each
frontier queue to a depth of bNnt+1, where Nnt is
the number of non-terminals in the derivation and
b is a search parameter set throughout to 10. By
starting with deep frontier queues and by forcing
them to grow during search we attempt to avoid
search errors by ensuring that the universe of items
within the frontier queues does not decrease as the
k-best lists are filled.
2.3 A Study of Hiero Search Errors in
Phrase-Based Translation
Experiments reported in this paper are based
on the NIST MT08 Arabic-to-English transla-
tion task. Alignments are generated over all al-
lowed parallel data, (?150M words per language).
Features extracted from the alignments and used
in translation are in common use: target lan-
guage model, source-to-target and target-to-source
phrase translation models, word and rule penalties,
number of usages of the glue rule, source-to-target
and target-to-source lexical models, and three rule
Figure 1: Spreading neighborhood exploration
within a cube, just before and after extraction
of the item C. Grey squares represent the fron-
tier queue; black squares are candidates already
extracted. Chiang (2007) would only consider
adding items X to the frontier queue, so the queue
would shrink. Spreading neighborhood explo-
ration adds candidates S to the frontier queue.
count features inspired by Bender et al (2007).
MET (Och, 2003) iterative parameter estimation
under IBM BLEU is performed on the develop-
ment set. The English language used model is a
4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. In addition to the
MT08 set itself, we use a development set mt02-
05-tune formed from the odd numbered sentences
of the NIST MT02 through MT05 evaluation sets;
the even numbered sentences form the validation
set mt02-05-test. The mt02-05-tune set has 2,075
sentences.
We first compare the cube pruning decoder to
the TTM (Kumar et al, 2006), a phrase-based
SMT system implemented with Weighted Finite-
State Tansducers (Allauzen et al, 2007). The sys-
tem implements either a monotone phrase order
translation, or an MJ1 (maximum phrase jump of
1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation
allowed by Hiero and other models, MJ1 is clearly
inferior (Dreyer et al, 2007); MJ1 was developed
with efficiency in mind so as to run with a mini-
mum of search errors in translation and to be eas-
ily and exactly realized via WFSTs. Even for the
382
large models used in an evaluation task, the TTM
system is reported to run largely without pruning
(Blackwood et al, 2008).
The Hiero decoder can easily be made to
implement MJ1 reordering by allowing only a
restricted set of reordering rules in addition to
the usual glue rule, as shown in left-hand column
of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible
to compare its performance to the exact WFST
TTM implementation and to identify any search
errors made by Hiero.
Table 2 shows the lowercased IBM BLEU
scores obtained by the systems for mt02-05-tune
with monotone and reordered search, and with
MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used
throughout. In the monotone case, all phrase-
based systems perform similarly although Hiero
does make search errors. For simple MJ1 re-
ordering, the basic Hiero search procedure makes
many search errors and these lead to degradations
in BLEU. Spreading neighborhood expansion re-
duces the search errors and improves BLEU score
significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is
the set over which MET is run: MET drives up the
likelihood of good hypotheses at the expense of
poor hypotheses, but search errors often increase
due to the expanded dynamic range of the hypoth-
esis scores.
Our aim in these experiments was to demon-
strate that spreading neighborhood exploration can
aid in avoiding search errors. We emphasize that
we are not proposing that Hiero should be used to
implement reordering models such as MJ1 which
were created for completely different search pro-
cedures (e.g. WFST composition). However these
experiments do suggest that search errors may be
an issue, particularly as the search space grows
to include the complex long-range movement al-
lowed by the hierarchical rules. We next study
various filtering procedures to reduce hierarchi-
cal rule sets to find a balance between translation
speed, memory usage, and performance.
3 Rule Filtering by Pattern
Hierarchical rules X ? ??,?? are composed of
sequences of terminals and non-terminals, which
Monotone MJ1 MJ1+MET
BLEU SE BLEU SE BLEU SE
a 44.7 - 47.2 - 49.1 -
b 44.5 342 46.7 555 48.4 822
c 44.7 77 47.1 191 48.9 360
Table 2: Phrase-based TTM and Hiero perfor-
mance on mt02-05-tune for TTM (a), Hiero (b),
Hiero with spreading neighborhood exploration
(c). SE is the number of Hiero hypotheses with
search errors.
we call elements. In the source, a maximum of
two non-adjacent non-terminals is allowed (Chi-
ang, 2007). Leaving aside rules without non-
terminals (i.e. phrase pairs as used in phrase-
based translation), rules can be classed by their
number of non-terminals, Nnt, and their number
of elements, Ne. There are 5 possible classes:
Nnt.Ne= 1.2, 1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class sep-
arately to control memory usage. Furthermore, we
extract from alignments only those rules which are
relevant to our given test set; for computation of
backward translation probabilities we log general
counts of target-side rules but discard unneeded
rules. Even with this restriction, our initial ruleset
for mt02-05-tune exceeds 175M rules, of which
only 0.62M are simple phrase pairs.
The question is whether all these rules are
needed for translation. If the rule set can be re-
duced without reducing translation quality, both
memory efficiency and translation speed can be
increased. Previously published approaches to re-
ducing the rule set include: enforcing a mini-
mum span of two words per non-terminal (Lopez,
2008), which would reduce our set to 115M rules;
or a minimum count (mincount) threshold (Zoll-
mann et al, 2008), which would reduce our set
to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al (2008) describe the result of filter-
ing rules by insisting that target-side rules are
well-formed dependency trees. This reduces their
rule set from 140M to 26M rules. This filtering
leads to a degradation in translation performance
(see Table 2 of Shen et al (2008)), which they
counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports
Chinese-to-English translation experiments based
on 5.5M rules.
Zollmann et al (2008) report that filtering rules
383
en masse leads to degradation in translation per-
formance. Rather than apply a coarse filtering,
such as a mincount for all rules, we follow a more
syntactic approach and further classify our rules
according to their pattern and apply different fil-
ters to each pattern depending on its value in trans-
lation. The premise is that some patterns are more
important than others.
3.1 Rule Patterns
Class Rule Pattern
Nnt.Ne ?source , target? Types
?wX1 , wX1? 1185028
1.2 ?wX1 , wX1w? 153130
?wX1 , X1w? 97889
1.3 ?wX1w , wX1w? 32903522
?wX1w , wX1? 989540
2.3 ?X1wX2 , X1wX2? 1554656
?X2wX1 , X1wX2? 39163
?wX1wX2 , wX1wX2? 26901823
?X1wX2w , X1wX2w? 26053969
2.4 ?wX1wX2 , wX1wX2w? 2534510
?wX2wX1 , wX1wX2? 349176
?X2wX1w , X1wX2w? 259459
?wX1wX2w , wX1wX2w? 61704299
?wX1wX2w , wX1X2w? 3149516
2.5 ?wX1wX2w , X1wX2w? 2330797
?wX2wX1w , wX1wX2w? 275810
?wX2wX1w , wX1X2w? 205801
Table 3: Hierarchical rule patterns classed by
number of non-terminals, Nnt, number of ele-
ments Ne, source and target patterns, and types in
the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and
target patterns by replacing every sequence of
non-terminals by a single symbol ?w? (indicating
word, i.e. terminal string, w ? T+). Each hierar-
chical rule has a unique source and target pattern
which together define the rule pattern.
By ignoring the identity and the number of ad-
jacent terminals, the rule pattern represents a nat-
ural generalization of any rule, capturing its struc-
ture and the type of reordering it encodes. In to-
tal, there are 66 possible rule patterns. Table 3
presents a few examples extracted for mt02-05-
tune, showing that some patterns are much more
diverse than others. For example, patterns with
two non-terminals (Nnt=2) are richer than pat-
terns with Nnt=1, as they cover many more dis-
tinct rules. Additionally, patterns with two non-
terminals which also have a monotonic relation-
ship between source and target non-terminals are
much more diverse than their reordered counter-
parts.
Some examples of extracted rules and their cor-
responding pattern follow, where Arabic is shown
in Buckwalter encoding.
Pattern ?wX1 , wX1w? :
?w+ qAl X1 , the X1said?
Pattern ?wX1w , wX1? :
?fy X1kAnwn Al>wl , on december X1?
Pattern ?wX1wX2 , wX1wX2w? :
?Hl X1lAzmp X2 , a X1solution to the X2crisis?
3.2 Building an Initial Rule Set
We describe a greedy approach to building a rule
set in which rules belonging to a pattern are added
to the rule set guided by the improvements they
yield on mt02-05-tune relative to the monotone
Hiero system described in the previous section.
We find that certain patterns seem not to con-
tribute to any improvement. This is particularly
significant as these patterns often encompass large
numbers of rules, as with patterns with match-
ing source and target patterns. For instance, we
found no improvement when adding the pattern
?X1w,X1w?, of which there were 1.2M instances
(Table 3). Since concatenation is already possible
under the general glue rule, rules with this pattern
are redundant. By contrast, the much less frequent
reordered counterpart, i.e. the ?wX1,X1w? pat-
tern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-
terminals (Nnt=2).
Based on exploratory analyses (not reported
here, for space) an initial rule set was built by
excluding patterns reported in Table 4. In to-
tal, 171.5M rules are excluded, for a remaining
set of 4.2M rules, 3.5M of which are hierarchi-
cal. We acknowledge that adding rules in this way,
by greedy search, is less than ideal and inevitably
raises questions with respect to generality and re-
peatability. However in our experience this is a
robust approach, mainly because the initial trans-
lation system runs very fast; it is possible to run
many exploratory experiments in a short time.
384
Excluded Rules Types
a ?X1w,X1w? , ?wX1,wX1? 2332604
b ?X1wX2,?? 2121594
?X1wX2w,X1wX2w? ,c ?wX1wX2,wX1wX2?
52955792
d ?wX1wX2w,?? 69437146
e Nnt.Ne= 1.3 w mincount=5 32394578
f Nnt.Ne= 2.3 w mincount=5 166969
g Nnt.Ne= 2.4 w mincount=10 11465410
h Nnt.Ne= 2.5 w mincount=5 688804
Table 4: Rules excluded from the initial rule set.
3.3 Shallow versus Fully Hierarchical
Translation
In measuring the effectiveness of rules in transla-
tion, we also investigate whether a ?fully hierarchi-
cal? search is needed or whether a shallow search
is also effective. In constrast to full Hiero, in the
shallow search, only phrases are allowed to be sub-
stituted into non-terminals. The rules used in each
case can be expressed as shown in the 2nd and 3rd
columns of Table 1. Shallow search can be con-
sidered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on
BLEU, while translation speed increases by a fac-
tor of 7. Of course, these results are specific to this
Arabic-to-English translation task, and need not
be expected to carry over to other language pairs,
such as Chinese-to-English translation. However,
the impact of this search simplification is easy to
measure, and the gains can be significant enough,
that it may be worth investigation even for lan-
guages with complex long distance movement.
mt02-05- -tune -test
System Time BLEU BLEU
HIERO 14.0 52.1 51.5
HIERO - shallow 2.0 52.1 51.4
Table 5: Translation performance and time (in sec-
onds per word) for full vs. shallow Hiero.
3.4 Individual Rule Filters
We now filter rules individually (not by class) ac-
cording to their number of translations. For each
fixed ? /? T+ (i.e. with at least 1 non-terminal),
we define the following filters over rules X ?
??,??:
? Number of translations (NT). We keep the
NT most frequent ?, i.e. each ? is allowed to
have at most NT rules.
? Number of reordered translations (NRT).
We keep the NRT most frequent ? with
monotonic non-terminals and the NRT most
frequent ? with reordered non-terminals.
? Count percentage (CP). We keep the most
frequent ? until their aggregated number of
counts reaches a certain percentage CP of the
total counts of X ? ??,??. Some ??s are al-
lowed to have more ??s than others, depend-
ing on their count distribution.
Results applying these filters with various
thresholds are given in Table 6, including num-
ber of rules and decoding time. As shown, all
filters achieve at least a 50% speed-up in decod-
ing time by discarding 15% to 25% of the base-
line rules. Remarkably, performance is unaffected
when applying the simple NT and NRT filters
with a threshold of 20 translations. Finally, the
CM filter behaves slightly worse for thresholds of
90% for the same decoding time. For this reason,
we select NRT=20 as our general filter.
mt02-05- -tune -test
Filter Time Rules BLEU BLEU
baseline 2.0 4.20 52.1 51.4
NT=10 0.8 3.25 52.0 51.3
NT=15 0.8 3.43 52.0 51.3
NT=20 0.8 3.56 52.1 51.4
NRT=10 0.9 3.29 52.0 51.3
NRT=15 1.0 3.48 52.0 51.4
NRT=20 1.0 3.59 52.1 51.4
CP=50 0.7 2.56 51.4 50.9
CP=90 1.0 3.60 52.0 51.3
Table 6: Impact of general rule filters on transla-
tion (IBM BLEU), time (in seconds per word) and
number of rules (in millions).
3.5 Pattern-based Rule Filters
In this section we first reconsider whether reintro-
ducing the monotonic rules (originally excluded as
described in rows ?b?, ?c?, ?d? in Table 4) affects
performance. Results are given in the upper rows
of Table 7. For all classes, we find that reintroduc-
ing these rules increases the total number of rules
385
mt02-05- -tune -test
Nnt.Ne Filter Time Rules BLEU BLEU
baseline NRT=20 1.0 3.59 52.1 51.4
2.3 +monotone 1.1 4.08 51.5 51.1
2.4 +monotone 2.0 11.52 51.6 51.0
2.5 +monotone 1.8 6.66 51.7 51.2
1.3 mincount=3 1.0 5.61 52.1 51.3
2.3 mincount=1 1.2 3.70 52.1 51.4
2.4 mincount=5 1.8 4.62 52.0 51.3
2.4 mincount=15 1.0 3.37 52.0 51.4
2.5 mincount=1 1.1 4.27 52.2 51.5
1.2 mincount=5 1.0 3.51 51.8 51.3
1.2 mincount=10 1.0 3.50 51.7 51.2
Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.
substantially, despite the NRT=20 filter, but leads
to degradation in translation performance.
We next reconsider the mincount threshold val-
ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-
nally described in Table 4 (rows ?e? to ?h?). Results
under various mincount cutoffs for each class are
given in Table 7 (middle five rows). For classes
2.3 and 2.5, the mincount cutoff can be reduced
to 1 (i.e. all rules are kept) with slight translation
improvements. In contrast, reducing the cutoff for
classes 1.3 and 2.4 to 3 and 5, respectively, adds
many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for
class 2.4 yields the same results with a smaller rule
set. Finally, we consider further filtering applied to
class 1.2 with mincount 5 and 10 (final two rows
in Table 7). The number of rules is largely un-
changed, but translation performance drops con-
sistently as more rules are removed.
Based on these experiments, we conclude that it
is better to apply separate mincount thresholds to
the classes to obtain optimal performance with a
minimum size rule set.
3.6 Large Language Models and Evaluation
Finally, in this section we report results of our
shallow hierarchical system with the 2.5 min-
count=1 configuration from Table 7, after includ-
ing the following N-best list rescoring steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore each 10000-best
list.
? Minimum Bayes Risk (MBR). We then rescore
the first 1000-best hypotheses with MBR,
taking the negative sentence level BLEU
score as the loss function to minimise (Ku-
mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02-
05-test, the NIST subsets from the MT06 evalu-
ation (mt06-nist-nw for newswire data and mt06-
nist-ng for newsgroup) and mt08, as measured by
lowercased IBM BLEU and TER (Snover et al,
2006). Mixed case NIST BLEU for this system on
mt08 is 42.5. This is directly comparable to offi-
cial MT08 evaluation results1.
4 Conclusions
This paper focuses on efficient large-scale hierar-
chical translation while maintaining good trans-
lation quality. Smart memoization and spreading
neighborhood exploration during cube pruning are
described and shown to reduce memory consump-
tion and Hiero search errors using a simple phrase-
based system as a contrast.
We then define a general classification of hi-
erarchical rules, based on their number of non-
terminals, elements and their patterns, for refined
extraction and filtering.
For a large-scale Arabic-to-English task, we
show that shallow hierarchical decoding is as good
1Full MT08 results are available at
http://www.nist.gov/speech/tests/mt/2008/. It is worth
noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
386
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08
HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6
+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1
Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-
els and MBR decoding.
as fully hierarchical search and that decoding time
is dramatically decreased. In addition, we describe
individual rule filters based on the distribution of
translations with further time reductions at no cost
in translation scores. This is in direct contrast
to recent reported results in which other filtering
strategies lead to degraded performance (Shen et
al., 2008; Zollmann et al, 2008).
We find that certain patterns are of much greater
value in translation than others and that separate
minimum count filters should be applied accord-
ingly. Some patterns were found to be redundant
or harmful, in particular those with two monotonic
non-terminals. Moreover, we show that the value
of a pattern is not directly related to the number of
rules it encompasses, which can lead to discarding
large numbers of rules as well as to dramatic speed
improvements.
Although reported experiments are only for
Arabic-to-English translation, we believe the ap-
proach will prove to be general. Pattern relevance
will vary for other language pairs, but we expect
filtering strategies to be equally worth pursuing.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government re-
search grant BES-2007-15956 (project TEC2006-
13694-C03-03).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceedings of FSMNLP, pages 27?35.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT
using efficient BLEU oracle computation. In Pro-
ceedings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-HLT, pages 1012?1020.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of HLT-EMNLP, pages
161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
387
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the ACL-HLT Second Workshop
on Syntax and Structure in Statistical Translation,
pages 10?18.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CONLL, pages 976?985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
pages 505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT, pages 1003?
1011.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of HLT-
NAACL, pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation
system combination. In Proceedings of ICASSP,
volume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous
binarization for machine translation. In Proceedings
of HLT-NAACL, pages 256?263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING, pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of NAACL Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of COLING, pages
1145?1152.
388
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110?118,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-Dependent Alignment Models for Statistical Machine Translation
Jamie Brunning, Adria` de Gispert and William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{jjjb2,ad465,wjb31}@eng.cam.ac.uk
Abstract
We introduce alignment models for Machine Trans-
lation that take into account the context of a source
word when determining its translation. Since the use
of these contexts alone causes data sparsity prob-
lems, we develop a decision tree algorithm for clus-
tering the contexts based on optimisation of the
EM auxiliary function. We show that our context-
dependent models lead to an improvement in align-
ment quality, and an increase in translation quality
when the alignments are used in Arabic-English and
Chinese-English translation.
1 Introduction
Alignment modelling for Statistical Machine Translation
(SMT) is the task of determining translational correspon-
dences between the words in pairs of sentences in parallel
text. Given a source language word sequence f J1 and a
target language word sequence eI1, we model the transla-
tion probability as P(eI1|fJ1 ) and introduce a hidden vari-
able aI1 representing a mapping from the target word po-
sitions to source word positions such that ei is aligned to
fai . Then P(eI1|f j1 ) =
?
aI1 P(eI1, aI1|f
j
1 ) (Brown et al,
1993).
Previous work on statistical alignment modelling has
not taken into account the source word context when de-
termining translations of that word. It is intuitive that a
word in one context, with a particular part-of-speech and
particular words surrounding it, may translate differently
when in a different context. We aim to take advantage
of this information to provide a better estimate of the
word?s translation. The challenge of incorporating con-
text information is maintaining computational tractability
of estimation and alignment, and we develop algorithms
to overcome this.
The development of efficient estimation procedures
for context-dependent acoustic models revolutionised the
field of Automatic Speech Recognition (ASR) (Young et
al., 1994). Clustering is used extensively for improv-
ing parameter estimation of triphone (and higher order)
acoustic models, enabling robust estimation of param-
eters and reducing the computation required for recog-
nition. Kannan et al (1994) introduce a binary tree-
growing procedure for clustering Gaussian models for
triphone contexts based on the value of a likelihood ra-
tio. We adopt a similar approach to estimate context-
dependent translation probabilities.
We focus on alignment with IBM Model 1 and HMMs.
HMMs are commonly used to generate alignments from
which state of the art SMT systems are built. Model 1 is
used as an intermediate step in the creation of more pow-
erful alignment models, such as HMMs and further IBM
models. In addition, it is used in SMT as a feature in Min-
imum Error Training (Och et al, 2004) and for rescor-
ing lattices of translation hypotheses (Blackwood et al,
2008). It is also used for lexically-weighted phrase ex-
traction (Costa-jussa` and Fonollosa, 2005) and sentence
segmentation of parallel text (Deng et al, 2007) prior to
machine translation.
1.1 Overview
We first develop an extension to Model 1 that allows the
use of arbitrary context information about a source word
to estimate context-dependent word-to-word translation
probabilities. Since there is insufficient training data to
accurately estimate translation probabilities for less fre-
quently occurring contexts, we develop a decision tree
clustering algorithm to form context classes. We go on to
develop a context-dependent HMM model for alignment.
In Section 3, we evaluate our context-dependent mod-
els on Arabic-English parallel text, comparing them to
our baseline context-independent models. We perform
morphological decomposition of the Arabic text using
MADA, and use part-of-speech taggers on both lan-
guages. Alignment quality is measured using Alignment
Error Rate (AER) measured against a manually-aligned
parallel text. Section 4 uses alignments produced by
110
our improved alignment models to initialise a statistical
machine translation system and evaluate the quality of
translation on several data sets. We also apply part-of-
speech tagging and decision tree clustering of contexts to
Chinese-English parallel text; translation results for these
languages are presented in Section 4.2.
1.2 Previous and related work
Brown et al (1993) introduce IBM Models 1-5 for align-
ment modelling; Vogel et al (1996) propose a Hidden
Markov Model (HMM) model for word-to-word align-
ment, where the words of the source sentence are viewed
as states of an HMM and emit target sentence words;
Deng and Byrne (2005a) extend this to an HMM word-to-
phrase model which allows many-to-one alignments and
can capture dependencies within target phrases.
Habash and Sadat (2006) perform morphological de-
composition of Arabic words, such as splitting of pre-
fixes and suffixes. This leads to gains in machine trans-
lation quality when systems are trained on parallel text
containing the modified Arabic and processing of Arabic
text is carried out prior to translation. Nie?en and Ney
(2001a) perform pre-processing of German and English
text before translation; Nie?en and Ney (2001b) use mor-
phological information of the current word to estimate
hierarchical translation probabilities.
Berger et al (1996) introduce maximum entropy mod-
els for machine translation, and use a window either side
of the target word as context information. Varea et al
(2002) test for the presence of specific words within a
window of the current source word to form features for
use inside a maximum entropy model of alignment.
Toutanova et al (2002) use part-of-speech informa-
tion in both the source and target languages to estimate
alignment probabilities, but this information is not in-
corporated into translation probabilities. Popovic? and
Ney (2004) use the base form of a word and its part-of-
speech tag during the estimation of word-to-word transla-
tion probabilities for IBM models and HMMs, but do not
defined context-dependent estimates of translation prob-
abilities.
Stroppa et al (2007) consider context-informed fea-
tures of phrases as components of the log-linear model
during phrase-based translation, but do not address align-
ment.
2 Use of source language context in
alignment modelling
Consider the alignment of the target sentence e = eI1 with
the source sentence f = fJ1 . Let a = aI1 be the align-
ments of the target words to the source words. Let cj be
the context information of fj for j = 1, . . . , J . This con-
text information can be any information about the word,
e.g. part-of-speech, previous and next words, part-of-
speech of previous and next words, or longer range con-
text information.
We follow Brown et al (1993), but extend their mod-
elling framework to include information about the source
word from which a target word is emitted. We model the
alignment process as:
P(eI1, aI1, I |fJ1 , cJ1 ) =
P(I |fJ1 , cJ1 )
I?
i=1
[P(ei|ai1, ei?11 , fJ1 , cJ1 , I)
? P(ai|ei?11 , ai?11 , fJ1 , cJ1 , I)
] (1)
We introduce word-to-word translation tables that depend
on the source language context for each word, i.e. the
probability that f translates to e given f has context c is
t(e|f, c). We assume that the context sequence is given
for a source word sequence. This assumption can be
relaxed to allow for multiple tag sequences as hidden
processes, but we assume here that a tagger generates
a single context sequence cJ1 for a word sequence fJ1 .
This corresponds to the assumption that, for a context se-
quence c?J1 , P(c?J1 |fJ1 ) = ?cJ1 (c?J1 ); hence
P(eI1, aI1|fJ1 ) =
?
c?J1
P(eI1, aI1, c?J1 |fJ1 ) = P(eI1, aI1|cJ1 , fJ1 )
For Model 1, ignoring the sentence length distribution,
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , cai). (2)
Estimating translation probabilities separately for ev-
ery possible context of a source word individually leads
to problems with data sparsity and rapid growth of the
translation table. We therefore wish to cluster source con-
texts which lead to similar probability distributions. Let
Cf denote the set of all observed contexts of source word
f . A particular clustering is denoted
Kf = {Kf,1, . . . ,Kf,Nf},
where Kf is a partition of Cf . We define a class mem-
bership function ?f such that for any context c, ?f (c)
is the cluster containing c. We assume that all contexts
in a cluster give rise to the same translation probability
distribution for that source word, i.e. for a cluster K,
t(e|f, c) = t(e|f, c?) for all contexts c, c? ? K and all
target words e; we write this shared translation probabil-
ity as t(e|f,K).
The Model 1 sentence translation probability for a
given alignment (Equation 2) becomes
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , ?f (cai)).
(3)
111
For HMM alignment, we assume that the transition prob-
abilities a(ai|ai?1) are independent of the word contexts
and the sentence translation probability is
PH(eI1, aI1|fJ1 , cJ1 ) =
I?
i=1
a(ai|ai?1, J)t(ei|fai , ?f (cai)).
(4)
Section 2.1.1 describes how the context classes are deter-
mined by optimisation of the EM auxiliary function. Al-
though the translation model is significantly more com-
plex than that of context-independent models, once class
membership is fixed, alignment and parameter estimation
use the standard algorithms.
2.1 EM parameter estimation
We train using Expectation Maximisation (EM), optimis-
ing the log probability of the training set {e(s), f (s)}Ss=1
(Brown et al, 1993). Given model parameters ??, we es-
timate new parameters ? by maximisation of the EM aux-
iliary function
?
s,a
P??(a|f (s), c(s), e(s)) log P?(e(s), a, I(s)|f (s), c(s)).
We assume the sentence length distribution and align-
ment probabilities do not depend on the contexts of the
source words; hence the relevant part of the auxiliary
function is
?
e
?
f
?
c?Cf
??(e|f, c) log t(e|f, c), (5)
where
??(e|f, c) = ?
s
I(s)?
i=1
J(s)?
j=1
[
?c(c(s)j )?e(e(s)i )?f (f (s)j )
? P??(ai = j|e(s), f (s), c(s))
]
Here ?? can be computed under Model 1 or the HMM,
and is calculated using the forward-backward algorithm
for the HMM.
2.1.1 Parameter estimation with clustered contexts
We can re-write the EM auxiliary function (Equation
5) in terms of the cluster-specific translation probabilities:
?
e
?
f
|Kf |?
l=1
?
c?Kf,l
??(e|f, c) log t(e|f, c)
= ?
e
?
f
?
K?Kf
??(e|f,K) log t(e|f,K) (6)
where ??(e|f,K) = ?
c?K
??(e|f, c)
Following the usual derivation, the EM update for the
class-specific translation probabilities becomes
t?(e|f,K) = ?
?(e|f,K)?
e? ??(e?|f,K)
. (7)
Standard EM training can be viewed a special case of this,
with every context of a source word grouped into a sin-
gle cluster. Another way to view these clustered context-
dependent models is that contexts belonging to the same
cluster are tied and share a common translation proba-
bility distribution, which is estimated from all training
examples in which any of the contexts occur.
2.2 Decision trees for context clustering
The objective for each source word is to split the contexts
into classes to maximise the likelihood of the training
data. Since it is not feasible to maximise the likelihood
of the observations directly, we maximise the expected
log likelihood by considering the EM auxiliary function,
in a similar manner to that used for modelling contextual
variations of phones for ASR (Young et al, 1994; Singer
and Ostendorf, 1996). We perform divisive clustering in-
dependently for each source word f , by building a binary
decision tree which forms classes of contexts which max-
imise the EM auxiliary function. Questions for the tree
are drawn from a set of questions Q = {q1, . . . , q|Q|}
concerning the context information of f .
Let K be any set of contexts of f , and define
L(K) = ?
e
?
c?K
??(e|f, c) log t(e|f, c)
= ?
e
?
c?K
??(e|f, c) log
?
c?K ??(e|f, c)?
e?
?
c?K ??(e?|f, c)
.
This is the contribution to the EM auxiliary function of
source word f occurring in the contexts of K. Let q be
a binary question about the context of f , and consider
the effect on the partial auxiliary function (Equation 6)
of splitting K into two clusters using question q. Define
Kq be the set of contexts in K which answer ?yes? to q
and Kq? be the contexts which answer ?no?. Define the
objective function
Qf,q(K) =
?
e
?
c?Kq
??(e|f, c) log t(e|f, c)
+?
e
?
c?Kq?
??(e|f, c) log t(e|f, c)
= L(Kq) + L(Kq?)
When the node is split using question q, the increase in
objective function is given by
Qf,q(K)? L(K) = L(Kq?) + L(Kq)? L(K).
112
We choose q to maximise this.
In order to build the decision tree for f , we take the set
of all contexts Cf as the initial cluster at the root node.
We then find the question q? such that Qf,q(Cf ) is maxi-
mal, i.e.
q? = arg max
q?Q
Qf,q(Cf )
This splits Cf , so our decision tree now has two nodes.
We iterate this process, at each iteration splitting (into
two further nodes) the leaf node that leads to the great-
est increase in objective function. This leads to a greedy
search to optimise the log likelihood over possible state
clusterings.
In order to control the growth of the tree, we put in
place two thresholds:
? Timp is the minimum improvement in objective func-
tion required for a node to be split; without it, we
would continue splitting nodes until each contained
only one context, even though doing so would cause
data sparsity problems.
? Tocc is the minimum occupancy of a node, based on
how often the contexts at that node occur in the train-
ing data; we want to ensure that there are enough ex-
amples of a context in the training data to estimate
accurately the translation probability distribution for
that cluster.
For each leaf node l and set of contextsKl at that node,
we find the question ql that, when used to split Kl, pro-
duces the largest gain in objective function:
ql = arg max
q?Q
[L(Kl,q) + L(Kl,q?)? L(Kl)]
= arg max
q?Q
[L(Kl,q) + L(Kl,q?)]
We then find the leaf node for which splitting gives the
largest improvement:
l? = arg max
l
[L(Kl,ql) + L(Kl,q?l)? L(Kl)]
If the following criteria are both satisfied at that node, we
split the node into two parts, creating two leaf nodes in
its place:
? The objective function increases sufficiently
L(Kl,ql) + L(Kl,q?l)? L(Kl) > Timp
? The occupancy threshold is exceeded for both child
nodes:
?
e
?
c?Kl,x
??(e|f, c) > Tocc for x = q, q?
We perform such clustering for every source word in the
parallel text.
shares NNS ? ? ? ? ? ? ?
bank NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
% PUNC ? ? ? ? ? ? ?
12 NN ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
deal NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
Sfq
pN
N
by
EN
N
12
NN
%
PU
NC
mn
IN
>
shm
NN
Al
bn
kN
N
city NN ? ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ? ?
in IN ? ? ? ? ? ? ? ?
liquor NN ? ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ? ?
were VBD ? ? ? ? ? ? ? ?
owners NNS ? ? ? ? ? ? ? ?
whose WP$ ? ? ? ? ? ? ? ?
houses NNS ? ? ? ? ? ? ? ?
several JJ ? ? ? ? ? ? ? ?
and CC ? ? ? ? ? ? ? ?
w+
CC
mn
Az
lN
N
Ed
pJ
J
>
SH
Ab
hA
NN
yb
yE
wn
VB
P
Al
xm
wr
NN
fy
IN
Al
md
yn
pN
N
Figure 1: Alignment of the English selling in different contexts.
In the first, it is preceded by of and links to the infinitive of the
Arabic verb byE; in the second, it is preceded by were and links
to an inflected form of the same Arabic verb, ybyEwn.
3 Evaluation of alignment quality
Our models were built using the MTTK toolkit (Deng
and Byrne, 2005b). Decision tree clustering was imple-
mented and the process parallelised to enable thousands
of decision trees to be built. Our context-dependent (CD)
Model 1 models trained on context-annotated data were
compared to the baseline context-independent (CI) mod-
els trained on untagged data.
The models were trained using data allowed for the
NIST 08 Arabic-English evaluation1, excluding the UN
collections, comprising 300k parallel sentence pairs, a to-
tal of 8.4M words of Arabic and 9.5M words of English.
The Arabic language incorporates into its words sev-
eral prefixes and suffixes which determine grammatical
features such as gender, number, person and voice. The
MADA toolkit (Habash and Sadat, 2006) was used to
perform Arabic morphological word decomposition and
part-of-speech tagging. It determines the best analysis
for each word in a sentence and splits word prefixes and
suffixes, based on the alternative analyses provided by
BAMA (Buckwalter, 2002). We use tokenisation scheme
1http://nist.gov/speech/tests/mt/2008
113
?D2?, which splits certain prefixes and has been reported
to improve machine translation performance (Habash and
Sadat, 2006). The alignment models are trained on this
processed data, and the prefixes and suffixes are treated
as words in their own right; in particular their contexts
are examined and clustered.
The TnT tagger (Brants, 2000), used as distributed
with its model trained on the Wall Street Journal portion
of the Penn treebank, was used to obtain part-of-speech
tags for the English side of the parallel text. Marcus et al
(1993) gives a complete list of part-of-speech tags pro-
duced. No morphological analysis is performed for En-
glish.
Automatic word alignments were compared to a
manually-aligned corpus made up of the IBM Arabic-
English Word Alignment Corpus (Ittycheriah et al,
2006) and the word alignment corpora LDC2006E86 and
LDC2006E93. This contains 28k parallel text sentences
pairs: 724k words of Arabic and 847k words of English.
The alignment links were modified to reflect the MADA
tokenisation; after modification, there are 946k word-to-
word alignment links.
Alignment quality was evaluated by computing Align-
ment Error Rate (AER) (Och and Ney, 2000) relative to
the manual alignments. Since the links supplied con-
tain only ?sure? links and no ?possible? links, we use the
following formula for computing AER given reference
alignment links S and hypothesised alignment links A:
AER = 1? 2|S?A||S|+|A| .
3.1 Questions about contexts
The algorithm presented in Section 2 allows for any infor-
mation about the context of the source word to be consid-
ered. We could consider general questions of the form ?Is
the previous word x?? and ?Does word y occur within n
words of this one??. To maintain computational tractabil-
ity, we restrict the questions to those concerning the part-
of-speech tag assigned to the current, previous and next
words. We do not ask questions about the identities of the
words themselves. For each part-of-speech tag T , we ask
the question ?Does w have tag T??. In addition, we group
part-of-speech tags to ask more general questions: e.g.
the set of contexts which satisfies ?Is w a noun?? contains
those that satisfy ?Is w a proper noun?? and ?Is w a sin-
gular or mass noun??. We also ask the same questions
of the previous and next words in the source sentence.
In English, this gives a total of 152 distinct questions,
each of which is considered when splitting a leaf node.
The MADA part-of-speech tagger uses a reduced tag set,
which produces a total of 68 distinct questions.
Figure 1 shows the links of the English source word
selling in two different contexts where it links to different
words in Arabic, which are both forms of the same verb.
The part-of-speech of the previous word is useful for dis-
-4.22e+07
-4.2e+07
-4.18e+07
-4.16e+07
-4.14e+07
-4.12e+07
-4.1e+07
-4.08e+07
-4.06e+07
-4.04e+07
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
-3.1e+06
-3.05e+06
-3e+06
-2.95e+06
-2.9e+06
-2.85e+06
-2.8e+06
-2.75e+06
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 2: Increase in log probability of training data during
training for varying Timp, with Model 1, for Arabic to English
(top) and English to Arabic (bottom)
criminating between the two cases, whereas a context-
independent model would assign the same probability to
both Arabic words.
3.2 Training Model 1
Training is carried out in both translation directions. For
Arabic to English, the Arabic side of the parallel text is
tagged and the English side remains untagged; we view
the English words as being generated from the Arabic
words and questions are asked about the context of the
Arabic words to determine clusters for the translation ta-
ble. For English to Arabic, the situation is reversed: we
used tagged English text as the source language and un-
tagged Arabic text, with morphological decomposition,
as the target language.
Standard CI Model 1 training, initialised with a uni-
form translation table so that t(e|f) is constant for all
source/target word pairs (f, e), was run on untagged data
for 10 iterations in each direction (Brown et al, 1993;
Deng and Byrne, 2005b). A decision tree was built to
cluster the contexts and a further 10 iterations of training
were carried out using the tagged words-with-context to
produce context-dependent models (CD Model 1). The
114
English question Frequency
Is Next Preposition 1523
Is Prev Determiner 1444
Is Prev Preposition 1209
Is Prev Adjective 864
Is Next Noun Singular Mass 772
Is Prev Noun Singular Mass 690
Is Next Noun Plural 597
Is Next Noun 549
Arabic question Frequency
Is Prev Preposition 1110
Is Next Preposition 993
Is Prev Noun 981
Is Next Noun 912
Is Prev Coordinating Conjunction 627
Is Prev Noun SingularMass 607
Is Next Punctuation 603
Is Next Adjective Adverb 559
Table 1: Most frequent root node context questions
models were then evaluated using AER at each train-
ing iteration. A number of improvement thresholds Timp
were tested, and performance compared to that of models
found after further iterations of CI Model 1 training on
the untagged data. In both alignment directions, the log
probability of the training data increases during training
(see Figure 2). As expected, the training set likelihood
increases as the threshold Timp is reduced, allowing more
clusters and closer fitting to the data.
3.2.1 Analysis of frequently used questions
Table 1 shows the questions used most frequently at
the root node of the decision tree when clustering con-
texts in English and Arabic. Because they are used first,
these are the questions that individually give the great-
est ability to discriminate between the different contexts
of a word. The list shows the importance of the left and
right contexts of the word in predicting its translation: of
the most common 50 questions, 25 concern the previous
word, 19 concern the next, and only 6 concern the part-
of-speech of the current word. For Arabic, of the most
frequent 50 questions, 21 concern the previous word, 20
concern the next and 9 the current word.
3.2.2 Alignment Error Rate
Since MT systems are usually built on the union of the
two sets of alignments (Koehn et al, 2003), we consider
the union of alignments in the two directions as well as
those in each direction. Figure 3 shows the change in
AER of the alignments in each direction, as well as the
alignment formed by taking their union at corresponding
thresholds and training iterations.
Timp Arabic-English (%) English-Arabic (%)
10 30601 (25.33) 26011 (39.87)
20 11193 (9.27) 18365 (28.15)
40 1874 (1.55) 9104 (13.96)
100 307 (0.25) 1128 (1.73)
Table 2: Words [number (percentage)] with context-dependent
translation for varying Timp
3.2.3 Variation of improvement threshold Timp
There is a trade-off between modelling the data accu-
rately, which requires more clusters, and eliminating data
sparsity problems, which requires each cluster to contain
contexts that occur frequently enough in the training data
to estimate the translation probabilities accurately. Use of
a smaller threshold Timp leads to more clusters per word
and an improved ability to fit to the data, but this can lead
to reduced alignment quality if there is insufficient data
to estimate the translation probability distribution accu-
rately for each cluster. For lower thresholds, we observe
over-fitting and the AER rises after the second iteration of
CD training, similar to the behaviour seen in Och (2002).
Setting Timp = 0 results in each context of a word having
its own cluster, which leads to data sparsity problems.
Table 2 shows the percentage of words for which the
contexts are split into multiple clusters for CD Model 1
with varying improvement thresholds. This occurs when
there are enough training data examples and sufficient
variability between the contexts of a word that splitting
the contexts into more than one cluster increases the EM
auxiliary function. For words where the contexts are not
split, all the contexts remain in the same cluster and pa-
rameter estimation is exactly the same as for the unclus-
tered context-independent models.
3.3 Training HMMs
Adding source word context to translation has so far led
to improvements in AER for Model 1, but the perfor-
mance does not match that of HMMs trained on untagged
data; we therefore train HMMs on tagged data.
We proceed with Model 1 and Model 2 trained in the
usual way, and context-independent (CI) HMMs were
trained for 5 iterations on the untagged data. Statistics
were then gathered for clustering at various thresholds,
after which 5 further EM iterations were performed with
tagged data to produce context-dependent (CD) HMMs.
The HMMs were trained in both the Arabic to English
and the English to Arabic directions. The log likelihood
of the training set varies with Timp in much the same
way as for Model 1, increasing at each iteration, with
greater likelihood at lower thresholds. Figure 4 shows
how the AER of the union alignment varies with Timp
during training. As with Model 1, the clustered HMM
115
 49.2
 49.4
 49.6
 49.8
 50
 50.2
 50.4
 50.6
 50.8
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
51.2
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Threshold 100
49.0
49.2
49.4
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 3: Variation of AER during Model 1 training for varying
Timp, for Arabic to English (top), English to Arabic (middle)
and their union (bottom)
34.4
34.5
34.6
34.7
34.8
34.9
35.0
35.1
35.2
35.3
 5  6  7  8  9  10
AE
R
Iteration
CI HMM
Threshold 10
Threshold 20
Threshold 60
Figure 4: AER of the union alignment for varying Timp with the
HMM model
 72
 74
 76
 78
 80
 40  45  50  55  60
Pr
ec
is
io
n
Recall
p0=0.95
p0=0.00
p0=0.95
p0=0.00
English-Arabic CD HMM
English-Arabic CI HMM
Arabic-English CD HMM
Arabic-English CI HMM
Figure 5: Precision/recall curves for the context-dependent
HMM and the baseline context-independent HMM, for Arabic
to English and English to Arabic. p0 varies from 0.00 to 0.95 in
steps of 0.05.
models produce alignments with a lower AER than the
baseline model, and there is evidence of over-fitting to
the training data.
3.3.1 Alignment precision and recall
The HMM models include a null transition probability,
p0, which can be modified to adjust the number of align-
ments to the null token (Deng and Byrne, 2005a). Where
a target word is emitted from null, it is not included in
the alignment links, so this target word is viewed as not
being aligned to any source word; this affects the preci-
sion and recall. The results reported above use p0 = 0.2
for English-Arabic and p0 = 0.4 for Arabic-English; we
can tune these values to produce alignments with the low-
est AER. Figure 5 shows precision-recall curves for the
CD HMMs compared to the CI HMMs for both transla-
tion directions. For a given value of precision, the CD
HMM has higher recall; for a given value of recall, the
CD HMM has higher precision.
We do not report F-score (Fraser and Marcu, 2006)
since in our experiments we have not found strong cor-
relation with translation performance, but we note that
these results for precision and recall should lead to im-
proved F-scores as well.
4 Evaluation of translation quality
We have shown that the context-dependent models pro-
duce a decrease in AER measured on manually-aligned
data; we wish to show this improved model performance
leads to an increase in translation quality, measured by
BLEU score (Papineni et al, 2001). In addition to the
Arabic systems already evaluated by AER, we also report
results for a Chinese-English translation system.
Alignment models were evaluated by aligning the
training data using the models in each translation direc-
116
tion. HiFST, a WFST-based hierarchical translation sys-
tem described in (Iglesias et al, 2009), was trained on
the union of these alignments. MET (Och, 2003) was
carried out using a development set, and the BLEU score
evaluated on two test sets. Decoding used a 4-gram lan-
guage model estimated from the English side of the entire
MT08 parallel text, and a 965M word subset of monolin-
gual data from the English Gigaword Third Edition.
For both Arabic and English, the CD HMM models
were evaluated as follows. Iteration 5 of the CI HMM
was used to produce alignments for the parallel text train-
ing data: these were used to train the baseline system.
The same data is aligned using CD HMMs after two
further iterations of training and a second WFST-based
translation system built from these alignments. The mod-
els are evaluated by comparing BLEU scores with those
of the baseline model.
4.1 Arabic to English translation
Alignment models were trained on the NIST MT08
Arabic-English parallel text, excluding the UN portion.
The null alignment probability was chosen based on the
AER, resulting in values of p0 = 0.05 for Arabic to
English and p0 = 0.10 for English to Arabic. We per-
form experiments on the NIST Arabic-English transla-
tion task. The mt02 05 tune and mt02 05 test data sets
are formed from the odd and even numbered sentences
of the NIST MT02 to MT05 evaluation sets respectively;
each contains 2k sentences and 60k words. We use
mt02 05 tune as a development set and evaluate the sys-
tem on mt02 05 test and the newswire portion of the
MT08 set, MT08-nw. Table 3 shows a comparison of the
system trained using CD HMMs with the baseline sys-
tem, which was trained using CI HMM models on un-
tagged data. The context-dependent models result in a
gain in BLEU score of 0.3 for mt02 05 test and 0.6 for
MT08-nw.
4.2 Chinese to English translation
The Chinese training set was 600k random parallel text
sentences of the newswire LDC collection allowed for
NIST MT08, a total of 15.2M words of Chinese and
16.6M words of English. The Chinese text was tagged us-
ing the MXPOST maximum-entropy part of speech tag-
ging tool (Ratnaparkhi, 1996) trained on the Penn Chi-
nese Treebank 5.1; the English text was tagged using the
TnT part of speech tagger (Brants, 2000) trained on the
Wall Street Journal portion of the English Penn treebank.
The development set tune-nw and validation set test-nw
contain a mix of the newswire portions of MT02 through
MT05 and additional developments sets created by trans-
lation within the GALE program. We also report results
on the newswire portion of the MT08 set. Again we see
an increase in BLEU score for both test sets: 0.5 for test-
Arabic-English
Alignments tune mt02 05 test MT08-nw
CI HMM 50.0 49.4 46.3
CD HMM 50.0 49.7 46.9
Chinese-English
Alignments tune test-nw MT08-nw
CI HMM 28.1 28.5 26.9
CD HMM 28.5 29.0 27.7
Table 3: Comparison, using BLEU score, of the CD HMM with
the baseline CI HMM
nw and 0.8 for MT08-nw.
5 Conclusions and future work
We have introduced context-dependent Model 1 and
HMM alignment models, which use context information
in the source language to improve estimates of word-
to-word translation probabilities. Estimation of parame-
ters using these contexts without smoothing leads to data
sparsity problems; therefore we have developed decision
tree clustering algorithms to cluster source word contexts
based on optimisation of the EM auxiliary function. Con-
text information is incorporated by the use of part-of-
speech tags in both languages of the parallel text, and the
EM algorithm is used for parameter estimation.
We have shown that these improvements to the model
lead to decreased AER compared to context-independent
models. Finally, we compare machine translation sys-
tems built using our context-dependent alignments. For
both Arabic- and Chinese-to-English translation, we
report an increase in translation quality measured by
BLEU score compared to a system built using context-
independent alignments.
This paper describes an initial investigation into
context-sensitive alignment models, and there are many
possible directions for future research. Clustering the
probability distributions of infrequently occurring may
produce improvements in alignment quality, different
model training schemes and extensions of the context-
dependence to more sophisticated alignment models will
be investigated. Further translation experiments will be
carried out.
Acknowledgements
This work was supported in part by the GALE program of
the Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022. J. Brunning is supported
by a Schiff Foundation graduate studentship. Thanks to
Yanjun Ma, Dublin City University, for training the Chi-
nese part of speech tagger.
117
References
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, and
William Byrne. 2008. European language translation with
weighted finite state transducers: The CUED MT system for
the 2008 ACL workshop on SMT. In Proceedings of the
Third Workshop on Statistical Machine Translation, pages
131?134, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proceedings of the 6th Applied Natural Language
Processing Conference: ANLP-2000, Seattle, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computational
Linguistics, 19(2):263?311.
T. Buckwalter. 2002. Buckwalter Arabic morphological ana-
lyzer.
Marta Ruiz Costa-jussa` and Jos?e A. R. Fonollosa. 2005.
Improving phrase-based statistical translation by modifying
phrase extraction and including several features. In Proceed-
ings of the ACL Workshop on Building and Using Parallel
Texts, pages 149?154, June.
Yonggang Deng and William Byrne. 2005a. HMM word and
phrase alignment for statistical machine translation. In Proc.
of HLT-EMNLP.
Yonggang Deng and William Byrne. 2005b. JHU-Cambridge
statistical machine translation toolkit (MTTK) user manual.
Yonggang Deng, Shankhar Kumar, and William Byrne. 2007.
Segmentation and alignment of parallel text for statistical
machine translation. Journal of Natural Language Engineer-
ing, 13:3:235?260.
Alexander Fraser and Daniel Marcu. 2006. Measuring word
alignment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern Califor-
nia, May.
Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-NAACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. 2009.
Hierarchical phrase-based translation with weighted finite
state transducers. In Procedings of NAACL-HLT, 2009,
Boulder, Colorado.
Abraham Ittycheriah, Yaser Al-Onaizan, and Salim Roukos.
2006. The IBM Arabic-English word alignment corpus, Au-
gust.
A. Kannan, M. Ostendorf, and J. R. Rohlicek. 1994. Maxi-
mum likelihood clustering of Gaussians for speech recogni-
tion. Speech and Audio Processing, IEEE Transactions on,
2(3):453?455, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human
Language Technology, pages 48?54.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine translation. In
Proceedings of MT Summit VIII, pages 247?252, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hierarchical
models for statistical machine translation of inflected lan-
guages. In Proceedings of the workshop on Data-driven
methods in machine translation, pages 1?8, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation. In Pro-
ceedings of the 18th conference on Computational Linguis-
tics, pages 1086?1090.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proceedings of NAACL.
Franz Josef Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D. thesis,
Franz Josef Och.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In ACL ?03: Proceedings of the 41st
Annual Meeting on Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pages 311?318.
Maja Popovic? and Hermann Ney. 2004. Improving word align-
ment quality using morpho-syntactic information. In In Pro-
ceedings of COLING, page 310.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing,
pages 133?142.
H. Singer and M. Ostendorf. 1996. Maximum likelihood suc-
cessive state splitting. Proceedings of ICASSP, 2:601?604.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for SMT using context-informed
features. In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation (TMI
2007), pages 231 ? 240.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP, pages 87?94.
Ismael Garc??a Varea, Franz J. Och, Hermann Ney, and Fran-
cisco Casacuberta. 2002. Improving alignment quality in
statistical machine translation using context-dependent max-
imum entropy models. In Proceedings of COLING, pages
1?7.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING, pages 836?841.
S. J. Young, J. J. Odell, and P. C. Woodland. 1994. Tree-based
state tying for high accuracy acoustic modelling. In HLT ?94:
Proceedings of the workshop on Human Language Technol-
ogy, pages 307?312.
118
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
1 Introduction
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al, 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
1.1 Related Work
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
433
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al, 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al, 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al, 2007; Rosti et al, 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al, 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
2 Hierarchical Translation with WFSTs
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N ? ??r,?r? / pr, with
?glue? rules, S ? ?X,X? and S ? ?S X,S X?. If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = ? log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings ?, ? ? {{S,X, V } ? T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N,x, y),
which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed
using a context-free grammar with rules N ? ?.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N,x, y) in the CYK grid, we build a
target language word lattice L(N,x, y). This lat-
tice contains every translation of sx+y?1x from every
derivation headed by N . These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S, 1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S, 1, J)
to obtain the final target language translation lattice
(Allauzen et al, 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al, 2007) to
implement our decoder.
2.1 Lattice Construction Over the CYK Grid
In each cell (N,x, y), the set of rule indices used
by the parser is denoted R(N,x, y), i.e. for r ?
R(N,x, y), N ? ??r,?r? was used in at least one
derivation involving that cell.
For each rule Rr, r ? R(N,x, y), we build a lat-
tice L(N,x, y, r). This lattice is derived from the
target side of the rule ?r by concatenating lattices
434
R1: X ? ?s1 s2 s3,t1 t2?
R2: X ? ?s1 s2,t7 t8?
R3: X ? ?s3,t9?
R4: S ? ?X,X?
R5: S ? ?S X,S X?
L(S, 1, 3) = L(S, 1, 3, 4) ? L(S, 1, 3, 5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) =
= A(t1)?A(t2)
L(S, 1, 3, 5) = L(S, 1, 2)? L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) =
= L(X, 1, 2, 2) = A(t7)?A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)
L(S, 1, 3) = (A(t1)?A(t2))? (A(t7)?A(t8)?A(t9))
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S,X). For
this reason it is divided in two subcolumns.
corresponding to the elements of ?r = ?r1...?r|?r |.
If an ?ri is a terminal, creating its lattice is straight-
forward. If ?ri is a non-terminal, it refers to a cell
(N ?, x?, y?) lower in the grid identified by the back-
pointer BP (N,x, y, r, i); in this case, the lattice
used is L(N ?, x?, y?). Taken together,
L(N,x, y, r) = ?
i=1..|?r|
L(N,x, y, r, i) (1)
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
L(N ?, x?, y?) else
(2)
where A(t), t ? T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N,x, y) is then built as the union of lattices cor-
responding to the rules in R(N,x, y):
L(N,x, y) = ?
r?R(N,x,y)
L(N,x, y, r) (3)
Lattice union and concatenation are performed
using the ? and ? WFST operations respectively, as
described by Allauzen et al(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N,x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S, 1, 3, 5) and
L(X, 1, 3, 1) in Figure 1; these are within {}.
2.2 A Procedure for Lattice Construction
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N,x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N,x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
435
R6: X ? ?s1,t20?
R7: X ? ?X1 s2 X2,X1 t10 X2?
R8: X ? ?X1 s2 X2,X2 t10 X1?
L(S, 1, 3) = L(S, 1, 3, 4) ?{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) =
={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)? L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) =
= A(t20)?A(t10)?A(t9)
L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)
L(S, 1, 3) = {(A(t1)?A(t2))} ?
?(A(t20)?A(t10)?A(t9))? (A(t9)?A(t10)?A(t20))?
?{(A(t7)?A(t8)?A(t9))}
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within {}.
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al, 2007).
1 function buildFst(N,x,y)
2 if ? L(N,x, y) return L(N,x, y)
3 for r ? R(N,x, y), Rr : N ? ??,??
4 for i = 1...|?|
5 if ?i ? T, L(N,x, y, r, i) = A(?i)
6 else
7 (N ?, x?, y?) = BP (?i)
8 L(N,x, y, r, i) = buildFst(N ?, x?, y?)
9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)
10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)
11 fstRmEpsilon L(N,x, y)
12 fstDeterminize L(N,x, y)
13 fstMinimize L(N,x, y)
14 return L(N,x, y)
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(S, 1, J). To make this exact, we define a function
g(N,x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N ?, x?, y?) = BP (N,x, y, r, i),
these special arcs are introduced as:
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N ?, x?, y?)) else
(4)
The resulting lattices L(N,x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices ?
such as lossless size reduction via determinization
and minimization ? can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice L(S, 1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al, 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ?lazy evaluation? techniques developed
by Mohri et al(2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
436
01
t1
2g(X,1,2)
3
g(X,1,1)
5
g(X,3,1)
7
t2
g(X,3,1)
4
t10
6t10
g(X,3,1)
g(X,1,1)
0
3g(X,1,1)
2g(X,1,2)
1
t1
4
g(X,3,1)
t10
6
g(X,3,1)
t2
5
t10
g(X,1,1)
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
delayed composition, etc.
2.4 Pruning in Lattice Construction
The final translation lattice L(S, 1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al, 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N,x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
3 Translation Experiments
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al, 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al, 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
437
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al, 2008).
? Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
3.1 Building the Rule Sets
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al (2009) we carry out two rule filtering strate-
gies:
? we exclude rules with two non-terminals with
the same order on the source and target side
? we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
3.2 Arabic-to-English Translation
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
X ? ??s,?s?
X ? ?V ,V ?
V ? ?s,t?
s, t ? T+; ?s, ?s ? ({V } ?T)+
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(?150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al, 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ?a? and ?b?, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
438
decoder mt02-05-tune mt02-05-test mt08
BLEU TER BLEU TER BLEU TER
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
3.3 Chinese-to-English Translation
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ?a?)
and by HiFST (row ?b?). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
439
decoder MET k-best tune-nw test-nw mt08
BLEU TER BLEU TER BLEU TER
a HCP HCP 31.6 59.7 31.9 59.7 ? ?
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
4 Conclusions
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557?
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings of NAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396?
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings of ASRU.
Jean-Ce?dric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
440
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP, pages 161?168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10?18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT, pages 1003?1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256?263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
441
Proceedings of NAACL HLT 2009: Short Papers, pages 73?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimum Bayes Risk Combination of Translation Hypotheses from
Alternative Morphological Decompositions
Adria` de Gispert? Sami Virpioja?
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
? Helsinki University of Technology. Adaptive Informatics Research Centre
P.O.Box 5400, 02015 TKK, Finland
{sami.virpioja,mikko.kurimo}@tkk.fi
Mikko Kurimo? William Byrne?
Abstract
We describe a simple strategy to achieve trans-
lation performance improvements by combin-
ing output from identical statistical machine
translation systems trained on alternative mor-
phological decompositions of the source lan-
guage. Combination is done by means of Min-
imum Bayes Risk decoding over a shared N-
best list. When translating into English from
two highly inflected languages such as Ara-
bic and Finnish we obtain significant improve-
ments over simply selecting the best morpho-
logical decomposition.
1 Introduction
Morphologically rich languages pose significant
challenges for natural language processing. The ex-
tensive use of inflection, derivation, and composi-
tion leads to a huge vocabulary, and sparsity in mod-
els estimated from data. Statistical machine transla-
tion (SMT) systems estimated from parallel text are
affected by this. This is particularly acute when ei-
ther the source or the target language, or both, are
morphologically complex.
Owing to these difficulties and to the natural in-
terest researchers take in complex linguistic phe-
nomena, many approaches to morphological anal-
ysis have been developed and evaluated. We fo-
cus on applications to SMT in Section 1.1, but we
note the recent general survey (Roark and Sproat,
2007) and the Morpho Challenge competitive evalu-
ations1. Prior evaluations of morphological analyz-
ers have focused on determining which analyzer was
1See http://www.cis.hut.fi/morphochallenge2009/ and links
best suited for some particular task. For translation,
we take a different approach and investigate whether
competing analyzers might have complementary in-
formation. Our method is straightforward. We train
two identical SMT systems with two versions of
the same parallel corpus, each with a different mor-
phological decomposition of the source language.
We combine their translation hypotheses perform-
ing Minimum Bayes Risk decoding over merged N-
best lists. Results are reported in the NIST 2008
Arabic-to-English MT task and an European Parlia-
ment Finnish-to-English task, with significant gains
over each individual system.
1.1 Prior Work
Several earlier works investigate word segmenta-
tion and transformation schemes, which may include
Part-Of-Speech or other information, to alleviate
the effect of morphological variation on translation
models. With different training corpus sizes, they
focus on translation into English from Arabic (Lee,
2004; Habash and Sadat, 2006; Zollmann et al,
2006), Czech (Goldwater and McClosky, 2005; Tal-
bot and Osborne, 2006), German (Nie?en and Ney,
2004) or Catalan, Spanish and Serbian (Popovic
and Ney, 2004). Some address the generation
challenge when translating from English into Span-
ish (Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). Unsupervised morphology learning is pro-
posed as a language-independent solution to reduce
the problems of rich morphology in (Virpioja et al,
there to earlier workshops. The combination scheme described
in this paper will be one of the evaluation tracks in the upcoming
workshop.
73
Arabic wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn
MADA D2 w+ qrrt >n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn
SAKHR w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn
English a preparatory committee of the whole of the general assembly is to be established at its fifty-second session
Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration.
2007). Factored models are introduced in (Koehn
and Hoang, 2007) for better integration of morpho-
syntactic information.
Gime?nez and Ma`rquez (2005) merge mul-
tiple word alignments obtained from several
linguistically-tagged versions of a Spanish-English
corpus, but only standard tokens are used in decod-
ing. Dyer et al (2008) report improvements from
multiple Arabic segmentations in translation to En-
glish translation, but their goal was to demonstrate
the value of lattice-based translation. From a model-
ing perspective their approach is unwieldy: multiple
analyses of the parallel text collections are merged
to create a large, heterogeneous training set; a sin-
gle set of models and alignments is produced; lattice
translation is then performed using a single system
to translate all morphological analyses. We find that
similar gains can be obtained much more easily.
The approach we take is Minimum Bayes Risk
(MBR) System Combination (Sim et al, 2007). N-
best lists from multiple SMT systems are merged;
the posterior distributions over the individual lists
are interpolated to form a new distribution over the
merged list. MBR hypotheses selection is then per-
formed using sentence-level BLEU score (Kumar
and Byrne, 2004). It is very likely that even greater
gains can be achieved by more complicated combi-
nation schemes (Rosti et al, 2007), although signif-
icantly more effort in tuning would be required.
2 Arabic-to-English Translation
For Arabic-to-English translation, we consider two
alternative segmentations of the Arabic words. We
first use the MADA toolkit (Habash and Rambow,
2005). After tagging, we split word prefixes and suf-
fixes according to scheme ?D2? (Habash and Sadat,
2006). Secondly, we take the segmentation gener-
ated by Sakhr Software in Egypt using their Arabic
Morphological Tagger, as an alternative segmenta-
tion into subword units. This scheme generates more
tokens as it segments all Arabic articles which other-
wise remain attached in the MADA D2 scheme (Ta-
ble 1).
Translation experiments are based on the NIST
MT08 Arabic-to-English translation task, includ-
ing all allowed parallel data as training material
(?150M English words, and 153M or 178M Arabic
words for MADA-segmented and Sakhr-segmented
text, respectively). In addition to the MT08 set itself,
we take the NIST MT02 through MT05 evaluation
sets and divide them into a development set (odd-
numbered sentences) and a test set (even-numbered
sentences), each containing ?2k sentences.
The SMT system used is HiFST, a hierarchical
phrase-based system implemented with Weighted
Finite-State Transducers (Iglesias et al, 2009). Two
identical systems are trained from each parallel cor-
pus, i.e. MADA-based and SAKHR-based. Both
systems use the same standard features and share
the first-pass English language model, a 4-gram es-
timated over the parallel text and a 965 million word
subset of monolingual data from the English Giga-
word Third Edition. Minimum Error Training pa-
rameter estimation under IBM BLEU is performed
on the development set (mt02-05-tune), and the out-
put translation lattice is rescored with large language
models estimated using ?4.7B words of English
newswire text, in the same fashion as (Iglesias et
al., 2009). Finally, the first 1000-best hypotheses
are rescored with MBR, taking the negative sentence
level BLEU score as the loss function to minimise.
For system combination, we obtain two sets of N-
best lists of depth N=500, one from each system.
Both lists are obtained after large-LM lattice rescor-
ing, i.e. prior to individual MBR. A joint MBR de-
coding is then carried out on the aggregated 1000-
best list with equal weight assigned to the posterior
distribution assigned to the hypotheses by each sys-
tem. Results are shown in Table 2.
As shown, the scores obtained via MBR combi-
nation outperform significantly those achieved via
MBR for the best-performing system (MADA). The
74
mt02-05-
-tune -test mt08
MADA-based 53.3 52.7 43.7
+MBR 53.7 53.3 44.0
SAKHR-based 52.7 52.8 43.3
+MBR 53.2 53.2 43.8
MBR-combined 54.6 54.6 45.6
Table 2: Arabic-to-English translation results. Lower-
cased IBM BLEU reported.
mixed case BLEU-4 for the MBR-combined system
on mt08 is 44.1. This is directly comparable to the
official MT08 Constrained Training Track evalua-
tion results.2
3 Finnish-to-English Translation
Finnish is a highly-inflecting, agglutinative lan-
guage. It has dozens of both inflectional and
derivational suffixes, that are concatenated together
with only moderately small changes in the sur-
face forms. For instance, one can inflect the
word ?kauppa? (shop) into ?kaupa+ssa+mme+kin?
(also in our shop) by glueing the suffixes to the
end. In addition, Finnish has many compound
words, sometimes consisting of several parts, such
as ?ulko+maa+n+kauppa+politiikka? (foreign trade
policy). Due to these properties, the number of dif-
ferent word forms that can be observed is enormous.
Morfessor (Creutz and Lagus, 2007) is a method
for modeling concatenative morphology in an un-
supervised manner. It tries to find morpheme-like
units, morphs, that are segments of the words. In-
spired by the minimum description length principle,
Morfessor tries to find a concise lexicon of morphs
that can effectively code the words in the train-
ing data. Unlike other unsupervised methods (e.g.,
Goldsmith (2001)), there is no restrictions on how
many morphs a word can have. After training the
model, the most likely segmentation of new words
to morphs can be found using the Viterbi algorithm.
There exist a few different versions of Morfessor.
The baseline algorithm has been found to be very
useful in automatic speech recognition of agglutina-
tive languages (Kurimo et al, 2006). However, it
2Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html
often oversegments morphemes that are rare or not
seen at all in the training data. Following the ap-
proach in (Virpioja et al, 2007), we use the Morfes-
sor Categories-MAP algorithm (Creutz and Lagus,
2005). It applies a hierarchical model with three sur-
face categories (prefix, stem and suffix), that allow
the algorithm to treat out-of-vocabulary words in a
convenient manner. For instance, if we encounter a
new name with a known suffix, it can usually sepa-
rate the suffix and leave the actual name intact.
Similarly to the Arabic-to-English task, we train
two identical HiFST systems. In this case, whereas
one is trained on Finnish morphs decomposed by
Morfessor (morph-based), the other is trained on
standard, unprocessed Finnish (word-based). For
this task we use the EuParl parallel corpus . Portions
from Q4/2000 was reserved for testing and Septem-
ber 2000 for development, both containing around
3,000 sentences. The training data comprised 23M
English words, and 17M or 27M Finnish tokens for
word-based or morph-based text, respectively.
The training set was also used to train the mor-
phological segmentation. The quality of the seg-
mentation is evaluated in (Virpioja et al, 2007). A
precision of 78.72% and recall of 52.29% was mea-
sured for the segmentation boundaries with respect
to a linguistic reference segmentation. As the recall
is not very high, the segmentation is more conserva-
tive than the linguistic reference. Table 4 shows an
example for a phrase in the training data.
Results are shown in Table 3, where again signifi-
cant gains are achieved when simply combining out-
put N-best lists via MBR. Only one reference was
available for scoring. In this case we did not ap-
ply large-LM rescoring, as no large additional par-
liamentary data was available. Individual MBR did
not yield gains for each of the systems.
devel test
Word-based 30.2 27.9
Morph-based 29.4 27.4
MBR-combined 30.5 28.9
Table 3: Finnish-to-English translation results. Lower-
cased IBM BLEU reported.
75
Finnish vaarallisten aineiden kuljetusten turvallisuusneuvonantaja
Morfessor vaaraSTM llistenSTM aineSTM idenSUF kuljetusPRE tenSTM turvallisuusPRE neuvoSTM nSUF antajaSTM
Linguistic vaara llis t en aine i den kuljet us t en turva llis uus neuvo n anta ja
English safety adviser for the transport of dangerous goods
Table 4: Example of Morfessor Categories-MAP segmentation and linguistic segmentation for a Finnish phrase. Sub-
scripts show the morph categories given by Morfessor: stem (STM), prefix (PRE) and suffix (SUF).
4 Conclusions
We demonstrated that multiple morphological anal-
yses can be the basis for SMT system combination.
These results will be of interest to researchers devel-
oping morphological analyzers, as it provides a new,
and potentially profitable way to evaluate compet-
ing analysers. The results should also interest SMT
researchers. SMT system combination is an active
area of research, but good gains from combination
usually require very different system architectures;
this can be a barrier to developing competitive sys-
tems. We find that the same architecture trained on
two different analyses is adequate to generate the di-
verse hypotheses needed for system combination.
Acknowledgments. This work was supported by the GALE
program of DARPA (HR0011-06-C-0022), the GSLT and AIRC
in the Academy of Finland, and the EMIME project and PAS-
CAL2 NoE in the EC?s FP7.
References
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Conf. on Adaptive Knowledge Representation
and Reasoning (AKRR).
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learning.
ACM Trans. Speech and Language Processing, 4(1).
A. de Gispert and J.B. Marin?o. 2008. On the impact
of morphology in English to Spanish statistical MT.
Speech Communication, 50.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In ACL-HLT.
J. Gime?nez and Ll. Ma`rquez. 2005. Combining linguis-
tic data views for phrase-based SMT. In ACL Work-
shop on Building and Using Parallel Texts.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
S. Goldwater and D. McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In HLT-
EMNLP.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-
NAACL: Short Papers.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In HLT-NAACL.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In EMNLP.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
M. Kurimo, A. Puurula, E. Arisoy, V. Siivola, T. Hir-
sima?ki, J. Pylkko?nen, T. Aluma?e, and M. Saraclar.
2006. Unlimited vocabulary speech recognition for
agglutinative languages. In HLT-NAACL.
Y.-S. Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL: Short Papers.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2).
M. Popovic and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In LREC.
B. Roark and R. Sproat. 2007. Computational Ap-
proaches to Morphology and Syntax. Oxford Univer-
sity Press.
A.V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In ACL.
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. C. Wood-
land. 2007. Consensus network decoding for sta-
tistical machine translation system combination. In
ICASSP, volume 4.
D. Talbot and M. Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In ACL.
N. Ueffing and H. Ney. 2003. Using POS information for
SMT into morphologically rich languages. In EACL.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. In MT Summit XI.
A. Zollmann, A. Venugopal, and S. Vogel. 2006. Bridg-
ing the inflection morphology gap for Arabic statistical
machine translation. In HLT-NAACL: Short Papers.
76
Proceedings of the Third Workshop on Statistical Machine Translation, pages 131?134,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
European Language Translation with Weighted Finite State Transducers:
The CUED MT System for the 2008 ACL Workshop on SMT
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|jjjb2|wjb31}@cam.ac.uk
Abstract
We describe the Cambridge University En-
gineering Department phrase-based statisti-
cal machine translation system for Spanish-
English and French-English translation in the
ACL 2008 Third Workshop on Statistical Ma-
chine Translation Shared Task. The CUED
system follows a generative model of trans-
lation and is implemented by composition of
component models realised as Weighted Fi-
nite State Transducers, without the use of a
special-purpose decoder. Details of system
tuning for both Europarl and News translation
tasks are provided.
1 Introduction
The Cambridge University Engineering Department
statistical machine translation system follows the
Transducer Translation Model (Kumar and Byrne,
2005; Kumar et al, 2006), a phrase-based generative
model of translation that applies a series of transfor-
mations specified by conditional probability distri-
butions and encoded as Weighted Finite State Trans-
ducers (Mohri et al, 2002).
The main advantages of this approach are its mod-
ularity, which facilitates the development and eval-
uation of each component individually, and its im-
plementation simplicity which allows us to focus on
modeling issues rather than complex decoding and
search algorithms. In addition, no special-purpose
decoder is required since standard WFST operations
can be used to obtain the 1-best translation or a lat-
tice of alternative hypotheses. Finally, the system
architecture readily extends to speech translation, in
which input ASR lattices can be translated in the
same way as for text (Mathias and Byrne, 2006).
This paper reviews the first participation of CUED
in the ACL Workshop on Statistical Machine Trans-
lation in 2008. It is organised as follows. Firstly,
section 2 describes the system architecture and its
main components. Section 3 gives details of the de-
velopment work conducted for this shared task and
results are reported and discussed in section 4. Fi-
nally, in section 5 we summarise our participation in
the task and outline directions for future work.
2 The Transducer Translation Model
Under the Transducer Translation Model, the gen-
eration of a target language sentence tJ1 starts with
the generation of a source language sentence sI1 by
the source language model PG(sI1). Next, the source
language sentence is segmented into phrases accord-
ing to the unweighted uniform phrasal segmenta-
tion model PW (uK1 ,K|sI1). This source phrase se-
quence generates a reordered target language phrase
sequence according to the phrase translation and re-
ordering model PR(xK1 |uK1 ). Next, target language
phrases are inserted into this sequence according to
the insertion model P?(vR1 |xK1 , uK1 ). Finally, the
sequence of reordered and inserted target language
phrases are transformed to word sequences tJ1 under
the target phrasal segmentation model P?(tJ1 |vR1 ).
These component distributions together form a joint
distribution over the source and target language sen-
tences and their possible intermediate phrase se-
quences as P (tJ1 , vR1 , xK1 , uK1 , sI1).
In translation under the generative model, we start
with the target sentence tJ1 in the foreign language
131
and search for the best source sentence s?I1. Encod-
ing each distribution as a WFST leads to a model of
translation as the series of compositions
L = G ? W ? R ? ? ?? ? T (1)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations ob-
tained during decoding. The most likely translation
s?I1 is the path in L with least cost.
2.1 TTM Reordering Model
The TTM reordering model associates a jump se-
quence with each phrase pair. For the experi-
ments described in this paper, the jump sequence
is restricted such that only adjacent phrases can be
swapped; this is the MJ1 reordering model of (Ku-
mar and Byrne, 2005). Although the reordering
probability for each pair of phrases could be esti-
mated from word-aligned parallel data, we here as-
sume a uniform reordering probability p tuned as de-
scribed in section 3.1. Figure 1 shows how the MJ1
reordering model for a pair of phrases x1 and x2 is
implemented as a WFST.
0 1
x : x
x2 : x1
x1 : x2
p / b=+1
1 / b=?1
1?p / b=0
Figure 1: The uniform MJ1 reordering transducer.
3 System Development
CUED participated in two of the WMT shared task
tracks: French?English and Spanish?English. For
both tracks, primary and contrast systems were sub-
mitted. The primary submission was restricted
to only the parallel and language model data dis-
tributed for the shared task. The contrast submission
incorporates large additional quantities of English
monolingual training text for building the second-
pass language model described in section 3.2.
Table 1 summarises the parallel training data, in-
cluding the total number of sentences, total num-
ber of words, and lower-cased vocabulary size. The
Spanish and French parallel texts each contain ap-
proximately 5% News Commentary data; the rest
is Europarl data. Various single-reference develop-
ment and test sets were provided for each of the
tracks. However, the 2008 evaluation included a new
News task, for which no corresponding development
set was available.
sentences words vocab
FR 39.9M 124k
EN
1.33M 36.4M 106k
ES 38.2M 140k
EN 1.30M 35.7M 106k
Table 1: Parallel corpora statistics.
All of the training and system tuning was per-
formed using lower-cased data. Word alignments
were generated using GIZA++ (Och and Ney, 2003)
over a stemmed version of the parallel text. Stems
for each language were obtained using the Snowball
stemmer1. After unioning the Viterbi alignments,
the stems were replaced with their original words,
and phrase-pairs of up to five foreign words in length
were extracted in the usual fashion (Koehn et al,
2003).
3.1 System Tuning
Minimum error training (Och, 2003) under
BLEU (Papineni et al, 2001) was used to optimise
the feature weights of the decoder with respect
to the dev2006 development set. The following
features are optimized:
? Language model scale factor
? Word and phrase insertion penalties
? Reordering scale factor
? Insertion scale factor
? Translation model scale factor: u-to-v
? Translation model scale factor: v-to-u
? Three phrase pair count features
The phrase-pair count features track whether each
phrase-pair occurred once, twice, or more than twice
1Available at http://snowball.tartarus.org
132
in the parallel text (Bender et al, 2007). All de-
coding and minimum error training operations are
performed with WFSTs and implemented using the
OpenFST libraries (Allauzen et al, 2007).
3.2 English Language Models
Separate language models are used when translating
the Europarl and News sets. The models are esti-
mated using SRILM (Stolcke, 2002) and converted
to WFSTs for use in TTM translation. We use the of-
fline approximation in which failure transitions are
replaced with epsilons (Allauzen et al, 2003).
The Europarl language model is a Kneser-
Ney (Kneser and Ney, 1995) smoothed default-
cutoff 5-gram back-off language model estimated
over the concatenation of the Europarl and News
language model training data. The News language
model is created by optimising the interpolation
weights of two component models with respect to
the News Commentary development sets since we
believe these more closely match the newstest2008
domain. The optimised interpolation weights were
0.44 for the Europarl corpus and 0.56 for the much
smaller News Commentary corpus. For our contrast
submission, we rescore the first-pass translation lat-
tices with a large zero-cutoff stupid-backoff (Brants
et al, 2007) language model estimated over approx-
imately five billion words of newswire text.
4 Results and Discussion
Table 2 reports lower-cased BLEU scores for the
French?English and Spanish?English Europarl
and News translation tasks. The NIST scores are
also provided in parentheses. The row labelled
?TTM+MET? shows results obtained after TTM
translation and minimum error training, i.e. our pri-
mary submission constrained to use only the data
distributed for the task. The row labelled ?+5gram?
shows translation results obtained after rescoring
with the large zero-cutoff 5-gram language model
described in section 3.2. Since this includes addi-
tional language model data, it represents the CUED
contrast submission.
Translation quality for the ES?EN task is
slightly higher than that of FR?EN. For Europarl
translation, most of the additional English language
model training data incorporated into the 5-gram
rescoring step is out-of-domain and so does not sub-
stantially improve the scores. Rescoring yields an
average gain of just +0.5 BLEU points.
Translation quality is significantly lower in both
language pairs for the new news2008 set. Two fac-
tors may account for this. The first is the change
in domain and the fact that no training or devel-
opment set was available for the News translation
task. Secondly, the use of a much freer translation
in the single News reference, which makes it dif-
ficult to obtain a good BLEU score. However, the
second-pass 5-gram language model rescoring gains
are larger than those observed in the Europarl sets,
with approximately +1.7 BLEU points for each lan-
guage pair. The additional in-domain newswire data
clearly helps to improve translation quality.
Finally, we use a simple 3-gram casing model
trained on the true-case workshop distributed
language model data, and apply the SRILM
disambig tool to restore true-case for our final
submissions. With respect to the lower-cased scores,
true-casing drops around 1.0 BLEU in the Europarl
task, and around 1.7 BLEU in the News Commen-
tary and News tasks.
5 Summary
We have reviewed the Cambridge University Engi-
neering Department first participation in the work-
shop on machine translation using a phrase-based
SMT system implemented with a simple WFST ar-
chitecture. Results are largely competitive with the
state-of-the-art in this task.
Future work will examine whether further im-
provements can be obtained by incorporating addi-
tional features into MET, such as the word-to-word
Model 1 scores or phrasal segmentation models. The
MJ1 reordering model could also be extended to al-
low for longer-span phrase movement. Minimum
Bayes Risk decoding, which has been applied suc-
cessfully in other tasks, could also be included.
The difference in the gains from 5-gram lattice
rescoring suggests that, particularly for Europarl
translation, it is important to ensure the language
model data is in-domain. Some form of count mix-
ing or alternative language model adaptation tech-
niques may prove useful for unconstrained Europarl
translation.
133
Task dev2006 devtest2006 test2007 test2008 newstest2008
FR?EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)
+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)
ES?EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)
+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)
Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.
Acknowledgements
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFST: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of the 2007 Automatic
Speech Understanding Workshop, pages 396?401.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lambert Mathias and William Byrne. 2006. Statistical
phrase-based speech translation. In 2006 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Meeting of the Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
134
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71?79,
Beijing, August 2010
Fluency Constraints for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood and Adria` de Gispert and William Byrne
Machine Intelligence Laboratory, Department of Engineering, Cambridge University
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
A novel and robust approach to improv-
ing statistical machine translation fluency
is developed within a minimum Bayes-
risk decoding framework. By segment-
ing translation lattices according to con-
fidence measures over the maximum like-
lihood translation hypothesis we are able
to focus on regions with potential transla-
tion errors. Hypothesis space constraints
based on monolingual coverage are ap-
plied to the low confidence regions to im-
prove overall translation fluency.
1 Introduction and Motivation
Translation quality is often described in terms of
fluency and adequacy. Fluency reflects the ?na-
tiveness? of the translation while adequacy indi-
cates how well a translation captures the meaning
of the original text (Ma and Cieri, 2006).
From a purely utilitarian view, adequacy should
be more important than fluency. But fluency and
adequacy are subjective and not easy to tease apart
(Callison-Burch et al, 2009; Vilar et al, 2007).
There is a human tendency to rate less fluent trans-
lations as less adequate. One explanation is that
errors in grammar cause readers to be more crit-
ical. A related phenomenon is that the nature of
translation errors changes as fluency improves so
that any errors in fluent translations must be rel-
atively subtle. It is therefore not enough to fo-
cus solely on adequacy. SMT systems must also
be fluent if they are to be accepted and trusted.
It is possible that the reliance on automatic met-
rics may have led SMT researchers to pay insuffi-
cient attention to fluency: BLEU (Papineni et al,
2002), TER (Snover et al, 2006), and METEOR
(Lavie and Denkowski, 2009) show broad corre-
lation with human rankings of MT quality, but are
incapable of fine distinctions between fluency and
adequacy.
There is concern that the fluency of current
SMT is inadequate (Knight, 2007b). SMT is ro-
bust, in that a translation is nearly always pro-
duced. But unlike translators who should be
skilled in at least one of the languages, SMT sys-
tems are limited in both source and target lan-
guage competence. Fluency and accuracy there-
fore tend to suffer together as translation quality
degrades. This should not be the case. Ideally, an
SMT system should never be any less fluent than
the best stochastic text generation system avail-
able in the target language (Oberlander and Brew,
2000). What is needed is a good way to enhance
the fluency of SMT hypotheses.
The maximum likelihood (ML) formulation
(Brown et al, 1990) of translation of source lan-
guage sentence F to target language sentence E?
E? = argmax
E
P (F |E)P (E) (1)
makes it clear why improving SMT fluency is a
difficult modelling problem. The language model
P (E), the closest thing to a ?fluency component?
in the original formulation, only affects candidates
likely under the translation model P (F |E). Given
the weakness of current translation models this is
a severe limitation. It often happens that SMT sys-
tems assign P (F |E?) = 0 to a correct reference
translation E? of F (see the discussion in Section
9). The problem is that in ML decoding the lan-
guage model can only encourage the production
of fluent translations; it cannot easily enforce con-
straints on fluency or introduce new hypotheses.
In Hiero (Chiang, 2007) and syntax-based SMT
(Knight and Graehl, 2005; Knight, 2007a), the
primary role of syntax is to drive the translation
process. Translations produced by these systems
respect the syntax of their translation models, but
71
this does not force them to be grammatical in the
way that a typical human sentence is grammati-
cal; they produce many translations which are not
fluent. The problem is robustness. Generating
fluent translations demands a tightly constraining
target language grammar but such a grammar is at
odds with broad-coverage parsing needed for ro-
bust translation.
We have described two problems in transla-
tion fluency: (1) SMT may fail to generate flu-
ent hypotheses and there is no simple way to in-
troduce them into the search; (2) SMT produces
many translations which are not fluent but enforc-
ing constraints to improve fluency can hurt robust-
ness. Both problems are rooted in the ML decod-
ing framework in which robustness and fluency
are conflicting objectives.
We propose a novel framework to improve the
fluency of any SMT system, whether syntactic or
phrase-based. We will perform Minimum Bayes-
risk search (Kumar and Byrne, 2004) over a space
of fluent hypotheses H:
E?MBR = argmin
E??H
?
E?E
L(E,E?)P (E|F ) (2)
In this approach the MBR evidence space E is
generated by an SMT system as a k-best list or lat-
tice. The system runs in its best possible config-
uration, ensuring both translation robustness and
good baselines. Rather than decoding in the out-
put of the baseline SMT system, translations will
be sought among a collection of fluent sentences
that are close to the top SMT hypotheses as deter-
mined by the loss function L(E,E?).
Decoupling the MBR hypothesis space from
first-pass translation offers great flexibility. Hy-
potheses in H may be arbitrarily constrained ac-
cording to lexical, syntactic, semantic, or other
considerations, with no effect on translation ro-
bustness. This is because constraints on fluency
do not affect the production of the evidence space
by the baseline system. Robustness and fluency
are no longer conflicting objectives. This frame-
work also allows the MBR hypothesis space to be
augmented with hypotheses produced by an NLG
system, although this is beyond the scope of the
present paper.
This paper focuses on searching out fluent
strings amongst the vast number of hypotheses en-
coded in SMT lattices. Oracle BLEU scores com-
puted over k-best lists (Och et al, 2004) show
that many high quality hypotheses are produced
by first-pass SMT decoding. We propose reducing
the difficulty of enhancing the fluency of complete
hypotheses by first identifying regions of high-
confidence in the ML translations and using these
to guide the fluency refinement process. This has
two advantages: (1) we keep portions of the base-
line hypotheses that we trust and search for alter-
natives elsewhere, and (2) the task is made much
easier since the fluency of sentence fragments can
be refined in context.
In what follows, we use posterior probabilities
over SMT lattices to identify useful subsequences
in the ML translations (Sections 2 & 3). These
subsequences drive the segmentation and transfor-
mation of lattices into smaller subproblems (Sec-
tions 4 & 5). Subproblems are mined for fluent
strings (Section 6), resulting in improved transla-
tion fluency (Sections 7 & 8). Our results show
that, when guided by the careful selection of sub-
problems, fluency can be improved with no real
degradation of the BLEU score.
2 Lattice MBR Decoding
The formulation of the MBR decoder in Equation
(2) separates the hypothesis space from the evi-
dence space. We apply the linearised lattice MBR
decision rule (Tromble et al, 2008)
E?LMBR = argmax
E??H
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
,
(3)
whereH is the hypothesis space, E is the evidence
space, N is the set of all n-grams in H (typically,
n = 1 . . . 4), and ? are constants estimated on
held-out data. The quantity p(u|E) is the path pos-
terior probability of n-gram u
p(u|E) =
?
E?Eu
P (E|F ), (4)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of paths containing n-gram u at least once.
The path posterior probabilities p(u|E) of Equa-
tion (4) can be efficiently calculated (Blackwood
et al, 2010) using general purpose WFST opera-
tions (Mohri et al, 2002).
72
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
p
r
e
c
is
io
n 
p n
,
?
 
 
1?gram
2?gram
3?gram
4?gram
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
20
25
30
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
n
?
g
r
a
m
 
c
o
u
n
t
s
 
 
1?grams
2?grams
3?grams
4?grams
Figure 1: Average n-gram precisions (left) and counts (right) for 2075 sentences of NIST
Arabic?English ML translations at a range of posterior probability thresholds 0 ? ? ? 1. The left
plot shows at ? = 0 the n-gram precisions used in the BLEU score of the ML baseline system.
3 Posterior Probability Confidence
Measures
In the formulation of Equations (3) and (4) the
path posterior n-gram probabilities play a crucial
role. MBR decoding under the linear approxima-
tion to BLEU is driven mainly by the presence
of high posterior n-grams in the lattice; the low
posterior n-grams contribute relatively little to the
MBR decision criterion. Here we investigate the
predictive power of these statistics. We will show
that the n-gram posterior is a good predictor as to
whether or not an n-gram is to be found in a set of
reference translations.
Let Nn denote the set of n-grams of order n
in the ML hypothesis E?, and let Rn denote the
set of n-grams of order n in the union of the ref-
erences. For confidence threshold ?, let Nn,? =
{u?Nn : p(u|E) ? ?} denote the n-grams inNn
with posterior probability greater than or equal to
?, where p(u|E) is computed using Equation (4).
This is equivalent to identifying all substrings of
length n in the translation hypotheses for which
the system assigns a posterior probability of ? or
higher. The precision at order n for threshold ? is
the proportion of n-grams in Nn,? also present in
the references:
Pn,? =
|Rn ? Nn,?|
|Nn,?|
(5)
The left plot in Figure 1 shows average per-
sentence n-gram precisions Pn,? at orders 1. . .4
for an Arabic?English translation task at a range
of thresholds 0 ? ? ? 1. Sentence start and end
tokens are ignored when computing unigram pre-
cisions. We note that precision at all orders im-
proves as the threshold ? increases. This confirms
that these intrinsic measures of translation confi-
dence have strong predictive power.
The right-hand side of the figure shows the av-
erage number of n-grams per sentence for the
same range of ?. We see that for high ?, there are
few n-grams with p(u|E) ? ?; this is as expected.
However, even at a high threshold of ? = 0.9
there are still on average three 4-grams per sen-
tence with posterior probabilities that exceed ?.
Even at this very high confidence level, high pos-
terior n-grams occur frequently enough that we
can expect them to be useful.
These precision results motivate our use of path
posterior n-gram probabilities as a confidence
measure. We assign confidence p(E?ji |E) to sub-
sequences E?i . . . E?j of the ML hypothesis.
Prior work focuses on word-level confidence
extracted from k-best lists and lattices (Ueffing
and Ney, 2007), while Zens and Ney (2006)
rescore k-best lists with n-gram posterior proba-
bilities. Similar experiments with a slightly dif-
ferent motivation are reported by DeNero et al
(2009); they show that expected n-gram counts in
a lattice can be used to predict which n-grams ap-
pear in the references.
4 Lattice Segmentation
We have shown that current SMT systems, al-
though flawed, can identify with confidence par-
73
the newspaper ? constitution ? quoted brigadier abdullah krishan , the chief of police in karak governorate ( 521 km
south @-@ west of amman ) as saying that the seizure took place after police received information that there were
attempts by the group to sell for more than $ 100 thousand dollars , the police rushed to the arrest in possession .
0 11/313.69
2
3
3
7/4.3574
4
23/0.068359
53580
63
7
23/0.0097656
81300
93/4.6934
104/5.7402
11
5/4.7529
12
23
38
1300/2.9102
13
23
14
3580/2.248
15
1300
18
3580
16
4/4.0488
1723/2.5654
1300
19
23/1.2598
3/0.66016
2023
23/1.5156
32
3
231300
21
3/2.8027
225/6.2256
4423
24
1300
23
4/2.5117 25
23
30
23
2623
29
3580
27
3/2.1895
2810/3.9199
1300
1300
3/1.1123
1300
23
33
4/4.4336
31
1300
3
4/1.6416
23
3580
3/2.1895
3
365614
34
3580
353580
375614
4/1.8594
23
39
23
415614
425614
4312316
5512316
575614
56
5614
69
12316
49
12316
50
12/2.9502
52
7359
53
12272/1.9209
51
42/1.04
4512/2.9502
46
42/1.3105
47
7359
48
12272/1.9209
54
8615/6.5205
58
12316
40
12/2.9502
42/1.3105
7359
12272/1.9209
5942
60
7359
618615/3.6523
62
12272/0.66504
112
58332/5.0586
63
4/0.5752
11036/6.5117
64
309/6.1533
65
755/6.3652
66
58332
68
58332
67
4/0.12109
7359
12272/1.9209
42/1.04
707359
7142
58332
72
4/0.5752
58332
1234/0.12109
7359
12272/0.66504
12316
58332/3.2891
747359
758615/3.9463
76
12272/0.91699
78
58332
774/0.69727
79
12
58332
80
4/0.52637
4
58332/4.6484
132
3
81
515/4.1318
82
755/2.9941
58332
83
515
84
5
4
85
3/4.2812
86
309/3.6953 87
515/3.8184
88
755/3.9072
4
755/3.9072
89
3/4.2812
90309/3.6953
91
515/3.8184
58332/3.1895
755/4.7812
3
515/3.9678
58332
7359
12272/0.91699
3
755/2.9941
3
94
12
10565
12272/1.9209
42/1.3105
73
7359
4/0.5752
58332
9358332
92
4/0.69727
58332/0.086914
12
58332
4/0.52637
4
755/3.9072
3/4.2812
309/3.6953
96515/3.8184
58332/4.7764
3
755/3.123
95
515/4.2607
97
10565
58332/3.1895
3
515/3.9678
139755
985
99
5
100
5
101
309
755
102
515/1.4756
103
515
104
755/1.0029
309/2.4023
105
5
106
5
755
755/1.0029
107
515
108
5
4
755/3.9072
515/3.8184 109
309/3.6953
3
755/3.123
111
5
5
58332
4/1.5498
113
3
114
309
115
3
116
95850/0.21289
138
9
1175
5
4/2.9111
95850/0.44141
118
95850309
1193/0.63672
120
95850/1.582
121
309
5
95850/0.44141
309
3/0.63672
515
122
3
95850/0.75391
124
309
9
95850/1.751
95850
125
8716
1263/0.24902
127
95850
309/0.16016
128
3
1298716
95850/0.39355
130
309
1311008
9
95850/0.91602
309
58332/0.84375
95850/0.12402
133
9
134
15/2.5664
153
16
309
95850/0.9082
16
95850/0.12402
9
136309
95850/0.074219
141
3
14213267
15713267
1358716
1505
143
16/6.207
1378716
2035/4.6826
15/2.5664
16
144
13267/5.418
16
1405
146
8716
1475108
14895850
13267
1495108
145
309
9
95850/6.04
5
168
8/2.209
151
368
154368
1703
171
12
1761001/1.9658
15/1.5693
16
13267/3.3818
12 1591001
160
5
152 156
5 1627375
1635108
155 5
161
7375/4.4355
166
7375
181
15
16715
8/4.0586
14226/5.498
169
368
158 16412 1721001
177
5
1657375/5.1436
1807375
17315
174
15
18320
175
20
12
1001/2.3379
5
4/0.94141 20
4/0.9873
20 1791976
368
1861976 189
13
1843/4.5869
16/6.084
178
15
4/0.9873
20
1821976/5.3818 18513
1923
1874854
1883
13
190714
1914854
196
336
195714
193
31/1.3125
197
57/1.459
1984854
205
185
204336
199
4/5.2324
200
185/1.6523
201626
202236
714
31/1.3125
57/1.459
194
63/6.7617
206
185
207
3
20813/3.3818
20924/1.8857
626/0.54102
210
90/0.18164
211
309/1.875
212505
203
626
214
505
215
185
217309
216
3/0.79297
213336
218
309
220309
219
90/3.8145
2213
222
505
22331
225505
224
90/1.124
226
90
227185
288
90
309
3/0.79297
228
24/5.6748
626/4.7695
229
90/4.7461
233
505
232
90/4.0713
231
309
230
90/3.8027
505
505
90/2.3574
234
57
235
90
236
90
237505
239
90
23831
240
5/4.6641
2418
2423
2978
243
505
24431
505
90/3.7266
2455
8/0.95117
266
57/1.7461
24790
24631
24850
5
8
5/2.1689
8
249
3/3.0986
250
505
25113
2523
2533
255
309
254
90/4.8203
25690
257505
2583
278
50
259
505
26013
261505
309 26213
263
83
264309
265309
505
57
267
8
8
2685/2.1689
269309
270
13
271
83
272
3029/2.8291
273
8
27483
275
29
337
63/1.9453
27613
27713
279
3
3083
2803
2814713
28283 63
28329/0.66406
28429
3
29
2853029
3563029
28683
83
2873029/2.8857
505
289309
290309
29113
63/0.8457
29
2923029
293
194
36217
29
63/1.5664
29529
296
63/0.29004
298
13
29913
30083
30117
302
87
303
3
30410/0.21484
306
194
307194
30983
31083 29
63/1.7354
3
305140/5.4287
312
10/0.21484
313
17
294 3
10/0.65234
140/6.0684
314
140
315
1188/5.5186
316
140
317
1188/3.1943
318
5
31987
32087
321309
63
29/1.043
63
140
1188/2.3926
323
3
32410/0.125
326
8
3255/0.27246
327
11/2.5967
328
1188/0.80762
329140
330
5
331
11/5.2559
332
1188/2.5459
333
140
334
1188
33517
33617
31113 32283 29/0.15527
63
400
140
1188/2.4033
338140
341
3/2.9023
342
1188/6.0127
343
3996
339
1188
340
2710/4.4717
401
3
344
5/4.9404
345
8/1.1172
346
11
347
3848/3.9785
348
8
2710/4.2734
349
1188
350
3
3848/3.3076
402
11
351
8/0.44629
8
352
11/2.667
354
8
353
4/0.53027
3
355
10/0.125
3
10/0.20996
11/2.8037
357
5
358
8/2.1064
359
1188/1.2041
360
5
3611188/2.4062
4/1.6289
365
8
363
3/5.5244
364
5/3.4756
366
11/1.3096
367
1579/6.1328
368
3848/3.5664
8
369
5/1.5518
370
11/1.7031
3711579
3848 374
11
372
3/3.1328
373
9/3.7803
375
81/4.0898
376
1188/2.4893
377
1704/4.7871
378
2710/5.6816
379
1579
1579
3848/0.19922
3/0.019531
380
3996
3996/4.1592
3821579
381
3/2.3301
383
1649/4.8389
384
3848/1.6953
385
11
3996
3512/3.4209
3996/2.5566
4/3.2705
388
8
3863/3.7451
387
5/2.584
389
11/3.0176
390
1579/1.8506
391
1649/5.4414
392
3848/0.073242
393
1579
3/2.3301
395
1579
3963848/1.6953
3943996
1579/0.65039
397
3
399
3996
398
8
140
17
1188
11
8/0.44629
1579/6.5
3848/6.1123
3996
406
3/5.6807
403
1579
1579/1.1162
3848/1.3164
404
3
405
1649/2.5039
3996/4.21391649/4.8936
407
3
408
1579/0.054688
409
3848/1.75
410
11
411
11
3
1579
412
5
413
9/2.7051
414
11/2.5723
415
1188/3.8848
416
1704/4.084
81
421
38/4.04
422
143/4.9678
423
188/6.5078
417
1188
4182710/0.94434
419
569
420
775/0.1084
714
163
775/2.3652
425
11
424
9/0.68457
426
20/4.0977
427
130/1.0742
4281704/0.60352
429
3431/4.0811
430
8/0.40527
431
21
432
9
4331704/1.2285
9/3.7803
438
11
439
1704/4.7871
775/5.4775
443
11441
5/4.5166
442
9/1.3047
444
20/5.0928
445
130/3.2822
446
1704/1.4395
440
1579
447
1704
81/3.7598
449
11
448
9/2.5488
450
130/3.375
451
1704/1.54
452
8246/3.0615
81
3848/6.1123
3996
3/5.6807
453
1579
1579/2.0078
3848/2.208
454
3
3/0.21191 3848/1.9619
455
1579
775/4.3457
5/3.6152
11
456
9/1.7539
457
1704/1.4248
458
1704
81/5.1875
11
459
9/4.9961
4601704/4.542
461
5
11
462
1579
463
3996
9/3.7803
81/5.3926
1704/4.7871
20/6.6523
464
11
1188
3996
8/1. 172
11
1704/1.0518
465
11
3848/2.8662
4661579
1704
467
1579
468
1579
5/4.5166
9/1.3047
20/5.0928
1704/1.4395
469
11
470
130/3.2822
11
9/2.5488
1704/1.54
471
130/3.375
472
8246/3.0615
81
473
38/3.3545
474
143/3.3359
81
475
38/4.7412
476
143/4.5361
81/0.95898
542
1188
477
2710/2.8506
478
569/0.63574
479
775
81
38/1.4326
481
1704
480
9/0.081055
21
775/1.7617
9/0.25488
20/3.3711
1704
482
11/1.3154
483
130/0.80176
11
1704/2.5254
484
9/0.31934
485
11
48611
487
561
488
74
489
188/1.9863
490
3250/0.032227
491
3745/1.8066
492
4816/0.54199
493
143
494
143
81
38/1.3213
497
3/3.3994
496
775
495
569/0.53906
498
3431
81
4993/1.8145
50038/1.2832
501
143/0.49512
502
3745/0.70996
503
8
504
21/0.38281
505
8
81
3/1.2666
50638/0.38281
81
507
3/1.6318
775/0.014648
508
569
8
4345
435
9/4.2754
436
11/3.8105
437
1188/3.6299
81/2.2656
1188
2710/2.8506
775
81/0.10449
509
38
130/0.4707
511
1704
510
9/0.081055
81
38/4.04
512
143/4.9678
21
513
5
5149/1.4326
515
11/0.9668
516
1704/2.2139
81
38/2.5791
3/5.5254
519
143/3.0596
775/0.79395
81
517
1704/0.10059
775/1.0488
518
569
520
3431
81/1.75
522
38
521
3/2.3906
523
143/0.81934
524
8
525
21/1.2314
526
8
81
38/3.6436
528
775
527
569/0.38184
81/0.98145
38
3/0.7793
21/1.4082
529
8
81
38/1.166
530
3/0.90234
531
143/0.37305
532
561/0.8584
5/1.2695
9/0.99316
11
533
1704/1.4629
3848/3.7578
534
1579
536
11
535
9/1.3047
537
1704/1.7061
569
775/0.43359
539
21
5388/0.4082
21
5408/0.33887
775
569/0.38184
21/0.69336
541
8
1188
11
9/1.3047
130/3.2822
1704/1.4395
11
1704/1.54
5
54311
81
38/4.04
188/6.5078
143/4.9678
38
5/2.417
9/1.4287
544
11
545
1704/2.7559
5
11/0.92969
20/4.6992
5
9/1.4326
11/0.9668
1704/2.2139
546
130/5.1309
81
38/2.5791
143/3.0596
81/1.75
38
143/0.81934
38
81
143/0.37305
547
561
74/0.45117
3250
4816/0.50977
548
561
3250
4816/0.50977
549
74/0.71582
775/2.4541
20/3.7178
1704/0.69238
3431/3.6172
550
9
551
11/0.39551
552
130/0.37207
553
9
55411
555
11
556
8
775
81
38/0.22363
3/2.1543
81
38/1.2832
143/0.49512
3745/0.70996
557
3/0.82227
775/0.99414
569
81/0.38965
558
10
81
559
38/1.5928
560
5/2.4922
561
3745
562
188
563
1191
565
1191
564
143/0.625
566143
567
1191/0.20703
1191
568
1191
57111
57011
572
561
8
81/0.56348
573
561
574
561
188/1.9551
3250
3745/1.7744
4816/0.50977
575143
81
577
38/0.38281
576
3/2.0371
81
578
3/1.6318
81/0.09082
38
579
561
580
561
581
11
582
561
21/0.38281
8
775
569/0.53906
74
3250/0.032227
4816/0.54199
775/2.917
81
583
11/3.3545
584569/3.9336
585
1704/2.4365
775
586
569/0.63574
81/0.10449
3/3.4795
38
21/0.51562
587
8
5888
589
11
188/2.8662
3250
3745/0.82031
4816/1.4219
590
74/0.55664
5918
593561
592
561
3250
3745/0.82031
4816/1.4219
81/1.4834
3/3.4424
38
81
594
3/0.89746
38
596
11
595
11
81/0.71484
38
597
561
3250
4816/0.50977
3745
598
5/0.037109
599
4816/0.43164
21
8/0.6377
5/2.0967
9/1.4287
11
1704/2.7559
81
38/2.8457
775/1.0488
600569
21/0.96484
6018
81
3/1.5
81
3/2.3506
38/0.38281
81
38/0.15137
81
38/0.38281
81
38/4.5273
81
38/1.9385
21
8/0.6377
38
602
3745
3745
188
603
81/0.34082
775/0.014648
610
569
81/0.25586
10
81
3/2.1406
38/1.5928
81
38/0.38281
81/0.56348
611
561
612
2339
613
561
614
188
615
143
6161191
617
3356
6193356
6181191
6201191
621
3356
622
3356
5691191
623
3356
81/0.28613
10
81
3/3.3311
38/1.5928
5
6245
625
4816/0.8916
3745
5/1.5254
6261191
628
561
627
561
629561
630
3745
631
5
10
5/4.0293
3745
81/0.56543
38
632
11
21/1.7168
633
8
634
11
81/1.1543
38
81/1.4834
38
81/0.25586
635
10
636
38/3.7988
188
637
3250/1.4678 638
3745/0.75
81/0.84473
38
5
639
3745
640
561
81
10/0.13965
81
3/1.7939
38/1.5928
5
4816/0.048828
641
188
642
1191
64311
81/1.2168
38
775
604569/0.53906
81
38/1.6504
3/3.9385
81
38/1.2832
3745/0.70996
606
143/0.49512
605
3/1.5205
607
561/3.8506
608
3250/3.5908
609
4593/3.8887
1191/3.2842
143
163
644
11
188/1.9551
3745/1.7744
646
3250
647
4816/0.50977
81/0.56348
645
561
3745
5/0.037109
648143
5
649
11
5
4816/0.8916
6509/1.6494
737
5
5
3745/0.52344
652
1191
651
143/2.3555
653
1191
654
3356
655
1032
656
3356
657
1032
6583356
659
1032
6601032
661
1032
662
2295/6.8135
663
188
1191/0.25391
664
143
6653356
5
6664816/1.8076
5/3.1465
3745
5
667
3745/1.3447
1191/3.7744
143
668
188
81
10/1.3828
81/1.2715
38
81/0.25586
669
10
670
2339
671
561
1191/0.5625 672
143
143/1.8193
1191
1191/2.8828
143
4816/2.0596
5
3745/1.3447
1191
6733356
81/0.25586
674
10
81/0.28613
10
38/3.8291
3745/2.3125
5
4816/0.8916
9/1.6494
1191
143/0.625
675
4376/3.2324
1191
676
143/2.5205
677
4376/2.5254
678
1191
81/0.52148
10
6794593
680
81/2.3428
681116/4.3838
682
188
6843356
6831191
6853356
686
1032
687
4
689
4
688
3/2.4316
690
71/1.3447
692
4
691
3/1.8115
693
4
3/1.8115
694
4
4
71/3.583
3/1.8115
695
4
696
6/6.2324
697
7/6.3906
698
9/5.1299
699
24/3.6904
700
77/4.2246
701
119/3.668
702
1032
143/1.918
1191
7031191
704
1032
143
1191/0.83887
143
1191
143/1.918
705
2339
5
706
9/4.5615
740
3745/1.1836
5
3745/0.57617
707
1191
7081032
709
2339
710
1032
7111191
712
1032
7133356
715
5
7173356
7181032
3/3.6035
71/2.5166
4
4
3/1.8115
7193
720
309/4.5137
721
9331/2.3145
309/2.1367
723
9331
724
22194/2.8037
722309
7259331
3
9331/2.3145
726
309
309/2.1367
9331
22194/2.8037
727
3/2.9238
3
71/4.9521
524/5.0078
119/4.9316
3
309/4.5137
9331/2.3145
22194/5.1182
728
24/5.543
48 7299331
730
3
731
9331
732
22194/1.9717
22194/1.3242
733
9331
734
3
735
3
736
3356
3/1.8115
4
5
3745/1.5127
738
4593
1191/1.6035
143
739
3356
4
5
3745/1.4502
741
4
742
3356 3/0.46582
743
4
744
1032
1191
143/0.66602
716
163
1191
143/1.6924
8
1191
143/1.5361
3/2.4316
71/1.3447
4
4
71/3.583
3/1.8115
119/3.668
745
24/3.6904
746
77/4.2246
747
309
749
9331
748
1899/1.582
750
8
751309/4.4062
7529331
8
309/4.4062
753
3/4.7422
754
8
7558
757
9331
756
1899/4.4609
758309
22194/0.54492
759
9331
760
8
761
309
762
8
3/0.42383
8
763
8
764
309
765
309
71/1.3447
8044
766
5
4
854
3
767
9331/0.080078
4
524/0.36816
3
9331/0.080078
4
9331
9331
1899/4.3027
803
9331
768
2925/6.7461
770
8
7692925
309/1.9824
771
3
772
8
7738
309
309
309/2.1914
7743
776
8
7752925
777
9331
778
8
779
3
780
9331
309/0.55273
3
309/0.96387
3
781
8/1.3115
78221215
9331
9331
783
4376
784
309
785
8
7878
786
1716
7888
1716/2.3975 789
3
790
2318/1.6875
791
309
792
2318
795
3
793309
794
8
1716/2.4541
3
7962318/1.7441
797
8
309/1.1377
3
798
309
799
8
8002318
8018
802
1032
3
805
9
8072318
8092318
808
1716/0.81934
810
7/0.6084
811
161
8/3.6729
8121716
813
11070/2.5137
161
11070/3.251
8162318
815
1716/0.81934
814
124/3.165
817
5763/4.4824
818
7235/5.5088
819
11744/5.4629
1716
11070/2.5137
820
2318
7/0.6084
823161
821
3/4.7285
879
4/3.4551
822
9/0.92676
3
2318/1.7441
1716
3
2318/1.7441
824
7
826
2318
825
3/2.9131
827
4
9331
828
3005
1716/2.4541
3
2318/1.7441
806
45/5.8838
829
2318
161
830
7/0.77637
833
9
832
7/0.30078
831
9
57/3.4434
834
9
835
109/2.0273
8363005/0.50586
837
9
9
7/4.2412
838
161/4.2617
839
9
161
7/0.98242
845
9/0.92676
9
843
7/0.30078
842
4/3.3135
4/5.543
9 841
7/4.8145
8405
844
9
9
9/0.125
846
7
9
848
4/3.5527
8497/2.3818
880
3005
881
9
45/2.6279
847
3005
85021215
9/0.92676
852
7/0.6084
853
161
8512318
3
5
8846/0.25
7/0.55762
9
9
45/4.8506
3005
45/2.6006
3005
57/2.3096
9
855
3/1.5703
856
5/0.34961
857
3005/0.019531
45
858
3005/0.84277859
63
5/2.6631
6
45/0.20508
860
3005
9
45/1.249
3005
861
45
45/0.48828
8629
21/3.7686
45/5.5586
57/2.3096
9
109/4.7979
3/3.0703
5/1.8926
3005/0.019531
864
63/5.4697
7/3.0781
863
9
865
3005
3005
9
6
5/0.94043
6
7/0.5791
9
3005/2.1006
866
9
86799
868
7/0.30078
9
109/2.0273
9
869
7/2.3818
8822
870
3005
3
5/3.1211
6
6
8835/0.24609
871
9
5/0.89355
6
872
2318
45
873
3005/0.84277
45
874
3005/2.4316
875
9
5/0.99414
6
45
45
3005/1.2275
45
3005/1.5234
9
3005/0.019531
9
5/2.0693
6
161/2.6025 6
8763005
877
7
5/0.24609
6
5
878
3005
5/0.28711
6
3/2.0576
57/1.9053
9
5
6/0.25098
H1 H2 H3 H4 H5 H6 H7 H8 H9
0 11
23
3
7
423
5
3580
63
7
23 8
1300
9
3
10
4
115
1223
13
1300
23
143580
25
1300
15
4
1623
17
3580
1300
18
23
3
1923
23
203
21
3
22
5 23
1300 24
23
23
4
2623
27
23
28
3
2910
30
3580
31
23
1300
1300
3
1300
321300
3
23
33
4
345614
3 35
3580
363580
37
5614
385614
23
4
3923
4012316
3
3580
41
5614
425614
43
12316
4412316
45
5614
46
12
47
42
48
7359
49
12272
50
12316
51
12316
52
12
53
42
64
7359
12272
12
42
7359
12272
548615
55
12316
56
42
7359
12272
57
8615
63
58332
58332
58
4
36
58332
4
42
7359
12272
7359
5942
7359
12272
58332
6012
7359
12272
42
12272
7359
58332
61
8615 62
12
58332
7359
12272
10565
5833212
10565
0 14 23 3755 45 5309 69 0
13
2
95850
38716
52035
8716
45 95850
0 116 213267 35108 4368 512 61001 75 87375 915 1020 111976 1213 133 144854
0 131
2
57
3
714
44
5
185
6236
7336
10185
3
8336
185
9185
3
0 1309 2505 390 413
0
183
2
3029
329
63
4
29
53029
6
194
717
887
9
3
1010
11
140
1217
13
140
14
1188
15140
16
1188
17
5
18
3
1910
205
21
8
2211
23
1188
24
140
255
2611
27
1188
28140
29
1188
30
140
1188
31
140
32
1188
332710
343
35
1188
36
3996
373
38
5
39
8
40
11
41
3848
42
8
2710
43
1188
443
3848
45
8
46
11
8
47
11
48
4
49
8
11
505
51
8
52
1188
535
54
1188
4
55
3
56
5
57
8
58
11
59
1579
603848
8
61
5
62
11
631579
3848
64
3
65
9
66
11
67
81
68
1188
69
1704
70
2710
711579
1579
3848
3
72
3996
3996
73
3
74
1579
751649
76
3848
77
11
3996
3996
3512
4
78
3
79
5
808
81
11
82
1579
83
1649
84
3848
851579
86
3996
3
87
1579
883848
1579
89
3
90
8
91
3996
1188
3996
8
11
1188
8
11
92
1579
1579
3848
93
3
94
1649
1579
3848
3996
95
3
3996
1649
96
3
97
1579
98
3848
99
11
100
11
3
1579
101
5
102
9
103
11
1041188
105
1704
1061188
1072710
108569
109
775
81
110
38
111
143
112
188
113
163775
9
114
11
115
20
116
130
117
1704
118
3431
1198
120
21
121
9
122
1704
5
123
9
124
11
125
1188
9
126
11
127
1704
128
1579
775
129
5
130
9
131
11
132
20
133
130
134
1704
1351704
81
9
136
11
137
130
138
1704
1398246
81
140
1579
1579
3848
3
3848
3996
3
3
3848
141
1579
775
5
11
9
1704
1704
81
11
1704
142
9
1435
11
9
11
130
1704
11
1704
144
1579
3996
9
81
1704
20
145
11
1704
146
11
3848
147
1579
1704
148
1579
149
1579
5
9
20
1704
150
11
151
130
9
11
1704
152
130
153
8246
81
154
38
155
143
81
156
38
157
143
81
158
1188
159
2710
569
160
775
81
38
1619
162
1704
21
775
9
20
1704
163
11
164
130
11
1704
165
9
166
11
167
11
168
561
169
74
170
188
171
3250
172
3745
173
4816
143
174
143
81
38
175
3
176
3431
81
177
3
178
38
179
143
180
3745
181
8
182
21
8
81
38
3
81
183
3
775
184
569
8
775
81
185
38
130
9
186
1704
81
38
143
21
187
5
188
9
189
11
190
1704
775
81
191
1704
775
192
569
81
38
3
193
143
3431
81
38
194
3
195
143
8
196
21
8
81
38
81
38
3
21
8
81
38
197
3
198
143
199
561
5
9
11
1704
11
9
1704
775
569
1188
5
81
38
188
143
38
5
9
11
200
1704
5
11
20
130
5
9
11
1704
81
38
143
81
38
143
38
81
143
201
561
74
3250
4816
202
561
3250
4816
203
74
775
20
1704
3431
11
2049
205
130
9
11
775
206
8
81
38
3
81
38
143
3745
207
3
775
569
81
208
10
81
38
209
5
2103745
188
211
1191
212
143
213
1191
1191
214
143
1191
2151191
216
561
8
81
217
561
218
561
188
3250
3745
4816
143
81
3
21938
81
3
220
561
22111
222
561
21
8
775
81
569
11
1704
775
223
569
81
3
38
8
21
8
22411
188
3250
3745
4816
225
74
226
561
3250
3745
4816
81
227
3
228
561
3250
4816
5
4816
3745
8
21
229
3745
3745
188
230
81
775
231
569
81
3250
38
3745
561
232
3
233
143
234
4593
81
38
81
235
561
236
2339
237
188
238
143
266
3356
239
1191
240
3356
2411191
2423356
5
4816
243
5
5
3745
244
561
5
10
5
3745
245
11
81
38
246
10
3250
3745
188
5
247
561
5
4816
1191
143
163
248
11
249
561
188
3745
250
3250
251
4816
5
48165
2529
253
5 1191
143
254
1191
267
1032
268
3356
255
1032
3356
256
1032
2572295
188
5
3745
81
25810
259
2339
4816
5
3745
81
38
10
4816
3745
5
9
143
1191
4376
1191
4376
2604593
81
188
261
116
3356
4
4
4
9
1032
262
2339
3745
5
263
9
5
8
3745
5
2644593 2655
4376
0 13 2309 39331 48
0
1
3
2
45
30
1716
32318
5763
7235
4
124
51716
6
2318
7
11070
811744
92318
35
9
10
3
11
4
12
7
13
9
14
161
155
4
9
16
7
9
174
18
7
9
7
9
7
9
19
3005
20
9
57
21
9
22
109
37
3005
45
3005
9
23
4
24
7
2545
45
9
7
26
9
63
21
45
57
3
9
109
3005
27
5
3005
28
6
45
45
29
3005
63
5
6
7
9
9
3005
312318
45
32
3005
3
332
6
34
5
36
7
5
161
6
3
57
9
433 1 4 1 6 1 6860 1 76
Figure 2: ML translation E?, word lattice E , and decomposition as a sequence of four string and five
sublattice regions H1 . . .H9 using n-gram posterior probability threshold p(u|E)?0.8.
tial hypotheses that can be trusted. We wish to
constrain MBR decoding to include these trusted
partial hypotheses but allow decoding to consider
alternatives in regions of low confidence. In this
way we aim to improve the best possible output of
the best available systems.
We use the path posterior n-gram probabilities
of Equation (4) to segment lattice E into regions of
high and low confidence. As shown in the exam-
ple of Figure 2, the lattice segmentation process
is performed relative to the ML hypothesis E?, i.e.
relative to the best path through E .
For confidence threshold ?, we find all 4-grams
u = E?i, . . . , E?i+3 in the ML translation for which
p(u|E) > ?. We then segment E? into regions
of high and low confidence where the high confi-
dence regions are identified by consecutive, over-
lapping high confidence 4-grams. The high confi-
dence regions are contiguous strings of words for
which there is consensus amongst the translations
in the lattice. If we trust the path posterior n-gram
probabilities, any hypothesised translation should
include these high confidence substrings. This ap-
proach differs from simple posterior-based prun-
ing in that we discard paths, rather than words
or n-grams, which are not consistent with high-
confidence regions of the ML hypothesis.
The hypothesis string E? is in this way seg-
mented into R alternating subsequences of high
and low confidence. The segment boundaries are
ir and jr so that E?jrir is either a high confidence
or a low confidence subsequence. Each subse-
quence is associated with an unweighted subspace
Hr; this subspace has the form of a string for high
confidence regions and the form of a lattice for
low confidence regions.
If the rth segment is a high confidence region
then Hr accepts only the string E?jrir . If the rth
segment is a region of low confidence, then Hr
is built to accept relevant substrings from E . It is
constructed as follows. The rth low confidence
region E?jrir has a high confidence left context e?r?1
and a high confidence right context e?r+1 formed
from subsequences of the ML translation hypoth-
esis E? as
e?r?1 = E?jr?1ir?1 , e?r+1 = E?
jr+1
ir+1
Note that when r = 1 the left context e?r?1 is the
empty string and when r = R the right context
e?r+1 is the empty string. We build a transducer
74
Tr for the regular expression /. ? e?r?1(.?)e?r+1. ?
/\1/.1 Composition with E yieldsHr = E?Tr , so
that Hr contains all the reasonable alternatives to
E?jrir in E consistent with the high confidence left
and right contexts e?r?1 and e?r+1. IfHr is aligned
to a high confidence subsequence of E?, we call
it a string region since it contains a single path;
if it is aligned to a low confidence region it is a
lattice and we call it a sublattice region. The se-
ries of high and low confidence subspace regions
H1, . . . ,HR defines the lattice segmentation.
5 Hypothesis Space Construction
We now describe a general framework for improv-
ing the fluency of the MBR hypothesis space.
The segmentation of the lattice described in
Section 4 considerably simplifies the problem of
improving the fluency of its hypotheses since each
region of low confidence may be considered in-
dependently. The low confidence regions can be
transformed one-by-one and then reassembled to
form a new MBR hypothesis space.
In order to transform the hypothesis region Hr
it is important to know the context in which it oc-
curs, i.e. the sequences of words that form its pre-
fix and suffix. Some transformations might need
only a short context; others may need a sentence-
level context, i.e. the full sequence of ML words
E?jr?11 and E?Nir+1 to the left and right of the region
Hr that is to be transformed.
To put this formally, each low confidence sub-
lattice region is transformed by the application of
some function ?:
Hr ? ?(E?jr?11 , Hr, E?Nir+1) (6)
The hypothesis space is then constructed from the
concatenation of high confidence string and trans-
formed low confidence sublattice regions
H = E ?
?
1?r?R
Hr (7)
The composition with the original lattice E dis-
cards any new hypotheses that might be created
via the unconstrained concatenation of strings
from theHr. It may be that in some circumstances
1In this notation parentheses indicate string matches so
that /. ? y(a?)w. ? /\1/ applied to xyaaawzz yields aaa.
the introduction of new paths is good, but in what
follows we test the ability to improve fluency by
searching among existing hypotheses, and this en-
sures that nothing new is introduced.
Size of the Hypothesis Space If no new hy-
potheses are introduced by the operations ?, the
size of the hypothesis space H is determined by
the posterior probability threshold ?. Only the
ML hypothesis remains at ? = 0, since all its
subsequences are of high confidence, i.e. can be
covered by n-grams with non-zero path posterior
probability. At the other extreme, for ? = 1, it
follows that H = E and no paths are removed,
since any string regions created are formed from
subsequences that occur on every path in E .
We can therefore use ? to tighten or relax
constraints on the LMBR hypothesis space. At
? = 0, LMBR returns only the ML hypothesis;
at ? = 1, LMBR is done over the full transla-
tion lattice. This is shown in Table 1, where the
BLEU score approaches the BLEU score of un-
constrained LMBR as ? increases.
Note also that the size of the resulting hypoth-
esis space is the product of the number of se-
quences in the sublattice regions. For Figure 2 at
? = 0.8, this product is ?5.4 billion hypotheses.
Even for fairly aggressive constraints on the hy-
pothesis space, many hypotheses remain.
6 Monolingual Coverage Constraints
This section describes one implementation of the
transformation function ? that we will show leads
to improved fluency of machine translation out-
put. This transformation is based on n-gram cov-
erage in a large target language text collection:
where possible, we filter the sublattice regions
so that they contain only long-span n-grams ob-
served in the text. Our motivation is that large
monolingual text collections are good guides to
fluency. If a hypothesis is composed entirely of
previously seen high order n-grams, it is likely to
be fluent and should be favoured.
Initial attempts to identify fluent hypotheses in
sublattice regions by ranking according to n-gram
LM scores were ineffective. Figure 3 shows the
difficulties. We see that both the 4-gram Kneser-
Ney and 5-gram stupid-backoff language models
75
LM Translation hypothesis E and n-gram orders used by the LM to score each word Score
4g <s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 atomic3 bomb2 .3 </s>4 -22.59
<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 the4 atomic2 bomb3 .4 </s>4 -23.61
5g <s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 atomic5 bomb2 .3 </s>4 -16.04
<s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 the4 atomic4 bomb5 .4 </s>5 -17.96
Figure 3: Scores and n-gram orders for hypotheses using 4-gram Kneser-Ney and 5-gram stupid-
backoff (estimated from 1.1B and 6.6B tokens, resp.) LMs. Low confidence regions are in italics.
favour the shorter but disfluent hypothesis; nor-
malising by length was not effective. However,
the stupid-backoff LM has better coverage and the
backing-off behaviour is a clue to the presence
of disfluency. Similar cues have been observed
in ASR analysis (Chase, 1997). The shorter hy-
pothesis backs off to a bigram for ?atomic bomb?,
whereas the longer hypothesis covers the same
words with 4-grams and 5-grams. We therefore
disregard the language model scores and focus on
n-gram coverage. This is an example where ro-
bustness and fluency are at odds. The n-gram
models are robust, but often favour less fluent hy-
potheses.
Let S denote the set of all n-grams in the mono-
lingual training data. To identify partial hypothe-
ses in sublattice regions that have complete mono-
lingual coverage at the maximum order n, we
build a coverage acceptor Cn with a similar form
to the WFST representation of an n-gram backoff
language model (Allauzen et al, 2003). Cn as-
signs a penalty to every n-gram not found in S .
In Cn word arcs have no cost and backoff arcs are
assigned a fixed cost of 1. Firstly, arcs from the
start state are added for each unigram w ? N1:
w
w/0?
Then for n-grams u ? S ? {?ni=2 Ni}, where
u = wn1 consisting of history h = wn?11 and target
word wn, arcs are added
wn/0h h+
where h+ = wn?12 if u has order n and h+ = wn1
if u has order less than n. Backoff arcs are added
for each u as
?/1h h?
where h? = wn?12 if u has order > 2, and bi-
grams backoff to the null history start state ?.
For each sublattice region Hr, we wish to pe-
nalise each path proportionally to the number of
its n-grams not found in the monolingual text col-
lection S . We wish to do this in context, so that
we include the effect of the neighbouring high
confidence regions Hr?1 and Hr+1. Given that
we are counting n-grams at order n we form the
left context machine Lr which accepts the last
n ? 1 words in Hr?1; similarly, Rr accepts the
first n ? 1 words of Hr+1. The concatenation
Xr = Lr?Hr?Rr represents the partial transla-
tion hypotheses inHr padded with n?1 words of
left and right context from the neighbouring high
confidence regions. Composing Xr ? Cn assigns
each partial hypothesis a cost equal to the number
of times it was necessary to back off to lower order
n-grams while reading the string. Partial hypothe-
ses with cost 0 did not back off at all and contain
only maximum order n-grams.
In the following experiments, we look at each
Xn ? Cn and if there are paths with cost 0, only
these are kept and all others discarded. We intro-
duce this as a constraint on the hypothesis space
which we will evaluate for improvement on flu-
ency. Here the transformation function ? returns
Hr as Xr ?Cn after pruning. If Xr ?Cn has no zero
cost paths, the transformation function ? returns
Hr as we find it, since there is not enough mono-
lingual coverage to guide the selection of fluent
hypotheses. After applying monolingual coverage
constraints to each region, the modified hypothe-
sis space used for MBR search is formed by con-
catenation using Equation (7).
We note that Cn is a simplistic NLG system. It
generates strings by concatenating n-grams found
in S . We do not allow it to run ?open loop? in these
experiments, but instead use it to find the strings
in Xr with good n-gram coverage.
7 LMBR Over Segmented Lattices
The effect of fluency constraints on LMBR de-
coding is evaluated in the context of the NIST
Arabic?English MT task. The set tune consists
76
ML ... view , especially with the open chinese economy to the world and ...
+LMBR ... view , especially with the open chinese economy to the world and ...
+LMBR+CC ... view , especially with the opening of the chinese economy to the world and ...
ML ... revision of the constitution of the japanese public , which dates back ...
+LMBR ... revision of the constitution of the japanese public , which dates back ...
+LMBR+CC ... revision of the constitution of japan , which dates back ...
Figure 4: Improved fluency through the application of monolingual coverage constraints to the hypoth-
esis space in MBR decoding of NIST MT 08 Arabic?English newswire lattices.
of the odd numbered sentences of the MT02?
MT05 testsets; the even numbered sentences form
test. MT08 performance on nw08 (newswire) and
ng08 (newsgroup) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. The first-pass LM is a modified Kneser-
Ney (Kneser and Ney, 1995) 4-gram estimated
over the English side of the parallel text and an
881M word subset of the English GigaWord 3rd
Edition. Prior to LMBR, the first-pass lattices are
rescored with zero-cutoff stupid-backoff 5-gram
language models (Brants et al, 2007) estimated
over more than 6B words of English text. The
LMBR factors ?0, . . . , ?4 are set as in Tromble et
al. (2008) using unigram precision p = 0.85 and
recall ratio r = 0.74.
The effect of performing LMBR over the seg-
mented hypothesis space is shown in Table 1. The
hypothesis subspaces Hr are constructed at var-
ious confidence thresholds as described in Sec-
tion 4 with H formed via Equation (7); no cover-
age constraints are applied yet. Constraining the
search space using ? = 0.6 leads to little degra-
dation in LMBR performance under BLEU. This
shows lattice segmentation works as intended.
We next investigate the effect of monolingual
coverage constraints on BLEU. We build accep-
tors Cn as described in Section 6 with S con-
sisting of all n-grams in the English GigaWord.
At ? = 0.6 we found 181 sentences with sub-
lattices Hr spanned by maximum order n-grams
from S , i.e. for which Xr ? Cn have paths with
cost 0; these are filtered as described. LMBR
over these coverage-constrained sublattices is de-
noted LMBR+CC. On nw08 the BLEU score for
LMBR+CC is 52.0 which is +0.7 over the ML de-
coder and only -0.2 BLEU below unconstrained
LMBR decoding. Done in this way, constraining
hypotheses to have 5-grams from the GigaWord
tune test nw08 ng08
ML 54.2 53.8 51.3 36.3
?
0.0 54.2 53.8 51.3 36.3
0.2 54.3 53.8 51.3 36.3
0.4 54.6 54.2 51.6 36.7
0.6 54.9 54.4 52.1 36.6
0.8 54.9 54.4 52.1 36.6
1.0 54.9 54.4 52.2 36.7
LMBR 54.9 54.4 52.2 36.8
Table 1: BLEU scores for ML hypotheses and
LMBR decoding inH over 0 ? ? ? 1.
has little impact on BLEU.
At this value of ?, 116 of the 813 nw08 sen-
tences have a low confidence region (1) com-
pletely covered by 5-grams, and (2) within which
the ML hypothesis and the LMBR+CC hypothe-
sis differ. It is these regions which we will inspect
for improved fluency.
8 Human Fluency Evaluation
We asked 17 native speakers to judge the fluency
of sentence fragments from nw08. We compared
hypotheses from the ML and the LMBR+CC de-
coders. Each fragment consisted of the partial
translation hypothesis from a low confidence re-
gion together with its left and right high confi-
dence contexts (examples given in Figure 4). For
each sample, judges were asked: ?Could this frag-
ment occur in a fluent sentence??
The results are shown in Table 2. Most of the
time, the ML and LMBR+CC sentence fragments
were both judged to be fluent; it often happened
that they differed by only a single noun or verb
substitution which didn?t affect fluency. In a small
number of cases, both ML and LMBR+CC were
judged to be disfluent. We are most interested in
the ?off-diagonal? cases. In cases when one sys-
tem was judged to be fluent and the other was not,
LMBR+CC was preferred about twice as often as
the ML baseline (26.9% to 9.7%). In other words,
the monolingual fluency constraints were judged
77
LMBR+CC
Fluent Not Fluent
ML Fluent 1175 (59.6%) 192 (9.7%)Not Fluent 530 (26.9%) 75 (3.8%)
Table 2: Partial hypothesis fluency judgements.
to have improved the fluency of the low confi-
dence region more than twice as often as a fluent
hypothesis was made disfluent.
Some examples of improved fluency are shown
in Figure 4. Although both the ML and un-
constrained LMBR hypotheses might satisfy ad-
equacy, they lack the fluency of the LMBR+CC
hypotheses generated using monolingual fluency
constraints.
9 Summary and Discussion
We have described a general framework for im-
proving SMT fluency. Decoupling the hypothesis
space from the evidence space allows for much
greater flexibility in lattice MBR search.
We have shown that high path posterior proba-
bility n-grams in the ML translation can be used to
guide the segmentation of a lattice into regions of
high and low confidence. Segmenting the lattice
simplifies the process of refining the hypothesis
space since low confidence regions can be refined
in the context of their high confidence neighbours.
This can be done independently before reassem-
bling the refined regions. Lattice segmentation
facilitates the application of post-processing and
rescoring techniques targeted to address particu-
lar deficiencies in ML decoding.
The techniques we presented are related to con-
sensus decoding and system combination for SMT
(Matusov et al, 2006; Sim et al, 2007), and to
segmental MBR for automatic speech recognition
(Goel et al, 2004). Mohit et al (2009) describe
an alternative approach to improving specific por-
tions of translation hypotheses. They use an SVM
classifier to identify a single phrase in each source
language sentence that is ?difficult to translate?;
such phrases are then translated using an adapted
language model estimated from parallel data. In
contrast to their approach, our approach is able
to exploit large collections of monolingual data to
refine multiple low confidence regions using pos-
terior probabilities obtained from a high-quality
evidence space of first-pass translations.
Testset Sentences Reachability
tune 2075 15%
test 2040 14%
nw08 813 11%
ng08 547 9%
Table 3: Arabic?English reference reachability.
We applied hypothesis space constraints based
on monolingual coverage to low confidence re-
gions resulting in improved fluency with no real
degradation in BLEU score relative to uncon-
strained LMBR decoding. This approach is lim-
ited by the coverage of sublattices using monolin-
gual text. We expect this to improve with larger
text collections or in tightly focused scenarios
where in-domain text is less diverse.
However, fluency will be best improved by inte-
grating more sophisticated natural language gen-
eration. NLG systems capable of generating sen-
tence fragments in context can be incorporated di-
rectly into this framework. If the MBR hypothe-
sis spaceH contains a generated hypothesis E? for
which P (F |E?) = 0, E? could still be produced as
a translation, since it can be ?voted for? by nearby
hypotheses produced by the underlying system.
Table 3 shows the proportion of NIST testset
sentences that can be aligned to any of the ref-
erence translations using our high quality base-
line hierarchical decoder with a powerful gram-
mar. The low level of reachability suggests that
NLG may be required to achieve high levels of
translation quality and fluency. Other rescoring
approaches (Kumar et al, 2009; Li et al, 2009)
may also benefit from NLG when the baseline is
incapable of generating the reference.
We note that our approach could also be used to
improve the fluency of ASR, OCR and other lan-
guage processing tasks where the goal is to pro-
duce fluent natural language output.
Acknowledgments
We would like to thank Matt Gibson and the
human judges who participated in the evalua-
tion. This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022 and the European Union Seventh
Framework Programme (FP7-ICT-2009-4) under
Grant Agreement No. 247762.
78
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL 2003.
Blackwood, Graeme, Adria` de Gispert, and William Byrne.
2010. Efficient path counting transducers for minimum
Bayes-risk decoding of statistical machine translation lat-
tices. In Proceedings of ACL 2010.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in ma-
chine translation. In Proceedings of the EMNLP 2007.
Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational
Linguistics, 16(2):79?85.
Callison-Burch, Chris, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In WMT 2009.
Chase, Lin Lawrance. 1997. Error-responsive feed-
back mechanisms for speech recognizers, Ph.D. Thesis,
Carnegie Mellon University.
Chiang, David. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
DeNero, John, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL-IJCNLP 2009.
Goel, V., S. Kumar, and W. Byrne. 2004. Segmental mini-
mum Bayes-risk decoding for automatic speech recogni-
tion. IEEE Transactions on Speech and Audio Process-
ing, 12:234?249.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo R. Banga, and
William Byrne. 2009. Hierarchical phrase-based trans-
lation with weighted finite state transducers. In Proceed-
ings of the 2009 Annual Conference of the NAACL.
Kneser, R. and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing.
Knight, K and J Graehl. 2005. An overview of probabilis-
tic tree transducers for natural language processing. In
Proceedings of CICLING 2005.
Knight, K. 2007a. Capturing practical natural language
transformations. Machine Translation, 21(2).
Knight, Kevin. 2007b. Automatic language translation gen-
eration help needs badly. In MT Summit XI Workshop on
Using Corpora for NLG: Keynote Address.
Kumar, Shankar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine translation. In
NAACL 2004.
Kumar, Shankar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL-IJCNLP 2009.
Lavie, Alon and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine trans-
lation. Machine Translation Journal.
Li, Zhifei, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine translation.
In Proceedings of ACL-IJCNLP 2009.
Ma, Xiaoyi and Christopher Cieri. 2006. Corpus support for
machine translation at LDC. In LREC 2006.
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney. 2006.
Computing consensus translation from multiple machine
translation systems using enhanced hypotheses align-
ment. In 11th Conference of the EACL.
Mohit, B., F. Liberato, and R. Hwa. 2009. Language model
adaptation for difficult-to-translate phrases. In Proceed-
ings of the 13th Annual Conference of the EAMT.
Mohri, Mehryar, Fernando Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition.
In CSL, volume 16, pages 69?88.
Oberlander, Jon and Chris Brew. 2000. Stochastic text gen-
eration. In Philosophical Transactions of the Royal Soci-
ety.
Och, F., D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features
for statistical machine translation. In Proceedings of the
HLT Conference of the NAACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL 2002.
Sim, K.-C., W. Byrne, M. Gales, H. Sahbi, and P.C. Wood-
land. 2007. Consensus network decoding for statisti-
cal machine translation system combination. In ICASSP
2007.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, , and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decoding
for statistical machine translation. In Proceedings of the
2008 Conference on EMNLP.
Ueffing, Nicola and Hermann Ney. 2007. Word-level confi-
dence estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vilar, D, G Leusch, H Ney, and R Banchs. 2007. Human
evaluation of machine translation through binary system
comparisons. In Proceedings of WMT 2007.
Zens, Richard and Hermann Ney. 2006. N -gram posterior
probabilities for statistical machine translation. In Pro-
ceedings of WMT 2006.
79
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545?554,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hierarchical Phrase-based Translation Grammars Extracted from
Alignment Posterior Probabilities
Adria` de Gispert, Juan Pino, William Byrne
Machine Intelligence Laboratory
Department of Engineering, University of Cambridge
Trumpington Street, CB2 1PZ, U.K.
{ad465|jmp84|wjb31}@eng.cam.ac.uk
Abstract
We report on investigations into hierarchi-
cal phrase-based translation grammars based
on rules extracted from posterior distributions
over alignments of the parallel text. Rather
than restrict rule extraction to a single align-
ment, such as Viterbi, we instead extract rules
based on posterior distributions provided by
the HMM word-to-word alignment model. We
define translation grammars progressively by
adding classes of rules to a basic phrase-based
system. We assess these grammars in terms
of their expressive power, measured by their
ability to align the parallel text from which
their rules are extracted, and the quality of the
translations they yield. In Chinese-to-English
translation, we find that rule extraction from
posteriors gives translation improvements. We
also find that grammars with rules with only
one nonterminal, when extracted from posteri-
ors, can outperform more complex grammars
extracted from Viterbi alignments. Finally, we
show that the best way to exploit source-to-
target and target-to-source alignment models
is to build two separate systems and combine
their output translation lattices.
1 Introduction
Current practice in hierarchical phrase-based trans-
lation extracts regular phrases and hierarchical rules
from word-aligned parallel text. Alignment models
estimated over the parallel text are used to generate
these alignments, but these models are then typically
used no further in rule extraction. This is less than
ideal because these alignment models, even if they
are not suitable for direct use in translation, can still
provide a great deal of useful information beyond a
single best estimate of the alignment of the parallel
text. Our aim is to use alignment models to generate
the statistics needed to build translation grammars.
The challenge in doing so is to extend the current
procedures, which are geared towards the use of a
single alignment, to make more of what can be pro-
vided by alignment models. The goal is to extract a
richer and more robust set of translation rules.
There are two aspects to hierarchical phrase-based
translation grammars which concern us. The first
is expressive power, which we take as the ability
to generate known reference translations from sen-
tences in the source language. This is determined
by the degree of phrase movements and the trans-
lations allowed by the rules of the grammar. For a
grammar with given types of rules, larger rule sets
will yield greater expressive power. This motivates
studies of grammars based on the rules which are ex-
tracted and the movement the grammar allows. The
second aspect is of course translation accuracy. If
the expressive power is adequate, then the desire is
that the grammar assigns a high score to a correct
translation.
We use posterior probabilities over parallel data to
address both of these aspects. These posteriors allow
us to build larger rule sets with improved transla-
tion accuracy. Ideally, for a sentence pair we wish to
consider all possible alignments between all possi-
ble source and target phrases within these sentences.
Given a grammar allowing certain types of move-
ment, we would then extract all possible parses that
are consistent with any alignments of these phrases.
545
To make this approach feasible, we consider only
phrase-to-phrase alignments with a high posterior
probability under the alignment models. In this way,
the alignment model probabilities guide rule extrac-
tion.
The paper is organized as follows. Section 2 re-
views related work on using posteriors to extract
phrases, as well as other approaches that tightly in-
tegrate word alignment and rule extraction. Sec-
tion 3 describes rule extraction based on word and
phrase posterior distributions provided by the HMM
word-to-word alignment model. In Section 4 we de-
fine translation grammars progressively by adding
classes of rules to a basic phrase-based system, mo-
tivating each rule type by the phrase movement it is
intended to achieve. In Section 5 we assess these
grammars in terms of their expressive power and the
quality of the translations they yield in Chinese-to-
English, showing that rule extraction from posteriors
gives translation improvements. We also find that
the best way to exploit source-to-target and target-
to-source alignment models is to build two sepa-
rate systems and combine their output translation
lattices. Section 6 presents the main conclusions of
this work.
2 Related Work
Some authors have previously addressed the limita-
tion caused by decoupling word alignment models
from grammar extraction. For instance Venugopal
et al (2008) extract rules from n-best lists of align-
ments for a syntax-augmented hierarchical system.
Alignment n-best lists are also used in Liu et al
(2009) to create a structure called weighted align-
ment matrices that approximates word-to-word link
posterior probabilities, from which phrases are ex-
tracted for a phrase-based system. Alignment pos-
teriors have been used before for extracting phrases
in non-hierarchical phrase-based translation (Venu-
gopal et al, 2003; Kumar et al, 2007; Deng and
Byrne, 2008).
In order to simplify hierarchical phrase-based
grammars and make translation feasible with rela-
tively large parallel corpora, some authors discuss
the need for various filters during rule extraction
(Chiang, 2007). In particular Lopez (2008) enforces
a minimum span of two words per nonterminal,
Zollmann et al (2008) use a minimum count thresh-
old for all rules, and Iglesias et al (2009) propose
a finer-grained filtering strategy based on rule pat-
terns. Other approaches include insisting that target-
side rules are well-formed dependency trees (Shen et
al., 2008).
We also note approaches to tighter coupling be-
tween translation grammars and alignments. Marcu
and Wong (2002) describe a joint-probability
phrase-based model for alignment, but the approach
is limited due to excessive complexity as Viterbi
inference becomes NP-hard (DeNero and Klein,
2008). More recently, Saers et al (2009) report
improvement on a phrase-based system where word
alignment has been trained with an inversion trans-
duction grammar (ITG) rather than IBM models.
Pauls et al (2010) also use an ITG to directly align
phrases to nodes in a string-to-tree model. Bayesian
methods have been recently developed to induce a
grammar directly from an unaligned parallel corpus
(Blunsom et al, 2008; Blunsom et al, 2009). Fi-
nally, Cmejrek et al (2009) extract rules directly
from bilingual chart parses of the parallel corpus
without using word alignments. We take a differ-
ent approach in that we aim to start with very strong
word alignment models and use them to guide gram-
mar extraction.
3 Rule Extraction from Alignment
Posteriors
The goal of rule extraction is to generate a set of
good-quality translation rules from a parallel cor-
pus. Rules are of the form X???,?,?? , where
?, ? ? {X ? T}+ are the source and target sides of
the rule, T denotes the set of terminals (words) and
? is a bijective function1 relating source and target
nonterminals X of each rule (Chiang, 2007). For
each ?, the probability over translations ? is set by
relative frequency over the extracted examples from
the corpus.
We take a general approach to rule extraction, as
described by the following procedure. For simplic-
ity we discuss the extraction of regular phrases, that
is, rules of the form X??w,w?, where w ? {T}+.
Section 3.3 extends this procedure to rules with non-
1This function is defined if there are at least two nontermi-
nals, and for clarity of presentation will be omitted in this paper
546
terminal symbols.
Given a sentence pair (fJ1 , eI1), the extraction al-
gorithm traverses the source sentence and, for each
sequence of terminals f j2j1 , it considers all possible
target-side sequences ei2i1 as translation candidates.
Each target-side sequence that satisfies the align-
ment constraints CA is ranked by the function fR.
For practical reasons, a set of selection criteria CS is
then applied to these ranked candidates and defines
the set of translations of the source sequence that are
extracted as rules. Each extracted rule is assigned a
count fC .
In this section we will explore variations of this
rule extraction procedure involving alternative def-
initions of the ranking and counting functions, fR
and fC , based on probabilities over alignment mod-
els.
Common practice (Koehn et al, 2003) takes a set
of word alignment links L and defines the alignment
constraints CA so that there is a consistency between
the links in the (f j2j1 , e
i2
i1) phrase pair. This is ex-
pressed by ?(j, i) ? L : (j ? [j1, j2]? i ? [i1, i2])?
(j 6? [j1, j2] ? i 6? [i1, i2]). If these constraints
are met, then alignment probabilities are ignored and
fR = fC = 1. We call this extraction Viterbi-based,
as the set of alignment links is generally obtained
after applying a symmetrization heuristic to source-
to-target and target-to-source Viterbi alignments.
In the following section we depart from this ap-
proach and apply novel functions to rank and count
target-side translations according to their quality in
the context of each parallel sentence, as defined by
the word alignment models. We also depart from
common practice in that we do not use a set of links
as alignment constraints. We thus find an increase
in the number of extracted rules, and consequently
better relative frequency estimates over translations.
3.1 Ranking and Counting Functions
We describe two alternative approaches to modify
the functions fR and fC so that they incorporate the
probabilities provided by the alignment models.
3.1.1 Word-to-word Alignment Posterior
Probabilities
Word-to-word alignment posterior probabilities
p(lji|fJ1 , eI1) express how likely it is that the words
in source position j and target position i are aligned
given a sentence pair. These posteriors can be effi-
ciently computed for Model 1, Model 2 and HMM,
as described in (Brown et al, 1993; Venugopal et al,
2003; Deng and Byrne, 2008).
We will use these posteriors in functions to
score phrase pairs. For a simple non-disjoint case
(f j2j1 , e
i2
i1) we use:
fR(f j2j1 , e
i2
i1) =
j2
?
j=j1
i2
?
i=i1
p(lji|fJ1 , eI1)
i2 ? i1 + 1
(1)
which is very similar to the score used for lexical
features in many systems (Koehn, 2010), with the
link posteriors for the sentence pair playing the role
of the Model 1 translation table.
For a particular source phrase, Equation 1 is not
a proper conditional probability distribution over all
phrases in the target sentence. Therefore it cannot be
used as such without further normalization. Indeed
we find that this distribution is too sharp and over-
emphasises short phrases, so we use fC = 1. How-
ever, it does allow us to rank target phrases as pos-
sible translations. In contrast to the common extrac-
tion procedure described in the previous section, the
ranking approach described here can lead to a much
more exhaustive extraction unless selection criteria
are applied. These we describe in Section 3.2.
We note that Equation 1 can be computed us-
ing link posteriors provided by alignment models
trained on either source-to-target or target-to-source
translation directions.
3.1.2 Phrase-to-phrase Alignment Posterior
Probabilities
Rather than limit ourselves to word-to-word
link posteriors we can define alignment proba-
bility distributions over phrase alignments. We
do this by defining the set of alignments A as
A(j1, j2; i1, i2) = {aJ1 : aj ? [i1, i2] iff j ?
[j1, j2]}, where aj is the random process that de-
scribes word-to-word alignments. These are the
alignments from which the phrase pair (f j2j1 , e
i2
i1)
would be extracted.
The posterior probability of these alignments
given the sentence pair is defined as follows:
p(A|eI1, fJ1 ) =
?
aJ1?A
p(fJ1 , aJ1 |eI1)
?
aJ1
p(fJ1 , aJ1 |eI1)
(2)
547
G0 G1 G2 G3
S??X,X? X??w X,X w? X??w X,X w? X??w X,X w?
S??S X,S X? X??X w,w X? X??X w,w X? X??X w,w X?
X??w,w? X??w X,w X? X??w X,w X?
X??w X w,w X w?
Table 1: Hierarchical phrase-based grammars containing different types of rules. The grammar expressivity is greater
as more types of rules are included. In addition to the rules shown in the respective columns, G1, G2 and G3 also
contain the rules of G0.
With IBM models 1 and 2, the numerator and de-
nominator in Equation 2 can be computed in terms
of posterior link probabilities (Deng, 2005). With
the HMM model, the denominator is computed us-
ing the forward algorithm while the numerator can
be computed using a modified forward algorithm
(Deng, 2005).
These phrase posteriors directly define a proba-
bility distribution over the alignments of translation
candidates, so we use them both for ranking and
scoring extracted rules, that is fR = fC = p. This
approach assigns a fractional count to each extracted
rule, which allows finer estimation of the forward
and backward translation probability distributions.
3.2 Alignment Constraints and Selection
Criteria
In order to keep this process computationally
tractable, some extraction constraints are needed. In
order to extract a phrase pair (f j2j1 , e
i2
i1), we define
the following:
? CA requires at least one pair of positions (j, i) :
(j ? [j1, j2] ? i ? [i1, i2]) with word-to-word
link posterior probability p(lji|fJ1 , eI1) > 0.5,
and that there is no pair of positions (j, i) : (j ?
[j1, j2]?i 6? [i1, i2])?(j 6? [j1, j2]?i ? [i1, i2])
with p(lji|fJ1 , eI1) > 0.5
? CS allows only the k best translation candidates
to be extracted. We use k = 3 for regular
phrases, and k = 2 for hierarchical rules.
Note that we do not discard rules according to
their scores fC at this point (unlike Liu et al
(2009)), since we prefer to add all phrases from
all sentence pairs before carrying out such filtering
steps.
Once all rules over the entire collection of paral-
lel sentences have been extracted, we require each
rule to occur at least nobs times and with a forward
translation probability p(?|?) > 0.01 to be used for
translation.
3.3 Extraction of Rules with Nonterminals
Extending the procedure previously described to
the case of more complex hierarchical rules includ-
ing one or even two nonterminals is conceptually
straightforward. It merely requires that we traverse
the source and target sentences and consider possi-
bly disjoint phrase pairs. Optionally, the alignment
constraints can also be extended to apply on the non-
terminal X.
Equation 1 is then only modified in the limits
of the product and summation, whereas Equation
2 remains unchanged, as long as the set of valid
alignments A is redefined. For example, for a rule
of the form X??w X w,w X w?, we use A ?
A(j1, j2; j3, j4; i1, i2; i3, i4).
4 Hierarchical Translation Grammar
Definition
In this section we define the hierarchical phrase-
based synchronous grammars we use for translation
experiments. Each grammar is defined by the type of
hierarchical rules it contains. The rule type can be
obtained by replacing every sequence of terminals
by a single symbol ?w?, thus ignoring the identity of
the words, but capturing its generalized structure and
the kind of reordering it encodes (this was defined as
rule pattern in Iglesias et al (2009)).
A monotonic phrase-based translation grammar
G0 can be defined as shown in the left-most col-
umn of Table 1; it includes all regular phrases, repre-
sented by the rule type X??w,w?, and the two glue
548
(G0) R1: S??X,X?
(G0) R2: X??s2 s3,t2?
(G1) R3: X??s1 X,X t3?
(G1) R4: X??X s4,t1 X?
(G2) R5: X??s1 X,t7 X?
(G3) R6: X??s1 X s4,t5 X t6?
Figure 1: Example of a hierarchical translation grammar and two parsing trees following alternative rule derivations
for the input sentence s1s2s3s4.
rules that allow concatenation. Our approach is now
simple: we extend this grammar by successively in-
corporating sets of hierarchical rules. The goal is to
obtain a grammar with few rule types but which is
capable of generating a rich set of translation candi-
dates for a given input sentence.
With this in mind, we define the following three
grammars, also summarized in Table 1:
? G1 := G0
?
{ X??w X,X w? , X??X w,w X? }. This
incorporates reordering capabilities with two
rule types that place the unique nonterminal
in an opposite position in each language; we
call these ?phrase swap rules?. Since all non-
terminals are of the same category X, nested
reordering is possible. However, this needs to
happen consecutively, i.e. a swap must apply
after a swap, or the rule is concatenated with
the glue rule.
? G2 := G1
?{ X??w X,w X? }. This
adds monotonic concatenation capabilities to
the previous translation grammar. The glue rule
already allows rule concatenation. However, it
does so at the S category, that is, it concate-
nates phrases and rules after they have been re-
ordered, in order to complete a sentence. With
this new rule type, G2 allows phrase/rule con-
catenation before reordering with another hier-
archical rule. Therefore, nested reordering does
not require successive swaps anymore.
? G3 := G2
?{ X??w X w,w X w? }. This
adds single nonterminal rules with disjoint ter-
minal sequences, which can encode a mono-
tonic or reordered relationship between them,
depending on what their alignment was in the
parallel corpus. Although one could expect the
movement captured by this phrase-disjoint rule
type to be also present in G2 (via two swaps or
one concatenation plus one swap), the terminal
sequences w may differ.
Figure 1 shows an example set of rules indicat-
ing to which of the previous grammars each rule be-
longs, and shows three translation candidates as gen-
erated by grammars G1 (left-most tree), G2 (mid-
dle tree) and G3 (right-most tree). Note that the
middle tree cannot be generated with G1 as it re-
quires monotonic concatenation before reordering
with rule R4.
The more rule types a hierarchical grammar con-
tains, the more different rule derivations and the
greater the search space of alternative translation
candidates. This is also connected to how many
rules are extracted per rule type. Ideally we would
like the grammar to be able to generate the correct
translation of a given input sentence, without over-
generating too many other candidates, as that makes
the translation task more difficult.
We will make use of the parallel data in measuring
the ability of a grammar to generate correct transla-
tions. By extracting rules from a parallel sentence,
we translate them and observe whether the transla-
tion grammar is able to produce the parallel target
translation. In Section 5.1 we evaluate this for a
Chinese-to-English task.
549
4.1 Reducing Grammar Redundancy
Let us discuss grammar G2 in more detail. As de-
scribed in the previous section, the motivation for in-
cluding rule type X??w X,w X? is that the gram-
mar be able to carry out monotonic concatenation
before applying another hierarchical rule with re-
ordering. This movement is permitted by this rule
type, but the use of a single nonterminal category X
also allows the grammar to apply the concatenation
after reordering, that is, immediately before the glue
rule is applied. This creates significant redundancy
in rule derivations, as this rule type is allowed to act
as a glue rule. For example, given an input sentence
s1s2 and the following simple grammar:
R0: S??X,X?
R1: S??S X,S X?
R2: X??s1,t1?
R3: X??s2,t2?
R4: X??s1 X,t1 X?
two derivations are possible: R2,R0,R3,R1 and
R3,R4,R0, and the translation result is identical.
To avoid this situation we introduce a nonterminal
M in the left-hand side of monotonic concatenation
rules of G2. All rules are allowed to use nontermi-
nals X and M in their right-hand side, except the
glue rules, which can only take X. In the context of
our example, R4 is substituted by:
R4a: M??s1 X,t1 X?
R4b: M??s1 M ,t1 M?
so that only the first derivation is possible:
R2,R0,R3,R1, because applying R3,R4a yields a non-
terminal M that cannot be taken by the glue rule R0.
5 Experiments
We report experiments in Chinese-to-English trans-
lation. Our system is trained on a subset of the
GALE 2008 evaluation parallel text;2 this is approx-
imately 50M words per language. We report trans-
lation results on a development set tune-nw and a
test set test-nw1. These contain translations pro-
duced by the GALE program and portions of the
newswire sections of MT02 through MT06. They
contain 1,755 sentences and 1,671 sentences respec-
tively. Results are also reported on a smaller held-
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
 30
 40
 50
 60
 70
 80
V-st
V-ts
V-union
V-gdf
V-merge
WP-st
WP-ts
WP-merge
G0
G1
G2
G3
Figure 2: Percentage of parallel sentences successfully
aligned for various extraction methods and grammars.
out test set test-nw2, containing 60% of the NIST
newswire portion of MT06, that is, 369 sentences.
The parallel texts for both language pairs are
aligned using MTTK (Deng and Byrne, 2008). For
decoding we use HiFST, a lattice-based decoder im-
plemented with Weighted Finite State Transducers
(de Gispert et al, 2010). Likelihood-based search
pruning is applied if the number of states in the
lattice associated with each CYK grid cell exceeds
10,000, otherwise the entire search space is ex-
plored. The language model is a 4-gram language
model estimated over the English side of the paral-
lel text and the AFP and Xinhua portions of the En-
glish Gigaword Fourth Edition (LDC2009T13), in-
terpolated with a zero-cutoff stupid-backoff (Brants
et al, 2007) 5-gram estimated using 6.6B words of
English newswire text. In tuning the systems, stan-
dard MERT (Och, 2003) iterative parameter estima-
tion under IBM BLEU3 is performed on the devel-
opment sets.
5.1 Measuring Expressive Power
We measure the expressive power of the grammars
described in the previous section by running the
translation system in alignment mode (de Gispert
et al, 2010) over the parallel corpus. Conceptually,
this is equivalent to replacing the language model by
the target sentence and seeing if the system is able to
find any candidate. Here the weights assigned to the
3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
550
Grammar Extraction # Rules tune-nw test-nw1 test-nw2
time prune BLEU BLEU BLEU
GH V-union 979149 3.7 0.3 35.1 35.6 37.6
V-union 613962 0.4 0.0 33.6 34.6 36.4
G1 WP-st 920183 0.9 0.0 34.3 34.8 37.5
PP-st 893542 1.4 0.0 34.4 35.1 37.7
V-union 734994 1.0 0.0 34.5 35.4 37.2
G2 WP-st 1132386 5.8 0.5 35.1 36.0 37.7
PP-st 1238235 7.8 0.7 35.5 36.4 38.2
V-union 966828 1.2 0.0 34.9 35.3 37.0
G3 WP-st 2680712 8.3 1.1 35.1 36.2 37.9
PP-st 5002168 10.7 2.6 35.5 36.4 38.5
Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEU
shown). Time (secs/word) and prune (times/word) measurements done on tune-nw set.
rules are irrelevant, as only the ability of the gram-
mar to create a desired hypothesis is important.
We compare the percentage of target sentences
that can be successfully produced by grammars G0,
G1, G2 and G3 for the following extraction meth-
ods:
? Viterbi (V). This is the standard extraction
method based on a set of alignment links. We
distinguish four cases, depending on the model
used to obtain the set of links: source-to-
target (V-st), target-to-source (V-ts), and two
common symmetrization strategies: union (V-
union) and grow-diag-final (V-gdf), described
in (Koehn et al, 2003).
? Word Posteriors (WP). The extraction method
is based on word alignment posteriors de-
scribed in Section 3.1.1. These rules can be ob-
tained either from the posteriors of the source-
to-target (WP-st) or the target-to-source (WP-
ts) alignment models. We apply the alignment
constraints and selection criteria described in
Section 3.2. We do not report alignment per-
centages when using phrase posteriors (as de-
scribed in Section 3.1.2) as they are roughly
identical to the WP case.
? Finally, in both cases, we also report results
when merging the extracted rules in both direc-
tions into a single rule set (V-merge and WP-
merge).
Figure 2 shows the results obtained for a random
selection of 10,000 parallel corpus sentences. As ex-
pected, we can see that for any extraction method,
the percentage of aligned sentences increases when
switching from G0 to G1, G2 and G3. Posterior-
based extraction is shown to outperform standard
methods based on a Viterbi set of alignments for
nearly all grammars. The highest alignment percent-
ages are obtained when merging rules obtained un-
der models trained in each direction (WP-merge),
approximately reaching 80% for grammar G3.
The maximum rule span in alignment was al-
lowed to be 15 words, so as to be similar to transla-
tion, where the maximum rule span is 10 words. Re-
laxing this in alignment to 30 words yields approxi-
mately 90% coverage for WP-merge under G3.
We note that if alignment constraints CA and se-
lection criteria CS were not applied, that is k = ?,
then alignment percentages would be 100% even
for G0, but the extracted grammar would include
many noisy rules with poor generalization power
and would suffer from overgeneration.
5.2 Translation Results
In this section we investigate the translation perfor-
mance of each hierarchical grammar, as defined by
rules obtained from three rule extraction methods:
? Viterbi union (V-union). Standard rule extrac-
tion from the union of the source-to-target and
target-to-source alignment link sets.
551
? Word Posteriors (WP-st). Extraction based
on word posteriors as described in Section
3.1.1. The posteriors are provided by the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 2.
? Phrase Posteriors (PP-st). Extraction based
on phrase alignment posteriors, as described
in Section 3.1.2, with fractional counts pro-
portional to the phrase probability under the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 0.2.
Table 2 reports the translation results, as well as
the number of extracted rules in each case. It also
shows the following decoding statistics as measured
on the tune-nw set: decoding time in seconds per in-
put word, and number of instances of search pruning
(described in Section 5) per input word.
As a contrast, we extract rules according to the
heuristics introduced in (Chiang, 2007) and apply
the filters described in (Iglesias et al, 2009) to gen-
erate a standard hierarchical phrase-based grammar
GH . This uses rules with up to two nonadjacent non-
terminals, but excludes identical rule types such as
X??w X,w X? or X??w X1 w X2,w X1 w X2?,
which were reported to cause computational difficul-
ties without a clear improvement in translation (Igle-
sias et al, 2009).
Grammar expressivity. As expected, for the stan-
dard extraction method (see rows entitled V-union),
grammar G1 is shown to underperform all other
grammars due to its structural limitations. On the
other hand, grammar G2 obtains much better scores,
nearly generating the same translation quality as
the baseline grammar GH . Finally, G3 does not
prove able to outperform G2, which suggests that
the phrase-disjoint rules with one nonterminal are
redundant for the translation grammar.
Rule extraction method. For all grammars, we
find that the proposed extraction methods based on
alignment posteriors outperform standard Viterbi-
based extraction, with improvements ranging from
0.5 to 1.1 BLEU points for test-nw1 (depending on
the grammar) and from 1.0 to 1.5 for test-nw2. In
all cases, the use of phrase posteriors PP is the best
option. Interestingly, we find that G2 extracted with
WP and PP methods outperforms the more complex
GH grammar as obtained from Viterbi alignments.
Rule set statistics. For grammar G2 evaluated
on the tune-nw set, standard Viterbi-based extrac-
tion produces 0.7M rules, whereas the WP and PP
extraction methods yield 1.1M and 1.2M rules re-
spectively. We further analyse the sets of rules
X???,?,?? in terms of the number of distinct
source and target sequences ? and ? which are ex-
tracted. Viterbi extraction yields 82k distinct source
sequences whereas the WP and PP methods yield
116k and 146k sequences, respectively. In terms
of the average number of target sequences for each
source sequence, Viterbi extraction yields an aver-
age of 8.7 while WP and PP yield 9.7 and 8.4 rules
on average. This shows that method PP yields wider
coverage but with sharper forward rule translation
probability distributions than method WP, as the av-
erage number of translations per rule is determined
by the p(?|?) > 0.01 threshold mentioned in Sec-
tion 3.2.
Decoding time and pruning in search. In connec-
tion to the previous comments, we find an increased
need for search pruning, and subsequently slower
decoding speed, as the search space grows larger
with methods WP and PP. A larger search space is
created by the larger rule sets, which allows the sys-
tem to generate new hypotheses of better quality.
5.3 Rule Concatenation in Grammar G2
In Section 4.1 we described a strategy to reduce
grammar redundancy by introducing an additional
nonterminal M for monotonic concatenation rules.
We find that without this distinction among nonter-
minals, search pruning and decoding time are in-
creased by a factor of 1.5, and there is a slight degra-
dation in BLEU (?0.2) as more search errors are in-
troduced.
Another relevant aspect of this grammar is the ac-
tual rule type selected for monotonic concatenation.
We described using type X??w X,w X? (con-
catenation on the right), but one could also include
X??X w,X w? (concatenation on the left), or both,
for the same purpose. We evaluated the three alter-
natives and found that scores are identical when ei-
ther including right or left concatenation types, but
including both is harmful for performance, as the
need to prune and decoding time increase by a fac-
552
tor of 5 and 4, respectively, and we observe again a
slight degradation in performance.
Rule Extraction tune-nw test-nw1 test-nw2
V-st 34.7 35.6 37.5
V-ts 34.0 34.8 36.6
V-union 34.5 35.4 37.2
V-gdf 34.4 35.3 37.1
WP-st 35.1 36.0 37.7
WP-ts 34.5 35.0 37.0
PP-st 35.5 36.4 38.2
PP-ts 34.8 35.3 37.2
PP-merge 35.5 36.4 38.4
PP-merge-MERT 35.5 36.4 38.3
LMBR(V-st) 35.0 35.8 38.4
LMBR(V-st,V-ts) 35.5 36.3 38.9
LMBR(PP-st) 36.1 36.8 38.8
LMBR(PP-st,PP-ts) 36.4 36.9 39.3
Table 3: Translation results under grammar G2 with indi-
vidual rule sets, merged rule sets, and rescoring and sys-
tem combination with lattice-based MBR (lower-cased
BLEU shown)
5.4 Symmetrizing Alignments of Parallel Text
In this section we investigate extraction from align-
ments (and posterior distributions) over parallel text
which are generated using alignment models trained
in the source-to-target (st) and target-to-source (ts)
directions. Our motivation is that symmetrization
strategies have been reported to be beneficial for
Viterbi extraction methods (Och and Ney, 2003;
Koehn et al, 2003).
Results are shown in Table 3 for grammar G2. We
find that rules extracted under the source-to-target
alignment models (V-st, WP-st and PP-st) consis-
tently perform better than the V-ts, WP-ts and PP-
ts cases. Also, for Viterbi extraction we find that the
source-to-target V-st case performs better than any
of the symmetrization strategies, which contradicts
previous findings for non-hierarchical phrase-based
systems(Koehn et al, 2003).
We use the PP rule extraction method to extract
two sets of rules, under the st and ts alignment mod-
els respectively. We now investigate two ways of
merging these sets into a single grammar for trans-
lation. The first strategy is PP-merge and merges
both rule sets by assigning to each rule the maximum
count assigned by either alignment model. We then
extend the previous strategy by adding three binary
feature functions to the system, indicating whether
the rule was extracted under the ?st? model, the ?ts?
model or both. The motivation is that MERT can
weight rules differently according to the alignment
model they were extracted from. However, we do
not find any improvement with either strategy.
Finally, we use linearised lattice minimum Bayes-
risk decoding (Tromble et al, 2008; Blackwood et
al., 2010) to combine translation lattices (de Gis-
pert et al, 2010) as produced by rules extracted
under each alignment direction (see rows named
LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains
are consistent when comparing this to applying
LMBR to each of the best individual systems (rows
named LMBR(V-st) and LMBR(PP-st)). Overall,
the best-performing strategy is to extract two sets of
translation rules under the phrase pair posteriors in
each translation direction, and then to perform trans-
lation twice and merge the results.
6 Conclusion
Rule extraction based on alignment posterior proba-
bilities can generate larger rule sets. This results in
grammars with more expressive power, as measured
by the ability to align parallel sentences. Assign-
ing counts equal to phrase posteriors produces bet-
ter estimation of rule translation probabilities. This
results in improved translation scores as the search
space grows.
This more exhaustive rule extraction method per-
mits a grammar simplification, as expressed by the
phrase movement allowed by its rules. In particular
a simple grammar with rules of only one nontermi-
nal is shown to outperform a more complex gram-
mar built on rules extracted from Viterbi alignments.
Finally, we find that the best way to exploit align-
ment models trained in each translation direction is
to extract two rule sets based on alignment posteri-
ors, translate the input independently with each rule
set and combine translation output lattices.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
553
Agency, Contract No. HR0011- 06-C-0022.
References
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Efficient Path Counting Transducers
for Minimum Bayes-Risk Decoding of Statistical Ma-
chine Translation Lattices. In Proceedings of ACL,
Short Papers, pages 27?32.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Ad-
vances in Neural Information Processing Systems, vol-
ume 21, pages 161?168.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the ACL, pages
782?790.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient Bilin-
gual Chart Parsing. In Proceedings of IWSLT, pages
136?143.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, Short Papers, pages 25?28.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Yonggang Deng. 2005. Bitext Alignment for Statisti-
cal Machine Translation. Ph.D. thesis, Johns Hopkins
University.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the EACL, pages 380?388.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 48?54.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proceedings of EMNLP-CoNLL, pages 42?
50.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP, pages 1017?
1026.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP, pages 133?139.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proceedings of
the HLT-NAACL, pages 118?126.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from stochastic
inversion transduction grammars. In Proceedings of
the HLT-NAACL Workshop on Syntax and Structure in
Statistical Translation, pages 28?36.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL, pages 319?
326.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
of AMTA, pages 192?201.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
554
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373?1383,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias? Cyril Allauzen? William Byrne?
Adria` de Gispert? Michael Riley?
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{gi212,wjb31,ad465}@eng.cam.ac.uk
? Google Research, 76 Ninth Avenue, New York, NY 10011
{allauzen,riley}@google.com
Abstract
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
1 Introduction
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M , this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} ? G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
2. Applying the language model to these target
translations: L=T ?M, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M .
3. Searching for the translation and language
model combination with the highest-probablity
path: L?=argmaxl?LL
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf . In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al, 2009a; de Gispert et al,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
1373
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) ? the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
1.1 Related Work
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al, 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al (2009a)
and de Gispert et al (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al, 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al, 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
2 Pushdown Automata
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
1374
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
(a) (b)
0
1
(
3
?
2
a
4(
)
5
b
)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
(c) (d)
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ? N}. (b) Regular (but not bounded-stack)
PDA accepting a?b?. (c) Bounded-stack PDA accepting
a?b? and (d) its expansion as an FSA.
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al, 2007).
2.1 Definitions
A (restricted) Dyck language consist of ?well-
formed? or ?balanced? strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a? denote f(a) if
a ? A and f?1(a) if a ? A. The Dyck language
DA over the alphabet A? = A ? A is then the lan-
guage defined by the following context-free gram-
mar: S ? , S ? SS and S ? aSa? for all a?A.
We define the mapping cA : A?? ? A?? as follow.
cA(x) is the string obtained by iteratively deleting
from x all factors of the form aa? with a ? A. Ob-
serve that DA=c?1A ().
Let A and B be two finite alphabets such that
B ? A, we define the mapping rB : A? ? B?
by rB(x1 . . . xn) = y1 . . . yn with yi = xi if xi ?B
and yi= otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ? {?},min,+,?, 0) is
a 9-tuple (?,?,?, Q,E, I, F, ?) where ? is the fi-
nite input alphabet, ? and ? are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ?Q the initial state, F ? Q the set of final states,
E ? Q ? (? ? ?? ? {}) ? (R ? {?}) ? Q a fi-
nite set of transitions, and ? : F ? R ? {?} the
final weight function. Let e= (p[e], i[e], w[e], n[e])
denote a transition in E.
A path pi is a sequence of transitions pi=e1 . . . en
such that n[ei]=p[ei+1] for 1 ? i < n. We then de-
fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] ? ? ? i[en],
and w[pi]=w[e1] + . . . + w[en].
A path pi is accepting if p[pi] = I and n[pi] ? F .
A path pi is balanced if r??(i[pi]) ?D?. A balanced
path pi accepts the string x ? ?? if it is a balanced
accepting path such that r?(i[pi])=x.
The weight associated by T to a string x ? ??
is T (x) = minpi?P (x) w[pi] + ?(n[pi]) where P (x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |= |Q|+|E|.
A PDA T has a bounded stack if there exists K ?
N such that for any sub-path pi of any balanced path
in T : |c?(r??(i[pi]))| ? K . If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weighted finite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
1375
2.2 Expansion Algorithm
Given a bounded-stack PDA T , the expansion of T
is the FSA T ? equivalent to T defined as follows.
A state in T ? is a pair (q, z) where q is a state in T
and z ???. A transition (q, a, w, q?) in T results in
a transition ((q, z), a?, w, (q?, z?)) in T ? only when:
(a) a?? ? {}, z? =z and a? =a, (b) a??, z? =za
and a? = , or (c) a ? ?, z? is such that z = z?a
and a? = . The initial state of T ? is I ? = (I, ). A
state (q, z) in T ? is final if q is final in T and z = 
(??((q, ))=?(q)). The set of states of T ? is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z| ? K).
The complexity of the algorithm is linear in
O(|T ?|)=O(e|T |). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
2.3 Intersection Algorithm
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al, 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1?T2, the intersection of T1 and T2, such
that for all x ? ??: (T1?T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input- transitions. When
T2 has input- transitions, an epsilon filter (Mohri,
2009; Allauzen et al, 2011) generalized to handle
parentheses can be used.
A state in T =T1?T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I=(I1, I2). Given a transition e1=(q1, a, w1, q?1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ? ?, then e1 can be matched with a tran-
sition (q2, a, w2, q?2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q?1, q?2)) in T .
If a = , then e1 is matched with staying in q2
resulting in a transition ((q1, q2), , w1, (q?1, q2)).
Finally, if a ? ??, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q?1, q2)) in T .
A state (q1, q2) in T is final when both q1 and q2
are final, and then ?((q1, q2))=?1(q1)+?2(q2).
SHORTESTDISTANCE(T )
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 GETDISTANCE(T, I)
4 return d[f, I ]
RELAX(q, s, w,S)
1 if d[q, s] > w then
2 d[q, s]? w
3 if q 6? S then
4 ENQUEUE(S , q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[q, s]??
3 d[s, s]? 0
4 Ss ? s
5 while Ss 6=? do
6 q ? HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e ? E[q] do
9 if i[e] ? ? ? {} then
10 RELAX(n[e], s, d[q, s] + w[e],Ss)
11 elseif i[e] ? ? then
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[q, s] +w[e] + d[p[e?], n[e]] + w[e?]
18 RELAX(n[e?], s, w,Ss)
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and ?(f)=0 to simplify the presentation.
The complexity of the algorithm is in O(|T1||T2|).
2.4 Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T |3 log |T |) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by Cs the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in Cs to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in Cs. The
1376
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
Ss, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e], i[e]]| is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N |Q| |E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is ?(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N |T |3 ?(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T |). When all
the Cs?s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T | becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T , there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e? such that e?B[n[e?], i[e?]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T |) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
2.5 Replacement Algorithm
A recursive transition network (RTN) can be speci-
fied by (N,?, (T?)??N , S) where N is an alphabet
of nonterminals, ? is the input alphabet, (T?)??N is
a family of FSAs with input alphabet ? ? N , and
S?N is the root nonterminal.
A string x ? ?? is accepted by R if there exists
an accepting path pi in TS such that recursively re-
placing any transition with input label ? ?N by an
accepting path in T? leads to a path pi? with input x.
The weight associated by R is the minimum over all
such pi? of w[pi?]+?S(n[pi?]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(?,?,?, Q,E, I, F, ?, ?) with ?=Q=???N Q? ,
I = IS , F = FS , ?= ?S , and E =
?
??N
?
e?E? Ee
where Ee = {e} if i[e] 6? N and Ee =
{(p[e], n[e], w[e], I?), (f, n[e], ??(f), n[e])|f ?F?}
with ?= i[e]?N otherwise.
The complexity of the construction is in O(|T |).
If |F? | = 1, then |T | = O(
?
??N |T? |) = O(|R|).
Creating a superfinal state for each T? would lead to
a T whose size is always linear in the size of R.
3 Hierarchical Phrase-Based Translation
Representation
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S?abXdg, S?acXfg, and X?bc.
Figure 3 shows several alternative representations of
T : Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
1377
SX
3 3
a
1 1
b
2
1
c
2
2
d
4
f
4
g
5 5
(a) Hypergraph
0
1a
6
a
2b
7c
3X 4d 5g
8X 9f 10g 11 12b 13c
S X
(b) RTN
0
1a
6
a
2b
7c
11
(
12b
3 4d 5g
[
8 9f 10g
13c
)
]
(c) PDA
0,?
1,?a
6,?
a
2,?b
7,?c
11,(?
11,[?
12,(b
12,[b
13,(c
13,[c
3,??
8,??
4,?d
9,?f
5,?g
10,?g
(d) FSA
Figure 3: Alternative translation representations
duced by the HiFST decoder (Iglesias et al, 2009a;
de Gispert et al, 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
0 1a
2b
3
c
4X
5X
6
d
f 7
g
0 1b 2c
S X
(a) RTN
0 1a
2b
3
c 8
(
[ 9
b
4
6
d
7g
5
f1 0c
)
]
(b) PDA
0 1a
2b
3
c
4b
5b
6c
7c
8
d
f 9
g
(c) FSA
Figure 4: Optimized translation representations
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al, 1964) with time and space complex-
ity O(|Th||M |3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M |). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M |) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M |.2
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M |4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M |. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq . Given a state (q1, q2)
in Tp ?M , (q1, q2) ? C(s1,s2) only if s1 = sq1 , and hence
(q1, q2) belongs to at most |M | components.
1378
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)
PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)
FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)
Table 1: Complexity using various target translation rep-
resentations.
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
4 Experimental Framework
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al, 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
0 7.5? 10?9 7.5? 10?8 7.5? 10?7
207.5 20.2 4.1 0.9
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different ? values (top row).
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al, 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
? A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p > 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
? An unrestricted one without the previous con-
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
1379
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of ? to reduce the size of
M1. We will call the resulting language model
as M?1 . Table 2 shows the number of n-grams
(in millions) obtained for different ? values.
3. We translate with M?1 using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and M?1 .
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width ?.
5. We remove the M?1 scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
Note that if ?=? or if ?=0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase ? to reduce the
size of the language model used at Step 3, ? will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If ? defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with M?1 would be possible without incurring search
errors M1. This is investigated next.
5 Entropy-Pruned LM in Rescoring
In Table 3 we show translation performance under
grammar G1 for different values of ?. Performance
is reported after first-pass decoding with M?1 (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses ? = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the M?1 involved in
decoding and the ? applied prior to rescoring.
As shown in row number 2, for ? ? 10?9 the
system provides the same performance to the base-
line when ? > 8, while decoding time is reduced
by roughly 40%. This is because M?1 is 10% of the
size of the original language model M1, as shown
in Table 2. As M?1 is further reduced by increas-
ing ? (see rows number 3 and 4), decoding time is
also reduced. However, the beam width ? required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as ? is wide enough,
given the particular M?1 used in first-pass decoding.
Interestingly, results show that a similar ? value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with ?=10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam ? is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (? =12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al, 2010).
6 Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
1380
HiFST (G1 + M?1 ) +M1 +M2
# ? tune-nw test-nw time ? tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5? 10?9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5? 10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5? 10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various ? is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
Exact search for G2 + M?1 with memory usage under 10 GB
# ? HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5? 10?9 12 51 37 40 8 52
3 7.5? 10?8 16 53 31 76 1 23
4 7.5? 10?7 18 53 29 99.8 0 0.2
Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M?1 ;
HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.
HiPDT (G2 + M?1 ) +M1 +M2
? tune-nw test-nw ? tune-nw test-nw tune-nw test-nw
7.5? 10?7 25.7 26.3 15 34.6 34.8 35.2 36.1
Table 5: HiPDT performance on grammar G2 with ? = 7.5 ? 10?7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various M?1 . With
?=7.5? 10?9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
M?1 is more reduced, and for ?=7.5?10?7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
7 Discussion and Conclusion
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under ? = 7.5 ? 10?8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
1381
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
?=7.5? 10?8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
8 Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 of LNCS, pages 28?38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116?150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343?354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
1382
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ?05, pages 65?73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1?18.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of NAACL-HLT, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380?388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings of Eurospeech, pages 1251?1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al (Drosde et al, 2009), chapter 6, pages
213?254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137?148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al (Drosde et
al., 2009), chapter 7, pages 257?289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings of IEEE
International Conference on Portable Information De-
vices, pages 1 ?5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
1383
Hierarchical Phrase-Based Translation with
Weighted Finite-State Transducers and
Shallow-n Grammars
Adria` de Gispert?
University of Cambridge
Gonzalo Iglesias??
University of Vigo
Graeme Blackwood?
University of Cambridge
Eduardo R. Banga??
University of Vigo
William Byrne?
University of Cambridge
In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation
and alignment. The decoder is implemented with standard Weighted Finite-State Transducer
(WFST) operations as an alternative to the well-known cube pruning procedure. We find that
the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in
fewer search errors, better parameter optimization, and improved translation performance. The
direct generation of translation lattices in the target language can improve subsequent rescoring
procedures, yielding further gains when applying long-span language models and Minimum
Bayes Risk decoding. We also provide insights as to how to control the size of the search space
defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation,
and other search constraints can help to match the power of the translation system to specific
language pairs.
1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising
approaches to statistical machine translation (SMT). Hiero SMT systems are based
on probabilistic synchronous context-free grammars (SCFGs) whose translation rules
? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K.
E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.
?? University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.
E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.
Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for
publication: 10 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
can be extracted automatically from word-aligned parallel text. These grammars can
produce a very rich space of candidate translations and, relative to simpler phrase-
based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in
translation between dissimilar languages, such as English and Chinese (Chiang 2005,
2007). Hiero is able to learn and apply complex patterns in movement and translation
that are not possible with simpler systems. Hiero can also be used to good effect on
?simpler? problems, such as translation between English and Spanish (Iglesias et al
2009c), even though there is not the same need for the full complexity of movement and
translation. If gains in using Hiero are small, however, the computational and modeling
complexity involved are difficult to justify. Such concerns would vanish if there were
reliable methods to match Hiero complexity for specific translation problems. Loosely
put, it would be a good thing if the complexity of a system was somehow proportional
to the improvement in translation quality the system delivers.
Another notable current trend in SMT is system combination. Minimum Bayes
Risk decoding is widely used to rescore and improve hypotheses produced by indi-
vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),
and more aggressive system combination techniques which synthesize entirely new
hypotheses from those of contributing systems can give even greater translation im-
provements (Rosti et al 2007; Sim et al 2007). It is now commonplace to note that even
the best available individual SMT system can be significantly improved upon by such
techniques. This puts a burden on the underlying SMT systems which is somewhat
unusual in NLP. The requirement is not merely to produce a single hypothesis that
is as good as possible. Ideally, the SMT systems should generate large collections of
candidate hypotheses that are simultaneously diverse and of good quality.
Relative to these concerns, previously published descriptions of Hiero have noted
certain limitations. Spurious ambiguity (Chiang 2005) was described as
a situation where the decoder produces many derivations that are distinct yet have the
same model feature vectors and give the same translation. This can result in n-best lists
with very few different translations which is problematic for the minimum-error-rate
training algorithm ...
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all
distinct hypotheses to a fixed depth by means of k-best hypothesis lists. If enumeration
was not necessary, or if the lists could be arbitrarily deep, there might still be many
duplicate derivations, but at least the hypothesis space would not be impoverished.
Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu
1997; Setiawan et al 2009). For our purposes we say that overgeneration occurs when
different derivations based on the same set of rules give rise to different translations.
An example is given in Figure 1.
This process is not necessarily a bad thing in that it allows new translations to be
synthesized from rules extracted from training data; a strong target language model,
such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are
introduced only to be subsequently discarded. The situation is further complicated by
search errors. Any search procedure which relies on pruning during search is at risk of
search errors and the risk is made worse if the grammars tend to introduce many similar
scoring hypotheses. In particular we have found that cube pruning is very prone to
search errors, that is, the hypotheses produced by cube pruning are not the top scoring
hypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).
506
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 1
Example of multiple translation sequences from a simple grammar fragment showing variability
in reordering in translation of the source sequence abc.
These limitations are clearly related to each other. Moreover, they become more
problematic as the amount of parallel text grows. As the number of rules in the grammar
increases, the grammars become more expressive, but the ability to search them does not
improve. This leads to a widening gap between the expressive power of the grammar
and the ability to search it to find good and diverse hypotheses.
In this article we describe the following two refinements to Hiero which are in-
tended to address some of the limitations in its original formulation.
Lattice-based hierarchical translation We describe how the cube pruning procedure
can be replaced by standard operations with Weighted Finite State Transducers
(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.
We find that keeping partial translation hypotheses in lattice form greatly reduces
search errors. In some instances it is possible to perform translation without
any pruning at all so that search errors are completely eliminated. Consistent
with the observation by Chiang (2005), this leads to improvements in minimum
error rate training. Furthermore, the direct generation of translation lattices can
improve gains from subsequent language model and Minimum Bayes Risk (MBR)
rescoring.
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
corporated into hierarchical translation rules for the purpose of tuning the size
of the Hiero search space for individual language pairs. Shallow-n grammars are
described and shown to control the level of rule nesting, low-level rule catenation,
and the minimum and maximum spans of individual translation rules. In trans-
lation experiments we find that a shallow-1 grammar (one level of rule nesting)
is sufficiently expressive for Arabic-to-English translation, but that a shallow-3
grammar is required in Chinese-to-English translation to match the performance
of a full Hiero system that allows arbitrary rule nesting. These nonterminals are
introduced to control the Hiero search space and do not require estimation from
annotated?or parsed?parallel text, as can be required by translation systems
based on linguistically motivated grammars. We use this approach as the basis
of a general approach to SMT modeling. To control overgeneration, we revisit
the synchronous context-free grammar defined by hierarchical rules and take a
shallow-1 grammar as a starting point. We then increase the complexity of the
rules until the desired translation quality is found.
507
Computational Linguistics Volume 36, Number 3
With these refinements we find that hierarchical phrase-based translation can be effi-
ciently carried out with no (or minimal) search errors in large-data tasks and can achieve
state-of-the-art translation performance.
There are many benefits to formulating Hiero translation in terms of WFSTs. Fol-
lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne
(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,
we can use WFST operations to make the operations of the Hiero decoder very clear. The
simplicity of the analysis makes it possible to focus on the underlying grammars and
avoid the complexities of heuristic search procedures. Once the decoder is formulated,
implementation is mostly straightforward using standard WFST techniques developed
for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due
to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as
extracting sufficient statistics for minimum error rate training, can be done relatively
easily and naturally.
1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system
based on the OpenFST WFST libraries (Allauzen et al 2007). We describe how trans-
lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for
parsing under Hiero. We also review some modeling issues needed for practical trans-
lation, such as the efficient handling of source language deletions and the extraction of
statistics for minimum error rate training. This requires running HiFST in ?alignment
mode? (Section 2.3) to find all the rule derivations that generate a given set of translation
hypotheses.
In Section 3 we investigate parameters that control the size and nature of the
hierarchical phrase-based search space as defined by hierarchical translation rules. To
efficiently explore the largest possible space and avoid pruning in search, we introduce
ways to easily adapt the grammar to the reordering needs of each language pair. We
describe the use of additional nonterminal categories to limit the degree of rule nesting,
and can directly control the minimum or maximum span each translation rule can cover.
In Section 4 we report detailed translation results for Arabic-to-English and
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-
to-English translation. In these experiments we contrast the performance of lattice-based
and cube pruning hierarchical decoding and we measure the impact on processing
time and translation performance due to changes in search parameters and grammar
configurations. We demonstrate that it is easy and feasible to compute the marginal
instead of the Viterbi probabilities when using WFSTs, and that this yields gains in
translation performance. And finally, we show that lattice-based translation performs
significantly better than k-best lists for the task of combining translation hypotheses
generated from alternative morphological segmentations of the data via lattice-based
MBR decoding.
2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and
a particular search space of translation candidates. Table 1 shows the type of rules in-
cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals
(words) and ? is a bijective function that relates the source and target nonterminals of
508
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 1
Rules contained in the standard hierarchical grammar.
standard hierarchical grammar
S??X,X? glue rule 1
S??S X,S X? glue rule 2
X???,?,?? , ?,? ? {X ? T}+ hiero rules
each rule (Chiang 2007). This function is defined if there are at least two nonterminals,
and for clarity of presentation may be omitted in general rule discussions. When ?,? ?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.
The HiFST translation system is based on a variant of the CYK algorithm closely
related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination
without pruning. The underlying model is a probabilisitic synchronous CFG consisting
of a set R = {Rr} of rules Rr : Nr ? ??r,?r? / pr, with ?glue? rules, S ? ?X,X? and
S ? ?S X,S X?. N denotes the set of nonterminal categories (examples are given in
Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless
otherwise noted we use the tropical semiring, so cr = ? log pr. T denotes the terminals
(words), and the grammar builds parses based on strings ?,? ? {N ? T}+. Each cell
in the CYK grid is specified by a nonterminal symbol and position in the CYK grid:
(N, x, y), which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed using a CFG with rules N ? ?. The
generation of translations is a second step that follows parsing. For this second step, we
describe a method to construct word lattices with all possible translations that can be
produced by the hierarchical rules. Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we
build a target language word lattice L(N, x, y). This lattice contains every translation of
sx+y?1x from every derivation headed by N. These lattices also contain the translation
scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the
analyses that cover the source sentence sJ1. Once this is built, we can apply a target lan-
guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,
Mohri, and Roark 2003).
2.1 Lattice Construction over the CYK Grid
In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that
is, for r ? R(N, x, y), the rule N ? ??r,?r? was used in at least one derivation involving
that cell.
For each rule Rr, r ? R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived
from the target side of the rule ?r by concatenating lattices corresponding to the ele-
ments of ?r = ?r1...?
r
|?r|. If an ?
r
i is a terminal, creating its lattice is straightforward. If
?ri is a nonterminal, it refers to a cell (N
?, x?, y?) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?). Taken together,
L(N, x, y, r) =
?
i=1..|?r|
L(N, x, y, r, i) (1)
509
Computational Linguistics Volume 36, Number 3
Figure 2
Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The grid is represented here in two dimensions (x, y). In practice only the first column accepts
both nonterminals (S,X). For this reason it is divided into two subcolumns.
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
L(N?, x?, y?) otherwise (2)
where A(t), t ? T returns a single-arc acceptor which accepts only the symbol t. The
lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in
R(N, x, y):
L(N, x, y) =
?
r?R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the? and?WFST operations,
respectively, as described by Allauzen et al (2007). If a rule Rr has a cost cr, it is applied
to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).
2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-
word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand
side shows the state of the CYK grid after parsing using the rules R1 to R5. These include
three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules
(R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the
uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses
covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice
L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of
these two rules. This process is explicitly derived in the right-hand side of Figure 2.
2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for
the same sentence. Three rules, R6,R7,R8, are added to the example of Figure 2, thus
providing two additional derivations. This makes use of sublattices already produced
in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.
510
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 3
Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear
within {}.
2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses
memoization: If a lattice for a requested cell already exists, it is returned (line 2);
otherwise it is constructed via Equations (1)?(3). For every rule, each element of the
target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a
terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the
lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this
cell is then found by union of all the component rules (line 10, Equation (3)); this lattice
is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this
point to remove any epsilon arcs which may have been introduced by the various WFST
union, concatenation, and replacement operations (Allauzen et al 2007).
We now address several important aspects of efficient implementation.
Figure 4
Recursive lattice construction from a CYK grid.
511
Computational Linguistics Volume 36, Number 3
Figure 5
Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after
minimization (right).
2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in
upper levels of the grid through the union and concatenation of lattices from lower
levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices,
the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This
effectively builds a skeleton for the desired lattice and delays the creation of the final
word lattice until a single replacement operation is carried out in the top cell (S, 1, J).
To make this exact, we define a function g(N, x, y) which returns a unique tag for each
lattice in each cell, and use it to redefine Equation (2). With the back-pointer (N?, x?, y?) =
BP(N, x, y, r, i), these special arcs are introduced as
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N?, x?, y?)) otherwise (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice
pointers (Figure 5, left). Each still represents the entire search space of all translation
hypotheses covering the span, however. Importantly, operations on these lattices?such
as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002)?can still be performed. Owing to the existence of multiple hierarchical rules
which share the same low-level dependencies, these operations can greatly reduce the
size of the skeleton lattice; Figure 5 shows the effect on the translation example. This
process is carried out for the lattice at every cell, even at the lowest level where there
are only sequences of word terminals. As stated, size reductions can be significant. Not
all redundancy is removed, however, because duplicate paths may arise through the
concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.
A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointers
by their lower-level lattices until no pointers are left, thus producing the complete
target word lattice for the whole source sentence. The use of the lattice pointer arc was
inspired by the ?lazy evaluation? techniques developed by Mohri, Pereira, and Riley
(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its
implementation uses the infrastructure provided by the OpenFST libraries for delayed
composition.
2.2.2 Top-level Pruning and Search Pruning. The final translation lattice L(S, 1, J) can
be quite large after the pointer arcs are expanded. We therefore apply a word-based
512
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 6
Transducers for filtering up to one (left) or two (right) consecutive deletions.
language model via WFST composition (Allauzen et al 2007) and perform likelihood-
based pruning based on the combined translation and language model scores. We call
this top-level pruning because it is performed over the topmost lattice.
Pruning can also be performed on the sublattices in each cell during search. One
simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).
If this number is above a threshold, we expand any pointer arcs and apply a word-based
language model via composition. The resulting lattice is then reduced by likelihood-
based pruning, after which the LM scores are removed. These pruning strategies can be
very selective, for example allowing the pruning threshold to depend on the height of
the cell in the grid. In this way the risk of search errors can be controlled.
The same n-gram language model can be used for top-level pruning and search
pruning, although different WFST realizations are required. For top-level pruning, a
standard implementation as described by Allauzen et al (2007) is appropriate. For
search pruning, the WFST must allow for incomplete language model histories, because
many sublattice paths are incomplete translation hypotheses which do not begin with
a sentence-start marker. The language model acceptor is constructed so that initial
substrings of length less than the language model order are assigned no weight under
the language model.
2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT
systems to delete some source words rather than to enforce their translation. Deletions
can be allowed in Hiero by including in the grammar a set of special deletion rules
of the type: X??s,NULL?. Unconstrained application of these rules can lead to overly
large and complex search spaces, however. We therefore limit the number of consecutive
source word deletions as we explore each cell of the CYK grid. This is done by standard
composition with an unweighted transducer that maps any word to itself, and up to k
NULL tokens to  arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations
with more than k consecutive deleted words.
2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in
alignment is to recover all the derivations which can produce a given translation. We do
this rather than keep track of the rules used during translation, because we find it faster
and more efficient first to generate translations and then, by running the system as an
aligner with a constrained target space, to extract all the relevant derivations with their
costs. As will be discussed in Section 2.3.1, this is useful for minimum error training,
where the contribution of each feature to the overall hypothesis cost is required for
system optimization.
513
Computational Linguistics Volume 36, Number 3
Figure 7
Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation
t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules:
R1: S??X,X?, R2: S??S X,S X? , R3: X??s1,t5?, R4: X??s2 s3,t8?, R5: X??s1 X s3,X t8? and R6:
X??s2,t5?.
Conceptually, we would like to create a transducer that represents the mapping
from all possible rule derivations to all possible translations, and then compose this
transducer with an acceptor for the translations of interest. Creating this transducer
which maps derivations to translations is not feasible for large translation grammars,
so we instead keep track of rules as they are used to generate a particular translation
output. We introduce two modifications into lattice construction over the CYK grid
described in Section 2.2:
1. In each cell transducers are constructed which map rule sequences to the
target language translation sequences they produce. In each transducer the
output strings are all possible translations of the source sentence span
covered by that cell; the input strings are all the rule derivations that
generate those translations. The rule derivations are expressed as
sequences of rule indices r given the set of rules R = {Rr}.
2. As these transducers are built they are composed with acceptors for
subsequences of the reference translations so that any translations not
present in the given set of reference translations are removed. In effect, this
replaces the general target language model used in translation with an
unweighted acceptor which accepts only specific sentences.
For alignment, Equations (1) and (2) are redefined as
L(N, x, y, r) = AT(r,)
?
i=1..|?r|
L(N, x, y, r, i) (5)
L(N, x, y, r, i) =
{
AT(,?i) if ?i ? T
L(N?, x?, y?) otherwise (6)
where AT(r, t), Rr ? R, t ? T returns a single-arc transducer which accepts the symbol
r in the input language (rule indices) and the symbol t in the output language (target
words). The weight assigned to each arc is the same in alignment as in translation. With
these definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the
input symbols and target words in the output symbols. A simple example is given
in Figure 7 where two rule derivations for the translation t5t8 are represented by the
transducer.
As we are only interested in those rule derivations that generate the given target
references, we can discard non-desired translations via standard FST composition of
514
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 8
Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its
substring acceptor (right). In alignment the substring acceptor can be used to filter out undesired
partial translations via standard FST composition operations.
the lattice transducer with the given reference acceptor. In principle, this would be done
in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until
the last cell may not be computationally feasible for many sentences. It is more desirable
to carry out this filtering composition in lower-level cells while constructing the lattice
over the CYK grid so as to avoid storing an increasing number of undesired translations
and derivations in the lattice. The lattice in each cell should contain translations formed
only from substrings of the references.
To achieve this we build an unweighted substring acceptor that accepts all sub-
strings of each target reference string. For instance, given the reference string t1t2 . . .
tJ, we build an acceptor for all substrings ti . . . tj, where 1 ? i ? j ? J. This is applied
to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost
cell we compose with the reference acceptor which accepts only complete reference
strings. Given a lattice of target references, the unweighted substring acceptor is
built as:
1. change all non-initial states into final states
2. add one initial state and add  arcs from it to all other states
Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and
t3t4. The substring acceptor also accepts an empty string, accounting for those rules
that delete source words, which in other words translate into NULL. In some instances
the final composition with the reference acceptor might return an empty lattice. If this
happens there is no rule sequence in the grammar that can generate the given source
and target sentences simultaneously.
We have described the use of transducers to encode mappings from rule deriva-
tions to translations. These transducers are somewhat impoverished relative to parse
trees and parse forests, which are more commonly used to encode this relationship. It
is easy to map from a parse tree to one of these transducers but the reverse essentially
requires re-parsing to recreate the tree structure. The structures of the parse trees asso-
ciated with a translation are not needed by many algorithms, however. In particular,
parameter optimization by MERT requires only the rules involved in translation. Our
approach keeps only what is needed by such algorithms. This approach also has prac-
tical advantages such as the ability to align directly to k-best lists or lattices of reference
translations.
515
Computational Linguistics Volume 36, Number 3
Figure 9
One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the
result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The
components of the final K-dimensional weight vector agree with the feature weights of the rule
sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . .K.
2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are
associated with hierarchical translation rules through a factoring into features within a
log-linear model (Och and Ney 2002). We assume that we have a collection of K features
and that the cost cr of each rule Rr is cr =
?K
k=1 ?kc
r,k, where cr,k is the value of the kth
feature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.
For a parse which makes use of the rules Rr1 . . .RrN , its cost
?N
n=1 c
rn can therefore
be written as
?K
k=1 ?k
?N
n=1 c
rn,k. The quantity
?N
n=1 c
rn,k is the contribution by the kth
feature to the overall translation score for that parse. These are the quantities which
need to be extracted from alignment lattices for use in procedures such as minimum
error rate training for estimation of the feature weights ?k.
The procedure described in Section 2.3 produces alignment lattices with scores
consistent with the total parse score. Further steps must be taken to factor this over-
all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the
input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature
value for that rule. Arcs have the form 0
Rr/wr?? 0 where wr = [cr,1, . . . , cr,K]. An example
of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores
are mapped to components of the weight vector. The same operations can be applied to
the (unweighted) alignment transducer on a much larger scale to extract the statistics
needed for minimum error rate training.
We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so
that only the best rule derivation that generated each translation candidate is taken
into account when extracting feature contributions for MERT. However, given the
alignment transducer L, this could also be performed in the log semiring (marginal
likelihoods), taking into account the feature contributions from all rule derivations, for
each translation candidate. This would be adequate if the translation system also car-
ried out decoding in the log semiring, an experiment which is partially explored in
Section 4.4.
We note that the contribution of the language model to the overall translation score
cannot be calculated in this scheme, since the language model score cannot be factored
in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al 2007) between an unweighted acceptor of the target
516
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 10
Hierarchical translation grammar example and two parse trees with different levels of rule
nesting for the input sentence s1s2s3s4.
sentences and the n-gram language model used in translation. After determinization,
the cost of each path in the acceptor is then the desired LM feature contribution.
3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration
and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-
fine the most constrained grammar that is capable of generating the desired movement
and translation, so that decoding can be performed without search errors.
Hiero can provide varying degrees of complexity in movement and translation.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined
by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown;
the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree
is shown the translation generated and the phrase-level alignment. Comparing the two
trees and alignments, the leftmost tree makes use of more reordering when translating
from source to target through the nested application of the hierarchical rules R3 and R4.
For some language pairs this level of reordering may be required in translation, but for
other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose
the grammar in this example is modified as follows:
1. A nonterminal X0 is introduced into hierarchical translation rules
R3:X??X0 s3,t5 X0?
R4:X??X0 s4,t3 X0?
2. Rules for lexical phrases are applied to X0
R5:X0??s1 s2,t1 t2?
R6:X0??s4,t7?
These modifications exclude parses in which hierarchical translation rules generate
other hierarchical rules, except at the 0th level which generate lexical phrases. Con-
sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only
allowable translation of s1s2s3s4. We call this a ?shallow-1? grammar: The maximum
517
Computational Linguistics Volume 36, Number 3
degree of rule nesting allowed is 1 and only the glue rule can be applied above this
level.
The use of additional nonterminal categories is an elegant way to easily control
important aspects that can have a strong impact on the search space. A shallow-n
translation grammar can be formally defined as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
4. hierarchical translation rules for levels n = 1, . . . ,N:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
5. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is
included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational
capability. To see the effect of this constraint, consider the following example with a
source sentence s1 s2 and a shallow-1 grammar defined by these four rules:
R1: S??X1,X1?
R2: X1??s1 s2,t2 t1?
R3: X1??s1 X0,X0 t1?
R4: X0??s2,t2?
There are two derivations R1R2 and R1R3R4 which yield identical translations. However
R2 would not be allowed under the constraint introduced here because it does not
rewrite an X1 to an X0.
3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal
category S to act within the glue rule. This can prevent some useful long-distance
movement, as might be needed to translate Arabic sentences in Verb-Subject-Object
order into English. It often happens that the initial Arabic verb requires long-distance
movement, but the subject which follows can be translated in monotonic order. For
instance, consider the following Romanized Arabic sentence:
TAlb AlwzrA? AlmjtmEyn Alywm fy dm$q <lY ...
(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...
where the verb ?TAlb? must be translated into English so that it follows the translations
of the five subsequent Arabic words ?AlwzrA? AlmjtmEyn Alywm fy dm$q?, which
518
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
are themselves translated monotonically. A shallow-1 grammar cannot generate this
movement except in the relatively unlikely case that the five words following the verb
can be translated as a single phrase.
A more powerful approach is to define grammars which allow low-level rules to
form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow
successive generation of k nonterminals XN?1 in monotonic order for both languages,
where K1 ? k ? K2. These act in the same manner as the glue rule does in the uppermost
level. Applying Mk nonterminals at the N?1 level allows one hierarchical rule to perform
a long-distance movement over the tree headed by Mk.
We further refine the definition of shallow-n grammars by specifying the allowable
values of k for the successive productions of nonterminals XN?1. There are many pos-
sible ways to formulate and constrain these grammars. If K2 = 1, then the grammar
is equivalent to the previous definition of shallow-n grammars, because monotonic
production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the
search space defined by the grammar is greater than the standard shallow-n grammar
as it includes structured long-distance movement. Finally, if K1 > 1 then the search
space is different from standard shallow-n as the n level is only used for long-distance
movement.
Introduction of Mk nonterminals redefines shallow-n grammars as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. a set of nonterminals {MK1 , . . . ,MK2} for K1 = 1, 2; K1 ? K2
4. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
5. hierarchical translation rules for level N:
R: XN???,?,?? , ?,? ? {{MK1 , . . . ,MK2} ? T}+
with the requirement that ? and ? contain at least one Mk
6. hierarchical translation rules for levels n = 1, . . . ,N ? 1:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
7. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
8. rules which generate k nonterminals XN?1:
if K1 == 2 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 3, . . . ,K2
R: M2??XN?1 XN?1,XN?1 XN?1,I?
if K1 == 1 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 2, . . . ,K2
R: M1??XN?1,XN?1?
where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M3 leads to the monotonic production of three
nonterminals X0, which leads to the production of three lexical phrase pairs; these can be
moved with a hierarchical rule of level 1. This is graphically represented by the leftmost
tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of
519
Computational Linguistics Volume 36, Number 3
Figure 11
Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with
K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit.
Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2
allows up to two levels of reordering.
two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This
movement cannot be achieved with a shallow-1 grammar.
3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical
translation rules in generating the search space. Parameters hmax and hmin specify
the maximum and minimum height at which any hierarchical translation rule can be
applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell
(x, y) if hmin? y ?hmax. Note that these parameters can also be set independently for
each nonterminal category.
3.4 Verb Movement Grammars for Arabic-to-English Translation
Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb
restriction so that the hierarchical translation rules (5) apply only if the source language
string ? contains a verb. This encourages translations in which the Arabic verb is moved
at the uppermost level N.
3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we
describe next.
Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
any monotonicity constraints, that is K1 = K2 = 1.
Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a
monotonic production of up to three target language phrases of level 0.
520
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a
monotonic catenation of up to three target language phrases of level 0, but only if
one of the source terminals is tagged as a verb.
Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic production
of up to three target language phrases of level 1, but only if one of the source
terminals is tagged as a verb.
4. Translation Experiments
In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-
lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al 2001) as
implemented for the NIST 2009 evaluation.1 The experiments are organized as follows:
- Lattice-based and cube pruning hierarchical decoding (Section 4.2)
- Grammar configurations and search parameters and their effect on
translation performance and processing time (Section 4.3)
- Marginalization over translation derivations (Section 4.4)
- Combining translation lattices obtained from alternative morphological
decompositions of the input (Section 4.5)
4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08
(and MT09) Arabic Constrained Data track (?150M words per language). In addition to
reporting results on the MT08 set itself, we make use of a development set mt02-05-tune
formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation
sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune
set has 2,075 sentences.
For Chinese-to-English translation we use all available parallel text for the GALE
2008 evaluation;2 this is approximately 250M words per language. We report translation
results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.
These tuning and test sets contain translations produced by the GALE program and
portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755
sentences, and test-nw set is similar.
The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne
2008). We extract hierarchical rules from the aligned parallel texts using the constraints
developed by Chiang (2007). We further filter the extracted rules by count and pattern
as described by Iglesias et al(2009a). The following features are extracted from the
parallel data and used to assign scores to translation rules: source-to-target and target-
to-source phrase translation models, word and rule penalties, number of usages of the
glue rule, source-to-target and target-to-source lexical models, and three rule count
features inspired by Bender et al (2007).
1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
2 See http://projects.ldc.upenn.edu/gale/data/catalog.html
521
Computational Linguistics Volume 36, Number 3
We use two types of language model in translation. In first-pass translation we use
4-gram language models estimated over the English side of the parallel text (for each
language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning
is needed during search. The main language model is a zero-cutoff stupid-backoff
(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English text
from the English Gigaword corpus. These language models are converted to WFSTs
as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct
application of back-off weights. In tuning the systems, standard MERT (Och 2003)
iterative parameter estimation under IBM BLEU is performed on the development sets.
4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k-best
decoder using cube pruning following the description by Chiang (2007); in our im-
plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),
which are obtained by extracting the 10,000 best candidates from each cell (including
the language model cost), using a priority queue to explore the cross-product of the
k-best lists from the cells pointed by nonterminals. We find that deeper k-best lists
(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depths
yields better performance than use of a likelihood threshold parameter. The second
decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that
scores assigned to hypotheses arise from a single minimum cost/maximum likelihood
derivation.
Translation proceeds as follows. After Hiero translation with optimized feature
weights and the first-pass language model, hypotheses are written to disk. For HCP we
save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-
pass results are then rescored with the main 5-gram language model. In this operation
the first-pass language model scores are removed before the main language model
scores are applied. We then perform MBR rescoring. For the n-best lists we rescore
the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss
function (Kumar and Byrne 2004); we have found that using a deeper k-best list is
impractically slow. For the HiFST lattices we use lattice-based MBR search procedures
described by Tromble et al (2008) in an implementation based on standard WFST
operations (Allauzen et al 2007).
4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with
shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed
only to generate target language phrases. Table 2 shows results for mt02-05-tune, mt02-
05-test, and mt08. In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close
comparison of decoder behavior, independent of parameter optimization.
In these experiments, the first-pass translation quality of the two systems (Table 2
a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the
decoders is their memory use. For example, for an input sentence of 105 words, HCP
uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To
run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass
language model is not applied until the lattice is fully built at the upper most cell of the
522
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 2
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds
per word. Both systems are optimized using MERT over the k-best lists generated by HCP.
decoder time mt02-05-tune mt02-05-test mt08
a HCP 1.1 52.5 51.9 42.8
+5g - 53.4 52.9 43.5
+5g+MBR - 53.6 53.0 43.6
b HiFST 0.5 52.5 51.9 42.8
+5g - 53.6 53.2 43.9
+5g+LMBR - 54.3 53.7 44.8
CYK grid. For this grammar, HiFST is able to produce exact translations without any
search errors.
Search Errors Because both decoders are constrained to use exactly the same features,
we can compare their search errors on a sentence-by-sentence basis. A search error is
assigned to one of the decoders if the other has found a hypothesis with lower cost. For
mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower
cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any
sentence. This is as expected: The HiFST decoder requires no pruning prior to applying
the first-pass language model, so search is exact.
Lattice/k-best Quality It is clear from the results that the lattices produced by HiFST
yield better rescoring results than the k-best lists produced by HCP. This is the case for
both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k-best lists
yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large
search space in lattice form during decoding is clear. The use of k-best lists in HCP limits
the gains from subsequent rescoring procedures.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST
cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves
much more efficient to process compact lattices containing many hypotheses rather than
independently processing each distinct hypothesis in k-best form.
4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English
with full hierarchical decoding: nonterminals are allowed to generate other hierarchical
rules in recursion.
We apply the constraint hmax=10 for nonterminal category X, as described in Sec-
tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is
applied in both HCP and HiFST.
In HiFST any lattice in the CYK grid is pruned if it covers at least three source words
and contains more than 10,000 states. The log-likelihood pruning threshold relative to
the best path in the sublattices is 9.0.
Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,
and mt08. The first two rows show results for HCP when using MERT parameters
optimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the latter
case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When
measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning
523
Computational Linguistics Volume 36, Number 3
Table 3
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder
generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of
newswire and 666 sentences of Web text.
decoder MERT k-best tune-nw test-nw mt08
a HCP HCP 32.8 33.1 ?
b HCP 32.9 33.4 28.2
+5g HiFST 33.4 33.8 28.7
+5g+MBR 33.6 34.0 28.9
c HiFST 33.1 33.4 28.1
+5g HiFST 33.8 34.3 29.0
+5g+LMBR 34.5 34.9 30.2
over the HiFST hypotheses and we conclude that using the k-best list obtained by the
HiFST decoder yields better parameters in optimization.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis
with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis
with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent
for this language pair. This is due to the larger search space required for full hierarchical
translation; the larger the search space, the more likely it is that search errors will be
introduced by the cube pruning algorithm.
Lattice/k-best Quality Large LMs and MBR both benefit from the richer space of
translation hypotheses contained in the lattices. Relative to the first-pass baseline in
MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k-best lists.
4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU
(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams in
the lattice; the low posterior n-grams have poor discriminatory power. In the following
experiment, we show that high posterior n-grams are more likely to be found in the
references, and that using the full evidence space of the lattice is much better than even
very large k-best lists for computing posterior probabilities. Let Ni = {w1, . . .,w|Ni|}
denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri =
{w1, . . .,w|Ri|} denote the set of n-grams of order i in the union of the references.
For confidence threshold ?, let Ni,? = {w ? Ni : p(w|L) ? ?} denote the set of all
n-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) is
the posterior probability of the n-gram w, that is, the sum of the posterior probabilities
of all translations containing at least one occurrence of w. The precision at order i for
threshold ? is the proportion of n-grams in Ni,? found in the references:
pi,? =
|Ri ?Ni,?|
|Ni,?|
. (7)
The average per-sentence 4-gram precision at a range of posterior probability thresholds
? is shown in Figure 12. The posterior probabilities are computed using either the full
524
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 12
4-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computed
using the full evidence space of the lattice and k-best lists of various sizes.
lattice L or a k-best list of the specified size. The 4-gram precision of the 1-best trans-
lations is approximately 0.35. At higher values of ?, the reference precision increases
considerably. Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightly
improves the precision but much higher precisions are observed when the full evidence
space of the lattice is used. The improved precision results from more accurate estimates
of n-gram posterior probabilites and emphasizes once more the advantage of lattice-
based decoding and rescoring techniques.
4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-
mar depth and the constraints on low-level rule concatenation (see Section 3). Unless
otherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S? nonterminal
category, where these constraints are not relevant).
4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-
lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger
5-gram language model for the most promising configurations (+5g rows). Decoding
time is reported for first-pass decoding only; rescoring time is negligible by comparison.
As shown in the upper part of Table 4, translation under a shallow-2 grammar does
not improve relative to a shallow-1 grammar, although decoding is much slower. This
indicates that the additional hypotheses generated when allowing a hierarchical depth
of two are not useful in Arabic-to-English translation. By contrast the shallow gram-
mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo and
shallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similar
decoding time. Performance differences increase when the larger 5-gram is applied
525
Computational Linguistics Volume 36, Number 3
Table 4
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various
grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.
grammar time mt02-05-tune mt02-05-test mt08
HiFST shallow-1 0.8 52.7 52.0 42.9
+K1,K2 = 1, 3 1.3 52.6 51.9 42.8
+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9
shallow-2 4.2 52.7 51.9 42.6
+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0
+5g shallow-1 - 53.9 53.4 44.9
+K1,K2 = 1, 3, vo - 54.1 53.6 45.0
shallow-2
+K1,K2 = 2, 3, vo
- 54.2 53.8 45.0
(Table 4, bottom). This is expected given that these grammars add valid translation
candidates to the search space with similar costs; a language model is needed to select
the good hypotheses among all those introduced.
4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-
mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improves
as the hierarchical depth of the grammar is increased (i.e., for larger n). Decoding time
also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades
by approximately 1.0 BLEU relative to the full hierarchical grammar.
However, we find that translation under the shallow-3 grammar yields performance
nearly as good as that of the full hiero grammars; translation times are shorter and
yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster
by constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-
tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation
of 0.2 to 0.3 BLEU relative to full Hiero.
Shallow-3 grammars describe a restricted search-space but appear to have expressive
power in Chinese-to-English translation which is very similar to that of a full Hiero
grammar. Each cell (x, y) is represented by a bigger set of nonterminals; this allows for
more effective pruning strategies during lattice construction. We note also that hmax
values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram
rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a
0.4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.4
3 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained with
a slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBM
BLEU; here we use the 2009 implementation by NIST.
4 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
526
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 5
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various
grammar configurations and search parameters. Decoding time is reported in sec/word for
tune-nw.
grammar time tune-nw test-nw mt08 (nw)
HiFST shallow-1 0.7 33.6 33.4 32.6
shallow-2 5.9 33.8 34.2 32.7
+hmin=5 5.6 33.8 34.1 32.9
+hmin=7 4.0 33.8 34.3 33.0
shallow-3 8.8 34.0 34.3 33.0
+hmin=7 7.7 34.0 34.4 33.1
+hmin=9 5.9 33.9 34.3 33.1
+hmin=9,5,2 3.8 34.0 34.3 33.0
+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0
+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1
full hiero 10.8 34.0 34.4 33.3
+5g shallow-1 - 34.1 34.5 33.4
shallow-2 - 34.3 35.1 34.0
shallow-3 - 34.6 35.2 34.4
+hmin=9,5,2 - 34.5 34.8 34.2
full hiero - 34.5 35.2 34.6
4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-
chine translation allows for multiple derivations of a target language sentence. Each
derivation corresponds to a particular combination of hierarchical rules and it has been
argued that the correct approach to translation is to accumulate translation probability
by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-
coding is computationally difficult, however. For this reason the translation probability
is commonly computed using the Viterbi max-derivation approximation. This is the
approach taken in the previous sections in which translations scores were accumulated
under the tropical semiring.
The use of WFSTs allows the sum over alternative derivations of a target string
to be computed efficiently. HiFST generates a translation lattice realized as a weighted
transducer with output labels encoding words and input labels encoding the sequence
of rules corresponding to a particular derivation, and the cost of each path in the lattice
is the negative log probability of the derivation that generated the hypothesis.
Determinization applies the ? operator to all paths with the same word se-
quence (Mohri 1997). When applied in the log semiring, this operator computes the
sum of two paths with the same word sequence as x ? y = ?log(e?x + e?y) so that the
probabilities of alternative derivations can be summed.
Currently this operation is only performed in the top cell of the hierarchical decoder
so it is still an approximation to the true translation probability. Computing the true
translation probability would require the same operation to be repeated in every cell
during decoding, which is very time consuming. Note that the translation lattice was
generated with a language model and so the language model costs must be removed
527
Computational Linguistics Volume 36, Number 3
Table 6
Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the
upper-most CYK cell with alternative semirings.
semiring mt02-05-tune mt02-05-test mt08
tropical HiFST 52.8 52.2 43.0
+5g 54.2 53.8 44.9
+5g+LMBR 55.0 54.6 45.5
log HiFST 53.1 52.6 43.2
+5g 54.6 54.2 45.2
+5g+LMBR 55.0 54.6 45.5
before determinization to ensure that only the derivation probabilities are included
in the sum. After determinization, the language model is reapplied and the 1-best
translation hypothesis can be extracted from the logarc determinized lattices.
Table 6 compares translation results obtained using the tropical semiring (Viterbi
likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows
small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2
for mt08. These gains show that the sum over alternative derivations can be easily
obtained in HiFST simply by changing semiring and that these alternative derivations
are beneficial to translation. The gains carry through to the large language model 5-gram
rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses
selected by LMBR are in almost all cases exactly the same regardless of the choice of
semiring. This may be due to the fact that our current marginalization procedure is only
an approximation to the true marginal likelihoods, since the log semiring determiniza-
tion operation is applied only in the uppermost cell of the CYK grid and MERT training
is performed using regular Viterbi likelihoods.
We note that a close study of the interaction between LMBR and marginalization
over derivations is beyond the scope of this paper. Our purpose here is to show how
easily these operations can be done using WFSTs.
4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation
hypotheses obtained from alternative morphological decompositions of the same source
data. In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-
to-English when taking k-best lists obtained from two morphological decompositions
of the source language. Here we extend this approach to the case of translation lat-
tices and experiment with more than two alternative decompositions. We will show
that working with translation lattices gives significant improvements relative to k-best
lists.
In lattice-based MBR system combination, first-pass decoding results in a set of I
distinct translation lattices L(i), i = 1. . .I for each foreign sentence, with each lattice
produced by translating one of the alternative morphological decompositions. The
evidence space for MBR decoding is formed as the union of these lattices L =
?I
i=1 L(i).
The posterior probability of n-gram w in the union of lattices is computed as a simple
528
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 7
Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic
decompositions, and their combination with k-best-based and lattice-based MBR.
configuration mt02-05-tune mt02-05-test mt08
a HiFST+5g 54.2 53.8 44.9
b HiFST+5g 53.8 53.6 45.0
c HiFST+5g 54.1 53.8 44.7
a+b +MBR 55.1 54.7 46.1
+LMBR 55.7 55.4 46.7
a+c +MBR 55.4 54.9 46.5
+LMBR 56.0 55.9 46.9
a+b+c +MBR 55.3 54.9 46.5
+LMBR 56.0 55.7 47.3
linear interpolation of the posterior probabilities according to the evidence space of each
individual lattice so that
p(w|L) =
I
?
i=1
?i pi(w|L(i) ), (8)
where the interpolation parameters 0 ? ?i ? 1 such that
?I
i=1 ?i = 1 specify the weight
associated with each system in the combination and are optimized with respect to
the tuning set. The system-specific posteriors required for the interpolation are com-
puted as
pi(w|L(i) ) =
?
E?L(i)w
Pi(E|F), (9)
where Pi(E|F) is the posterior probability of translation E given source sentence F and
the sum is taken over the subset L(i)w = {E ? L(i) : #w(E) > 0} of the lattice L(i) containing
paths with at least one occurrence of the n-gram w. These posterior probabilities are
used in MBR decoding under the linear approximation to the BLEU score described
in Tromble et al (2008). We find that for system combination, decoding often produces
output that is slightly shorter than required. A fixed per-word factor optimized on the
tuning set is applied when computing the gain and this results in output with improved
BLEU score and reduced brevity penalty.
Table 7 shows translation results in Arabic-to-English using three alternative mor-
phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-
position an independent set of hierarchical rules is obtained from the respective parallel
corpus alignments. The decompositions were generated by the MADA toolkit (Habash
and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic
Morphological Tagger, developed by Sakhr Software in Egypt.
The following rows show the results when combining with MBR the translation
hypotheses obtained from two or three decompositions. The table also shows a contrast
529
Computational Linguistics Volume 36, Number 3
Figure 13
Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune
single-system MBR 1-best translations and the 1-best obtained through MBR system
combination.
between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-
ing the unioned translation lattices (rows named LMBR). In line with the findings of
de Gispert et al (2009), we find significant gains from combining k-best lists with respect
to using any one segmentation alone. Interestingly, here we find further gains when
applying lattice-based MBR instead of a k-best approach, obtaining consistent gains of
0.6?0.8 BLEU across all sets.
The results reported in Table 7 are very competitive. The mixed-case NIST BLEU-4
score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,
the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track.6
4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-
coding of multiple lattices generated from alternative morphological segmentations
leads to significant improvements in BLEU score. We now show that one reason for
the improved performance is that lattice combination leads to better n-gram posterior
probability estimates. To combine two equally weighted lattices L(1) and L(2), the in-
terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+
p2(w|L(2))). Figure 13 plots average per-sentence reference precisions for the 4-grams in
the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of
posterior probability thresholds 0 ? ? ? 1. Systems a and b have similar precisions at
all values of ?, confirming that the optimal interpolation weights for this combination
should be equal. The precision obtained using n-gram posterior probabilities computed
5 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease
530
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
from the combined lattices is higher than that of the individual systems. A higher
proportion of the n-grams assigned high posterior probability under the interpolated
distribution are found in the references and this is one of the reasons for the large gains
in BLEU in lattice-based MBR system combination.
4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for
other language pairs, such as Spanish-to-English and Finnish-to-English.
For Spanish-to-English we carried out experiments on the shared task of the ACL
2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based on
the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of
34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not
found to improve scores for this task.
In Finnish-to-English, we conducted translation experiments based on the Europarl
corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-
erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas
the full hierarchical grammar only achieved 27.6. This is further evidence that full-
hierarchical grammars are not appropriate in all instances. In this case we suspect that
the use of Finnish words without morphological decomposition leads to data sparsity
problems and complicates the task of learning complex translation rules. The lack of
a large English language model suitable for this domain may also make it harder to
select the right hypothesis when the translation grammar produces many more English
alternatives.
5. Conclusions
We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k-best lists to
represent the space of translation hypotheses. We describe a lattice-based Hiero de-
coder, with which we find reductions in search errors, better parameter optimization,
and improved translation performance. Relative to these reductions in search errors,
direct generation of target language translation lattices also leads to further translation
improvements through subsequent rescoring steps, such as MBR decoding and the
application of large n-gram language models. These steps can be carried out easily via
standard WFST operations.
As part of the machinery needed for our experiments we develop WFST procedures
for alignment and feature extraction so that statistics needed for system optimization
can be easily obtained and represented as transducers. In particular, we make use of
a lattice-based representation of sequences of rule applications, which proves useful
for minimum error rate training. In all instances we find that using lattices as compact
representations of translation hypotheses offers clear modeling advantages.
We also investigate refinements in translation search space through shallow-n gram-
mars, structured long-distance movement, and constrained word deletion. We find
that these techniques can be used to fit the complexity of Hiero translation systems to
individual language pairs. In translation from Arabic into English, shallow grammars
make it possible to explore the entire search space and to do so more quickly but with the
same translation quality as the full Hiero grammar. Even in complex translation tasks,
such as Chinese to English, we find significant speed improvements with minimal loss
531
Computational Linguistics Volume 36, Number 3
in performance using these methods. We take the view that it is better to perform exact
search of a constrained space than to risk search errors in translation.
We note finally that Chiang introduced Hiero as a model ?based on a synchronous
CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns
1968).? We have taken this formulation as a starting point for the development of
novel realizations of Hiero. Our motivation has mainly been practical in that we seek
improved translation quality and efficiency through better models and algorithms.
Our approach suggests close links between Hiero and Recursive Transition Net-
works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this
paper, we do note that Hiero translation requires keeping track of two grammars, one
based on the Hiero translation rules and the other based on n-gram language model
probabilities. These two grammars have very different dependencies which suggests
that a full implementation of Hiero translation such as we have addressed does not
have a simple expression as an RTN.
Acknowledgments
This work was supported in part by the
GALE program of the Defense Advanced
Research Projects Agency, Contract No.
HR0011- 06-C-0022, and in part by the
Spanish government and the ERDF under
projects TEC2006-13694-C03-03 and
TEC2009-14094-C04-04.
References
Allauzen, Cyril, Mehryar Mohri, and Brian
Roark. 2003. Generalized algorithms for
constructing statistical language models.
In Proceedings of ACL, pages 557?564,
Sapporo.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23, Prague.
Bender, Oliver, Evgeny Matusov, Stefan
Hahn, Sasa Hasan, Shahram Khadivi,
and Hermann Ney. 2007. The RWTH
Arabic-to-English spoken language
translation system. In Proceedings of
ASRU, pages 396?401, Kyoto.
Blunsom, Phil, Trevor Cohn, and Miles
Osborne. 2008. A discriminative latent
variable model for statistical machine
translation. In Proceedings of ACL-HLT,
pages 200?208, Columbus, OH.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2008. Further meta-evaluation
of machine translation. In Proceedings
of the ACL Workshop on Statistical
Machine Translation, pages 70?106,
Columbus, OH.
Chappelier, Jean-Ce?dric and Martin Rajman.
1998. A generalized CYK algorithm for
parsing stochastic CFG. In Proceedings of
TAPD, pages 133?137, Paris.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of ACL,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum Bayes-Risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of HLT/NAACL, Companion
Volume: Short Papers, pages 73?76,
Boulder, CO.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Graehl, Jonathan, Kevin Knight, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging and morphological
disambiguation in one fell swoop. In
Proceedings of the ACL, pages 573?580,
Ann Arbor, MI.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Rule filtering by pattern for
532
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
efficient hierarchical translation. In
Proceedings of the EACL, pages 380?388,
Athens.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009b.
Hierarchical phrase-based translation with
weighted finite state transducers. In
Proceedings of HLT/NAACL, pages 433?441,
Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009c. The
HiFST system for the Europarl
Spanish-to-English task. In Proceedings of
SEPLN, pages 207?214, Donosti.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Third Conference of the
AMTA on Machine Translation and the
Information Soup, pages 421?437,
Langhorne, PA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton.
Kumar, Shankar and William Byrne. 2004.
Minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lewis, P. M., II, and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM, 15(3):465?488.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23:269?311.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231:17?32.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2002. Weighted finite-state
transducers in speech recognition.
Computer Speech and Language, 16:69?88.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the ACL,
pages 295?302, Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of ACL,
pages 311?318, Toulouse.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing
Xiang, Spyros Matsoukas, Richard
Schwartz, and Bonnie Dorr. 2007.
Combining outputs from multiple
machine translation systems. In
Proceedings of HLT-NAACL, pages 228?235,
Rochester, NY.
Setiawan, Hendra, Min Yen Kan, Haizhou Li,
and Philip Resnik. 2009. Topological
ordering of function words in hierarchical
phrase-based translation. In Proceedings
of the ACL-IJCNLP, pages 324?332,
Singapore.
Sim, Khe Chai, William Byrne, Mark Gales,
Hichem Sahbi, and Phil Woodland. 2007.
Consensus network decoding for statistical
machine translation system combination.
In Proceedings of ICASSP, pages 105?108,
Honolulu, HI.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
Minimum Bayes-Risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Honolulu, HI.
Varile, Giovanni B. and Peter Lau. 1988.
Eurotra practical experience with a
multilingual machine translation system
under development. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 160?167,
Austin, TX.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591?606.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
533

Proceedings of the ACL 2010 Conference Short Papers, pages 27?32,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood, Adria` de Gispert, William Byrne
Machine Intelligence Laboratory
Cambridge University Engineering Department
Trumpington Street, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
This paper presents an efficient imple-
mentation of linearised lattice minimum
Bayes-risk decoding using weighted finite
state transducers. We introduce transduc-
ers to efficiently count lattice paths con-
taining n-grams and use these to gather
the required statistics. We show that these
procedures can be implemented exactly
through simple transformations of word
sequences to sequences of n-grams. This
yields a novel implementation of lattice
minimum Bayes-risk decoding which is
fast and exact even for very large lattices.
1 Introduction
This paper focuses on an exact implementation
of the linearised form of lattice minimum Bayes-
risk (LMBR) decoding using general purpose
weighted finite state transducer (WFST) opera-
tions1. The LMBR decision rule in Tromble et al
(2008) has the form
E? = argmax
E??E
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
(1)
where E is a lattice of translation hypotheses, N
is the set of all n-grams in the lattice (typically,
n = 1 . . . 4), and the parameters ? are constants
estimated on held-out data. The quantity p(u|E)
we refer to as the path posterior probability of the
n-gram u. This particular posterior is defined as
p(u|E) = p(Eu|E) =
?
E?Eu
P (E|F ), (2)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of lattice paths containing the n-gram u at least
1We omit an introduction to WFSTs for space reasons.
See Mohri et al (2008) for details of the general purpose
WFST operations used in this paper.
once. It is the efficient computation of these path
posterior n-gram probabilities that is the primary
focus of this paper. We will show how general
purpose WFST algorithms can be employed to ef-
ficiently compute p(u|E) for all u ? N .
Tromble et al (2008) use Equation (1) as an
approximation to the general form of statistical
machine translation MBR decoder (Kumar and
Byrne, 2004):
E? = argmin
E??E
?
E?E
L(E,E?)P (E|F ) (3)
The approximation replaces the sum over all paths
in the lattice by a sum over lattice n-grams. Even
though a lattice may have many n-grams, it is
possible to extract and enumerate them exactly
whereas this is often impossible for individual
paths. Therefore, while the Tromble et al (2008)
linearisation of the gain function in the decision
rule is an approximation, Equation (1) can be com-
puted exactly even over very large lattices. The
challenge is to do so efficiently.
If the quantity p(u|E) had the form of a condi-
tional expected count
c(u|E) =
?
E?E
#u(E)P (E|F ), (4)
it could be computed efficiently using counting
transducers (Allauzen et al, 2003). The statis-
tic c(u|E) counts the number of times an n-gram
occurs on each path, accumulating the weighted
count over all paths. By contrast, what is needed
by the approximation in Equation (1) is to iden-
tify all paths containing an n-gram and accumulate
their probabilities. The accumulation of probabil-
ities at the path level, rather than the n-gram level,
makes the exact computation of p(u|E) hard.
Tromble et al (2008) approach this problem by
building a separate word sequence acceptor for
each n-gram in N and intersecting this acceptor
27
with the lattice to discard all paths that do not con-
tain the n-gram; they then sum the probabilities of
all paths in the filtered lattice. We refer to this as
the sequential method, since p(u|E) is calculated
separately for each u in sequence.
Allauzen et al (2010) introduce a transducer
for simultaneous calculation of p(u|E) for all un-
igrams u ? N1 in a lattice. This transducer is
effective for finding path posterior probabilities of
unigrams because there are relatively few unique
unigrams in the lattice. As we will show, however,
it is less efficient for higher-order n-grams.
Allauzen et al (2010) use exact statistics for
the unigram path posterior probabilities in Equa-
tion (1), but use the conditional expected counts
of Equation (4) for higher-order n-grams. Their
hybrid MBR decoder has the form
E? = argmax
E??E
{
?0|E?|
+
?
u?N :1?|u|?k
?u#u(E?)p(u|E)
+
?
u?N :k<|u|?4
?u#u(E?)c(u|E)
}
, (5)
where k determines the range of n-gram orders
at which the path posterior probabilities p(u|E)
of Equation (2) and conditional expected counts
c(u|E) of Equation (4) are used to compute the
expected gain. For k < 4, Equation (5) is thus
an approximation to the approximation. In many
cases it will be perfectly fine, depending on how
closely p(u|E) and c(u|E) agree for higher-order
n-grams. Experimentally, Allauzen et al (2010)
find this approximation works well at k = 1 for
MBR decoding of statistical machine translation
lattices. However, there may be scenarios in which
p(u|E) and c(u|E) differ so that Equation (5) is no
longer useful in place of the original Tromble et
al. (2008) approximation.
In the following sections, we present an efficient
method for simultaneous calculation of p(u|E) for
n-grams of a fixed order. While other fast MBR
approximations are possible (Kumar et al, 2009),
we show how the exact path posterior probabilities
can be calculated and applied in the implementa-
tion of Equation (1) for efficient MBR decoding
over lattices.
2 N-gram Mapping Transducer
We make use of a trick to count higher-order n-
grams. We build transducer ?n to map word se-
quences to n-gram sequences of order n. ?n has a
similar form to the WFST implementation of an n-
gram language model (Allauzen et al, 2003). ?n
includes for each n-gram u = wn1 arcs of the form:
wn-11 wn2wn:u
The n-gram lattice of order n is called En and is
found by composing E ??n, projecting on the out-
put, removing ?-arcs, determinizing, and minimis-
ing. The construction of En is fast even for large
lattices and is memory efficient. En itself may
have more states than E due to the association of
distinct n-gram histories with states. However, the
counting transducer for unigrams is simpler than
the corresponding counting transducer for higher-
order n-grams. As a result, counting unigrams in
En is easier than counting n-grams in E .
3 Efficient Path Counting
Associated with each En we have a transducer ?n
which can be used to calculate the path posterior
probabilities p(u|E) for all u ? Nn. In Figures
1 and 2 we give two possible forms2 of ?n that
can be used to compute path posterior probabilities
over n-grams u1,2 ? Nn for some n. No modifica-
tion to the ?-arc matching mechanism is required
even in counting higher-order n-grams since all n-
grams are represented as individual symbols after
application of the mapping transducer ?n.
Transducer ?Ln is used by Allauzen et al (2010)
to compute the exact unigram contribution to the
conditional expected gain in Equation (5). For ex-
ample, in counting paths that contain u1, ?Ln re-
tains the first occurrence of u1 and maps every
other symbol to ?. This ensures that in any path
containing a given u, only the first u is counted,
avoiding multiple counting of paths.
We introduce an alternative path counting trans-
ducer ?Rn that effectively deletes all symbols ex-
cept the last occurrence of u on any path by en-
suring that any paths in composition which count
earlier instances of u do not end in a final state.
Multiple counting is avoided by counting only the
last occurrence of each symbol u on a path.
We note that initial ?:? arcs in ?Ln effectively
create |Nn| copies of En in composition while
searching for the first occurrence of each u. Com-
2The special composition symbol ? matches any arc; ?
matches any arc other than those with an explicit transition.
See the OpenFst documentation: http://openfst.org
28
01
2
3
u1:u1
u2:u2?:?
?:?
?:?
?:?
?:?
Figure 1: Path counting transducer ?Ln matching
first (left-most) occurrence of each u ? Nn.
0
1
3
2
4
u1:u1
u2:u2
u1:?
u2:?
?:?
?:?
?:?
Figure 2: Path counting transducer ?Rn matching
last (right-most) occurrence of each u ? Nn.
posing with ?Rn creates a single copy of En while
searching for the last occurrence of u; we find this
to be much more efficient for large Nn.
Path posterior probabilities are calculated over
each En by composing with ?n in the log semir-
ing, projecting on the output, removing ?-arcs, de-
terminizing, minimising, and pushing weights to
the initial state (Allauzen et al, 2010). Using ei-
ther ?Ln or ?Rn , the resulting counts acceptor is Xn.
It has a compact form with one arc from the start
state for each ui ? Nn:
0 iui/- log p(ui|E)
3.1 Efficient Path Posterior Calculation
Although Xn has a convenient and elegant form,
it can be difficult to build for large Nn because
the composition En ? ?n results in millions of
states and arcs. The log semiring ?-removal and
determinization required to sum the probabilities
of paths labelled with each u can be slow.
However, if we use the proposed ?Rn , then each
path in En ? ?Rn has only one non-? output la-
bel u and all paths leading to a given final state
share the same u. A modified forward algorithm
can be used to calculate p(u|E) without the costly
?-removal and determinization. The modification
simply requires keeping track of which symbol
u is encountered along each path to a final state.
More than one final state may gather probabilities
for the same u; to compute p(u|E) these proba-
bilities are added. The forward algorithm requires
that En??Rn be topologically sorted; although sort-
ing can be slow, it is still quicker than log semiring
?-removal and determinization.
The statistics gathered by the forward algo-
rithm could also be gathered under the expectation
semiring (Eisner, 2002) with suitably defined fea-
tures. We take the view that the full complexity of
that approach is not needed here, since only one
symbol is introduced per path and per exit state.
Unlike En ??Rn , the composition En ??Ln does
not segregate paths by u such that there is a di-
rect association between final states and symbols.
The forward algorithm does not readily yield the
per-symbol probabilities, although an arc weight
vector indexed by symbols could be used to cor-
rectly aggregate the required statistics (Riley et al,
2009). For large Nn this would be memory in-
tensive. The association between final states and
symbols could also be found by label pushing, but
we find this slow for large En ??n.
4 Efficient Decoder Implementation
In contrast to Equation (5), we use the exact values
of p(u|E) for all u ? Nn at orders n = 1 . . . 4 to
compute
E? = argmin
E??E
{
?0|E?|+
4
?
n=1
gn(E,E?)
}
, (6)
where gn(E,E?) =
?
u?Nn ?u#u(E
?)p(u|E) us-
ing the exact path posterior probabilities at each
order. We make acceptors ?n such that E ? ?n
assigns order n partial gain gn(E,E?) to all paths
E ? E . ?n is derived from ?n directly by assign-
ing arc weight ?u?p(u|E) to arcs with output label
u and then projecting on the input labels. For each
n-gram u = wn1 in Nn arcs of ?n have the form:
wn-11 wn2wn/?u ? p(u|E)
To apply ?0 we make a copy of E , called E0,
with fixed weight ?0 on all arcs. The decoder is
formed as the composition E0 ??1 ??2 ??3 ??4
and E? is extracted as the maximum cost string.
5 Lattice Generation for LMBR
Lattice MBR decoding performance and effi-
ciency is evaluated in the context of the NIST
29
mt0205tune mt0205test mt08nw mt08ng
ML 54.2 53.8 51.4 36.3
k
0 52.6 52.3 49.8 34.5
1 54.8 54.4 52.2 36.6
2 54.9 54.5 52.4 36.8
3 54.9 54.5 52.4 36.8
LMBR 55.0 54.6 52.4 36.8
Table 1: BLEU scores for Arabic?English maximum likelihood translation (ML), MBR decoding using
the hybrid decision rule of Equation (5) at 0 ? k ? 3, and regular linearised lattice MBR (LMBR).
mt0205tune mt0205test mt08nw mt08ng
Posteriors
sequential 3160 3306 2090 3791
?Ln 6880 7387 4201 8796
?Rn 1746 1789 1182 2787
Decoding sequential 4340 4530 2225 4104
?n 284 319 118 197
Total
sequential 7711 8065 4437 8085
?Ln 7458 8075 4495 9199
?Rn 2321 2348 1468 3149
Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decoding
using sequential method and left-most (?Ln) or right-most (?Rn ) counting transducer implementations.
Arabic?English machine translation task3. The
development set mt0205tune is formed from the
odd numbered sentences of the NIST MT02?
MT05 testsets; the even numbered sentences form
the validation set mt0205test. Performance on
NIST MT08 newswire (mt08nw) and newsgroup
(mt08ng) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. Word alignments are generated using
MTTK (Deng and Byrne, 2008) over 150M words
of parallel text for the constrained NIST MT08
Arabic?English track. In decoding, a Shallow-
1 grammar with a single level of rule nesting is
used and no pruning is performed in generating
first-pass lattices (Iglesias et al, 2009).
The first-pass language model is a modified
Kneser-Ney (Kneser and Ney, 1995) 4-gram esti-
mated over the English parallel text and an 881M
word subset of the GigaWord Third Edition (Graff
et al, 2007). Prior to LMBR, the lattices are
rescored with large stupid-backoff 5-gram lan-
guage models (Brants et al, 2007) estimated over
more than 6 billion words of English text.
The n-gram factors ?0, . . . , ?4 are set according
to Tromble et al (2008) using unigram precision
3http://www.itl.nist.gov/iad/mig/tests/mt
p = 0.85 and average recall ratio r = 0.74. Our
translation decoder and MBR procedures are im-
plemented using OpenFst (Allauzen et al, 2007).
6 LMBR Speed and Performance
Lattice MBR decoding performance is shown in
Table 1. Compared to the maximum likelihood
translation hypotheses (row ML), LMBR gives
gains of +0.8 to +1.0 BLEU for newswire data and
+0.5 BLEU for newsgroup data (row LMBR).
The other rows of Table 1 show the performance
of LMBR decoding using the hybrid decision rule
of Equation (5) for 0 ? k ? 3. When the condi-
tional expected counts c(u|E) are used at all orders
(i.e. k = 0), the hybrid decoder BLEU scores are
considerably lower than even the ML scores. This
poor performance is because there are many un-
igrams u for which c(u|E) is much greater than
p(u|E). The consensus translation maximising the
conditional expected gain is then dominated by
unigram matches, significantly degrading LMBR
decoding performance. Table 1 shows that for
these lattices the hybrid decision rule is an ac-
curate approximation to Equation (1) only when
k ? 2 and the exact contribution to the gain func-
tion is computed using the path posterior probabil-
ities at orders n = 1 and n = 2.
30
We now analyse the efficiency of lattice MBR
decoding using the exact path posterior probabil-
ities of Equation (2) at all orders. We note that
the sequential method and both simultaneous im-
plementations using path counting transducers ?Ln
and ?Rn yield the same hypotheses (allowing for
numerical accuracy); they differ only in speed and
memory usage.
Posteriors Efficiency Computation times for
the steps in LMBR are given in Table 2. In calcu-
lating path posterior n-gram probabilities p(u|E),
we find that the use of ?Ln is more than twice
as slow as the sequential method. This is due to
the difficulty of counting higher-order n-grams in
large lattices. ?Ln is effective for counting uni-
grams, however, since there are far fewer of them.
Using ?Rn is almost twice as fast as the sequential
method. This speed difference is due to the sim-
ple forward algorithm. We also observe that for
higher-order n, the composition En ? ?Rn requires
less memory and produces a smaller machine than
En ? ?Ln . It is easier to count paths by the final
occurrence of a symbol than by the first.
Decoding Efficiency Decoding times are signif-
icantly faster using ?n than the sequential method;
average decoding time is around 0.1 seconds per
sentence. The total time required for lattice MBR
is dominated by the calculation of the path pos-
terior n-gram probabilities, and this is a func-
tion of the number of n-grams in the lattice |N |.
For each sentence in mt0205tune, Figure 3 plots
the total LMBR time for the sequential method
(marked ?o?) and for probabilities computed using
?Rn (marked ?+?). This compares the two tech-
niques on a sentence-by-sentence basis. As |N |
grows, the simultaneous path counting transducer
is found to be much more efficient.
7 Conclusion
We have described an efficient and exact imple-
mentation of the linear approximation to LMBR
using general WFST operations. A simple trans-
ducer was used to map words to sequences of n-
grams in order to simplify the extraction of higher-
order statistics. We presented a counting trans-
ducer ?Rn that extracts the statistics required for
all n-grams of order n in a single composition and
allows path posterior probabilities to be computed
efficiently using a modified forward procedure.
We take the view that even approximate search
0 1000 2000 3000 4000 5000 6000
0
10
20
30
40
50
60
70
 
 
sequential
simultaneous ?Rn
to
ta
lt
im
e
(se
co
n
ds
)
lattice n-grams
Figure 3: Total time in seconds versus |N |.
criteria should be implemented exactly where pos-
sible, so that it is clear exactly what the system is
doing. For machine translation lattices, conflat-
ing the values of p(u|E) and c(u|E) for higher-
order n-grams might not be a serious problem, but
in other scenarios ? especially where symbol se-
quences are repeated multiple times on the same
path ? it may be a poor approximation.
We note that since much of the time in calcula-
tion is spent dealing with ?-arcs that are ultimately
removed, an optimised composition algorithm that
skips over such redundant structure may lead to
further improvements in time efficiency.
Acknowledgments
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Cyril Allauzen, Shankar Kumar, Wolfgang Macherey,
Mehryar Mohri, and Michael Riley. 2010. Expected
31
sequence similarity maximization. In Human Lan-
guage Technologies 2010: The 11th Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Los Angeles,
California, June.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 858?867.
Yonggang Deng and William Byrne. 2008. HMM
word and phrase alignment for statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing, 16(3):494?507.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 1?8, Philadel-
phia, July.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state trans-
ducers. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 433?441, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech,
and Signal Processing, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2004 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 169?176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?
171, Suntec, Singapore, August. Association for
Computational Linguistics.
M. Mohri, F.C.N. Pereira, and M. Riley. 2008. Speech
recognition with weighted finite-state transducers.
Handbook on Speech Processing and Speech Com-
munication.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFst: An Open-Source, Weighted Finite-
State Transducer Library and its Applications to
Speech and Language. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
32
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
