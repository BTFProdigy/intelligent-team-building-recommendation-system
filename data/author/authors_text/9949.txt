Proceedings of ACL-08: HLT, pages 683?691,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Contextual Preferences
Idan Szpektor, Ido Dagan, Roy Bar-Haim
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
{szpekti,dagan,barhair}@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat Gan, Israel
goldbej@eng.biu.ac.il
Abstract
The validity of semantic inferences depends
on the contexts in which they are applied.
We propose a generic framework for handling
contextual considerations within applied in-
ference, termed Contextual Preferences. This
framework defines the various context-aware
components needed for inference and their
relationships. Contextual preferences extend
and generalize previous notions, such as se-
lectional preferences, while experiments show
that the extended framework allows improving
inference quality on real application data.
1 Introduction
Applied semantic inference is typically concerned
with inferring a target meaning from a given text.
For example, to answer ?Who wrote Idomeneo??,
Question Answering (QA) systems need to infer the
target meaning ?Mozart wrote Idomeneo? from a
given text ?Mozart composed Idomeneo?. Following
common Textual Entailment terminology (Giampic-
colo et al, 2007), we denote the target meaning by h
(for hypothesis) and the given text by t.
A typical applied inference operation is matching.
Sometimes, h can be directly matched in t (in the
example above, if the given sentence would be liter-
ally ?Mozart wrote Idomeneo?). Generally, the tar-
get meaning can be expressed in t in many differ-
ent ways. Indirect matching is then needed, using
inference knowledge that may be captured through
rules, termed here entailment rules. In our exam-
ple, ?Mozart wrote Idomeneo? can be inferred using
the rule ?X compose Y ? X write Y ?. Recently,
several algorithms were proposed for automatically
learning entailment rules and paraphrases (viewed
as bi-directional entailment rules) (Lin and Pantel,
2001; Ravichandran and Hovy, 2002; Shinyama et
al., 2002; Szpektor et al, 2004; Sekine, 2005).
A common practice is to try matching the struc-
ture of h, or of the left-hand-side of a rule r, within
t. However, context should be considered to allow
valid matching. For example, suppose that to find
acquisitions of companies we specify the target tem-
plate hypothesis (a hypothesis with variables) ?X ac-
quire Y ?. This h should not be matched in ?children
acquire language quickly?, because in this context
Y is not a company. Similarly, the rule ?X charge
Y ? X accuse Y ? should not be applied to ?This
store charged my account?, since the assumed sense
of ?charge? in the rule is different than its sense in
the text. Thus, the intended contexts for h and r
and the context within the given t should be prop-
erly matched to verify valid inference.
Context matching at inference time was of-
ten approached in an application-specific manner
(Harabagiu et al, 2003; Patwardhan and Riloff,
2007). Recently, some generic methods were pro-
posed to handle context-sensitive inference (Dagan
et al, 2006; Pantel et al, 2007; Downey et al, 2007;
Connor and Roth, 2007), but these usually treat
only a single aspect of context matching (see Sec-
tion 6). We propose a comprehensive framework for
handling various contextual considerations, termed
Contextual Preferences. It extends and generalizes
previous work, defining the needed contextual com-
ponents and their relationships. We also present and
implement concrete representation models and un-
683
supervised matching methods for these components.
While our presentation focuses on semantic infer-
ence using lexical-syntactic structures, the proposed
framework and models seem suitable for other com-
mon types of representations as well.
We applied our models to a test set derived from
the ACE 2005 event detection task, a standard In-
formation Extraction (IE) benchmark. We show the
benefits of our extended framework for textual in-
ference and present component-wise analysis of the
results. To the best of our knowledge, these are also
the first unsupervised results for event argument ex-
traction in the ACE 2005 dataset.
2 Contextual Preferences
2.1 Notation
As mentioned above, we follow the generic Tex-
tual Entailment (TE) setting, testing whether a target
meaning hypothesis h can be inferred from a given
text t. We allow h to be either a text or a template,
a text fragment with variables. For example, ?The
stock rose 8%? entails an instantiation of the tem-
plate hypothesis ?X gain Y ?. Typically, h represents
an information need requested in some application,
such as a target predicate in IE.
In this paper, we focus on parse-based lexical-
syntactic representation of texts and hypotheses, and
on the basic inference operation of matching. Fol-
lowing common practice (de Salvo Braz et al, 2005;
Romano et al, 2006; Bar-Haim et al, 2007), h is
syntactically matched in t if it can be embedded in
t?s parse tree. For template hypotheses, the matching
induces a mapping between h?s variables and their
instantiation in t.
Matching h in t can be performed either directly
or indirectly using entailment rules. An entailment
rule r: ?LHS ? RHS? is a directional entailment
relation between two templates. h is matched in t us-
ing r if LHS is matched in t and h matches RHS.
In the example above, r: ?X rise Y ?X gain Y ? al-
lows us to entail ?X gain Y ?, with ?stock? and ?8%?
instantiating h?s variables. We denote vars(z) the
set of variables of z, where z is a template or a rule.
2.2 Motivation
When matching considers only the structure of hy-
potheses, texts and rules it may result in incorrect
inference due to contextual mismatches. For exam-
ple, an IE system may identify mentions of public
demonstrations using the hypothesis h: ?X demon-
strate?. However, h should not be matched in ?Engi-
neers demonstrated the new system?, due to a mis-
match between the intended sense of ?demonstrate?
in h and its sense in t. Similarly, when looking for
physical attack mentions using the hypothesis ?X at-
tack Y ?, we should not utilize the rule r: ?X accuse
Y ?X attack Y ?, due to a mismatch between a ver-
bal attack in r and an intended physical attack in h.
Finally, r: ?X produce Y ? X lay Y ? (applicable
when X refers to poultry and Y to eggs) should not
be matched in t: ?Bugatti produce the fastest cars?,
due to a mismatch between the meanings of ?pro-
duce? in r and t. Overall, such incorrect inferences
may be avoided by considering contextual informa-
tion for t, h and r during their matching process.
2.3 The Contextual Preferences Framework
We propose the Contextual Preferences (CP) frame-
work for addressing context at inference time. In this
framework, the representation of an object z, where
z may be a text, a template or an entailment rule, is
enriched with contextual information denoted cp(z).
This information helps constraining or disambiguat-
ing the meaning of z, and is used to validate proper
matching between pairs of objects.
We consider two components within cp(z): (a)
a representation for the global (?topical?) context
in which z typically occurs, denoted cpg(z); (b)
a representation for the preferences and constraints
(?hard? preferences) on the possible terms that can
instantiate variables within z, denoted cpv(z). For
example, cpv(?X produce Y ? X lay Y ?) may
specify that X?s instantiations should be similar to
?chicken? or ?duck?.
Contextual Preferences are used when entailment
is assessed between a text t and a hypothesis h, ei-
ther directly or by utilizing an entailment-rule r. On
top of structural matching, we now require that the
Contextual Preferences of the participants in the in-
ference will also match. When h is directly matched
in t, we require that each component in cp(h) will
be matched with its counterpart in cp(t). When r is
utilized, we additionally require that cp(r) will be
matched with both cp(t) and cp(h). Figure 1 sum-
marizes the matching relationships between the CP
684
Figure 1: The directional matching relationships between
a hypothesis (h), an entailment rule (r) and a text (t) in the
Contextual Preferences framework.
components of h, t and r.
Like Textual Entailment inference, Contextual
Preferences matching is directional. When matching
h with t we require that the global context prefer-
ences specified by cpg(h) would subsume those in-
duced by cpg(t), and that the instantiations of h?s
variables in t would adhere to the preferences in
cpv(h) (since t should entail h, but not necessarily
vice versa). For example, if the preferred global con-
text of a hypothesis is sports, it would match a text
that discusses the more specific topic of basketball.
To implement the CP framework, concrete models
are needed for each component, specifying its repre-
sentation, how it is constructed, and an appropriate
matching procedure. Section 3 describes the specific
CP models that were implemented in this paper.
The CP framework provides a generic view of
contextual modeling in applied semantic inference.
Mapping from a specific application to the generic
framework follows the mappings assumed in the
Textual Entailment paradigm. For example, in QA
the hypothesis to be proved corresponds to the affir-
mative template derived from the question (e.g. h:
?X invented the PC? for ?Who invented the PC??).
Thus, cpg(h) can be constructed with respect to
the question?s focus while cpv(h) may be gener-
ated from the expected answer type (Moldovan et
al., 2000; Harabagiu et al, 2003). Construction of
hypotheses? CP for IE is demonstrated in Section 4.
3 Contextual Preferences Models
This section presents the current models that we im-
plemented for the various components of the CP
framework. For each component type we describe
its representation, how it is constructed, and a cor-
responding unsupervised match score. Finally, the
different component scores are combined to yield
an overall match score, which is used in our exper-
iments to rank inference instances by the likelihood
of their validity. Our goal in this paper is to cover the
entire scope of the CP framework by including spe-
cific models that were proposed in previous work,
where available, and elsewhere propose initial mod-
els to complete the CP scope.
3.1 Contextual Preferences for Global Context
To represent the global context of an object z we
utilize Latent Semantic Analysis (LSA) (Deerwester
et al, 1990), a well-known method for representing
the contextual-usage of words based on corpus sta-
tistics. We use LSA analysis of the BNC corpus1,
in which every term is represented by a normalized
vector of the top 100 SVD dimensions, as described
in (Gliozzo, 2005).
To construct cpg(z) we first collect a set of terms
that are representative for the preferred general con-
text of z. Then, the (single) vector which is the sum
of the LSA vectors of the representative terms be-
comes the representation of cpg(z). This LSA vec-
tor captures the ?average? typical contexts in which
the representative terms occur.
The set of representative terms for a text t con-
sists of all the nouns and verbs in it, represented
by their lemma and part of speech. For a rule r:
?LHS ? RHS?, the representative terms are the
words appearing in LHS and in RHS. For exam-
ple, the representative terms for ?X divorce Y ? X
marry Y ? are {divorce:v, marry:v}. As mentioned
earlier, construction of hypotheses and their contex-
tual preferences depends on the application at hand.
In our experiments these are defined manually, as
described in Section 4, derived from the manual de-
finitions of target meanings in the IE data.
The score of matching the cpg components of two
objects, denoted by mg(?, ?), is the Cosine similarity
of their LSA vectors. Negative values are set to 0.
3.2 Contextual Preferences for Variables
3.2.1 Representation
For comparison with prior work, we follow (Pan-
tel et al, 2007) and represent preferences for vari-
1http://www.natcorp.ox.ac.uk/
685
able instantiations using a distributional approach,
and in addition incorporate a standard specification
of named-entity types. Thus, cpv is represented by
two lists. The first list, denoted cpv:e, contains ex-
amples for valid instantiations of that variable. For
example, cpv:e(X kill Y ? Y die of X) may be
[X: {snakebite, disease}, Y : {man, patient}]. The
second list, denoted cpv:n, contains the variable?s
preferred named-entity types (if any). For exam-
ple, cpv:n(X born in Y ) may be [X: {Person}, Y :
{Location}]. We denote cpv:e(z)[j] and cpv:n(z)[j]
as the lists for a specific variable j of the object z.
For a text t, in which a template p is matched, the
preference cpv:e(t) for each template variable is sim-
ply its instantiation in t. For example, when ?X eat
Y ? is matched in t: ?Many Americans eat fish reg-
ularly?, we construct cpv:e(t) = [X: {Many Ameri-
cans}, Y : {fish}]. Similarly, cpv:n(t) for each vari-
able is the named-entity type of its instantiation in
t (if it is a named entity). We identify entity types
using the default Lingpipe2 Named-Entity Recog-
nizer (NER), which recognizes the types Location,
Person and Organization. In the above example,
cpv:n(t)[X] would be {Person}.
For a rule r: LHS ? RHS, we automatically
add to cpv:e(r) all the variable instantiations that
were found common for both LHS and RHS in a
corpus (see Section 4), as in (Pantel et al, 2007; Pen-
nacchiotti et al, 2007). To construct cpv:n(r), we
currently use a simple approach where each individ-
ual term in cpv:e(r) is analyzed by the NER system,
and its type (if any) is added to cpv:n(r).
For a template hypothesis, we currently repre-
sent cpv(h) only by its list of preferred named-entity
types, cpv:n. Similarly to cpg(h), the preferred types
for each template variable were adapted from those
defined in our IE data (see Section 4).
To allow compatible comparisons with previous
work (see Sections 5 and 6), we utilize in this
paper only cpv:e when matching between cpv(r)
and cpv(t), as only this representation was exam-
ined in prior work on context-sensitive rule applica-
tions. cpv:n is utilized for context matches involving
cpv(h). We denote the score of matching two cpv
components by mv(?, ?).
2http://www.alias-i.com/lingpipe/
3.2.2 Matching cpv:e
Our primary matching method is based on repli-
cating the best-performing method reported in (Pan-
tel et al, 2007), which utilizes the CBC distribu-
tional word clustering algorithm (Pantel, 2003). In
short, this method extends each cpv:e list with CBC
clusters that contain at least one term in the list, scor-
ing them according to their ?relevancy?. The score
of matching two cpv:e lists, denoted here SCBC(?, ?),
is the score of the highest scoring member that ap-
pears in both lists.
We applied the final binary match score presented
in (Pantel et al, 2007), denoted here binaryCBC:
mv:e(r, t) is 1 if SCBC(r, t) is above a threshold and
0 otherwise. As a more natural ranking method, we
also utilize SCBC directly, denoted rankedCBC,
having mv:e(r, t) = SCBC(r, t).
In addition, we tried a simpler method that di-
rectly compares the terms in two cpv:e lists, uti-
lizing the commonly-used term similarity metric of
(Lin, 1998a). This method, denoted LIN , uses the
same raw distributional data as CBC but computes
only pair-wise similarities, without any clustering
phase. We calculated the scores of the 1000 most
similar terms for every term in the Reuters RVC1
corpus3. Then, a directional similarity of term a
to term b, s(a, b), is set to be their similarity score
if a is in b?s 1000 most similar terms and 0 other-
wise. The final score of matching r with t is deter-
mined by a nearest-neighbor approach, as the score
of the most similar pair of terms in the correspond-
ing two lists of the same variable: mv:e(r, t) =
maxj?vars(r)[maxa?cpv:e(t)[j],b?cpv:e(r)[j][s(a, b)]].
3.2.3 Matching cpv:n
We use a simple scoring mechanism for compar-
ing between two named-entity types a and b, s(a, b):
1 for identical types and 0.8 otherwise.
A variable j has a single preferred entity type
in cpv:n(t)[j], the type of its instantiation in t.
However, it can have several preferred types for h.
When matching h with t, j?s match score is that
of its highest scoring type, and the final score is
the product of all variable scores: mv:n(h, t) =?
j?vars(h)(maxa?cpv:n(h)[j][s(a, cpv:n(t)[j])]).
Variable j may also have several types in r, the
3http://about.reuters.com/researchandstandards/corpus/
686
types of the common arguments in cpv:e(r). When
matching h with r, s(a, cpv:n(t)[j]) is replaced with
the average score for a and each type in cpv:n(r)[j].
3.3 Overall Score for a Match
A final score for a given match, denoted allCP, is
obtained by the product of all six matching scores
of the various CP components (multiplying by 1
if a component score is missing). The six scores
are the results of matching any of the two compo-
nents of h, t and r: mg(h, t), mv(h, t), mg(h, r),
mv(h, r), mg(r, t) and mv(r, t) (as specified above,
mv(r, t) is based on matching cpv:e while mv(h, r)
and mv(h, t) are based on matching cpv:n). We use
rankedCBC for calculating mv(r, t).
Unlike previous work (e.g. (Pantel et al, 2007)),
we also utilize the prior score of a rule r, which
is provided by the rule-learning algorithm (see next
section). We denote by allCP+pr the final match
score obtained by the product of the allCP score
with the prior score of the matched rule.
4 Experimental Settings
Evaluating the contribution of Contextual Prefer-
ences models requires: (a) a sample of test hypothe-
ses, and (b) a corresponding corpus that contains
sentences which entail these hypotheses, where all
hypothesis matches (either direct or via rules) are an-
notated. We found that the available event mention
annotations in the ACE 2005 training set4 provide a
useful test set that meets these generic criteria, with
the added value of a standard real-world dataset.
The ACE annotation includes 33 types of events,
for which all event mentions are annotated in the
corpus. The annotation of each mention includes the
instantiated arguments for the predicates, which rep-
resent the participants in the event, as well as general
attributes such as time and place. ACE guidelines
specify for each event type its possible arguments,
where all arguments are optional. Each argument is
associated with a semantic role and a list of possible
named-entity types. For instance, an Injure event
may have the arguments {Agent, Victim, Instrument,
Time, Place}, where Victim should be a person.
For each event type we manually created a small
set of template hypotheses that correspond to the
4http://projects.ldc.upenn.edu/ace/
given event predicate, and specified the appropri-
ate semantic roles for each variable. We consid-
ered only binary hypotheses, due to the type of
available entailment rules (see below). For In-
jure, the set of hypotheses included ?A injure V?
and ?injure V in T? where role(A)={Agent, In-
strument}, role(V)={Victim}, and role(T)={Time,
Place}. Thus, correct match of an argument corre-
sponds to correct role identification. The templates
were represented as Minipar (Lin, 1998b) depen-
dency parse-trees.
The Contextual Preferences for h were con-
structed manually: the named-entity types for
cpv:n(h) were set by adapting the entity types given
in the guidelines to the types supported by the Ling-
pipe NER (described in Section 3.2). cpg(h) was
generated from a short list of nouns and verbs that
were extracted from the verbal event definition in
the ACE guidelines. For Injure, this list included
{injure:v, injury:n, wound:v}. This assumes that
when writing down an event definition the user
would also specify such representative keywords.
Entailment-rules for a given h (rules in which
RHS is equal to h) were learned automatically by
the DIRT algorithm (Lin and Pantel, 2001), which
also produces a quality score for each rule. We im-
plemented a canonized version of DIRT (Szpektor
and Dagan, 2007) on the Reuters corpus parsed by
Minipar. Each rule?s arguments for cpv(r) were also
collected from this corpus.
We assessed the CP framework by its ability to
correctly rank, for each predicate (event), all the
candidate entailing mentions that are found for it
in the test corpus. Such ranking evaluation is suit-
able for unsupervised settings, with a perfect rank-
ing placing all correct mentions before any incor-
rect ones. The candidate mentions are found in the
parsed test corpus by matching the specified event
hypotheses, either directly or via the given set of en-
tailment rules, using a syntactic matcher similar to
the one in (Szpektor and Dagan, 2007). Finally, the
mentions are ranked by their match scores, as de-
scribed in Section 3.3. As detailed in the next sec-
tion, those candidate mentions which are also an-
notated as mentions of the same event in ACE are
considered correct.
The evaluation aims to assess the correctness of
inferring a target semantic meaning, which is de-
687
noted by a specific predicate. Therefore, we elim-
inated four ACE event types that correspond to mul-
tiple distinct predicates. For instance, the Transfer-
Money event refers to both donating and lending
money, which are not distinguished by the ACE an-
notation. We also omitted three events with less than
10 mentions and two events for which the given set
of learned rules could not match any mention. We
were left with 24 event types for evaluation, which
amount to 4085 event mentions in the dataset. Out of
these, our binary templates can correctly match only
mentions with at least two arguments, which appear
2076 times in the dataset.
Comparing with previous evaluation methodolo-
gies, in (Szpektor et al, 2007; Pantel et al, 2007)
proper context matching was evaluated by post-hoc
judgment of a sample of rule applications for a sam-
ple of rules. Such annotation needs to be repeated
each time the set of rules is changed. In addition,
since the corpus annotation is not exhaustive, re-
call could not be computed. By contrast, we use a
standard real-world dataset, in which all mentions
are annotated. This allows immediate comparison
of different rule sets and matching methods, without
requiring any additional (post-hoc) annotation.
5 Results and Analysis
We experimented with three rule setups over the
ACE dataset, in order to measure the contribution
of the CP framework. In the first setup no rules are
used, applying only direct matches of template hy-
potheses to identify event mentions. In the other two
setups we also utilized DIRT?s top 50 or 100 rules
for each hypothesis.
A match is considered correct when all matched
arguments are extracted correctly according to their
annotated event roles. This main measurement is de-
noted All. As an additional measurement, denoted
Any, we consider a match as correct if at least one
argument is extracted correctly.
Once event matches are extracted, we first mea-
sure for each event its Recall, the number of correct
mentions identified out of all annotated event men-
tions5 and Precision, the number of correct matches
out of all extracted candidate matches. These figures
5For Recall, we ignored mentions with less than two argu-
ments, as they cannot be correctly matched by binary templates.
quantify the baseline performance of the DIRT rule
set used. To assess our ranking quality, we measure
for each event the commonly used Average Preci-
sion (AP) measure (Voorhees and Harmann, 1998),
which is the area under the non-interpolated recall-
precision curve, while considering for each setup all
correct extracted matches as 100% Recall. Overall,
we report Mean Average Precision (MAP), macro
average Precision and macro average Recall over the
ACE events. Tables 1 and 2 summarize the main re-
sults of our experiments. As far as we know, these
are the first published unsupervised results for iden-
tifying event arguments in the ACE 2005 dataset.
Examining Recall, we see that it increases sub-
stantially when rules are applied: by more than
100% for the top 50 rules, and by about 150% for
the top 100, showing the benefit of entailment-rules
to covering language variability. The difference be-
tween All and Any results shows that about 65%
of the rules that correctly match one argument also
match correctly both arguments.
We use two baselines for measuring the CP rank-
ing contribution: Precision, which corresponds to
the expected MAP of random ranking, and MAP
of ranking using the prior rule score provided by
DIRT. Without rules, the baseline All Precision is
34.1%, showing that even the manually constructed
hypotheses, which correspond directly to the event
predicate, extract event mentions with limited accu-
racy when context is ignored. When rules are ap-
plied, Precision is very low. But ranking is consider-
ably improved using only the prior score (from 1.4%
to 22.7% for 50 rules), showing that the prior is an
informative indicator for valid matches.
Our main result is that the allCP and allCP+pr
methods rank matches statistically significantly bet-
ter than the baselines in all setups (according to the
Wilcoxon double-sided signed-ranks test at the level
of 0.01 (Wilcoxon, 1945)). In the All setup, ranking
is improved by 70% for direct matching (Table 1).
When entailment-rules are also utilized, prior-only
ranking is improved by about 35% and 50% when
using allCP and allCP+pr, respectively (Table 2).
Figure 2 presents the average Recall-Precision curve
of the ?50 Rules, All? setup for applying allCP or
allCP+pr, compared to prior-only ranking baseline
(other setups behave similarly). The improvement
in ranking is evident: the drop in precision is signif-
688
R P MAP (%)
(%) (%) cpv cpg allCP
All 14.0 34.1 46.5 52.2 60.2
Any 21.8 66.0 72.2 80.5 84.1
Table 1: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when only matching template hypotheses
directly.
# R P MAP (%)
Rules (%) (%) prior allCP allCP+pr
All 50 29.6 1.4 22.7 30.6 34.1100 34.9 0.7 20.5 26.3 30.2
Any 50 46.5 3.5 41.2 43.7 48.6100 52.9 1.8 35.5 35.1 40.8
Table 2: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when also using rules for matching.
icantly slower when CP is used. The behavior of CP
with and without the prior is largely the same up to
50% Recall, but later on our implemented CP mod-
els are noisier and should be combined with the prior
rule score.
Templates are incorrectly matched for several rea-
sons. First, there are context mismatches which are
not scored sufficiently low by our models. Another
main cause is incorrect learned rules in which LHS
and RHS are topically related, e.g. ?X convict Y ?
X arrest Y ?, or rules that are used in the wrong en-
tailment direction, e.g. ?X marry Y ?X divorce Y ?
(DIRT does not learn rule direction). As such rules
do correspond to plausible contexts of the hypothe-
sis, their matches obtain relatively high CP scores.
In addition, some incorrect matches are caused by
our syntactic matcher, which currently does not han-
dle certain phenomena such as co-reference, modal-
ity or negation, and due to Minipar parse errors.
5.1 Component Analysis
Table 3 displays the contribution of different CP
components to ranking, when adding only that com-
ponent?s match score to the baselines, and under ab-
lation tests, when using all CP component scores ex-
cept the tested component, with or without the prior.
As it turns out, matching h with t (i.e. cp(h, t),
which combines cpg(h, t) and cpv(h, t)) is most use-
ful. With our current models, using only cp(h, t)
along with the prior, while ignoring cp(r), achieves
50 Rules  -  All
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100Relative Recall
Prec
ision
baseline CP CP + prior
Figure 2: Recall-Precision curves for ranking using: (a)
only the prior (baseline); (b) allCP; (c) allCP+pr.
the highest score in the table. The strong impact of
matching h and t?s preferences is also evident in Ta-
ble 1, where ranking based on either cpg or cpv sub-
stantially improves precision, while their combina-
tion provides the best ranking. These results indicate
that the two CP components capture complementary
information and both are needed to assess the cor-
rectness of a match.
When ignoring the prior rule score, cp(r, t) is the
major contributor over the baseline Precision. For
cpv(r, t), this is in synch with the result in (Pantel
et al, 2007), which is based on this single model
without utilizing prior rule scores. On the other
hand, cpv(r, t) does not improve the ranking when
the prior is used, suggesting that this contextual
model for the rule?s variables is not stronger than the
context-insensitive prior rule score. Furthermore,
relative to this cpv(r, t) model from (Pantel et al,
2007), our combined allCP model, with or without
the prior (first row of Table 2), obtains statistically
significantly better ranking (at the level of 0.01).
Comparing between the algorithms for match-
ing cpv:e (Section 3.2.2) we found that while
rankedCBC is statistically significantly better than
binaryCBC, rankedCBC and LIN generally
achieve the same results. When considering the
tradeoffs between the two, LIN is based on a much
simpler learning algorithm while CBC?s output is
more compact and allows faster CP matches.
689
Addition To Ablation From
P prior allCP allCP+pr
Baseline 1.4 22.7 30.6 34.1
cpg(h, t) ?10.4 ?35.4 32.4 33.7
cpv(h, t) ?11.0 29.9 27.6 32.9
cp(h, t) ?8.9 ?37.5 28.6 30.0
cpg(r, t) ?4.2 ?30.6 32.5 35.4
cpv(r, t) ?21.7 21.9 ?12.9 33.6
cp(r, t) ?26.0 ?29.6 ?17.9 36.8
cpg(h, r) ?8.1 22.4 31.9 34.3
cpv(h, r) ?10.7 22.7 ?27.9 34.4
cp(h, r) ?16.5 22.4 ?29.2 34.4
cpg(h, r, t) ?7.7 ?30.2 ?27.5 ?29.2
cpv(h, r, t) ?27.5 29.2 ?7.7 30.2
? Indicates statistically significant changes compared to the baseline,
according to the Wilcoxon test at the level of 0.01.
Table 3: MAP(%), under the ?50 rules, All? setup, when
adding component match scores to Precision (P) or prior-
only MAP baselines, and when ranking with allCP or
allCP+pr methods but ignoring that component scores.
Currently, some models do not improve the re-
sults when the prior is used. Yet, we would like to
further weaken the dependency on the prior score,
since it is biased towards frequent contexts. We
aim to properly identify also infrequent contexts (or
meanings) at inference time, which may be achieved
by better CP models. More generally, when used
on top of all other components, some of the mod-
els slightly degrade performance, as can be seen by
those figures in the ablation tests which are higher
than the corresponding baseline. However, due to
their different roles, each of the matching compo-
nents might capture some unique preferences. For
example, cp(h, r) should be useful to filter out rules
that don?t match the intended meaning of the given
h. Overall, this suggests that future research for bet-
ter models should aim to obtain a marginal improve-
ment by each component.
6 Related Work
Context sensitive inference was mainly investigated
in an application-dependent manner. For exam-
ple, (Harabagiu et al, 2003) describe techniques for
identifying the question focus and the answer type in
QA. (Patwardhan and Riloff, 2007) propose a super-
vised approach for IE, in which relevant text regions
for a target relation are identified prior to applying
extraction rules.
Recently, the need for context-aware inference
was raised (Szpektor et al, 2007). (Pantel et al,
2007) propose to learn the preferred instantiations of
rule variables, termed Inferential Selectional Prefer-
ences (ISP). Their clustering-based model is the one
we implemented for mv(r, t). A similar approach
is taken in (Pennacchiotti et al, 2007), where LSA
similarity is used to compare between the preferred
variable instantiations for a rule and their instanti-
ations in the matched text. (Downey et al, 2007)
use HMM-based similarity for the same purpose.
All these methods are analogous to matching cpv(r)
with cpv(t) in the CP framework.
(Dagan et al, 2006; Connor and Roth, 2007) pro-
posed generic approaches for identifying valid appli-
cations of lexical rules by classifying the surround-
ing global context of a word as valid or not for that
rule. These approaches are analogous to matching
cpg(r) with cpg(t) in our framework.
7 Conclusions
We presented the Contextual Preferences (CP)
framework for assessing the validity of inferences
in context. CP enriches the representation of tex-
tual objects with typical contextual information that
constrains or disambiguates their meaning, and pro-
vides matching functions that compare the prefer-
ences of objects involved in the inference. Experi-
ments with our implemented CP models, over real-
world IE data, show significant improvements rela-
tive to baselines and some previous work.
In future research we plan to investigate improved
models for representing and matching CP, and to ex-
tend the experiments to additional applied datasets.
We also plan to apply the framework to lexical infer-
ence rules, for which it seems directly applicable.
Acknowledgements
The authors would like to thank Alfio Massimiliano
Gliozzo for valuable discussions. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
the NEGEV project (www.negev-initiative.org) and
the FBK-irst/Bar-Ilan University collaboration.
690
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of the European Conference on Machine
Learning (ECML).
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of ACL.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of ACL.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis. Advisor-
Carlo Strapparava.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain textual question answer-
ing techniques. Nat. Lang. Eng., 9(3):231?267.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and
Vasile Rus. 2000. The structure and performance of
an open-domain question answering system. In Pro-
ceedings of the 38th Annual Meeting of ACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference of
NAACL; Proceedings of the Main Conference.
Patrick Andre Pantel. 2003. Clustering by committee.
Ph.D. thesis. Advisor-Dekang Lin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning selectional prefer-
ences for entailment or paraphrasing rules. In Pro-
ceedings of RANLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of the 11th Conference of the
EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference.
Idan Szpektor and Ido Dagan. 2007. Learning canonical
forms of entailment rules. In Proceedings of RANLP.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP 2004,
pages 41?48, Barcelona, Spain.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
ACL.
Ellen M. Voorhees and Donna Harmann. 1998.
Overview of the seventh text retrieval conference
(trec?7). In The Seventh Text Retrieval Conference.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
691
Learning Entailment Relations by Global
Graph Structure Optimization
Jonathan Berant?
Tel Aviv University
Ido Dagan??
Bar-Ilan University
Jacob Goldberger?
Bar-Ilan University
Identifying entailment relations between predicates is an important part of applied semantic
inference. In this article we propose a global inference algorithm that learns such entailment
rules. First, we define a graph structure over predicates that represents entailment relations as
directed edges. Then, we use a global transitivity constraint on the graph to learn the optimal set
of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is
applied in a setting where, given a target concept, the algorithm learns on the fly all entailment
rules between predicates that co-occur with this concept. Results show that our global algorithm
improves performance over baseline algorithms by more than 10%.
1. Introduction
The Textual Entailment (TE) paradigm is a generic framework for applied semantic
inference. The objective of TE is to recognize whether a target textual meaning can
be inferred from another given text. For example, a question answering system has
to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces
blood pressure to answer the question What affects blood pressure? In the TE framework,
entailment is defined as a directional relationship between pairs of text expressions,
denoted by T, the entailing text, and H, the entailed hypothesis. The text T is said to
entail the hypothesis H if, typically, a human reading T would infer that H is most likely
true (Dagan et al 2009).
TE systems require extensive knowledge of entailment patterns, often captured as
entailment rules?rules that specify a directional inference relation between two text
fragments (when the rule is bidirectional this is known as paraphrasing). A common
type of text fragment is a proposition, which is a simple natural language expression
that contains a predicate and arguments (such as alcohol affects blood pressure), where
the predicate denotes some semantic relation between the concepts that are expressed
? Tel-Aviv University, P.O. Box 39040, Tel-Aviv, 69978, Israel. E-mail: jonatha6@post.tau.ac.il.
?? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: dagan@cs.biu.ac.il.
? Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: goldbej@eng.biu.ac.il.
Submission received: 28 September 2010; revised submission received: 5 May 2011; accepted for publication:
5 July 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
by the arguments. One important type of entailment rule specifies entailment between
propositional templates, that is, propositions where the arguments are possibly re-
placed by variables. A rule corresponding to the aforementioned example may be X
reduce blood pressure ? X affect blood pressure. Because facts and knowledge are mostly
expressed by propositions, such entailment rules are central to the TE task. This has
led to active research on broad-scale acquisition of entailment rules for predicates
(Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010).
Previous work has focused on learning each entailment rule in isolation. It is clear,
however, that there are interactions between rules. A prominent phenomenon is that
entailment is inherently a transitive relation, and thus the rules X ? Y and Y ? Z imply
the rule X ? Z.1 In this article we take advantage of these global interactions to improve
entailment rule learning.
After reviewing relevant background (Section 2), we describe a structure termed
an entailment graph that models entailment relations between propositional templates
(Section 3). Next, we motivate and discuss a specific type of entailment graph, termed a
focused entailment graph, where a target concept instantiates one of the arguments of
all propositional templates. For example, a focused entailment graph about the target
concept nausea might specify the entailment relations between propositional templates
like X induce nausea, X prevent nausea, and nausea is a symptom of X.
In the core section of the article, we present an algorithm that uses a global approach
to learn the entailment relations, which comprise the edges of focused entailment
graphs (Section 4). We define a global objective function and look for the graph that
maximizes that function given scores provided by a local entailment classifier and a
global transitivity constraint. The optimization problem is formulated as an Integer
Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal
solution with respect to the global function. In Section 5 we demonstrate that this
algorithm outperforms by 12?13% methods that utilize only local information as well
as methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006)
rather than an ILP solver.
The article also includes a comprehensive investigation of the algorithm and its
components. First, we perform manual comparison between our algorithm and the
baselines and analyze the reasons for the improvement in performance (Sections 5.3.1
and 5.3.2). Then, we analyze the errors made by the algorithm against manually pre-
pared gold-standard graphs and compare them to the baselines (Section 5.4). Last, we
perform a series of experiments in which we investigate the local entailment classifier
and specifically experiment with various sets of features (Section 6). We conclude and
suggest future research directions in Section 7.
This article is based on previous work (Berant, Dagan, and Goldberger 2010), while
substantially expanding upon it. From a theoretical point of view, we reformulate the
two ILPs previously introduced by incorporating a prior. We show a theoretical relation
between the two ILPs and prove that the optimization problem tackled is NP-hard.
From an empirical point of view, we conduct many new experiments that examine
both the local entailment classifier as well as the global algorithm. Last, a rigorous
analysis of the algorithm is performed and an extensive survey of previous work is
provided.
1 Assuming that Y has the same sense in both X ? Y and Y ? Z, as we discuss later in Section 3.
74
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
2. Background
In this section we survey methods proposed in past literature for learning entailment
rules between predicates. First, we discuss local methods that assess entailment given a
pair of predicates, and then global methods that perform inference over a larger set of
predicates.
2.1 Local Learning
Three types of information have primarily been utilized in the past to learn entailment
rules between predicates: lexicographic methods, distributional similarity methods, and
pattern-based methods.
Lexicographic methods use manually prepared knowledge bases that contain in-
formation about semantic relations between lexical items. WordNet (Fellbaum 1998b),
by far the most widely used resource, specifies relations such as hyponymy, synonymy,
derivation, and entailment that can be used for semantic inference (Budanitsky and
Hirst 2006). For example, if WordNet specifies that reduce is a hyponym of affect, then
one can infer that X reduces Y ? X affects Y. WordNet has also been exploited to
automatically generate a training set for a hyponym classifier (Snow, Jurafsky, and Ng
2004), and we make a similar use of WordNet in Section 4.1.
A drawback of WordNet is that it specifies semantic relations for words and terms
but not for more complex expressions. For example, WordNet does not cover a complex
predicate such as X causes a reduction in Y. Another drawback of WordNet is that it only
supplies semantic relations between lexical items, but does not provide any information
on how to map arguments of predicates. For example, WordNet specifies that there is
an entailment relation between the predicates pay and buy, but does not describe the
way in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus,
using WordNet directly to derive entailment rules between predicates is possible only
for semantic relations such as hyponymy and synonymy, where arguments typically
preserve their syntactic positions on both sides of the rule.
Some knowledge bases try to overcome this difficulty: Nomlex (Macleod et al
1998) is a dictionary that provides the mapping of arguments between verbs and their
nominalizations and has been utilized to derive predicative entailment rules (Meyers
et al 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is
a lexicographic resource that is arranged around ?frames?: Each frame corresponds to
an event and includes information on the predicates and arguments relevant for that
specific event supplemented with annotated examples that specify argument positions.
Consequently, FrameNet was also used to derive entailment rules between predicates
(Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional man-
ually constructed resources for predicates include PropBank (Kingsbury, Palmer, and
Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000).
Distributional similarity methods are used to learn broad-scale resources, because
lexicographic resources tend to have limited coverage. Distributional similarity algo-
rithms employ ?the distributional hypothesis? (Harris 1954) and predict a semantic
relation between two predicates by comparing the arguments with which they occur.
Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al 2004;
Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al 2010), which differ in terms of the specifics of the ways in which
predicates are represented, the features that are extracted, and the function used to com-
pute feature vector similarity. Next, we elaborate on some of the prominent methods.
75
Computational Linguistics Volume 38, Number 1
Lin and Pantel (2001) proposed an algorithm that is based on a mutual information
criterion. A predicate is represented by a binary template, which is a dependency path
between two arguments of a predicate where the arguments are replaced by variables.
Note that in a dependency tree, a path between two arguments must pass through their
common predicate. Also note that if a predicate has more than two arguments, then it
is represented by more than one binary template, where each template corresponds to
a different aspect of the predicate. For example, the proposition I bought a gift for her
contains a predicate and three arguments, and therefore is represented by the following
three binary templates: X
subj
??? buys
obj
?? Y, X
obj
?? buys
prep
??? for
pcomp?n
?????? Y and X
subj
??? buys
prep
??? for
pcomp?n
?????? Y.
For each binary template Lin and Pantel compute two sets of features Fx and Fy,
which are the words that instantiate the arguments X and Y, respectively, in a large
corpus. Given a template t and its feature set for the X variable Ftx, every fx ? F
t
x is
weighted by the pointwise mutual information between the template and the feature:
wtx( fx) = log
Pr( fx|t)
Pr( fx )
, where the probabilities are computed using maximum likelihood
over the corpus. Given two templates u and v, the Lin measure (Lin 1998a) is computed
for the variable X in the following manner:
Linx(u, v) =
?
f?Fux?Fvx
[wux ( f ) + w
v
x( f )]
?
f?Fux
wux ( f ) +
?
f?Fvx
wvx( f )
(1)
The measure is computed analogously for the variable Y and the final distributional
similarity score, termed DIRT, is the geometric average of the scores for the two
variables:
DIRT(u, v) =
?
Linx(u, v) ? Liny(u, v) (2)
If DIRT(u, v) is high, this means that the templates u and v share many ?informative?
arguments and so it is possible that u ? v. Note, however, that the DIRT similarity
measure computes a symmetric score, which is appropriate for modeling synonymy
but not entailment, an inherently directional relation.
To remedy that, Szpektor and Dagan (2008) suggested a directional distributional
similarity measure. In their work, Szpektor and Dagan chose to represent predicates
with unary templates, which are identical to binary templates, only they contain a pred-
icate and a single argument, such as: X
subj
??? buys. Szpektor and Dagan explain that unary
templates are more expressive than binary templates, and that some predicates can only
be encoded using unary templates. They propose that if for two unary templates u ? v,
then relatively many of the features of u should be covered by the features of v. This
is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we
omit the subscript x from Fux and F
v
x because in their setting there is only one argument):
Cover(u, v) =
?
f?Fu?Fv w
u( f )
?
f?Fu w
u( f )
(3)
The final directional score, termed BInc (Balanced Inclusion), is the geometric average
of the Lin measure and the Cover measure:
BInc(u, v) =
?
Lin(u, v) ? Cover(u, v) (4)
76
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score for
each argument separately, effectively decoupling the arguments from one another. It is
clear, however, that although this alleviates sparsity problems, it disregards an impor-
tant piece of information, namely, the co-occurrence of arguments. For example, if one
looks at the following propositions: coffee increases blood pressure, coffee decreases fatigue,
wine decreases blood pressure, wine increases fatigue, one can notice that the predicates occur
with similar arguments and might mistakenly infer that decrease ? increase. However,
looking at pairs of arguments reveals that the predicates do not share a single pair of
arguments.
Yates and Etzioni (2009) address this issue and propose a generative model that
estimates the probability that two predicates are synonymous (synonymy is simply
bidirectional entailment) by comparing pairs of arguments. They represent predicates
and arguments as strings and compute for every predicate a feature vector that counts
that number of times it occurs with any ordered pair of words as arguments. Their main
modeling decision is to assume that two predicates are synonymous if the number of
pairs of arguments they share is maximal. An earlier work by Szpektor et al (2004)
also tried to learn entailment rules between predicates by using pairs of arguments as
features. They utilized an algorithm that learns new rules by searching for distributional
similarity information on the Web for candidate predicates.
Pattern-based methods. Although distributional similarity measures excel at iden-
tifying the existence of semantic similarity between predicates, they are often unable
to discern the exact type of semantic similarity and specifically determine whether it is
entailment. Pattern-based methods are used to automatically extract pairs of predicates
for a specific semantic relation. Pattern-based methods identify a semantic relation
between two predicates by observing that they co-occur in specific patterns in sentences.
For example, from the single proposition He scared and even startled me one might infer
that startle is semantically stronger than scare and thus startle ? scare. Chklovski and
Pantel (2004) manually constructed a few dozen patterns and learned semantic relations
between predicates by looking for these patterns on the Web. For example, the pattern
X and even Y implies that Y is stronger than X, and the pattern to X and then Y indicates
that Y follows X. The main disadvantage of pattern-based methods is that they are based
on the co-occurrence of two predicates in a single sentence in a specific pattern. These
events are quite rare and require working on a very large corpus, or preferably, the Web.
Pattern-based methods were mainly utilized so far to extract semantic relations
between nouns, and there has been some work on automatically learning patterns for
nouns (Snow, Jurafsky, and Ng 2004). Although these methods can be expanded for
predicates, we are unaware of any attempt to automatically learn patterns that describe
semantic relations between predicates (as opposed to the manually constructed patterns
suggested by Chklovski and Pantel [2004]).
2.2 Global Learning
It is natural to describe entailment relations between predicates (or language expres-
sions in general) by a graph. Nodes represent predicates, and edges represent entail-
ment between nodes. Nevertheless, using a graph for global learning of all entailment
relations within a set of predicates, rather then between pairs of predicates, has attracted
little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-
mapped WordNet, providing entailment relations for predicates in WordNet. This re-
source was built on top of WordNet and augments it with mapping of arguments for
predicates using NomLex (Macleod et al 1998) and a corpus-based resource (Szpektor
77
Computational Linguistics Volume 38, Number 1
and Dagan 2008). Their resource makes simple use of WordNet?s global graph structure:
New rules are suggested by transitively chaining graph edges, and then verified using
distributional similarity measures. Effectively, this is equivalent to using the intersection
of the set of rules derived by this transitive chaining and the set of rules in a distribu-
tional similarity knowledge base.
The most similar work to ours is Snow, Jurafsky, and Ng?s (2006) algorithm for
taxonomy induction, although it involves learning the hyponymy relation between
nouns, which is a special case of entailment, rather than learning entailment between
predicates. We provide here a brief review of a simplified form of this algorithm.
Snow, Jurafsky, and Ng define a taxonomy T to be a set of pairs of words, expressing
the hyponymy relation between them. The notation Huv ? T means that the noun u is a
hyponym of the noun v in T. They define D to be the set of observed data over all pairs of
words, and define Duv ? D to be the observed evidence we have in the data for the event
Huv ? T. Snow, Jurafsky, and Ng assume a model exists for inferring P(Huv ? T|Duv):
the posterior probability of the event Huv ? T, given the data. Their goal is to find the
taxonomy that maximizes the likelihood of the data, that is, to find
T? = argmax
T
P(D|T) (5)
Using some independence assumptions and Bayes rule, the likelihood P(D|T) is
expressed:
P(D|T) =
?
Huv?T
P(Huv ? T|Duv)P(Duv)
P(Huv ? T)
?
?
Huv/?T
P(Huv /? T|Duv)P(Duv)
P(Huv /? T)
(6)
Crucially, they demand that the taxonomy learned respects the constraint that hy-
ponymy is a transitive relation. To ensure that, they propose the following greedy
algorithm: At each step they go over all pairs of words (u, v) that are not in the taxonomy,
and try to add the single hyponymy relation Huv. Then, they calculate the set of relations
Suv that Huv will add to the taxonomy due to the transitivity constraint (all of the
relations Huw, where w is a hypernym of v in the taxonomy). Last, they choose to
add that set of relations Suv that maximizes P(D|T) out of all the possible candidates.
This iterative process stops when P(D|T) starts dropping. Their implementation of the
algorithm uses a hyponym classifier presented in an earlier work (Snow, Jurafsky, and
Ng 2004) as a model for P(Huv ? T|Duv) and a single sparsity parameter k =
P(Huv/?T)
P(Huv?T)
. In
this article we tackle a similar problem of learning a transitive relation, but we use linear
programming (Vanderbei 2008) to solve the optimization problem.
2.3 Linear Programming
A Linear Program (LP) is an optimization problem where a linear objective function is
minimized (or maximized) under linear constraints.
min
x?Rd
cx (7)
such that Ax ? b
where c ? Rd is a coefficient vector, and A ? Rn ? Rd and b ? Rn specify the constraints.
In short, we wish to find the optimal assignment for the d variables in the vector x, such
78
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
that all n linear constraints specified by the matrix A and the vector b are satisfied by this
assignment. If the variables are forced to be integers, the problem is termed an Integer
Linear Program (ILP). ILP has attracted considerable attention recently in several
fields of NLP, such as semantic role labeling, summarization, and parsing (Althaus,
Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and
Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we
formulate the entailment graph learning problem as an ILP, which leads to an optimal
solution with respect to the objective function (vs. a greedy optimization algorithm
suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP
in a related task of learning taxonomic relations between nouns, utilizing constraints
between sibling nodes and ancestor?child nodes in small graphs of three nodes.
3. Entailment Graph
In this section we define a structure termed the entailment graph that describes the
entailment relations between propositional templates (Section 3.1), and a specific type
of entailment graph, termed the focused entailment graph, that concentrates on entail-
ment relations that are relevant for some pre-defined target concept (Section 3.2).
3.1 Entailment Graph: Definition and Properties
The nodes of an entailment graph are propositional templates. A propositional tem-
plate is a binary template2 where at least one of the two arguments is a variable whereas
the second may be instantiated. In addition, the sense of the predicate is specified (ac-
cording to some sense inventory, such as WordNet) and so each sense of a polysemous
predicate corresponds to a separate template (and a separate graph node). For example,
X
subj
??? treats#1
obj
?? Y and X
subj
??? treats#2
obj
?? nausea are propositional templates for the
first and second sense of the predicate treat, respectively. An edge (u, v) represents the
fact that template u entails template v. Note that the entailment relation transcends
hyponymy/troponomy. For example, the template X is diagnosed with asthma entails the
template X suffers from asthma, although one is not a hyponym of the other. An example
of an entailment graph is given in Figure 1.
Because entailment is a transitive relation, an entailment graph is transitive, that is,
if the edges (u, v) and (v, w) are in the graph, so is the edge (u, w). Note that the property
of transitivity does not hold when the senses of the predicates are not specified. For
example, X buys Y ? X acquires Y and X acquires Y ? X learns Y, but X buys Y X learns
Y. This violation occurs because the predicate acquire has two distinct senses in the two
templates, but this distinction is lost when senses are not specified.
Transitivity implies that in each strongly connected component3 of the graph all
nodes entail each other. For example, in Figure 1 the nodes X-related-to-nausea and X-
associated-with-nausea form a strongly connected component. Moreover, if we merge
every strongly connected component to a single node, the graph becomes a Directed
Acyclic Graph (DAG), and a hierarchy of predicates can be obtained.
2 We restrict our discussion to templates with two arguments, but generalization is straightforward.
3 A strongly connected component is a subset of nodes in the graph where there is a path from any
node to any other node.
79
Computational Linguistics Volume 38, Number 1
Figure 1
A focused entailment graph. For clarity, edges that can be inferred by transitivity are omitted.
The single strongly connected component is surrounded by a dashed line.
3.2 Focused Entailment Graphs
In this article we concentrate on learning a type of entailment graph, termed the focused
entailment graph. Given a target concept, such as nausea, a focused entailment graph
describes the entailment relations between propositional templates for which the target
concept is one of the arguments (see Figure 1). Learning such entailment rules in real
time for a target concept is useful in scenarios such as information retrieval and question
answering, where a user specifies a query about the target concept. The need for such
rules has been also motivated by Clark et al (2007), who investigated what types
of knowledge are needed to identify entailment in the context of the RTE challenge,
and found that often rules that are specific to a certain concept are required. Another
example for a semantic inference algorithm that is utilized in real time is provided by
Do and Roth (2010), who recently described a system that, given two terms, determines
the taxonomic relation between them on the fly. Last, we have recently suggested an
application that uses focused entailment graphs to present information about a target
concept according to a hierarchy of entailment (Berant, Dagan, and Goldberger 2010).
The benefit of learning focused entailment graphs is three-fold. First, the target
concept that instantiates the propositional template usually disambiguates the predicate
and hence the problem of predicate ambiguity is greatly reduced. Thus, we do not
employ any form of disambiguation in this article, but assume that every node in a
focused entailment graph has a single sense (we further discuss this assumption when
describing the experimental setting in Section 5.1), which allows us to utilize transitivity
constraints.
An additional (albeit rare) reason that might also cause violations of transitivity
constraints is the notion of probabilistic entailment. Whereas troponomy rules
(Fellbaum 1998a) such as X walks ? X moves can be perceived as being almost always
correct, rules such as X coughs ? X is sick might only be true with some probability.
Consequently, chaining a few probabilistic rules such as A ? B, B ? C, and C ? D
might not guarantee the correctness of A ? D. Because in focused entailment graphs
the number of nodes and diameter4 are quite small (for example, in the data set we
4 The distance between two nodes in a graph is the number of edges in a shortest path connecting them.
The diameter of a graph is the maximal distance between any two nodes in the graph.
80
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
present in Section 5 the maximal number of nodes is 26, the average number of nodes
is 22.04, the maximal diameter is 5, and the average diameter is 2.44), we do not find
this to be a problem in our experiments in practice.
Last, the optimization problem that we formulate is NP-hard (as we show in Sec-
tion 4.2). Because the number of nodes in focused entailment graphs is rather small, a
standard ILP solver is able to quickly reach the optimal solution.
To conclude, the algorithm we suggest next is applied in our experiments on
focused entailment graphs. However, we believe that it is suitable for any entailment
graph whose properties are similar to those of focused entailment graphs. For brevity,
from now on the term entailment graph will stand for focused entailment graph.
4. Learning Entailment Graph Edges
In this section we present an algorithm that, given the set of propositional templates
constituting the nodes of an entailment graph, learns its edges (i.e., the entailment
relations between all pairs of nodes). The algorithm comprises two steps (described in
Sections 4.1 and 4.2): In the first step we use a large corpus and a lexicographic resource
(WordNet) to train a generic entailment classifier that given any pair of propositional
templates estimates the likelihood that one template entails the other. This generic step
is performed only once, and is independent of the specific nodes of the target entailment
graph whose edges we want to learn. In the second step we learn on the fly the edges of
a specific target graph: Given the graph nodes, we use a global optimization approach
that determines the set of edges that maximizes the probability (or score) of the entire
graph. The global graph decision is determined by the given edge probabilities (or
scores) supplied by the entailment classifier and by the graph constraints (transitivity
and others).
4.1 Training an Entailment Classifier
We describe a procedure for learning a generic entailment classifier, which can be used
to estimate the entailment likelihood for any given pair of templates. The classifier
is constructed based on a corpus and a lexicographic resource (WordNet) using the
following four steps:
(1) Extract a large set of propositional templates from the corpus.
(2) Use WordNet to automatically generate a training set of pairs of
templates?both positive and negative examples.
(3) Represent each training set example with a feature vector of various
distributional similarity scores.
(4) Train a classifier over the training set.
(1) Template extraction. We parse the corpus with the Minipar dependency parser
(Lin 1998b) and use the Minipar representation to extract all binary templates from
every parse tree, employing the procedure described by Lin and Pantel (2001), which
considers all dependency paths between every pair of nouns in the parse tree. We
also apply over the extracted paths the syntactic normalization procedure described
by Szpektor and Dagan (2007), which includes transforming passive forms into active
forms and removal of conjunctions, appositions, and abbreviations. In addition, we use
81
Computational Linguistics Volume 38, Number 1
Table 1
Positive and negative examples for entailment in the training set. The direction of entailment is
from the left template to the right template.
Positive examples Negative examples
(X
subj
??? desires
obj
?? Y, X
subj
??? wants
obj
?? Y) (X
subj
??? pushes
obj
?? Y,X
subj
??? blows
obj
?? Y)
(X
subj
??? causes vrel?? Y, X
subj
??? creates vrel?? Y) (X
subj
??? issues vrel?? Y,X
subj
??? signs vrel?? Y)
a simple heuristic to filter out templates that probably do not include a predicate: We
omit ?uni-directional? templates where the root of template has a single child, such as
therapy
prep
???in
p?comp
?????patient nn??cancer, unless one of the edges is labeled with a passive
relation, such as in the template nausea
vrel???characterized
subj
???poisoning, which contains
the Minipar passive label vrel.5 Last, the arguments are replaced by variables, resulting
in propositional templates such as X
subj
??? affect
obj
?? Y. The lexical items that remain in
the template after replacing the arguments by variables are termed predicate words.
(2) Training set generation. WordNet is used to automatically generate a training
set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set
of propositional templates extracted from the corpus. For each ti ? T with two variables
and a single predicate word w, we extract from WordNet the set H of direct hypernyms
(distance of one in WordNet) and synonyms of w. For every h ? H, we generate a new
template tj from ti by replacing w with h. If tj ? T, we consider (ti, tj) to be a positive
example. Negative examples are generated analogously, only considering direct co-
hyponyms of w, which are direct hyponyms of direct hypernyms of w that are not
synonymous to w. It has been shown in past work that in most cases co-hyponym terms
do not entail one another (Mirkin, Dagan, and Gefet 2006). A few examples for positive
and negative training examples are given in Table 1.
This generation method is similar to the ?distant supervision? method proposed by
Snow, Jurafsky, and Ng (2004) for training a noun hypernym classifier. It differs in some
important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example
to be any Wordnet hypernym, irrespective of the distance, whereas we look only at
direct hypernyms. This is because predicates are mainly verbs and precision drops
quickly when looking at verb hypernyms in WordNet at a longer distance. Second,
Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where
one is not the hypernym of the other. In the spirit of ?contrastive estimation? (Smith and
Eisner 2005), we prefer to generate negative examples that are ?hard,? that is, negative
examples that, although not entailing, are still semantically similar to positive examples
and thus focus the classifier?s attention on determining the boundary of the entailment
class. Last, we use a balanced number of positive and negative examples, because
classifiers tend to perform poorly on the minority class when trained on imbalanced
data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008).
(3) Distributional similarity representation. We aim to train a classifier that for an
input template pair (t1, t2) determines whether t1 entails t2. Our approach is to represent
a template pair by a feature vector where each coordinate is a different distributional
similarity score for the pair of templates. The different distributional similarity scores
5 This passive construction is not handled by the normalization scheme employed by Szpektor and Dagan
(2007).
82
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
are obtained by utilizing various distributional similarity algorithms that differ in one
or more of their characteristics. In this way we hope to combine the various methods
proposed in the past for measuring distributional similarity. The distributional similar-
ity algorithms we employ vary in one or more of the following dimensions: the way the
predicate is represented, the way the features are represented, and the function used to
measure similarity between the feature representations of the two templates.
Predicate representation. As mentioned, we represent predicates over dependency
tree structures. However, some distributional similarity algorithms measure similarity
between binary templates directly (Lin and Pantel 2001; Szpektor et al 2004; Bhagat,
Pantel, and Hovy 2007; Yates and Etzioni 2009), whereas others decompose binary
templates into two unary templates, estimate similarity between two pairs of unary
templates, and combine the two scores into a single score (Szpektor and Dagan 2008).
Feature representation. The features of a template are some function of the terms that
instantiated the argument variables in a corpus. Two representations that are used in
our experiments are derived from an ontology that maps natural language phrases to
semantic identifiers (see Section 5). Another variant occurs when using binary tem-
plates: a template may be represented by a pair of feature vectors, one for each variable
as in the DIRT algorithm (Lin and Pantel 2001), or by a single vector, where features
represent pairs of instantiations (Szpektor et al 2004; Yates and Etzioni 2009). The
former variant reduces sparsity problems, whereas Yates and Etzioni showed the latter
is more informative and performs favorably on their data.
Similarity function. We consider two similarity functions: The symmetric Lin (Lin
and Pantel 2001) similarity measure, and the directional BInc (Szpektor and Dagan
2008) similarity measure, reviewed in Section 2. Thus, information about the direction
of entailment is provided by the BInc measure.
We compute for any pair of templates (t1, t2) 12 distributional similarity scores using
all possible combinations of the aforementioned dimensions. These scores are then used
as 12 features representing the pair (t1, t2). (A full description of the features is given in
Section 5.) This is reminiscent of Connor and Roth (2007), who used the output of unsu-
pervised classifiers as features for a supervised classifier in a verb disambiguation task.
(4) Training a classifier Two types of classifiers may be trained in our scheme over
the training set: margin classifiers (such as SVM) and probabilistic classifiers. Given a
pair of templates (u, v) and their feature vector Fuv, we denote by an indicator variable
Iuv the event that u entails v. A margin classifier estimates a score Suv for the event
Iuv = 1, which indicates the positive or negative distance of the feature vector Fuv from
the separating hyperplane. A probabilistic classifier provides the posterior probability
Puv = P(Iuv = 1|Fuv).
4.2 Global Learning of Edges
In this step we get a set of propositional templates as input, and we would like to learn
all of the entailment relations between these propositional templates. For every pair of
templates we can compute the distributional similarity features and get a score from
the trained entailment classifier. Once all the scores are calculated we try to find the
optimal graph?that is, the best set of edges over the propositional templates. Thus, in
this scenario the input is the nodes of the graph and the output are the edges.
To learn edges we consider global constraints, which allow only certain graph
topologies. Because we seek a global solution under transitivity and other constraints,
ILP is a natural choice, enabling the use of state-of-the-art ILP optimization packages.
Given a set of nodes V and a weighting function f : V ? V ? R (derived from the
83
Computational Linguistics Volume 38, Number 1
entailment classifier in our case), we want to learn the directed graph G = (V, E), where
E = {(u, v)| Iuv = 1}, by solving the following ILP over the variables Iuv:
G? = argmax
G
?
u=v
f (u, v) ? Iuv (8)
s.t. ?u,v,w?V Iuv + Ivw ? Iuw ? 1 (9)
?u,v?Ayes Iuv = 1 (10)
?u,v?Ano Iuv = 0 (11)
?u=v Iuv ? {0, 1} (12)
The objective function in Equation (8) is simply a sum over the weights of the graph
edges. The global constraint is given in Equation (9) and states that the graph must
respect transitivity. This constraint is equivalent to the one suggested by Finkel and
Manning (2008) in a coreference resolution task, except that the edges of our graph
are directed. The constraints in Equations (10) and (11) state that for a few node pairs,
defined by the sets Ayes and Ano, respectively, we have prior knowledge that one node
does or does not entail the other node. Note that if (u, v) ? Ano, then due to transitivity
there must be no path in the graph from u to v, which rules out additional edge combi-
nations. We elaborate on how the sets Ayes and Ano are computed in our experiments in
Section 5. Altogether, this Integer Linear Program contains O(|V|2) variables and O(|V|3)
constraints, and can be solved using state-of-the-art optimization packages.
A theoretical aspect of this optimization problem is that it is NP-hard. We can phrase
it as a decision problem in the following manner: Given V, f , and a threshold k, we
wish to know if there is a set of edges E that respects transitivity and
?
(u,v)?E
f (u, v) ? k.
Yannakakis (1978) has shown that the simpler problem of finding in a graph G? =
(V?, E?) a subset of edges A ? E? that respects transitivity and |A| ? k is NP-hard. Thus,
we can conclude that our optimization problem is also NP-hard by the trivial poly-
nomial reduction defining the function f that assigns the score 0 for node pairs (u, v) /? E?
and the score 1 for node pairs (u, v) ? E?. Because the decision problem is NP-hard, it is
clear that the corresponding maximization problem is also NP-hard. Thus, obtaining a
solution using ILP is quite reasonable and in our experiments also proves to be efficient
(Section 5).
Next, we describe two ways of obtaining the weighting function f , depending on
the type of entailment classifier we prefer to train.
4.2.1 Score-Based Weighting Function. In this case, we assume that we choose to train a
margin entailment classifier estimating the score Suv (a positive score if the classifier
predicts entailment, and a negative score otherwise) and define f score(u, v) = Suv ? ?.
This gives rise to the following objective function:
G?score = argmax
G
?
u=v
(Suv ? ?) ? Iuv = argmax
G
?
?
?
u=v
Suv ? Iuv
?
?? ? ? |E| (13)
The term ? ? |E| is a regularization term reflecting the fact that edges are sparse. Intu-
itively, this means that we would like to insert into the graph only edges with a score
84
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Suv > ?, or in other words to ?push? the separating hyperplane towards the positive
half space by ?. Note that the constant ? is a parameter that needs to be estimated and
we discuss ways of estimating it in Section 5.2.
4.2.2 Probabilistic Weighting Function. In this case, we assume that we choose to train
a probabilistic entailment classifier. Recall that Iuv is an indicator variable denoting
whether u entails v, that Fuv is the feature vector for the pair of templates u and v, and de-
fine F to be the set of feature vectors for all pairs of templates in the graph. The classifier
estimates the posterior probability of an edge given its features: Puv = P(Iuv = 1|Fuv),
and we would like to look for the graph G that maximizes the posterior probability
P(G|F). In Appendix A we specify some simplifying independence assumptions under
which this graph maximizes the following linear objective function:
G?prob = argmax
G
?
u=v
(log
Puv
1 ? Puv
+ log?) ? Iuv = argmax
G
?
u=v
log
Puv
1 ? Puv
? Iuv + log? ? |E|
(14)
where ? = P(Iuv=1)P(Iuv=0) is the prior odds ratio for an edge in the graph, which needs to be
estimated in some manner. Thus, the weighting function is defined by fprob(u, v) =
log Puv1?Puv + log?.
Both the score-based and the probabilistic objective functions obtained are quite
similar: Both contain a weighted sum over the edges and a regularization component
reflecting the sparsity of the graph. Next, we show that we can provide a probabilistic
interpretation for our score-based function (under certain conditions), which will allow
us to use a margin classifier and interpret its output probabilistically.
4.2.3 Probabilistic Interpretation of Score-Based Weighting Function. We would like to use
the score Suv, which is bounded in (?,??), and derive from it a probability Puv. To
that end we project Suv onto (0, 1) using the sigmoid function, and define Puv in the
following manner:
Puv =
1
1 + exp(?Suv)
(15)
Note that under this definition the log probability ratio is equal to the inverse of the
sigmoid function:
log
Puv
1 ? Puv
= log
1
1+exp(?Suv )
exp(?Suv )
1+exp(?Suv )
= log 1
exp(?Suv)
= Suv (16)
Therefore, when we derive Puv from Suv with the sigmoid function, we can rewrite
G?prob as:
G?prob = argmax
G
?
u=v
Suv ? Iuv + log? ? |E| = G?score (17)
where we see that in this scenario the two objective functions are identical and the
regularization term ? is related to the edge prior odds ratio by: ? = ? log?.
85
Computational Linguistics Volume 38, Number 1
Moreover, assume that the score Suv is computed as a linear combination over n
features (such as a linear-kernel SVM), that is, Suv =
?n
i=1 S
i
uv ? ?i, where S
i
uv denotes
feature values and ?i denotes feature weights. In this case, the projected probability
acquires the standard form of a logistic classifier:
Puv =
1
1 + exp(?
n
?
i=1
Siuv ? ?i)
(18)
Hence, we can train the weights ?i using a margin classifier and interpret the output
of the classifier probabilistically, as we do with a logistic classifier. In our experiments
in Section 5 we indeed use a linear-kernel SVM to train the weights ?i and then we
can interchangeably interpret the resulting ILP as either score-based or probabilistic
optimization.
4.2.4 Comparison to Snow, Jurafsky, and Ng (2006). Our work resembles Snow, Jurafsky,
and Ng?s work in that both try to learn graph edges given a transitivity constraint. There
are two key differences in the model and in the optimization algorithm, however. First,
they employ a greedy optimization algorithm that incrementally adds hyponyms to a
large taxonomy (WordNet), whereas we simultaneously learn all edges using a global
optimization method, which is more sound and powerful theoretically, and leads to
the optimal solution. Second, Snow, Jurafsky, and Ng?s model attempts to determine
the graph that maximizes the likelihood P(F|G) and not the posterior P(G|F). If we cast
their objective function as an ILP we get a formulation that is almost identical to ours,
only containing the inverse prior odds ratio log 1? = ? log? rather than the prior odds
ratio as the regularization term (cf. Section 2):
G?Snow = argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv ? log? ? |E| (19)
This difference is insignificant when ? ? 1, or when ? is tuned empirically for optimal
performance on a development set. If, however, ? is statistically estimated, this might
cause unwarranted results: Their model will favor dense graphs when the prior odds
ratio is low (? < 1 or P(Iuv = 1) < 0.5), and sparse graphs when the prior odds ratio is
high (? > 1 or P(Iuv = 1) > 0.5), which is counterintuitive. Our model does not suffer
from this shortcoming because it optimizes the posterior rather than the likelihood. In
Section 5 we show that our algorithm significantly outperforms the algorithm presented
by Snow, Jurafsky, and Ng.
5. Experimental Evaluation
This section presents an evaluation and analysis of our algorithm.
5.1 Experimental Setting
A health-care corpus of 632MB was harvested from the Web and parsed using the Mini-
par parser (Lin 1998b). The corpus contains 2,307,585 sentences and almost 50 million
86
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 2
The similarity score features used to represent pairs of templates. The columns specify the
corpus over which the similarity score was computed, the template representation, the
similarity measure employed, and the feature representation (as described in Section 4.1).
# Corpus Template Similarity measure Feature representation
1 health-care binary BInc pair of CUI tuples
2 health-care binary BInc pair of CUIs
3 health-care binary BInc CUI tuple
4 health-care binary BInc CUI
5 health-care binary Lin pair of CUI tuples
6 health-care binary Lin pair of CUIs
7 health-care binary Lin CUI tuple
8 health-care binary Lin CUI
9 health-care unary BInc CUI tuple
10 health-care unary BInc CUI
11 health-care unary Lin CUI tuple
12 health-care unary Lin CUI
13 RCV1 binary Lin lexical items
14 RCV1 unary Lin lexical items
15 RCV1 unary BInc lexical items
16 Lin & Pantel binary Lin lexical items
word tokens. We used the Unified Medical Language System (UMLS)6 to annotate
medical concepts in the corpus. The UMLS is a database that maps natural language
phrases to over one million concept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases that are in the UMLS with their
(possibly multiple) CUIs. We now provide the details of training an entailment classifier
as explained in Section 4.1.
We extracted all templates from the corpus where both argument instantiations are
medical concepts, that is, annotated with a CUI (?50,000 templates). This was done to
increase the likelihood that the extracted templates are related to the health-care domain
and reduce problems of ambiguity.
As explained in Section 4.1, a pair of templates constitutes an input example for
the entailment classifier, and should be represented by a set of features. The features
we used were different distributional similarity scores for the pair of templates, as
summarized in Table 2. Twelve distributional similarity measures were computed over
the health-care corpus using the aforementioned variations (Section 4.1), where two
feature representations were considered: in the UMLS each natural language phrase
may be mapped not to a single CUI, but to a tuple of CUIs. Therefore, in the first
representation, each feature vector coordinate counts the number of times a tuple of
CUIs was mapped to the term instantiating the template argument, and in the second
representation it counts the number of times each single CUI was one of the CUIs
mapped to the term instantiating the template argument. In addition, we obtained the
original template similarity lists learned by Lin and Pantel (2001), and had available
three distributional similarity measures learned by Szpektor and Dagan (2008), over the
RCV1 corpus,7 as detailed in Table 2. Thus, each pair of templates is represented by a
total of 16 distributional similarity scores.
6 http://www.nlm.nih.gov/research/umls.
7 http://trec.nist.gov/data/reuters/reuters.html.
87
Computational Linguistics Volume 38, Number 1
We automatically generated a balanced training set of 20,144 examples using Word-
Net and the procedure described in Section 4.1, and trained the entailment classifier
with SVMperf (Joachims 2005). We use the trained classifier to obtain estimates for Puv
and Suv, given that the score-based and probabilistic scoring functions are equivalent
(cf. Section 4.2.3).
To evaluate the performance of our algorithm, we manually constructed gold-
standard entailment graphs. First, 23 medical target concepts, representing typical top-
ics of interest in the medical domain, were manually selected from a (longer) list of
the most frequent concepts in the health-care corpus. The 23 target concepts are: alcohol,
asthma, biopsy, brain, cancer, CDC, chemotherapy, chest, cough, diarrhea, FDA, headache, HIV,
HPV, lungs, mouth, muscle, nausea, OSHA, salmonella, seizure, smoking, and x-ray. For each
concept, we wish to learn a focused entailment graph (cf. Figure 1). Thus, the nodes of
each graph were defined by extracting all propositional templates in which the corre-
sponding target concept instantiated an argument at least K(= 3) times in the health-
care corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13).
Ten medical students were given the nodes of each graph (propositional templates)
and constructed the gold standard of graph edges using a Web interface. We gave an
oral explanation of the annotation process to each student, and the first two graphs
annotated by every student were considered part of the annotator training phase and
were discarded. The annotators were able to select every propositional template and
observe all of the instantiations of that template in our health-care corpus. For example,
selecting the template X helps with nausea might show the propositions relaxation helps
with nausea, acupuncture helps with nausea, and Nabilone helps with nausea. The concept
of entailment was explained under the framework of TE (Dagan et al 2009), that is, the
template t1 entails the template t2 if given that the instantiation of t1 with some concept
is true then the instantiation of t2 with the same concept is most likely true.
As explained in Section 3.2, we did not perform any disambiguation because a
target concept disambiguates the propositional templates in focused entailment graphs.
In practice, cases of ambiguity were very rare, except for a single scenario where in
templates such as X treats asthma, annotators were unclear whether X is a type of doctor
or a type of drug. The annotators were instructed in such cases to select the template,
read the instantiations of the template in the corpus, and choose the sense that is most
prevalent in the corpus. This instruction was applicable to all cases of ambiguity.
Each concept graph was annotated by two students. Following the current recog-
nizing TE (RTE) practice (Bentivogli et al 2009), after initial annotation the two students
met for a reconciliation phase. They worked to reach an agreement on differences and
corrected their graphs. Inter-annotator agreement was calculated using the kappa statis-
tic (Siegel and Castellan 1988) both before (? = 0.59) and after (? = 0.9) reconciliation.
Each learned graph was evaluated against the two reconciliated graphs.
Summing the number of possible edges over all 23 concept graphs we get 10,364
possible edges, of which 882 on average were included by the annotators (averaging
over the two gold-standard annotations for each graph). The concept graphs were
randomly split into a development set (11 concepts) and a test set (12 concepts).
We used the lpsolve8 package to learn the edges of the graphs. This package ef-
ficiently solves the model without imposing integer restrictions9 and then uses the
branch-and-bound method to find an optimal integer solution. We note that in the
8 http://lpsolve.sourceforge.net/5.5/.
9 While ILP is an NP-hard problem, LP is a polynomial problem and can be solved efficiently.
88
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
experiments reported in this article the optimal solution without integer restrictions
was already integer. Thus, although in general our optimization problem is NP-hard,
in our experiments we were able to reach an optimal solution for the input graphs
very efficiently (we note that in some scenarios not reported in this article the optimal
solution was not integer and so an integer solution is not guaranteed a priori).
As mentioned in Section 4.2, we added a few constraints in cases where there was
strong evidence that edges are not in the graph. This is done in the following scenarios
(examples given in Table 3): (1) When two templates u and v are identical except for
a pair of words wu and wv, and wu is an antonym of wv, or a hypernym of wv at
distance ? 2 in WordNet. (2) When two nodes u and v are transitive ?opposites,? that
is, if u = X
subj
??? w
obj
?? Y and v = X
obj
?? w
subj
??? Y, for any word w. We note that there are
some transitive verbs that express a reciprocal activity, such as X marries Y, but usually
reciprocal events are not expressed using a transitive verb structure.
In addition, in some cases we have strong evidence that edges do exist in the graph.
This is done in a single scenario (see Table 3), which is specific to the output of Minipar:
when two templates differ by a single edge and the first is of the type X
obj
?? Y and
the other is of the type X
vrel??? Y, which expresses a passive verb modifier of nouns.
Altogether, these initializations took place in less than 1% of the node pairs in the
graphs. We note that we tried to use WordNet relations such as hypernym and synonym
as ?positive? hard constraints (using the constraint Iuv = 1), but this resulted in reduced
performance because the precision of WordNet was not high enough.
The graphs learned by our algorithm were evaluated by two measures. The first
measure evaluates the graph edges directly, and the second measure is motivated by
semantic inference applications that utilize the rules in the graph. The first measure is
simply the F1 of the set of learned edges compared to the set of gold-standard edges.
In the second measure we take the set of learned rules and infer new propositions by
applying the rules over all propositions extracted from the health-care corpus. We apply
the rules iteratively over all propositions until no new propositions are inferred. For
example, given the corpus proposition relaxation reduces nausea and the edges X reduces
nausea ? X helps with nausea and X helps with nausea ? X related to nausea, we eval-
uate the set {relaxation reduces nausea, relaxation helps with nausea, relaxation related to
nausea}. For each graph we measure the F1 of the set of propositions inferred by the
learned graphs when compared to the set of propositions inferred by the gold-standard
graphs. For both measures the final score of an algorithm is a macro-average F1 over
the 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 test
concepts).
Table 3
Scenarios in which we added hard constraints to the ILP.
Scenario Example Initialization
antonym (X
subj
??? decrease
obj
?? Y,X
subj
??? increase
obj
?? Y) Iuv = 0
hypernym ? 2 (X
subj
??? affect
obj
?? Y,X
subj
??? irritate
obj
?? Y) Iuv = 0
transitive opposite (X
subj
??? cause
obj
?? Y,Y
subj
??? cause
obj
?? X) Iuv = 0
syntactic variation (X
subj
??? follow
obj
?? Y,Y
subj
??? follow vrel?? X) Iuv = 1
89
Computational Linguistics Volume 38, Number 1
Learning the edges of a graph given an input concept takes about 1?2 seconds on a
standard desktop.
5.2 Evaluated Algorithms
First, we describe some baselines that do not utilize the entailment classifier or the
ILP solver. For each of the 16 distributional similarity measures (Table 2) and for each
template t, we computed a list of templates most similar to t (or entailing t for directional
measures). Then, for each measure we learned graphs by inserting an edge (u, v), when
u is in the top K templates most similar to v. The parameter K can be optimized either on
the automatically generated training set (from WordNet) or on the manually annotated
development set. We also learned graphs using WordNet: We inserted an edge (u, v)
when u and v differ by a single word wu and wv, respectively, and wu is a direct hyponym
or synonym of wv. Next, we describe algorithms that utilize the entailment classifier.
Our algorithm, named ILP-Global, utilizes global information and an ILP formula-
tion to find maximum a posteriori graphs. Therefore, we compare it to the following
three variants: (1) ILP-Local: An algorithm that uses only local information. This is
done by omitting the global transitivity constraints, and results in an algorithm that
inserts an edge (u, v) if and only if (Suv ? ?) > 0. (2) Greedy-Global: An algorithm that
looks for the maximum a posteriori graphs but only employs the greedy optimization
procedure as described by Snow, Jurafsky, and Ng (2006). (3) ILP-Global-Likelihood:
An ILP formulation where we look for the maximum likelihood graphs, as described by
Snow, Jurafsky, and Ng (cf. Section 4.2).
We evaluate these algorithms in three settings which differ in the method by which
the edge prior odds ratio, ? (or ?), is estimated: (1) ? = 1 (? = 0), which means that
no prior is used. (2) Tuning ? and using the value that maximizes performance over the
development set. (3) Estimating ? using maximum likelihood over the development set,
which results in ? ? 0.1 (? ? 2.3), corresponding to the edge density P(Iuv = 1) ? 0.09.
For all local algorithms whose output does not respect transitivity constraints, we
added all edges inferred by transitivity. This was done because we assume that the rules
learned are to be used in the context of an inference or entailment system. Because such
systems usually perform chaining of entailment rules (Raina, Ng, and Manning 2005;
Bar-Haim et al 2007; Harmeling 2009), we conduct this chaining as well. Nevertheless,
we also measured performance when edges inferred by transitivity are not added: We
once again chose the edge prior value that maximizes F1 over the development set
and obtained macro-average recall/precision/F1 of 51.5/34.9/38.3. This performance is
comparable to the macro-average recall/precision/F1 of 44.5/45.3/38.1 we report next
in Table 4.
5.3 Experimental Results and Analysis
In this section we present experimental results and analysis that show that the
ILP-Global algorithm improves performance over baselines, specifically in terms of
precision.
Tables 4?7 and Figure 2 summarize the performance of the algorithms. Table 4
shows our main result when the parameters ? and K are optimized to maximize per-
formance over the development set. Notice that the algorithm ILP-Global-Likelihood
is omitted, because when optimizing ? over the development set it conflates with
ILP-Global. The rows Local1 and Local2 present the best algorithms that use a single
distributional similarity resource. Local1 and Local2 correspond to the configurations
90
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 4
Results when tuning for performance over the development set.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global (? = 0.45) 46.0 50.1 43.8 67.3 69.6 66.2
Greedy-Global (? = 0.3) 45.7 37.1 36.6 64.2 57.2 56.3
ILP-Local (? = 1.5) 44.5 45.3 38.1 65.2 61.0 58.6
Local1 (K = 10) 53.5 34.9 37.5 73.5 50.6 56.1
Local2 (K = 55) 52.5 31.6 37.7 69.8 50.0 57.1
Table 5
Results when the development set is not used to estimate ? and K.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 58.0 28.5 35.9 76.0 46.0 54.6
Greedy-Global 60.8 25.6 33.5 77.8 41.3 50.9
ILP-Local 69.3 19.7 26.8 82.7 33.3 42.6
Local1 (K = 100) 92.6 11.3 20.0 95.3 18.9 31.1
Local2 (K = 100) 63.1 25.5 34.0 77.7 39.9 50.9
WordNet 10.8 44.1 13.2 39.9 72.4 47.3
Table 6
Results with prior estimated on the development set, that is ? = 0.1, which is equivalent to
? = 2.3.
Edges Propositions
Recall Precision F1 Recall Precision F1
ILP-Global 16.8 67.1 24.4 43.9 86.8 56.3
ILP-Global-Likelihood 91.8 9.8 17.5 94.0 16.7 28.0
Greedy-Global 14.7 62.9 21.2 43.5 86.6 56.2
Greedy-Global-Likelihood 100.0 9.3 16.8 100.0 15.5 26.5
described in Table 2 by features no. 5 and no. 1, respectively (see also Table 8). ILP-
Global improves performance by at least 13%, and significantly outperforms all local
methods, as well as the greedy optimization algorithm both on the edges F1 measure
(p < 0.05) and on the propositions F1 measure (p < 0.01).
10
Table 5 describes the results when the development set is not used to estimate the
parameters ? and K: A uniform prior (Puv = 0.5) is assumed for algorithms that use
the entailment classifier, and the automatically generated training set is employed to
estimate K. Again ILP-Global-Likelihood is omitted in the absence of a prior. ILP-Global
outperforms all other methods in this scenario as well, although by a smaller margin
for a few of the baselines. Comparing Table 4 to Table 5 reveals that excluding the
10 We tested significance using the two-sided Wilcoxon rank test (Wilcoxon 1945).
91
Computational Linguistics Volume 38, Number 1
Table 7
Results per concept for the ILP-Global.
Concept R P F1
Smoking 58.1 81.8 67.9
Seizure 64.7 51.2 57.1
Headache 60.9 50.0 54.9
Lungs 50.0 56.5 53.1
Diarrhea 42.1 60.0 49.5
Chemotherapy 44.7 52.5 48.3
HPV 35.2 76.0 48.1
Salmonella 27.3 80.0 40.7
X-ray 75.0 23.1 35.3
Asthma 23.1 30.6 26.3
Mouth 17.7 35.5 23.7
FDA 53.3 15.1 23.5
sparse prior indeed increases recall at a price of a sharp decrease in precision. Note,
however, that local algorithms are more vulnerable to this phenomenon. This makes
sense because in local algorithms eliminating the prior adds edges that in turn add more
edges due to the constraint of transitivity and so recall dramatically rises at the expense
of precision. Global algorithms are not as prone to this effect because they refrain from
adding edges that eventually lead to the addition of many unwarranted edges.
Table 5 also shows that WordNet, a manually constructed resource, has notably
the highest precision and lowest recall. The low recall exemplifies how the entailment
relations given by the gold-standard annotators transcend much beyond simple lexical
relations that appear in WordNet: Many of the gold-standard entailment relations are
missing from WordNet or involve multi-word phrases that do not appear in WordNet
at all.
Note that although the precision of WordNet is the highest in Table 5, its absolute
value (44.1%) is far from perfect. This illustrates that hierarchies of predicates are quite
Figure 2
Recall-precision curve comparing ILP-Global with Greedy-Global and ILP-Local.
92
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 8
Results of all distributional similarity measures when tuning K over the development set.
We encode the description of the measures presented in Table 2 in the following manner?
h = health-care corpus; R = RCV1 corpus; b = binary templates; u = unary templates; L = Lin similarity
measure; B = BInc similarity measure; pCt = pair of CUI tuples representation; pC = pair of CUIs
representation; Ct = CUI tuple representation; C = CUI representation; Lin & Pantel = similarity
lists learned by Lin and Pantel.
Edges Propositions
Dist. sim. measure Recall Precision F1 Recall Precision F1
h-b-B-pCt 52.5 31.6 37.7 69.8 50.0 57.1
h-b-B-pC 50.5 26.5 30.7 67.1 43.5 50.1
h-b-B-Ct 10.4 44.5 15.4 39.1 78.9 51.6
h-b-B-C 7.6 42.9 11.1 37.9 79.8 50.7
h-b-L-pCt 53.4 34.9 37.5 73.5 50.6 56.1
h-b-L-pC 47.2 35.2 35.6 68.6 52.9 56.2
h-b-L-Ct 47.0 26.6 30.2 64.9 47.4 49.6
h-b-L-C 34.6 22.9 22.5 57.2 52.6 47.6
h-u-B-Ct 5.1 37.4 8.5 35.1 91.0 49.7
h-u-B-C 7.2 42.4 11.5 36.1 90.3 50.1
h-u-L-Ct 22.8 22.0 18.3 49.7 49.2 44.5
h-u-L-C 16.7 26.3 17.8 47.0 56.8 48.1
R-b-L-l 49.4 21.8 25.2 72.4 39.0 45.5
R-u-L-l 24.1 30.0 16.8 47.1 55.2 42.1
R-u-B-l 9.5 57.1 14.1 37.2 84.0 49.5
Lin & Pantel 37.1 32.2 25.1 58.9 54.6 48.6
ambiguous and thus using WordNet directly yields relatively low precision. WordNet
is vulnerable to such ambiguity because it is a generic domain-independent resource,
whereas our algorithm learns from a domain-specific corpus. For example, the words
have and cause are synonyms according to one of the senses in WordNet and so the
erroneous rule X have asthma ? X cause asthma is learned using WordNet. Another
example is the rule X follows chemotherapy ? X takes chemotherapy, which is incorrectly
inferred because follow is a hyponym of take according to one of WordNet?s senses (she
followed the feminist movement). Due to these mistakes made by WordNet, the precision
achieved by our automatically trained ILP-Global algorithm when tuning parameters
on the development set (Table 4) is higher than that of WordNet.
Table 6 shows the results when the prior ? is estimated using maximum likelihood
over the development set (by computing the edge density over all the development
set graphs), and not tuned empirically with grid search. This allows for a comparison
between our algorithm that maximizes the a posteriori probability and Snow, Jurafsky,
and Ng?s (2006) algorithm that maximizes the likelihood. The gold-standard graphs are
quite sparse (? ? 0.1); therefore, as explained in Section 4.2.4, the effect of the prior is
substantial. ILP-Global and Greedy-Global learn sparse graphs with high precision and
low recall, whereas ILP-Global-Likelihood and Greedy-Global-Likelihood learn dense
graphs with high recall but very low precision. Overall, optimizing the a posteriori
probability is substantially better than optimizing likelihood, but still leads to a large
degradation in performance. This can be explained because our algorithm is not purely
probabilistic: The learned graphs are the product of mixing a probabilistic objective
function with non-probabilistic constraints. Thus, plugging the estimated prior into this
model results in performance that is far from optimal. In future work, we will examine
93
Computational Linguistics Volume 38, Number 1
a purely probabilistic approach that will allow us to reach good performance when
estimating ? directly. Nevertheless, currently optimal results are achieved when the
prior ? is tuned empirically.
Figure 2 shows a recall?precision curve for ILP-Global, Greedy-Global, and ILP-
Local, obtained by varying the prior parameter, ?. The figure clearly demonstrates the
advantage of using global information and ILP. ILP-Global is better than Greedy-Global
and ILP-Local in almost every point of the recall?precision curve, regardless of the exact
value of the prior parameter. Last, we present for completeness in Table 7 the results of
ILP-Global for all concepts in the test set.
In Table 8 we present the results obtained for all 16 distributional similarity mea-
sures. The main conclusion we can derive from this table is that the best distributional
similarity measures are those that represent templates using pairs of argument instan-
tiations rather than each argument separately. A similar result was found by Yates and
Etzioni (2009), who described the RESOLVER paraphrase learning system and have
shown that it outperforms DIRT. In their analysis, they attribute this result to their
representation that utilizes pairs of arguments comparing to DIRT, which computes a
separate score for each argument.
In the next two sections we perform a more thorough qualitative and quantitative
comparison trying to analyze the importance of using global information in graph
learning (Section 5.3.1), as well as the contribution of using ILP rather than a greedy
optimization procedure (Section 5.3.2). We note that the analysis presented in both sec-
tions is for the results obtained when optimizing parameters over the development set.
5.3.1 Global vs. Local Information. We looked at all edges in the test-set graphs where
ILP-Global and ILP-Local disagree and checked which algorithm was correct. Table 9
presents the result. The main advantage of using ILP-Global is that it avoids inserting
wrong edges into the graph. This is because ILP-Local adds any edge (u, v) such that
Puv crosses a certain threshold, disregarding edges that will be consequently added due
to transitivity (recall that for local algorithms we add edges inferred by transitivity, cf.
Section 5.2). ILP-Global will avoid such edges of high probability if it results in inserting
many low probability edges. This results in an improvement in precision, as exhibited
by Table 4.
Figures 3 and 4 show fragments of the graphs learned by ILP-Global and ILP-
Local (prior to adding transitive edges) for the test-set concepts diarrhea and seizure,
and illustrate qualitatively how global considerations improve precision. In Figure 3,
we witness that the single erroneous edge X results in diarrhea ? X prevents diarrhea
inserted by the local algorithm because Puv is high, effectively bridges two strongly
connected components and induces a total of 12 wrong edges (all edges from the
upper component to the lower component), whereas ILP-Global refrains from inserting
this edge. Figure 4 depicts an even more complex scenario. First, ILP-Local induces
a strongly connected component of five nodes for the predicates control, treat, stop,
Table 9
Comparing disagreements between ILP-Global and ILP-Local against the gold-standard graphs.
Global=True/Local=False Global=False/Local=True
Gold standard=true 48 42
Gold standard=false 78 494
94
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 3
A comparison between ILP-Global and ILP-local for two fragments of the test-set concept
diarrhea.
reduce, and prevent, whereas ILP-Global splits this strongly connected component into
two, which although not perfect, is more compatible with the gold-standard graphs.
In addition, ILP-Local inserts four erroneous edges that connect two components of
size 4 and 5, which results in adding eventually 30 wrong edges. On the other hand,
Figure 4
A comparison between ILP-Global and ILP-Local for two fragments of the test-set concept
seizure.
95
Computational Linguistics Volume 38, Number 1
ILP-Global is aware of the consequences of adding these four seemingly good edges,
and prefers to omit them from the learned graph, leading to much higher precision.
Although the main contribution of ILP-Global, in terms of F1, is in an increase in
precision, we also notice an increase in recall in Table 4. This is because the optimal
prior is ? = 0.45 in ILP-Global but ? = 1.5 in ILP-Local. Thus, any edge (u, v) such that
0.45 < Suv < 1.5 will have positive weight in ILP-Global and might be inserted into the
graph, but will have negative weight in ILP-Local and will be rejected. The reason is that
in a local setting, reducing false positives is handled only by applying a large penalty
for every wrong edge, whereas in a global setting wrong edges can be rejected because
they induce more ?bad? edges. Overall, this leads to an improved recall in ILP-Global.
This also explains why ILP-Local is severely harmed when no prior is used at all, as
shown in Table 5.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 7 graphs with an average advantage of 11.7 points, ILP-Local achieves
better F1 over the edges in 4 graphs with an average advantage of 3.0 points, and one
performance is equal.
5.3.2 Greedy vs. Non-Greedy Optimization. We would like to understand how using an
ILP solver improves performance compared with a greedy optimization procedure.
Table 4 demonstrates that ILP-Global and Greedy-Global reach a similar level of re-
call, although ILP-Global achieves far better precision. Again, we investigated edges
for which the two algorithms disagree and checked which one was correct. Table 10
demonstrates that the higher precision is because ILP-Global avoids inserting wrong
edges into the graph.
Figure 5 illustrates some of the reasons ILP-Global performs better than Greedy-
Global. Parts A1?A3 show the progression of Greedy-Global, which is an incremental
algorithm, for a fragment of the headache graph. In part A1 the learning algorithm still
separates the nodes X prevents headache and X reduces headache from the nodes X causes
headache and X results in headache (nodes surrounded by a bold oval shape constitute
a strongly connected component). After two iterations, however, the four nodes are
joined into a single strongly connected component, which is an error in principle
but at this point seems to be the best decision to increase the posterior probability
of the graph. This greedy decision has two negative ramifications. First, the strongly
connected component can no longer be untied. Thus, in A3 we observe that in future
iterations the strongly connected component expands further and many more wrong
edges are inserted into the graph. On the other hand, in B we see that ILP-Global takes
into consideration the global interaction between the four nodes and other nodes of the
graph, and decides to split this strongly connected component in two, which improves
the precision of ILP-Global. Second, note that in A3 the nodes Associate X with headache
and Associate headache with X are erroneously isolated. This is because connecting them
to the strongly connected component that contains six nodes will add many edges with
Table 10
Comparing disagreements between ILP-Global and Greedy-Global against the gold-standard
graphs.
ILP=True/Greedy=False ILP=False/Greedy=True
Gold standard=true 66 56
Gold standard=false 44 480
96
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 5
A comparison between ILP-Global and Greedy-Global. Parts A1?A3 depict the incremental
progress of Greedy Global for a fragment of the headache graph. Part B depicts the corresponding
fragment in ILP-Global. Nodes surrounded by a bold oval shape are strongly connected
components.
low probability and so this is avoided by Greedy-Global. Because in ILP-Global the
strongly connected component was split in two, it is possible to connect these two nodes
to some of the other nodes and raise the recall of ILP-Global. Thus, we see that greedy
optimization might get stuck in local maxima and consequently suffer in terms of both
precision and recall.
Last, we note that across the 12 test-set graphs, ILP-Global achieves better F1 over
the edges in 9 graphs with an average advantage of 10.0 points, Greedy-Global achieves
better F1 over the edges in 2 graphs with an average advantage of 1.5 points, and in one
case performance is equal.
5.4 Error Analysis
In this section, we compare the results of ILP-Global with the gold-standard graphs
and perform error analysis. Error analysis was performed by comparing the 12 graphs
learned by ILP-Global to the corresponding 12 gold-standard graphs (randomly sam-
pling from the two available gold-standard graphs), and manually examining all edges
for which the two disagree. We found that the number of false positives and false
negatives is almost equal: 282 edges were learned by ILP-Global but are not in the gold-
standard graphs (false positive) and 287 edges were in the gold-standard graphs but
were not learned by ILP-Global (false negatives).
97
Computational Linguistics Volume 38, Number 1
Table 11
Error analysis for false positives and false negatives.
False positives False negatives
Total count 282 Total count 287
Classifier error 84.8% Classifier error 73.5%
Co-hyponym error 18.0% Long-predicate error 36.2%
Direction error 15.1% Generality error 26.8%
String overlap error 20.9%
Table 11 presents the results of our manual error analysis. Most evident is the fact
that the majority of mistakes are misclassifications of the entailment classifier. For 73.5%
of the false negatives the classifier?s probability was Puv < 0.5 and for 84.8% of the false
positives the classifier?s probability was Puv > 0.5. This shows that our current classifier
struggles to distinguish between positive and negative examples. Figure 6 illustrates
some of this difficulty by showing the distribution of the classifier?s probability, Puv,
over all node pairs in the 12 test-set graphs. Close to 80% of the scores are in the range
0.45?0.5, most of which are simply node pairs for which all distributional similarity
features are zero. Although in the great majority of such node pairs (t1, t2) t1 indeed
does not entail t2, there are also some cases where t1 does entail t2. This implies that the
current feature representation is not rich enough, and in the next section we explore a
larger feature set.
Table 11 also shows some other reasons found for false positives. Many false posi-
tives are pairs of predicates that are semantically related, that is, 18% of false positives
are templates that are hyponyms of a common predicate (co-hyponym error), and 15.1%
of false positives are pairs where we err in the direction of entailment (direction error).
For example ILP-Global learns that place X in mouth ? remove X from mouth, which is a
Figure 6
Distribution of probabilities given by the classifier over all node pairs of the test-set graphs.
98
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
co-hyponym error, and also that X affects lungs ? X damages lungs, which is a direction
error because entailment holds in the other direction. This illustrates the infamous
difficulty of distributional similarity features to discern the type of semantic relation
between two predicates.
Table 11 also shows additional reasons for false negatives. We found that in 36.2% of
false negatives one of the two templates contained a ?long? predicate, that is a predicate
composed of more than one content word, such as Ingestion of X causes injury to Y. This
might indicate that the size of the health-care corpus is too small to collect sufficient
statistics for complex predicates. In addition, 26.8% of false negatives were manually
analyzed as ?generality errors.? An example is the edge HPV strain causes X ? associate
HPV with X that is in the gold-standard graph but was missed by ILP-Global. Indeed,
this edge falls within the definition of textual entailment and is correct: For example,
if some HPV strain causes cervical cancer then cervical cancer is associated with HPV.
Because the entailed template is much more general than the entailing template, how-
ever, they are not instantiated by similar arguments in the corpus and distributional
similarity features fail to capture their semantic similarity. Last, we note that in 20.9%
of the false negatives, there was some string overlap between the entailing and entailed
templates, for example in X controls asthma symptoms ? X controls asthma. In the next
section we experiment with a feature that is based on string similarity.
Tables 8 and 9 show that there are cases where ILP-Global makes a mistake, whereas
ILP-Local or Greedy-Global are correct. An illustrating example for such a case is
shown in Figure 7. Looking at ILP-Local we see that the entailment classifier correctly
classifies the edges X triggers asthma ? X causes asthma and X causes asthma ? Associate
X with asthma, but misclassifies X triggers asthma ? Associate X with asthma. Because this
configuration violates a transitivity constraint, ILP-Global must make a global decision
whether to add the edge X triggers asthma ? Associate X with asthma or to omit one of
Figure 7
A scenario where ILP-Global makes a mistake, but ILP-Local is correct.
99
Computational Linguistics Volume 38, Number 1
the correct edges. The optimal global decision in this case causes a mistake with respect
to the gold standard. More generally, a common phenomenon of ILP-Global is that it
splits components that are connected in ILP-Local, for example, in Figures 3 and 4. ILP-
Global splits the components in a way that is optimal according to the scores of the local
entailment classifier, but these are not always accurate according to the gold standard.
Figure 5 exemplifies a scenario where ILP-Global errs, but Greedy-Global is (partly)
correct. ILP-Global mistakenly learns entailment rules from the templates Associate
X with headache and Associate headache with X to the templates X causes headache and
X results in headache, whereas Greedy-Global isolates the templates Associate X with
headache and Associate headache with X in a separate component. This happens because
of the greedy nature of Greedy-Global. Notice that in step A2 the templates X causes
headache and X results in headache are already included (erroneously) in a connected
component with the templates X prevents headache and X reduces headache. Thus, adding
the rules from Associate X with headache and Associate headache with X to X causes headache
and X results in headache would also add the rules to X reduces headache and X prevents
headache and the Greedy-Global avoids that. ILP-Global does not have that problem: It
simply chooses the optimal choice according to the entailment classifier, which splits the
connected component presented in A2. Thus, once again we see that mistakes made by
ILP-Global are often due to the inaccuracies of the scores given by the local entailment
classifier.
6. Local Classifier Extensions
The error analysis in Section 5.4 exemplified that most errors are the result of misclassi-
fications made by the local entailment classifier. In this section, we investigate the local
entailment classifier component, focusing on the set of features used for classification.
We first present an experimental setting in which we consider a wider set of features,
then we present the results of the experiment, and last we perform feature analysis and
draw conclusions.
6.1 Feature Set and Experimental Setting
In previous sections we employed a distant supervision framework: We generated
training examples automatically with WordNet, and represented each example with
distributional similarity features. Distant supervision comes with a price, however?it
prevents us from utilizing all sources of information. For example, looking at the pair of
gold-standard templates X manages asthma and X improves asthma management, one can
exploit the fact that management is a derivation of manage to improve the estimation of
entailment. The automatically generated training set was generated by looking at Word-
Net?s hypernym, synonym, and co-hyponyms relations, however, and hence no such
examples appear in the training set, rendering this type of feature useless. Moreover,
one cannot use WordNet?s hypernym, synonym, and co-hyponym relations as features
because the generated training set is highly biased?all positive training examples are
either hypernyms or synonyms and all negative examples are co-hyponyms.
In this section we would like to examine the utility of various features, while avoid-
ing the biases that occur due to distant supervision. Therefore, we use the 23 manually
annotated gold-standard graphs for both training and testing, in a cross-validation
setting. Although this reduces the size of the training set it allows us to estimate the
utility of various features in a setting where the training set and test set are sampled
from the same underlying distribution, without the aforementioned biases.
100
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
We would like to extract features that express information that is diverse and
orthogonal to the one given by distributional similarity. Therefore, we turn to existing
knowledge resources that were created using both manual and automatic methods,
expressing various types of linguistic and statistical information that is relevant for
entailment prediction:
1. WordNet: contains manually annotated relations such as hypernymy,
synonymy, antonymy, derivation, and entailment.
2. VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such as
stronger-than and similar that were learned with pattern-based methods.
3. CATVAR12 (Habash and Dorr 2003): contains word derivations such as
develop?development.
4. FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment
rules between templates learned automatically from FrameNet.
5. NomLex14 (Macleod et al 1998): contains English nominalizations
including their argument mapping to the corresponding verbal form.
6. BAP15 (Kotlerman et al 2010): contains directional distributional
similarity scores between lexical terms (rather than propositional
templates) calculated with the BAP similarity scoring function.
Table 12 describes the 16 new features that were generated for each of the gold-
standard examples (resulting in a total of 32 features). The first 15 features were gen-
erated by the aforementioned knowledge bases. The last feature measures the edit
distance between templates: Given a pair of templates (t1, t2), we concatenate the words
in each template and derive a pair of strings (s1, s2). Then we compute the Levenshtein
string edit-distance (Cohen, Ravikumar, and Fienberg 2003) between s1 and s2 and
divide the score by |s1|+ |s2| for normalization.
Table 12 also describes for each feature the number and percentage of examples for
which the feature value is non-zero (out of the examples generated from the 23 gold-
standard graphs). A salient property of many of the new features is that they are sparse:
The four antonymy features as well as the Derivation, Entailment, Nomlex, and FRED
features occur in very few examples in our data set, which might make training with
these features difficult.
After generating the new features we employ a leave-one-graph-out strategy to
maximally exploit the manually annotated gold standard for training. For each of the
test-set graphs, we train over all development and test-set graphs except for the one
that is left out,16 after tuning the algorithm?s parameters and test. Parameter tuning is
done by cross-validation over the development set, tuning to maximize the F1 of the set
11 http://demo.patrickpantel.com/demos/verbocean/.
12 http://clipdemos.umiacs.umd.edu/catvar/.
13 http://u.cs.biu.ac.il/?nlp/downloads/FRED.html.
14 http://nlp.cs.nyu.edu/nomlex/index.html.
15 http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html.
16 As described in Section 5, we train with a balanced number of positive and negative examples. Because
the number of positive examples in the gold standard is smaller than the number of negative examples,
we use all positives and randomly sample the same number of negatives, resulting in ? 1, 500 training
examples.
101
Computational Linguistics Volume 38, Number 1
Table 12
The set of new features. The last two columns denote the number and percentage of examples
for which the value of the feature is non-zero in examples generated from the 23 gold-standard
graphs.
Name Type Description # %
Hyper. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a hypernym (distance ? 2) of w1
in WordNet.
120 1.1
Syno. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a synonym of w1 in WordNet.
94 0.9
Co-hypo. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is a co-hyponym of w1 in WordNet.
302 2.8
WN Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in WordNet.
6 0.06
VO Ant. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is an antonym of w1 in VerbOcean.
25 0.2
WN Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in WordNet.
22 0.2
VO Ant. 2 boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is an antonym of w1 in VerbOcean.
73 0.7
Derivation boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is a derivation of w1 in WordNet or CATVAR.
78 0.7
Entailment boolean Whether there exists in (t1, t2) a pair of words (w1, w2)
such that w2 is entailed by w1 in WordNet.
20 0.2
FRED boolean Whether t1 entails t2 in FRED. 9 0.08
Nomlex boolean Whether t1 entails t2 in Nomlex. 8 0.07
VO strong boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is stronger than w1 in VerbOcean.
104 1
VO simil. boolean Whether (t1, t2) are identical except for a pair of words
(w1, w2) such that w2 is similar to w1 in VerbOcean.
191 1.8
Positive boolean Disjunction of the features Hypernym, Synonym, Nom-
lex, and VO stronger.
289 2.7
BAP real maxw1?t1,w2?t2BAP(w1, w2). 506 4.7
Edit real Normalized edit-distance. 100
of learned edges (the development and test set are described in Section 5). Graphs are
always learned with the LP-Global algorithm.
Our main goal is to check whether the added features improve performance, and
therefore we run the experiment both with and without the new features. In addi-
tion, we would like to test whether using different classifiers affects performance.
Therefore, we run the experiments with a linear-kernel SVM, a square-kernel SVM,
a Gaussian-kernel SVM, logistic regression, and naive Bayes. We use the SVMPerf
package (Joachims 2005) to train the SVM classifiers and the Weka package (Hall et al
2009) for logistic regression and naive Bayes.
6.2 Experiment Results
Table 13 describes the macro-average recall, precision, and F1 of all classifiers both with
and without the new features on the development set and test set. Using all features is
denoted by Xall, and using the original features is denoted by Xold.
102
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Table 13
Macro-average recall, precision, and F1 on the development set and test set using the parameters
that maximize F1 of the learned edges over the development set.
Development set Test set
Algorithm Recall Precision F1 Recall Precision F1
Linearall 48.1 31.9 36.3 51.7 37.7 40.3
Linearold 40.3 33.3 34.8 47.2 42.2 41.1
Gaussianall 41.8 32.4 35.1 48.0 41.1 40.7
Gaussianold 41.1 31.2 33.9 50.3 39.7 40.5
Squareall 39.9 32.0 34.1 43.7 39.8 38.9
Squareold 38.0 31.6 32.9 50.2 41.0 41.3
Logisticall 34.4 27.6 29.1 39.8 41.7 37.8
Logisticold 39.3 31.2 33.5 45.4 40.9 39.9
Bayesall 20.8 33.2 24.5 27.4 46.0 31.7
Bayesold 20.3 34.9 24.6 26.4 45.4 30.9
Examining the results it does not appear that the new features improve perfor-
mance. Whereas on the development set the new features add 1.2?1.5 F1 points for all
SVM classifiers, on the test set using the new features decreases performance for the
linear and square classifiers. This shows that even if there is some slight increase in
performance when using SVM on the development set, it is masked by the variance
added in the process of parameter tuning. In general, including the new features does
not yield substantial differences in performance.
Secondly, the SVM classifiers perform better than the logistic and naive Bayes clas-
sifiers. Using the more complex square and Gaussian kernels does not seem justified,
however, as the differences between the various kernels are negligible. Therefore, in our
analysis we will use a linear kernel SVM classifier.
Last, we note that although we use supervised learning rather than distant super-
vision, the results we get are slightly lower than those presented in Section 5. This is
probably due to the fact that our manually annotated data set is rather small. Nev-
ertheless, this shows that the quality of the distant supervision training set generated
automatically from WordNet is reasonable.
Next, we perform analysis of the different features of the classifier to better under-
stand the reasons for the negative result obtained.
6.3 Feature Analysis
We saw that the new features slightly improved performance for SVM classifiers on
the development set, although no clear improvement was witnessed on the test set.
To further check whether the new features carry useful information we measured the
training set accuracy for each of the 12 training sets (leaving out each time one test-
set graph). Using the new features improved the average training set accuracy from
71.6 to 72.3. More importantly, it improved performance consistently in all 12 training
sets by 0.4?1.2 points. This strengthens our belief that the new features do carry a
certain amount of information, but this information is too sparse to affect the overall
103
Computational Linguistics Volume 38, Number 1
performance of the algorithm. In addition, notice that the absolute accuracy on the
training set is low?72.3. This shows that separating entailment from non-entailment
using the current set of features is challenging.
Next, we would like to perform analysis on each of the features. First, we perform
an ablation test over the features by omitting each one of them and re-training the
classifier Linearall. In Table 14, the columns ablation F1 and ? show the F1 obtained and
the difference in performance from the Linearall classifier, which scored 40.3 F1 points.
Results show that there is no ?bad? feature that deteriorates performance. For almost all
features ablation causes a decrease in performance, although this decrease is relatively
small. There are only four features for which ablation decreases performance by more
than one point: three distributional similarity features, but also the new hypernym
feature.
The next three columns in the table describe the precision and recall of the new
boolean features. The column Feature type indicates whether we expect a feature to
indicate entailment or non-entailment and the columns Prec. and Recall specify the
Table 14
Results of feature analysis. The second column denotes the proportion of manually annotated
examples for which the feature value is non-zero. A detailed explanation of the other columns is
provided in the body of the article.
Feature name % Ablation F1 ? Feature type Prec. Recall Classification F1
h-b-B-pCt 8.2 39.3 ?1 14.9
h-b-B-pC 6.9 39.5 ?0.8 33.2
h-b-B-Ct 1.6 40.3 0 15.4
h-b-B-C 1.6 40.5 0.2 11.2
h-b-L-pCt 23.6 38.3 ?2.0 37.0
h-b-L-pC 21.4 39.4 ?0.9 35.2
h-b-L-Ct 9.7 40.1 ?0.2 27.3
h-b-L-C 8.1 39.7 ?0.6 14.1
h-u-B-Ct 1.0 39.4 ?0.9 10.9
h-u-B-C 1.1 39.8 ?0.5 12.6
h-u-L-Ct 6.1 39.8 ?0.5 18.5
h-u-L-C 6.3 39.2 ?1.1 19.3
R-b-L-l 22.5 40.1 ?0.2 26.7
R-u-L-l 8.3 39.4 ?0.9 23.2
R-u-B-l 1.9 39.8 ?0.5 16.7
Lin & Pantel 8.8 38.7 ?1.6 23.0
Hyper. 1.1 38.7 ?1.6 + 37.1 4.9 9.7
Syno. 0.9 40.3 0 + 43.1 4.5 15.8
Co-hypo. 2.8 40.1 ?0.2 ? 82.0 2.5 17.9
WN ant. 0.06 39.8 ?0.5 ? 75.0 0.05 1.2
VO ant. 0.2 40.1 ?0.2 ? 96.0 0.2 2.2
WN ant. 2. 0.2 39.4 ?0.9 ? 59.1 0.1 2.7
VO ant. 2 0.7 40.2 ?0.1 ? 98.6 0.7 2.2
Derivation 0.7 39.5 ?0.8 + 47.4 4.1 10.2
Entailment 0.2 39.7 ?0.6 + 15.0 0.3 1.2
FRED 0.08 39.7 ?0.6 + 77.8 0.8 3.2
NomLex 0.07 39.8 ?0.5 + 75.0 0.7 3.3
VO strong. 1 39.4 ?0.9 + 34.6 4 6.9
VO simil. 1.8 39.4 ?0.9 + 28.8 6.1 12.5
Positive 2.7 39.8 ?0.5 + 36.7 11.8
BAP 4.7 40.1 ?0.2 13.3
Edit 100 39.9 ?0.4 15.5
104
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
precision and recall of that feature. For example, the feature FRED is a positive feature
that we expect to support entailment, and indeed 77.8% of the gold-standard examples
for which it is turned on are positive examples. It is turned on only in 0.8% of the
positive examples, however. Similarly, VO ant. is a negative feature that we expect to
support non-entailment, and indeed 96% of the gold-standard examples for which it is
on are negative examples, but it is turned on in only 0.2% of the negative examples.
The precision results are quite reasonable: For most positive features the precision is
well over the proportion of positive examples in the gold standard, which is about
10% (except for the Entailment feature whose precision is only 15%). For the negative
features it seems that the precision of VerbOcean features is very high (though they are
sparse), and the precision of WordNet antonyms and co-hyponyms is lower. Looking
at the recall we can see that the coverage of the boolean features is low.
The last column in the table describes results of training the classifier with a single
feature. For each feature we train a linear kernel SVM, tune the sparsity parameter on
the development set, and measure F1 over the test set. Naturally, classifiers that are
trained on sparse features yield low performance.
This column allows us once again (cf. Table 8) to examine the original distributional
similarity features. There are three distributional similarity features that achieve F1 of
more than 30 points, and all three represent features using pairs of argument instan-
tiations rather than treat each argument separately, as we have already witnessed in
Section 5.
Note also that the feature h-b-L-pCt, which uses binary templates, the Lin similarity
measure, and features that are pairs of CUI tuples, is the best feature both in terms of the
ablation test and when it is used as a single feature for the classifier. The result obtained
by this feature is only 3.3 points lower than that obtained when using the entire feature
set. We believe this is for two reasons: First, the 16 distributional similarity features are
correlated with one another and thus using all of them does not boost performance
substantially. For example, the Pearson correlation coefficients between the features
h-b-B-pCt, h-b-B-Ct, h-b-L-pCt, h-b-L-Ct, h-u-B-Ct, and h-u-L-Ct (all utilize CUI tuples) and
h-b-B-pC, h-b-B-C, h-b-L-pC, h-b-L-C, h-u-B-C, and h-u-L-C (all use CUIs), respectively, are
over 0.9. The second reason for gaining only 3.3 points by the remaining features is that,
as discussed, the new set of features is relatively sparse.
To sum up, we suggest several hypotheses that explain our results and analysis:
 The new features are too sparse to substantially improve the performance
of the local entailment classifier in our data set. This perhaps can be
attributed to the nature of our domain-specific health-care corpus. In the
future, we would like to examine the sparsity of these features in a general
domain.
 Looking at the training set accuracy, ablations, and precision of the new
features, it seems that the behavior of most of them is reasonable. Thus,
it is possible that in a different learning scheme that does not use the
resources as features the information they provide may become beneficial.
For example, in a simple ?back-off? approach one can use rules from
precise resources to determine entailment, and apply a classifier only
when no precise resource contains a relevant rule.
 In our corpus representing distributional similarity features with
pairs of argument instantiations is better than treating each argument
independently.
105
Computational Linguistics Volume 38, Number 1
 Given the current training set accuracy and the sparsity of the new
features, it is important to develop methods that gather large-scale
information that is orthogonal to distributional similarity. In our opinion,
the most promising direction for acquiring such rich information is by
methods that look at co-occurrence of predicates or templates on the Web
(Chklovski and Pantel 2004; Pekar 2008).
7. Conclusions and Future Work
This article presented a global optimization algorithm for learning entailment rules
between predicates, represented as propositional templates. Most previous work on
learning entailment rules between predicates focused on local learning methods, which
consider each pair of predicates in isolation. To the best of our knowledge, this is the
most comprehensive attempt to date to exploit global interactions between predicates
for improving the set of learned entailment rules.
We modeled the problem as a graph learning problem, and searched for the best
graph under a global transitivity constraint. Two objective functions were defined for
the optimization procedure, one score-based and the other probabilistic, and we have
shown that under certain conditions (specified in Appendix A) the score-based function
can be interpreted probabilistically. This allowed us to use both margin as well as
probabilistic classifiers for the underlying entailment classifier. We solved the optimiza-
tion problem using Integer Linear Programming, which provides an optimal solution
(compared to the greedy algorithm suggested by Snow, Jurafsky, and Ng [2006]), and
demonstrated empirically that this method outperforms local algorithms as well as
a state-of-the-art greedy optimization algorithm on the graph learning task. We also
analyzed quantitatively and qualitatively the reasons for the improved performance of
our global algorithm and performed detailed error analysis. Last, we experimented with
various entailment classifiers that utilize different sets of features from many knowledge
bases.
The experiments and analysis performed indicate that the current performance of
the local entailment classifier needs to be improved. We believe that the most promising
direction for improving the local classifier is to use methods that look for co-occurrence
of predicates in sentences or documents on the Web, because these methods excel at
identifying specific semantic relations. It is also possible to use other sources of infor-
mation such as lexicographic resources, although this probably will require a learning
scheme that is robust to the relatively low coverage of these resources. Increasing the
size of the training corpus is also an important direction for improving the entailment
classifier.
Another important direction for future work is to apply our algorithm to graphs
that are larger by a few orders of magnitude than the focused entailment graphs dealt
with in this article. This will introduce a challenge to our current optimization algorithm
due to complexity issues, as our ILP contains O(|V|3) constraints. In addition, this will
require careful handling of predicate ambiguity, which interferes with the transitivity
of entailment and will become a pertinent issue in large graphs. Some first steps in this
direction have already been carried out (Berant, Dagan, and Goldberger 2011).
In addition, our graphs currently contain a single type of edge, namely, the entail-
ment relation. We would like to model more types of edges in the graph, representing
additional semantic relations such as co-hyponymy, and to explicitly describe the inter-
actions between the various types of edges, aiming to further improve the quality of the
learned entailment rules.
106
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Figure 8
A hierarchical summary of propositions involving nausea as an argument, such as headache is
related to nausea, acupuncture helps with nausea, and Lorazepam treats nausea.
Last, in Section 3.1 we mentioned that by merging strongly connected components
in entailment graphs, hierarchies of predicates can be generated (recall Figure 1). As
proposed by Berant, Dagan, and Goldberger (2010), we believe that these hierarchies can
be useful not only in the context of semantic inference applications, but also in the field
of faceted search and hierarchical text exploration (Stoica, Hearst, and Richardson 2007).
Figure 8 exemplifies how a set of propositions can be presented to a user according to
the hierarchy of predicates shown in Figure 1. In the field of faceted search, information
is presented using a number of hierarchies, corresponding to different facets or dimen-
sions of the data. One can easily use the hierarchy of predicates learned by our algorithm
as an additional facet in the context of a text-exploration application. In future work,
we intend to implement this application and perform user experiments to test whether
adding this hierarchy facilitates exploration of textual information.
Appendix A: Derivation of the Probabilistic Objective Function
In this section we provide a full derivation for the probabilistic objective function
given in Section 4.2.2. Given two nodes u and v from a set of nodes V, we denote by
Iuv = 1 the event that u entails v, by Fuv the feature vector representing the ordered
pair (u, v), and by F the set of feature vectors over all ordered pairs of nodes, that is,
F = ?u=vFuv. We wish to learn a set of edges E, such that the posterior probability P(G|F)
is maximized, where G = (V, E). We assume that we have a ?local? model estimating the
edge posterior probability Puv = P(Iuv = 1|Fuv). Because this model was trained over a
balanced training set, the prior for the event that u entails v under the model is uniform:
P(Iuv = 1) = P(Iuv = 0) =
1
2 . Using Bayes?s rule we get:
P(Iuv = 1|Fuv) =
P(Iuv = 1)
P(Fuv)
? P(Fuv|Iuv = 1) = a ? P(Fuv|Iuv = 1) (A.1)
P(Iuv = 0|Fuv) =
P(Iuv = 0)
P(Fuv)
? P(Fuv|Iuv = 0) = a ? P(Fuv|Iuv = 0) (A.2)
107
Computational Linguistics Volume 38, Number 1
where a = 12?P(Fuv ) is a constant with respect to any graph. Thus, we conclude that
P(Iuv|Fuv) = a ? P(Fuv|Iuv). Next, we make three independence assumptions (the first two
are following Snow, Jurafsky, and Ng [2006]):
P(F|G) =
?
u=v
P(Fuv|G) (A.3)
P(Fuv|G) = P(Fuv|Iuv) (A.4)
P(G) =
?
u=v
P(Iuv) (A.5)
Assumption A.3 states that each feature vector is independent from other feature
vectors given the graph. Assumption A.4 states that the features Fuv for the pair (u, v)
are generated by a distribution depending only on whether entailment holds for (u, v).
Last, Assumption A.5 states that edges are independent and the prior probability of a
graph is a product of the prior probabilities of the edges. Using these assumptions and
equations A.1 and A.2, we can now express the posterior P(G|F):
P(G|F) ? P(G) ? P(F|G) (A.6)
=
?
u=v
[P(Iuv) ? P(Fuv|Iuv)] (A.7)
=
?
u=v
P(Iuv) ?
P(Iuv|Fuv)
a (A.8)
?
?
u=v
P(Iuv) ? Puv (A.9)
=
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.10)
Note that under the ?local model? the prior for an edge in the graph was uniform,
because the model was trained over a balanced training set. Generally, however, this is
not the case, and thus we introduce an edge prior into the model when formulating the
global objective function. Now, we can formulate P(G|F) as a linear function:
G? = argmax
G
?
(u,v)?E
P(Iuv = 1) ? Puv ?
?
(u,v)/?E
P(Iuv = 0) ? (1 ? Puv) (A.11)
= argmax
G
?
(u,v)?E
log(Puv ? P(Iuv = 1)) +
?
(u,v)/?E
log[(1 ? Puv) ? P(Iuv = 0)] (A.12)
= argmax
G
?
u=v
(
Iuv ? log(Puv ? P(Iuv = 1)) + (1 ? Iuv) ? log[(1 ? Puv) ? P(Iuv = 0)]
)
(A.13)
= argmax
G
?
u=v
(
log
Puv ? P(Iuv = 1)
(1 ? Puv) ? P(Iuv = 0)
? Iuv + (1 ? Puv) ? P(Iuv = 0)
)
(A.14)
108
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
= argmax
G
?
u=v
log
Puv
(1 ? Puv)
? Iuv + log? ? |E| (A.15)
In the last transition we omit
?
u=v(1 ? Puv) ? P(Iuv = 0), which is a constant with
respect to the graph and denote the prior odds ratio by ? = P(Iuv=1)P(Iuv=0) . This leads to the
final formulation described in Section 4.2.2.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel, and the anonymous reviewers for
their useful comments. We also thank Dafna
Berant and the nine students who prepared
the gold-standard data set. This work was
developed under the collaboration of
FBK-irst/University of Haifa and was
partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship, and has
carried out this research in partial fulfilment
of the requirements for the Ph.D. degree.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of the
ACL, pages 399?406, Barcelona.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley framenet
project. In Proceedings of COLING-ACL,
pages 86?90, Montreal.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic inference
at the lexical-syntactic level. In Proceedings
of AAAI, pages 871?876, Vancouver.
Ben Aharon, Roni, Idan Szpektor, and Ido
Dagan. 2010. Generating entailment rules
from framenet. In Proceedings of ACL,
pages 241?246, Uppsala.
Bentivogli, Luisa, Ido Dagan, Hoa Trang
Dang, Danilo Giampiccolo, and Bernarde
Magnini. 2009. The fifth Pascal recognizing
textual entailment challenge. In Proceedings
of TAC-09, pages 14?24, Gaithersburg, MD.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2010. Global learning of
focused entailment graphs. In Proceedings
of ACL, pages 1220?1229, Uppsala.
Berant, Jonathan, Ido Dagan, and Jacob
Goldberger. 2011. Global learning of typed
entailment rules. In Proceedings of ACL,
pages 610?619, Portland, OR.
Bhagat, Rahul, Patrick Pantel, and Eduard
Hovy. 2007. LEDIR: An unsupervised
algorithm for learning directionality of
inference rules. In Proceedings of
EMNLP-CoNLL, pages 161?170, Prague.
Budanitsky, Alexander and Graeme Hirst.
2006. Evaluating Wordnet-based measures
of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47.
Chklovski, Timothy and Patrick Pantel.
2004. VerbOcean: Mining the Web for
fine-grained semantic verb relations.
In Proceedings of EMNLP, pages 33?40,
Barcelona.
Clark, Peter, William Murray, John
Thompson, Phil Harrison, Jerry Hobbs,
and Christiane Fellbaum. 2007. On the role
of lexical and world knowledge in RTE3.
In Proceedings of the Workshop on Textual
Entailment and Paraphrasing, pages 54?59,
Prague.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:273?381.
Cohen, William, Pradeep Ravikumar,
and Stephen E. Fienberg. 2003. A
comparison of string distance metrics for
name-matching tasks. In Proceedings of
IIWeb, pages 73?78, Acapulco.
Connor, Michael and Dan Roth. 2007.
Context sensitive paraphrasing with
a single unsupervised classifier.
In Proceedings of ECML, pages 104?115,
Warsaw.
Coyne, Bob and Owen Rambow. 2009.
Lexpar: A freely available English
paraphrase lexicon automatically extracted
from Framenet. In Proceedings of the IEEE
International Conference on Semantic
Computing, pages 53?58, Berkeley, CA.
Dagan, Ido, Bill Dolan, Bernardo Magnini,
and Dan Roth. 2009. Recognizing textual
entailment: Rational, evaluation and
approaches. Natural Language Engineering,
15(4):1?17.
Do, Quang and Dan Roth. 2010. Constraints
based taxonomic relation classification. In
Proceedings of EMNLP, pages 1099?1109,
Cambridge, MA.
Fellbaum, Christiane. 1998a. A semantic
network of English: The mother of all
109
Computational Linguistics Volume 38, Number 1
wordNets. Natural Language Engineering,
32:209?220.
Fellbaum, Christiane, editor. 1998b. WordNet:
An Electronic Lexical Database (Language,
Speech, and Communication). The MIT Press,
Cambridge, MA.
Finkel, Jenny R. and Christopher D.
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of
ACL-08: HLT, Short Papers, pages 45?48,
Columbus, OH.
Habash, Nizar and Bonnie Dorr. 2003.
A categorial variation database for
English. In Proceedings of the NAACL,
pages 17?23, Edmonton.
Hall, Mark, Eibe Frank, Geoffrey Holmes,
Bernhard Pfahringer, Peter Reutemann,
and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD
Explorations, 11(1):10?18.
Harmeling, Stefan. 2009. Inferring textual
entailment with a probabilistically sound
calculus. Natural Language Engineering,
15(4):459?477.
Harris, Zellig. 1954. Distributional structure.
Word, 10(23):146?162.
Joachims, Thorsten. 2005. A support vector
method for multivariate performance
measures. In Proceedings of ICML,
pages 377?384, Bonn.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank.
In Proceedings of HLT, pages 252?256,
San Diego, CA.
Kipper, Karin, Hoa T. Dang, and Martha
Palmer. 2000. Class-based construction
of a verb lexicon. In Proceedings of AAAI,
pages 691?696, Austin, TX.
Kotlerman, Lili, Ido Dagan, Idan Szpektor,
and Maayan Zhitomirsky-Geffet. 2010.
Directional distributional similarity for
lexical inference. Natural Language
Engineering, 16:359?389.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words.
In Proceedings of COLING-ACL,
pages 768?774, Montreal.
Lin, Dekang. 1998b. Dependency-based
evaluation of Minipar. In Proceedings of the
Workshop on Evaluation of Parsing Systems at
LREC, pages 317?329, Granada.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):343?360.
Macleod, Catherine, Ralph Grishman,
Adam Meyers, Leslie Barrett, and
Ruth Reeves. 1998. Nomlex: A lexicon of
nominalizations. In Proceedings of Euralex,
pages 187?193, Lieg`e.
Martins, Andre, Noah Smith, and Eric Xing.
2009. Concise integer linear programming
formulations for dependency parsing.
In Proceedings of ACL, pages 342?350,
Singapore.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekeley, Veronika
Zielinska, and Brian Young. 2004.
The cross-breeding of dictionaries. In
Proceedings of LREC, pages 1095?1098,
Lisbon.
Mirkin, Shachar, Ido Dagan, and Maayan
Gefet. 2006. Integrating pattern-based
and distributional similarity methods
for lexical entailment acquisition.
In Proceedings of COLING-ACL,
pages 579?586, Sydney.
Nikulin, Vladimir. 2008. Classification of
imbalanced data with random sets and
mean-variance filtering. International
Journal of Data Warehousing and Mining,
4(2):63?78.
Pekar, Viktor. 2008. Discovery of event
entailment knowledge from text corpora.
Computer Speech & Language, 22(1):1?16.
Raina, Rajat, Andrew Ng, and Christopher
Manning. 2005. Robust textual inference
via learning and abductive reasoning.
In Proceedings of AAAI, pages 1099?1105,
Pittsburgh, PA.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of EMNLP, pages 129?137,
Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of CoNLL, pages 1?8,
Boston, MA.
Schoenmackers, Stefan, Jesse Davis,
Oren Etzioni, and Daniel S. Weld.
2010. Learning first-order horn
clauses from Web text. In Proceedings
of EMNLP, pages 1088?1098,
Cambridge, MA.
Sekine, Satoshi. 2005. Automatic paraphrase
discovery based on context and keywords
between NE pairs. In Proceedings of IWP,
pages 80?87, Jeju Island.
Siegel, Sidney and N. John Castellan. 1988.
Non-parametric Statistics for the Behavioral
Sciences. McGraw-Hill, New-York.
Smith, Noah and Jason Eisner. 2005.
Contrastive estimation: Training log-linear
models on unlabeled data. In Proceedings
of ACL, pages 354?362, Ann Arbor, MI.
110
Berant et al Learning Entailment Relations by Global Graph Structure Optimization
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2004. Learning syntactic patterns for
automatic hypernym discovery. In
Proceedings of NIPS, pages 1297?1304,
Vancouver.
Snow, Rion, Daniel Jurafsky, and Andrew Y.
Ng. 2006. Semantic taxonomy induction
from heterogenous evidence. In
Proceedings of ACL, pages 801?808, Prague.
Stoica, Emilia, Marti Hearst, and Megan
Richardson. 2007. Automating creation of
hierarchical faceted metadata structures. In
Proceedings of NAACL-HLT, pages 244?251,
Rochester, NY.
Szpektor, Idan and Ido Dagan. 2007.
Learning canonical forms of entailment
rules. In Proceedings of RANLP, pages 1?8,
Borovetz.
Szpektor, Idan and Ido Dagan. 2008.
Learning entailment rules for unary
templates. In Proceedings of COLING,
pages 849?856, Manchester.
Szpektor, Idan and Ido Dagan. 2009.
Augmenting Wordnet-based inference
with argument mapping. In Proceedings
of TextInfer, pages 27?35, Singapore.
Szpektor, Idan, Hristo Tanev, Ido Dagan,
and Bonaventura Coppola. 2004. Scaling
Web-based acquisition of entailment
relations. In Proceedings of EMNLP,
pages 41?48, Barcelona.
Van Hulse, Jason, Taghi Khoshgoftaar, and
Amri Napolitano. 2007. Experimental
perspectives on learning from imbalanced
data. In Proceedings of ICML,
pages 935?942, Corvallis, OR.
Vanderbei, Robert. 2008. Linear Programming:
Foundations and Extensions. Springer,
New-York.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity.
In Proceedings of EMNLP, pages 81?88,
Sapporo.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1:80?83.
Yannakakis, Mihalis. 1978. Node-and
edge-deletion NP-complete problems. In
STOC ?78: Proceedings of the Tenth Annual
ACM Symposium on Theory of Computing,
pages 253?264, New York, NY.
Yates, Alexander and Oren Etzioni. 2009.
Unsupervised methods for determining
object and relation synonyms on the web.
Journal of Artificial Intelligence Research,
34:255?296.
111
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Global Learning of Focused Entailment Graphs
Jonathan Berant
Tel-Aviv University
Tel-Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
We propose a global algorithm for learn-
ing entailment relations between predi-
cates. We define a graph structure over
predicates that represents entailment rela-
tions as directed edges, and use a global
transitivity constraint on the graph to learn
the optimal set of edges, by formulating
the optimization problem as an Integer
Linear Program. We motivate this graph
with an application that provides a hierar-
chical summary for a set of propositions
that focus on a target concept, and show
that our global algorithm improves perfor-
mance by more than 10% over baseline al-
gorithms.
1 Introduction
The Textual Entailment (TE) paradigm (Dagan et
al., 2009) is a generic framework for applied se-
mantic inference. The objective of TE is to recog-
nize whether a target meaning can be inferred from
a given text. For example, a Question Answer-
ing system has to recognize that ?alcohol affects
blood pressure? is inferred from ?alcohol reduces
blood pressure? to answer the question ?What af-
fects blood pressure??
TE systems require extensive knowledge of en-
tailment patterns, often captured as entailment
rules: rules that specify a directional inference re-
lation between two text fragments (when the rule
is bidirectional this is known as paraphrasing). An
important type of entailment rule refers to propo-
sitional templates, i.e., propositions comprising
a predicate and arguments, possibly replaced by
variables. The rule required for the previous ex-
ample would be ?X reduce Y ? X affect Y?. Be-
cause facts and knowledge are mostly expressed
by propositions, such entailment rules are central
to the TE task. This has led to active research
on broad-scale acquisition of entailment rules for
predicates, e.g. (Lin and Pantel, 2001; Sekine,
2005; Szpektor and Dagan, 2008).
Previous work has focused on learning each en-
tailment rule in isolation. However, it is clear that
there are interactions between rules. A prominent
example is that entailment is a transitive relation,
and thus the rules ?X ? Y ? and ?Y ? Z? imply
the rule ?X ? Z?. In this paper we take advantage
of these global interactions to improve entailment
rule learning.
First, we describe a structure termed an entail-
ment graph that models entailment relations be-
tween propositional templates (Section 3). Next,
we show that we can present propositions accord-
ing to an entailment hierarchy derived from the
graph, and suggest a novel hierarchical presenta-
tion scheme for corpus propositions referring to a
target concept. As in this application each graph
focuses on a single concept, we term those focused
entailment graphs (Section 4).
In the core section of the paper, we present an
algorithm that uses a global approach to learn the
entailment relations of focused entailment graphs
(Section 5). We define a global function and look
for the graph that maximizes that function under
a transitivity constraint. The optimization prob-
lem is formulated as an Integer Linear Program
(ILP) and solved with an ILP solver. We show that
this leads to an optimal solution with respect to
the global function, and demonstrate that the algo-
rithm outperforms methods that utilize only local
information by more than 10%, as well as meth-
ods that employ a greedy optimization algorithm
rather than an ILP solver (Section 6).
2 Background
Entailment learning Two information types have
primarily been utilized to learn entailment rules
between predicates: lexicographic resources and
distributional similarity resources. Lexicographic
1220
resources are manually-prepared knowledge bases
containing information about semantic relations
between lexical items. WordNet (Fellbaum,
1998), by far the most widely used resource, spec-
ifies relations such as hyponymy, derivation, and
entailment that can be used for semantic inference
(Budanitsky and Hirst, 2006). WordNet has also
been exploited to automatically generate a training
set for a hyponym classifier (Snow et al, 2005),
and we make a similar use of WordNet in Section
5.1.
Lexicographic resources are accurate but tend
to have low coverage. Therefore, distributional
similarity is used to learn broad-scale resources.
Distributional similarity algorithms predict a se-
mantic relation between two predicates by com-
paring the arguments with which they occur. Quite
a few methods have been suggested (Lin and Pan-
tel, 2001; Bhagat et al, 2007; Yates and Etzioni,
2009), which differ in terms of the specifics of the
ways in which predicates are represented, the fea-
tures that are extracted, and the function used to
compute feature vector similarity. Details on such
methods are given in Section 5.1.
Global learning It is natural to describe en-
tailment relations between predicates by a graph.
Nodes represent predicates, and edges represent
entailment between nodes. Nevertheless, using a
graph for global learning of entailment between
predicates has attracted little attention. Recently,
Szpektor and Dagan (2009) presented the resource
Argument-mapped WordNet, providing entailment
relations for predicates in WordNet. Their re-
source was built on top of WordNet, and makes
simple use of WordNet?s global graph structure:
new rules are suggested by transitively chaining
graph edges, and verified against corpus statistics.
The most similar work to ours is Snow et al?s al-
gorithm for taxonomy induction (2006). Snow et
al.?s algorithm learns the hyponymy relation, un-
der the constraint that it is a transitive relation.
Their algorithm incrementally adds hyponyms to
an existing taxonomy (WordNet), using a greedy
search algorithm that adds at each step the set of
hyponyms that maximize the probability of the ev-
idence while respecting the transitivity constraint.
In this paper we tackle a similar problem of
learning a transitive relation, but we use linear pro-
gramming. A Linear Program (LP) is an optimiza-
tion problem, where a linear function is minimized
(or maximized) under linear constraints. If the
variables are integers, the problem is termed an In-
teger Linear Program (ILP). Linear programming
has attracted attention recently in several fields of
NLP, such as semantic role labeling, summariza-
tion and parsing (Roth and tau Yih, 2005; Clarke
and Lapata, 2008; Martins et al, 2009). In this
paper we formulate the entailment graph learning
problem as an Integer Linear Program, and find
that this leads to an optimal solution with respect
to the target function in our experiment.
3 Entailment Graph
This section presents an entailment graph struc-
ture, which resembles the graph in (Szpektor and
Dagan, 2009).
The nodes of an entailment graph are propo-
sitional templates. A propositional template is a
path in a dependency tree between two arguments
of a common predicate1 (Lin and Pantel, 2001;
Szpektor and Dagan, 2008). Note that in a de-
pendency parse, such a path passes through the
predicate. We require that a variable appears in at
least one of the argument positions, and that each
sense of a polysemous predicate corresponds to a
separate template (and a separate graph node): X
subj
??? treat#1
obj
??? Y and X
subj
??? treat#1
obj
??? nau-
sea are propositional templates for the first sense
of the predicate treat. An edge (u, v) represents
the fact that template u entails template v. Note
that the entailment relation transcends beyond hy-
ponymy. For example, the template X is diagnosed
with asthma entails the template X suffers from
asthma, although one is not a hyponoym of the
other. An example of an entailment graph is given
in Figure 1, left.
Since entailment is a transitive relation, an en-
tailment graph is transitive, i.e., if the edges (u, v)
and (v, w) are in the graph, so is the edge (u,w).
This is why we require that nodes be sense-
specified, as otherwise transitivity does not hold:
Possibly a ? b for one sense of b, b ? c for an-
other sense of b, but a9 c.
Because graph nodes represent propositions,
which generally have a clear truth value, we can
assume that transitivity is indeed maintained along
paths of any length in an entailment graph, as en-
tailment between each pair of nodes either occurs
or doesn?t occur with very high probability. We
support this further in section 4.1, where we show
1We restrict our discussion to templates with two argu-
ments, but generalization is straightforward.
1221
X-related-to-nausea X-associated-with-nauseaX-prevent-nausea X-help-with-nauseaX-reduce-nausea X-treat-nausea
related to nauseaheadacheOxicontine
help with nausea
prevent nausea
acupuncture
ginger
reduce nausearelaxationtreat nauseadrugsNabiloneLorazepam
Figure 1: Left: An entailment graph. For clarity, edges that can be inferred by transitivity are omitted. Right: A hierarchical
summary of propositions involving nausea as an argument, such as headache is related to nausea, acupuncture helps with
nausea, and Lorazepam treats nausea.
that in our experimental setting the length of paths
in the entailment graph is relatively small.
Transitivity implies that in each strong connec-
tivity component2 of the graph, all nodes are syn-
onymous. Moreover, if we merge every strong
connectivity component to a single node, the
graph becomes a Directed Acyclic Graph (DAG),
and the graph nodes can be sorted and presented
hierarchically. Next, we show an application that
leverages this property.
4 Motivating Application
In this section we propose an application that pro-
vides a hierarchical view of propositions extracted
from a corpus, based on an entailment graph.
Organizing information in large collections has
been found to be useful for effective information
access (Kaki, 2005; Stoica et al, 2007). It allows
for easier data exploration, and provides a compact
view of the underlying content. A simple form of
structural presentation is by a single hierarchy, e.g.
(Hofmann, 1999). A more complex approach is
hierarchical faceted metadata, where a number of
concept hierarchies are created, corresponding to
different facets or dimensions (Stoica et al, 2007).
Hierarchical faceted metadata categorizes con-
cepts of a domain in several dimensions, but does
not specify the relations between them. For ex-
ample, in the health-care domain we might have
facets for categories such as diseases and symp-
toms. Thus, when querying about nausea, one
might find it is related to vomitting and chicken
pox, but not that chicken pox is a cause of nausea,
2A strong connectivity component is a subset of nodes in
the graph where there is a path from any node to any other
node.
while nausea is often accompanied by vomitting.
We suggest that the prominent information
in a text lies in the propositions it contains,
which specify particular relations between the
concepts. Propositions have been mostly pre-
sented through unstructured textual summaries or
manually-constructed ontologies, which are ex-
pensive to build. We propose using the entail-
ment graph structure, which describes entailment
relations between predicates, to naturally present
propositions hierarchically. That is, the entailment
hierarchy can be used as an additional facet, which
can improve navigation and provide a compact hi-
erarchical summary of the propositions.
Figure 1 illustrates a scenario, on which we
evaluate later our learning algorithm. Assume a
user would like to retrieve information about a tar-
get concept such as nausea. We can extract the set
of propositions where nausea is an argument auto-
matically from a corprus, and learn an entailment
graph over propositional templates derived from
the extracted propositions, as illustrated in Figure
1, left. Then, we follow the steps in the process
described in Section 3: merge synonymous nodes
that are in the same strong connectivity compo-
nent, and turn the resulting DAG into a predicate
hierarchy, which we can then use to present the
propositions (Figure 1, right). Note that in all
propositional templates one argument is the tar-
get concept (nausea), and the other is a variable
whose corpus instantiations can be presented ac-
cording to another hierarchy (e.g. Nabilone and
Lorazepam are types of drugs).
Moreover, new propositions are inferred from
the graph by transitivity. For example, from the
proposition ?relaxation reduces nausea? we can in-
1222
fer the proposition ?relaxation helps with nausea?.
4.1 Focused entailment graphs
The application presented above generates entail-
ment graphs of a specific form: (1) Propositional
templates have exactly one argument instantiated
by the same entity (e.g. nausea). (2) The predicate
sense is unspecified, but due to the rather small
number of nodes and the instantiating argument,
each predicate corresponds to a unique sense.
Generalizing this notion, we define a focused
entailment graph to be an entailment graph where
the number of nodes is relatively small (and con-
sequently paths in the graph are short), and predi-
cates have a single sense (so transitivity is main-
tained without sense specification). Section 5
presents an algorithm that given the set of nodes
of a focused entailment graph learns its edges, i.e.,
the entailment relations between all pairs of nodes.
The algorithm is evaluated in Section 6 using our
proposed application. For brevity, from now on
the term entailment graph will stand for focused
entailment graph.
5 Learning Entailment Graph Edges
In this section we present an algorithm for learn-
ing the edges of an entailment graph given its set
of nodes. The first step is preprocessing: We use
a large corpus and WordNet to train an entail-
ment classifier that estimates the likelihood that
one propositional template entails another. Next,
we can learn on the fly for any input graph: given
the graph nodes, we employ a global optimiza-
tion approach that determines the set of edges that
maximizes the probability (or score) of the entire
graph, given the edge probabilities (or scores) sup-
plied by the entailment classifier and the graph
constraints (transitivity and others).
5.1 Training an entailment classifier
We describe a procedure for learning an entail-
ment classifier, given a corpus and a lexicographic
resource (WordNet). First, we extract a large set of
propositional templates from the corpus. Next, we
represent each pair of propositional templates with
a feature vector of various distributional similar-
ity scores. Last, we use WordNet to automatically
generate a training set and train a classifier.
Template extraction We parse the corpus with
a dependency parser and extract all propositional
templates from every parse tree, employing the
procedure used by Lin and Pantel (2001). How-
ever, we only consider templates containing a
predicate term and arguments3. The arguments are
replaced with variables, resulting in propositional
templates such as X
subj
??? affect
obj
??? Y.
Distributional similarity representation We
aim to train a classifier that for an input template
pair (t1, t2) determines whether t1 entails t2. A
template pair is represented by a feature vector
where each coordinate is a different distributional
similarity score. There are a myriad of distribu-
tional similarity algorithms. We briefly describe
those used in this paper, obtained through varia-
tions along the following dimensions:
Predicate representation Most algorithms mea-
sure the similarity between templates with two
variables (binary templates) such as X
subj
??? af-
fect
obj
??? Y (Lin and Pantel, 2001; Bhagat et al,
2007; Yates and Etzioni, 2009). Szpketor and Da-
gan (2008) suggested learning over templates with
one variable (unary templates) such as X
subj
??? af-
fect, and using them to estimate a score for binary
templates.
Feature representation The features of a tem-
plate are some representation of the terms that in-
stantiated the argument variables in a corpus. Two
representations are used in our experiment (see
Section 6). Another variant occurs when using bi-
nary templates: a template may be represented by
a pair of feature vectors, one for each variable (Lin
and Pantel, 2001), or by a single vector, where fea-
tures represent pairs of instantiations (Szpektor et
al., 2004; Yates and Etzioni, 2009). The former
variant reduces sparsity problems, while Yates and
Etzioni showed the latter is more informative and
performs favorably on their data.
Similarity function We consider two similarity
functions: The Lin (2001) similarity measure, and
the Balanced Inclusion (BInc) similarity measure
(Szpektor and Dagan, 2008). The former is a
symmetric measure and the latter is asymmetric.
Therefore, information about the direction of en-
tailment is provided by the BInc measure.
We then generate for any (t1, t2) features that
are the 12 distributional similarity scores using all
combinations of the dimensions. This is reminis-
cent of Connor and Roth (2007), who used the out-
put of unsupervised classifiers as features for a su-
pervised classifier in a verb disambiguation task.
3Via a simple heuristic, omitted due to space limitations
1223
Training set generation Following the spirit of
Snow et al (2005), WordNet is used to automati-
cally generate a training set of positive (entailing)
and negative (non-entailing) template pairs. Let
T be the set of propositional templates extracted
from the corpus. For each ti ? T with two vari-
ables and a single predicate word w, we extract
from WordNet the set H of direct hypernyms and
synonyms of w. For every h ? H , we generate a
new template tj from ti by replacing w with h. If
tj ? T , we consider (ti, tj) to be a positive exam-
ple. Negative examples are generated analogously,
by looking at direct co-hyponyms of w instead of
hypernyms and synonyms. This follows the no-
tion of ?contrastive estimation? (Smith and Eisner,
2005), since we generate negative examples that
are semantically similar to positive examples and
thus focus the classifier?s attention on identifying
the boundary between the classes. Last, we filter
training examples for which all features are zero,
and sample an equal number of positive and neg-
ative examples (for which we compute similarity
features), since classifiers tend to perform poorly
on the minority class when trained on imbalanced
data (Van Hulse et al, 2007; Nikulin, 2008).
5.2 Global learning of edges
Once the entailment classifier is trained we learn
the graph edges given its nodes. This is equiv-
alent to learning all entailment relations between
all propositional template pairs for that graph.
To learn edges we consider global constraints,
which allow only certain graph topologies. Since
we seek a global solution under transitivity and
other constraints, linear programming is a natural
choice, enabling the use of state of the art opti-
mization packages. We describe two formulations
of integer linear programs that learn the edges: one
maximizing a global score function, and another
maximizing a global probability function.
Let Iuv be an indicator denoting the event that
node u entails node v. Our goal is to learn the
edges E over a set of nodes V . We start by formu-
lating the constraints and then the target functions.
The first constraint is that the graph must re-
spect transitivity. Our formulation is equivalent to
the one suggested by Finkel and Manning (2008)
in a coreference resolution task:
?u,v,w?V Iuv + Ivw ? Iuw ? 1
In addition, for a few pairs of nodes we have
strong evidence that one does not entail the other
and so we add the constraint Iuv = 0. Combined
with the constraint of transitivity this implies that
there must be no path from u to v. This is done in
the following two scenarios: (1) When two nodes
u and v are identical except for a pair of words wu
and wv, and wu is an antonym of wv, or a hyper-
nym of wv at distance ? 2. (2) When two nodes
u and v are transitive opposites, that is, if u =
X
subj
??? w
obj
??? Y and v = X
obj
??? w
subj
??? Y ,
for any word w4.
Score-based target function We assume an en-
tailment classifier estimating a positive score Suv
if it believes Iuv = 1 and a negative score other-
wise (for example, an SVM classifier). We look
for a graph G that maximizes the sum of scores
over the edges:
G? = argmax
G
S(G)
= argmax
G
?
?
?
u6=v
SuvIuv
?
?? ?|E|
where ?|E| is a regularization term reflecting
the fact that edges are sparse. Note that this con-
stant needs to be optimized on a development set.
Probabilistic target function Let Fuv be the
features for the pair of nodes (u, v) and F =
?u6=vFuv. We assume an entailment classifier es-
timating the probability of an edge given its fea-
tures: Puv = P (Iuv = 1|Fuv). We look for the
graph G that maximizes the posterior probability
P (G|F ):
G? = argmax
G
P (G|F )
Following Snow et al, we make two inde-
pendence assumptions: First, we assume each
set of features Fuv is independent of other sets
of features given the graph G, i.e., P (F |G) =
?
u6=v P (Fuv|G). Second, we assume the features
for the pair (u, v) are generated by a distribution
depending only on whether entailment holds for
(u, v). Thus, P (Fuv|G) = P (Fuv|Iuv). Last,
for simplicity we assume edges are independent
and the prior probability of a graph is a product
of the prior probabilities of the edge indicators:
4We note that in some rare cases transitive verbs are in-
deed reciprocal, as in ?X marry Y?, but in the grand ma-
jority of cases reciprocal activities are not expressed using
a transitive-verb structure.
1224
P (G) =
?
u6=v P (Iuv). Note that although we
assume edges are independent, dependency is still
expressed using the transitivity constraint. We ex-
press P (G|F ) using the assumptions above and
Bayes rule:
P (G|F ) ? P (G)P (F |G)
=
?
u6=v
[P (Iuv)P (Fuv|Iuv)]
=
?
u6=v
P (Iuv)
P (Iuv|Fuv)P (Fuv)
P (Iuv)
?
?
u6=v
P (Iuv|Fuv)
=
?
(u,v)?E
Puv ?
?
(u,v)/?E
(1? Puv)
Note that the prior P (Fuv) is constant with re-
spect to the graph. Now we look for the graph that
maximizes logP (G|F ):
G? = argmax
G
?
(u,v)?E
logPuv +
?
(u,v)/?E
log(1? Puv)
= argmax
G
?
u6=v
[Iuv ? logPuv
+ (1? Iuv) ? log(1? Puv)]
= argmax
G
?
u6=v
log
Puv
1? Puv
? Iuv
(in the last transition we omit the constant
?
u6=v log(1?Puv)). Importantly, while the score-
based formulation contains a parameter ? that re-
quires optimization, this probabilistic formulation
is parameter free and does not utilize a develop-
ment set at all.
Since the variables are binary, both formula-
tions are integer linear programs with O(|V |2)
variables and O(|V |3) transitivity constraints that
can be solved using standard ILP packages.
Our work resembles Snow et al?s in that both
try to learn graph edges given a transitivity con-
straint. However, there are two key differences
in the model and in the optimization algorithm.
First, Snow et al?s model attempts to determine
the graph that maximizes the likelihood P (F |G)
and not the posterior P (G|F ). Therefore, their
model contains an edge prior P (Iuv) that has to
be estimated, whereas in our model it cancels out.
Second, they incrementally add hyponyms to a
large taxonomy (WordNet) and therefore utilize a
greedy algorithm, while we simultaneously learn
all edges of a rather small graph and employ in-
teger linear programming, which is more sound
theoretically, and as shown in Section 6, leads to
an optimal solution. Nevertheless, Snow et al?s
model can also be formulated as a linear program
with the following target function:
argmax
G
?
u6=v
log
Puv ? P (Iuv = 0)
(1? Puv) ? P (Iuv = 1)
Iuv
Note that if the prior inverse odds k =
P (Iuv=0)
P (Iuv=1)
= 1, i.e., P (Iuv = 1) = 0.5, then
this is equivalent to our probabilistic formulation.
We implemented Snow et als model and optimiza-
tion algorithm and in Section 6.3 we compare our
model and optimization algorithm to theirs.
6 Experimental Evaluation
This section presents our evaluation, which is
geared for the application proposed in Section 4.
6.1 Experimental setting
A health-care corpus of 632MB was harvested
from the web and parsed with the Minipar parser
(Lin, 1998). The corpus contains 2,307,585
sentences and almost 50 million word tokens.
We used the Unified Medical Language System
(UMLS)5 to annotate medical concepts in the cor-
pus. The UMLS is a database that maps nat-
ural language phrases to over one million con-
cept identifiers in the health-care domain (termed
CUIs). We annotated all nouns and noun phrases
that are in the UMLS with their possibly multi-
ple CUIs. We extracted all propositional templates
from the corpus, where both argument instantia-
tions are medical concepts, i.e., annotated with a
CUI (?50,000 templates). When computing dis-
tributional similarity scores, a template is repre-
sented as a feature vector of the CUIs that instan-
tiate its arguments.
To evaluate the performance of our algo-
rithm, we constructed 23 gold standard entailment
graphs. First, 23 medical concepts, representing
typical topics of interest in the medical domain,
were manually selected from a list of the most fre-
quent concepts in the corpus. For each concept,
nodes were defined by extracting all propositional
5http://www.nlm.nih.gov/research/umls
1225
Using a development set Not using a development set
Edges Propositions Edges Propositions
R P F1 R P F1 R P F1 R P F1
LP 46.0 50.1 43.8 67.3 69.6 66.2 48.7 41.9 41.2 67.9 62.0 62.3
Greedy 45.7 37.1 36.6 64.2 57.2 56.3 48.2 41.7 41.0 67.8 62.0 62.4
Local-LP 44.5 45.3 38.1 65.2 61.0 58.6 69.3 19.7 26.8 82.7 33.3 42.6
Local1 53.5 34.9 37.5 73.5 50.6 56.1 92.9 11.1 19.7 95.4 18.6 30.6
Local2 52.5 31.6 37.7 69.8 50.0 57.1 63.2 24.9 33.6 77.7 39.3 50.5
Local?1 53.5 38.0 39.8 73.5 54.6 59.1 92.6 11.3 20.0 95.3 18.9 31.1
Local?2 52.5 32.1 38.1 69.8 50.6 57.4 63.1 25.5 34.0 77.7 39.9 50.9
WordNet - - - - - - 10.8 44.1 13.2 39.9 72.4 47.3
Table 1: Results for all experiments
templates for which the target concept instanti-
ated an argument at least K(= 3) times (average
number of graph nodes=22.04, std=3.66, max=26,
min=13).
Ten medical students constructed the gold stan-
dard of graph edges. Each concept graph was
annotated by two students. Following RTE-5
practice (Bentivogli et al, 2009), after initial an-
notation the two students met for a reconcili-
ation phase. They worked to reach an agree-
ment on differences and corrected their graphs.
Inter-annotator agreement was calculated using
the Kappa statistic (Siegel and Castellan, 1988)
both before (? = 0.59) and after (? = 0.9) rec-
onciliation. 882 edges were included in the 23
graphs out of a possible 10,364, providing a suf-
ficiently large data set. The graphs were randomly
split into a development set (11 graphs) and a test
set (12 graphs)6. The entailment graph fragment
in Figure 1 is from the gold standard.
The graphs learned by our algorithm were eval-
uated by two measures, one evaluating the graph
directly, and the other motivated by our applica-
tion: (1) F1 of the learned edges compared to the
gold standard edges (2) Our application provides
a summary of propositions extracted from the cor-
pus. Note that we infer new propositions by prop-
agating inference transitively through the graph.
Thus, we compute F1 for the set of propositions
inferred from the learned graph, compared to the
set inferred based on the gold standard graph. For
example, given the proposition from the corpus
?relaxation reduces nausea? and the edge ?X re-
duce nausea? X help with nausea?, we evaluate
the set {?relaxation reduces nausea?, ?relaxation
helps with nausea?}. The final score for an algo-
rithm is a macro-average over the 12 graphs of the
6Test set concepts were: asthma, chemotherapy, diarrhea,
FDA, headache, HPV, lungs, mouth, salmonella, seizure,
smoking and X-ray.
test set.
6.2 Evaluated algorithms
Local algorithms We described 12 distributional
similarity measures computed over our corpus
(Section 5.1). For each measure we computed for
each template t a list of templates most similar to
t (or entailing t for directional measures). In ad-
dition, we obtained similarity lists learned by Lin
and Pantel (2001), and replicated 3 similarity mea-
sures learned by Szpektor and Dagan (2008), over
the RCV1 corpus7. For each distributional similar-
ity measure (altogether 16 measures), we learned a
graph by inserting any edge (u, v), when u is in the
top K templates most similar to v. We also omit-
ted edges for which there was strong evidence that
they do not exist, as specified by the constraints
in Section 5.2. Another local resource was Word-
Net where we inserted an edge (u, v) when v was
a direct hypernym or synonym of u. For all algo-
rithms, we added all edges inferred by transitivity.
Global algorithms We experimented with all
6 combinations of the following two dimensions:
(1) Target functions: score-based, probabilistic
and Snow et al?s (2) Optimization algorithms:
Snow et al?s greedy algorithm and a standard ILP
solver. A training set of 20,144 examples was au-
tomatically generated, each example represented
by 16 features using the distributional similarity
measures mentioned above. SVMperf (Joachims,
2005) was used to train an SVM classifier yield-
ing Suv, and the SMO classifier from WEKA (Hall
et al, 2009) estimated Puv. We used the lpsolve8
package to solve the linear programs. In all re-
sults, the relaxation ?u,v0 ? Iuv ? 1 was used,
which guarantees an optimal output solution. In
7http://trec.nist.gov/data/reuters/reuters.html. The simi-
larity lists were computed using: (1) Unary templates and
the Lin function (2) Unary templates and the BInc function
(3) Binary templates and the Lin function
8http://lpsolve.sourceforge.net/5.5/
1226
Global=T/Local=F Global=F/Local=T
GS= T 50 143
GS= F 140 1087
Table 2: Comparing disagreements between the best local
and global algorithms against the gold standard
all experiments the output solution was integer,
and therefore it is optimal. Constructing graph
nodes and learning its edges given an input con-
cept took 2-3 seconds on a standard desktop.
6.3 Results and analysis
Table 1 summarizes the results of the algorithms.
The left half depicts methods where the develop-
ment set was needed to tune parameters, and the
right half depicts methods that do not require a
(manually created) development set at all. Hence,
our score-based LP (tuned-LP), where the param-
eter ? is tuned, is on the left, and the probabilis-
tic LP (untuned-LP) is on the right. The row
Greedy is achieved by using the greedy algorithm
instead of lpsolve. The row Local-LP is achieved
by omitting global transitivity constraints, making
the algorithm completely local. We omit Snow et
al.?s formulation, since the optimal prior inverse
odds k was almost exactly 1, which conflates with
untuned-LP.
The rows Local1 and Local2 present the best
distributional similarity resources. Local1 is
achieved using binary templates, the Lin function,
and a single vector with feature pairs. Local2 is
identical but employs the BInc function. Local?1
and Local?2 also exploit the local constraints men-
tioned above. Results on the left were achieved
by optimizing the top-K parameter on the devel-
opment set, and on the right by optimizing on the
training set automatically generated from Word-
Net.
The global methods clearly outperform local
methods: Tuned-LP outperforms significantly all
local methods that require a development set both
on the edges F1 measure (p<.05) and on the
propositions F1 measure (p<.01)9. The untuned-
LP algorithm also significantly outperforms all lo-
cal methods that do not require a development
set on the edges F1 measure (p<.05) and on
the propositions F1 measure (p<.01). Omitting
the global transitivity constraints decreases perfor-
mance, as shown by Local-LP. Last, local meth-
9We tested significance using the two-sided Wilcoxon
rank test (Wilcoxon, 1945)
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 2: Subgraph of tuned-LP output for ?headache?
Global
X-treat-headache
X-prevent-headache
X-reduce-headache
X-report-headache
X-suffer-from-headache
X-experience-headache
Figure 3: Subgraph of Local?1 output for?headache?
ods are sensitive to parameter tuning and in the
absence of a development set their performance
dramatically deteriorates.
To further establish the merits of global algo-
rithms, we compare (Table 2) tuned-LP, the best
global algorithm, with Local?1, the best local al-
gorithm. The table considers all edges where the
two algorithms disagree, and counts how many
are in the gold standard and how many are not.
Clearly, tuned-LP is superior at avoiding wrong
edges (false positives). This is because tuned-
LP refrains from adding edges that subsequently
induce many undesirable edges through transitiv-
ity. Figures 2 and 3 illustrate this by compar-
ing tuned-LP and Local?1 on a subgraph of the
Headache concept, before adding missing edges
to satisfy transitivity to Local?1 . Note that Local
?
1
inserts a single wrong edge X-report-headache?
X-prevent-headache, which leads to adding 8 more
wrong edges. This is the type of global considera-
tion that is addressed in an ILP formulation, but is
ignored in a local approach and often overlooked
when employing a greedy algorithm. Figure 2 also
illustrates the utility of a local entailment graph for
information presentation. Presenting information
according to this subgraph distinguishes between
propositions dealing with headache treatments and
1227
propositions dealing with headache risk groups.
Comparing our use of an ILP algorithm to
the greedy one reveals that tuned-LP significantly
outperforms its greedy counterpart on both mea-
sures (p<.01). However, untuned-LP is practically
equivalent to its greedy counterpart. This indicates
that in this experiment the greedy algorithm pro-
vides a good approximation for the optimal solu-
tion achieved by our LP formulation.
Last, when comparing WordNet to local distri-
butional similarity methods, we observe low recall
and high precision, as expected. However, global
methods achieve much higher recall than WordNet
while maintaining comparable precision.
The results clearly demonstrate that a global ap-
proach improves performance on the entailment
graph learning task, and the overall advantage of
employing an ILP solver rather than a greedy al-
gorithm.
7 Conclusion
This paper presented a global optimization algo-
rithm for learning entailment relations between
predicates represented as propositional templates.
We modeled the problem as a graph learning prob-
lem, and searched for the best graph under a global
transitivity constraint. We used Integer Linear
Programming to solve the optimization problem,
which is theoretically sound, and demonstrated
empirically that this method outperforms local al-
gorithms as well as a greedy optimization algo-
rithm on the graph learning task.
Currently, we are investigating a generalization
of our probabilistic formulation that includes a
prior on the edges, and the relation of this prior
to the regularization term introduced in our score-
based formulation. In future work, we would like
to learn general entailment graphs over a large
number of nodes. This will introduce a challenge
to our current optimization algorithm due to com-
plexity issues, and will require careful handling of
predicate ambiguity. Additionally, we will inves-
tigate novel features for the entailment classifier.
This paper used distributional similarity, but other
sources of information are likely to improve per-
formance further.
Acknowledgments
We would like to thank Roy Bar-Haim, David
Carmel and the anonymous reviewers for their
useful comments. We also thank Dafna Berant
and the nine students who prepared the gold stan-
dard data set. This work was developed under
the collaboration of FBK-irst/University of Haifa
and was partially supported by the Israel Science
Foundation grant 1112/08. The first author is
grateful to the Azrieli Foundation for the award of
an Azrieli Fellowship, and has carried out this re-
search in partial fulllment of the requirements for
the Ph.D. degree.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernarde Magnini. 2009. The
fifth Pascal recognizing textual entailment chal-
lenge. In Proceedings of TAC-09.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273?381.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a single unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15(4):1?17.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Thomas Hofmann. 1999. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In Proceedings of IJCAI.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings
of ICML.
1228
Mika Kaki. 2005. Findex: Search results categories
help users when document ranking fails. In Pro-
ceedings of CHI.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
Minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Dan Roth and Wen tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In Proceedings of ICML, pages 737?744.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Sideny Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New-York.
Noah Smith and Jason Eisner. 2005. Contrastive es-
timation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL.
Emilia Stoica, Marti Hearst, and Megan Richardson.
2007. Automating creation of hierarchical faceted
metadata structures. In Proceedings of NAACL-
HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of TextInfer-2009.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri
Napolitano. 2007. Experimental perspectives on
learning from imbalanced data. In Proceedings of
ICML.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255?296.
1229
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 610?619,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Global Learning of Typed Entailment Rules
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Jacob Goldberger
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Abstract
Extensive knowledge bases of entailment rules
between predicates are crucial for applied se-
mantic inference. In this paper we propose an
algorithm that utilizes transitivity constraints
to learn a globally-optimal set of entailment
rules for typed predicates. We model the task
as a graph learning problem and suggest meth-
ods that scale the algorithm to larger graphs.
We apply the algorithm over a large data set
of extracted predicate instances, from which a
resource of typed entailment rules has been re-
cently released (Schoenmackers et al, 2010).
Our results show that using global transitiv-
ity information substantially improves perfor-
mance over this resource and several base-
lines, and that our scaling methods allow us
to increase the scope of global learning of
entailment-rule graphs.
1 Introduction
Generic approaches for applied semantic infer-
ence from text gained growing attention in recent
years, particularly under the Textual Entailment
(TE) framework (Dagan et al, 2009). TE is a
generic paradigm for semantic inference, where the
objective is to recognize whether a target meaning
can be inferred from a given text. A crucial com-
ponent of inference systems is extensive resources
of entailment rules, also known as inference rules,
i.e., rules that specify a directional inference rela-
tion between fragments of text. One important type
of rule is rules that specify entailment relations be-
tween predicates and their arguments. For example,
the rule ?X annex Y? X control Y? helps recognize
that the text ?Japan annexed Okinawa? answers the
question ?Which country controls Okinawa??. Thus,
acquisition of such knowledge received considerable
attention in the last decade (Lin and Pantel, 2001;
Sekine, 2005; Szpektor and Dagan, 2009; Schoen-
mackers et al, 2010).
Most past work took a ?local learning? approach,
learning each entailment rule independently of oth-
ers. It is clear though, that there are global inter-
actions between predicates. Notably, entailment is
a transitive relation and so the rules A ? B and
B ? C imply A? C.
Recently, Berant et al (2010) proposed a global
graph optimization procedure that uses Integer Lin-
ear Programming (ILP) to find the best set of entail-
ment rules under a transitivity constraint. Imposing
this constraint raised two challenges. The first of
ambiguity: transitivity does not always hold when
predicates are ambiguous, e.g., X buy Y? X acquire
Y and X acquire Y ? X learn Y, but X buy Y 9 X
learn Y since these two rules correspond to two dif-
ferent senses of acquire. The second challenge is
scalability: ILP solvers do not scale well since ILP
is an NP-complete problem. Berant et al circum-
vented these issues by learning rules where one of
the predicate?s arguments is instantiated (e.g., ?X re-
duce nausea? X affect nausea?), which is useful for
learning small graphs on-the-fly, given a target con-
cept such as nausea. While rules may be effectively
learned when needed, their scope is narrow and they
are not useful as a generic knowledge resource.
This paper aims to take global rule learning one
step further. To this end, we adopt the represen-
tation suggested by Schoenmackers et al (2010),
who learned inference rules between typed predi-
cates, i.e., predicates where the argument types (e.g.,
city or drug) are specified. Schoenmackers et al uti-
610
lized typed predicates since they were dealing with
noisy and ambiguous web text. Typing predicates
helps disambiguation and filtering of noise, while
still maintaining rules of wide-applicability. Their
method employs a local learning approach, while the
number of predicates in their data is too large to be
handled directly by an ILP solver.
In this paper we suggest applying global opti-
mization learning to open domain typed entailment
rules. To that end, we show how to construct a
structure termed typed entailment graph, where the
nodes are typed predicates and the edges represent
entailment rules. We suggest scaling techniques that
allow to optimally learn such graphs over a large
set of typed predicates by first decomposing nodes
into components and then applying incremental ILP
(Riedel and Clarke, 2006). Using these techniques,
the obtained algorithm is guaranteed to return an op-
timal solution. We ran our algorithm over the data
set of Schoenmackers et al and release a resource
of 30,000 rules1 that achieves substantially higher
recall without harming precision. To the best of our
knowledge, this is the first resource of that scale
to use global optimization for learning predicative
entailment rules. Our evaluation shows that global
transitivity improves the F1 score of rule learning by
27% over several baselines and that our scaling tech-
niques allow dealing with larger graphs, resulting in
improved coverage.
2 Background
Most work on learning entailment rules between
predicates considered each rule independently of
others, using two sources of information: lexico-
graphic resources and distributional similarity.
Lexicographic resources are manually-prepared
knowledge bases containing semantic information
on predicates. A widely-used resource is WordNet
(Fellbaum, 1998), where relations such as synonymy
and hyponymy can be used to generate rules. Other
resources include NomLex (Macleod et al, 1998;
Szpektor and Dagan, 2009) and FrameNet (Baker
and Lowe, 1998; Ben Aharon et al, 2010).
Lexicographic resources are accurate but have
1The resource can be downloaded from
http://www.cs.tau.ac.il/j?onatha6/homepage files/resources
/ACL2011Resource.zip
low coverage. Distributional similarity algorithms
use large corpora to learn broader resources by as-
suming that semantically similar predicates appear
with similar arguments. These algorithms usually
represent a predicate with one or more vectors and
use some function to compute argument similarity.
Distributional similarity algorithms differ in their
feature representation: Some use a binary repre-
sentation: each predicate is represented by one fea-
ture vector where each feature is a pair of argu-
ments (Szpektor et al, 2004; Yates and Etzioni,
2009). This representation performs well, but suf-
fers when data is sparse. The binary-DIRT repre-
sentation deals with sparsity by representing a pred-
icate with a pair of vectors, one for each argument
(Lin and Pantel, 2001). Last, a richer form of repre-
sentation, termed unary, has been suggested where
a different predicate is defined for each argument
(Szpektor and Dagan, 2008). Different algorithms
also differ in their similarity function. Some employ
symmetric functions, geared towards paraphrasing
(bi-directional entailment), while others choose di-
rectional measures more suited for entailment (Bha-
gat et al, 2007). In this paper, We employ several
such functions, such as Lin (Lin and Pantel, 2001),
and BInc (Szpektor and Dagan, 2008).
Schoenmackers et al (2010) recently used dis-
tributional similarity to learn rules between typed
predicates, where the left-hand-side of the rule may
contain more than a single predicate (horn clauses).
In their work, they used Hearst-patterns (Hearst,
1992) to extract a set of 29 million (argument, type)
pairs from a large web crawl. Then, they employed
several filtering methods to clean this set and au-
tomatically produced a mapping of 1.1 million ar-
guments into 156 types. Examples for (argument,
type) pairs are (EXODUS, book), (CHINA, coun-
try) and (ASTHMA, disease). Schoenmackers et
al. then utilized the types, the mapped arguments
and tuples from TextRunner (Banko et al, 2007)
to generate 10,672 typed predicates (such as con-
quer(country,city) and common in(disease,place)),
and learn 30,000 rules between these predicates2. In
this paper we will learn entailment rules over the
same data set, which was generously provided by
2The rules and the mapping of arguments into types can
be downloaded from http://www.cs.washington.edu/research/
sherlock-hornclauses/
611
Schoenmackers et al
As mentioned above, Berant et al (2010) used
global transitivity information to learn small entail-
ment graphs. Transitivity was also used as an in-
formation source in other fields of NLP: Taxonomy
Induction (Snow et al, 2006), Co-reference Reso-
lution (Finkel and Manning, 2008), Temporal Infor-
mation Extraction (Ling and Weld, 2010), and Un-
supervised Ontology Induction (Poon and Domin-
gos, 2010). Our proposed algorithm applies to any
sparse transitive relation, and so might be applicable
in these fields as well.
Last, we formulate our optimization problem as
an Integer Linear Program (ILP). ILP is an optimiza-
tion problem where a linear objective function over
a set of integer variables is maximized under a set of
linear constraints. Scaling ILP is challenging since
it is an NP-complete problem. ILP has been exten-
sively used in NLP lately (Clarke and Lapata, 2008;
Martins et al, 2009; Do and Roth, 2010).
3 Typed Entailment Graphs
Given a set of typed predicates, entailment rules can
only exist between predicates that share the same
(unordered) pair of types (such as place and coun-
try)3. Hence, every pair of types defines a graph
that describes the entailment relations between pred-
icates sharing those types (Figure 1). Next, we show
how to represent entailment rules between typed
predicates in a structure termed typed entailment
graph, which will be the learning goal of our algo-
rithm.
A typed entailment graph is a directed graph
where the nodes are typed predicates. A typed pred-
icate is a triple p(t1, t2) representing a predicate in
natural language. p is the lexical realization of the
predicate and the types t1, t2 are variables repre-
senting argument types. These are taken from a
set of types T , where each type t ? T is a bag
of natural language words or phrases. Examples
for typed predicates are: conquer(country,city) and
contain(product,material). An instance of a typed
predicate is a triple p(a1, a2), where a1 ? t1 and
a2 ? t2 are termed arguments. For example, be
common in(ASTHMA,AUSTRALIA) is an instance of
be common in(disease,place). For brevity, we refer
3Otherwise, the rule would contain unbound variables.
to typed entailment graphs and typed predicates as
entailment graphs and predicates respectively.
Edges in typed entailment graphs represent en-
tailment rules: an edge (u, v) means that predicate
u entails predicate v. If the type t1 is different
from the type t2, mapping of arguments is straight-
forward, as in the rule ?be find in(material,product)
? contain(product,material)?. We term this a two-
types entailment graph. When t1 and t2 are equal,
mapping of arguments is ambiguous: we distin-
guish direct-mapping edges where the first argu-
ment on the left-hand-side (LHS) is mapped to
the first argument on the right-hand-side (RHS),
as in ?beat(team,team)
d
?? defeat(team,team)?, and
reversed-mapping edges where the LHS first argu-
ment is mapped to the RHS second argument, as
in ?beat(team,team)
r
?? lose to(team,team)?. We
term this a single-type entailment graph. Note
that in single-type entailment graphs reversed-
mapping loops are possible as in ?play(team,team)
r
?? play(team,team)?: if team A plays team B, then
team B plays team A.
Since entailment is a transitive relation, typed-
entailment graphs are transitive: if the edges (u, v)
and (v, w) are in the graph so is the edge (u,w).
Note that in single-type entailment graphs one needs
to consider whether mapping of edges is direct or re-
versed: if mapping of both (u, v) and (v, w) is either
direct or reversed, mapping of (u,w) is direct, oth-
erwise it is reversed.
Typing plays an important role in rule transitiv-
ity: if predicates are ambiguous, transitivity does not
necessarily hold. However, typing predicates helps
disambiguate them and so the problem of ambiguity
is greatly reduced.
4 Learning Typed Entailment Graphs
Our learning algorithm is composed of two steps:
(1) Given a set of typed predicates and their in-
stances extracted from a corpus, we train a (local)
entailment classifier that estimates for every pair of
predicates whether one entails the other. (2) Using
the classifier scores we perform global optimization,
i.e., learn the set of edges over the nodes that maxi-
mizes the global score of the graph under transitivity
and background-knowledge constraints.
Section 4.1 describes the local classifier training
612
province of(place,country)
be part of(place,country)
annex(country,place)
invade(country,place)
be relate to(drug,drug)be derive from(drug,drug)
be process from(drug,drug)
be convert into(drug,drug)
Figure 1: Top: A fragment of a two-types entailment
graph. bottom: A fragment of a single-type entailment
graph. Mapping of solid edges is direct and of dashed
edges is reversed.
procedure. Section 4.2 gives an ILP formulation for
the optimization problem. Sections 4.3 and 4.4 pro-
pose scaling techniques that exploit graph sparsity
to optimally solve larger graphs.
4.1 Training an entailment classifier
Similar to the work of Berant et al (2010), we
use ?distant supervision?. Given a lexicographic re-
source (WordNet) and a set of predicates with their
instances, we perform the following three steps (see
Table 1):
1) Training set generation We use WordNet to
generate positive and negative examples, where each
example is a pair of predicates. Let P be the
set of input typed predicates. For every predicate
p(t1, t2) ? P such that p is a single word, we extract
from WordNet the set S of synonyms and direct hy-
pernyms of p. For every p? ? S, if p?(t1, t2) ? P
then p(t1, t2) ? p?(t1, t2) is taken as a positive ex-
ample.
Negative examples are generated in a similar
manner, with direct co-hyponyms of p (sister nodes
in WordNet) and hyponyms at distance 2 instead of
synonyms and direct hypernyms. We also generate
negative examples by randomly sampling pairs of
typed predicates that share the same types.
2) Feature representation Each example pair of
predicates (p1, p2) is represented by a feature vec-
tor, where each feature is a specific distributional
Type example
hyper. beat(team,team)? play(team,team)
syno. reach(team,game)? arrive at(team,game)
cohypo. invade(country,city) 9 bomb(country,city)
hypo. defeat(city,city) 9 eliminate(city,city)
random hold(place,event) 9 win(place,event)
Table 1: Automatically generated training set examples.
similarity score estimating whether p1 entails p2.
We compute 11 distributional similarity scores for
each pair of predicates based on the arguments ap-
pearing in the extracted arguments. The first 6
scores are computed by trying all combinations of
the similarity functions Lin and BInc with the fea-
ture representations unary, binary-DIRT and binary
(see Section 2). The other 5 scores were provided
by Schoenmackers et al (2010) and include SR
(Schoenmackers et al, 2010), LIME (McCreath and
Sharma, 1997), M-estimate (Dzeroski and Brakto,
1992), the standard G-test and a simple implementa-
tion of Cover (Weeds and Weir, 2003). Overall, the
rationale behind this representation is that combin-
ing various scores will yield a better classifier than
each single measure.
3) Training We train over an equal number of
positive and negative examples, as classifiers tend to
perform poorly on the minority class when trained
on imbalanced data (Van Hulse et al, 2007; Nikulin,
2008).
4.2 ILP formulation
Once the classifier is trained, we would like to learn
all edges (entailment rules) of each typed entailment
graph. Given a set of predicates V and an entail-
ment score function f : V ? V ? R derived from
the classifier, we want to find a graph G = (V,E)
that respects transitivity and maximizes the sum of
edge weights
?
(u,v)?E f(u, v). This problem is
NP-hard by a reduction from the NP-hard Transitive
Subgraph problem (Yannakakis, 1978). Thus, em-
ploying ILP is an appealing approach for obtaining
an optimal solution.
For two-types entailment graphs the formulation
is simple: The ILP variables are indicators Xuv de-
noting whether an edge (u, v) is in the graph, with
the following ILP:
613
G? = argmax
?
u6=v
f(u, v) ?Xuv (1)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1 (2)
?u,v?Ayes Xuv = 1 (3)
?u,v?Ano Xuv = 0 (4)
?u6=v Xuv ? {0, 1} (5)
The objective in Eq. 1 is a sum over the weights
of the eventual edges. The constraint in Eq. 2 states
that edges must respect transitivity. The constraints
in Eq. 3 and 4 state that for known node pairs, de-
fined by Ayes and Ano, we have background knowl-
edge indicating whether entailment holds or not. We
elaborate on how Ayes and Ano were constructed in
Section 5. For a graph with n nodes we get n(n?1)
variables and n(n?1)(n?2) transitivity constraints.
The simplest way to expand this formulation for
single-type graphs is to duplicate each predicate
node, with one node for each order of the types, and
then the ILP is unchanged. However, this is inef-
ficient as it results in an ILP with 2n(2n ? 1) vari-
ables and 2n(2n?1)(2n?2) transitivity constraints.
Since our main goal is to scale the use of ILP, we
modify it a little. We denote a direct-mapping edge
(u, v) by the indicator Xuv and a reversed-mapping
edge (u, v) by Yuv. The functions fd and fr provide
scores for direct and reversed mappings respectively.
The objective in Eq. 1 and the constraint in Eq. 2 are
replaced by (Eq. 3, 4 and 5 still exist and are carried
over in a trivial manner):
argmax
?
u6=v
fd(u, v)Xuv +
?
u,v
fr(u, v)Yuv (6)
s.t. ?u,v,w?V Xuv +Xvw ?Xuw ? 1
?u,v,w?V Xuv + Yvw ? Yuw ? 1
?u,v,w?V Yuv +Xvw ? Yuw ? 1
?u,v,w?V Yuv + Yvw ?Xuw ? 1
The modified constraints capture the transitivity
behavior of direct-mapping and reversed-mapping
edges, as described in Section 3. This results in
2n2 ? n variables and about 4n3 transitivity con-
straints, cutting the ILP size in half.
Next, we specify how to derive the function f
from the trained classifier using a probabilistic for-
mulation4. Following Snow et al (2006) and Be-
rant et al (2010), we utilize a probabilistic entail-
ment classifier that computes the posterior Puv =
P (Xuv = 1|Fuv). We want to use Puv to derive the
posterior P (G|F ), where F = ?u6=vFuv and Fuv is
the feature vector for a node pair (u, v).
Since the classifier was trained on a balanced
training set, the prior over the two entailment
classes is uniform and so by Bayes rule Puv ?
P (Fuv|Xuv = 1). Using that and the exact same
three independence assumptions described by Snow
et al (2006) and Berant et al (2010) we can show
that (for brevity, we omit the full derivation):
G? = argmaxG logP (G|F ) = (7)
argmax
?
u6=v
(log
Puv ? P (Xuv = 1)
(1? Puv)P (Xuv = 0)
)Xuv
= argmax
?
u6=v
(log
Puv
1? Puv
)Xuv + log ? ? |E|
where ? = P (Xuv=1)P (Xuv=0) is the prior odds ratio for
an edge in the graph. Comparing Eq. 1 and 7 we
see that f(u, v) = log Puv ?P (Xuv=1)(1?Puv)P (Xuv=0) . Note that f
is composed of a likelihood component and an edge
prior expressed by P (Xuv = 1), which we assume
to be some constant. This constant is a parameter
that affects graph sparsity and controls the trade-off
between recall and precision.
Next, we show how sparsity is exploited to scale
the use of ILP solvers. We discuss two-types entail-
ment graphs, but generalization is simple.
4.3 Graph decomposition
Though ILP solvers provide an optimal solution,
they substantially restrict the size of graphs we can
work with. The number of constraints is O(n3),
and solving graphs of size > 50 is often not feasi-
ble. To overcome this, we take advantage of graph
sparsity: most predicates in language do not entail
one another. Thus, it might be possible to decom-
pose graphs into small components and solve each
4We describe two-types graphs but extending to single-type
graphs is straightforward.
614
Algorithm 1 Decomposed-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: E? = {(u, v) : f(u, v) > 0 ? f(v, u) > 0}
2: V1, V2, ..., Vk ? connected components of
G? = (V,E?)
3: for i = 1 to k do
4: Ei ? ApplyILPSolve(Vi,f)
5: end for
6: E? ?
?k
i=1Ei
component separately. This is formalized in the next
proposition.
Proposition 1. If we can partition a set of nodes
V into disjoint sets U,W such that for any cross-
ing edge (u,w) between them (in either direction),
f(u,w) < 0, then the optimal set of edgesEopt does
not contain any crossing edge.
Proof Assume by contradiction that Eopt con-
tains a set of crossing edges Ecross. We can
construct Enew = Eopt \ Ecross. Clearly?
(u,v)?Enew f(u, v) >
?
(u,v)?Eopt f(u, v), as
f(u, v) < 0 for any crossing edge.
Next, we show that Enew does not violate tran-
sitivity constraints. Assume it does, then the viola-
tion is caused by omitting the edges in Ecross. Thus,
there must be a node u ? U and w ? W (w.l.o.g)
such that for some node v, (u, v) and (v, w) are in
Enew, but (u,w) is not. However, this means either
(u, v) or (v, w) is a crossing edge, which is impossi-
ble since we omitted all crossing edges. Thus, Enew
is a better solution than Eopt, contradiction.
This proposition suggests a simple algorithm (see
Algorithm 1): Add to the graph an undirected edge
for any node pair with a positive score, then find the
connected components, and apply an ILP solver over
the nodes in each component. The edges returned
by the solver provide an optimal (not approximate)
solution to the optimization problem.
The algorithm?s complexity is dominated by the
ILP solver, as finding connected components takes
O(V 2) time. Thus, efficiency depends on whether
the graph is sparse enough to be decomposed into
small components. Note that the edge prior plays an
important role: low values make the graph sparser
and easier to solve. In Section 5 we empirically test
Algorithm 2 Incremental-ILP
Input: A set V and a function f : V ? V ? R
Output: An optimal set of directed edges E?
1: ACT,VIO? ?
2: repeat
3: E? ? ApplyILPSolve(V,f,ACT)
4: VIO? violated(V,E?)
5: ACT? ACT ? VIO
6: until |VIO| = 0
how typed entailment graphs benefit from decompo-
sition given different prior values.
From a more general perspective, this algo-
rithm can be applied to any problem of learning
a sparse transitive binary relation. Such problems
include Co-reference Resolution (Finkel and Man-
ning, 2008) and Temporal Information Extraction
(Ling and Weld, 2010). Last, the algorithm can be
easily parallelized by solving each component on a
different core.
4.4 Incremental ILP
Another solution for scaling ILP is to employ in-
cremental ILP, which has been used in dependency
parsing (Riedel and Clarke, 2006). The idea is
that even if we omit the transitivity constraints, we
still expect most transitivity constraints to be satis-
fied, given a good local entailment classifier. Thus,
it makes sense to avoid specifying the constraints
ahead of time, but rather add them when they are
violated. This is formalized in Algorithm 2.
Line 1 initializes an active set of constraints and a
violated set of constraints (ACT;VIO). Line 3 applies
the ILP solver with the active constraints. Lines 4
and 5 find the violated constraints and add them to
the active constraints. The algorithm halts when no
constraints are violated. The solution is clearly op-
timal since we obtain a maximal solution for a less-
constrained problem.
A pre-condition for using incremental ILP is that
computing the violated constraints (Line 4) is effi-
cient, as it occurs in every iteration. We do that in
a straightforward manner: For every node v, and
edges (u, v) and (v, w), if (u,w) /? E? we add
(u, v, w) to the violated constraints. This is cubic
in worst-case but assuming the degree of nodes is
bounded by a constant it is linear, and performs very
615
fast in practice.
Combining Incremental-ILP and Decomposed-
ILP is easy: We decompose any large graph into
its components and apply Incremental ILP on each
component. We applied this algorithm on our evalu-
ation data set (Section 5) and found that it converges
in at most 6 iterations and that the maximal num-
ber of active constraints in large graphs drops from
? 106 to ? 103 ? 104.
5 Experimental Evaluation
In this section we empirically answer the follow-
ing questions: (1) Does transitivity improve rule
learning over typed predicates? (Section 5.1) (2)
Do Decomposed-ILP and Incremental-ILP improve
scalability? (Section 5.2)
5.1 Experiment 1
A data set of 1 million TextRunner tuples (Banko
et al, 2007), mapped to 10,672 distinct typed predi-
cates over 156 types was provided by Schoenmack-
ers et al (2010). Readers are referred to their pa-
per for details on mapping of tuples to typed predi-
cates. Since entailment only occurs between pred-
icates that share the same types, we decomposed
predicates by their types (e.g., all predicates with the
types place and disease) into 2,303 typed entailment
graphs. The largest graph contains 118 nodes and
the total number of potential rules is 263,756.
We generated a training set by applying the proce-
dure described in Section 4.1, yielding 2,644 exam-
ples. We used SVMperf (Joachims, 2005) to train a
Gaussian kernel classifier and computed Puv by pro-
jecting the classifier output score, Suv, with the sig-
moid function: Puv = 11+exp(?Suv) . We tuned two
SVM parameters using 5-fold cross validation and a
development set of two typed entailment graphs.
Next, we used our algorithm to learn rules. As
mentioned in Section 4.2, we integrate background
knowledge using the sets Ayes and Ano that contain
predicate pairs for which we know whether entail-
ment holds. Ayes was constructed with syntactic
rules: We normalized each predicate by omitting the
first word if it is a modal and turning passives to ac-
tives. If two normalized predicates are equal they are
synonymous and inserted into Ayes. Ano was con-
structed from 3 sources (1) Predicates differing by a
single pair of words that are WordNet antonyms (2)
Predicates differing by a single word of negation (3)
Predicates p(t1, t2) and p(t2, t1) where p is a transi-
tive verb (e.g., beat) in VerbNet (Kipper-Schuler et
al., 2000).
We compared our algorithm (termed ILPscale) to
the following baselines. First, to 10,000 rules re-
leased by Schoenmackers et al (2010) (Sherlock),
where the LHS contains a single predicate (Schoen-
mackers et al released 30,000 rules but 20,000 of
those have more than one predicate on the LHS,
see Section 2), as we learn rules over the same data
set. Second, to distributional similarity algorithms:
(a) SR: the score used by Schoenmackers et al as
part of the Sherlock system. (b) DIRT: (Lin and
Pantel, 2001) a widely-used rule learning algorithm.
(c) BInc: (Szpektor and Dagan, 2008) a directional
rule learning algorithm. Third, we compared to the
entailment classifier with no transitivity constraints
(clsf ) to see if combining distributional similarity
scores improves performance over single measures.
Last, we added to all baselines background knowl-
edge with Ayes and Ano (adding the subscript Xk to
their name).
To evaluate performance we manually annotated
all edges in 10 typed entailment graphs - 7 two-
types entailment graphs containing 14, 22, 30, 53,
62, 86 and 118 nodes, and 3 single-type entailment
graphs containing 7, 38 and 59 nodes. This annota-
tion yielded 3,427 edges and 35,585 non-edges, re-
sulting in an empirical edge density of 9%. We eval-
uate the algorithms by comparing the set of edges
learned by the algorithms to the gold standard edges.
Figure 2 presents the precision-recall curve of the
algorithms. The curve is formed by varying a score
threshold in the baselines and varying the edge prior
in ILPscale5. For figure clarity, we omit DIRT and
SR, since BInc outperforms them.
Table 2 shows micro-recall, precision and F1 at
the point of maximal F1, and the Area Under the
Curve (AUC) for recall in the range of 0-0.45 for all
algorithms, given background knowledge (knowl-
edge consistently improves performance by a few
points for all algorithms). The table also shows re-
sults for the rules from Sherlockk.
5we stop raising the prior when run time over the graphs
exceeds 2 hours. Often when the solver does not terminate in 2
hours, it also does not terminate after 24 hours or more.
616
00 . 2
0 . 4
0 . 6
0 . 8
1
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9
prec
isio
n
recall
BInc
clsf
BInc_k
clsf_k
ILP_scale
Figure 2: Precision-recall curve for the algorithms.
micro-average
R (%) P (%) F1 (%) AUC
ILPscale 43.4 42.2 42.8 0.22
clsfk 30.8 37.5 33.8 0.17
Sherlockk 20.6 43.3 27.9 N/A
BInck 31.8 34.1 32.9 0.17
SRk 38.4 23.2 28.9 0.14
DIRTk 25.7 31.0 28.1 0.13
Table 2: micro-average F1 and AUC for the algorithms.
Results show that using global transitivity
information substantially improves performance.
ILPscale is better than all other algorithms by a large
margin starting from recall .2, and improves AUC
by 29% and the maximal F1 by 27%. Moreover,
ILPscale doubles recall comparing to the rules from
the Sherlock resource, while maintaining compara-
ble precision.
5.2 Experiment 2
We want to test whether using our scaling tech-
niques, Decomposed-ILP and Incremental-ILP, al-
lows us to reach the optimal solution in graphs that
otherwise we could not solve, and consequently in-
crease the number of learned rules and the overall
recall. To check that, we run ILPscale, with and with-
out these scaling techniques (termed ILP?).
We used the same data set as in Experiment 1
and learned edges for all 2,303 entailment graphs
in the data set. If the ILP solver was unable to
hold the ILP in memory or took more than 2 hours
log ? # unlearned # rules 4 Red.
-1.75 9/0 6,242 / 7,466 20% 75%
-1 9/1 16,790 / 19,396 16% 29%
-0.6 9/3 26,330 / 29,732 13% 14%
Table 3: Impact of scaling techinques (ILP?/ILPscale).
for some graph, we did not attempt to learn its
edges. We ran ILPscale and ILP? in three den-
sity modes to examine the behavior of the algo-
rithms for different graph densities: (a) log ? =
?0.6: the configuration that achieved the best
recall/precision/F1 of 43.4/42.2/42.8. (b) log ? =
?1 with recall/precision/F1 of 31.8/55.3/40.4. (c)
log ? = ?1.75: A high precision configuration with
recall/precision/F1 of 0.15/0.75/0.23 6.
In each run we counted the number of graphs that
could not be learned and the number of rules learned
by each algorithm. In addition, we looked at the
20 largest graphs in our data (49-118 nodes) and
measured the ratio r between the size of the largest
component after applying Decomposed-ILP and the
original size of the graph. We then computed the av-
erage 1?r over the 20 graphs to examine how graph
size drops due to decomposition.
Table 3 shows the results. Column # unlearned
and # rules describe the number of unlearned graphs
and the number of learned rules. Column 4 shows
relative increase in the number of rules learned and
column Red. shows the average 1? r.
ILPscale increases the number of graphs that we
are able to learn: in our best configuration (log ? =
?0.6) only 3 graphs could not be handled com-
paring to 9 graphs when omitting our scaling tech-
niques. Since the unlearned graphs are among the
largest in the data set, this adds 3,500 additional
rules. We compared the precision of rules learned
only by ILPscale with that of the rules learned by
both, by randomly sampling 100 rules from each and
found precision to be comparable. Thus, the addi-
tional rules learned translate into a 13% increase in
relative recall without harming precision.
Also note that as density increases, the number of
rules learned grows and the effectiveness of decom-
position decreases. This shows how Decomposed-
ILP is especially useful for sparse graphs. We re-
6Experiment was run on an Intel i5 CPU with 4GB RAM.
617
lease the 29,732 rules learned by the configuration
log ? = ?0.6 as a resource.
To sum up, our scaling techniques allow us to
learn rules from graphs that standard ILP can not
handle and thus considerably increase recall without
harming precision.
6 Conclusions and Future Work
This paper proposes two contributions over two re-
cent works: In the first, Berant et al (2010) pre-
sented a global optimization procedure to learn en-
tailment rules between predicates using transitivity,
and applied this algorithm over small graphs where
all predicates have one argument instantiated by a
target concept. Consequently, the rules they learn
are of limited applicability. In the second, Schoen-
mackers et al learned rules of wider applicability by
using typed predicates, but utilized a local approach.
In this paper we developed an algorithm that uses
global optimization to learn widely-applicable en-
tailment rules between typed predicates (where both
arguments are variables). This was achieved by
appropriately defining entailment graphs for typed
predicates, formulating an ILP representation for
them, and introducing scaling techniques that in-
clude graph decomposition and incremental ILP.
Our algorithm is guaranteed to provide an optimal
solution and we have shown empirically that it sub-
stantially improves performance over Schoenmack-
ers et al?s recent resource and over several baselines.
In future work, we aim to scale the algorithm
further and learn entailment rules between untyped
predicates. This would require explicit modeling of
predicate ambiguity and using approximation tech-
niques when an optimal solution cannot be attained.
Acknowledgments
This work was performed with financial support
from the Turing Center at The University of Wash-
ington during a visit of the first author (NSF grant
IIS-0803481). We deeply thank Oren Etzioni and
Stefan Schoenmackers for providing us with the data
sets for this paper and for numerous helpful discus-
sions. We would also like to thank the anonymous
reviewers for their useful comments. This work
was developed under the collaboration of FBK-
irst/University of Haifa and was partially supported
by the Israel Science Foundation grant 1112/08. The
first author is grateful to IBM for the award of an
IBM Fellowship, and has carried out this research
in partial fulllment of the requirements for the Ph.D.
degree.
References
J. Fillmore Baker, C. F. and J. B. Lowe. 1998. The
Berkeley framenet project. In Proc. of COLING-ACL.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of ACL.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
EMNLP.
Saso Dzeroski and Ivan Brakto. 1992. Handling noise
in inductive logic programming. In Proceedings of the
International Workshop on Inductive Logic Program-
ming.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
ICML.
Karin Kipper-Schuler, Hoa Trand Dang, and Martha
Palmer. 2000. Class-based construction of verb lex-
icon. In Proceedings of AAAI/IAAI.
618
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of COL-
ING.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL.
Eric McCreath and Arun Sharma. 1997. ILP with noise
and fixed example size: a bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on artificial intelligence - Volume 2.
Vladimir Nikulin. 2008. Classification of imbalanced
data with random sets and mean-variance filtering.
IJDWM, 4(2):63?78.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
ACL.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Jason Van Hulse, Taghi Khoshgoftaar, and Amri Napoli-
tano. 2007. Experimental perspectives on learning
from imbalanced data. In Proceedings of ICML.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Mihalis Yannakakis. 1978. Node-and edge-deletion NP-
complete problems. In STOC ?78: Proceedings of the
tenth annual ACM symposium on Theory of comput-
ing, pages 253?264, New York, NY, USA. ACM.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
619
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 558?563,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Probabilistic Modeling Framework for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
Recognizing entailment at the lexical level is
an important and commonly-addressed com-
ponent in textual inference. Yet, this task has
been mostly approached by simplified heuris-
tic methods. This paper proposes an initial
probabilistic modeling framework for lexical
entailment, with suitable EM-based parame-
ter estimation. Our model considers promi-
nent entailment factors, including differences
in lexical-resources reliability and the impacts
of transitivity and multiple evidence. Evalu-
ations show that the proposed model outper-
forms most prior systems while pointing at re-
quired future improvements.
1 Introduction and Background
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). This task requires deciding whether a tex-
tual statement (termed the hypothesis-H) can be in-
ferred (entailed) from another text (termed the text-
T ). Since it was first introduced, the six rounds
of the Recognizing Textual Entailment (RTE) chal-
lenges1, currently organized under NIST, have be-
come a standard benchmark for entailment systems.
These systems tackle their complex task at vari-
ous levels of inference, including logical represen-
tation (Tatu and Moldovan, 2007; MacCartney and
Manning, 2007), semantic analysis (Burchardt et al,
2007) and syntactic parsing (Bar-Haim et al, 2008;
Wang et al, 2009). Inference at these levels usually
1http://www.nist.gov/tac/2010/RTE/index.html
requires substantial processing and resources (e.g.
parsing) aiming at high performance.
Nevertheless, simple entailment methods, per-
forming at the lexical level, provide strong baselines
which most systems did not outperform (Mirkin
et al, 2009; Majumdar and Bhattacharyya, 2010).
Within complex systems, lexical entailment model-
ing is an important component. Finally, there are
cases in which a full system cannot be used (e.g.
lacking a parser for a targeted language) and one
must resort to the simpler lexical approach.
While lexical entailment methods are widely
used, most of them apply ad hoc heuristics which do
not rely on a principled underlying framework. Typ-
ically, such methods quantify the degree of lexical
coverage of the hypothesis terms by the text?s terms.
Coverage is determined either by a direct match of
identical terms in T and H or by utilizing lexi-
cal semantic resources, such as WordNet (Fellbaum,
1998), that capture lexical entailment relations (de-
noted here as entailment rules). Common heuristics
for quantifying the degree of coverage are setting a
threshold on the percentage coverage of H?s terms
(Majumdar and Bhattacharyya, 2010), counting ab-
solute number of uncovered terms (Clark and Har-
rison, 2010), or applying an Information Retrieval-
style vector space similarity score (MacKinlay and
Baldwin, 2009). Other works (Corley and Mihal-
cea, 2005; Zanzotto and Moschitti, 2006) have ap-
plied a heuristic formula to estimate the similarity
between text fragments based on a similarity func-
tion between their terms.
These heuristics do not capture several important
aspects of entailment, such as varying reliability of
558
entailment resources and the impact of rule chaining
and multiple evidence on entailment likelihood. An
additional observation from these and other systems
is that their performance improves only moderately
when utilizing lexical resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. Inspired by the earlier steps
in the evolution of Statistical Machine Translation
methods (such as the initial IBM models (Brown et
al., 1993)), we formulate a concrete generative prob-
abilistic modeling framework that captures the basic
aspects of lexical entailment. Parameter estimation
is addressed by an EM-based approach, which en-
ables estimating the hidden lexical-level entailment
parameters from entailment annotations which are
available only at the sentence-level.
While heuristic methods are limited in their abil-
ity to wisely integrate indications for entailment,
probabilistic methods have the advantage of be-
ing extendable and enabling the utilization of well-
founded probabilistic methods such as the EM algo-
rithm.
We compared the performance of several model
variations to previously published results on RTE
data sets, as well as to our own implementation
of typical lexical baselines. Results show that
both the probabilistic model and our percentage-
coverage baseline perform favorably relative to prior
art. These results support the viability of the proba-
bilistic framework while pointing at certain model-
ing aspects that need to be improved.
2 Probabilistic Model
Under the lexical entailment scope, our modeling
goal is obtaining a probabilistic score for the like-
lihood that all H?s terms are entailed by T. To that
end, we model prominent aspects of lexical entail-
ment, which were mostly neglected by previous lex-
ical methods: (1) distinguishing different reliabil-
ity levels of lexical resources; (2) allowing transi-
tive chains of rule applications and considering their
length when estimating their validity; and (3) con-
sidering multiple entailments when entailing a term.
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
cha
in
t 1
t?
Re
so
urc
e 2
t n
h 1
h i
h m
t j
Tex
t:
Hyp
oth
es
is:
.
 
.
 
.
Re
so
urc
e 1
.
 
.
 
.
.
 
.
 
.MA
TC
H
Re
so
urc
e 1
.
 
.
 
.
Re
so
urc
e 3
Figure 1: The generative process of entailing terms of a hy-
pothesis from a text. Edges represent entailment rules. There
are 3 evidences for the entailment of hi: a rule from Resource1,
another one from Resource3 both suggesting that tj entails it,
and a chain from t1 through an intermediate term t?.
2.1 Model Description
For T to entail H it is usually a necessary, but not
sufficient, that every term h ? H would be en-
tailed by at least one term t ? T (Glickman et al,
2006). Figure 1 describes the process of entailing
hypothesis terms. The trivial case is when identical
terms, possibly at the stem or lemma level, appear
in T and H (a direct match as tn and hm in Fig-
ure 1). Alternatively, we can establish entailment
based on knowledge of entailing lexical-semantic
relations, such as synonyms, hypernyms and mor-
phological derivations, available in lexical resources
(e.g the rule inference? reasoning from WordNet).
We denote by R(r) the resource which provided the
rule r.
Since entailment is a transitive relation, rules may
compose transitive chains that connect a term t ? T
to a term h ? H through intermediate terms. For
instance, from the rules infer? inference and infer-
ence ? reasoning we can deduce the rule infer ?
reasoning (were inference is the intermediate term
as t? in Figure 1).
Multiple chains may connect t to h (as for tj and
hi in Figure 1) or connect several terms in T to h
(as t1 and tj are indicating the entailment of hi in
Figure 1), thus providing multiple evidence for h?s
entailment. It is reasonable to expect that if a term t
indeed entails a term h, it is likely to find evidences
for this relation in several resources.
Taking a probabilistic perspective, we assume a
559
parameter ?R for each resource R, denoting its re-
liability, i.e. the prior probability that applying a
rule from R corresponds to a valid entailment in-
stance. Direct matches are considered as a special
?resource?, called MATCH, for which ?MATCH is ex-
pected to be close to 1.
We now present our probabilistic model. For a
text term t ? T to entail a hypothesis term h by a
chain c, denoted by t
c
?? h, the application of every
r ? c must be valid. Note that a rule r in a chain c
connects two terms (its left-hand-side and its right-
hand-side, denoted lhs ? rhs). The lhs of the first
rule in c is t ? T and the rhs of the last rule in it is
h ? H . We denote the event of a valid rule applica-
tion by lhs
r
?? rhs. Since a-priori a rule r is valid
with probability ?R(r), and assuming independence
of all r ? c, we obtain Eq. 1 to specify the prob-
ability of the event t
c
?? h. Next, let C(h) denote
the set of chains which suggest the entailment of h.
The probability that T does not entail h at all (by
any chain), specified in Eq. 2, is the probability that
all these chains are not valid. Finally, the probabil-
ity that T entails all of H , assuming independence
of H?s terms, is the probability that every h ? H is
entailed, as given in Eq. 3. Notice that there could
be a term h which is not covered by any available
rule chain. Under this formulation, we assume that
each such h is covered by a single rule coming from
a special ?resource? called UNCOVERED (expecting
?UNCOVERED to be relatively small).
p(t
c
?? h) =
?
r?c
p(lhs
r
?? rhs) =
?
r?c
?R(r)(1)
p(T 9 h) =
?
c?C(h)
[1? p(t
c
?? h)] (2)
p(T ? H) =
?
h?H
p(T ? h) (3)
As can be seen, our model indeed distinguishes
varying resource reliability, decreases entailment
probability as rule chains grow and increases it when
entailment of a term is supported by multiple chains.
The above treatment of uncovered terms in H ,
as captured in Eq. 3, assumes that their entailment
probability is independent of the rest of the hypoth-
esis. However, when the number of covered hypoth-
esis terms increases the probability that the remain-
ing terms are actually entailed by T increases too
(even though we do not have supporting knowledge
for their entailment). Thus, an alternative model is
to group all uncovered terms together and estimate
the overall probability of their joint entailment as a
function of the lexical coverage of the hypothesis.
We denote Hc as the subset of H?s terms which are
covered by some rule chain and Huc as the remain-
ing uncovered part. Eq. 3a then provides a refined
entailment model for H , in which the second term
specifies the probability that Huc is entailed given
that Hc is validly entailed and the corresponding
lengths:
p(T?H) = [
?
h?Hc
p(T?h)]?p(T?Huc | |Hc|,|H|)
(3a)
2.2 Parameter Estimation
The difficulty in estimating the ?R values is that
these are term-level parameters while the RTE-
training entailment annotation is given for the
sentence-level. Therefore, we use EM-based esti-
mation for the hidden parameters (Dempster et al,
1977). In the E step we use the current ?R values
to compute all whcr(T,H) values for each training
pair. whcr(T,H) stands for the posterior probability
that application of the rule r in the chain c for h ? H
is valid, given that either T entails H or not accord-
ing to the training annotation (see Eq. 4). Remember
that a rule r provides an entailment relation between
its left-hand-side (lhs) and its right-hand-side (rhs).
Therefore Eq. 4 uses the notation lhs
r
?? rhs to des-
ignate the application of the rule r (similar to Eq. 1).
E :
whcr(T,H) =
?
????????
????????
p(lhs
r
?? rhs|T ? H) =
p(T?H|lhs
r??rhs)p(lhs r??rhs)
p(T?H)
if T ? H
p(lhs
r
?? rhs|T 9 H) =
p(T9H|lhs
r??rhs)p(lhs r??rhs)
p(T9H)
if T 9 H
(4)
After applying Bayes? rule we get a fraction with
Eq. 3 in its denominator and ?R(r) as the second term
of the numerator. The first numerator term is defined
as in Eq. 3 except that for the corresponding rule ap-
plication we substitute ?R(r) by 1 (per the condition-
ing event). The probabilistic model defined by Eq.
1-3 is a loop-free directed acyclic graphical model
560
(aka a Bayesian network). Hence the E-step proba-
bilities can be efficiently calculated using the belief
propagation algorithm (Pearl, 1988).
The M step uses Eq. 5 to update the parameter set.
For each resourceR we average thewhcr(T,H) val-
ues for all its rule applications in the training, whose
total number is denoted nR.
M : ?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr(T,H)
(5)
For Eq. 3a we need to estimate also p(T?Huc |
|Hc|,|H|). This is done directly via maximum likeli-
hood estimation over the training set, by calculating
the proportion of entailing examples within the set
of all examples of a given hypothesis length (|H|)
and a given number of covered terms (|Hc|). As
|Hc| we take the number of identical terms in T and
H (exact match) since in almost all cases terms in
H which have an exact match in T are indeed en-
tailed. We also tried initializing the EM algorithm
with these direct estimations but did not obtain per-
formance improvements.
3 Evaluations and Results
The 5th Recognizing Textual Entailment challenge
(RTE-5) introduced a new search task (Bentivogli
et al, 2009) which became the main task in RTE-
6 (Bentivogli et al, 2010). In this task participants
should find all sentences that entail a given hypothe-
sis in a given document cluster. This task?s data sets
reflect a natural distribution of entailments in a cor-
pus and demonstrate a more realistic scenario than
the previous RTE challenges.
In our system, sentences are tokenized and
stripped of stop words and terms are lemmatized and
tagged for part-of-speech. As lexical resources we
use WordNet (WN) (Fellbaum, 1998), taking as en-
tailment rules synonyms, derivations, hyponyms and
meronyms of the first senses of T and H terms, and
the CatVar (Categorial Variation) database (Habash
and Dorr, 2003). We allow rule chains of length up
to 4 in WordNet (WN4).
We compare our model to two types of baselines:
(1) RTE published results: the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge; (2) our implementation of lexical
coverage model, tuning the percentage-of-coverage
threshold for entailment on the training set. This
model uses the same configuration as our probabilis-
tic model. We also implemented an Information Re-
trieval style baseline3 (both with and without lex-
ical expansions), but given its poorer performance
we omit its results here.
Table 1 presents the results. We can see that
both our implemented models (probabilistic and
coverage) outperform all RTE lexical baselines on
both data sets, apart from (Majumdar and Bhat-
tacharyya, 2010) which incorporates additional lex-
ical resources, a named entity recognizer and a
co-reference system. On RTE-5, the probabilis-
tic model is comparable in performance to the best
full system, while the coverage model achieves con-
siderably better results. We notice that our imple-
mented models successfully utilize resources to in-
crease performance, as opposed to typical smaller
or less consistent improvements in prior works (see
Section 1).
Model
F1%
RTE-5 RTE-6
R
T
E
avg. of all systems 30.5 33.8
2nd best lexical system 40.31 44.02
best lexical system 44.43 47.64
best full system 45.63 48.05
co
ve
ra
ge
no resource 39.5 44.8
+ WN 45.8 45.1
+ CatVar 47.2 45.5
+ WN + CatVar 48.5 44.7
+ WN4 46.3 43.1
pr
ob
ab
il
is
ti
c no resource 41.8 42.1
+ WN 45.0 45.3
+ CatVar 42.0 45.9
+ WN + CatVar 42.8 45.5
+ WN4 45.8 42.6
Table 1: Evaluation results on RTE-5 and RTE-6. RTE systems
are: (1)(MacKinlay and Baldwin, 2009), (2)(Clark and Harri-
son, 2010), (3)(Mirkin et al, 2009)(2 submitted runs), (4)(Ma-
jumdar and Bhattacharyya, 2010) and (5)(Jia et al, 2010).
While the probabilistic and coverage models are
comparable on RTE-6 (with non-significant advan-
tage for the former), on RTE-5 the latter performs
3Utilizing Lucene search engine (http://lucene.apache.org)
561
better, suggesting that the probabilistic model needs
to be further improved. In particular, WN4 performs
better than the single-step WN only on RTE-5, sug-
gesting the need to improve the modeling of chain-
ing. The fluctuations over the data sets and impacts
of resources suggest the need for further investiga-
tion over additional data sets and resources. As for
the coverage model, under our configuration it poses
a bigger challenge for RTE systems than perviously
reported baselines. It is thus proposed as an easy to
implement baseline for future entailment research.
4 Conclusions and Future Work
This paper presented, for the first time, a principled
and relatively rich probabilistic model for lexical en-
tailment, amenable for estimation of hidden lexical-
level parameters from standard sentence-level an-
notations. The positive results of the probabilistic
model compared to prior art and its ability to exploit
lexical resources indicate its future potential. Yet,
further investigation is needed. For example, analyz-
ing current model?s limitations, we observed that the
multiplicative nature of eqs. 1 and 3 (reflecting inde-
pendence assumptions) is too restrictive, resembling
a logical AND. Accordingly we plan to explore re-
laxing this strict conjunctive behavior through mod-
els such as noisy-AND (Pearl, 1988). We also in-
tend to explore the contribution of our model, and
particularly its estimated parameter values, within a
complex system that integrates multiple levels of in-
ference.
Acknowledgments
This work was partially supported by the NEGEV
Consortium of the Israeli Ministry of Industry,
Trade and Labor (www.negev-initiative.org), the
PASCAL-2 Network of Excellence of the European
Community FP7-ICT-2007-1-216886, the FIRB-
Israel research project N. RBIN045PXH and by the
Israel Science Foundation grant 1112/08.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proceedings
of Text Analysis Conference (TAC).
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of Text Analysis Conference (TAC).
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACLWorkshop on Empirical Modeling of Semantic
Equivalence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?179. As-
sociation for Computational Linguistics.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the North
American Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participa-
tion at TAC 2010 RTE and summarization track. In
Proceedings of Text Analysis Conference (TAC).
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
562
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of Text Analysis Conference (TAC).
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of Text Analysis Confer-
ence (TAC).
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009. Addressing discourse and document structure in
the RTE search task. In Proceedings of Text Analysis
Conference (TAC).
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proceedings of Text Analysis
Conference (TAC).
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics.
563
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117?125,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-based Approximation for Entailment Graph Learning
Jonathan Berant?, Ido Dagan?, Meni Adler?, Jacob Goldberger?
? The Blavatnik School of Computer Science, Tel Aviv University
? Department of Computer Science, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
jonatha6@post.tau.ac.il
{dagan,goldbej}@{cs,eng}.biu.ac.il
adlerm@cs.bgu.ac.il
Abstract
Learning entailment rules is fundamental in
many semantic-inference applications and has
been an active field of research in recent years.
In this paper we address the problem of learn-
ing transitive graphs that describe entailment
rules between predicates (termed entailment
graphs). We first identify that entailment
graphs exhibit a ?tree-like? property and are
very similar to a novel type of graph termed
forest-reducible graph. We utilize this prop-
erty to develop an iterative efficient approxi-
mation algorithm for learning the graph edges,
where each iteration takes linear time. We
compare our approximation algorithm to a
recently-proposed state-of-the-art exact algo-
rithm and show that it is more efficient and
scalable both theoretically and empirically,
while its output quality is close to that given
by the optimal solution of the exact algorithm.
1 Introduction
Performing textual inference is in the heart of many
semantic inference applications such as Question
Answering (QA) and Information Extraction (IE). A
prominent generic paradigm for textual inference is
Textual Entailment (TUE) (Dagan et al, 2009). In
TUE, the goal is to recognize, given two text frag-
ments termed text and hypothesis, whether the hy-
pothesis can be inferred from the text. For example,
the text ?Cyprus was invaded by the Ottoman Em-
pire in 1571? implies the hypothesis ?The Ottomans
attacked Cyprus?.
Semantic inference applications such as QA and
IE crucially rely on entailment rules (Ravichandran
and Hovy, 2002; Shinyama and Sekine, 2006) or
equivalently inference rules, that is, rules that de-
scribe a directional inference relation between two
fragments of text. An important type of entailment
rule specifies the entailment relation between natu-
ral language predicates, e.g., the entailment rule ?X
invade Y ? X attack Y? can be helpful in inferring
the aforementioned hypothesis. Consequently, sub-
stantial effort has been made to learn such rules (Lin
and Pantel, 2001; Sekine, 2005; Szpektor and Da-
gan, 2008; Schoenmackers et al, 2010).
Textual entailment is inherently a transitive rela-
tion , that is, the rules ?x ? y? and ?y ? z? imply
the rule ?x ? z?. Accordingly, Berant et al (2010)
formulated the problem of learning entailment rules
as a graph optimization problem, where nodes are
predicates and edges represent entailment rules that
respect transitivity. Since finding the optimal set of
edges respecting transitivity is NP-hard, they em-
ployed Integer Linear Programming (ILP) to find the
exact solution. Indeed, they showed that applying
global transitivity constraints improves rule learning
comparing to methods that ignore graph structure.
More recently, Berant et al (Berant et al, 2011) in-
troduced a more efficient exact algorithm, which de-
composes the graph into connected components and
then applies an ILP solver over each component.
Despite this progress, finding the exact solution
remains NP-hard ? the authors themselves report
they were unable to solve some graphs of rather
moderate size and that the coverage of their method
is limited. Thus, scaling their algorithm to data sets
with tens of thousands of predicates (e.g., the extrac-
tions of Fader et al (2011)) is unlikely.
117
In this paper we present a novel method for learn-
ing the edges of entailment graphs. Our method
computes much more efficiently an approximate so-
lution that is empirically almost as good as the exact
solution. To that end, we first (Section 3) conjecture
and empirically show that entailment graphs exhibit
a ?tree-like? property, i.e., that they can be reduced
into a structure similar to a directed forest.
Then, we present in Section 4 our iterative ap-
proximation algorithm, where in each iteration a
node is removed and re-attached back to the graph in
a locally-optimal way. Combining this scheme with
our conjecture about the graph structure enables a
linear algorithm for node re-attachment. Section 5
shows empirically that this algorithm is by orders of
magnitude faster than the state-of-the-art exact al-
gorithm, and that though an optimal solution is not
guaranteed, the area under the precision-recall curve
drops by merely a point.
To conclude, the contribution of this paper is two-
fold: First, we define a novel modeling assumption
about the tree-like structure of entailment graphs and
demonstrate its validity. Second, we exploit this as-
sumption to develop a polynomial approximation al-
gorithm for learning entailment graphs that can scale
to much larger graphs than in the past. Finally, we
note that learning entailment graphs bears strong
similarities to related tasks such as Taxonomy In-
duction (Snow et al, 2006) and Ontology induction
(Poon and Domingos, 2010), and thus our approach
may improve scalability in these fields as well.
2 Background
Until recently, work on learning entailment rules be-
tween predicates considered each rule independently
of others and did not exploit global dependencies.
Most methods utilized the distributional similarity
hypothesis that states that semantically similar pred-
icates occur with similar arguments (Lin and Pan-
tel, 2001; Szpektor et al, 2004; Yates and Etzioni,
2009; Schoenmackers et al, 2010). Some meth-
ods extracted rules from lexicographic resources
such as WordNet (Szpektor and Dagan, 2009) or
FrameNet (Bob and Rambow, 2009; Ben Aharon et
al., 2010), and others assumed that semantic rela-
tions between predicates can be deduced from their
co-occurrence in a corpus via manually-constructed
patterns (Chklovski and Pantel, 2004).
Recently, Berant et al (2010; 2011) formulated
the problem as the problem of learning global entail-
ment graphs. In entailment graphs, nodes are predi-
cates (e.g., ?X attack Y?) and edges represent entail-
ment rules between them (?X invade Y ? X attack
Y?). For every pair of predicates i, j, an entailment
score wij was learned by training a classifier over
distributional similarity features. A positive wij in-
dicated that the classifier believes i? j and a nega-
tive wij indicated that the classifier believes i 9 j.
Given the graph nodes V (corresponding to the pred-
icates) and the weighting function w : V ? V ? R,
they aim to find the edges of a graph G = (V,E)
that maximize the objective
?
(i,j)?E wij under the
constraint that the graph is transitive (i.e., for every
node triplet (i, j, k), if (i, j) ? E and (j, k) ? E,
then (i, k) ? E).
Berant et al proved that this optimization prob-
lem, which we term Max-Trans-Graph, is NP-hard,
and so described it as an Integer Linear Program
(ILP). Let xij be a binary variable indicating the ex-
istence of an edge i ? j in E. Then, X = {xij :
i 6= j} are the variables of the following ILP for
Max-Trans-Graph:
argmax
X
?
i 6=j
wij ? xij (1)
s.t. ?i,j,k?V xij + xjk ? xik ? 1
?i,j?V xij ? {0, 1}
The objective function is the sum of weights over the
edges of G and the constraint xij + xjk ? xik ? 1
on the binary variables enforces that whenever xij=
xjk=1, then also xik = 1 (transitivity).
Since ILP is NP-hard, applying an ILP solver di-
rectly does not scale well because the number of
variables isO(|V |2) and the number of constraints is
O(|V |3). Thus, even a graph with?80 nodes (predi-
cates) has more than half a million constraints. Con-
sequently, in (Berant et al, 2011), they proposed a
method that efficiently decomposes the graph into
smaller components and applies an ILP solver on
each component separately using a cutting-plane
procedure (Riedel and Clarke, 2006). Although this
method is exact and improves scalability, it does
not guarantee an efficient solution. When the graph
does not decompose into sufficiently small compo-
nents, and the weights generate many violations of
118
transitivity, solving Max-Trans-Graph becomes in-
tractable. To address this problem, we present in
this paper a method for approximating the optimal
set of edges within each component and show that
it is much more efficient and scalable both theoreti-
cally and empirically.
Do and Roth (2010) suggested a method for a re-
lated task of learning taxonomic relations between
terms. Given a pair of terms, a small graph is con-
structed and constraints are imposed on the graph
structure. Their work, however, is geared towards
scenarios where relations are determined on-the-fly
for a given pair of terms and no global knowledge
base is explicitly constructed. Thus, their method
easily produces solutions where global constraints,
such as transitivity, are violated.
Another approximation method that violates tran-
sitivity constraints is LP relaxation (Martins et al,
2009). In LP relaxation, the constraint xij ? {0, 1}
is replaced by 0 ? xij ? 1, transforming the prob-
lem from an ILP to a Linear Program (LP), which
is polynomial. An LP solver is then applied on the
problem, and variables xij that are assigned a frac-
tional value are rounded to their nearest integer and
so many violations of transitivity easily occur. The
solution when applying LP relaxation is not a transi-
tive graph, but nevertheless we show for comparison
in Section 5 that our method is much faster.
Last, we note that transitive relations have been
explored in adjacent fields such as Temporal Infor-
mation Extraction (Ling and Weld, 2010), Ontol-
ogy Induction (Poon and Domingos, 2010), and Co-
reference Resolution (Finkel and Manning, 2008).
3 Forest-reducible Graphs
The entailment relation, described by entailment
graphs, is typically from a ?semantically-specific?
predicate to a more ?general? one. Thus, intuitively,
the topology of an entailment graph is expected to be
?tree-like?. In this section we first formalize this in-
tuition and then empirically analyze its validity. This
property of entailment graphs is an interesting topo-
logical observation on its own, but also enables the
efficient approximation algorithm of Section 4.
For a directed edge i ? j in a directed acyclic
graphs (DAG), we term the node i a child of node
j, and j a parent of i. A directed forest is a DAG
Xdisease be 
epidemic in 
 Ycountry 
Xdisease 
common in 
 Ycountry 
Xdisease 
occur in 
 Ycountry 
Xdisease 
frequent in 
 Ycountry 
Xdisease 
begin in 
 Ycountry 
be epidemic in 
common in 
frequent in 
occur in 
begin in 
be epidemic in 
common in 
 frequent in 
occur in 
begin in 
(a) 
(b) 
(c) 
Figure 1: A fragment of an entailment graph (a), its SCC
graph (b) and its reduced graph (c). Nodes are predicates
with typed variables (see Section 5), which are omitted in
(b) and (c) for compactness.
where all nodes have no more than one parent.
The entailment graph in Figure 1a (subgraph from
the data set described in Section 5) is clearly not a
directed forest ? it contains a cycle of size two com-
prising the nodes ?X common in Y? and ?X frequent in
Y?, and in addition the node ?X be epidemic in Y? has
3 parents. However, we can convert it to a directed
forest by applying the following operations. Any
directed graph G can be converted into a Strongly-
Connected-Component (SCC) graph in the follow-
ing way: every strongly connected component (a set
of semantically-equivalent predicates, in our graphs)
is contracted into a single node, and an edge is added
from SCC S1 to SCC S2 if there is an edge in G from
some node in S1 to some node in S2. The SCC graph
is always a DAG (Cormen et al, 2002), and if G is
transitive then the SCC graph is also transitive. The
graph in Figure 1b is the SCC graph of the one in
119
Xcountry annex  Yplace 
Xcountry invade  Yplace Yplace be part of Xcountry  
Figure 2: A fragment of an entailment graph that is not
an FRG.
Figure 1a, but is still not a directed forest since the
node ?X be epidemic in Y? has two parents.
The transitive closure of a directed graph G is
obtained by adding an edge from node i to node j
if there is a path in G from i to j. The transitive
reduction of G is obtained by removing all edges
whose absence does not affect its transitive closure.
In DAGs, the result of transitive reduction is unique
(Aho et al, 1972). We thus define the reduced graph
Gred = (Vred, Ered) of a directed graph G as the
transitive reduction of its SCC graph. The graph in
Figure 1c is the reduced graph of the one in Fig-
ure 1a and is a directed forest. We say a graph is a
forest-reducible graph (FRG) if all nodes in its re-
duced form have no more than one parent.
We now hypothesize that entailment graphs are
FRGs. The intuition behind this assumption is
that the predicate on the left-hand-side of a uni-
directional entailment rule has a more specific mean-
ing than the one on the right-hand-side. For instance,
in Figure 1a ?X be epidemic in Y? (where ?X? is a type
of disease and ?Y? is a country) is more specific than
?X common in Y? and ?X frequent in Y?, which are
equivalent, while ?X occur in Y? is even more gen-
eral. Accordingly, the reduced graph in Figure 1c
is an FRG. We note that this is not always the case:
for example, the entailment graph in Figure 2 is not
an FRG, because ?X annex Y? entails both ?Y be part
of X? and ?X invade Y?, while the latter two do not
entail one another. However, we hypothesize that
this scenario is rather uncommon. Consequently, a
natural variant of the Max-Trans-Graph problem is
to restrict the required output graph of the optimiza-
tion problem (1) to an FRG. We term this problem
Max-Trans-Forest.
To test whether our hypothesis holds empirically
we performed the following analysis. We sampled
7 gold standard entailment graphs from the data set
described in Section 5, manually transformed them
into FRGs by deleting a minimal number of edges,
and measured recall over the set of edges in each
graph (precision is naturally 1.0, as we only delete
gold standard edges). The lowest recall value ob-
tained was 0.95, illustrating that deleting a very
small proportion of edges converts an entailment
graph into an FRG. Further support for the prac-
tical validity of this hypothesis is obtained from
our experiments in Section 5. In these experiments
we show that exactly solving Max-Trans-Graph and
Max-Trans-Forest (with an ILP solver) results in
nearly identical performance.
An ILP formulation for Max-Trans-Forest is sim-
ple ? a transitive graph is an FRG if all nodes in
its reduced graph have no more than one parent. It
can be verified that this is equivalent to the following
statement: for every triplet of nodes i, j, k, if i ? j
and i ? k, then either j ? k or k ? j (or both).
Therefore, the ILP is formulated by adding this lin-
ear constraint to ILP (1):
?i,j,k?V xij+xik+(1? xjk)+(1? xkj) ? 3 (2)
We note that despite the restriction to FRGs, Max-
Trans-Forest is an NP-hard problem by a reduction
from the X3C problem (Garey and Johnson, 1979).
We omit the reduction details for brevity.
4 Sequential Approximation Algorithms
In this section we present Tree-Node-Fix, an efficient
approximation algorithm for Max-Trans-Forest, as
well as Graph-Node-Fix, an approximation for Max-
Trans-Graph.
4.1 Tree-Node-Fix
The scheme of Tree-Node-Fix (TNF) is the follow-
ing. First, an initial FRG is constructed, using some
initialization procedure. Then, at each iteration a
single node v is re-attached (see below) to the FRG
in a way that improves the objective function. This
is repeated until the value of the objective function
cannot be improved anymore by re-attaching a node.
Re-attaching a node v is performed by removing
v from the graph and connecting it back with a better
set of edges, while maintaining the constraint that it
is an FRG. This is done by considering all possible
edges from/to the other graph nodes and choosing
120
(a) 
d 
c 
v ? c v 
c 
d1 ? d2 
v 
? ? ? 
r1 r2 
v (b) (b?) (c) 
r3 
? 
Figure 3: (a) Inserting v into a component c ? Vred. (b)
Inserting v as a child of c and a parent of a subset of c?s
children in Gred. (b?) A node d that is a descendant but
not a child of c can not choose v as a parent, as v becomes
its second parent. (c) Inserting v as a new root.
the optimal subset, while the rest of the graph re-
mains fixed. Formally, let Sv?in =
?
i 6=v wiv ? xiv
be the sum of scores over v?s incoming edges and
Sv?out =
?
k 6=v wvk ? xvk be the sum of scores over
v?s outgoing edges. Re-attachment amounts to opti-
mizing a linear objective:
argmax
Xv
(Sv-in + Sv-out) (3)
where the variables Xv ? X are indicators for all
pairs of nodes involving v. We approximate a solu-
tion for (1) by iteratively optimizing the simpler ob-
jective (3). Clearly, at each re-attachment the value
of the objective function cannot decrease, since the
optimization algorithm considers the previous graph
as one of its candidate solutions.
We now show that re-attaching a node v is lin-
ear. To analyze v?s re-attachment, we consider the
structure of the directed forest Gred just before v is
re-inserted, and examine the possibilities for v?s in-
sertion relative to that structure. We start by defin-
ing some helpful notations. Every node c ? Vred
is a connected component in G. Let vc ? c be an
arbitrary representative node in c. We denote by
Sv-in(c) the sum of weights from all nodes in c and
their descendants to v, and by Sv-out(c) the sum of
weights from v to all nodes in c and their ancestors:
Sv-in(c) =
?
i?c
wiv +
?
k /?c
wkvxkvc
Sv-out(c) =
?
i?c
wvi +
?
k /?c
wvkxvck
Note that {xvck, xkvc} are edge indicators in G
and not Gred. There are two possibilities for re-
attaching v ? either it is inserted into an existing
component c ? Vred (Figure 3a), or it forms a new
component. In the latter, there are also two cases:
either v is inserted as a child of a component c (Fig-
ure 3b), or not and then it becomes a root in Gred
(Figure 3c). We describe the details of these 3 cases:
Case 1: Inserting v into a component c ? Vred.
In this case we add in G edges from all nodes in c
and their descendants to v and from v to all nodes in
c and their ancestors. The score (3) in this case is
s1(c) , Sv-in(c) + Sv-out(c) (4)
Case 2: Inserting v as a child of some c ? Vred.
Once c is chosen as the parent of v, choosing v?s
children in Gred is substantially constrained. A node
that is not a descendant of c can not become a child
of v, since this would create a new path from that
node to c and would require by transitivity to add a
corresponding directed edge to c (but all graph edges
not connecting v are fixed). Moreover, only a direct
child of c can choose v as a parent instead of c (Fig-
ure 3b), since for any other descendant of c, v would
become a second parent, and Gred will no longer be
a directed forest (Figure 3b?). Thus, this case re-
quires adding in G edges from v to all nodes in c and
their ancestors, and also for each new child of v, de-
noted by d ? Vred, we add edges from all nodes in
d and their descendants to v. Crucially, although the
number of possible subsets of c?s children in Gred is
exponential, the fact that they are independent trees
in Gred allows us to go over them one by one, and
decide for each one whether it will be a child of v
or not, depending on whether Sv-in(d) is positive.
Therefore, the score (3) in this case is:
s2(c) , Sv-out(c)+
?
d?child(c)
max(0, Sv-in(d)) (5)
where child(c) are the children of c.
Case 3: Inserting v as a new root in Gred. Similar
to case 2, only roots of Gred can become children of
v. In this case for each chosen root r we add in G
edges from the nodes in r and their descendants to
v. Again, each root can be examined independently.
Therefore, the score (3) of re-attaching v is:
s3 ,
?
r
max(0, Sv-in(r)) (6)
where the summation is over the roots of Gred.
It can be easily verified that Sv-in(c) and
Sv-out(c) satisfy the recursive definitions:
121
Algorithm 1 Computing optimal re-attachment
Input: FRG G = (V,E), function w, node v ? V
Output: optimal re-attachment of v
1: remove v and compute Gred = (Vred, Ered).
2: for all c ? Vred in post-order compute Sv-in(c) (Eq.
7)
3: for all c ? Vred in pre-order compute Sv-out(c) (Eq.
8)
4: case 1: s1 = maxc?Vred s1(c) (Eq. 4)
5: case 2: s2 = maxc?Vred s2(c) (Eq. 5)
6: case 3: compute s3 (Eq. 6)
7: re-attach v according to max(s1, s2, s3).
Sv-in(c) =
?
i?c
wiv +
?
d?child(c)
Sv-in(d), c ? Vred (7)
Sv-out(c) =
?
i?c
wvi + Sv-out(p), c ? Vred (8)
where p is the parent of c in Gred. These recursive
definitions allow to compute in linear time Sv-in(c)
and Sv-out(c) for all c (given Gred) using dynamic
programming, before going over the cases for re-
attaching v. Sv-in(c) is computed going over Vred
leaves-to-root (post-order), and Sv-out(c) is com-
puted going over Vred root-to-leaves (pre-order).
Re-attachment is summarized in Algorithm 1.
Computing an SCC graph is linear (Cormen et al,
2002) and it is easy to verify that transitive reduction
in FRGs is also linear (Line 1). Computing Sv-in(c)
and Sv-out(c) (Lines 2-3) is also linear, as explained.
Cases 1 and 3 are trivially linear and in case 2 we go
over the children of all nodes in Vred. As the reduced
graph is a forest, this simply means going over all
nodes of Vred, and so the entire algorithm is linear.
Since re-attachment is linear, re-attaching all
nodes is quadratic. Thus if we bound the number
of iterations over all nodes, the overall complexity is
quadratic. This is dramatically more efficient and
scalable than applying an ILP solver. In Section
5 we ran TNF until convergence and the maximal
number of iterations over graph nodes was 8.
4.2 Graph-node-fix
Next, we show Graph-Node-Fix (GNF), a similar
approximation that employs the same re-attachment
strategy but does not assume the graph is an FRG.
Thus, re-attachment of a node v is done with an
ILP solver. Nevertheless, the ILP in GNF is sim-
pler than (1), since we consider only candidate edges
v  
i  k  
v  
i  k  
v
i k
v  
i  k  
Figure 4: Three types of transitivity constraint violations.
involving v. Figure 4 illustrates the three types of
possible transitivity constraint violations when re-
attaching v. The left side depicts a violation when
(i, k) /? E, expressed by the constraint in (9) below,
and the middle and right depict two violations when
the edge (i, k) ? E, expressed by the constraints
in (10). Thus, the ILP is formulated by adding the
following constraints to the objective function (3):
?i,k?V \{v} if (i, k) /? E, xiv + xvk ? 1 (9)
if (i, k) ? E, xvi ? xvk, xkv ? xiv (10)
xiv, xvk ? {0, 1} (11)
Complexity is exponential due to the ILP solver;
however, the ILP size is reduced by an order of mag-
nitude to O(|V |) variables and O(|V |2) constraints.
4.3 Adding local constraints
For some pairs of predicates i, j we sometimes have
prior knowledge whether i entails j or not. We term
such pairs local constraints, and incorporate them
into the aforementioned algorithms in the following
way. In all algorithms that apply an ILP solver, we
add a constraint xij = 1 if i entails j or xij = 0 if i
does not entail j. Similarly, in TNF we incorporate
local constraints by settingwij =? orwij = ??.
5 Experiments and Results
In this section we empirically demonstrate that TNF
is more efficient than other baselines and its output
quality is close to that given by the optimal solution.
5.1 Experimental setting
In our experiments we utilize the data set released
by Berant et al (2011). The data set contains 10 en-
tailment graphs, where graph nodes are typed pred-
icates. A typed predicate (e.g., ?Xdisease occur in
Ycountry?) includes a predicate and two typed vari-
ables that specify the semantic type of the argu-
ments. For instance, the typed variable Xdisease can
be instantiated by arguments such as ?flu? or ?dia-
betes?. The data set contains 39,012 potential edges,
122
of which 3,427 are annotated as edges (valid entail-
ment rules) and 35,585 are annotated as non-edges.
The data set alo contains, for every pair of pred-
icates i, j in every graph, a local score sij , which is
the output of a classifier trained over distributional
similarity features. A positive sij indicates that the
classifier believes i? j. The weighting function for
the graph edges w is defined as wij = sij??, where
? is a single parameter controlling graph sparseness:
as ? increases, wij decreases and becomes nega-
tive for more pairs of predicates, rendering the graph
more sparse. In addition, the data set contains a set
of local constraints (see Section 4.3).
We implemented the following algorithms for
learning graph edges, where in all of them the graph
is first decomposed into components according to
Berant et als method, as explained in Section 2.
No-trans Local scores are used without transitiv-
ity constraints ? an edge (i, j) is inserted iffwij > 0.
Exact-graph Berant et al?s exact method (2011)
for Max-Trans-Graph, which utilizes an ILP solver1.
Exact-forest Solving Max-Trans-Forest exactly
by applying an ILP solver (see Eq. 2).
LP-relax Solving Max-Trans-Graph approxi-
mately by applying LP-relaxation (see Section 2)
on each graph component. We apply the LP solver
within the same cutting-plane procedure as Exact-
graph to allow for a direct comparison. This also
keeps memory consumption manageable, as other-
wise all |V |3 constraints must be explicitly encoded
into the LP. As mentioned, our goal is to present
a method for learning transitive graphs, while LP-
relax produces solutions that violate transitivity.
However, we run it on our data set to obtain empiri-
cal results, and to compare run-times against TNF.
Graph-Node-Fix (GNF) Initialization of each
component is performed in the following way: if the
graph is very sparse, i.e. ? ? C for some constantC
(set to 1 in our experiments), then solving the graph
exactly is not an issue and we use Exact-graph. Oth-
erwise, we initialize by applying Exact-graph in a
sparse configuration, i.e., ? = C.
Tree-Node-Fix (TNF) Initialization is done as in
GNF, except that if it generates a graph that is not an
FRG, it is corrected by a simple heuristic: for every
node in the reduced graph Gred that has more than
1We use the Gurobi optimization package in all experiments.
l
l
l
l
l
l
l
?0.8 ?0.6 ?0.4 ?0.2 0.0
10
50
100
500
500
0
500
00
?lambda
sec
l Exact?graphLP?relaxGNFTNF
Figure 5: Run-time in seconds for various ?? values.
one parent, we choose from its current parents the
single one whose SCC is composed of the largest
number of nodes in G.
We evaluate algorithms by comparing the set of
gold standard edges with the set of edges learned by
each algorithm. We measure recall, precision and
F1 for various values of the sparseness parameter
?, and compute the area under the precision-recall
Curve (AUC) generated. Efficiency is evaluated by
comparing run-times.
5.2 Results
We first focus on run-times and show that TNF is
efficient and has potential to scale to large data sets.
Figure 5 compares run-times2 of Exact-graph,
GNF, TNF, and LP-relax as ?? increases and the
graph becomes denser. Note that the y-axis is in
logarithmic scale. Clearly, Exact-graph is extremely
slow and run-time increases quickly. For ? = 0.3
run-time was already 12 hours and we were unable
to obtain results for ? < 0.3, while in TNF we easily
got a solution for any ?. When ? = 0.6, where both
Exact-graph and TNF achieve best F1, TNF is 10
times faster than Exact-graph. When ? = 0.5, TNF
is 50 times faster than Exact-graph and so on. Most
importantly, run-time for GNF and TNF increases
much more slowly than for Exact-graph.
2Run on a multi-core 2.5GHz server with 32GB of RAM.
123
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.0
0.2
0.4
0.6
0.8
1.0
recall
prec
isio
n
l
l
ll l
l
l
l l
l
l
l l l
l
l l l
l
l
l
l Exact?graphTNFNo?trans
Figure 6: Precision (y-axis) vs. recall (x-axis) curve.
Maximal F1 on the curve is .43 for Exact-graph, .41 for
TNF, and .34 for No-trans. AUC in the recall range 0-0.5
is .32 for Exact-graph, .31 for TNF, and .26 for No-trans.
Run-time of LP-relax is also bad compared to
TNF and GNF. Run-time increases more slowly than
Exact-graph, but still very fast comparing to TNF.
When ? = 0.6, LP-relax is almost 10 times slower
than TNF, and when ? = ?0.1, LP-relax is 200
times slower than TNF. This points to the difficulty
of scaling LP-relax to large graphs.
As for the quality of learned graphs, Figure 6 pro-
vides a precision-recall curve for Exact-graph, TNF
and No-trans (GNF and LP-relax are omitted from
the figure and described below to improve readabil-
ity). We observe that both Exact-graph and TNF
substantially outperform No-trans and that TNF?s
graph quality is only slightly lower than Exact-graph
(which is extremely slow). Following Berant et al,
we report in the caption the maximal F1 on the curve
and AUC in the recall range 0-0.5 (the widest range
for which we have results for all algorithms). Note
that compared to Exact-graph, TNF reduces AUC by
a point and the maximal F1 score by 2 points only.
GNF results are almost identical to those of TNF
(maximal F1=0.41, AUC: 0.31), and in fact for all
? configurations TNF outperforms GNF by no more
than one F1 point. As for LP-relax, results are just
slightly lower than Exact-graph (maximal F1: 0.43,
AUC: 0.32), but its output is not a transitive graph,
and as shown above run-time is quite slow. Last, we
note that the results of Exact-forest are almost iden-
tical to Exact-graph (maximal F1: 0.43), illustrating
that assuming that entailment graphs are FRGs (Sec-
tion 3) is reasonable in this data set.
To conclude, TNF learns transitive entailment
graphs of good quality much faster than Exact-
graph. Our experiment utilized an available data
set of moderate size; However, we expect TNF to
scale to large data sets (that are currently unavail-
able), where other baselines would be impractical.
6 Conclusion
Learning large and accurate resources of entailment
rules is essential in many semantic inference appli-
cations. Employing transitivity has been shown to
improve rule learning, but raises issues of efficiency
and scalability.
The first contribution of this paper is a novel mod-
eling assumption that entailment graphs are very
similar to FRGs, which is analyzed and validated
empirically. The main contribution of the paper is
an efficient polynomial approximation algorithm for
learning entailment rules, which is based on this
assumption. We demonstrate empirically that our
method is by orders of magnitude faster than the
state-of-the-art exact algorithm, but still produces an
output that is almost as good as the optimal solution.
We suggest our method as an important step to-
wards scalable acquisition of precise entailment re-
sources. In future work, we aim to evaluate TNF on
large graphs that are automatically generated from
huge corpora. This of course requires substantial ef-
forts of pre-processing and test-set annotation. We
also plan to examine the benefit of TNF in learning
similar structures, e.g., taxonomies or ontologies.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT). The first author has carried out
this research in partial fulfilment of the requirements
for the Ph.D. degree.
124
References
Alfred V. Aho, Michael R. Garey, and Jeffrey D. Ullman.
1972. The transitive reduction of a directed graph.
SIAM Journal on Computing, 1(2):131?137.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics.
Coyne Bob and Owen Rambow. 2009. Lexpar: A freely
available english paraphrase lexicon automatically ex-
tracted from framenet. In Proceedings of IEEE Inter-
national Conference on Semantic Computing.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Empirical Methods in
Natural Language Processing.
Thomas H. Cormen, Charles E. leiserson, Ronald L.
Rivest, and Clifford Stein. 2002. Introduction to Al-
gorithms. The MIT Press.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based tax-
onomic relation classification. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of Empirical Methods in Nat-
ural Language Processing.
J. R. Finkel and C. D. Manning. 2008. Enforcing transi-
tivity in coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael R. Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Dan S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the 24th AAAI Con-
ference on Artificial Intelligence.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of Empirical Methods
in Natural Language Processing.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of Empirical
Methods in Natural Language Processing.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
125
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Two Level Model for Context Sensitive Inference Rules
Oren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Computer Science Department, Stanford University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
joberant@stanford.edu
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
1 Introduction
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y? can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like ?Which drugs relieve headache??. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al, 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al, 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al, 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al, 2010; Berant et
al., 2011; Nakashole et al, 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate?s arguments. For example, ?AT&T acquire T-
Mobile ? AT&T purchase T-Mobile?, is a valid
application of the rule ?X acquire Y ? X pur-
chase Y?, while ?Children acquire skills ? Chil-
dren purchase skills? is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al, 2007; Szpektor et
al., 2008; Ritter et al, 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
1331
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
?base? similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
2 Background and Model Setting
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
2.1 Context-insensitive Rule Learning
A predicate inference rule ?LHS ? RHS?, such
as ?X acquire Y ? X purchase Y?, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
?AT&T acquires T-Mobile?, the above rule infers
?AT&T purchases T-Mobile?.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vxpred
and vypred, respectively. Each entry of a vector vcorresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred, w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
?LHS ? RHS?. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT?s reliability score is defined as
follows:
(1)scoreDIRT(LHS ? RHS)
=
?
sim(vxl , vxr ) ? sim(v
y
l , v
y
r )
where sim(v, v?) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
1332
measure from (Lin, 1998), defined as follows:
(2)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
We note that the general DIRT scheme may be
used while employing other ?base? vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al, 2007; Szpektor and Dagan,
2008; Kotlerman et al, 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule?s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ?acquire ? purchase? than for
?acquire ? learn?, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
2.2 Context-sensitive Rule Applications
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al,
2007) and an LSA-based model (Szpektor et al,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al, 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al, 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (Se?aghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dypred, respectively. In comparison tothe distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set ofwords.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d,w) ? p(w|t) ? p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d,w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ?ac-
quire Y? in the context of the argument ?IBM?, we
expect high relevance for a topic about companies,
while in the context of the argument ?knowledge?
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d,w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
1333
Consider the application of an inference rule
?LHS ? RHS? in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
(3)scoreTopic(LHS ? RHS, wx, wy)
=
?
sim(dxl , dxr , wx) ? sim(d
y
l , d
y
r , wy)
where sim(d, d?, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al (2010) utilized the dot
product form for their similarity measure:
(4)simDC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?, w)]
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
(5)simSC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?)]
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v?) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d?, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
3 Two-level Context-sensitive Inference
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
(6)scoreWT(LHS ? RHS, wx, wy)
=
?
sim(vxl , vxr , wx) ? sim(v
y
l , v
y
r , wy)
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v?, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v?, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
3.1 Topic-biased Word-vector Similarities
Given a pair of word vectors v and v?, and
any desired ?base? vector similarity measure sim
(e.g. simLin), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v?). simt(v, v?) is computed by applying
1334
the original similarity measure over topic-biased
versions of v and v?, denoted by vt and v?t:
simt(v, v?) = sim(vt, v?t)
where
vt(w) = v(w) ? p(t|dv, w)
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ?acquire?.
As can be seen, the topic-biased score Lint for ?ac-
quire? learn? for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ?acquire? but
not with ?learn?, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
?acquire? purchase?.
3.2 Context-sensitive Similarity
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v?, w), where v and v? are the slot?s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule?s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its ?relevance? for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
acquire ? learn
Lint(v, v?) 0.040 0.334
Lin(v, v?) 0.165
acquire ? purchase
Lint(v, v?) 0.427 0.241
Lin(v, v?) 0.267
Table 1: Two characteristic topics for the Y slot of
?acquire?, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ?acquire? is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
tured by p(t|dv, w):
simWT(v, v?, w) =
?
t
[p(t|dv, w) ? simt(v, v?)]
(7)
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ?ac-
quire?. We observe that relative to the fixed
context-insensitive Lin score, the score of ?ac-
quire ? learn? is substantially promoted for
the argument ?skill? while being demoted for
?Skype?. The opposite behavior is observed for
?acquire ? purchase?, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
4 Experimental Settings
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
1335
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
?acquire Skype ? learn Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.039
Lin(v, v?) 0.165
?acquire Skype ? purchase Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.417
Lin(v, v?) 0.267
?acquire skill ? learn skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.251
Lin(v, v?) 0.165
?acquire skill ? purchase skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.181
Lin(v, v?) 0.267
Table 2: Context-sensitive similarity scores (in
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
4.1 Evaluated Rule Application Methods
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al, 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al, 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate?s slot X , and y, the instantiation of the tem-
plate?s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ?X can accommodate up
to Y?. Yet, many of these templates share a similar
meaning, e.g. ?X accommodate up to Y?, ?X can
accommodate up to Y?, ?X will accommodate up
to Y?, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
3ReVerb is available at http://reverb.cs.
washington.edu/
1336
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
Table 3: Sizes of rule application test set for each
learned rule-set.
model.4
4.2 Evaluation Task
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ?Jack agree
with Jill 9 Jack feel sorry for Jill? is a rule ap-
plication in this dataset, labeled as incorrect, and
?Registration open this month? Registration be-
gin this month? is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (?Jack?,?agree with?,?Jill?) and then sampling
possible rules for each, such as ?agree with? feel
sorry for?. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al?s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin?s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule appli-
cation ranking computed by this method. In order
4Our resource is available at: http://www.cs.biu.
ac.il/? nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/?nlp/downloads/
annotation-rule-application.htm
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
Table 4: MAP values on corresponding test set ob-
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters ? = 0.01 and ? = 0.1
for K < 600 and ? = 50K for K >= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p < 0.01 for BInc and
Lin, and p < 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
1337
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ?John bought my
car 9 John sold my car? the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ?my boss raised
my salary 9 my boss constructed my salary? the
context {?my boss?, ?my salary?} for applying
?raise? construct? is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p < 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
test-setivc test-setvc
Size
(valid:invalid)
432
(266:166)
645
(266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
Table 5: MAP results for the two split Lin test-
sets.
6 Discussion and Future Work
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al, 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
1338
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010). Other works
(Erk and Pado?, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
1339
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
1340
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283?288,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Lexical Expansion to Learn Inference Rules from Sparse Data
Oren Melamud?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates is widely addressed by com-
puting distributional similarity scores be-
tween vectors of argument words. In
this scheme, prior work typically refrained
from learning rules for low frequency
predicates associated with very sparse ar-
gument vectors due to expected low reli-
ability. To improve the learning of such
rules in an unsupervised way, we propose
to lexically expand sparse argument word
vectors with semantically similar words.
Our evaluation shows that lexical expan-
sion significantly improves performance
in comparison to state-of-the-art baselines.
1 Introduction
The benefit of utilizing template-based inference
rules between predicates was demonstrated in
NLP tasks such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y?, between the templates ?X treat Y? and ?X re-
lieve Y? may be useful to identify the answer to
?Which drugs relieve stomach ache??.
The predominant unsupervised approach for
learning inference rules between templates is via
distributional similarity (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Szpektor and Da-
gan, 2008). Specifically, each argument slot in
a template is represented by an argument vector,
containing the words (or terms) that instantiate this
slot in all of the occurrences of the template in a
learning corpus. Two templates are then deemed
semantically similar if the argument vectors of
their corresponding slots are similar.
Ideally, inference rules should be learned for
all templates that occur in the learning corpus.
However, many templates are rare and occur only
few times in the corpus. This is a typical NLP
phenomenon that can be associated with either a
small learning corpus, as in the cases of domain
specific corpora and resource-scarce languages, or
with templates with rare terms or long multi-word
expressions such as ?X be also a risk factor to Y?
or ?X finish second in Y?, which capture very spe-
cific meanings. Due to few occurrences, the slots
of rare templates are represented with very sparse
argument vectors, which in turn lead to low relia-
bility in distributional similarity scores.
A common practice in prior work for learn-
ing predicate inference rules is to simply disre-
gard templates below a minimal frequency thresh-
old (Lin and Pantel, 2001; Kotlerman et al, 2010;
Dinu and Lapata, 2010; Ritter et al, 2010). Yet,
acquiring rules for rare templates may be benefi-
cial both in terms of coverage, but also in terms
of more accurate rule application, since rare tem-
plates are less ambiguous than frequent ones.
We propose to improve the learning of rules be-
tween infrequent templates by expanding their ar-
gument vectors. This is done via a ?dual? distribu-
tional similarity approach, in which we consider
two words to be similar if they instantiate similar
sets of templates. We then use these similarities
to expand the argument vector of each slot with
words that were identified as similar to the original
arguments in the vector. Finally, similarities be-
tween templates are computed using the expanded
vectors, resulting in a ?smoothed? version of the
original similarity measure.
Evaluations on a rule application task show
that our lexical expansion approach significantly
improves the performance of the state-of-the-art
DIRT algorithm (Lin and Pantel, 2001). In addi-
tion, our approach outperforms a similarity mea-
sure based on vectors of latent topics instead of
word vectors, a common way to avoid sparseness
issues by means of dimensionality reduction.
283
2 Technical Background
The distributional similarity score for an inference
rule between two predicate templates, e.g. ?X re-
sign Y? X quit Y?, is typically computed by mea-
suring the similarity between the argument vec-
tors of the corresponding X slots and Y slots of
the two templates. To this end, first the argument
vectors should be constructed and then a similarity
measure between two vectors should be provided.
We note that we focus here on binary templates
with two slots each, but this approach can be ap-
plied to any template.
A common starting point is to compute a
co-occurrence matrix M from a learning cor-
pus. M ?s rows correspond to the template slots
and the columns correspond to the various terms
that instantiate the slots. Each entry Mi,j , e.g.
Mx quit,John, contains a count of the number of
times the term j instantiated the template slot i in
the corpus. Thus, each row Mi,? corresponds to
an argument vector for slot i. Next, some func-
tion of the counts is used to assign weights to all
Mi,j entries. In this paper we use pointwise mu-
tual information (PMI), which is common in prior
work (Lin and Pantel, 2001; Szpektor and Dagan,
2008).
Finally, rules are assessed using some similar-
ity measure between corresponding argument vec-
tors. The state-of-the-art DIRT algorithm (Lin and
Pantel, 2001) uses the highly cited Lin similarity
measures (Lin, 1998) to score rules between bi-
nary templates as follows:
(1)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
(2)DIRT (l ? r)
=
?
Lin(vl:x, vr:x) ? Lin(vl:y, vr:y)
where v and v? are two argument vectors, l and
r are the templates participating in the inference
rule and vl:x corresponds to the argument vector
of slot X of template l, etc. While the original
DIRT algorithm utilizes the Lin measure, one can
replace it with any other vector similarity measure.
A separate line of research for word simi-
larity introduced directional similarity measures
that have a bias for identifying generaliza-
tion/specification relations, i.e. relations be-
tween predicates with narrow (or specific) seman-
tic meanings to predicates with broader meanings
inferred by them (unlike the symmetric Lin). One
such example is the Cover measure (Weeds and
Weir, 2003):
(3)Cover(v, v?) =
?
w?v?v? [v(w)]?
w?v?v? [v(w)]
As can be seen, in the core of the Lin and Cover
measures, as well as in many other well known
distributional similarity measures such as Jaccard,
Dice and Cosine, stand the number of shared ar-
guments vs. the total number of arguments in the
two vectors. Therefore, when the argument vec-
tors are sparse, containing very few non-zero fea-
tures, these scores become unreliable and volatile,
changing greatly with every inclusion or exclusion
of a single shared argument.
3 Lexical Expansion Scheme
We wish to overcome the sparseness issues in rare
feature vectors, especially in cases where argu-
ment vectors of semantically similar predicates
comprise similar but not exactly identical argu-
ments. To this end, we propose a three step
scheme. First, we learn lexical expansion sets for
argument words, such as the set {euros, money}
for the word dollars. Then we use these sets to ex-
pand the argument word vectors of predicate tem-
plates. For example, given the template ?X can
be exchanged for Y?, with the following argument
words instantiating slot X {dollars, gold}, and
the expansion set above, we would expand the ar-
gument word vector to include all the following
words {dollars, euros, money, gold}. Finally, we
use the expanded argument word vectors to com-
pute the scores for predicate inference rules with a
given similarity measure.
When a template is instantiated with an ob-
served word, we expect it to also be instantiated
with semantically similar words such as the ones
in the expansion set of the observed word. We
?blame? the lack of such template occurrences
only on the size of the corpus and the sparseness
phenomenon in natural languages. Thus, we uti-
lize our lexical expansion scheme to synthetically
add these expected but missing occurrences, ef-
fectively smoothing or generalizing over the ex-
plicitly observed argument occurrences. Our ap-
proach is inspired by query expansion (Voorhees,
1994) in Information Retrieval (IR), as well as by
the recent lexical expansion framework proposed
in (Biemann and Riedl, 2013), and the work by
284
Miller et al (2012) on word sense disambigua-
tion. Yet, to the best of our knowledge, this is the
first work that applies lexical expansion to distri-
butional similarity feature vectors. We next de-
scribe our scheme in detail.
3.1 Learning Lexical Expansions
We start by constructing the co-occurrence matrix
M (Section 2), where each entry Mt:s,w indicates
the number of times that word w instantiates slot
s of template t in the learning corpus, denoted by
?t:s?, where s can be either X or Y.
In traditional distributional similarity, the rows
Mt:s,? serve as argument vectors of template slots.
However, to learn expansion sets we take a ?dual?
view and consider each matrix column M?:?,w (de-
noted vw) as a feature vector for the argument
word w. Under this view, templates (or more
specifically, template slots) are the features. For
instance, for the word dollars the respective fea-
ture vector may include entries such as ?X can be
exchanged for?, ?can be exchanged for Y?, ?pur-
chase Y? and ?sell Y?.
We next learn an expansion set per each word
w by computing the distributional similarity be-
tween the vectors of w and any other argument
word w?, sim(vw, vw?). Then we take the N most
similar words as w?s expansion set with degree
N , denoted by LNw = {w?1, ..., w?N}. Any simi-
larity measure could be used, but as our experi-
ments show, different measures generate sets with
different properties, and some may be fitter for ar-
gument vector expansion than others.
3.2 Expanding Argument Vectors
Given a row count vector Mt:s,? for slot s of tem-
plate t, we enrich it with expansion sets as fol-
lows. For each w in Mt:s,?, the original count in
vt:s(w) is redistributed equally between itself and
all words in w?s expansion set, i.e. all w? ? LNw ,
(possibly yielding fractional counts) where N is a
global parameter of the model. Specifically, the
new count that is assigned to each word w is its
remaining original count after it has been redis-
tributed (or zero if no original count), plus all the
counts that were distributed to it from other words.
Next, PMI weights are recomputed according to
the new counts, and the resulting expanded vector
is denoted by v+t:s. Similarity between template
slots is now computed over the expanded vectors
instead of the original ones, e.g. Lin(v+l:x, v+r:x).
4 Experimental Settings
We constructed a relatively small learning corpus
for investigating the sparseness issues of such cor-
pora. To this end, we used a random sample from
the large scale web-based ReVerb corpus1 (Fader
et al, 2011), comprising tuple extractions of pred-
icate templates with their argument instantiations.
We applied some clean-up preprocessing to these
extractions, discarding stop words, rare words and
non-alphabetical words that instantiated either the
X or the Y argument slots. In addition, we dis-
carded templates that co-occur with less than 5
unique argument words in either of their slots, as-
suming that such few arguments cannot convey re-
liable semantic information, even with expansion.
Our final corpus consists of around 350,000 ex-
tractions and 14,000 unique templates. In this cor-
pus around one third of the extractions refer to
templates that co-occur with at most 35 unique ar-
guments in both their slots.
We evaluated the quality of inference
rules using the dataset constructed by Zeich-
ner et al (2012)2, which contains about 6,500
manually annotated template rule applications,
each labeled as correct or not. For example,
?The game develop eye-hand coordination9 The
game launch eye-hand coordination? is a rule
application in this dataset of the rule ?X develop
Y ? X launch Y?, labeled as incorrect, and
?Captain Cook sail to Australia? Captain Cook
depart for Australia? is a rule application of the
rule ?X sail to Y ? X depart for Y?, labeled as
correct. Specifically, we induced two datasets
from Zeichner et al?s dataset, denoted DS-5-35
and DS-5-50, which consist of all rule applica-
tions whose templates are present in our learning
corpus and co-occurred with at least 5 and at
most 35 and 50 unique argument words in both
their slots, respectively. DS-5-35 includes 311
rule applications (104 correct and 207 incorrect)
and DS-5-50 includes 502 rule applications (190
correct and 312 incorrect).
Our evaluation task is to rank all rule applica-
tions in each test set based on the similarity scores
of the applied rules. Optimal performance would
rank all correct rule applications above the in-
correct ones. As a baseline for rule scoring we
1http://reverb.cs.washington.edu/
2http://www.cs.biu.ac.il/nlp/
downloads/annotation-rule-application.
htm
285
used the DIRT algorithm scheme, denoted DIRT-
LE-None. We then compared between the perfor-
mance of this baseline and its expanded versions,
testing two similarity measures for generating the
expansion sets of arguments: Lin and Cover. We
denote these expanded methods DIRT-LE-SIM-N,
where SIM is the similarity measure used to gen-
erate the expansion sets and N is the lexical expan-
sion degree, e.g. DIRT-LE-Lin-2.
We remind the reader that our scheme utilizes
two similarity measures. The first measure as-
sesses the similarity between the argument vectors
of the two templates in the rule. This measure
is kept constant in our experiments and is iden-
tical to DIRT?s similarity measure (Lin). 3 The
second measure assesses the similarity between
words and is used for the lexical expansion of ar-
gument vectors. Since this is the research goal
of this paper, we experimented with two different
measures for lexical expansion: a symmetric mea-
sure (Lin) and an asymmetric measure (Cover).
To this end we evaluated their effect on DIRT?s
rule ranking performance and compared them to a
vanilla version of DIRT without lexical expansion.
As another baseline, we follow Dinu and La-
pata (2010) inducing LDA topic vectors for tem-
plate slots and computing predicate template infer-
ence rule scores based on similarity between these
vectors. We use standard hyperparameters for
learning the LDA model (Griffiths and Steyvers,
2004). This method is denoted LDA-K, where K is
the number of topics in the model.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule applica-
tion ranking computed by this method. In order
to compute MAP values and corresponding sta-
tistical significance, we randomly split each test
set into 30 subsets. For each method we com-
puted Average Precision on every subset and then
took the average as the MAP value. We varied
the degree of the lexical expansion in our model
and the number of topics in the topic model base-
line to analyze their effect on the performance of
these methods on our datasets. We note that in our
model a greater degree of lexical expansion cor-
3Experiments with Cosine as the template similarity mea-
sure instead of Lin for both DIRT and its expanded versions
yielded similar results. We omit those for brevity.
responds to more aggressive smoothing (or gen-
eralization) of the explicitly observed data, while
the same goes for a lower number of topics in the
topic model. The results on DS-5-35 and DS-5-50
are illustrated in Figure 1.
The most dramatic improvement over the base-
lines is evident in DS-5-35, where DIRT-LE-
Cover-2 achieves a MAP score of 0.577 in com-
parison to 0.459 achieved by its DIRT-LE-None
baseline. This is indeed the dataset where we ex-
pected expansion to affect most due the extreme
sparseness of argument vectors. Both DIRT-LE-
Cover-N and DIRT-LE-Lin-N outperform DIRT-
LE-None for all tested values of N , with statisti-
cal significance via a paired t-test at p < 0.05 for
DIRT-LE-Cover-N where 1 ? N ? 5, and p <
0.01 for DIRT-LE-Cover-2. On DS-5-50, improve-
ment over the DIRT-LE-None baseline is still sig-
nificant with both DIRT-LE-Cover-N and DIRT-
LE-Lin-N outperforming DIRT-LE-None. DIRT-
LE-Cover-N again performs best and achieves a
relative improvement of over 10% with statistical
significance at p < 0.05 for 2 ? N ? 3.
The above shows that expansion is effective for
improving rule learning between infrequent tem-
plates. Furthermore, the fact that DIRT-LE-Cover-
N outperforms DIRT-LE-Lin-N suggests that us-
ing directional expansions, which are biased to
generalizations of the observed argument words,
e.g. vehicle as an expansion for car, is more ef-
fective than using symmetrically related words,
such as bicycle or automobile. This conclusion
appears also to be valid from a semantic reason-
ing perspective, as given an observed predicate-
argument occurrence, such as ?drive car? we can
more likely infer that a presumed occurrence of
the same predicate with a generalization of the ar-
gument, such as ?drive vehicle?, is valid, i.e. ?drive
car ? drive vehicle?. On the other hand while
?drive car ? drive automobile? is likely to be
valid, ?drive car ? drive bicycle? and ?drive ve-
hicle? drive bicycle? are not.
Figure 1 also depicts the performance of LDA
as a vector smoothing approach. LDA-K out-
performs the DIRT-LE-None baseline under DS-
5-35 but with no statistical significance. Under
DS-5-50 LDA-K performs worst, slightly outper-
forming DIRT-LE-None only for K=450. Further-
more, under both datasets, LDA-K is outperformed
by DIRT-LE-Cover-N. These results indicate that
LDA is less effective than our expansion approach.
286
Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,
and for the compared smoothing methods as follows. DIRT with varied degrees of lexical expansion
is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is
denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),
depending on the tested method.
One reason may be that in our model, every expan-
sion set may be viewed as a cluster around a spe-
cific word, an outstanding difference in compari-
son to topics, which provide a global partition over
all words. We note that performance improve-
ment of singleton document clusters over global
partitions was also shown in IR (Kurland and Lee,
2009).
In order to further illustrate our lexical expan-
sion scheme we focus on the rule application
?Captain Cook sail to Australia? Captain Cook
depart for Australia?, which is labeled as correct
in our test set and corresponds to the rule ?X sail
to Y ? X depart for Y?. There are 30 words in-
stantiating the X slot of the predicate ?sail to?
in our learning corpus including {Columbus, em-
peror, James, John, trader}. On the other hand,
there are 18 words instantiating the X slot of the
predicate ?depart for? including {Amanda, Jerry,
Michael, mother, queen}. While semantic simi-
larity between these two sets of words is evident,
they share no words in common, and therefore the
original DIRT algorithm, DIRT-LE-None, wrongly
assigns a zero score to the rule.
The following are descriptions of some of the
argument word expansions performed by DIRT-
LE-Cover-2 (using the notation LNw defined in Sec-
tion 3.1) for the X slot of ?sail to? L2John = {mr.,
dr.}, L2trader = {people, man}, and for the X slot
of ?depart for?, L2Michael = {John, mr.}, L2mother =
{people, woman}. Given these expansions the two
slots now share the following words {mr. ,people,
John} and the rule score becomes positive.
It is also interesting to compare the expansions
performed by DIRT-LE-Lin-2 to the above. For
instance in this case L2mother = {father, sarah},
which does not identify people as a shared argu-
ment for the rule.
6 Conclusions
We propose to improve the learning of infer-
ence rules between infrequent predicate templates
with sparse argument vectors by utilizing a novel
scheme that lexically expands argument vectors
with semantically similar words. Similarities be-
tween argument words are discovered using a dual
distributional representation, in which templates
are the features.
We tested the performance of our expansion
approach on rule application datasets that were
biased towards rare templates. Our evaluation
showed that rule learning with expanded vectors
outperformed the baseline learning with original
vectors. It also outperformed an LDA-based simi-
larity model that overcomes sparseness via dimen-
sionality reduction.
In future work we plan to investigate how our
scheme performs when integrated with manually
constructed resources for lexical expansion, such
as WordNet (Fellbaum, 1998).
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
287
References
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modeling,
1(1).
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In Proceedings
of COLING: Posters.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Oren Kurland and Lillian Lee. 2009. Clusters, lan-
guage models, and ad hoc information retrieval.
ACM Transactions on Information Systems (TOIS),
27(3):13.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
KDD.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING,
Mumbai, India.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of NAACL.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Ellen M Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
288
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97?102,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PLIS: a Probabilistic Lexical Inference System
Eyal Shnarch1, Erel Segal-haLevi1, Jacob Goldberger2, Ido Dagan1
1Computer Science Department, Bar-Ilan University, Israel
2Faculty of Engineering, Bar-Ilan University, Israel
{shey,erelsgl,dagan}@cs.biu.ac.il
goldbej@eng.biu.ac.il
Abstract
This paper presents PLIS, an open source
Probabilistic Lexical Inference System
which combines two functionalities: (i)
a tool for integrating lexical inference
knowledge from diverse resources, and (ii)
a framework for scoring textual inferences
based on the integrated knowledge. We
provide PLIS with two probabilistic im-
plementation of this framework. PLIS is
available for download and developers of
text processing applications can use it as
an off-the-shelf component for injecting
lexical knowledge into their applications.
PLIS is easily configurable, components
can be extended or replaced with user gen-
erated ones to enable system customiza-
tion and further research. PLIS includes
an online interactive viewer, which is a
powerful tool for investigating lexical in-
ference processes.
1 Introduction and background
Semantic Inference is the process by which ma-
chines perform reasoning over natural language
texts. A semantic inference system is expected to
be able to infer the meaning of one text from the
meaning of another, identify parts of texts which
convey a target meaning, and manipulate text units
in order to deduce new meanings.
Semantic inference is needed for many Natural
Language Processing (NLP) applications. For in-
stance, a Question Answering (QA) system may
encounter the following question and candidate
answer (Example 1):
Q: which explorer discovered the New World?
A: Christopher Columbus revealed America.
As there are no overlapping words between the
two sentences, to identify that A holds an answer
for Q, background world knowledge is needed
to link Christopher Columbus with explorer and
America with New World. Linguistic knowledge
is also needed to identify that reveal and discover
refer to the same concept.
Knowledge is needed in order to bridge the gap
between text fragments, which may be dissimilar
on their surface form but share a common mean-
ing. For the purpose of semantic inference, such
knowledge can be derived from various resources
(e.g. WordNet (Fellbaum, 1998) and others, de-
tailed in Section 2.1) in a form which we denote as
inference links (often called inference/entailment
rules), each is an ordered pair of elements in which
the first implies the meaning of the second. For in-
stance, the link ship?vessel can be derived from
the hypernym relation of WordNet.
Other applications can benefit from utilizing in-
ference links to identify similarity between lan-
guage expressions. In Information Retrieval, the
user?s information need may be expressed in rele-
vant documents differently than it is expressed in
the query. Summarization systems should identify
text snippets which convey the same meaning.
Our work addresses a generic, application in-
dependent, setting of lexical inference. We there-
fore adopt the terminology of Textual Entailment
(Dagan et al, 2006), a generic paradigm for ap-
plied semantic inference which captures inference
needs of many NLP applications in a common un-
derlying task: given two textual fragments, termed
hypothesis (H) and text (T ), the task is to recog-
nize whether T implies the meaning of H , denoted
T?H. For instance, in a QA application, H rep-
resents the question, and T a candidate answer. In
this setting, T is likely to hold an answer for the
question if it entails the question.
It is challenging to properly extract the needed
inference knowledge from available resources,
and to effectively utilize it within the inference
process. The integration of resources, each has its
own format, is technically complex and the quality
97
? 
Lexical Inference 
Lexical Integrator 
? ? ? ?  
WordNet 
Wikipedia 
VerbOcean 
Text 
Hypothesis ?1 ?2 ?3 ?4 
?1 ?3 
?(? ? ?3) 
?2 
?? 
Lexical 
Resources 
?(?3 ? ?2) 
Figure 1: PLIS schema - a text-hypothesis pair is processed
by the Lexical Integrator which uses a set of lexical resources
to extract inference chains which connect the two. The Lexi-
cal Inference component provides probability estimations for
the validity of each level of the process.
of the resulting inference links is often unknown in
advance and varies considerably. For coping with
this challenge we developed PLIS, a Probabilis-
tic Lexical Inference System1. PLIS, illustrated in
Fig 1, has two main modules: the Lexical Integra-
tor (Section 2) accepts a set of lexical resources
and a text-hypothesis pair, and finds all the lex-
ical inference relations between any pair of text
term ti and hypothesis term hj , based on the avail-
able lexical relations found in the resources (and
their combination). The Lexical Inference module
(Section 3) provides validity scores for these rela-
tions. These term-level scores are used to estimate
the sentence-level likelihood that the meaning of
the hypothesis can be inferred from the text, thus
making PLIS a complete lexical inference system.
Lexical inference systems do not look into the
structure of texts but rather consider them as bag
of terms (words or multi-word expressions). These
systems are easy to implement, fast to run, practi-
cal across different genres and languages, while
maintaining a competitive level of performance.
PLIS can be used as a stand-alone efficient in-
ference system or as the lexical component of any
NLP application. PLIS is a flexible system, al-
lowing users to choose the set of knowledge re-
sources as well as the model by which inference
1The complete software package is available at http://
www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online in-
teractive viewer is available for examination at http://irsrv2.
cs.biu.ac.il/nlp-net/PLIS.html.
is done. PLIS can be easily extended with new
knowledge resources and new inference models. It
comes with a set of ready-to-use plug-ins for many
common lexical resources (Section 2.1) as well
as two implementation of the scoring framework.
These implementations, described in (Shnarch et
al., 2011; Shnarch et al, 2012), provide probabil-
ity estimations for inference. PLIS has an inter-
active online viewer (Section 4) which provides a
visualization of the entire inference process, and is
very helpful for analysing lexical inference mod-
els and lexical resources usability.
2 Lexical integrator
The input for the lexical integrator is a set of lex-
ical resources and a pair of text T and hypothe-
sis H . The lexical integrator extracts lexical in-
ference links from the various lexical resources to
connect each text term ti?T with each hypothesis
term hj ?H2. A lexical inference link indicates a
semantic relation between two terms. It could be
a directional relation (Columbus?navigator) or a
bidirectional one (car?? automobile).
Since knowledge resources vary in their rep-
resentation methods, the lexical integrator wraps
each lexical resource in a common plug-in inter-
face which encapsulates resource?s inner repre-
sentation method and exposes its knowledge as a
list of inference links. The implemented plug-ins
that come with PLIS are described in Section 2.1.
Adding a new lexical resource and integrating it
with the others only demands the implementation
of the plug-in interface.
As the knowledge needed to connect a pair of
terms, ti and hj , may be scattered across few re-
sources, the lexical integrator combines inference
links into lexical inference chains to deduce new
pieces of knowledge, such as Columbus resource1???????
navigator resource2??????? explorer. Therefore, the only
assumption the lexical integrator makes, regarding
its input lexical resources, is that the inferential
lexical relations they provide are transitive.
The lexical integrator generates lexical infer-
ence chains by expanding the text and hypothesis
terms with inference links. These links lead to new
terms (e.g. navigator in the above chain example
and t? in Fig 1) which can be further expanded,
as all inference links are transitive. A transitivity
2Where i and j run from 1 to the length of the text and
hypothesis respectively.
98
limit is set by the user to determine the maximal
length for inference chains.
The lexical integrator uses a graph-based rep-
resentation for the inference chains, as illustrates
in Fig 1. A node holds the lemma, part-of-speech
and sense of a single term. The sense is the ordi-
nal number of WordNet sense. Whenever we do
not know the sense of a term we implement the
most frequent sense heuristic.3 An edge represents
an inference link and is labeled with the semantic
relation of this link (e.g. cytokine?protein is la-
beled with the WordNet relation hypernym).
2.1 Available plug-ins for lexical resources
We have implemented plug-ins for the follow-
ing resources: the English lexicon WordNet
(Fellbaum, 1998)(based on either JWI, JWNL
or extJWNL java APIs4), CatVar (Habash and
Dorr, 2003), a categorial variations database,
Wikipedia-based resource (Shnarch et al, 2009),
which applies several extraction methods to de-
rive inference links from the text and structure
of Wikipedia, VerbOcean (Chklovski and Pantel,
2004), a knowledge base of fine-grained semantic
relations between verbs, Lin?s distributional simi-
larity thesaurus (Lin, 1998), and DIRECT (Kotler-
man et al, 2010), a directional distributional simi-
larity thesaurus geared for lexical inference.
To summarize, the lexical integrator finds all
possible inference chains (of a predefined length),
resulting from any combination of inference links
extracted from lexical resources, which link any
t, h pair of a given text-hypothesis. Developers
can use this tool to save the hassle of interfac-
ing with the different lexical knowledge resources,
and spare the labor of combining their knowledge
via inference chains.
The lexical inference model, described next,
provides a mean to decide whether a given hypoth-
esis is inferred from a given text, based on weigh-
ing the lexical inference chains extracted by the
lexical integrator.
3 Lexical inference
There are many ways to implement an infer-
ence model which identifies inference relations
between texts. A simple model may consider the
3This disambiguation policy was better than considering
all senses of an ambiguous term in preliminary experiments.
However, it is a matter of changing a variable in the configu-
ration of PLIS to switch between these two policies.
4http://wordnet.princeton.edu/wordnet/related-projects/
number of hypothesis terms for which inference
chains, originated from text terms, were found. In
PLIS, the inference model is a plug-in, similar to
the lexical knowledge resources, and can be easily
replaced to change the inference logic.
We provide PLIS with two implemented base-
line lexical inference models which are mathemat-
ically based. These are two Probabilistic Lexical
Models (PLMs), HN-PLM and M-PLM which are
described in (Shnarch et al, 2011; Shnarch et al,
2012) respectively.
A PLM provides probability estimations for the
three parts of the inference process (as shown in
Fig 1): the validity probability of each inference
chain (i.e. the probability for a valid inference re-
lation between its endpoint terms) P (ti ? hj), the
probability of each hypothesis term to be inferred
by the entire text P (T ? hj) (term-level proba-
bility), and the probability of the entire hypothesis
to be inferred by the text P (T ? H) (sentence-
level probability).
HN-PLM describes a generative process by
which the hypothesis is generated from the text.
Its parameters are the reliability level of each of
the resources it utilizes (that is, the prior proba-
bility that applying an arbitrary inference link de-
rived from each resource corresponds to a valid in-
ference). For learning these parameters HN-PLM
applies a schema of the EM algorithm (Demp-
ster et al, 1977). Its performance on the recog-
nizing textual entailment task, RTE (Bentivogli et
al., 2009; Bentivogli et al, 2010), are in line with
the state of the art inference systems, including
complex systems which perform syntactic analy-
sis. This model is improved by M-PLM, which de-
duces sentence-level probability from term-level
probabilities by a Markovian process. PLIS with
this model was used for a passage retrieval for a
question answering task (Wang et al, 2007), and
outperformed state of the art inference systems.
Both PLMs model the following prominent as-
pects of the lexical inference phenomenon: (i)
considering the different reliability levels of the
input knowledge resources, (ii) reducing inference
chain probability as its length increases, and (iii)
increasing term-level probability as we have more
inference chains which suggest that the hypothesis
term is inferred by the text. Both PLMs only need
sentence-level annotations from which they derive
term-level inference probabilities.
To summarize, the lexical inference module
99
?(? ? ?) 
?(??? ??) 
?(? ? ??) 
configuration 
1 
2 
3 
4 
Figure 2: PLIS interactive viewer with Example 1 demonstrates knowledge integration of multiple inference chains and
resource combination (additional explanations, which are not part of the demo, are provided in orange).
provides the setting for interfacing with the lexi-
cal integrator. Additionally, the module provides
the framework for probabilistic inference models
which estimate term-level probabilities and inte-
grate them into a sentence-level inference deci-
sion, while implementing prominent aspects of
lexical inference. The user can choose to apply
another inference logic, not necessarily probabilis-
tic, by plugging a different lexical inference model
into the provided inference infrastructure.
4 The PLIS interactive system
PLIS comes with an online interactive viewer5 in
which the user sets the parameters of PLIS, inserts
a text-hypothesis pair and gets a visualization of
the entire inference process. This is a powerful
tool for investigating knowledge integration and
lexical inference models.
Fig 2 presents a screenshot of the processing of
Example 1. On the right side, the user configures
the system by selecting knowledge resources, ad-
justing their configuration, setting the transitivity
limit, and choosing the lexical inference model to
be applied by PLIS.
After inserting a text and a hypothesis to the
appropriate text boxes, the user clicks on the in-
fer button and PLIS generates all lexical inference
chains, of length up to the transitivity limit, that
connect text terms with hypothesis terms, as avail-
able from the combination of the selected input re-
5http://irsrv2.cs.biu.ac.il/nlp-net/PLIS.html
sources. Each inference chain is presented in a line
between the text and hypothesis.
PLIS also displays the probability estimations
for all inference levels; the probability of each
chain is presented at the end of its line. For each
hypothesis term, term-level probability, which
weighs all inference chains found for it, is given
below the dashed line. The overall sentence-level
probability integrates the probabilities of all hy-
pothesis terms and is displayed in the box at the
bottom right corner.
Next, we detail the inference process of Exam-
ple 1, as presented in Fig 2. In this QA example,
the probability of the candidate answer (set as the
text) to be relevant for the given question (the hy-
pothesis) is estimated. When utilizing only two
knowledge resources (WordNet and Wikipedia),
PLIS is able to recognize that explorer is inferred
by Christopher Columbus and that New World is
inferred by America. Each one of these pairs has
two independent inference chains, numbered 1?4,
as evidence for its inference relation.
Both inference chains 1 and 3 include a single
inference link, each derived from a different rela-
tion of the Wikipedia-based resource. The infer-
ence model assigns a higher probability for chain
1 since the BeComp relation is much more reliable
than the Link relation. This comparison illustrates
the ability of the inference model to learn how to
differ knowledge resources by their reliability.
Comparing the probability assigned by the in-
100
ference model for inference chain 2 with the prob-
abilities assigned for chains 1 and 3, reveals the
sophisticated way by which the inference model
integrates lexical knowledge. Inference chain 2
is longer than chain 1, therefore its probability is
lower. However, the inference model assigns chain
2 a higher probability than chain 3, even though
the latter is shorter, since the model is sensitive
enough to consider the difference in reliability lev-
els between the two highly reliable hypernym re-
lations (from WordNet) of chain 2 and the less re-
liable Link relation (from Wikipedia) of chain 3.
Another aspect of knowledge integration is ex-
emplified in Fig 2 by the three circled probabili-
ties. The inference model takes into consideration
the multiple pieces of evidence for the inference
of New World (inference chains 3 and 4, whose
probabilities are circled). This results in a term-
level probability estimation for New World (the
third circled probability) which is higher than the
probabilities of each chain separately.
The third term of the hypothesis, discover, re-
mains uncovered by the text as no inference chain
was found for it. Therefore, the sentence-level
inference probability is very low, 37%. In order
to identify that the hypothesis is indeed inferred
from the text, the inference model should be pro-
vided with indications for the inference of dis-
cover. To that end, the user may increase the tran-
sitivity limit in hope that longer inference chains
provide the needed information. In addition, the
user can examine other knowledge resources in
search for the missing inference link. In this ex-
ample, it is enough to add VerbOcean to the in-
put of PLIS to expose two inference chains which
connect reveal with discover by combining an in-
ference link from WordNet and another one from
VerbOcean. With this additional information, the
sentence-level probability increases to 76%. This
is a typical scenario of utilizing PLIS, either via
the interactive system or via the software, for ana-
lyzing the usability of the different knowledge re-
sources and their combination.
A feature of the interactive system, which is
useful for lexical resources analysis, is that each
term in a chain is clickable and links to another
screen which presents all the terms that are in-
ferred from it and those from which it is inferred.
Additionally, the interactive system communi-
cates with a server which runs PLIS, in a full-
duplex WebSocket connection6. This mode of op-
eration is publicly available and provides a method
for utilizing PLIS, without having to install it or
the lexical resources it uses.
Finally, since PLIS is a lexical system it can
easily be adjusted to other languages. One only
needs to replace the basic lexical text processing
tools and plug in knowledge resources in the tar-
get language. If PLIS is provided with bilingual
resources,7 it can operate also as a cross-lingual
inference system (Negri et al, 2012). For instance,
the text in Fig 3 is given in English, while the hy-
pothesis is written in Spanish (given as a list of
lemma:part-of-speech). The left side of the figure
depicts a cross-lingual inference process in which
the only lexical knowledge resource used is a man-
ually built English-Spanish dictionary. As can be
seen, two Spanish terms, jugador and casa remain
uncovered since the dictionary alone cannot con-
nect them to any of the English terms in the text.
As illustrated in the right side of Fig 3,
PLIS enables the combination of the bilingual
dictionary with monolingual resources to pro-
duce cross-lingual inference chains, such as foot-
baller hypernym???????player manual??????jugador. Such in-
ference chains have the capability to overcome
monolingual language variability (the first link
in this chain) as well as to provide cross-lingual
translation (the second link).
5 Conclusions
To utilize PLIS one should gather lexical re-
sources, obtain sentence-level annotations and
train the inference model. Annotations are avail-
able in common data sets for task such as QA,
Information Retrieval (queries are hypotheses and
snippets are texts) and Student Response Analysis
(reference answers are the hypotheses that should
be inferred by the student answers).
For developers of NLP applications, PLIS of-
fers a ready-to-use lexical knowledge integrator
which can interface with many common lexical
knowledge resources and constructs lexical in-
ference chains which combine the knowledge in
them. A developer who wants to overcome lex-
ical language variability, or to incorporate back-
ground knowledge, can utilize PLIS to inject lex-
6We used the socket.io implementation.
7A bilingual resource holds inference links which connect
terms in different languages (e.g. an English-Spanish dictio-
nary can provide the inference link explorer?explorador).
101
Figure 3: PLIS as a cross-lingual inference system. Left: the process with a single manual bilingual resource. Right: PLIS
composes cross-lingual inference chains to increase hypothesis coverage and increase sentence-level inference probability.
ical knowledge into any text understanding appli-
cation. PLIS can be used as a lightweight infer-
ence system or as the lexical component of larger,
more complex inference systems.
Additionally, PLIS provides scores for infer-
ence chains and determines the way to combine
them in order to recognize sentence-level infer-
ence. PLIS comes with two probabilistic lexical
inference models which achieved competitive per-
formance levels in the tasks of recognizing textual
entailment and passage retrieval for QA.
All aspects of PLIS are configurable. The user
can easily switch between the built-in lexical re-
sources, inference models and even languages, or
extend the system with additional lexical resources
and new inference models.
Acknowledgments
The authors thank Eden Erez for his help with
the interactive viewer and Miquel Espla` Gomis
for the bilingual dictionaries. This work was par-
tially supported by the European Community?s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 287923 (EXCITEMENT)
and the Israel Science Foundation grant 880/12.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth PASCAL recognizing textual entailment chal-
lenge. In Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge.
In Proc. of TAC.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the royal statistical soci-
ety, series [B], 39(1):1?38.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proc. of NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLOING-ACL.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entail-
ment for content synchronization. In Proc. of Se-
mEval.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proc. of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
Towards a probabilistic model for lexical entailment.
In Proc. of the TextInfer Workshop.
Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012.
A probabilistic lexical model for ranking textual in-
ferences. In Proc. of *SEM.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In Proc. of EMNLP.
102
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237?245,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
A Probabilistic Lexical Model for Ranking Textual Inferences
Eyal Shnarch and Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan 52900, Israel
{shey,dagan}@cs.biu.ac.il
Jacob Goldberger
Faculty of Engineering
Bar-Ilan University
Ramat-Gan 52900, Israel
goldbej@eng.biu.ac.il
Abstract
Identifying textual inferences, where the
meaning of one text follows from another, is
a general underlying task within many natu-
ral language applications. Commonly, it is ap-
proached either by generative syntactic-based
methods or by ?lightweight? heuristic lexical
models. We suggest a model which is confined
to simple lexical information, but is formu-
lated as a principled generative probabilistic
model. We focus our attention on the task of
ranking textual inferences and show substan-
tially improved results on a recently investi-
gated question answering data set.
1 Introduction
The task of identifying texts which share semantic
content arises as a general need in many natural lan-
guage processing applications. For instance, a para-
phrasing application has to recognize texts which
convey roughly the same content, and a summariza-
tion application needs to single out texts which con-
tain the content stated by other texts. We refer to this
general task as textual inference similar to prior use
of this term (Raina et al, 2005; Schoenmackers et
al., 2008; Haghighi et al, 2005).
In many textual inference scenarios the setting re-
quires a classification decision of whether the infer-
ence relation holds or not. But in other scenarios
ranking according to inference likelihood would be
the natural task. In this work we focus on ranking
textual inferences; given a sentence and a corpus,
the task is to rank the corpus passages by their plau-
sibility to imply as much of the sentence meaning as
possible. Most naturally, this is the case in question
answering (QA), where systems search for passages
that cover the semantic components of the question.
A recent line of research was dedicated to this task
(Wang et al, 2007; Heilman and Smith, 2010; Wang
and Manning, 2010).
A related scenario is the task of Recognizing Tex-
tual Entailment (RTE) within a corpus (Bentivogli
et al, 2010)1. In this task, inference systems should
identify, for a given hypothesis, the sentences which
entail it in a given corpus. Even though RTE was
presented as a classification task, it has an appeal-
ing potential as a ranking task as well. For instance,
one may want to find texts that validate a claim such
as cellular radiation is dangerous for children, or to
learn more about it from a newswire corpus. To that
end, one should look for additional mentions of this
claim such as extensive usage of cell phones may be
harmful for youngsters. This can be done by rank-
ing the corpus passages by their likelihood to entail
the claim, where the top ranked passages are likely
to contain additional relevant information.
Two main approaches have been used to address
textual inference (for either ranking or classifica-
tion). One is based on transformations over syntac-
tic parse trees (Echihabi and Marcu, 2003; Heilman
and Smith, 2010). Some works in this line describe
a probabilistic generative process in which the parse
tree of the question is generated from the passage
(Wang et al, 2007; Wang and Manning, 2010).
In the second approach, lexical models have been
employed for textual inference (MacKinlay and
Baldwin, 2009; Clark and Harrison, 2010). Typi-
1http://www.nist.gov/tac/2010/RTE/index.html
237
cally, lexical models consider a text fragment as a
bag of terms and split the inference decision into
two steps. The first is a term-level estimation of the
inference likelihood for each term independently,
based on direct lexical match and on lexical knowl-
edge resources. Some commonly used resources are
WordNet (Fellbaum, 1998), distributional-similarity
thesauri (Lin, 1998), and web knowledge resources
such as (Suchanek et al, 2007). The second step
is making a final sentence-level decision based on
these estimations for the component terms. Lex-
ical models have the advantage of being fast and
easy to utilize (e.g. no dependency on parsing tools)
while being highly competitive with top performing
systems, e.g. the system of Majumdar and Bhat-
tacharyya (2010).
In this work, we investigate how well such lexi-
cal models can perform in textual inference ranking
scenarios. However, while lexical models usually
apply heuristic methods, we would like to pursue a
principled learning-based generative framework, in
analogy to the approaches for syntactic-based infer-
ence. An attractive work in this spirit is presented in
(Shnarch et al, 2011a), that propose a model which
is both lexical and probabilistic. Later, Shnarch et
al. (2011b) improved this model and reported re-
sults that outperformed previous lexical models and
were on par with state-of-the-art RTE models.
Whereas their term-level model provides means
to integrate lexical knowledge in a probabilistic
manner, their sentence-level model depends to a
great extent on heuristic normalizations which were
introduced to incorporate prominent aspects of the
sentence-level decision. This deviates their model
from a pure probabilistic methodology.
Our work aims at amending this deficiency and
proposes a new probabilistic sentence-level model
based on a Markovian process. In that model, all
parameters are estimated by an EM algorithm. We
evaluate this model on the tasks of ranking passages
for QA and ranking textual entailments within a cor-
pus, and show that eliminating the need for heuris-
tic normalizations greatly improves state-of-the-art
performance. The full implementation of our model
is available for download2 and can be used as an
easy-to-install and highly competitive inference en-
2http://www.cs.biu.ac.il/?nlp/downloads/probLexModel.html
gine that operates only on lexical knowledge, or as a
lexical component integrated within a more complex
inference system.
2 Background
Wang et al (2007) provided an annotated data set,
based on the Text REtrieval Conference (TREC) QA
tracks3, specifically for the task of ranking candidate
answer passages. We adopt their experimental setup
and next review the line of syntactic-based works
which reported results on this data set.
2.1 Syntactic generative models
Wang et al (2007) propose a quasi-synchronous
grammar formulation which specifies the generation
of the question parse tree, loosely conditioned on the
parse tree of the candidate answer passage. Their
model showed improvement over previous syntac-
tic models for QA: Punyakanok et al (2004), who
computed similarity between question-answer pairs
with a generalized tree-edit distance, and Cui et al
(2005), who developed an information measure for
sentence similarity based on dependency paths of
aligned words. Wang et al (2007) reproduced these
methods and extended them to utilize WordNet.
More recently, Heilman and Smith (2010) im-
proved Wang et al (2007) results with a classifica-
tion based approach. Feature for the classifier were
extracted from a greedy algorithm which searches
for tree-edit sequences which transform the parse
tree of the candidate answer into the one of the ques-
tion. Unlike other works reviewed here, this one
does not utilize lexical knowledge resources.
Similarly, Wang and Manning (2010) present an
extended tree-edit operations set and search for edit
sequences to generate the question from the answer
candidate. Their CRF-based classifier models these
sequences as latent variables.
An important merit of these methods is that they
offer principled, often probabilistic, generative mod-
els for the task of ranking candidate answers. Their
drawback is the need for syntactic analysis which
makes them slower to run, dependent on parsing per-
formance, which is often mediocre in many text gen-
res, and inadequate for languages which lack proper
parsing tools.
3http://trec.nist.gov/data/qamain.html
238
2.2 Lexical models
Lexical models, on the other hand, are faster, eas-
ier to implement and are more practical for vari-
ous genres and languages. Such models derive from
knowledge resources lexical inference rules which
indicate that the meaning of a lexical term can be
inferred from the meaning of another term (e.g.
youngsters? children and harmful? dangerous).
They are common in the Recognizing Textual En-
tailment (RTE) systems and we present some rep-
resentative methods for that task. We adopt textual
entailment terminology and henceforth use Hypoth-
esis (denoted H) for the inferred text fragment and
Text (denoted T ) for the text from which it is being
inferred4.
Majumdar and Bhattacharyya (2010) utilized a
simple union of lexical rules derived from vari-
ous lexical resources for the term-level step. They
derived their sentence-level decision based on the
number of matched hypothesis terms. The results
of this simple model were only slightly worse than
the best results of the RTE-6 challenge which were
achieved by a syntactic-based system (Jia et al,
2010). Clark and Harrison (2010), on the other hand,
considered the number of mismatched terms in es-
tablishing their sentence-level decision. MacKinlay
and Baldwin (2009) represented text and hypothe-
sis as word vectors augmented with lexical knowl-
edge. For sentence-level similarity they used a vari-
ant of the cosine similarity score. Common to most
of these lexical models is the application of heuris-
tic methods in both the term and the sentence level
steps.
Targeted to replace heuristic methods with princi-
pled ones, Shnarch et al (2011a) present a model
which aims at combining the advantages of a proba-
bilistic generative model with the simplicity of lex-
ical methods. In some analogy to generative parse-
tree based models, they propose a generative process
for the creation of the hypothesis from the text.
At the term-level, their model combines knowl-
edge from various input resources and has the ad-
vantages of considering the effect of transitive rule
application (e.g. mobile phone? cell phone? cel-
lular) as well as the integration of multiple pieces
4In the task of passage ranking for QA, the hypothesis is the
question and the text is the candidate passage.
of evidence for the inference of a term (e.g. both
the appearance of harmful and risky in T provide
evidence for the inference of dangerous in H). We
denote this term-level Probabilistic Lexical Model
as PLMTL, and have reproduced it in our work as
presented in Section 4.1. For the sentence-level de-
cision they describe an AND gate mechanism, i.e.
deducing a positive inference decision for H as a
whole only if all its terms were inferred from T .
In an extension to that work, Shnarch et al
(2011b) modified PLMTL to improve the sentence-
level step. They pointed out some prominent aspects
for the sentence-level decision. First, they suggest
that a hypothesis as a whole can be inferred from
the text even if some of its terms are not inferred.
To model this, they introduced a noisy-AND mech-
anism (Pearl, 1988). Additionally, they emphasized
the effect of hypothesis length and the dependency
between terms on the sentence-level decision. How-
ever, they did not fully achieve their target of pre-
senting a fully coherent probabilistic model, as their
model included heuristic normalization formulae.
On the contrary, the model we present is the first
along this line to be fully specified in terms of a
generative setting and formulated in pure probabilis-
tic terms. We introduce a Markovian-style proba-
bilistic model for the sentence-level decision. This
model receives as input term-level probabilistic es-
timates, which may be provided by any term-level
model. In our implementation we embed PLMTL as
the term-level model and present a complete coher-
ent Markovian-based Probabilistic Lexical Model,
which we term M-PLM.
3 Markovian sentence-level model
The goal of a sentence-level model is to integrate
term-level inputs into an inference decision for the
hypothesis as a whole. For a hypothesis H =
h1, . . . , hn and a text T , term-level models first esti-
mate independently for each term ht its probability
to be inferred from T . Let xt be a binary random
variable representing the event that ht is indeed in-
ferred from T (i.e., xt = 1 if ht is inferred and 0
otherwise).
Given these term-level probabilities, a sentence-
level model is employed to estimate the probability
that H as a whole is inferred from T . This step is
239
term
-leve
l
sente
nce-l
evel
Text: Hypo
:
t 1
t m
?
h 1
h 2
h n
?
x 1
x 2
x n
?
y 1
y 2
y n
?
Figure 1: A probabilistic lexical model: the upper part is the
term-level input to the sentence-level Markovian process, de-
picted in the lower part. xi is a binary variable representing the
inference of hi and yj is a variable for the accumulative infer-
ence decision for the first j terms of Hypo. The final sentence-
level decision is given by yn.
the focus of our work. We assume that the term-
level probabilities are given as input. Section 4.1
describes PLMTL, as a concrete method for deriving
these probabilities.
Our sentence-level model is based on a Marko-
vian process and is described in Section 3.1. In par-
ticular, it takes into account, in probabilistic terms,
the prominent factors in lexical entailment, men-
tioned in Section 2. An efficient inference algorithm
for our model is given in Section 3.2 and EM-based
learning is specified in Section 3.3.
3.1 Markovian sentence-level decision
The motivation for proposing a Markovian process
for the sentence-level is to establish an intermedi-
ate model, lying between two extremes: assuming
full independence between hypothesis terms versus
assuming that every term is dependent on all other
terms. The former alternative is too weak, while
the latter alternative is computationally hard and
not very informative, and thus hard to capture in
a model. Our model specifies a Markovian depen-
dence structure, which limits the dependence scope
to adjacent terms, as follows.
We define a binary variable yt to be the accumu-
lated sentence-level inference decision up to ht. In
other words, yt=1 if the subset {h1, . . . , ht} of H?s
terms is inferred as a whole from T .
Note that this means that yt can be 1 even if some
terms amongst h1, . . . , ht are not inferred. As yn is
the decision for the complete hypothesis, our model
addresses this way the prominent aspect that the hy-
pothesis as a whole may be inferred even if some of
its terms are not inferred. The reason for allowing
this is that such un-inferred terms may be inferred
from the global context of T , or alternatively, are ac-
tually inferred from T but the knowledge resources
in use do not contain the proper lexical rule to make
such inference.
Figure 1 describes both steps of a full lexical in-
ference model. Its lower part depicts our Markovian
process. In the proposed model the inference deci-
sion at each position t is a combination of xt, the
variable for the event of ht being inferred, and yt?1,
the accumulated decision at the previous position.
Therefore, the transition parameters of M-PLM can
be modeled as:
qij(k)=P (yt=k|yt?1 = i, xt=j) ?k, i, j?{0, 1}
where y1=x1. For instance, q01(1) is the probability
that yt=1, given that yt?1 =0 and xt=1.
Applying the Markovian process on the entire
hypothesis we get yn, which represents the final
sentence-level decision, where a soft decision is ob-
tained by computing the probability of yn=1:
P (yn=1) =
?
x1, ..., xn
y2, ..., yn?1, yn=1
P (x1)
n?
t=2
P (xt)P (yt|yt?1, xt)
The summation is done over all possible binary
values of the term-level variables x1, ..., xn and the
accumulated sentence-level variables y2, ..., yn?1
where yn=1. Note that for clarity, in this formula xt
and yt denote the binary values at the corresponding
variable positions. A tractable form for computing
P (yn=1) is presented in Section 3.2.
Overall, the prominent factors in lexical entail-
ment, raised by prior works, are incorporated within
the core structure of this probabilistic model, with-
out the need to resort to heuristic normalizations.
Reducing the negative affect of hypothesis length on
the entailment probability is achieved by having yt,
at each position, being directly dependent only on xt
and yt?1 as opposed to being affected by all hypoth-
esis terms. The second factor, modeling the depen-
dency between hypothesis terms, is addressed by the
240
indirect dependency of yn on all preceding hypothe-
sis terms. This dependency arises from the recursive
nature of the Markovian model, as can be seen in the
next section.
Our proposed Markovian process presents a linear
dependency between terms which, to some extent,
poses an anomaly with respect to the structure of the
entailment phenomenon. Yet, as we do want to limit
the dependence structure, following the natural or-
der of the sentence words seems the most reasonable
choice, as common in many other types of sequential
models. We also tried randomizing the word order
which, on average, did not improve performance.
3.2 Inference
The accumulated sentence-level inference can be
efficiently computed using a typical forward algo-
rithm. We denote the probability of xt=j, j?{0, 1}
by ht(j) = P (xt = j). The forward step is given in
Eq. (1) and its initialization is defined in Eq. (2).
?t(k) = P (yt=k)=
?
i,j?{0,1}
?t?1(i)ht(j)qij(k) (1)
?1(k) = P (x1 =k) (2)
where k?{0, 1} and t = 2, ..., n.
?t(k) is the probability that the accumulated de-
cision at position t is k. It is calculated by sum-
ming over the probabilities of all four combinations
of ?t?1(i) and ht(j), multiplied by the correspond-
ing transition probability, qij(k).
The soft sentence-level decision can be efficiently
calculated by:
P (yn=a) = ?n(a) a?{0, 1} (3)
3.3 Learning
Typically, natural language applications work at the
sentence-level. The training data for such applica-
tions is, therefore, available as annotations at the
sentence-level. Term-level alignments between pas-
sage terms and question terms are rarely available.
Hence, we learn our term-level parameters from
available sentence-level annotations, using the gen-
erative process described above to bridge the gap be-
tween these two levels.
For learning we use the typical backwards algo-
rithm which is described by Eq. (4) and Eq. (5),
where ?t(a|i) is the probability that the full hypoth-
esis inference value is a given that yt= i.
?n(a|i) = P (yn=a|yn= i) = 1{a=i} (4)
?t(a|i) = P (yn=a|yt= i) =
=
?
j,k?{0,1}
ht+1(j)qij(k)?t+1(a|k) (5)
where t = n?1, .., 1, a ? {0, 1} and 1{condition} is
the indicator function which returns 1 if condition
holds and 0 otherwise.
To estimate qij(k), the parameters of the Marko-
vian process, we employ the EM algorithm:
E-step: For each (T,H) pair in the training
data set, annotated with a ? {0, 1} as its sentence-
level inference value, we evaluate the expected
probability of every transition given the annotation
value a:
wtijk(T,H) = P (yt?1 = i, xt=j, yt=k|yn=a)
=
?t?1(i)ht(j)qij(k)?t(a|k)
P (yn=a)
(6)
?i, j, k?{0, 1} and t = 2, ..., |H|.
M-step: Given the values of wtijk(T,H) we
can estimate each qij(1), i, j?{0, 1}, by taking the
proportion of transitions in which yt?1 = i, xt = j
and yt = 1, out of the total transitions in which
yt?1 = i and xt=j:
qij(1)?
?
(T,H)
?|H|
t=2wtij1(T,H)
?
(T,H)
?|H|
t=2
?
k?{0,1}wtijk(T,H)
(7)
qij(0) = 1?qij(1)
4 Complete model implementation
We next describe the end-to-end probabilistic lexical
inference model we used in our evaluations. We im-
plemented PLMTL as our term-level model to pro-
vide us with ht(j), the term-level probabilities. We
chose this model since it is fully lexical, has the ad-
vantages of lexical knowledge integration described
in Section 2 and achieved top results on RTE data
sets. Next, we summarize PLMTL, and in Appendix
A we show how to adjust the learning schema to fit
into our sentence-level model.
241
4.1 PLMTL
Shnarch et al (2011a) provide a term-level model
which integrates lexical rules from various knowl-
edge resources. As described below it also consid-
ers transitive chains of rule applications as well as
the impact of parallel chains which provide multiple
evidence that h?H is inferred from T .
Their model assumes a parameter ?R for each
knowledge resource R in use. ?R specifies the re-
source?s reliability, i.e. the prior probability that ap-
plying a rule from R to an arbitrary text-hypothesis
pair would yield a valid inference.
Next, transitive chains may connect a text term to
a hypothesis term via intermediate term(s). For in-
stance, starting from the text term T-Mobile, a chain
that utilizes the lexical rules T-Mobile? telecom
and telecom? cell phone enables the inference of
the term cell phone from T . They compute, for each
step in a chain, the probability that this step is valid
based on the ?R values. Denoting the resource which
provided a rule r by R(r), Eq. (8) specifies that the
validity probability of the inference step correspond-
ing to the application of the rule r within the chain c
pointing at ht (as represented by xtcr) is ?R(r).
Next, for a chain c pointing at ht (represented by
xtc) to be valid, all its rule steps should be valid for
this pair. Eq. (9) estimates this probability by the
joint probability that the applications of all rules r?
c are valid, assuming independence of rules.
Several chains may connect terms in T to ht, thus
providing multiple pieces of evidence that ht is in-
ferred from T . For instance, both youngsters and
kids in T may indicate the inference of children in
H . For a term ht to be inferred from the entire sen-
tence T it is enough that at least one of the chains
from T to ht is valid. This is the complement event
of ht not being inferred from T which happens when
all chains which suggest the inference of ht, denoted
by C(ht), are invalid. Eq. (10) specifies this proba-
bility (again assuming independence of chains).
P (xtcr = 1) = ?R(r) (8)
P (xtc = 1) =
?
r?c
P (xtcr = 1) (9)
ht(1) = P (xt = 1) = 1?P (xt = 0) (10)
= 1?
?
c?C(ht)
P (xtc = 0)
With respect to the contributions of our work, we
note that previous works resorted to applying some
heuristic amendments on these equations to achieve
valuable results. In contrast, our work is the first
to present a purely generative model. This achieve-
ment shows that it is possible to shift from ad-hoc
heuristic methods, which are common practice, to
more solid mathematically-based methods.
Finally, for ranking text passages from a corpus
for a given hypothesis (question in the QA scenario),
our Markovian sentence-level model takes as its in-
put the outcome of Eq. (10) for each ht ? H . For
PLMTL we need to estimate the model parameters,
that is the various ?R values. In our Markovian
model this is done by the scheme detailed in Ap-
pendix A. Given these term-level probabilities, our
model computes for each hypothesis its probabil-
ity to be inferred from each of the corpus passages,
namely P (yn = 1) in Eq (3). Passages are then
ranked according to this probability.
5 Evaluations and Results
To evaluate the performance of M-PLM for ranking
textual inferences we focused on the task of ranking
candidate answer passages for question answering
(QA) as presented in Section 5.1. Additionally, we
demonstrate the added value of our sentence-level
model in another ranking experiment based on RTE
data sets, described in Section 5.2.
5.1 Answer ranking for question answering
Data set We adopted the experimental setup of
Wang et al (2007) who also provided an annotated
data set for answer passage ranking in QA5.
In their data set an instance is a pair of a factoid
question and a candidate answer passage (a single
sentence in this data set). It was constructed from the
data of the QA tracks at TREC 8?13. The question-
candidate pairs were manually judged and a pair was
annotated as positive if the candidate passage indi-
cates the correct answer for the question. The train-
ing and test sets roughly contain 5700 and 1500 pairs
correspondingly.
5The data set was kindly provided to us by
Mengqiu Wang and is available for download at
http://www.cs.stanford.edu/?mengqiu/data/qg-emnlp07-
data.tgz.
242
Method PLMTL utilizes WordNet and the Catvar
(Categorial Variation) derivations database (Habash
and Dorr, 2003) as generic and publicly available
lexical knowledge resources, when question and
answer terms are restricted to the first WordNet
sense. In order to be consistent with (Shnarch et al,
2011b), the best performing model of prior work,
we restricted our model to utilize only these two re-
sources which they used. However, additional lexi-
cal resources can be provided as input to our model
(e.g. a distributional similarity-base thesaurus).
We report Mean Average Precision (MAP) and
Mean Reciprocal Rank (MRR), the standard mea-
sures for ranked lists. In the cases of tie we took
a conservative approach and ranked positive anno-
tated instances below the negative instances scored
with the same probability. Hence, the reported fig-
ures are lower-bounds for any tie-breaking method
that could have been applied.
Results We compared our model to all 5 mod-
els evaluated for this data set, described in Sec-
tion 2, and to our own implementation of (Shnarch
et al, 2011b). We term this model Heuristically-
Normalized Probabilistic Lexical Model, HN-PLM,
since it modifies PLMTL by introducing heuristic
normalization formulae. As explained earlier, both
M-PLM and HN-PLM embed PLMTL in their im-
plementation but they differ in their sentence-level
model. In our implementation of both models,
PLMTL applies chains of transitive rule applications
whose maximal length is 3.
As seen in Table 1, M-PLM outperforms all prior
models by a large margin. A comparison of M-PLM
and HN-PLM reveals the major positive effect of
choosing the Markovian process for the sentence-
level decision. By avoiding heuristically-normalized
formulae and having all our parameters being part of
the Markovian model, we managed to increases both
MAP and MRR by nearly 2.5%6.
Ablation Test As an additional examination of
the impact of the Markovian process components,
we evaluated the contribution of having 4 transition
parameters. The AND-logic applied by (Shnarch et
6The difference is not significant according to the Wilcoxon
test, however we note that given the data set size it is hard to get
a significant difference and that both Heilman and Smith (2010)
and Wang and Manning (2010) improvements over the results
of Wang et al (2007) were not statistically significant.
System MAP MRR
Punyakanok et al 41.89 49.39
Cui et al 43.50 55.69
Wang & Manning 59.51 69.51
Wang et al 60.29 68.52
Heilman & Smith 60.91 69.17
Shnarch et al HN-PLM 61.89 70.24
M-PLM 64.38 72.69
Table 1: Results (in %) for the task of answer ranking for
question answering (sorted by MAP).
al., 2011a) to their sentence-level decision roughly
corresponds to 2 of the Markovian parameters. A
binary AND outputs 1 if both its inputs are 1. This
corresponds to q11(1) which is indeed estimate to be
near 1. In any other case an AND gate outputs 0.
This corresponds to q00(1) which was estimated to
be near zero.
The two parameters q01 and q10 are novel to the
Markovian process and do not have counterparts in
(Shnarch et al, 2011a). These parameters are the
cases in which the sentence-level decision accumu-
lated so far and the term-level decision do not agree.
Introducing these 2 parameters enables our model to
provide a positive decision for the hypothesis as a
whole (or for a part of it) even if some of its terms
were not inferred. We performed an ablation test on
each of these two parameters by forcing the value of
the ablated parameter to be zero. The notable perfor-
mance drop presented in Table 2 indicates the crucial
contribution of these parameters to our model.
Ablated parameter ? MAP ? MRR
q01(1) = 0 -2.61 -4.91
q10(1) = 0 -2.12 -2.86
Table 2: Ablation test for the novel parameters of the Marko-
vian process. Results (in %) indicate performance drop when
forcing a parameter to be zero.
5.2 RTE evaluations
To assess the added value of our model on an addi-
tional ranking evaluation, we utilize the search task
data sets of the recent Recognizing Textual Entail-
ment (RTE) benchmarks (Bentivogli et al, 2009;
Bentivogli et al, 2010), which were originally con-
243
structed for the task of entailment classification. In
that task a hypothesis is given with a corpus and the
goal is to identify which sentences of the corpus en-
tail the hypothesis. This setting naturally lends itself
to a ranking scenario, in which the desired output is
a list of the corpus sentences ranked by their proba-
bility to entail the given hypothesis.
To that end, we employed the same method-
ology as described in the previous section. Ta-
ble 3 presents the improvement of our model over
HN-PLM, whose classification performance was re-
ported to be on par with best-performing systems on
these data sets7. As can be seen, the improvement
is substantial for both measures on both data sets.
These results further assess the contribution of our
Markovian sentence-level model.
RTE-5 RTE-6
MAP MRR MAP MRR
HN-PLM 58.0 82.9 54.0 71.9
M-PLM 61.6 84.8 60.0 79.2
? +3.6 +1.9 +6.0 +7.3
Table 3: Improvements of our sentence-level model over
HN-PLM. Results (in %) are shown for the last RTE and
for the search task in RTE-5.
6 Discussion
This paper investigated probabilistic lexical mod-
els for ranking textual inferences focusing on pas-
sage ranking for QA. We showed that our coher-
ent probabilistic model, whose sentence-level model
is based on a Markovian process, considerably im-
proves five prior syntactic-based models as well as
a heuristically-normalized lexical model. Therefore,
it raises the baseline for future methods.
In future work we would like to further explore
a broader range of related probabilistic models. Es-
pecially, as our Markovian process is dependent on
term order, it would be interesting to investigate
models which are not order dependent.
Initial experiments on the classification task show
that M-PLM performs well above the average sys-
tem but below HN-PLM, since it does not normalize
7RTE data sets were only used for the classification task
so far, therefore there are no state-of-the-art results to compare
with, when utilizing them for the ranking task.
the estimated probability well across hypothesis. We
therefore suggest a future work on better classifica-
tion models.
Finally, we view this work as joining a line of re-
search which develops principled probabilistic mod-
els for the task of textual inference and demonstrates
their superiority over heuristic methods.
A Appendix: Adaptation of PLMTL
learning
M-PLM embeds PLMTL as its term-level model.
PLMTL introduces ?R values as additional parame-
ters for the complete model. We show how we mod-
ify (Shnarch et al, 2011a) E-step formula to fit our
Markovian modeling, described in Section 3.1. The
M-step formula remains exactly the same.
Eq. (11) estimates the a-posteriori validity prob-
ability of a single application of the rule r in the
transitive chain c pointing at ht, given that the an-
notation of the pair is a.
wtcr(T,H) = P (xtcr = 1|yn = a) =
(11)?
i,j,k?{0,1} ?t?1(i)P (xt=j|xtcr =1)?R(r)qij(k)?t(a|k)
P (yn = a)
where t=2 . . . n and P (xt =j|xtcr =1) is the prob-
ability that the inference value of xt is j, given that
the application of r provides a valid inference step.
As appeared in (Shnarch et al, 2011b) this probabil-
ity can be evaluated as follows:
P (xt=1|xtcr =1)=1?
P (xt = 0)
P (xtc = 0)
(
1?
P (xtc = 1)
?R(r)
)
For t = 1 there is no accumulated sentence-level
decision at the previous position (i.e. no ?t?1) there-
fore Eq. (11) becomes:
w1cr(T,H) =
?
j?{0,1}P (x1 =j|x1cr =1)?R(r)?1(a|j)
P (yn = a)
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
244
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of the Text Analysis Conference.
Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tat
seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
SIGIR.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
the Conference of the North American Chapter of the
Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at the Text Analysis Conference 2010 RTE and sum-
marization track. In Proceedings of the Text Analysis
Conference.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of the Text Analysis Conference.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of the Text Analysis
Conference.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the International
Symposium on Artificial Intelligence and Mathemat-
ics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Stefan Schoenmackers, Oren Etzioni, and Daniel Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011a.
A probabilistic modeling framework for lexical entail-
ment. In Proceedings of ACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011b.
Towards a probabilistic model for lexical entailment.
In Proceedings of the TextInfer Workshop on Textual
Entailment.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of WWW.
Mengqiu Wang and Christopher Manning. 2010. Proba-
bilistic tree-edit models with structured latent variables
for textual entailment and question answering. In Pro-
ceedings of Coling.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
245
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10?19,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards a Probabilistic Model for Lexical Entailment
Eyal Shnarch
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
shey@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat-Gan, Israel
goldbej@eng.biu.ac.il
Ido Dagan
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
dagan@cs.biu.ac.il
Abstract
While modeling entailment at the lexical-level
is a prominent task, addressed by most textual
entailment systems, it has been approached
mostly by heuristic methods, neglecting some
of its important aspects. We present a prob-
abilistic approach for this task which cov-
ers aspects such as differentiating various re-
sources by their reliability levels, considering
the length of the entailed sentence, the num-
ber of its covered terms and the existence of
multiple evidence for the entailment of a term.
The impact of our model components is vali-
dated by evaluations, which also show that its
performance is in line with the best published
entailment systems.
1 Introduction
Textual Entailment was proposed as a generic
paradigm for applied semantic inference (Dagan et
al., 2006). Given two textual fragments, termed hy-
pothesis (H) and text (T ), the text is said to textually
entail the hypothesis (T?H) if a person reading the
text can infer the meaning of the hypothesis. Since it
was first introduced, the six rounds of the Recogniz-
ing Textual Entailment (RTE) challenges1 have be-
come a standard benchmark for entailment systems.
Entailment systems apply various techniques to
tackle this task, including logical inference (Tatu
and Moldovan, 2007; MacCartney and Manning,
2007), semantic analysis (Burchardt et al, 2007)
and syntactic parsing (Bar-Haim et al, 2008; Wang
1http://www.nist.gov/tac/
et al, 2009). Inference at these levels usually re-
quires substantial processing and resources, aim-
ing at high performance. Nevertheless, simple lex-
ical level entailment systems pose strong baselines
which most complex entailment systems did not out-
perform (Mirkin et al, 2009a; Majumdar and Bhat-
tacharyya, 2010). Additionally, within a complex
system, lexical entailment modeling is one of the
most effective component. Finally, the simpler lex-
ical approach can be used in cases where complex
systems cannot be used, e.g. when there is no parser
for a targeted language.
For these reasons lexical entailment systems are
widely used. They derive sentence-level entailment
decision base on lexical-level entailment evidence.
Typically, this is done by quantifying the degree of
lexical coverage of the hypothesis terms by the text
terms (where a term may be multi-word). A hy-
pothesis term is covered by a text term if either they
are identical (possibly at the stem or lemma level)
or there is a lexical entailment rule suggesting the
entailment of the former by the latter. Such rules
are derived from lexical semantic resources, such
as WordNet (Fellbaum, 1998), which capture lexi-
cal entailment relations.
Common heuristics for quantifying the degree of
coverage are setting a threshold on the percentage
of coverage of H?s terms (Majumdar and Bhat-
tacharyya, 2010), counting the absolute number of
uncovered terms (Clark and Harrison, 2010), or ap-
plying an Information Retrieval-style vector space
similarity score (MacKinlay and Baldwin, 2009).
Other works (Corley and Mihalcea, 2005; Zanzotto
and Moschitti, 2006) have applied heuristic formu-
10
las to estimate the similarity between text fragments
based on a similarity function between their terms.
The above mentioned methods do not capture sev-
eral important aspects of entailment. Such aspects
include the varying reliability levels of entailment
resources and the impact of rule chaining and multi-
ple evidence on entailment likelihood. An additional
observation from these and other systems is that
their performance improves only moderately when
utilizing lexical-semantic resources2.
We believe that the textual entailment field would
benefit from more principled models for various en-
tailment phenomena. In this work we formulate a
concrete generative probabilistic modeling frame-
work that captures the basic aspects of lexical entail-
ment. A first step in this direction was proposed in
Shnarch et al (2011) (a short paper), where we pre-
sented a base model with a somewhat complicated
and difficult to estimate extension to handle cover-
age. This paper extends that work to a more mature
model with new extensions.
We first consider the ?logical? structure of lexical
entailment reasoning and then interpret it in proba-
bilistic terms. Over this base model we suggest sev-
eral extensions whose significance is then assessed
by our evaluations. Learning the parameters of a
lexical model poses a challenge since there are no
lexical-level entailment annotations. We do, how-
ever, have sentence-level annotations available for
the RTE data sets. To bridge this gap, we formu-
late an instance of the EM algorithm (Dempster et
al., 1977) to estimate hidden lexical-level entailment
parameters from sentence-level annotations.
Overall, we suggest that the main contribution of
this paper is in presenting a probabilistic model for
lexical entailment. Such a model can better integrate
entailment indicators and has the advantage of being
able to utilize well-founded probabilistic methods
such as the EM algorithm. Our model?s performance
is in line with the best entailment systems, while
opening up directions for future improvements.
2 Background
We next review several entailment systems, mostly
those that work at the lexical level and in particular
2See ablation tests reports in http://aclweb.org/aclwiki/ in-
dex.php?title=RTE Knowledge Resources#Ablation Tests
those with which we compare our results on the RTE
data sets.
The 5th Recognizing Textual Entailment chal-
lenge (RTE-5) introduced a new pilot task (Ben-
tivogli et al, 2009) which became the main task in
RTE-6 (Bentivogli et al, 2010). In this task the goal
is to find all sentences that entail each hypothesis in
a given document cluster. This task?s data sets re-
flect a natural distribution of entailments in a corpus
and demonstrate a more realistic scenario than the
earlier RTE challenges.
As reviewed in the following paragraphs there are
several characteristic in common to most entailment
systems: (1) lexical resources have a minimal im-
pact on their performance, (2) they heuristically uti-
lize lexical resources, and (3) there is no principled
method for making the final entailment decision.
The best performing system of RTE-5 was pre-
sented by Mirkin et. al (2009a). It applies super-
vised classifiers over a parse tree representations to
identify entailment. They reported that utilizing lex-
ical resources only slightly improved their perfor-
mance.
MacKinlay and Baldwin (2009) presented the
best lexical-level system at RTE-5. They use a vec-
tor space method to measure the lexical overlap be-
tween the text and the hypothesis. Since usually
texts of RTE are longer than their corresponding hy-
potheses, the standard cosine similarity score came
out lower than expected. To overcome this prob-
lem they suggested a simple ad-hoc variant of the
cosine similarity score which removed from the text
all terms which did not appear in the correspond-
ing hypothesis. While this heuristic improved per-
formance considerably, they reported a decrease in
performance when utilizing synonym and derivation
relations from WordNet.
On the RTE-6 data set, the syntactic-based sys-
tem of Jia et. al (2010) achieved the best results,
only slightly higher than the lexical-level system
of (Majumdar and Bhattacharyya, 2010). The lat-
ter utilized several resources for matching hypoth-
esis terms with text terms: WordNet, VerbOcean
(Chklovski and Pantel, 2004), utilizing two of its
relations, as well as an acronym database, num-
ber matching module, co-reference resolution and
named entity recognition tools. Their final entail-
ment decision was based on a threshold over the
11
number of matched hypothesis terms. They found
out that hypotheses of different length require dif-
ferent thresholds.
While the above systems measure the number of
hypothesis terms matched by the text, Clark and
Harrison (2010) based their entailment decision on
the number of mismatched hypothesis terms. They
utilized both WordNet and the DIRT paraphrase
database (Lin and Pantel, 2001). With WordNet,
they used one set of relations to identify the concept
of a term while another set of relations was used to
identify entailment between concepts. Their results
were inconclusive about the overall effect of DIRT
while WordNet produced a net benefit in most con-
figurations. They have noticed that setting a global
threshold for the entailment decision, decreased per-
formance for some topics of the RTE-6 data set.
Therefore, they tuned a varying threshold for each
topic based on an idiosyncracy of the data, by which
the total number of entailments per topic is approxi-
mately a constant.
Glickman et al (2005) presented a simple model
that recasted the lexical entailment task as a variant
of text classification and estimated entailment prob-
abilities solely from co-occurrence statistics. Their
model did not utilize any lexical resources.
In contrary to these systems, our model shows
improvement when utilizing high quality resources
such as WordNet and the CatVar (Categorial Varia-
tion) database (Habash and Dorr, 2003). As Majum-
dar and Bhattacharyya (2010), our model considers
the impact of hypothesis length, however it does not
require the tuning of a unique threshold for each
length. Finally, most of the above systems do not
differentiate between the various lexical resources
they use, even though it is known that resources re-
liability vary considerably (Mirkin et al, 2009b).
Our probabilistic model, on the other hand, learns
a unique reliability parameter for each resource it
utilizes. As mentioned above, this work extends the
base model in (Shnarch et al, 2011), which is de-
scribed in the next section.
3 A Probabilistic Model
We aim at obtaining a probabilistic score for the like-
lihood that the hypothesis terms are entailed by the
terms of the text. There are several prominent as-
cro
wd 
 
sur
rou
nd  
Jag
uar
Text Hyp
othe
sis
h jh j
h nh n
t 1t 1
t it i
t mt m
t'
Res
ourc
e 1
chain
yy
OR
OR
50 p
eop
le s
urr
ou
nd c
ar
OR
OR
so
cial gro
up
Res
ourc
e 1 O
R
Res
ourc
e 1
Res
ourc
e 3
Res
ourc
e 2 h 1h 1
Res
ourc
e 1
MA
TCH
MA
TCH
Res
ourc
e 3
Res
ourc
e 2
UNC
OVE
RED
Figure 1: Left: the base model of entailing a hypothesis from
a text; Right: a concrete example for it (stop-words removed).
Edges in the upper part of the diagram represent entailment
rules. Rules compose chains through AND gates (omitted for
visual clarity). Chains are gathered by OR gates to entail terms,
and the final entailment decision y is the result of their AND
gate.
pects of entailment, mostly neglected by previous
lexical methods, which our model aims to capture:
(1) the reliability variability of different lexical re-
sources; (2) the effect of the length of transitive rule
application chain on the likelihood of its validity;
and (3) addressing cases of multiple entailment evi-
dence when entailing a term.
3.1 The Base Model
Our base model follows the one presented in
(Shnarch et al, 2011), which is described here in
detail to make the current paper self contained.
3.1.1 Entailment generation process
We first specify the process by which a decision
of lexical entailment between T andH using knowl-
edge resources should be determined, as illustrated
in Figure 1 (a general description on the left and
a concrete example on the right). There are two
ways by which a term h ? H is entailed by a term
t ? T . A direct MATCH is the case in which t and
h are identical terms (possibly at the stem or lemma
level). Alternatively, lexical entailment can be es-
tablished based on knowledge of entailing lexical-
12
semantic relations, such as synonyms, hypernyms
and morphological derivations, available in lexical
resources. These relations provide lexical entail-
ment rules, e.g. Jaguar ? car. We denote the re-
source which provided the rule r by R(r).
It should be noticed at this point that such rules
specify a lexical entailment relation that might hold
for some (T,H) pairs but not necessarily for all
pairs, e.g. the rule Jaguar ? car does not hold
in the wildlife context. Thus, the application of an
available rule to infer lexical entailment in a given
(T,H) pair might be either valid or invalid. We note
here the difference between covering a term and en-
tailing it. A term is covered when the available re-
sources suggest its entailment. However, since a rule
application may be invalid for the particular (T,H)
context, a term is entailed only if there is a valid rule
application from T to it.
Entailment is a transitive relation, therefore rules
may compose transitive chains that connect t to h
via intermediate term(s) t? (e.g. crowd ? social
group ? people). For a chain to be valid for the
current (T,H) pair, all its composing rule applica-
tions should be valid for this pair. This corresponds
to a logical AND gate (omitted in Figure 1 for visual
clarity) which takes as input the validity values (1/0)
of the individual rule applications.
Next, multiple chains may connect t to h (as for
ti and hj in Figure 1) or connect several terms in
T to h (as t1 and ti are indicating the entailment of
hj in Figure 1), thus providing multiple evidence for
h?s entailment. For a term h to be entailed by T it
is enough that at least one of the chains from T to
h would be valid. This condition is realized in the
model by an OR gate. Finally, for T to lexically en-
tail H it is usually assumed that every h?H should
be entailed by T (Glickman et al, 2006). Therefore,
the final decision follows an AND gate combining
the entailment decisions for all hypothesis terms.
Thus, the 1-bit outcome of this gate y corresponds
to the sentence-level entailment status.
3.1.2 Probabilistic Setting
When assessing entailment for (T,H) pair, we do
not know for sure which rule applications are valid.
Taking a probabilistic perspective, we assume a pa-
rameter ?R for each resourceR, denoting its reliabil-
ity, i.e. the prior probability that applying a rule from
R for an arbitrary (T,H) pair corresponds to valid
entailment3. Under this perspective, direct MATCHs
are considered as rules coming from a special ?re-
source?, for which ?MATCH is expected to be close to
1. Additionally, there could be a term h which is not
covered by any of the resources at hand, whose cov-
erage is inevitably incomplete. We assume that each
such h is covered by a single rule coming from a
dummy resource called UNCOVERED, while expect-
ing ?UNCOVERED to be relatively small. Based on the
?R values we can now estimate, for each entailment
inference step in Figure 1, the probability that this
step is valid (the corresponding bit is 1).
Equations (1) - (3) correspond to the three steps in
calculating the probability for entailing a hypothesis.
p(t
c
?? h) =
?
r?c
p(L
r
?? R) =
?
r?c
?R(r) (1)
p(T?h) =1?p(T9h)=1?
?
c?C(h)
[1?p(t
c
?? h)] (2)
p(T?H) =
?
h?H
p(T?h) (3)
First, Eq. (1) specifies the probability of a partic-
ular chain c, connecting a text term t to a hypothesis
term h, to correspond to a valid entailment between
t and h. This event is denoted by t
c
??h and its prob-
ability is the joint probability that the applications
of all rules r ? c are valid. Note that every rule r
in a chain c connects two terms, its left-hand-side L
and its right-hand-side R. The left-hand-side of the
first rule in c is t? T and the right-hand-side of the
last rule in it is h ? H . Let us denote the event of
a valid rule application by L
r
??R. Since a-priori a
rule r is valid with probability ?R(r), and assuming
independence of all r?c, we obtain Eq. (1).
Next, Eq. (2) utilizes Eq. (1) to specify the prob-
ability that T entails h (at least by one chain). Let
C(h) denote the set of chains which suggest the en-
tailment of h. The requested probability is equal to
1 minus the probability of the complement event,
that is, T does not entail h by any chain. The lat-
ter probability is the product of probabilities that all
3Modeling a conditional probability for the validity of r,
which considers contextual aspects of r?s validity in the current
(T,H) context, is beyond the scope of this paper (see discus-
sion in Section 6)
13
chains c?C(h) are not valid (again assuming inde-
pendence of chains).
Finally, Eq. (3) gives the probability that T entails
all of H (T ? H), assuming independence of H?s
terms. This is the probability that every h ? H is
entailed by T , as specified by Eq. (2).
Altogether, these formulas fall out of the standard
probabilistic estimate for the output of AND and OR
gates when assuming independence amongst their
input bits.
As can be seen, the base model distinguishes
varying resource reliabilities, as captured by ?R, de-
creases entailment probability as rule chain grows,
having more elements in the product of Eq. (1), and
increases it when entailment of a term is supported
by multiple chains with more inputs to the OR gate.
Next we describe two extensions for this base model
which address additional important phenomena of
lexical entailment.
3.2 Relaxing the AND Gate
Based on term-level decisions for the entailment of
each h ? H , the model has to produce a sentence-
level decision of T ? H . In the model described so
far, for T to entailH it must entail all its terms. This
demand is realized by the AND gate at the bottom of
Figure 1. In practice, this demand is too strict, and
we would like to leave some option for entailing H
even if not every h?H is entailed. Thus, it is desired
to relax this strict demand enforced by the AND gate
in the model.
OR
AND
b1
OR
xn
bn
x1
Noisy-AND
y
Figure 2: A noisy-AND gate
The Noisy-AND model (Pearl, 1988), depicted in
Figure 2, is a soft probabilistic version of the AND
gate, which is often used to describe the interaction
between causes and their common effect. In this
variation, each one of the binary inputs b1, ..., bn of
the AND gate is first joined with a ?noise? bit xi by
an OR gate. Each ?noise? bit is 1 with probability p,
which is the parameter of the gate. The output bit y
is defined as:
y = (b1 ? x1) ? (b2 ? x2) ? ? ? ? ? (bn ? xn)
and the conditional probability for it to be 1 is:
p(y = 1|b1, ..., bn, n) =
n?
i=1
p(1?bi) = p(n?
?
i bi)
If all the binary input values are 1, the output is de-
terministically 1. Otherwise, the probability that the
output is 1 is proportional to the number of ones in
the input, where the distribution depends on the pa-
rameter p. In case p = 0 the model reduces to the
regular AND.
In our model we replace the final strict AND with
a noisy-AND, thus increasing the probability of T to
entail H , to account for the fact that sometimes H
might be entailed from T even though some h ?H
is not directly entailed.
The input size n for the noisy-AND is the length
of the hypotheses and therefore it varies from H to
H . Had we used the same model parameter p for all
lengths, the probability to output 1 would have de-
pended solely on the number of 0 bits in the input
without considering the number of ones. For exam-
ple, the probability to entail a hypothesis with 10
terms given that 8 of them are entailed by T (and 2
are not) is p2. The same probability is obtained for a
hypothesis of length 3 with a single entailed term.
We, however, expect the former to have a higher
probability since a larger portion of its terms is en-
tailed by T .
There are many ways to incorporate the length of
a hypothesis into the noisy-AND model in order to
normalize its parameter. The approach we take is
defining a separate parameter pn for each hypothesis
length n such that pn = ?
1
n
NA, where ?NA becomes
the underlying parameter value of the noisy-AND,
i.e.
p(y = 1|b1, ..., bn, n) = p
(n?
?
bi)
n = ?
n?
?
bi
n
NA
This way, if non of the hypothesis terms is entailed,
the probability for its entailment is ?NA, indepen-
dent of its length:
p(y = 1|0, 0, ..., 0, n) = pnn = ?NA
14
As can be seen from Figure 1, replacing the final
AND gate by a noisy-AND gate is equivalent to
adding an additional chain to the OR gate of each
hypothesis term. Therefore we update Eq. (2) to:
p(T ? h) =1? p(T 9 h)
=1? [(1? ?
1
n
NA) ?
?
c?C(h)
[1? p(t
c
?? h)]]
(2?)
In the length-normalized noisy-AND model the
value of the parameter p becomes higher for longer
hypotheses. This increases the probability to entail
such hypotheses, compensating for the lower proba-
bility to strictly entail all of their terms.
3.3 Considering Coverage Level
The second extension of the base model follows our
observation that the prior validity likelihood for a
rule application, increases as more of H?s terms are
covered by the available resources. In other words,
if we have a hypothesis H1 with k covered terms
and a hypothesis H2 in which only j < k terms are
covered, then an arbitrary rule application for H1 is
more likely to be valid than an arbitrary rule appli-
cation for H2.
We chose to model this phenomenon by normal-
izing the reliability ?R of each resource according
to the number of covered terms in H . The normal-
ization is done in a similar manner to the length-
normalized noisy-AND described above, obtaining
a modified version of Eq. (1):
p(t
c
?? h) =
?
r?c
?
1
#covered
R(r) (1
?)
As a results, the larger the number of covered terms
is, the larger ?R values our model uses and, in total,
the entailment probability increases.
To sum up, we have presented the base model,
providing a probabilistic estimate for the entailment
status in our generation process specified in 3.1.
Two extensions were then suggested: one that re-
laxes the strict AND gate and normalizes this re-
laxation by the length of the hypothesis; the second
extension adjusts the validity of rule applications as
a function of the number of the hypothesis covered
terms. Overall, our full model combines both exten-
sions over the base probabilistic model.
4 Parameter Estimation
The difficulty in estimating the ?R values from train-
ing data arises because these are term-level param-
eters while the RTE-training entailment annotation
is given for the sentence-level, each (T,H) pair in
the training is annotated as either entailing or not.
Therefore, we use an instance of the EM algorithm
(Dempster et al, 1977) to estimate these hidden pa-
rameters.
4.1 E-Step
In the E-step, for each application of a rule r in a
chain c for h?H in a training pair (T,H), we com-
pute whcr(T,H), the posterior probability that the
rule application was valid given the training annota-
tion:
whcr(T,H) =
?
?
?
p(L
r
??R|T?H) if T?H
p(L
r
??R|T9H) if T9H
(4)
where the two cases refer to whether the training pair
is annotated as entailing or non-entailing. For sim-
plicity, we write whcr when the (T,H) context is
clear.
The E-step can be efficiently computed using
dynamic programming as follows; For each train-
ing pair (T,H) we first compute the probability
p(T ? H) and keep all the intermediate computa-
tions (Eq. (1)- (3)). Then, the two cases of Eq. (4),
elaborated next, can be computed from these expres-
sions. For computing Eq. (4) in the case that T?H
we have:
p(L
r
?? R|T?H) = p(L
r
?? R|T ? h) =
p(T?h|L
r
?? R)p(L
r
??R)
p(T?h)
The first equality holds since when T entails H ev-
ery h ? H is entailed by it. Then we apply Bayes?
rule. We have already computed the denominator
(Eq. (2)), p(L
r
?? R) ? ?R(r) and it can be shown
4
that:
p(T?h|L
r
??R) = 1?
p(T9h)
1? p(t
c
??h)
? (1?
p(t
c
??h)
?R(r)
)
(5)
4The first and second denominators reduce elements from
the products in Eq. 2 and Eq. 1 correspondingly
15
where c is the chain which contains the rule r.
For computing Eq. (4), in the second case, that
T9H , we have:
p(L
r
??R|T9H) =
p(T9H|L r??R)p(L r??R)
p(T9H)
In analogy to Eq. (5) it can be shown that
p(T9H|L r??R) = 1?
p(T?H)
p(T?h)
?p(T?h|L
r
??R)
(6)
while the expression for p(T?h|L
r
??R) appears in
Eq. (5).
This efficient computation scheme is an instance
of the belief-propagation algorithm (Pearl, 1988) ap-
plied to the entailment process, which is a loop-free
directed graph (Bayesian network).
4.2 M-Step
In the M-step we need to maximize the EM auxiliary
function Q(?) where ? is the set of all resources re-
liability values. Applying the derivation of the aux-
iliary function to our model (first without the exten-
sions) we obtain:
Q(?) =
?
T,H
?
h?H
?
c?C(h)
?
r?c
(whcr log ?R(r) +
(1? whcr) log(1? ?R(r)))
We next denote by nR the total number of applica-
tions of rules from resource R in the training data.
We can maximize Q(?) for each R separately to ob-
tain the M-step parameter-updating formula:
?R =
1
nR
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
whcr (7)
The updated parameter value averages the posterior
probability that rules from resource R have been
validly applied, across all its utilizations in the train-
ing data.
4.3 EM for the Extended Model
In case we normalize the noisy-AND parameter by
the hypothesis length, for each length we use a dif-
ferent parameter value for the noisy-AND and we
cannot simply merge the information from all the
training pairs (T,H). To find the optimal param-
eter value for ?NA, we need to maximize the fol-
lowing expression (the derivation of the auxiliary
function to the hypothesis-length-normalized noisy-
AND ?resource?):
Q(?NA) =
?
T,H
?
h?H
(whNA log(?
1
n
NA) +
(1? whNA) log(1? ?
1
n
NA)) (8)
where n is the length of H , ?NA is the parameter
value of the noisy-AND model andwhNA is the pos-
terior probability that the noisy-AND was used to
validly entail the term h5, i.e.
whNA(T,H) =
?
??
??
p(T
NA
???h|T?H) if T?H
p(T
NA
???h|T9H) if T9H
The two cases of the above equation are similar to
Eq. (4) and can be efficiently computed in analogy
to Eq. (5) and Eq. (6).
There is no close-form expression for the param-
eter value ?NA that maximizes expression (8). Since
?NA?[0, 1] is a scalar parameter, we can find ?NA
value that maximizes Q(?NA) using an exhaustive
grid search on the interval [0, 1], in each iteration of
the M-step. Alternatively, for an iterative procedure
to maximize expression (8), see Appendix A.
In the same manner we address the normalization
of the reliability ?R of each resourcesR by the num-
ber of H?s covered terms. Expression (8) becomes:
Q(?R) =
?
T,H
?
h?H
?
c?C(h)
?
r?c|R(r)=R
(whcr log(?
cov
R ) + (1? whcr) log(1? ?
cov
R ))
were 1cov is the number of H terms which are cov-
ered. We can find the ?R that maximizes this equa-
tion in one of the methods described above.
5 Evaluation and Results
For our evaluation we use the RTE-5 pilot task and
the RTE-6 main task data sets described in Sec-
tion 2. In our system, sentences are tokenized and
stripped of stop words and terms are tagged for part-
of-speech and lemmatized. We utilized two lexical
resources, WordNet (Fellbaum, 1998) and CatVar
5In contrary to Eq. 4, here there is no specific t ? T that
entails h, therefore we write T
NA
???h
16
(Habash and Dorr, 2003). From WordNet we took as
entailment rules synonyms, derivations, hyponyms
and meronyms of the first senses of T and H terms.
CatVar is a database of clusters of uninflected words
(lexemes) and their categorial (i.e. part-of-speech)
variants (e.g. announce (verb), announcer and an-
nouncement(noun) and announced (adjective)). We
deduce an entailment relation between any two lex-
emes in the same cluster. Model?s parameters were
estimated from the development set, taken as train-
ing. Based on these parameters, the entailment prob-
ability was estimated for each pair (T,H) in the test
set, and the classification threshold was tuned by
classification over the development set.
We next present our evaluation results. First we
investigate the impact of utilizing lexical resources
and of chaining rules. In section 5.2 we evaluate the
contribution of each extension of the base model and
in Section 5.3 we compare our performance to that
of state-of-the-art entailment systems.
5.1 Resources and Rule-Chaining Impact
As mentioned in Section 2, in the RTE data sets it
is hard to show more than a moderate improvement
when utilizing lexical resources. Our analysis as-
cribes this fact to the relatively small amount of rule
applications in both data sets. For instance, in RTE-
6 there are 10 times more direct matches of identi-
cal terms than WordNet and CatVar rule applications
combined, while in RTE-5 this ratio is 6. As a result
the impact of rule applications can be easily shad-
owed by the large amount of direct matches.
Table 1 presents the performance of our (full)
model when utilizing no resources at all, WordNet,
CatVar and both, with chains of a single step. We
also considered rule chains of length up to 4 and
present here the results of 2 chaining steps with
WordNet-2 and (WordNet+CatVar)-2.
Overall, despite the low level of rule applications,
we see that incorporating lexical resources in our
model significantly6 and quite consistently improves
performance over using no resources at all. Natu-
rally, the optimal combination of resources may vary
somewhat across the data sets.
In RTE-6 WordNet-2 significantly improved per-
6All significant results in this section are according to Mc-
Nemar?s test with p < 0.01 unless stated otherwise
formance over the single-stepped WordNet. How-
ever, mostly chaining did not help, suggesting the
need for future work to improve chain modeling in
our framework.
Model
F1%
RTE-5 RTE-6
no resources 41.6 44.9
WordNet 45.8 44.6
WordNet-2 45.7 45.5
CatVar 46.9 45.6
WordNet + CatVar 48.3 45.6
(WordNet + CatVar)-2 47.1 44.0
Table 1: Evaluation of the impact of resources and chaining.
5.2 Model Components impact
We next assess the impact of each of our proposed
extensions to the base probabilistic model. To that
end, we incorporate WordNet+CatVar (our best con-
figuration above) as resources for the base model
(Section 3.1) and compare it with the noisy-AND
extension (Eq. (2?)), the covered-norm extension
which normalizes the resource reliability parame-
ter by the number of covered terms (Eq. (1?)) and
the full model which combines both extensions. Ta-
ble 2 presents the results: both noisy-AND and
covered-norm extensions significantly increase F1
over the base model (by 4.5-8.4 points). This scale
of improvement was observed with all resources and
chain-length combinations. In both data sets, the
combination of noisy-AND and covered-norm ex-
tensions in the full model significantly outperforms
each of them separately7, showing their complemen-
tary nature. We also observed that applying noisy-
AND without the hypothesis length normalization
hardly improved performance over the base model,
emphasising the importance of considering hypothe-
sis length. Overall, we can see that both base model
extensions improve performance.
Table 3 illustrates a set of maximum likelihood
parameters that yielded our best results (full model).
The parameter value indicates the learnt reliability
of the corresponding resource.
7With the following exception: in RTE-5 the full model is
better than the noisy-AND extension with significance of only
p = 0.06
17
Model
F1%
RTE-5 RTE-6
base model 36.2 38.5
noisy-AND 44.6 43.1
covered-norm 42.8 44.7
full model 48.3 45.6
Table 2: Impact of model components.
?MATCH ?WORDNET ?CATVAR ?UNCOVERED ?NA
0.80 0.70 0.65 0.17 0.05
Table 3: A parameter set of the full model which maximizes
the likelihood of the training set.
5.3 Comparison to Prior Art
Finally, in Table 4, we put these results in the con-
text of the best published results on the RTE task.
We compare our model to the average of the best
runs of all systems, the best and second best per-
forming lexical systems and the best full system of
each challenge. For both data sets our model is situ-
ated high above the average system. For the RTE-6
data set, our model?s performance is third best with
Majumdar and Bhattacharyya (2010) being the only
lexical-level system which outperforms it. However,
their system utilized additional processing that we
did not, such as named entity recognition and co-
reference resolution8. On the RTE-5 data set our
model outperforms any other published result.
Model
F1%
RTE-5 RTE-6
full model 48.3 45.6
avg. of all systems 30.5 33.8
2nd best lexical system 40.3a 44.0b
best lexical system 44.4c 47.6d
best full system 45.6c 48.0e
Table 4: Comparison to RTE-5 and RTE-6 best entailment
systems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark and
Harrison, 2010), (c)(Mirkin et al, 2009a)(2 submitted runs),
(d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al,
2010).
8We note that the submitted run which outperformed our re-
sult utilized a threshold which was a manual modification of the
threshold obtained systematically in another run. The latter run
achieved F1 of 42.4% which is below our result.
We conclude that our probabilistic model demon-
strates quality results which are also consistent,
without applying heuristic methods of the kinds re-
viewed in Section 2
6 Conclusions and Future Work
We presented, a probabilistic model for lexical en-
tailment whose innovations are in (1) considering
each lexical resource separately by associating an
individual reliability value for it, (2) considering the
existence of multiple evidence for term entailment
and its impact on entailment assessment, (3) setting
forth a probabilistic method to relax the strict de-
mand that all hypothesis terms must be entailed, and
(4) taking account of the number of covered terms in
modeling entailment reliability.
We addressed the impact of the various compo-
nents of our model and showed that its performance
is in line with the best state-of-the-art inference sys-
tems. Future work is still needed to reflect the im-
pact of transitivity. We consider replacing the AND
gate on the rules of a chain by a noisy-AND, to relax
its strict demand that all its input rules must be valid.
Additionally, we would like to integrate Contextual
Preferences (Szpektor et al, 2008) and other works
on Selectional Preference (Erk and Pado, 2010) to
verify the validity of the application of a rule in a
specific (T,H) context. We also intend to explore
the contribution of our model within a complex sys-
tem that integrates multiple levels of inference as
well as its contribution for other applications, such
as Passage Retrieval.
References
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
2008. Efficient semantic deduction and approximate
matching over compact parse forests. In Proc. of TAC.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proc. of TAC.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and
Anette Frank. 2007. A semantic approach to textual
18
entailment: System evaluation and task analysis. In
Proc. of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. of EMNLP.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proc. of TAC.
Courtney Corley and Rada Mihalcea. 2005. Measuring
the semantic similarity of texts. In Proc. of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Lecture Notes in Computer Science, vol-
ume 3944, pages 177?190.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the royal statistical society, se-
ries [B], 39(1):1?38.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. A
probabilistic classification approach for lexical textual
entailment. In Proc. of AAAI.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of the EMNLP.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proc. of NAACL.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at TAC 2010 RTE and summarization track. In Proc.
of TAC.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Proc.
of TAC.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proc. of TAC.
Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, Ido
Dagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.
2009a. Addressing discourse and document structure
in the RTE search task. In Proc. of TAC.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009b.
Evaluating the inferential utility of lexical-semantic re-
sources. In Proc. of EACL.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical entail-
ment. In Proc. of ACL, pages 558?563.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Proc.
of ACL-08: HLT.
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE
3. In Proc. of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Rui Wang, Yi Zhang, and Guenter Neumann. 2009. A
joint syntactic-semantic representation for recognizing
textual relatedness. In Proc. of TAC.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In Proc. of ACL.
A Appendix: An Iterative Procedure to
Maximize Q(?NA)
There is no close-form expression for the parameter
value ?NA that maximizes expression (8) from Sec-
tion 4.3. Instead we can apply the following iterative
procedure. The derivative of Q(?NA) is:
dQ(?NA)
d?NA
=
?
(
l?whNA
?NA
?
(1?whNA)l??
(l?1)
NA
1? ?lNA
)
where 1l is the hypothesis length and the summation
is over all terms h in the training set. Setting this
derivative to zero yields an equation which the opti-
mal value satisfies:
?NA =
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
(9)
Eq. (9) can be utilized as a heuristic iterative proce-
dure to find the optimal value of ?NA:
?NA ?
?
l?whNA
? (1?whNA)l??
(l?1)
NA
1??lNA
19
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87?97,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Focused Entailment Graphs for Open IE Propositions
Omer Levy
?
Ido Dagan
?
Jacob Goldberger
?
? Computer Science Department ? Faculty of Engineering
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
Abstract
Open IE methods extract structured propo-
sitions from text. However, these propo-
sitions are neither consolidated nor gen-
eralized, and querying them may lead
to insufficient or redundant information.
This work suggests an approach to or-
ganize open IE propositions using entail-
ment graphs. The entailment relation uni-
fies equivalent propositions and induces a
specific-to-general structure. We create a
large dataset of gold-standard proposition
entailment graphs, and provide a novel
algorithm for automatically constructing
them. Our analysis shows that predicate
entailment is extremely context-sensitive,
and that current lexical-semantic resources
do not capture many of the lexical infer-
ences induced by proposition entailment.
1 Introduction
Open information extraction (open IE) extracts
natural language propositions from text without
pre-defined schemas as in supervised relation ex-
traction (Etzioni et al., 2008). These proposi-
tions represent predicate-argument structures as
tuples of natural language strings. Open IE en-
ables knowledge search by aggregating billions of
propositions from the web
1
. It may also be per-
ceived as capturing an unsupervised knowledge
representation schema, complementing supervised
knowledge bases such as Freebase (Bollacker et
al., 2008), as suggested by Riedel et al (2013).
However, language variability obstructs open IE
from becoming a viable knowledge representation
framework. As it does not consolidate natural lan-
guage expressions, querying a database of open IE
propositions may lead to either insufficient or re-
dundant information. As an illustrative example,
1
See demo: openie.cs.washington.edu
querying the demo (footnote 1) for the generally
equivalent relieves headache or treats headache
returns two different lists of entities; out of the top
few results, the only answers these queries seem
to agree on are caffeine and sex. This is a major
drawback relative to supervised knowledge rep-
resentations, which map natural language expres-
sions to structured formal representations, such as
treatments in Freebase.
In this work, we investigate an approach for or-
ganizing and consolidating open IE propositions
using the novel notion of proposition entailment
graphs (see Figure 1) ? graphs in which each
node represents a proposition and each directed
edge reflects an entailment relation, in the spirit
of textual entailment (Dagan et al., 2013). En-
tailment provides an effective structure for ag-
gregating natural-language based information; it
merges semantically equivalent propositions into
cliques, and induces specification-generalization
edges between them. For example, (aspirin, elim-
inate, headache) entails, and is more specific than,
(headache, respond to, painkiller).
We thus propose the task of constructing an
entailment graph over a set of open IE proposi-
tions (Section 3), which is closely related to Be-
rant et al?s work (2012) who introduced predicate
entailment graphs. In contrast, our work explores
propositions, which are essentially predicates in-
stantiated with arguments, and thus semantically
richer. We provide a dataset of 30 such graphs,
which represent 1.5 million pairwise entailment
decisions between propositions (Section 4).
To approach this task, we extend the state-of-
the-art method for building entailment graphs (Be-
rant et al., 2012) from predicates to complete
propositions. Both Snow et al (2006) and Berant et
al used WordNet as distant supervision when train-
ing a local pairwise model of lexical entailment.
However, analyzing our data revealed that the lex-
ical inferences captured in WordNet are quite dif-
87
Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure
denote cliques, meaning that all propositions within them are equivalent.
ferent from the real lexical inferences induced by
proposition entailment, making WordNet a mis-
leading form of supervision. We therefore employ
direct proposition-level supervision, and design a
probabilistic model that captures the underlying
lexical-component inferences (Section 5). We ex-
plore a variety of natural extensions to prior art as
baselines (Section 6) and show that our model out-
performs them (Section 7).
While our model increases performance on this
task, there is still much room for improvement. A
deeper analysis (Section 8) shows that common
lexical-semantic resources, on which we rely as
well, are either too noisy or provide inadequate re-
call regarding lexical entailment. In particular, we
find that predicate inference within propositions
often goes beyond inference between the predi-
cates? linguistic meanings. While pneumonia re-
quires antibiotics and pneumonia is treated by an-
tibiotics mean the same, the inherent meanings of
require and treat are different. These inferences
pertain to specific world knowledge, and warrant
future research.
Our work also contributes to textual entailment
research. First, we extend entailment graphs to
complete propositions. Secondly, we investigate
an intermediate problem of recognizing entail-
ment between language-based predicate-argument
tuples. Though this problem is simpler than
sentence-level entailment, it does capture entail-
ment of complete statements, which proves to be
quite challenging indeed.
2 Background
Our work builds upon two major research threads:
open IE, and entailment graphs.
2.1 Open Information Extraction
Research in open IE (Etzioni et al., 2008) has fo-
cused on transforming text to predicate-argument
tuples (propositions). The general approach is to
learn proposition extraction patterns, and use them
to create tuples while denoting extraction confi-
dence. Various methods differ in the type of pat-
terns they acquire. For instance, (Banko et al.,
2007) and (Fader et al., 2011) used surface pat-
terns, while (Mausam et al., 2012) and (Xu et al.,
2013) used syntactic dependencies.
Yates and Etzioni (2009) tried to mitigate the
issue of language variability (as exemplified in
the introduction) by clustering synonymous predi-
cates and arguments. While these clusters do con-
tain semantically related items, they do not neces-
sarily reflect equivalence or implication. For ex-
ample, coffee, tea, and caffeine may all appear
in one cluster, but coffee does not imply tea; on
the other hand, separating any element from this
cluster removes a valid implication. Entailment,
however, can capture the fact that both beverages
imply caffeine, but not one another. Also related,
Riedel et al (2013) try to generalize over open IE
extractions by combining knowledge from Free-
base and globally predicting which unobserved
propositions are true. In contrast, our work identi-
fies inference relations between concrete pairs of
observed propositions.
2.2 Entailment Graphs of Words and Phrases
Previous work focused on entailment graphs or
similar structures at the sub-propositional level.
In these graphs, each node represents a natu-
ral language word or phrase, and each directed
edge an entailment (or generalization) relation.
Snow et al (2006) created a taxonomy of sense-
88
disambiguated nouns and their hyponymy rela-
tions. Berant et al (2012) constructed entailment
graphs of predicate templates. Recently, Mehdad
et al (2013) built an entailment graph of noun
phrases and partial sentences for topic labeling.
The notion of proposition entailment graphs, how-
ever, is novel. This distinction is critical, be-
cause apparently, entailment in the context of spe-
cific propositions does not behave like context-
oblivious lexical entailment (see Section 8).
Berant et al?s work was implemented in Adler
et al?s (2012) text exploration demo, which instan-
tiated manually-annotated predicate entailment
graphs with arguments, and used an additional
lexical resource to determine argument entail-
ment. The combined graphs of predicate and argu-
ment entailments induced a proposition entailment
graph, which could then be explored in a faceted-
search scheme. Our work goes beyond, and at-
tempts to build entailment graphs of propositions
automatically.
2.2.1 Berant et al?s Algorithm for Predicate
Entailment Graph Construction
We present Berant et al?s algorithm in detail, as we
rely on it later on. Given a set of predicates {i}
1..n
as input (constituting the graph nodes), it returns
a set of entailment decisions (i, j), which become
the directed edges of the entailment graph. The
method works in two phases: (1) local estimation,
and (2) global optimization.
The local estimation model considers every po-
tential edge (i, j) and estimates the probability p
ij
that this edge indeed exists, i.e. that i entails j.
Each predicate pair is represented with distribu-
tional similarity features, providing some indica-
tion of whether i entails j. The estimator then uses
logistic regression (or a linear SVM) over those
features to predict the probability of entailment. It
is trained with distant supervision from WordNet,
employing synonyms, hypernyms, and (WordNet)
entailments as positive examples, and antonyms,
hyponyms, and cohyponyms as negative.
The global optimization phase then searches
for the most probable transitive entailment graph,
given the local probability estimations. It does so
with an integer linear program (ILP), where each
pair of predicates is represented by a binary vari-
able x
ij
, denoting whether there is an entailment
edge from i to j. The objective function corre-
sponds to the log likelihood of the assignment:
?
i 6=j
x
ij
(
log
(
p
ij
1?p
ij
)
+ log
(
pi
1?pi
))
. The prior
term pi is the probability of a random pair of pred-
icates to be in an entailment relation, and can be
estimated in advance. The ILP solver searches
for the optimal assignment that maximizes the ob-
jective function under transitivity constraints, ex-
pressed as linear constraints ?
i,j,k
x
ij
+ x
jk
?
x
ik
? 1.
3 Task Definition
A proposition entailment graph is a directed graph
where each node is a proposition s
i
(s for sen-
tence) and each edge (s
i
, s
j
) represents an en-
tailment relation from s
i
to s
j
. A proposi-
tion s
i
is a predicate-argument structure s
i
=
(
p
i
, a
1
i
, a
2
i
, ..., a
m
i
i
)
with one predicate p
i
and its
arguments. A proposition-level entailment (s
i
, s
j
)
holds if the verbalization of s
i
implies s
j
, accord-
ing to the definition of textual entailment (Dagan
et al., 2013); i.e. if humans reading s
i
would typi-
cally infer that s
j
is most likely true. Given a set of
propositions (graph nodes), the task of construct-
ing a proposition entailment graph is to recognize
all the entailments among the propositions, i.e.
deciding which directional edges connect which
pairs of nodes.
In this paper, we consider the narrower task
of constructing focused proposition entailment
graphs, following Berant et al?s methodology
in creating focused predicate entailment graphs.
First, all predicates are binary (have two argu-
ments) and are denoted s
i
=
(
a
1
i
, p
i
, a
2
i
)
. Sec-
ondly, we assume that the propositions were re-
trieved by querying for a particular concept; out
of the two arguments, one argument t (topic) is
common to all the propositions in a single graph.
We denote the non-topic argument as a
i
. Figure 1
presents an example of an informative entailment
graph focused on the topic headache.
Though confined, this setting still challenges
the state-of-the-art in textual entailment (see Sec-
tion 7). Moreover, these restrictions facilitate
piece-wise investigation of the entailment problem
(see Section 8).
4 Dataset
To construct our dataset of open IE extractions, we
found Google?s syntactic ngrams (Goldberg and
Orwant, 2013) as a useful source of high-quality
propositions. Based on a corpus of 3.5 million En-
glish books, it aggregates every syntactic ngram
89
? subtree of a dependency parse ? with at most
4 dependency arcs. The resource contains only
tree fragments that appeared at least 10 times in
the corpus, filtering out many low-quality syntac-
tic ngrams.
We extracted the syntactic ngrams that reflect
propositions, i.e. subject-verb-object fragments
where object modifies the verb with either dobj
or pobj. Prepositions in pobj were concatenated
to the verb (e.g. use with). In addition, both sub-
ject and object must each be a noun phrase con-
taining two tokens at most, which are either nouns
or adjectives. Each token in the extracted frag-
ments was then lemmatized using WordNet. After
lemmatization, we grouped all identical proposi-
tions and aggregated their counts. Approximately
68 million propositions were collected.
We chose 30 topics from the healthcare domain
(such as influenza, hiv, and penicillin). For each
topic, we collected the set of propositions con-
taining it, and manually filtered noisy extractions.
This yielded 30 high-quality sets of 5,714 propo-
sitions in total, where each set becomes the set of
nodes in a separate focused entailment graph. The
graphs range from 55 propositions (scurvy) to 562
(headache), with an average of over 190 proposi-
tions per graph. Summing the number of propo-
sition pairs within each graph amounts to a total
of 1.5 million potential entailment edges, which
makes it by far the largest annotated textual entail-
ment dataset to date.
We used a semi-automatic annotation process,
which dramatically narrows down the number of
manual decisions, and hence, the required anno-
tation time. In short, the annotators are given a
series of small clustering tasks before annotating
entailment between those clusters.
2
The annotation process was carried out by two
native English speakers, with the aid of encyclope-
dic knowledge for unfamiliar medical terms. The
agreement on a subset of five randomly sampled
graphs was ? = 0.77. Annotating a single graph
took about an hour and a half on average.
Positive entailment judgements constituted only
8.4% of potential edges, and were found to be
100% transitive. We observe that in nearly all of
those cases, a natural alignment between entail-
ing components occurs: predicates align with each
other, the topic is shared, and the remaining non-
2
The annotated dataset is publicly available on the first
author?s website.
topic argument aligns with its counterpart. Con-
sider the topic arthritis and the entailing proposi-
tion pair (arthritis, cause, pain)?(symptom, as-
sociate with, arthritis); cause?associate with,
while pain?symptom. Rarely, some mis-
alignments do occur; for instance (vaccine,
protects, body)?(vaccine, provides, protection).
However, it is almost always the case that proposi-
tions entail if and only if their aligned lexical com-
ponents entail as well.
5 Algorithm
In this section, we extend Berant et al?s algorithm
(2012) to construct entailment graphs of proposi-
tions. As described in Section 2.2.1, their method
first performs local estimation of predicate entail-
ment and then global optimization. We modify the
local estimation phase to estimate proposition en-
tailment instead, and then apply the same global
optimization in the second phase.
In Section 4, we observed the alignment-based
relationship between proposition and lexical en-
tailment. We leverage this observation to predict
proposition entailment with lexical entailment fea-
tures (as Berant et al), using the Component En-
tailment Conjunction (CEC) model in Section 5.1.
Following Snow et al (2006) and Berant et
al, we could train CEC using distant supervision
from WordNet. In fact, we did try this approach
(presented as baseline methods, Section 6) and
found that it performed poorly. Furthermore, our
analysis (Section 8) suggests that WordNet rela-
tions do not adequately capture the lexical infer-
ences induced by proposition-level entailment. In-
stead, we use a more realistic signal to train CEC ?
direct supervision from the annotated dataset. Sec-
tion 5.2 describes how we propagate proposition-
level entailment annotations to the latent lexical
components.
5.1 Component Entailment Conjunction
CEC assumes that proposition-level entailment
is the result of entailment within each pair of
aligned components, i.e. a pair of propositions
entail if and only if both their predicate and ar-
gument pairs entail. This assumption stems from
our observation of alignment in Section 4. Fur-
thermore, CEC leverages this interdependence to
learn separate predicate-entailment and argument-
entailment features through proposition-level su-
pervision.
90
Formally, for every ordered pair of propositions
(i, j) we denote proposition entailment as a binary
random variable x
s
ij
and predicate and argument
entailments as x
p
ij
and x
a
ij
, respectively. In our
setting, proposition entailment (x
s
ij
) is observed,
but component entailments (x
p
ij
, x
a
ij
) are hidden.
We use logistic regression, with features ?
p
ij
and
parameter w
p
, as a probabilistic model of predi-
cate entailment (and so for arguments with ?
a
ij
and
w
a
):
p
ij
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
= ?
(
?
p
ij
? w
p
)
a
ij
= P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= ?
(
?
a
ij
? w
a
)
(1)
where ? is the sigmoid ? (z) =
1
1+e
?z
. We then
define proposition entailment as the conjunction of
its binary components: x
s
ij
= x
p
ij
?x
a
ij
. Therefore,
the probability of proposition entailment given the
component features is:
s
ij
= P
(
x
s
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1, x
a
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
? P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= p
ij
? a
ij
The proposition entailment probability is thus the
product of component entailment probabilities.
Given the proposition-level information
{
x
s
ij
}
,
the log-likelihood is:
` (w
p
, w
a
)=
?
i 6=j
logP
(
x
s
ij
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
=
?
i 6=j
(
x
s
ij
log (p
ij
a
ij
) +
(
1? x
s
ij
)
log (1? p
ij
a
ij
)
)
5.2 Learning Component Models
We wish to learn the model?s parameters (w
p
, w
a
).
Our approach uses direct proposition-level super-
vision from our annotated dataset to train the com-
ponent logistic regression models. Since compo-
nent entailment (x
p
ij
, x
a
ij
) is not observed in the
data, we apply the iterative EM algorithm (Demp-
ster et al., 1977). In the E-step we estimate their
probabilities from proposition-level labels (x
s
ij
),
and in the M-step we use those estimates as ?soft?
labels to learn the component-level model param-
eters (w
p
, w
a
).
E-Step During the E-step in iteration t + 1,
we compute the probability of component entail-
ments given the proposition entailment informa-
tion, based on the parameters at iteration t (w
p
t
,
w
a
t
). The predicate probabilities are given by:
c
p
ij
= P
(
x
p
ij
= 1
?
?
?
x
s
ij
, ?
p
ij
, ?
a
ij
;w
p
t
, w
a
t
)
(2)
and are computed with Bayes? law:
c
p
ij
=
?
?
?
1 if x
s
ij
= 1
p
t
ij
(
1?a
t
ij
)
1?p
t
ij
a
t
ij
if x
s
ij
= 0
(3)
where p
t
ij
is computed as in Equations 1, with the
parameters at iteration t (w
p
t
). Argument entail-
ment probabilities (c
a
ij
) are computed analogously.
M-Step In the M-step, we compute new values
for the parameters (w
p
t+1
, w
a
t+1
). In our case, there
is no closed-form formula for updating the param-
eters. Instead, at each iteration, we solve a sepa-
rate logistic regression for each component. While
we have each component model?s features (?
p
ij
,
assuming predicates for notation), we do not ob-
serve the component-level entailment labels (x
p
ij
);
instead, we obtain their probabilities (c
p
ij
) from the
expectation step.
To learn the parameters (w
p
t+1
, w
a
t+1
) from the
component entailment probabilities (c
p
ij
), we em-
ploy a weighted variant of logistic regression, that
can utilize ?soft? class labels (i.e. a probability
distribution over {0, 1}). To solve such a logistic
regression (e.g. for w
p
t+1
), we maximize the log-
likelihood:
`
(
w
p
t+1
)
=
?
ij
(
c
p
ij
log
(
P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
+
(
1? c
p
ij
)
log
(
P
(
x
p
ij
= 0
?
?
?
?
p
ij
;w
p
t+1
)))
For optimization, we calculate the derivative, and
use gradient ascent to update w
p
t+1
:
?w
p
t+1
=
?`
(
w
p
t+1
)
?w
p
t+1
=
?
ij
(
c
p
ij
? P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
?
p
ij
This optimization is concave, and therefore the
unique global maximum can be efficiently ob-
tained.
5.3 Features
Similar to Berant et al, we used three types of fea-
tures to describe both predicate pairs (?
p
ij
) and ar-
gument pairs (?
a
ij
): distributional similarities, lex-
ical resources, and string distances.
91
We used the entire database of 68 million ex-
tracted propositions (see Section 4) to create a
word-context matrix; context was defined as other
words that appeared in the same proposition, and
each word was represented as (string, role), role
being the location within the proposition, either
a
1
, p, or a
2
. The matrix was then normalized with
pointwise mutual information (Church and Hanks,
1990). We used various metrics to measure dif-
ferent types of similarities between each compo-
nent pair, including: cosine similarity, Lin?s sim-
ilarity (1998), inclusion (Weeds and Weir, 2003),
average precision, and balanced average precision
(Kotlerman et al., 2010). Weed?s and Kotlerman?s
metrics are directional (asymmetric) and indicate
the direction of a potential entailment relation.
These features were used for both predicates and
arguments. In addition, we used Melamud et al?s
(2013) method to learn a context-sensitive model
of predicate entailment, which estimates predicate
similarity in the context of the given arguments.
We leveraged the Unified Medical Language
System (UMLS) to check argument entailment,
using the parent and synonym relations. A single
feature indicated whether such a connection ex-
ists. We also used WordNet relations as features,
specifically: synonyms, hypernyms, entailments,
hyponyms, cohyponyms, antonyms. Each Word-
Net relation constituted a different feature for both
predicates and arguments.
Finally, we added a string equality feature and a
Levenshtein distance feature (Levenshtein, 1966)
for different spellings of the same word to both
predicate and argument feature vectors.
6 Baseline Methods
We consider four algorithms that naturally ex-
tend the state-of-the-art to propositions, while us-
ing distant supervision (from WordNet). Since
CEC uses direct supervision, we also examined
another (simpler) directly-supervised algorithm.
As a naive unsupervised baseline, we use Argu-
ment Equality, which returns ?entailing? if the ar-
gument pair is identical. Predicate Equality is de-
fined similarly for predicates.
Component-Level Distant Supervision The
following methods use distant supervision from
WordNet (as in Berant et al?s work, Section 2.2.1)
to explicitly train component-level entailment esti-
mators. Specifically, we train a logistic regression
model for each component as specified in Equa-
tions 1 in Section 5.1. We present four methods,
which differ in the way they obtain global graph-
level entailment decisions for propositions, based
on the local component entailment estimates (p
ij
,
a
ij
in Section 5.1).
The first method, Opt(Arg ? Pred), uses the
product of both component models to estimate lo-
cal proposition-level entailment: s
ij
= p
ij
? a
ij
.
The global set of proposition entailments is then
determined using Berant et al?s global optimiza-
tion, according to the proposition-level scores s
ij
.
Note that this method is identical to CEC dur-
ing inference, but differs in the way the local es-
timators are learned (with component-level super-
vision from WordNet).
An alternative is Opt(Arg) ? Opt(Pred). It
first obtains local probabilities (p
ij
, a
ij
) for each
component as in Opt(Arg ? Pred), but then em-
ploys component-level global optimization (tran-
sitivity enforcement), yielding two sets of entail-
ment decisions, x
p
ij
and x
a
ij
. Proposition entail-
ment is then determined by the conjunction x
s
ij
=
x
p
ij
? x
a
ij
, as in (Adler et al., 2012).
Finally, Opt(Arg) ignores the predicate com-
ponent. Instead, it uses only the argument en-
tailment graph (as produced by Opt(Arg) ?
Opt(Pred)) to decide on proposition entailment;
i.e. a pair of propositions entail if and only if their
arguments entail. Opt(Pred) is defined analo-
gously.
Proposition-Level Direct Supervision A sim-
pler alternative to CEC that also employs
proposition-level supervision is Joint Features,
which concatenates the component level features
into a unified feature vector: ?
s
ij
= ?
p
ij
? ?
a
ij
. We
then couple them with the gold-standard annota-
tions x
s
ij
to create a training set for a single logistic
regression. We use the trained logistic regression
to estimate the local probability of proposition en-
tailment, and then perform global optimization to
construct the entailment graph.
7 Empirical Evaluation
We evaluate the models in Sections 5 & 6 on the
30 annotated entailment graphs presented in Sec-
tion 4. During testing, each graph was evaluated
separately. The results presented in this section
are all micro-averages, though macro-averages
were also computed and found to reflect the same
trends. Models trained with distant supervision
were evaluated on all graphs. For directly super-
92
vised methods, we used 2 ? 6-fold cross valida-
tion (25 training graphs per fold). In this scenario,
each graph induced a set of labeled examples ?
its edges being positive examples, and the miss-
ing potential edges being negative ones ? and the
union of these sets was used as the training set of
that cross-validation fold.
7.1 Results
Table 1 compares the performance of CEC with
that of the baseline methods.
While Joint Features and CEC share exactly the
same features, CEC exploits the inherent conjunc-
tion between predicate and argument entailments
(as observed in Section 4 and modeled in Sec-
tion 5.1), and forces both components to decide on
entailment separately. This differs from the sim-
pler log-linear model (Joint Features) where, for
example, a very strong predicate entailment fea-
ture might override the overall proposition-level
decision, even if there was no strong indication
of argument entailment. As a result, CEC dom-
inates Joint Features in both precision and recall.
The F
1
difference between these methods is sta-
tistically significant with McNemar?s test (1947)
with p  0.01. Specifically, CEC corrected Joint
Features 7621 times, while the opposite occurred
only 4048 times.
CEC also yields relatively high precision
and recall. While it has 2% less recall than
Opt(Arg) (the highest-recall baseline), it sur-
passes Opt(Arg)?s precision by 14%. Along with
a similar comparison to Argument Equality (the
highest precision baseline), CEC notably outper-
forms all baselines.
It is also evident that both directly super-
vised methods outperform the distantly super-
vised methods. Our analysis (Section 8.1) shows
that WordNet lacks significant coverage, and may
therefore be a problematic source of supervision.
Perhaps the most surprising result is the com-
plete failure of WordNet-supervised methods that
consider predicate information. A deeper analy-
sis (Section 8.2) shows that predicate inference is
highly context-sensitive, and deviates beyond the
lexical inferences provided by WordNet.
7.2 Learning Curve
We measure the supervision needed to train the di-
rectly supervised models by their learning curves
(Figure 2). Each point is the average F
1
score
Supervision Method Prec. Rec. F1
None
Argument
81.6% 42.2% 55.6%
Equality
Predicate
9.3% 1.5% 2.6%
Equality
Component
(WordNet)
Opt(Arg
73.8% 3.8% 7.2%
? Pred)
Opt(Arg) ?
72.3% 3.2% 6.0%
Opt(Pred)
Opt(Arg) 64.6% 55.4% 59.7%
Opt(Pred) 11.0% 6.2% 8.0%
Proposition
(Annotated)
Joint
76.3% 51.7% 61.6%
Features
CEC 78.7% 53.5% 63.7%
Table 1: Performance on gold-standard (micro averaged).
Figure 2: Learning curve of directly supervised methods.
across 12 cross-validation folds; e.g. for 10 train-
ing graphs, we used 4 ? 3-fold cross validation.
Even 5 training graphs (a day?s worth of annota-
tion) are enough for CEC to perform on-par with
the best distantly supervised method, and with 15
training graphs it outperforms every baseline, in-
cluding Joint Features trained with 25 graphs.
7.3 Effects of Global Optimization
We evaluate the effects of enforcing transitivity by
considering CEC with and without the global op-
timization phase. Table 2 shows how many entail-
ment edges were added (and removed) by enforc-
ing transitivity, and measures how many of those
modifications were correct. Apparently, transi-
tivity?s greatest effect is the removal of incorrect
entailment edges. The same phenomenon was
also observed in the work on predicate entailment
graphs (Berant et al., 2012). Overall, transitivity
made 4,848 correct modifications out of 6,734 in
total. A ?
2
test reveals that the positive contribu-
tion of enforcing transitivity is indeed statistically
significant (p 0.01).
93
Gold Global Opt Global Opt
Standard Added Edge Removed Edge
Edge Exists 1150 482
No Edge 1404 3698
Table 2: The modifications made by enforcing transitivity
w.r.t. the gold standard. 55% of the edges added by enforcing
transitivity are incorrect, but it removed even more incorrect
edges, improving the overall performance.
8 Analysis of Lexical Inference
Although CEC had a statistically-significant im-
provement upon the baselines, its absolute perfor-
mance leaves much room for improvement. We
hypothesize that the lexical entailment features we
used, following state-of-the-art lexical entailment
modeling, do not capture many of the actual lexi-
cal inferences induced by proposition entailment.
We demonstrate that this is indeed the case.
8.1 Argument Entailment
To isolate the effect of different features on pre-
dicting argument entailment, we collected all
proposition pairs that shared exactly the same
predicate and topic, and thus differed in only their
?free? argument. This yielded 20,336 aligned ar-
gument pairs, whose entailment annotations are
equal to the corresponding proposition-entailment
annotation in the dataset.
Using WordNet synonyms and hypernyms to
predict entailment yielded a precision of about
88%, at 40% recall. Though relatively precise,
WordNet?s coverage is limited, and misses many
inferences. We describe three typical types of in-
ferences that were absent from WordNet.
The first type constitutes of widely-
used paraphrases such as people?persons,
woman?female, and pain?ache. These may be
seen as weaker types of synonyms, which may
have nuances, but are typically interchangeable.
Another type is metonymy, in which a concept
is not referred to by its own name, but by that of
an associated concept. This is very common in
our healthcare dataset, where a disease is often re-
ferred to by its underlying pathogen and vice-versa
(e.g. pneumonia?pneumococcus).
The third type of missing inferences is causal-
ity. Many instances of metonymy (such as the
disease-pathogen example) may be seen as causal-
ity as well. Other examples can be drug and ef-
fect (laxative?diarrhea) or condition and symp-
tom (influenza?fever).
WordNet?s lack of such common-sense infer-
ences, which are abundant in our proposition en-
tailment dataset, might make WordNet a problem-
atic source of distant supervision. The fact that
60% of the entailing examples in our dataset are
labeled by WordNet as non-entailing, means that
for each truly positive training example, there is a
higher chance that it will have a negative label.
Distributional similarity is commonly used to
capture such missing inferences and complement
WordNet-like resources. On this dataset, how-
ever, it failed to do so. One of the more in-
dicative similarity measures, inclusion (Weeds and
Weir, 2003), yielded only 27% precision at 40%
recall when tuning a threshold to optimize F
1
. In-
creasing precision caused a dramatic drop in re-
call: 50% precision limited recall to 3.2%. Other
similarity measures performed similarly or worse.
It seems that current methods of distributional
word similarity also capture relations quite differ-
ent from inference, such as cohyponyms and do-
main relatedness, and might be less suitable for
modeling lexical entailment on their own.
8.2 Context-Sensitive Predicate Entailment
The proposition-level entailment annotation in-
duces an entailment relation between the predi-
cates, which holds in the particular context of the
proposition pair. We wish to understand the na-
ture of this predicate-level entailment, and how it
compares to classic lexical inference as portrayed
in the lexical semantics literature. To that end, we
collected all the entailing proposition pairs with
equal arguments, and extracted the corresponding
predicate pairs (which, assuming alignment, are
necessarily entailing in that context). This list con-
tains 52,560 predicate pairs.
In our first analysis, we explored which Word-
Net relations correlate with predicate entailment,
by checking how well each relation covers the set
of entailed predicate pairs. Synonyms and hyper-
nyms, which are considered positive entailment
indicators, covered only about 8% each. Sur-
prisingly, the hyponym and cohyponym relations
(which are considered negative entailment indica-
tors) covered over 9% and 14%, respectively. Ta-
ble 3 shows the exact details.
It seems that WordNet relations are hardly cor-
related with the context-sensitive predicate-level
entailments in our dataset, and that the classic in-
terpretation of WordNet relations with respect to
entailment does not hold in practice, where en-
94
Interpretation WordNet Relation Coverage
Positive
Synonyms 7.85%
Direct Hypernyms 5.62%
Indirect Hypernyms 3.14%
Entailment 0.33%
Negative
Antonyms 0.31%
Direct Hyponyms 5.74%
Indirect Hyponyms 3.51%
Cohyponyms 14.30%
Table 3: The portion of positive predicate entailments cov-
ered by each WordNet relation. WordNet relations are di-
vided according to their common interpretations with respect
to lexical entailment.
tailments are judged in the context of concrete
propositions. In fact, negative indicators in Word-
Net seem to cover more predicate entailments
than positive ones. This explains the failure of
WordNet-supervised methods with predicate en-
tailment features (Section 7.1).
Since we do not expect WordNet to cover all
shades of entailment, we conducted a manual anal-
ysis as well. 100 entailing predicate pairs were
randomly sampled, and manually annotated for
lexical-level entailment, without seeing their argu-
ments. To compensate for the lack of context, we
guided the annotators to assume a general health-
care scenario, and use a more lenient interpretation
of textual entailment (biased towards positive en-
tailment decisions). Nevertheless, only 56% of the
predicate pairs were labeled as entailing, indicat-
ing that the context-sensitive predicate inferences
captured in our dataset can be quite different from
generic predicate inferences.
We suggest that this phenomenon goes one step
beyond what the current literature considers as
context-sensitive entailment, and that it is more
specific than determining an appropriate lexical
sense. To demonstrate, we present four such
predicate-entailment phenomena.
First, there are cases in which an appropriate
lexical sense could exist in principle, but it is too
specific to be practically covered by a manual re-
source. For example, cures cancer?kills cancer,
but the appropriate sense for kill (cause to cease
existing) does not exist, and in turn, neither does
the hypernymy relation from cure to kill. It is hard
to expect these kinds of obscure senses or relation-
ships to comprehensively appear in a manually-
constructed resource.
In many cases, such a specific sense does not
exist. For example, (pneumonia, require, antibi-
otic)?(pneumonia, treated by, antibiotics), but re-
quire does not have a general sense which means
treat by. The inference in this example does not
stem from the linguistic meaning of each predi-
cate, but rather from the real-world situation their
encapsulating propositions describe.
Another aspect of predicate entailment that
may change when considering propositional con-
text is the direction of inference. For instance,
cause9trigger. While it may be the case that trig-
ger entails cause, the converse is not necessarily
true since cause is far more general. However,
when considering (caffeine, cause, headache) and
(caffeine, trigger, headache), both propositions de-
scribe the same real-world situation, and thus both
propositions are mutually entailing. In this con-
text, cause does indeed entail trigger as well.
Finally, figures of speech (such as metaphors)
are abundant and diverse. Though it may not be
so common to read about a drug that ?banishes?
headaches, most readers would understand the un-
derlying meaning. These phenomena exceed the
current scope of lexical-semantic resources such
as WordNet, and require world knowledge.
9 Conclusion
This paper proposes a novel approach, based on
entailment graphs, for consolidating information
extracted from large corpora. We define the prob-
lem of building proposition entailment graphs, and
provide a large annotated dataset. We also present
the CEC model, which models the connection be-
tween proposition entailment and lexical entail-
ment. Although it outperforms the state-of-the-
art, its performance is not ideal because it relies on
inadequate lexical-semantic resources that do not
capture the common-sense and context-sensitive
inferences which are inherent in proposition en-
tailment. In future work, we intend to further in-
vestigate lexical entailment as induced by proposi-
tion entailment, and hope to develop richer meth-
ods of lexical inference that address the phenom-
ena exhibited in this setting.
Acknowledgements
This work has been supported by the Israeli Min-
istry of Science and Technology grant 3-8705, the
Israel Science Foundation grant 880/12, and the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT). We would like to
thank our reviewers for their insightful comments.
95
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of the
System Demonstrations of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 79?84.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, volume 7, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1?220.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1?
38.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the Web. Communications of the ACM,
51(12):68?74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535?1545, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241?247, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet Physics Doklady, volume 10, page 707.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768?774,
Montreal, Quebec, Canada, August. Association for
Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq Joty. 2013. Towards topic labeling
with phrase entailment and aggregation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
179?189, Atlanta, Georgia, June. Association for
Computational Linguistics.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1331?1340, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics (ACL-COLING 2006), pages
801?808.
Julie Weeds and David Weir. 2003. A general
framework for distributional similarity. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88.
96
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 868?877, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
97
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 181?190,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Modeling of Joint-context in Distributional Similarity
Oren Melamud
?
, Ido Dagan
?
, Jacob Goldberger
?
, Idan Szpektor
?
, Deniz Yuret
?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
? Koc? University
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com, dyuret@ku.edu.tr
Abstract
Most traditional distributional similarity
models fail to capture syntagmatic patterns
that group together multiple word features
within the same joint context. In this work
we introduce a novel generic distributional
similarity scheme under which the power
of probabilistic models can be leveraged
to effectively model joint contexts. Based
on this scheme, we implement a concrete
model which utilizes probabilistic n-gram
language models. Our evaluations sug-
gest that this model is particularly well-
suited for measuring similarity for verbs,
which are known to exhibit richer syntag-
matic patterns, while maintaining compa-
rable or better performance with respect
to competitive baselines for nouns. Fol-
lowing this, we propose our scheme as a
framework for future semantic similarity
models leveraging the substantial body of
work that exists in probabilistic language
modeling.
1 Introduction
The Distributional Hypothesis is commonly
phrased as ?words which are similar in meaning
occur in similar contexts? (Rubenstein and Good-
enough, 1965). Distributional similarity models
following this hypothesis vary in two major as-
pects, namely the representation of the context and
the respective computational model. Probably the
most prominent class of distributional similarity
models represents context as a vector of word fea-
tures and computes similarity using feature vector
arithmetics (Lund and Burgess, 1996; Turney et
al., 2010). To construct the feature vectors, the
context of each target word token
1
, which is com-
monly a word window around it, is first broken
1
We use word type to denote an entry in the vocabulary,
and word token for a particular occurrence of a word type.
into a set of individual independent words. Then
the weights of the entries in the word feature vec-
tor capture the degree of association between the
target word type and each of the individual word
features, independently of one another.
Despite its popularity, it was suggested that
the word feature vector approach misses valu-
able information, which is embedded in the co-
location and inter-relations of words (e.g. word
order) within the same context (Ruiz-Casado et al.,
2005). Following this motivation, Ruiz-Casado
et al. (2005) proposed an alternative composite-
feature model, later adopted in (Agirre et al.,
2009). This model adopts a richer context repre-
sentation by considering entire word window con-
texts as features, while keeping the same compu-
tational vector-based model. Although showing
interesting potential, this approach suffers from a
very high-dimensional feature space resulting in
data sparseness problems. Therefore, it requires
exceptionally large learning corpora to consider
large windows effectively.
A parallel line of work adopted richer context
representations as well, with a different compu-
tational model. These works utilized neural net-
works to learn low dimensional continuous vector
representations for word types, which were found
useful for measuring semantic similarity (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
These vectors are trained by optimizing the pre-
diction of target words given their observed con-
texts (or variants of this objective). Most of these
models consider each observed context as a joint
set of context words within a word window.
In this work we follow the motivation in the pre-
vious works above to exploit richer joint-context
representations for modeling distributional simi-
larity. Under this approach the set of features in
the context of each target word token is consid-
ered to jointly reflect on the meaning of the target
word type. To further facilitate this type of mod-
181
eling we propose a novel probabilistic computa-
tional scheme for distributional similarity, which
leverages the power of probabilistic models and
addresses the data sparseness challenge associated
with large joint-contexts. Our scheme is based on
the following probabilistic corollary to the distri-
butional hypothesis:
(1)?words are similar in meaning if
they are likely to occur in the same contexts?
To realize this corollary, our distributional sim-
ilarity scheme assigns high similarity scores to
word pairs a and b, for which a is likely in the con-
texts that are observed for b and vice versa. The
scheme is generic in the sense that various under-
lying probabilistic models can be used to provide
the estimates for the likelihood of a target word
given a context. This allows concrete semantic
similarity models based on this scheme to lever-
age the capabilities of probabilistic models, such
as established language models, which typically
address the modeling of joint-contexts.
We hypothesize that an underlying model that
could capture syntagmatic patterns in large word
contexts, yet is flexible enough to deal with data
sparseness, is desired. It is generally accepted
that the semantics of verbs in particular are cor-
related with their syntagmatic properties (Levin,
1993; Hanks, 2013). This provides grounds to ex-
pect that such model has the potential to excel for
verbs. To capture syntagmatic patterns, we choose
in this work standard n-gram language models as
the basis for a concrete model implementing our
scheme. This choice is inspired by recent work on
learning syntactic categories (Yatbaz et al., 2012),
which successfully utilized such language mod-
els to represent word window contexts of target
words. However, we note that other richer types
of language models, such as class-based (Brown
et al., 1992) or hybrid (Tan et al., 2012), can be
seamlessly integrated into our scheme.
Our evaluations suggest that our model is in-
deed particularly advantageous for measuring se-
mantic similarity for verbs, while maintaining
comparable or better performance with respect to
competitive baselines for nouns.
2 Background
In this section we provide additional details re-
garding previous works that we later use as base-
lines in our evaluations.
To implement the composite-feature approach,
Ruiz-Casado et al. (2005) used a Web search en-
gine to compare entire window contexts of target
word types. For example, a single feature that
could be retrieved this way for the target word like
is ?Children cookies and milk?. They showed
good results on detecting synonyms in the 80
multiple-choice questions TOEFL test. Agirre et
al. (2009) constructed composite-feature vectors
using an exceptionally large 1.6 Teraword learn-
ing corpus. They found that this approach out-
performs the traditional independent feature vec-
tor approach on a subset of the WordSim353 test-
set (Finkelstein et al., 2001), which is designed to
test the more restricted relation of semantic simi-
larity (to be distinguished from looser semantic re-
latedness). We are not aware of additional works
following this approach, of using entire word win-
dows as features.
Neural networks have been used to train lan-
guage models that are based on low dimensional
continuous vector representations for word types,
also called word embeddings (Bengio et al., 2003;
Mikolov et al., 2010). Although originally de-
signed to improve language models, later works
have shown that such word embeddings are useful
in various other NLP tasks, including measuring
semantic similarity with vector arithmetics (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
Specifically, the recent work by Mikolov et al.
(2013) introduced the CBOW and Skip-gram mod-
els, achieving state-of-the-art results in detecting
semantic analogies. The CBOW model is trained
to predict a target word given the set of context
words in a word window around it, where this
context is considered jointly as a bag-of-words.
The Skip-gram model is trained to predict each of
the context words independently given the target
word.
3 Probabilistic Distributional Similarity
3.1 Motivation
In this section we briefly demonstrate the bene-
fits of considering joint-contexts of words. As an
illustrative example, we note that the target words
like and surround may share many individual word
features such as ?school? and ?campus? in the sen-
tences ?Mary?s son likes the school campus? and
?The forest surrounds the school campus?. This
potentially implies that individual features may
not be sufficient to accurately reflect the difference
182
between such words. Alternatively, we could use
the following composite features to model the con-
text of these words, ?Mary?s son the school
campus? and ?The forest the school campus?.
This would discriminate better between like and
surround. However, in this case sentences such as
?Mary?s son likes the school campus? and ?John?s
son loves the school campus? will not provide any
evidence to the similarity between like and love,
since ?Mary?s son the school campus? is a dif-
ferent feature than ?John?s son the school cam-
pus?.
In the remainder of this section we propose
a modeling scheme and then a concrete model,
which can predict that like and love are likely to
occur in each other?s joint-contexts, whereas like
and surround are not, and then assign similarity
scores accordingly.
3.2 The probabilistic similarity scheme
We now present a computational scheme that re-
alizes our proposed corollary (1) to the distribu-
tional hypothesis and facilitates robust probabilis-
tic modeling of joint contexts. First, we slightly
rephrase this corollary as follows: ?words a and
b are similar in meaning if word b is likely in
the contexts of a and vice versa?. We denote the
probability of an occurrence of a target word b
given a joint-context c by p(b|c). For example,
p(love|?Mary?s son the school campus?) is the
probability of the word love to be the filler of the
?place-holder? in the given joint-context ?Mary?s
son the school campus?. Similarly, we denote
p(c|a) as the probability of a joint-context c given
a word a, which fills its place-holder. We now
propose p
sim
(b|a) to reflect how likely b is in the
joint-contexts of a. We define this measure as:
(2)p
sim
(b|a) =
?
c
p(c|a) ? p(b|c)
where c goes over all possible joint-contexts in the
language.
To implement this measure we need to find
an efficient estimate for p
sim
(b|a). The most
straight forward strategy is to compute sim-
ple corpus count ratio estimates for p(b|c) and
p(c|a), denoted p
#
(b|c) =
count(b,c)
count(?,c)
and
p
#
(c|a) =
count(a,c)
count(a,?)
. However, when consid-
ering large joint-contexts for c, this approach be-
comes similar to the composite-feature approach
since it is based on co-occurrence counts of tar-
get words with large joint-contexts. Therefore, we
expect in this case to encounter the data sparse-
ness problems mentioned in Section 1, where se-
mantically similar word type pairs that share only
few or no identical joint-contexts yield very low
p
sim
(b|a) estimates.
To address the data sparseness challenge and
adopt more advanced context modeling, we aim to
use a more robust underlying probabilistic model
? for our scheme and denote the probabilities es-
timated by this model by p
?
(b|c) and p
?
(c|a). We
note that contrary to the count ratio model, given a
robust model ?, such as a language model, p
?
(b|c)
and p
?
(c|a) can be positive even if the target words
b and a were not observed with the joint-context c
in the learning corpus.
While using p
?
(b|c) and p
?
(c|a) to estimate the
value of p
sim
(b|a) addresses the sparseness chal-
lenge, it introduces a computational challenge.
This is because estimating p
sim
(b|a) would re-
quire computing the sum over all of the joint-
contexts in the learning corpus regardless of
whether they were actually observed with either
word type a or b. For that reason we choose a
middle ground approach, estimating p(b|c) with
?, while using a count ratio estimate for p(c|a),
as follows. We denote the collection of all joint-
contexts observed for the target word a in the
learning corpus by C
a
, where |C
a
|= count(a, ?).
For example, C
like
= {c
1
=?Mary?s son the
school campus?, c
2
=?John?s daughter to read
poetry?,...}. We note that this collection is a multi-
set, where the same joint-context can appear more
than once.
We now approximate p
sim
(b|a) from Equation
(2) as follows:
(3)
p?
sim
?
(b|a) =
?
c
p
#
(c|a) ? p
?
(b|c) =
1
|C
a
|
?
?
c?C
a
p
?
(b|c)
We note that this formulation still addresses
sparseness of data by using a robust model, such as
a language model, to estimate p
?
(b|c). At the same
time it requires our model to sum only over the
joint-contexts in the collection C
a
, since contexts
not observed for a yield p
#
(c|a) = 0. Even so,
since the size of these context collections grows
linearly with the corpus size, considering all ob-
served contexts may still present a scalability chal-
lenge. Nevertheless, we expect our approximation
p?
sim
?
(b|a) to converge with a reasonable sample
183
size from a?s joint-contexts. Therefore, in order
to bound computational complexity, we limit the
size of the context collections used to train our
model to a maximum of N by randomly sampling
N entries from larger collections. In all our ex-
periments we use N = 10, 000. Higher values
of N yielded negligible performances differences.
Overall we see that our model estimates p?
sim
?
(b|a)
as the average probability predicted for b in (a
large sample of) the contexts observed for a.
Finally, we define our similarity measure for tar-
get word types a and b:
(4)sim
?
(a, b) =
?
p?
sim
?
(b|a) ? p?
sim
?
(a|b)
As intended, this similarity measure promotes
word pairs in which both b is likely in the con-
texts of a and vice versa. Next, we describe a
model which implements this scheme with an n-
gram language model as a concrete choice for ?.
3.3 Probabilistic similarity using language
models
In this work we focus on the word window context
representation, which is the most common. We
define a word window of order k around a target
word as a window with up to k words to each side
of the target word, not crossing sentence bound-
aries. The word window does not include the tar-
get word itself, but rather a ?place-holder? for it.
Since word windows are sequences of words,
probabilistic language models are a natural choice
of a model ? for estimating p
?
(b|c). Language
models assign likelihood estimates to sequences
of words using approximation strategies. In
this work we choose n-gram language models,
aiming to capture syntagmatic properties of the
word contexts, which are sensitive to word or-
der. To approximate the probability of long se-
quences of words, n-gram language models com-
pute the product of the estimated probability of
each word in the sequence conditioned on at most
the n ? 1 words preceding it. Furthermore, they
use ?discounting? methods to improve the esti-
mates of conditional probabilities when learning
data is sparse. Specifically, in this work we use
the Kneser-Ney n-gram model (Kneser and Ney,
1995).
We compute p
?
(b|c) as follows:
(5)p
?
(b|c) =
p
?
(b, c)
p
?
(c)
where p
?
(b, c) is the probability of the word se-
quence comprising the word window c, in which
the word b fills the place-holder. For instance, for
c = ?I drive my to work every? and b = car,
p
?
(b, c) is the estimated language model probabil-
ity of ?I drive my car to work every?. p
?
(c) is the
marginal probability of p
?
(?, c) over all possible
words in the vocabulary.
2
4 Experimental Settings
Although sometimes used interchangeably, it is
common to distinguish between semantic simi-
larity and semantic relatedness (Budanitsky and
Hirst, 2001; Agirre et al., 2009). Semantic simi-
larity is used to describe ?likeness? relations, such
as the relations between synonyms, hypernym-
hyponyms, and co-hyponyms. Semantic relat-
edness refers to a broader range of relations in-
cluding also meronymy and various other asso-
ciative relations as in ?pencil-paper? or ?penguin-
Antarctica?. In this work we focus on semantic
similarity and evaluate all compared methods on
several semantic similarity tasks.
Following previous works (Lin, 1998; Riedl and
Biemann, 2013) we use Wordnet to construct large
scale gold standards for semantic similarity evalu-
ations. We perform the evaluations separately for
nouns and verbs to test our hypothesis that our
model is particularly well-suited for verbs. To fur-
ther evaluate our results on verbs we use the verb
similarity test-set released by (Yang and Powers,
2006), which contains pairs of verbs associated
with semantic similarity scores based on human
judgements.
4.1 Compared methods
We compare our model with a traditional fea-
ture vector model, the composite-feature model
(Agirre et al., 2009), and the recent state-of-the-art
word embedding models, CBOW and Skip-gram
(Mikolov et al., 2013), all trained on the same
learning corpus and evaluated on equal grounds.
We denote the traditional feature vector baseline
by IFV
W?k
, where IFV stands for ?Independent-
Feature Vector? and k is the order of the con-
text word window considered. Similarly, we
2
Computing p
?
(c) by summing over all possible place-
holder filler words, as we did in this work, is computation-
ally intensive. However, this can be done more efficiently
by implementing customized versions of (at least some) n-
gram language models with little computational overhead,
e.g. by counting the learning corpus occurrences of n-gram
templates, in which one of the elements matches any word.
184
denote the composite-feature vector baseline by
CFV
W?k
, where CFV stands for ?Composite-
Feature Vector?. This baseline constructs
traditional-like feature vectors, but considers en-
tire word windows around target word tokens as
single features. In both of these baselines we use
Cosine as the vector similarity measure, and posi-
tive pointwise mutual information (PPMI) for the
feature vector weights. PPMI is a well-known
variant of pointwise mutual information (Church
and Hanks, 1990), and the combination of Cosine
with PPMI was shown to perform particularly well
in (Bullinaria and Levy, 2007).
We denote Mikolov?s CBOW and Skip-gram
baseline models by CBOW
W?k
and SKIP
W?k
respectively, where k denotes again the order of
the window used to train these models. We used
Mikolov?s word2vec utility
3
with standard param-
eters (600 dimensions, negative sampling 15) to
learn the word embeddings, and Cosine as the vec-
tor similarity measure between them.
As the underlying probabilistic language model
for our method we use the Berkeley implementa-
tion
4
(Pauls and Klein, 2011) of the Kneser-Ney
n-gram model with the default discount parame-
ters. We denote our model PDS
W?k
, where PDS
stands for ?Probabilistic Distributional Similar-
ity?, and k is the order of the context word win-
dow. In order to avoid giving our model an un-
fair advantage of tuning the order of the language
model n as an additional parameter, we use a fixed
n = k + 1. This means that the conditional prob-
abilities that our n-gram model learns consider a
scope of up to half the size of the window, which
is the distance in words between the target word
and either end of the window. We note that this
is the smallest reasonable value for n, as smaller
values effectively mean that there will be context
words within the window that are more than n
words away from the target word, and therefore
will not be considered by our model.
As learning corpus we used the first CD of
the freely available Reuters RCV1 dataset (Rose
et al., 2002). This learning corpus contains ap-
proximately 100M words, which is comparable in
size to the British National Corpus (BNC) (As-
ton, 1997). We first applied part-of-speech tag-
ging and lemmatization to all words. Then we
represented each word w in the corpus as the pair
3
http://code.google.com/p/word2vec
4
http://code.google.com/p/berkeleylm/
[pos(w), lemma(w)], where pos(w) is a coarse-
grained part-of-speech category and lemma(w) is
the lemmatized form of w. Finally, we converted
every pair [pos(w), lemma(w)] that occurs less
than 100 times in the learning corpus to the pair
[pos(w), ? ], which represents all rare words of the
same part-of-speech tag. Ignoring rare words is a
common practice used in order to clean up the cor-
pus and reduce the vocabulary size (Gorman and
Curran, 2006; Collobert and Weston, 2008).
The above procedure resulted in a word vocabu-
lary of 27K words. From this vocabulary we con-
structed a target verb set with over 2.5K verbs by
selecting all verbs that exist in Wordnet (Fellbaum,
2010). We repeated this procedure to create a tar-
get noun set with over 9K nouns. We used our
learning corpus for all compared methods and had
them assign a semantic similarity score for every
pair of verbs and every pair of nouns in these tar-
get sets. These scores were later used in all of our
evaluations.
4.2 Wordnet evaluation
There is a shortage of large scale test-sets for se-
mantic similarity. Popular test-sets such as Word-
Sim353 and the TOEFL synonyms test contain
only 353 and 80 test items respectively, and there-
fore make it difficult to obtain statistically signif-
icant results. To automatically construct larger-
scale test-sets for semantic similarity, we sampled
large target word subsets from our corpus and used
Wordnet as a gold standard for their semantically
similar words, following related previous evalua-
tions (Lin, 1998; Riedl and Biemann, 2013). We
constructed two test-sets for our primary evalua-
tion, one for verb similarity and another for noun
similarity.
To perform the verb similarity evaluation, we
randomly sampled 1,000 verbs from the target
verb set, where the probability of each verb to be
sampled is set to be proportional to its frequency in
the learning corpus. Next, for each sampled verb
a we constructed a Wordnet-based gold standard
set of semantically similar words. In this set each
verb a
?
is annotated as a ?synonym? of a if at least
one of the senses of a
?
is a synonym of any of the
senses of a. In addition, each verb a
?
is annotated
as a ?semantic neighbor? of a if at least one of the
senses of a
?
is a synonym, co-hyponym, or a di-
rect hypernym/hyponym of any of the senses of a.
We note that by definition all verbs annotated as
185
synonyms of a are annotated as semantic neigh-
bors as well. Next, per each verb a and an evalu-
ated method, we generated a ranked list of all other
verbs, which was induced according to the similar-
ity scores of this method.
Finally, we evaluated the compared methods
on two tasks, ?synonym detection? and ?seman-
tic neighbor detection?. In the synonym detection
task we evaluated the methods? ability to retrieve
as much verbs annotated in our gold standard as
?synonyms?, in the top-n entries of their ranked
lists. Similarly, we evaluated all methods on the
?semantic neighbors? task. The synonym detec-
tion task is designed to evaluate the ability of the
compared methods to identify a more restrictive
interpretation of semantic similarity, while the se-
mantic neighbor detection task does the same for
a somewhat broader interpretation.
We repeated the above procedure for sam-
pling 1,000 target nouns, constructing the noun
Wordnet-based gold standards and evaluating on
the two semantic similarity tasks.
4.3 VerbSim evaluation
The publicly available VerbSim test-set contains
130 verb pairs, each annotated with an average of
6 human judgements of semantic similarity (Yang
and Powers, 2006). We extracted a 107 pairs sub-
set of this dataset for which all verbs are in our
learning corpus. We followed works such as (Yang
and Powers, 2007; Agirre et al., 2009) and com-
pared the Spearman correlations between the verb-
pair similarity scores assigned by the compared
methods and the manually annotated scores in this
dataset.
5 Results
For each method and verb a in our 1,000 tested
verbs, we used the Wordnet gold standard to com-
pute the precision at top-1, top-5 and top-10 of the
ranked list generated by this method for a. We
then computed mean precision values averaged
over all verbs for each of the compared methods,
denoted as P@1, P@5 and P@10. The detailed
report of P@10 results is omitted for brevity, as
they behave very similarly to P@5. We varied the
context window order used by all methods to test
its effect on the results. We measured the same
metrics for nouns.
The results of our Wordnet-based 1,000 verbs
evaluation are presented in the upper part of Fig-
ure 1. The results show significant improvement
of our method over all baselines, with a margin
between 2 to 3 points on the synonyms detection
task and 5 to 7 points on the semantic neighbors
detection task. Our best performing configura-
tions are PDS
W?3
and PDS
W?4
, outperform-
ing all other baselines on both tasks and in all pre-
cision categories. This difference is statistically
significant at p < 0.001 using a paired t-test in all
cases except for the P@1 in the synonyms detec-
tion task. Within the baselines, the composite fea-
ture vector (CFV) performs somewhat better than
the independent feature vector (IFV) baseline, and
both methods perform best around window order
of two, with gradual decline for larger windows.
The word embedding baselines, CBOW and SKIP,
perform comparably to the feature vector base-
lines and to one another, with best performance
achieved around window order of four.
When gradually increasing the context window
order within the range of up to 4 words, our PDS
model shows improvement. This is in contrast to
the feature vector baselines, whose performance
declines for context window orders larger than 2.
This suggests that our approach is able to take ad-
vantage of larger contexts in comparison to stan-
dard feature vector models. The decline in perfor-
mance for the independent feature vector baseline
(IFV) may be related to the fact that independent
features farther away from the target word are gen-
erally more loosely related to it. This seems con-
sistent with previous works, where narrow win-
dows of the order of two words performed well
(Bullinaria and Levy, 2007; Agirre et al., 2009;
Bruni et al., 2012) and in particular so when eval-
uating semantic similarity rather than relatedness.
On the other hand, the decline in performance for
the composite feature vector baseline (CFV) may
be attributed to the data sparseness phenomenon
associated with larger windows. The performance
of the word embedding baselines (CBOW and
SKIP) starts declining very mildly only for win-
dow orders larger than 4. This might be attributed
to the fact that these models assign lower weights
to context words the farther away they are from the
center of the window.
The results of our Wordnet-based 1,000 nouns
evaluation are presented in the lower part of Fig-
ure 1. These results are partly consistent with the
results achieved for verbs, but with a couple of
notable differences. First, though our model still
186
Figure 1: Mean precision scores as a function of window order, obtained against the Wordnet-based gold
standard, on both the verb and noun test-sets with both the synonyms and semantic neighbor detection
tasks. ?P@n? stands for precision in the top-n words of the ranked lists. Note that the Y-axis scale varies
between graphs.
outperforms or performs comparably to all other
baselines, in this case the advantage of our model
over the feature vector baselines is much more
moderate and not statistically significant. Second,
the word embedding baselines generally perform
worst (with CBOW performing a little better than
SKIP), and our model outperforms them in both
P@5 and P@10 with a margin of around 2 points
for the synonyms detection task and 3-4 points for
the neighbor detection task, with statistical signif-
icance at p < 0.001.
Next, to reconfirm the particular applicability
of our model to verb similarity as apparent from
the Wordnet evaluation, we performed the Verb-
Sim evaluation and present the results in Table 1.
We compared the Spearman correlation obtained
for the top-performing window order of each of
the evaluated methods in the Wordnet verbs eval-
uation. We present two sets of results. The ?all
scores? results follow the standard evaluation pro-
cedure, considering all similarity scores produced
by each method. In the ?top-100 scores? results,
for each method we converted to zero the scores
that it assigned to word pairs, where neither of
the words is in the top-100 most similar words
of the other. Then we performed the evaluation
with these revised scores. This procedure focuses
on evaluating the quality of the methods? top-
100 ranked word lists. The results show that our
method outperforms all baselines by a nice mar-
187
Method All scores top-100 scores
PDS W-4 0.616 0.625
CFV W-2 0.477 0.497
IFV W-2 0.467 0.546
SKIP W-4 0.469 0.512
CBOW W-5 0.528 0.469
Table 1: Spearman correlation values obtained for
the VerbSim evaluation. Each method was evalu-
ated with the optimal window order found in the
Wordnet verbs evaluation.
gin of more than 8 points with the score of 0.616
and 0.625 for the ?all scores? and ?top-100 scores?
evaluations respectively. Though not statistically
significant, due to the small test-set size, these re-
sults support the ones from the Wordnet evalu-
ation, suggesting that our model performs better
than the baselines on measuring verb similarity.
In summary, our results suggest that in lack of a
robust context modeling scheme it is hard for dis-
tributional similarity models to effectively lever-
age larger word window contexts for measuring
semantic similarity. It appears that this is some-
what less of a concern when it comes to noun sim-
ilarity, as the simple feature vector models reach
near-optimal performance with small word win-
dows of order 2, but it is an important factor for
verb similarity. In his recent book, Hanks (2013)
claims that contrary to nouns, computational mod-
els that are to capture the meanings of verbs must
consider their syntagmatic patterns in text. Our
particularly good results on verb similarity sug-
gest that our modeling approach is able to cap-
ture such information in larger context windows.
We further conjecture that the reason the word em-
bedding baselines did not do as well as our model
on verb similarity might be due to their particular
choice of joint-context formulation, which is not
sensitive to word order. However, these conjec-
tures should be further validated with additional
evaluations in future work.
6 Future Directions
In this paper we investigated the potential for im-
proving distributional similarity models by model-
ing jointly the occurrence of several features under
the same context. We evaluated several previous
works with different context modeling approaches
and suggest that the type of the underlying con-
text modeling may have significant effect on the
performance of the semantic model. Further-
more, we introduced a generic probabilistic distri-
butional similarity approach, which can leverage
the power of established probabilistic language
models to effectively model joint-contexts for the
purpose of measuring semantic similarity. Our
concrete model utilizing n-gram language models
outperforms several competitive baselines on se-
mantic similarity tasks, and appears to be partic-
ularly well-suited for verbs. In the remainder of
this section we describe some potential future di-
rections that can be pursued.
First, the performance of our generic scheme
is largely inherited from the nature of its under-
lying language model. Therefore, we see much
potential in exploring the use of other types of
language models, such as class-based (Brown et
al., 1992), syntax-based (Pauls and Klein, 2012)
or hybrid (Tan et al., 2012). Furthermore, a sim-
ilar approach to ours could be attempted in word
embedding models. For instance, our syntagmatic
joint-context modeling approach could be investi-
gated by word embedding models to generate bet-
ter embeddings for verbs.
Another direction relates to the well known ten-
dency of many words, and particularly verbs, to
assume different meanings (or senses) under dif-
ferent contexts. To address this phenomenon con-
text sensitive similarity and inference models have
been proposed (Dinu and Lapata, 2010; Melamud
et al., 2013). Similarly to many semantic similar-
ity models, our current model aggregates informa-
tion from all observed contexts of a target word
type regardless of its different senses. However,
we believe that our approach is well suited to ad-
dress context sensitive similarity with proper en-
hancements, as it considers joint-contexts that can
more accurately disambiguate the meaning of tar-
get words. As an example, it is possible to con-
sider the likelihood of word b to occur in a subset
of the contexts observed for word a, which is bi-
ased towards a particular sense of a.
Finally, we note that our model is not a classic
vector space model and therefore common vec-
tor composition approaches (Mitchell and Lap-
ata, 2008) cannot be directly applied to it. In-
stead, other methods, such as similarity of com-
positions (Turney, 2012), should be investigated to
extend our approach for measuring similarity be-
tween phrases.
188
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT) and the Scien-
tific and Technical Research Council of Turkey
(T
?
UB
?
ITAK, Grant Number 112E277).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL. Association for Computational Lin-
guistics.
Guy Aston. 1997. The BNC Handbook Exploring the
British National Corpus with SARA Guy Aston and
Lou Burnard.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.
Alexander Budanitsky and Graeme Hirst. 2001.
Semantic distance in wordnet: An experimental,
application-oriented evaluation of five measures. In
Workshop on WordNet and Other Lexical Resources.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP.
Christiane Fellbaum. 2010. WordNet. Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web. ACM.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Pro-
ceedings of ACL.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing. IEEE.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago press.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N -Gram Language Models. In Proceedings of ACL.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL.
Martin Riedl and Chris Biemann. 2013. Scaling to
large?3 data: An efficient and effective method to
compute distributional thesauri. In Proceedings of
EMNLP.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1-from Yester-
day?s News to Tomorrow?s Language Resources. In
LREC.
189
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Using context-window overlapping
in synonym discovery and ontology extension. Pro-
ceedings of RANLP.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic,
and lexical language model. Computational Lin-
guistics, 38(3):631?671.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of artificial intelligence research.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Researc,
44(1):533?585, May.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In the 3rd
International WordNet Conference (GWC-06).
Dongqiang Yang and David M. W. Powers. 2007.
An empirical investigation into grammatically con-
strained contexts in predicting distributional similar-
ity. In Australasian Language Technology Workshop
2007, pages 117?124.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP.
190
