Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 660?666, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
LASIGE: using Conditional Random Fields and ChEBI ontology
Tiago Grego
Dep. de Informa?tica
Faculdade de Cie?ncias
Universidade de Lisboa
Portugal
tgrego@fc.ul.pt
Francisco Pinto
Dep. de Qu??mica e Bioqu??mica
Faculdade de Cie?ncias
Universidade de Lisboa
Portugal
frpinto@fc.ul.pt
Francisco M. Couto
Dep. de Informa?tica
Faculdade de Cie?ncias
Universidade de Lisboa
Portugal
fcouto@di.fc.ul.pt
Abstract
For participating in the SemEval 2013 chal-
lenge of recognition and classification of
drug names, we adapted our chemical en-
tity recognition approach consisting in Condi-
tional Random Fields for recognizing chemi-
cal terms and lexical similarity for entity res-
olution to the ChEBI ontology. We obtained
promising results, with a best F-measure of
0.81 for the partial matching task when us-
ing post-processing. Using only Conditional
Random Fields the results are slightly lower,
achieving still a good result in terms of F-
measure. Using the ChEBI ontology allowed
a significant improvement in precision (best
precision of 0.93 in partial matching task),
which indicates that taking advantage of an
ontology can be extremely useful for enhanc-
ing chemical entity recognition.
1 Introduction
Most chemical named entity recognition systems
can be classified in two approaches: dictionary
based and machine learning based approaches. Dic-
tionary based approaches are usually easier to im-
plement and maintain, but require a reference chem-
ical term dictionary and are dependent on its com-
pleteness and quality. The availability of public
chemical databases has been an issue until recently,
when several publicly available databases such as
PubChem (Wang et al, 2009), DrugBank (Wishart
et al, 2006) and ChEBI (Degtyarenko et al, 2007)
were released. An example of a popular system that
uses this approach is Whatizit (Rebholz-Schuhmann
et al, 2008). Machine learning based approaches
are not limited to a terminology and are thus better
suited for finding novel chemical terms that are yet
to be inserted in reference databases. However this
approach requires training data for a classifier to be
able to successfully learn and perform the chemi-
cal entity recognition task. Some methods combine
both approaches and thus are hybrid systems that
aim to take the best out of both approaches (Jessop
et al, 2011; Rockta?schel et al, 2012).
An annotated corpus of patent documents was re-
leased by ChEBI, and using such corpus as train-
ing data we developed an chemical entity recogni-
tion system (Grego et al, 2009) that uses a ma-
chine learning approach based on Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). We fur-
thermore expanded our method to allow resolution
of recognized entities to the ChEBI ontology (Grego
et al, 2012).
This paper describes how our system (Grego et
al., 2012) was adapted to perform the task of recog-
nition and classification of drug names, and presents
the results obtained in the task 9.1 of the 7th Interna-
tional Workshop on Semantic Evaluation (SemEval
2013).
2 Task and Dataset
The Task 9 of SemEval 2013 involved two sub-tasks:
(9.1) recognition and classification of drug names,
and (9.2) extraction of drug-drug interactions from
Biomedical Texts (SemEval, 2013). The recognition
and classification of drug names (Task 9.1) com-
prises two steps. First is chemical named entity
recognition, that consists in finding in a sentence
the offsets for the start and end of a chemical entity.
660
An exact match is achieved by correctly identifying
both the start and end offset, as curators manually
provided them. If there is a mismatch in the offsets
but there is some overlap with a manual annotation,
then it is considered a partial match, otherwise it is
a recognition error.
The second step consists in classifying each rec-
ognized entity in one of four possible entity types:
i) Drug is any pharmaceutical product approved for
human use; ii) Brand is a drug that was first devel-
oped by a pharmaceutical company; iii) Group refers
to a class or group of drugs; iv) Drug n is an ac-
tive substance that has not been approved for human
use. Thus, the evaluation takes into account not only
entity recognition, but also the assigned type. Type
matching assessment considers the entity type evalu-
ation from partial matching entity recognition, while
strict matching considers the entity type evaluation
from exact matching.
For training, the DDI corpus dataset was provided
(Segura-Bedmar et al, 2006). This dataset contains
two sub-datasets. One that consists of MedLine ab-
stracts, and other that contains DrugBank abstracts.
An unannotated test dataset was provided for testing
and evaluating the systems.
3 CRF entity recognition
Our method uses CRFs for building probabilis-
tic models based on training datasets. We used
the MALLET (McCallum, 2002) implementation of
CRFs. MALLET is a Java-based package for sta-
tistical natural language processing, document clas-
sification, clustering, topic modeling, information
extraction, and other machine learning applications
to text, which includes an implementation of linear
chain CRFs.
A required first step in our method in the tok-
enization of the input text. For this task we have
used a specifically adapted tokenizer for chemical
text adapted from an open source project (Corbett et
al., 2007).
Each token is then represented as a set of features.
We kept using a set of features derived in our previ-
ous work (Grego et al, 2009), which includes for
each token:
Stem: The stem of the token.
Prefix: The first three characters of the token.
Suffix: The last three characters of the token.
Number: Boolean that indicates if the token con-
tains digits.
In addition to the set of features, each token is also
given a label in accordance to the training data:
NO: A non-chemical token.
NE: A chemical entity represented by a single to-
ken.
S-NE: The first token of a multi-token chemical en-
tity.
M-NE: A middle token of a multi-token chemi-
cal entity (only exists for entities composed by
three or more tokens).
E-NE: The last token of a multi-token chemical en-
tity.
The task of entity recognition will be the assign-
ment of such labels to new, unannotated text, based
on a model. The assigned label allows for named
entities to be recognized and offsets provided.
For creating a model, it is required as input a set
of annotated documents. Our method was initially
developed using an annotated patent document cor-
pus released to the public by the ChEBI team. This
corpus can be found at 1, and we decided to keep us-
ing it as training data for a model. Together with this
corpus, the DDI corpus training dataset provided for
the task was used. The model produced by using
this combination of training data, that we called All
model, will be suited for general purpose chemical
entity recognition.
We then prepared four datasets based on the DDI
corpus dataset but containing only one type of anno-
tated entities each. With that training data we pre-
pared four more models, each trained only with one
kind on entity type. Thus we have in total prepared
five models:
All: A model trained with all entity types of the DDI
corpus dataset, and the ChEBI released patent
dataset.
1http://chebi.cvs.sourceforge.net/
viewvc/chebi/chapati/patentsGoldStandard/
661
Drug: A model trained only with the entities of type
drug in the DDI corpus dataset.
Brand: A model trained only with the entities of
type brand in the DDI corpus dataset.
Group: A model trained only with the entities of
type group in the DDI corpus dataset.
Drug n: A model trained only with the entities of
type drug n in the DDI corpus dataset.
Using the type specific models it is possible to
annotate text with only one entity type. Thus our
method now has the capability of entity type classifi-
cation in addition to named entity recognition, using
these type specific models.
4 ChEBI resolution
After having recognized the named chemical enti-
ties, our method tries to perform their resolution to
the ChEBI ontology. ChEBI (Chemical Entities of
Biological Interest) is a freely available dictionary
of small molecular entities. In addition to molecular
entities, ChEBI contains groups (parts of molecular
entities) and classes of entities, allowing for an onto-
logical classification that specifies the relationships
between molecular entities or classes of entities and
their parents and/or children. The ontology structure
provides an integrated overview of the relationships
between chemical entities, both structural and func-
tional.
The resolution method takes as input the string
identified as being a chemical compound name and
returns the most relevant ChEBI identifier along
with a confidence score.
To perform the search for the most likely ChEBI
term for a given entity an adaptation of FiGO, a
lexical similarity method (Couto et al, 2005). Our
adaptation compares the constituent words in the in-
put string with the constituent words of each ChEBI
term, to which different weights have been assigned
according to its frequency in the ontology vocabu-
lary (Grego et al, 2012). A resolution score between
0 and 1 is provided with the mapping, which corre-
sponds to a maximum value in the case of a ChEBI
term that has the exact name as the input string, and
is lower otherwise.
5 Post-processing
To further improve the quality of the annotations
provided by our method, some na??ve rules were cre-
ated and external resources used.
One of the rules implemented is derived from the
resolution process, and corresponds in classifying
an entity as type Group if its ChEBI name is plu-
ral. This is because ChEBI follows the convention
of naming its terms always as a singular name, ex-
cept for terms that represent classes of entities where
a plural name can be used.
We have also used other resources in the post-
processing besides ChEBI, namely a list of brand
names extracted from DrugBank. This list of brand
names was used to check if a given entity was part
of that list, and if it was the entity should be of the
type Brand.
A common English words list was also used as ex-
ternal resource in post-processing. If a recognized
chemical entity was part of this list then it was a
recognition error and should be filtered out and not
be considered a chemical entity.
Some simple rules were also implemented in an
effort to improve the quality of the annotations. For
instance, if the recognized entity was found to be
composed entirely by digits, then it should be fil-
tered out because it is most certainly an annotation
error. Also, if an entity starts or ends with a char-
acter such as ?*?, ?-?, ?.?, ?,? or ???, then those
characters should be removed from the entity and the
offsets corrected accordingly.
With such na??ve but efficient rules it was expected
that the performance of entity recognition would im-
prove. An overview of the system architecture is
provided in Figure 1.
6 Testing runs
Using different combinations of the described meth-
ods, three runs were submitted for evaluation and are
now described.
Run 1: This run uses all of the described methods.
Entity recognition is performed using all mod-
els, and the type classification is performed by
using the type specific models in the following
priority: if an entity was recognized using the
Drug n model, then type is Drug n, else if it
662
Figure 1: Overview of the system architecture. Based on
annotated corpus, CRF models are created and used to
annotate new documents.
was recognized using the Brand model, then
type is Brand, else if it was recognized using
the Group model, then type is Group, else and
finally it is assigned the type Drug. Resolution
to ChEBI is performed and all of the described
post-processing rules applied.
Run 2: In this run only the classifiers are used. This
means that the entity recognition is performed
using all models, and the type classification is
performed by using the type specific models as
described in Run 1. However no extra process-
ing is performed and the results are submitted
as obtained directly from the classifiers.
Run 3: This run performs entity recognition in
a similar way described in run 1, and per-
forms entity recognition to the ChEBI ontol-
ogy. However, only the entities successfully
mapped to ChEBI, with a resolution score of
at least 0.8, are considered. All the other en-
tities are discarded in this phase. After reso-
lution and the filtering of entities according to
the resolution to ChEBI, all the described post-
processing rules are applied in a similar way to
Run 1.
7 Results and Discussion
The official evaluation results are presented in Ta-
ble 1. We can observe that the obtained results are
better for the DrugBank dataset than for the Med-
Line dataset. This may have happened because the
DrugBank dataset is four times larger than the Med-
Line dataset, but also because while the DrugBank
abstracts are quite focused in drug descriptions and
use mostly systematic names, the MedLine ones are
usually more generic and make more extensive use
of trivial drug names. We obtained for the Run 1 a
top F-measure of 0.81 in the full dataset for a par-
tial matching assessment, and that value decreased
to 0.78 when an exact matching assessment is con-
sidered. The values are very close, which means that
our method is being able to efficiently find the cor-
rect offsets of the entities. However the F-measure
decreases to 0.69 for partial matching and 0.66 for
exact matching when the assignment of the entity
type is considered. This means that there is room to
improve in the task of classifying the chemical enti-
ties to the correct entity type.
Run 2 obtained results very similar to Run 1, only
slightly less F-measure. The difference between
those two runs was that Run 2 used only the classi-
fiers, while Run 1 used rules and external resources
in an effort to improve the results. We can thus con-
clude that the classifiers alone produce already good
results and more sophisticated post-processing is re-
quired to obtain significant performance gains. Our
post-processing was very simple as explained ear-
lier, and can only slightly improve the results ob-
tained with the CRF classifiers alone.
Run 3 obtained improved precision in all assess-
ments. In this run only the entities that were success-
fully mapped to ChEBI were considered, and thus
the precision of recognition was the best of our runs.
This is because ChEBI contains high quality, man-
ually validated chemical terms. If a recognized en-
tity can be successfully mapped to this data source,
then there is a good indication that it is, in fact, a
valid chemical entity. However F-measure has de-
creased because of a loss in recall. ChEBI is still a
young project containing slightly over 30,000 chem-
663
Assessment Run
MedLine Dataset DrugBank Dataset Full Dataset
P R F1 P R F1 P R F1
Strict matching
1 0.6 0.54 0.57 0.82 0.72 0.77 0.7 0.62 0.66
2 0.54 0.54 0.54 0.82 0.73 0.77 0.65 0.62 0.64
3 0.66 0.48 0.56 0.83 0.58 0.68 0.73 0.52 0.61
Exact matching
1 0.78 0.7 0.74 0.89 0.78 0.83 0.83 0.74 0.78
2 0.73 0.74 0.73 0.88 0.78 0.83 0.79 0.76 0.77
3 0.82 0.6 0.69 0.91 0.63 0.74 0.86 0.61 0.72
Partial matching
1 0.81 0.73 0.77 0.91 0.8 0.85 0.86 0.76 0.81
2 0.76 0.77 0.76 0.91 0.8 0.85 0.82 0.78 0.8
3 0.86 0.63 0.72 0.93 0.65 0.76 0.89 0.64 0.74
Type matching
1 0.64 0.58 0.61 0.85 0.75 0.8 0.73 0.65 0.69
2 0.57 0.58 0.58 0.85 0.75 0.8 0.69 0.66 0.67
3 0.71 0.52 0.6 0.87 0.61 0.71 0.78 0.56 0.65
Table 1: Results obtained in Task 9.1 for the different assessments. Exact and Partial matching do not consider the
entity type, while Strict and Type matching consider the entity type for Exact and Partial matching entity recognition
respectively.
Entity Type Run
MedLine Dataset DrugBank Dataset Full Dataset
P R F1 P R F1 P R F1
Drug
1 0.58 0.82 0.68 0.85 0.78 0.82 0.69 0.8 0.74
2 0.51 0.82 0.63 0.83 0.81 0.82 0.64 0.82 0.72
3 0.66 0.74 0.7 0.88 0.67 0.76 0.75 0.7 0.73
Brand
1 1 0.5 0.67 0.77 0.45 0.57 0.79 0.46 0.58
2 0.67 0.33 0.44 0.91 0.4 0.55 0.88 0.39 0.54
3 1 0.5 0.67 0.65 0.21 0.31 0.7 0.24 0.35
Group
1 0.7 0.54 0.61 0.82 0.85 0.83 0.76 0.67 0.71
2 0.64 0.56 0.6 0.82 0.83 0.82 0.72 0.67 0.7
3 0.7 0.47 0.56 0.83 0.69 0.76 0.76 0.56 0.65
Drug n
1 0.48 0.11 0.18 0 0 0 0.42 0.11 0.17
2 0.5 0.12 0.2 0 0 0 0.42 0.12 0.18
3 0.48 0.1 0.17 0 0 0 0.41 0.1 0.16
Table 2: Results obtained in Task 9.1 for each entity type. In this evaluation only the entities of a specific type are
considered at a time.
Run
MedLine Dataset DrugBank Dataset Full Dataset
P R F1 P R F1 P R F1
1 0.69 0.50 0.58 0.61 0.52 0.56 0.67 0.51 0.58
2 0.58 0.46 0.51 0.64 0.51 0.57 0.67 0.50 0.57
3 0.71 0.45 0.55 0.59 0.39 0.47 0.66 0.4 0.5
Table 3: Macro-average measures obtained for each run.
664
ical entities, which is still a low amount of entities
when compared with other chemical databases (for
example, PubChem contains more than 10 times that
amount). However ChEBI is growing at a steady
pace and we believe its coverage will keep increas-
ing while maintaining the high quality that allows
for an excellent precision. Thus, as ChEBI evolves,
our approach will mantain the high levels of preci-
sion but with a lower reduction in recall.
ChEBI is not only a chemical dictionary, but
an ontology. This allows for a comparison recog-
nized entities through semantic similarity measures
that can be used to further enhance chemical en-
tity recognition (Ferreira and Couto , 2010; Couto
and Silva , 2011). This comparison can also be ex-
tremely useful in other task such as drug-drug inter-
action extraction. Moreover, even if with a relatively
small ChEBI, it can be possible to increase coverage
by integrating other available resources using Ontol-
ogy Matching techniques (Faria et al, 2012).
In Table 2 we have the official results obtained
for each entity type, and we can observe that our
method is efficient in correctly classifying the Drug
and Group types, where it achieves an F-measure
of 0.74 and 0.71 correspondingly. However our
method has some difficulties in correctly classify-
ing entities of the Brand type, where an F-measure
of 0.58 was obtained. The Drug n entity type has
proven to be a very challenging type to be correctly
classified, and our system failed the correct classi-
fication of this type in most situations. This is pos-
sibly because the percentage of entities of this type
is very limited, and also because the difference be-
tween this type and the Drug type is the fact that
the later has been approved for human use, while
the former has not. The feature set used cannot ef-
ficiently discriminate this information and external
information about drug approval for human usage
must be used for efficient detection of this type.
Overall, Run 1 has obtained the best results. How-
ever, the results from Run 2 have been very similar,
which shows that the classifiers have been success-
ful and the post-processing of Run 1 has been mini-
mal. Run 3 was designed for high precision, because
only the entities successfully mapped to the ChEBI
ontology were considered. It does improve the ob-
tained precision, but suffers a drop in recall. Table 3
presents the macro-average measures obtained for
each run.
8 Conclusions
This paper presents our participation in the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013) using a CRF-based chemical entity
recognition method and a lexical similarity based
resolution method. We prepared type-specific CRF
models to allow both recognition and type classifi-
cation of the chemical entities. Mapping of the en-
tities to the ChEBI ontology was performed using
a lexical similarity based method, and several post-
processing rules using external resources where im-
plemented.
We submitted different runs on annotated test data
using different combination of such methods, and
obtained a best precision of 0.89 and a best F-
measure of 0.81 in the entity recognition task. For
the task of entity recognitions and classification we
have obtained a best precision of 0.78 and a best F-
measure of 0.69. We concluded that the classifiers
provide already good results by their own, that can
be slightly improved by using some na??ve external
resources and rules.
However, using ChEBI allows for a significant in-
crease of precision, which is encouraging. We be-
lieve this result is a good indication that as ChEBI
matures, the methods that take advantage of its on-
tology structure for entity recognition and classifica-
tion will benefit more from its usage, increasing the
F-measure obtained in the task.
9 Acknowledgments
The authors want to thank the Portuguese
Fundac?a?o para a Cie?ncia e Tecnologia through
the financial support of the SPNet project
(PTDC/EBB-EBI/113824/2009), the SOMER
project (PTDC/EIA-EIA/119119/2010) and the
PhD grant SFRH/BD/36015/2007 and through the
LASIGE multi-annual support. The authors also
wish to thank the European Commission for the
financial support of the EPIWORK project under the
Seventh Framework Programme (Grant #231807).
References
P. Corbett, C. Batchelor and S. Teufel. 2007. Annotation
of chemical named entities. Proceedings of the Work-
665
shop on BioNLP 2007: Biological, Translational, and
Clinical Language Processing, 57?64.
F. M. Couto and M. J. Silva. 2011. Disjunctive shared
information between ontology concepts: application
to Gene Ontology. Journal of Biomedical Semantics,
2(5).
F. M. Couto, P. M. Coutinho and M. J. Silva. 2005.
Finding genomic ontology terms in text using evidence
content. BMC Bioinformatics, 6 (Suppl 1), S21.
K. Degtyarenko, P. Matos, M. Ennis, J. Hastings, M.
Zbinden, A. McNaught, R. Alcantara, M. Darsow, M.
Guedj and M. Ashburner. 2007. ChEBI: a database
and ontology for chemical entities of biological inter-
est. Nucleic Acids Research, 36, D344.
D. Faria, C. Pesquita, E. Santos, F. M. Couto, C. Stroe
and I. F. Cruz 2012. Testing the AgreementMaker
System in the Anatomy Task of OAEI 2012. CoRR,
abs/1212.1625, arXiv:1212.1625.
J. D. Ferreira and F. M. Couto 2010. Semantic similarity
for automatic classification of chemical compounds.
PLoS Computational Biology, 6(9).
T. Grego, C. Pesquita, H. P. Bastos and F. M. Couto.
2012. Chemical Entity Recognition and Resolution to
ChEBI. ISRN Bioinformatics, Article ID 619427.
T. Grego, P. Pezik, F. M. Couto and D. Rebholz-
Schuhmann. 2009. Identification of Chemical Enti-
ties in Patent Documents. Distributed Computing, Ar-
tificial Intelligence, Bioinformatics, Soft Computing,
and Ambient Assisted Living, volume 5518 of Lecture
Notes in Computer Science, 934?941.
T. Grego, F. Pinto and F. M. Couto. 2012. Identifying
Chemical Entities based on ChEBI. Software Demon-
stration at the International Conference on Biomedical
Ontologies (ICBO).
D. M. Jessop, S. E. Adams, E. L. Willighagen, L. Hawizy
and P. Murray-Rust 2011. OSCAR4: a flexible archi-
tecture for chemical text-mining. Journal of Chemin-
formatics, 3(41).
J. Lafferty, A. McCallum and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
the 18th International Conference on Machine Learn-
ing, 282?289.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.
edu.
D. Rebholz-Schuhmann, M. Arregui, S. Gaudan, H.
Kirsch and A. Jimeno. 2008. Text processing
through Web services: calling Whatizit. Bioinformat-
ics, 24(2):296?298.
T. Rockta?schel, M. Weidlich and U. Leser. 2012.
ChemSpot: A Hybrid System for Chemical Named
Entity Recognition. Bioinformatics, 28(12): 1633-
1640.
I. Segura-Bedmar, P. Mart??nez and C. de Pablo-Sa?nchez
2006. Using a shallow linguistic kernel for drug-drug
interaction extraction. Journal of Biomedical Infor-
matics, 44(5): 789?804.
Y. Wang, J. Xiao, T. O. Suzek, J. Zhang, J. Wang and
S. H. Bryant. 2009. PubChem: a public information
system for analyzing bioactivities of small molecules.
Nucleic Acids Research, 37, W623.
D. S. Wishart, C. Knox, A. C. Guo, S. Shrivastava, M.
Hassanali, P. Stothard, Z. Chang and J. Woolsey. 2006.
DrugBank: a comprehensive resource for in silico drug
discovery and exploration. Nucleic Acids Research,
34, D668.
SemEval 2013. In Proceedings of the 7th International
Workshop on Semantic Evaluation
666
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 711?715,
Dublin, Ireland, August 23-24, 2014.
ULisboa: Identification and Classification of Medical Concepts
Andr
?
e Leal
+
, Diogo Gonc?alves
+
, Bruno Martins
?
, and Francisco M. Couto
+
+
LASIGE, Faculdade de Ci?encias, Universidade de Lisboa, 1749-016 Lisboa, Portugal.
?
INESC-ID, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal
{aleal,dgoncalves}@lasige.di.fc.ul.pt , bruno.g.martins@ist.ul.pt , fcouto@di.fc.ul.pt
Abstract
This paper describes our participation on
Task 7 of SemEval 2014, which fo-
cused on the recognition and disambigua-
tion of medical concepts. We used an
adapted version of the Stanford NER sys-
tem to train CRF models to recognize tex-
tual spans denoting diseases and disor-
ders, within clinical notes. We consid-
ered an encoding that accounts with non-
continuous entities, together with a rich
set of features (i) based on domain spe-
cific lexicons like SNOMED CT, or (ii)
leveraging Brown clusters inferred from a
large collection of clinical texts. Together
with this recognition mechanism, we used
a heuristic similarity search method, to
assign an unambiguous identifier to each
concept recognized in the text.
Our best run on Task A (i.e., in the recog-
nition of medical concepts in the text)
achieved an F-measure of 0.705 in the
strict evaluation mode, and a promising
F-measure of 0.862 in the relaxed mode,
with a precision of 0.914. For Task B (i.e.,
the disambiguation of the recognized con-
cepts), we achieved less promising results,
with an accuracy of 0.405 in the strict
mode, and of 0.615 in the relaxed mode.
1 Introduction
Currently, many off-the-shelf named entity recog-
nition solutions are available, and these can be
used to recognize mentions in clinical notes de-
noting diseases and disorders. We decided to use
the Stanford NER tool (Finkel et al., 2005) to train
CRF models based on annotated biomedical text.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The use of unsupervised methods for inferring
word representations is nowadays also known to
increase the accuracy of entity recognition mod-
els (Turian et al., 2010). Thus, we also used Brown
clusters (Brown et al., 1992; Turian et al., 2009)
inferred from a large collection of non-annotated
clinical texts, together with domain specific lexi-
cons, to build features for our CRF models.
An important challenge in entity recognition re-
lates to the recognition of overlapping and non-
continuous entities (Alex et al., 2007). In this
paper, we describe how we modified the Stan-
ford NER system to be able to recognize non-
continuous entities, through an adapted version of
the SBIEO scheme (Ratinov and Roth, 2009).
Besides the recognition of medical concepts, we
also present the strategy used to map each of the
recognized concepts into a SNOMED CT identi-
fier (Cornet and de Keizer, 2008). This task is
particularly challenging, since there are many am-
biguous cases. We describe our general approach
to address the aforementioned CUI mapping prob-
lem, based on similarity search and on the infor-
mation content of SNOMED CT concept names.
2 Task and Datasets
Task 7 of SemEval 2014 actually consisted of two
smaller tasks: recognition of mentions of medi-
cal concepts (Task A) and mapping each medical
concept, recognized in clinical notes, to a unique
UMLS CUI (Task B). In the first task, recogni-
tion of medical concepts, systems have to detect
continuous and discontinuous medical concepts
that belong to the UMLS semantic group disor-
ders. The second task, concerning with normal-
ization and mapping, is limited to UMLS CUIs
of SNOMED CT codes (i.e., although the UMLS
meta-thesaurus integrates several resources, we
are only interested in SNOMED CT). Each con-
cept that was previously recognized can have a
unique CUI associated to it, or none at all (CUI-
711
LESS). The goal here is to disambiguate the con-
cepts and choose the right CUI for each case. For
supporting the recognition and CUI mapping of
medical concepts, we retrieved the disorders sub-
set of SNOMED CT directly from UMLS
1
.
The evaluation can be done in a strict or a in
a relaxed way. For the case of strict evaluation,
an exact match must be achieved in the recogni-
tion, by having correct start and end offsets, within
the text, for the continuous concepts, and a cor-
rect set of start and end offsets for the discontin-
uous concepts. In the relaxed evaluation, there is
some space for errors in the offset values from the
recognition task. If there is some overlap between
the concepts, then the result is considered a partial
match, otherwise it is a recognition error.
A set of annotated biomedical texts was given
to the participants, separated in three categories:
trial, development and training. We also received a
final test set, and a large set of non-annotated texts.
All the provided texts were initially converted into
a common tokenized format, to be used as input
to the tools that we considered for developing our
approach. After processing, we converted the re-
sults back into the format used by SemEval 2014,
this way generating the official runs.
3 Entity Recognition
Our entity recognition approach was based on the
usage of the Stanford NER software, which em-
ploys a linear chain Conditional Random Field
(CRF) approach for building probabilistic mod-
els based on training data (Finkel et al., 2005).
In Stanford NER, model training is based on the
L-BFGS algorithm, and model decoding is made
through the Viterbi algorithm.
This tool requires all input texts to be tokenized
and encoded according to a named entity recog-
nition scheme such as SBIEO (Ratinov and Roth,
2009), characterized by only being able to recog-
nize continuous entities. As we also need to rec-
ognize non-continuous entities, we modified the
Stanford NER software to use a SBIEON encod-
ing scheme. This new scheme has the following
specific token-tag associations:
S: Single, that indicates if the token individually
constitutes an entity to be recognized.
B: Begin, identifying the beginning of the entity.
This tag is only given to the first word of the
1
http://www.nlm.nih.gov/research/umls/
entity, being followed is most cases by tokens
labeled as being inside the entity.
I: Inside, representing the continuation of a non
single word entity (i.e., the middle tokens).
E: Ending, representing the last word in the case
of entities composed by more than one word.
N: Non-Continuous, which identifies all the
words that are between the beginning and the
end of an entity, but that do not belong to it.
This label specifically allows us to model the
in-between tokens of non-continuous entities.
O: Other, which is associated to all other words
that are not part of entities.
We developed a Java parser that converts the
biomedical text, provided to the participants, into
a tokenized format. This tokenized format, in the
case of the annotated texts, associates individual
tokens to their SBIEON or SBIEO tags, so that the
datasets can be used as input to train CRF models.
3.1 Concept Recognition Models
As we said, SBIEON tokenization differs from
SBIEO by the fact that the first one gives support
to non-continuous entities. Based on these two in-
put schemes, we generated two different models:
Only continuous entities: A 2nd-order CRF
model was trained based on the SBIOE entity en-
coding scheme, which only recognizes continuous
entities. Non-continuous and overlapping entities
will thus, in this case, only be partially modeled
(i.e., we only considered the initial span of text as-
sociated to the non-continuous entities).
Non-continuous entities: A 2nd-order CRF
model was trained based on the SBIOEN entity
encoding scheme, accepting continuous and non-
continuous entities, although still not supporting
the case of overlapping entities. In these last cases,
only the first entity in each of the overlapping
groups will be modeled correctly, while the oth-
ers will only be partially modeled (i.e., by only
considering the non-overlapping spans).
Our CRF models relied on a standard set of fea-
ture templates that includes (i) word tokens within
a window of size 2, (ii) the token shape (e.g., if it
is uppercased, capitalized, numeric, etc.), (iii) to-
ken prefixes and suffixes, (iv) token position (e.g.,
at the beginning or ending of a sentence), and (v)
conjunctions of the current token with the previ-
ous 2 tags. Besides these standard features, we
also considered (a) domain-specific lexicons, and
(b) word representations based on Brown clusters.
712
3.2 Word Clusters
In addition to the annotated training dataset, par-
ticipants were also provided with 403876 non-
annotated texts, containing a total of 828509 to-
kens. We used this information to induce general-
ized cluster-based word representations.
Brown et al. proposed a greedy agglomera-
tive hierarchical clustering procedure that groups
words to maximize the mutual information of bi-
grams (Brown et al., 1992). According to Brown?s
clustering procedure, clusters are initialized as
consisting of a single word each, and are then
greedily merged according to a mutual informa-
tion criterion, based on bi-grams, to form a lower-
dimensional representation of a vocabulary that
can mitigate the feature sparseness problem. In
the context of named entity recognition studies,
several authors have previously noted that using
these types of cluster-based word representations
can indeed result in improvements (Turian et al.,
2009). The hierarchical nature of the clustering
allows words to be represented at different levels
in the hierarchy and, in our case, we considered
1000 different clusters of similar words.
We specifically used the set of training docu-
ments, together with the non-annotated documents
that were provided by the organizers, to induce
word representations based on Brown?s clustering
procedure, using an open-source implementation
that follows the description given by (Turian et al.,
2010). The word clusters are latter used as features
within the Stanford NER package, by considering
that each word can be replaced by the correspond-
ing cluster, this way adding some other back-off
features to the models (i.e., features that are less
sparse, in the sense that they will appear more fre-
quently associated to some of the instances).
4 Disambiguating Concepts
For mapping entities to concept IDs (Task B), we
used a heuristic method based on similarity search,
supported on Lucene indexes (MacCandless et al.,
2010). We look for SNOMED CT concepts that
have a high n-gram overlap with the entity name
occurring in the text, together with the information
content of each SNOMED CT concept.
In our implementation, we used Lucene to re-
trieve candidate SNOMED CT concepts according
to different string distance algorithms: the NGram
distance (Kondrak, 2005) first, then according to
the Jaro-Winkler distance (Winkler, 1990), and fi-
nally according to the Levenshtein distance. The
most similar candidate is chosen as the disam-
biguation. The specific order for the similar-
ity metrics was based on the intuition that met-
rics based on individual character-level matches
are probably not as informative as metrics based
on longer sequences of characters, although they
can be useful for dealing with spelling variations.
However, for future work, we plan to explore more
systematic approaches (e.g., based on learning to
rank) for combining multiple similarity metrics.
Additionally to the aforementioned similarity
metrics, a measure of the Information Content (IC)
of each SNOMED CT concept was also employed,
to further disambiguate the mappings (i.e., to se-
lect the SNOMED CT identifier that is more gen-
eral, and thus more likely to be associated to a par-
ticular concept descriptor). Notice that the IC of
a concept corresponds to a measure of its speci-
ficity, where higher values correspond to more
specific concepts, and lower values to more gen-
eral ones. Given the frequency freq(c) for each
concept c in a corpus (i.e., the same corpus that
was used to infer the word clusters), the informa-
tion content of this concept can be computed from
the ratio between its frequency (including its de-
scendants) and the maximum frequency of all con-
cepts (Resnik, 1995):
IC(c) = ? log
(
freq(c)
maxFreq
)
In the formula, maxFreq represents the maximum
frequency of a concept, i.e. the frequency of the
root concept, when it exists. The frequency of a
concept can be computed using an extrinsic ap-
proach that counts the exact matches of the con-
cept names on a large text corpus.
5 Evaluation Experiments
We submitted three distinct runs to the SemEval
competition. These runs were as follows:
Run 1: A SBIOEN model was used to recog-
nize non-continuous entities. This model was
trained using only the annotated texts from
the provided training set. We also used some
domain specific lexicons like SNOMED CT,
or lists with names for drugs and diseases
retrieved from DBPedia. Finally, the recog-
nition model also used Brown clusters gen-
erated from the non-annotated datasets pro-
vided in the competition.
713
For assigning a SNOMED CT identifier to
each entity, we used the disambiguation tech-
nique supported by Lucene indexes. In
this specific run we used all the considered
heuristics for similarity search.
Run 2: A simpler model based on the SBIOE
scheme was used in this case, which can only
recognize continuous entities. The same fea-
tures from Run 1 were used for training the
recognition model.
For assigning the SNOMED CT identifier to
each entity, we also used the same strategy
that was presented for Run 1.
Run 3: A similar SBIOE model to that from Run
2 was used for the recognition.
For assigning the corresponding SNOMED
CT identifier to each entity, we in this case
limited the heuristic rules that were used.
Instead of using the string similarity algo-
rithms, we used only exact matches, together
with the information content measure and the
neighboring terms for disambiguation.
6 Results and Discussion
We present our official results in Table 1, which
highlights our best results for each task.
Specifically in Task A, we achieved encourag-
ing results. Run 1 achieved an F-measure of 0.705
in the strict evaluation, and of 0.862 in the relaxed
evaluation. Since Runs 2 and 3 used the same
recognition strategy (i.e., models that attempted to
capture only the continuous entities), we obtained
the same results for Task A in both these runs. Ta-
ble 1 also shows that our performance in Task B
was significantly lower than in Task A.
As we can see in the table, our first run was the
one with the best results for Task A. The model
used on this run recognizes non-continuous enti-
ties, and this is perhaps the main reason for the
higher results (i.e., the other two runs used the
same features for the recognition models).
On what concerns the results of Task B, it is im-
portant to notice the distinct results from the first
and second runs, which used exactly the same dis-
ambiguation strategy. The differences in the re-
sults are a consequence from the use of a different
recognition model in Task A. We can see that the
ability to recognize non-continuous entities leads
to the generation of worse mappings, when con-
sidering our specific disambiguation strategy. Our
last run is the best in terms of the performance over
Task B, but the difference is subtle.
7 Conclusions and Future Work
This paper described our participation in Task 7 of
the SemEval 2014 competition, which was divided
into two subtasks, namely (i) the recognition of
continuous and non-continuous medical concepts,
and (ii) the mapping of each recognized concept to
a SNOMED CT identifier.
For the first task, we used the Stanford NER
software (Finkel et al., 2005), modified by us
to recognize not only continuous, but also non-
continuous entities. This was possible by intro-
ducing the SBIEON scheme, derived from the tra-
ditional SBIEO encoding. To increase the accu-
racy and precision of the recognition we have also
used domain specific lexicons and Brown clusters
inferred from non-annotated documents.
For the second task, we used a heuristic method
based on similarity search, for matching concepts
in the text against concepts from SNOMED CT,
together with a measure of information content
to disambiguate the cases of term polysemy in
SNOMED CT. We implemented our disambigua-
tion approach through the Lucene software frame-
work (MacCandless et al., 2010).
In the first task (Task A) we achieved some par-
ticularly encouraging results, showing that an off-
the-shelf NER system can be easily adapted to
the recognition of medical concepts in biomedi-
cal text. Our specific modifications to the Stanford
NER system, in order to support the recognition of
non-continuous entity names, indeed increased the
precision and recall on Task A. However, our ap-
proach for the disambiguation of the recognized
concepts (Task B) performed much worse, achiev-
ing an accuracy of 0.615 in the case of the relaxed
evaluation. Future developments will therefore fo-
cus on improving the component that addressed
the entity disambiguation subtask.
Specifically on what regards future work, we
plan to experiment with the usage of machine
learning methods for the disambiguation subtask,
instead of relying on a purely heuristic approach.
We are interested in experimenting with the us-
age of Learning to Rank (L2R) methods, similar to
those employed on the DNorm system (Leaman et
al., 2013), to optimally combine different heuris-
tics such as the ones that were used in our current
approach. A L2R model can be used to rank candi-
714
Task A Task B
Strict Evaluation Relaxed Evaluation Strict Relaxed
Run Precision Recall F-measure Precision Recall F-measure Accuracy Accuracy
1 0.753 0.663 0.705 0.914 0.815 0.862 0.402 0.606
2 0.752 0.660 0.703 0.909 0.806 0.855 0.404 0.612
3 0.752 0.660 0.703 0.909 0.806 0.855 0.405 0.615
Table 1: Our official results for Tasks A and B of the SemEval challenge focusing on clinical text.
date disambiguations (e.g., retrieved through sim-
ilarity search with basis on Lucene) according to a
combination of multiple criteria, and we can then
choose the top candidate as the disambiguation.
Additionally, we plan to use ontology-based
similarity measures to validate and improve the
mappings (Couto and Pinto, 2013). For example,
by assuming that all entities in a given span of text
are semantically related with each other, we can
use ontology relations to filter likely misannota-
tions (Grego and Couto, 2013; Grego et al., 2013).
Acknowledgments
The authors would like to thank Fundac??ao para a
Ci?encia e Tecnologia (FCT) for the financial sup-
port of SOMER (PTDC/EIA-EIA/119119/2010),
LASIGE (PEst-OE/EEI/UI0408/2014) and
INESC-ID (Pest-OE/EEI/LA0021/2013).
We also would like to thank our colleagues
Berta Alves, for her support in evaluating devel-
opment errors, and Lu??s Atalaya, for the develop-
ment of some of the data pre-processing scripts.
References
Beatrice Alex, Barry Haddow, and Claire Grover.
2007. Recognising nested named entities in biomed-
ical text. In Proceedings of the ACL-07 Workshop
on Biological, Translational, and Clinical Language
Processing, pages 65?72.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Ronald Cornet and Nicolette de Keizer. 2008. Forty
years of SNOMED: A literature review. BMC
Medical Informatics and Decision Making, 8(Suppl
1:S2):1?6.
Francisco M. Couto and Helena Sofia Pinto. 2013. The
next generation of similarity measures that fully ex-
plore the semantics in biomedical ontologies. Jour-
nal of Bioinformatics and Computational Biology,
11(05):1?11.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370.
Tiago Grego and Francisco M Couto. 2013. Enhance-
ment of chemical entity identification in text using
semantic similarity validation. PloS ONE, 8(5):1?9.
Tiago Grego, Francisco Pinto, and Francisco Couto.
2013. LASIGE: using conditional random fields and
chebi ontology. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation, pages
660?666.
Grzegorz Kondrak. 2005. N-gram similarity and dis-
tance. In Proceedings of the 12th International
Conference String Processing and Information Re-
trieval, pages 115?126.
Robert Leaman, Rezarta Islamaj Do?gan, and Zhiy-
ong Lu. 2013. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909?2917.
Michael MacCandless, Erik Hatcher, and Otis Gospod-
neti?c. 2010. Lucene in Action. Manning Publica-
tions Company.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning, pages 147?155.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In Pro-
ceedings of the NIPS-09 Workshop on Grammar In-
duction, Representation of Language and Language
Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394.
William E Winkler. 1990. String comparator metrics
and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Sec-
tion on Survey Research of the American Statistical
Association, pages 354?359.
715
