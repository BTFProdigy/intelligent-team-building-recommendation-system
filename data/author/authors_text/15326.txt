Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 204?212,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Strategies to Integrate Multiword Expression Recognition
and Parsing
Matthieu Constant
Universite? Paris-Est
LIGM, CNRS
France
mconstan@univ-mlv.fr
Anthony Sigogne
Universite? Paris-Est
LIGM, CNRS
France
sigogne@univ-mlv.fr
Patrick Watrin
Universite? de Louvain
CENTAL
Belgium
patrick.watrin
@uclouvain.be
Abstract
The integration of multiword expressions in a
parsing procedure has been shown to improve
accuracy in an artificial context where such
expressions have been perfectly pre-identified.
This paper evaluates two empirical strategies
to integrate multiword units in a real con-
stituency parsing context and shows that the
results are not as promising as has sometimes
been suggested. Firstly, we show that pre-
grouping multiword expressions before pars-
ing with a state-of-the-art recognizer improves
multiword recognition accuracy and unlabeled
attachment score. However, it has no statis-
tically significant impact in terms of F-score
as incorrect multiword expression recognition
has important side effects on parsing. Sec-
ondly, integrating multiword expressions in
the parser grammar followed by a reranker
specific to such expressions slightly improves
all evaluation metrics.
1 Introduction
The integration of Multiword Expressions (MWE)
in real-life applications is crucial because such ex-
pressions have the particularity of having a certain
level of idiomaticity. They form complex lexical
units which, if they are considered, should signifi-
cantly help parsing.
From a theoretical point of view, the integra-
tion of multiword expressions in the parsing pro-
cedure has been studied for different formalisms:
Head-Driven Phrase Structure Grammar (Copestake
et al, 2002), Tree Adjoining Grammars (Schuler
and Joshi, 2011), etc. From an empirical point of
view, their incorporation has also been considered
such as in (Nivre and Nilsson, 2004) for depen-
dency parsing and in (Arun and Keller, 2005) in con-
stituency parsing. Although experiments always re-
lied on a corpus where the MWEs were perfectly
pre-identified, they showed that pre-grouping such
expressions could significantly improve parsing ac-
curacy. Recently, Green et al (2011) proposed in-
tegrating the multiword expressions directly in the
grammar without pre-recognizing them. The gram-
mar was trained with a reference treebank where
MWEs were annotated with a specific non-terminal
node.
Our proposal is to evaluate two discriminative
strategies in a real constituency parsing context:
(a) pre-grouping MWE before parsing; this would
be done with a state-of-the-art recognizer based
on Conditional Random Fields; (b) parsing with
a grammar including MWE identification and then
reranking the output parses thanks to a Maxi-
mum Entropy model integrating MWE-dedicated
features. (a) is the direct realistic implementation of
the standard approach that was shown to reach the
best results (Arun and Keller, 2005). We will evalu-
ate if real MWE recognition (MWER) still positively
impacts parsing, i.e., whether incorrect MWER does
not negatively impact the overall parsing system.
(b) is a more innovative approach to MWER (de-
spite not being new in parsing): we select the final
MWE segmentation after parsing in order to explore
as many parses as possible (as opposed to method
(a)). The experiments were carried out on the French
Treebank (Abeille? et al, 2003) where MWEs are an-
notated.
204
The paper is organized as follows: section 2 is
an overview of the multiword expressions and their
identification in texts; section 3 presents the two dif-
ferent strategies and their associated models; sec-
tion 4 describes the resources used for our exper-
iments (the corpus and the lexical resources); sec-
tion 5 details the features that are incorporated in the
models; section 6 reports on the results obtained.
2 Multiword expressions
2.1 Overview
Multiword expressions are lexical items made up
of multiple lexemes that undergo idiosyncratic con-
straints and therefore offer a certain degree of id-
iomaticity. They cover a wide range of linguistic
phenomena: fixed and semi-fixed expressions, light
verb constructions, phrasal verbs, named entities,
etc. They may be contiguous (e.g. traffic light) or
discontinuous (e.g. John took your argument into
account). They are often divided into two main
classes: multiword expressions defined through lin-
guistic idiomaticity criteria (lexicalized phrases in
the terminology of Sag et al (2002)) and those de-
fined by statistical ones (i.e. simple collocations).
Most linguistic criteria used to determine whether a
combination of words is a MWE are based on syn-
tactic and semantic tests such as the ones described
in (Gross, 1986). For instance, the utterance at night
is a MWE because it does display a strict lexical
restriction (*at day, *at afternoon) and it does not
accept any inserting material (*at cold night, *at
present night). Such linguistically defined expres-
sions may overlap with collocations which are the
combinations of two or more words that cooccur
more often than by chance. Collocations are usu-
ally identified through statistical association mea-
sures. A detailed description of MWEs can be found
in (Baldwin and Nam, 2010).
In this paper, we focus on contiguous MWEs that
form a lexical unit which can be marked by a part-of-
speech tag (e.g. at night is an adverb, because of is a
preposition). They can undergo limited morphologi-
cal and lexical variations ? e.g. traffic (light+lights),
(apple+orange+...) juice ? and usually do not al-
low syntactic variations1 such as inserts (e.g. *at
1Such MWEs may very rarely accept inserts, often limited
to single word modifiers: e.g. in the short term, in the very short
cold night). Such expressions can be analyzed at the
lexical level. In what follows, we use the term com-
pounds to denote such expressions.
2.2 Identification
The idiomaticity property of MWEs makes them
both crucial for Natural Language Processing appli-
cations and difficult to predict. Their actual iden-
tification in texts is therefore fundamental. There
are different ways for achieving this objective. The
simpler approach is lexicon-driven and consists in
looking the MWEs up in an existing lexicon, such
as in (Silberztein, 2000). The main drawback is
that this procedure entirely relies on a lexicon and
is unable to discover unknown MWEs. The use
of collocation statistics is therefore useful. For in-
stance, for each candidate in the text, Watrin and
Franc?ois (2011) compute on the fly its association
score from an external ngram base learnt from a
large raw corpus, and tag it as MWE if the associa-
tion score is greater than a threshold. They reach ex-
cellent scores in the framework of a keyword extrac-
tion task. Within a validation framework (i.e. with
the use of a reference corpus annotated in MWEs),
Ramisch et al (2010) developped a Support Vector
Machine classifier integrating features correspond-
ing to different collocation association measures.
The results were rather low on the Genia corpus
and Green et al (2011) confirmed these bad results
on the French Treebank. This can be explained by
the fact that such a method does not make any dis-
tinctions between the different types of MWEs and
the reference corpora are usually limited to certain
types of MWEs. Furthermore, the lexicon-driven
and collocation-driven approaches do not take the
context into account, and therefore cannot discard
some of the incorrect candidates. A recent trend is
to couple MWE recognition with a linguistic ana-
lyzer: a POS tagger (Constant and Sigogne, 2011)
or a parser (Green et al, 2011). Constant and Si-
gogne (2011) trained a unified Conditional Random
Fields model integrating different standard tagging
features and features based on external lexical re-
sources. They show a general tagging accuracy of
94% on the French Treebank. In terms of Multi-
word expression recognition, the accuracy was not
term.
205
clearly evaluated, but seemed to reach around 70-
80% F-score. Green et al (2011) proposed to in-
clude the MWER in the grammar of the parser. To
do so, the MWEs in the training treebank were anno-
tated with specific non-terminal nodes. They used a
Tree Substitution Grammar instead of a Probabilis-
tic Context-free Grammar (PCFG) with latent anno-
tations in order to capture lexicalized rules as well
as general rules. They showed that this formalism
was more relevant to MWER than PCFG (71% F-
score vs. 69.5%). Both methods have the advantage
of being able to discover new MWEs on the basis
of lexical and syntactic contexts. In this paper, we
will take advantage of the methods described in this
section by integrating them as features of a MWER
model.
3 Two strategies, two discriminative
models
3.1 Pre-grouping Multiword Expressions
MWER can be seen as a sequence labelling task
(like chunking) by using an IOB-like annotation
scheme (Ramshaw and Marcus, 1995). This implies
a theoretical limitation: recognized MWEs must be
contiguous. The proposed annotation scheme is
therefore theoretically weaker than the one proposed
by Green et al (2011) that integrates the MWER in
the grammar and allows for discontinuous MWEs.
Nevertheless, in practice, the compounds we are
dealing with are very rarely discontinuous and if so,
they solely contain a single word insert that can be
easily integrated in the MWE sequence. Constant
and Sigogne (2011) proposed to combine MWE seg-
mentation and part-of-speech tagging into a single
sequence labelling task by assigning to each token a
tag of the form TAG+X where TAG is the part-of-
speech (POS) of the lexical unit the token belongs to
and X is either B (i.e. the token is at the beginning
of the lexical unit) or I (i.e. for the remaining posi-
tions): John/N+B hates/V+B traffic/N+B jams/N+I.
In this paper, as our task consists in jointly locating
and tagging MWEs, we limited the POS tagging to
MWEs only (TAG+B/TAG+I), simple words being
tagged by O (outside): John/O hates/O traffic/N+B
jams/N+I.
For such a task, we used Linear chain Conditional
Ramdom Fields (CRF) that are discriminative prob-
abilistic models introduced by Lafferty et al (2001)
for sequential labelling. Given an input sequence of
tokens x = (x1, x2, ..., xN ) and an output sequence
of labels y = (y1, y2, ..., yN ), the model is defined
as follows:
P?(y|x) =
1
Z(x)
.
N?
t
K?
k
log?k.fk(t, yt, yt?1, x)
where Z(x) is a normalization factor depending
on x. It is based on K features each of them be-
ing defined by a binary function fk depending on
the current position t in x, the current label yt, the
preceding one yt?1 and the whole input sequence
x. The tokens xi of x integrate the lexical value
of this token but can also integrate basic properties
which are computable from this value (for example:
whether it begins with an upper case, it contains a
number, its tags in an external lexicon, etc.). The
feature is activated if a given configuration between
t, yt, yt?1 and x is satisfied (i.e. fk(t, yt, yt?1, x) =
1). Each feature fk is associated with a weight ?k.
The weights are the parameters of the model, to be
estimated. The features used for MWER will be de-
scribed in section 5.
3.2 Reranking
Discriminative reranking consists in reranking the n-
best parses of a baseline parser with a discriminative
model, hence integrating features associated with
each node of the candidate parses. Charniak and
Johnson (2005) introduced different features that
showed significant improvement in general parsing
accuracy (e.g. around +1 point in English). For-
mally, given a sentence s, the reranker selects the
best candidate parse p among a set of candidates
P (s) with respect to a scoring function V?:
p? = argmaxp?P (s)V?(p)
The set of candidates P (s) corresponds to the n-best
parses generated by the baseline parser. The scor-
ing function V? is the scalar product of a parameter
vector ? and a feature vector f :
V?(p) = ?.f(p) =
m?
j=1
?j .fj(p)
where fj(p) corresponds to the number of occur-
rences of the feature fj in the parse p. According to
206
Charniak and Johnson (2005), the first feature f1 is
the probability of p provided by the baseline parser.
The vector ? is estimated during the training stage
from a reference treebank and the baseline parser
ouputs.
In this paper, we slightly deviate from the original
reranker usage, by focusing on improving MWER
in the context of parsing. Given the n-best parses,
we want to select the one with the best MWE seg-
mentation by keeping the overall parsing accuracy as
high as possible. We therefore used MWE-dedicated
features that we describe in section 5. The training
stage was performed by using a Maximum entropy
algorithm as in (Charniak and Johnson, 2005).
4 Resources
4.1 Corpus
The French Treebank2 [FTB] (Abeille? et al, 2003)
is a syntactically annotated corpus made up of jour-
nalistic articles from Le Monde newspaper. We
used the latest edition of the corpus (June 2010)
that we preprocessed with the Stanford Parser pre-
processing tools (Green et al, 2011). It contains
473,904 tokens and 15,917 sentences. One benefit of
this corpus is that its compounds are marked. Their
annotation was driven by linguistic criteria such as
the ones in (Gross, 1986). Compounds are identified
with a specific non-terminal symbol ?MWX? where
X is the part-of-speech of the expression. They have
a flat structure made of the part-of-speech of their
components as shown in figure 1.
MWN


HH
H
N
part
P
de
N
marche?
Figure 1: Subtree of MWE part de marche? (market
share): The MWN node indicates that it is a multiword
noun; it has a flat internal structure N P N (noun ? pre-
prosition ? noun)
The French Treebank is composed of 435,860 lex-
ical units (34,178 types). Among them, 5.3% are
compounds (20.8% for types). In addition, 12.9%
2http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-
fr.php
of the tokens belong to a MWE, which, on average,
has 2.7 tokens. The non-terminal tagset is composed
of 14 part-of-speech labels and 24 phrasal ones (in-
cluding 11 MWE labels). The train/dev/test split is
the same as in (Green et al, 2011): 1,235 sentences
for test, 1,235 for development and 13,347 for train-
ing. The development and test sections are the same
as those generally used for experiments in French,
e.g. (Candito and Crabbe?, 2009).
4.2 Lexical resources
French is a resource-rich language as attested by
the existing morphological dictionaries which in-
clude compounds. In this paper, we use two large-
coverage general-purpose dictionaries: Dela (Cour-
tois, 1990; Courtois et al, 1997) and Lefff (Sagot,
2010). The Dela was manually developed in the
90?s by a team of linguists. We used the distribution
freely available in the platform Unitex3 (Paumier,
2011). It is composed of 840,813 lexical entries in-
cluding 104,350 multiword ones (91,030 multiword
nouns). The compounds present in the resources re-
spect the linguistic criteria defined in (Gross, 1986).
The lefff is a freely available dictionary4 that has
been automatically compiled by drawing from dif-
ferent sources and that has been manually validated.
We used a version with 553,138 lexical entries in-
cluding 26,311 multiword ones (22,673 multiword
nouns). Their different modes of acquisition makes
those two resources complementary. In both, lexical
entries are composed of a inflected form, a lemma,
a part-of-speech and morphological features. The
Dela has an additional feature for most of the mul-
tiword entries: their syntactic surface form. For in-
stance, eau de vie (brandy) has the feature NDN be-
cause it has the internal flat structure noun ? prepo-
sition de ? noun.
In order to compare compounds in these lexical
resources with the ones in the French Treebank, we
applied on the development corpus the dictionar-
ies and the lexicon extracted from the training cor-
pus. By a simple look-up, we obtained a prelimi-
nary lexicon-based MWE segmentation. The results
are provided in table 1. They show that the use of
external resources may improve recall, but they lead
3http://igm.univ-mlv.fr/?unitex
4http://atoll.inria.fr/?sagot/lefff.html
207
to a decrease in precision as numerous MWEs in the
dictionaries are not encoded as such in the reference
corpus; in addition, the FTB suffers from some in-
consistency in the MWE annotations.
T L D T+L T+D T+L+D
recall 75.9 31.7 59.0 77.3 83.4 84.0
precision 61.2 52.0 55.6 58.7 51.2 49.9
f-score 67.8 39.4 57.2 66.8 63.4 62.6
Table 1: Simple context-free application of the lexical
resources on the development corpus: T is the MWE lex-
icon of the training corpus, L is the lefff, D is the Dela.
The given scores solely evaluate MWE segmentation and
not tagging.
In terms of statistical collocations, Watrin and
Franc?ois (2011) described a system that lists all the
potential nominal collocations of a given sentence
along with their association measure. The authors
provided us with a list of 17,315 candidate nominal
collocations occurring in the French treebank with
their log-likelihood and their internal flat structure.
5 MWE-dedicated Features
The two discriminative models described in sec-
tion 3 require MWE-dedicated features. In order to
make these models comparable, we use two compa-
rable sets of feature templates: one adapted to se-
quence labelling (CRF-based MWER) and the other
one adapted to reranking (MaxEnt-based reranker).
The MWER templates are instantiated at each posi-
tion of the input sequence. The reranker templates
are instantiated only for the nodes of the candidate
parse tree, which are leaves dominated by a MWE
node (i.e. the node has a MWE ancestor). We define
a template T as follows:
? MWER: for each position n in the input se-
quence x,
T = f(x, n)/yn
? RERANKER: for each leaf (in position n)
dominated by a MWE node m in the current
parse tree p,
T = f(p, n)/label(m)/pos(p, n)
where f is a function to be defined; yn is the out-
put label at position n; label(m) is the label of node
m and pos(p, n) indicates the position of the word
corresponding to n in the MWE sequence: B (start-
ing position), I (remaining positions).
5.1 Endogenous Features
Endogenous features are features directly extracted
from properties of the words themselves or from a
tool learnt from the training corpus (e.g. a tagger).
Word n-grams. We use word unigrams and bigrams
in order to capture multiwords present in the training
section and to extract lexical cues to discover new
MWEs. For instance, the bigram coup de is often
the prefix of compounds such as coup de pied (kick),
coup de foudre (love at first sight), coup de main
(help).
POS n-grams. We use part-of-speech unigrams
and bigrams in order to capture MWEs with irreg-
ular syntactic structures that might indicate the id-
iomacity of a word sequence. For instance, the POS
sequence preposition ? adverb associated with the
compound depuis peu (recently) is very unusual in
French. We also integrated mixed bigrams made up
of a word and a part-of-speech.
Specific features. Due to their different use, each
model integrates some specific features. In order to
deal with unknown words and special tokens, we in-
corporate standard tagging features in the CRF: low-
ercase forms of the words, word prefixes of length 1
to 4, word suffice of length 1 to 4, whether the word
is capitalized, whether the token has a digit, whether
it is an hyphen. We also add label bigrams. The
reranker models integrate features associated with
each MWE node, the value of which is the com-
pound itself.
5.2 Exogenous Features
Exogenous features are features that are not entirely
derived from the (reference) corpus itself. They are
computed from external data (in our case, our lexical
resources). The lexical resources might be useful to
discover new expressions: usually, expressions that
have standard syntax like nominal compounds and
are difficult to predict from the endogenous features.
The resources are applied to the corpus through a
lexical analysis that generates, for each sentence, a
finite-state automaton TFSA which represents all the
possible analyses. The features are computed from
the automaton TFSA.
Lexicon-based features. We associate each word
with its part-of-speech tags found in our external
morphological lexicon. All tags of a word constitute
208
an ambiguity class ac. If the word belongs to a com-
pound, the compound tag is also incorporated in the
ambiguity class. For instance, the word night (either
a simple noun or a simple adjective) in the context at
night, is associated with the class adj noun adv+I as
it is located inside a compound adverb. This feature
is directly computed from TFSA. The lexical anal-
ysis can lead to a preliminary MWE segmentation
by using a shortest path algorithm that gives priority
to compound analyses. This segmentation is also a
source of features: a word belonging to a compound
segment is assigned different properties such as the
segment part-of-speech mwt and its syntactic struc-
turemws encoded in the lexical resource, its relative
position mwpos in the segment (?B? or ?I?).
Collocation-based features. In our collocation re-
source, each candidate collocation of the French
treebank is associated with its internal syntactic
structure and its association score (log-likelihood).
We divided these candidates into two classes: those
whose score is greater than a threshold and the other
ones. Therefore, a given word in the corpus can be
associated with different properties whether it be-
longs to a potential collocation: the class c and the
internal structure cs of the collocation it belongs to,
its position cpos in the collocation (B: beginning; I:
remaining positions; O: outside). We manually set
the threshold to 150 after some tuning on the devel-
opment corpus.
All feature templates are given in table 2.
Endogenous Features
w(n+ i), i ? {?2,?1, 0, 1, 2}
w(n+ i)/w(n+ i+ 1), i ? {?2,?1, 0, 1}
t(n+ i), i ? {?2,?1, 0, 1, 2}
t(n+ i)/t(n+ i+ 1), i ? {?2,?1, 0, 1}
w(n+ i)/t(n+ j), (i, j) ? {(1, 0), (0, 1), (?1, 0), (0,?1)}
Exogenous Features
ac(n)
mwt(n)/mwpos(n)
mws(n)/mwpos(n)
c(n)/cs(n)/cpos(n)
Table 2: Feature templates (f ) used both in the MWER
and the reranker models: n is the current position in the
sentence, w(i) is the word at position i; t(i) is the part-
of-speech tag of w(i); if the word at absolute position i
is part of a compound in the Shortest Path Segmentation,
mwt(i) and mws(i) are respectively the part-of-speech
tag and the internal structure of the compound,mwpos(i)
indicates its relative position in the compound (B or I).
6 Evaluation
6.1 Experiment Setup
We carried out 3 different experiments. We first
tested a standalone MWE recognizer based on CRF.
We then combined MWE pregrouping based on
this recognizer and the Berkeley parser5 (Petrov
et al, 2006) trained on the FTB where the com-
pounds were concatenated (BKYc). Finally, we
combined the Berkeley parser trained on the FTB
where the compounds are annotated with specific
non-terminals (BKY), and the reranker. In all exper-
iments, we varied the set of features: endo are all en-
dogenous features; coll and lex include all endoge-
nous features plus collocation-based features and
lexicon-based ones, respectively; all is composed of
both endogenous and exogenous features. The CRF
recognizer relies on the software Wapiti6 (Lavergne
et al, 2010) to train and apply the model, and on
the software Unitex (Paumier, 2011) to apply lexical
resources. The part-of-speech tagger used to extract
POS features was lgtagger7 (Constant and Sigogne,
2011). To train the reranker, we used a MaxEnt al-
gorithm8 as in (Charniak and Johnson, 2005).
Results are reported using several standard mea-
sures, the F1score, unlabeled attachment and Leaf
Ancestor scores. The labeled F1score [F1]9, de-
fined by the standard protocol called PARSEVAL
(Black et al, 1991), takes into account the brack-
eting and labeling of nodes. The unlabeled attache-
ment score [UAS] evaluates the quality of unlabeled
5We used the version adapted to French in
the software Bonsai (Candito and Crabbe?, 2009):
http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html.
The original version is available at:
http://code.google.com/p/berkeleyparser/. We trained the
parser as follows: right binarization, no parent annotation, six
split-merge cycles and default random seed initialisation (8).
6Wapiti can be found at http://wapiti.limsi.fr/. It was con-
figured as follows: rprop algorithm, default L1-penalty value
(0.5), default L2-penalty value (0.00001), default stopping cri-
terion value (0.02%).
7Available at http://igm.univ-
mlv.fr/?mconstan/research/software/.
8We used the following mathematical libraries PETSc et
TAO, freely available at http://www.mcs.anl.gov/petsc/ and
http://www.mcs.anl.gov/research/projects/tao/
9Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We
also used the evaluation by category implemented in the class
EvalbByCat in the Stanford Parser.
209
dependencies between words of the sentence10. And
finally, the Leaf-Ancestor score [LA]11 (Sampson,
2003) computes the similarity between all paths (se-
quence of nodes) from each terminal node to the root
node of the tree. The global score of a generated
parse is equal to the average score of all terminal
nodes. Punctuation tokens are ignored in all met-
rics. The quality of MWE identification was evalu-
ated by computing the F1 score on MWE nodes. We
also evaluated the MWE segmentation by using the
unlabeled F1 score (U). In order to compare both ap-
proaches, parse trees generated by BKYc were auto-
matically transformed in trees with the same MWE
annotation scheme as the trees generated by BKY.
In order to establish the statistical significance of
results between two parsing experiments in terms of
F1 and UAS, we used a unidirectional t-test for two
independent samples12. The statistical significance
between two MWE identification experiments was
established by using the McNemar-s test (Gillick
and Cox, 1989). The results of the two experiments
are considered statistically significant with the com-
puted value p < 0.01.
6.2 Standalone Multiword recognition
The results of the standalone MWE recognizer are
given in table 3. They show that the lexicon-based
system (lex) reaches the best score. Accuracy is im-
proved by an absolute gain of +6.7 points as com-
pared with BKY parser. The strictly endogenous
system has a +4.9 point absolute gain, +5.4 points
when collocations are added. That shows that most
of the work is done by fully automatically acquired
features (as opposed to features coming from a man-
ually constructed lexicon). As expected, lexicon-
based features lead to a 5.3 point recall improve-
ment (with respect to non-lexicon based features)
whereas precision is stable. The more precise sys-
tem is the base one because it almost solely detects
compounds present in the training corpus; neverthe-
less, it is unable to capture new MWEs (it has the
10This score is computed by using the tool available at
http://ilk.uvt.nl/conll/software.html. The constituent trees are
automatically converted into dependency trees with the tool
Bonsai.
11Leaf-ancestor assessment tool available at
http://www.grsampson.net/Resources.html
12Dan Bikel?s tool available at
http://www.cis.upenn.edu/?dbikel/software.html.
lowest recall). BKY parser has the best recall among
the non lexicon-based systems, i.e. it is the best one
to discover new compounds as it is able to precisely
detect irregular syntactic structures that are likely to
be MWEs. Nevertheless, as it does not have a lex-
icalized strategy, it is not able to filter out incorrect
candidates; the precision is therefore very low (the
worst).
P R F1 F1 ? 40 U
base 78.0 68.3 72.8 71.2 74.3
endo 75.5 74.5 75.0 74.0 76.3
coll 76.6 74.4 75.5 74.9 77.0
lex 76.0 79.8 77.8 77.8 79.0
all 76.2 79.2 77.7 77.3 78.8
BKY 67.6 75.1 71.1 70.7 72.5
Stanford* - - - 70.1 -
DP-TSG* - - - 71.1 -
Table 3: MWE identification with CRF: base are the
features corresponding to token properties and word n-
grams. The differences between all systems are statisti-
cally significant with respect to McNemar?s test (Gillick
and Cox, 1989), except lex/all and all/coll;
lex/coll is ?border-line?. The results of the systems
based on the Stanford Parser and the Tree Substitution
Parser (DP-TSG) are reported from (Green et al, 2011).
6.3 Combination of Multiword Expression
Recognition and Parsing
We tested and compared the two proposed dis-
criminative strategies by varying the sets of MWE-
dedicated features. The results are reported in ta-
ble 4. Table 5 compares the parsing systems, by
showing the score differences between each of the
tested system and the BKY parser.
Strat. Feat. Parser F1 LA UAS F1(MWE)
- - BKY 80.61 92.91 82.99 71.1
pre - BKYc 75.47 91.10 76.74 0.0
pre endo BKYc 80.23 92.69 83.62 74.9
pre coll BKYc 80.32 92.73 83.77 75.5
pre lex BKYc 80.66 92.81 84.16 77.4
pre all BKYc 80.51 92.77 84.05 77.2
post endo BKY 80.87 92.94 83.49 72.9
post coll BKY 80.71 92.85 83.16 71.2
post lex BKY 81.08 92.98 83.98 74.5
post all BKY 81.03 92.96 83.97 74.3
pre gold BKYc 83.73 93.77 90.08 95.8
Table 4: Parsing evaluation: pre indicates a MWE pre-
grouping strategy, whereas post is a reranking strategy
with n = 50. The feature gold means that we have ap-
plied the parser on a gold MWE segmentation.
210
?F1 ?UAS ?F1(MWE)
pre post pre post pre post
endo -0.38 +0.26 +0.63 +0.50 +3.8 +1.8
coll -0.29 +0.10 +0.78 +0.17 +4.4 +0.1
lex +0.05 +0.47 +1.17 +0.99 +6.3 +3.4
Table 5: Comparison of the strategies with respect to
BKY parser.
Firstly, we note that the accuracy of the best re-
alistic parsers is much lower than that of a parser
with a golden MWE segmentation13 (-2.65 and -5.92
respectively in terms of F-score and UAS), which
shows the importance of not neglecting MWE recog-
nition in the framework of parsing. Furthermore,
pre-grouping has no statistically significant impact
on the F-score14, whereas reranking leads to a sta-
tistically significant improvement (except for col-
locations). Both strategies also lead to a statisti-
cally significant UAS increase. Whereas both strate-
gies improve the MWE recognition, pre-grouping
is much more accurate (+2-4%); this might be due
to the fact that an unlexicalized parser is limited in
terms of compound identification, even within n-
best analyses (cf. Oracle in table 6). The benefits of
lexicon-based features are confirmed in this experi-
ment, whereas the use of collocations in the rerank-
ing strategy seems to be rejected.
endo coll lex all oracle
n=1 80.61
(71.1)
n=5 80.74 80.88 81.03 81.05 83.17
(71.5) (71.7) (73.4) (73.3) (74.6)
n=20 80.98 80.72 81.09 81.01 84.76
(72.9) (70.6) (73.6) (73.0) (75.5)
n=50 80.87 80.71 81.08 81.03 85.21
(72.9) (71.2) (74.5) (74.3) (76.4)
n=100 80.69 80.53 81.12 80.93 85.54
(72.0) (70.0) (74.4) (73.7) (76.4)
Table 6: Reranker F1 evaluation with respect to n and the
types of features. The F1(MWE) is given in parenthesis.
Table 7 shows the results by category. It indi-
cates that both discriminative strategies are of in-
terest in locating multiword adjectives, determiners
and prepositions; the pre-grouping method appears
to be particularly relevant for multiword nouns and
13The F1(MWE) is not 100% with a golden segmentation be-
cause of tagging errors by the parser.
14Note that we observe an increase of +0.5 in F1 on the de-
velopment corpus with lexicon-based features.
adverbs. However, it performs very poorly in multi-
word verb recognition. In terms of standard parsing
accuracy, the pre-grouping approach has a very het-
erogeneous impact: Adverbial and Adjective Modi-
fier phrases tend to be more accurate; verbal kernels
and higher level constituents such as relative and
subordinate clauses see their accuracy level drop,
which shows that pre-recognition of MWE can have
a negative impact on general parsing accuracy as
MWE errors propagate to higher level constituents.
cat #gold BKY endo lex endo lex
(pre) (pre) (post) (post)
MWET 4 0.0 N/A N/A N/A N/A
MWA 22 37.2 +15.2 +21.3 +0.9 +4.7
MWV 47 62.1 -9.7 -13.2 +1.7 +2.5
MWD 24 62.1 +7.3 +10.2 0.0 +1.2
MWN 860 68.2 +4.0 +7.0 +1.7 +4.2
MWADV 357 72.1 +3.8 +6.4 +3.4 +4.1
MWPRO 31 84.2 -3.5 -0.9 0.0 0.0
MWP 294 79.1 +4.3 +5.8 +0.4 +1.1
MWC 86 85.7 +0.9 +3.7 +0.2 +1.0
Sint 209 47.2 -7.7 -8.7 +0.1 -0.2
AdP 86 48.8 +1.2 +3.0 +3.4 +5.1
Ssub 406 60.8 -1.1 -1.1 -0.3 -0.5
VPpart 541 63.2 -2.8 -2.1 -0.5 -1.6
Srel 408 74.8 -3.4 -3.5 -0.3 -0.6
VPinf 781 75.2 0.0 -0.1 -0.3 -0.3
COORD 904 75.2 +0.2 +0.4 -0.3 -0.4
PP 4906 76.7 -0.8 -0.3 +0.5 +0.7
AP 1482 74.5 +3.2 +3.9 +0.7 +1.6
NP 9023 79.8 -1.1 -0.8 +0.1 +0.2
VN 3089 94.0 -2.0 -1.0 0.0 0.0
Table 7: Evaluation by category with respect to BKY
parser. The BKY column indicates the F1 of BKY parser.
7 Conclusions and Future Work
In this paper, we evaluated two discriminative strate-
gies to integrate Multiword Expression Recognition
in probabilistic parsing: (a) pre-grouping MWEs
with a state-of-the-art recognizer and (b) MWE
identification with a reranker after parsing. We
showed that MWE pre-grouping significantly im-
proves compound recognition and unlabeled depen-
dency annotation, which implies that this strategy
could be useful for dependency parsing. The rerank-
ing procedure evenly improves all evaluation scores.
Future work could consist in combining both strate-
gies: pre-grouping could suggest a set of potential
MWE segmentations in order to make it more flexi-
ble for a parser; final decisions would then be made
by the reranker.
211
Acknowlegments
The authors are very grateful to Spence Green for his
useful help on the treebank, and to Jennifer Thewis-
sen for her careful proof-reading.
References
A. Abeille? and L. Cle?ment and F. Toussenel. 2003.
Building a treebank for French. Treebanks. In A.
Abeille? (Ed.). Kluwer. Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of French. In
ACL.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini and T. Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings of the DARPA Speech
and Natural Language Workshop.
T. Baldwin and K.S. Nam. 2010. Multiword Ex-
pressions. Handbook of Natural Language Process-
ing, Second Edition. CRC Press, Taylor and Francis
Group.
M. -H. Candito and B. Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. Proceedings of IWPT 2009.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05).
M. Constant and A. Sigogne. 2011. MWU-aware Part-
of-Speech Tagging with a CRF model and lexical re-
sources. In Proceedings of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World (MWE?11).
A. Copestake, F. Lambeau, A. Villavicencio, F. Bond,
T. Baldwin, I. Sag, D. Flickinger. 2002. Multi-
word Expressions: Linguistic Precision and Reusabil-
ity. Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002).
B. Courtois. 1990. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise. Vol. 87.
B. Courtois, M. Garrigues, G. Gross, M. Gross, R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein and R. Vive?s. 1997. Dic-
tionnaire e?lectronique DELAC : les mots compose?s bi-
naires. Technical Report. n. 56. LADL, University
Paris 7.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP?89.
S. Green, M.-C. de Marneffe, J. Bauer and C. D. Man-
ning. 2011. Multiword Expression Identification with
Tree Substitution Grammars: A Parsing tour de force
with French. In Empirical Method for Natural Lan-
guage Processing (EMNLP?11).
M. Gross. 1986. Lexicon Grammar. The Representa-
tion of Compound Words. In Proceedings of Compu-
tational Linguistics (COLING?86).
J. Lafferty and A. McCallum and F. Pereira. 2001. Con-
ditional random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML 2001).
T. Lavergne, O. Cappe? and F. Yvon. 2010. Practical Very
Large Scale CRFs. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntac-
tic parsing. In Methodologies and Evaluation of Mul-
tiword Units in Real-World Applications (MEMURA).
S. Paumier. 2011. Unitex 3.9 documentation.
http://igm.univ-mlv.fr/?unitex.
S. Petrov, L. Barrett, R. Thibaux and D. Klein. 2006.
Learning accurate, compact and interpretable tree an-
notation. In ACL.
C. Ramisch, A. Villavicencio and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identi-
fication. In LREC.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd Workshop on Very Large Corpora.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake and D.
Flickinger. 2002. Multiword Expressions: A Pain in
the Neck for NLP. In CICLING 2002. Springer.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10).
G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parsing accuracy. Natural Lan-
guage Engineering. Vol. 9 (4).
Seddah D., Candito M.-H. and Crabb B. 2009. Cross-
parser evaluation and tagset variation: a French tree-
bank study. Proceedings of International Workshop
on Parsing Technologies (IWPT?09).
W. Schuler, A. Joshi. 2011. Tree-rewriting models of
multi-word expressions. Proceedings of the Workshop
on Multiword Expressions: from Parsing and Genera-
tion to the Real World (MWE?11).
M. Silberztein. 2000. INTEX: an FST toolbox. Theoret-
ical Computer Science, vol. 231(1).
P. Watrin and T. Franc?ois. 2011. N-gram frequency
database reference to handle MWE extraction in NLP
applications. In Proceedings of the 2011 Workshop on
MultiWord Expressions.
212
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 83?91,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
An N-gram frequency database reference to handle MWE extraction in NLP
applications
Patrick Watrin
Centre for Natural Language Processing
Institut Langage et Communication
UCLouvain
patrick.watrin@uclouvain.be
Thomas Fran?ois
Aspirant F.N.R.S.
Centre for Natural Language Processing
Institut Langage et Communication
UCLouvain
thomas.francois@uclouvain.be
Abstract
The identification and extraction of Multiword
Expressions (MWEs) currently deliver satis-
factory results. However, the integration of
these results into a wider application remains
an issue. This is mainly due to the fact that
the association measures (AMs) used to detect
MWEs require a critical amount of data and
that the MWE dictionaries cannot account for
all the lexical and syntactic variations inherent
in MWEs. In this study, we use an alterna-
tive technique to overcome these limitations. It
consists in defining an n-gram frequency data-
base that can be used to compute AMs on-the-
fly, allowing the extraction procedure to effi-
ciently process all the MWEs in a text, even if
they have not been previously observed.
1 Introduction
Multiword Expressions (MWEs) are commonly
defined as ?recurrent combinations of words that
co-occur more often than expected by chance and
that correspond to arbitrary word usages? (Smadja,
1993, 143). Their importance in the field of natu-
ral language processing (NLP) is undeniable. Al-
though composed of several words, these sequences
are nonetheless considered as simple units with re-
gard to part-of-speech at the lexical as well as syn-
tactic levels. Their identification is therefore essen-
tial to the efficiency of applications such as parsing
(Nivre and Nilsson, 2004), machine translation (Ren
et al, 2009), information extraction, or information
retrieval (Vechtomova, 2005). In these systems, the
principle of syntactic or semantic/informational unit
is particularly important.
Although the identification and extraction of
MWEs now deliver satisfactory results (Evert and
Krenn, 2001; Pearce, 2002), their integration into
a broader applicative context remains problematic
(Sag et al, 2001). The explanations for this situation
are twofold.
1. The most effective extraction methods resort
to statistical association measures based on the
frequency of lexical structures. They, therefore,
require a critical amount of data and cannot
function properly from a simple phrase or even
from a short text.
2. Since the syntactic and lexical variability of
MWEs may be high, lexical resources learned
from a corpus cannot take it into account. The
coverage of these resources is indeed too limi-
ted when applied to a new text.
To address these two limitations, this article des-
cribes how an n-gram frequency database can be
used to compute association measures (AMs) effi-
ciently, even for small texts. The specificity of this
new technique is that AMs are computed on-the-fly,
freeing it from the coverage limitation that afflicts
more simple techniques based on a dictionary.
We start off focussing on our extraction method,
and more particularly on the process via which a
candidate structure is statistically validated (Section
2). This presentation principally aims to identify
the precise needs of a frequency database reference,
both in terms of the interrogation process and in the
type of information to be kept in the database. Then,
we will address various issues of storage and query
performance raised by the design of the frequency
83
database (Section 3). Finally, Section 4 reports the
results of our experiments and Section 5 concludes
and open up future perspectives.
2 Extraction process
Our extraction procedure is comparable to those
developed by Smadja (1993) and Daille (1995).
They use a linguistic filter upstream of the statisti-
cal estimation. Unlike purely statistical techniques,
this solution provides less coverage but greater ac-
curacy. It also allows us to assign a unique morpho-
syntactic category to each extracted unit (as well as
a description of its internal structure), which facili-
tates its integration into a more complex procedure.
Concretely, we first tagged the texts to clear any
lexical ambiguities 1. We then identified all MWE
candidates in the tagged text with the help of a li-
brary of transducers 2 (or syntactic patterns). Finally,
the list of candidates was submitted to the statistical
validation module which assigns an AM to each of
these.
2.1 Linguistic filters
In this study, we consider four basic types of
nominal structures 3 : adjective-noun (AN), noun-
adjective (NA), noun-preposition-noun (N prepN),
and noun-noun (NN), which are likely to undergo
three types of variations : modification (mainly ad-
verbial insertion and / or adjectival), coordination,
and juxtaposition (e.g. N prepN prepN, N prepNN,
etc). This enables us to identify a wide variety of
sequences that are labelled by XML tags which spe-
cify :
? the lexical heads of the various components ;
? the adjectival and prepositional dependencies ;
? any possible coordination.
This information can be exploited later to carry out
the syntactic decomposition of the extracted struc-
tures and also to limit the statistical validation to the
content words of each structure.
1. The tagging is done with the TreeTagger (Schmid, 1994).
2. To apply our transducers to the tagged text, we use Unitex
(Paumier, 2003). The output of the process is a file containing
only the recognized sequences.
3. As we work in the field of indexation, we limit our ex-
traction to nominal terms.
2.2 Statistical validation
Association measures are conventionally used
to automatically determine whether an extracted
phrase is an MWE or not. They are mathematical
functions that aim to capture the degree of cohesion
or association between the constituents. The most
frequently used measures are the log-likelihood ratio
(Dunning, 1993), the mutual information (Church
and Hanks, 1990) or the ?2 (Church and Gale, 1991),
although up to 82 measures have been considered by
Pecina and Schlesinger (2006). In this paper, we did
not aim to compare AMs, but simply to select some
effective ones in order to evaluate the relevance of a
reference for MWE extraction.
However, association measures present two main
shortcomings that were troublesome for us : they are
designed for bigrams, although longer MWEs are
quite frequent in any corpus 4, and they require the
definition of a threshold above which an extracted
phrase is considered as an MWE. The first aspect is
very limiting when dealing with real data where lon-
ger units are common. The second may be dealt with
some experimental process to obtain the optimal va-
lue for a given dataset, but is prone to generalization
problems. In the next two sections, we present the
strategies we have used to overcome these two limi-
tations.
2.2.1 Beyond bigrams
A common way to go beyond the bigram limita-
tion is to compute the AMs at the bigram level and
then use the results as input for the computation of
higher order AMs (Seretan et al, 2003). However,
our preliminary experimentations have yielded un-
satisfactory results for this technique when it is ap-
plied to all words and not to heads only. This is pro-
bably a side effect of high frequency bigrams such
as preposition-determiner (prep det) in French.
Another strategy explored by Silva and
Lopes (1999) is the fair dispersion point normaliza-
tion. For a given n-gram, which has n?1 dispersion
points that define n ? 1 "pseudo-bigrams", they
compute the arithmetic mean of the probabilities of
the various combinations rather than attempting to
pick up the right point. This technique enables the
4. In our test corpus (see Section 4), 2044 MWEs out of
3714 are longer than the bigrams.
84
authors to generalize various conventional measures
beyond the bigram level. Among these, we selected
the fair log-likelihood ratio as the second AM for
our experiments (see Equation 1), given that the
classic log-likelihood ratio has been found to be
one of the best measures (Dunning, 1993; Evert and
Krenn, 2001).
LogLik f (w1 ? ? ?wn) = 2? logL(p f 1,k f 1,n f 1)
+ logL(p f 2,k f 2,n f 2)
? logL(p f ,k f 1,n f 1)
? logL(p f ,k f 2,n f 2) (1)
where
k f 1 = f (w1 ? ? ?wn) n f 1 = Avy
k f 2 = Avx? k f 1 n f 2 = N ?n f 1
Avx = 1
n?1
i=n?1
?
i=1
f (w1 ? ? ?wi)
Avy = 1
n?1
i=n
?
i=2
f (wi ? ? ?wn)
p f = k f 1+k f 2N p f 1 = k f 1n f 1 p f 2 = k f 2n f 2
and N is the number of n-grams in the corpus.
Silva and Lopes (1999) also suggested an AM of
their own : the Symmetrical Conditional Probabi-
lity, which corresponds to P(w1|w2)P(w2|w1) for a
bigram. They defined the fair dispersion point nor-
malization to extend it to larger n-grams, as shown
in Equation 2.
SCPf ([w1 ? ? ?wn]) =
p(w1 ? ? ?wn)2
Avp
(2)
where w1 ? ? ?wn is the n-gram considered and Avp is
defined as follows :
Avp = 1
n?1
i=n?1
?
i=1
p(w1 ? ? ?wi)? p(wi+1 ? ? ?wn) (3)
Finally, we considered a last AM : the Mutual Ex-
pectation (Dias et al, 1999) (see Equation 4). Its
specificity lies in its ability to take into account non-
contiguous MWEs such as ?to take __ decision? or
?a __ number of?, which can also be realized using
the heads (see above).
ME(w1 ? ? ?wn) =
f (w1 ? ? ?wn)? p(w1 ? ? ?wn)
FPE
(4)
where FPE is defined as follows :
FPE = 1
n
[p(w2 ? ? ?wn)+
n
?
i=2
p(w1 ? ? ? w?i ? ? ?wn)] (5)
It should be noted that the expression w1 ? ? ? w?i ? ? ?wn,
where the ? indicates an omitted term, represents all
the n (n-1)-grams the candidate MWE comprises.
FPE is then able to estimate the ?glue? between
all the constituents separated by a gap, but this ne-
vertheless requires a more complex string matching
process.
To summarize, we have selected the three follo-
wing association measures for n-grams : the fair log-
likelihood ratio, SCPf , and ME. Their efficiency is
further discussed in Section 4.
2.2.2 Selection of MWEs
The second problem that arises when one wants to
locate all the MWEs in a given text is the classifica-
tion criterion. For the log-likelihood ratio, which fol-
lows a chi-square distribution once it is transformed
as ?2? log?, a first solution is to base the decision
on the p-value. However, significance tests become
highly unreliable for large corpora, since the high
frequencies produce high scores for the chi-square
and all phenomena then appear significant (Kilgar-
riff, 2005).
A second technique commonly used in the MWE
literature is to select a threshold for the AM above
which an analyzed phrase is considered as an MWE.
Again, this threshold depends on the size of the cor-
pus used and cannot be fixed once and for all for
a specific AM. It must be obtained empirically for
each application of an MWE extractor to a new text
or to a new domain. In order not to resort to a thres-
hold, (Silva et al, 1999) suggested the LocalMax al-
gorithm that selects MWEs whose AMs are higher
than those of their neighborhood. In other words, a
given unit is classified as an MWE if g(w1 ? ? ?wn),
the associative function, is a local maximum.
In our case, since the notion of reference implies
a large corpus and high frequencies, we rejected
the first of these three approaches. We experimen-
ted with the second and third and show in Section 5
how the use of a reference could partially solve the
threshold issues.
85
3 Reference Building
The integration of MWEs in an NLP system is
usually done via a dictionary. MWEs are then re-
garded as a sequence of simple words separated by
spaces (Sag et al, 2001). As a result, their lexical
and syntactic structure is fixed and cannot be used
to take into account variation at this level.
Several methods have been proposed to overcome
this limitation. Nerima et al (2006) and Sag et
al. (2001) associate each MWE with a feature struc-
ture specifying the nature of units and the type of
fixedness. This approach requires a manual valida-
tion of the features when inserting them into the
dictionary. Watrin (2007) considers a simpler tech-
nique that consists in identifying, for each type of
structure, all the possible insertion points and spe-
cifying the lexical and syntactic nature of possible
modifiers. In this case, each MWE takes the form of
a regular expression formalizing all possible varia-
tions from the canonical form.
Both solutions enable to consider more MWEs
but fail to express all possible variations. For ins-
tance, phenomena such as coordination or juxta-
position do not seem to be taken into account by
the authors mentioned above including Nerima et
al. (2006). Moreover, they limit lexical variations to
a finite set of canonical structures that have been en-
countered and are therefore unable to recognize new
candidates.
The notion of reference which we define in this
article aims to overcome these two limitations. Ra-
ther than providing a list of MWEs that are pre-
computed on a corpus, we suggest storing the in-
formation needed to calculate various AMs within
a database. Hence, we no longer restrict MWEs to
a finite set of lexical entries but allow the on-the-fly
computation of AMs for any MWE candidate, wha-
tever the size of the input text.
3.1 Implementation details
From a computational point of view, this idea in-
volves the compression of a large number of lexi-
cal structures of order N as well as their absolute
frequency. Moreover, the calculation of the various
AMs considered in this study also requires the fre-
quencies of all structures of order n, strictly lower
than N (0 < n < N). The second type of informa-
tion can however be inferred from the frequency of
the structures of order N, provided the storage and
questioning system is efficient enough for real-time
applications. The need for efficiency also applies to
queries related to the ME measure or the LocalMax
algorithm that partly involve the use of wildcards.
This type of search tool can be efficiently im-
plemented with a PATRICIA tree (Morrison, 1968).
This data structure enables the compression of n-
grams that share a common prefix and of the nodes
that have only one child. The latter compression is
even more effective as most of the n-grams have a
unique suffix (Sekine, 2008). Beyond the compres-
sion that this structure allows, it also guarantees a
very fast access to data insofar as a query is a simple
tree traversal that can be done in constant time.
In order to further optimize the final data struc-
ture, we store the vocabulary in a table and associate
an integer as a unique identifier for every word. In
this way, we avoid the word repetition (whose size
in memory far exceeds that of an integer) in the tree.
Moreover, this technique also enables to speed up
the query mechanism, since the keys are smaller.
We derived two different implementations of this
structure. The first stores the data directly in me-
mory. While it enables easy access to data, the num-
ber of n-grams that can be stored is limited by the
capacity of the RAM. Therefore, in order to take a
huge number of n-grams into account, we also im-
plemented a ?disk? version of the tree.
Finally, in order to treat wildcard queries nee-
ded by the ME and the LocalMax, we enhanced our
structure with a set of indexes to improve access to
each word, whatever its depth within the tree. Ob-
viously, this mechanism might not be robust enough
for a system multiplying the number of wildcards,
but it is perfectly suited to the needs of an MWEs
extraction process.
3.2 References used
Once the computational aspects of reference buil-
ding have been dealt with, a corpus from which to
populate the database needs to be selected. This as-
pect raises two issues : the size and the nature of the
corpus used. Dunning (1993) has demonstrated that
the size of the corpus from which MWEs are extrac-
ted matters. On the other hand, common characteris-
tics of a corpus, such as its register, the contempora-
86
Reference # 5-Grams # Nodes
500 K 500,648 600,536
1000 K 1,001,080 1,183,346
5000 K 5,004,987 5,588,793
Google 1,117,140,444 62,159,203
TABLE 1: Number of 5-grams and nodes in the references
used
neity of its language or the nature of the topics co-
vered, may impact the performances of a reference
when used on a text with different characteristics.
Given these issues, four corpora were selected (cf.
Table 1). The first three are made up of articles pu-
blished in the Belgian daily newspaper Le Soir in
2009, with 500K, 1000K and 5000K words respec-
tively. They share many characteristics with our test
corpus. The last corpus is made up of the largest
amount of n-grams publicly available for French :
the Google 5-grams 5 (Michel et al, 2011). Its size
reaches 1T words 6, and its coverage in terms of to-
pic and register is supposedly wider than corpora of
newspaper articles only. In a sense, the Google re-
ference may be viewed as an attempt to a universal
reference.
4 Evaluation
Most evaluations of MWE extraction systems are
based on human judgments and restrict the valida-
tion process to the n-best candidates. Inevitably par-
tial, this method is unable to estimate performance
in terms of recall. To overcome these limitations,
we use the evaluation method described by Evert
and Krenn (2001). They propose an automatic me-
thod that consists in computing both recall and pre-
cision using various n-best samples. It involves the
formation of a golden standard (i.e. a list of MWEs
manually identified in a corpus) and a sorted list of
MWEs extracted automatically by applying AM on
the same corpus. The recall and precision rates are
therefore calculated by comparing the n-best (where
n increases from 0 till n in steps of x) to the golden
5. For the purposes of comparison, we also limited the size
of the n-grams indexed in Le Soir to 5 words.
6. In order to model a contemporary language, we only kept
the frequencies observed in texts written between 2000 and
2008.
standard list 7.
4.1 The test corpus
In this study, we use the corpus described in La-
porte et al (2006). It is a French corpus in which all
MWEs have been manually annotated. It consists of
two sub-corpora :
? the transcription, in a written style, of the Oc-
tober 3rd and 4th, 2006 meetings of the French
National Assembly (FNA), and
? the complete text of Jules Verne?s novel
"Around the World in 80 Days", published in
1873 (JV).
These two sub-corpora respectively contain 98,969
and 69,877 words for a total of 3,951 and 1,103
MWEs 8. We limit our evaluation to the FNA cor-
pus in order to keep data consistent both in terms
of register and time. We assume that these two va-
riables have a direct impact on the use of MWEs, a
hypothesis that seems to be confirmed by the rate of
MWEs in both sub-corpora.
4.2 Extractor Parameters
Before evaluating the performance of each of the
above mentioned references, we first assessed the in-
fluence of the various parameters involved in the ex-
traction process and which affect the performance
of the AMs. These parameters are the LocalMax,
the smoothing technique, the lemmatization of the
MWE constituents (LEMMA) 9 and the head-driven
validation (HDV) 10. To select the optimal parame-
ters for our extractor, we established an additional
reference (1000K words from Le Soir).
7. We build these lists from MWE types to avoid introdu-
cing a bias in the evaluation process. Well-recognised high fre-
quency MWEs might indeed gloss over poorly recognised low-
frequency MWEs.
8. These occurrences correspond to 1,384 MWE types for
the FNA corpus and 521 for the JV corpus.
9. The lemmatization of the MWE constituents is based on
the assumption that the inflexion of the lemmas implies a dis-
persal of the frequency mass (the overall frequency of a lemma
is split between its inflected forms) that may affect the behavior
of the AMs.
10. The HDV aims to focus on the lexical heads of the MWE
candidates. Therefore, function words (prepositions, conjunc-
tions, etc.) are ignored and replaced by wildcards in the queries
sent to the reference in order to keep the distance information.
For instance, from the sequence ministre de l?agriculture (Mi-
nister for Agriculture), we derive the form ministre * * agricul-
ture.
87
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
Measures -- Precision
Fair Log-Likelihood
Mutual Expectation
Symmetric Conditional Probability
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
Measures -- Recall
Fair Log-Likelihood
Mutual Expectation
Symmetric Conditional Probability
FIGURE 1: Evaluation of AMs
The first step of this selection procedure was to
define a baseline. For this purpose, we compared
the precision and recall rates of our three AMs (see
Figure 1) and kept only the best, namely the log-
likelihood ratio, for the rest of our experiments.
While the ME provides better precision for the top
five percent of the extracted units, the log-likelihood
ratio appears more reliable in that it maintains its
efficiency over time (for recall as well as precision).
The SCP, for its part, displays more stable results but
does not reach sufficient precision.
On the basis of this baseline, we then separately
compared the contribution of each of the four para-
meters. Results are reported in Figure 2 and detailed
in the following subsections.
4.2.1 The LocalMax
Figure 2 shows that the LocalMax significantly
improves the precision of the extraction. It emerges
as the most relevant parameter at this level. Howe-
 10
 20
 30
 40
 50
 60
 70
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
Parameters -- Precision
Lemmatization
Add Text Smoothing
LocalMax
Head
Baseline
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
Parameters -- Recall
Lemmatization
Add Text Smoothing
LocalMax
Head
Baseline
FIGURE 2: Evaluation of the parameters
ver, unlike other parameters, its application directly
affects the recall that falls below our baseline. This
may not be a problem for certain applications. In our
case, we aim to index and classify documents. The-
refore, while we can accommodate a lower preci-
sion, we cannot entirely neglect the recall. We thus
abandoned this parameter which, moreover, indubi-
tably increases the processing time in that it requires
the use of approximate matching (see Section 3.1).
4.2.2 The Add-text smoothing
Smoothing is another aspect worthy of considera-
tion. No matter how large the reference used is, it
will never constitute more than a subset of the lan-
guage. Therefore, it is necessary to find a solution
to estimate the frequency of unobserved n-grams.
For the baseline, we used a simple "add-one? (or
Laplace) smoothing (Manning and Sch?tze, 1999)
which presents a severe flaw when the size of the n-
grams to smooth increases : the normalization pro-
88
cess discounts too much probability mass from ob-
served events.
We therefore compare this simple method with
another one we consider more ?natural? : the ?add-
text? smoothing that adds the text to process to the
reference. We view this method as more natural to
the extent that it simulates a standard MWE extrac-
tion process. In this case, the reference complements
the frequency universe of the input corpus as if it for-
med a homogeneous whole. Figure 2 demonstrates a
clear superiority of the second smoothing procedure
over the first one which was therefore discarded.
4.2.3 Lemmatization and HDV
The lemmatization and HDV follow a similar
curve with regard to precision, although HDV is bet-
ter for recall. Nonetheless, this difference only ap-
pears when precision falls below 35%. This does
not seem sufficient to reject the lemmatization pro-
cess whose computation time is significantly lower
than for the HDV. We therefore limit the use of this
last parameter to the reference built from Google
whose n-grams cannot be lemmatized due to lack of
context. 11
4.3 Evaluation of the references
The estimation of the parameters allowed us to es-
tablish a specific evaluation framework. Two sets of
parameters were defined depending on whether they
apply to Google (ATS + HDV) or to the references
built from Le Soir (ATS + LEMMA). From a prac-
tical standpoint, we limited the MWE extraction to
nominal units of size inferior to five in order to meet
the characteristics of our test corpus (the annotations
of which are limited to nominal sequences), on the
one hand, and to allow comparability of results on
the other hand (the n-grams from Google do not ex-
ceed the order 5).
Initially, we considered the extraction of MWEs
in the whole evaluation corpus. Results displayed in
Figure 3 provide an advantage over the use of a refe-
rence with respect to the extraction carried out on the
test corpus only. In addition, we see a clear improve-
ment in performance with respect to that obtainable
with a dictionary of MWEs. 12
11. References constructed on the basis of the newspaper Le
Soir have been reindexed from a lemmatized text.
12. The MWE dictionary used in this experiment was ini-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
References -- Precision
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
References -- Recall
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
FIGURE 3: Evaluation on the 100K Corpus
In a second step, we wanted to test the efficiency
of our references in the more adverse context of a
short text. We randomly selected 3K words of our
test corpus to simulate a short text while maintai-
ning a sufficient number of MWEs (i.e. 151 nominal
MWEs). Results shown in Figure 4 further confirm
our first experience and validate our concept of a re-
ference in a real application context.
Beyond validating the use of a frequency base,
these results also confirm the general idea that the
size of the corpus used for the reference matters. The
differences between the references of 500K, 1000K
and 5000K words showed a continuous improve-
ment both in precision and recall. The results obtai-
ned with the Google reference are more surprising,
since they do not meet that growing trend. Howe-
ver, given the number of errors that those n-grams
contain (mainly due to the OCR-ization and tokeni-
tially derived from the corpus of 5000K words used to build the
corresponding reference.
89
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
MWE (%)
References (3K) -- Precision
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
 0
 10
 20
 30
 40
 50
 60
 0  10  20  30  40  50  60  70  80  90  100
R
ec
al
l (%
)
MWE (%)
References (3K) -- Recall
500K Words
1000K Words
5000K Words
Google (1T Words)
Text Only (3K)
External Dictionary
FIGURE 4: Evaluation on the 3K Corpus
zation processes), the result remains satisfactory. It
even confirms to some extent the importance of size
in the sense that preprocessing errors are being miti-
gated by the global mass of the frequencies.
5 Conclusion and perspectives
In this paper, we presented an MWE extraction
system based on the use of frequency references. We
have shown that its use enables MWE extraction on
short texts with performances that are at least com-
parable to those achieved by standard solutions and
far superior to solutions based on the use of MWE
dictionaries.
Moreover, as this system has been integrated wi-
thin an indexing engine, various issues were rai-
sed, some of which constitute avenues for future re-
search. First, since our indexer aims at the identifi-
cation of entities and terms specific to a given spe-
cialty area, the question of data representativeness
is of particular importance. It is not clear to what
MWE 500 K 1000 K 5000 K Google
m?me
groupe
0.73 1.44 3.85 1,746.03
nouveaux
instruments
3.81 3.3 49.83 2,793.65
lettres de
noblesse
33.99 52.43 232.51 27,202.17
TABLE 2: Examples of MWEs candidates whose log-
likelihood ratio is not significant on a small corpus and
becomes extremely significant on a large corpus. They
are compared to the score of an actual MWE.
extent a given reference can be applied to various
types of texts. We only noticed that the Google refe-
rence, whose features were less similar to the test
corpus, nevertheless yielded satisfactory results in
comparison with our other references that better fit-
ted the test corpus features.
In addition, our results show that the threshold is-
sue remains relevant. Although the LocalMax seems
to allow better discrimination of the MWE candi-
dates, it is not selective enough to keep only the ac-
tual MWEs. On the other hand, as the size of the
references increases, some results of the AMs based
on the log-likelihood ratio reach high values that can
no longer be interpreted by a chi-square significance
test (see Table 2).
We believe that our references offer an interes-
ting perspective to face this problem. The stability of
their frequencies makes it possible to define a thre-
shold corresponding to a specific percentage of pre-
cision and recall (set according to the needs of a gi-
ven application). Therefore, as long as the size of
the analyzed texts remains limited ? which can be
controlled ?, the efficiency of this threshold should
remain constant. Further experimentations on this
aspect are however required to determine to what
extent this assumption stands true as the size of the
analyzed texts grows.
References
K.W. Church and W.A. Gale. 1991. Concordances for
parallel text. In Proceedings of the Seventh Annual
Conference of the UW Centre for the New OED and
Text Research, pages 40?62.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1) :22?29.
90
J. da Silva and G.P. Lopes. 1999. A local maxima me-
thod and a fair dispersion normalization for extracting
multi-word units from corpora. In Sixth Meeting on
Mathematics of Language.
B. Daille. 1995. Combined approach for terminology
extraction : lexical statistics and linguistic filtering.
Technical report, Lancaster University.
G. Dias, S. Guillor?, and J.G.P. Lopes. 1999. Language
independent automatic acquisition of rigid multiword
units from unrestricted text corpora. Proceedings of
the 6th Conference on the Traitement Automatique des
Langues Naturelles (TALN1999), pages 333?339.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational linguistics,
19(1) :61?74.
S. Evert and B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Procee-
dings of the 39th Annual Meeting on Association for
Computational Linguistics, pages 188?195.
A. Kilgarriff. 2005. Language is never ever ever random.
Corpus linguistics and linguistic theory, 1(2) :263?
276.
E. Laporte, T. Nakamura, and S. Voyatzi. 2006. A french
corpus annotated for multiword expressions with ad-
verbial function. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC) : Linguis-
tic Annotation Workshop, pages 48?51.
C.D. Manning and H. Sch?tze, editors. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray,
The Google Books Team, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, S. Pinker, M.A. No-
wak, and E.L. Aiden. 2011. Quantitative analysis
of culture using millions of digitized books. Science,
331(6014) :176?182.
D.R. Morrison. 1968. PATRICIA?practical algorithm
to retrieve information coded in alphanumeric. Jour-
nal of the ACM, 15(4) :514?534.
L. Nerima, V. Seretan, and E. Wehrli. 2006. Le pro-
bl?me des collocations en TAL. Nouveaux cahiers de
linguistique fran?aise, 27 :95?115.
J. Nivre and J. Nilsson. 2004. Multiword units in syn-
tactic parsing. In Proceedings of LREC-04 Workshop
on Methodologies & Evaluation of Multiword Units in
Real-world Applications, pages 37?46.
S. Paumier. 2003. De la reconnaissance de formes lin-
guistiques ? l?analyse syntaxique. Ph.D. thesis, Uni-
versit? de Marne-la-Vall?e.
D. Pearce. 2002. A comparative evaluation of colloca-
tion extraction techniques. In Proc. of the 3rd Inter-
national Conference on Language Resources and Eva-
luation (LREC 2002), pages 1530?1536.
P. Pecina and P. Schlesinger. 2006. Combining associa-
tion measures for collocation extraction. In Procee-
dings of the 21th International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (CO-
LING/ACL 2006), pages 651?658.
Z. Ren, Y. L, J. Cao, Q. Liu, and Y. Huang. 2009. Im-
proving statistical machine translation using domain
bilingual multiword expressions. In Proceedings of
the Workshop on Multiword Expressions : Identifica-
tion, Interpretation, Disambiguation and Applications,
pages 47?54.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Fli-
ckinger. 2001. Multiword expressions : A pain in the
neck for NLP. In In Proc. of the 3rd International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 1?15.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
volume 12. Manchester, UK.
S. Sekine. 2008. A linguistic knowledge discovery tool :
Very large ngram database search with arbitrary wild-
cards. In COLING : Companion volume : Demonstra-
tions, pages 181?184.
V. Seretan, L. Nerima, and E. Wehrli. 2003. Extrac-
tion of Multi-Word Collocations Using Syntactic Bi-
gram Composition. In Proceedings of the 4th In-
ternational Conference on Recent Advances in NLP
(RANLP2003), pages 424?431.
J. da Silva, G. Dias, S. Guillor?, and J. Pereira Lopes.
1999. Using localmaxs algorithm for the extraction
of contiguous and non-contiguous multiword lexical
units. Progress in Artificial Intelligence, pages 849?
849.
F. Smadja. 1993. Retrieving collocations from text :
Xtract. Computational Linguistics, 19 :143?177.
O. Vechtomova. 2005. The role of multi-word units
in interactive information retrieval. In D.E. Losada
and J.M. Fern?ndez-Luna, editors, ECIR 2005, LNCS
3408, pages 403?420. Springer-Verlag, Berlin.
P. Watrin. 2007. Collocations et traitement automatique
des langues. In Actes du 26e Colloque international
sur le lexique et la grammaire, pages 1530?1536.
91
