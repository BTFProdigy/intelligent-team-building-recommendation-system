Introduction to the 3rd ROMAND workshop on Robust Methods in
Analysis of Natural Language Data
Vincenzo Pallotta
Swiss Federal Institute of Technology ? Lausanne, CH
University of California ? Berkeley, CA
Vincenzo.Pallotta@epfl.ch
Amalia Todirascu
University of Troyes - France
University "I.A. Cuza" of Iasi - Romania
Amalia.Todirascu@utt.fr
Robustness in Computational Linguistics has been recently recognized as a central issue for the
design of interactive applications based on natural language communication. If a failure of the
system can be acceptable in batch applications requiring a human intervention, an on-line system
should be capable of dealing with unforeseen situations in a more flexible way. When we talk about
system failure we do not think at inherent program failures like infinite loops or system exception,
we consider, rather, failures related to the processing of the input and its assimilation in the system's
knowledge base. A failure of this kind means simply that the system does not "understand" the
input. The automated analysis of natural language data has become a central issue in the design of
Intelligent Information Systems. Processing unconstrained natural language data is still considered
an AI-hard task. However, various analysis techniques have been proposed in order to address
specific aspects of natural language. In particular, recent interest has been focused on providing
approximate analysis techniques, assuming that when perfect analysis is not possible, partial results
may be still very useful.
Interpretation of natural language data is a subjective cognitive process. Its formalisation could be
either a straightforward or a hard task, depending on the perspective taken. The human ability to
interpret language is the result of thousands of years of evolution and cultural development. We, as
humans, are capable of mapping surface language forms into meaning representations, but we can
only observe the final result of this process without exactly knowing what happens in our brain. In
contrast, we can model understanding in an abstract domain where the process of reaching the
meaning is decomposed at least in two parts. We are interested in establishing to what degree:
? the mapping is sound (i.e. if the competence model enables us to extract correct meanings);
? the mapping is complete (i.e. if the competence model enables us to deal with all the
language phenomena).
These two measures are fairly general, but in specific applications they might correspond to the
classic evaluation metrics of Information Retrieval (i.e. precision and recall).
Even human interpretation of language is not infallible. For instance, in areas where people lack
context, or have different views on the context, people can fail to understand each other and can
have different opinions on utterances' interpretation. Apparently, it is always possible to provide an
interpretation of any kind of data. Actually, one not always provides the right or the best
interpretation among the possible ones. This happens for humans and, why not, for artificial
systems. When switching to artificial systems, what is worth considering is the human ability to
provide different degrees of approximate interpretations, ranging from the full understanding to the
complete ignorance. In addition, humans are able to overcome their limitations by their learning
capabilities. In the interpretation process, the lack of knowledge, the uncertainty, vagueness,
ambiguity, and misconception should be explicitly represented and considered at a meta-level in
order to handle linguistic applications 'robustly'.
There are many ways in which the topic of robustness may be tackled: as a competency problem, as
a problem of achieving interesting partial results, as a shallow analysis method, etc. What these
approaches have in common is that the simple (rigid) combination of 'complete' analysis modules at
different linguistic levels does not guarantees the system's robustness. Rather, robustness must be
considered as a system-wide concern. We believe that the problem of robustness in NLP may be
tackled by adopting the following two complementary approaches:
1. as an engineering `add-on': completing an existing system with additional features in order
to overcome the problem of  its inability to cope with real-world data;
2. as a basic element of the underlying language theories: extending them by assuming that the
understanding of the domain can be incomplete.
Both approaches may be effective under certain circumstances. We thus propose to consider two
different perspectives on the role of robustness in software architectures for natural language
processing and understanding, namely: robustness "in the small" and "in the large".
With Robustness "in the small" we mean the robustness in language analysis is achieved at the
individual linguistic levels, such as the morpho-syntactic analysis, semantic interpretation,
conversational analysis, dialogue acts recognition, anaphora resolution, and discourse analysis.
With Robustness "in the large" we mean the robustness achieved in integrated NLP/NLU
architectures possibly implementing hybrid approaches to language analysis, and incorporating the
different methods into a competitive/cooperative system.
ROMAND 2004 is the third of a series of workshops aimed at bringing together researchers that
work in fields like artificial intelligence, computational linguistics, human-computer interaction,
cognitive science and are interested in robust methods in natural language processing and
understanding. Theoretical aspects of robustness in Natural Language Processing (NLP) and
Understanding (NLU) are concerned by the workshop?s theme, as well as engineering and industrial
experiences.
This volume contains an extended abstract of the invited talk and 11 papers selected by peer review
out of 16 submissions. The accepted papers cover topics related to robust syntactic parsing, robust
semantic parsing and applications using robust analysis methods (semantic tagging, information
extraction, question answering, document clustering).
The third edition of ROMAND workshop features an exceptional invited speaker. Frank Keller
from Edinburgh University accepted to talk about robustness aspects in cognitive, computational
and stochastic models of human parsing surveying and discussing the weaknesses and strengths of
most recent advanced theories.
The papers dedicated to robust syntactic parsing methods cover topics as combinations of statistical
and deep-linguistic syntactic analysis, as well as a parser's evaluation. The paper proposed by G.
Schneider, J. Dowdall and F. Rinaldi, ?A Robust and Hybrid Deep-Linguistic Theory Applied to
Large-Scale Parsing?, presents an efficient state-of-the-art hybrid parser combining statistical and
rule-based parsing as well as shallow and deep parsing using a combination of phrase-structure and
functional dependency grammars. The paper ?Syntactic parser combination for improved
dependency analysis? describes how F. Brunet-Manquant improves parsing efficiency in building
complex dependency structures by combining the results of the three concurrent parsers: the
Incremental Finite-State Parser, the GREYC parser combining tagging methods to build non-
recursive chunks, and the Xerox Incremental Parser. A difficult problem of evaluation as well as the
evaluation of the GETARUNS system is proposed and discussed in Delmonte?s paper ?Evaluating
GETARUNS Parser with GREVAL Test Suite?.
Robustness plays an important role in semantic interpretation. Semantic interpretations are
generated by robust syntactic parsers output in specific representation languages: as logical
formulae in Minimal Recursion Semantics or as semantic hypergraphs in Unified Network
Language respectively in the papers ?A step towards incremental generation of logical forms? by L.
Coheur, N. Mamede, G. B?s, and ?Using an incremental robust parser to automatically generate
semantic UNL graphs? by N. Gala. Existing knowledge bases (FrameNet and WordNet) are
exploited to build complex semantic structures directly from free texts, as proposed in ?An
Algorithm for Open Text Semantic Parsing? by L. Shi and R. Mihalcea. J. Bryant in his paper
?Recovering Coherent Interpretations Using Semantic Integration of Partial Parses? presents a
robust semantic parser for Embodied Construction Grammars used to reconstruct full semantic
interpretations from semantic chunks in the framework of psycholinguistics studies on language
acquisition.
A direct use of robust analysis methods is featured by several Information Extraction applications:
Part of Speech tagging, building knoweldge maps from texts, Question-Answering, and document
clustering. Robust morphological analysis for POS and restricted semantic tagging in Bulgarian is
achieved by for learning robust ending guessing rules in the P. Nakov and E. Paskaleva?s paper
?Robust Ending Guessing Rules with Application to Slavonic Languages?. Extraction of
Associative Term Networks from texts including co-occurences of several content words sharing
similar contexts is the goal of the paper ?Knowledge Extraction Using Dynamical Updating of
Representation? by A. Dragoni, L. Lella, G. Tascini, and W. Giordano. Answer Validation is an
important module of a Question-Answering system for which a method exploiting co-occurrence
frequencies of keywords extracted from Web documents is proposed in the paper ?Answer
Validation by Keyword Association? by M. Tonoike, T. Utsuro, and S. Sato. Document clustering
algorithms could help the users for Web browsing, where several document representations are
compared (as POS or as WordNet synsets) and exploited by an efficient clustering algorithm
discussed in the paper ?WordNet-based text document clustering? by J. Sedding and D. Kazakov.
We believe that the output of the ROMAND 2004 workshop will contribute to a better
understanding of various aspects of robust analysis in Natural Language Processing and
Understanding by presenting relevant advances in morphology, syntax, semantics, pragmatics, and
evaluation, as well as examples of large-scale Information-Extraction applications relying on robust
NLP/NLU techniques and architectures.
We would like to thank all the people who have supported the 3rd edition of ROMAND, in
particular the authors who submitted their works, the members of the scientific program committee,
the COLING workshop program committee, and the local organizing staff.  

A Weighted Robust Parsing Approach to Semantic Annotat ion 
Hatem Ghorbe l  and  V incenzo  Pa l lo t ta  
L ITH-MEDIA  group  
Swiss Federa l  Ins t i tu te  of  Techno logy  
IN Ecub lens ,  1015 Lausanne,  Swi tzer land  
{ ghorbel, pallotta} @di. epfl. ch 
Abst ract  
This paper proposes a grammar-based approach to 
semantic annotation which combines the notions of 
robust parsing and fuzzy grammars. We present an 
overview of a preliminary research aimed to gener- 
alize some results from a recent project on interac- 
tion through speech with information systems where 
techniques based on the above notions have been 
successfully applied. The goal of the article is to 
give a development environment to linguists? 
1 In t roduct ion  
In this article we are mainly interested in seman- 
tic annotation (Sperberg-McQueen and Burnard, 
1994). We are considering the Information Extrac- 
tion (I.E.) problem as a semantic annotation prob- 
lem: extracting information is finding the relevant 
terms that contribute to describe an appropriate se- 
mantic structure of the text. Some of the most ira-. 
portant works in I.E. have been dealing with domain 
dependent documents like (Moll et al, 1998; Hobbs 
et al, 1996). Both systems employ complex analysis 
schemas. Assigning semantic field tags is in general 
a difficult task. This is due at least to the crucial 
need of the domain knowledge and also of the lin- 
guistic knowledge. Our approach considers that for 
some specific domains a semantic annotation can be 
achieved by a light parsing of the text which is based 
on the user of certain cue-words as a heuristic for de- 
scribing its semantic structure. 
1.1 h case s tudy  in query  generat ion  
The availability of a large collection of annotated 
telephone calls for querying the Swiss phone-book 
database (i.e the Swiss French PolyPhone corpus) al- 
lowed us to experiment our recent findings in robust 
text analysis obtained in the context of the Swiss 
National Fund research project ROTA (Robust Text 
Analysis), and in the recent Swisscom funded project 
ISIS (Interaction through Speech with Information 
Systems) 1 (Chappelier et al, 1999). This database 
contains 4293 simulated recordings related to the 
1The final report of ISIS project is available at 
ht tp: / / l i thwww.epf l .ch/ -pa l lot ta / is is .html  
"111" Swisscom service calls. For instance a query 
call like: 
Bonjour j'aimerais un num4ro de 
t~l~phone & Saignelegier c'est 
Mottaz m o deux ta z Monique rue du 
printemps num~ro quatre 
would produce the following query frame filling for 
the Swiss Phone-book database: 
Nom de famille / Firme: MOTTAZ 
Pr~nom / Autres informations: MONIQUE 
Rue, num~ro: rue du PRINTEMPS, 4 
NPA, localitY: SAIGNELEGIER. 
The goal of semantic annotation is to provide a tree 
structure which can be superposed over the flat sen- 
tence. This structure can be supported by a PRO- 
LOG compound term consisting of "tag" functors 
and list arguments. Moreover this same structure 
can also be supported by the SGML structural rep- 
resentation. The translation between the two models 
is an intuitive task and an example of such transla- 
tion is provided by the two following corresponding 
representations for a possible query call schema. 
PROLOG:  
S(\[ .... announce(\[...\] ), 
query ( \[ . . . .  
name ( \[ . . . .  
f am_name ( C.. ? \] ) . . . . .  
first_name(\[...\]), 
...\]) ..... 
address  ( \[ . . . .  
s t reet ( \ [ . . . \ ] )  . . . . .  
city(\[...\]), 
. . . \ ] ) ,  
. . . \ ] )  
. . . \ ] ) .  
SGML:  
<announce> 
</~ounce> 
<query> 
<name> 
19 
? . ? 
<fam_name> 
<first_name> 
</name> 
<address> 
<street> 
<city> 
</address> 
? .. 
</query> 
. . .  < / ram_name> 
? . .  < / f i r s t _name> 
... </street> 
. . .  </city> 
1.1.1 Processing phases  
The processing of the corpus data is performed at 
various linguistic levels by modules organized into 
a pipeline? Each module assumes as input the out- 
put of the preceding module? The main goal of this 
architecture is to understand how far it is possible 
to go without using any kind of feedback and in- 
teractions among different linguistic modules? At 
a first stage, morphologic and syntactic processing 2 
is applied to the output from the speech recognizer 
module which usually produces a huge word-graph 
hypothesis. Thus the forest of syntactic trees pro- 
duced by this phase have been used to achieve two 
goals: 
1." The n-best analyses are used to disambiguate 
speech recognizer hypotheses 
2. They served as supplementary input for the ro- 
bust semantic analysis that we performed, that 
had as goal the production of query frames for 
the information system? 
Although robustness can be considered as being ap- 
plied at either a syntactic or semantic level, we be- 
lieve it is generally at the semantic level that it is 
most effective. This robust analysis needs a model 
of the domain in which the system operates, and 
a way of linking this model to the lexicon used by 
the other components. The degree of detail required 
of the domain model used by the robust analyzer 
SOur partner institution ISSCO (Institute Dalle Molle, 
University of Geneva) performed this analysis phase using 
tools that were developed in the European Linguistics En- 
gineering project MULTEXT. For syntactic analysis, ISSCO 
developed a Feature Unification Grammar based on a small 
sample of the Polyphone data. This grammar was taken by 
another of our partners (the Laboratory for Artificial Intel- 
ligence of the Swiss Federal Institute of Technology, Lau- 
sanne) and converted into a probabilistic ontext-free gram- 
mar, which was then applied to a sample of 500 entries from 
the Polyphone data. 
depends upon the ultimate task that must be per- 
formed - -  in our case, furnishing a query to an infor- 
mation system. The results of the processing phase 
of the previous example is represented below as an 
SGML annotation: 
<announce> 
Bonjour j'aimerais un 
</announce> 
<query> num~ro de t~l@phone ~ Saignelegier 
<name> 
<ram_name> c~est Mottaz m o deux ta z 
</f am_name> 
<first_name> Monique </first_name> 
</name> 
<address> 
<street> rue  du  printemps </street> 
<number> num~ro quatre </number> 
<city> </city> 
</address> 
</query> 
2 Methodo logy  
In this section we propose the use of a "light-parser" 
for doing sentence-level semantic annotation? The 
main idea comes from the observation that annota- 
tion does not always need to rely on the deep struc- 
ture of the sentence (e.g. at morpho-syntactic level). 
It is sometimes ufficient to find some cue-words 
which allow us to locate the logical sub-structures 
of the sentence? If the domain is simple enough, this 
task can be easily mechanized. A similar approach, 
using finite state parsing technology, has been pro- 
posed by Grefenstette in (Grefenstette, 1996) where 
the main applications are slanted to the extraction 
of syntactic information? 
2.1 Robust  Def in i te  C lause Grammars  
LHIP (Left-corner Head-driven Island Parser) (Bal- 
lim and Russell, 1994; Lieske and Ballim, 1998) is a 
system which performs robust analysis of its input, 
using a grammar defined in an extended form of the 
PROLOG Definite Clause Grammars (DCGs). The 
chief modifications to the standard PROLOG 'gram- 
mar rule' format are of two types: one or more right- 
hand side (RHS) items may be marked as 'heads' 
(e.g. using a leading '*'), and one or more RHS items 
may be marked as 'ignorable' (e.g. using a leading 
'-'). LHIP employs a different control strategy from 
that used by PROLOG DCGs, in order to allow it to 
cope with ungrammatical or unforeseen i put? The 
behavior of LHIP can best be understood in terms 
of the complementary notions of span and cover? 
A grammar ule is said to produce an island which 
spans input terminals ti to ti+,~ if the island starts 
at the i ~h terminal, and the i + n th terminal is the 
terminal immediately to the right of the last termi- 
nal of the island? A rule is said to cover m items 
if m terminals are consumed in the span of the rule. 
20 
Thus m < n. If m = n then the rule has completely 
covered the span. As implied here, rules need not 
cover all of the input in order to succeed. 
2.1.1 Weighted  LH IP  ru les 
The main goal of introducing weights into LHIP 
rules is to induce a partial order over the generated 
hypotheses. The following schema illustrate how to 
build a simple weighted rule in a compositional fash- 
ion where the resulting weight is computed from 
the sub-constituents u ing the minimum operator. 
Weights are real numbers in the interval \[0, 1\]. 
cat (cat (Hyp) ,Weight )  "~> 
sub_cat l (H l ,Wl ) ,  
? . . ~  
sub_catn(Hn,Wn), 
{app_list(\[Hl . . . . .  Hn\],Hyp), 
min_list(\[Wl ..... Wn\] ,Weight)}. 
This strategy is not the only possible since the LHIP 
formalism allows a greater flexibility. Without en- 
tering into formal details we can observe that if we 
strictly follow the above schema and we impose a 
cover threshold of 1 we are dealing with fuzzy DCG 
grammars (Lee and Zadeh, 1969; Asveld, 1996). We 
actually extend this class of grammars with a no- 
tion of fuzzy-robustness where weights are used to 
compute confidence factors for the membership of is- 
lands to categories 3. The order of constituents may 
play an important role in assigning weights for dif- 
ferent rules having the same number and type of 
constituents. Each LHIP rule returns a weight to- 
gether with a term'which will contribute to build 
the resulting structure. The confidence factor for a 
pre-terminal rule has been assigned statically on the 
basis of the rule designer's domain knowledge. 
2.2 The  methodo logy  at work  
In our case study we try to integrate the above prin- 
ciples in order to effectively compute annotation hy- 
potheses for the query generation task. This can 
be done by building a lattice of annotation hypothe- 
ses and possibly selecting the best one. This lattice 
is generated by means of a LHIP weighted gram- 
mar which is used to extract and assemble what 
we called semantic constituents. At the end of this 
process we presumably obtain suitable annotations 
from which we will able to extract the content of 
the query (e.g. name, address, city, etc.). The rules 
are designed taking into consideration the following 
kind of knowledge: 
Domain  Knowledge is exploited to provide quan- 
titative support (or confidence factor) to our 
rules. 
3Development of this notion is currently under investiga- 
tion and not yet formalized. 
Linguist ic  Knowledge (as for instance previous 
POS tagging or syntactic analysis) is used for 
determining constraints in order to prune the 
hypotheses space. 
Lexical  knowledge:  As pointed out in (Basili 
and M.T., 1997), lexical knowledge plays an im- 
portant role in Information Extraction since it can 
contribute in guiding the analysis process at various 
linguistic level. In our case we are concerned with 
lexical knowledge when we need to specify lexical 
LHIP rules which represent the building blocks of 
our parsing system. Semantic markers are domain- 
dependent word patterns and must be defined for 
a given corpus. They identify cue-words serving 
both as separators among logical subparts of the 
same sentence and as introducers of semantic con- 
stituents. In our specific ase they allow us to search 
for the content of the query only in interesting parts 
of the sentence. One of the most important sep- 
arators is the announcement-query separator. The 
LHIP clauses defining this separator can be one or 
more words covering rule like for instance: 
ann_query_separator ( IX\] ,0.7) #I. 0 "'> 
~terminal (X) , 
{X=' t~l~pbone ' }. 
ann_query_separator(\[X,Y\], i) #I.0 "~> 
~terminal (X), 
?terminal (Y), 
{\[X = 'num4ro*,Y = 'de'\]}. 
As an example of semantic onstituents introducers 
we propose here the follo~;ing rule: 
street_intro(\[T,Prep\] ,I) #I.0 "'> 
* street_type(T), 
preposition (Prep). 
which make use of some word knowledge about street 
types coming from an external thesaurus like: 
street_type(X) "'> 
?terminal (X), 
{thesaurus (street, W),member (X, N) }. 
It should be noted that we mix weighted and non- 
weighted rules, simply because non-weighted rules 
are rules with the highest weight 1. 
2.2.1 Generat ion  of  hypotheses  
The generation of annotation hypotheses is per- 
formed by: composing weighted rules, assembling 
constituents and filtering possible hypotheses. In 
this case the grammar should provide a means to 
provide an empty constituent when all possible hy- 
pothesis rules have failed. The highest level con- 
stituent is represented by the whole sentence struc- 
ture which simply specifies the possible orders of 
constituents relative to annotation hypotheses. 
21 
s(s(\[Ann,Query\]), W) - '> 
ann(Ann), 
query(Query,W2). 
ann(ann(Ann)) "-> closK(word(Ann)). 
query(query(Q) ,W) "'> 
* ann_query_separator (qSep, W1), 
target (Target, W2), 
address (Addr,W3) 
{app_list ( \[Qsep, \[Target\], 
\[Addr\] \ ,  Q), 
rain_list ( \[Wl,W2, W3\] ,W) }. 
In the ann rule we have made use of the Kleene 
closure operator closK which allow LHIP to sim- 
ply formulate regular expressions. In the query rule 
we have specified a possible order of constituents in- 
terleaved by semantic markers (e.g. separators and 
introducers). In this case we did not provide any lin- 
guistic constraint (e.g. preferring names belonging 
to the minimal common syntactic sub-tree or those 
having the longest sequence of proper names belong- 
ing to the same sub-tree). 
3 Conc lus ions  and  fu ture  works  
In this paper we summarized a proposal for a frame- 
work for designing grammar-based automated an- 
notation applications. Starting with a case study 
and following an approach which combines the no- 
tions of fuzziness and robustness in sentence parsing, 
we showed how to build practical domain-dependent 
rules which can be applied whenever it is possible to 
superimpose a sentence-level semantic structure to 
a text without relying on a previous deep syntacti- 
cal analysis. Even if the query generation problem 
may not seem a critical application one should bear 
in mind that the sentence processing must be done 
on-line. 
As we have previously seen, the cue-words used as 
semantic markers are domain-dependent. Even their 
relevance disposal and their weight within the rules 
depends on their linguistic usage. Therefore, a com- 
plete automatic annotation system based on the ap- 
proach proposed in this article seems to be adequate 
to give precise results. However, a semi-automatic 
system could satisfy our needs. This system should 
be based on the following techniques to achieve a 
high level of performance: 
1. For each annotation, the system offers a list 
of propositions based on standard grammars 
as well as on external knowledge (ontologies, 
knowledge bases ...) 
2. According to the grammar initially proposed, 
the user may change the annotation accord- 
ing to his needs. These modifications are held 
within the system to change the grammar ules 
as well as their weights. This makes the system 
interactive and enhanced by a learning phase. 
3. We could imagine that rule design process can 
be partially automated and we intend to pursue 
some research on developing methods for both 
assisted rule design and corpus based rule in- 
duction. 
Re ferences  
Peter. R.J. Asveld. 1996. Towards robustness in 
parsing - fuzzifying context-free language recog- 
nition. In J. Dassow, G. Rozemberg, and A. Sa- 
lomaa, editors, Developments in Language Theory 
H-  At the Crossroad of Mathematics, Computer 
Science and Biology, pages 443-453. World Scien- 
tific, Singapore. 
A. Ballim and G. Russell. 1994. LHIP: Extended 
DCGs for Configurable Robust Parsing. In Pro- 
ceedings of the 15th International Conference on 
Computational Linguistics, pages 501 - 507, Ky- 
oto, Japan. ACL. 
R. Basili and Pazienza M.T. 1997. Lexical ac- 
quisitiion and information extraction. In Pazienza 
M.T., editor, Information Extraction - A multi- 
diciplinary approach to an ermerging information 
technology, volume 1299 of LNAI, pages 44-72. 
Springer Verlag. 
J-C. Chappelier, M. Rajman, P. Bouillon, S. Arm- 
strong, V. Pallotta, and A Ballim. 1999. Isis 
project: final report. Technical report, Computer 
Science Department - Swiss Federal Institute of 
Technology, September.. 
G. Grefenstette. 1996. Light parsing as finite-state 
filtering. In Kornai A., editor, Proceedings of 
the ECAI 96 Workshop on Extended Finite State 
Models of Language, pages 20-25. 
J. Hobbs, D. Appelt, J. Bear, D. Israel, 
M. Kameyama, M. Stickel, and M. Tyson. 1996. 
Fastus: a cascaded finite-state transducer for ex- 
tracting information from natural-language t xt. 
In E. Roche and Y. Schabes, editors, Finite State 
Devices for Natural Language Processing. MIT 
Press, Cambridge MA. 
E.T. Lee and L.A. Zadeh. 1969. Note on fuzzy lan- 
guages. Information Science, 1:421-434. 
C. Lieske and A. Ballim. 1998. Rethinking nat- 
ural language processing with prolog. In Pro- 
ceedings of Practical Applications of Prolog and 
Practical Applications of Constraint Technology 
(PAPPACTS98), London,UK. Practical Applica- 
tion Company. 
D. Moll, J. Berri, and M. Hess. 1998. A real world 
implementation of answer extraction. In Proe. of 
the 9th International Conference and Workshop 
on Database and Expert Systems. Workshop on 
Natural Language and Information Systems, vol- 
ume NLIS'98, pages 143-148, Vienna. 
22 
C.M. Sperberg-McQueen a d L. Burnard, editors. 
1994. Guidelines for Electronic Text Encoding and 
Interchange, Text Encoding Initiative. Chicago 
and Oxford. 
23 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008?1015,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
User Requirements Analysis for Meeting Information Retrieval  
Based on Query Elicitation 
Vincenzo Pallotta 
Department of Computer Science 
University of Fribourg 
Switzerland 
Vincenzo.Pallotta@unifr.ch 
Violeta Seretan 
Language Technology Laboratory  
University of Geneva 
Switzerland 
seretan@lettres.unige.ch 
Marita Ailomaa 
Artificial Intelligence Laboratory  
Ecole Polytechnique F?d?rale  
de Lausanne (EPFL), Switzerland 
Marita.Ailomaa@epfl.ch 
 
 
 
Abstract 
We present a user requirements study for 
Question Answering on meeting records 
that assesses the difficulty of users ques-
tions in terms of what type of knowledge is 
required in order to provide the correct an-
swer. We grounded our work on the em-
pirical analysis of elicited user queries. We 
found that the majority of elicited queries 
(around 60%) pertain to argumentative 
processes and outcomes. Our analysis also 
suggests that standard keyword-based In-
formation Retrieval can only deal success-
fully with less than 20% of the queries, and 
that it must be complemented with other 
types of metadata and inference. 
1 Introduction 
Meeting records constitute a particularly important 
and rich source of information. Meetings are a 
frequent and sustained activity, in which multi-
party dialogues take place that are goal-oriented 
and where participants perform a series of actions, 
usually aimed at reaching a common goal: they 
exchange information, raise issues, express 
opinions, make suggestions, propose solutions, 
provide arguments (pro or con), negotiate 
alternatives, and make decisions. As outcomes of 
the meeting, agreements on future action items are 
reached, tasks are assigned, conflicts are solved, 
etc. Meeting outcomes have a direct impact on the 
efficiency of organization and team performance, 
and the stored and indexed meeting records serve 
as reference for further processing (Post et al, 
2004). They can also be used in future meetings in 
order to facilitate the decision-making process by 
accessing relevant information from previous 
meetings (Cremers et al, 2005), or in order to 
make the discussion more focused (Conklin, 2006).  
Meetings constitute a substantial and important 
source of information that improves corporate or-
ganization and performance (Corrall, 1998; Ro-
mano and Nunamaker, 2001). Novel multimedia 
techniques have been dedicated to meeting record-
ing, structuring and content analysis according to 
the metadata schema, and finally, to accessing the 
analyzed content via browsing, querying or filter-
ing (Cremers et al, 2005; Tucker and Whittaker, 
2004). 
This paper focuses on debate meetings (Cugini 
et al, 1997) because of their particular richness in 
information concerning the decision-making proc-
ess. We consider that the meeting content can be 
organized on three levels: (i) factual level (what 
happens: events, timeline, actions, dynamics); (ii) 
thematic level (what is said: topics discussed and 
details); (iii) argumentative level (which/how com-
mon goals are reached).  
The information on the first two levels is ex-
plicit information that can be usually retrieved di-
rectly by searching the meeting records with ap-
propriate IR techniques (i.e., TF-IDF). The third 
level, on the contrary, contains more abstract and 
tacit information pertaining to how the explicit in-
formation contributes to the rationale of the meet-
ing, and it is not present as such in raw meeting 
data: whether or not the meeting goal was reached, 
what issues were debated, what proposals were 
made, what alternatives were discussed, what ar-
guments were brought, what decisions were made, 
what task were assigned, etc.  
The motivating scenario is the following: A user 
1008
needs information about a past meeting, either in 
quality of a participant who wants to recollect a 
discussion (since the memories of co-participants 
are often inconsistent, cf. Banerjee et al, 2005), or 
as a non-participant who missed that meeting. 
Instead of consulting the entire meeting-related 
information, which is usually heterogeneous and 
scaterred (audio-video recordings, notes, minutes, 
e-mails, handouts, etc.), the user asks natural 
language questions to a query engine which 
retrieves relevant information from the meeting 
records. 
In this paper we assess the users' interest in 
retrieving argumentative information from 
meetings and what kind of knowledge is required 
for answering users' queries. Section 2 reviews 
previous user requirements studies for the meeting 
domain. Section 3 describes our user requirements 
study based on the analysis of elicited user queries, 
presents its main findings, and discusses the 
implications of these findings for the design of 
meeting retrieval systems. Section 4 concludes the 
paper and outlines some directions for future work. 
2  Argumentative Information in Meeting 
Information Retrieval 
Depending on the meeting browser type1, different 
levels of meeting content become accessible for 
information retrieval. Audio and video browsers 
deal with factual and thematic information, while 
artifact browsers might also touch on deliberative 
information, as long as it is present, for instance, in 
the meeting minutes. In contrast, derived-data 
browsers aim to account for the argumentative in-
formation which is not explicitly present in the 
meeting content, but can be inferred from it. If 
minutes are likely to contain only the most salient 
deliberative facts, the derived-data browsers are 
much more useful, in that they offer access to the 
full meeting record, and thus to relevant details 
about the deliberative information sought. 
2 .1  Importance of Argumentative Structure  
As shown by Rosemberg and Silince (1999), track-
ing argumentative information from meeting dis-
                                                
1 (Tucker and Whittaker, 2004) identifies 4 types of meeting 
browsers: audio browsers, video browsers, artifacts browsers 
(that exploit meeting minutes or other meeting-related docu-
ments), and browsers that work with derived data (such as 
discourse and temporal structure information). 
cussions is of central importance for building pro-
ject memories since, in addition to the "strictly fac-
tual, technical information", these memories must 
also store relevant information about deci-
sion-making processes. In a business context, the 
information derived from meetings is useful for 
future business processes, as it can explain phe-
nomena and past decisions and can support future 
actions by mining and assessment (Pallotta et al, 
2004). The argumentative structure of meeting dis-
cussions, possibly visualized in form of argumen-
tation diagrams or maps, can be helpful in meeting 
browsing. To our knowledge, there are at least 
three meeting browsers that have adopted argu-
mentative structure: ARCHIVUS (Lisowska et al, 
2004b), ViCoDe (Marchand-Maillet and Bruno, 
2005), and the Twente-AMI JFerret browser 
(Rienks and Verbree, 2006).  
2 .2  Query Elicitation Studies  
The users' interest in argumentation dimension of 
meetings has been highlighted by a series of recent 
studies that attempted to elicit the potential user 
questions about meetings (Lisowska et al, 2004a; 
Benerjee at al., 2005; Cremers et al, 2005). 
The study of Lisowska et al (2004a), part of the 
IM2 research project2, was performed in a simu-
lated environment in which users were asked to 
imagine themselves in a particular role from a se-
ries of scenarios. The participants were both IM2 
members and non-IM2 members and produced 
about 300 retrospective queries on recorded meet-
ings. Although this study has been criticized by 
Post et al (2004), Cremers et al (2005), and Ban-
erjee et al (2005) for being biased, artificial, ob-
trusive, and not conforming to strong HCI method-
ologies for survey research, it shed light on poten-
tial queries and classified them in two broad cate-
gories, that seem to correspond to our argumenta-
tive/non-argumentative distinction (Lisowska et 
al., 2004a: 994): 
? ?elements related to the interaction among par-
ticipants: acceptance/rejection, agree-
ment/disagreement; proposal, argumentation 
(for and against); assertions, statements; deci-
sions; discussions, debates; reactions; ques-
tions; solutions?; 
                                                
2 http://www.im2.ch 
1009
?  ?concepts from the meeting domains: dates, 
times; documents; meeting index: current, pre-
vious, sets; participants; presentations, talks; 
projects; tasks, responsibilities; topics?.  
Unfortunately, the study does not provide precise 
information on the relative proportions of queries 
for the classification proposed, but simply suggests 
that overall more queries belong to the second 
category, while queries requiring understanding of 
the dialogue structure still comprise a sizeable 
proportion. 
The survey conducted by Banerjee et al (2005) 
concerned instead real, non-simulated interviews 
of busy professionals about actual situations, re-
lated either to meetings in which they previously 
participated, or to meetings they missed. More than 
half of the information sought by interviewees 
concerned, in both cases, the argumentative dimen-
sion of meetings. 
For non-missed meetings, 15 out of the 26 in-
stances (i.e., 57.7%) concerned argumentative as-
pects: what the decision was regarding a topic (7); 
what task someone was assigned (4); who made a 
particular decision (2); what was the participants' 
reaction to a particular topic (1); what the future 
plan is (1). The other instances (42.3%) relate to 
the thematic dimension, i.e., specifics of the dis-
cussion on a topic (11).  
As for missed meetings, the argumentative in-
stances were equally represented (18/36): decisions 
on a topic (7); what task was assigned to inter-
viewee (4); whether a particular decision was made 
(3); what decisions were made (2); reasons for a 
decision (1); reactions to a topic (1). The thematic 
questions concern topics discussed, announce-
ments made, and background of participants.  
The study also showed that the recovery of in-
formation from meeting recordings is significantly 
faster when discourse annotations are available, 
such as the distinction between discussion, presen-
tation, and briefing. 
Another unobtrusive user requirements study 
was performed by Cremers et al (2005) in a "semi-
natural setting" related to the design of a meeting 
browser. The top 5 search interests highlighted by 
the 60 survey participants were: decisions made, 
participants/speakers, topics, agenda items, and 
arguments for decision. Of these, the ones shown 
in italics are argumentative. In fact, the authors 
acknowledge the necessity to include some "func-
tional" categories as innovative search options. 
Interestingly, from the user interface evaluation 
presented in their paper, one can indirectly infer 
how salient the argumentative information is per-
ceived by users: the icons that the authors intended 
for emotions, i.e., for a emotion-based search facil-
ity, were actually interpreted by users as referring 
to people?s opinion: What is person X's opinion?  ? 
positive, negative, neutral. 
3  User Requirements Analysis 
The existing query elicitation experiments reported 
in Section 2 highlighted a series of question types 
that users typically would like to ask about meet-
ings. It also revealed that the information sought 
can be classified into two broad categories: argu-
mentative information (about the argumentative 
process and the outcome of debate meetings), and 
non-argumentative information (factual, i.e., about 
the meeting as a physical event, or thematic, i.e., 
about what has been said in terms of topics). 
The study we present in this section is aimed at 
assessing how difficult it is to answer the questions 
that users typically ask about a meeting. Our goal 
is to provide insights into:  
? how many queries can be answered using stan-
dard IR techniques on meeting artefacts only 
(e.g., minutes, written agenda, invitations); 
? how many queries can be answered with IR on 
meeting recordings; 
? what kind of additional information and infer-
ence is needed when IR does not apply or it is 
insufficient (e.g., information about the par-
ticipants and the meeting dynamics, external 
information about the meeting?s context such 
as the relation to a project, semantic interpreta-
tion of question terms and references, compu-
tation of durations, aggregation of results, etc). 
Assessing the level of difficulty of a query based 
on the two above-mentioned categories might not 
provide insightful results, because these would be 
too general, thus less interpretable. Also, the com-
plex queries requiring mixed information would 
escape observation because assigned to a too gen-
eral class. We therefore considered it necessary to 
perform a separate analysis of each query instance, 
as this provides not only detailed, but also trace-
able information. 
1010
3 .1  Data: Collecting User Queries 
Our analysis is based on a heterogeneous collec-
tion of queries for meeting data. In general, an un-
biased queries dataset is difficult to obtain, and the 
quality of a dataset can vary if the sample is made 
of too homogenous subjects (e.g., people belong-
ing to the same group as members of the same pro-
ject). In order to cope with this problem, our strat-
egy was to use three different datasets collected in 
different settings:  
? First, we considered the I M2 dataset  collected 
by Lisowska et al (2004a), the only set of user 
queries on meetings available to date. It com-
prises 270 questions (shortly described in Sec-
tion 2) annotated with a label showing whether 
or not the query was produced by an IM2-
member. These queries are introspective and 
not related to any particular recorded meeting. 
? Second, we cross-validated this dataset with a 
large corpus of 294 natural language state-
ments about existing meetings records. This 
dataset, called the B ET observations  (Wellner 
et al, 2005), was collected by subjects who 
were asked to watch several meeting record-
ings and to report what the meeting partici-
pants appeared to consider interesting. We use 
it as a ?validation? set for the IM2 queries: an 
IM2 query is considered as ?realistic? or ?em-
pirically grounded? if there is a BET observa-
tion that represents a possible answer to the 
query. For instance, the query Why was the 
proposal made by X not accepted?  matches the 
BET observation Denis eliminated Silence of 
the Lambs as it was too violent . 
? Finally, we collected a new set of ?real? queries 
by conducting a survey of user requirements 
on meeting querying in a natural business set-
ting. The survey involved 3 top managers from 
a company and produced 35 queries. We called 
this dataset Manager Survey Set  (MS-Set). 
The queries from the IM2-set (270 queries) and the 
MS-Set (35 queries) were analyzed by two differ-
ent teams of two judges. Each team discussed each 
query, and classified it along the two main dimen-
sions we are interested in: 
? query type : the type of meeting content to 
which the query pertains; 
? query difficulty : the type of information re-
quired to provide the answer. 
3 .2  Query Type Analysis 
Each query was assigned exactly one of the follow-
ing four possible categories (the one perceived as 
the most salient): 
1. factual: the query pertains to the factual meet-
ing content; 
2. thematic: the query pertains to the thematic 
meeting content; 
3.  process : the query pertains to the argumenta-
tive meeting content, more precisely to the ar-
gumentative process; 
4. outcome: the query pertains to the argumenta-
tive meeting content, more precisely to the 
outcome of the argumentative process. 
IM 2- s et 
(s iz e:2 70) 
MS-S et  
(s iz e: 3 5) Cate go ry 
Tea m 1  Tea m 2  Tea m 1  Tea m 2  
Fac tu al 24. 8 %  20. 0 %  20. 0 %  
The m atic 18. 5 %  45. 6 %  20. 0 %  11. 4 %  
Proc es s 30. 0 %  32. 6 %  22. 9 %  28. 6 %  
Outc o me 26. 7 %  21. 8 %  37. 1 %  40. 0 %  
Proc es s + O utc o me 56. 7 %  54. 4 %  60. 0 %  68. 6 %  
Table 1. Query classification according to the 
meeting content type. 
Results from this classification task for both query 
sets are reported in Table 1. In both sets, the 
information most sought was argumentative: about 
55% of the IM2-set queries are argumentative 
(process or outcome). This invalidates the initial 
estimation of Lisowska et al (2004a:994) that the 
non-argumentative queries prevail, and confirms 
the figures obtained in (Banerjee et al, 2005), ac-
cording to which 57.7% of the queries are argu-
mentative. In our real managers survey, we ob-
tained even higher percentages for the argumenta-
tive queries (60% or 68.6%, depending on the an-
notation team). The argumentative queries are fol-
lowed by factual and thematic ones in both query 
sets, with a slight advantage for factual queries. 
The inter-annotator agreement for this first clas-
sification is reported in Table 2. The proportion of 
queries on which annotators agree in classifying 
them as argumentative is significantly high. We 
only report here the agreement results for the indi-
vidual argumentative categories (Process, Out-
come) and both (Process & Outcome). There were 
213 queries (in IM2-set) and 30 queries (in MS-
1011
set) that were consistently annotated by the two 
teams on both categories. Within this set, a high 
percentage of queries were argumentative, that is, 
they were annotated as either Process or Outcome 
(label AA in the table). 
IM 2- s et (s iz e: 27 0) MS-s e t (s iz e: 3 5)  C ate go ry rati o k app a  rati o k app a  
Proc es s 84. 8 %  82. 9 %  88. 6 %  87. 8 %  
Outc o me 90. 7 %  89. 6 %  91. 4 %  90. 9 %  
Proc es s & 
Outc o me 78. 9 %  76. 2 %  85. 7 %  84. 8 %  
AA 11 7/2 13 =  54. 9 %   
19/ 30 =  
63. 3 %   
Table 2. Inter-annotator agreement for query-type 
classification. 
Furthermore, we provided a re-assessment of the 
proportion of argumentative queries with respect to 
query origin for the IM2-set (IM2 members vs. 
non-IM2 members): non-IM2 members issued 
30.8% of agreed argumentative queries, a propor-
tion that, while smaller compared to that of IM2 
members (69.2%), is still non-negligible. This con-
trasts with the opinion expressed in (Lisowska et 
al., 2004a) that argumentative queries are almost 
exclusively produced by IM2 members.  
Among the 90 agreed IM2 queries that were 
cross-validated with the BET-observation set, 
28.9% were argumentative. We also noted that the 
ratio of BET statements that contain argumentative 
information is quite high (66.9%). 
3 .3  Query Difficulty Analysis 
In order to assess the difficulty in answering a 
query, we used the following categories that the 
annotators could assign to each query, according to 
the type of information and techniques they judged 
necessary for answering it: 
1. Role of I R : states the role of standard3  Informa-
tion Retrieval (in combination with Topic Ex-
traction4) techniques in answering the query. 
Possible values:  
a. Irrelevant (IR techniques are not appli-
cable).  Example: What decisions have 
been made?  
                                                
3  By standard IR we mean techniques based on bag-of-word 
search and TF-IDF indexing. 
4 Topic extraction techniques are based on topic shift detec-
tion (Galley et al, 2003) and keyword extraction (van der Plas 
et al, 2004). 
b. successful (IR techniques are sufficient). 
Example: Was the budget approved?  
c. insufficient (IR techniques are necessary, 
but not sufficient alone since they re-
quire additional inference and informa-
tion, such as argumentative, cross-
meeting, external corporate/project 
knowledge). Example: Who rejected the 
proposal made by X on issue Y?  
2. Artefacts : information such as agenda, min-
utes of previous meetings, e-mails, invita-
tions and other documents related and avail-
able before the meeting. Example: Who was 
invited to the meeting?  
3.  Recordings : the meeting recordings (audio, 
visual, transcription). This is almost always 
true, except for queries where Artefacts or 
Metadata are sufficient, such as What was 
the agenda?,  Who was invited to the meet-
ing? ). 
4 .  Metadata : context knowledge kept in static 
metadata (e.g., speakers, place, time). Ex-
ample: Who were the participants at the 
meeting? 
5. Dialogue Acts & Adjacency Pairs : Example: 
What was John?s response to my comment 
on the last meeting?  
6. Argumentation : metadata (annotations) 
about the argumentative structure of the 
meeting content. Example: Did  everybody 
agree on the decisions, or were there differ-
ences of opinion?  
7.  Semantics : semantic interpretation of terms 
in the query and reference resolution, in-
cluding deictics (e.g., for how long, usually, 
systematically, criticisms; this, about me, I ). 
Example: What decisions got made easily ?  
The term requiring semantic interpretation is 
underlined.  
8. Inference : inference (deriving information 
that is implicit), calculation, and aggregation 
(e.g., for ?command? queries asking for lists 
of things ? participants, issues, proposals). 
Example: What would be required from me?  
1012
9. Multiple meetings : availability of multiple 
meeting records. Example: Who usually at-
tends the project meetings?  
10. External : related knowledge, not explicitly 
present in the meeting records (e.g., infor-
mation about the corporation or the projects 
related to the meeting). Example: Did some-
body talk about me or about my work?  
Results of annotation reported on the two query 
sets are synthesized in Table 3: IR is sufficient for 
answering 14.4% of the IM2 queries, and 20% of 
the MS-set queries. In 50% and 25.7% of the cases, 
respectively, it simply cannot be applied (irrele-
vant). Finally, IR alone is not enough in 35.6% of 
the queries from the IM2-set, and in 54.3% of the 
MS-set; it has to be complemented with other 
techniques.  
IM 2- s et MS-s e t 
IR is : all  
qu eri es AA 
all  
qu eri es AA 
Suff ic ie nt 39/ 27 0 =  14. 4 %  
1/1 17 =  
0.8 %  
7/3 5 =  
20. 0 %  
1/1 9 =  
5.3 %  
Irrel ev a nt 13 5/2 70 =  50. 0 %   
55/ 11 7 =  
47. 0 %  
9/3 5 =  
25. 7 %  
3/1 9 =  
15. 8 %  
Ins uf fic i ent  96/ 27 0 =  35. 6 %  
61/ 11 7 =  
52. 1 %  
19/ 35 =  
54. 3 %  
15/ 19 =  
78. 9 %  
Table 3.  The role of IR (and topic extraction) in 
answering users? queries. 
If we consider agreed argumentative queries 
(Section 3.2), IR is effective in an extremely low 
percentage of cases (0.8% for IM2-set and 5.3% 
for MS-Set). IR is insufficient in most of the cases 
(52.1% and 78.9%) and inapplicable in the rest of 
the cases (47% and 15.8%). Only one argumenta-
tive query from each set was judged as being an-
swerable with IR alone: What were the decisions to 
be made (open questions) regarding the topic t1? 
When is the NEX T M E E TIN G planned? (e.g. to 
follow up on action items) . 
Table 4 shows the number of queries in each set 
that require argumentative information in order to 
be answered, distributed according to the query 
types. As expected, no argumentation information 
is necessary for answering factual queries, but 
some thematic queries do need it, such as What 
was decided about topic T?  (24% in the IM2-set 
and 42.9% in the M.S.-set).  
Overall, the majority of queries in both sets re-
quire argumentation information in order to be an-
swered (56.3% from IM2 queries, and 65.7% from 
MS queries). 
IM 2- s et, An no ta tio n 1  MS-s e t, A nn ot ati on 1 
Cate go ry  tot al  Req.  arg. Rati o Tot al  
Req.  
arg. Rati o 
Fac tu al 67  0  0%  7  0  0%  
The m atic  50  12  24. 0 %  7  3  42. 9 %  
Proc es s 81  73  90. 1 %  8  7  87. 5 %  
Outc o me  72  67  93. 1 %  13  13  10 0%  
All 27 0  15 2  56. 3 %  35  23  65. 7 %  
Table 4. Queries requiring argumentative informa-
tion. 
We finally looked at what kind of information is 
needed in those cases where IR is perceived as in-
sufficient or irrelevant. Table 5 lists the most fre-
quent combinations of information types required 
for the IM2-set and the MS-set. 
3 .4  Summary of Findings 
The analysis of the annotations obtained for the 
305 queries (35 from the Manager Survey set, and 
270 from the IM2-set) revealed that: 
? The information most sought by users from 
meetings is argumentative (i.e., pertains to the 
argumentative process and its outcome). It 
constitutes more than half of the total queries, 
while factual and thematic information are 
similar in proportions (Table 1); 
? There was no significant difference in this re-
spect between the IM2-set and the MS-set 
(Table 1); 
? The decision as to whether a query is argumen-
tative or not is easy to draw, as suggested by 
the high inter-annotator agreement shown in 
Table 2; 
? Standard IR and topic extraction techniques 
are perceived as insufficient in answering most 
of the queries. Only less than 20% of the 
whole query set can be answered with IR, and 
almost no argumentative question (Table 3). 
? Argumentative information is needed in an-
swering the majority of the queries (Table 4); 
? When IR alone fails, the information types that 
are needed most are (in addition to recordings): 
Argumentation, Semantics, Inference, and 
Metadata (Table 5); see Section 3.3 for their 
description. 
 
1013
 IR a lo ne fa ils IM 2-s et 
Inf orm at io n ty p es IR i ns uff ic ie nt             96 c as es   3 5.6 % IR irr el ev an t         13 5 c as es    50 %  
Artef ac ts         x     
Rec ord in gs x x x x x x x x x x x   
Me ta- da ta   x  x   x  x  x x 
Dlg ac ts & A dj . p airs              
Argu m en tat io n x x x x x x x x x  x   
Se ma ntic s x x x x x   x x x x x  
Inf ere nc e x  x x   x x x x x x  
Mu lti pl e me et in gs    x        x  
Ex tern al              
                  Cas es 15 11 9 8 7 5 4 14 9 8 8 7 5 
                  Ra tio (% )  15. 6  11. 5  9.4  8.3  7.3  5.2  4.2  10. 4  6.7  5.9  5.9  5.2  3.7  
 
IR a lo ne fa ils MS-s e t 
Inf orm at io n ty p es IR i ns uff ic ie nt     19 c as es   5 4.3 %  IR irr el ev an t   9 c as es   54. 3 %  
Artef ac ts     x x 
Rec ord in gs x x x x   
Me ta- da ta     x x 
Dlg ac ts & A dj . p airs       
Argu m en tat io n x x x x   
Se ma ntic s x  x x x  
Inf ere nc e x x  x x  
Mu lti pl e me et in gs       
Ex tern al    x   
                  Cas es 6 4 2 2 2 2 
                  Ra tio (% )  31. 6 21 10. 5 10. 5 22. 2 22. 2 
Table 5. Some of the most frequent combinations of information required for answering the queries in the 
IM2-Set and in the MS-set when IR alone fails. 
3 .5  Discussion 
Searching relevant information through the re-
corded meeting dialogues poses important prob-
lems when using standard IR indexing techniques 
(Baeza-Yates and Ribeiro-Nieto, 2000), because 
users ask different types of queries for which a 
single retrieval strategy (e.g., keywords-based) is 
insufficient. This is the case when looking at an-
swers that require some sort of entailment, such as 
inferring that a proposal has been rejected when a 
meeting participant says Are you kidding? .  
Spoken-language information retrieval (Vinci-
arelli, 2004) and automatic dialogue-act extraction 
techniques (Stolke et al, 2000; Clark and Popescu-
Belis, 2004; Ang et al, 2005) have been applied to 
meeting recordings and produced good results un-
der the assumption that the user is interested in 
retrieving either topic-based or dialog act-based 
information. But this assumption is partially in-
validated by our user query elicitation analysis, 
which showed that such information is only sought 
in a relatively small fraction of the users? queries. 
A particular problem for these approaches is that 
the topic looked for is usually not a query itself 
( Was topic T mentioned?) , but just a parameter in 
more structured questions ( What was decided 
about T? ). Moreover, the relevant participants? 
contributions (dialog acts) need to be retrieved in 
combination, not in isolation (The reactions  to the 
proposal made by X ). 
4  Conclusion and Future Work 
While most of the research community has ne-
glected the importance of argumentative queries in 
meeting information retrieval, we provided evi-
dence that this type of queries is actually very 
common. We quantified the proportion of queries 
involving the argumentative dimension of the 
meeting content by performing an in-depth analy-
sis of queries collected in two different elicitation 
surveys. The analysis of the annotations obtained 
for the 305 queries (270 from the IM2-set, 35 from 
MS-set) was aimed at providing insights into dif-
ferent matters: what type of information is typi-
cally sought by users from meetings; how difficult 
it is, and what kind of information and techniques 
are needed in order to answer user queries.  
This work represents an initial step towards a 
better understanding of user queries on the meeting 
domain. It could provide useful intuitions about 
1014
how to perform the automatic classification of an-
swer types and, more importantly, the automatic 
extraction of argumentative features and their rela-
tions with other components of the query (e.g., 
topic, named entities, events). 
In the future, we intend to better ground our first 
empirical findings by i) running the queries against 
a real IR system with indexed meeting transcripts 
and evaluate the quality of the obtained answers; 
ii) ask judges to manually rank the difficulty of 
each query, and iii) compare the two rankings. We 
would also like to see how frequent argumentative 
queries are in other domains (such as TV talk 
shows or political debates) in order to generalize 
our results. 
Acknowledgements 
We wish to thank Martin Rajman and Hatem 
Ghorbel for their constant and valuable feedback. 
This work has been partially supported by the 
Swiss National Science Foundation NCCR IM2 
and by the SNSF grant no. 200021-116235. 
References 
Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005. 
Automatic Dialog Act Segmentation and Classification in 
Multiparty Meetings. In Proceedings of IE E E IC A S S P 
2 0 0 5 , Philadelphia, PA, USA. 
Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000. 
Modern Information Retrieval . Addison Wesley. 
Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky. 
2005. The Necessity of a Meeting Recording and Playback 
System, and the Benefit of Topic-Level Annotations to 
Meeting Browsing. In Proceedings of INT E R A C T 2 0 0 5 , 
Rome, Italy. 
Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level 
Dialogue Act Tags. In Proceedings of SIG D IA L ' 0 4 , pages 
163?170. Cambridge, MA, USA. 
Jeff Conklin. 2006. Dialogue Mapping: Building Shared 
Understanding of Wicked Problems . John Wiley & Sons. 
Sheila Corrall. 1998. Knowledge management. Are we in the 
knowledge management business? A R IA D N E : the Web 
version,  18. 
Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S 
Vermeeren. 2005. ?What was discussed by whom, how, 
when and where?" personalized browsing of annotated 
multimedia meeting recordings. In Proceedings of HC I 
2 0 0 5 , pages 1?10, Edinburgh, UK.  
John Cugini, Laurie Damianos, Lynette Hirschman, Robyn 
Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz. 
1997. Methodology for evaluation of collaborative 
systems. Technical Report Rev. 3.0, The Evaluation 
Working Group of the DARPA Intelligent Collaboration 
and Visualization Program. 
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and 
Hongyan Jing. 2003. Discourse Segmentation of Multi-
Party Conversation. In Proceedings of AC L 2 0 0 3 , pages 
562?569, Sapporo, Japan. 
Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong. 
2004a. User Query Analysis for the Specification and 
Evaluation of a Dialogue Processing and Retrieval System. 
In Proceedings LR E C 2 0 0 4 , pages 993?996, Lisbon, 
Portugal. 
Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b. 
ARCHIVUS: A System for Accesssing the Content of 
Recorded Multimodal Meetings. In Proceedings of ML M I 
2 0 0 4 , Martigny, Switzerland. 
St?phane Marchand-Maillet and Eric Bruno. 2005. Collection 
Guiding: A new framework for handling large multimedia 
collections. In Proceeding of AV IV DiLib05 , Cortona, Italy. 
Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes 
Lisowska and St?phane Marchand-Maillet. 2004. Towards 
meeting information systems: Meeting knowledge 
management. In Proceedings of ICE IS 2 0 0 5 , pages 464?
469, Porto, Portugal. 
Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and 
Hatem Ghorbel. 2004. Automatic keyword extraction from 
spoken text: A comparison between two lexical resources: 
the EDR and WordNet. In Proceedings of the LR E C 2 0 0 4 , 
pages 2205?2208, Lisbon, Portugal. 
Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson 
Henkemans. 2004. A Research Environment for Meeting 
Behavior. In Proceedings of the 3rd Workshop on Social 
Intelligence Design,  pages 159?165, University of Twente, 
Enschede, The Netherlands. 
Rutger Rienks and Daan Verbree. 2006. About the Usefulness 
and Learnability of Argument?Diagrams from Real 
Discussions. In Proceedings of ML MI 2 0 0 6 , Washington 
DC, USA. 
Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001. 
Meeting Analysis: Findings from Research and Practice. In 
Proceedings of HIC S S-3 4 , Maui, HI, IEEE Computer 
Society. 
Duska Rosemberg and John A.A. Silince. 1999. Common 
ground in computer-supported collaborative argumentation. 
In Proceedings of the CL S C L 9 9 , Stanford, CA, USA. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, 
Rachel Martin, Carol Van Ess-Dykema and Marie Meteer. 
2000. Dialog Act Modeling for Automatic Tagging and 
Recognition of Conversational Speech. Computational 
Linguistics,  26(3):339?373.  
Simon Tucker and Steve Whittaker. 2004. Accessing 
multimodal meeting data: systems, problems and 
possibilities. In Proceedings of ML M I 2 0 0 4 , Martigny, 
Switzerland. 
Alessandro Vinciarelli. 2004. Noisy text categorization. In 
Proceedings of ICP R 2 0 0 4 , Cambridge, UK. 
Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker. 
2005. A Meeting Browser Evaluation Test. In Proceedings 
of CHI 2 0 0 5 , Portand, Oregon, USA. 
1015
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 40?41,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Scaling up a NLU system from text to dialogue understanding  R. Delmonte, A. Bristot, G. Voltolina Department of Language Science - Universit? Ca? Foscari - 30123 - VENEZIA delmont@unive.it 
 Vincenzo Pallotta Webster University, Geneva Switzerland pallotta@webster.ch 
Abstract In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkely project. In a final section we present preliminary evaluation of the system on non-referential pronominals individuation. 1 Introduction Very much like other deep linguistic processing systems (see Allen et al), our system is a generic text/dialogue understanding system that can be used in connection with an ontology ? WordNet - and/or a repository of commonsense knowledge like CONCEPTNET. Word sense disambiguation takes place at the level of semantic interpretation and is represented in the Discourse Model.  Computing semantic representations for spoken dialogues is a particularly hard task which ? when compared to written text processing - requires the following additional information to be made available: - adequate treatment of fragments; - adequate treatment of short turns, in particular one-word turns; - adequate treatment of first person singular and plural pronominal expressions; - adequate treatment of disfluencies, thus including cases of turns made up of just such expressions, or cases when they are found inside the utterance; - adequate treatment of overlaps; - adequate treatment of speaker identity for pronominal coreference; In our system, then, every dialogue turn receives one polarity label, indicating negativity or 
positivity, and this is computed by looking into a dictionary of polarity items. This is subsequently used to decide on argumentative automatic classification.  The Berkeley ICSI dialogues are characterized by the need to argument in a exhaustive manner the topics to be debated which are the theme of each multiparty dialogue. The mean length of utterances/turns in each dialogue we parsed was rather long.  2 The System GETARUNS GETARUNS1, the system for text understanding developed at the University of Venice, is organized as a pipeline which includes two versions of the system: what we call the Partial and the Deep GETARUNS (Delmonte 2007;2009). The Deep version is equipped with three main modules: a lower module for parsing, where sentence strategies are implemented; a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation takes place.  2.1 The Algorithm for Overlaps Overlaps are an important component of all spoken dialogue analysis. In all dialogue transcription, overlaps are treated as a separate turn from the one in which they occur, which usually follows it.  On the contrary, when computing overlaps we set as our first goal that of recovering the temporal order. This is done because overlaps may introduce linguistic elements which influence the local context. Eventually, they may determine the interpretation of the current utterance.                                                  1 The system has been tested in STEP competition, and can be downloaded at, http://project.cgm.unive.it/html/sharedtask/. 
40
For these reasons, they cannot be moved to a separate turn because they must be semantically interpreted where they temporally belong.  The algorithm we built looks at time stamps, and everytime the following turn begins at a time preceding the ending time of current turn it enters a special recursive procedure. It looks for internal interruption in the current turn and splits the utterance where the interruption occurs. Then it parses it split initial portion of current utterance and continues with the overlapping turn. This may be reiterated in case another overlap follows which again begins before the end of current utterance. Eventually, it returns to the analysis of the current turn with the remaining portion of current utterance. 2.2 The Treatment of Fragments and Short Turns Fragments and short turns are filtered by a lexical lookup procedure that searches for specific linguistic elements which are part of a list of backchannels, acknowledgements expressions and other similar speech acts. In case this procedure has success, no further computation takes place. However, this only applies to utterances shorter than 5 words, and should be made up only of such special words. No other linguistic element should be present apart from non-words, that is words which are only partially produced and have been transcribed with a dash at the end. Otherwise we proceed as follows: - graceful failure procedures for ungrammatical sentences, which might be fullfledged utterances but semantically uninterpretable due to the presence of repetitions, false starts and similar disfluency phenomena. Or else they may be just fragments, i.e. partial or incomplete utterances, hence non-interpretable as such; this is done by imposing grammatical constraints of wellformedness in the parser. We implemented a principled treatment of elliptical utterances and contribute one specific speech act. They may express agreement/ disagreement, acknowledgements, assessments, continuers etc. All these items are computed as being complements of abstract verb SAY which is introduced in the analysis, and has as subject, the name of current speaker. 
3 The Experiment We set up an experiment in order to test the new version of the system, that is detecting referential from nonreferential uses of personal pronouns ?you?, ?we? and ?it?.  In order to take decisions as to whether pronouns are to be interpreted as referential or not a recursive procedure checks the type of governing predicate. Referential pronouns are then passed on to the pronominal binding algorithm that looks for local antecedents if any. Otherwise, the pronouns is labeled as having External coreference in the previous discourse stretch. The Anaphora Resolution module will then take care of the antecedent and a suitable semantic identifier will be associated to it. On the contrary, if the pronouns are judged to be referentially empty or generic, no binding takes place. Here below is a table containing total values for pronouns WE/YOU/IT in all the 10 dialogues analysed.   Referential Generic Total WE 1186 706 1892 YOU 1045 742 1787 IT 1593 1008 2601   Total 3824 2456 6280 Table 1. Overall count of pronominal expressions Results for the experiment are as follows   Recall Precision F-Score WE 98.2% 60.59% 74.94% YOU 99.3% 70.99% 82.79% IT 97.6% 64.2% 77.45% Table 2. Results for pronominal expressions  References  Allen, J., M. Dzikovska, M. Manshadi, and M. Swift. 2007. Deep linguistic processing for spoken dialogue systems. In ACL 2007 Workshop on Deep Linguistic Processing, pp. 49?56.  Delmonte R. 2007. Computational Linguistic Text Processing ? Logical Form, Semantic Interpretation, Discourse Relations and Question Answering, Nova Science Publishers, New York. Delmonte R. 2009. Computational Linguistic Text Processing ? Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science Publishers, New York.  
41
