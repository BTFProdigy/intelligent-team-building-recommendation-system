Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 13?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Cascade Method for Detecting Hedges and their Scope in Natural
Language Text
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, Shixi Fan
Key Laboratory of Network Oriented Intelligent Computation
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, Guangdong, China
{tangbuzhou,yuanbo.hitsz}@gmail.com
{wangxl,wangxuan,fanshixi}@insun.hit.edu.cn
Abstract
Detecting hedges and their scope in nat-
ural language text is very important for
information inference. In this paper,
we present a system based on a cascade
method for the CoNLL-2010 shared task.
The system composes of two components:
one for detecting hedges and another one
for detecting their scope. For detecting
hedges, we build a cascade subsystem.
Firstly, a conditional random field (CRF)
model and a large margin-based model are
trained respectively. Then, we train an-
other CRF model using the result of the
first phase. For detecting the scope of
hedges, a CRF model is trained according
to the result of the first subtask. The ex-
periments show that our system achieves
86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia cor-
pus for hedge detection, and 49.95% F-
measure on biological corpus for hedge
scope detection. Among them, 86.36%
is the best result on biological corpus for
hedge detection.
1 Introduction
Hedge cues are very common in natural language
text. Vincze et al (2008) report that 17.70% of
the sentences in the abstract section and 19.94% of
sentences in the full paper section contain hedges
on BioScope corpus. As Vincze et al (2008)
suggest that information that falls in the scope
of hedges can not be presented as factual in-
formation. Detecting hedges and their scope in
natural language text is very important for in-
formation inference. Recently, relative research
has received considerable interest in the biomed-
ical NLP community, including detecting hedges
and their in-sentence scope in biomedical texts
(Morante and Daelemans, 2009). The CoNLL-
2010 has launched a shared task for exploiting the
hedge scope annotated in the BioScope (Vincze et
al., 2008) and publicly available Wikipedia (Gan-
ter and Strube, 2009) weasel annotations. The
shared task contains two subtasks (Farkas et al,
2010): 1. learning to detect hedges in sentences on
BioScope and Wikipedia; 2. learning to detect the
in-sentence scope of these hedges on BioScope.
In this paper, we present a system based on a
cascade method for the CoNLL-2010 shared task.
The system composes of two components: one
for detecting hedges and another one for detect-
ing their scope. For detecting hedges, we build
a cascade subsystem. Firstly, conditional ran-
dom field (CRF) model and a large margin-based
model are trained respectively. Then, we train
another CRF model using the result of the first
phase. For detecting the scope of hedges, a CRF
model is trained according to the result of the first
subtask. The experiments show that our system
achieves 86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia corpus for
hedge detection, and 49.95% F-measure on bio-
logical corpus for hedge scope detection. Among
them, 86.36% is the best result on biological cor-
pus for hedge detection.
2 System Description
As there are two subtasks, we present a system
based on a cascade supervised machine learning
methods for the CoNLL-2010 shared task. The ar-
chitecture of our system is shown in Figure 1.
The system composes of two subsystems for
two subtasks respectively, and the first subsystem
is a two-layer cascaded classifier.
2.1 Hedge Detection
The hedges are represented by indicating whether
a token is in a hedge and its position in the
CoNLL-2010 shared task. Three tags are used for
13
Figure 1: System architecture
this scheme, where O cue indicates a token out-
side of a hedge, B cue indicates a token at the
beginning of a hedge and I cue indicates a to-
ken inside of a hedge. In this subsystem, we do
preprocessing by GENIA Tagger (version 3.0.1)1
at first, which does lemma extraction, part-of-
speech (POS), chunking and named entity recog-
nition (NER) for feature extraction. For the out-
put of GENIA Tagger, we convert the first char
of a lemma into lower case and BIO chunk tag
into BIOS chunk tag, where S indicates a token
is a chunk, B indicates a token at the beginning
of a chunk, I indicates a token inside of a chunk,
and O indicates a token outside of a chunk. Then
a two-layer cascaded classifier is built for pre-
diction. There are a CRF classifier and a large
margin-based classifier in the first layer and a CRF
classifier in the second layer.
In the first layer, the following features are used
in our system:
? Word andWord Shape of the lemma: we used
the similar scheme as shown in (Tsai et al,
2005).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Prefix and Suffix with length 3-5.
? Context of the lemma, POS and the chunk in
the window [-2,2].
? Combined features including L0C0, LiP0
and LiC0, where ?1 ? i ? 1 L denotes the
lemma of a word, P denotes a POS and C
denotes a chunk tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? Whether a token is a part of the pairs ?neither
... nor? and ?either ... or? as both tokens of a
pair are always labeled with the same tag.
? Whether a token can possibly be classified
into B cue, I cue or O cue; its lemma, POS
and chunk tag for each possible case: these
features are extracted according to a dictio-
nary extracted from training corpus, which
lists all possible hedge tag for each word in
the training corpus.
In the second layer, we used some features
about the result of the last layer besides those men-
tioned above. They are listed as follow:
? The lemma and POS sequences of the hedge
predicted by each classifier.
? The times of a token classified into B cue,
I cue and O cue by the first two classifiers.
? Whether a token is the last token of the hedge
predicted by each classifier.
2.2 Hedge Scope Detection
We follow the way of Morante and Daelemans
(2009) to represent the scope of a hedge, where
F scope indicates a token at the beginning of a
scope sequence, L scope indicates a token at the
last of a scope sequence, and NONE indicates
others. In this phase, we do preprocessing by
GDep Tagger (version beta1)2 at first, which does
lemma extraction, part-of-speech (POS), chunk-
ing, named entity recognition (NER) and depen-
dency parse for feature extraction. For the out-
put of GDep Tagger, we deal with the lemma and
chunk tag using the same way mentioned in the
last section. Then, a CRF classifier is built for pre-
diction, which uses the following features:
2http://www.cs.cmu.edu/ sagae/parser/gdep
14
? Word.
? Context of the lemma, POS, the chunk, the
hedge and the dependency relation in the
window [-2,2].
? Combined features including L0C0,
L0H0, L0D0, LiP0, PiC0,PiH0, CiH0,
PiD0,CiD0, where ?1 ? i ? 1 L denotes
the lemma of a word, P denotes a POS, C
denotes a chunk tag, H denotes a hedge tag
and D denotes a dependency relation tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? The type of a hedge; the lemma, POS and
chunk sequences of it.
? The lemma, POS, chunk, hedge and depen-
dency relation sequences of 1st and 2nd de-
pendency relation edges; the lemma, POS,
chunk, hedge and dependency relation se-
quences of the path from a token to the root.
? Whether there are hedges in the 1st, 2nd de-
pendency relation edges or path from a token
to the root.
? The location of a token relative to the nega-
tion signal: previous the first hedge, in the
first hedge, between two hedge cues, in the
last hedge, post the last hedge.
At last, we provided a postprocessing system
for the output of the classifier to build the com-
plete sequence of tokens that constitute the scope.
We applied the following postprocessing:
? If a hedge is bracketed by a F scope and a
L scope, its scope is formed by the tokens be-
tween them.
? If a hedge is only bracketed by a F scope, and
there is no L scope in the sentence, we search
the first possible word from the end of the
sentence according to a dictionary, which ex-
tracted from the training corpus, and assign it
as L scope. The scope of the hedge is formed
by the tokens between them.
? If a hedge is only bracketed by a F scope, and
there are at least one L scope in the sentence,
we think the last L scope is the L scope of the
hedge, and its scope is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there is no F scope in the sentence, we
search the first possible word from the begin-
ning of the sentence to the hedge according to
the dictionary, and assign it as F scope. The
scope of the hedge is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there are at least one F scope in the sen-
tence, we search the first possible word from
the hedge to the beginning of the sentence ac-
cording to the dictionary, and think it as the
F scope of the hedge. The scope of the hedge
is formed by the tokens between them.
? If a hedge is bracketed by neither of them, we
remove it.
3 Experiments and Results
Two annotated corpus: BioScope and Wikipedia
are supplied for the CoNLL-2010 shared task. The
BioScope corpus consists of two parts: biological
paper abstracts and biological full papers, and it
is used for two subtasks. The Wikipedia corpus is
only used for hedge detection. The detailed infor-
mation of these two corpora is shown in Table 1
and Table 2, respectively.
Abstracts Papers Test
#Documents 1273 9 15
#Sentences 11871 2670 5003
%Hedge sent. 17.70 19.44 15.75
#Hedges 2694 682 1043
#AvL. of sent. 30.43 27.95 31.30
#AvL. of scopes 17.27 14.17 17.51
Table 1: The detailed information of BioScope
corpus. ?AvL.? stands for average length.
Train Test
#Documents 2186 2737
#Sentences 11111 9634
%Hedge sentences 22.36 23.19
#Hedges 3133 3143
#AvL. of sentences 23.07 20.82
Table 2: The detail information of Wikipedia cor-
pus. ?AvL.? stands for average length.
In our experiments, CRF++-0.533 implemen-
3http://crfpp.sourceforge.net/
15
tation is employed to CRF, and svm hmm 3.104
implementation is employed to the large margin
method. All parameters are default except C
(the trade-off between training error and margin,
C=8000, for selecting C, the training corpus is par-
titioned into three parts, two of them are used for
training and the left one is used as a development
dataset) in svm hmm. Both of them are state-of-
the-art toolkits for the sequence labeling problem.
3.1 Hedge Detection
We first compare the performance of each single
classifier with the cascaded system on two corpora
in domain, respectively. Each model is trained by
whole corpus, and the performance of them was
evaluated by the official tool of the CoNLL-2010
shared task. There were two kinds of measure:
one for sentence-level performance and another
one for cue-match performance. Here, we only
focused on the first one, and the results shown in
Table 3.
Corpus System Prec. Recall F1
CRF 87.12 86.46 86.79
BioScope LM 85.24 87.72 86.46
CAS 85.03 87.72 86.36
CRF 86.10 35.77 50.54
Wikipedia LM 82.28 41.36 55.05
CAS 82.28 41.36 55.05
Table 3: In-sentence performance of the hedge
detection subsystem for in-domain test. ?Prec.?
stands for precision, ?LM? stands for large mar-
gin, and ?CAS? stands for cascaded system.
From Table 3, we can see that the cascaded sys-
tem is not better than other two single classifiers
and the single CRF classifier achieves the best per-
formance with F-measure 86.79%. The reason for
selecting this cascaded system for our final sub-
mission is that the cascaded system achieved the
best performance on the two training corpus when
we partition each one into three parts: two of them
are used for training and the left one is used for
testing.
For cross-domain test, we train a cascaded clas-
sifier using BioScope+Wikipedia cropus. Table 4
shows the results.
As shown in Table 5, the performance of cross-
domain test is worse than that of in-domain test.
4http://www.cs.cornell.edu/People/tj/svm light/svm-
hmm.html
Corpus Precision Recall F1
BioScope 89.91 73.29 80.75
Wikipedia 81.56 40.20 53.85
Table 4: Results of the hedge detection for cross-
domain test. ?LM? stands for large margin, and
?CAS? stands for cascaded system.
3.2 Hedge Scope Detection
For test the affect of postprocessing for hedge
scope detection, we test our system using two eval-
uation tools: one for scope tag and the other one
for sentence-level scope (the official tool). In or-
der to evaluate our system comprehensively, four
results are used for comparison. The ?gold? is the
performance using golden hedge tags for test, the
?CRF? is the performance using the hedge tags
prediction of single CRF for test, the ?LM? is the
performance using the hedge tag prediction of sin-
gle large margin for test, and ?CAS? is the per-
formance of using the hedge tag prediction of cas-
caded subsystem for test. The results of scope tag
and scope sentence-level are listed in Table 5 and
Table 6, respectively. Here, we should notice that
the result listed here is different with that submit-
ted to the CoNLL-2010 shared task because some
errors for feature extraction in the previous system
are revised here.
HD tag Precision Recall F1
F scope 92.06 78.83 84.94
gold L scope 80.56 68.67 74.14
NONE 99.68 99.86 99.77
F scope 78.83 66.89 72.37
CRF L scope 72.52 60.50 65.97
NONE 99.56 99.75 99.65
F scope 77.25 67.57 72.09
LM L scope 72.33 61.41 66.42
NONE 99.56 99.73 99.31
F scope 77.32 67.86 72.29
CAS L scope 72.00 61.29 66.22
NONE 99.57 99.73 99.65
Table 5: Results of the hedge scope tag. ?HD?
stands for hedge detection subsystem we used,
?LM? stands for large margin, and ?CAS? stands
for cascaded system.
As shown in Table 5, the performance of
L scope is much lower than that of F scope.
Therefore, the first problem we should solve is
16
HD subsystem Precision Recall F1
gold 57.92 55.95 56.92
CRF 52.36 48.40 50.30
LM 51.06 48.89 49.95
CAS 50.96 48.98 49.95
Table 6: Results of the hedge scope in-sentence.
?HD? stands for hedge detection subsystem we
used, ?LM? stands for large margin, and ?CAS?
stands for cascaded system.
how to improve the prediction performance of
L scope. Moreover, compared the performance
shown in Table 5 and 6, about 15% (F1 of L scope
in Table 5 - F1 in Table 6) scope labels are mis-
matched. An efficient postprocessing is needed to
do F-L scope pair match.
As ?CRF? hedge detection subsystem is bet-
ter than the other two subsystems, our system
achieves the best performance with F-measure
50.30% when using the ?CRF? subsystem.
4 Conclusions
This paper presents a cascaded system for the
CoNLL-2010 shared task, which contains two
subsystems: one for detecting hedges and an-
other one for detecting their scope. Although
the best performance of hedge detection subsys-
tem achieves F-measure 86.79%, the best per-
formance of the whole system only achieves F-
measure 50.30%. How to improve it, we think
some complex features such as context free gram-
mar may be effective for detecting hedge scope.
In addition, the postprocessing can be further im-
proved.
Acknowledgments
We wish to thank the organizers of the CoNLL-
2010 shared task for preparing the datasets
and organizing the challenge shared tasks.
We also wish to thank all authors supply-
ing the toolkits used in this paper. This
research has been partially supported by the
National Natural Science Foundation of China
(No.60435020 and No.90612005), National 863
Program of China (No.2007AA01Z194) and the
Goal-oriented Lessons from the National 863 Pro-
gram of China (No.2006AA01Z197).
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu.
2005. Using Maximum Entropy to Extract Biomed-
ical Named Entities without Dictionaries. In Sec-
ond International Joint Conference on Natural Lan-
guage Processing, pages 268?273.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
17
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 76?82,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Mixed Deterministic Model for Coreference Resolution 
         
Bo Yuan1, Qingcai Chen, Yang Xiang, Xiaolong Wang2 
Liping Ge, Zengjian Liu, Meng Liao, Xianbo Si 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, Harbin Institute of Technology 
Shenzhen graduate School, Shenzhen, Guangdong, 518055, China 
{yuanbo.hitsz1, windseedxy, qingcai.chen, geliping123, 
autobotsonearth, dream2009gd, sixianbo}@gmail.com 
wangxl@insun.hit.edu.cn2 
 
 
 
Abstract 
This paper presents a mixed deterministic 
model for coreference resolution in the 
CoNLL-2012 shared task. We separate the 
two main stages of our model, mention 
detection and coreference resolution, into 
several sub-tasks which are solved by 
machine learning method and  
deterministic rules based on multi-filters, 
such as lexical, syntactic, semantic, gender 
and number information. We participate in 
the closed track for English and Chinese, 
and also submit an open result for Chinese 
using tools to generate the required features. 
Finally, we reach the average F1 scores 
58.68, 60.69 and 61.02 on the English 
closed task, Chinese closed and open tasks.  
1 Introduction 
The coreference resolution task is a complicated 
and challenging issue of natural language 
processing. Although many sub-problems, such as 
noun phrase to noun phrase and pronouns to noun 
phrase, are contained in this issue, it is interesting 
that humans do not get too confused when they 
determine whether two mentions refer to the same 
entity. We also believe that automatic systems 
should copy the human behavior (Kai-Wei et al, 
2011). In our understanding, the basis for human 
making judgment on different sub-problems is 
different and limited. Although there are some 
complicated and ambiguous cases in this task, and 
we are not able to cover all the prior knowledge of 
human mind, which plays a vital role in his 
solution, the mixed deterministic model we 
constructed can solve a big part of this task. We 
present a mixed deterministic model for 
coreference resolution in the CoNLL-2012 shared 
task (Sameer et al, 2011). 
Different methods such as Relaxation labeling 
(Emili et al, 2011), Best-Link (Kai-Wei et al, 
2011), Entropy Guided Transformation Learning 
(Cicero et al, 2011) and deterministic models 
(Heeyoung et al, 2011), were attempted in the 
CoNLL-2011 shared task (Sameer et al, 2011). 
The system performance reported by the task 
shows that a big part of this task has been solved 
but some sub-problems need more exploration. 
We also participate in the Chinese closed and 
open tracks. However, the lack of linguistic 
annotations makes it more difficult to build a 
deterministic model. Basic solutions such as Hobbs 
Algorithm and Center Theory have been listed in 
(Wang et al, 2002; Jun et al, 2007). The recent 
research on Chinese contains non-anaphors 
detection using a composite kernel (Kong Fang, et 
al., 2012(a)) and a tree kernel method to anaphora 
resolution of pronouns (Kong Fang et al, 2012(b)). 
We accept the thought of Stanford (Karthik et al, 
2010; Heeyoung et al, 2011). In Stanford system 
the coreference resolution task is divided into 
several problems and each problem is solved by 
rule based methods. For English we did some 
research on mention detection which uses Decision 
Tree to decide whether the mention ?it? should 
refer to some other mention. For Chinese we 
submit closed and open result. The lack of gender, 
76
number and name entities make it more difficult 
for the Chinese closed task and we try to extract 
information from the training data to help enhance 
the performance. For the open task, we use some 
dictionaries such as appellation dictionary, gender 
dictionary, geographical name dictionary and 
temporal word dictionary (Bo et al, 2009), and 
some tools such as conversion of pinyin-to-
character and LTP which is a Chinese parser that 
can generate the features such as Part-of-Speech, 
Parse bit, Named Entities (Liu et al, 2011) to 
generate the similar information. 
We describe the system architecture in section 2. 
Section 3 illustrates the mention detection process. 
Section 4 describes the core process of coreference 
resolution. In section 5, we show the results and 
discussion of several experiments. Finally, we give 
the conclusion of our work in section 6.  
2 System Architecture 
Our system mainly contains mention detection and 
coreference resolution. Recall is the determining 
factor in mention detection stage. The reason is 
that if some mention is missed in this stage, the 
coreference resolution part will miss the chains 
which contain this mention. Yet some mentions 
still need to be distinguished because in some cases 
they refer to no entity. For example ?it?, in the 
sentence ?it + be + weather/ time?, ?it? should refer 
to no entity. But the ?it? in the phrase ?give it to 
me? might refer to some entity. The coreference 
resolution module of our system follows the idea 
of Stanford. In the English task we did some more 
exploration on mention detection, pronoun 
coreference and partial match of noun phrases. The 
Chinese task is more complicated and because 
gender, number and name entities are not provided, 
the feature generation from the training data has to 
be added before the coreference resolution process. 
Some Chinese idiomatic usages are also considered 
in this stage.  
3 Mention detection  
All the NPs, pronouns and the phrases which are 
indexed as named entities are selected as 
candidates. NPs are extracted from the parse tree. 
Yet some mentions do not refer to any entity in 
some cases. In our system we attempt to 
distinguish these mentions in this stage. The reason 
is that the deterministic rules in coreference 
resolution part are not complete to distinguish 
these mentions. The methods below can also be 
added to the coreference resolution part as a pre-
processing. For the conveniences of system design, 
we finish this work in this stage. 
For English, the pronoun ?it? and NPs ?this, that, 
those and these? need to be distinguished. We take 
?it? as an example to illustrate the process. First we 
use regular expressions to select ?it?, which refers 
to no entity, such as ?it + be + weather/ time?, ?it 
happened that? and ?it makes (made) sense that?.  
Second we use Decision Tree (C4.5) to classify the 
two kinds of ?it? based on the training data. The 
features contain the Part-of-Speech, Parse bit, 
Predicate Arguments of ?it?, the word before and 
after ?it?. The number of total ?it? is 9697 and 4043 
of them have an entity to refer to in the training 
data. 
 
Category Precision Recall F 
no entity refered 
entity refered 
0.576 
0.747 
0.596 
0.731 
0.586
0.739
total 0.682 0.679 0.68
 
Table 1: Results of ?it? classification using C4.5 
 
Table 1 shows the classification result of ?it? in 
the development data v4. The number of total ?it? 
is 1401 and 809 of them have an entity to refer to. 
The result is not perfect but can help enhance the 
performance of coreference resolution. However, 
the results of ?this, that, those and these? are not 
acceptable and we skip over these words. We did 
not do any process on ?verb? mention detection and 
coreference resolution. 
In addition, we divide mentions into groups in 
which they are nested in position. And for 
mentions which have the same head word in one 
group, only the mentions with the longest span 
should be left (for the English task and a set of 
Chinese articles). For some Chinese articles of 
which names contain ?chtb?, both in the training 
data and the development data, the nest is 
permitted based on the statistic results.  
For Chinese we also attempt to train a model for 
pronouns ???(you) and ???(that). However, the 
results are not acceptable either since the features 
we select are not enough for the classifier. 
After the mentions have been extracted, the 
related features of each mention are also extracted. 
We transform the ?conll? document into mention 
77
document. Each mention has basic features such as 
position, part-of-speech, parse tree, head word, 
speaker, Arguments, and the gender and number of 
head word. The head word feature is very 
important and regular expression can almost 
accomplish the process but not perfectly. Firstly, 
we extract the key NPs of a mention based on 
parse feature. Then the regular expressions are to 
extract the head word. For example, the mention: 
 (NP (DNP (LCP (NP (NP (NR ??)) (NP (NN ??))) 
(LC ?)) (DEG ?)) (NP (NR ??)) (NP (NN ??))) (NP 
(DNP (LCP (NP (NP (NR ??)) (NP (NN ??))) (LC ?)) 
(DEG ?)) (NP (NR ??)) (NP (NN ??)))  
The key NPs of this mention is: 
(NP (NR ??)) (NP (NN ??)) .The head word of 
this mention is: NN ?? 
However, there are still some cases that need to 
be discussed. For example, the head word of ?the 
leader of people? should be ?leader?, while the head 
word of ?the city of Beijing? should be ?city? and 
?Beijing? for the mentions of ?the city? and 
?Beijing? both have the same meaning with ?the 
city of Beijing?. Finally, we only found the words 
of ?city? and ?country? should be processed. 
4 Coreference resolution  
The deterministic rules are the core methods to 
solve the coreference resolution task. All the 
mentions in the same part can be seen as a list. The 
mentions which refer to the same entity will be 
clustered based on the deterministic rules. After all 
the clusters have generated, the merge program 
will merge the clusters into chains based on the 
position information. The mentions in one chain 
cannot be reduplicative in position. Basically the 
nested mentions are not allowed. 
The process contains two parts NP-NP and NP-
pronoun. Each part has several sub-problems to be 
discussed. First, the same process of English task 
and Chinese task will be illustrated. Then the 
different parts will be discussed separately. 
4.1 NP-NP 
Exact match: the condition of exact match is the 
two NP mentions which have no other larger 
parent mentions in position are coreferential if they 
are exactly the same. The stop words such as ?a?, 
?the?, ?this? and ?that? have been removed.  
Partial match: there are two conditions for 
partial match which are the two mentions have the 
same head word and one of them is a part of the 
other in form simultaneously.  
Alias and abbreviation: some mentions have 
alias or abbreviation. For example the mentions 
?USA? and ?America? should refer to the mention 
?the United States?. 
Similar match:  there are three forms of this 
match. The first one is all the modifiers of two NPs 
are same and the head words are similar based on 
WordNet1 which is provided for the English closed 
task. We only use the English synonym sets of the 
WordNet to solve the first form. The second one is 
the head words are same and the modifiers are not 
conflicted. The third form is that the head words 
and modifiers are all different. The result of similar 
match may be reduplicative with that of exact 
match and partial match. This would be eliminated 
by the merge process. 
4.2 Pronoun - NP 
There are seven categories of pronoun to NP in our 
system. For English second person, it is difficult to 
distinguish the plural form from singular form and 
we put them in one deterministic rule. For each 
kind of pronouns shown below, the first cluster is 
the English form and the second cluster is the 
Chinese form.  
First Person (singular) = {'I', 'my', 'me', 'mine', 
'myself'}{???} 
Second Person= {'you', 'your', 'yours', 'yourself', 
'yourselves'}{???? ????} 
Third Person (male) = {'he', 'him', 'his', 
'himself'}{???} 
Third Person (female) = {'she', 'her', 'hers', 
'herself'}{???} 
Third Person (object) = {'it', 'its', 'itself'}{???} 
First Person (plural) = {'we', 'us', 'our', 'ours', 
'ourselves'}{????} 
Third Person (plural) = {'they', 'them', 'their', 
'theirs', 'themselves'}{????? ?????????} 
In the Chinese task the possessive form of 
pronoun is not considered. For example, the 
mention ???  ? ?(our) is a DNP in the parse 
feature and it contains two words ???? and ???. 
We only selected the NP ????as a mention. The 
reflexive pronouns are composed by two words 
which are the pronoun itself and the word ????. 
                                                          
1 http://wordnet.princeton.edu/ 
78
For example, the mention ??  ???(myself) is 
processed as ???(I or me).  
Gender, number and distance between pronoun 
and NP are the most important features for this part 
(Shane et al, 2006). We only allow pronoun to 
find NPs at first. We find out the first mention of 
which all the features are satisfied ahead of the 
pronoun. If there is no matching mention, search 
backward from the pronoun. For the first person 
and second person, we merged all the pronouns 
with the same form and the same speaker. If the 
context is a conversation of two speakers, the 
second person of a speaker should refer to the first 
person of the other speaker. The scene of multi-
speakers conversation is too difficult to be solved. 
In the Chinese task there are some other 
pronouns. The pronoun ????(both sides) should 
refer to a plural mention which contain ???(and) 
in the middle. The pronoun ?? ? has similar 
meaning of third person and refers to the largest 
NP mention before it. The pronouns ?? ?(this), 
?? ?(that), ??? ?(here), ??? ?(there) are not 
processed for we did not find a good solution.  
However in some cases the provided gender and 
number are not correct or missing and we had to 
label these mentions based on the appellation 
words of the training data. For example, if the 
appellation word of a person is ?Mr.? or ?sir?, the 
gender should be male.  
4.3 Chinese closed task  
For the Chinese closed task NE, the gender and 
number are not provided. We used regular patterns 
to generate these features from the training data. 
In the NE (named entities) feature ?PERSON? is 
a very important category because most pronouns 
will refer to the person entity. To extract 
?PERSON?, we build a PERSON dictionary which 
contains all the PERSON mentions in training data, 
such as ????(Mr.) and ????(Professor).  If the 
same mention appears in the test data, we believe it 
is a person entity. However, the PERSON 
dictionary cannot cover all the PERSON mentions. 
The appellation words are extracted before or after 
the person entity. When some appellation word 
appears in the test data, the NP mention before or 
after the appellation word should be a person entity, 
if they compose a larger NP mention.  
The Gender feature was generated at the same 
time of the ?PERSON? generation. We separate the 
?PERSON? dictionary and appellation dictionary 
into male cluster and female cluster by the 
pronouns in the same chain.  
The generation of number feature is a little 
complicated. Since the Chinese word does not have 
plural form, the numerals and the quantifiers of the 
mention are the main basis to extract the number 
feature. We extract the numerals and the 
quantifiers from the training data and built regular 
expressions for determine the number feature of a 
mention in test data. Other determinative rules for 
number feature extraction are shown below: 
If the word ??? appears in a mention tail, this 
mention is plural. For example ????(student) is 
singular and ?????(students) is plural. 
If the word ???(and) appears in the middle of a 
mention A, and the two parts separated by ??? are 
sub-mentions of A,  mention A should be plural. 
Other words which have the similar meaning of 
???, such as ???, ??? and ???, are considered.  
The time and date coreference resolution is also 
considered. The NP mentions which contain 
temporal words are processed separately since 
these categories of name entity are not provided. 
These temporal words are also extracted from 
training data. Since the head words of these 
mentions are themselves, the two time or date 
mentions are coreferential if they are the same or 
one must be a part of the other?s tail. For example 
??????(this September) and ????(September) 
which are not nested should be coreferential. 
4.4 Chinese open task 
For the Chinese open task we use several tools to 
generate features we need. 
NE generation: LTP is a Chinese parser that can 
generate the features such as Part-of-Speech, Parse 
bit, Named Entities (Liu et al, 2011). We only use 
LTP for the NE generation. However, the NE 
labels of LTP are different with that provided by 
the gold training data and need to be transformed. 
The difference of word segmentation between LTP 
and the provided data also made some errors. At 
last we find the NE feature from LTP does not 
perform well and it will be discussed in section 5. 
The conversion of pinyin-to-character is also 
used in the Chinese open task. The speaker 
provided in the training data is given in pinyin 
form. The speaker might be the ?PERSON? 
mention in the context. When we determine the 
79
pronoun coreference, we need to know whether the 
speaker and the ?PERSON? mention are same.  
Other tools used in open task contain appellation 
dictionary, gender dictionary, geographical name 
dictionary and temporal word dictionary (Bo et al, 
2009). These dictionaries are more complete than 
those used in the closed task, although the 
enhancements are also limited. 
5 Results and Discussion  
Table 2 to table 4 show the results of English 
coreference resolution on the gold and auto 
development and the test data. The results of the 
auto development data and the test data are close 
and lower than that of the gold data. Since the 
deterministic rules can not cover all the cases, 
there is still an improvement if we could make the 
deterministic rules more complete. 
 
Measure R  P F1 
Mention detection 
MUC 
B3 
77.7 
65.1 
69.2 
71.8 
62.9 
70.9 
74.6
64 
70.1
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.4 48.9 47.6
60.6
 
Table 2: Results of the English gold development 
data  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
72.4 
62.3 
66.7 
71.5 
62.8 
71.8 
72 
62 
69.1
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.4 44.9 45.6
58.9
 
Table 3: Results of the English auto development 
data  
 
Measure R P F1 
Mention detection 
MUC 
B3 
73.2 
62.1 
66.2 
71.9
63 
70.5
72.53
63 
68.3
CEAF(E) 
CEAF(M) 
 BLANC 
(CEAF(E)+MUC+B3)/3 
45.7 
57.3 
72.1 
44.7
57.3
76.9
45.2
57.3
74.2
58.68
 
Table 4: Results of English test data  
 
The results of the closed Chinese performance 
on the gold and auto development and the test data 
are shown in table 5 to table 7. The performance of 
the auto development data and the test data has 
about 4% decline to that of the gold development 
on F1 of coreference resolution. It means the 
Chinese results are also partly affected by the parse 
feature. In fact we attempted to revise the parse 
feature of the auto development data using regular 
expressions. Yet the complicacy and unacceptable 
results made us abandon that.  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
 82.3 
71.6 
76.7 
69.8
64.3
74.2
75.5 
67.7 
75.4 
CEAF(E)
(CEAF(E)+MUC+B3)/3
49 56.5 52.5 
65.2 
 
Table 5: Closed results of the Chinese gold 
development data  
 
Measure R P  F1 
Mention detection 
MUC 
B3 
74.2 
63.6 
73.1 
66 
60 
73.5
70 
61.7 
73.3 
CEAF(E)
(CEAF(E)+MUC+B3)/3
47.3 50.6 48.9 
61.3 
 
Table 6: Closed results of the Chinese auto 
development data 
 
Measure R P F1 
Mention detection 
MUC 
B3 
72.8 
62.4 
73.1 
64.1 
58.4 
72.7 
68.15
60.3
72.9
CEAF(E) 
CEAF(M) 
BLANC 
(CEAF(E)+MUC+B3)/3
47.1 
59.6 
73.7 
50.7 
59.6 
78.2 
48.8
59.6
75.8
60.69
 
Table 7: Closed results of the Chinese test data  
 
The results of the open Chinese performance on 
the gold and auto development and the test data are 
shown in table 8 to table 10. The performance is 
similar with that of the closed task. However, the 
improvement between F1 of the open task and F1 
of the closed task is limited. We also get the F1 of 
the closed and open test results using gold parser 
which are 66.46 and 66.38. The open result is even 
80
lower. This can be explained. The performance 
enhanced by the dictionaries we used for the open 
task are limited because the open dictionaries 
information which appears in the test data is not 
much more than that of the closed dictionaries 
which generated from the training data, although 
the total information of the former is much larger.  
The named entities generated by LTP have some 
errors such as person identification errors and will 
caused coreferential errors in Pronoun-NP stage. 
For the time we did not use LTP well and some 
other open tools such as Wikipedia and Baidu 
Baike should be applied in the open task.  
  
Measure R P F1 
Mention detection 
MUC 
B3 
82.4 
72.3 
77.7 
69.3
63.8
73.3
75.3 
67.8 
75.4 
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
48.3 56.8 52.2 
65.1 
 
Table 8: Open results of the Chinese gold 
development data  
 
Measure R  P F1 
Mention detection 
MUC 
B3 
75.1 
64.9 
74.2 
65.7
59.9
72.6
70.1 
62.3 
73.4 
CEAF(E) 
(CEAF(E)+MUC+B3)/3 
46.7 51.5 49 
61.6 
 
Table 9: Open results of the Chinese auto 
development data 
 
Measure R P F1 
Mention detection 
MUC 
B3 
73.7 
63.7 
74 
64 
58.5
72.2
68.49
61 
73.1 
CEAF(E) 
CEAF(M) 
BLANC 
(CEAF(E)+MUC+B3)/3 
60.1 
46.8 
74.3 
60.1
51.5
78 
60.1 
49 
76 
61.02
 
Table 10: Open results of the Chinese test data  
 
The results of the gold-mention-boundaries and 
gold-mentions data of the English and Chinese 
closed task are shown in table 11 and 12. Although 
the mention detection stage is optimized by the 
gold-mention-boundaries and gold-mentions data 
and the final performance is enhanced, there is still 
space to enhance in the coreference resolution 
stage. The recall of mention detection of gold-
mentions is 99.8. This problem will be explored in 
our future work.  
 
Data R P F1 
Mention detection(A) 
gold-mention-boundaries
Mention detection(B) 
gold-mentions 
75.7 
 
80 
70.8
 
100
73.2 
59.50
88.91
69.88
 
Table 11: Results of the English closed gold-
mention-boundaries and gold-mentions data, (A) is 
the mention detection score of the gold-mention-
boundaries and (B) is the score of the gold-
mentions. 
 
Data R P F1 
Mention detection(A) 
gold-mention-boundaries
Mention detection(B) 
gold-mentions 
82.9 
 
81.7 
66.9
 
99.8
74.02
64.42
89.85
76.05
 
Table 12: Results of the Chinese closed gold-
mention-boundaries and gold-mentions data  
6 Conclusion 
In this paper we described a mixed deterministic 
model for coreference resolution of English and 
Chinese. We start the mention detection from 
extracting candidates based on the parse feature. 
The pre-processing which contains static rules and 
decision tree is applied to remove the defective 
candidates. In the coreference resolution stage the 
task is divided into several sub-problems and for 
each sub-problem the deterministic rules are 
constructed based on limited features. For the 
Chinese closed task we use regular patterns to 
generate named entities, gender and number from 
the training data. Several tools and dictionaries are 
applied for the Chinese open task. The result is not 
as good as we supposed since the feature errors 
caused by these tools also made the coreferential 
errors.  
However, a deeper error analysis is needed in 
the construction of deterministic rules. The feature 
of the predicate arguments is not used well. 
Although the open performance of the Chinese 
task is not good, we still believe that complete and 
accurate prior knowledge can help solve the task.  
81
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 
61173075 and 60973076), ZTE Foundation and 
Science and Technology Program of Shenzhen.  
References  
Bo Yuan, Qingcai Chen, Xiaolong Wang, Liwei Han. 
2009. Extracting Event Temporal Information based 
on Web. 2009 Second International Symposium on 
Knowledge Acquisition and Modeling, pages.346-
350  
Cicero Nogueira dos Santos, Davi Lopes Carvalho. 
2011. Rule and Tree Ensembles for Unrestricted 
Coreference Resolution. Proceedings of the 15th 
Conference on Computational Natural Language 
Learning: Shared Task, pages 51?55.  
Emili Sapena, Llu??s Padr?o and Jordi Turmo. 2011. 
RelaxCor Participation in CoNLL Shared Task on 
Coreference Resolution. Proceedings of the 15th 
Conference on Computational Natural Language 
Learning: Shared Task, pages 35?39. 
Heeyoung Lee, Yves Peirsman, Angel Chang, 
Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford?s Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared Task. 
Proceedings of the 15th Conference on 
Computational Natural Language Learning: Shared 
Task, pages 28?34, Portland, Oregon. 
Liu Ting, Che Wanxiang, Li Zhenghua. 2011. Language 
Technology Platform. Journal of Chinese 
Information Processing. 25(6): 53-62 
Jun Lang, Bing Qin, Ting Liu, Sheng Li. 2007. Intra-
document Coreference Resolution: The state of the 
art. Journal of Chinese Language and Computing. 17 
(4):227-253 
Kai-Wei Chang Rajhans Samdani. 2011. Inference 
Protocols for Coreference Resolution. Proceedings of 
the 15th Conference on Computational Natural 
Language Learning: Shared Task, pages 40?44, 
Portland, Oregon. 
Karthik Raghunathan, Heeyoung Lee, Sudarshan 
Rangarajan, Nathanael Chambers, Mihai Surdeanu, 
Dan Jurafsky, Christopher Manning. 2010. A Multi-
Pass Sieve for Coreference Resolution. In EMNLP. 
Kong Fang, Zhu Qiaoming and Zhou Guodong. 2012(a). 
Anaphoricity determination for coreference 
resolution in English and Chinese languages. 
Computer Research and Development (Chinese).  
Kong Fang and Zhou Guodong. 2012(b). Tree kernel-
based pronoun resolution in English and Chinese 
languages. Journal of Software (Chinese). Accepted: 
23(8).  
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, 
Martha Palmer, Ralph Weischedel and Nianwen 
Xue.2011. CoNLL-2011 Shared Task: Modeling 
Unrestricted Coreference in OntoNotes. Proceedings 
of the Fifteenth Conference on Computational 
Natural Language Learning (CoNLL 2011). Portland, 
OR. 
Sameer Pradhan and Alessandro Moschitti and Nianwen 
Xue and Olga Uryupina and Yuchen Zhang. 2012. 
CoNLL-2012 Shared Task: Modeling Multilingual 
Unrestricted Coreference in OntoNotes. Proceedings 
of the Sixteenth Conference on Computational 
Natural Language Learning (CoNLL 2012). Jeju, 
Korea. 
Shane Bergsma and Dekang Lin. 2006. Bootstrapping 
Path-Based Pronoun Resolution Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
pages 33?40, Sydney 
Wang Houfeng. 2002. Survey: Computational Models 
and Technologies in Anaphora Resolution. Journal of 
Chinese Information Processing. 16(6): 9-17. 
82
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115?122,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Hybrid Model for Grammatical Error Correction 
 
 
Yang Xiang, Bo Yuan, Yaoyun Zhang*, Xiaolong Wang?, 
Wen Zheng, Chongqiang Wei 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, 
Harbin Institute of Technology Shenzhen Graduate School, 
Shenzhen, Guangdong, 518055, P.R. China 
{windseedxy, yuanbo.hitsz, xiaoni5122, zhengwen379, weichongqiang}@gmail.com  
wangxl@insun.hit.edu.cn? 
 
 
 
Abstract 
This paper presents a hybrid model for the 
CoNLL-2013 shared task which focuses on the 
problem of grammatical error correction. This 
year?s task includes determiner, preposition, 
noun number, verb form, and subject-verb 
agreement errors which is more comprehen-
sive than previous error correction tasks. We 
correct these five types of errors in different 
modules where either machine learning based 
or rule-based methods are applied. Pre-
processing and post-processing procedures are 
employed to keep idiomatic phrases from be-
ing corrected. We achieved precision of 
35.65%, recall of 16.56%, F1 of 22.61% in the 
official evaluation and precision of 41.75%, 
recall of 20.29%, F1 of 27.3% in the revised 
version. Some further comparisons employing 
different strategies are made in our experi-
ments.   
1 Introduction 
Automatic Grammatical Error Correction (GEC) 
for non-native English language learners has at-
tracted more and more attention with the devel-
opment of natural language processing, machine 
learning and big-data techniques. ?The CoNLL-
2013 shared task focuses on the problem of GEC 
in five different error types including determiner, 
preposition, noun number, verb form, and sub-
ject-verb agreement which is more complicated 
and challenging than previous correction tasks. 
Other than most previous works which concen-
trate most on determiner and preposition errors, 
more error types introduces the possibility of 
correcting multiple interacting errors such as de-
                                                 
? Corresponding author 
terminer vs. noun number and preposition vs. 
verb form. 
Generally, for GEC on annotated data such as 
the NUCLE corpus (Dahlmeier et al, 2013) in 
this year?s shared task which contains both origi-
nal errors and human annotations, there are two 
main types of approaches. One of them is the 
employment of external language materials. Alt-
hough there are minor differences on strategies, 
the main idea of this approach is to use frequen-
cies as a filter, such as n-gram counts, and take 
those phrases that have relatively high frequen-
cies as the correct ones. Typical works are shown 
in (Yi et al, 2008) and (Bergsma et al, 2009). 
Similar methods also exist in HOO shared tasks1 
such as the web 1TB n-gram features used by 
Dahlmeier and Ng (2012a) and the large-scale n-
gram model described by Heilman et al (2012). 
The other type is machine learning based ap-
proach which considers most on local context 
including syntactic and semantic features. Han et 
al. (2006) take maximum entropy as their classi-
fier and apply some simple parameter tuning 
methods. Felice and Pulman (2008) present their 
classifier-based models together with a few rep-
resentative features. Seo et al (2012) invite a 
meta-learning approach and show its effective-
ness. Dahlmeier and Ng (2011) introduce an al-
ternating structure optimization based approach. 
Most of the works mentioned above focus on 
determiner and preposition errors. Besides, Lee 
and Seneff (2008) propose a method to correct 
verb form errors through combining the features 
of parse trees and n-gram counts. To our 
knowledge, no one focused on noun form errors 
in specific researches. 
In this paper, we propose a hybrid model to 
solve the problem of GEC for five error types. 
                                                 
1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 
115
Machine learning based methods are applied to 
solve determiner (ArtOrDet), preposition (Prep) 
and noun form (Nn) problems while rule-based 
methods are proposed for subject-verb agreement 
(SVA) and verb form (Vform) problems. We 
treat corrections of errors in each type as indi-
vidual sub problems the results of which are 
combined through a result combination module. 
Solutions on interacting error corrections were 
considered originally but dropped at last because 
of the bad effects brought about by them such as 
the accumulation of errors which lead to a very 
low performance. We perform feature selection 
and confidence tuning in machine learning based 
modules which contribute a lot to our perfor-
mance. Also, pre-processing and post-processing 
procedures are employed to keep idiomatic 
phrases from being corrected.  
Through experiments, we found that the result 
of the system was affected by many factors such 
as the selection of training samples and features, 
and the settings of confidence parameters in clas-
sifiers. Some of the factors make the whole sys-
tem too sensitive that it can easily be trapped into 
a local optimum. Some comparisons are shown 
in our experiments section. 
No other external language materials are in-
cluded in our model except for several NLP tools 
which will be introduced in ?5.2. We achieved 
precision of 35.65%, recall of 16.56% and F1 of 
22.61% in the official score of our submitted re-
sult. However, it was far from satisfactory main-
ly due to the ill settings of confidence parameters. 
Trying to find out a set of optimal confidence 
parameters, our model is able to reach an upper 
bound of precision of 34.23%, recall of 25.56% 
and F1 of 29.27% on the official test set. For the 
revised version, we achieved precision of 
41.75%, recall of 20.29%, and F1 of 27.3%. 
The remainder of this paper is arranged as fol-
lows. The next section introduces our system 
architecture. Section 3 describes machine learn-
ing based modules. Section 4 shows rule based 
modules. Experiments and analysis are arranged 
in Section 5. Finally, we give our discussion and 
conclusion in Section 6 and 7. 
2 System Architecture 
Initially, we treat errors of each type as individu-
al sub problems. Machine learning based meth-
ods are applied to solve ArtOrDet, Prep and Nn 
problems where similar problem solving steps 
are shared: sample generation, feature extraction, 
training, confidence tuning in development data, 
and testing. We apply some hand-crafted heuris-
tic rules in solving subject-verb agreement (SVA) 
and verb form (Vform) problems. Finally, results 
from different modules are combined together. 
The whole architecture of this GEC system is 
described in Figure 1. 
A pre-processing and a post-processing filter 
are utilized which include filters for some idio-
matic phrases extracted from the training dataset. 
The Frequent Pattern Growth Algorithm (FP-
Growth) is widely used for frequent pattern min-
ing in machine learning. In pre-processing, we 
firstly apply FP-Growth to gather the frequent 
items in the training set. Through some manual 
refinements, a few idiomatic phrases are re-
moved from the candidate set to be corrected. In 
post-processing, the idiomatic phrase list is used 
to check whether a certain collocation is still 
grammatical after several corrections are per-
formed. There are 996 idiomatic phrases in our 
list which is composed by mainly patterns from 
the training set and a series of hand-crafted ones. 
Typical phrases we extracted are in general, 
have/need to be done, on the other hand, a 
large/big number/amount of, at the same time, in 
public, etc.  
 
Figure 1. Architecture of our GEC system. 
3 Machine Learning Based Modules 
For the error types ArtOrDet, Prep and Nn, we 
choose machine learning based methods because 
we consider there is not enough evidence to di-
rectly determine which word or form to be used. 
Moreover, it is impossible to transfer all the cas-
es we encounter into rules. In this section, we 
describe our processing ideas for each error type 
respectively and then specifically introduce our 
feature selection and confidence tuning approach. 
3.1 Determiners 
Determiners in the error type ?ArtOrDet? contain 
articles a/an, the and other determiners such as 
Original texts 
Pre-processing 
Machine learning 
based modules 
Rule based mod-
ules 
Post-processing 
Corrected texts Result combination 
116
this, those, etc. This type of error accounts for a 
large proportion which is of great impact on the 
final result. We consider only articles since the 
other determiners are rarely used and the usages 
of them are sometimes ambiguous. Like ap-
proaches described in some previous works 
(Dahlmeier and Ng, 2012a; Felice and Pulman, 
2008), we assign three types a/an, the and empty 
for each article position and build a multi-class 
classifier.  
For training, developing and testing, all noun 
phrases (NPs) are chosen as candidate samples to 
be corrected. For NPs whose articles have been 
annotated in the corpus, the correct ones are their 
target categories, and for those haven?t been an-
notated, the target categories are their observed 
article types. Samples we make use of can be 
divided into two basic types in each category: 
with and without a wrong article. Two examples 
are shown below: 
with: a/empty big apples ~ empty category 
without: the United States ~ the category 
For each category in a, the, and empty, we use 
the whole with data and take samples of without 
ones from the set of correct NPs to make up 
training instances of one category. The reason 
why we make samples of the without ones is for 
the consideration that the classifier would always 
predicts the observed article and never proposes 
any corrections if given too many without sam-
ples, the case of which is mentioned in (Dahl-
meier and Ng, 2012a). However, we found that 
the ratio of with-without shows little effect in our 
model. The article a is regulated to a or an ac-
cording to pronunciation. 
Syntactic and semantic features are considered 
in feature extraction with the help of WordNet 
and the ?.conll? file provided. We adopt syntac-
tic features such as the surface word, phrase, 
part-of-speech, n-grams, constituent parse tree, 
dependency parse tree and headword of an NP; 
semantic features like noun category and hyper-
nym. Some expand operations are also done 
based on them (reference to Dahlmeier and Ng, 
2012a; Felice and Pulman, 2008). After feature 
extraction, we apply a genetic algorithm to do 
feature subset selection in order to reduce dimen-
sionality and filter out noisy features which is to 
be described in ?3.4. 
Maximum Entropy (ME) has been proven to 
behave well for heterogeneous features in natural 
language processing tasks and we adopt it to 
train our model. We have also tried several other 
classifiers including SVM, decision tree, Na?ve 
Bayes, and RankSVM but finally find ME per-
forms well and stably. It provides confidence 
scores for each category which we will make use 
of downstream.  
3.2 Prepositions 
Preposition error correction task is similar to the 
previous one except the different categories and 
corresponding features. Since there are 36 com-
mon prepositions listed by the shared task, origi-
nally, we assign 37 types including 36 preposi-
tions and empty for each preposition position and 
build a multi-class classifier. For training, devel-
oping and testing, each preposition as well as the 
empty position directly after a verb is considered 
as a candidate. Syntactic and semantic features 
extracted are similar to those in article error cor-
rection except for some specific cases for prepo-
sitions such as the verbs related to prepositions 
and the dependency relations. Similarly, we treat 
those preposition phrases with and without a cer-
tain preposition as the two types of samples in 
training (as described in ?3.1). Two examples are 
listed below: 
with: on/in the 1860s~ in category 
without: have to be done ~ to category 
Through statistics on the training data, we 
found that most prepositions have very few sam-
ples which may not contribute to the perfor-
mance at all and even bring about noise when 
assigned to wrong categories. After several 
rounds of experiments, we finally adopt a classi-
fier with seven prepositions which are frequently 
used in the whole corpus. They are on, of, in, at, 
to, with and for. As to the classifier, ME also 
outperforms the others. 
3.3 Noun Form 
Noun form may be interacting with determiners 
and verbs which may also have errors in the orig-
inal text. So errors may occur in the context fea-
tures extracted from the original text. However, 
if we use the context features that have been cor-
rected, more errors would be employed due to 
the low performance of the previous steps. 
Through statistics, we found that co-occurrence 
between two types of errors such as SVA and 
ArtOrDet only accounts for a small proportion. 
After a few experiments, we decided to give up 
interacting errors so as to avoid accumulated er-
rors.  
This is a binary classification problem. All 
head nouns in NPs are considered as candidates. 
Each category contains with and without samples 
similar to the cases in ?3.1 and ?3.2. Features are 
highly related to the deterministic factors for the 
117
head noun form such as the countability, Word-
Net type, name entity and whether there some 
specific dependency relations including det, 
amod etc.  
ME also outperforms other classifiers. 
3.4 Feature Selection Using Genetic Algo-
rithm 
Features we extracted are excessive and sparse 
after binarization. They bring noise in quality as 
well as complexity in computation and need to 
be selected a priori. In our work, it is a wrapper 
feature selection task. That is, we have to select a 
combination of features that perform well to-
gether rather than make sure each of them be-
haves well. This GEC task is interesting in fea-
ture selection because word surface features that 
are observed only once are also effective while 
we think that they overfit. Genetic algorithm 
(GA) has been proven to be useful in selecting 
wrapper features in classification (ElAlami, 2009; 
Anba-rasi et al 2010). We used GA to select fea-
tures as well as reduce feature dimensionality.  
We convert the features into a binary sequence 
in which each character represents one dimen-
sion.  Let ?1? indicates that we keep this dimen-
sion while ?0? means that we drop it, we use a 
binary sequence such as ?0111000?100? to de-
note a combination of feature dimensions. GA 
functions on the feature sequences and finally 
decides which features should be kept. The fit-
ness function we used is the evaluation measure 
F1 described in ?5.3. 
3.5 Confidence Tuning 
The Maximum Entropy classifier returns a confi-
dence score for each category given a testing 
sample. However, for different samples, the dis-
tribution of predicted scores varies a lot. For 
some samples, the classifier may have a very 
high predicted score for a certain category which 
means the classifier is confident enough to per-
form this prediction. But for some other samples, 
two or more categories may share close scores, 
the case of which means the classifier hesitates 
when telling them apart. 
We introduce a confidence tuning approach on 
the predicted results through a comparison be-
tween the observed category and the predicted 
category which is similar to the ?thresholding? 
approach described in Tetreault and Chodorow 
(2008). The main idea of the confidence tuning 
algorithm is: the choice between keep and drop is 
based on the difference between the confidence 
scores of the predicted category and the observed 
category. If this difference goes beyond a thresh-
old t, the prediction is kept while if it is under t, 
we won?t do any corrections. We believe this 
tuning strategy is especially appropriate in this 
task since to distinguish whether the observed 
category is correct or not affects a lot to the pre-
dicted result.  
The confidence threshold for each category is 
generated through a hill climbing algorithm in 
the development data aimed at maximizing F1-
meaure of the result.  
4 Rule-based Modules 
A few hand-crafted rules are applied to solve the 
verb related corrections including SVA and 
Vform. In these cases, the verb form is only re-
lated to some specific features as described by 
Lee and Seneff (2008). 
4.1 SVA 
SVA (Subject-verb-agreement) is particularly 
related to the noun subject that a verb determines. 
In the dependency tree, the number of the noun 
which has a relation nsubj with the verb deter-
mines the form of this verb. Through observation, 
we find that the verbs to be considered in SVA 
contain only bes (including am, is, are, was, 
were) and the verbs in simple present tense 
whose POSs are labeled with VBZ (singular) or 
VBP(plural).  
To pick out the noun subject is easy except for 
the verb that contained in a subordinate clause. 
We use semantic role labeling (SRL) to help 
solve this problem in which the coordinated can 
be extracted through a trace with the label ?R-
Argument?. The following Figure is an example 
generated by the SRL toolkit mate-tools (Bernd 
Bohnet, 2010)2. 
 
 
Figure 2. SRL for the demo sentence ?Jack, who 
will show me the way, is very tall.? The subject of 
the verb show can be traced through R-A0 -> A0. 
 
  However, the performance of this part is partly 
correlated with the noun form that may have er-
rors in the original text and the wrong SRL result 
brought about because of wrong sentence gram-
mars. 
                                                 
2 http://code.google.com/p/mate-tools/ 
118
4.2 Verb Form 
The cases are more complicated in the verb form 
error correction task. Modal, aspect and voice are 
all forms that should be considered for a verb. 
And sometimes, two or more forms are com-
bined together to perform its role in a sentence. 
For example, in the sentence: 
He has been working in this position for a 
long time. 
The bolded verb has been working is a com-
bination of the active voice work, the progressive 
aspect be+VBG and the perfect aspect has+VBN. 
It is a bit difficult for us to take all cases into 
consideration, so we just apply several simple 
rules and solve a subset of problems for this type. 
Some typical rules are listed below: 
1. The verb that has a dependency relation aux 
to preposition to is modified to its base form. 
2. The verb that has a dependency relation 
pcomp to preposition by is modified to its past 
form. 
3. The verb related to other prepositions (ex-
cept to and by) is modified to ~ing form. 
4. The verb depends on auxiliary do and mod-
al verb (including its inflections and negative 
form) is modified to its base form. 
We have also tried to use SRL and transitivity 
of a verb to determine the active and passive 
voice but it didn?t work well. 
5 Experiments and Analysis 
5.1 Data Description 
The NUCLE corpus introduced by NUS (Nation-
al University of Singapore) contains 1414 essays 
written by L2 students with relatively high profi-
ciency of English in which grammatical errors 
have been well annotated by native tutors. It has 
a small proportion of annotated errors which is 
much lower than other similar corpora (Dahl-
meier et al, 2013). In our experiments, we divide 
the whole corpus into 80%, 10% and 10% for 
training, developing and testing. And we use 90% 
and 10% for training and developing for the final 
test. 
5.2 External tools and corpora 
External tools we used include WordNet (Fell-
baum, 1998) for word base form and noun cate-
gory generation, Morphg (Minnen et al, 2000)3 
to generate inflections of nouns and verbs, mate-
tools (Bohnet, 2010) for SRL, Stanford-ner 
                                                 
3 http://www.informatics.sussex.ac.uk/research/groups/nlp 
/carroll/morph.html 
(Finkel et al, 2005)4 for name entity extraction 
and Longman online dictionary5  for generation 
of noun countability and verb transitivity.  
We didn?t employ any external corpora in our 
system. 
5.3 Experiments 
The performance of each machine learning mod-
ule is affected by the selection of training sam-
ples, features and confidence tuning for the max-
imum entropy classifier. All these factors con-
tribute more or less to the final performance and 
need to be carefully developed. In our experi-
ments, we focus on machine learning based 
modules and make comparisons on sample selec-
tion, confidence tuning and feature selection and 
list a series of results before and after applying 
our strategies.  
In our experiment, the performance is meas-
ured with precision, recall and F1-measure where 
1
2 precision recallF precision recall
? ?? ?
 
Precision is the amount of predicted correc-
tions that are also corrected by the manual anno-
tators divided by the whole amount of predicted 
corrections. Recall has the same numerator as 
precision while its denominator is the amount of 
manually corrected errors. They are in accord-
ance with those measurements generated by the 
official m2scorer (Dahlmeier and Ng, 2012c) to a 
great extent and easily to be integrated in our 
program. 
As we have mentioned in Section 3, we don?t 
employ all samples but make use of all with 
(with errors and annotations) instances and sam-
ple the without ones (without errors) for training. 
And the sampling for without type is totally ran-
dom without loss of generality. We apply the 
same strategy in all of these three error types 
(ArtOrDet, Prep and Nn) and try several ratios of 
with-without to find out whether this ratio has 
great impact on the final result and which ratio 
performs best. We use the 80%-10%-10% data 
(mentioned in ?5.1) for our experiments and 
make comparisons of different ratios on develop-
ing data. The experimental results are described 
in detail in Figure 3. 
Confidence tuning is applied in all these three 
error types which contributes most to the final 
performance in our model. We compare the re-
sults before and after tuning in all sample ratios 
                                                 
4 http://www-nlp.stanford.edu/software/CRF-NER.shtml 
5 http://www.ldoceonline.com/ 
119
that we designed and they are also depicted in 
Figure 3.  
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
precision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-1. Comparisons before and after tuning 
in ArtOrDet. 1:all means to use the whole without 
samples. 
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.1
.2
.3
.4
.5
.6
presision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-2. Comparisons before and after tuning in 
Prep.  
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
1.0
precision before and after tuning
recall before and after tuning
F
1
 before and after tuning
Sample with:without
 
Figure 3-3. Comparisons before and after tuning 
in Nn. 
 
From the three groups of data in Figure 3, we 
notice that the ratio of samples has little impact 
on F1. This phenomenon shows that our conclu-
sion goes against the previous work by Dahl-
meier and Ng (2012a). We believe it is mainly 
due to our confidence tuning which makes the 
parameters vary much under different sample 
ratios, that is, if given the same parameters, the 
effect of sample ratio selection may become ob-
vious. Unfortunately, we didn?t do such a sys-
tematic comparison in our work. The improve-
ment under confidence tuning can be seen clearly 
in all ratios of with-without samples. The confi-
dence tuning algorithm employed in our work is 
better than the traditional tuning methods that 
assign a fixed threshold for each category or for 
all categories (about 1%~2% better measured by 
F1).  
However, although we are able to pick out the 
training data with a high F1 through confidence 
tuning for the developing data, it is difficult for 
us to choose a set of confidence parameters that 
also fits the test data well. Given several close 
F1s, the numerical values of denominators and 
numerators which determine the precision and 
recall can vary a lot. For example, one set that 
has a high precision and low recall may share the 
similar F1 with another set that has a low preci-
sion and high recall. Our work lacked of the de-
velopment on how to control the number of pro-
posed errors to make leverage on the perfor-
mance between developing set and testing set. It 
resulted in that the developing set and the testing 
set were not balanced at all, and our model was 
not able to keep the sample distribution as the 
training set. This is the main factor that leads to a 
low performance in our submitted result which 
can be clearly seen in Table 1. The upper bound 
performance of our system achieves precision of 
34.23%, recall of 25.56% and F1 of 29.27%, in 
which the F1 goes 7% beyond our submitted sys-
tem. We notice that results of all metrics of the 
three error types where machine learning algo-
rithms are applied improve with the simultaneous 
increase of numerators and denominators. This is 
especially noticeable in Prep. 
For the other two types SVA and Vform, we 
just apply several heuristic rules to solve a subset 
of problems and the case of Vform has not been 
solved well such as tense and voice. 
Genetic Algorithm (GA) is applied to process 
feature reduction and subset selection. This is 
done in ArtOrDet type in which we extract as 
many as 350,000 binary features. For error type 
Prep and Nn, the feature dimensionalities we 
constructed were not as high as that in ArtOrDet, 
and the improvements under GA were not obvi-
ous which we would not discuss in this work. 
Through experiments on a few sample ratios, we 
notice that feature selection using genetic algo-
rithm is able to reduce the feature dimensionality 
to about 170,000 which greatly lowers down the 
120
downstream computational complexity. However, 
the improvement contributed by GA after confi-
dence tuning is not obvious as that before confi-
dence tuning. We think it is partly because of the 
bad initialization of GA which is to be improved 
in our future work. The unfixed parameters may 
also lead to such a result which we didn?t discuss 
enough in our work. The comparison before and 
after GA is described in Figure 4. 
 
 Our submission% Upper bound% 
P(Det) 41.38(168/406) 36.44(254/697) 
R(Det) 24.35(168/690) 36.81(254/690) 
F1(Det) 30.66 36.63 
P(Prep) 13.79(4/29) 26.12(35/134) 
R(Prep) 1.29(4/311) 11.25(35/311) 
F1(Prep) 2.35 15.73 
P(Nn) 24.81(65/262) 27.27(102/374) 
R(Nn) 16.41(65/396) 25.76(102/396) 
F1(Nn) 19.76 26.49 
P(SVA) 
R(SVA) 
F1(SVA) 
24.42(21/86) 
16.94(21/124) 
20.00 
24.42(21/86) 
16.94(21/124) 
20.00 
P(Vform) 
R(Vform) 
F1(Vform) 
19.35(6/31) 
4.92(6/122) 
7.84 
19.35(6/31) 
4.92(6/122) 
7.84 
P(all) 35.65(272/763) 34.23(420/1227) 
R(all) 16.56(272/1643) 25.56(420/1643) 
F1(all) 22.61 29.27 
Table 1. Different performances according to dif-
ferent confidence parameters. Det stands for Ar-
tOrDet. 
 
Pre-processing and post-processing we pro-
pose also contribute to some extent which we 
could see from Table 2. Some idiomatic phrases 
are excluded from being corrected in pre-
processing which enhances precision while some 
are being modified in post-processing to improve 
recall. 
 
 Without pre-processing 
and post-processing% 
Final% 
P 
R 
F1 
33.72(265/768) 
16.13(265/1643) 
21.82 
35.65(272/763) 
16.56(272/1643) 
22.61 
Table 2. Comparison with and without pre-
processing and post-processing. 
 
We didn?t do much on the interacting errors 
problem since we didn?t work out perfect plans 
to solve it. So, in the result combination module, 
we just simply combine the result of each part 
together. 
Sample positive:negative
1:1 1:2 1:3 1:6
F
1
0.00
.05
.10
.15
.20
.25
.30
ME
ME+GA
ME+Tuning
ME+GA+Tuning
 
Figure 4. Comparisons before and after Genet-
ic Algorithm on ArtOrDet error type. ME, GA, 
and Tuning stand for Maximum Entropy, Ge-
netic Algorithm and confidence tuning. 
 
In the revised version, under further correc-
tions for the gold annotations, our model 
achieves precision of 41.75%, recall of 20.19% 
and F1 of 27.3%. 
6  Discussion 
Which factor contributes most to the final result 
in the problem of grammatical error correction? 
Since we didn?t include any external corpora, we 
discuss it here only according to the local classi-
fiers and context features.  
Based on our experiments, we find that, in our 
machine learning based modules, a tiny modifi-
cation of confidence parameter setting for each 
category, no matter which type of error, can have 
great impact on the final result. It results in that 
our model is much too sensitive to parameters 
which may easily lead to a poor behavior. Per-
haps a sufficient consideration of how to keep 
the distribution of samples, such as cross-
validation, may be helpful. In addition, the selec-
tion of classifiers, features and training samples 
all have effect on the result more or less, but not 
as obvious as that of the confidence threshold 
setting. 
7 Conclusion 
In this paper, we propose a hybrid model 
combining machine learning based modules and 
rule-based modules to solve the grammatical er-
ror correction task. We are able to solve a subset 
of the correction problems in which ArtOrDet 
and Nn perform better. However, our result in 
the testing data shows that our model is sensitive 
121
to parameters. How to keep the distribution of 
training samples needs to be further developed. 
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 612-
72383 and 61173075). 
References  
Bernd Bohnet. Top Accuracy and Fast Depend-
ency Parsing is not a Contradiction. In Pro-
ceedings of COLING, 2010. 
C. Fellbaum. WordNet: An Electronic Lexical 
Data-base. MIT Press. 1998. 
Daniel Dahlmeier, and Hwee Tou Ng. Grammat-
ical error correction with alternating structure 
optimization. In Proceedings of ACL. Associa-
tion for Computational Linguistics, 2011. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun 
Feng Ng. NUS at the HOO 2012 Shared Task. 
In Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012a. 
Daniel Dahlmeier and Hwee Tou Ng. A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the EMNLP. Associa-
tion for Computational Linguistics, 2012b. 
Daniel Dahlmeier and Hwee Tou Ng. Better 
Evaluation for Grammatical Error Correction. 
In Proceedings of NAACL, Association for 
Computational Linguistics, 2012c. 
Daniel Dahlmeier, Hwee Tou Ng and Siew Mei 
Wu. Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner 
English. In Proceedings of the 8th Workshop 
on Innovative Use of NLP for Building Educa-
tional Applications (BEA), 2013. 
De Felice, Rachele and Stephen G. Pulman. A 
classifier-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of COLING. Association for 
Computational Linguistics, 2008. 
G. Minnen, J. Carroll and D. Pearce. Robust, 
applied morphological generation. In Proceed-
ings of the 1st International Natural Language 
Generation Conference, 2000. 
Jenny Rose Finkel, Trond Grenager, and Chris-
topher Manning. Incorporating Non-local In-
formation into Information Extraction Systems 
by Gibbs Sampling. In Proceedings of ACL , 
2005. 
Joel R. Tetreault and Martin Chodorow. The ups 
and downs of preposition error detection in 
ESL writing. In Proceedings of COLING, As-
sociation for Computational Linguistics, 2008. 
John Lee and Stephanie Seneff. Correcting mis-
use of verb forms. In Proceedings of ACL: 
HLT, 2008. 
M Anbarasi, E Anupriya, and NC Iyengar. En-
hanced prediction of heart disease with feature 
subset selection using genetic algorithm. In-
ternational Journal of Engineering Science 
and Technology,Vol.2(10),2010: 5370-5376. 
ME ElAlami. A filter model for feature subset 
selection based on genetic algorithm. 
Knowledge-Based Systems,Vol.22(5), 2009: 
356-362. 
Michael Heilman, Aoife Cahill, and Joel 
Tetreault. Precision isn't everything: a hybrid 
approach to grammatical error detection. In 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012. 
Hongsuck Seo et al A meta learning approach to 
grammatical error correction. In Proceedings 
of ACL. Association for Computational Lin-
guistics, 2012. 
N.R. Han, M. Chodorow, and C. Leacock. 2006. 
Detecting errors in English article usage by 
non-native speakers. Natural Language Engi-
neering, Vol.12(02):115-129. 
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-
scale ngram models for lexical disambiguation. 
In Proceedings of IJCAI.2009. 
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-
based English proofing system for English as a 
second language users. In Proceedings of 
IJCNLP.2008. 
122
