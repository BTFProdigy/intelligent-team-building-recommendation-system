Extraposition: A case study in German sentence realization 
Michael GAMON?, Eric RINGGER?, Zhu ZHANG?, 
Robert MOORE?, Simon CORSTON-OLIVER? 
 
?Microsoft Research 
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, bobmoore, 
simonco}@microsoft.com 
 
 
?University of Michigan 
Ann Arbor, MI 48109 
zhuzhang@umich.edu 
 
Abstract 
We profile the occurrence of clausal 
extraposition in corpora from different 
domains and demonstrate that extraposition 
is a pervasive phenomenon in German that 
must be addressed in German sentence 
realization. We present two different 
approaches to the modeling of extraposition, 
both based on machine learned decision tree 
classifiers. The two approaches differ in their 
view of the movement operation: one 
approach models multi-step movement 
through intermediate nodes to the ultimate 
target node, while the other approach models 
one-step movement to the target node. We 
compare the resulting models, trained on data 
from two domains and discuss the 
differences between the two types of models 
and between the results obtained in the 
different domains. 
Introduction 
Sentence realization, the last stage in natural 
language generation, derives a surface string 
from a more abstract representation. Numerous 
complex operations are necessary to produce 
fluent output, including syntactic aggregation, 
constituent ordering, word inflection, etc. We 
argue that for fluent output from German 
sentence realization, clausal extraposition needs 
to be included. We show how to accomplish this 
task by applying machine learning techniques. 
A comparison between English and German 
illustrates that it is possible in both languages to 
extrapose clausal material to the right periphery 
of a clause, as the following examples show: 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave 
the country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumor has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Unlike obligatory movement phenomena such as 
Wh-movement, extraposition is subject to 
pragmatic variability. A widely-cited factor 
influencing extraposition is clausal heaviness; in 
general, extraposition of heavy clauses is 
preferred over leaving them in place. Consider 
the following example from the technical 
domain: 
German: Es werden Datenstrukturen 
verwendet, die f?r die Benutzer nicht 
sichtbar sind. 
English: Data structures are used which 
are not visible to the user. 
This perfectly fluent sentence contains an 
extraposed relative clause. If the relative clause is 
left in place, as in the following example, the 
result is less fluent, though still grammatical: 
? Es werden Datenstrukturen, die f?r die 
Benutzer nicht sichtbar sind, verwendet. 
Data structures which are not visible to 
the users are used. 
Table 1 presents a quantitative analysis of the 
frequency of extraposition in different corpora in 
both English and German. This analysis is based 
on automatic data profiling using the NLPWin 
system (Heidorn 2000). The technical manual 
corpus consists of 100,000 aligned 
English-German sentence pairs from Microsoft 
technical manuals. The Encarta corpora consist 
of 100,000 randomly selected sentences from the 
Encarta encyclopedia in both English and 
German. The output of the parser was 
post-processed to identify relative clauses 
(RELCL), infinitival clauses (INFCL), and 
complement clauses (COMPCL) that have been 
moved from a position adjacent to the term they  
modify. According to this data profile, 
approximately one third of German relative 
clauses are extraposed in technical writing, while 
only 0.22% of English relative clauses are 
extraposed in the corresponding sentence set. The 
high number of extraposed relative clauses in 
German is corroborated by numbers from the 
German hand-annotated NEGRA corpus. In 
NEGRA, 26.75% of relative clauses are 
extraposed. Uszkoreit et al (1998) report 24% of 
relative clauses being extraposed in NEGRA, but 
their number is based on an earlier version of 
NEGRA, which is about half the size of the 
current NEGRA corpus. 
We also used the NEGRA corpus to verify the 
accuracy of our data profiling with NLPWin. 
These results are presented in Table 2. We only 
took into account sentences that received a 
complete parse in NLPWin. Of the 20,602 
sentences in NEGRA, 17,756 (86.19%) fell into 
that category. The results indicate that NLPWin 
is sufficiently reliable for the identification of 
relative clauses to make our conclusions 
noteworthy and to make learning from 
NLPWin-parsed data compelling. 
Extraposition is so rare in English that a sentence 
realization module may safely ignore it and still 
yield fluent output. The fluency of sentence 
realization for German, however, will suffer from 
the lack of a good extraposition mechanism.
 
 
German  
technical 
manuals 
English  
technical 
manuals 
German  
Encarta 
English  
Encarta 
RELCL 34.97% 0.22% 18.97% 0.30% 
INFCL 3.2% 0.53% 2.77% 0.33% 
COMPCL 1.50% 0.00% 2.54% 0.15% 
Table 1: Percentage of extraposed clauses in English and German corpora 
Relative clause 
identification overall 
Identification of 
extraposed relative clauses 
Identification of non-
extraposed relative clauses 
Recall Precision Recall Precision Recall Precision 
94.55 93.40 74.50 90.02 94.64 87.76 
Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus 
 
This evidence makes it clear that any serious 
sentence realization component for German 
needs to be able to produce extraposed relative 
clauses in order to achieve reasonable fluency. In 
the German sentence realization module, 
code-named Amalgam (Gamon et al 2002, 
Corston-Oliver et al 2002), we have successfully 
implemented both extraposition models as 
described here. 
1 Two strategies for modeling 
extraposition 
The linguistic and pragmatic factors involved in 
clause extraposition are inherently complex. We 
use machine learning techniques to leverage large 
amounts of data for discovering the relevant 
conditioning features for extraposition. As a 
machine learning technique for the problem at 
hand, we chose decision tree learning, a practical 
approach to inductive inference in widespread 
use. We employ decision tree learning to 
approximate discrete-valued functions from large 
feature sets that are robust to noisy data. Decision 
trees provide an easily accessible inventory of the 
selected features and some indication of their 
relative importance in predicting the target 
features in question. The particular tool we used 
to build our decision trees is the WinMine toolkit 
(Chickering et al, 1997, n.d.). Decision trees 
built by WinMine predict a probability 
distribution over all possible target values. 
We consider two different strategies for the 
machine-learned modeling of extraposition. The 
two strategies are a series of movements versus a 
single reattachment. 
1.1 Multi-step movement 
In the multi-step movement approach, the 
question to model for each potential attachment 
site of an extraposable clause is whether the 
clause should move up to its grandparent (a ?yes? 
answer) or remain attached to its current parent (a 
?no? answer). In other words, we have cast the 
problem as a staged classification task. At 
generation runtime, for a given extraposable 
clause, the movement question is posed, and if 
the DT classifier answers ?yes?, then the clause is 
reattached one level up, and the question is posed 
again. The final attachment site is reached when 
the answer to the classification task is ?no?, and 
hence further movement is barred. Figure 1 
illustrates the multi-step movement of a clause 
(lower triangle) through two steps to a new 
landing site (the reattached clause is the upper 
triangle). Note that in both Figure 1 and Figure 2 
linear order is ignored; only the hierarchical 
aspects of extraposition are represented. 
 
Figure 1: Multi-step movement 
1.2 One-step movement 
Modeling extraposition as a one-step movement 
involves a classification decision for each node in 
the parent chain of an extraposable clause. The 
classification task can be formulated as ?should 
the extraposable clause move up to this target 
from its base position??. Figure 2 shows the 
one-step movement approach to extraposition in 
the same structural configuration as in Figure 1. 
In this example, out of the three potential landing 
sites, only one qualifies. At generation runtime, if 
more than one node in the parent chain qualifies 
as a target for extraposition movement, the node 
with the highest probability of being a target is 
chosen. In the event of equally likely target nodes, 
the target node highest in the tree is chosen. 
 
Figure 2: One-step movement 
2 Data and features 
We employed two different sets of data to build 
the models for German: the 100,000 sentence 
technical manual corpus, and the 100,000 
sentence Encarta corpus. The data were split 
70/30 for training and parameter tuning purposes, 
respectively. We extracted features for each data 
point, using the syntactic and semantic analysis 
provided by the Microsoft NLPWin system (see 
Gamon et al 2002 for more details). We only 
considered sentences for feature extraction which 
received a complete spanning parse in NLPwin. 
85.14% of the sentences in the technical domain, 
and 88.37% of the sentences in the Encarta 
corpus qualified. The following features were 
extracted: 
? syntactic label of the node under 
consideration (i.e., the starting node for a 
single-step movement), its parent and 
grandparent, and the extraposable clause 
? semantic relation to the parent node of 
the node under consideration, the parent 
and the grandparent, and the 
extraposable clause 
? status of the head of the node under 
consideration as a separable prefix verb, 
the same for the parent and the 
grandparent 
? verb position information (verb-second 
versus verb-final) for the node under 
consideration, the parent and grandparent 
? all available analysis features and 
attributes in NLPWin (see Gamon et al 
2002 for a complete list of the currently 
used features and attributes) on the node 
under consideration, the parent and 
grandparent, and on the extraposable 
clause and its parent and grandparent 
? two features indicating whether the 
extraposable node has any verbal 
ancestor node with verb-final or 
verb-second properties 
? ?heaviness? of extraposable clause as 
measured in both number of words and 
number of characters 
? ?heaviness? of the whole sentence as 
measured in both number of words and 
number of characters 
A total of 1397 features were extracted for the 
multi-step movement model. For the single-step 
movement model, we extracted an additional 21 
features. Those features indicate for each of the 
21 labels for non-terminal nodes whether a node 
with that label intervenes between the parent of 
the extraposable clause and the putative landing 
site. 
Another linguistic feature commonly cited as 
influencing extraposition is the length and 
complexity of the part of the structure between 
the original position and the extraposed clause. 
Since in the Amalgam generation module 
extraposition is applied before word and 
constituent order is established, length of 
intervening strings is not accessible as a feature.  
For each training set, we built decision trees at 
varying levels of granularity (by manipulating the 
prior probability of tree structures to favor 
simpler structures) and selected the model with 
maximal accuracy on the corresponding 
parameter tuning data set. 
Since the syntactic label of the extraposable 
clause is one of the extracted features, we decided 
to build one general extraposition model, instead 
of building separate models for each of the three 
extraposable clause types (complement clause 
COMPCL, infinitival clause INFCL, and relative 
clause RELCL). If different conditions apply to 
the three types of extraposition, the decision tree 
model is expected to pick up on the syntactic 
label of the extraposable clause as a predictive 
feature. If, on the other hand, conditions for 
extraposition tend to be neutral with respect to the 
type of extraposable clause, the modeling of 
INFCL and COMPCL extraposition can greatly 
benefit from the much larger set of data points in 
relative clause extraposition. 
3 Comparison  
To compare the one-step and multi-step models, 
we processed a new blind test set of 10,000 
sentences from each domain, Microsoft technical 
manuals and Encarta, respectively. These 
sentences were extracted randomly from data in 
these domains that were neither included in the 
training nor in the parameter tuning set. For each 
extraposable clause, three different outputs were 
computed: the observed behavior, the prediction 
obtained by iteratively applying the multi-step 
model as described in Section 1.1, and the 
prediction obtained by applying the one-step 
model. The values for these outputs were either 
?no extraposition? or a specific target node. If 
either the general extraposition prediction or the 
predicted specific target node did not match the 
observed behavior, this was counted as an error. 
3.1 One-step versus multi-step in the 
technical domain 
Accuracy data on a blind set of 10,000 sentences 
from the technical manuals domain are presented 
in Table 3. 
 One-step Multi-step Baseline 
RELCL 81.56% 83.87% 60.93% 
INFCL 93.70% 92.02% 93.70% 
COMPCL 98.10% 98.57% 94.29% 
Overall 84.42% 86.12% 67.58% 
Table 3: Accuracy numbers for the two models in 
the technical domain 
The baseline score is the accuracy for a system 
that never extraposes. Both models outperform 
the overall baseline by a large margin; the 
multi-step movement model achieves an 
accuracy 1.7% higher than the one-step model. 
The baselines in INFCL and COMPCL 
extraposition are very high. In the test set there 
were only 15 cases of extraposed INFCLs and 12 
cases of extraposed COMPCLs, making it 
impossible to draw definite conclusions. 
3.2 One-step versus multi-step in the 
Encarta domain 
Results from a blind test set of 10,000 sentences 
from the Encarta domain are presented in Table 
4. 
 One-step Multi-step Baseline 
RELCL 87.59% 88.45% 80.48% 
INFCL 97.73% 97.48% 95.72% 
COMPCL 97.32% 97.32% 95.97% 
Overall 89.99% 90.61% 84.15% 
Table 4: Accuracy numbers for the two models in 
the Encarta domain 
As in the technical domain, the multi-step model 
outperforms the one-step model, and both 
outperform the baseline significantly. Again, 
extraposed COMPCLs and INFCLs are rare in 
the dataset (there were only 17 and 6 instances, 
respectively), making the results on these types of 
clauses inconclusive. 
3.3 Domain-specificity of the models 
Since we have data from two very different 
domains we considered the extent to which the 
domain-specific models overlapped. This is a 
linguistically interesting question: from a 
linguistic perspective one would expect both 
universal properties of extraposition as well as 
domain specific generalizations to emerge from 
such a comparison. 
3.3.1 Feature selection in the technical domain 
versus Encarta 
Of the 1397 features that were extracted for the 
multi-step model, the best model for the technical 
domain was created by the WinMine tools by 
selecting 60 features. In the Encarta domain, 49 
features were selected. 27 features are shared by 
the two models. This overlap in selected features 
indicates that the models indeed capture 
linguistic generalizations that are valid across 
domains. The shared features fall into the 
following categories (where node refers to the 
starting node for multi-step movement): 
? features relating to verbal properties of 
the node 
o a separable prefix verb as 
ancestor node 
o tense and mood of ancestor 
nodes 
o presence of a verb-final or 
verb-second VP ancestor 
o presence of Modals attribute 
(indicating the presence of a 
modal verb) on ancestors 
o verb-position in the current node 
and ancestors 
? ?heaviness?-related features on the 
extraposable clause and the whole 
sentence: 
o sentence length in characters 
o number of words in the 
extraposable clause 
? syntactic labels 
? the presence of a prepositional relation 
? the presence of semantic subjects and 
objects on the node and ancestors 
? definiteness features 
? the presence of modifiers on the parent 
? person and number features 
? some basic subcategorization features 
(e.g., transitive versus intransitive) 
Interestingly, the features that are not shared (33 
in the model for the technical domain and 27 in 
the model for the Encarta domain) fall roughly 
into the same categories as the features that are 
shared. To give some examples: 
? The Encarta model refers to the presence 
of a possessor on the parent node, the 
technical domain model does not. 
? The technical domain model selects more 
person and number features on ancestors 
of the node and ancestors of the 
extraposable clause than the Encarta 
model. 
For the one-step model, 1418 total features were 
extracted. Of these features, the number of 
features selected as being predictive is 49 both in 
the Encarta and in the technical domain. 
Twenty-eight of the selected features are shared 
by the models in the two domains. Again, this 
overlap indicates that the models do pick up on 
linguistically relevant generalizations. 
The shared features between the one-step models 
fall into the same categories as the shared features 
between the multi-step models. 
The results from these experiments suggest that 
the categories of selected features are 
domain-independent, while the choice of 
individual features from a particular category 
depends on the domain. 
3.3.2 Model complexity 
In order to assess the complexity of the models, 
we use the simple metric of number of branching 
nodes in the decision tree. The complexity of the 
models clearly differs across domains. Table 5 
illustrates that for both multi-step and one-step 
movement the model size is considerably smaller 
in the Encarta domain versus the technical 
domain. 
 One-step Multi-step 
Encarta 68 82 
Technical 87 116 
Table 5: Number of branching nodes in the 
decision trees 
We hypothesize that this difference in model 
complexity may be attributable to the fact that 
NLPWin assigns a higher percentage of spanning 
parses to the Encarta data, indicating that in 
general, the Encarta data may yield more reliable 
parsing output. 
3.3.3 Cross-domain accuracy 
The results in Table 3 and Table 4 above show 
that the models based on the Encarta domain 
achieve a much higher overall accuracy (89.99% 
and 90.61%) than the models based on the 
technical domain (84.42% and 86.12%), but they 
are also based on a much higher baseline of 
non-extraposed clauses (84.15% versus 67.58% 
in the technical domain). To quantify the domain 
specificity of the models, we applied the models 
across domains; i.e., we measured the 
performance of the Encarta models on the 
technical domain and vice versa. The results 
contrasted with the in-domain overall accuracy 
from Table 3 and Table 4 are given in Table 6. 
Encarta Model Technical Model  
1-step Multi 1-step Multi 
On 
Enc. 
89.99% 90.61% 84.42% 86.12% 
On 
Tech. 
79.39% 83.03% 88.54% 89.20% 
Table 6: Cross-domain accuracy of the models 
The results show that for both one-step and 
multi-step models, the models trained on a given 
domain will outperform the models trained on a 
different domain. These results are not surprising; 
they confirm domain-specificity of the 
phenomenon. Viewed from a linguistic 
perspective, this indicates that the generalizations 
governing clausal extraposition cannot be 
formulated independently of the text domain. 
Conclusion 
We have shown that it is possible to model 
extraposition in German using decision tree 
classifiers trained on automatic linguistic 
analyses of corpora. This method is particularly 
effective for extraposed relative clauses, which 
are pervasive in German text in domains as 
disparate as news, technical manuals, and 
encyclopedic text. Both one-step and multi-step 
models very clearly outperform the baseline in 
the two domains in which we experimented. This 
in itself is a significant result, given the 
complexity of the linguistic phenomenon of 
clausal extraposition. The machine learning 
approach to extraposition has two clear 
advantages: it eliminates the need for 
hand-coding of complex conditioning 
environments for extraposition, and it is 
adaptable to new domains. The latter point is 
supported by the cross-domain accuracy 
experiment and the conclusion that extraposition 
is governed by domain-specific regularities. 
We have shown that across domains, the 
multi-step model outperforms the one-step model. 
In the German sentence realization system 
code-named Amalgam (Corston-Oliver et al 
2002, Gamon et al 2002), we have experimented 
with implementations of both the one-step and 
multi-step extraposition models, and based on the 
results reported here we have chosen the 
multi-step model for inclusion in the end-to-end 
system. 
As we have shown, extraposed relative clauses 
outnumber other extraposed clause types by a 
large margin. Still, the combined model for 
clausal extraposition outperforms the baseline 
even for infinitival clauses and complement 
clauses, although the conclusions here are not 
very firm, given the small number of relevant 
data points in the test corpus. Since the syntactic 
label of the extraposed clause is one of the 
features extracted from the training data, 
however, the setup that we have used will adapt 
easily once more training data (especially for 
infinitival and complement clauses) become 
available. The models will automatically pick up 
distinctions between the generalizations covering 
relative clauses versus infinitival/complement 
clauses when they become relevant, by selecting 
the syntactic label feature as predictive. 
Finally, evaluation of the types of features that 
were selected by the extraposition models show 
that besides the ?heaviness? of the extraposed 
clause, a number of other factors from the 
structural context enter the determination of 
likelihood of extraposition. This, in itself, is an 
interesting result: it shows how qualitative 
inspection of a machine learned model can yield 
empirically based linguistic insights. 
Acknowledgements 
Our thanks go to Max Chickering for his 
assistance with the WinMine toolkit and to the 
anonymous reviewers for helpful comments. 
References  
Chickering D. M., Heckerman D. and Meek C. (1997). 
A Bayesian approach to learning Bayesian 
networks with local structure. In "Uncertainty in 
Artificial Intelligence: Proceedings of the 
Thirteenth Conference", D. Geiger and P. Punadlik 
Shenoy, ed., Morgan Kaufman, San Francisco, 
California,  pp. 80-89. 
Chickering, D. Max. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/To
oldoc.htm 
Corston-Oliver S., Gamon M., Ringger E. and Moore 
R. (2002). An overview of Amalgam: a 
machine-learned generation module. To appear in 
Proceedings of the Second International Natural 
Language Generation Conference 2002, New York. 
Gamon M., Ringger E., Corston-Oliver S.. (2002). 
Amalgam: A machine-learned generation module. 
Microsoft Technical Report MSR-TR-2002-57. 
Heidorn, G. E. (2000): Intelligent Writing Assistance. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text", R. Dale, H. Moisl, and H. 
Somers (ed.), Marcel Dekker, New York, pp. 
181-207. 
Uszkoreit, H., Brants T., Duchier D., Krenn B., 
Konieczny L., Oepen S. and Skut W. (1998). 
Aspekte der Relativsatzextraposition im Deutschen. 
Claus-Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
Linguistic correlates of style: authorship classification with deep linguistic 
analysis features 
 
Michael Gamon 
Microsoft Research 
Microsoft Corp. 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
 
Abstract 
The identification of authorship falls into the 
category of style classification, an interesting 
sub-field of text categorization that deals with 
properties of the form of linguistic expression 
as opposed to the content of a text. Various fea-
ture sets and classification methods have been 
proposed in the literature, geared towards ab-
stracting away from the content of a text, and 
focusing on its stylistic properties. We demon-
strate that in a realistically difficult authorship 
attribution scenario, deep linguistic analysis 
features such as context free production fre-
quencies and semantic relationship frequencies 
achieve significant error reduction over more 
commonly used ?shallow? features such as 
function word frequencies and part of speech 
trigrams. Modern machine learning techniques 
like support vector machines allow us to ex-
plore large feature vectors, combining these dif-
ferent feature sets to achieve high classification 
accuracy in style-based tasks. 
1 Introduction 
Authorship identification has been a long stand-
ing topic in the field of stylometry, the analysis of 
literary style (Holmes 1998). From a broader per-
spective, issues of style, genre, and authorship are 
an interesting sub-area of text categorization. 
Typically, text categorization concerns itself with 
classifying texts according to topics. For that ob-
jective, it is crucial to extract information about the 
content of a text. In contrast, issues of style, genre 
and authorship are about the ?form? of a text. The 
analysis of style needs to abstract away from the 
content and focus on content-independent form 
properties of the linguistic expressions in a text. 
This makes style analysis a prime candidate for the 
use of linguistic processing to extract structural 
features. Viewed from a different angle, the ab-
stractness of style assessment features makes them 
highly domain-independent and reusable, as long 
as they are used with a classification technique that 
tolerates large feature vectors. 
Previously suggested methods of style categori-
zation and authorship identification have made use 
of a number of content independent features: 
? frequencies of function words (Mosteller et 
al. 1964) 
? word length and sentence length statistics 
(dating back to 1851 according to Holmes 
1998) 
? word tags and tag n-grams (Argamon et al 
1998, Koppel et al 2003, Santini 2004) 
? ?stability features? (Koppel et al 2003) cap-
turing the extent to which an item can be re-
placed by a semantically similar item 
? rewrite rules in an automatic parse (Baayen 
et al 1996) 
In this paper, we demonstrate that a combination 
of features based on shallow linguistic analysis 
(function word frequencies, part of speech tri-
grams) and a set of deep linguistic analysis features 
(context free grammar production frequencies and 
features derived from semantic graphs) yields very 
high accuracy in attributing a short random text 
sample to one of the three Bront? sisters as its au-
thor. Through feature ablation experiments we 
show that both the syntactic information captured 
in syntactic rewrite rules and the semantic informa-
tion from semantic graphs contribute to the final 
classification accuracy. We also argue that by us-
ing support vector machines as a machine learning 
technique, we can leverage a very large number of 
features, effectively combining the different fea-
ture sets into large feature vectors, eliminating the 
need for laborious manual search for features that 
may be correlated with style. 
2 Data 
To test our approach to authorship identifica-
tion, we used texts from Anne, Charlotte and 
Emily Bront?. This decision was motivated by the 
fact that we could keep gender, education and his-
toric style differences to a minimum in order to 
focus on authorship identification, and by the easy 
availability of electronic versions of several 
lengthy texts from these authors. The texts we used 
were: 
Charlotte Bront?: Jane Eyre, The Professor 
Anne Bront?: The Tenant of Wildfell Hall, 
Agnes Grey 
Emily Bront?: Wuthering Heights 
For each of the three authors we collected all 
sentences from those titles and randomized their 
order. The total number of sentences for each au-
thor is: 13220 sentences for Charlotte, 9263 for 
Anne and 6410 for Emily. We produced artificial 
documents of 20 sentences in length from these 
sets of sentences. We split the resulting 1441 
documents 80/20 for training and test. This split 
yields 288 documents for test, and 1153 documents 
for training. All numbers reported in this paper are 
based on 5-fold cross validation. 
3 Features 
All linguistic features have been automatically 
extracted using the NLPWin system (for an over-
view see Heidorn 2000). Note that this system pro-
duces partial constituent analyses for sentences 
even if no spanning parse can be found. The only 
exception are sentences of more than 50 words 
which do not result in any assignment of linguistic 
structure. 
3.1 Length features 
We measure average length of sentences, noun-
phrases, adjectival/adverbial phrases, and subordi-
nate clauses per document. 
3.2 Function word frequencies 
We measure the frequencies of function word 
lemmas as identified by the NLPWin system. In 
order to be maximally ?content-independent?, we 
normalized all personal pronouns to an artificial 
form ?perspro? in order to not pick up on ?she? or 
?he? frequencies which would be linked to the gen-
der of characters in the works of fiction rather than 
author style. The number of observed function 
word lemmas is 474. 
3.3 Part-of-speech trigrams 
We extract part-of-speech (POS) trigrams from 
the documents and use the frequencies of these 
trigrams as features. The NLPWin system uses a 
set of 8 POS tags. 819 different POS trigrams are 
observed in the data. 
3.4 Syntactic productions 
The parses provided by the NLPWin system al-
low us to extract context-free grammar productions 
for each sentence, similar to the features in Baayen 
et al (1996). Examples of common productions 
are: 
PP ? PP DETP NOUN 
INFCL ? INFTO VERB NP 
DECL ? VP CONJ VP CHAR 
For each observed production, we measure the 
per-document frequency of the productions. 15.443 
individual productions (types) occurred in our data, 
the total number of production tokens is 618.500. 
3.5 Semantic information 
We extract two kinds of information from the 
semantic dependency graphs produced by the 
NLPWin system: binary semantic features and se-
mantic modification relations. Examples of seman-
tic features are number and person features on 
nouns and pronouns, tense and aspectual features 
on verbs, and subcategorization features (indicat-
ing realized as opposed to potential subcategoriza-
tion) on verbs. There is a total of 80 such semantic 
features. 
Semantic modification relations are represented 
in a form where for each node A in a semantic 
graph the POS of A, the POS of all its n daughters 
B1..n, and the semantic relations SR1..n of all its 
daughters B1..n are given. Some common modifica-
tion structures are illustrated below: 
Noun Possr Pron (a nominal node with a pro-
nominal possessor) 
Verb Tsub Pron Tobj Noun (a verbal node with 
a pronominal deep subject and a nominal deep 
object) 
Noun Locn Noun (a nominal node with a nomi-
nal modifier indicating location) 
As with the previously discussed features, we 
measure per-document frequency of the observed 
modification structures. There are a total of 9377 
such structures. 
3.6 n-gram frequency features 
The use of word n-gram frequencies is not ap-
propriate for style classification tasks since these 
features are not sufficiently content-independent. 
In our experiments, for example, they could pick 
up on nouns referring to events or locations that 
are part of the story told in the work of fiction at 
hand. We included these features in our experi-
ments only as a point of comparison for the purely 
?form-based? features. In order to prevent the most 
obvious content-dependency in the word n-gram 
frequency features, we normalized proper nouns to 
?NAME? and singular personal pronouns to ?Per-
spro?. 
3.7 Feature selection 
While the total number of observed syntactic 
and semantic patterns is very high, most of the pat-
terns occur only very few times, or even only once. 
In order to eliminate irrelevant features, we em-
ployed a simple frequency cutoff, where the fre-
quency of a pattern that occurs less than n times is 
not included as a feature. 
4 The machine learning technique: Sup-
port vector machines 
For our experiments we have used support vec-
tor machines (SVMs), a machine learning algo-
rithm that constructs a plane through a multi-
dimensional hyperspace, separating the training 
cases into the target classes. SVMs have been used 
successfully in text categorization and in other 
classification tasks involving highly dimensional 
feature vectors (e.g. Joachims 1998, Dumais et al 
1998). Diederich et al (2003) have applied support 
vector machines to the problem of authorship attri-
bution. For our experiments we have used John 
Platt?s Sequential Minimal Optimization (SMO) 
tool (Platt 1999). In the absence of evidence for the 
usefulness of more complicated kernel functions in 
similar experiments (Diederich et al 2003), we 
used linear SVMs exclusively. 
5 Results 
All results discussed in this section should be in-
terpreted against a simple baseline accuracy 
achieved by guessing the most frequent author 
(Charlotte). That baseline accuracy is 45.8%. All 
accuracy differences have been determined to be 
statistically significant at the .99 confidence level. 
5.1 Feature sets in isolation 
Classification accuracy using the different fea-
ture sets (POS trigram frequencies, function word 
frequencies, syntactic features, semantic features) 
are shown in Figure 1. The four length features 
discussed in section 3.1 yielded a classification 
accuracy of only 54.85% and are not shown in 
Figure 1. 
 
Feature sets in isolation
82
84
86
88
90
92
94
96
98
100
5 10 20 50 75 100 200 500
frequency threshold
ac
cu
ra
cy
function word frequency pos trigram frequencies
syntactic features semantic features
 
Figure 1: Classification accuracy using the feature sets in isolation 
5.2 Feature sets combined 
The combination of all feature sets yields a 
much increased classification accuracy across fre-
quency thresholds as shown in Figure 2. Combin-
ing all features, including length features, 
consistently outperforms all other scenarios. Re-
stricting features to those that only utilize shallow 
linguistic analysis, such as the POS trigram fea-
tures and the function word frequency features re-
duces accuracy by about one percent. Interestingly, 
the use of syntactic and semantic features alone 
yields classification accuracy below the other fea-
ture combinations. In combination, though, these 
features contribute strongly to the overall accuracy. 
Semantic features which constitute the most ab-
stract and linguistically sophisticated class, add to 
the accuracy of the classifier. This is evidenced by 
comparing the top two lines in Figure 2 which 
show the accuracy using all features, and the accu-
racy using all features except the semantic features. 
Also included in Figure 2 is the accuracy ob-
tainable by using ?content-dependent? bigram and 
trigram frequency features. As stated above, these 
features are not adequate for style assessment pur-
poses since they pick up on content, whereas style 
assessment needs to abstract away from content 
and measure the form of linguistic expression. It is 
noteworthy, however, that the true stylistic and 
?content-independent? features produce a classifi-
cation accuracy that outperforms the ngram fea-
tures by a wide margin. 
Precision and recall numbers using all features 
with a frequency threshold of 75 (which yields the 
highest accuracy at 97.57%) are shown in Table 1. 
Target Precision Recall F-measure 
Anne 97.20 98.08 97.64 
Charlotte 98.18 98.20 98.19 
Emily 96.81 95.52 96.16 
Table 1: precision, recall and F-measure for the best 
model series with all features at frequency cutoff 75. 
Feature combinations 
90
91
92
93
94
95
96
97
98
99
100
5 10 20 50 75 100 200 500
frequency threshold
ac
c
u
ra
cy
all features function words and POS trigrams
syntactic and LF features no semantic features
ngram features
 
Figure 2: Classification accuracy based on combinations of feature sets 
Table 2 shows error reduction rates for the addi-
tion of deep linguistic analysis features to the 
?shallow? baseline of function word frequencies 
and POS trigrams. 
Frequency 
cutoff 
+ syntactic 
features 
+ syntactic and 
semantic features 
5 15.70% 21.60% 
10 3.80% 11.50% 
20 14.50% 32.70% 
50 11.30% 30.20% 
75 20.40% 28.60% 
100 14.50% 35.50% 
200 26.20% 32.80% 
500 16.40% 20.90% 
Table 2: Error reduction rates achieved by adding deep 
linguistic analysis features to a baseline of POS trigram 
features and function word frequencies 
5.3 Number of features and frequency 
threshold 
Table 3 shows the number of features at each 
frequency cutoff. The total number of style-related 
features ranges from 6018 at a frequency cutoff of 
at least 5 observed instances to 546 at a frequency 
cutoff of 500. The size of these feature vectors is at 
the high end of what has typically been reported in 
the literature for similar experiments: For example, 
Argamon-Engelson et al (1998) use feature vec-
tors of size 1185 for newspaper style detection, 
Finn and Kushmerick (2003) have 36 POS features 
and 152 text statistics features for detection of ?ob-
jective? and ?subjective? genre, Koppel et al 
(2004) use 130 features for authorship verification. 
Frequency 
cutoff 
All features 
 
Function 
words 
POS 
trigrams 
Syntactic 
features 
semantic 
features 
Ngrams 
 
5 6018 315 695 3107 1896 28820 
10 3947 238 650 1885 1170 12312 
20 2714 186 613 1176 735 5437 
50 1730 140 542 623 421 1789 
75 1421 125 505 442 345 1102 
100 1233 116 466 355 292 781 
200 870 88 385 201 192 357 
500 546 62 257 101 122 114 
Table 3: The number of features at different frequency cutoffs 
6 Discussion 
We believe that the results presented in the pre-
vious section allow a number of interesting conclu-
sions for research into automatic style and 
authorship assessment. First, in our experiments 
the addition of deep linguistic analysis features 
increases classification accuracy. 
From a linguistic perspective this is no surprise: 
it is clear that matters of linguistic form are those 
that can be captured by a syntactic and to some 
extent by a semantic analysis (as long as the se-
mantic analysis is not so abstract that it completely 
abstracts away from any form properties of the 
sentence). It was less clear, though, whether an 
automatic language analysis system can be reliable 
enough to provide the necessary feature functions. 
This has been categorically denied in some of the 
literature (e.g. Stamatos et al 2000). These state-
ments, however, did not take into account that as 
long as a language analysis system is consistent in 
the errors it makes, machine learning techniques 
can pick up on correlations between linguistic fea-
tures and style even though the label of a linguistic 
feature (the ?quality? it measures) is mislabeled. 
Secondly, we would like to emphasize that the 
results we have achieved are not based on deliber-
ate selection of a small set of features as likely 
candidates for correlation with style. We have se-
lected sets of features to be included in our ex-
periments, but whether or not an individual feature 
plays a role was left to the machine learning tech-
nique to decide. Ideally, then, we would pass any 
number of features to the classifier algorithm and 
expect it to select relevant features during the train-
ing process. While this is possible with a large 
number of training cases, a smaller number of 
training cases poses a limit to the number of fea-
tures that should be used to achieve optimal classi-
fication accuracy and prevent overfitting. In order 
to prevent overfitting it is desirable to reduce the 
vector size to a number that does not exceed the 
number of training cases. Support vector machines 
are very robust to overfitting, and in our experi-
ments we find that classification results were quite 
robust to feature vectors with up to 4 times the size 
of the training set. However, it is still the case that 
optimal accuracy is achieved where the size of the 
feature vector comes close to the training sample 
(at a frequency cutoff of 75 for the vector contain-
ing all sets of features). 
We also examined the features that carried high 
weights in the SVMs. Among the most highly 
weighted features we found a mix of different fea-
ture types. Below is a very small sample from the 
top-weighted features (recall that all features 
measure frequency): 
? punctuation character starting a sentence 
(quote, double dash etc) 
? but 
? NOUN CONJ NOUN sequence 
? on 
? prepositional phrases consisting of preposi-
tion and pronoun 
? VERB ADVERB CHAR sequences 
? progressive verbs 
? verbal predicates with a pronominal subject 
and a clausal object 
In order to determine whether our results hold 
on sample documents of smaller size, we con-
ducted a second round of experiments where 
document length was scaled down to five sen-
tences per document. This yielded a total of 5767 
documents, which we subjected to the same 80/20 
split and 5fold cross-validation as in the previous 
experiments. Results as shown in Table 4 are very 
encouraging: using all features, we achieve a 
maximum classification accuracy of 85%. As in 
our previous experiments, removing deep linguistic 
analysis features degrades the results.
 
Frequency 
threshold 
Number of 
all features 
Number of  
shallow features 
Accuracy using  
all features 
Accuracy using 
shallow features 
5 6018 1011 85.00 81.65 
10 3947 889 84.96 81.56 
20 2714 800 84.84 81.25 
75 1421 631 84.53 80.59 
Table 4: results on documents of a length of 5 sentences 
It should also be clear that simple frequency 
cutoffs are a crude way of reducing the number of 
features. Not every frequent feature is likely to be 
discriminative (in our example, it is unlikely that a 
period at the end of a sentence is discriminative), 
and not every infrequent feature is likely to be non-
discriminative. In fact, hapax legomena, the single 
occurrence of a certain lexeme has been used to 
discriminate authors. Baayen et al (1996) also 
have pointed out the discriminatory role of infre-
quent syntactic patterns. What we need, then, is a 
more sophisticated thresholding technique to re-
strict the feature vector size. We have begun ex-
perimenting with log likelihood ratio (Dunning 
1993) as a thresholding technique. 
To assess at least anecdotally whether our re-
sults hold in a different domain, we also tested on 
sentences from speeches of George Bush Jr. and 
Bill Clinton (2231 sentences from the former, 2433 
sentences from the latter). Using document sam-
ples with 5 sentences each, 10-fold cross-
validation and a frequency cutoff of 5, we achieved 
87.63% classification accuracy using all features, 
and 83.00% accuracy using only shallow features 
(function word frequencies and POS trigrams). 
Additional experiments with similar methodology 
are under way for a stylistic classification task 
based on unedited versus highly edited documents 
within the technical domain. 
7 Conclusion 
We have shown that the use of deep linguistic 
analysis features in authorship attribution can yield 
a significant reduction in error rate over the use of 
shallow linguistic features such as function word 
frequencies and part of speech trigrams. We have 
furthermore argued that by using a modern ma-
chine learning technique that is robust to large fea-
ture vectors, combining different feature sets yields 
optimal results. Reducing the number of features 
(i.e. the number of parameters to be estimated by 
the learning algorithm) by frequency cutoffs to be 
in the range of the number of training cases pro-
duced good results, although it is to be expected 
that more intelligent thresholding techniques such 
as log likelihood ratio will further increase per-
formance. These results hold up even if document 
size is reduced to only five sentences. 
We believe that these results show that the 
common argument of the ?unreliability? of auto-
matic linguistic processing used for feature extrac-
tion for style assessment is not as strong as it 
seems. As long as the errors introduced by a parser 
are systematic, a machine learning system pre-
sented with a large number of features can still 
learn relevant correlations. 
Areas for further research in this area include 
experimentation with additional authorship and 
style classification tasks/scenarios, experiments 
with different thresholding techniques and possibly 
with additional linguistic feature sets. 
Additionally, we plan to investigate the possibil-
ity of training different classifiers, each of which 
contains features from one of the four major fea-
ture sets (function word frequencies, POS trigram 
frequencies, syntactic production frequencies, se-
mantic feature frequencies), and maximally n such 
features where n is the number of training cases. 
The votes from the ensemble of four classifiers 
could then be combined with a number of different 
methods, including simple voting, weighted vot-
ing, or ?stacking? (Dietterich 1998). 
Acknowledgements 
We thank Anthony Aue, Eric Ringger (Microsoft 
Research) and James Lyle (Microsoft Natural Lan-
guage Group) for many helpful discussions. 
References 
Shlomo Argamon-Engelson, Moshe Koppel, and 
Galit Avneri. 1998. Style-Based Text Categori-
zation: What Newspaper am I Reading? Pro-
ceedings of AAAI Workshop on Learning for 
Text Categorization, 1-4. 
Harald Baayen, Hans van Halteren, and Fiona 
Tweedie. 1996. Outside the Cave of Shadows: 
Using Syntactic Annotation to Enhance Author-
ship Attribution. Literary and Linguistic Com-
puting 11(3): 121-131. 
Joachim Diederich, J?rg Kindermann, Edda Leo-
pold, and Gerhard Paass.2003. Authorship At-
tribution with Support Vector Machines. 
Applied Intelligence 19(1):109-123. 
Thomas G. Dietterich. 1998. Machine Learning 
Research: Four Current Directions. The AI 
Magazine 18(4): 97-136. 
Susan Dumais, John Platt, David Heckerman, and 
Mehran Sahami. 1998. Inductive Learning Al-
gorithms and Representations for Text Categori-
zation. Proceedings of the 7th International 
Conference on Information and Knowledge 
Management: 148-155. 
Ted Dunning. 1993. Accurate Methods for the Sta-
tistics of Surprise and Coincidence. Computa-
tional Linguistics 19: 61-74. 
Aidan Finn and Nicholas Kushmerick. 2003. 
Learning to Classify Documents According to 
Genre. IJCAI-2003 Workshop on Computa-
tional Approaches to Text Style and Synthesis, 
Acapulco, Mexico. 
George Heidorn. 2000. Intelligent Writing Assis-
tance. In R. Dale, H. Moisl and H. Somers, eds., 
Handbook of Natural Language Processing. 
Marcel Dekker. 
David I. Holmes. 1998. The Evolution of Stylome-
try in Humanities Scholarship. Literary and 
Linguistic Computing 13(3):111-117. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machines: Learning with many 
Relevant Features. Proceedings of the tenth 
European Conference on Machine Learn-
ing:137-142. 
Moshe Koppel, Navot Akiva and Ido Dagan. 2003. 
A Corpus-Independent Feature Set for Style-
Based Text Categorization. IJCAI-2003 Work-
shop on Computational Approaches to Text 
Style and Synthesis, Acapulco, Mexico. 
Moshe Koppel, Jonathan Schler and Droz Mughaz. 
2004. Text Categorization for Authorship Veri-
fication. Paper presented at the 8th Symposium 
on Artifical Intelligence and Mathematics, Fort 
Lauderdale, Florida. 
Moshe Koppel, Shlomo Argamon, and Anat R. 
Shimoni. 2003. Automatically Categorizing 
Written Texts by Author Gender. Literary and 
Linguistic Computing 17(4): 401-412. 
F. Mosteller. and D. L. Wallace. 1964. Applied 
Bayesian and Classical Inference: The Case of 
the Federalist Papers. Addison-Wesley, Read-
ing, MA. 
John Platt. 1999. Fast Training of SVMs Using 
Sequential Minimal Optimization. In: 
B.Sch?lkopf, C. Burges and A. Smola (eds.) Ad-
vances in kernel methods: support vector learn-
ing. MIT Press, Cambridge, MA, 185-208. 
Marina Santini. 2004. A Shallow Approach to Syn-
tactic Feature Extraction for Genre Classifica-
tion. Proceedings of the 7th Annual Colloquium 
for the UK Special Interest Group for Computa-
tional Linguistics.  
Efstathios Stamatos, Nikos Fakotakis and George 
Kokkinakis. 2000. Automatic Text Categoriza-
tion in Terms of Genre and Author. Computa-
tional Linguistics 26(4): 471-495. 
Linguistically Informed Statistical Models of Constituent Structure for 
Ordering in Sentence Realization 
Eric RINGGER1, Michael GAMON1, Robert C. MOORE1, 
David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER1 
 
1Microsoft Research 
One Microsoft Way 
Redmond, Washington 98052, USA 
{ringger, mgamon, bobmoore, msmets, 
simonco}@microsoft.com 
2Butler Hill Group, LLC 
& Indiana University Linguistics Dept. 
1021 East 3rd Street, MM 322 
Bloomington, Indiana 47405, USA 
drojas@indiana.edu 
 
 
Abstract 
We present several statistical models of syntactic 
constituent order for sentence realization. We 
compare several models, including simple joint 
models inspired by existing statistical parsing 
models, and several novel conditional models. The 
conditional models leverage a large set of linguistic 
features without manual feature selection. We apply 
and evaluate the models in sentence realization for 
French and German and find that a particular 
conditional model outperforms all others. We 
employ a version of that model in an evaluation on 
unordered trees from the Penn TreeBank. We offer 
this result on standard data as a reference-point for 
evaluations of ordering in sentence realization. 
1 Introduction 
Word and constituent order play a crucial role in 
establishing the fluency and intelligibility of a 
sentence. In some systems, establishing order 
during the sentence realization stage of natural 
language generation has been accomplished by 
hand-crafted generation grammars in the past (see 
for example, Aikawa et al (2001) and Reiter and 
Dale (2000)). In contrast, the Nitrogen (Langkilde 
and Knight, 1998a, 1998b) system employs a word 
n-gram language model to choose among a large 
set of word sequence candidates which vary in 
constituent order, word order, lexical choice, and 
morphological inflection. Nitrogen?s model does 
not take into consideration any non-surface 
linguistic features available during realization.  
The Fergus system (Bangalore and Rambow, 
2000) employs a statistical tree model to select 
probable trees and a word n-gram model to rank 
the string candidates generated from the best trees. 
Like Nitrogen, the HALogen system (Langkilde, 
2000; Langkilde-Geary, 2002a, 2002b) uses word 
n-grams, but it extracts the best-scoring surface 
realizations efficiently from a packed forest by 
constraining the search first within the scope of 
each constituent.  
Our research is carried out within the Amalgam 
broad coverage sentence realization system. 
Amalgam generates sentence strings from abstract 
predicate-argument structures (Figure 1), using a 
pipeline of stages, many of which employ 
machine-learned models to predict where to 
perform specific linguistic operations based on the 
linguistic context (Corston-Oliver et al, 2002; 
Gamon et al, 2002a, 2002b; Smets et al, 2003). 
Amalgam has an explicit ordering stage that 
determines the order of constituents and their 
daughters. The input for this stage is an unordered 
tree of constituents; the output is an ordered tree of 
constituents or a ranked list of such trees. For 
ordering, Amalgam leverages tree constituent 
structure and, importantly, features of those 
constituents and the surrounding context. By 
separately establishing order within constituents, 
Amalgam heavily constrains the possible 
alternatives in later stages of the realization 
process.  The design allows for interaction between 
ordering choices and other realization decisions, 
such as lexical choice (not considered in the 
present work), through score combinations from 
distinct Amalgam pipeline stages. 
Most previous research into the problem of 
establishing order for sentence realization has 
focused on English, a language with fairly strict 
word and constituent order. In the experiments 
described here we first focus on German and 
French which present novel challenges.1 We also 
describe an English experiment involving data 
from the Penn Treebank. Our ultimate goal is to 
develop a model that handles all ordering 
phenomena in a unified and elegant way across 
typologically diverse languages. In the present 
paper, we explore the space of possible models and 
examine some of these closely. 
                                                          
1
 For an overview of some of the issues in 
determining word and constituent order in German and 
French, see (Ringger et al, 2003).  
 Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence: 
In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet. 
(The options and their functions are listed in the following table.) 
2 Models of Constituent Order 
In order to develop a model of constituent 
structure that captures important order phenomena, 
we will consider the space of possible joint and 
conditional models in increasing complexity. For 
each of the models, we will survey the 
independence assumptions and the feature set used 
in the models. 
Our models differ from the previous statistical 
approaches in the range of input features. Like the 
knowledge-engineered approaches, the models 
presented here incorporate lexical features, parts-
of-speech, constituent-types, constituent 
boundaries, long-distance dependencies, and 
semantic relations between heads and their 
modifiers. 
Our experiments do not cover the entire space of 
possible models, but we have chosen significant 
points in the space for evaluation and comparison. 
2.1 Joint Models 
We begin by considering joint models of 
constituent structure of the form ( ),P ? ?  over 
ordered syntax trees ?  and unordered syntax trees 
? . An ordered tree ?  contains non-terminal 
constituents C, each of which is the parent of an 
ordered sequence of daughters ( 1,..., nD D ), one of 
which is the head constituent H.2 Given an ordered 
tree ? , the value of the function 
_ ( )unordered tree ?  is an unordered tree ?  
corresponding to ?  that contains a constituent B 
for each C in ? , such that 
( ) ( )
1
_ ( )
{ ,..., }n
unordered set daughters Cdaughters B
D D
=
=
 
again with iH D=  for some i in ( )1..n . The 
hierarchical structure of ?  is identical to that of 
? . 
We employ joint models for scoring alternative 
ordered trees as follows: given an unordered 
syntax tree ? , we want the ordered syntax tree ??  
that maximizes the joint probability: 
                                                          
2 All capital Latin letters denote constituents, and 
corresponding lower-case Latin letters denote their 
labels (syntactic categories). 
( ) ( )
: _ ( )
? arg max , arg max
unordered tree
P P
? ? ? ?
? ? ? ?
=
= =    (1) 
As equation (1) indicates, we can limit our search 
to those trees ?  which are alternative orderings of 
the given tree ? . 
Inter-dependencies among ordering decisions 
within different constituents (e.g., for achieving 
parallelism) make the global sentence ordering 
problem challenging and are certainly worth 
investigating in future work.  For the present, we 
constrain the possible model types considered here 
by assuming that the ordering of any constituent is 
independent of the ordering within other 
constituents in the tree, including its daughters; 
consequently, 
( ) ( )
( )C constits
P P C
?
?
?
= ?  
Given this independence assumption, the only 
possible ordered trees are trees built with non-
terminal constituents computed as follows: for 
each ( )B constits ?? , 
( )
: _ ( )
* arg max
C B unordered set C
C P C
=
=  
In fact, we can further constrain our search for the 
best ordering of each unordered constituent B, 
since C?s head must match B?s head: 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C
=
=
=  
Thus, we have reduced the problem to finding the 
best ordering of each constituent of the unordered 
tree. 
Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows: 
( ) ( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P x P C x
=
=
=  
If x is truly a feature of ?  and does not depend on 
any particular ordering of any constituent in ? , 
then ( )P x  is constant, and we do not need to 
compute it in practice. In other words, 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C x
=
=
=       (2) 
Hence, even for a joint model ( )P C , we can 
condition on features that are fixed in the given 
unordered tree ?  without first predicting them. 
The joint models described here are of this form. 
For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, we 
are actually describing the part of the joint model 
that is of interest. As justified above, we do not 
need to compute ( )P x  and will simply present 
alternative forms of ( )P C x . 
We can factor a distribution ( )P C x  in many 
different ways using the chain rule. As our starting 
point we adopt the class of models called Markov 
grammars.3 We first consider a left-to-right 
Markov grammar of order j that expands C by 
predicting its daughters 1,..., nD D  from left-to-
right, one at a time, as shown in Figure 2: in the 
figure. iD  depends only on ( i jD ? , ?, 1iD ? ), and 
the parent category C ., according to the 
distribution in equation (3). 
 
i?
Figure 2: Left-to-right Markov grammar. 
( ) ( )1
1
,..., , ,
n
i i i j
i
P C h P d d d c h
? ?
=
= ?  (3) 
In order to condition on another feature of each 
ordered daughter iD , such as its semantic relation 
i? to the head constituent H, we also first predict 
it, according to the chain rule. The result is the 
semantic Markov grammar in equation (4):  
( ) ( )( )
1 1
1 1 1
, ,..., , , ,
, , ,..., , , ,
n i i i i j i j
i i i i i i j i j
P d d c h
P C h
P d d d c h
? ? ?
? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (4) 
Thus, the model predicts semantic relation i? and 
then the label id  in the context of that semantic 
relation. We will refer to this model as Type 1 
(T1). 
As an extension to model Type 1, we include 
features computed by the following functions on 
the set i?  of daughters of C already ordered (see 
Figure 2): 
? Number of daughters already ordered (size 
of i? ) 
? Number of daughters in i?  having a 
particular label for each of the possible 
constituent labels {NP, AUXP, VP, etc.} 
(24 for German, 23 for French) 
We denote that set of features in shorthand as ( )if ? . With this extension, a model of Markov 
                                                          
3
 A ?Markov grammar? is a model of constituent 
structure that starts at the root of the tree and assigns 
probability to the expansion of a non-terminal one 
daughter at a time, rather than as entire productions 
(Charniak, 1997 & 2000). 
order j can potentially have an actual Markov order 
greater than j. Equation (5) is the extended model, 
which we will refer to as Type 2 (T2): 
( ) ( )( )( )( )
1 1
1 1 1
, ,..., , , , ,
, , ,..., , , , ,
n i i i i j i j i
i i i i i i j i j i
P d d c h f
P C h
P d d d c h f
? ? ? ?
? ? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (5) 
As an alternative to a left-to-right expansion, we 
can also expand a constituent in a head-driven 
fashion. We refer the reader to (Ringger et al, 
2003) for details and evaluations of several head-
driven models (the missing ?T3?, ?T4?, and ?T6? 
in this discussion). 
2.2 Conditional Models 
We now consider more complex models that use 
additional features. We define a function ( )g X on 
constituents, where the value of ( )g X represents a 
set of many lexical, syntactic, and semantic 
features of X (see section 5.2 for more details). No 
discourse features are included for the present 
work. We condition on 
? ( )g B , where B is the unordered constituent 
being ordered 
? ( )g H , where H is the head of B 
? ( )Bg P , where BP  is the parent of B, and 
? ( )Bg G , where BG  is the grandparent of B. 
These features are fixed in the given unordered tree 
? , as in the discussion of equation (2), hence the 
resulting complex model is still a joint model.   
Up until this point, we have been describing joint 
generative models that describe how to generate an 
ordered tree from an unordered tree. These models 
require extra effort and capacity to accurately 
model the inter-relations among all features. Now 
we move on to truly conditional models by 
including features that are functions on the set i?  
of daughters of C yet to be ordered. In the 
conditional models we do not need to model the 
interdependencies among all features. We include 
the following: 
? Number of daughters remaining to be 
ordered (size of i? ) 
? Number of daughters in i?  having a 
particular label 
As before, we denote these feature sets in 
shorthand as ( )if ? . The resulting distribution is 
represented in equation (6), which we will refer to 
as Type 5 (T5): 
( )
( )
( )
1 1
1 1 1
( ), ( ), ( ), ( )
, ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
, , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
B B
i i i j i j
i
n B B i i
i i i i i j i j
i
B B i i
P C g H g B g P g G
d d c h
P
g H g B g P g G f f
d d c h
P d
g H g B g P g G f f
? ?
?
? ?
? ? ?
? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
=
? ?
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
?
   (6) 
All models in this paper are nominally Markov 
order 2, although those models incorporating the 
additional feature functions ( )if ?  and ( )if ?  
defined in Section 2.2 can be said to have higher 
order. 
2.3 Binary conditional model 
We introduce one more model type called the 
binary conditional model. It estimates a much 
simpler distribution over the binary variable ?  
called ?sort-next? with values in {yes, no} 
representing the event that an as-yet unordered 
member D of i?  (the set of as-yet unordered 
daughters of parent C, as defined above) should be 
?sorted? next, as illustrated in Figure 3. 
i?i?
?
 
Figure 3: Binary conditional model. 
The conditioning features are almost identical to 
those used in the left-to-right conditional models 
represented in equation (6) above, except that id  
and i?  (the semantic relation of D with head H) 
appear in the conditional context and need not first 
be predicted. In its simple form, the model 
estimates the following distribution: 
( )
1 1, , , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
i i i i i j i j
i
B B i i
d d d c h
P
g H g B g P g G f f
? ? ?
?
? ?
? ? ? ?
? ?
? ?
? ?
? ?
   (7) 
In our shorthand, we will call this Type 7 (T7). We 
describe how to apply this model directly in a left-
to-right ?sorting? search later in the section on 
search. 
3 Estimation 
We estimate a model?s distributions with 
probabilistic decision trees (DTs).4 We build 
decision trees using the WinMine toolkit 
(Chickering, 2002). WinMine-learned decision 
trees are not just classifiers; each leaf is a 
conditional probability distribution over the target 
random variable, given all features available in 
training; hence the tree as a whole is an estimate of 
the conditional distribution of interest. The primary 
advantage of using decision trees, is the automatic 
feature selection and induction from a large pool of 
features. 
We train four models for German and French 
each. One model is joint (T1); one is joint with 
additional features on the set of daughters already 
ordered (T2); one is conditional (T5). In addition, 
we employ one binary conditional DT model (T7), 
both with and without normalization (see equation 
(8)). 
                                                          
4
 Other approaches to feature selection, feature 
induction, and distribution estimation are certainly 
possible, but they are beyond the scope of this paper. 
One experiment using interpolated language modeling 
techniques is described in (Ringger et al, 2003) 
4 Search 
4.1 Exhaustive search 
Given an unordered tree ?  and a model of 
constituent structure O of any of the types already 
presented, we search for the best ordered tree ?  
that maximizes ( )OP ?  or ( )OP ? ? , as 
appropriate, with the context varying according to 
the complexity of the model. Each of our models 
(except the binary conditional model) estimates the 
probability of an ordering of any given constituent 
C in ? , independently of the ordering inside other 
constituents in ? . The complete search is a 
dynamic programming (DP) algorithm, either left-
to-right in the daughters of C (or head-driven, 
depending on the model type). The search can 
optionally maintain one non-statistical constraint 
we call Input-Output Coordination Consistency 
(IOCC), so that the order of coordinated 
constituents is preserved as they were specified in 
the given unordered tree. For these experiments, 
we employ the constraint. 
4.2 Greedy search for binary conditional 
model 
The binary conditional model can be applied in a 
left-to-right ?sorting? mode (Figure 3). At stage i, 
for each unordered daughter jD , in i? , the model 
is consulted for the probability of j yes? = , 
namely the probability that jD  should be placed to 
the right of the already ordered sister constituents 
i? . The daughter in i?  with the highest 
probability is removed from i?  to produce 1i? +  
and added to the right of i? to produce 1i? + . The 
search proceeds through the remaining unordered 
constituents until all constituents have been 
ordered in this greedy fashion. 
4.3 Exhaustive search for binary conditional 
model 
In order to apply the binary conditional model in 
the exhaustive DP search, we normalize the model 
at every stage of the search and thereby coerce it 
into a probability distribution over the remaining 
daughters in i? . We represent the distribution in 
?equation? (7) in short-hand as ( ), , iP d? ? ? , 
with i?  representing the contextual features for the 
given search hypothesis at search stage i. Thus, our 
normalized distribution for stage i is given by 
equation (8). Free variable j represents an index on 
unordered daughters in i? , as does k. 
( ) ( )
( )
1
, ,
, ,
, ,
i
j j j i
j j j i
k k k i
k
P yes d
P D d
P yes d
?
? ?
?
? ?
=
= ?
? =
= ?
?
 (8) 
This turns out to be the decision tree analogue of a 
Maximum Entropy Markov Model (MEMM) 
(McCallum et al, 2000), which we can refer to as a 
DTMM. 
5 Experiments 
5.1 Training 
We use a training set of 20,000 sentences, both 
for French and German. The data come from 
technical manuals in the computer domain. For a 
given sentence in our training set, we begin by 
analyzing the sentence as a surface syntax tree and 
an abstract predicate argument structure using the 
NLPWin system (Heidorn, 2000). By consulting 
these two linked structures, we produce a tree with 
all of the characteristics of trees seen by the 
Amalgam ordering stage at generation run-time 
with one exception: these training trees are 
properly ordered. The training trees include all 
features of interest, including the semantic 
relations among a syntactic head and its modifiers. 
We train our order models from the constituents of 
these trees. NLPWin parser output naturally 
contains errors; hence, the Amalgam training data 
is imperfect. 
5.2 Selected Features 
A wide range of linguistic features is extracted 
for the different decision tree models. The number 
of selected features for German reaches 280 (out of 
651 possible features) in the binary conditional 
model T7. For the French binary conditional 
model, the number of selected features is 218 (out 
of 550). The binary conditional models draw from 
the full set of available features, including: 
? lexical sub-categorization features such as 
transitivity and compatibility with clausal 
complements 
? lemmas (word-stems) 
? semantic features such as the semantic 
relation and the presence of 
quantificational operators 
? length of constituent in words 
? syntactic information such as the label and 
the presence of syntactic modifiers 
5.3 Evaluation 
To evaluate the constituent order models in 
isolation, we designed our experiments to be 
independent of the rest of the Amalgam sentence 
realization process. We use test sets of 1,000 
sentences, also from technical manuals, for each 
language. To isolate ordering, for a given test 
sentence, we process the sentence as in training to 
produce an ordered tree ?  (the reference for 
evaluation) and from it an unordered tree ? . 
Given ? , we then search for the best ordered tree 
hypothesis ??  using the model in question. 
We then compare ?  and ?? . Because we are 
only ordering constituents, we can compare ? and 
??  by comparing their respective constituents. For 
each C in ? , we measure the per-constituent edit 
distance D, between C and its counterpart C? in ??  
as  follows: 
1. Let d be the edit distance between the 
ordered set of daughters in each, with the 
only possible edit operators being insert and 
delete 
2. Let the number of moves / 2m d= , since 
insertions and deletions can be paired 
uniquely 
3. Divide by the total number of 
daughters: ( )/D m daughters C=  
This metric is like the ?Generation Tree Accuracy? 
metric of Bangalore & Rambow (2000), except 
that there is no need to consider cross-constituent 
moves. The total score for the hypothesis tree ??  is 
the mean of the per-constituent edit distances. 
For each of the models under consideration and 
each language, we report in Table 1 the average 
score across the test set for the given language. The 
first row is a baseline computed from randomly 
scrambling constituents (mean over four 
iterations). 
Model German French 
Baseline (random) 35.14 % 34.36 % 
T1: DT joint 5.948% 3.828% 
T2: DT joint 
with ( )if ?   5.852% 4.008% 
T5: DT conditional 6.053% 4.271% 
T7: DT binary cond., 
greedy search 3.516% 1.986% 
T7: DT normalized 
binary conditional, 
exhaustive search 
3.400% 1.810% 
Table 1: Mean per-constituent edit distances for 
German & French. 
5.4 Discussion 
For both German and French, the binary 
conditional DT model outperforms all other 
models. Normalizing the binary conditional model 
and applying it in an exhaustive search performs 
better than a greedy search. All score differences 
are statistically significant; moreover, manual 
inspection of the differences for the various models 
also substantiates the better quality of those models 
with lower scores. 
With regard to the question of conditional versus 
joint models, the joint models (T1, T2) outperform 
their conditional counterparts (T5). This may be 
due to a lack of sufficient training data for the 
conditional models. At this time, the training time 
of the conditional models is the limiting factor. 
There is a clear disparity between the 
performance of the German models and the 
performance of the French models. The best 
German model is twice as bad as the best French 
model.  (For a discussion of the impact of 
modeling German verb position, please consult 
(Ringger et al, 2003).) 
 Baseline 
(random) 
Greedy, 
IOCC Greedy 
DP,  
IOCC DP 
Total Sentences 2416 2416 2416 2416 2416 
Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 
Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 
Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% 
Coverage 100% 100% 100% 100% 100% 
Mean Per-Const. Edit Dist. 38.3% 6.02% 6.84% 5.25% 4.98% 
Mean NIST SSA -16.75 74.98 67.19 74.65 73.24 
Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836 
Table 2: DSIF-Amalgam ordering performance on WSJ section 23. 
6 Evaluation on the Penn TreeBank 
Our goal in evaluating on Penn Tree Bank (PTB) 
data is two-fold: (1) to enable a comparison of 
Amalgam?s performance with other systems 
operating on similar input, and (2) to measure 
Amalgam?s capabilities on less domain-specific 
data than technical software manuals. We derive 
from the bracketed tree structures in the PTB using 
a deterministic procedure an abstract 
representation we refer to as a Dependency 
Structure Input Format (DSIF), which is only 
loosely related to NLPWin?s abstract predicate-
argument structures. 
The PTB to DSIF transformation pipeline 
includes the following stages, inspired by 
Langkilde-Geary?s (2002b) description: 
A. Deserialize the tree 
B. Label heads, according to Charniak?s head 
labeling rules (Charniak, 2000) 
C. Remove empty nodes and flatten any 
remaining empty non-terminals 
D. Relabel heads to conform more closely to the 
head conventions of NLPWin 
E. Label with logical roles, inferred from PTB 
functional roles 
F. Flatten to maximal projections of heads 
(MPH), except in the case of conjunctions 
G. Flatten non-branching non-terminals 
H. Perform dictionary look-up and 
morphological analysis 
I. Introduce structure for material between 
paired delimiters and for any coordination 
not already represented in the PTB 
J. Remove punctuation 
K. Remove function words 
L. Map the head of each maximal projection to 
a dependency node, and map the head?s 
modifiers to the first node?s dependents, 
thereby forming a complete dependency tree. 
To evaluate ordering performance alone, our data 
are obtained by performing all of the steps above 
except for (J) and (K). We employ only a binary 
conditional ordering model, found in the previous 
section to be the best of the models considered. To 
train the order models, we use a set of 10,000 
sentences drawn from the standard PTB training 
set, namely sections 02?21 from the Wall Street 
Journal portion of the PTB (the full set contains 
approx. 40,000 sentences). For development and 
parameter tuning we used a separate set of 2000 
sentences drawn from sections 02?21. 
Decision trees are trained for each of five 
constituent types characterized by their head 
labels: adjectival, nominal, verbal, conjunctions 
(coordinated material), and other constituents not 
already covered. The split DTs can be thought of 
as a single DT with a five-way split at the top 
node. 
Our DSIF test set consists of the blind test set 
(section 23) of the WSJ portion of the PTB. At 
run-time, for each converted tree in the test set, all 
daughters of a given constituent are first permuted 
randomly with one another (scrambled), with the 
option for coordinated constituents to remain 
unscrambled, according to the Input-Output 
Coordination Consistency (IOCC) option. For a 
given unordered (scrambled) constituent, the 
appropriate order model (noun-head, verb-head, 
etc.) is used in the ordering search to order the 
daughters. Note that for the greedy search, the 
input order can influence the final result; therefore, 
we repeat this process for multiple random 
scramblings and average the results. 
We use the evaluation metrics employed in 
published evaluations of HALogen, FUF/SURGE, 
and FERGUS (e.g., Calloway, 2003), although our 
results are for ordering only. Coverage, or the 
percentage of inputs for which a system can 
produce a corresponding output, is uninformative 
for the Amalgam system, since in all cases, it can 
generate an output for any given DSIF. In addition 
to processing time per input, we apply four other 
metrics: exact match, NIST simple string accuracy 
(the complement of the familiar word error rate), 
the IBM Bleu score (Papineni et al, 2001), and the 
intra-constituent edit distance metric introduced 
earlier. 
We evaluate against ideal trees, directly 
computed from PTB bracketed tree structures. The 
results in Table 2 show the effects of varying the 
IOCC parameter. For both trials involving a greedy 
search, the results were averaged across 25 
iterations. As should be expected, turning on the 
input-output faithfulness option (IOCC) improves 
the performance of the greedy search. Keeping 
coordinated material in the same relative order 
would only be called for in applications that plan 
discourse structure before or during generation. 
7 Conclusions and Future Work 
The experiments presented here provide 
conclusive reasons to favor the binary conditional 
model as a model of constituent order. The 
inclusion of linguistic features is of great value to 
the modeling of order, specifically in verbal 
constituents for both French and German. 
Unfortunately space did not permit a thorough 
discussion of the linguistic features used. Judging 
from the high number of features that were 
selected during training for participation in the 
conditional and binary conditional models, the 
availability of automatic feature selection is 
critical. 
Our conditional and binary conditional models 
are currently lexicalized only for function words; 
the joint models not at all. Experiments by Daum? 
et al(2002) and the parsing work of Charniak 
(2000) and others indicate that further 
lexicalization may yield some additional 
improvements for ordering. However, the parsing 
results of Klein & Manning (2003) involving 
unlexicalized grammars suggest that gains may be 
limited. 
For comparison, we encourage implementers of 
other sentence realization systems to conduct 
order-only evaluations using PTB data. 
Acknowledgements 
We wish to thank Irene Langkilde-Geary and 
members of the MSR NLP group for helpful 
discussions.  Thanks also go to the anonymous 
reviewers for helpful feedback. 
References  
Aikawa T., Melero M., Schwartz L. Wu A. 2001. 
Multilingual sentence generation. In Proc. of 8th 
European Workshop on NLG. pp. 57-63. 
Bangalore S. Rambow O. 2000. Exploiting a 
probabilistic hierarchical model for generation. 
In Proc. of COLING. pp. 42-48. 
Calloway, C. 2003. Evaluating Coverage for Large 
Symbolic NLG Grammars.  In Proc. of IJCAI 
2003. pp 811-817. 
Charniak E. 1997. Statistical Techniques for 
Natural Language Parsing, In AI Magazine. 
Charniak E. 2000. A Maximum-Entropy-Inspired 
Parser. In Proc. of ACL. pp.132-139. 
Chickering D. M. 2002. The WinMine Toolkit. 
Microsoft Technical Report 2002-103. 
Corston-Oliver S., Gamon M., Ringger E., Moore 
R. 2002. An overview of Amalgam: a machine-
learned generation module. In Proc. of INLG. 
pp.33-40. 
Daum? III H., Knight K., Langkilde-Geary I., 
Marcu D., Yamada K. 2002. The Importance of 
Lexicalized Syntax Models for Natural 
Language Generation Tasks. In Proc. of INLG. 
pp. 9-16. 
Gamon M., Ringger E., Corston-Oliver S. 2002a. 
Amalgam: A machine-learned generation 
module. Microsoft Technical Report 2002-57. 
Gamon M., Ringger E., Corston-Oliver S., Moore 
R. 2002b. Machine-learned contexts for 
linguistic operations in German sentence 
realization. In Proc. of ACL. pp. 25-32. 
Heidorn G. 2000. Intelligent Writing Assistance. In 
A Handbook of Natural Language Processing,, 
R. Dale, H. Moisl, H. Somers (eds.). Marcel 
Dekker, NY. 
Klein D., Manning C. 2003. "Accurate 
Unlexicalized Parsing." In Proceedings of ACL-
03. 
Langkilde I. 2000. Forest-Based Statistical 
Sentence generation. In Proc. of NAACL. pp. 
170-177. 
Langkilde-Geary I. 2002a. An Empirical 
Verification of Coverage and Correctness for a 
General-Purpose Sentence Generator. In Proc. of 
INLG. pp.17-24. 
Langkilde-Geary, I. 2002b. A Foundation for 
General-purpose Natural Language Generation: 
Sentence Realization Using Probabilistic Models 
of Language. PhD Thesis, University of 
Southern California. 
Langkilde I., Knight K. 1998a. The practical value 
of n-grams in generation. In Proc. of 9th 
International Workshop on NLG. pp. 248-255. 
Langkilde I., Knight K. 1998b. Generation that 
exploits corpus-based statistical knowledge. In 
Proc. of ACL and COLING. pp. 704-710. 
McCallum A., Freitag D., & Pereira F. 2000. 
?Maximum Entropy Markov Models for 
Information Extraction and Segmentation.? In 
Proc. Of ICML-2000. 
Papineni, K.A., Roukos, S., Ward, T., and Zhu, 
W.J. 2001. Bleu: a method for automatic 
evaluation of machine translation. IBM 
Technical Report RC22176 (W0109-022). 
Reiter E. and Dale R. 2000. Building natural 
language generation systems. Cambridge 
University Press, Cambridge. 
Ringger E., Gamon M., Smets M., Corston-Oliver 
S. and Moore R. 2003 Linguistically informed 
models of constituent structure for ordering in 
sentence realization. Microsoft Research 
technical report MSR-TR-2003-54. 
Smets M., Gamon M., Corston-Oliver S. and 
Ringger E. (2003) The adaptation of a machine-
learned sentence realization system to French. 
In Proceedings of EACL. 
Sentiment classification on customer feedback data: noisy data, large feature 
vectors, and the role of linguistic analysis 
Michael Gamon 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
 
Abstract 
We demonstrate that it is possible to perform 
automatic sentiment classification in the very noisy 
domain of customer feedback data. We show that 
by using large feature vectors in combination with 
feature reduction, we can train linear support 
vector machines that achieve high classification 
accuracy on data that present classification 
challenges even for a human annotator. We also 
show that, surprisingly, the addition of deep 
linguistic analysis features to a set of surface level 
word n-gram features contributes consistently to 
classification accuracy in this domain. 
1 Introduction 
Software companies typically receive high 
volumes of electronic customer feedback every 
day, some of it in the form of elicited surveys, 
some of it in the form of unsolicited comments, 
suggestions, criticism. In order to react to that 
feedback quickly, and to direct it to the appropriate 
channels inside the company, it is desirable to 
provide intelligent and automatic classification of 
the feedback along two dimensions: 
What is the feedback about? 
Is the feedback positive or negative? 
The first question is addressed by text mining 
tools. Automatic sentiment classification addresses 
the second question. Text mining tools can help 
make large quantities of feedback more 
manageable by splitting them into clusters based 
on keywords or topics. Sentiment analysis, which 
is the focus of this paper, adds a second dimension 
to the analysis. It makes it possible to focus the 
text mining on areas in need of improvement 
(negative feedback) or on areas of success 
(positive feedback). 
Sentiment classification is a special case of text 
categorization, where the criterion of classification 
is the attitude expressed in the text, rather than the 
?content? or topic. Faced with the task of having to 
automatically classify a piece of text as expressing 
positive or negative sentiment, a reasonable first 
approach would consist of paying special attention 
to words that tend to express a positive or negative 
attitude. Pang et al (2002) have demonstrated, 
however, that this is not as straightforward as one 
may think, given that sentiment is often expressed 
in more subtle and indirect ways. 
The literature on sentiment classification can be 
divided into approaches that rely on semantic 
resources, such as a sentiment or affect lexicon 
(Nasukawa and Yi 2003, Subasic and Huettner 
2001), or a large scale knowledge base (Liu et al
2003) on the one hand, and approaches that try to 
learn patterns directly from tagged data, without 
additional resources (Dave et al2003, Pang et al 
2003). Much research is also being directed at 
acquiring affect lexica automatically (Turney 2002, 
Turney and Littman 2002). 
There is also a considerable amount of research 
on classification of text as ?subjective? or 
?objective? (Wiebe et al2001, Yu and 
Hatzivassiloglou 2003), a task that is not relevant 
for the processing of very brief pieces of direct 
customer feedback. 
In many studies, research on sentiment 
classification is conducted on review-type data, 
such as movie or restaurant reviews. These data 
often consist of relatively well-formed, coherent 
and at least paragraph-length pieces of text. The 
results we present in this paper are based on 
customer feedback data from web surveys, which, 
as we will discuss below, are particularly noisy and 
fragmentary. 
For our purpose of automatic classification of 
customer feedback, we decided to use machine-
learning directly on the customer feedback, instead 
of relying on additional semantic resources of any 
kind. This decision was motivated by practical 
considerations: first, the customer feedback data 
we are facing are often very short and sometimes 
very incoherent. This makes it seem unlikely that a 
detailed semantic resource would be of particular 
help. Second, we believe that an appropriately 
chosen machine-learning technique will be able to 
draw its own conclusions from the distribution of 
lexical elements in a piece of feedback. 
We conducted our sentiment classification 
experiments using support vector machines. 
Support vector machines (SVMs) have a good 
track record in text classification (Joachims 1998, 
Dumais et al 1998), they can be trained using a 
large number of features, and both training and 
classification for linear SVMs are fast with 
optimized learning algorithms. For our 
experiments we use John Platt?s Sequential 
Minimal Optimization (SMO) tool (Platt 1999). In 
the absence of any evidence that would suggest a 
more complicated kernel function such as a 
polynomial or an RBF kernel, we have decided to 
train linear SVMs for our classification task (see 
also the results in Joachims 1998). 
The procedure, as is standard in supervised 
machine learning tasks, consists of training a 
classifier on pretagged training data and then 
evaluating the performance of the classifier on a 
held-out set of test data. 
The two main questions we wanted to assess 
with our experiments are: 
1. which features and feature sets are 
relevant for sentiment classification on 
customer feedback? 
2. what is the maximum classification 
accuracy that can be achieved on this 
data set? 
2 Data 
Our data consists of 11399 feedback items from 
a Global Support Services survey, and 29485 
feedback items from a Knowledge Base survey for 
a total of 40884 items. We excluded pieces of 
feedback without any verbatim from the data. 
Along with the verbatim, customers provided a 
numeric satisfaction score on a scale from 1 (not 
satisfied) to 4 (very satisfied) for each of those 
pieces of feedback. The numeric score served as 
the target tag in our experiments, making it 
unnecessary to perform any costly human 
evaluation and tagging. The distribution of items 
across numerical scores is given in Table 1. 
Category 1 2 3 4 
Number 
of documents 
8596 9060 14573 8655 
Table 1: number of documents in each 
satisfaction category 
The data is extremely noisy, and a human 
evaluation of a random set of 200 pieces of 
feedback could only assign a positive or negative 
sentiment to 117 (58.5%) items, the rest was either 
balanced (16 cases or 8%), expressed no sentiment 
(50 cases or 25%), or too incoherent or random to 
be classified (17 cases or 8.5%). Amongst the 117 
classifiable cases, the human evaluator assigned 
the category ?positive?: to 26 cases (or 22.2%) and 
the category ?negative? to 91 cases (or 77.8%). 
After automatic sentence breaking into one 
sentence per line, the individual files contained an 
average of 2.56 lines. For our experiments we split 
the data 90/10 into training and held-out test data. 
We performed 10-fold cross validation for each of 
the experiments reported in this paper. 
For each of the various classification tasks, we 
trained a linear SVM using the standard settings of 
the SMO tool, and calculated accuracy, precision 
and recall numbers on the held-out test data, 
averaging them across the 10-fold cross validation. 
3 Features 
3.1 Feature vectors 
We experimented with a range of different 
feature sets. Most importantly, we wanted to 
establish whether we would gain any significant 
advantage in the sentiment classification task by 
using features based on deep linguistic analysis or 
whether surface-based features would suffice. 
Previous results in authorship attribution and style 
classification experiments had indicated that 
linguistic features contribute to the overall 
accuracy of the classifiers, although our null 
hypothesis based on a review of the relevant 
literature for sentiment classification was that we 
would not gain much by using these features. The 
surface features we used were lemma unigrams, 
lemma bigrams, and lemma trigrams. 
For the linguistic features, we performed a 
linguistic analysis of the data with the NLPWin 
natural language processing system developed in 
Microsoft Research (an overview can be found in 
Heidorn 2000). NLPWin provides us with a phrase 
structure tree and a logical form for each string, 
from which we can extract an additional set of 
features: 
? part-of-speech trigrams 
? constituent specific length measures 
(length of sentence, clauses, 
adverbial/adjectival phrases, and noun 
phrases) 
? constituent structure in the form of context 
free phrase structure patterns for each 
constituent in a parse tree. Example: 
DECL::NP VERB NP (a declarative 
sentence consisting of a noun phrase a 
verbal head and a second noun phrase) 
? Part of speech information coupled with 
semantic relations (e.g. ?Verb - Subject - 
Noun? indicating a nominal subject to a 
verbal predicate) 
? Logical form features provided by 
NLPWin, such as transitivity of a 
predicate, tense information etc. 
For each of these features, except for the length 
features, we extract a binary value, corresponding 
to the presence or absence of that feature in a given 
document. Using binary values for 
presence/absence as opposed to frequency values is 
motivated by the rather extreme brevity of these 
documents. 
3.2 Feature reduction 
Feature reduction is an important part of 
optimizing the performance of a (linear) classifier 
by reducing the feature vector to a size that does 
not exceed the number of training cases as a 
starting point. Further reduction of vector size can 
lead to more improvements if the features are noisy 
or redundant. 
Reducing the number of features in the feature 
vector can be done in two different ways: 
? reduction to the top ranking n features 
based on some criterion of 
?predictiveness? 
? reduction by elimination of sets of 
features (e.g. elimination of linguistic 
analysis features etc.) 
Experimenting with the elimination of feature 
sets provides an answer to the question as to which 
qualitative sets of features play a significant role in 
the classification task 
Of course these methods can also be combined, 
for example by eliminating sets of features and 
then taking the top ranking n features from the 
remaining set. 
We used both techniques (and their 
combinations) in our experiments. The measure of 
?predictiveness? we employed is log likelihood 
ratio with respect to the target variable (Dunning 
1993). 
In the experiments described below, n (in the n 
top-ranked features) ranged from 1000 to 40,000. 
The different feature set combinations we used 
were: 
? ?all features? 
? ?no linguistic features? (only word 
ngrams) 
? ?surface features? (word ngrams, 
function word frequencies and POS 
ngrams) 
? ?linguistic features only? (no word 
ngrams) 
4 Results 
Given the four different rankings associated by 
users with their feedback, we experimented with 
two distinct classification scenarios: 
1. classification of documents as belonging 
to category 1 versus category 4 
2. classification of documents as belonging 
to categories 1 or 2 on the one hand, and 
3 or 4 on the other 
Two additional scenarios can be envisioned. In 
the first, two classifiers (?1 versus 2/3/4? and ?4 
versus 1/2/3?) would be trained and their votes 
would be combined either through weighted 
probability voting or other classifier combination 
methods (Dietterich 1997). A second possibility 
is to learn a three-way distinction ?1 versus 2/3 
versus 4?. In this paper we restrict ourselves to 
the scenarios 1 and 2 above. Initial experiments 
suggest that the combination of two classifiers 
yields only minimal improvements. 
4.1 Classification of category 1 versus 
category 4 
Figure 1 below illustrates the accuracy of the ?1 
versus 4? classifier at different feature reduction 
cutoffs and with different feature sets. The 
accuracy differences are statistically significant at 
the .99 confidence level, based on the 10fold cross 
validation scenario. Figure 2and Figure 3 show the 
F1-measure for target value 4 (?good sentiment?) 
and target value 1 (?bad sentiment?) respectively. 
The baseline for this experiment is 50.17% 
(choosing category 4 as the value for the target 
feature by default). 
Accuracy peaks at 77.5% when the top 2000 
features in terms of log likelihood ratio are used, 
and when the feature set is not restricted, i.e. when 
these top 2000 features are drawn from linguistic 
and surface features. We will return to the role of 
linguistic features in section 4.4. 
F1-measure for both target 4 (Figure 2) and 
target 1 (Figure 3) exhibit a similar picture, again 
we achieve maximum performance by using the 
top 2000 features from the complete pool of 
features. 
Accuracy 1 versus 4
70
71
72
73
74
75
76
77
78
79
80
20k 10k 5k 2k 1k
number of features
ac
cu
ra
cy
all features
no linguistic features
surface features
linguistic features only
 
Figure 1: Accuracy of the 1 versus 4 classifier 
Target 4: F-measure 1 vs 4 classifier
70
71
72
73
74
75
76
77
78
79
80
20k 10k 5k 2k 1k
number of features
F-
m
ea
su
re
all features
no linguistic features
surface features
linguistic features only
 
Figure 2: F1-measure for target category 4 
Target 1: F-measure 1 vs 4 classifier
70
71
72
73
74
75
76
77
78
79
80
20k 10k 5k 2k 1k
number of features
F-
m
ea
su
re
all features
no linguistic features
surface features
linguistic features only
 
Figure 3: F1-measure for target category 1 
4.2 Classification of categories 1 and 2 versus 
3 and 4 
Accuracy and F1-measure results for the ?1/2 
versus 3/4? task are shown in Figure 4, Figure 5 
and Figure 6. Again, the accuracy differences are 
statistically significant. The baseline in this 
scenario is at 56.81% (choosing category 3/4 for 
the target feature by default). Classification 
accuracy is lower than in the ?1 versus 4? scenario, 
as can be expected since the fuzzy categories 2 and 
3 are included in the training and test data. 
Similarly to the ?1 versus 4? classification, 
accuracy is maximal at 69.48% when the top 2000 
features from the complete feature set are used. 
The F1-measure for the target value 1/2 peaks at 
the same feature reduction cutoff, whereas the F1-
measure for the target value 3/4 benefits from more 
drastic feature reduction to a set of only the top-
ranked 1000 features. 
 Accuracy 1/2 versus 3/4
65
66
67
68
69
70
71
72
73
74
75
20k 10k 5k 2k 1k
number of features
ac
cu
ra
cy
all features
no linguistic features
surface features
linguistic features only
 
Figure 4: Accuracy of the 1/2 versus 3/4 classifier 
Target 3/4: F-measure 1/2 versus 3/4 classifier
71
71.5
72
72.5
73
73.5
74
74.5
75
75.5
76
20k 10k 5k 2k 1k
number of features
F-
m
ea
su
re
all features
no linguistic features
surface features
linguistic features only
 
Figure 5: F1-measure for target category 3/4 
Target 1/2: F-measure 1/2 versus 3/4 classifier
55
56
57
58
59
60
61
62
63
64
65
20k 10k 5k 2k 1k
number of features
F-
m
ea
su
re
all features
no linguistic features
surface features
linguistic features only
 
Figure 6: F1-measure for target category 1/2 
4.3 Results compared to human classification 
The numbers reported in the previous sections 
are substantially lower than results that have been 
reported on other data sets such as movie or 
restaurant reviews. Pang et al (2002), for example, 
report a maximum accuracy of 82.9% on movie 
reviews. As we have observed in section 2, the 
data that we are dealing with here are extremely 
noisy. Recall that on a random sample of 200 
pieces of feedback even a human evaluator could 
only assign a sentiment classification to 117 of the 
documents, the remaining 83 being either balanced 
in their sentiment, or too unclear or too short to be 
classifiable at all. In order to assess performance of 
our classifiers on ?cleaner? data, we used the 117 
humanly classifiable pieces of customer feedback 
as a test set for the best performing classifier 
scenario. For that purpose, we retrained both ?1 
versus 4? and ?1/2 versus 3/4? classifiers with the 
top-ranked 2000 features on our data set, with the 
humanly evaluated cases removed from the 
training set. Results are shown in Table 2, the 
baseline in this experiment is at 77.78% (choosing 
the ?bad? sentiment as a default). 
 1 versus 4 
using top 2k 
features 
1/2 versus 3/4 
using top 2k 
features 
Accuracy 85.47 69.23 
F-measure 
?good? 
74.62 58.14 
F-measure 
?bad? 
89.82 75.67 
Table 2: Results of the two best classifiers on 
humanly classifiable data 
Accuracy of 85.47% as achieved by the ?1 
versus 4? scenario is in line with accuracy numbers 
reported for less noisy domains. 
4.4 The role of linguistic analysis features 
Figure 1 through Figure 6 also show the effect of 
eliminating whole feature sets from the training 
process. A result that came as a surprise to us is the 
fact that the presence of very abstract linguistic 
analysis features based on constituent structure and 
semantic dependency graphs improves the 
performance of the classifiers. The only exception 
to this observation is the F1-measure for the 
?good? sentiment case in the ?1/2 versus 3/4? 
scenario (Figure 5), where the different feature sets 
yield very much similar performance across the 
feature reduction spectrum, with the ?no linguistic 
features? even outperforming the other feature sets 
by a very small margin (0.18%). While the 
improvement in practice may be too small to 
warrant the overhead of linguistic analysis, it is 
very interesting from a linguistic point of view that 
even in a domain as noisy as this one, there seem 
to be robust stylistic and linguistic correlates with 
sentiment. Note that in the ?1 versus 4? scenario 
we can achieve classification accuracy of 74.5% by 
using only linguistic features (Figure 1), without 
the use of any word n-gram features (or any other 
word-based information) at all. This clearly 
indicates that affect and style are linked in a more 
significant way than has been previously suggested 
in the literature. 
4.5 Relevant features 
Given that linguistic features play a consistent 
role in the experiments described here, we 
inspected the models to see which features play a 
particularly big role as indicated by their 
associated weights in the linear svm. This is 
particularly interesting in light of the fact that in 
previous research on sentiment classification, 
affect lexica or other special semantic resources 
have served as a source for features (see references 
in section 1). When looking at the top 100 
weighted features in the best classifier (?1 versus 
4?), we found an interesting mix of the obvious, 
and the not-so-obvious. Amongst the obviously 
?affect?-charged terms and features in the top 100 
are: 
+Neg1, unable to, thanks, the good, easy to, ease 
of, lack of, not find, not work, no help, much 
accurate, a simple 
On the other hand, there are many features that 
carry high weights, but are not what one would 
intuitively think of as a typical affect indicator: 
try the, of, off, ++Univ2, ADV PRON PREP3, 
NP::PRON:CHAR 4 , @@Adj Props Verb Tsub 
Pron5, AUXP::VERB, your 
We conclude from this inspection of individual 
features that within a specific domain it is not 
necessarily advisable to start out with a resource 
that has been geared towards containing 
particularly affect-charged terminology. See Pang 
et al (2002) for a similar argument. As our 
numbers and feature sets suggest, there are many 
terms (and grammatical patterns) associated with 
sentiment in a given domain that may not fall into 
a typical affect class. 
We believe that these results show that as with 
many other classification tasks in the machine 
learning literature, it is preferable to start without 
an artificially limited ?hand-crafted? set of 
features. By using large feature sets which are 
derived from the data, and by paring down the 
number of features through a feature reduction 
procedure if necessary, relevant patterns in the data 
can be identified that may not have been obvious 
to the human intuition. 
5 Conclusion 
We have shown that in the very noisy domain of 
customer feedback, it is nevertheless possible to 
perform sentiment classification. This can be 
achieved by using large initial feature vectors 
combined with feature reduction based on log 
                                                     
1
 this semantic feature indicates a negated context. 
2
 Universal quantification. 
3
 part of speech trigram. 
4
 An NP consisting of a pronoun followed by a 
punctuation character. 
5
 An adjectival semantic node modified by a verbal 
proposition and a pronominal subject. This is in fact the 
representation for a copular construction of the form 
?pronoun be adjective to verb...? as in ?I am happy to 
report...? 
likelihood ratio. A second, more surprising result is 
that the use of abstract linguistic analysis features 
consistently contributes to the classification 
accuracy in sentiment classification. While results 
like this have been reported in the area of style 
classification (Baayen et al 1996, Gamon 2004), 
they are noteworthy in a domain where stylistic 
markers have not been considered in the past, 
indicating the need for more research into the 
stylistic correlations of affect in text. 
6 Acknowledgements 
We thank Anthony Aue and Eric Ringger 
(Microsoft Research) and Hang Li (Microsoft 
Research Asia) for helpful comments and 
discussions, and Chris Moore (Microsoft Product 
Support Services UK) for the initial request for 
sentiment classification based on the needs of 
Support Services at Microsoft. Thanks also go to 
Karin Berghoefer of the Butler-Hill group for 
manually annotating a subset of the data. 
References  
Harald Baayen, Hans van Halteren, and Fiona 
Tweedie. 1996. Outside the Cave of Shadows: 
Using Syntactic Annotation to Enhance 
Authorship Attribution. Literary and Linguistic 
Computing 11(3): 121-131. 
Thomas G. Dietterich (1997): ?Machine-learning 
research: Four current directions?. In: AI 
Magazine, 18 (4), pp.97-136. 
Susan Dumais, John Platt, David Heckerman, 
Mehran Sahami (1998): ?Inductive Learning 
Algorithms and Representations for Text 
Categorization?. Proceedings of CIKM-98, pp. 
148-155. 
Ted Dunning. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence. 
Computational Linguistics 19: 61-74. 
Aidan Finn and Nicholas Kushmerick (2003): 
?Learning to classify documents according to 
genre?. IJCAI-03 Workshop on Computational 
Approaches to Text Style and Synthesis. 
Michael Gamon (2004): ?Linguistic correlates of 
style: authorship classification with deep 
linguistic analysis features?. Paper to be 
presented at COLING 2004. 
George Heidorn. (2000): ?Intelligent Writing 
Assistance.? In R. Dale, H. Moisl and H. 
Somers, eds., Handbook of Natural Language 
Processing. Marcel Dekker. 
Thorsten Joachims (1998): ?Text Categorization 
with Support Vector Machines: Learning with 
Many Relevant Features?. Proceedings of ECML 
1998, pp. 137-142. 
Kushal Dave, Steve Lawrence and David M. 
Pennock (2003): ?Mining the Peanut Gallery: 
Opinion Extraction and Semantic Classification 
of Product Reviews?. In: Proceedings of the 
Twelfth International World Wide Web 
Conference, pp. 519-528. 
Hugo Liu, Henry Lieberman and Ted Selker 
(2003): ?A Model of Textual Affect Sensing 
using Real-World Knowledge?. In: Proceedings 
of the Seventh Conference on Intelligent User 
Interfaces, pp. 125-132. 
Tetsuya Nasukawa and Jeonghee Yi (2003): 
?Sentiment Analysis: Capturing Favorability 
Using Natural Language Processing?. In: 
proceedings of the International Conference on 
Knowledge Capture, pp. 70-77. 
Bo Pang, Lillian Lee and Shivakumar 
Vaithyanathan (2002): ?Thumbs up? Sentiment 
Classification using Machine Learning 
Techniques?. Proceedings of EMNLP 2002, pp. 
79-86. 
John Platt (1999): ?Fast training of SVMs using 
sequential minimal optimization?. In: B. 
Schoelkopf, C. Burges and A. Smola (eds.) 
?Advances in Kernel Methods: Support Vector 
Learning?, MIT Press, Cambridge, MA, pp. 185-
208. 
Pero Subasic and Alison Huettner (2001): ?Affect 
Analysis of Text Using Fuzzy Semantic 
Typing?. In: Proceedings of the Tenth IEEE 
International Conference on Fuzzy Systems, pp. 
483-496. 
Ljup?o Todorovski and Sa?o D?eroski (2003): 
?Combining Classifiers with Meta Decision 
Trees?. In: Machine Learning, 50, pp.223-249. 
Peter D. Turney (2002): ?Thumbs up or thumbs 
down? Semantic orientation applied to 
unsupervised classification of reviews?. In: 
Proceedings of ACL 2002, pp. 417-424. 
Peter D. Turney and M. L. Littman (2002): 
?Unsupervised lLearning of Semantic 
Orientation from a Hundred-Billion-Word 
Corpus.? Technical report ERC-1094 (NRC 
44929), National research Council of Canada. 
Janyce Wiebe, Theresa Wilson and Matthew Bell 
(2001): ?Identifying Collocations for 
Recognizing Opinions?. In: Proceedings of the 
ACL/EACL Workshop on Collocation. 
Hong Yu and Vasileios Hatzivassiloglou (2003): 
?Towards Answering pinion Questions: 
Separating Facts from Opinions and Identifying 
the Polarity of Opinion Sentences?. In: 
Proceedings of EMNLP 2003. 
323
324
325
326
327
328
329
330
Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
A machine learning approach
to the automatic evaluation of machine translation
Simon Corston-Oliver, Michael Gamon and Chris Brockett
Microsoft Research
One Microsoft Way
Redmond WA 98052, USA
{simonco, mgamon, chrisbkt}@microsoft.com
Abstract
We present a machine learning
approach to evaluating the well-
formedness of output of a machine
translation system, using classifiers that
learn to distinguish human reference
translations from machine translations.
This approach can be used to evaluate
an MT system, tracking improvements
over time; to aid in the kind of failure
analysis that can help guide system
development; and to select among
alternative output strings. The method
presented is fully automated and
independent of source language, target
language and domain.
1 Introduction
Human evaluation of machine translation (MT)
output is an expensive process, often
prohibitively so when evaluations must be
performed quickly and frequently in order to
measure progress. This paper describes an
approach to automated evaluation designed to
facilitate the identification of areas for
investigation and improvement. It focuses on
evaluating the wellformedness of output and
does not address issues of evaluating content
transfer.
Researchers are now applying automated
evaluation in MT and natural language
generation tasks, both as system-internal
goodness metrics and for the assessment of
output. Langkilde and Knight (1998), for
example, employ n-gram metrics to select
among candidate outputs in natural language
generation, while Ringger et al (2001) use n-
gram perplexity to compare the output of MT
systems. Su et al (1992), Alshawi et al (1998)
and Bangalore et al (2000) employ string edit
distance between reference and output sentences
to gauge output quality for MT and generation.
To be useful to researchers, however,
assessment must provide linguistic information
that can guide in identifying areas where work is
required. (See Nyberg et al, 1994 for useful
discussion of this issue.)
The better the MT system, the more its
output will resemble human-generated text.
Indeed, MT might be considered a solved
problem should it ever become impossible to
distinguish automated output from human
translation. We have observed that in general
humans can easily and reliably categorize a
sentence as either machine- or human-generated.
Moreover, they can usually justify their
decision. This observation suggests that
evaluation of the wellformedness of output
sentences can be treated as a classification
problem: given a sentence, how accurately can
we predict whether it has been translated by
machine? In this paper we cast the problem of
MT evaluation as a machine learning
classification task that targets both linguistic
features and more abstract features such as n-
gram perplexity.
2 Data
Our corpus consists of 350,000 aligned Spanish-
English sentence pairs taken from published
computer software manuals and online help
documents. We extracted 200,000 English
sentences for building language models to
evaluate per-sentence perplexity. From the
remainder of the corpus, we extracted 100,000
aligned sentence pairs. The Spanish sentences in
this latter sample were then translated by the
Microsoft machine translation system, which
was trained on documents from this domain
(Richardson et al, 2001). This yielded a set of
200,000 English sentences, one half of which
were English reference sentences, and the other
half of which were MT output. (The Spanish
sentences were not used in building or
evaluating the classifiers). We split the 200,000
English sentences 90/10, to yield 180,000
sentences for training classifiers and 20,000
sentences that we used as held-out test data.
Training and test data were evenly divided
between reference English sentences and
Spanish-to-English translations.
3 Features
The selection of features used in our
classification task was motivated by failure
analysis of system output. We were particularly
interested in those linguistic features that could
aid in qualitative analysis, as we discuss in
section 5. For each sentence we automatically
extracted 46 features by performing a syntactic
parse using the Microsoft NLPWin natural
language processing system (Heidorn, 2000) and
language modeling tools. The features extracted
fall into two broad categories:
(i) Perplexity measures were extracted using the
CMU-Cambridge Statistical Language Modeling
Toolkit (Clarkson and Rosenfeld, 1997). We
calculated two sets of values: lexicalized trigram
perplexity, with values discretized into deciles
and part of speech (POS) trigram perplexity. For
the latter we used the following sixteen POS
tags: adjective, adverb, auxiliary, punctuation,
complementizer, coordinating conjunction,
subordinating conjunction, determiner,
interjection, noun, possessor, preposition,
pronoun, quantifier, verb, and other.
(ii) Linguistic features fell into several
subcategories: branching properties of the parse;
function word density, constituent length, and
other miscellaneous features
We employed a selection of features to
provide a detailed assessment of the branching
properties of the parse tree. The linguistic
motivation behind this was twofold. First, it had
become apparent from failure analysis that MT
system output tended to favor right-branching
structures over noun compounding. Second, we
hypothesized that translation from languages
whose branching properties are radically
different from English (e.g. Japanese, or a verb-
second language like German) might pollute the
English output with non-English characteristics.
For this reason, assessment of branching
properties is a good candidate for a language-
pair independent measure. The branching
features we employed are given below. Indices
are scalar counts; other measures are normalized
for sentence length.
? number of right-branching nodes across
all constituent types
? number of right-branching nodes for
NPs only
? number of left-branching nodes across
all constituent types
? number of left-branching nodes for NPs
only
? number of premodifiers across all
constituent types
? number of premodifiers within NPs only
? number of postmodifiers across all
constituent types
? number of postmodifiers within NPs
only
? branching index across all constituent
types, i.e. the number of right-branching
nodes minus number of left-branching
nodes
? branching index for NPs only
? branching weight index: number of
tokens covered by right-branching
nodes minus number of tokens covered
by left-branching nodes across all
categories
? branching weight index for NPs only
? modification index, i.e. the number of
premodifiers minus the number of
postmodifiers across all categories
? modification index for NPs only
? modification weight index: length in
tokens of all premodifiers minus length
in tokens of all postmodifiers across all
categories
? modification weight index for NPs only
? coordination balance, i.e. the maximal
length difference in coordinated
constituents
We considered the density of function words,
i.e. the ratio of function words to content words,
because of observed problems in WinMT
output. Pronouns received special attention
because of frequent problems detected in failure
analysis. The density features are:
? overall function word density
? density of determiners/quantifiers
? density of pronouns
? density of prepositions
? density of punctuation marks,
specifically commas and semicolons
? density of auxiliary verbs
? density of conjunctions
? density of different pronoun types: Wh,
1st, 2nd, and 3rd person pronouns
We also measured the following constituent
sizes:
? maximal and average NP length
? maximal and average AJP length
? maximal and average PP length
? maximal and average AVP length
? sentence length
On a lexical level, the presence of out of
vocabulary (OOV) words is frequently caused
by the direct transfer of source language words
for which no translation could be found. The
top-level syntactic template, i.e. the labels of the
immediate children of the root node of a
sentence, was also used, as was subject-verb
disagreement. The final five features are:
? number of OOV words
? the presence of a word containing a non-
English letter, i.e. an extended ASCII
character. This is a special case of the
OOV problem.
? label of the root node of the sentence
(declarative, imperative, question, NP,
or "FITTED" for non-spanning parses)
? sentence template, i.e. the labels of the
immediate children of the root node.
? subject-verb disagreement
4 Decision Trees
We used a set of automated tools to construct
decision trees (Chickering et al, 1997) based on
the features extracted from the reference and
MT sentences. To avoid overfitting, we
specified that nodes in the decision tree should
not be split if they accounted for fewer than fifty
cases. In the discussion below we distinguish the
perplexity features from the linguistic features.
4.1 Decision trees built using all
training data
Table 1 gives the accuracy of the decision trees,
when trained on all 180,000 training sentences
and evaluated against the 20,000 held-out test
sentences. Since the training data and test data
contain an even split between reference human
translations and machine translations, the
baseline for comparison is 50.00%. As Table 1
shows, the decision trees dramatically
outperform this baseline. Using only perplexity
features or only linguistic features yields
accuracy substantially above this baseline.
Combining the two sets of features yields the
highest accuracy, 82.89%.
Features used Accuracy (%)
All features 82.89
Perplexity features only 74.73
Linguistic features only 76.51
Table 1 Accuracy of the decision trees
Notably, most of the annotated features
were selected by the decision tree tools. Two
features were found not to be predictive. The
first non-selected feature is the presence of a
word containing an extended ASCII character,
suggesting that general OOV features were
sufficient and subsume the effect of this
narrower feature. Secondly, subject-verb
disagreement was also not predictive, validating
the consistent enforcement of agreement
constraints in the natural language generation
component of the MT system. In addition, only
eight of approximately 5,200 observed sentence
templates turned out to be discriminatory.
For a different use of perplexity in
classification, see Ringger et al (2001) who
compare the perplexity of a sentence using a
language model built solely from reference
translations to the perplexity using a language
model built solely from machine translations.
The output of such a classifier could be used as
an input feature in building decision trees.
Effect of training data size
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
0
10
,00
0
20
,00
0
30
,00
0
40
,00
0
50
,00
0
60
,00
0
70
,00
0
80
,00
0
90
,00
0
10
0,0
00
11
0,0
00
12
0,0
00
13
0,0
00
14
0,0
00
15
0,0
00
16
0,0
00
17
0,0
00
18
0,0
00
Training cases
A
v
g
be
st
ac
cu
ra
cy
All features Perplexity only Linguistic only
Figure 1 Accuracy with varying amounts of training data
4.2 Varying the amount of training data
For our experiments, we had access to several
hundred thousand sentences from the target
domain. To measure the effect of reducing the
size of the training data set on the accuracy of
the classifier, we built classifiers using samples
of the training data and evaluating against the
same held-out sample of 20,000 sentences. We
randomly extracted ten samples containing the
following numbers of sentences: {1,000, 2,000,
3,000, 4,000, 5,000, 6,000, 12,000, 25,000,
50,000, 100,000, 150,000}. Figure 1 shows the
effect of varying the size of the training data.
The data point graphed is the average accuracy
over the ten samples at a given sample size, with
error bars showing the range from the least
accurate decision tree at that sample size to the
most accurate.
As Figure 1 shows, the models built using
only perplexity features do not benefit from
additional training data. The models built using
linguistic features, however, benefit
substantially, with accuracy leveling off after
150,000 training cases. With only 2,000 training
cases, the classifiers built using all features
range in accuracy from 75.06% to 78.84%,
substantially above the baseline accuracy of
50%.
5 Discussion
As the results in section 4 show, it is possible to
build classifiers that can distinguish human
reference translations from the output of a
machine translation system with high accuracy.
We thus have an automatic mechanism that can
perform the task that humans appear to do with
ease, as noted in section 1. The best result, a
classifier with 82.89% accuracy, is achieved by
combining perplexity calculations with a set of
finer-grained linguistic features. Even with as
few as 2,000 training cases, accuracy exceeded
75%. In the discussion below we consider the
advantages and possible uses of this automatic
evaluation methodology.
5.1 Advantages of the approach
Once an appropriate set of features has been
selected and tools to automatically extract those
features are in place, classifiers can be built and
evaluated quickly. This overcomes the two
problems associated with traditional manual
evaluation of MT systems: manual evaluation is
both costly and time-consuming. Indeed, an
automated approach is essential when dealing
with an MT system that is under constant
development in a collaborative research
environment. The output of such a system may
change from day to day, requiring frequent
feedback to monitor progress.
The methodology does not crucially rely on
any particular set of features. As an MT system
matures, more and more subtle cues might be
necessary to distinguish between human and
machine translations. Any linguistic feature that
can be reliably extracted can be proposed as a
candidate feature to the decision tree tools.
The methodology is also not sensitive to the
domain of the training texts. All that is needed
to build classifiers for a new domain is a
sufficient quantity of aligned translations.
5.2 Possible applications of the
approach
The classifiers can be used for evaluating a
system overall, providing feedback to aid in
system development, and in evaluating
individual sentences.
Evaluating an MT system overall
Evaluating the accuracy of the classifier against
held-out data is equivalent to evaluating the
fluency of the MT system. As the MT system
improves, its output will become more like the
human reference translations. To measure
improvement over time, we would hold the set
of features constant and build and evaluate new
classifiers using the human reference
translations and the output of the MT system at a
given point in time. Using the same set of
features, we expect the accuracy of the
classifiers to go down over time as the MT
output becomes more like human translations.
Feedback to aid system development
Our primary interest in evaluating an MT system
is to identify areas that require improvement.
This has been the motivation for using linguistic
features in addition to perplexity measures.
From the point of view of system development,
perplexity is a rather opaque measure. This can
be viewed as both a strength and a weakness. On
the one hand, it is difficult to tune a system with
the express goal of causing perplexity to
improve, rendering perplexity a particularly
good objective measurement. On the other hand,
given a poor perplexity score, it is not clear how
to improve a system without additional failure
analysis.
We used the DNETVIEWER tool (Heckerman
et al, 2000), a visualization tool for viewing
decision trees and Bayesian networks, to explore
the decision trees and identify problem areas in
our MT system. In one visualization, shown in
Figure 2, DNETVIEWER allows the user to adjust
a slider to see the order in which the features
were selected during the heuristic search that
guides the construction of decision trees. The
most discriminatory features are those which
cause the MT translations to look most awful, or
are characteristics of the reference translations
that ought to be emulated by the MT system. For
the coarse model shown in Figure 2, the distance
between pronouns (nPronDist) is the highest
predictor, followed by the number of second
person pronouns (n2ndPersPron), the number of
function words (nFunctionWords), and the
distance between prepositions (nPrepDist).
Using DNETVIEWER we are able to explore
the decision tree, as shown in Figure 3. Viewing
the leaf nodes in the decision tree, we see a
probability distribution over the possible states
of the target variable. In the case of the binary
classifier here, this is the probability that a
sentence will be a reference translation. In
Figure 3, the topmost leaf node shows that
p(Human translation) is low. We modified
DNETVIEWER so that double-clicking on the leaf
node would display reference translations and
MT sentences from the training data. We display
a window showing the path through the decision
tree, the probability that the sentence is a
reference translation given that path, and the
sentences from the training data identified by the
features on the path. This visualization allows
the researcher to view manageable groups of
similar problem sentences with a view to
identifying classes of problems within the
groups. A goal for future research is to select
additional linguistic features that will allow us to
pinpoint problem areas in the MT system and
thereby further automate failure analysis.
Figure 2 Using the slider to view the best predictors
Figure 3 Examining sentences at a leaf node in the decision tree
Figure 4 Examining sentences at a leaf node in the decision tree
Decision trees are merely one form of
classifier that could be used for the automated
evaluation of an MT system. In preliminary
experiments, the accuracy of classifiers using
support vector machines (SVMs) (Vapnik, 1998;
Platt et al, 2000) exceeded the accuracy of the
decision tree classifiers by a little less than one
percentage point using a linear kernel function,
and by a slightly greater margin using a
polynomial kernel function of degree three. We
prefer the decision tree classifiers because they
allow a researcher to explore the classification
system and focus on problem areas and
sentences. We find this method for exploring the
data more intuitive than attempting to visualize
the location of sentences in the high-
dimensional space of the corresponding SVM.
Evaluating individual sentences
In addition to system evaluation and failure
analysis, classifiers could be used on a per-
sentence basis to guide the output of an MT
system by selecting among multiple candidate
strings. If no candidate is judged sufficiently
similar to a human reference translation, the
sentence could be flagged for human post-
editing.
6 Conclusion
We have presented a method for evaluating the
fluency of MT, using classifiers based on
linguistic features to emulate the human ability
to distinguish MT from human translation. The
techniques we have described are system- and
language-independent. Possible applications of
our approach include system evaluation, failure
analysis to guide system development, and
selection among alternative possible outputs.
We have focused on structural aspects of a
text that can be used to evaluate fluency. A full
evaluation of MT quality would of course need
to include measurements of idiomaticity and
techniques to verify that the semantic and
pragmatic content of the source language had
been successfully transferred to the target
language.
Acknowledgements
Our thanks go to Eric Ringger and Max
Chickering for programming assistance with the
tools used in building and evaluating the
decision trees, and to Mike Carlson for help in
sampling the initial datasets. Thanks also to
John Platt for helpful discussion on parameter
setting for the SVM tools, and to the members
of the MSR NLP group for feedback on the uses
of the methodology presented here.
References
Alshawi, H., S. Bangalore, and S. Douglas. 1998.
Automatic acquisition of hierarchical transduction
models for machine translation. In Proceedings of
the 36th Annual Meeting of the Association for
Computational Linguistics, Montreal Canada, Vol.
I: 41-47.
Bangalore, S., O. Rambow, and S. Whittaker. 2000.
Evaluation Metrics for Generation. In Proceedings
of the International Conference on Natural
Language Generation (INLG 2000), Mitzpe
Ramon, Israel. 1-13.
Chickering, D. M., D. Heckerman, and C. Meek.
1997. A Bayesian approach to learning Bayesian
networks with local structure. In Geiger, D. and P.
Punadlik Shenoy (Eds.), Uncertainty in Artificial
Intelligence: Proceedings of the Thirteenth
Conference. 80-89.
Clarkson, P. and R. Rosenfeld. 1997. Statistical
Language Modeling Using the CMU-Cambridge
Toolkit. Proceedings of Eurospeech97. 2707-
2710.
Heckerman, D., D. M. Chickering, C. Meek, R.
Rounthwaite, and C. Kadie. 2000. Dependency
networks for inference, collaborative filtering and
data visualization. Journal of Machine Learning
Research 1:49-75.
Heidorn, G. E., 2000. Intelligent writing assistance.
In R. Dale, H. Moisl and H. Somers (Eds.).
Handbook of Natural Language Processing. New
York, NY. Marcel Dekker. 181-207.
Langkilde, I., and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, and
17th International Conference on Computational
Linguistics, Montreal, Canada. 704-710.
Nyberg, E. H., T. Mitamura, and J. G. Carbonnell.
1994. Evaluation Metrics for Knowledge-Based
Machine Translation. In Proceedings of the 15th
International Conference on Computational
Linguistics, Kyoto, Japan (Coling 94). 95-99.
Platt, J., N. Cristianini, J. Shawe-Taylor. 2000. Large
margin DAGs for multiclass classification. In
Advances in Neural Information Processing
Systems 12, MIT Press. 547-553.
Richardson, S., B. Dolan, A. Menezes, and J.
Pinkham. 2001. Achieving commercial-quality
translation with example-based methods.
Submitted for review.
Ringger, E., M. Corston-Oliver, and R. Moore. 2001.
Using Word-Perplexity for Automatic Evaluation
of Machine Translation. Manuscript.
Su, K., M. Wu, and J. Chang. 1992. A new
quantitative quality measure for machine
translation systems. In Proceedings of COLING-
92, Nantes, France. 433-439.
Vapnik, V. 1998. Statistical Learning Theory, Wiley-
Interscience, New York.
Machine-learned contexts for linguistic operations 
in German sentence realization 
 
Michael GAMON, Eric RINGGER, Simon CORSTON-OLIVER, Robert MOORE 
Microsoft Research  
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, simonco, bobmoore}@microsoft.com 
 
Abstract 
We show that it is possible to learn the 
contexts for linguistic operations which 
map a semantic representation to a 
surface syntactic tree in sentence 
realization with high accuracy. We cast 
the problem of learning the contexts for 
the linguistic operations as 
classification tasks, and apply 
straightforward machine learning 
techniques, such as decision tree 
learning. The training data consist of 
linguistic features extracted from 
syntactic and semantic representations 
produced by a linguistic analysis 
system. The target features are extracted 
from links to surface syntax trees. Our 
evidence consists of four examples from 
the German sentence realization system 
code-named Amalgam: case 
assignment, assignment of verb position 
features, extraposition, and syntactic 
aggregation 
1 Introduction 
The last stage of natural language generation, 
sentence realization, creates the surface string 
from an abstract (typically semantic) 
representation. This mapping from abstract 
representation to surface string can be direct, or it 
can employ intermediate syntactic representations 
which significantly constrain the output. 
Furthermore, the mapping can be performed 
purely by rules, by application of statistical 
models, or by a combination of both techniques. 
Among the systems that use statistical or 
machine learned techniques in sentence 
realization, there are various degrees of 
intermediate syntactic structure. Nitrogen 
(Langkilde and Knight, 1998a, 1998b) produces a 
large set of alternative surface realizations of an 
input structure (which can vary in abstractness). 
This set of candidate surface strings, represented 
as a word lattice, is then rescored by a word-
bigram language model, to produce the best-
ranked output sentence. FERGUS (Bangalore and 
Rambow, 2000), on the other hand, employs a 
model of syntactic structure during sentence 
realization. In simple terms, it adds a tree-based 
stochastic model to the approach taken by the 
Nitrogen system. This tree-based model chooses a 
best-ranked XTAG representation for a given 
dependency structure. Possible linearizations of 
the XTAG representation are generated and then 
evaluated by a language model to pick the best 
possible linearization, as in Nitrogen. 
In contrast, the sentence realization system 
code-named Amalgam (A Machine Learned 
Generation Module) (Corston-Oliver et al, 2002; 
Gamon et al, 2002b) employs a series of 
linguistic operations which map a semantic 
representation to a surface syntactic tree via 
intermediate syntactic representations. The 
contexts for most of these operations in Amalgam 
are machine learned. The resulting syntactic tree 
contains all the necessary information on its leaf 
nodes from which a surface string can be read.  
The goal of this paper is to show that it is 
possible to learn accurately the contexts for 
linguistically complex operations in sentence 
realization. We propose that learning the contexts 
for the application of these linguistic operations 
can be viewed as per-operation classification 
problems. This approach combines advantages of 
a linguistically informed approach to sentence 
realization with the advantages of a machine 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 25-32.
                         Proceedings of the 40th Annual Meeting of the Association for
learning approach. The linguistically informed 
approach allows us to deal with complex linguistic 
phenomena, while machine learning automates the 
discovery of contexts that are linguistically 
relevant and relevant for the domain of the data. 
The machine learning approach also facilitates 
adaptation of the system to a new domain or 
language. Furthermore, the quantitative nature of 
the machine learned models permits finer 
distinctions and ranking among possible solutions. 
To substantiate our claim, we provide four 
examples from Amalgam: assignment of case, 
assignment of verb position features, 
extraposition, and syntactic aggregation. 
2 Overview of Amalgam 
Amalgam takes as its input a sentence-level 
semantic graph representation with fixed lexical 
choices for content words (the logical form graph 
of the NLPWin system ? see (Heidorn, 2000)). 
This representation is first degraphed into a tree, 
and then gradually augmented by the insertion of 
function words, assignment of case and verb 
position features, syntactic labels, etc., and 
transformed into a syntactic surface tree. A 
generative statistical language model establishes 
linear order in the surface tree (Ringger et al, in 
preparation), and a surface string is generated 
from the leaf nodes. Amalgam consists of eight 
stages. We label these ML (machine-learned 
context) or RB (rule-based). 
Stage 1 Pre-processing (RB): 
? degraphing of the semantic representation 
? retrieval of lexical information 
Stage 2 Flesh-out (ML): 
? assignment of syntactic labels 
? insertion of function words 
? assignment of case and verb position 
features 
Stage 3 Conversion to syntactic tree (RB): 
? introduction of syntactic representation 
for coordination 
? splitting of separable prefix verbs based 
on both lexical information and 
previously assigned verb position features 
? reversal of heads (e.g., in quantitative 
expressions) (ML) 
Stage 4 Movement: 
? extraposition (ML) 
? raising, wh movement (RB) 
Stage 5 Ordering (ML): 
? ordering of constituents and leaf nodes in 
the tree 
Stage 6 Surface cleanup (ML): 
? lexical choice of determiners and relative 
pronouns 
? syntactic aggregation 
Stage 7 Punctuation (ML) 
Stage 8 Inflectional generation (RB) 
All machine learned components, with the 
exception of the generative language model for 
ordering of constituents (stage 5), are decision tree 
classifiers built with the WinMine toolkit 
(Chickering et al, 1997; Chickering, nd.). There 
are a total of eighteen decision tree classifiers in 
the system. The complexity of the decision trees 
varies with the complexity of the modeled task. 
The number of branching nodes in the decision 
tree models in Amalgam ranges from 3 to 447. 
3 Data and feature extraction 
The data for all of the models were drawn from a 
set of 100,000 sentences from technical software 
manuals and help files. The sentences are 
analyzed by the NLPWin system, which provides 
a syntactic and logical form analysis. Nodes in the 
logical form representation are linked to the 
corresponding syntactic nodes, allowing us to 
learn contexts for the mapping from the semantic 
representation to a surface syntax tree. The data is 
split 70/30 for training versus model parameter 
tuning. For each set of data we built decision trees 
at several different levels of granularity (by 
manipulating the prior probability of tree 
structures to favor simpler structures) and selected 
the model with the maximal accuracy as 
determined on the parameter tuning set. All 
models are then tested on data extracted from a 
separate blind set of 10,000 sentences from the 
same domain. For both training and test, we only 
extract features from sentences that have received 
a complete, spanning parse: 85.14% of the 
sentences in the training and parameter tuning set, 
and 84.59% in the blind test set fall into that 
category. Most sentences yield more than one 
training case. 
We attempt to standardize as much as possible 
the set of features to be extracted. We exploit the 
full set of features and attributes available in the 
analysis, instead of pre-determining a small set of 
potentially relevant features (Gamon et al, 
2002b). This allows us to share the majority of 
code between the individual feature extraction 
tasks. More importantly, it enables us to discover 
new linguistically interesting and/or domain-
specific generalizations from the data. Typically, 
we extract the full set of available analysis 
features of the node under investigation, its parent 
and its grandparent, with the only restriction being 
that these features need to be available at the stage 
where the model is consulted at generation run-
time. This provides us with a sufficiently large 
structural context for the operations. In addition, 
for some of the models we add a small set of 
features that we believe to be important for the 
task at hand, and that cannot easily be expressed 
as a combination of analysis features/attributes on 
constituents. Most features, such as lexical 
subcategorization features and semantic features 
such as [Definite] are binary. Other features, such 
as syntactic label or semantic relation, have as 
many as 25 values. Training time on a standard 
500MHz PC ranges from one hour to six hours. 
4 Assignment of case 
In German sentence realization, proper 
assignment of morphological case is essential for 
fluent and comprehensible output. German is a 
language with fairly free constituent order, and the 
identification of functional roles, such as subject 
versus object, is not determined by position in the 
sentence, as in English, but by morphological 
marking of one of the four cases: nominative, 
accusative, genitive or dative. In Amalgam, case 
assignment is one of the last steps in the Flesh-out 
stage (stage 2). Morphological realization of case 
can be ambiguous in German (for example, a 
feminine singular NP is ambiguous between 
accusative and nominative case). Since the 
morphological realization of case depends on the 
gender, number and morphological paradigm of a 
given NP, we chose to only consider NP nodes 
with unambiguous case as training data for the 
model1. As the target feature for this model is 
                                                     
1
 Ideally, we should train the case assignment model on 
a corpus that is hand-disambiguated for case. In the 
absence of such a corpus, though, we believe that our 
approach is linguistically justified. The case of an NP 
depends solely on the syntactic context it appears in. 
morphological case, it has four possible values for 
the four cases in German. 
4.1 Features in the case assignment 
model 
For each data point, a total of 712 features was 
extracted. Of the 712 features available to the 
decision tree building tools, 72 were selected as 
having predictive value in the model. The selected 
features fall into the following categories: 
? syntactic label of the node, its parent and 
grandparent 
? lemma (i.e., citation form) of the parent, 
and lemma of the governing preposition 
? subcategorization information, including 
case governing properties of governing 
preposition and parent 
? semantic relation of the node itself to its 
parent, of the parent to its grandparent, 
and of the grandparent to its great-
grandparent 
? number information on the parent and 
grandparent 
? tense and mood on the parent and 
grandparent 
? definiteness on the node, its parent and 
grandparent 
? the presence of various semantic 
dependents such as subject, direct and 
indirect objects, operators, attributive 
adjuncts and unspecified modifiers on the 
node and its parent and grandparent 
? quantification, negation, coordination on 
the node, the parent and grandparent 
? part of speech of the node, the parent and 
the grandparent 
? miscellaneous semantic features on the 
node itself and the parent 
4.2 The case assignment model 
The decision tree model for case assignment 
has 226 branching nodes, making it one of the 
most complex models in Amalgam. For each 
nominal node in the 10,000 sentence test set, we 
compared the prediction of the model to the 
                                                                                  
Since we want to learn the syntactically determining 
factors for case, using unambiguously case marked NPs 
for training seems justified. 
morphological case compatible with that node. 
The previously mentioned example of a singular 
feminine NP, for example, would yield a ?correct? 
if the model had predicted nominative or 
accusative case (because the NP is 
morphologically ambiguous between accusative 
and nominative), and it would yield an ?incorrect? 
if the model had predicted genitive or dative. This 
particular evaluation setup was a necessary 
compromise because of the absence of a hand-
annotated corpus with disambiguated case in our 
domain. The caveat here is that downstream 
models in the Amalgam pipeline that pick up on 
case as one of their features rely on the absolute 
accuracy of the assigned case, not the relative 
accuracy with respect to morphological 
ambiguity. Accuracy numbers for each of the four 
case assignments are given in Table 1. Note that it 
is impossible to give precision/recall numbers, 
without a hand-disambiguated test set. The 
baseline for this task is 0.7049 (accuracy if the 
most frequent case (nominative) had been 
assigned to all NPs). 
Table 1. Accuracy of the case assignment model. 
Value Accuracy 
Dat 0.8705 
Acc 0.9707 
Gen 0.9457 
Nom 0.9654 
overall 0.9352 
5 Assignment of verb position 
features 
One of the most striking properties of German is 
the distributional pattern of verbs in main and 
subordinate clauses. Most descriptive accounts of 
German syntax are based on a topology of the 
German sentence that treats the position of the 
verb as the fixed frame around which other 
syntactic constituents are organized in relatively 
free order (cf. Eisenberg, 1999; Engel, 1996). The 
position of the verb in German is non-negotiable; 
errors in the positioning of the verb result in 
gibberish, whereas most permutations of other 
constituents only result in less fluent output. 
Depending on the position of the finite verb, 
German sentences and verb phrases are classified 
as being ?verb-initial?, ?verb-second? or ?verb-
final?. In verb-initial clauses (e.g., in imperatives), 
the finite verb is in initial position. Verb-second 
sentences contain one constituent preceding the 
finite verb, in the so-called ?pre-field?. The finite 
verb is followed by any number of constituents in 
the ?middle-field?, and any non-finite verbs are 
positioned at the right periphery of the clause, 
possibly followed by extraposed material or 
complement clauses (the ?post-field?). Verb-final 
clauses contain no verbal element in the verb-
second position: all verbs are clustered at the right 
periphery, preceded by any number of constituents 
and followed only by complement clauses and 
extraposed material. 
During the Flesh-out stage in Amalgam, a 
decision tree classifier is consulted to make a 
classification decision among the four verb 
positions: ?verb-initial?, ?verb-second?, ?verb-
final?, and ?undefined?. The value ?undefined? 
for the target feature of verb position is extracted 
for those verbal constituents where the local 
syntactic context is too limited to make a clear 
distinction between initial, second, or final 
position of the verb. The number of ?undefined? 
verb positions is small compared to the number of 
clearly established verb positions: in the test set, 
there were only 690 observed cases of 
?undefined? verb position out of a total of 15,492 
data points. At runtime in Amalgam, verb position 
features are assigned based on the classification 
provided by the decision tree model. 
5.1 Features in the verb position model 
For each data point, 713 features were extracted. 
Of those features, 41 were selected by the decision 
tree algorithm. The selected features fall into the 
following categories: 
? syntactic label of the node and the parent 
? subcategorization features 
? semantic relations of the node to its parent 
and of the parent node to its parent 
? tense and mood features 
? presence of empty, uncontrolled subject 
? semantic features on the node and the 
parent 
5.2 The verb position model 
The decision tree model for verb position has 
115 branching nodes. Precision, recall and F-
measure for the model are given in Table 2. As a 
point of reference for the verb position classifier, 
assigning the most frequent value (second) of the 
target feature yields a baseline score of 0.4240. 
Table 2. Precision, recall, and F-measure for the verb 
position model. 
Value Precision Recall F-measure 
Initial 0.9650 0.9809 0.9729 
Second 0.9754 0.9740 0.9743 
Final 0.9420 0.9749 0.9581 
Undefined 0.5868 0.3869 0.4663 
Overall 
accuracy 
0.9491 
6 Extraposition 
In both German and English it is possible to 
extrapose clausal material to the right periphery of 
the sentence (extraposed clauses underlined in the 
examples below): 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave the 
country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumour has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Extraposition is not obligatory like other types 
of movement (such as Wh-movement). Both 
extraposed and non-extraposed versions of a 
sentence are acceptable, with varying degrees of 
fluency. 
The interesting difference between English and 
German is the frequency of this phenomenon. 
While it can easily be argued that English 
sentence realization may ignore extraposition and 
still result in very fluent output, the fluency of 
sentence realization for German will suffer much 
more from the lack of a good extraposition 
mechanism. We profiled data from various 
domains (Gamon et al 2002a) to substantiate this 
linguistic claim (see Uszkoreit et al 1998 for 
similar results). In the technical domain, more 
than one third of German relative clauses are 
extraposed, as compared to a meagre 0.22% of 
English relative clauses. In encyclopaedia text 
(Microsoft Encarta), approximately every fifth 
German relative clause is extraposed, compared to 
only 0.3% of English relative clauses. For 
complement clauses and infinitival clauses, the 
differences are not as striking, but still significant: 
in the technical and encyclopaedia domains, 
extraposition of infinitival and complement 
clauses in German ranges from 1.5% to 3.2%, 
whereas English only shows a range from 0% to 
0.53%. 
We chose to model extraposition as an iterative 
movement process from the original attachment 
site to the next higher node in the tree (for an 
alternative one-step solution and a comparison of 
the two approaches see (Gamon et al, 2002a)). 
The target feature of the model is the answer to 
the yes/no question ?Should the clause move from 
node X to the parent of node X??. 
6.1 Features in the extraposition model 
The tendency of a clause to be extraposed depends 
on properties of both the clause itself (e.g., some 
notion of ?heaviness?) and the current attachment 
site. Very coarse linguistic generalizations are that 
a relative clause tends to be extraposed if it is 
sufficiently ?heavy? and if it is followed by verbal 
material in the same clause. Feature extraction for 
this model reflects that fact by taking into 
consideration features on the extraposition 
candidate, the current attachment site, and 
potential next higher landing site. This results in a 
total of 1168 features. Each node in the parent 
chain of an extraposable clause, up to the actual 
attachment node, constitutes a single data point 
During the decision tree building process, 60 
features were selected as predictive. They can be 
classified as follows: 
General feature: 
? overall sentence length 
Features on the extraposable clause: 
? presence of verb-final and verb-second 
ancestor nodes 
? ?heaviness? both in number of characters 
and number of tokens 
? various linguistic features in the local 
context (parent node and grandparent 
node): number and person, definiteness, 
voice, mood, transitivity, presence of 
logical subject and object, presence of 
certain semantic attributes, coordination, 
prepositional relations 
? syntactic label 
? presence of modal verbs 
? prepositional relations 
? transitivity 
Features on the attachment site 
? presence of logical subject 
? status of the parent and grandparent as a 
separable prefix verb 
? voice and presence of modal verbs on the 
parent and grandparent 
? presence of arguments and transitivity 
features on the parent and grandparent 
? number, person and definiteness; the same 
on parent and grandparent 
? syntactic label; the same on the parent and 
grandparent 
? verb position; the same on the parent 
? prepositional relation on parent and 
grandparent 
? semantic relation that parent and 
grandparent have to their respective 
parent node 
6.2 The extraposition model 
During testing of the extraposition model, the 
model was consulted for each extraposable clause 
to find the highest node to which that clause could 
be extraposed. In other words, the target node for 
extraposition is the highest node in the parent 
chain for which the answer to the classification 
task ?Should the clause move from node X to the 
parent of node X?? is ?yes? with no interceding 
?no? answer. The prediction of the model was 
compared with the actual observed attachment site 
of the extraposable clause to yield the accuracy 
figures shown in Table 3. The model has 116 
branching nodes. The baseline for this task is 
calculated by applying the most frequent value for 
the target feature (?don't move?) to all nodes. The 
baseline for extraposition of infinitival and 
complement clauses is very high. The number of 
extraposed clauses of both types in the test set 
(fifteen extraposed infinitival clauses and twelve 
extraposed complement clauses) is very small, so 
it comes as no surprise that the model accuracy 
ranges around the baseline for these two types of 
extraposed clauses. 
Table 3. Accuracy of the extraposition model. 
Extraposable clause Accuracy Baseline 
RELCL 0.8387 0.6093 
INFCL 0.9202 0.9370 
COMPCL 0.9857 0.9429 
Overall 0.8612 0.6758 
7 Syntactic aggregation 
Any sentence realization component that 
generates from an abstract semantic representation 
and strives to produce fluent output beyond simple 
templates will have to deal with coordination and 
the problem of duplicated material in 
coordination. This is generally viewed as a sub-
area of aggregation in the generation literature 
(Wilkinson, 1995; Shaw, 1998; Reape and 
Mellish, 1999; Dalianis and Hovy, 1993). In 
Amalgam, the approach we take is strictly intra-
sentential, along the lines of what has been called 
conjunction reduction in the linguistic literature 
(McCawley, 1988). While this may seem a fairly 
straightforward task compared to inter-sentential, 
semantic and lexical aggregation, it should be 
noted that the cross-linguistic complexity of the 
phenomenon makes it much less trivial than a first 
glance at English would suggest. In German, for 
example, position of the verb in the coordinated 
VPs plays an important role in determining which 
duplicated constituent can be omitted. 
The target feature for the classification task is 
formulated as follows: ?In which coordinated 
constituent is the duplicated constituent to be 
realized??. There are three values for the target 
feature: ?first?, ?last?, and ?middle?. The third 
value (?middle?) is a default value for cases where 
neither the first, nor the last coordinated 
constituent can be identified as the location for the 
realization of duplicated constituents. At 
generation runtime, multiple realizations of a 
constituent in coordination are collected and the 
aggregation model is consulted to decide on the 
optimal position in which to realize that 
constituent. The constituent in that position is 
retained, while all other duplicates are removed 
from the tree. 
7.1 Features in the syntactic aggregation 
model 
A total of 714 features were extracted for the 
syntactic aggregation model. Each instance of 
coordination which exhibits duplicated material at 
the semantic level without corresponding 
duplication at the syntactic level constitutes a data 
point. 
Of these features, 15 were selected as 
predictive in the process of building the decision 
tree model: 
? syntactic label and syntactic label of the 
parent node 
? semantic relation to the parent of the 
duplicated node, its parent and grandparent 
? part of speech of the duplicated node 
? verb position across the coordinated node 
? position of the duplicated node in 
premodifiers or postmodifiers of the parent 
? coordination of the duplicated node and 
the grandparent of the duplicated node 
? status of parent and grandparent as a 
proposition 
? number feature on the parent 
? transitivity and presence of a direct object 
on the parent 
7.2 The syntactic aggregation model 
The syntactic aggregation model has 21 branching 
nodes. Precision, recall and F-measure for the 
model are given in Table 4. As was to be expected 
on the basis of linguistic intuition, the value 
?middle? for the target feature did not play any 
role. In the test set there were only 2 observed 
instances of that value. The baseline for this task 
is 0.8566 (assuming ?first? as the default value). 
Table 4. Precision, recall, and F-measure for the 
syntactic aggregation model. 
Value Precision Recall F-measure 
last 0.9191 0.9082 0.9136 
first 0.9837 0.9867 0.9851 
middle 0.0000 0.0000 0.0000 
overall 
accuracy 
0.9746 
8 Conclusion and future research 
We have demonstrated on the basis of four 
examples that it is possible to learn the contexts 
for complex linguistic operations in sentence 
realization with high accuracy. We proposed to 
standardize most of the feature extraction for the 
machine learning tasks to all available linguistic 
features on the node, and its parent and 
grandparent node. This generalized set of features 
allows us to rapidly train on new sets of data and 
to experiment with new machine learning tasks. 
Furthermore, it prevents us from focusing on a 
small set of hand-selected features for a given 
phenomenon; hence, it allows us to learn new (and 
unexpected) generalizations from new data. 
We have found decision trees to be useful for 
our classification problems, but other classifiers 
are certainly applicable. Decision trees provided 
an easily accessible inventory of the selected 
features and some indication of their relative 
importance in predicting the target features in 
question. Although our exposition has focused on 
the preferred value (the mode) predicted by the 
models, decision trees built by WinMine predict a 
probability distribution over all possible target 
values. For a system such as Amalgam, built as a 
pipeline of stages, this point is critical, since 
finding the best final hypothesis requires the 
consideration of multiple hypotheses and the 
concomitant combination of probabilities assigned 
by the various models in the pipeline to all 
possible target values. For example, our 
extraposition model presented above depends 
upon the value of the verb-position feature, which 
is predicted upstream in the pipeline. Currently, 
we greedily pursue the best hypothesis, which 
includes only the mode of the verb-position 
model?s prediction. However, work in progress 
involves a search that constructs multiple 
hypotheses incorporating each of the predictions 
of the verb-position model and their scores, and 
likewise for all other models. 
We have found the combination of knowledge-
engineered linguistic operations with machine-
learned contexts to be advantageous. The 
knowledge-engineered choice of linguistic 
operations, allows us to deal with complex 
linguistic phenomena. Machine learning, on the 
other hand, automates the discovery of general 
and domain-specific contexts. This facilitates 
adaptation of the system to a new domain or even 
to a new language. 
It should also be noted that none of the learned 
models can be easily replaced by a rule. While 
case assignment, for example, depends to a high 
degree on the lexical properties of the governing 
preposition or governing verb, other factors such 
as semantic relations, etc., play a significant role, 
so that any rule approaching the accuracy of the 
model would have to be quite complex.  
We are currently adapting Amalgam to the task 
of French sentence realization, as a test of the 
linguistic generality of the system. Initial results 
are encouraging. It appears that much of the 
feature extraction and many of the linguistic 
operations are reusable. 
Acknowledgements 
Our thanks go to Max Chickering for assistance 
with the WinMine decision tree tools and to Zhu 
Zhang who made significant contributions to the 
development of the extraposition models. 
References 
S. Bangalore and O. Rambow 2000. Exploiting a 
probabilistic hierarchical model for generation. 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000). 
Saarbr?cken, Germany. 42-48. 
D. M. Chickering. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/Tool
doc.htm 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian approach to learning Bayesian networks 
with local structure. In ?Uncertainty in Artificial 
Intelligence: Proceedings of the Thirteenth 
Conference?, D. Geiger and P. Punadlik Shenoy, 
ed., Morgan Kaufman, San Francisco, California, 
pp. 80-89. 
S. Corston-Oliver, M. Gamon, E. Ringger, and R. 
Moore. 2002. An overview of Amalgam: A machine-
learned generation module. To be presented at 
INLG 2002. 
H. Dalianis and E. Hovy 1993 Aggregation in natural 
language generation. Proceedings of the 4th 
European Workshop on Natural Language 
Generation, Pisa, Italy. 
P. Eisenberg 1999. Grundriss der deutschen 
Grammatik. Band2: Der Satz. Metzler, 
Stuttgart/Weimar. 
U. Engel. 1996. Deutsche Grammatik. Groos, 
Heidelberg. 
M. Gamon, E. Ringger, Z. Zhang, R. Moore and S. 
Corston-Oliver. 2002a. Extraposition: A case study 
in German sentence realization. To be presented at 
the 19th International Conference on Computational 
Linguistics (COLING) 2002. 
M. Gamon, E. Ringger, S. Corston-Oliver. 2002b. 
Amalgam: A machine-learned generation module. 
Microsoft Research Technical Report, to appear. 
G. E. Heidorn. 2002. Intelligent Writing Assistance. In 
?A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text?, R. Dale, H. Moisl, and H. 
Somers (ed.), Marce Dekker, New York. 
I. Langkilde. and K. Knight. 1998a. The practical value 
of n-grams in generation. Proceedings of the 9th 
International Workshop on Natural Language 
Generation, Niagara-on-the-Lake, Canada. pp. 248-
255. 
I. Langkilde and K. Knight. 1998b. Generation that 
exploits corpus-based statistical knowledge. 
Proceedings of the 36th ACL and 17th COLING 
(COLING-ACL 1998). Montr?al, Qu?bec, Canada. 
704-710. 
J. D. McCawley. 1988 The Syntactic Phenomena of 
English. The University of Chicago Press, Chicago 
and London. 
M. Reape. and C. Mellish. 1999. Just what is 
aggregation anyway? Proceedings of the 7th 
European Workshop on Natural Language 
Generation, Toulouse, France. 
E. Ringger, R. Moore, M. Gamon, and S. Corston-
Oliver. In preparation. A Linguistically Informed 
Generative Language Model for Intra-Constituent 
Ordering during Sentence Realization. 
J. Shaw. 1998 Segregatory Coordination and Ellipsis in 
Text Generation. Proceedings of COLING-ACL, 
1998, pp 1220-1226. 
H. Uszkoreit, T. Brants, D. Duchier, B. Krenn, L. 
Konieczny, S. Oepen and W. Skut. 1998. Aspekte 
der Relativsatzextraposition im Deutschen. Claus-
Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
J. Wilkinson 1995 Aggregation in Natural Language 
Generation: Another Look. Co-op work term report, 
Department of Computer Science, University of 
Waterloo. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 249?256,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Correcting ESL Errors Using Phrasal SMT Techniques 
 
 Chris Brockett, William B. Dolan, and Michael Gamon 
Natural Language Processing Group 
Microsoft Research  
One Microsoft Way, Redmond, WA 98005, USA  
{chrisbkt,billdol,mgamon}@microsoft.com  
 
  
 
Abstract 
This paper presents a pilot study of the 
use of phrasal Statistical Machine Trans-
lation (SMT) techniques to identify and 
correct writing errors made by learners of 
English as a Second Language (ESL). 
Using examples of mass noun errors 
found in the Chinese Learner Error Cor-
pus (CLEC) to guide creation of an engi-
neered training set, we show that applica-
tion of the SMT paradigm can capture er-
rors not well addressed by widely-used 
proofing tools designed for native speak-
ers. Our system was able to correct 
61.81% of mistakes in a set of naturally-
occurring examples of mass noun errors 
found on the World Wide Web, suggest-
ing that efforts to collect alignable cor-
pora of pre- and post-editing ESL writing 
samples offer can enable the develop-
ment of SMT-based writing assistance 
tools capable of repairing many of the 
complex syntactic and lexical problems 
found in the writing of ESL learners. 
1 Introduction 
Every day, in schools, universities and busi-
nesses around the world, in email and on blogs 
and websites, people create texts in languages 
that are not their own, most notably English. Yet, 
for writers of English as a Second Language 
(ESL), useful editorial assistance geared to their 
needs is surprisingly hard to come by. Grammar 
checkers such as that provided in Microsoft 
Word have been designed primarily with native 
speakers in mind. Moreover, despite growing 
demand for ESL proofing tools, there has been 
remarkably little progress in this area over the 
last decade. Research into computer feedback for 
ESL writers remains largely focused on small-
scale pedagogical systems implemented within 
the framework of CALL (Computer Aided Lan-
guage Learning) (Reuer 2003; Vanderventer 
Faltin, 2003), while commercial ESL grammar 
checkers remain brittle and difficult to customize 
to meet the needs of ESL writers of different 
first-language (L1) backgrounds and skill levels.  
  Some researchers have begun to apply statis-
tical techniques to identify learner errors in the 
context of essay evaluation (Chodorow & Lea-
cock, 2000; Lonsdale & Strong-Krause, 2003), to 
detect non-native text (Tomokiyo & Jones, 2001), 
and to support lexical selection by ESL learners 
through first-language translation (Liu et al, 
2000). However, none of this work appears to 
directly address the more general problem of 
how to robustly provide feedback to ESL writ-
ers?and for that matter non-native writers in 
any second language?in a way that is easily tai-
lored to different L1 backgrounds and second-
language (L2) skill levels.  
In this paper, we show that a noisy channel 
model instantiated within the paradigm of Statis-
tical Machine Translation (SMT) (Brown et al, 
1993) can successfully provide editorial assis-
tance for non-native writers. In particular, the 
SMT approach provides a natural mechanism for 
suggesting a correction, rather than simply 
stranding the user with a flag indicating that the 
text contains an error. Section 2 further motivates 
the approach and briefly describes our SMT sys-
tem. Section 3 discusses the data used in our ex-
periment, which is aimed at repairing a common 
type of ESL error that is not well-handled by cur-
rent grammar checking technology: mass/count 
noun confusions. Section 4 presents experimental 
results, along with an analysis of errors produced 
by the system. Finally we present discussion and 
some future directions for investigation.  
249
2 Error Correction as SMT 
2.1 Beyond Grammar Checking 
A major difficulty for ESL proofing is that errors 
of grammar, lexical choice, idiomaticity, and 
style rarely occur in isolation. Instead, any given 
sentence produced by an ESL learner may in-
volve a complex combination of all these error 
types. It is difficult enough to design a proofing 
tool that can reliably correct individual errors; 
the simultaneous combination of multiple errors 
is beyond the capabilities of current proofing 
tools designed for native speakers. Consider the 
following example, written by a Korean speaker 
and found on the World Wide Web, which in-
volves the misapplication of countability to a 
mass noun:  
 
And I knew many informations 
about Christmas while I was 
preparing this article. 
 
The grammar and spelling checkers in Microsoft 
Word 2003 correctly suggest many ? much 
and informations ? information. 
Accepting these proposed changes, however, 
does not render the sentence entirely native-like. 
Substituting the word much for many leaves 
the sentence stilted in a way that is probably un-
detectable to an inexperienced non-native 
speaker, while the use of the word knew repre-
sents a lexical selection error that falls well out-
side the scope of conventional proofing tools. A 
better rewrite might be: 
 
And I learned a lot of in-
formation about Christmas 
while I was preparing this 
article. 
 
or, even more colloquially: 
 
And I learned a lot about 
Christmas while I was pre-
paring this article 
 
Repairing the error in the original sentence, 
then, is not a simple matter of fixing an agree-
ment marker or substituting one determiner for 
another. Instead, wholesale replacement of the 
phrase knew many informations with 
the phrase learned a lot is needed to pro-
duce idiomatic-sounding output. Seen in these 
terms, the process of mapping from a raw, ESL-
authored string to its colloquial equivalent looks 
remarkably like translation. Our goal is to show 
that providing editorial assistance for writers 
should be viewed as a special case of translation. 
Rather than learning how strings in one language 
map to strings in another, however, ?translation? 
now involves learning how systematic patterns of 
errors in ESL learners? English map to corre-
sponding patterns in native English    
2.2 A Noisy Channel Model of ESL Errors 
If ESL error correction is seen as a translation 
task, the task can be treated as an SMT problem 
using the noisy channel model of (Brown et al, 
1993): here the L2 sentence produced by the 
learner can be regarded as having been corrupted 
by noise in the form of interference from his or 
her L1 model and incomplete language models 
internalized during language learning. The task, 
then, is to reconstruct a corresponding valid sen-
tence of L2 (target). Accordingly, we can seek to 
probabilistically identify the optimal correct tar-
get sentence(s) T* of an ESL input sentence S by 
applying the familiar SMT formula: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
In the context of this model, editorial assis-
tance becomes a matter of identifying those seg-
ments of the optimal target sentence or sentences 
that differ from the writer?s original input and 
displaying them to the user. In practice, the pat-
terns of errors produced by ESL writers of spe-
cific L1 backgrounds can be captured in the 
channel model as an emergent property of train-
ing data consisting ESL sentences aligned with 
their corrected edited counterparts. The highest 
frequency errors and infelicities should emerge 
as targets for replacement, while lesser frequency 
or idiosyncratic problems will in general not sur-
face as false flags. 
2.3 Implementation 
In this paper, we explore the use of a large-scale 
production statistical machine translation system 
to correct a class of ESL errors. A detailed de-
scription of the system can be found in (Menezes 
& Quirk 2005) and (Quirk et al, 2005). In keep-
ing with current best practices in SMT, our sys-
tem is a phrasal machine translation system that 
attempts to learn mappings between ?phrases? 
(which may not correspond to linguistic units) 
rather than individual words. What distinguishes 
250
this system from other phrasal SMT systems is 
that rather than aligning simple sequences of 
words, it maps small phrasal ?treelets? generated 
by a dependency parse to corresponding strings 
in the target. This ?Tree-To-String? model holds 
promise in that it allows us to potentially benefit 
from being able to access a certain amount of 
structural information during translation, without 
necessarily being completely tied to the need for 
a fully-well-formed linguistic analysis of the in-
put?an important consideration when it is 
sought to handle ungrammatical or otherwise ill-
formed ESL input, but also simultaneously to 
capture relationships not involving contiguous 
strings, for example determiner-noun relations.  
In our pilot study, this system was em-
ployed without modification to the system archi-
tecture. The sole adjustment made was to have 
both Source (erroneous) and Target (correct) sen-
tences tokenized using an English language to-
kenizer. N-best results for phrasal alignment and 
ordering models in the decoder were optimized 
by lambda training via Maximum Bleu, along the 
lines described in (Och, 2003).  
3 Data Development 
3.1 Identifying Mass Nouns 
In this paper, we focus on countability errors as-
sociated with mass nouns. This class of errors 
(involving nouns that cannot be counted, such as 
information, pollution, and home-
work) is characteristically encountered in ESL 
writing by native speakers of several East Asian 
languages (Dalgish, 1983; Hua & Lee, 2004).1 
We began by identifying a list of English nouns 
that are frequently involved in mass/count errors 
in by writing by Chinese ESL learners, by taking 
the intersection of words which: 
? occurred in either the Longman Dictionary 
of Contemporary English or the American 
Heritage Dictionary with a mass sense 
? were involved in n ? 2 mass/count errors in 
the Chinese Learner English Corpus 
CLEC (Gui and Yang, 2003), either tagged 
as a mass noun error or else with an adja-
cent tag indicating an article error.2  
                                                 
1
 These constructions are also problematic for hand-
crafted MT systems (Bond et al, 1994). 
2
 CLEC tagging is not comprehensive; some common 
mass noun errors (e.g., make a good progress) 
are not tagged in this corpus. 
This procedure yielded a list of 14 words: 
knowledge, food, homework, fruit, 
news, color, nutrition, equipment, 
paper, advice, haste, information, 
lunch, and tea. 3   Countability errors in-
volving these words are scattered across 46 sen-
tences in the CLEC corpus.   
For a baseline representing the level of writing 
assistance currently available to the average ESL 
writer, we submitted these sentences to the 
proofing tools in Microsoft Word 2003. The 
spelling and grammar checkers correctly identi-
fied 21 of the 46 relevant errors, proposed one 
incorrect substitution (a few advice ? a few 
advices), and failed to flag the remaining 25 
errors. With one exception, the proofing tools 
successfully detected as spelling errors incorrect 
plurals on lexical items that permit only mass 
noun interpretations (e.g., informations), 
but ignored plural forms like fruits and pa-
pers even when contextually inappropriate. The 
proofing tools in Word 2003 also detected singu-
lar determiner mismatches with obligatory plural 
forms (e.g. a news).  
3.2 Training Data 
The errors identified in these sentences provided 
an informal template for engineering the data in 
our training set, which was created by manipulat-
ing well-formed, edited English sentences. Raw 
data came from a corpus of ~484.6 million words 
of Reuters Limited newswire articles, released 
between 1995 and 1998, combined with a 
~7,175,000-word collection of articles from mul-
tiple news sources from 2004-2005. The result-
ing dataset was large enough to ensure that all 
targeted forms occurred with some frequency. 
From this dataset we culled about 346,000 
sentences containing examples of the 14 targeted 
words. We then used hand-constructed regular 
expressions to convert these sentences into 
mostly-ungrammatical strings that exhibited 
characteristics of the CLEC data, for example:  
? much ? many: much advice ? 
many advice  
? some ? a/an: some advice ? 
an advice  
? conversions to plurals: much good 
advice ? many good advices  
                                                 
3
 Terms that also had a function word sense, such as 
will, were eliminated for this experiment.  
251
? deletion of counters: piece(s)/ 
item(s)/sheet(s) of)  
? insertion of determiners  
These were produced in multiple combinations 
for broad coverage, for example: 
 
I'm not trying to give you 
legal advice. ? 
? I'm not trying to give you a 
legal advice. 
? I'm not trying to give you 
the legal advice. 
? I'm not trying to give you 
the legal advices. 
A total of 24128 sentences from the news data 
were ?lesioned? in this manner to create a set of 
65826 sentence pairs. To create a balanced train-
ing set that would not introduce too many arti-
facts of the substitution (e.g., many should not 
always be recast as much just because that is the 
only mapping observed in the training data), we 
randomly created an equivalent number of iden-
tity-mapped pairs from the 346,000 examples, 
with each sentence mapping to itself. 
Training sets of various sizes up to 45,000 
pairs were then randomly extracted from the le-
sioned and non-lesioned pairs so that data from 
both sets occurred in roughly equal proportions.  
Thus the 45K data set contains approximately 
22,500 lesioned examples. An additional 1,000 
randomly selected lesioned sentences were set 
aside for lambda training the SMT system?s or-
dering and replacement models. 
4  Evaluation 
4.1 Test Data 
The amount of tagged data in CLEC is too small 
to yield both development and test sets from the 
same data. In order to create a test set, we had a 
third party collect 150 examples of the 14 words 
from English websites in China. After minor 
cleanup to eliminate sentences irrelevant to the 
task,4 we ended up with 123 example sentences 
to use as test set. The test examples vary widely 
in style, from the highly casual to more formal 
public announcements. Thirteen examples were 
determined to contain no errors relevant to our 
experiment, but were retained in the data.5  
4.2 Results 
Table 1 shows per-sentence results of translating 
the test set on systems built with training data 
sets of various sizes (given in thousands of sen-
tence pairs). Numbers for the proofing tools in 
Word 2003 are presented by way of comparison, 
with the caveat that these tools have been inten-
tionally implemented conservatively so as not to 
potentially irritate native users with false flags. 
For our purposes, a replacement string is viewed 
as correct if, in the view of a native speaker who 
might be helping an ESL writer, the replacement 
would appear more natural and hence potentially 
useful as a suggestion in the context of that sen-
tence taken in isolation. Number disagreement 
on subject and verb were ignored for the pur-
poses of this evaluation, since these errors were 
not modeled when we introduced lesions into the 
data. A correction counted as Whole if the sys-
tem produced a contextually plausible substitu-
tion meeting two criteria: 1) number and 2) de-
terminer/quantifier selection (e.g., many in-
formations ? much information). 
Transformations involving bare singular targets 
(e.g., the fruits ? fruit) also counted 
as Whole.  Partial corrections are those where 
only one of the two criteria was met and part of 
the desired correction was missing (e.g., an 
                                                 
4
 In addition to eliminating cases that only involved 
subject-verb number agreement, we excluded a small 
amount of spam-like word salad, several instances of 
the word homework being misused to mean ?work 
done out of the home?, and one misidentified quota-
tion from Scott?s Ivanhoe. 
5
 This test set may be downloaded at 
http://research.microsoft.com/research/downloads 
Data Size Whole Partial Correctly Left New Error Missed Word Order  Error 
45K 55.28  0.81  8.13  12.20  21.14  1.63  
30K 36.59  4.07  7.32  16.26  32.52  3.25  
15K 47.15  2.44  5.69  11.38  29.27  4.07  
cf. Word 29.27  0.81  10.57  1.63  57.72  N/A 
 
 
Table 1.  Replacement percentages (per sentence basis) using different training data sets  
 
252
equipments ? an equipment versus the 
targeted bare noun equipment). Incorrect sub-
stitutions and newly injected erroneous material 
anywhere in the sentence counted as New Errors, 
even if the proposed replacement were otherwise 
correct. However, changes in upper and lower 
case and punctuation were ignored.  
The 55.28% per-sentence score for Whole 
matches in the system trained on the 45K data set 
means that it correctly proposed full corrections 
in 61.8% of locations where corrections needed 
to be made. The percentage of Missed errors, i.e., 
targeted errors that were ignored by the system, 
is correspondingly low. On the 45K training data 
set, the system performs nearly on a par with 
Word in terms of not inducing corrections on 
forms that did not require replacement, as shown 
in the Correctly Left column.  The dip in accu-
racy in the 30K sentence pair training set is an 
artifact of our extraction methodology: the rela-
tively small lexical set that we are addressing 
here appears to be oversensitive to random varia-
tion in the engineered training data. This makes 
it difficult to set a meaningful lower bound on 
the amount of training data that might be needed 
for adequate coverage. Nonetheless, it is evident 
from the table, that given sufficient data, SMT 
techniques can successfully offer corrections for 
a significant percentage of cases of the phenom-
ena in question.  
Table 2 shows some sample inputs together 
with successful corrections made by the system. 
Table 3 illustrates a case where two valid correc-
tions are found in the 5-best ranked translations; 
intervening candidates were identical with the 
top-ranked candidate.   
4.3 Error Analysis 
Table 1 also indicates that errors associated with 
the SMT system itself are encouragingly few. A 
small number of errors in word order were found, 
one of which resulted in a severely garbled sen-
tence in the 45K data set. In general, the percent-
age of this type of error declines consistently 
with growth of the training data size. Linearity of 
the training data may play a role, since the sen-
tence pairs differ by only a few words. On the 
whole, however, we expect the system?s order 
model to benefit from more training data.  
The most frequent single class of newly intro-
duced error relates to sporadic substitution of the 
word their for determiners a/the. This is 
associated with three words, lunch, tea, and 
haste, and is the principal contributor to the 
lower percentages in the Correctly Left bin, as 
compared with Word. This overgeneralization 
error reflects our attempt to engineer the discon-
tinuous mapping the X of them ? their 
X, motivated by examples like the following, 
encountered in the CLEC dataset: 
Input Shanghai residents can buy the fruits for a cheaper price than before.  
Replacement Shanghai residents can buy fruit for a cheaper price than before . 
Input Thank u for giving me so many advice. 
Replacement thank u for giving me so much advice . 
Input Acquiring the knowledge of information warfare is key to 
winning wars 
Replacement acquiring knowledge of information warfare is key to win-
ning wars 
Input Many knowledge about Li Bai can be gain through it. 
Replacement much knowledge about Li Bai can be gain through it . 
Input I especially like drinking the tea. 
Replacement i especially like drinking tea . 
Input Icons printed on a paper have been brought from Europe, 
and were pasted on boards on Taiwan. 
Replacement icons printed on paper have been brought from Europe , and 
were pasted on boards on Taiwan . 
 
Table 2.  Sample corrections, using 45K engineered training data 
253
In this equal world, lots of 
people are still concerned 
on the colors of them ? 
 
The inability of our translation system to handle 
such discontinuities in a unitary manner reflects 
the limited ability of current SMT modeling 
techniques to capture long-distance effects. Simi-
lar alternations are rife in bilingual data, e.g., 
ne?pas in French (Fox, 2002) and separable 
prefixes in German (Collins et al 2005). As 
SMT models become more adept at modeling 
long-distance effects in a principled manner, 
monolingual proofing will benefit as well. 
The Missed category is heterogeneous. The 
SMT system has an inherent bias against deletion, 
with the result that unwanted determiners tended 
not to be deleted, especially in the smaller train-
ing sets.  
Other errors related to coverage in the devel-
opment data set. Several occurrences of green-
grocer?s apostrophes (tea?s, equipment?s) 
caused correction failures: these were not antici-
pated when engineering the training data. Like-
wise, the test data presented several malformed 
quantifiers and quantifier-like phrases (plenty 
tea ? plenty of tea, a lot infor-
mation ? a lot of information, 
few information ? too little in-
formation) that had been unattested in the 
development set. Examples such as these high-
light the difficulty in obtaining complete cover-
age when using handcrafted techniques, whether 
to engineer errors, as in our case, or to handcraft 
targeted correction solutions.    
The system performed poorly on words that 
commonly present both mass and count noun 
senses in ways that are apt to confuse L2 writers. 
One problematic case was paper. The follow-
ing sentences, for example, remained uncor-
rected:  
  
He published many paper in 
provincial and national pub-
lication. 
He has published thirty-two 
pieces of papers. 
 
Large amounts of additional training data 
would doubtless be helpful in providing contex-
tual resolutions to the problems. Improved 
alignment models may also play a role here in 
capturing complex structures of the kind repre-
sented by constructions involving counters.     
5 Discussion 
The artificially-engineered training data that we 
relied on for our experiments proved surprisingly 
useful in modeling real errors made by non-
native speakers. However, this is obviously a less 
than ideal data source, since the errors introduced 
by regular expressions are homogenously dis-
tributed in a way that naturally-occurring errors 
are not, creating artifacts that undoubtedly impair 
our SMT models.  
Artificial data of this sort may be useful as 
proof of concept, but hand engineering such data 
plainly does not present a viable path to develop-
ing real world applications. In order to be able to 
handle the rich panoply of errors and error inter-
actions encountered in the text of second lan-
guage learners large quantities of naturally-
occurring ?before? and ?after? texts will need to 
be collected. By way of illustration, Table 4 
shows the output of results of ?translating? our 
test data into more natural English by hand and 
dumping the pre- and post-editing pairs to the 
45K training set.6 Although we were unable to 
exactly recover the target sentences, inspection 
showed that 25 sentences had improved, some 
significantly, as Table 4 shows. Under the right 
conditions, the SMT system can capture contex-
tual morphological alternations (nutri-
tion/nutritious), together with complex 
mappings represented by the dependencies 
learn ? knowledge ? many (ESL) and 
                                                 
6
 Since a single example of each pair was insufficient 
to override the system?s inherent bias towards uni-
gram mappings, 5 copies of each pair were appended 
to the training data. 
Input: And we can learn many knowledge or new information from TV 
Candidate 1: And we can learn much knowledge or new information from TV 
Candidate 5: And we can learn a lot of knowledge or new information from TV 
 
Table 3.  Multiple replacement candidates generated by 45K training set 
 
254
gain ? knowledge ? a lot of (Eng-
lish). In a rule-based correction system, an im-
mense amount of hand-coding would be required 
to handle even a small subset of the potential 
range of such mismatches between learner and 
native-like English. This knowledge, we believe, 
is best acquired from data.  
5.1 The Need for Data Collection 
Given a sufficiently large corpus of aligned sen-
tences containing error patterns produced by ESL 
writers of the same L1 background and their cor-
rected counterparts we expect eventually to be 
able to capture the rich complexity of non-native 
error within a noisy-channel based SMT model.  
As a practical matter, however, parallel data of 
the kind needed is far from easy to come by. This 
does not mean, however, that such data does not 
exist. The void left by commercial grammar 
checkers is filled, largely unobserved, by a num-
ber of services that provide editorial assistance, 
ranging from foreign language teachers, to lan-
guage helpdesks in multinational corporations, to 
mentoring services for conferences. Translation 
bureaus frequently offer editing services for non-
native speakers. Yet, unlike translation, the ?be-
fore? and ?after? texts are rarely recycled in a 
form that can be used to build translation models. 
Although collecting this data will involve a large 
investment in time, effort, and infrastructure, a 
serious effort along these lines is likely to prove 
fruitful in terms of making it possible to apply 
the SMT paradigm to ESL error correction.  
5.2 Feedback to SMT 
One challenge faced by the SMT model is the 
extremely high quality that will need to be at-
tained before a system might be usable. Since it 
is highly undesirable that learners should be pre-
sented with inaccurate feedback that they may 
not have the experience or knowledge to assess, 
the quality bar imposed on error correction is far 
higher than is that tolerated in machine transla-
tion. Exploration of error correction and writing 
assistance using SMT models may thus prove an 
important venue for testing new SMT models. 
5.3 Advantages of the SMT Approach 
Statistical Machine Translation has provided a 
hugely successful research paradigm within the 
field of natural language processing over the last 
decade. One of the major advantages of using 
SMT in ESL writing assistance is that it can be 
expected to benefit automatically from any pro-
gress made in SMT itself. In fact, the approach 
presented here benefits from all the advantages 
of statistical machine translation. Since the archi-
tecture is not dependent on hard-to-maintain 
rules or regular expressions, little or no linguistic 
expertise will be required in developing and 
maintain applications. As with SMT, this exper-
tise is pushed into the data component, to be 
handled by instructors and editors, who do not 
need programming or scripting skills.  
We expect it to be possible, moreover, once 
parallel data becomes available, to quickly ramp 
up new systems to accommodate the needs of 
Input sentence And we can learn many knowledge or new information from 
TV. 
45K system output and we can learn much knowledge or new information from 
TV . 
45K + translation sys-
tem output 
we can gain a lot of knowledge or new information from 
TV . 
Input sentence The following is one of the homework for last week. 
45K system output the following is one of their homework for last week . 
45K + translation sys-
tem output 
the following is one of the homework assignments for 
last week . 
Input sentence i like mushroom,its very nutrition 
45K system output i like mushroom , its very nutrition 
45K + translation sys-
tem output i like mushroom , its very nutritious 
 
Table 4.  Contextual corrections before and after adding ?translations? to 45K training data 
255
learners with different first-language back-
grounds and different skill levels and to writing 
assistance for learners of L2s other than English. 
It is also likely that this architecture may have 
applications in pedagogical environments and as 
a tool to assist editors and instructors who deal 
regularly with ESL texts, much in the manner of 
either Human Assisted Machine Translation or 
Machine Assisted Human Translation. We also 
believe that this same architecture could be ex-
tended naturally to provide grammar and style 
tools for native writers.  
6 Conclusion and Future Directions 
In this pilot study we have shown that SMT tech-
niques have potential to provide error correction 
and stylistic writing assistance to L2 learners. 
The next step will be to obtain a large dataset of 
pre- and post-editing ESL text with which to 
train a model that does not rely on engineered 
data. A major purpose of the present study has 
been to determine whether our hypothesis is ro-
bust enough to warrant the cost and effort of a 
collection or data creation effort.  
Although we anticipate that it will take a sig-
nificant lead time to assemble the necessary 
aligned data, once a sufficiently large corpus is 
in hand, we expect to begin exploring ways to 
improve our SMT system by tailoring it more 
specifically to the demands of editorial assistance. 
In particular, we expect to be looking into alter-
native word alignment models and possibly en-
hancing our system?s decoder using some of the 
richer, more structured language models that are 
beginning to emerge. 
Acknowledgements 
The authors have benefited extensively from dis-
cussions with Casey Whitelaw when he interned 
at Microsoft Research during the summer of 
2005. We also thank the Butler Hill Group for 
collecting the examples in our test set.   
References 
Bond, Francis, Kentaro Ogura and Satoru Ikehara. 
1994. Countability and Number in Japanese-to-
English Machine Translation. COLING-94. 
Peter E Brown, Stephen A. Della Pietra, Robert L. 
Mercer, and Vincent J. Della Pietra. 1993. The 
Mathematics of Statistical Machine Translation. 
Computational Linguistics, Vol. 19(2): 263-311.  
Martin Chodorow and Claudia Leacock. 2000. An 
Unsupervised Method for Detecting Grammatical 
Errors. NAACL 2000.  
Michael Collins, Philipp Koehn and Ivona Ku?erov?. 
2005. Clause Restructuring for Statistical machine 
Translation. ACL 2005, 531-540.  
Gerard M. Dalgish. 1984. Computer-Assisted ESL 
Research. CALICO Journal.  2(2): 32-33 
Heidi J. Fox.  2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP 2002. 
Shicun Gui and Huizhong Yang (eds). 2003 Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai: Shanghai Waiyu 
Jiaoyu Chubanshe. (In Chinese). 
Hua Dongfan and Thomas Hun-Tak Lee. 2004.  Chi-
nese ESL Learners' Understanding of the English 
Count-Mass Distinction. In Proceedings of the 7th 
Generative Approaches to Second Language Ac-
quisition Conference (GASLA 2004). 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun, 
and Changning Huang. 2000. PENS: A Machine-
aided English Writing System for Chinese Users. 
ACL 2000.  
Deryle Lonsdale and Diane Strong-Krause. 2003.  
Automated Rating of ESL Essays. In Proceedings 
of the HLT/NAACL Workshop: Building Educa-
tional Applications Using Natural Language Proc-
essing.   
Arul Menezes, and Chris Quirk. 2005. Microsoft Re-
search Treelet Translation System: IWSLT Evalua-
tion. Proceedings of the International Workshop on 
Spoken Language Translation.  
Franz Josef Och, 2003. Minimum error rate training 
in statistical machine translation. ACL 2003. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models.  ACL 2000. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency Tree Translation: Syntactically In-
formed Phrasal SMT. ACL 2005. 
Veit Reuer. 2003. Error Recognition and Feedback 
with Lexical Functional Grammar. CALICO Jour-
nal, 20(3): 497-512. 
Laura Mayfield Tomokiyo and Rosie Jones. 2001. 
You?re not from round here, are you? Naive Bayes 
Detection of Non-Native Utterance Text. NAACL 
2001. 
Anne Vandeventer Faltin. 2003. Natural language 
processing tools for computer assisted language 
learning. Linguistik online 17, 5/03 (http:// 
www.linguistik-online.de/17_03/vandeventer.html) 
 
256
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 444?451,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Obfuscating Document Stylometry to Preserve Author Anonymity 
 
 
Gary Kacmarcik         Michael Gamon 
Natural Language Processing Group 
Microsoft Research 
Redmond, WA  USA 
{garykac,mgamon}@microsoft.com 
 
  
 
Abstract 
This paper explores techniques for reduc-
ing the effectiveness of standard author-
ship attribution techniques so that an au-
thor A can preserve anonymity for a par-
ticular document D. We discuss feature 
selection and adjustment and show how 
this information can be fed back to the 
author to create a new document D? for 
which the calculated attribution moves 
away from A. Since it can be labor inten-
sive to adjust the document in this fash-
ion, we attempt to quantify the amount of 
effort required to produce the ano-
nymized document and introduce two 
levels of anonymization: shallow and 
deep. In our test set, we show that shal-
low anonymization can be achieved by 
making 14 changes per 1000 words to 
reduce the likelihood of identifying A as 
the author by an average of more than 
83%. For deep anonymization, we adapt 
the unmasking work of Koppel and 
Schler to provide feedback that allows 
the author to choose the level of ano-
nymization. 
1 Introduction 
Authorship identification has been a long stand-
ing topic in the field of stylometry, the analysis 
of literary style (Holmes 1998). Issues of style, 
genre, and authorship are an interesting sub-area 
of text categorization. In authorship detection it 
is not the topic of a text but rather the stylistic 
properties that are of interest. The writing style 
of a particular author can be identified by analyz-
ing the form of the writing, rather than the con-
tent. The analysis of style therefore needs to ab-
stract away from the content and focus on the 
content-independent form of the linguistic ex-
pressions in a text. 
Advances in authorship attribution have raised 
concerns about whether or not authors can truly 
maintain their anonymity (Rao and Rohatgi 
2000). While there are clearly many reasons for 
wanting to unmask an anonymous author, nota-
bly law enforcement and historical scholarship, 
there are also many legitimate reasons for an au-
thor to wish to remain anonymous, chief among 
them the desire to avoid retribution from an em-
ployer or government agency. Beyond the issue 
of personal privacy, the public good is often 
served by whistle-blowers who expose wrong-
doing in corporations and governments. The loss 
of an expectation of privacy can result in a chill-
ing effect where individuals are too afraid to 
draw attention to a problem, because they fear 
being discovered and punished for their actions. 
It is for this reason that we set out to investi-
gate the feasibility of creating a tool to support 
anonymizing a particular document, given the 
assumption that the author is willing to expend a 
reasonable amount of effort in the process. More 
generally, we sought to investigate the sensitivity 
of current attribution techniques to manipulation. 
For our experiments, we chose a standard data 
set, the Federalist Papers, since the variety of 
published results allows us to simulate author-
ship attribution ?attacks? on the obfuscated docu-
ment. This is important since there is no clear 
consensus as to which features should be used 
for authorship attribution. 
2 Document Obfuscation 
Our approach to document obfuscation is to 
identify the features that a typical authorship at-
tribution technique will use as markers and then 
adjust the frequencies of these terms to render 
them less effective on the target document. 
444
While it is obvious that one can affect the attri-
bution result by adjusting feature values, we 
were concerned with: 
? How easy is it to identify and present the 
required changes to the author? 
? How resilient are the current authorship 
detection techniques to obfuscation? 
? How much work is involved for the au-
thor in the obfuscation process? 
The only related work that we are aware of is 
(Rao and Rohatgi 2000) who identify the prob-
lem and suggest (somewhat facetiously, they 
admit) using a round-trip machine translation 
(MT) process (e.g., English ? French ? Eng-
lish) to obscure any traces of the original au-
thor?s style. They note that the current quality of 
MT would be problematic, but this approach 
might serve as a useful starting point for some-
one who wants to scramble the words a bit be-
fore hand-correcting egregious errors (taking 
care not to re-introduce their style). 
2.1 The Federalist Papers 
One of the standard document sets used in au-
thorship attribution is the Federalist Papers, a 
collection of 85 documents initially published 
anonymously, but now known to have been writ-
ten by 3 authors: Alexander Hamilton, John 
Madison and John Jay. Due to illness, Jay only 
wrote 5 of the papers, and most of the remaining 
papers are of established authorship (Hamilton = 
51; Madison = 14; and 3 of joint authorship be-
tween Hamilton and Madison). The 12 remaining 
papers are disputed between Hamilton and Madi-
son. In this work we limit ourselves to the 65 
known single-author papers and the 12 disputed 
papers. 
While we refer to these 12 test documents as 
?disputed?, it is generally agreed (since the work 
of Mosteller and Wallace (1964)) that all of the 
disputed papers were authored by Madison. In 
our model, we accept that Madison is the author 
of these papers and adopt the fiction that he is 
interested in obscuring his role in their creation. 
2.2 Problem Statement 
A more formal problem statement is as follows: 
We assume that an author A (in our case, Madi-
son) has created a document D that needs to be 
anonymized. The author self-selects a set K of N 
authors (where A ? K) that some future agent 
(the ?attacker? following the convention used in 
cryptography) will attempt to select between. 
The goal is to use authorship attribution tech-
niques to create a new document D? based on D 
but with features that identify A as the author 
suppressed. 
3 Document Preparation 
Before we can begin with the process of obfus-
cating the author style in D, we need to gather a 
training corpus and normalize all of the docu-
ments. 
3.1 Training Corpus 
While the training corpus for our example is 
trivially obtained, authors wishing to anonymize 
their documents would need to gather their own 
corpus specific for their use. 
The first step is to identify the set of authors K 
(including A) that could have possibly written the 
document. This can be a set of co-workers or a 
set of authors who have published on the topic. 
Once the authors have been selected, a suitable 
corpus for each author needs to be gathered. This 
can be emails or newsgroup postings or other 
documents. In our experiments, we did not in-
clude D in the corpus for A, although it does not 
seem unreasonable to do so. 
For our example of the Federalist Papers, K is 
known to be {Hamilton, Madison} and it is al-
ready neatly divided into separate documents of 
comparable length. 
3.2 Document Cleanup 
Traditional authorship attribution techniques rely 
primarily on associating idiosyncratic formatting, 
language usage and spelling (misspellings, typos, 
or region-specific spelling) with each author in 
the study. Rao and Rohatgi (2000) and Koppel 
and Schler (2003) both report that these words 
serve as powerful discriminators for author attri-
bution. Thus, an important part of any obfusca-
tion effort is to identify these idiosyncratic usage 
patterns and normalize them in the text. 
Koppel and Schler (2003) also note that many 
of these patterns can be identified using the basic 
spelling and grammar checking tools available in 
most word processing applications. Correcting 
the issues identified by these tools is an easy first 
step in ensuring the document conforms to con-
ventional norms. This is especially important for 
work that will not be reviewed or edited since 
these idiosyncrasies are more likely to go unno-
ticed. 
445
However, there are distinctive usage patterns 
that are not simple grammar or spelling errors 
that also need to be identified. A well-known 
example of this is the usage of while/whilst by 
the authors of the Federalist Papers. 
 
 Hamilton Madison Disputed 
while 36 0 0 
whilst 1 12 9 
 
Table 1  : Occurrence counts of ?while? and ?whilst? 
in the Federalist Papers (excluding documents au-
thored by Jay and those which were jointly authored). 
 
In the disputed papers, ?whilst? occurs in 6 of 
the documents (9 times total) and ?while? occurs 
in none. To properly anonymize the disputed 
documents, ?whilst? would need to be eliminated 
or normalized. 
This is similar to the problem with idiosyn-
cratic spelling in that there are two ways to apply 
this information. The first is to simply correct the 
term to conform to the norms as defined by the 
authors in K. The second approach is to incorpo-
rate characteristic forms associated with a par-
ticular author. While both approaches can serve 
to reduce the author?s stylometric fingerprint, the 
latter approach carries the risk of attempted style 
forgery and if applied indiscriminately may also 
provide clues that the document has been ano-
nymized (if strong characteristics of multiple 
authors can be detected). 
For our experiments, we opted to leave these 
markers in place to see how they were handled 
by the system. We did, however, need to normal-
ize the paragraph formatting, remove all capitali-
zation and convert all footnote references to use 
square brackets (which are otherwise unused in 
the corpus). 
3.3 Tokenization 
To tokenize the documents, we separated se-
quences of letters using spaces, newlines and the 
following punctuation marks: .,()-:;`'?![]. No 
stemming or morphological analysis was per-
formed. This process resulted in 8674 unique 
tokens for the 65 documents in the training set. 
4 Feature Selection 
The process of feature selection is one of the 
most crucial aspects of authorship attribution. By 
far the most common approach is to make use of 
the frequencies of common function words that 
are content neutral, but practitioners have also 
made use of other features such as letter metrics 
(e.g., bi-grams), word and sentence length met-
rics, word tags and parser rewrite rules. For this 
work, we opted to limit our study to word fre-
quencies since these features are generally ac-
knowledged to be effective for authorship attri-
bution and are transparent, which allows the au-
thor to easily incorporate the information for 
document modification purposes. 
We wanted to avoid depending on an initial 
list of candidate features since there is no guaran-
tee that the attackers will limit themselves to any 
of the commonly used lists. Avoiding these lists 
makes this work more readily useful for non-
English texts (although morphology or stemming 
may be required). 
We desire two things from our feature selec-
tion process beyond the actual features. First, we 
need a ranking of the features so that the author 
can focus efforts on the most important features. 
The second requirement is that we need a thresh-
old value so that the author knows how much the 
feature frequency needs to be adjusted. 
To rank and threshold the features, we used 
decision trees (DTs) and made use of the readily 
available WinMine toolkit (Chickering 2002). 
DTs produced by WinMine for continuously val-
ued features such as frequencies are useful since 
each node in the tree provides the required 
threshold value. For term-ranking, we created a 
Decision Tree Root (DTR) ranking metric to or-
der the terms based on how discriminating they 
are. DTR Rank is computed by creating a series 
of DTs where we remove the root feature, i.e. the 
most discriminating feature, before creating the 
next DT. In this fashion we create a ranking 
based on the order in which the DT algorithm 
determined that the term was most discrimina-
tory. The DTR ranking algorithm is as follows: 
1) Start with a set of features 
2) Build DT and record root feature 
3) Remove root feature from list of features 
4) Repeat from step 2 
It is worth noting that the entire DT need not 
be calculated since only the root is of interest. 
The off-the-shelf DT toolkit could be replaced 
with a custom implementation1 that returned only 
the root (also known as a decision stump). Since 
                                                 
1
 Many DT learners are information-gain based, but 
the WinMine toolkit uses a Bayesian scoring criterion 
described in Chickering et al (1997) with normal-
Wishart parameter priors used for continuously val-
ued features. 
446
our work is exploratory, we did not pursue op-
timizations along these lines. 
For our first set of experiments, we applied 
DTR ranking starting with all of the features 
(8674 tokens from the training set) and repeated 
until the DT was unable to create a tree that per-
formed better than the baseline of p(Hamilton) = 
78.46%. In this fashion, we obtained an ordered 
list of 2477 terms, the top 10 of which are shown 
in Table 2, along with the threshold and bias. 
The threshold value is read directly from the DT 
root node and the bias (which indicates whether 
we desire the feature value to be above or below 
the threshold) is determined by selecting the 
branch of the DT which has the highest ratio of 
non-A to A documents.  
Initially, this list looks promising, especially 
since known discriminating words like ?upon? 
and ?whilst? are the top two ranked terms. How-
ever, when we applied the changes to our base-
line attribution model (described in detail in the 
Evaluation section), we discovered that while it 
performed well on some test documents, others 
were left relatively unscathed. This is shown in 
Figure 1 which graphs the confidence in assign-
ing the authorship to Madison for each disputed 
document as each feature is adjusted. We expect 
the confidence to start high on the left side and 
move downward as more features are adjusted. 
After adjusting all of the identified features, half 
of the documents were still assigned to Madison 
(i.e., confidence > 0.50). 
Choosing just the high-frequency terms was 
also problematic since most of them were not 
considered to be discriminating by DTR ranking 
(see Table 3). The lack of DTR rank not only 
means that these are poor discriminators, but it 
also means that we do not have a threshold value 
to drive the feature adjustment process. 
 
Token DTR Frequency Token DTR Frequency 
the 
, 
of 
to 
. 
and 
in 
a 
be 
that 
- 
595 
- 
39 
- 
185 
119 
515 
- 
- 
0.094227 
0.068937 
0.063379 
0.038404 
0.027977 
0.025408 
0.023838 
0.021446 
0.020139 
0.014823 
it 
is 
which 
as 
by 
; 
this 
would 
have 
or 
- 
- 
- 
- 
58 
57 
575 
477 
- 
- 
0.013404 
0.011873 
0.010933 
0.008811 
0.008614 
0.007773 
0.007701 
0.007149 
0.006873 
0.006459 
 
Table 3  : Top 20 terms sorted by frequency.  
 
We next combined the DTR and the term fre-
quency approaches by computing DTR one the 
set of features whose frequency exceeds a speci-
fied threshold for any one of the authors. Select-
ing a frequency of 0.001 produces a list of 35 
terms, the first 14 of which are shown in Table 4. 
 
Token Frequency Threshold ? 49 
upon 
on 
powers 
there 
to 
men 
; 
by 
less 
in 
at 
those 
and 
any 
0.002503 
0.004429 
0.001485 
0.002707 
0.038404 
0.001176 
0.007773 
0.008614 
0.001176 
0.023838 
0.002990 
0.002615 
0.025408 
0.002930 
> 0.003111 
< 0.004312 
< 0.002012 
< 0.002911 
> 0.039071 
> 0.001531 
< 0.007644 
< 0.008110 
< 0.001384 
> 0.023574 
> 0.003083 
> 0.002742 
< 0.025207 
> 0.003005 
+6 
-9 
0 
+3 
+7 
+1 
0 
-2 
-1 
+6 
0 
+4 
-1 
+2 
 
Table 4  : Top 14 DTR(0.001) ranked items. The last 
column is the number of changes required to achieve 
the threshold frequency for document #49. 
 
Results for this list were much more promising 
and are shown in Figure 2. The confidence of 
attributing authorship to Madison is reduced by 
an average of 84.42% (? = 12.51%) and all of the 
documents are now correctly misclassified as 
being written by Hamilton. 
 
Token DTR Threshold Occurrence #49 
upon 
whilst 
on 
powers 
there 
few 
kind 
consequently 
wished 
although 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
> 0.003111 
< 0.000516 
< 0.004312 
< 0.002012 
> 0.002911 
< 0.000699 
> 0.001001 
< 0.000513 
> 0.000434 
< 0.000470 
0 ? 6 
1 ? 0 
16 ? 7 
2 ? 2 
2 ? 5 
1 ? 2 
0 ? 2 
1 ? 0 
1 ? 0 
0 ? 0 
 
Table 2  : Top 10 DTR Rank ordered terms with threshold 
and corresponding occurrence count (original document ? 
obfuscated version) for one of the disputed documents 
(#49). 
 
 
0.00
0.25
0.50
0.75
1.00
up
on
wh
ilst on
po
we
rs
the
re few kin
d
co
ns
eq
ue
ntl
y
wis
he
d
alt
ho
ug
h
 
 
Figure 1 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Each line cor-
responds to one of the 12 disputed documents. Features are 
ordered by DTR Rank and the attribution model is SVM30. 
Values above 0.5 are assigned to Madison and those below 
0.5 are assigned to Hamilton. 
447
0.00
0.25
0.50
0.75
1.00
up
on on
po
we
rs
the
re to
m
en by ; les
s in at
tho
se an
d
an
y
 
 
Figure 2 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Feature order 
is DTR(0.001) and the attribution model is SVM30. 
 
5 Evaluation 
Evaluating the effectiveness of any authorship 
obfuscation approach is made difficult by the 
fact that it is crucially dependent on the author-
ship detection method that is being utilized. An 
advantage of using the Federalist Papers as the 
test data set is that there are numerous papers 
documenting various methods that researchers 
have used to identify the authors of the disputed 
papers. 
However, because of differences in the exact 
data set2 and machine learning algorithm used, it 
is not reasonable to create an exact and complete 
implementation of each system. For our experi-
ments, we used only the standard Federalist Pa-
pers documents and tested each feature set using 
linear-kernel SVMs, which have been shown to 
be effective in text categorization (Joachims 
1998). To train our SVMs we used a sequential 
minimal optimization (SMO) implementation 
described in (Platt 1999). 
The SVM feature sets that we used for the 
evaluation are summarized in Table 5. 
For the early experiments described in the 
previous section we used SVM30, which incor-
porates the final set of 30 terms that Mosteller & 
Wallace used for their study. As noted earlier, 
they made use of a different data set than we did, 
so we did expect to see some differences in the 
results. The baseline model (plotted as the left-
most column of points in Figure 1 and Figure 2) 
assigned all of the disputed papers to Madison 
except one3. 
                                                 
2
 Mosteller & Wallace and some others augmented the 
Federalist Papers with additional document samples 
(5 Hamilton and 36 Madison), but this has not been 
done universally by all researchers. 
3
 Document #55. However, this is not inconsistent 
with Mosteller &Wallace?s results: ?Madison is ex-
tremely likely [?] to have written all the disputed 
SVM70 (Mosteller & Wallace 
1964) 
70 common function 
words.4 
SVM30 (Mosteller & Wallace 
1964) 
Final 30 terms.5 
SVM11 (Tweedie, Singh & 
Holmes 1996) 
on, upon, there, any, 
an, every, his, from, 
may, can, do 
SVM08 (Holmes & Forsyth 
1995) 
upon, both, on, there, 
whilst, kind, by, 
consequently 
SVM03 (Bosch & Smith 1998) upon, our, are 
 
Table 5  : Summary of feature words used in other Federal-
ist Papers studies. 
 
5.1 Feature Modification 
Rather than applying the suggested modifications 
to the original documents and regenerating the 
document feature vectors from scratch each time, 
we simplified the evaluation process by adjusting 
the feature vector directly and ignoring the im-
pact of the edits on the overall document prob-
abilities. The combination of insertions and dele-
tions results in the total number of words in the 
document being increased by an average of 19.58 
words (? = 7.79), which is less than 0.5% of the 
document size. We considered this value to be 
small enough that we could safely ignore its im-
pact. 
Modifying the feature vector directly also al-
lows us to consider each feature in isolation, 
without concern for how they might interact with 
each other (e.g. converting whilst?while or re-
writing an entire sentence). It also allows us to 
avoid the problem of introducing rewrites into 
the document with our distinctive stylometric 
signature instead of a hypothetical Madison re-
write. 
5.2 Experiments 
We built SVMs for each feature set listed in 
Table 5 and applied the obfuscation technique 
described above by adjusting the values in the 
feature vector by increments of the single-word 
probability for each document. The results that 
we obtained were the same as observed with our 
test model ? all of the models were coerced to 
prefer Hamilton for each of the disputed docu-
ments. 
 
                                                                          
Federalists [?] with the possible exception of No. 55. 
For No. 55 our evidence is relatively weak [?].? 
(Mosteller & Wallace 1964) p.263. 
4
 ibid p.38. 
5
 ibid p.66. 
448
0.00
0.25
0.50
0.75
1.00
up
on on
po
we
rs
the
re to
m
en by ; les
s in at
tho
se an
d
an
y
 
 
Figure 3 : Confidence in assigning disputed papers to 
Madison graphed as each feature is adjusted. Feature order 
is DTR(0.001) and the attribution model is SVM70. 
 
Figure 3 shows the graph for SVM70, the 
model that was most resilient to our obfuscation 
techniques. The results for all models are sum-
marized in Table 6. The overall reduction 
achieved across all models is 86.86%. 
 
 % Reduction ? 
SVM70 74.66% 12.97% 
SVM30 84.42% 12.51% 
SVM11 82.65% 10.99% 
SVM08 93.54% 4.44% 
SVM03 99.01% 0.74% 
 
Table 6  : Percent reduction in the confidence 
of assigning the disputed papers to Madison 
for each of the tested feature sets. 
 
Of particular note in the results are those for 
SVM03, which proved to be the most fragile 
model because of its low dimension. If we con-
sider this case an outlier and remove it from 
study, our overall reduction becomes 83.82%. 
5.3 Feature Changes 
As stated earlier, an important aspect of any ob-
fuscation approach is the number of changes re-
quired to effect the mis-attribution. Table 7 
summarizes the absolute number of changes 
(both insertions and deletions) and also expresses 
this value related to the original document size. 
The average number of changes required per 
1000 words in the document is 14.2. While it is 
difficult to evaluate how much effort would be 
required to make each of these individual 
changes, this value seems to be within the range 
that a motivated person could reasonably under-
take. 
More detailed summaries of the number of 
feature changes required for single document 
(#49) are given in Table 2 and Table 4. 
By calculating the overall number of changes 
required, we implicitly consider insertions and 
deletions to be equally weighted. However, while 
deletion sites in the document are easy to identify, 
 
Document Changes Doc Size Changes/1000 
49 42 3849 10.9 
50 46 2364 19.5 
51 67 4039 16.6 
52 52 3913 13.3 
53 62 4592 13.5 
54 53 4246 12.5 
55 52 4310 12.1 
56 59 3316 17.8 
57 60 4610 13.0 
58 54 4398 12.3 
62 78 5048 15.5 
63 91 6429 14.2 
 
Table 7  : Changes required per document 
 
proposing insertion sites can be more problem-
atic. We do not address this difference in this 
paper, although it is clear that more investigation 
is required in this area. 
6 Deep Obfuscation 
The techniques described above result in what 
we term shallow obfuscation since they focus on 
a small number of features and are only useful as 
a defense against standard attribution attacks. 
More advanced attribution techniques, such as 
that described in (Koppel and Schler 2004) look 
deeper into the author?s stylometric profile and 
can identify documents that have been obfus-
cated in this manner. 
Koppel and Schler introduce an approach they 
term ?unmasking? which involves training a se-
ries of SVM classifiers where the most strongly 
weighted features are removed after each itera-
tion. Their hypothesis is that two texts from dif-
ferent authors will result in a steady and rela-
tively slow decline of classification accuracy as 
features are being removed. In contrast, two texts 
from the same author will produce a relatively 
fast decline in accuracy. According to the authors, 
a slow decline indicates deep and fundamental 
stylistic differences in style - beyond the ?obvi-
ous? differences in the usage of a few frequent 
words. A fast decline indicates that there is an 
underlying similarity once the impact of a few 
superficial distinguishing markers has been re-
moved. 
We repeated their experiments using 3-fold 
cross-validation to compare Hamilton and Madi-
son with each other and the original (D) and ob-
fuscated (D?) documents. The small number of 
documents required that we train the SVM using 
the 50 most frequent words. Using a larger pool 
of feature words resulted in unstable models, es-
pecially when comparing Madison (14 docu-
ments) with D and D? (12 documents). The re-
sults of this comparison are shown in Figure 4. 
449
0.3000
0.4000
0.5000
0.6000
0.7000
0.8000
0.9000
1.0000
HvD
HvD'
HvM
MvD
MvD'
 
 
Figure 4 : Unmasking the obfuscated document. The y-axis 
plots the accuracy of a classifier trained to distinguish be-
tween two authors; the x-axis plots each iteration of the 
unmasking process. The top three lines compare Hamilton 
(H) versus Madison (M), the original document (D) and the 
obfuscated document (D?). The bottom line is M vs. D and 
the middle line is M vs. D?. 
 
In this graph, the comparison of Hamilton and 
the modified document (MvD?) exhibits the 
characteristic curve described by Koppel and 
Schler, which indicates that the original author 
can still be detected. However, the curve has 
been raised above the curve for the original 
document which suggests that our approach does 
help insulate against attacks that identify deep 
stylometric features. 
Modifying additional features continues this 
trend and raises the curve further. Figure 5 sum-
marizes this difference by plotting the difference 
between the accuracy of the HvD? and MvD? 
curves for documents at different levels of fea-
ture modification. An ideal curve in this graph 
would be one that hugged the x-axis since this 
would indicate that it was as difficult to train a 
classifier to distinguish between M and D? as it is 
to distinguish between H and D?. In this graph, 
the ?0? curve corresponds to the original docu-
ment, and the ?14? curve to the modified docu-
ment shown in Figure 4. The ?35? curve uses all 
of the DTR(0.001) features. 
This graph demonstrates that using DTR rank-
ing to drive feature adjustment can produce 
documents that are increasingly harder to detect 
as being written by the author. While it is unsur-
prising that a deep level of obfuscation is not 
achieved when only a minimal number of fea-
tures are modified, this graph can be used to 
measure progress so that the author can deter-
mine enough features have been modified to 
achieve the desired level of anonymization. 
Equally unsurprising is that this increased ano-
nymization comes at an additional cost, summa-
rized in Table 8. 
 
Num Features Changes/1000 
7 9.9 
14 14.2 
21 18.3 
28 22.5 
35 25.1 
 
Table 8  : Relationship between number 
of features modified and corresponding 
changes required per 1000 words. 
 
While in this work we limited ourselves to the 
35 DTR(0.001) features, further document modi-
fication can be driven by lowering the DTR 
probability threshold to identify additional terms 
in an orderly fashion. 
7 Conclusion 
In this paper, we have shown that the standard 
approaches to authorship attribution can be con-
founded by directing the author to selectively 
edit the test document. We have proposed a tech-
nique to automatically identify distinctive fea-
tures and their frequency thresholds. By using a 
list of features that are both frequent and highly 
ranked according to this automatic technique, the 
amount of effort required to achieve reasonable 
authorship obfuscation seems to be well within 
the realm of a motivated author. While we make 
no claim that this is an easy task, and we make 
the assumption that the author has undertaken 
basic preventative measures (like spellchecking 
and grammar checking), it does not seem to be 
an onerous task for a motivated individual. 
It not surprising that we can change the out-
come by adjusting the values of features used in 
authorship detection. Our contribution, however, 
is that many of the important features can be de-
termined by simultaneously considering term-
frequency and DTR rank, and that this process 
results in a set of features and threshold values 
that are transparent and easy to control. 
 
-0.1000
0.0000
0.1000
0.2000
0.3000
0.4000
0.5000
0
14
35
 
 
Figure 5 : Overall impact of feature modification for dif-
ferent levels of obfuscation. The y-axis plots the accuracy 
delta between the HvD' and MvD' curves; the x-axis plots 
each iteration of the unmasking process. The legend indi-
cates the number of features modified for each curve. 
450
Given this result, it is not unreasonable to ex-
pect that a tool could be created to provide feed-
back to an author who desires to publish a docu-
ment anonymously. A sophisticated paraphrase 
tool could theoretically use the function word 
change information to suggest rewrites that 
worked toward the desired term frequency in the 
document. 
For our experiments, we used a simplified 
model of the document rewrite process by evalu-
ating the impact of each term modification in 
isolation. However, modifying the document to 
increase or decrease the frequency of a term will 
necessarily impact the frequencies of other terms 
and thus affect the document's stylometric signa-
ture. Further experimentation is clearly needed in 
this area needs to address the impact of this in-
terdependency. 
One limitation to this approach is that it ap-
plies primarily to authors that have a reasonably-
sized corpus readily available (or easily created). 
However, for situations where a large corpus is 
not available, automated authorship attribution 
techniques are likely to be less effective (and 
thus obfuscation is less necessary) since the 
number of possible features can easily exceed the 
number of available documents. An interesting 
experiment would be to explore how this ap-
proach applies to different types of corpora like 
email messages. 
We also recognize that these techniques could 
be used to attempt to imitate another author?s 
style. We do not address this issue other than to 
say that our thresholding approach is intended to 
push feature values just barely across the thresh-
old away from A rather than to mimic any one 
particular author. 
Finally, in these results, there is a message for 
those involved in authorship attribution: simple 
SVMs and low-dimensional models (like 
SVM03) may appear to work well, but are far 
less resilient to obfuscation attempts than Koppel 
and Schler?s unmasking approach. Creating clas-
sifiers with the minimum number of features 
produces a model that is brittle and more suscep-
tible to even simplistic obfuscation attempts. 
8 Acknowledgements 
Thanks are in order to the reviewers of earlier 
drafts of this document, notably Chris Brockett 
and our anonymous reviewers. In addition, Max 
Chickering provided useful information regard-
ing his implementation of DTs in the WinMine 
toolkit. 
References 
R. A. Bosch and J. A. Smith. 1998. Separating Hy-
perplanes and the Authorship of the Federalist Pa-
pers. American Mathematical Monthly, Vol. 105 
#7 pp. 601-608. 
D. M. Chickering, D. Heckerman and C. Meek. 1997. 
A Bayesian Approach to Learning Bayesian Net-
works with Local Structure. In Proceedings of the 
Thirteenth Conference on Uncertainty in Artificial 
Intelligence (UAI97 Providence, RI), pp. 80-89. 
D. M. Chickering. 2002. The WinMine Toolkit. 
Technical Report MSR-TR-2002-103. 
D. I. Holmes and R. S. Forsyth. 1995. The Federalist 
Revisited: New Directions in Authorship Attribu-
tion. Literary and Linguistic Computing 10(2), 
pp.111-127. 
D. I. Holmes. 1998. The Evolution of Stylometry in 
Humanities Scholarship. Literary and Linguistic 
Computing 13(3), pp.111-117. 
T. Joachims. 1998. Text Categorization with Support 
Vector Machines: Learning with many Relevant 
Features. In Proceedings of the 10th European 
Conference on Machine Learning, pp.137-142. 
M. Koppel and J. Schler. 2003. Exploiting Stylistic 
Idiosyncrasies for Authorship Attribution. In Pro-
ceedings of IJCAI'03 Workshop on Computational 
Approaches to Style Analysis and Synthesis (Aca-
pulco, Mexico). pp.69-72. 
M. Koppel and J. Schler, 2004. Authorship Verifica-
tion as a One-Class Classification Problem. In Pro-
ceedings of the Twenty-First International Confer-
ence on Machine Learning (ICML 04 Banff, Al-
berta, Canada), pp.489-495. 
F. Mosteller and D. L. Wallace. 1964. Inference and 
Disputed Authorship: The Federalist. Addison-
Wesley (Reading, Massachusetts, USA). 
J. Platt. 1999. Fast Training of SVMs Using Sequen-
tial Minimal Optimization. In B. Sch?lkopf, C. 
Burges and A. Smola (eds.) Advances in Kernel 
Methods: Support Vector Learning. MIT Press 
(Cambridge, MA, USA), pp.185-208. 
J. R. Rao and P. Rohatgi. 2000. Can Pseudonymity 
Really Guarantee Privacy?, In Proceedings of the 
9th USENIX Security Symposium (Denver, Colo-
rado, USA), pp.85-96. 
F. J. Tweedie, S. Singh and D. I. Holmes. 1996. Neu-
ral Network Applications in Stylometry: The Fed-
eralist Papers. In Computers and the Humanities 
30(1), pp.1-10. 
 
451
        Task-focused Summarization of Email 
Simon Corston-Oliver, Eric Ringger, Michael Gamon and Richard Campbell 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{simonco, ringger, mgamon, richcamp}@microsoft.com 
 
 
 
 
Abstract 
 We describe SmartMail, a prototype system for 
automatically identifying action items (tasks) in 
email messages. SmartMail presents the user with 
a task-focused summary of a message. The 
summary consists of a list of action items extracted 
from the message. The user can add these action 
items to their ?to do? list. 
1 Introduction 
Email for many users has evolved from a mere 
communication system to a means of organizing 
workflow, storing information and tracking tasks 
(i.e. ?to do? items) (Bellotti et al, 2003; Cadiz et 
al., 2001). Tools available in email clients for 
managing this information are often cumbersome 
or even so difficult to discover that users are not 
aware that the functionality exists. For example, in 
one email client, Microsoft Outlook, a user must 
switch views and fill in a form in order to create a 
task corresponding to the current email message. 
By automatically identifying tasks that occur in the 
body of an email message, we hope to simplify the 
use of email as a tool for task creation and 
management. 
In this paper we describe SmartMail, a prototype 
system that automatically identifies tasks in email, 
reformulates them, and presents them to the user in 
a convenient interface to facilitate adding them to a 
?to do? list.  
SmartMail performs a superficial analysis of an 
email message to distinguish the header, message 
body (containing the new message content), and 
forwarded sections. 1  SmartMail breaks the 
                                                                 
1  This simple division into header, message body, and 
forwarded sections was sufficient for the corpus of email 
messages we considered. Messages containing original 
messages interleaved with new content were extremely 
message body into sentences, then determines 
the speech act of each sentence in the message 
body by consulting a machine-learned classifier. 
If the sentence is classified as a task, SmartMail 
performs additional linguistic processing to 
reformulate the sentence as a task description. 
This task description is then presented to the 
user. 
2 Data 
We collected a corpus of 15,741 email 
messages. The messages were divided into 
training, development test and blind test. The 
training set contained 106,700 sentences in 
message bodies from 14,535 messages. To 
avoid overtraining to individual writing styles, 
we limited the number of messages from a 
given sender to 50. To ensure that our 
evaluations are indicative of performance on 
messages from previously unencountered 
senders, we selected messages from 3,098 
senders, assigning all messages from a given 
sender to either the training or the test sets. 
Three human annotators labeled the message 
body sentences, selecting one tag from the 
following set: Salutation, Chit-chat (i.e., social 
discussion unrelated to the main purpose of the 
message), Task, Meeting (i.e., a proposal to 
meet), Promise, Farewell, various components 
of an email signature (Sig_Name, Sig_Title, 
Sig_Affiliation, Sig_Location, Sig_Phone, 
Sig_Email, Sig_URL, Sig_Other), and the 
default category ?None of the above?. The set of 
tags can be considered a set of application-
specific speech acts analogous to the rather 
particular tags used in the Verbmobil project, 
such as ?Suggest_exclude_date? and 
                                                                                                
uncommon in our corpus. Most senders were using 
Microsoft Outlook, which places the insertion point for 
new content at the top of the message. 
?Motivate_appointment? (Warnke et al, 1997; 
Mast et al, 1996) or the form-based tags of Stolcke 
et al (1998). 
All three annotators independently labeled 
sentences in a separate set of 146 messages not 
included in the training, development or blind test 
sets. We measured inter-annotator agreement for 
the assignment of tags to sentences in the message 
bodies using Cohen?s Kappa. Annotator 1 and 
annotator 2 measured 85.8%; annotator 1 and 
annotator 3 measured 82.6%; annotator 2 and 
annotator 3 measured 82.3%. We consider this 
level of inter-annotator agreement good for a novel 
set of application-specific tags. 
The development test and blind test sets of 
messages were tagged by all three annotators, and 
the majority tag for each sentence was taken. If any 
sentence did not have a majority tag, the entire 
message was discarded, leaving a total of 507 
messages in the development test set and 699 
messages in the blind test set. 
The set of tags was intended for a series of 
related experiments concerning linguistic 
processing of email. For example, greetings and 
chit-chat could be omitted from messages 
displayed on cell phones, or the components of an 
email signature could be extracted and stored in a 
contact database. In the current paper we focus 
exclusively on the identification of tasks. 
Annotators were instructed to mark a sentence 
as containing a task if it looked like an appropriate 
item to add to an on-going ?to do? list. By this 
criterion, simple factual questions would not 
usually be annotated as tasks; merely responding 
with an answer fulfills any obligation. Annotators 
were instructed to consider the context of an entire 
message when deciding whether formulaic endings 
to email such as Let me know if you have any 
questions were to be interpreted as mere social 
convention or as actual requests for review and 
comment. The following are examples of actual 
sentences annotated as tasks in our data: 
Since Max uses a pseudo-
random number generator, you 
could possibly generate the 
same sequence of numbers to 
select the same cases. 
 
Sorry, yes, you would have to 
retrain. 
 
An even fast [sic] thing 
would be to assign your own 
ID as a categorical feature. 
 
Michael, it?d be great if 
you could add some stuff re 
MSRDPS. 
 
Could you please remote 
desktop in and try running 
it on my machine. 
 
If CDDG has its own notion 
of what makes for good 
responses, then we should 
use that. 
 
3 Features 
Each sentence in the message body is described 
by a vector of approximately 53,000 features. 
The features are of three types: properties of the 
message (such as the number of addressees, the 
total size of the message, and the number of 
forwarded sections in the email thread), 
superficial features and linguistic features. 
The superficial features include word 
unigrams, bigrams and trigrams as well as 
counts of special punctuation symbols (e.g. @, 
/, #), whether the sentence contains words with 
so-called ?camel caps? (e.g., SmartMail), 
whether the sentence appears to contain the 
sender?s name or initials, and whether the 
sentence contains one of the addressees? names. 
The linguistic features were obtained by 
analyzing the given sentence using the NLPWin 
system (Heidorn 2000). The linguistic features 
include abstract lexical features, such as part-of-
speech bigrams and trigrams, and structural 
features that characterize the constituent 
structure in the form of context-free phrase 
structure rewrites (e.g., DECL:NP-VERB-NP; 
i.e., a declarative sentence consisting of a noun 
phrase followed by a verb and another noun 
phrase). Deeper linguistic analysis yielded 
features that describe part-of-speech 
information coupled with grammatical relations 
(e.g., Verb-Subject-Noun indicating a nominal 
subject of a verb) and features of the logical 
form analysis such as transitivity, tense and 
mood. 
 
4 Results 
We trained support vector machines (SVMs) 
(Vapnik, 1995) using an implementation of the 
sequential minimal optimization algorithm 
(Platt, 1999). We trained linear SVMs, which 
have proven effective in text categorization with 
large feature vectors (Joachims, 1998; Dumais et 
al., 1998).  
Figure 1 illustrates the precision-recall curve for 
the SVM classifier trained to distinguish tasks vs. 
non-tasks measured on the blind test set. 
We conducted feature ablation experiments on 
the development test set to assess the contribution 
of categories of features to overall classification 
performance. In particular we were interested in 
the role of linguistic analysis features compared to 
using only surface features. Within the linguistic 
features, we distinguished deep linguistic features 
(phrase structure features and semantic features) 
from POS n-gram features. We conducted 
experiments with three feature sets: 
1. all features (message level features + word 
unigram, bigram and trigram  
2. features + POS bigram and trigram 
features + linguistic analysis features) 
3. no deep linguistic features (no phrase 
structure or semantic features) 
4. no linguistic features at all (no deep 
linguistic features and no POS n-gram 
features) 
Based on these experiments on the development 
test set, we chose the feature set used for our run-
time applications.  
 
Figure 1 shows final results for these feature 
sets on the blind test set: for recall between 
approximately 0.2 and 0.4 and between 
approximately 0.5 and 0.6 the use of all features 
produces the best results. The distinction 
between the ?no linguistic features? and ?no 
deep linguistic features? scenarios is negligible; 
word n-grams appear to be highly predictive. 
Based on these results, we expect that for 
languages where we do not have an NLPWin 
parser, we can safely exclude the deeper 
linguistic features and still expect good 
classifier performance. 
 
 
Figure 2 illustrates the accuracy of 
distinguishing messages that contain tasks from 
those that do not, using all features. A message 
was marked as containing a task if it contained 
at least one sentence classified as a task. Since 
only one task has to be found in order for the 
entire message to be classified as containing a 
task, accuracy is substantially higher than on a 
per-sentence basis. In section 6, we discuss the 
scenarios motivating the distinction between 
sentence classification and message 
classification. 
 
 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
Pr
ec
is
io
n
All features
No deep linguistic features
No linguistic features
 
 
Figure 1: Precision-Recall curves for ablation experiments 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
P
re
ci
si
on
Per sentence
Per message
 
 
Figure 2: Precision-Recall curves comparing message classification and sentence classification 
 
 
5 Reformulation of Tasks 
SmartMail performs post-processing of sentences 
identified as containing a task to reformulate them 
as task-like imperatives. The process of 
reformulation involves four distinct knowledge-
engineered steps:  
1. Produce a logical form (LF) for the 
extracted sentence (Campbell and Suzuki, 
2001). The nodes of the LF correspond to 
syntactic constituents. Edges in the LF 
represent semantic and deep syntactic 
relations among nodes. Nodes bear 
semantic features such as tense, number 
and mood. 
2. Identify the clause in the logical form that 
contains the task; this may be the entire 
sentence or a subpart. We consider such 
linguistic properties as whether the clause 
is imperative, whether its subject is second 
person, and whether modality words such 
as please or a modal verb are used. All 
parts of the logical form not subsumed by 
the task clause are pruned. 
3. Transform the task portion of the LF to 
exclude extraneous words (e.g. please, 
must, could), extraneous subordinate 
clauses, adverbial modifiers, and vocative 
phrases. We replace certain deictic 
elements (i.e., words or phrases whose 
denotation varies according to the writer or 
the time and place of utterance) with non-
deictic expressions. For example, first 
person pronouns are replaced by either the 
name of the sender of the email or by a 
third person pronoun, if such a pronoun 
would unambiguously refer to the sender. 
Similarly, a temporal expression such as 
Thursday, which may refer to a different 
date depending on the week in which it is 
written, is replaced by an absolute date 
(e.g., 4/1/2004). 
4. Pass the transformed LF to a sentence 
realization module to yield a string 
(Aikawa et al, 2001). 
Below we illustrate the reformulation of tasks with 
some examples from our corpus. 
 
Example 1: 
On the H-1 visa issue, I am 
positive that you need to go 
to the Embassy in London to 
get your visa stamped into 
your passport. 
Reformulation: 
Go to the Embassy in London to 
get your visa stamped into 
your passport. 
 
In this example, the embedded sentential 
complement, that is, the part of the sentence 
following positive, is selected as the part of the 
sentence containing the task, because of the modal 
verb need and the second person subject; only that 
part of the sentence gets reformulated. The modal 
verb and the second person subject are deleted to 
form an imperative sentence. 
 
Example 2: 
Can you please send me the 
follow up information for the 
demo(s) listed in this Email 
ASAP. 
Reformulation: 
Send Kendall the follow up 
information for the demo 
listed in this Email ASAP. 
 
In this example, the whole sentence is selected 
as containing the task (modal verb, second person 
subject); modal elements including please are 
deleted along with the second person subject to 
form an imperative. In addition, the first person 
pronoun me is replaced by a reference to the 
sender, Kendall in this instance. 
 
Example 3: 
I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way). 
Reformulation: 
On June 5, 2002 Pablo wrote: 
?I've been Wednesday at the 
lecture on Amalgam you gave in 
the 113/1021 Room (which I 
really liked), and I've been 
wondering how feasible would 
it be to use Amalgam for 
learning requirements or code 
corpus structures and rules 
(and eventually rephrase them 
in some way).' 
 
This example illustrates what happens when 
NLPWin is unable to produce a spanning parse and 
hence a coherent LF; in this case NLPWin 
misanalyzed the clause following wondering as a 
main clause, instead of correctly analyzing it as a 
complement clause. SmartMail?s back-off strategy 
for non-spanning parses is to enclose the entire 
original sentence in quotes, prefixed with a matrix 
sentence indicating the date and the name of the 
sender. 
 
 
6 Task-Focused Summarization 
We have considered several scenarios for 
presenting the tasks that SmartMail identifies. 
Under the most radical scenario, SmartMail would 
automatically add extracted tasks to the user?s ?to 
do? list. This scenario has received a fairly 
negative reception when we have suggested it to 
potential users of a prototype. From an application 
perspective, this scenario is ?fail hard?; i.e., 
classification errors might result in garbage being 
added to the ?to do? list, with the result that the 
user would have to manually remove items. Since 
our goal is to reduce the workload on the user, this 
outcome would seem to violate the maxim ?First, 
do no harm?. 
 
Figure 3 and Figure 4 illustrate several ideas for 
presenting tasks to the user of Microsoft Outlook. 
Messages that contain tasks are flagged, using the 
existing flag icons in Outlook for proof of concept. 
Users can sort mail to see all messages containing 
tasks. This visualization amounts to summarizing 
the message down to one bit, i.e., +/- Task, and is 
conceptually equivalent to performing document 
classification. 
The right-hand pane in Figure 3 is magnified as 
Figure 4 and shows two more visualizations. At the 
top of the pane, the tasks that have been identified 
are presented in one place, with a check box beside 
them. Checking the box adds the task to the Tasks 
or ?to do? list, with a link back to the original 
message. This presentation is ?fail soft?: the user 
can ignore incorrectly classified tasks, or tasks that 
were correctly identified but which the user does 
not care to add to the ?to do? list. This list of tasks 
amounts to a task-focused summary of the 
document. This summary is intended to be read as 
a series of disconnected sentences, thus side-
stepping the issue of producing a coherent text 
from a series of extracted sentences. In the event 
that users prefer to view these extracted sentences 
as a coherent text, it may prove desirable to 
attempt to improve the textual cohesion by using 
anaphoric links, cue phrases and so on. 
Finally, Figure 3 also shows tasks highlighted in 
context in the message, allowing the user to skim 
the document and read the surrounding text. 
In the prototype we allow the user to vary the 
precision and recall of the classifier by adjusting a 
slider (not illustrated here) that sets the probability 
threshold on the probability of Task. 
 
Figure 3 and Figure 4 illustrate a convention that 
we observed in a handful of emails: proper names 
occur as section headings. These names have scope 
over the tasks enumerated beneath them, i.e. there 
is a list of tasks assigned to Matt, a list assigned to 
Eric or Mo, and a list assigned to Mo. SmartMail 
does not currently detect this explicit assignment 
of tasks to individuals. 
Important properties of tasks beyond the text of 
the message could also be automatically extracted. 
For example, the schema for tasks in Outlook 
includes a field that specifies the due date of the 
task. This field could be filled with date and time 
information extracted from the sentence containing 
the task. Similarly the content of the sentence 
containing the task or inferences about social 
relationships of the email interlocutors could be 
used to mark the priority of tasks as High, Low, or 
Normal in the existing schema. 
7 Conclusion 
In this paper we have presented aspects of 
SmartMail, which provides a task-oriented 
summary of email messages. This summary is 
produced by identifying the task-related sentences 
in the message and then reformulating each task-
related sentence as a brief (usually imperative) 
summation of the task. The set of tasks extracted 
and reformulated from a given email message is 
thus a task-focused summary of that message. 
We plan to conduct user studies by distributing 
the prototype as an Outlook add-in to volunteers 
who would use it to read and process their own 
mail over a period of several weeks. We intend to 
measure more than the precision and recall of our 
classifier by observing how many identified tasks 
users actually add to their ?to do? list and by 
administering qualitative surveys of user 
satisfaction. 
The ability to reformulate tasks is in principle 
separate from the identification of tasks. In our 
planned usability study we will distribute variants 
of the prototype to determine the effect of 
reformulation. Do users prefer to be presented with 
the extracted sentences with no additional 
processing, the tasks reformulated as described in 
Section 5, or an even more radical reformulation to 
a telegraphic form consisting of a verb plus object, 
such as Send information or Schedule subjects? 
 
 
 
 
 
 
 
Figure 3: Prototype system showing ways of visualizing tasks 
 
 
Figure 4: Magnified view of prototype system showing message with enumerated tasks 
 
 
 
8  Acknowledgements 
Many of the ideas presented here were formulated 
in discussion with Bob Atkinson, Dave Reed and 
Malcolm Pearson. Our thanks go to Jeff 
Stevenson, Margaret Salome and Kevin Gaughen 
for annotating the data. 
References 
Aikawa, Takako, Maite Melero, Lee Schwartz and 
Andi Wu. 2001. Multilingual natural language 
generation. EAMT. 
Bellotti, Victoria, Nicolas Ducheneaut, Mark 
Howard , Ian Smith. 2003. Taking email to 
task: the design and evaluation of a task 
management centered email tool. Proceedings 
of the conference on human factors in 
computing systems, pages 345-352. 
Cadiz, J. J., Dabbish, L., Gupta, A., & Venolia, G. 
D. 2001. Supporting email workflow. MSR-TR-
2001-88: Microsoft Research. 
Campbell, Richard and Hisami Suzuki. 2002. 
Language neutral representation of syntactic 
structure. Proceedings of SCANALU 2002. 
Dumais, Susan, John Platt, David Heckerman, 
Mehran Sahami 1998: Inductive learning 
algorithms and representations for text 
categorization. Proceedings of CIKM-98, pages 
148-155. 
Heidorn, George. 2000. Intelligent writing 
assistance. In R. Dale, H. Moisl and H. Somers, 
(eds.), Handbook of Natural Language 
Processing. Marcel Dekker. 
Joachims, Thorsten. 1998. Text categorization 
with support vector machines: Learning with 
many relevant features. Proceedings of ECML 
1998, pages 137-142. 
Mast, M., Kompe, R., Harbeck, S., Kiessling, A., 
Niemann, H., N?th, E., Schukat-Talamazzini, 
E. G. and Warnke., V. 1996. Dialog act 
classification with the help of prosody. ICSLP 
96. 
Platt, John. 1999. Fast training of SVMs using 
sequential minimal optimization. In B. 
Schoelkopf, C. Burges and A. Smola (eds.) 
Advances in Kernel Methods: Support Vector 
Learning, pages 185-208, MIT Press, 
Cambridge, MA.  
Stolcke, A., E. Shriberg, R. Bates, N. Coccaro, D. 
Jurafsky, R. Martin, M. Meteer, K. Ries, P. 
Taylor and C. Van Ess-Dykema. 1998. Dialog 
act modeling for conversational speech. 
Proceedings of the AAAI-98 Spring Symposium 
on Applying Machine Learning to Discourse 
Processing.  
Vapnik, V. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Warnke, V., R. Kompe, H. Niemann and E. N?th. 
1997. Integrated dialog act segmentation and 
classification using prosodic features and 
language models. Proc. European Conf. on 
Speech Communication and Technology, vol 1, 
pages 207?210. 
 
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 57?64,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic identification of sentiment vocabulary: exploiting low associa-
tion with known sentiment terms 
 
 
Michael Gamon Anthony Aue 
Natural Language Processing Group Natural Language Processing Group 
Microsoft Research Microsoft Research 
mgamon@microsoft.com anthaue@microsoft.com 
 
 
 
 
Abstract 
We describe an extension to the technique 
for the automatic identification and label-
ing of sentiment terms described in Tur-
ney (2002) and Turney and Littman 
(2002). Their basic assumption is that 
sentiment terms of similar orientation 
tend to co-occur at the document level. 
We add a second assumption, namely that 
sentiment terms of opposite orientation 
tend not to co-occur at the sentence level. 
This additional assumption allows us to 
identify sentiment-bearing terms very re-
liably. We then use these newly identified 
terms in various scenarios for the senti-
ment classification of sentences. We show 
that our approach outperforms Turney?s 
original approach. Combining our ap-
proach with a Naive Bayes bootstrapping 
method yields a further small improve-
ment of classifier performance. We finally 
compare our results to precision and recall 
figures that can be obtained on the same 
data set with labeled data. 
1 Introduction 
The field of sentiment classification has received 
considerable attention from researchers in recent 
years (Pang and Lee 2002, Pang et al 2004, Tur-
ney 2002, Turney and Littman 2002, Wiebe et al 
2001, Bai et al 2004, Yu and Hatzivassiloglou 
2003 and many others). The identification and 
classification of sentiment constitutes a problem 
that is orthogonal to the usual task of text classifi-
cation. Whereas in traditional text classification the 
focus is on topic identification, in sentiment classi-
fication the focus is on the assessment of the 
writer?s sentiment toward the topic. 
Movie and product reviews have been the main 
focus of many of the recent studies in this area 
(Pang and Lee 2002, Pang et al 2004, Turney 
2002, Turney and Littman 2002). Typically, these 
reviews are classified at the document level, and 
the class labels are ?positive? and ?negative?. In 
this work, in contrast, we narrow the scope of in-
vestigation to the sentence level and expand the set 
of labels, making a threefold distinction between 
?positive?, ?neutral?, and ?negative?. The narrow-
ing of scope is motivated by the fact that for realis-
tic text mining on customer feedback, the 
document level is too coarse, as described in Ga-
mon et al (2005). The expansion of the label set is 
also motivated by real-world concerns; while it is a 
given that review text expresses positive or nega-
tive sentiment, in many cases it is necessary to also 
identify the cases that don?t carry strong expres-
sions of sentiment at all. 
Traditional approaches to text classification re-
quire large amounts of labeled training data. Ac-
quisition of such data can be costly and time-
consuming. Due to the highly domain-specific na-
ture of the sentiment classification task, moving 
from one domain to another typically requires the 
acquisition of a new set of training data. For this 
reason, unsupervised or very weakly supervised 
methods for sentiment classification are especially 
57
desirable.1 Our focus, therefore, is on methods that 
require very little data annotation. 
We describe a method to automatically identify 
the sentiment vocabulary in a domain. This method 
rests on three special properties of the sentiment 
domain: 
1. the presence of certain words can serve as 
a proxy for the class label 
2. sentiment terms of similar orientation tend 
to co-occur 
3. sentiment terms of opposite orientation 
tend to not co-occur at the sentence level. 
Turney (2002) and Turney and Littman (2002) 
exploit the first two generalizations for unsuper-
vised sentiment classification of movie reviews. 
They use the two terms excellent and poor as seed 
terms to determine the semantic orientation of 
other terms. These seed terms can be viewed as 
proxies for the class labels ?positive? and ?nega-
tive?, allowing for the exploitation of otherwise 
unlabeled data: Terms that tend to co-occur with 
excellent in documents tend to be of positive orien-
tation, and vice versa for poor. Turney (2002) 
starts from a small (2 word) set of terms with 
known orientation (excellent and poor). Given a set 
of terms with unknown sentiment orientation, Tur-
ney (2002) then uses the PMI-IR algorithm (Tur-
ney 2001) to issue queries to the web and 
determine, for each of these terms, its pointwise 
mutual information (PMI) with the two seed words 
across a large set of documents. Term candidates 
are constrained to be adjectives, which tend to be 
the strongest bearers of sentiment. The sentiment 
orientation (SO) of a term is then determined by 
the difference between its association (PMI) with 
the positive seed term excellent and its association 
with the negative seed term poor. The resulting list 
of terms and associated sentiment orientations can 
then be used to implement a classifier: semantic 
orientation of the terms in a document of unknown 
sentiment is added up, and if the overall score is 
positive, the document is classified as being of 
positive sentiment, otherwise it is classified as 
negative. 
Yu and Hatzivassiloglou (2003) extend this ap-
proach by (1) applying it at the sentence level (in-
stead of the document-level), (2) taking into 
account non-adjectival parts-of-speech, and (3) 
                                                          
1
 For domain-specificity of sentiment classification see Eng-
str?m (2004) and Aue and Gamon (2005). 
using larger sets of seed words. Their classification 
goal also differs from Turney?s: it is to distinguish 
opinion sentences from factual statements. 
Turney et al?s approach is based on the assump-
tion that sentiment terms of similar orientation tend 
to co-occur in documents. Our approach takes ad-
vantage of a second assumption: At the sentence 
level, sentiment terms of opposite orientation tend 
not to co-occur. This is, of course, an assumption 
that will only hold in general, with exceptions. Ba-
sically, the assumption is that sentences of the fol-
lowing form: 
I dislike X. 
I really like X. 
are more frequent than ?mixed sentiment? sen-
tences such as 
I dislike X but I really like Y. 
It has been our experience that this generaliza-
tion does hold often enough to be useful. 
We propose to utilize this assumption to identify 
a set of sentiment terms in a domain. We select the 
terms that have the lowest PMI scores on the sen-
tence level with respect to a set of manually se-
lected seed words. If our assumption about low 
association at the sentence level is correct, this set 
of low-scoring terms will be particularly rich in 
sentiment terms. We can then use this newly iden-
tified set to: 
(1) use Turney?s method to find the orienta-
tion for the terms and employ the terms 
and their scores in a classifier, and 
(2) use Turney?s method to find the orienta-
tion for the terms and add the new terms 
as additional seed terms for a second it-
eration 
As opposed to Turney (2002), we do not use the 
web as a resource to find associations, rather we 
apply the method directly to in-domain data. This 
has the disadvantage of not being able to apply the 
classification to any arbitrary domain. It is worth 
noting, however, that even in Turney (2002) the 
choice of seed words is explicitly motivated by 
domain properties of movie reviews. 
In the remainder of the paper we will describe 
results from various experiments based on this as-
sumption. We also show how we can combine this 
method with a Naive Bayes bootstrapping ap-
proach that takes further advantage of the unla-
beled data (Nigam et al 2000). 
58
2 Data 
For our experiments we used a set of car reviews 
from the MSN Autos web site. The data consist of 
406,818 customer car reviews written over a four-
year period. Aside from filtering out examples con-
taining profanity, the data was not edited. The re-
views range in length from a single sentence (56% 
of all cases) to 50 sentences (a single review). Less 
than 1% of reviews contain ten or more sentences. 
There are almost 900,000 sentences in total. When 
customers submitted reviews to the website, they 
were asked for a recommendation on a scale of 1 
(negative) to 10 (positive). The average score was 
very high, at 8.3, yielding a strong skew in favor of 
positive class labels. We annotated a randomly-
selected sample of 3,000 sentences for sentiment. 
Each sentence was viewed in isolation and classi-
fied as positive, negative or neutral. The neutral 
category was applied to sentences with no dis-
cernible sentiment, as well as to sentences that ex-
pressed both positive and negative sentiment. 
Three annotators had pair-wise agreement scores 
(Cohen?s Kappa score, Cohen 1960) of 70.10%, 
71.78% and 79.93%, suggesting that the task of 
sentiment classification on the sentence level is 
feasible but difficult even for people. This set of 
data was split into a development test set of 400 
sentences and a blind test set of 2600 sentences. 
Sentences are represented as vectors of binary 
unigram features. The total number of observed 
unigram features is 72988. In order to restrict the 
number of features to a manageable size, we disre-
gard features that occur less than 10 times in the 
corpus. With this restriction we obtain a reduced 
feature set of 13317 features. 
3 Experimental Setup 
Our experiments were performed as follows: We 
started with a small set of manually-selected and 
annotated seed terms. We used 4 positive and 6 
negative seed terms. We decided to use a few more 
negative seed words because of the inherent posi-
tive skew in the data that makes the identification 
of negative sentences particularly hard. The terms 
we used are: 
 positive: negative: 
 good  bad 
 excellent lousy 
 love  terrible 
 happy  hate 
   suck 
   unreliable 
There was no tuning of the set of initial seed 
terms; the 10 words were originally chosen intui-
tively, as words that we observed frequently when 
manually inspecting the data. 
We then used these seed terms in two basic 
ways: (1) We used them as seeds for a Turney-
style determination of the semantic orientation of 
words in the corpus (semantic orientation, or SO 
method). As mentioned above, this process is 
based on the assumption that terms of similar ori-
entation tend to co-occur. (2) We used them to 
mine sentiment vocabulary from the unlabeled data 
using the additional assumption that sentiment 
terms of opposite orientation tend not to co-occur 
at the sentence level (sentiment mining, or SM 
method). This method yields a set of sentiment 
terms, but no orientation for that set of terms. We 
continue by using the SO method to find the se-
mantic orientation for this set of sentiment terms, 
effectively using SM as a feature selection method 
for sentiment terminology. 
Pseudo-code for the SO and SM approaches is 
provided in Figure 1 and Figure 2. As a first step 
for both SO and SM methods (not shown in the 
pseudocode), PMI needs to be calculated for each 
pair (f, s) of feature f and seed word s over the col-
lection of feature vectors. 
 
Figure 1: SO method for determining semantic orienta-
tion 
59
 Figure 2: SM method for mining sentiment terms 
In the first scenario (using straightforward SO), 
features F range over all observed features in the 
data (modulo the aforementioned count cutoff of 
10). In the second scenario (SM + SO), features F 
range over the n% of features with the lowest PMI 
scores with respect to any of the seed words that 
were identified using the sentiment mining tech-
nique in Figure 2. 
The result of both SO and SM+SO is a list of 
unigram features which have an associated seman-
tic orientation score, indicating their sentiment ori-
entation: the higher the score, the more ?positive? 
a term, and vice versa. 
This list of features and associated scores can be 
used to construct a simple classifier: for each sen-
tence with unknown sentiment, we take the sum of 
the semantic orientation scores for all of the uni-
grams in that sentence. This overall score deter-
mines the classification of the sentence as 
?positive?, ?neutral? or ?negative? as shown in 
Figure 3. 
Scoring and classifying sentence vectors:
(1) assigning a sentence score:
FOREACH feature f in sentence vector v:
Score(v) = Score(v) + SO(f)
(2) assigning a class label based on the sentence score:
IF Score(v) > threshold1:
Class(v) = ?positive? 
ELSE IF Score(v) < threshold1 AND Score(v) > threshold2:
Class(v) = ?neutral?
ELSE
Class(v) = ?negative?
 
Figure 3: Using SO scores for sentence scoring and 
classification 
The two thresholds used in classification need to 
be determined empirically by taking the distribu-
tion of class values in the corpus into account. For 
our experiments we simply took the distribution of 
class labels in the 400 sentence development test 
set as an approximation of the overall class label 
distribution: we determined that distribution to be 
15.5% for negative sentences, 21.5% for neutral 
sentences, and 63.0% for positive sentences. 
Scores for all sentence vectors in the corpus are 
then collected using the scoring part of the algo-
rithm in Figure 3. The scores are sorted and the 
thresholds are determined as the cutoffs for the top 
63% and bottom 15.5% of scores respectively. 
4 Results 
4.1. Comparing SO and SM+SO 
In our first set of experiments we manipulated the 
following parameters: 
1. the choice of SO or SM+SO method 
2. the choice of n when selecting the n% se-
mantic terms with lowest PMI score in the 
SM method 
The tables below show the results of classifying 
sentence vectors using the unigram features and 
associated scores produced by SO and SO+SM. 
We used the 2,600-sentence manually-annotated 
test set described previously to establish these 
numbers. Since the data exhibit a strong skew in 
favor of the positive class label, we measure per-
formance not in terms of accuracy but in terms of 
average precision and recall across the three class 
labels, as suggested in (Manning and Sch?tze 
2002). 
 Avg precision Avg recall 
SO  0.4481 0.4511 
Table 1: Using the SO approach. 
Table 1 shows results of using the SO method 
on the data. Table 2 presents the results of combin-
ing the SM and SO methods for different values of 
n. The best results are shown in boldface. 
As a comparison between Table 1 and Table 2 
shows, the highest average precision and recall 
scores were obtained by combining the SM and SO 
methods. Using SM as a feature selection mecha-
nism also reduces the number of features signifi-
cantly. While the SO method employed on 
sentence-level vectors uses 13,000 features, the 
best-performing SM+SO combination uses only 
20% of this feature set, indicating that SM is in-
deed effective in selecting the most important sen-
timent-bearing terms. 
60
We also determined that the positive impact of 
SM is not just a matter of reducing the number of 
features. If SO - without the SM feature selection 
step - is reduced to a comparable number of fea-
tures by taking the top features according to abso-
lute score, average precision is at 0.4445 and 
average recall at 0.4464. 
N=10 N=20 N=30 N=40 N=50  
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
Avg 
prec 
Avg 
rec 
SM+SO 
SO from 
docu-
ment 
level 
0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475 
Table 2: combining SM and SO. 
Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms 
excellent, terrible, broke, junk, alright, bargain, 
grin, highest, exceptional, exceeded, horrible, 
loved, waste, ok, death, leaking, outstanding, 
cracked, rebate, warped, hooked, sorry, refuses, 
excellant, satisfying, died, biggest, competitive, 
delight, avoid, awful, garbage, loud, okay, com-
petent, upscale, dated, mistake, sucks, superior, 
high, kill, neither 
excellent, happy, stylish, sporty, smooth, love, 
quiet, overall, pleased, plenty, dependable, solid, 
roomy, safe, good, easy, smaller, luxury, comfort-
able, style, loaded, space, classy, handling, joy, 
small, comfort, size, perfect, performance, room, 
choice, recommended, package, compliments, 
awesome, unique, fun, holds, comfortably, ex-
tremely, value, free, satisfied, little, recommend, 
limited, great, pleasure 
Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms 
alternative, wont, below, surprisingly, main-
tained, choosing, comparing, legal, vibration, 
seemed, claim, demands, assistance, knew, engi-
neering, accelleration, ended, salesperson, per-
formed, started, midsize, site, gonna, lets, plugs, 
industry, alternator, month, told, vette, 180, 
powertrain, write, mos, walk, causing, lift, es, 
segment, $250, 300m, wanna, february, mod, 
$50, nhtsa, suburbans, manufactured, tiburon, 
$10, f150, 5000, posted, tt, him, saw, jan,  
condition, very, handles, milage, definitely, defi-
nately, far, drives, shape, color, price, provides, 
options, driving, rides, sports, heated, ride, sport, 
forward, expected, fairly, anyone, test, fits, stor-
age, range, family, sedan, trunk, young, weve, 
black, college, suv, midsize, coupe, 30, shopping, 
kids, player, saturn, bose, truck, town, am, leather, 
stereo, car, husband 
Table 3: the top 100 terms identified by SM and SO
Table 3 shows the top 100 terms that were identi-
fied by each SM and SO methods. The terms are 
categorized into sentiment-bearing and non-
sentiment bearing terms by human judgment. The 
two sets seem to differ in both strength and orien-
tation of the identified terms. The SM-identified 
words have a higher density of negative terms (22 
out of 43 versus 2 out of 49 for the SO-identified 
terms). The SM-identified terms also express sen-
timent more strongly, but this conclusion is more 
tentative since it may be a consequence of the 
higher density of negative terms. 
4.2. Multiple iterations: increasing the 
number of seed features by SM+SO 
In a second set of experiments, we assessed the 
question of whether it is possible to use multiple 
iterations of the SM+SO method to gradually build 
the list of seed words. We do this by adding the top 
n% of features selected by SM, along with their 
orientation as determined by SO, to the initial set 
of seed words. The procedure for this round of ex-
periments is as follows: 
? take the top n% of features identified by 
SM (we used n=1 for the reported re-
61
sults, since preliminary experiments 
with other values for n did not improve 
results) 
? perform SO for these features to deter-
mine their orientation 
? take the top 15.5% negative and top 
63% positive (according to class label 
distribution in the development test set) 
of the features and add them as nega-
tive/positive seed features respectively 
This iteration increases the number of seed fea-
tures from the original 10 manually-selected fea-
tures to a total of 111 seed features. 
With this enhanced set of seed features we then 
re-ran a subset of the experiments in Table 2. Re-
sults are shown in Table 4. Increasing the number 
of seed features through the SM feature selection 
method increases precision and recall by several 
percentage points. In particular, precision and re-
call for negative sentences are boosted. 
 
Avg 
precision 
Avg 
recall 
SM + SO, n=10, 
SO from document vectors 0.4826 0.48.76 
SM + SO, n=30, 
SO from document vectors 0.4957 0.4995 
SM + SO, n=50, 
SO from document vectors 0.4914 0.4952 
Table 4: Using 2 iterations to increase the seed feature 
set 
We also confirmed that these results are truly at-
tributable to the use of the SM method for the first 
iteration. If we take an equivalent number of fea-
tures with strongest semantic orientation according 
to the SO method and add them to the list of seed 
features, our results degrade significantly (the re-
sulting classifier performance is significantly dif-
ferent at the 99.9% level as established by the 
McNemar test). This is further evidence that SM is 
indeed an effective method for selecting sentiment 
terms. 
4.3. Using the SO classifier to bootstrap a 
Naive Bayes classifier 
In a third set of experiments, we tried to improve 
on the results of the SO classifier by combining it 
with the bootstrapping approach described in (Ni-
gam et al 2000). The basic idea here is to use the 
SO classifier to label a subset of the data DL. This 
labeled subset of the data is then used to bootstrap 
a Naive Bayes (NB) classifier on the remaining 
unlabeled data DU using the Expectation Maximi-
zation (EM) algorithm: 
(1) An initial naive Bayes classifier with 
parameters ? is trained on the docu-
ments in DL. 
(2) This initial classifier is used to estimate 
a probability distribution over all classes 
for each of the documents in DU. (E-
Step) 
(3) The labeled and unlabeled data are then 
used to estimate parameters for a new 
classifier. (M-Step) 
Steps 2 and 3 are repeated until convergence is 
achieved when the difference in the joint probabil-
ity of the data and the parameters falls below the 
configurable threshold ? between iterations. An-
other free parameter, ?, can be used to control how 
much weight is given to the unlabeled data. 
For our experiments we used classifiers from the 
best SM+SO combination (2 iterations at n=30) 
from Table 4 above to label 30% of the total data. 
Table 5 shows the average precision and recall 
numbers for the converged NB classifier.2 In addi-
tion to improving average precision and recall, the 
resulting classifier also has the advantage of pro-
ducing class probabilities instead of simple scores.3 
 Avg 
precision 
Avg 
recall 
Bootstrapped NB 
classifier 0.5167 0.52 
Table 5: Results obtained by bootstrapping a NB classi-
fier 
4.4. Results from supervised learning: 
using small sets of labeled data 
Given infinite resources, we can always annotate 
enough data to train a classifier using a supervised 
algorithm that will outperform unsupervised or 
weakly-supervised methods. Which approach to 
take depends entirely on how much time and 
money are available and on the accuracy require-
ments for the task at hand. 
                                                          
2
 In this experiment, ? was set to 0.1 and ? was set to 0.05. 
3
 We also experimented with labeling the whole data set with the best of our SO 
score classifiers, and then training a linear Support Vector Machine classifier on 
the data. The results were considerably worse than any of the reported numbers, 
so they are not included in this paper. 
62
To help situate the precision and recall numbers 
presented in the tables above, we trained Support 
Vector Machines (SVMs) using small amounts of 
labeled data. SVMs were trained with 500, 1000, 
2000, and 2500 labeled sentences. Annotating 
2500 sentences represents approximately eight per-
son-hours of work. The results can be found in Ta-
ble 5. We were pleasantly surprised at how well 
the unsupervised classifiers described above per-
form in comparison to state-of-the-art supervised 
methods (albeit trained on small amounts of data). 
Labeled ex-
amples 
Avg. Preci-
sion 
Avg. Recall 
500 .4878 .4967 
1000 .5161 .5105 
2000 .5297 .5256 
2500 .5017 .5083 
Table 6: Average precision and recall for SVMs for 
small numbers of labeled examples  
4.5. Results on the movie domain 
We also performed a small set of experiments on 
the movie domain using Pang and Lee?s 2004 data 
set. This set consists of 2000 reviews, 1000 each of 
very positive and very negative reviews. Since this 
data set is balanced and the task is only a two-way 
classification between positive and negative re-
views, we only report accuracy numbers here. 
 accuracy Training data 
Turney 
(2002) 66% unsupervised 
Pang & Lee 
(2004) 87.15% supervised 
Aue & Ga-
mon (2005) 91.4% supervised 
SO 73.95% unsupervised 
SM+SO to 
increase seed 
words, then 
SO 
74.85% weakly super-
vised 
Table 7: Classification accuracy on the movie review 
domain 
Turney (2002) achieves 66% accuracy on the 
movie review domain using the PMI-IR algorithm 
to gather association scores from the web. Pang 
and Lee (2004) report 87.15% accuracy using a 
unigram-based SVM classifier combined with sub-
jectivity detection. Aue and Gamon (2005) use a 
simple linear SVM classifier based on unigrams, 
combined with LLR-based feature reduction, to 
achieve 91.4% accuracy. Using the Turney SO 
method on in-domain data instead of web data 
achieves 73.95% accuracy (using the same two 
seed words that Turney does). Using one iteration 
of SM+SO to increase the number of seed words, 
followed by finding SO scores for all words with 
respect to the enhanced seed word set, yields a 
slightly higher accuracy of 74.85%. With addi-
tional parameter tuning, this number can be pushed 
to 76.4%, at which point we achieve statistical sig-
nificance at the 0.95 level according to the McNe-
mar test, indicating that there is more room here 
for improvement. Any reduction of the number of 
overall features in this domain leads to decreased 
accuracy, contrary to what we observed in the car 
review domain. We attribute this observation to the 
smaller data set. 
5 Discussion 
5.1 A note on statistical significance 
We used the McNemar test to assess whether two 
classifiers are performing significantly differently. 
This test establishes whether the accuracy of two 
classifiers differs significantly - it does not guaran-
tee significance for precision and recall differ-
ences. For the latter, other tests have been 
proposed (e.g. Chinchor 1995), but time con-
straints prohibited us from implementing any of 
those more computationally costly tests. 
For the results presented in the previous sections 
the McNemar test established statistical signifi-
cance at the 0.99 level over baseline (i.e. the SO 
results in Table 1) for the multiple iterations results 
(Table 4) and the bootstrapping approach (Table 
5), but not for the SM+SO approach (Table 2). 
5.2 Future work 
This exploratory set of experiments indicates a 
number of interesting directions for future work. A 
shortcoming of the present work is the manual tun-
ing of cutoff parameters. This problem could be 
alleviated in at least two possible ways: 
First, using a general combination of the ranking 
of terms according to SM and SO. In other words, 
calculate the semantic weight of a term as a com-
bination of SO and its rank in the SM scores. 
63
Secondly, following a suggestion by an anony-
mous reviewer, the Naive Bayes bootstrapping ap-
proach could be used in a feedback loop to inform 
the SO score estimation in the absence of a manu-
ally annotated parameter tuning set. 
5.3 Summary 
Our results demonstrate that the SM method can 
serve as a valid tool to mine sentiment-rich vo-
cabulary in a domain. SM will yield a list of terms 
that are likely to have a strong sentiment orienta-
tion. SO can then be used to find the polarity for 
the selected features by association with the senti-
ment terms of known polarity in the seed word list. 
Performing this process iteratively by first enhanc-
ing the set of seed words through SM+SO yields 
the best results. While this approach does not com-
pare to the results that can be achieved by super-
vised learning with large amounts of labeled data, 
it does improve on results obtained by using SO 
alone. 
We believe that this result is relevant in two re-
spects. First, by improving average precision and 
recall on the classification task, we move closer to 
the goal of unsupervised sentiment classification. 
This is a very important goal in itself given the 
need for ?out of the box? sentiment techniques in 
business intelligence and the notorious difficulty of 
rapidly adapting to a new domain (Engstr?m 2004, 
Aue and Gamon 2005). Second, the exploratory 
results reported here may indicate a general source 
of information for feature selection in natural lan-
guage tasks: features that have a tendency to be in 
complementary distribution (especially in smaller 
linguistic units such as sentences) may often form 
a class that shares certain properties. In other 
words, it is not only the strong association scores 
that should be exploited but also the particularly 
weak (negative) associations. 
References  
Anthony Aue and Michael Gamon (2005): ?Customiz-
ing Sentiment Classifiers to a New Domain: A Case 
Study. Under review. 
Xue Bai, Rema Padman, and Edoardo Airoldi. (2004). 
Sentiment Extraction from Unstructured Text Using 
Tabu Search-Enhanced Markov Blanket. In: Proceed-
ings of the International Workshop on Mining for 
and from the Semantic Web (MSW 2004), pp 24-35. 
Nancy A. Chinchor (1995): Statistical significance of 
MUC-6 results. Proceedings of the Sixth Message 
Understanding Conference, pp. 39-44. 
J. Cohen (1960): ?A coefficient of agreement for nomi-
nal scales.? In: Educational and Psychological meas-
urements 20, pp. 37?46 
Charlotta Engstr?m. 2004. Topic dependence in Senti-
ment Classification. MPhil thesis, University of 
Cambridge. 
Michael Gamon, Anthony Aue, Simon Corston-Oliver, 
and Eric Ringger. (2005): ?Pulse: Mining Customer 
Opinions from Free Text?. Under review. 
Christopher D. Manning and Hinrich Sch?tze (2002): 
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, London. 
Kamal Nigam, Andrew McCallum, Sebastian Thrun and 
Tom Mitchell (2000): Text Classification from La-
beled and Unlabeled Documents using EM. In: Ma-
chine Learning 39 (2/3), pp. 103-134. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan 
(2002): ?Thumbs up? Sentiment Classification using 
Machine Learning Techniques?. Proceedings of 
EMNLP 2002, pp. 79-86. 
Bo Pang and Lillian Lee. (2004). A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of 
ACL 2004, pp.217-278. 
Peter D. Turney (2001): ?Mining the Web for Syno-
nyms: PMI-IR versus LSA on TOEFL.? In Proceed-
ings of the Twelfth European Conference on 
Machine Learning, pp. 491-502. 
Peter D. Turney (2002): ?Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised classi-
fication of reviews?. In: Proceedings of ACL 2002, 
pp. 417-424. 
Peter D. Turney and M. L. Littman (2002): ?Unsuper-
vised Learning of Semantic Orientation from a Hun-
dred-Billion-Word Corpus.? Technical report ERC-
1094 (NRC 44929), National Research Council of 
Canada. 
Janyce Wiebe, Theresa Wilson and Matthew Bell 
(2001): ?Identifying Collocations for Recognizing 
Opinions?. In: Proceedings of the ACL/EACL Work-
shop on Collocation. 
Hong Yu and Vasileios Hatzivassiloglou (2003): ?To-
wards Answering opinion Questions: Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion Sentences?. In: Proceedings of EMNLP 
2003. 
64
Workshop on TextGraphs, at HLT-NAACL 2006, pages 17?24,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph-Based Text Representation for Novelty Detection 
 
 Michael Gamon 
 
 Microsoft Research  
 Redmond, WA 98052  
 mgamon@microsoft.com  
   
 
 
 
 
Abstract 
We discuss several feature sets for 
novelty detection at the sentence level, 
using the data and procedure established 
in task 2 of the TREC 2004 novelty track. 
In particular, we investigate feature sets 
derived from graph representations of 
sentences and sets of sentences. We show 
that a highly connected graph produced 
by using sentence-level term distances 
and pointwise mutual information can 
serve as a source to extract features for 
novelty detection. We compare several 
feature sets based on such a graph 
representation. These feature sets allow us 
to increase the accuracy of an initial 
novelty classifier which is based on a bag-
of-word representation and KL 
divergence. The final result ties with the 
best system at TREC 2004. 
1 Introduction 
Novelty detection is the task of identifying novel 
information given a set of already accumulated 
background information. Potential applications of 
novelty detection systems are abundant, given the 
?information overload? in email, web content etc. 
Gabrilovich et al(2004), for example, describe a 
scenario in which a newsfeed is personalized based 
on a measure of information novelty: the user can 
be presented with pieces of information that are 
novel, given the documents that have already been 
reviewed. This will spare the user the task of 
sifting through vast amounts of duplicate and 
redundant information on a topic to find bits and 
pieces of information that are of interest. 
In 2002 TREC introduced a novelty track 
(Harman 2002), which continued ? with major 
changes ? in 2003 (Soboroff and Harman 2003) 
and 2004 (Voorhees 2004). In 2002 the task was to 
identify the set of relevant and novel sentences 
from an ordered set of documents within a TREC 
topic. Novelty was defined as ?providing new 
information that has not been found in any 
previously picked sentences?. Relevance was 
defined as ?relevant to the question or request 
made in the description section of the topic?. Inter-
annotator agreement was low (Harman 2002). 
There were 50 topics for the novelty task in 2002. 
For the 2003 novelty track a number of major 
changes were made. Relevance and novelty 
detection were separated into different tasks, 
allowing a separate evaluation of relevance 
detection and novelty detection. In the 2002 track, 
the data proved to be problematic since the 
percentage of relevant sentences in the documents 
was small. This, in turn, led to a very high 
percentage of relevant sentences being novel, 
given that amongst the small set of relevant 
sentences there was little redundancy. 50 new 
topics were created for the 2003 task, with a better 
balance of relevant and novel sentences. Slightly 
more than half of the topics dealt with ?events,? 
the rest with ?opinions.? 
The 2004 track used the same tasks, the same 
number of topics and the same split between event 
and opinion topics as the 2003 track. 
For the purpose of this paper, we are only 
concerned with novelty detection, specifically with 
task 2 of the 2004 novelty track, as described in 
more detail in the following section. 
17
The question that we investigate here is: what is 
a meaningful feature set for text representation for 
novelty detection? This is obviously a far-reaching 
and loaded question. Possibilities range from 
simple bag-of-word features to features derived 
from sophisticated linguistic representations. 
Ultimately, the question is open-ended since there 
will always be another feature or feature 
combination that could/should be exploited. For 
our experiments, we have decided to focus more 
narrowly on the usefulness of features derived 
from graph representations and we have restricted 
ourselves to representations that do not require 
linguistic analysis. Simple bag-of-word metrics 
like KL divergence establish a baseline for 
classifier performance. More sophisticated metrics 
can be defined on the basis of graph 
representations. Graph representations of text can 
be constructed without performing linguistic 
analysis, by using term distances in sentences and 
pointwise mutual information between terms to 
form edges between term-vertices. A term-distance 
based representation has been used successfully for 
a variety of tasks in Mihalcea (2004) and Mihalcea 
and Tarau (2004). 
2 Previous work 
There were 13 participants and 54 submitted runs 
for the 2004 TREC novelty track task 2. Each 
participant submitted up to five runs with different 
system configurations. Metrics and approaches 
varied widely, from purely string based approaches 
to systems that used sophisticated linguistic 
components for synonymy resolution, coreference 
resolution and named entity recognition. Many 
systems employed a thresholding approach to the 
task, defining a novelty metric and then 
determining a sentence to be novel if the threshold 
is exceeded (e.g. Blott et al 2004, Zhang et al 
2004, Abdul-Jaleel et al 2004, Eichmann et al 
2004, Erkan 2004). Thresholds are either 
determined on the 2003 data, are based on a notion 
of mean score, or are determined in an ad hoc 
manner1. Tomiyama et al(2004), similar to our 
approach, use an SVM classifier to make the 
binary classification of a sentence as novel or not. 
The baseline result for the 2004 task 2 was an 
average F-measure of 0.577. This baseline is 
                                                          
1
 Unfortunately, some of the system descriptions are unclear about the exact 
rationale for choosing a particular threshold. 
achieved if all relevant sentences are categorized 
as novel. The difficulty of the novelty detection 
task is evident from the relatively low score 
achieved by even the best systems. The five best-
performing runs were: 
1. Blott et al (2004) (Dublin City 
University): using a tf.idf based metric of 
?importance value? at an ad hoc threshold: 
0.622. 
2. Tomiyama et al (2004) (Meiji University): 
using an SVM classifier trained on 2003 
data, features based on conceptual fuzzy 
sets derived from a background corpus: 
0.619. 
3. Abdul-Jaleel et al (2004) (UMass): using 
named entity recognition, using cosine 
similarity as a metric and thresholds 
derived from the 2003 data set: 0.618. 
4. Schiffman and McKeown (2004) 
(Columbia): using a combination of tests 
based on weights (derived from a 
background corpus) for previously unseen 
words with parameters trained on the 2003 
data set, and taking into account the 
novelty status of the previous sentence: 
0.617. 
5. Tomiyama et al(2004) (Meiji University): 
slight variation of the system described 
above, with one of the features (scarcity 
measure) eliminated: 0.617. 
As this list shows, there was no clear tendency 
of any particular kind of approach outperforming 
others. Among the above four systems and five 
runs, there are thresholding and classification 
approaches, systems that use background corpora 
and conceptual analysis and systems that do not. 
3 Experimental setup 
3.1 The task 
Task 2 of the 2004 novelty track is formulated as 
follows: 
Task 2: Given the relevant sentences in the 
complete document set (for a given topic), 
identify all novel sentences. 
The procedure is sequential on an ordered list of 
sentences per topic. For each Sentence Si the 
determination needs to be made whether it is novel 
given the previously seen sentences S1 through Si-1. 
18
The evaluation metric for the novelty track is F1-
measure, averaged over all 50 topics. 
3.2 Novelty detection as classification 
For the purpose of this paper we view novelty 
detection as a supervised classification task. While 
the supervised approach has its limitations in real-
life scenarios where annotated data are hard to 
come by, it can serve as a testing ground for the 
question we are interested in: the evaluation of 
feature sets and text representations. 
At training time, a feature vector is created for 
each tagged sentence S and the set of sentences 
that comprise the already seen information that S is 
compared to. Features in the vector can be features 
of the tagged sentence, features of the set of 
sentences comprising the given background 
information and features that capture a relation 
between the tagged sentence and the set of 
background sentences. A classifier is trained on the 
set of resulting feature vectors. At evaluation time, 
a feature vector is extracted from the sentence to 
be evaluated and from the set of sentences that 
form the background knowledge. The classifier 
then determines whether, given the feature values 
of that vector, the sentence is more likely to be 
novel or not. 
We use the TREC 2003 data set for training, 
since it is close to the 2004 data set in its makeup. 
We train Support Vector Machines (SVMs) on the 
2003 data, using the LibSVM tool (Chang and Lin 
2001). Following the methodology outlined in 
Chang and Lin 2003, we use radial basis function 
(RBF) kernels and perform a grid search on two-
fold cross validated results on the training set to 
identify optimal parameter settings for the penalty 
parameter C and the RBF parameter ?. 
Continuously valued features are scaled to values 
between -1 and 1. The scaling range is determined 
on the training set and the same range is applied to 
the test set. 
The text was minimally preprocessed before 
extracting features: stop words were removed, 
tokens were lowercased and punctuation was 
stripped from the strings. 
4 Text representations and features 
4.1 KL divergence as a feature 
Treating sentences as an unordered collection of 
terms, the information-theoretic metric of KL 
divergence (or relative entropy) has been 
successfully used to measure ?distance? between 
documents by simply comparing the term 
distributions in a document compared to another 
document or set of documents. The notions of 
distance and novelty are closely related: if a new 
document is very distant from the collection of 
documents that has been seen previously, it is 
likely to contain new, previously unseen 
information. Gabrilovich et al (2004), for 
example, report on a successful use of KL 
divergence for novelty detection. KL divergence is 
defined in Equation 1: 
( )( ) log ( )
d
d
w R
p wp w
p w?  
Equation 1: KL divergence. 
w belongs to the set of words that are shared 
between document d and document (set) R. pd and 
pR are the probability distributions of words in d 
and R, respectively. Both pd(w) and pR(w) need to 
be non-zero in the equation above. We used simple 
add-one smoothing to ensure non-zero values. 
While it is conceivable that KL divergence could 
take into account other features than just bag-of-
words information, we restrict ourselves to this 
particular use of the measure since it corresponds 
to the typical use in novelty detection. 
4.2 Term distance graphs: from text to 
graph without linguistic analysis 
KL divergence as described above treats a 
document or sentence as an unordered collection of 
words. Language obviously provides more 
structure than that. Linguistic resources can impose 
structure on a string of words through consultation 
of linguistic knowledge (either hand-coded or 
learned from a tagged corpus). Even without any 
outside knowledge, however, the order of words in 
a sentence provides a means to construct a highly 
connected undirected graph with the words as 
vertices. The intuition here is: 
19
1. All words in a sentence have some 
relationship to all other words in the 
sentence, modulo a ?window size? 
outside of which the relationship is not 
taken into consideration 
2. The closer two words are to each other, 
the stronger their connection tends to 
be2 
It follows from (2) that weights on the edges 
will be inversely proportional to the distance 
between two words (vertices). In the remainder of 
the paper we will refer to these graphs as TD (term 
distance) graphs. Of course (1) and (2) are rough 
generalizations with many counterexamples, but 
without the luxury of linguistic analysis this seems 
to be a reasonable step to advance beyond simple 
bag-of-word assumptions. Multiple sentence 
graphs can then be combined into a highly 
connected graph to represent text. Mihalcea (2004) 
and Mihalcea and Tarau (2004) have successfully 
explored very similar graph representations for 
extractive summarization and key word extraction. 
In addition to distance, we also employ 
pointwise mutual information as defined in 
Equation 2 between two words/vertices to enter 
into the calculation of edge weight3. This 
combination of distance and a cooccurrence 
measure such as PMI is reminiscent of decaying 
language models, as described for IR, for example, 
in Gao et al (2002)4. Cooccurrence is counted at 
the sentence level, i.e. ( , )P i j  is estimated by the 
number of sentences that contain both terms wi and 
wj, and ( )P i  and ( )P j  are estimated by counting 
the total sentences containing wi and wj, 
respectively. As the set of seen sentences grows 
and cooccurrence between words becomes more 
prevalent, PMI becomes more influential on edge 
weights, strengthening edges between words that 
have high PMI. 
( , ) 2
( , )log ( ) ( )i j
P i jPMI
P i P j=  
Equation 2: Pointwise Mutual Information (PMI) 
between two terms i and j. 
                                                          
2
 This view is supported by examining dependency structures derived from the 
Penn Tree Bank and mapping the probability of a dependency to the distance 
between words. See also Eisner and Smith (2005) who explore this 
generalization for dependency parsing. 
3
 We also computed results from a graph where the edge weight is determined 
only by term distance, without PMI. These results were consistently worse than 
the ones reported here. 
4
 We are grateful to an anonymous reviewer for pointing this out. 
Formally, the weight wt for each edge in the 
graph is defined as in Equation 3, where di,j is the 
distance between words wi and wj.and PMI(i,j) is 
the pointwise mutual information between words 
wi and wj, given the sentences seen so far. For the 
purpose of Equation 3 we ignored negative PMI 
values, i.e. we treated negative PMI values as 0. 
, 2
,
1 ( , )
i j
i j
PMI i j
wt
d
+
=  
Equation 3: Assigning weight to an edge between 
two vertices. 
We imposed a ?window size? as a limit on the 
maximum distance between two words to enter an 
edge relationship. Window size was varied 
between 3 and 8; on the training set a window size 
of 6 proved to be optimal. 
On a TD graph representation, we can calculate 
various features based on the strengths and number 
of connections between words. In novelty 
detection, we can model the growing store of 
background information by adding each 
?incoming? sentence graph to the existing 
background graph. If an ?incoming? edge already 
exists in the background graph, the weight of the 
?incoming? edge is added to the existing edge 
weight. 
Figure 1 shows a subset of a TD graph for the 
first two sentences of topic N57. The visualization 
is generated by the Pajek tool (Bagatelj and 
Mrvar). 
 
Figure 1: A subset of a TD graph of the first two 
sentences of topic N57. 
20
4.3 Graph features 
4.3.1 Simple Graph features 
In novelty detection, graph based features allow to 
assess the change a graph undergoes through the 
addition of a new sentence. The intuition behind 
these features is that the more a graph changes 
when a sentence is added, the more likely the 
added sentence is to contain novel information. 
After all, novel information may be conveyed even 
if the terms involved are not novel. Establishing a 
new relation (i.e. edge in the graph) between two 
previously seen terms would have exactly that 
effect: old terms conveying new information. KL 
divergence or any other measure of distributional 
similarity is not suited to capture this scenario. As 
an example consider a news story thread about a 
crime. The various sentences in the background 
information may mention the victim, multiple 
suspects, previous convictions, similar crimes etc. 
When a new sentence is encountered where one 
suspect?s name is mentioned in the same sentence 
with the victim, at a close distance, none of these 
two terms are new. The fact that suspect and victim 
are mentioned in one sentence, however, may 
indicate a piece of novel information: a close 
relationship between the two that did not exist in 
the background story. 
We designed 21 graph based features, based on 
the following definitions: 
? Background graph: the graph representing 
the previously seen sentences. 
? G(S): the graph of the sentence that is 
currently being evaluated.  
? Reinforced background edge: an edge that 
exists both in the background graph and 
in G(S). 
? Added background edge: a new edge in 
G(S) that connects two vertices that 
already exist in the background graph. 
? New edge: an edge in G(S) that connects 
two previously unseen vertices. 
? Connecting edge: an edge in G(S) between 
a previously unseen vertex and a 
previously seen vertex. 
The 21 features are: 
? number of new edges 
? number of added background edges 
? number of background edges 
? number of background vertices 
? number of connecting edges 
? sum of weights on new edges 
? sum of weights on added background 
edges 
? sum of weights on connecting edges 
? background connectivity (ratio between 
edges and vertices) 
? connectivity added by S 
? ratio between added background edges and 
new edges 
? ratio between new edges and connecting 
edges 
? ratio between added background edges and 
connecting edges 
? ratio between the sum of weights on new 
edges and the sum of weights on added 
background edges 
? ratio between the sum of weights on new 
edges and the sum of weights on 
connecting edges 
? ratio between the sum of weights on added 
background edges and the sum of weights 
on connecting edges 
? ratio between sum of weights on added 
background edges and the sum of pre-
existing weights on those edges 
? ratio between sum of weights on new 
edges and sum of weight on background 
edges 
? ratio between sum of weights added to 
reinforced background edges and sum of 
background weights 
? ratio between number of added 
background edges and reinforced 
background edges 
? number of background edges leading from 
those background vertices that have been 
connected to new vertices by G(S) 
We refer to this set of 21 features as simple 
graph features, to distinguish them from a second 
set of graph-based features that are based on 
TextRank. 
4.3.2 TextRank features 
The TextRank metric, as described in Mihalcea 
and Tarau (2004) is inspired by the PageRank 
21
metric which is used for web page ranking5. 
TextRank is designed to work well in text graph 
representations: it can take edge weights into 
account and it works on undirected graphs. 
TextRank calculates a weight for each vertex, 
based on Equation 4. 
( )
( )
( ) (1 ) * ( )
j i
k j
ji
i j
V NB V jk
V NB V
wt
TR V d d TR V
wt?
?
? ?? ?
= ? + ? ?? ?? ?
? ?
Equation 4: The TextRank metric. 
where TR(Vi) is the TextRank score for vertex i, 
NB(Vi) is the set of neighbors of Vi, i.e. the set of 
nodes connected to Vi by a single edge, wtxy is the 
weight of the edge between vertex x and vertex y, 
and d is a constant ?dampening factor?, set at 
0.856. To calculate TR, an initial score of 1 is 
assigned to all vertices, and the formula is applied 
iteratively until the difference in scores between 
iterations falls below a threshold of 0.0001 for all 
vertices (as in Mihalcea and Tarau 2004). 
The TextRank score itself is not particularly 
enlightening for novelty detection. It measures the 
?importance? rather than the novelty of a vertex - 
hence its usefulness in keyword extraction. We 
can, however, derive a number of features from the 
TextRank scores that measure the change in scores 
as a result of adding a sentence to the graph of the 
background information. The rationale is that the 
more the TextRank scores are ?disturbed? by the 
addition of a new sentence, the more likely it is 
that the new sentence carries novel information. 
We normalize the TextRank scores by the number 
of vertices to obtain a probability distribution. The 
features we define on the basis of the (normalized) 
TextRank metric are: 
1. sum of TR scores on the nodes of S, 
after adding S 
2. maximum TR score on any nodes of S 
3. maximum TR score on any background 
node before adding S 
4. delta between 2 and 3 
5. sum of TR scores on the background 
nodes (after adding S) 
                                                          
5
 Erkan and Radev (2005) introduced LexRank where a graph representation of a 
set of sentences is derived from the cosine similarity between sentences. 
Kurland and Lee (2004) derive a graph representation for a set of documents by 
linking documents X and Y with edges weighted by the score that a language 
model trained on X assigns to Y. 
6
 Following Mihalcea and Tarau (2004), who in turn base their default setting on 
Brin and Page (1998). 
6. delta between 5 and 1 
7. variance of the TR scores before adding 
S 
8. variance of TR scores after adding S 
9. delta between 7 and 8 
10. ratio of 1 to 5 
11. KL divergence between the TR scores 
before and after adding S 
5 Results 
To establish a baseline, we used a simple bag-of-
words approach and KL divergence as a feature for 
classification. Employing the protocol described 
above, i.e. training the classifier on the 2003 data 
set, and optimizing the parameters on 2 folds of the 
training data, we achieve a surprisingly high result 
of 0.618 average F-measure on the 2004 data. This 
result would place the run at a tie for third place 
with the UMass system in the 2004 competition. 
In the tables below, KL refers to the KL 
divergence feature, TR to the TextRank based 
features and SG to the simple graph based features. 
Given that the feature sets we investigate 
possibly capture orthogonal properties, we were 
also interested in using combinations of the three 
feature sets. For the graph based features we 
determined on the training set that results were 
optimal at a ?window size? of 6, i.e. if graph edges 
are produced only if the distance between terms is 
six tokens or less. All results are tabulated in Table 
1, with the best results boldfaced. 
Feature set Average F measure 
KL 0.618 
TR 0.600 
SG 0.619 
KL + SG 0.622 
KL + SG + TR 0.621 
SG + TR 0.615 
TR + KL 0.618 
Table 1: Performance of the different feature sets. 
We used the McNemar test to determine 
pairwise statistical significance levels between the 
novelty classifiers based on different feature sets7. 
The two (boldfaced) best results from Table 1 are 
significantly different from the baseline at 0.999 
confidence. Individual sentence level 
                                                          
7
 We could not use the Wilcoxon rank test for our results since we only had 
binary classification results for each sentence, as opposed to individual (class 
probability) scores. 
22
classifications from the official 2004 runs were not 
available to us, so we were not able to test for 
statistical significance on our results versus TREC 
results. 
6 Summary and Conclusion 
We showed that using KL divergence as a feature 
for novelty classification establishes a surprisingly 
good result at an average F-measure of 0.618, 
which would top all but 3 of the 54 runs submitted 
for task 2 in the TREC novelty track in 2004. To 
improve on this baseline we computed graph 
features from a highly connected graph built from 
sentence-level term cooccurrences with edges 
weighted by distance and pointwise mutual 
information. A set of 21 ?simple graph features? 
extracted directly from the graph perform slightly 
better than KL divergence, at 0.619 average F-
measure. We also computed TextRank features 
from the same graph representation. TextRank 
features by themselves achieve 0.600 average F-
measure. The best result is achieved by combining 
feature sets: Using a combination of KL features 
and simple graph features produces an average F-
measure of 0.622. 
Being able to establish a very high baseline with 
just the use of KL divergence as a feature was 
surprising to us: it involves a minimal approach to 
novelty detection. We believe that the high 
baseline indicates that a classification approach to 
novelty detection is promising. This is 
corroborated by the very good performance of the 
runs from Meiji University which also used a 
classifier. 
The second result, i.e. the benefit obtained by 
using graph based features was in line with our 
expectations. It is a reasonable assumption that the 
graph features would be able to add to the 
information that a feature like KL divergence can 
capture. The gains were statistically significant but 
very modest, which poses a number of questions. 
First, our feature engineering may be less than 
optimal, missing important information from a 
graph-based representation. Second, the 
classification approach may be suffering from 
inherent differences between the training data 
(TREC 2003) and the test data (TREC 2004). To 
explore this hypothesis, we trained SVMs on the 
KL + SG feature set with default settings on three 
random folds of the 2003 and 2004 data. For these 
experiments we simply measured accuracy. The 
baseline accuracy (predicting the majority class 
label) was 65.77% for the 2003 data and 58.59% 
for the 2004 data. Average accuracy for the 
threefold crossvalidation on 2003 data was 
75.72%, on the 2004 data it was 64.88%. Using the 
SVMs trained on the 2003 data on the three folds 
of the 2004 data performed below baseline at 
55.07%. These findings indicate that the 2003 data 
are indeed not an ideal fit as training material for 
the 2004 task. 
With these results indicating that graph features 
can be useful for novelty detection, the question 
becomes which graph representation is best suited 
to extract these features from. A highly connected 
term-distance based graph representation, with the 
addition of pointwise mutual information, is a 
computationally relatively cheap approach. There 
are at least two alternative graph representations 
that are worth exploring. 
First, a ?true? dependency graph that is based on 
linguistic analysis would provide a less connected 
alternative. Such a graph would, however, contain 
more information in the form of directed edges and 
edge labels (labels of semantic relations) that could 
prove useful for novelty detection. On the 
downside, it would necessarily be prone to errors 
and domain specificity in the linguistic analysis 
process. 
Second, one could use the parse matrix of a 
statistical dependency parser to create the graph 
representation. This would yield a dependency 
graph that has more edges than those coming from 
a ?1-best? dependency parse. In addition, the 
weights on the edges could be based on 
dependency probability estimates, and analysis 
errors would not be as detrimental since several 
alternative analyses enter into the graph 
representations. 
It is beyond the scope of this paper to present a 
thorough comparison between these different 
graph representations. However, we were able to 
demonstrate that a computationally simple graph 
representation, which is based solely on pointwise 
mutual information and term distance, allows us to 
successfully extract useful features for novelty 
detection. The results that can be achieved in this 
manner only present a modest gain over a simple 
approach using KL divergence as a classification 
feature. The best achieved result, however, would 
tie for first place in the 2004 TREC novelty track, 
23
in comparison to many systems which relied on 
relatively heavy analysis machinery and additional 
data resources. 
References 
Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, 
Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D. 
Smucker and Courtney Wade. 2004. UMass at TREC 
2004: Novelty and HARD. NIST Special Publication 
500-261: The Thirteenth Text REtrieval Conference 
(TREC 2004). 
Vladimir Batagelj, Andrej Mrvar: Pajek - Program for 
Large Network Analysis. Home page 
http://vlado.fmf.uni-lj.si/pub/networks/pajek/. 
Stephen Blott, Oisin Boydell, Fabrice Camous, Paul 
Ferguson, Georgina Gaughan, Cathal Gurrin, Gareth 
J. F. Jones, Noel Murphy, Noel O?Connor, Alan F. 
Smeaton, Barry Smyth, Peter Wilkins. 2004. 
Experiments in Terabyte Searching, Genomic 
Retrieval and Novelty Detection for TREC-2004. 
NIST Special Publication 500-261: The Thirteenth 
Text REtrieval Conference (TREC 2004). 
Sergey Brin and Lawrence Page. 1998. The anatomy of 
a large-scale hypertextual web search engine. 
Computer Networks and ISDN Systems 30: 107-117. 
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a 
library for support vector machines, 2001. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Chih-Chung Chang and Chih-Jen Lin. 2003. A Practical 
Guide to Support Vector Classification. 
David Eichmann, Yi Zhang, Shannon Bradshaw, Xin 
Ying Qiu, Li Zhou, Padmini Srinivasan, Aditya 
Kumar Sehgal and Hudon Wong. 2004. Novelty, 
Question Answering and Genomics: The University 
of Iowa Response. NIST Special Publication 500-
261: The Thirteenth Text REtrieval Conference 
(TREC 2004). 
Jason Eisner and Noah A. Smith. 2005. Parsing with 
Soft and Hard Constraints on Dependency Length. 
Proceedings of the International Workshop on 
Parsing Technologies (IWPT). 
G?ne? Erkan. 2004. The University of Michigan in 
Novelty 2004. NIST Special Publication 500-261: 
The Thirteenth Text REtrieval Conference (TREC 
2004). 
G?ne? Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text 
Summarization. Journal of Artificial Intelligence 
Research 22, pp. 457-479. 
Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz. 
2004. Newsjunkie: Providing Personalized 
Newsfeeds via Analysis of Information Novelty. 
WWW13, 2004. 
Jianfeng Gao, Jian-Yun Nie, Hongzhao He, Weijun 
Chena and Ming Zhou. 2002. Resolving Query 
Translation Ambiguity using a Decaying Co-
occurrence Model and Syntactic Dependency 
Relations. Proceedings of SIGIR 2002, 183-190. 
Donna Harman. 2002. Overview of the TREC 2002 
Novelty Track. NIST Special Publication 500-251: 
The Eleventh Text REtrieval Conference (TREC 
2002). 
Oren Kurland and Lillian Lee. 2004. PageRank without 
hyperlinks: structural re-ranking using links induced 
by language models. Proceedings of SIGIR 2005, pp. 
306-313. 
Rada Mihalcea. 2004. Graph-based Ranking Algorithms 
for Sentence Extraction, Applied to Text 
Summarization. Proceedings of the 42nd Annual 
Meeting of the Association for Computational 
Linguistics, companion volume (ACL 2004). 
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing Order into Texts. Proceedings of the 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2004). 
Barry Schiffman and Kathleen R. McKeown. 2004. 
Columbia University in the Novelty Track at TREC 
2004. NIST Special Publication 500-261: The 
Thirteenth Text REtrieval Conference (TREC 2004). 
Ian Soboroff and Donna Harman. 2003. Overview of the 
TREC 2003 Novelty Task. NIST Special Publication 
500-255: The Twelfth Text REtrieval Conference 
(TREC 2003). 
Tomoe Tomiyama, Kosuke Karoji, Takeshi Kondo, 
Yuichi Kakuta and Tomohiro Takagi. 2004. Meiji 
University Web, Novelty and Genomics Track 
Experiments. NIST Special Publication 500-261: The 
Thirteenth Text REtrieval Conference (TREC 2004). 
Ellen M. Voorhees. 2004. Overview of TREC 2004. 
NIST Special Publication 500-261: The Thirteenth 
Text REtrieval Conference (TREC 2004). 
Hua-Ping Zhang, Hong-Bo Xu, Shuo Bai, Bin Wang 
and Xue-Qi Cheng. 2004. Experiments in TREC 
2004 Novelty Track at CAS-ICT. NIST Special 
Publication 500-261: The Thirteenth Text REtrieval 
Conference (TREC 2004). 
24
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User Input and Interactions on Microsoft Research ESL Assistant 
 
 
Claudia Leacock Michael Gamon Chris Brockett 
Butler Hill Group Microsoft Research Microsoft Research 
P.O. Box 935 One Microsoft Way One Microsoft Way 
Ridgefield, CT, 06877, USA Redmond, WA, 98052, USA Redmond, WA, 98052, USA 
claudia.leacock@gmail.com mgamon@microsoft.com chrisbkt@microsoft.com 
 
 
 
 
 
 
Abstract 
ESL Assistant is a prototype web-based writ-
ing-assistance tool that is being developed for 
English Language Learners. The system fo-
cuses on types of errors that are typically 
made by non-native writers of American Eng-
lish. A freely-available prototype was dep-
loyed in June 2008. User data from this 
system are manually evaluated to identify 
writing domain and measure system accuracy. 
Combining the user log data with the eva-
luated rewrite suggestions enables us to de-
termine how effectively English language 
learners are using the system, across rule 
types and across writing domains. We find 
that repeat users typically make informed 
choices and can distinguish correct sugges-
tions from incorrect.  
1 Introduction 
Much current research in grammatical error detec-
tion and correction is focused on writing by Eng-
lish Language Learners (ELL). The Microsoft 
Research ESL Assistant is a web-based proofread-
ing tool designed primarily for ELLs who are na-
tive speakers of East-Asian languages. Initial 
system development was informed by pre-existing 
ELL error corpora, which were used both to identi-
fy common ELL mistakes and to evaluate system 
performance. These corpora, however, were 
created from data collected under arguably artifi-
cial classroom or examination conditions, leaving 
unresolved the more practical question as to 
whether the ESL Assistant can actually help a per-
son who produced the text to improve their English 
language writing skills in course of more realistic 
everyday writing tasks. 
In June of 2008, a prototype version of this sys-
tem was made freely available as a web service1
2 Related Work 
. 
Both the writing suggestions that visitors see and 
the actions that they then take are recorded. As 
these more realistic data begin to accumulate, we 
can now begin to answer the above question. 
Language learner error correction techniques  typi-
cally fall into either of two categories: rule-based 
or data-driven. Eeg-Olofsson and Knutsson (2003) 
report on a rule-based system that detects and cor-
rects preposition errors in non-native Swedish text. 
Rule-based approaches have also been used to pre-
dict definiteness and indefiniteness of Japanese 
noun phrases as a preprocessing step for Japanese 
to English machine translation (Murata and Nagao 
1993; Bond et al 1994; Heine, 1998), a task that is 
similar to the prediction of English articles. More 
recently, data-driven approaches have gained 
popularity and been applied to article prediction in 
English (Knight and Chander 1994; Minnen et al 
2000; Turner and Charniak 2007), to an array of 
Japanese learners? errors in English (Izumi et al 
2003), to verb errors (Lee and Seneff, 2008), and 
to article and preposition correction in texts written 
by non-native ELLs (Han et al 2004, 2006; Nagata 
et al 2005; Nagata et al 2006; De Felice and Pul-
man, 2007; Chodorow et al 2007; Gamon et al 
2008, 2009; Tetreault and Chodorow, 2008a). 
                                                          
1  http://www.eslassistant.com 
73
3 ESL Assistant 
ESL Assistant takes a hybrid approach that com-
bines statistical and rule-based techniques. Ma-
chine learning is used for those error types that are 
difficult to identify and resolve without taking into 
account complex contextual interactions, like ar-
ticle and preposition errors. Rule-based approaches 
handle those error types that are amenable to simp-
ler solutions. For example, a regular expression is 
sufficient for identifying when a modal is (incor-
rectly) followed by a tensed verb. 
The output of all modules, both machine-learned 
and rule-based, is filtered through a very large lan-
guage model. Only when the language model finds 
that the likelihood of the suggested rewrite is suffi-
ciently larger than the original text is a suggestion 
shown to the user. For a detailed description of 
ESL Assistant?s architecture, see Gamon et al
(2008, 2009). 
Although this and the systems cited in section 2 
are designed to be used by non-native writers, sys-
tem performance is typically reported in relation to 
native text ? the prediction of a preposition, for 
example, will ideally be consistent with usage in 
native, edited text. An error is counted each time 
the system predicts a token that differs from the 
observed usage and a correct prediction is counted 
each time the system predicts the usage that occurs 
in the text. Although somewhat artificial, this ap-
proach to evaluation offers the advantages of being 
fully automatable and having abundant quantities 
N
ou
n 
R
el
at
ed
 
(6
1%
) 
Articles (ML) 
We have just checked *the our stock. 
life is *journey/a journey, travel it well! 
I think it 's *a/the best way to resolve issues like this. 
Noun Number 
London is one of the most attractive *city/cities in the world. 
You have to write down all the details of each *things/thing to do. 
Conversion always takes a lot of *efforts/effort. 
Noun Of Noun 
Please send the *feedback of customer/customer feedback to me by 
mail. 
P
re
po
si
ti
on
 
R
el
at
ed
 
(2
7%
) Preposition (ML) 
I'm *on home today, call me if you have a problem.  
It seems ok and I did not pay much attention *on/to it. 
Below is my contact, looking forward *your/to your response, thanks! 
Verb and Preposition 
Ben is involved *this/in this transaction. 
I should *to ask/ask a rhetorical question ? 
But I?ll think *it/about it a second time. 
V
er
b 
R
el
at
ed
  
(1
0%
) 
Gerund / Infinitive 
(ML) 
He got me *roll/to roll up my sleeve and make a fist. 
On Saturday, I with my classmate went *eating/to eat. 
After *get/getting a visa, I want to study in New York. 
Auxiliary Verb (ML) 
To learn English we should *be speak/speak  it as much as possible . 
Hope you will *happy/be happy in Taiwan . 
what *is/do you want to say? 
Verb formation 
If yes, I will *attached/attach and resend to Geoff . 
The time and setting are *display/displayed at the same time. 
You had *order/ordered 3 items ? this time. 
I am really *hope/hoping to visit UCLA. 
Cognate/Verb Con-
fusion 
We cannot *image/imagine what the environment really is at the site 
of end user . 
Irregular Verbs I *teached/taught him all the things that I know ? 
A
dj
 
R
el
at
ed
 
(2
%
) Adjective Confu-
sions 
She is very *interesting/interested in the problem. 
So *Korea/Korean Government is intensively fostering trade . 
? and it is *much/much more reliable than your Courier Service. 
Adjective order Employing the *Chinese ancient/ancient Chinese proverb, that is  ? 
Table 1: ESL Assistant grammatical error modules.  ML modules are machine learned. 
 
74
of edited data readily available. With respect to 
prepositions and articles, the ESL Assistant's clas-
sifiers achieve state-of-the-art performance when 
compared to results reported in the literature (Ga-
mon et al 2008), inasmuch as comparison is possi-
ble when the systems are evaluated on different 
samples of native text. For articles, the system had 
86.76% accuracy as compared to 86.74% reported 
by Turner and Charniak (2007), who have the most 
recently reported results. For the harder problem of 
prepositions, ESL Assistant?s accuracy is compara-
ble to those reported by Tetreault and Chodorow 
(2008a) and De Felice and Pulman (2007).  
3.1 Error Types 
The ELL grammatical errors that  ESL Assistant 
tries to correct were distilled from analysis of the 
most frequent errors made in Chinese and Japanese 
English language learner corpora (Gui and Yang, 
2001; Izumi et al 2004). The error types are shown 
in Table 1: modules identified with ML are ma-
chine-learned, while the remaining modules are 
rule-based. ESL Assistant does not attempt to iden-
tify those errors currently found by Microsoft 
Word?, such as subject/verb agreement.  
ESL Assistant further contains a component to 
help address lexical selection issues. Since this 
module is currently undergoing major revision, we 
will not report on the results here. 
3.2 System Development  
Whereas evaluation on native writing is essential 
for system development and enables us to compare 
ESL Assistant performance with that of other re-
ported results, it tells us little about how the system 
would perform when being used by its true target 
audience ? non-native speakers of English engaged 
in real-life writing tasks. In this context, perfor-
mance measurement inevitably entails manual 
evaluation, a process that is notoriously time con-
suming, costly and potentially error-prone. Human 
inter-rater agreement is known to be problematic 
 
Figure 1: Screen shot of ESL Assistant 
 
75
on this task: it is likely to be high in the case of 
certain user error types, such as over-regularized 
verb inflection (where the system suggests replac-
ing ?writed? with ?wrote?), but other error types 
are difficult to evaluate, and much may hinge upon 
who is performing the evaluation: Tetreault and 
Chodorow (2008b) report that for the annotation of 
preposition errors ?using a single rater as a gold 
standard, there is the potential to over- or under-
estimate precision by as much as 10%.?  
With these caveats in mind, we employed a sin-
gle annotator to evaluate system performance on 
native data from the 1-million-word Chinese 
Learner?s of English corpus (Gui and Yang, 2001; 
2003). Half of the corpus was utilized to inform 
system development, while the remaining half was 
held back for "unseen" evaluation. While the abso-
lute numbers for some modules are more reliable 
than for others, the relative change in numbers 
across evaluations has proven a beneficial 
yardstick of improved or degraded performance in 
the course of development.  
3.3 The User Interface and Data Collection 
Figure 1 shows the ESL Assistant user interface. 
When a visitor to the site types or pastes text into 
the box provided and clicks the "Check" button, 
the text is sent to a server for analysis. Any loca-
tions in the text that trigger an error flag are then 
displayed as underscored with a wavy line (known 
as a "squiggle"). If the user hovers the mouse over 
a squiggle, one or more suggested rewrites are dis-
played in a dropdown list. Then, if the user hovers 
over one of these suggestions, the system launches 
parallel web searches for both original and rewrite 
phrases in order to allow the user to compare real-
word examples found on the World Wide Web. To 
accept a suggestion, the user clicks on the sug-
gested rewrite, and the text is emended. Each of 
these actions, by both system and user, are logged 
on the server. 
Since being launched in June, 2008, ESL Assis-
tant has been visited over 100,000 times. Current-
ly, the web page is being viewed between one to 
two thousand times every day. From these numbers 
alone it seems safe to conclude that there is much 
public interest in an ESL proofreading tool. 
Fifty-three percent of visitors to the ESL Assis-
tant web site are from countries in East Asia ? its 
primary target audience ? and an additional 15% 
are from the United States. Brazil, Canada, Ger-
many, and the United Kingdom each account for 
about 2% of the site?s visitors. Other countries 
represented in the database each account for 1% or 
less of all those who visit the site.  
3.4 Database of User Input 
User data are collected so that system performance 
can be evaluated on actual user input ? as opposed 
to running pre-existing learner corpora through the 
system. User data provide invaluable insight into 
which rewrite suggestions users spend time view-
ing, and what action they subsequently take on the 
basis of those suggestions.  
These data must be screened, since not all of the 
textual material entered by users in the web site is 
valid learner English language data. As with any 
publicly deployed web service, we find that nu-
merous users will play with the system, entering 
nonsense strings or copying text from elsewhere on 
the website and pasting it into the text box.  
To filter out the more obvious non-English data, 
we eliminate input that contains, for example, no 
alphabetic characters, no vowels/consonants in a 
sentence, or no white space. ?Sentences? consist-
ing of email subject lines are also removed, as are 
all the data entered by the ESL Assistant develop-
ers themselves. Since people often enter the same 
sentence many times within a session, we also re-
move repetitions of identical sentences within a 
single session.  
Approximately 90% of the people who have vi-
sited the web site visit it once and never return. 
This behavior is far from unusual on the web, 
where site visits may have no clear purpose beyond 
idle curiosity. In addition, some proportion of visi-
tors may in reality be automated "bots" that can be 
nearly indistinguishable from human visitors. 
Nevertheless, we observe a significant number 
of repeat visitors who return several times to use 
the system to proofread email or other writing, and 
these are the users that we are intrinsically interest-
ed in. To measure performance, we therefore de-
cided to evaluate on data collected from users who 
logged on and entered plausibly English-like text 
on at least four occasions. As of 2/10/2009, the 
frequent user database contained 39,944 session-
unique sentences from 578 frequent users in 5,305 
sessions.  
76
Data from these users were manually annotated 
to identify writing domains as shown in Table 2. 
Fifty-three percent of the data consists of people 
proofreading email.2
 
 The dominance of email data 
is presumably due to an Outlook plug-in that is 
available on the web site, and automates copying 
email content into the tool. The non-technical do-
main consists of student essays, material posted on 
a personal web site, or employees writing about 
their company ? for example, its history or 
processes. The technical writing is largely confe-
rence papers or dissertations in the fields of, for 
example, medicine and computer science. The 
?other? category includes lists and resumes (a writ-
ing style that deliberately omits articles and gram-
matical subjects), as well as text copied from 
online newspapers or other media and pasted in. 
Writing Domain Percent 
Email 53% 
Non-technical / essays 24% 
Technical / scientific 14% 
Other (lists, resumes, etc) 4% 
Unrelated sentences 5% 
Table 2: Writing domains of frequent users 
 
Sessions categorized as ?unrelated sentences? typi-
cally consist of a series of short, unrelated sen-
tences that each contain one or more errors. These 
users are testing the system to see what it does. 
While this is a legitimate use of any grammar 
checker, the user is unlikely to be proofreading his 
or her writing, so these data are excluded from 
evaluation.  
4 System Evaluation & User Interactions 
We are manually evaluating the rewrite sugges-
tions that ESL Assistant generated in order to de-
termine both system accuracy and whether user 
acceptances led to an improvement in their writing.  
These categories are shown in Table 3. Note that 
results reported for non-native text look very dif-
ferent from those reported for native text (dis-
cussed in Section 3) because of the neutral 
categories which do not appear in the evaluation of 
native text. Systems reporting 87% accuracy on 
native text cannot achieve anything near that on 
                                                          
2 These are anonymized to protect user privacy. 
non-native ELL text because almost one third of 
the flags fall into a neutral category. 
In 51% of the 39,944 frequent user sentences, 
the system generated at least one grammatical error 
flag, for a total of 17,832 flags. Thirteen percent of 
the time, the user ignored the flags. The remaining 
87% of the flags were inspected by the user, and of 
those, the user looked at the suggested rewrites 
without taking further action 31% of the time. For 
28% of the flags, the user hovered over a sugges-
tion to trigger a parallel web search but did not 
accept the proposed rewrite. Nevertheless, 41% of 
inspected rewrites were accepted, causing the orig-
inal string in the text to be revised. Overall, the 
users inspected about 15.5K suggested rewrites to 
accept about 6.4K. A significant number of users 
appear to be inspecting the suggested revisions and 
making deliberate choices to accept or not accept. 
The next question is: Are users making the right 
choices? To help answer this question, 34% of the 
user sessions have been manually evaluated for 
system accuracy ? a total of approximately 5.1K 
grammatical error flags. For each error category 
and for the three major writing domains, we: 
Evaluation Subcategory: Description 
Good 
Correct flag: The correction fixes a 
problem in the user input. 
Neutral 
Both Good: The suggestion is a legiti-
mate alternative to well-formed original 
input: I like working/to work. 
Misdiagnosis: the original input con-
tained an error but the suggested rewrite 
neither improves nor further degrades 
the input: If you have fail machine on 
hand. 
Both Wrong: An error type is correctly 
diagnosed but the suggested rewrite 
does not correct the problem: can you 
give me suggestion
Non-ascii: A non-ascii or text markup 
character is in the immediate context. 
. (suggests the in-
stead of a) 
Bad 
False Flag: The suggestion resulted in 
an error or would otherwise lead to a 
degradation over the original user input. 
Table 3: Evaluation categories 
 
77
1. Calculated system accuracy for all flags, 
regardless of user actions. 
2. Calculated system accuracy for only those 
rewrites that the user accepted 
3. Compared the ratio of good to bad flags. 
 
Results for the individual error categories are 
shown in Figure 2. Users consistently accept a 
greater proportion of good suggestions than they 
do bad ones across all error categories. This is 
most pronounced for the adjective-related modules, 
where the overall rate of good suggestions im-
proved 17.6% after the user made the decision to 
accept a  suggestion, while the system?s false posi-
tive rate dropped 14.1% after the decision. For the 
noun-related modules, the system?s most produc-
 
 Rewrite Suggestion Evaluation Accepted Suggestion 
Noun  
Related  
Modules 
 
3,017 suggestions 
   972 acceptances 
  
Preposition  
Related  
Modules 
 
1,465 suggestions 
   479 acceptances 
  
Verb  
Related  
Modules 
 
469 suggestions 
157 acceptances 
  
Adjective 
Related  
Modules 
 
125 suggestions 
  40 acceptances 
  
Figure 2: User interactions by module category 
good
56%
neut
28%
bad
16%
good
63%
neut
26%
bad
11%
good
37%
neut
39%
bad
24% good
45%
neut
42%
bad
13%
good
62%
neut
32%
bad
6%
good
72%
neut
25%
bad
3%
good
45%
neut
32%
bad
23%
good
63%
neut
28%
bad
9%
78
tive modules, the overall good flag rate increased 
by 7% while the false positive rate dropped 5%. 
All differences in false positive rates are statistical-
ly significant in Wilcoxon?s signed-ranks test.  
When all of the modules are evaluated across 
the three major writing domains, shown in figure 3, 
the same pattern of user discrimination between 
good and bad flags holds. This is most evident in 
the technical writing domain, where the overall 
rate of good suggestions improved 13.2% after 
accepting the suggestion and the false positive rate 
dropped 15.1% after the decision. It is least marked 
for the essay/nontechnical writing domain. Here 
the overall good flag rate increased by only .3% 
while the false positive rate dropped 1.6%. Again, 
all of the differences in false positive rates are sta-
tistically significant in Wilcoxon?s signed-ranks 
test. These findings are consistent with those for 
the machine learned articles and prepositions mod-
ules in the email domain (Chodorow et al under 
review).  
A probable explanation for the differences seen 
across the domains is that those users who are 
proofreading non-technical writing are, as a whole, 
less proficient in English than the users who are 
writing in the other domains. Users who are proof-
reading technical writing are typically writing a 
dissertation or paper in English and therefore tend 
to relatively fluent in the language. The email do-
main comprises people who are confident enough 
in their English language skills to communicate 
with colleagues and friends by email in English. 
With the essay/non-technical writers, it often is not 
clear who the intended audience is. If there is any 
indication of audience, it is often an instructor. Us-
ers in this domain appear to be the least English-
 Rewrite Suggestion Evaluation Accepted Suggestion 
Email  
Domain 
 
2,614 suggestions 
   772 acceptances 
  
Non-Technical 
Writing 
 Domain 
 
1,437 suggestions 
   684 acceptances 
  
Technical  
Writing 
Domain 
 
1,069 suggestions 
    205 acceptances 
  
Figure 3: User interactions by writing domain 
good
53%
neutral
32%
bad
15%
good
63%
neutral
28%
bad
9%
good
56%
neutral
32%
bad
12%
good
56%
neutral
34%
bad
10%
good
38%
neutral
28%
bad
34%
good
52%
neutral
29%
bad
19%
79
language proficient of the ESL Assistant users, so it 
is unsurprising that they are less effective in dis-
criminating between good and bad flags than their 
more proficient counterparts. Thus it appears that 
those users who are most in need of the system are 
being helped by it least ? an important direction for 
future work. 
Finally, we look at whether the neutral flags, 
which account for 29% of the total flags, have any 
effect. The two neutral categories highlighted in 
Table 3, flags that either misdiagnose the error or 
that diagnose it but do not correct it, account for 
74% of ESL Assistant?s neutral flags. Although 
these suggested rewrites do not improve the sen-
tence, they do highlight an environment that con-
tains an error. The question is: What is the effect of 
identifying an error when the rewrite doesn?t im-
prove the sentence?  
To estimate this, we searched for cases where 
ESL Assistant produced a neutral flag and, though 
the user did not accept the suggestion, a revised 
sentence that generated no flag was subsequently 
submitted for analysis. For example, one user en-
tered: ?This early morning  i got a from head office 
??. ESL Assistant suggested deleting from, which 
does not improve the sentence. Subsequently, in 
the same session, the user submitted, ?This early 
morning I heard from the head office ?? In this 
instance, the system correctly identified the loca-
tion of an error. Moreover, even though the sug-
gested rewrite was not a good solution, the 
information was sufficient to enable the user to fix 
the error on his or her own. 
Out of 1,349 sentences with neutral suggestions 
that were not accepted, we identified (using a 
fuzzy match) 215 cases where the user voluntarily 
modified the sentence so that it contained no flag, 
without accepting the suggestion. In 44% of these 
cases, the user had simply typed in the suggested 
correction instead of accepting it ? indicating that 
true acceptance rates might be higher than we orig-
inally estimated. Sixteen percent of the time, the 
sentence was revised but there remained an error 
that the system failed to detect. In the other 40% of 
cases, the voluntary revision improved the sen-
tence. It appears that merely pointing out the poss-
ible location of an error to the user is often 
sufficient to be helpful. 
5 Conclusion 
In conclusion, judging from the number of people 
who have visited the ESL Assistant web site, there 
is considerable interest in ESL proofreading tools 
and services. 
When using the tool to proofread text, users do 
not accept the proposed corrections blindly ? they 
are selective in their behavior. More importantly, 
they are making informed choices ? they can dis-
tinguish correct suggestions from incorrect ones. 
Sometimes identifying the location of an error, 
even when the solution offered is wrong, itself ap-
pears sufficient to cause the user to repair a prob-
lem on his or her own. Finally, the user 
interactions that we have recorded indicate that 
current state-of-the-art grammatical error correc-
tion technology has reached a point where it can be 
helpful to English language learners in real-world 
contexts. 
Acknowledgments 
We thank Bill Dolan, Lucy Vanderwende, Jianfeng 
Gao, Alexandre Klementiev and Dmitriy Belenko 
for their contributions to the ESL Assistant system. 
We are also grateful to the two reviewers of this 
paper who provided valuable feedback. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. In Proceedings of the 15th Confe-
rence on Computational Linguistics (pp. 32-38). 
Kyoto, Japan. 
Martin Chodorow, Michael Gamon, and Joel Tetreault. 
Under review. The utility of grammatical error detec-
tion systems for English language learners: Feedback 
and Assessment. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions (pp. 25-30). Pra-
gue, Czech Republic. 
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions (pp. 45-50). Prague, Czech Republic. 
Jens Eeg-Olofsson and Ola Knutsson.  2003. Automatic 
grammar checking for second language learners ? the 
use of prepositions.  Proceedings of NoDaLiDa 2003. 
Reykjavik, Iceland. 
80
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language 
Processing (pp. 449-455). Hyderabad, India. 
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan,  Jianfeng Gao, Dmitriy Belenko, and 
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct ESL errors. To ap-
pear in CALICO Journal, Special Issue on Automatic 
Analysis of Learner Language. 
Shicun Gui and Huizhong Yan. 2001. Computer analy-
sis of Chinese learner English. Presentation at Hong 
Kong University of Science and Technolo-
gy.http://lc.ust.hk/~centre/conf2001/keynote/subsect4 
/yang.pdf. 
Shicun Gui and Huizhong Yang. (Eds.). 2003. Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner 
English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. 
(In Chinese). 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2004). Detecting errors in English article usage with 
a maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th Interna-
tional Conference on Language Resources and 
Evaluation. Lisbon, Portugal. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineer-
ing, 12(2), 115-129. 
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (pp. 519-525). Montreal, 
Canada. 
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Automatic 
error detection in the Japanese learners' English spo-
ken data. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics (pp. 
145-148). Sapporo, Japan. 
Kevin Knight and Ishwar Chander,. 1994. Automatic 
postediting of documents. In Proceedings of the 12th 
National Conference on Artificial Intelligence (pp. 
779-784). Seattle: WA. 
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the Human Language Technology Confe-
rence of the North American Chapter of the 
Association for Computational Linguistics (pp. 31-
36). Boston, MA. 
John Lee and Stephanie Seneff. 2008. Correcting mi-
suse of verb forms. In Proceedings of ACl-08/HLT  
(pp. 174-182). Columbus, OH. 
Guido Minnen, Francis Bond, and Anne Copestake. 
2000. Memory-based learning for article generation. 
In Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning and of the 
Second Learning Language in Logic Workshop (pp. 
43-48). Lisbon, Portugal. 
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in 
Japanese sentences for machine translation into Eng-
lish. In Proceedings of the Fifth International Confe-
rence on Theoretical and Methodological Issues in 
Machine Translation (pp. 218-225). Kyoto, Japan. 
Ryo Nagata, Takahiro Wakana, Fumito Masui, Atsui 
Kawai, and Naoki Isu. 2005. Detecting article errors 
based on the mass count distinction. In R. Dale, W. 
Kam-Fie, J. Su and O.Y. Kwong (Eds.) Natural Lan-
guage Processing - IJCNLP 2005, Second Interna-
tional Joint Conference Proceedings (pp. 815-826). 
New York: Springer. 
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and 
Naoki Isu. 2006. A feedback-augmented method for 
detecting errors in the writing of learners of English. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(pp. 241-248). Sydney, Australia. 
Joel Tetreault and Martin Chodorow. 2008a. The ups 
and downs of preposition error detection in ESL. 
COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2008b. Native 
judgments of non-native usage: Experiments in pre-
position error detection. In Proceedings of the Work-
shop on Human Judgments in Computational 
Linguistics, 22nd International Conference on Com-
putational Linguistics (pp 43-48). Manchester, UK. 
Jenine Turner and Eugene Charniak. 2007. Language 
modeling for determiner selection. In Human Lan-
guage Technologies 2007: The Conference of the 
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short 
Papers (pp. 177-180). Rochester, NY. 
 
 
81
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1477?1488, Dublin, Ireland, August 23-29 2014.
Predicting Interesting Things in Text 
 
 
Michael Gamon 
Microsoft Corp. 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
Arjun Mukherjee 
Department of Computer 
Science 
University of Houston 
Houston, TX 77004 
ar-
jun4787@gmail.com 
Patrick Pantel 
Microsoft Corp. 
One Microsoft Way 
 Redmond, WA 98052 
ppantel@microsoft.com 
 
  
 
Abstract 
While reading a document, a user may encounter concepts, entities, and topics that she is interested in 
exploring more. We propose models of ?interestingness?, which aim to predict the level of interest a user 
has in the various text spans in a document. We obtain naturally occurring interest signals by observing 
user browsing behavior in clicks from one page to another. We cast the problem of predicting interesting-
ness as a discriminative learning problem over this data. We leverage features from two principal sources: 
textual context features and topic features that assess the semantics of the document transition. We learn 
our topic features without supervision via probabilistic inference over a graphical model that captures the 
latent joint topic space of the documents in the transition. We train and test our models on millions of real-
world transitions between Wikipedia documents as observed from web browser session logs. On the task 
of predicting which spans are of most interest to users, we show significant improvement over various 
baselines and highlight the value of our latent semantic model. 
1 Introduction 
Reading inevitably leads people to discover interesting concepts, entities, and topics. Predicting what 
interests a user while reading a document has important applications ranging from augmenting the doc-
ument with supplementary information, to ad placement, to content recommendation. We define the task 
of predicting interesting things (ITs) as ranking text spans in an unstructured document according to 
whether a user would want to know more about them. This desire to learn more serves as our proxy for 
interestingness. 
There are many types of observable behavior that indicate user interest in a text span. The closest one 
to our problem definition is found in web browsing, where users click from one document to another 
via named anchors. The click process is generally governed by the user?s interest (modulo erroneous 
clicks). As such, the anchor name can be viewed as a text span of interest for that user. Furthermore, the 
frequency with which users, in aggregate, click on an anchor serves as a good proxy for the level of 
interest1. 
What is perceived as interesting is influenced by many factors. The semantics of the document and 
candidate IT are important. For example, we find that when users read an article about a movie, they are 
more likely to browse to an article about an actor or character than to another movie or the director. 
Also, user profile and geo-temporal information are relevant. For example, interests can differ depend-
ing on the cultural and socio-economic background of a user as well as the time of the session (e.g., 
weekday versus weekend, daytime versus late night, etc.). 
1 Other naturally occurring expressions of user interest, albeit less fitting to our problem, are found in web search queries, 
social media engagement, and others. 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
                                                 
1477
Strictly speaking, human interestingness is a psychological and cognitive process (Varela et al., 1991). 
Clicks and long dwell times are salient observed behavioral signals of interestingness that have been 
well accepted in the information retrieval literature (Claypool et al., 2001; Mueller and Lockerd, 2001). 
In this paper, we utilize the observed user?s browsing behavior as a supervision signal for modeling 
interestingness. Specifically, we cast the prediction of ITs as a discriminative learning task. We use a 
regression model to predict the likelihood of an anchor in a Wikipedia article to be clicked, which as we 
have seen above can serve as a proxy for interestingness. Based on an empirical study of a sample of 
our data, we use features in our model from the document context (such as the position of the anchor 
text, frequency of the anchor text in the current paragraph, etc.) as well as semantic features that aim to 
capture the latent topic space of the documents in the browsing transition. These semantic features are 
obtained in an unsupervised manner via a joint topic model of source and target documents in browsing 
transitions. We show empirical evidence that our discriminative model is effective in predicting ITs and 
we demonstrate that the automatically learned latent semantic features contribute significantly to the 
model performance. The main contributions of this paper are: 
? We introduce the task of predicting interesting things as identifying what a user likely wants to 
learn more about while reading a document. 
? We use browsing transitions as a proxy for interestingness and model our task using a discrimina-
tive training approach. 
? We propose a semantic probabilistic model of interestingness, which captures the latent aspects 
that drive a user to be interested in browsing from one document to another. Features derived from 
this semantic model are used in our discriminative learner. 
? We show empirical evidence of the effectiveness of our model on an application scenario. 
2 Related Work 
Salience: A notion that might at first glance be confused with interestingness is that of salience (Paranjpe 
2009; Gamon et al. 2013). Salience can be described as the centrality of a term to the content of a 
document. Put another way, it represents what the document is about. Though salience and interesting-
ness can interact, There are clear differences. For example, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average user would probably not be interested in learning more 
about Obama while reading that article. 
Click Prediction: Click prediction models are used pervasively by search engines. Query based click 
prediction aims at computing the probability that a given document in a search-result page is clicked on 
after a user enters some query (Joachims, 2002; Joachims et al., 2005; Agichtein et al., 2006; Guo et al., 
2009a). Click prediction for online advertising is a core signal for estimating the relevance of an ad to a 
search result page or a document (Chatterjee et al., 2003; Broder et al., 2007; Craswell et al., 2008; 
Graepel et al., 2010). Also related are personalized click models, e.g., (Shen et al., 2012), which use 
user-specific click through rate (CTR). Although these applications and our task share the use of CTR 
as a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/fea-
ture at runtime, our task specifically aims at predicting interestingness in the absence of web usage 
features: Our input is completely unstructured and there is no assumption of prior user interaction data. 
Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has re-
semblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009). 
However, these are tailored for blogs and associated comment discussions which is very different from 
our source to destination browsing transition logs. Guo et al., (2009b) used probabilistic models for 
discovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric search 
were explored. Gao et al. (2011) employ statistical machine translation to connect two types of content, 
learning semantic translation of queries to document titles.  None of the above models, however, are 
directly applicable to the joint topic mappings that are involved in source to destination browsing tran-
sitions which are the focus of our work. 
Predicting Popular Content: Modeling interestingness is also related to predicting popular content 
in the Web and content recommenders (Lerman and Hogg, 2010; Szabo and Huberman, 2010; Bandari 
et al., 2012). In contrast to these tasks, we strive to predict what term a user is likely to be interested in 
when reading content. We do not rely on prior browsing history, since we aim to predict interestingness 
1478
in unstructured text with no interaction history. We show in our experiments that a popularity signal 
alone is not a sufficient predictor for interestingness. 
3 The Interestingness Task 
The process of identifying interesting things (ITs) on a page consists of two parts: (1) generating candi-
date things (e.g., entities, concepts, topics); and (2) scoring and ranking these according to interesting-
ness. In this paper, we fix step 1 and focus our effort on step 2, i.e., the assignment of an interestingness 
score to a candidate. We believe that this scope is appropriate in order to understand the factors that 
enter into what is perceived as interesting by a user. Once we have gained an understanding of the 
interestingness scoring problem, however, there are opportunities in identifying candidates automati-
cally, which we leave for future work. 
In this section we begin by formally defining our task. We then introduce our data set of naturally 
occurring interest signals, followed by an investigation of the factors that influence them. 
3.1 Formal Task Definition 
We define our task as follows. Let ??  be the set of all documents and ?? be the set of all candidate text 
spans in all documents in ?? , generated by some candidate generator. Let ???? ? ??  be the set of candi-
dates in ?? ? ?? . We formally define the interestingness task as learning the function below, where 
??(??, ??) is the interestingness of candidate ?? in 2: 
??:?? ? ?? ? ?  (1) 
3.2 Data Set 
User browsing events on the web (i.e., a user clicking from one document to another) form a naturally 
occurring collection of interestingness signals. That is when a user clicks on an anchor in a document, 
we can postulate that the user is interested in learning more about it, modulo erroneous clicks. 
We collect a large set of many millions of such user browsing events from session logs of a commer-
cial web browser. Specifically, we collect from these logs each occurrence of a user click from one 
Wikipedia page to another during a one month period, from all users in all parts of the world. We refer 
to each such event as a transition. For each transition, our browser log provides metadata, including user 
profile information, geo-location information and session information (e.g., time of click, source/target 
dwell time, etc.) Our data set includes millions of transitions between Wikipedia pages.  
For our task we require: (1) a mechanism for generating candidate things; (2) ample clicks to serve 
as a reliable signal of interestingness for training our models; and (3) accessible content. Our focus on 
Wikipedia satisfies all. First, Wikipedia pages tend to contain many anchors, which can serve as the set 
of candidate things to be ranked. Second, Wikipedia attracts enough traffic to obtain robust browsing 
transition data. Finally, Wikipedia provides full content3 dumps. It is important here to note that our 
choice of Wikipedia as a test bed for our experiments does not restrict the general applicability of our 
approach: We propose a semantic model (Section 4.2) for mining latent features relevant to the phenom-
enon of interestingness which is general and can be applied to generic Web document collections. 
Using uniform sampling, we split our data into three sets: a development set (20%), a training set 
(60%) and a test set (20%). We further subdivide the test set by assigning each transition as belonging 
to the HEAD, TORSO, or TAIL, which we compute using inverse CDF sampling on the test set. We do 
so by assigning the most frequently occurring transitions, accounting for 20% of the (source) traffic, to 
the HEAD. Similarly, the least frequently occurring transitions, accounting for 20% of the (source) traf-
fic, are assigned to the TAIL. The remaining transitions are assigned to the TORSO. This three-way 
split reflects a common practice in the IR community and is based on the observation that web traffic 
frequencies show a very skewed distribution, with a small set of web pages attracting a large amount of 
traffic, and a very long tail of infrequently visited sites. Different regions in that distribution often show 
marked differences in behaviour, and models that are useful in one region are not necessarily as useful 
in another. 
2 We fix ??(??, ??) = 0 for all ?? ? ????. 
3 We utilize the May 3, 2013 English Wikipedia dump from http://dumps.wikimedia.org, consisting of roughly 4.1 million 
articles. 
                                                 
1479
3.3 What Factors Influence Interestingness? 
We manually inspected 200 random transitions from our development set. Below, we summarize our 
observations. 
Only few things on a page are interesting: The average number of anchors on a Wikipedia page is 79. 
Of these, only very few are actually clicked by users. For example, the Wikipedia article on the TV 
series ?The Big Bang Theory? leads to clicks on anchors linking to the pages of the series? actors for 
90% of transitions (while these anchors account for only a small fraction of all unique anchors on that 
page). 
The semantics of source and destination pages is important: We manually determined the entity type 
of the Wikipedia articles in our sample, according to schema.org classes. 49% of all source urls in our 
data sample are of the Creative Work category, reflecting the strong popular interest in movies 
(37%), actors (22%), artists (18%), and television series (8%). The next three most prominent categories 
are Organization (12%), Person (11%) and Place (6%). We observed that transitions are influ-
enced by these categories. For example, when the source article category is Movie, the most frequently 
clicked pages are of category Actor (63%) and Character (13%). For source articles of the 
TVSeries category, Actor destination articles account for 86% of clicks. Actor articles lead to 
clicks on Movie articles (45%) and other Actor articles (26%), whereas Artist articles lead to 
clicks on other Artist articles (29%), Movie articles (17%) and MusicRecording articles (18%). 
The structure of the source page plays a role: It is well known that the position of a link on a page 
influences user click behavior: links that are higher on a page or in a more prominent position tend to 
attract more clicks. We noticed similar trends in our data. 
The user plays a role: We hypothesized that users from different geographic and cultural backgrounds 
might exhibit different interests, or that interests are time-bound (e.g., interests on weekends differ from 
those on week days, daytime from nighttime, etc.) Initial experiments showed small effects of these 
factors, however, a more thorough analysis on a larger sample is necessary, which we leave for future 
work. 
4 Modeling Interestingness 
We cast the problem of learning the interestingness function ?? (see Eq. 1) as a discriminative regression 
learning problem. Below, we first describe this model, and then we introduce our semantic topic model 
which serves to provide semantic features for the discriminative learner. 
4.1 Discriminative Model 
Although our task is to predict ITs from unstructured documents, we can leverage the user interactions 
in our data, described in Section 3.2 as our training signal. 
Given a source document ?? ? ?? , and an anchor in s leading to destination document d, we use the 
aggregate click frequency of this anchor as a proxy for its interestingness, i.e.: 
??(??, ??) = ??(??|??)                       (2) 
where ??(??|??) is the probability of a user clicking on the anchor to ?? when viewing ??3F4. We use ??(??|??) 
as our regression target computed from our training data. 
For our learning algorithm, we use boosted decision trees (Friedman, 1999). We tune our hyperpa-
rameters (i.e., number of iterations, learning rate, minimum instances in leaf nodes, and the maximum 
number of leaves) using cross-validation on the development set. Each transition in our training data is 
represented as a vector of features, where the features fall into three basic families: 
1 Anchor features (Anc): position of the anchor in the document, frequency of the anchor, anchor 
density in the paragraph, and whether the anchor text matches the title of the destination page. 
2 User session features (Ses): city, country, postal code, region, state and timezone of the user, as 
well as day of week, hour, and weekend vs. workday of the occurrence of the transition. 
4 For notational convenience, we use ??(??, ??) even though Eq. 1 defines its second argument as being a candidate text span. 
Here, it is implicit that d consists of both the target document and the anchor text (which serves as the candidate text span). 
                                                 
1480
3 Semantic features: sourced in various experimental configurations from (1) Wikipedia page cate-
gories as assigned by Wikipedia editors (Wiki) or from (2) an unsupervised joint topic transition 
model (JTT) of source and destination pages (described in detail in the next section). 
In some experimental configurations we use Wikipedia page categories as semantic features. We show 
in our experiments (see Section 5) that these are highly discriminative. It is important to note that editor-
labeled category information is available in the Wikipedia domain but not in others. In other words, we 
can use this information to verify that semantics indeed is influential for interestingness, but we should 
design our models to not rely on this. We thus build an unsupervised semantic model of source and 
destination pages, which serves the purpose of providing semantic information without any domain-
specific annotation. 
4.2 The Semantics of Interestingness 
As indicated in Section 3, the semantics of source and destination pages, ?? and ??, influence the likeli-
hood that a user is interested in ?? after viewing ??. In this section we propose an unsupervised method 
for modeling the transition semantics between ?? and ??. As outlined in the previous section, this model 
then serves to generate semantic features for our discriminative model of interestingness. 
Referring to the notations in Table 1, we start by positing a distribution over the joint latent transition 
topics (in the higher level of semantic space), ???? for each transition ??. The corresponding source ??(??) 
and destination ??(??) articles of a given transition ?? are assumed to be admixtures of latent topics that are 
conditioned on the joint topic transition distribution, ????. For ease of reference, we will refer to this model 
as the Joint Transition Topic Model (JTT). The variable names and their descriptions are provided in 
Table 1. Figure 1 shows the plate notation of our model and the generative process: 
 
        
     
                        T
? 
zd zs 
ws wd
              
            K
?  
? 
? 
Ns Nd
 
Figure 1. Generative Process and Plate Notation of JTT. 
1. For each topic ??, draw ???? ~ ??????(??) 
2. For each transition ??: 
a. Draw the joint topic transition distribution, ???? ~ 
??????(??) 
b. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????  ~ ????????(????) 
c. For each word token ?? ? {1? ????}: 
i. Draw ????,????  ~ ????????(????) 
ii. Emit ????,????   ~ ????????(????) 
1481
Variable Description Variable Description 
?? A transition ?? ???? , ???? Set of all topics in src, dest pages 
??(??), ??(??) The src and dest pages of ?? ?? ??, ?? ?? Set of all word tokens in src, dest pages 
????~??????(??) Joint src/dest topic distribution ? = {????} 
Set of all latent joint transition topic dis-
tributions 
????, ???? Latent topics of  ??(??), ??(??) ? = {????} Set of all latent topics 
????, ???? Observed word tokens of  ??(??), ??(??) ????,?? Contribution of topic ?? in transition ?? 
???? ~ ??????(??  
Latent topic-word distributions for 
topic ?? 
????,??
?? , ????,??
??  ??th word of transition ?? in ??(??), ??(??) 
??, ??  Dirichlet parameters for ??, ?? ????,???? , ????,????   Latent topic of ??th word of ?? in ??(??), ??(??) 
????, ???? No. of terms in src and dest pgs of ?? ????(??),??
??  No. of words in ??(??) assigned to topic ?? 
?? = {??} Set of all transitions, ?? ????(??),??
??  No. of words in ??(??) assigned to ?? 
?? No. of topics ????,??
??  No. of times word ?? assigned to ?? in ?? ?? 
??  No. of unique terms in the vocab. ????,????  No. of times word ?? assigned to ?? in ?? ?? 
Table 1. List of notations. 
Exact inference for JTT is intractable. Hence, we use Markov Chain Monte Carlo (MCMC) Gibbs sam-
pling. Rao-Blackwellization (Bishop, 2006) is used to reduce sampling variance by collapsing latent 
variables ?? and ??. Owing to space constraints, we omit the full derivation details. The full joint can be 
written succinctly as follows: 
??(????,????,????, ????) = ??
???????(??),[ ]
??  + ????(??),[ ]
?? + ???
??(??)
??
??=1
? = ??
??(????,[ ]
??  + ????,[ ]
?? +??)
??(??)
??
??=1
? (3) 
Omission of a latter index in the count variables, denoted by [ ], corresponds to the row vector span-
ning over the latter index. The corresponding Gibbs conditional distributions for ???? and ???? are detailed 
below, where the subscript ??(??, ??)? denotes the value of the expression excluding the counts of the 
term (??, ??): 
???????,??
?? = ??| ? ? ?
?????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +??
? ??????(??),??
?? ?
?(??,??)
+ ????(??),??
?? +?????
??=1
?
?????,??
?? ?
?(??,??)
+ ????,??
?? +??
? ??????,??
?? ?
?(??,??)
+ ????,??
?? +?????
??=1
  (4) 
???????,??
?? = ??| ? ? ?
????(??),??
??  +?????(??),??
?? ?
?(??,??)
+??
? ?????(??),??
??  +?????(??),??
?? ?
?(??,??)
+?????
??=1
?
????,??
??  + ?????,??
?? ?
?(??,??)
+??
? ?????,??
??  + ?????,??
?? ?
?(??,??)
+?????
??=1
 (5) 
We learn our joint topic model from a random traffic-weighted sample of 10,000 transitions, which are 
randomly sampled from the development set outlined in Section 3.25. The decision to use this sample 
of 10,000 transitions is based on the observation that there were no statistically significant performance 
gains for models trained on more than 10k transitions. The Dirichlet hyperparameters are set to ?? = 
50/?? and ?? = 0.1 according to the values suggested in (Griffiths and Steyvers, 2004). The number of 
topics, ??, is empirically set to 50. We also conducted pilot experiments with other hyperparameter set-
tings, larger transition sets and more topics but we found no substantial difference in the end-to-end 
performance. Although increasing the number of topics and modeling more volume usually results in 
lowering perplexities and better fitting in topic models (Blei et al., 2003), it can also result in redun-
dancy in topics which may not be very useful for downstream applications (Chen et al., 2013). For all 
reported experiments we use the posterior estimates of our joint model learned according to the above 
settings. In our discriminative interestingness model, we use three classes of features from JTT to cap-
ture the latent topic distributions of the source page, the destination page, and the joint topics for that 
transition. These correspond to source topic features (????, labeled as JTTsrc in charts), destination topic 
features (????, labeled as JTTdst), and transition topic features (?, labeled as JTTtrans). Each of these 
three sets comprises 50 features, for a total of 150.? is the distribution over joint src and dst topics that 
5 Note that we use the development set to train our semantic model since it is ultimately used to generate features for our dis-
criminative learner of Section 4. Since the learner is trained using the training set, this strategy avoids overfitting our seman-
tic model to the training set. 
                                                 
1482
appear in a particular transition. ???? and ???? are the actual topic assignments for individual words in src 
and dst. Upon learning the JTT model, for each K topics, we get a probability of that topic appearing in 
the transition, in the src, and in the dst document (by taking the posterior point estimates for latent 
variables  ?, ????, ???? respectively). The GBDT implementation we use for our discriminative model per-
forms binning of these real-valued features over an ensemble of DTs.  
5 Experiments 
We evaluate our interestingness model on the task of proposing ?? anchors on a page that the user will 
find interesting (highlighting task). Recall the interestingness function ?? from Eq. 1. In the highlighting 
task, a user is reading a document ?? ? ??  and is interested in learning more about a set of anchors. Our 
goal in this task is to select ?? anchors that maximize the cumulative degree of interest of the user, i.e.: 
argmax
??????=(??1,?,????|?????????)
? ??(??, ????)???????????
         (6)  
In other words, we consider the ideal selection to consist of the k most interesting anchors according to 
??(??, ??).We compare the interestingness ranking of our models against a gold standard function, ???, com-
puted from our test set. Recall that we use the aggregate click frequency of an anchor as a proxy for its 
interestingness. As such, the gold standard function for the test set is computed as: 
???(??, ??) = ??(??|??)                      (7) 
where ??(??|??) is the probability of a user clicking on the anchor ?? when viewing ??. 
Given a source document ??, we measure the quality of a model?s interestingness ranking against the 
ideal ranking defined above using the standard nDCG metric (Manning et al., 2008). We use the inter-
estingness score of the gold standard as the relevance score. 
Table 2 shows the nDCG results for two baselines and a range of different feature sets. The first high-
level observation is that the task is difficult, given the low baseline results. Since there are many anchors 
on an average page, picking a random set of anchors yields very low nDCG scores. The nDCG numbers 
of our baselines increase as we move from HEAD to TORSO to TAIL, due to the fact that the average 
number of links per page (not unique) decreases in these sets from 170 to 94 to 416. The second baseline 
illustrates that it is not sufficient to simply pick the top n anchors on a page. 
Next, we see that using our set of anchor features (see Section 4.1) in the regression model greatly 
improves performance over the baselines, with the strongest numbers on the HEAD set and decreasing 
effectiveness in TORSO and TAIL. This shows that the distribution of interesting anchors on a page 
differs according to the popularity of the source content, possibly also with the length of the page. Our 
best performing model is the one using anchor features and all three sets of latent semantic features 
(Table 2, row 6; source, destination, and transition topics). 
The biggest improvement is obtained on the HEAD data. This is not surprising given that the topic 
model is trained on a traffic weighted sample of Wikipedia articles and that HEAD pages tend to have 
more content, making the identification of topics more reliable. Regarding the individual contributions 
of the latent semantic features (Table 2, rows 4, 5), destination features alone hurt performance on the 
HEAD set. Latent semantic source features lead to a boost across the board, and the addition of latent 
semantic transition topic features produces the best model, with gains especially pronounced on the 
HEAD data. Figure 2 further shows the performance of our best configuration across ALL, HEAD, 
TORSO, and TAIL. Interestingly, the TAIL exhibits better performance of the model than the TORSO 
(with the exception of nDCG at rank 3 or higher). We hypothesize that this is because the average num-
ber of anchors in a TAIL page is less than half of that in a TORSO page. 
6 Wikipedia editors tend to spend more time on more frequently viewed documents, hence they tend contain more content and 
more anchors. 
                                                 
1483
 nDCG % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 4.07 4.90 6.24 8.10 3.56 4.83 7.66 10.92 6.20 11.74 19.50 25.82 
Baseline: first n an-
chors 
9.99 12.47 17.72 24.33 7.17 9.87 17.06 23.97 9.06 16.66 27.35 34.82 
Anc 21.46 22.50 25.30 29.47 13.85 16.80 22.85 28.20 10.88 19.16 29.33 36.48 
Anc+JTTdst 13.97 16.33 19.69 23.78 11.37 14.17 19.67 24.66 11.62 19.69 29.69 36.35 
Anc+JTTdst+JTTsrc 26.62 30.03 34.82 39.38 17.05 20.82 27.15 32.48 12.27 21.56 31.88 38.85 
Anc+JTT-
dst+JTTsrc+JTTtrans 
34.49 35.21 38.01 41.80 18.32 21.69 28.03 33.22 13.06 21.68 32.13 38.01 
Table 2. Highlighting performance (% nDCG @ n) for different feature sets across HEAD, TORSO, 
and TAIL. Bold indicates statistically significant best systems (with 95% confidence). 
Not shown in these results are the effects of using user session features. We consistently found that 
these features did not improve upon the configurations where anchor and JTT features are used. We do 
not, however, rule out the potential of such features on this task, especially in light of our data analysis 
observations from Section 3.3, which suggest an effect from these factors. We leave a more in-depth 
study of the potential contribution of these types of features for future research. 
We now address the question how our unsupervised latent semantic features perform compared to the 
editor-assigned categories for Wikipedia pages, for two reasons. First, it is reasonable to consider the 
manually assigned Wikipedia categories as a (fine-grained) oracle for topic assignments. Second, out-
side of Wikipedia, we do not have the luxury of manually assigned categories/topics. As illustrated in 
Figure 3, we found that Wikipedia categories outperform the JTT topic features, but the latter can re-
cover about two thirds of the nDCG gain compared to Wikipedia categories. 
Finally, in the HEAD part of the data, we have enough historical clickthrough data that we could 
directly leverage for prediction. We conducted experiments where we used the prior probability ??(??|??) 
obtained from the development data (both smoothed and unsmoothed). Following this strategy we can 
achieve up to 65% nDCG@10 as shown in Figure 4 where the use of prior history (labeled ?History: 
Target | Source Prior?) is compared to our best model and to baselines. As stressed before, in most real-
life applications, this is not a viable option since anchors or user-interaction logs are unavailable. Even 
in web browsing scenarios, the TORSO and TAIL have no or only very sparse histories. Furthermore, 
the information is not available in a ?cold start? scenario involving new and unseen pages. We also 
examined whether the general popularity of a target page is sufficient to predict an anchor?s interesting-
ness, and we found that this signal performs better than the baselines, but significantly worse than our 
models. This series is labeled ?History: Target Prior? in Figure 4. 
 
Figure 2. NDCG comparison across overall performance (ALL) versus HEAD, TORSO, and TAIL 
subsets, on the Highlighting task. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Best Configuration (Anc+JTT)
NDCG @ Rank (ALL vs. HEAD vs. TORSO vs. TAIL)
ALL
HEAD
TORSO
TAIL
1484
  
Figure 3. JTT features versus Wikipedia category features on Highlighting task. 
 
Figure 4. Highlighting task comparison between baselines, best configuration using JTT, and models 
with historical transitions. 
Our highlights task reflects the main goal of our paper, i.e., to predict interestingness in the context of 
any document, whether it be a web page, an email, or a book. A natural extension of our work, especially 
in our experimental setting with Wikipedia transitions, is to predict the next click of a user, i.e., click 
prediction. 
There is a subtle but important difference between the two tasks. Highlights aims to identify a set of 
interesting nuggets for a source document. A user may ultimately click on only a subset of the nuggets, 
and perhaps not in the order of most interest. Our experimental metric, nDCG, reflects this ranking task 
well. Click prediction is an inherently more difficult task, where we focus on predicting exactly the next 
click of a specific user. Unlike in the highlights task, there is no partial credit for retrieving other inter-
esting anchors. Only the exact clicked anchor is considered a correct result. As such, we utilize a differ-
ent metric than nDCG on this task. We measure our model?s performance on the task of click prediction 
using cumulative precision. Given a unique transition event ?(s,a,d) by a particular user at a particular 
time, we present the transition, minus the gold anchor a and destination d, to our models, which in turn 
predict an ordered list of most likely anchors on which the user will click. The cumulative precision at 
k of a model, is 1 if any of the predicted anchors matched a, and 0 otherwise. 
0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Semantic Features: JTT vs. Wikipedia Categories
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
Anc
Anc+Wiki_Src+Wik
i_tar
Anc+JTT_src+JTT_t
ar+JTT_trans
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
N
D
CG
Rank
Interest Models vs. Access to Historical Transitions
NDCG @ Rank (ALL)
Baseline: random
Baseline: first
History: Target Prior
History: Target |
Source Prior
Anc+JTT_src+JTT_tar
+JTT_trans
1485
Table 3 outlines the results on this task and Figure 5 shows the corresponding chart for our best 
configuration. Note that in the click prediction task, the model performs best on the TAIL, followed by 
TORSO and HEAD. This seems to be a reflection of the fact that in this harder task, the total number of 
anchors per page is the most influential factor in model performance. 
Cumulative  
Precision % HEAD TORSO TAIL 
n @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10 
Baseline: random 1.07 2.08 5.29 10.55 1.94 3.91 9.71 19.00 5.97 11.66 26.43 44.94 
Baseline: first n an-
chors 
2.68 5.77 16.73 33.78 4.10 8.19 22.86 42.08 8.77 16.57 36.80 58.52 
Anc 8.40 12.55 22.04 34.22 8.70 14.37 27.56 42.68 10.59 19.08 38.27 59.04 
Anc+JTTdst 5.48 9.19 17.77 29.14 6.93 12.07 23.90 38.00 11.23 19.59 38.46 57.87 
Anc+JTTdst+JTTsrc 9.02 15.65 30.05 44.72 10.11 17.42 32.08 47.07 11.95 21.47 40.96 61.24 
Anc+JTT-
dst+JTTsrc+JTTtrans 
11.53 18.43 31.93 45.36 10.86 18.19 32.96 47.66 12.64 21.58 41.27 61.28 
Table 3. Click prediction results for different feature sets across HEAD, TORSO, and TAIL. Bold indicates sta-
tistically significant best systems (with 95% confidence). 
 
Figure 5. Overall performance (ALL) versus HEAD, TORSO, and TAIL subsets on click prediction. 
6 Conclusion and Future Directions 
We presented a notion of an IT on a page that is grounded in observable browsing behavior during 
content consumption. We implemented a model for prediction of interestingness that we trained and 
tested within the domain of Wikipedia. The model design is generic and not tied to our experimental 
choice of the Wikipedia domain and can be applied to other domains. Our model takes advantage of 
semantic features that we derive from a novel joint topic transition model. This semantic model takes 
into account the topic distributions for the source, destination, and transitions from source to destination. 
We demonstrated that the latent semantic features from our topic model contribute significantly to the 
performance of interestingness prediction, to the point where they perform nearly as well as using editor-
assigned Wikipedia categories as features. We also showed that the transition topics improve results 
over just using source and destination semantic features alone. 
A number of future directions immediately suggest themselves. First, for an application that marks 
interesting ITs on an arbitrary page, we would need a detector for IT candidates. A simple first approach 
would be to use a state-of-the-art Named Entity Recognition (NER) system to cover at least a subset of 
potential candidates. This does not solve the problem entirely, since we know that named entities are 
not the only interesting nuggets ? general terms and concepts can also be of interest to a reader. On the 
other hand we do have reason to believe that entities play a very prominent role in web content con-
sumption, based on the frequency with which entities are searched for (see, for example Lin et al. 2012 
and the references cited therein). Using an NER system as a candidate generator would also allow us to 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Cu
m
ul
at
iv
e 
Pr
ec
is
io
n
Rank
Best Configuration (Anc+JTT)
Click Prediction - Cumulative Precision @ Rank
ALL
HEAD
TORSO
TAIL
1486
add another potentially useful feature to our interestingness prediction model: the type of the entity. One 
could also envision jointly modeling interestingness and candidate detection. 
A second point concerns the observation from the previous section on the different regularities that 
seem to be at play according to the popularity and possibly the length of an article. More detailed ex-
periments are needed to tease out this influence and possibly improve the predictive power of the model. 
User session features did not contribute to model performance when used in conjunction with other 
feature families, but closer investigation of these features is warranted for more personalized models of 
interestingness. Finally, a number of options regarding JTT  could be explored further. Being trained on 
a traffic-weighted sample of articles, the topic model predominantly picks up on popular topics. This 
could be remedied by training on a non-weighted sample, or, more promisingly, on a larger non-
weighted sample with a larger ??, i.e. more permissible total topics. 
References 
Agichtein, E., Brill, E., and Dumais, S. 2006. Improving web search ranking by incorporating user behavior infor-
mation. In Proceedings of SIGIR. pp. 19-26. 
Bandari, R., Asur, S., and Huberman, B. A. 2012. The Pulse of News in Social Media: Forecasting Popularity. In 
Proceedings of ICWSM. 
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent dirichlet allocation. Journal of Machine Learning Re-
search, 3, 993-1022. 
Broder, A., Fontoura, M., Josifovski, V., and Riedel, L. 2007. A semantic approach to contextual advertising. In 
Proceedings of SIGIR. pp. 559-566. 
Bishop, C.M. 2006. Pattern Recognition and Machine Learning. Springer.  
Chatterjee, P., Hoffman, D. L., and Novak, T. P. 2003. Modeling the clickstream: Implications for web-based 
advertising efforts. Marketing Science, 22(4), 520-541. 
Chen, Z., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M. and Ghosh, R. 2013. Leveraging Multi-Domain Prior 
Knowledge in Topic Models. In Proceedings of IJCAI. pp. 2071-2077. 
Claypool, M., Le, P., Wased, M., & Brown, D. 2001a. Implicit interest indicators. In Proceedings of the 6th inter-
national conference on Intelligent user interfaces (pp. 33-40). ACM. 
Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B. 2008. An experimental comparison of click position-bias 
models. In Proceedings of WSDM. pp. 87-94. 
Erosheva, E., Fienberg, S., and Lafferty, J. 2004. Mixed membership models of scientific publications. In Pro-
ceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1). pp. 5220-5227. 
Friedman, J. H. 1999. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189-
1232, 1999. 
Gamon, M., Yano, T., Song, X., Apacible, J. and Pantel, P. 2013. Identifying Salient Entities in Web Pages. In 
Proceedings CIKM. pp. 2375-2380. 
Gao, J., Toutanova, K., and Yih, W. T. 2011. Clickthrough-based latent semantic models for web search. In Pro-
ceedings of SIGIR. pp. 675-684. 
Graepel, T., Candela, J.Q., Borchert, T., and Herbrich, R. 2010. Web-scale Bayesian Click-Through Rate Predic-
tion for Sponsored Search Advertising in Microsoft?s Bing Search Engine. In Proceedings of ICML. pp. 13-20. 
Griffiths, T.L and Steyvers, M. 2004. Finding Scientific Topics. Proceedings of the National Academy of Science, 
101, suppl 1, 5228-5235. 
Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.-M, and Faloutsos, C. 2009a. Click chain model in 
web search. In Proceedings of WWW. pp. 11-20. 
Guo, J., Xu, G., Cheng, X., and Li, H. 2009b. Named entity recognition in query. In Proceedings of SIGIR. pp. 
267-274. 
Joachims, T. 2002. Optimizing search engines using clickthrough data. In Proceedings of KDD. pp. 133-142. 
Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay, G. 2005. Accurately interpreting clickthrough data as 
implicit feedback. In Proceedings of SIGIR. pp. 154-161. 
1487
Lerman, K., and Hogg, T. 2010. Using a model of social dynamics to predict popularity of news. In Proceedings 
of WWW. pp. 621-630. 
Lin, T., Pantel, P., Gamon, M., Kannan, A., and Fuxman, A. 2012. Active objects: actions for entity-centric search. 
In Proceedings of WWW. pp. 589-598. 
Manning, C. D., Raghavan, P., and Schutze, H. 2008. Introduction to Information Retrieval. Cambridge University 
Press. 
Mueller, F., & Lockerd, A. 2001. Cheese: tracking mouse movement activity on websites, a tool for user modeling. 
In CHI'01 extended abstracts on Human factors in computing systems (pp. 279-280). ACM. 
Paranjpe, D. 2009. Learning document aboutness from implicit user feedback and document structure. In Proceed-
ings of CIKM. pp. 365-374. 
Shen, S., Hu, B., Chen, W., and Yang, Q. 2012. Personalized click model through collaborative filtering. In Pro-
ceedings of WSDM. pp. 323-333. 
Szabo, G., and Huberman, B. A. 2010. Predicting the popularity of online content. Com-munications of the 
ACM, 53(8), 80-88. 
Varela, F. J., Thompson, E. T., & Rosch, E. 1991. The embodied mind: Cognitive science and human experi-
ence. The MIT Press. 
Yano, T., Cohen, W. W., & Smith, N. A. 2009. Predicting response to political blog posts with topic models. In 
Proceedings of NAACL. pp. 477-485. 
1488
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Modeling Interestingness with Deep Neural Networks 
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.com 
 
 
Abstract 
This paper presents a deep semantic simi-
larity model (DSSM), a special type of 
deep neural networks designed for text 
analysis, for recommending target docu-
ments to be of interest to a user based on a 
source document that she is reading. We 
observe, identify, and detect naturally oc-
curring signals of interestingness in click 
transitions on the Web between source and 
target documents, which we collect from 
commercial Web browser logs. The DSSM 
is trained on millions of Web transitions, 
and maps source-target document pairs to 
feature vectors in a latent space in such a 
way that the distance between source doc-
uments and their corresponding interesting 
targets in that space is minimized. The ef-
fectiveness of the DSSM is demonstrated 
using two interestingness tasks: automatic 
highlighting and contextual entity search. 
The results on large-scale, real-world da-
tasets show that the semantics of docu-
ments are important for modeling interest-
ingness and that the DSSM leads to signif-
icant quality improvement on both tasks, 
outperforming not only the classic docu-
ment models that do not use semantics but 
also state-of-the-art topic models. 
1 Introduction 
Tasks of predicting what interests a user based on 
the document she is reading are fundamental to 
many online recommendation systems. A recent 
survey is due to Ricci et al. (2011). In this paper, 
we exploit the use of a deep semantic model for 
two such interestingness tasks in which document 
semantics play a crucial role: automatic highlight-
ing and contextual entity search. 
Automatic Highlighting. In this task we want 
a recommendation system to automatically dis-
cover the entities (e.g., a person, location, organi-
zation etc.) that interest a user when reading a doc-
ument and to highlight the corresponding text 
spans, referred to as keywords afterwards. We 
show in this study that document semantics are 
among the most important factors that influence 
what is perceived as interesting to the user. For 
example, we observe in Web browsing logs that 
when a user reads an article about a movie, she is 
more likely to browse to an article about an actor 
or character than to another movie or the director. 
Contextual entity search. After identifying 
the keywords that represent the entities of interest 
to the user, we also want the system to recommend 
new, interesting documents by searching the Web 
for supplementary information about these enti-
ties. The task is challenging because the same key-
words often refer to different entities, and interest-
ing supplementary information to the highlighted 
entity is highly sensitive to the semantic context. 
For example, ?Paul Simon? can refer to many peo-
ple, such as the singer and the senator. Consider 
an article about the music of Paul Simon and an-
other about his life. Related content about his up-
coming concert tour is much more interesting in 
the first context, while an article about his family 
is more interesting in the second. 
At the heart of these two tasks is the notion of 
interestingness. In this paper, we model and make 
use of this notion of interestingness with a deep 
semantic similarity model (DSSM). The model, 
extending from the deep neural networks shown 
recently to be highly effective for speech recogni-
tion (Hinton et al., 2012; Deng et al., 2013) and 
computer vision (Krizhevsky et al., 2012; Mar-
koff, 2014), is semantic because it maps docu-
ments to feature vectors in a latent semantic space, 
also known as semantic representations. The 
model is deep because it employs a neural net-
work with several hidden layers including a spe-
cial convolutional-pooling structure to identify 
keywords and extract hidden semantic features at 
different levels of abstractions, layer by layer. The 
semantic representation is computed through a 
deep neural network after its training by back-
propagation with respect to an objective tailored 
2
to the respective interestingness tasks. We obtain 
naturally occurring ?interest? signals by observ-
ing Web browser transitions, from a source docu-
ment to a target document, in Web usage logs of a 
commercial browser. Our training data is sampled 
from these transitions. 
The use of the DSSM to model interestingness 
is motivated by the recent success of applying re-
lated deep neural networks to computer vision 
(Krizhevshy et al. 2012; Markoff, 2014), speech 
recognition (Hinton et al. 2012), text processing 
(Collobert et al. 2011),  and Web search (Huang 
et al. 2013). Among them, (Huang et al. 2013) is 
most relevant to our work. They also use a deep 
neural network to map documents to feature vec-
tors in a latent semantic space. However, their 
model is designed to represent the relevance be-
tween queries and documents, which differs from 
the notion of interestingness between documents 
studied in this paper. It is often the case that a user 
is interested in a document because it provides 
supplementary information about the entities or 
concepts she encounters when reading another 
document although the overall contents of the sec-
ond documents is not highly relevant. For exam-
ple, a user may be interested in knowing more 
about the history of University of Washington af-
ter reading the news about President Obama?s 
visit to Seattle. To better model interestingness, 
we extend the model of Huang et al. (2013) in two 
significant aspects. First, while Huang et al. treat 
a document as a bag of words for semantic map-
ping, the DSSM treats a document as a sequence 
of words and tries to discover prominent key-
words. These keywords represent the entities or 
concepts that might interest users, via the convo-
lutional and max-pooling layers which are related 
to the deep models used for computer vision 
(Krizhevsky et al., 2013) and speech recognition 
(Deng  et al., 2013a) but are not used in Huang et 
al.?s model. The DSSM then forms the high-level 
semantic representation of the whole document 
based on these keywords. Second, instead of di-
rectly computing the document relevance score 
using cosine similarity in the learned semantic 
space, as in Huang et al. (2013), we feed the fea-
tures derived from the semantic representations of 
documents to a ranker which is trained in a super-
vised manner. As a result, a document that is not 
highly relevant to another document a user is read-
ing (i.e., the distance between their derived feature 
                                                          
1 We stress here that, although the click signal is available to 
form a dataset and a gold standard ranker (to be described in 
vectors is big) may still have a high score of inter-
estingness because the former provides useful in-
formation about an entity mentioned in the latter. 
Such information and entity are encoded, respec-
tively, by (some subsets of) the semantic features 
in their corresponding documents. In Sections 4 
and 5, we empirically demonstrate that the afore-
mentioned two extensions lead to significant qual-
ity improvements for the two interestingness tasks 
presented in this paper.  
Before giving a formal description of the 
DSSM in Section 3, we formally define the inter-
estingness function, and then introduce our data 
set of naturally occurring interest signals. 
2 The Notion of Interestingness 
Let ?  be the set of all documents. Following 
Gamon et al. (2013), we formally define the inter-
estingness modeling task as learning the mapping 
function: 
	 ?: ? ? ? ? ??	 		
where the function ???, ?? is the quantified degree 
of interest that the user has  in the target document 
? ? ? after or while reading the source document 
? ? ?. 
Our notion of a document is meant in its most 
general form as a string of raw unstructured text. 
That is, the interestingness function should not 
rely on any document structure such as title tags, 
hyperlinks, etc., or Web interaction data. In our 
tasks, documents can be formed either from the 
plain text of a webpage or as a text span in that 
plain text, as will be discussed in Sections 4 and 5. 
2.1 Data 
We can observe many naturally occurring mani-
festations of interestingness on the Web. For ex-
ample, on Twitter, users follow shared links em-
bedded in tweets. Arguably the most frequent sig-
nal, however, occurs in Web browsing events 
where users click from one webpage to another 
via hyperlinks. When a user clicks on a hyperlink, 
it is reasonable to assume that she is interested in 
learning more about the anchor, modulo cases of 
erroneous clicks. Aggregate clicks can therefore 
serve as a proxy for interestingness. That is, for a 
given source document, target documents that at-
tract the most clicks are more interesting than doc-
uments that attract fewer clicks1.  
Section 4), our task is to model interestingness between un-
structured documents, i.e., without access to any document 
structure or Web interaction data. Thus, in our experiments, 
3
We collect a large dataset of user browsing 
events from a commercial Web browser. Specifi-
cally, we sample 18 million occurrences of a user 
click from one Wikipedia page to another during 
a one year period. We restrict our browsing events 
to Wikipedia since its pages tend to contain many 
anchors (79 on average, where on average 42 have 
a unique target URL). Thus, they attract enough 
traffic for us to obtain robust browsing transition 
data2. We group together all transitions originat-
ing from the same page and randomly hold out 
20% of the transitions for our evaluation data 
(EVAL), 20% for training the DSSM described in 
Section 3.2 (TRAIN_1), and the remaining 60% 
for training our task specific rankers described in 
Section 3.3 (TRAIN_2). In our experiments, we 
used different settings for the two interestingness 
tasks. Thus, we postpone the detailed description 
of these datasets and other task-specific datasets 
to Sections 4 and 5. 
3 A Deep Semantic Similarity Model 
(DSSM) 
This section presents the architecture of the 
DSSM, describes the parameter estimation, and 
the way the DSSM is used in our tasks. 
                                                          
we remove all structural information (e.g., hyperlinks and 
XML tags) in our documents, except that in the highlighting 
experiments (Section 4) we use anchor texts to simulate the 
candidate keywords to be highlighted. We then convert each 
3.1 Network Architecture 
The heart of the DSSM is a deep neural network 
with convolutional structure, as shown in Figure 
1. In what follows, we use lower-case bold letters, 
such as ?, to denote column vectors, ???? to de-
note the ??? element of ?, and upper-case letters, 
such as ?, to denote matrices. 
Input Layer ?. It takes two steps to convert a doc-
ument ?, which is a sequence of words, into a vec-
tor representation ? for the input layer of the net-
work: (1) convert each word in ? to a word vector, 
and (2) build ? by concatenating these word vec-
tors. To convert a word ? into a word vector, we 
first represent ? by a one-hot vector using a vo-
cabulary that contains ?  high frequent words 
(? ? 150K in this study). Then, following Huang 
et al. (2013), we map ? to a separate tri-letter vec-
tor. Consider the word ?#dog#?, where # is a word 
boundary symbol. The nonzero elements in its tri-
letter vector are ?#do?, ?dog?, and ?og#?. We then 
form the word vector of ? by concatenating its 
one-hot vector and its tri-letter vector. It is worth 
noting that the tri-letter vector complements the 
one-hot vector representation in two aspects. First, 
different OOV (out of vocabulary) words can be 
represented by tri-letter vectors with few colli-
sions. Second, spelling variations of the same 
word can be mapped to the points that are close to 
each other in the tri-letter space. Although the 
number of unique English words on the Web is 
extremely large, the total number of distinct tri-
letters in English is limited (restricted to the most 
frequent 30K in this study). As a result, incorpo-
rating tri-letter vectors substantially improves the 
representation power of word vectors while keep-
ing their size small.  
To form our input layer ? using word vectors, 
we first identify a text span with a high degree of 
relevance, called focus, in ?  using task-specific 
heuristics (see Sections 4 and 5 respectively). Sec-
ond, we form ? by concatenating each word vec-
tor in the focus and a vector that is the summation 
of all other word vectors, as shown in Figure 1. 
Since the length of the focus is much smaller than 
that of its document, ? is able to capture the con-
textual information (for the words in the focus) 
Web document into plain text, which is white-space to-
kenized and lowercased. Numbers are retained and no stem-
ming is performed. 
2 We utilize the May 3, 2013 English Wikipedia dump con-
sisting of roughly 4.1 million articles from http://dumps.wiki-
media.org. 
Figure 1: Illustration of the network architec-
ture and information flow of the DSSM 
 
4
useful to the corresponding tasks, with a manage-
able vector size. 
Convolutional Layer ? . A convolutional layer 
extracts local features around each word ??	in a 
word sequence of length ?  as follows. We first 
generate a contextual vector ??  by concatenating 
the word vectors of ?? and its surrounding words defined by a window (the window size is set to 3 
in this paper). Then, we generate for each word a 
local feature vector ??  using a tanh  activation 
function and a linear projection matrix ??, which 
is the same across all windows ? in the word se-
quence, as: 
?? ? tanh??????? , where	? ? 1? 1) ?) 
Max-pooling Layer ?. The size of the output ? 
depends on the number of words in the word se-
quence. Local feature vectors have to be com-
bined to obtain a global feature vector, with a 
fixed size independent of the document length, in 
order to apply subsequent standard affine layers. 
We design ? by adopting the max operation over 
each ?time? ? of the sequence of vectors computed 
by (1), which forces the network to retain only the 
most useful, partially invariant local features pro-
duced by the convolutional layer: 
???? ? max???,?,??u????? (2) 
where the max operation is performed for each di-
mension of ? across ? ? 1,? , ? respectively.  
That convolutional and max-pooling layers are 
able to discover prominent keywords of a docu-
ment can be demonstrated using the procedure in 
Figure 2 using a toy example. First, the convolu-
tional layer of (1) generates for each word in a 5-
word document a 4-dimensional local feature vec-
tor, which represents a distribution of four topics. 
For example, the most prominent topic of ?? within its three word context window is the first 
topic, denoted by ???1?, and the most prominent 
topic of ?? is ???3?. Second, we use max-pooling of (2) to form a global feature vector, which rep-
resents the topic distribution of the whole docu-
ment. We see that ??1? and ??3? are two promi-
nent topics. Then, for each prominent topic, we 
trace back to the local feature vector that survives 
max-pooling: 
??1? ? max???,?,?????1?? ? ???1?  
??3? ? max???,?,?????3?? ? ???3?.  
Finally, we label the corresponding words of these 
local feature vectors, ?? and ??, as keywords of the document.  
Figure 3 presents a sample of document snip-
pets and their keywords detected by the DSSM ac-
cording to the procedure elaborated in Figure 2. It 
is interesting to see that many names are identified 
as keywords although the DSSM is not designed 
explicitly for named entity recognition. 
Fully-Connected Layers ?  and ? . The fixed 
sized global feature vector ? of (2) is then fed to 
several standard affine network layers, which are 
stacked and interleaved with nonlinear activation 
functions, to extract highly non-linear features ? 
at the output layer. In our model, shown in Figure 
1, we have: 
? ? tanh?????? (3) 
? ? tanh?????? (4) 
where ?? and ?? are learned linear projection matri-ces. 
3.2 Training the DSSM 
To optimize the parameters of the DSSM of Fig-
ure 1, i.e., ? ? ???,??,???, we use a pair-wise rank loss as objective (Yih et al. 2011). Consider 
a source document ?  and two candidate target 
documents ??	and ??, where ?? is more interesting 
than ??  to a user when reading ?. We construct 
two pairs of documents ??, ??? and ??, ???, where the former is preferred and should have a higher 
u1 u2 u3 u4 u5
w1 w2 w3 w4 w5
2
3
4
1
 
w1 w2 w3 w4 w5
v
2
3
4
1
Figure 2: Toy example of (upper) a 5-word 
document and its local feature vectors ex-
tracted using a convolutional layer, and (bot-
tom) the global feature vector of the document 
generated after max-pooling. 
 
 
5
interestingness score. Let ? be the difference of 
their interestingness scores: ?	? ???, ??? ?
???, ??? , where ?  is the interestingness score, computed as the cosine similarity: 
???, ?? ? sim???, ?? ?
?????
???????? 
(5) 
where ?? and ?? are the feature vectors of ? and ?, respectively, which are generated using the 
DSSM, parameterized by ?. Intuitively, we want 
to learn ? to maximize ?. That is, the DSSM is 
learned to represent documents as points in a hid-
den interestingness space, where the similarity be-
tween a document and its interesting documents is 
maximized.  
We use the following logistic loss over ? , 
which can be shown to upper bound the pairwise 
accuracy: 
???; ?? ? log?1 ? exp?????? (6) 
                                                          
3 In our experiments, we observed better results by sampling 
more negative training examples (e.g., up to 100) although 
this makes the training much slower. An alternative approach 
The loss function in (6) has a shape similar to the 
hinge loss used in SVMs. Because of the use of 
the cosine similarity function, we add a scaling 
factor ? that magnifies ? from [-2, 2] to a larger 
range. Empirically, the value of ? makes no dif-
ference as long as it is large enough. In the exper-
iments, we set ? ? 10. Because the loss function 
is differentiable, optimizing the model parameters 
can be done using gradient-based methods. Due to 
space limitations, we omit the derivation of the 
gradient of the loss function, for which readers are 
referred to related derivations (e.g., Collobert et 
al. 2011; Huang et al. 2013; Shen et al. 2014). 
In our experiments we trained DSSMs using 
mini-batch Stochastic Gradient Descent. Each 
mini-batch consists of 256 source-target docu-
ment pairs. For each source document ?, we ran-
domly select from that batch four target docu-
ments which are not paired with ?  as negative 
training samples3. The DSSM trainer is imple-
mented using a GPU-accelerated linear algebra li-
brary, which is developed on CUDA 5.5. Given 
the training set (TRAIN_1 in Section 2), it takes 
approximately 30 hours to train a DSSM as shown 
in Figure 1, on a Xeon E5-2670 2.60GHz machine 
with one Tesla K20 GPU card. 
In principle, the loss function of (6) can be fur-
ther regularized (e.g. by adding a term of 2? norm) 
to deal with overfitting. However, we did not find 
a clear empirical advantage over the simpler early 
stop approach in a pilot study, hence we adopted 
the latter in the experiments in this paper. Our ap-
proach adjusts the learning rate ?  during the 
course of model training. Starting with ? ? 1.0, 
after each epoch (a pass over the entire training 
data), the learning rate is adjusted as ? ? 0.5 ? ? 
if the loss on validation data (held-out from 
TRAIN_1) is not reduced. The training stops if 
either ?  is smaller than a preset threshold 
(0.0001) or the loss on training data can no longer 
be reduced significantly. In our experiments, the 
DSSM training typically converges within 20 
epochs. 
3.3 Using the DSSM 
We experiment with two ways of using the DSSM 
for the two interestingness tasks. First, we use the 
DSSM as a feature generator. The output layer of 
the DSSM can be seen as a set of semantic fea-
tures, which can be incorporated in a boosted tree 
is to approximate the partition function using Noise Contras-
tive Estimation (Gutmann and Hyvarinen 2010). We leave it 
to future work.  
? the comedy festival formerly known as 
the us comedy arts festival is a comedy 
festival held each year in las vegas 
nevada from its 1985 inception to 2008 
. it was held annually at the wheeler 
opera house and other venues in aspen 
colorado . the primary sponsor of the 
festival was hbo with co-sponsorship by 
caesars palace . the primary venue tbs 
geico insurance twix candy bars and 
smirnoff vodka hbo exited the festival 
business in 2007 and tbs became the pri-
mary sponsor the festival includes 
standup comedy performances appearances 
by the casts of television shows? 
 
? bad samaritans is an american comedy
series produced by walt becker kelly
hayes and ross putman . it premiered on 
netflix on march 31 2013 cast and char-
acters . the show focuses on a community 
service parole group and their parole 
officer brian kubach as jake gibson an 
aspiring professional starcraft player 
who gets sentenced to 2000 hours of com-
munity service for starting a forest 
fire during his breakup with drew prior 
to community service he had no real am-
bition in life other than to be a pro-
fessional gamer and become wealthy 
overnight like mark zuckerberg as in 
life his goal during ? 
Figure 3: A sample of document snippets and 
the keywords (in bold) detected by the DSSM. 
 
 
6
based ranker (Friedman 1999) trained discrimina-
tively on the task-specific data. Given a source-
target document pair ??, ??, the DSSM generates 
600 features (300 from the output layers ?? and ?? 
for each ? and ?, respectively). 
Second, we use the DSSM as a direct imple-
mentation of the interestingness function ?. Re-
call from Section 3.2 that in model training, we 
measure the interestingness score for a document 
pair using the cosine similarity between their cor-
responding feature vectors (?? and ??). Similarly 
at runtime, we define	? ? 	sim???, ?? as (5). 
4 Experiments on Highlighting 
Recall from Section 1 that in this task, a system 
must select ? most interesting keywords in a doc-
ument that a user is reading. To evaluate our mod-
els using the click transition data described in Sec-
tion 2, we simulate the task as follows. We use the 
set of anchors in a source document ? to simulate 
the set of candidate keywords that may be of in-
terest to the user while reading ?, and treat the text 
of a document that is linked by an anchor in ? as a 
target document ?. As shown in Figure 1, to apply 
DSSM to a specific task, we need to define the fo-
cus in source and target documents. In this task, 
the focus in s is defined as the anchor text, and the 
focus in t is defined as the first 10 tokens in t. 
We evaluate the performance of a highlighting 
system against a gold standard interestingness 
function ?? which scores the interestingness of an 
anchor as the number of user clicks on ? from the 
anchor in ? in our data. We consider the ideal se-
lection to then consist of the ?  most interesting 
anchors according to ??. A natural metric for this 
task is Normalized Discounted Cumulative Gain 
(NDCG) (Jarvelin and Kekalainen 2000). 
We evaluate our models on the EVAL dataset 
described in Section 2. We utilize the transition 
distributions in EVAL to create three other test 
sets, following the stratified sampling methodol-
ogy commonly employed in the IR community, 
for the frequently, less frequently, and rarely 
viewed source pages, referred to as HEAD, 
TORSO, and TAIL, respectively. We obtain 
these sets by first sorting the unique source docu-
ments according to their frequency of occurrence 
in EVAL. We then partition the set so that HEAD 
corresponds to all transitions from the source 
pages at the top of the list that account for 20% of 
the transitions in EVAL; TAIL corresponds to the 
transitions at the bottom also accounting for 20% 
of the transitions in EVAL; and TORSO corre-
sponds to the remaining transitions. 
4.1 Main Results 
Table 1 summarizes the results of various models 
over the three test sets using NDCG at truncation 
levels 1, 5, and 10. 
Rows 1 to 3 are simple heuristic baselines. 
RAND selects ?  random anchors, 1stK selects 
the first ? anchors and LastK the last ? anchors.  
The other models in Table 1 are boosted tree 
based rankers trained on TRAIN_2 described in 
Section 2. They vary only in their features. The 
ranker in Row 4 uses Non-Semantic Features 
(NSF) only. These features are derived from the 
 # Models HEAD TORSO TAIL 
   @1 @5 @10 @1 @5 @10 @1 @5 @10 
src
  o
nly
 
1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.258 
2 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.348 
3 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.219
4 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.365 
5 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.386 
6 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.372 
8 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382 
src
+ta
r 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.380 
12 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398 
Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test (? ?
0.05). 
 
 
7
source document s and from user session infor-
mation in the browser log. The document features 
include: position of the anchor in the document, 
frequency of the anchor, and anchor density in the 
paragraph.  
The rankers in Rows 5 to 12 use the NSF and 
the semantic features computed from source and 
target documents of a browsing transition. We 
compare semantic features derived from three dif-
ferent sources. The first feature source comes 
from our DSSMs (DSSM and DSSM_BOW) us-
ing the output layers as feature generators as de-
scribed in Section 3.3. DSSM is the model de-
scribed in Section 3 and DSSM_BOW is the 
model proposed by Huang et al. (2013) where 
documents are view as bag of words (BOW) and 
the convolutional and max-pooling layers are not 
used. The two other sources of semantic features 
are used as a point of comparison to the DSSM. 
One is a generative semantic model (Joint Transi-
tion Topic model, or JTT) (Gamon et al. 2013). 
JTT is an LDA-style model (Blei et al. 2003) that 
is trained jointly on source and target documents 
linked by browsing transitions. JTT generates a 
total of 150 features from its latent variables, 50 
each for the source topic model, the target topic 
model and the transition model. The other seman-
tic model of contrast is a manually defined one, 
which we use to assess the effectiveness of auto-
matically learned models against human model-
ers. To this effect, we use the page categories that 
editors assign in Wikipedia as semantic features 
(WCAT). These features number in the multiple 
thousands. Using features such as WCAT is not a 
viable solution in general since Wikipedia catego-
ries are not available for all documents. As such, 
we use it solely as a point of comparison against 
DSSM and JTT. 
We also distinguish between two types of 
learned rankers: those which draw their features 
only from the source (src only) document and 
those that draw their features from both the source 
and target (src+tar) documents. Although our 
task setting allows access to the content of both 
source and target documents, there are practical 
scenarios where a system should predict what in-
terests the user without looking at the target doc-
ument because the extra step of identifying a suit-
able target document for each candidate concept 
or entity of interest is computationally expensive.  
4.2 Analysis of Results 
As shown in Table 1, NSF+DSSM, which incor-
porates our DSSM, is the overall best performing 
system across test sets. The task is hard as evi-
denced by the weak baseline scores. One reason is 
the large average number of candidates per page. 
On HEAD, we found an average of 170 anchors 
(of which 95 point to a unique target URL). For 
TORSO and TAIL, we found the average number 
of anchors to be 94 (52 unique targets) and 41 (19 
unique targets), respectively. 
Clearly, the semantics of the documents form 
important signals for this task: WCAT, JTT, 
DSSM_BOW, and DSSM all significantly boost 
the performance over NSF alone. There are two 
interesting comparisons to consider: (a) manual 
semantics vs. learned semantics; and (b) deep se-
mantic models vs. generative topic models. On 
(a), we observe somewhat surprisingly that the 
learned DSSM produces features that outperform 
the thousands of features coming from manually 
(editor) assigned Wikipedia category features 
(WCAT), in all but the TAIL where the two per-
form statistically the same. In contrast, features 
from the generative model (JTT) perform worse 
than WCAT across the board except on TAIL 
where JTT and WCAT are statistically tied. On 
(b), we observe that DSSM outperforms a state-
of-the-art generative model (JTT) on HEAD and 
TORSO. On TAIL, they are statistically indistin-
guishable. 
We turn now to inspecting the scenario where 
features are only drawn from the source document 
(Rows 1-8 in Table 1). Again we observe that se-
mantic features significantly boost the perfor-
mance against NSF alone, however they signifi-
cantly deteriorate when compared to using fea-
tures from both source and target documents. In 
this scenario, the manual semantics from WCAT 
outperform all other models, but with a diminish-
ing effect as we move from HEAD through 
TORSO to TAIL. DSSM is the best performing 
learned semantic model. 
Finally, we present the results to justify the two 
modifications we made to extend the model of 
Huang et al. (2013) to the DSSM, as described in 
Section 1. First, we see in Table 1 that 
DSSM_BOW, which has the same network struc-
ture of Huang et al.?s model, is much weaker than 
DSSM, demonstrating the benefits of using con-
volutional and max-pooling layers to extract se-
mantic features for the highlighting task. Second, 
we conduct several experiments by using the co-
sine scores between the output layers of DSSM 
for ? and ? as features (following the procedure in 
Section 3.3 for using the DSSM as a direct imple-
mentation of ?). We found that adding the cosine 
8
features to NSF+DSSM does not lead to any im-
provement. We also combined NSF with solely 
the cosine features from DSSM (i.e., without the 
other semantic features drawn from its output lay-
ers). But we still found no improvement over us-
ing NSF alone. Thus, we conclude that for this 
task it is much more effective to feed the features 
derived from DSSM to a supervised ranker than 
directly computing the interestingness score using 
cosine similarity in the learned semantic space, as 
in Huang et al. (2013). 
5 Experiments on Entity Search 
We construct the evaluation data set for this sec-
ond task by randomly sampling a set of documents 
from a traffic-weighted set of Web documents. In 
a second step, we identify the entity names in each 
document using an in-house named entity recog-
nizer. We issue each entity name as a query to a 
commercial search engine, and retain up to the 
top-100 retrieved documents as candidate target 
documents. We form for each entity a source doc-
ument which consists of the entity text and its sur-
rounding text defined by a 200-word window. We 
define the focus (as in Figure 1) in ? as the entity 
text, and the focus in ? as the first 10 tokens in ?. 
The final evaluation data set contains 10,000 
source documents. On average, each source docu-
ment is associated with 87 target documents. Fi-
nally, the source-target document pairs are labeled 
in terms of interestingness by paid annotators. The 
label is on a 5-level scale, 0 to 4, with 4 meaning 
the target document is the most interesting to the 
source document and 0 meaning the target is of no 
interest. 
We test our models on two scenarios. The first 
is a ranking scenario where ?  interesting docu-
ments are displayed to the user. Here, we select 
the top-? ranked documents according to their in-
terestingness scores. We measure the performance 
via NDCG at truncation levels 1 and 3. The sec-
ond scenario is to display to the user all interesting 
results. In this scenario, we select all target docu-
ments with an interestingness score exceeding a 
predefined threshold. We evaluate this scenario 
using ROC analysis and, specifically, the area un-
der the curve (AUC). 
5.1 Main Results 
The main results are summarized in Table 2. Rows 
1 to 6 are single model results, where each model 
is used as a direct implementation of the interest-
ingness function ?. Rows 7 to 9 are ranker results, 
where ? is defined as a boosted tree based ranker 
that incorporates different sets of features ex-
tracted from source and target documents, includ-
ing the features derived from single models. As in 
the highlighting experiments, all the machine-
learned single models, including the DSSM, are 
trained on TRAIN_1, and all the rankers are 
trained on TRAIN_2. 
5.2 Analysis of Results 
BM25 (Rows 1 and 2 in Table 2) is the classic 
document model (Robertson and Zaragoza 2009). 
It uses the bag-of-words document representation 
and the BM25 term weighting function. In our set-
ting, we define the interestingness score of a doc-
ument pair as the dot product of their BM25-
weighted term vectors. To verify the importance 
of using contextual information, we compare two 
different ways of forming the term vector of a 
source document. The first only uses the entity 
text (Row 1). The second (Row 2) uses both the 
entity text and and its surrounding text in a 200-
word window (i.e., the entire source document). 
Results show that the model using contextual in-
formation is significantly better. Therefore, all the 
other models in this section use both the entity 
texts and their surrounding text. 
WTM (Row 3) is our implementation of the 
word translation model for IR (Berger and Laf-
ferty 1999; Gao et al. 2010). WTM defines the in-
terestingness score as: 
???, ?? ? ? ? ????|???????|?????????? ,  
# Models @1 @3 AUC 
1 BM25 (entity)  0.133 0.195 0.583 
2 BM25 0.142 0.227 0.675 
3 WTM 0.191 0.287 0.678 
4 BLTM 0.214 0.306 0.704 
5 DSSM 0.259* 0.356* 0.711* 
6 DSSM_BOW 0.223 0.322 0.699 
7 Baseline ranker 0.283 0.360 0.723 
8 7 + DSSM(1) 0.301# 0.385# 0.758# 
9 7 + DSSM(600) 0.327## 0.402## 0.782##
Table 2: Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test (? ?
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. 
 
 
 
9
where ????|?? is the unigram probability of word 
?? in ?, and ????|??? is the probability of trans-
lating ?? into ??, trained on source-target docu-ment pairs using EM (Brown et al. 1993). The 
translation-based approach allows any pair of 
non-identical but semantically related words to 
have a nonzero matching score. As a result, it sig-
nificantly outperforms BM25. 
BTLM (Row 4) follows the best performing 
bilingual topic model described in Gao et al. 
(2011), which is an extension of PLSA (Hofmann 
1999). The model is trained on source-target doc-
ument pairs using the EM algorithm with a con-
straint enforcing a source document ? and its tar-
get document ? to not only share the same prior 
topic distribution, but to also have similar frac-
tions of words assigned to each topic. BLTM de-
fines the interestingness score between s and t as: 
???, ?? ? ? ? ????|??????|???????? .  
The model assumes the following story of gener-
ating ? from ?. First, for each topic ? a word dis-
tribution ?? is selected from a Dirichlet prior with 
concentration parameter ? . Second, given ? , a 
topic distribution ??  is drawn from a Dirichlet 
prior with parameter ? . Finally, ?  is generated 
word by word. Each word ?? is generated by first 
selecting a topic ?  according to ?? , and then 
drawing a word from ?? . We see that BLTM models interestingness by taking into account the 
semantic topic distribution of the entire docu-
ments. Our results in Table 2 show that BLTM 
outperforms WTM by a significant margin in 
both NDCG and AUC. 
DSSM (Row 5) outperforms all the competing 
single models, including the state-of-the-art topic 
model BLTM. Now, we inspect the difference be-
tween DSSM and BLTM in detail. Although both 
models strive to generate the semantic representa-
tion of a document, they use different modeling 
approaches. BLTM by nature is a generative 
model. The semantic representation in BLTM is a 
distribution of hidden semantic topics. Such a dis-
tribution is learned using Maximum Likelihood 
Estimation in an unsupervised manner, i.e., max-
imizing the log-likelihood of the source-target 
document pairs in the training data. On the other 
hand, DSSM represents documents as points in a 
hidden semantic space using a supervised learning 
method, i.e., paired documents are closer in that 
latent space than unpaired ones. We believe that 
the superior performance of DSSM is largely due 
to the fact that the model parameters are discrimi-
natively trained using an objective that is tailored 
to the interestingness task.  
In addition to the difference in training meth-
ods, DSSM and BLTM also use different model 
structures. BLTM treats a document as a bag of 
words (thus losing some important contextual in-
formation such as word order and inter-word de-
pendencies), and generates semantic representa-
tions of documents using linear projection. 
DSSM, on the other hand, treats text as a sequence 
of words and better captures local and global con-
text, and generates highly non-linear semantic 
features via a deep neural network. To further ver-
ify our analysis, we inspect the results of a variant 
of DSSM, denoted as DSSM_BOW (Row 6), 
where the convolution and max-pooling layers are 
removed. This model treats a document as a bag 
of words, just like BLTM. These results demon-
strate that the effectiveness of DSSM can also be 
attributed to the convolutional architecture in the 
neural network, in addition to being deep and be-
ing discriminative. 
We turn now to discussing the ranker results in 
Rows 7 to 9. The baseline ranker (Row 7) uses 158 
features, including many counts and single model 
scores, such as BM25 and WMT. DSSM (Row 5) 
alone is quite effective, being close in perfor-
mance to the baseline ranker with non-DSSM fea-
tures. Integrating the DSSM score computed in (5) 
as one single feature into the ranker (Row 8) leads 
to a significant improvement over the baseline. 
The best performing combination (Row 9) is ob-
tained by incorporating the DSSM feature vectors 
of source and target documents (i.e., 600 features 
in total) in the ranker. 
We thus conclude that on both tasks, automatic 
highlighting and contextual entity search, features 
drawn from the output layers of our deep semantic 
model result in significant gains after being added 
to a set of non-semantic features, and in compari-
son to other types of semantic models used in the 
past. 
6 Related Work 
In addition to the notion of relevance as described 
in Section 1, related to interestingness is also the 
notion of salience (also called aboutness) (Gamon 
et al. 2013; 2014; Parajpe 2009; Yih et al. 2006). 
Salience is the centrality of a term to the content 
of a document. Although salience and interesting-
ness interact, the two are not the same. For exam-
ple, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average 
user would probably not be interested in learning 
more about Obama while reading that article.  
10
There are many systems that identify popular 
content in the Web or recommend content (e.g., 
Bandari et al. 2012; Lerman and Hogg 2010; 
Szabo and Huberman 2010), which is closely re-
lated to the highlighting task. In contrast to these 
approaches, we strive to predict what term a user 
is likely to be interested in when reading content, 
which may or may not be the same as the most 
popular content that is related to the current docu-
ment. It has empirically been demonstrated in 
Gamon et al. (2013) that popularity is in fact a ra-
ther poor predictor for interestingness. The task of 
contextual entity search, which is formulated as an 
information retrieval problem in this paper, is also 
related to research on entity resolution (Stefanidis 
et al. 2013).  
Latent Semantic Analysis (Deerwester et al. 
1990) is arguably the earliest semantic model de-
signed for IR. Generative topic models widely 
used for IR include PLSA (Hofmann 1990) and 
LDA (Blei et al. 2003). Recently, these models 
have been extended to handle cross-lingual cases, 
where there are pairs of corresponding documents 
in different languages (e.g., Dumais et al. 1997; 
Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). 
By exploiting deep architectures, deep learning 
techniques are able to automatically discover from 
training data the hidden structures and the associ-
ated features at different levels of abstraction use-
ful for a variety of tasks (e.g., Collobert et al. 
2011; Hinton et al. 2012; Socher et al. 2012; 
Krizhevsky et al., 2012; Gao et al. 2014). Hinton 
and Salakhutdinov (2010) propose the most origi-
nal approach based on an unsupervised version of 
the deep neural network to discover the hierar-
chical semantic structure embedded in queries and 
documents. Huang et al. (2013) significantly ex-
tends the approach so that the deep neural network 
can be trained on large-scale query-document 
pairs giving much better performance. The use of 
the convolutional neural network for text pro-
cessing, central to our DSSM, was also described 
in Collobert et al. (2011) and Shen et al. (2014) 
but with very different applications. The DSSM 
described in Section 3 can be viewed as a variant 
of the deep neural network models used in these 
previous studies. 
7 Conclusions 
Modeling interestingness is fundamental to many 
online recommendation systems. We obtain natu-
rally occurring interest signals by observing Web 
browsing transitions where users click from one 
webpage to another. We propose to model this 
?interestingness? with a deep semantic similarity 
model (DSSM), based on deep neural networks 
with special convolutional-pooling structure, 
mapping source-target document pairs to feature 
vectors in a latent semantic space. We train the 
DSSM using browsing transitions between docu-
ments. Finally, we demonstrate the effectiveness 
of our model on two interestingness tasks: auto-
matic highlighting and contextual entity search. 
Our results on large-scale, real-world datasets 
show that the semantics of documents computed 
by the DSSM are important for modeling interest-
ingness and that the new model leads to signifi-
cant improvements on both tasks. DSSM is shown 
to outperform not only the classic document mod-
els that do not use (latent) semantics but also state-
of-the-art topic models that do not have the deep 
and convolutional architecture characterizing the 
DSSM. 
One area of future work is to extend our 
method to model interestingness given an entire 
user session, which consists of a sequence of 
browsing events. We believe that the prior brows-
ing and interaction history recorded in the session 
provides additional signals for predicting interest-
ingness. To capture such signals, our model needs 
to be extended to adequately represent time series 
(e.g., causal relations and consequences of ac-
tions). One potentially effective model for such a 
purpose is based on the architecture of recurrent 
neural networks (e.g., Mikolov et al. 2010; Chen 
and Deng, 2014), which can be incorporated into 
the deep semantic model proposed in this paper. 
Additional Authors 
Yelong Shen (Microsoft Research, One Microsoft 
Way, Redmond, WA 98052, USA, email: 
yeshen@microsoft.com). 
Acknowledgments 
The authors thank Johnson Apacible, Pradeep 
Chilakamarri, Edward Guo, Bernhard Kohlmeier, 
Xiaolong Li, Kevin Powell, Xinying Song and 
Ye-Yi Wang for their guidance and valuable dis-
cussions. We also thank the three anonymous re-
viewers for their comments. 
References 
Bandari, R., Asur, S., and Huberman, B. A. 2012. 
The pulse of news in social media: forecasting 
popularity. In ICWSM. 
11
Bengio, Y., 2009. Learning deep architectures for 
AI. Fundamental Trends in Machine Learning, 
2(1):1?127. 
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet allocation. Journal of Machine 
Learning Research, 3. 
Broder, A., Fontoura, M., Josifovski, V., and 
Riedel, L. 2007. A semantic approach to contex-
tual advertising. In SIGIR. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. 
J., and Mercer, R. L. 1993. The mathematics of 
statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263-
311. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML, pp. 89-96.  
Chen, J. and Deng, L. 2014. A primal-dual method 
for training recurrent neural networks con-
strained by the echo-state property. In ICLR. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P., 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 12. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Indexing 
by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
Deng, L., Hinton, G., and Kingsbury, B. 2013. 
New types of deep neural network learning for 
speech recognition and related applications: An 
overview. In ICASSP. 
Deng, L., Abdel-Hamid, O., and Yu, D., 2013a. A 
deep convolutional neural network using heter-
ogeneous pooling for trading acoustic invari-
ance with phonetic confusion. In ICASSP. 
Dumais, S. T., Letsche, T. A., Littman, M. L., and 
Landauer, T. K. 1997. Automatic cross-linguis-
tic information retrieval using latent semantic 
indexing. In AAAI-97 Spring Symposium Series: 
Cross-Language Text and Speech Retrieval. 
Friedman, J. H. 1999. Greedy function approxi-
mation: a gradient boosting machine. Annals of 
Statistics, 29:1189-1232. 
Gamon, M., Mukherjee, A., Pantel, P. 2014. Pre-
dicting interesting things in text. In COLING. 
Gamon, M., Yano, T., Song, X., Apacible, J. and 
Pantel, P. 2013. Identifying salient entities in 
web pages. In CIKM. 
Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM. pp. 
1139-1148. 
Gao, J., He, X., Yih, W-t., and Deng, L. 2014. 
Learning continuous phrase representations for 
translation modeling. In ACL. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR. pp. 675-684.  
Graves, A., Mohamed, A., and Hinton, G. 2013. 
Speech recognition with deep recurrent neural 
networks. In ICASSP. 
Gutmann, M. and Hyvarinen, A. 2010. Noise-con-
trastive estimation: a new estimation principle 
for unnormalized statistical models. In Proc. 
Int. Conf. on Artificial Intelligence and Statis-
tics (AISTATS2010). 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, 29:82-97. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering binary codes for documents by learning 
deep generative models. Topics in Cognitive 
Science, pp. 1-18. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR. pp. 50-57. 
Huang, P., He, X., Gao, J., Deng, L., Acero, A., 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM. 
Jarvelin, K. and Kekalainen, J. 2000. IR evalua-
tion methods for retrieving highly relevant doc-
uments. In SIGIR. pp. 41-48. 
Krizhevsky, A., Sutskever, I. and Hinton, G. 
2012. ImageNet classification with deep convo-
lutional neural networks. In NIPS. 
Lerman, K., and Hogg, T. 2010. Using a model of 
social dynamics to predict popularity of news. 
In WWW. pp. 621-630. 
Markoff, J. 2014. Computer eyesight gets a lot 
more accurate. In New York Times. 
Mikolov, T.. Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In 
INTERSPEECH. pp. 1045-1048. 
Paranjpe, D. 2009. Learning document aboutness 
from implicit user feedback and document 
structure. In CIKM. 
12
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual document representations from 
discriminative projections. In EMNLP. pp. 251-
261. 
Ricci, F., Rokach, L., Shapira, B., and Kantor, P. 
B. (eds) 2011. Recommender System Handbook, 
Springer. 
Robertson, S., and Zaragoza, H. 2009. The proba-
bilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information 
Retrieval, 3(4):333-389. 
Shen, Y., He, X., Gao. J., Deng, L., and Mesnil, G. 
2014. A latent semantic model with convolu-
tional-pooling structure for information re-
trieval. In CIKM. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic compositionality through recursive 
matrix-vector spaces. In EMNLP. 
Stefanidis, K., Efthymiou, V., Herschel, M., and 
Christophides, V. 2013. Entity resolution in the 
web of data.  CIKM?13 Tutorial. 
Szabo, G., and Huberman, B. A. 2010. Predicting 
the popularity of online content. Communica-
tions of the ACM, 53(8). 
Wu, Q., Burges, C.J.C., Svore, K., and Gao, J. 
2009. Adapting boosting for information re-
trieval measures. Journal of Information Re-
trieval, 13(3):254-270. 
Yih, W., Goodman, J., and Carvalho, V. R. 2006. 
Finding advertising keywords on web pages. In 
WWW. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
 
13
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163?171,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Using Mostly Native Data to Correct Errors in Learners' Writing: 
A Meta-Classifier Approach 
 
Michael Gamon 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
 
Abstract 
We present results from a range of experi-
ments on article and preposition error correc-
tion for non-native speakers of English. We 
first compare a language model and error-
specific classifiers (all trained on large Eng-
lish corpora) with respect to their performance 
in error detection and correction. We then 
combine the language model and the classifi-
ers in a meta-classification approach by com-
bining evidence from the classifiers and the 
language model as input features to the meta-
classifier. The meta-classifier in turn is trained 
on error-annotated learner data, optimizing the 
error detection and correction performance on 
this domain. The meta-classification approach 
results in substantial gains over the classifier-
only and language-model-only scenario. Since 
the meta-classifier requires error-annotated 
data for training, we investigate how much 
training data is needed to improve results over 
the baseline of not using a meta-classifier. All 
evaluations are conducted on a large error-
annotated corpus of learner English. 
1 Introduction 
Research on the automatic correction of grammati-
cal errors has undergone a renaissance in the past 
decade. This is, at least in part, based on the recog-
nition that non-native speakers of English now 
outnumber native speakers by 2:1 in some esti-
mates, so any tool in this domain could be of tre-
mendous value. While earlier work in both native 
and non-native error correction was focused on the 
construction of grammars and analysis systems to 
detect and correct specific errors (see Heift and 
Schulze, 2005 for a detailed overview), more re-
cent approaches have been based on data-driven 
methods. 
The majority of the data-driven methods use a 
classification technique to determine whether a 
word is used appropriately in its context, continu-
ing the tradition established for contextual spelling 
correction by Golding (1995) and Golding and 
Roth (1996). The words investigated are typically 
articles and prepositions. They have two distinct 
advantages as the subject matter for investigation: 
They are a closed class and they comprise a sub-
stantial proportion of learners? errors. The investi-
gation of preposition corrections can even be 
narrowed further: amongst the more than 150 Eng-
lish prepositions, the usage of the ten most fre-
quent prepositions accounts for 82% of preposition 
errors in the 20 million word Cambridge Universi-
ty Press Learners? Corpus. Learning correct article 
use is most difficult for native speakers of an L1 
that does not overtly mark definiteness and indefi-
niteness as English does.  Prepositions, on the oth-
er hand, pose difficulties for language learners 
from all L1 backgrounds (Dalgish, 1995; Bitchener 
et al, 2005). 
Contextual classification methods represent the 
context of a preposition or article as a feature vec-
tor gleaned from a window of a few words around 
the preposition/article. Different systems typically 
vary along three dimensions: choice of features, 
choice of classifier, and choice of training data. 
Features range from words and morphological in-
formation (Knight and Chander, 1994) to the inclu-
sion of part-of-speech tags (Minnen et al, 2000; 
Han et al, 2004, 2006; Chodorow et al, 2007; 
Gamon et al, 2008, 2009; Izumi et al, 2003, 2004; 
Tetrault and Chodorow, 2008) to features based on 
linguistic analysis and on WordNet (Lee, 2004; 
DeFelice and Pulman, 2007, 2008). Knight and 
Chander (1994) and Gamon et al (2008) used de-
cision tree classifiers but, in general, maximum 
entropy classifiers have become the classification 
163
algorithm of choice. Training data are normally 
drawn from sizeable corpora of native English text 
(British National Corpus for DeFelice and Pulman 
(2007, 2008), Wall Street Journal in Knight and 
Chander (1994), a mix of Reuters and Encarta in 
Gamon et al (2008, 2009). In order to partially 
address the problem of domain mismatch between 
learners? writing and the news-heavy data sets of-
ten used in data-driven NLP applications, Han et 
al. (2004, 2006) use 31.5 million words from the 
MetaMetrics corpus, a diverse corpus of fiction, 
non-fiction and textbooks categorized by reading 
level. 
In addition to the classification approach to error 
detection, there is a line of research - going back to 
at least Atwell (1987) - that uses language models. 
The idea here is to detect errors in areas where the 
language model score is suspiciously low. Atwell 
(1987) uses a part-of-speech tag language model to 
detect errors, Chodorow and Leacock (2000) use 
mutual information and chi square statistics to 
identify unlikely function word and part-of-speech 
tag sequences, Turner and Charniak (2007) employ 
a language model based on a generative statistical 
parser, and Stehouwer and van Zaanen (2009) in-
vestigate a diverse set of language models with 
different backoff strategies to determine which 
choice, from a set of confusable words, is most 
likely in a given context. Gamon et al (2008, 
2009) use a combination of error-specific classifi-
ers and a large generic language model with hand-
tuned heuristics for combining their scores to max-
imize precision. Finally, Yi et al (2008) and Her-
met et al (2008) use n-gram counts from the web 
as a language model approximation to identify 
likely errors and correction candidates. 
2 Our Approach  
We combine evidence from the two kinds of data-
driven models that have been used for error detec-
tion and correction (error-specific classifiers and a 
language model) through a meta-classifier. We use 
the term primary models for both the initial error-
specific classifiers and a large generic language 
model. The meta-classifier takes the output of the 
primary models (language model scores and class 
probabilities) as input. Using a meta-classifier for 
ensemble learning has been proven effective for 
many machine learning problems (see e.g. Diette-
rich 1997), especially when the combined models 
are sufficiently different to make distinct kinds of 
errors. The meta-classification approach also has 
an advantage in terms of data requirements: Our 
primary models are trained on large sets of widely 
available well-formed English text. The meta-
classifier, in contrast, is trained on a smaller set of 
error-annotated learner data. This allows us to ad-
dress the problem of domain mismatch: We can 
leverage large well-formed data sets that are sub-
stantially different from real-life learner language 
for the primary models, and then fine-tune the out-
put to learner English using a much smaller set of 
expensive and hard-to-come-by annotated learner 
writing. 
For the purpose of this paper, we restrict our-
selves to article and preposition errors. The ques-
tions we address are: 
1. How effective is the meta-classification ap-
proach compared to either a classifier or a lan-
guage model alone? 
2. How much error-annotated data are sufficient 
to produce positive results above the baseline 
of using either a language model or a classifier 
alone? 
Our evaluation is conducted on a large data set 
of error-annotated learner data. 
3 Experimental Design 
3.1 Primary Models 
Our error-specific primary models are maximum 
entropy classifiers (Rathnaparki 1997) for articles 
and for prepositions. Features include contextual 
features from a window of six tokens to the right 
and left, such as lexical features (word), part-of-
speech tags, and a handful of ?custom features?, 
for example lexical head of governing VP or go-
verned NP (as determined by part-of-speech-tag 
based heuristics). For both articles and preposi-
tions, we employ two classifiers: the first deter-
mines the probability that a preposition/article is 
present in a given context (presence classifier), the 
second classifier determines the probability that a 
specific article or preposition is chosen (choice 
classifier). A training event for the presence clas-
sifier is any noun phrase boundary that is a poten-
tial location for a preposition or article. Whether a 
location is an NP boundary and a potential site for 
an article/preposition is determined by a simple 
heuristic based on part-of-speech tags.  
164
The candidates for article choice are the and 
a/an, and the choice for prepositions is limited to 
twelve very frequent prepositions (in, at, on, for, 
since, with, to, by, about, from, of, as) which ac-
count for 86.2 % of preposition errors in our learn-
er data. At prediction time, the presence and choice 
classifiers produce a list of potential changes in 
preposition/article usage for the given context. 
Since the application of our system consists of 
suggesting corrections to a user, we do not consid-
er identity operations where the suggested word 
choice equals the actual word choice. For a poten-
tial preposition/article location where there is no 
preposition/article, each of the candidates is consi-
dered for an insertion operation. For a potential 
location that contains a preposition/article, the 
possible operations include deletion of the existing 
token or substitution with another preposi-
tion/article from the candidate set. Training data 
for the classifiers is a mix of primarily well-formed 
data sources: There are about 2.5 million sen-
tences, distributed roughly equally across Reuters 
newswire, Encarta encyclopedia, UN proceedings, 
Europarl and web-scraped general domain data1. 
From the total set of candidate operations (substi-
tutions, insertions, and deletions) that each combi-
nation of presence and choice classifier produces 
for prepositions, we consider only the top three 
highest-scoring operations2. 
Our language model is trained on the Gigaword 
corpus (Linguistic Data Consortium, 2003) and 
utilizes 7-grams with absolute discount smoothing 
(Gao, Goodman, and Miao, 2001; Nguyen, Gao, 
and Mahajan, 2007). Each suggested revision from 
the preposition/article classifiers (top three for pre-
positions, all revisions from the article classifiers) 
are scored by the language model: for each revi-
sion, the language model score of the original and 
the suggested rewrite is recorded, as is the lan-
guage model entropy (defined as the language 
model probability of the sentence, normalized by 
sentence length). 
                                                          
1 We are not able to train the error-specific classifiers on a 
larger data set like the one we use for the language model. 
Note that the 2.5 million sentences used in the classifier train-
ing already produce 16.5 million training vectors.  
2 This increases runtime performance because fewer calls need 
to be made to the language model which resides on a server. In 
addition, we noticed that overall precision is increased by not 
considering the less likely suggestions by the classifier. 
3.2 Meta-Classifier 
For the meta-classifier we chose to use a decision 
tree, trained with the WinMine toolkit (Chickering 
2002). The motivation for this choice is that deci-
sion trees are well-suited for continuously valued 
features and for non-linear decision surfaces. An 
obvious alternative would be to use a support vec-
tor machine with non-linear kernels, a route that 
we have not explored yet. The feature set for the 
meta-classifier consists of the following scores 
from the primary models, including some arithmet-
ic combinations of scores: 
 Ratio and delta of Log LM score of the origi-
nal word choice and the suggested revision (2 
features) 
 Ratio and delta of the LM entropy for origi-
nal and suggested revision (2 features).  
 Products of the above ratios/deltas and clas-
sifier choice/presence probabilities 
 Type of operation: deletion, insertion, substi-
tution (3 features) 
 P(presence) (1 feature) 
 For each preposition/article choice: 
P(choice): 13 features for prepositions (12 
prepositions and other for a preposition not 
in that set), 2 for articles 
 Original token: none (for insertion) or the 
original preposition/article (13 features for 
prepositions, 2 for articles) 
 Suggested token: none (for deletion) or the 
suggested preposition/article (13 features for 
prepositions, 2 for articles) 
The total number of features is 63 for preposi-
tions and 36 for articles. 
The meta-classifier is trained by collecting sug-
gested corrections from the primary models on the 
error annotated data. The error-annotation provides 
the binary class label, i.e. whether the suggested 
revision is correct or incorrect. If the suggested 
revision matches an annotated correction, it counts 
as correct, if it does not match it counts as incor-
rect. To give an example, the top three preposition 
operations for the position before this test in the 
sentence I rely to this test are: 
Change_to_on 
Delete_to 
Change_to_of 
The class label in this example is "suggestion 
correct", assuming that the change of preposition is 
165
annotated in the data. The operation Change_to_on  
in this example has the following feature values for 
the basic classifier and LM scores: 
classifier P(choice): 0.755 
classifier P(presence): 0.826 
LM logP(original): -17.373 
LM logP(rewrite): -14.184 
An example of a path through the decision tree 
meta-classifier for prepositions is: 
LMLogDelta is Not < -8.59  and 
LMLogDelta is Not < -3.7  and 
ProductRewriteLogRatioConf is Not < -
0.00115  and 
LMLogDelta is Not < -1.58  and 
ProductOrigEntropyRatioChoiceConf is Not < -
0.00443  and 
choice_prob is Not < 0.206  and 
Original_of is 0  and 
choice_prob is Not < 0.329  and 
to_prob is < 0.108  and 
Suggested_on is 1  and 
Original_in is 0  and 
choice_prob is Not < 0.497  and 
choice_prob is Not < 0.647  and 
presence_prob is Not < 0.553 
The leaf node at the end of this path has a 0.21 
probability of changing ?to? to ?on? being a cor-
rect rewrite suggestion. 
The features selected by the decision trees range 
across all of the features discussed above. For both 
the article and preposition meta-classifiers, the 
ranking of features by importance (as measured by 
how close to the root the decision tree uses the fea-
ture) follows the order in which features are listed. 
3.3 Data 
In contrast to the training data for the primary 
models, the meta-classifier is trained on error-
annotated data from the Cambridge University 
Press Learners? Corpus (CLC). The version of 
CLC that we have licensed currently contains a 
total of 20 million words from learner English es-
says written as part of one of Cambridge?s English 
Language Proficiency Tests (ESOL) ? at all profi-
ciency levels. The essays are annotated for error 
type, erroneous span and suggested correction.  
We first perform a random split of the essays in-
to 70% training, 20% test and 10% for parameter 
tuning. Next, we create error-specific training, tun-
ing and test sets by performing a number of clean-
up steps on the data. First, we correct all errors that 
were flagged as being spelling errors, since we 
presume that the user will perform a spelling check 
on the data before proceeding to grammatical 
proofing. Spelling errors that were flagged as mor-
phology errors were left alone. By the same token, 
we corrected confused words that are covered by 
MS Word. We then revised British English spel-
ling to American English spelling conventions. In 
addition, we eliminated all annotations for non-
pertinent errors (i.e., non-preposition/article errors, 
or errors that do not involve any of the targeted 
prepositions), but we maintained the original (er-
roneous) text for these. This makes our task harder 
since we will have to learn how to make predic-
tions in text containing multiple errors, but it also 
is a more realistic scenario given real learner writ-
ing. Finally, we eliminated sentences containing 
nested errors and immediately adjacent errors 
when they involve pertinent (preposition/article) 
errors. For example, an annotated error "take a pic-
ture" with the correction "take pictures" is anno-
tated as two consecutive errors: "delete a" and 
"rewrite picture as pictures". Since the error in-
volves operations on both the article and the noun, 
which our article correction module is not designed 
to cover, we eliminated the sentence from the data. 
(This last step eliminated 31% of the sentences 
annotated with preposition errors and 29% or the 
sentences annotated with article errors.) Sentences 
that were flagged for a replacement error but con-
tained no replacement were also eliminated from 
the data.  
The final training, tuning and test set sizes are as 
follows (note that for prepositions we had to re-
duce the size of the training set by an additional 
20% in order to avoid memory limitations of our 
decision tree tools). 
Prepositions: 
      train: 584,485 sentences, 68,806 prep errors 
      tuning: 105,166 sentences, 9918 prep errors 
      test:  208,724 sentences, 19,706 prep errors 
Articles: 
      train: 737,091 sentences, 58,356 article errors 
      tuning: 106,052 sentences, 8341 article errors 
      test: 210,577 sentences, 16,742 article errors 
This mix is strongly biased towards ?correct? 
usage. After all, there are many more correct uses 
of articles and prepositions in the CLC data than 
incorrect ones. Again, this is likely to make our 
task harder, but more realistic, since both at train-
166
ing and test time we are working with the error 
distribution that is observed in learner data. 
3.4 Evaluation 
To evaluate, we run our meta-classifier system on 
the preposition and article test sets described in 
above and calculate precision and recall. Precision 
and recall for the overall system are controlled by 
thresholding the meta-classifier class probability. 
As a point of comparison, we also evaluate the per-
formance of the primary models (the error-specific 
classifier and the language model) in isolation. 
Precision and recall for the error-specific classifier 
is controlled by thresholding class probability. To 
control the precision-recall tradeoff for the lan-
guage model, we calculate the difference between 
the log probabilities of the original user input and 
the suggested correction. We then vary that differ-
ence across all observed values in small incre-
ments, which affects precision and recall: the 
higher the difference, the fewer instances we find, 
but the higher the reliability of these instances is.  
This evaluation differs from many of the evalua-
tions reported in the error detection/correction lite-
rature in several respects. First, the test set is a 
broad random sample across all proficiency levels 
in the CLC data. Second, it is far larger than any 
sets that have been so far to report results of prepo-
sition/article correction on learner data. Finally, we 
are only considering cases in which the system 
suggests a correction. In other words, we do not 
count as correct instances where the system's pre-
diction matches a correct preposition/article. 
This evaluation scheme, however, ignores one 
aspect of a real user scenario. Of all the suggested 
changes that are counted as wrong in our evalua-
tion because they do not match an annotated error, 
some may in fact be innocuous or even helpful for 
a real user. Such a situation can arise for a variety 
of reasons: In some cases, there are legitimate al-
ternative ways to correct an error. In other cases, 
the classifier has identified the location of an error 
although that error is of a different kind (which can 
be beneficial because it causes the user to make a 
correction - see Leacock et al, 2009). Gamon et al 
(2009), for example manually evaluate preposition 
suggestions as belonging to one of three catego-
ries: (a) properly correcting an existing error, (b) 
offering a suggestion that neither improves nor 
degrades the user sentence, (c) offering a sugges-
tion that would degrade the user input. Obviously, 
(c) is a more serious error than (b). Similarly, Te-
trault and Chodorow (2008) annotate their test set 
with preposition choices that are valid alternatives. 
We do not have similar information in the CLC 
data, but we can perform a manual analysis of a 
random subset of test data to estimate an "upper 
bound" for our precision/recall curve. Our annota-
tor manually categorized each suggested correction 
into one of seven categories. 
Details of the distribution of suggested correc-
tions into the seven categories are shown in Table 
1. 
Category preps. articles 
Corrects a CLC error 32.87% 33.34% 
Corrects an  error that 
was not annotated as be-
ing that error type in CLC 11.67% 12.16% 
Corrects a CLC error, but 
uses an alternative cor-
rection 3.62% 2.26% 
Original and suggested 
correction are equally 
good 9.60% 11.30% 
Error correctly detected, 
but the correction is 
wrong 8.73% 5.03% 
Identifies an error site, 
but the actual error is not 
a preposition error 19.17% 12.64% 
Introduces an error 14.65% 23.26% 
Table 1: Manual analysis of suggested corrections on 
CLC data. 
This analysis involves costly manual evaluation, 
so we only performed it at one point of the preci-
sion/recall curve (our current runtime system set-
ting). The sample size was 6,000 sentences for 
prepositions and 5981 sentences for articles (half 
of the sentences were flagged as containing at least 
one article/preposition error while the other half 
were not). On this manual evaluation, we achieve 
32.87% precision if we count all flags that do not 
perfectly match a CLC annotation as a false posi-
tive. Only counting the last category (introduction 
of an error) as a false positive, precision is at 
85.34%. Similarly, for articles, the manual estima-
tion arrives at 76.74% precision, where pure CLC 
annotation matching gives us 33.34%. 
167
4 Results 
Figure 1 and Figure 2 show the evaluation results 
of the meta-classifier for prepositions and articles, 
compared to the performance of the error-specific 
classifier and language model alone. For both pre-
positions and articles, the first notable observation 
is that the language model outperforms the clas-
sifier by a large margin. This came as a surprise to 
us, given the recent prevalence of classification 
approaches in this area of research and the fact that 
our classifiers produce state-of-the art performance 
when compared to other systems, on well-formed 
data. Second, the combination of scores from the 
classifier and language model through a meta-
classifier clearly outperforms either one of them in 
isolation. This result, again, is consistent across 
prepositions and articles.  
We had previously used a hand-tuned score 
combination instead of a meta-classifier. We also 
established that this heuristic performs worse than 
the language model for prepositions, and just about 
at the same level as the language model for ar-
ticles. Note, though, that the manual tuning was 
performed to optimize performance against a dif-
ferent data set (the Chinese Learners of English 
Corpus: CLEC), so the latter point is not really 
comparable and hence is not included in the charts. 
 
Figure 1: Precision and recall for prepositions. 
 
Figure 2: Precision and recall for articles. 
We now turn to the question of the required 
amount of annotated training data for the meta-
classifier. CLC is commercially available, but it is 
obvious that for many researchers such a corpus 
will be too expensive and they will have to create 
or license their own error-annotated corpus. Thus 
the question of whether one could use less anno-
tated data to train a meta-classifier and still achieve 
reasonable results becomes important. Figure 3 and 
Figure 4 show results obtained by using decreasing 
amounts of training data. The dotted line shows the 
language model baseline. Any result below the 
language model performance shows that the train-
ing data is insufficient to warrant the use of a meta-
classifier. In these experiments there is a clear dif-
ference between prepositions and articles. We can 
reduce the amount of training data for prepositions 
to 10% of the original data and still outperform the 
language model baseline. 10% of the data corres-
ponds to 6,800 annotated preposition errors and 
58,400 sentences. When we reduce the training 
data to 1% of the original amount (680 annotated 
errors, 5,800 sentences) we clearly see degraded 
results compared to the language model. With ar-
ticles, the system is much less data-hungry. Reduc-
ing the training data to 1% (580 annotated errors, 
7,400 sentences) still outperforms the language 
model alone. This result can most likely be ex-
plained by the different complexity of the preposi-
tion and article tasks. Article operations include 
only six distinct operations: deletion of the, dele-
tion of a/an, insertion of the, insertion of a/an, 
change of the to a/an, and change of a/an to the. 
For the twelve prepositions that we work with, the 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6
P
re
ci
si
on
Recall
Prepositions
LM only classifier only
learned thresholds
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
P
re
ci
si
on
Recall
Articles
Learned thresholds classifier only
LM only
168
total number of insertions, deletions and substitu-
tions that require sufficient training events and 
might need different score combinations is 168, 
making the problem much harder. 
 
Figure 3: Using different amounts of annotated training 
data for the preposition meta-classifier. 
 
Figure 4: Using different amounts of annotated training 
data for the article meta-classifier. 
To find out if it is possible to reduce the re-
quired amount of annotated preposition errors for a 
system that still covers more than one third of the 
preposition errors, we ran the same learning curve 
experiments but now only taking the four most 
frequent prepositions into account: to, of, in, for. In 
the CLC, these four prepositions account for 
39.8% of preposition error flags. As in the previous 
experiments, however, we found that we are not 
able to outperform the baseline by using just 1% of 
annotated data. 
5 Error Analysis 
We have conducted a failure analysis on examples 
where the system produces a blatantly bad sugges-
tion in order to see whether this decision could be 
attributed to the error-specific classifier or to the 
language model, or both, and what the underlying 
cause is. This preliminary analysis highlights two 
common causes for bad flags. One is that of fre-
quent lower order n-grams that dominate the lan-
guage model score. Consider the CLEC sentence I 
get to know the world outside the campus by news-
paper and television. The system suggests deleting 
by. The cause of this bad decision is that the bi-
gram campus newspaper is extremely likely, 
trumping all other n-grams, and  leading to a high 
probability for the suggested string compared to 
the original: Log (P(original)) = -26.2 and Log 
(P(suggestion)) = -22.4. This strong imbalance of 
the language model score causes the meta-
classifier to assign a relatively high probability to 
this being a correct revision, even though the error-
specific classifier is on the right track and gives a 
relatively high probability for the presence of a 
preposition and the choice of by. A similar exam-
ple, but for substitution, occurs in They give dis-
counts to their workers on books. Here the bigram 
in books has a very high probability and the system 
incorrectly suggests replacing on with in. An ex-
ample for insertion is seen in Please send me the 
letter back writing what happened. Here, the bi-
gram back to causes the bad suggestion of inserting 
to after back. Since the language model is general-
ly more accurate than the error-specific classifier, 
the meta-classifier tends to trust its score more than 
that of the classifier. As a result we see this kind of 
error quite frequently. 
Another common error class is the opposite situ-
ation: the language model is on the right track, but 
the classifier makes a wrong assessment. Consider 
Whatever direction my leg fought to stretch, with 
the suggested insertion of on before my leg. Here 
Log (P(original)) = -31.5 and Log (P(suggestion)) 
= -32.1, a slight preference for the original string. 
The error-specific classifier, however, assigns a 
probability of 0.65 for a preposition to be present, 
and 0.80 for that preposition to be on. The contex-
tual features that are important in that decision are: 
the insertion site is between a pronoun and a noun, 
it is relatively close to the beginning of the sen-
tence, and the head of the NP my leg has a possible 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6
P
re
ci
si
on
Recall
Prepositions
100% training data LM only
10% training data 1% training data
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
P
re
ci
si
on
Recall
Articles
100% training data 10% training data
Language model alone 1% training data
169
mass noun sense. An example involving deletion is 
in Someone came to sort of it. While the language 
model assigns a high probability for deleting of, 
the error-specific classifier does not. Similarly, for 
substitution, in Your experience is very interesting 
for our company, the language model suggests 
substituting for with to while the classifier gives 
the substitution a very low probability. 
As can be seen from the learner sentences cited 
above, often, even though the sentences are gram-
matical, they are not idiomatic, which can confuse 
all of the classifiers.  
6 Conclusion and Future Work 
We have addressed two questions in this paper: 
1. How effective is a meta-classification ap-
proach that combines language modeling and 
error-specific classification to the detection 
and correction of preposition and article errors 
by non-native speakers? 
2. How much error-annotated data is sufficient to 
produce positive results using that approach? 
We have shown that a meta-classifier approach 
outperforms using a language model or a classifier 
alone. An interesting side result is that the lan-
guage model solidly outperforms the contextual 
classifier for both article and preposition correc-
tion, contrary to current practice in the field. Train-
ing data requirements for the meta-classifier vary 
significantly between article and preposition error 
detection. The article meta-classifier can be trained 
with as few as 600 annotated errors, but the prepo-
sition meta-classifier requires more annotated data 
by an order of magnitude. Still, the overall amount 
of expensive error-annotated data is relatively 
small, and the meta-classification approach makes 
it possible to leverage large amounts of well-
formed text in the primary models, tuning to the 
non-native domain in the meta-classifier. 
We believe that the logical next step is to com-
bine more primary models in the meta-classifier. 
Candidates for additional primary models include 
(1) more classifiers trained either on different data 
sets or with a different classification algorithm, and 
(2) more language models, such as skip models or 
part-of-speech n-gram language models. 
Acknowledgments 
We thank Claudia Leacock from the Butler Hill 
Group for detailed error analysis and the anonym-
ous reviewers for helpful and constructive feed-
back. 
References  
Eric Steven Atwell. 1987. How to detect grammatical 
errors in a text without parsing it. In Proceedings of 
the 3rd EACL (pp 38 ? 45). Copenhagen. 
John Bitchener, Stuart Young, and Denise Cameron. 
2005. The effect of different types of corrective feed-
back on ESL student writing. Journal of Second Lan-
guage Writing, 14(3), 191-205. 
David Maxwell Chickering. 2002. The WinMine Tool-
kit. Microsoft Technical Report 2002-103. Redmond. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions (pp. 25-30). Pra-
gue. 
Gerard M. Dalgish. 1985. Computer-assisted ESL re-
search and courseware development. Computers and 
Composition, 2(4), 45-62. 
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions (pp. 45-50). Prague. 
Rachele De Felice and Stephen Pulman.  2008.  A clas-
sifier-based approach to preposition and determiner 
error correction in L2 English. In Proceedings of 
COLING. Manchester, UK. 
Thomas G. Dietterich. 1997. Machine learning research: 
Four current directions. AI Magazine, 18(4), 97-136. 
Ted Dunning. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational Linguis-
tics, 19, 61-74. 
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan, Jianfeng Gao, Dmitriy Belenko, and 
Alexandre Klementiev,. 2009. Using statistical tech-
niques and web search to correct ESL errors. 
CALICO Journal, 26(3). 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of IJCNLP, Hydera-
bad, India.  
Jianfeng Gao, Joshua Goodman, and Jiangbo Miao. 
2001. The use of clustering techniques for language 
modeling?Application to Asian languages. Compu-
170
tational Linguistics and Chinese Language 
Processing, 6(1), 27-60. 
Andrew Golding. 1995. A Bayesian Hybrid for Context 
Sensitive Spelling Correction. In Proceedings of the 
3rd Workshop on Very Large Corpora (pp. 39?53). 
Cambridge, USA. 
Andrew R. Golding and Dan Roth. 1996. Applying 
Winnow to context-sensitive spelling correction. In 
Proceedings of the Int. Conference on Machine 
Learning  (pp 182 ?190). 
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th International 
Conference on Language Resources and Evaluation. 
Lisbon. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Trude Heift and Mathias Schulze. 2007. Errors and 
Intelligence in Computer-Assisted Language Learn-
ing: Parsers and Pedagogues. New York & London: 
Routledge. 
Matthieu Hermet, Alain D?silets, and Stan Szpakowicz. 
2008. Using the web as a linguistic resource to auto-
matically correct lexico-yyntactic errors. In Proceed-
ings of the 6th Conference on Language Resources 
and Evaluation (LREC), (pp. 874 - 878). 
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi and Hitoshi Isahara. 2003. Automatic 
error detection in the Japanese learners' English spo-
ken data. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics  
(pp. 145-148). 
Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. SST speech corpus of Japanese learners' Eng-
lish and automatic detection of learners' errors. In 
Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC), (Vol 4, 
pp. 31-48). 
Kevin Knight and Ishwar Chander,. 1994. Automatic 
postediting of documents. In Proceedings of the  12th 
National Conference on Artificial Intelligence (pp. 
779-784). Seattle: Morgan Kaufmann. 
Claudia Leacock, Michael Gamon, and Chris Brockett. 
2009. User Input and Interactions on Microsoft ESL 
Assistant. In Proceedings of the Fourth Workshop on 
Innovative Use of NLP for Building Educational Ap-
plications (pp. 73-81). 
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the Human Language Technology Confe-
rence of the North American Chapter of the 
Association for Computational Linguistics,  (pp. 31-
36). Boston. 
Guido Minnen, Francis Bond, and Anne Copestake. 
2000. Memory-based learning for article generation. 
In Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning and of the 
Second Learning Language in Logic Workshop (pp. 
43-48). Lisbon. 
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 
2007. MSRLM: A scalable language modeling tool-
kit. Microsoft Technical Report 2007-144. Redmond. 
Adwait Ratnaparkhi. 1997. A simple introduction to 
maximum entropy models for natural language 
processing. Technical Report IRCS Report 97-98, In-
stitute for Research in Cognitive Science, University 
of Pennsylvania. 
Herman Stehouwer and Menno van Zaanen. 2009. Lan-
guage models for contextual error detection and cor-
rection. In Proceedings of the EACL 2009 Workshop 
on Computational Linguistic Aspects of Grammatical 
Inference ( pp. 41-48). Athens. 
Joel Tetreault and Martin Chodorow. 2008a. The ups 
and downs of preposition error detection in ESL. In 
Proceedings of COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2008b. Native 
judgments of non-native usage: Experiments in pre-
position error detection. In Proceedings of the Work-
shop on Human Judgments in Computational 
Linguistics, 22nd International Conference on Com-
putational Linguistics  (pp 43-48). Manchester, UK. 
Jenine Turner and Eugene Charniak. 2007. Language 
modeling for determiner selection. In Human Lan-
guage Technologies 2007: NAACL; Companion Vo-
lume, Short Papers (pp. 177-180). Rochester, NY. 
Wikipedia. English Language. 
http://en.wikipedia.org/wiki/English_language 
Xing Yi, Jianfeng Gao, and Bill Dolan.  2008.  A web-
based English proofing system for English as a 
second language users.  In Proceedings of the Third 
International Joint Conference on Natural Language 
Processing (IJCNLP). Hyderabad, India. 
171
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 602?606,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Responses to Microblog Posts
Yoav Artzi ?
Computer Science & Engineering
University of Washington
Seattle, WA, USA
yoav@cs.washington.edu
Patrick Pantel, Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA, USA
{ppantel,mgamon}@microsoft.com
Abstract
Microblogging networks serve as vehicles for
reaching and influencing users. Predicting
whether a message will elicit a user response
opens the possibility of maximizing the viral-
ity, reach and effectiveness of messages and
ad campaigns on these networks. We propose
a discriminative model for predicting the like-
lihood of a response or a retweet on the Twit-
ter network. The approach uses features de-
rived from various sources, such as the lan-
guage used in the tweet, the user?s social net-
work and history. The feature design process
leverages aggregate statistics over the entire
social network to balance sparsity and infor-
mativeness. We use real-world tweets to train
models and empirically show that they are ca-
pable of generating accurate predictions for a
large number of tweets.
1 Introduction
Microblogging networks are increasingly evolving
into broadcasting networks with strong social as-
pects. The most popular network today, Twitter, re-
ported routing 200 million tweets (status posts) per
day in mid-2011. As the network is increasingly
used as a channel for reaching out and marketing
to its users, content generators aim to maximize the
impact of their messages, an inherently challeng-
ing task. However, unlike for conventionally pro-
duced news, Twitter?s public network allows one to
observe how messages are reaching and influencing
users. One such direct measure of impact are mes-
sage responses.
? This work was conducted at Microsoft Research.
In this work, we describe methods to predict if a
given tweet will elicit a response. Twitter provides
two methods to respond to messages: replies and
retweets (re-posting of a message to one?s follow-
ers). Responses thus serve both as a measure of dis-
tribution and as a way to increase it. Being able to
predict responses is valuable for any content gener-
ator, including advertisers and celebrities, who use
Twitter to increase their exposure and maintain their
brand. Furthermore, this prediction ability can be
used for ranking, allowing the creation of better op-
timized news feeds.
To predict if a tweet will receive a response prior
to its posting we use features of the individual tweet
together with features aggregated over the entire so-
cial network. These features, in combination with
historical activity, are used to train a prediction
model.
2 Related Work
The public nature of Twitter and the unique char-
acteristics of its content have made it an attractive
research topic over recent years. Related work can
be divided into several types:
Twitter Demographics One of the most fertile av-
enues of research is modeling users and their inter-
actions on Twitter. An extensive line of work char-
acterizes users (Pear Analytics, 2009) and quantifies
user influence (Cha et al, 2010; Romero et al, 2011;
Wu et al, 2011; Bakshy et al, 2011). Popescu and
Jain (2011) explored how businesses use Twitter to
connect with their customer base. Popescu and Pen-
nacchiotti (2011) and Qu et al (2011) investigated
602
how users react to events on social media. There
also has been extensive work on modeling conver-
sational interactions on Twitter (Honeycutt and Her-
ring, 2009; Boyd et al, 2010; Ritter et al, 2010;
Danescu-Niculescu-Mizil et al, 2011). Our work
builds on these findings to predict response behavior
on a large scale.
Mining Twitter Social media has been used to de-
tect events (Sakaki et al, 2010; Popescu and Pennac-
chiotti, 2010; Popescu et al, 2011), and even predict
their outcomes (Asur and Huberman, 2010; Culotta,
2010). Similarly to this line of work, we mine the
social network for event prediction. In contrast, our
focus is on predicting events within the network.
Response Prediction There has been significant
work addressing the task of response prediction in
news articles (Tsagkias et al, 2009; Tsagkias et al,
2010) and blogs (Yano et al, 2009; Yano and Smith,
2010; Balasubramanyan et al, 2011). The task of
predicting responses in social networks has been in-
vestigated previously: Hong et al (2011) focused
on predicting responses for highly popular items,
Rowe et al (2011) targeted the prediction of con-
versations and their length and Suh et al (2010) pre-
dicted retweets. In contrast, our work targets tweets
regardless of their popularity and attempts to predict
both replies and retweets. Furthermore, we present
a scalable method to use linguistic lexical features in
discriminative models by leveraging global network
statistics. A related task to ours is that of response
generation, as explored by Ritter et al (2011). Our
work complements their approach by allowing to
detect when the generation of a response is appro-
priate. Lastly, the task of predicting the spread of
hashtags in microblogging networks (Tsur and Rap-
poport, 2012) is also closely related to our work and
both approaches supplement each other as measures
of impact.
Ranking in News Feeds Different approaches
were suggested for ranking items in social media
(Das Sarma et al, 2010; Lakkaraju et al, 2011). Our
work provides an important signal, which can be in-
corporated into any ranking approach.
3 Response Prediction on Twitter
Our goal is to learn a function f that maps a tweet
x to a binary value y ? {0, 1}, where y indicates if
x will receive a response. In this work we make no
distinction between different kinds of responses.
In addition to x, we assume access to a social net-
work S, which we view as a directed graph ?U,E?.
The set of vertices U represents the set of users. For
each u?, u?? ? U , ?u?, u??? ? E if and only if there
exists a following relationship from u? to u??.
For the purpose of defining features we denote xt
as the text of the tweet x and xu ? U the user who
posted x. For training we assume access to a set of
n labeled examples {?xi, yi? : i = 1 . . . n}, where
the label indicates whether the tweet has received a
response or not.
3.1 Features
For prediction we represent a given tweet x using six
feature families:
Historical Features Historical behavior is often
strong evidence of future trends. To account for this
information, we compute the following features: ra-
tio of tweets by xu that received a reply, ratio of
tweets by xu that were retweeted and ratio of tweets
by xu that received both a reply and retweet.
Social Features The immediate audience of a user
xu is his followers. Therefore, incorporating social
features into our model is likely to contribute to its
prediction ability. For a user xu ? U we include
features for the number of followers (indegree in S),
the number of users xu follows (outdegree in S) and
the ratio between the two.
Aggregate Lexical Features To detect lexical
items that trigger certain response behavior we de-
fine features for all bigrams and hashtags in our set
of tweets. To avoid sparsity and maintain a manage-
able feature space we compress the features using
the labels: for each lexical item l we define Rl to
be the set of tweets that include l and received a re-
sponse, and Nl to be the set of tweets that contain l
and received no response. We then define the inte-
ger n to be the rounding of |Rl||Nl| to the nearest integer.
For each such integer we define a feature, which we
increase by 1 when the lexical item l is present in xt.
603
We use this process separately for bigrams and hash-
tags, creating separate sets of aggregate features.
Local Content Features We introduce 45 features
to capture how the content of xt influences response
behavior, including features such as the number of
stop words and the percentage of English words. In
addition we include features specific to Twitter, such
as the number of hash tags and user references.
Posting Features Past analysis of Twitter showed
that posting time influences response potential (Pear
Analytics, 2009). To examine temporal influences,
we include features to account for the user?s local
time and day of the week when x was created.
Sentiment Features To measure how sentiment
influences response behavior we define features that
count the number of positive and negative sentiment
words in xt. To detect sentiment words we use a pro-
prietary Microsoft lexicon of 7K positive and nega-
tive terms.
4 Evaluation
4.1 Learning Algorithm
We experimented with two different learning al-
gorithms: Multiple Additive Regression-Trees
(MART) (Wu et al, 2008) and a maximum entropy
classifier (Berger et al, 1996). Both provide fast
classification, a natural requirement for large-scale
real-time tasks.
4.2 Dataset
In our evaluation we focus on English tweets only.
Since we use local posting time in our features, we
filtered users whose profile did not contain location
information. To collect Tweeter messages we used
the entire public feed of Twitter (often referred to as
the Twitter Firehose). We randomly sampled 943K
tweets from one week of data. We allowed an ex-
tra week for responses, giving a response window
of two weeks. The majority of tweets in our set
(90%) received no response. We used 750K tweets
for training and 188K for evaluation. A separate data
set served as a development set. For the computation
of aggregate lexical features we used 186M tweets
from the same week, resulting in 14M bigrams and
400K hash tags. To compute historical features, we
sampled 2B tweets from the previous three months.
Figure 1: Precision-recall curves for predicting that a
tweet will get a response. The marked area highlights
the area of the curve we focus on in our evaluation.
Figure 2: Precision-recall curves with increasing number
of features removed for the marked area in Figure 1. For
each curve we removed one additional feature set from
the one above it.
4.3 Results
Our evaluation focuses on precision-recall curves
for predicting that a given tweet will get a response.
The curves were generated by varying the confi-
dence measure threshold, which both classifiers pro-
vided. As can be seen in Figure 1, MART outper-
forms the maximum entropy model. We can also see
that it is hard to predict response behavior for most
tweets, but for a large subset we can provide a rela-
tively accurate prediction (highlighted in Figure 1).
The rest of our analysis focuses on this subset and
on results based on MART.
To better understand the contribution of each fea-
ture set, we removed features in a greedy manner.
After learning a model and testing it, we removed
the feature family that was overall most highly
ranked by MART (i.e., was used in high-level splits
in the decision trees) and learned a new model. Fig-
ure 2 shows how removing feature sets degrades pre-
diction performance. Removing historical features
lowers the model?s prediction abilities, although pre-
diction quality remains relatively high. Removing
social features creates a bigger drop in performance.
Lastly, removing aggregate lexical features and lo-
604
cal content features further decreases performance.
At this point, removing posting time features is not
influential. Following the removal of posting time
features, the model includes only sentiment features.
5 Discussion and Conclusion
The first trend seen by removing features is that local
content matters less, or at least is more complex to
capture and use for response prediction. Despite the
influence of chronological trends on posting behav-
ior on Twitter (Pear Analytics, 2009), we were un-
able to show influence of posting time on response
prediction. Historical features were the most promi-
nent in our experiments. Second were social fea-
tures, showing that developing one?s network is crit-
ical for impact. The third most prominent set of fea-
tures, aggregate lexical features, shows that users are
sensitive to certain expressions and terms that tend
to trigger responses.
The natural path for future work is to improve per-
formance using new features. These may include
clique-specific language features, more properties of
the user?s social network, mentions of named enti-
ties and topics of tweets. Another direction is to dis-
tinguish between replies and retweets and to predict
the number of responses and the length of conversa-
tions that a tweet may generate. There is also po-
tential in learning models for the prediction of other
measures of impact, such as hashtag adoption and
inclusion in ?favorites? lists.
Acknowledgments
We would like to thank Alan Ritter, Bill Dolan,
Chris Brocket and Luke Zettlemoyer for their sug-
gestions and comments. We wish to thank Chris
Quirk and Qiang Wu for providing us with access
to their learning software. Thanks to the reviewers
for the helpful comments.
References
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. In Proceedings of the International
Conference on Web Intelligence and Intelligent Agent
Technology.
E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.
2011. Everyone?s an influencer: quantifying influence
on twitter. In Peoceedings of the ACM International
Conference on Web Search and Data Mining.
R. Balasubramanyan, W.W. Cohen, D. Pierce, and D.P.
Redlawsk. 2011. What pushes their buttons? predict-
ing comment polarity from the content of political blog
posts. In Proceedings of the Workshop on Language in
Social Media.
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics.
D. Boyd, S. Golder, and G. Lotan. 2010. Tweet, tweet,
retweet: Conversational aspects of retweeting on twit-
ter. In Proceedings of the International Conference on
System Sciences.
M. Cha, H. Haddadi, F. Benevenuto, and K.P. Gummadi.
2010. Measuring user influence in twitter: The million
follower fallacy. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
A. Culotta. 2010. Towards detecting influenza epidemics
by analyzing twitter messages. In Proceedings of the
Workshop on Social Media Analytics.
C. Danescu-Niculescu-Mizil, M. Gamon, and S. Dumais.
2011. Mark my words!: linguistic style accommoda-
tion in social media. In Proceedings of the Interna-
tional Conference on World Wide Web.
A. Das Sarma, A. Das Sarma, S. Gollapudi, and R. Pan-
igrahy. 2010. Ranking mechanisms in twitter-like fo-
rums. In Proceedings of the ACM International Con-
ference on Web Search and Data Mining.
C. Honeycutt and S.C. Herring. 2009. Beyond mi-
croblogging: Conversation and collaboration via twit-
ter. In Proceedings of the International Conference on
System Sciences.
L. Hong, O. Dan, and B. D. Davison. 2011. Predict-
ing popular messages in twitter. In Proceedings of the
International Conference on World Wide Web.
H. Lakkaraju, A. Rai, and S. Merugu. 2011. Smart
news feeds for social networks using scalable joint la-
tent factor models. In Proceedings of the International
Conference on World Wide Web.
Pear Analytics. 2009. Twitter study.
A.M. Popescu and A. Jain. 2011. Understanding the
functions of business accounts on twitter. In Proceed-
ings of the International Conference on World Wide
Web.
A.M. Popescu and M. Pennacchiotti. 2010. Detect-
ing controversial events from twitter. In Proceedings
of the International Conference on Information and
Knowledge Management.
A.M. Popescu and M. Pennacchiotti. 2011. Dancing
with the stars, nba games, politics: An exploration of
twitter users response to events. In Proceedings of the
605
International AAAI Conference on Weblogs and Social
Media.
A.M. Popescu, M. Pennacchiotti, and D. Paranjpe. 2011.
Extracting events and event descriptions from twit-
ter. In Proceedings of the International Conference
on World Wide Web.
Y. Qu, C. Huang, P. Zhang, and J. Zhang. 2011. Mi-
croblogging after a major disaster in china: a case
study of the 2010 yushu earthquake. In Proceedings
of the ACM Conference on Computer Supported Co-
operative Work.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised
modeling of twitter conversations. In Proceedings of
the Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Ritter, C. Cherry, and B. Dolan. 2011. Data-driven
response generation in social media. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
D. Romero, W. Galuba, S. Asur, and B. Huberman. 2011.
Influence and passivity in social media. Machine
Learning and Knowledge Discovery in Databases,
pages 18?33.
M. Rowe, S. Angeletou, and H. Alani. 2011. Predicting
discussions on the social semantic web. In Proceed-
ings of the Extended Semantic Web Conference.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors. In Proceedings of the International
Conference on World Wide Web.
B. Suh, L. Hong, P. Pirolli, and E. H. Chi. 2010. Want to
be retweeted? large scale analytics on factors impact-
ing retweet in twitter network. In Proceedings of the
IEEE International Conference on Social Computing.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2009.
Predicting the volume of comments on online news
stories. In Proceedings of the ACM Conference on In-
formation and Knowledge Management.
M. Tsagkias, W. Weerkamp, and M. De Rijke. 2010.
News comments: Exploring, modeling, and online
prediction. Advances in Information Retrieval, pages
191?203.
O. Tsur and A. Rappoport. 2012. What?s in a hash-
tag?: content based prediction of the spread of ideas
in microblogging communities. In Proceedings of the
ACM International Conference on Web Search and
Data Mining.
Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2008.
Ranking, boosting, and model adaptation. Tecnical
Report, MSR-TR-2008-109.
S. Wu, J.M. Hofman, W.A. Mason, and D.J. Watts. 2011.
Who says what to whom on twitter. In Proceedings of
the International Conference on World Wide Web.
T. Yano and N.A. Smith. 2010. Whats worthy of com-
ment? content and comment volume in political blogs.
Proceedings of the International AAAI Conference on
Weblogs and Social Media.
T. Yano, W.W. Cohen, and N.A. Smith. 2009. Predict-
ing response to political blog posts with topic mod-
els. In Proceedings of the Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
606
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 563?571,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mining Entity Types from Query Logs via User Intent Modeling
Patrick Pantel
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
ppantel@microsoft.com
Thomas Lin
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
tlin@cs.washington.edu
Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
mgamon@microsoft.com
Abstract
We predict entity type distributions in Web
search queries via probabilistic inference in
graphical models that capture how entity-
bearing queries are generated. We jointly
model the interplay between latent user in-
tents that govern queries and unobserved en-
tity types, leveraging observed signals from
query formulations and document clicks. We
apply the models to resolve entity types in new
queries and to assign prior type distributions
over an existing knowledge base. Our mod-
els are efficiently trained using maximum like-
lihood estimation over millions of real-world
Web search queries. We show that modeling
user intent significantly improves entity type
resolution for head queries over the state of the
art, on several metrics, without degradation in
tail query performance.
1 Introduction
Commercial search engines are providing ever-
richer experiences around entities. Querying for a
dish on Google yields recipe filters such as cook
time, calories, and ingredients. Querying for a
movie on Yahoo triggers user ratings, cast, tweets
and showtimes. Bing further allows the movie to
be directly added to the user?s Netflix queue. En-
tity repositories such as Freebase, IMDB, Facebook
Pages, Factual, Pricegrabber, and Wikipedia are in-
creasingly leveraged to enable such experiences.
There are, however, inherent problems in the en-
tity repositories: (a) coverage: although coverage of
head entity types is often reliable, the tail can be
sparse; (b) noise: created by spammers, extraction
errors or errors in crowdsourced content; (c) am-
biguity: multiple types or entity identifiers are of-
ten associated with the same surface string; and (d)
over-expression: many entities have types that are
never used in the context of Web search.
There is an opportunity to automatically tailor
knowledge repositories to the Web search scenario.
Desirable capabilities of such a system include: (a)
determining the prior type distribution in Web search
for each entity in the repository; (b) assigning a type
distribution to new entities; (c) inferring the correct
sense of an entity in a particular query context; and
(d) adapting to a search engine and time period.
In this paper, we build such a system by lever-
aging Web search usage logs with large numbers of
user sessions seeking or transacting on entities. We
cast the task as performing probabilistic inference
in a graphical model that captures how queries are
generated, and then apply the model to contextually
recognize entity types in new queries. We motivate
and design several generative models based on the
theory that search users? (unobserved) intents gov-
ern the types of entities, the query formulations, and
the ultimate clicks on Web documents. We show that
jointly modeling user intent and entity type signifi-
cantly outperforms the current state of the art on the
task of entity type resolution in queries. The major
contributions of our research are:
? We introduce the idea that latent user intents
can be an important factor in modeling type dis-
tributions over entities in Web search.
? We propose generative models and inference
procedures using signals from query context,
click, entity, entity type, and user intent.
563
? We propose an efficient learning technique and
a robust implementation of our models, using
real-world query data, and a realistic large set
of entity types.
? We empirically show that our models outper-
form the state of the art and that modeling latent
intent contributes significantly to these results.
2 Related Work
2.1 Finding Semantic Classes
A closely related problem is that of finding the se-
mantic classes of entities. Automatic techniques for
finding semantic classes include unsupervised clus-
tering (Schu?tze, 1998; Pantel and Lin, 2002), hy-
ponym patterns (Hearst, 1992; Pantel et al, 2004;
Kozareva et al, 2008), extraction patterns (Etzioni
et al, 2005), hidden Markov models (Ritter et al,
2009), classification (Rahman and Ng, 2010) and
many others. These techniques typically lever-
age large corpora, while projects such as WordNet
(Miller et al, 1990) and Freebase (Bollacker et al,
2008) have employed editors to manually enumerate
words and entities with their semantic classes.
The aforementioned methods do not use query
logs or explicitly determine the relative probabilities
of different entity senses. A method might learn that
there is independently a high chance of eBay being a
website and an employer, but does not specify which
usage is more common. This is especially problem-
atic, for example, if one wishes to leverage Freebase
but only needs the most commonly used senses (e.g.,
Al Gore is a US Vice President), rather than
all possible obscure senses (Freebase contains 30+
senses, including ones such as Impersonated
Celebrity and Quotation Subject). In
scenarios such as this, our proposed method can in-
crease the usability of systems that find semantic
classes. We also expand upon text corpora meth-
ods in that the type priors can adapt to Web search
signals.
2.2 Query Log Mining
Query logs have traditionally been mined to improve
search (Baeza-Yates et al, 2004; Zhang and Nas-
raoui, 2006), but they can also be used in place of
(or in addition to) text corpora for learning seman-
tic classes. Query logs can contain billions of en-
tries, they provide an independent signal from text
corpora, their timestamps allow the learning of type
priors at specific points in time, and they can contain
information such as clickthroughs that are not found
in text corpora. Sekine and Suzuki (2007) used fre-
quency features on context words in query logs to
learn semantic classes of entities. Pas?ca (2007) used
extraction techniques to mine instances of semantic
classes from query logs. Ru?d et al (2011) found
that cross-domain generalizations learned from Web
search results are applicable to NLP tasks such as
NER. Alfonseca et al (2010) mined query logs to
find attributes of entity instances. However, these
projects did not learn relative probabilities of differ-
ent senses.
2.3 User Intents in Search
Learning from query logs also allows us to lever-
age the concept of user intents. When users sub-
mit search queries, they often have specific intents in
mind. Broder (2002) introduced 3 top level intents:
Informational (e.g., wanting to learn), Navigational
(wanting to visit a site), and Transactional (e.g.,
wanting to buy/sell). Rose and Levinson (2004) fur-
ther divided these into finer-grained subcategories,
and Yin and Shah (2010) built hierarchical tax-
onomies of search intents. Jansen et al (2007), Hu
et al (2009), and Radlinski et al (2010) examined
how to infer the intent of queries. We are not aware
of any other work that has leveraged user intents to
learn type distributions.
2.4 Topic Modeling on Query Logs
The closest work to ours is Guo et al?s (2009) re-
search on Named Entity Recognition in Queries.
Given an entity-bearing query, they attempt to iden-
tify the entity and determine the type posteriors. Our
work significantly scales up the type posteriors com-
ponent of their work. While they only have four
potential types (Movie, Game, Book, Music) for
each entity, we employ over 70 popular types, allow-
ing much greater coverage of real entities and their
types. Because they only had four types, they were
able to hand label their training data. In contrast,
our system self-labels training examples by search-
ing query logs for high-likelihood entities, and must
handle any errors introduced by this process. Our
models also expand upon theirs by jointly modeling
564
entity type with latent user intents, and by incorpo-
rating click signals.
Other projects have also demonstrated the util-
ity of topic modeling on query logs. Carman et
al. (2010) modeled users and clicked documents to
personalize search results and Gao et al (2011) ap-
plied topic models to query logs in order to improve
document ranking for search.
3 Joint Model of Types and User Intents
We turn our attention now to the task of mining the
type distributions of entities and of resolving the
type of an entity in a particular query context. Our
approach is to probabilistically describe how entity-
bearing queries are generated in Web search. We
theorize that search queries are governed by a latent
user intent, which in turn influences the entity types,
the choice of query words, and the clicked hosts. We
develop inference procedures to infer the prior type
distributions of entities in Web search as well as to
resolve the type of an entity in a query, by maximiz-
ing the probability of observing a large collection of
real-world queries and their clicked hosts.
We represent a query q by a triple {n1, e, n2},
where e represents the entity mentioned in the query,
n1 and n2 are respectively the pre- and post-entity
contexts (possibly empty), referred to as refiners.
Details on how we obtain our corpus are presented
in Section 4.2.
3.1 Intent-based Model (IM)
In this section we describe our main model, IM, il-
lustrated in Figure 1. We derive a learning algorithm
for the model in Section 3.2 and an inference proce-
dure in Section 3.3.
Recall our discussion of intents from Section 2.3.
The unobserved semantic type of an entity e in a
query is strongly correlated with the unobserved
user intent. For example, if a user queries for
?song?, then she is likely looking to ?listen to it?,
?download it?, ?buy it?, or ?find lyrics? for it. Our
model incorporates this user intent as a latent vari-
able.
The choice of the query refiner words, n1 and n2,
is also clearly influenced by the user intent. For
example, refiners such as ?lyrics? and ?words? are
more likely to be used in queries where the intent is
For each query/click pair {q, c}
type t ?Multinomial(?)
intent i ?Multinomial(?t)
entity e ?Multinomial(?t)
switch s1 ? Bernoulli(?i)
switch s2 ? Bernoulli(?i)
if (s1) l-context n1 ?Multinomial(?i)
if (s2) r-context n2 ?Multinomial(?i)
click c ?Multinomial(?i)
Table 1: Model IM: Generative process for entity-
bearing queries.
to ?find lyrics? than in queries where the intent is to
?listen?. The same is true for clicked hosts: clicks on
?lyrics.com? and ?songlyrics.com? are more likely
to occur when the intent is to ?find lyrics?, whereas
clicks on ?pandora.com? and ?last.fm? are more
likely for a ?listen? intent.
Model IM leverages each of these signals: latent
intent, query refiners, and clicked hosts. It generates
entity-bearing queries by first generating an entity
type, from which the user intent and entity is gen-
erated. In turn, the user intent is then used to gen-
erate the query refiners and the clicked host. In our
data analysis, we observed that over 90% of entity-
bearing queries did not contain any refiner words n1
and n2. In order to distribute more probability mass
to non-empty context words, we explicitly represent
the empty context using a switch variable that deter-
mines whether a context will be empty.
The generative process for IM is described in Ta-
ble 1. Consider the query ?ymca lyrics?. Our model
first generates the type song, then given the type
it generates the entity ?ymca? and the intent ?find
lyrics?. The intent is then used to generate the pre-
and post-context words ? and ?lyrics?, respectively,
and a click on a host such as ?lyrics.com?.
For mathematical convenience, we assume that
the user intent is generated independently of the
entity itself. Without this assumption, we would
require learning a parameter for each intent-type-
entity configuration, exploding the number of pa-
rameters. Instead, we choose to include these depen-
dencies at the time of inference, as described later.
Recall that q = {n1, e, n2} and let s = {s1, s2},
where s1 = 1 if n1 is not empty and s2 = 1 if n2 is
not empty, 0 otherwise. The joint probability of the
model is the product of the conditional distributions,
as given by:
565
y  
Q
tt
n
2
e
f f 
T
tt
E
Guo?09
y  
Q
tt
n
2
e
ss  
f f 
Ttt E
T
Model M0
y  
Q
tt
n
2
e
w Tcss  
f f 
Ttt E
T
Model M1 Model IM
tt
Q
tt
n
2
qq
T
iie
w Kcss  
f f 
K
y  
T
K
Figure 1: Graphical models for generating entity-bearing queries. Guo?09 represents the current state of the art (Guo
et al, 2009). Models M0 and M1 add an empty context switch and click information, respectively. Model IM further
constrains the query by the latent user intent.
P (t, i, q, c | ?,?,?, ?,?,?) =
P (t | ?)P (i | t,?)P (e | t,?)P (c | i,?)
2?
j=1
P (nj | i,?)
I[sj=1]P (sj |i, ?)
We now define each of the terms in the joint dis-
tribution. Let T be the number of entity types. The
probability of generating a type t is governed by a
multinomial with probability vector ? :
P (t=t?) =
T?
j=1
? I[j=t?]j , s.t.
T?
j=1
?j = 1
where I is an indicator function set to 1 if its condi-
tion holds, and 0 otherwise.
Let K be the number of latent user intents that
govern our query log, where K is fixed in advance.
Then, the probability of intents i is defined as a
multinomial distribution with probability vector ?t
such that ? = [?1, ?2, ..., ?T ] captures the matrix of
parameters across all T types:
P (i=i? | t=t?) =
K?
j=1
?I[j=i?]
t?,j
, s.t. ?t
K?
j=1
?t,j = 1
LetE be the number of known entities. The prob-
ability of generating an entity e is similarly governed
by a parameter ? across all T types:
P (e=e? | t=t?) =
E?
j=1
?I[j=e?]
t?,j
, s.t. ?t
E?
j=1
?t,j = 1
The probability of generating an empty or non-
empty context s given intent i is given by a Bernoulli
with parameter ?i:
P (s | i=i?) = ?I[s=1]
i?
(1? ?i?)
I[s=0]
Let V be the shared vocabulary size of all query
refiner words n1 and n2. Given an intent, i, the
probability of generating a refiner n is given by a
multinomial distribution with probability vector ?i
such that ? = [?1, ?2, ..., ?K ] represents parame-
ters across intents:
P (n=n? | i=i?) =
V?
v=1
?I[v=n?]
i?,v
, s.t. ?i
V?
v=1
?i,v = 1
Finally, we assume there areH possible click val-
ues, corresponding to H Web hosts. A click on a
host is similarly determined by an intent i and is gov-
erned by parameter ? across all K intents:
P (c=c? | i=i?) =
H?
h=1
?I[h=c?]
i?,h
, s.t. ?i
H?
h=1
?i,h = 1
3.2 Learning
Given a query corpus Q consisting of N inde-
pendently and identically distributed queries qj =
{nj1, e
j , nj2} and their corresponding clicked hosts
cj , we estimate the parameters ? , ?, ?, ?, ?, and
? by maximizing the (log) probability of observing
Q. The logP (Q) can be written as:
logP (Q) =
N?
j=1
?
t,i
P j(t, i | q, c) logP j(q, c, t, i)
In the above equation, P j(t, i | q, c) is the poste-
rior distribution over types and user intents for the
jth query. We use the Expectation-Maximization
(EM) algorithm to estimate the parameters. The
parameter updates are obtained by computing the
derivative of logP (Q) with respect to each parame-
ter, and setting the resultant to 0.
The update for ? is given by the average of the
posterior distributions over the types:
566
?t? =
?N
j=1
?
i P
j(t=t?, i | q, c)
?N
j=1
?
t,i P
j(t, i | q, c)
For a fixed type t, the update for ?t is given by
the weighted average of the latent intents, where the
weights are the posterior distributions over the types,
for each query:
?t?,?i =
?N
j=1 P
j(t=t?, i=i? | q, c)
?N
j=1
?
i P
j(t=t?, i | q, c)
Similarly, we can update ?, the parameters that
govern the distribution over entities for each type:
?t?,e? =
?N
j=1
?
i P
j(t=t?, i | q, c)I[ej=e?]
?N
j=1
?
i P
j(t=t?, i | q, c)
Now, for a fixed user intent i, the update for
?i is given by the weighted average of the clicked
hosts, where the weights are the posterior distribu-
tions over the intents, for each query:
?i?,c? =
?N
j=1
?
t P
j(t, i=i? | q, c)I[cj=c?]
?N
j=1
?
t P
j(t, i=i? | q, c)
Similarly, we can update ? and ?, the parameters
that govern the distribution over query refiners and
empty contexts for each intent, as:
?i?,n?=
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[nj1=n?]I[s
j
1=1]+I[n
j
2=n?]I[s
j
2=1]
]
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[sj1=1]+I[s
j
2=1]
]
and
?i? =
?N
j=1
?
t P
j(t, i=i? | q, c)
[
I[s1=1] + I[s2=1]
]
2
?N
j=1
?
t P
j(t, i=i? | q, c)
3.3 Decoding
Given a query/click pair {q, c}, and the learned IM
model, we can apply Bayes? rule to find the poste-
rior distribution, P (t, i | q, c), over the types and
intents, as it is proportional to P (t, i, q, c). We com-
pute this quantity exactly by evaluating the joint for
each combination of t and i, and the observed values
of q and c.
It is important to note that at runtime when a new
query is issued, we have to resolve the entity in the
absence of any observed click. However, we do have
access to historical click probabilities, P (c | q).
We use this information to compute P (t | q) by
marginalizing over i as follows:
P (t | q) =
?
i
H?
j=1
P (t, i | q, cj)P (cj | q) (1)
3.4 Comparative Models
Figure 1 also illustrates the current state-of-the-art
model Guo?09 (Guo et al, 2009), described in Sec-
tion 2.4, which utilizes only query refinement words
to infer entity type distributions. Two extensions to
this model that we further study in this paper are also
shown: Model M0 adds the empty context switch
parameter and Model M1 further adds click infor-
mation. In the interest of space, we omit the update
equations for these models, however they are triv-
ial to adapt from the derivations of Model IM pre-
sented in Sections 3.1 and 3.2.
3.5 Discussion
Full Bayesian Treatment: In the above mod-
els, we learn point estimates for the parameters
(?,?,?, ?,?,?). One can take a Bayesian ap-
proach and treat these parameters as variables (for
instance, with Dirichlet and Beta prior distribu-
tions), and perform Bayesian inference. However,
exact inference will become intractable and we
would need to resort to methods such as variational
inference or sampling. We found this extension un-
necessary, as we had a sufficient amount of training
data to estimate all parameters reliably. In addition,
our approach enabled us to learn (and perform infer-
ence in) the model with large amounts of data with
reasonable computing time.
Fitting to an existing Knowledge Base: Al-
though in general our model decodes type distribu-
tions for arbitrary entities, in many practical cases
it is beneficial to constrain the types to those ad-
missible in a fixed knowledge base (such as Free-
base). As an example, if the entity is ?ymca?,
admissible types may include song, place, and
educational institution. When resolving
types, during inference, one can restrict the search
space to only these admissible types. A desirable
side effect of this strategy is that only valid ambigu-
ities are captured in the posterior distribution.
567
4 Evaluation Methodology
We refer to QL as a set of English Web search
queries issued to a commercial search engine over
a period of several months.
4.1 Entity Inventory
Although our models generalize to any entity reposi-
tory, we experiment in this paper with entities cover-
ing a wide range of web search queries, coming from
73 types in Freebase. We arrived at these types by
grepping for all entities in Freebase within QL, fol-
lowing the procedure described in Section 4.2, and
then choosing the top most frequent types such that
50% of the queries are covered by an entity of one
of these types1.
4.2 Training Data Construction
In order to learn type distributions by jointly mod-
eling user intents and a large number of types, we
require a large set of training examples containing
tagged entities and their potential types. Unlike in
Guo et al (2009), we need a method to automatically
label QL to produce these training cases since man-
ual annotation is impossible for the range of entities
and types that we consider. Reliably recognizing en-
tities in queries is not a solved problem. However,
for training we do not require high coverage of en-
tities in QL, so high precision on a sizeable set of
query instances can be a proper proxy.
To this end, we collect candidate entities in
QL via simple string matching on Freebase entity
strings within our preselected 73 types. To achieve
high precision from this initial (high-recall, low-
precision) candidate set we use a number of heuris-
tics to only retain highly likely entities. The heuris-
tics include retaining only matches on entities that
appear capitalized more than 50% in their occur-
rences in Wikipedia. Also, a standalone score fil-
ter (Jain and Pennacchiotti, 2011) of 0.9 is used,
which is based on the ratio of string occurrence as
1In this process, we omitted any non-core Freebase type
(e.g., /user/* and /base/*), types used for representation
(e.g., /common/* and /type/*), and too general types (e.g.,
/people/person and /location/location) identi-
fied by if a type contains multiple other prominent subtypes.
Finally, we conflated seven of the types that overlapped with
each other into four types (such as /book/written work
and /book/book).
an exact match in queries to how often it occurs as a
partial match.
The resulting queries are further filtered by keep-
ing only those where the pre- and post-entity con-
texts (n1 and n2) were empty or a single word (ac-
counting for a very large fraction of the queries). We
also eliminate entries with clicked hosts that have
been clicked fewer than 100 times over the entire
QL. Finally, for training we filter out any query with
an entity that has more than two potential types2.
This step is performed to reduce recognition er-
rors by limiting the number of potential ambiguous
matches. We experimented with various thresholds
on allowable types and settled on the value two.
The resulting training data consists of several mil-
lion queries, 73 different entity types, and approx-
imately 135K different entities, 100K different re-
finer words, and 40K clicked hosts.
4.3 Test Set Annotation
We sampled two datasets, HEAD and TAIL, each
consisting of 500 queries containing an entity be-
longing to one of the 73 types in our inventory, from
a frequency-weighted random sample and a uniform
random sample of QL, respectively.
We conducted a user study to establish a gold
standard of the correct entity types in each query.
A total of seven different independent and paid pro-
fessional annotators participated in the study. For
each query in our test sets, we displayed the query,
associated clicked host, and entity to the annotator,
along with a list of permissible types from our type
inventory. The annotator is tasked with identifying
all applicable types from that list, or marking the test
case as faulty because of an error in entity identifi-
cation, bad click host (e.g. dead link) or bad query
(e.g. non-English). This resulted in 2,092 test cases
({query, entity, type}-tuples). Each test case was
annotated by two annotators. Inter-annotator agree-
ment as measured by Fleiss? ? was 0.445 (0.498
on HEAD and 0.386 on TAIL), considered moderate
agreement.
From HEAD and TAIL, we eliminated three cat-
egories of queries that did not offer any interesting
type disambiguation opportunities:
? queries that contained entities with only one
2For testing we did not omit any entity or type.
568
HEAD TAIL
nDCG MAP MAPW Prec@1 nDCG MAP MAPW Prec@1
BFB 0.71 0.60 0.45 0.30 0.73 0.64 0.49 0.35
Guo?09 0.79? 0.71? 0.62? 0.51? 0.80? 0.73? 0.66? 0.52?
M0 0.79? 0.72? 0.65? 0.52? 0.82? 0.75? 0.67? 0.57?
M1 0.83? 0.76? 0.72? 0.61? 0.81? 0.74? 0.67? 0.55?
IM 0.87? 0.82? 0.77? 0.73? 0.80? 0.72? 0.66? 0.52?
Table 2: Model analysis on HEAD and TAIL. ? indicates statistical significance over BFB, and ? over both BFB and
Guo?09. Bold indicates statistical significance over all non-bold models in the column. Significance is measured
using the Student?s t-test at 95% confidence.
potential type from our inventory;
? queries where the annotators rated all potential
types as good; and
? queries where judges rated none of the potential
types as good
The final test sets consist of 105 head queries with
359 judged entity types and 98 tail queries with 343
judged entity types.
4.4 Metrics
Our task is a ranking task and therefore the classic
IR metrics nDCG (normalized discounted cumula-
tive gain) and MAP (mean average precision) are
applicable (Manning et al, 2008).
Both nDCG and MAP are sensitive to the rank
position, but not the score (probability of a type) as-
sociated with each rank, S(r). We therefore also
evaluate a weighted mean average precision score
MAPW, which replaces the precision component
of MAP, P (r), for the rth ranked type by:
P (r) =
?r
r?=1 I(r?)S(r?)?r
r?=1 S(r?)
(2)
where I(r) indicates if the type at rank r is judged
correct.
Our fourth metric is Prec@1, i.e. the precision of
only the top-ranked type of each query. This is espe-
cially suitable for applications where a single sense
must be determined.
4.5 Model Settings
We trained all models in Figure 1 using the training
data from Section 4.2 over 100 EM iterations, with
two folds per model. For Model IM, we varied the
number of user intents (K) in intervals from 100 to
400 (see Figure 3), under the assumption that multi-
ple intents would exist per entity type.
We compare our results against two baselines.
The first baseline is an assignment of Freebase types
according to their frequency in our query set BFB,
and the second is Model Guo?09 (Guo et al, 2009)
illustrated in Figure 1.
5 Experimental Results
Table 2 lists the performance of each model on the
HEAD and TAIL sets over each metric defined in
Section 4.4. On head queries, the addition of the
empty context parameter ? and click signal ? to-
gether (Model M1) significantly outperforms both
the baseline and the state-of-the-art model Guo?09.
Further modeling the user intent in Model IM re-
sults in significantly better performance over all
models and across all metrics. Model IM shows
its biggest gains in the first position of its ranking as
evidenced by the Prec@1 metric.
We observe a different behavior on tail queries
where all models significantly outperform the base-
line BFB, but are not significantly different from
each other. In short, the strength of our proposed
model is in improving performance on the head at
no noticeable cost in the tail.
We separately tested the effect of adding the
empty context parameter ?. Figure 2 illustrates the
result on the HEAD data. Across all metrics, ? im-
proved performance over all models3. The more
expressive models benefitted more than the less ex-
pressive ones.
Table 2 reports results for Model IM using K =
200 user intents. This was determined by varying
K and selecting the top-performing value. Figure 3
illustrates the performance of Model IM with dif-
ferent values of K on the HEAD.
3Note that model M0 is just the addition of the ? parameter
over Guo?09.
569
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
M0 M1 IMRe
lati
ve 
gai
n o
f sw
itch
 vs.
 no
 sw
itch
 
Effect of Empty Switch Parameter (s) on HEAD 
No switch
nDCG
MAP
MAPW
Prec@1
Figure 2: The switch parameter ? improves performance
of every model and metric.
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1 Varying K (latent intents) -  TAIL  
0.60.65
0.70.75
0.80.85
0.90.95
1
100 150 200 300 400K  
Model IM -  Varying K (latent intents)  
nDCG
MAP
MAPW
Prec@1
Figure 3: Model performance vs. the number of latent
intents (K).
Our models can also assign a prior type distribu-
tion to each entity by further marginalizing Eq. 1
over query contexts n1 and n2. We measured the
quality of our learned type priors using the subset
of queries in our HEAD test set that consisted of
only an entity without any refiners. The results for
Model IM were: nDCG = 0.86, MAP = 0.80,
MAPW = 0.75, and Prec@1 = 0.70. All met-
rics are statistically significantly better than BFB,
Guo?09 and M0, with 95% confidence. Compared
to Model M1, Model IM is statistically signifi-
cantly better on Prec@1 and not significantly dif-
ferent on the other metrics.
Discussion and Error Analysis: Contrary to
our results, we had expected improvements for
both HEAD and TAIL. Inspection of the TAIL
queries revealed that entities were greatly skewed
towards people (e.g., actor, author, and
politician). Analysis of the latent user in-
tent parameter ? in Model IM showed that most
people types had most of their probability mass
assigned to the same three generic and common in-
tents for people types: ?see pictures of?, ?find bio-
graphical information about?, and ?see video of?. In
other words, latent intents in Model IM are over-
expressive and they do not help in differentiating
people types.
The largest class of errors came from queries
bearing an entity with semantically very similar
types where our highest ranked type was not judged
correct by the annotators. For example, for the
query ?philippine daily inquirer? our system ranked
newspaper ahead of periodical but a judge
rejected the former and approved the latter. For
?ikea catalogue?, our system ranked magazine
ahead of periodical, but again a judge rejected
magazine in favor of periodical.
An interesting success case in the TAIL is high-
lighted by two queries involving the entity ?ymca?,
which in our data can either be a song, place,
or educational institution. Our system
learns the following priors: 0.63, 0.29, and 0.08,
respectively. For the query ?jamestown ymca ny?,
IM correctly classified ?ymca? as a place and for
the query ?ymca palomar? it correctly classified it
as an educational institution. We further
issued the query ?ymca lyrics? and the type song
was then highest ranked.
Our method is generalizable to any entity collec-
tion. Since our evaluation focused on the Freebase
collection, it remains an open question how noise
level, coverage, and breadth in a collection will af-
fect our model performance. Finally, although we
do not formally evaluate it, it is clear that training
our model on different time spans of queries should
lead to type distributions adapted to that time period.
6 Conclusion
Jointly modeling the interplay between the under-
lying user intents and entity types in web search
queries shows significant improvements over the
current state of the art on the task of resolving entity
types in head queries. At the same time, no degrada-
tion in tail queries is observed. Our proposed models
can be efficiently trained using an EM algorithm and
can be further used to assign prior type distributions
to entities in an existing knowledge base and to in-
sert new entities into it.
Although this paper leverages latent intents in
search queries, it stops short of understanding the
nature of the intents. It remains an open problem
to characterize and enumerate intents and to iden-
tify the types of queries that benefit most from intent
models.
570
References
Enrique Alfonseca, Marius Pasca, and Enrique Robledo-
Arnuncio. 2010. Acquisition of instance attributes
via labeled and related instances. In Proceedings of
SIGIR-10, pages 58?65, New York, NY, USA.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Men-
doza. 2004. Query recommendation using query logs
in search engines. In EDBT Workshops, Lecture Notes
in Computer Science, pages 588?596. Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD ?08, pages
1247?1250, New York, NY, USA.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3?10.
Mark James Carman, Fabio Crestani, Morgan Harvey,
and Mark Baillie. 2010. Towards query log based per-
sonalization using topic models. In CIKM?10, pages
1849?1852.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: An
experimental study. volume 165, pages 91?134.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In Proceedings of SIGIR ?11, pages 675?
684, New York, NY, USA. ACM.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proceedings
of SIGIR-09, pages 267?274, New York, NY, USA.
ACM.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545.
Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian tao
Sun, and Zheng Chen. 2009. Understanding user?s
query intent with wikipedia. In WWW, pages 471?480.
Alpa Jain and Marco Pennacchiotti. 2011. Domain-
independent entity extraction from web search query
logs. In Proceedings of WWW ?11, pages 63?64, New
York, NY, USA. ACM.
Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.
2007. Determining the user intent of web search en-
gine queries. In Proceedings of WWW ?07, pages
1149?1150, New York, NY, USA. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. volume 3,
pages 235?244.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07, pages 683?690, New York, NY, USA. ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In SIGKDD, pages 613?619, Ed-
monton, Canada.
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In
COLING, pages 771?777.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations and
clicks. In Proceedings of the 19th international con-
ference on World wide web, WWW ?10, pages 1171?
1172, New York, NY, USA. ACM.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING, pages
931?939.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discov-
ery. In Proceedings of AAAI-09 Spring Symposium on
Learning by Reading and Learning to Read, pages 88?
93.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In Proceedings of
the 13th international conference on World Wide Web,
WWW ?04, pages 13?19, New York, NY, USA. ACM.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL ?11, pages 965?975,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24:97?123,
March.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring on-
tological knowledge from query logs. In Proceedings
of the 16th international conference on World Wide
Web, WWW ?07, pages 1223?1224, New York, NY,
USA. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries. In
WWW, pages 1001?1010.
Z. Zhang and O. Nasraoui. 2006. Mining search en-
gine query logs for query recommendation. In WWW,
pages 1039?1040.
571
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1524?1533,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Smart Selection
Patrick Pantel
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
ppantel@microsoft.com
Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
mgamon@microsoft.com
Ariel Fuxman
Microsoft Research
1065 La Avenida St.
Mountain View, CA 94043, USA
arielf@microsoft.com
Abstract
Natural touch interfaces, common now in
devices such as tablets and smartphones,
make it cumbersome for users to select
text. There is a need for a new text selec-
tion paradigm that goes beyond the high
acuity selection-by-mouse that we have re-
lied on for decades. In this paper, we in-
troduce such a paradigm, called Smart Se-
lection, which aims to recover a user?s in-
tended text selection from her touch input.
We model the problem using an ensemble
learning approach, which leverages mul-
tiple linguistic analysis techniques com-
bined with information from a knowledge
base and a Web graph. We collect a dataset
of true intended user selections and simu-
lated user touches via a large-scale crowd-
sourcing task, which we release to the
academic community. We show that our
model effectively addresses the smart se-
lection task and significantly outperforms
various baselines and standalone linguistic
analysis techniques.
1 Introduction
The process of using a pointing device to select
a span of text has a long history dating back to
the invention of the mouse. It serves to access
functions on text spans, such as copying/pasting,
looking up a word in a dictionary, searching the
Web, or accessing other accelerators. As con-
sumers move from traditional PCs to mobile de-
vices (e.g., tablets and smartphones), touch inter-
action is replacing the pointing devices of yore.
Although more intuitive and arguably a more natu-
ral form of interaction, touch offers much less acu-
ity (colloquially referred to as the fat finger prob-
lem). To select multi-word spans today, mobile
devices require an intricate series of gestures that
results in cumbersome user experiences
1
. Conse-
quently, there is an opportunity to reinvent the way
users select text in such devices.
Our task is, given a single user touch, to pre-
dict the span that the user likely intended to se-
lect. We call this task smart selection. We re-
strict our prediction task to cases where a user in-
tends to perform research on a text span (dictio-
nary/thesaurus lookup, translation, searching). We
specifically consider operations on text spans that
do not form a single unit (i.e., an entity, a concept,
a topic, etc.) to be out of scope. For example, full
sentences, paragraph and page fragments are out
of scope.
Smart selection, as far as we know, is a new re-
search problem. Yet there are many threads of re-
search in the NLP community that identify multi-
word sequences, which have coherent properties.
For example, named-entity recognizers identify
entities such as people/places/organizations, chun-
kers and parsers identify syntactic constituents
such as noun phrases, key phrase detectors or term
segmentors identify term boundaries. While each
of these techniques retrieve meaningful linguistic
units, our problem is a semantic one of recovering
a user?s intent, and as such none alone solves the
entire smart selection problem.
In this paper, we model the problem of smart
selection using an ensemble learning approach.
We leverage various linguistic techniques, such as
those discussed above, and augment them with
other sources of information from a knowledge
1
In order to select a multi-word span, a user would first
have to touch on either word, then drag the left and right
boundary handles to expand it to the adjacent words.
1524
base and a web graph. We evaluate our meth-
ods using a novel dataset constructed for our
task. We construct our dataset of true user-
intended selections by crowdsourcing the task of
a user selecting spans of text in a researching
task. We obtain 13,681 data points. For each in-
tended selection, we construct test cases for each
individual sub-word, simulating the user select-
ing via touch. The resulting testset consists of
33,912 ?simulated selection, intended selection?-
pairs, which we further stratify into head, torso,
and tail subsets. We release the full dataset and
testset to the academic community for further re-
search on this new NLP task. Finally, we empir-
ically show that our ensemble model significantly
improves upon various baseline systems.
In summary, the major contributions of our re-
search are:
? We introduce a new natural language pro-
cessing task, called smart selection, which
aims to address an important problem in text
selection for touch-enabled devices;
? We conduct a large crowd-sourced user study
to collect a dataset of intended selections and
simulated user selections, which we release
to the academic community;
? We propose a machine-learned ensemble
model for smart selection, which combines
various linguistic annotation methods with
information from a large knowledge base and
web graph;
? We empirically show that our model can ef-
fectively address the smart selection task.
2 Related Work
Related work falls into three broad categories: lin-
guistic unit detection, human computer interaction
(HCI), and intent detection.
2.1 Linguistic Unit Detection
Smart selection is closely related to the detection
of syntactic and semantic units: user selections are
often entities, noun phrases, or concepts. A first
approach to solving smart selection is to select an
entity, noun phrase, or concept that subsumes the
user selection. However, no single approach alone
can cover the entire smart selection problem. For
example, consider an approach that uses a state-of-
the-art named-entity recognizer (NER) (Chinchor,
1998; Tjong Kim Sang and De Meulder, 2003;
Finkel et al, 2005; Ratinov and Roth, 2009). We
found in our dataset (see Section 3.2) that only
a quarter of what users intend to select consists
in fact of named entities. Although an NER ap-
proach can be very useful, it is certainly not suf-
ficient. The remainder of the data can be partially
addressed with noun phrase (NP) detectors (Ab-
ney, 1991; Ramshaw and Marcus, 1995; Mu?noz et
al., 1999; Kudo and Matsumoto, 2001) and lists of
items in a knowledge base (KB), but again, each is
not alone sufficient. NP detectors and KB-based
methods are further very susceptible to the gen-
eration of false positives (i.e., text contains many
nested noun phrases and knowledge base items in-
clude highly ambiguous terms).
In our work, we leverage all three techniques in
order to benefit from their complementary cover-
age of user selections. We further create a novel
unit detector, called the hyperlink intent model.
Based on the assumption that Wikipedia anchor
texts are similar in nature to what users would se-
lect in a researching task, it models the problem
of recovering Wikipedia anchor texts from partial
selections.
2.2 Human Computer Interaction
There is a substantial amount of research in the
HCI community on how to facilitate interaction
of a user with touch and speech enabled devices.
To give but a few examples of trends in this field,
Gunawardana et al (2010) address the fat finger
problem in the use of soft keyboards on mobile de-
vices, Kumar et al (2012) explore a novel speech
interaction paradigm for text entry, and Sakamoto
et al (2013) introduce a technique that combines
touch and voice input on a mobile device for im-
proved navigation of user interface elements such
as commands and controls. To the best of our
knowledge, however, the problem of smart selec-
tion as we defined it has not been addressed.
2.3 Intent detection
There is a long line of research in the web lit-
erature on understanding user intent. The clos-
est to smart selection is query recommendation
(Baeza-Yates et al, 2005; Zhang and Nasraoui,
2006; Boldi et al, 2008), where the goal is to sug-
gest queries that may be related to a user?s intent.
Query recommendation techniques are based ei-
ther on clustering queries by their co-clicked URL
patterns (Baeza-Yates et al, 2005) or on leverag-
ing co-occurrences of sequential queries in web
1525
search sessions (Zhang and Nasraoui, 2006; Boldi
et al, 2008; Sadikov et al, 2010). The key dif-
ference from smart selection is that in our task the
output is a selection that is relevant to the context
of the document where the original selection ap-
pears (e.g., by adding terms neighboring the selec-
tion). In query recommendation, however, there is
no notion of a document being read by the user
and, instead, the recommendations are based ex-
clusively on the aggregation of behavior of multi-
ple users.
3 Problem Setting and Data
3.1 Smart Selection Definition
Let D be the set of all documents. We define a
selection to be a character ?offset, length?-tuple in
a document d ? D. Let S be the set of all possible
selections in D and let S
d
be the set of all possible
selections in d.
We define a scored smart selection, ?, in a doc-
ument d, as a pair ? = ?x, y? where x ? S
d
is a
selection and y ? R
+
is a score for the selection.
We formally define the smart selection function
? as producing a ranked scored list of all possi-
ble selections from a document and user selection
pair
2
:
? : D? S? (?
1
, ..., ?
|S
d
|
| x
i
? S
d
, y
i
? y
i+1
)
(1)
Consider a user who selects s in a document d.
Let ? be the target selection that best captures what
the user intended to select. We define the smart
selection task as recovering ? given the pair ?d, s?.
Our problem then is to learn a function ? that best
recovers the target selection from any user selec-
tion.
Note that even for a human, reconstructing an
intended selection from a single word selection is
not trivial. While there are some fairly clear cut
cases such as expanding the selection ?Obama?
to Barack Obama in the sentence ?While in
DC, Barack Obama met with...?, there are cases
where the user intention depends on extrinsic fac-
tors such as the user?s interests. For example, in
a phrase ?University of California at Santa Cruz?
with a selection ?California?, some (albeit proba-
bly few) users may indeed be interested in the state
of California, others in the University
2
The output consists of a ranked list of selections instead
of a single selection to allow experiences such as proposing
an n-best list to the user.
of California system of universities, and
yet others specifically in the University of
California at Santa Cruz. In the next
section, we describe how we obtained a dataset of
true intended user selections.
3.2 Data
In order to obtain a representative dataset for the
smart selection task, we focus on a real-world ap-
plication of users interacting with a touch-enabled
e-reader device. In this application, a user is read-
ing a book and chooses phrases for which she
would like to get information from resources such
as a dictionary, Wikipedia, or web search. Yet, be-
cause of the touch interface, she may only touch
on a single word.
3.2.1 Crowdsourced Intended Selections
We obtain the intended selections through the fol-
lowing crowdsourcing exercise. We use the en-
tire collection of textbooks in English from Wik-
ibooks
3
, a repository of publicly available text-
books. The corpus consists of 2,696 textbooks that
span a large variety of categories such as Comput-
ing, Humanities, Science, etc. We first produce
a uniform random sample of 100 books, and then
sample one paragraph from each book. The result-
ing set of 100 paragraphs is then sent to the crowd-
sourcing system. Each paragraph is evaluated by
100 judges, using a pool of 152 judges. For each
paragraph, we request the judges to select com-
plete phrases for which they would like to ?learn
more in resources such as Wikipedia, search en-
gines and dictionaries?, i.e., our true user intended
selections. As a result of this exercise, we obtain
13,681 judgments, corresponding to 4,067 unique
intended selections. The distribution of number of
unique judges who selected each unique intended
selection, in a log-log scale, is shown in Figure
1. Notice that this is a Zipfian distribution since it
follows a linear trend in the log-log scale.
Intuitively, the likelihood that a phrase is of
interest to a user correlates with the number of
judges who select that phrase. We thus use the
number of judges who selected each phrase as a
proxy for the likelihood that the phrase will be
chosen by users.
The resulting dataset consists of 4,067 ?d, ??-
pairs where d is a Wikibook document paragraph
and ? is an intended selection, along with the num-
ber of judges who selected it. We further assigned
3
Available at http://wikibooks.org.
1526
0
2
4
6
8
10
12
0 1 2 3 4 5 6
LOG 2(
Uniqu
e inte
nded 
select
ions)
LOG2(Unique judges that selected the intended selection)
Figure 1: Zipfian distribution of unique intended
selections vs. the number of judges who selected
them, in log-log scale.
each pair to one of five randomly chosen folds,
which are used for cross-validation experiments.
3.2.2 Testset Construction
We define a test case as a triple ?d, s, ?? where
s is a simulated user selection. For each ?d, ??-
pair in our dataset we construct n correspond-
ing test cases by simulating the user selections
{?d, ?, s
1
?, . . . , ?d, ?, s
n
?}where s
1
, . . . , s
n
corre-
spond to the individual words in ? . In other words,
each word in ? is considered as a candidate user
selection.
We discard all target selections that only a sin-
gle judge annotated since we observed that these
mostly contained errors and noise, such as full sen-
tences or nonsensical long sentence fragments.
Our first testset, labeled T
ALL
, is the resulting
traffic-weighted multiset. That is, each test case
?d, s, ?? appears k times, where k is the number
of judges who selected ? in d. T
ALL
consists of
33,913 test cases.
We further utilize the distribution of judgments
in the creation of three other testsets. Following
the stratified sampling methodology commonly
employed in the IR community, we construct
testsets for the frequently, less frequently, and
rarely annotated intended selections, which we
call HEAD, TORSO, and TAIL, respectively. We
obtain these testsets by first sorting each unique
selection according to their frequency of occur-
rence, and then partitioning the set so that HEAD
corresponds to the elements at the top of the list
that account for 20% of the judgments; TAIL cor-
responds to the elements at the bottom also ac-
counting for 20% of the judgments; and TORSO
corresponds to the remaining elements. The re-
sulting test sets, T
HEAD
, T
TORSO
, T
TAIL
consist of
114, 2115, and 5798 test cases, respectively
4
.
Test sets along with fold assignments
and annotation guidelines are avail-
able at http://research.microsoft.com/en-
us/downloads/eb42522c-068e-404c-b63f-
cf632bd27344/.
3.3 Discussion
Our focus on single word selections is motivated
by the touchscreen scenario presented in Sec-
tion 1. Although our touch simulation assumes
that each word in a target selection is equally likely
to be selected by a user, in fact we expect this dis-
tribution to be non-uniform. For example, users
may tend to select the first or last word more fre-
quently than words in the middle of the target se-
lection. Or perhaps users tend to select nouns and
verbs more frequently than function words. We
consider this out of scope for our paper, but view it
as an important avenue of future investigation. Fi-
nally, for non-touchscreen environments, such as
the desktop case, it would also be interesting to
study the problem on multi-word user selections.
To get an idea of the kind of intended selections
that comprise our dataset, we broke them down ac-
cording to whether they referred to named entities
or not. Perhaps surprisingly, the fraction of named
entities in the dataset is quite low, 24.3%
5
. The
rest of the intended selections mostly correspond
to concepts and topics such as embouchure forma-
tion, vocal fold relaxation, NHS voucher values,
time-domain graphs, etc.
4 Model
As argued in Section 1, existing techniques,
such as NER taggers, chunkers, Knowledge Base
lookup, etc., are geared towards aspects of the
task (i.e., NEs, concepts, KB entries), but not the
task as a whole. We can, however, combine the
outputs of these systems with a learned ?meta-
model?. The meta-model ranks the combined can-
didates according to a criterion that is derived from
data that resembles real usage of smart selection
as closely as possible. This technique is known
4
We stress that T
ALL
is a multi-set, reflecting the over-
all expected user traffic from our 100 judges per paragraph.
T
HEAD
, T
TORSO
, T
TAIL
, in contrast, are not multi-sets since
judgment frequency is already accounted for in the stratifi-
cation process, as commonly done in the IR community.
5
Becker et al (2012) report a similar finding, showing that
only 26% of questions, which a user might ask after reading
a Wikipedia article, are focused on named entities.
1527
in the machine learning community as ensemble
learning (Dietterich, 1997).
Our ensemble approach, described in this sec-
tion, serves as our main implementation of the
smart selection function ? of Equation 1. Each of
the ensemble members are themselves a separate
implementation of ? and will be used as a point
of comparison in our experiments. Below, we de-
scribe the ensemble members before turning to the
ensemble learner.
4.1 Ensemble Members
4.1.1 Hyperlink Intent Model
The Hyperlink Intent Model (HIM), which lever-
ages web graph information, is a machine-learned
system based on the intuition that anchor texts in
Wikipedia are good representations of what users
might want to learn about. We build upon the fact
that Wikipedia editors write anchor texts for enti-
ties, concepts, and things of potential interest for
follow-up to other content. HIM learns to recover
anchor texts from their single word subselections.
Specifically, HIM iteratively decides whether to
expand the current selection (initially a single
word) one word to the left or right via greedy bi-
nary decisions, until a stopping condition is met.
At each step, two binary classifiers are consulted.
The first one scores the left expansion decision
and the second one scores the right expansion de-
cision. In addition, we use the same two classi-
fiers to evaluate the expansion decision ?from the
outside in?, i.e., from the word next to the current
selection (left and right, respectively) to the clos-
est word in the current selection. If the probabil-
ity for expansion of any model exceeds a prede-
fined threshold, then the most probable expansion
is chosen and we continue the iteration with the
newly expanded selection as input. The algorithm
is illustrated in Figure 2.
We automatically create our training set for HIM
by first taking a random sample of 8K Wikipedia
anchor texts. We treat each anchor text as an in-
tended selection, and each word in the anchor text
as a simulated user selection. For each word to the
left (or the right) of the user selection that is part
of the anchor text, we create a positive training ex-
ample. Similarly, for each word to the left (or the
right) that is outside of the anchor text, we create a
negative training example. We include additional
negative examples using random word selections
from Wikipedia content. For this purpose we sam-
Left Context Right Context
Current selection Candidate RightSelectedWord 2
Candidate Left SelectedWord 1
Context 1Context 2
Context 4 Context 3
Figure 2: Hyperlink Intent Model (HIM) decoding
flow for smart selection.
ple random words that are not part of an anchor
text. Our final data consists of 2.6M data points,
with a 1:20 ratio of positive to negative examples
6
.
We use logistic regression as the classification
algorithm for our binary classifiers. The fea-
tures used by each model are computed over three
strings: the current selection s (initially the single-
word simulated user selection), the candidate ex-
pansion word w, and one word over from the
right or left of s. The features fall into five fea-
ture families: (1) character-level features, includ-
ing capitalization, all-cap formatting, character
length, presence of opening/closing parentheses,
presence and position of digits and non-alphabetic
characters, and minimum and average character
uni/bi/trigram frequencies (based on frequency ta-
bles computed offline from Wikipedia article con-
tent); (2) stopword features, which indicate the
presence of a stop word (from a stop word list);
(3) tf.idf scores precomputed from Wikipedia con-
tent statistics; (4) knowledge base features, which
indicate whether a string matches an item or a sub-
string of an item in the knowledge base described
in Section 4.1.2 below; and (5) lexical features,
which capture the actual string of the current se-
lection and the candidate expansion word.
4.1.2 Unit Spotting
Our second qualitative class of ensemble members
use notions of unit that are either based on linguis-
tic constituency or knowledge base presence. The
general process is that any unit that subsumes the
user selection is treated as a smart selection can-
didate. Scoring of candidates is by normalized
length, under the assumption that in general the
most specific (longest) unit is more likely to be the
intended selection.
6
Note that this training set is generated automatically and
is, by design, of a different nature than the manually labeled
data we use to train and test the ensemble model.
1528
Our first unit spotter, labeled NER is geared
towards recognizing named entities. We use
a commercial and proprietary state-of-the-art
NER system, trained using the perceptron algo-
rithm (Collins, 2002) over more than a million
hand-annotated labels.
Our second approach uses purely syntactic in-
formation and treats noun phrases as units. We la-
bel this model as NP. For this purpose we parse the
sentence containing the user selection with a syn-
tactic parser following (Ratnaparkhi, 1999). We
then treat every noun phrase that subsumes the
user selection as a candidate smart selection.
Finally, our third unit spotter, labeled KB, is
based on the assumption that concepts and other
entries in a knowledge base are, by nature, things
that can be of interest to people. For our knowl-
edge base lookup, we use a proprietary graph con-
sisting of knowledge from Wikipedia, Freebase,
and paid feeds from various providers from do-
mains such as entertainment, local, and finance.
4.1.3 Heuristics
Our third family of ensemble members imple-
ments simple heuristics, which tend to be high pre-
cision especially in the HEAD of our data.
The first heuristic, representing the current
touch-enabled selection paradigm seen in many of
today?s tablets and smartphones, is labeled CUR. It
simply assumes that the intended selection is al-
ways the user-selected word.
The second is a capitalization-based heuristic
(CAP), which simply expands every selected capi-
talized word selection to the longest uninterrupted
sequence of capitalized words.
4.2 Ensemble Learning
In this section, we describe how we train our meta-
learner, labeled ENS, which takes as input the can-
didate lists produced by the ensemble members
from Section 4.1, and scores each candidate, pro-
ducing a final scored ranked list.
We use logistic regression as a classification al-
gorithm to address this task. Our 22 features in
ENS consist of three main classes: (1) features
related to the individual ensemble members; (2)
features related to the user selection; and (3) fea-
tures related to the candidate smart selection. For
(1), the features consist of whether a particular
ensemble member generated the candidate smart
selection and its score for that candidate. If the
candidate smart selection is not in the candidate
list of an ensemble member, its score is set to
zero. For both (2) and (3), features account for
length and capitalization properties of the user se-
lection and the candidate smart selection (e.g., to-
ken length, ratio of capitalized tokens, ratio of cap-
italized characters, whether or not the first and last
tokens are capitalized.)
Although training data for the HIM model was
automatically generated from Wikipedia, for ENS
we desire training data that reflects the true ex-
pected user experience. For this, we use five-
fold cross-validation over our data collection de-
scribed in Section 3.2. That is, to decode a fold
with our meta-learner, we train ENS with the other
four folds. Note that every candidate selection for
a ?document, user selection?-pair, ?d, s?, for the
same d and s, are assigned to a single fold, hence
the training process does not see any user selection
from the test set.
5 Experimental Results
5.1 Experimental Setup
Recall our testsets T
ALL
, T
HEAD
, T
TORSO
, and T
TAIL
from Section 3.2.2, where a test case is defined as
a triple ?d, s, ??, and where d is a document, s is a
user selection, and ? is the intended user selection.
In this section, we describe our evaluation metric
and summarize the system configurations that we
evaluate.
5.1.1 Metric
In our evaluation, we apply the smart selection
function ?(d, s) (see Eq. 1) to each test case and
measure how well it recovers ? .
Let A be the set of ?d, ??-pairs from our dataset
described in Section 3.2.1 that corresponds to a
testset T. Let T
?d,??
be the set of all test cases
in T with a fixed d and ? . We define the macro
precision of a smart selection function, P
?
, as fol-
lows:
P
?
=
1
| A |
?
?d,???A
P
?
(d, ?) (2)
P
?
(d, ?) =
1
| T
?d,??
|
?
?d,s,???T
?d,??
P
?
(d, s, ?)
P
?
(d, s, ?) =
1
| ?(d, s) |
?
???(d,s)
I(?, ?)
I(?, ?) =
{
1 if ? = ?x, y? ? x = ?
0 otherwise
1529
CP@1 CP@2 CP@3 CP@4 CP@5
CUR 39.3 - - - -
CAP 48.9 51.0 51.2 51.8 51.8
NER 43.5 - - - -
NP 34.1 50.2 55.5 57.1 57.6
KB 50.2 50.8 50.9 50.9 50.9
HIM 48.1 48.8 48.8 48.8 48.8
ENS 56.8
?
76.0
?
82.6
?
85.2
?
86.6
?
Table 1: Smart selection performance, as a func-
tion of CP, on T
ALL
.
?
and
?
indicate statistical
significance with p = 0.01 and 0.05, respectively.
An oracle ensemble would achieve an upper bound
CP of 87.3%.
We report cumulative macro precision at
rank (CP@k) in our experiments since our
testsets contain a single true user-intended
selection for each test case
7
. However,
this is an overly conservative metric since
in many cases an alternative smart selection
might equally please the user. For example,
if our testset contains a user intended selec-
tion ? = The University of Southern
California, then given the simulated selec-
tion ?California?, both ? and University of
Southern California would most likely
equally satisfy the user intent (whereas the latter
would be considered incorrect in our evaluation).
In fact, the ideal testset would further evaluate the
distance or relevance of the smart selection to the
intended user selection. We would then find per-
haps that Southern California is a more
reasonable smart selection than of Southern
California. However, precisely defining such
a relevance function and designing the guidelines
for a user study is non-trivial and left for future
work.
5.1.2 Systems
In our experiments, we evaluate the follow-
ing systems, each described in detail in Sec-
tion 4: Passthrough (CUR), Capitalization (CAP),
Named-Entity Recognizer (NER), Noun Phrase
(NP), Knowledge Base (KB), Hyperlink Intent
Model (HIM), Ensemble (ENS).
5.2 Results
Table 1 reports the smart selection performance on
the full traffic weighted testset T
ALL
, as a func-
7
Because there is only a single true intended selection for
each test case, Recall@k = CP@k.
tion of CP@k. Our ensemble approach recovers
the true user-intended selection in 56.8% of the
cases. In its top-2 and top-3 ranked smart selec-
tions, the true user-intended selection is retrieved
76.0% and 82.6% of the time, respectively. In po-
sition 1, ENS significantly outperforms all other
systems with 95% confidence. Moreover, we no-
tice that the divergence between ENS and the other
systems greatly increases for K ? 2, where the
significance is now at the 99% level.
The CUR system models the selection paradigm
of today?s consumer touch-enabled devices (i.e., it
assumes that the intented selection is always the
touched word). Without changing the user inter-
face, we report a 45% improvement in predicting
what the user intended to select over this baseline.
If we changed the user interface to allow two or
three options to be displayed to the user, then we
would improve by 93% and 110%, respectively.
For CUR and NER, we report results only at
K = 1 since these systems only ever return a sin-
gle smart selection. Note also that when no named
entity is found by NER, or no noun phrase is found
by NP or no knowledge base entry is found by KB,
the corresponding systems return the original user
selection as their smart selection.
CAP does not vary much across K: when the
intended selection is a capitalized multi-word, the
longest string tends to be the intended selection.
The same holds for KB.
Whereas Table 1 reports the aggregate expected
traffic performance, we further explore the per-
formance against the stratified T
HEAD
, T
TORSO
, and
T
TAIL
testsets. The results are summarized in Ta-
ble 2. As outlined in Section 3.2, the HEAD se-
lections tend to be disproportionately entities and
capitalized terms when compared to the TORSO
and TAIL. Hence CAP, NER and KB perform much
better on the HEAD. In fact, on the HEAD, CAP per-
forms statistically as well as the ENS model. This
means that at position 1, for systems that need to
focus only on the HEAD, a very simple solution is
adequate. For TORSO and TAIL, however, ENS
performs better. At positions 2 and 3, across all
strata, the ENS model significantly outperforms all
other systems (with 99% confidence).
Next, we studied the relative contribution of
each ensemble member to the ENS model. Fig-
ure 3 illustrates the results of the ablation study.
The ensemble member that results in the biggest
performance drop when removed is HIM. Perhaps
1530
HEAD TORSO TAIL
CP@1 CP@2 CP@3 CP@1 CP@2 CP@3 CP@1 CP@2 CP@3
CUR 48.5 - - 36.7 - - 26.6 - -
CAP 74.2 74.7 74.8 43.0 45.0 45.1 26.1 27.4 28.2
NER 60.6 - - 39.2 - - 26.7 - -
NP 52.3 64.9 69.4 31.0 48.2 53.8 20.0 32.2 35.7
KB 66.7 66.7 66.7 47.0 47.9 48.1 29.9 30.1 30.1
HIM 64.4 65.7 65.7 44.7 45.2 45.4 27.9 28.2 28.2
ENS 75.8 91.8
?
96.5
?
52.7
?
73.7
?
81.5
?
32.4
?
50.7
?
58.5
?
Table 2: Smart selection performance, as a function of CP, on the T
HEAD
, T
TORSO
, and T
TAIL
testsets.
?
and
?
indicate statistical significance with p = 0.01 and 0.05, respectively. An oracle ensemble would
achieve an upper bound CP of 98.5%, 86.8% and 64.8% for T
HEAD
, T
TORSO
, and T
TAIL
, respectively.
0.7
0.75
0.8
0.85
0.9
0.95
1
S mart Selection Cumulative Precision @ Rank (ALL)
Ensemble Member Ablation
E NS
-H IM
-KB
-NE R
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5
Cumul
ative
 Preci
sion
Rank
Smart Selection Cumulative Precision @ Rank (ALL)
Ensemble Member Ablation
ENS
- HIM
- KB
- NE R
- NP
Figure 3: Ablation of ensemble model members
over T
ALL
. Each consecutive model removes one
member specified in the series name.
surprisingly, a first ablation of either the CAP or
KB model, two of the better individual performing
models from Table 1, leads to an ablated-ENS per-
formance that is nearly identical to the full ENS
model. One possible reason is that both tend to
generate similar candidates (i.e., many entities in
our KB are capitalized). Although the HIM model
as a standalone system does not outperform sim-
ple linguistic unit selection models, it appears to
be the most important contributor to the overall
ensemble.
5.3 Error Analysis: Oracle Ensemble
We begin by assessing an upper bound for our en-
semble, i.e., an oracle ensemble, by assuming that
if a correct candidate is generated by any ensem-
ble member, the oracle ensemble model places it
in first position. For T
ALL
the oracle performance
is 87.3%. In other words, our choice of ensemble
members was able to recover a correct smart se-
lection as a candidate in 87.3% of the user study
cases. For T
HEAD
, T
TORSO
, and T
TAIL
, the oracle
performance is 98.5%, 86.8%, and 64.8%, respec-
tively.
Although our ENS model?s CP@3 is within 2-6
points of the oracle, there is room to significantly
improve our CP@1, see Table 1 and Table 2. We
analyze this opportunity by inspecting a random
sample of 200 test cases where ENS produced an
incorrect smart selection in position 1. The break-
down of these cases is: 1 case from T
HEAD
; 50
cases from T
TORSO
; 149 cases from T
TAIL
, i.e.,
most errors occur in the TAIL.
For 146 of these cases (73%), not a single en-
semble member produced the correct target selec-
tion ? as a candidate. We analyze these cases in
detail in Section 5.4. Of the remaining cases, 25,
10, 9, 4, 4, and 2 were correct in positions 2, 3, 4,
5, 6, 7, respectively. Table 3 lists some examples.
In 18 cases (33%), the result in position 1 is
very reasonable given the context and user selec-
tion (see lines 1-4 in Table 3 for examples). Often
the target selection was also found in second po-
sition. These cases highlight the need for a more
relaxed, relevance-based user study, as pointed out
at the end of Section 5.1.1.
We attributed 7 (13%) of the cases to data prob-
lems: some cases had a punctuation as a sole char-
acter user selection, some had a mishandled es-
caped quotation character, and some had a UTF-8
encoding error.
The remaining 29 (54%) were truly model er-
rors. Some examples are shown in lines 5-8 in Ta-
ble 3. We found three categories of errors here.
First, our model has learned a strong prior on pre-
ferring the original user selection (see example
line 5). From a user experience point of view,
when the model is unsure of itself, it is in fact
better not to alter her selection. Second, we also
learned a strong capitalization prior, i.e., to trust
the CAP member (see example line 6). Finally, we
noticed that we have difficulty handling user selec-
tions consisting of a stopword (we noted determin-
ers, prepositions, and the word ?and?). Adding a
few simple features to ENS based on a stopwords
list or a list of closed-class words should address
this problem.
1531
Text Snippet User Selection ENS 1st Result
1 ?The Russian conquest of the South Caucasus in the 19th century split the
speech community across two states...?
Caucasus South Caucasus
2 ?...are generally something that transportation agencies would like to mini-
mize...?
transportation transportation agencies
3 ?The vocal organ of birds, the syrinx, is located at the base of the blackbird?s
trachea.?
vocal vocal organ
4 ?An example of this may be an idealised waveform like a square wave...? waveform idealised waveform
5 ?Tickets may be purchased from either the ticket counter or from automatic
machines...?
counter counter
6 ?PBXT features include the following: MVCC Support: MVCC stands for
Multi-version Concurrency Control.?
MVCC MVCC Support
7 ?Centers for song production pathways include the High vocal center; ro-
bust nucleus of archistriatum (RA); and the tracheosyringeal part of the hy-
poglossal nucleus...?
robust robust nucleus
8 ?...and get an 11gR2 RAC cluster database running inside virtual ma-
chines...?
cluster RAC cluster
Table 3: Position 1 errors when applying ENS to our test cases. The text snippet is a substring of a
paragraph presented to our judges with the target selection (? ) indicated in bold.
5.4 Error Analysis: Ensemble Members
Over all test cases, the distribution of cases with-
out a correct candidate generated by an ensem-
ble member in the HEAD, TORSO, TAIL is 0.3%,
34.6%, and 65.1%, respectively. We manually in-
spected a random sample of 100 such test cases.
The majority of them, 83%, were large sentence
fragments, which we consider out of scope ac-
cording to our prediction task definition outlined
in Section 1. The average token length of the tar-
get selection ? for these was 15.3. In compari-
son, we estimate the average token length of the
task-admissable cases to be 2.7 tokens. Although
most of these long fragment selections seem to
be noise, a few cases are statements that a user
would reasonably want to know more about, such
as: (i) ?Talks of a merger between the NHL and
the WHA were growing? or (ii) ?NaN + NaN *
1.0i?.
In 10% of the cases, we face a punctuation-
handling issue, and in each case our ensemble was
able to generate a correct candidate when fixing
the punctuation. For example, for the book title
? = What is life?, our ensemble found the
candidate What is life, dropping the ques-
tion mark. For ? = Near Earth Asteroid.
our ensemble found Near Earth Asteroid,
dropping the period. Similar problems occurred
with parentheses and quotation marks.
In two cases, our ensemble members dropped
a leading ?the? token, e.g., for ? = the Hume
Highway, we found Hume Highway.
Finally, 2 cases were UTF-8 encoding mistakes,
leaving five ?true error? cases.
6 Conclusion and Future Work
We introduced a new paradigm, smart selection,
to address the cumbersome text selection capabil-
ities of today?s touch-enabled mobile devices. We
report 45% improvement in predicting what the
user intended to select over current touch-enabled
consumer platforms, such as iOS, Android and
Windows. We release to the community a dataset
of 33, 912 crowdsourced true intended user selec-
tions and corresponding simulated user touches.
There are many avenues for future work, includ-
ing understanding the distribution of user touches
on their intended selection, other interesting sce-
narios (e.g., going beyond the e-reader towards
document editors and web browsers may show dif-
ferent distributions in what users select), leverag-
ing other sources of signal such as a user?s profile,
her interests and her local session context, and ex-
ploring user interfaces that leverage n-best smart
selection prediction lists, for example by provid-
ing selection options to the user after her touch.
With the release of our 33, 912-crowdsourced
dataset and our model analyses, it is our hope that
the research community can help accelerate the
progress towards reinventing the way text selec-
tion occurs today, the initial steps for which we
have taken in this paper.
7 Acknowledgments
The authors thank Aitao Chen for sharing his
NER tagger for our experiments, and Bernhard
Kohlmeier, Pradeep Chilakamarri, Ashok Chan-
dra, David Hamilton, and Bo Zhao for their guid-
ance and valuable discussions.
1532
References
Steven. P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257?278. Kluwer,
Dordrecht.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo
Mendoza. 2005. Query recommendation using
query logs in search engines. In Current Trends
in Database Technology-EDBT 2004 Workshops,
pages 588?596. Springer.
Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the gap: Learning to choose gaps for
question generation. In Proceedings of NAACL HLT
?12, pages 742?751.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applica-
tions. In Proceedings of CIKM ?08, pages 609?618.
ACM.
Nancy A. Chinchor. 1998. Named entity task defini-
tion. In Proceedings of the Seventh Message Under-
standing Conference (MUC-7), Fairfax, VA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Thomas G. Dietterich. 1997. Machine Learning Re-
search - Four Current Directions. AI Magazine,
18:4:97?136.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In In ACL, pages 363?370.
Asela Gunawardana, Tim Paek, and Christopher Meek.
2010. Usability guided key-target resizing for soft
keyboards. In Proceedings of IUI ?10, pages 111?
118.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
?01, pages 1?8.
Anuj Kumar, Tim Paek, and Bongshin Lee. 2012.
Voice typing: A new speech interaction model for
dictation on touchscreen devices. In Proceedings of
CHI?12, pages 2277?2286.
Marcia Mu?noz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP/VLC, pages 168?
178.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the 3rd ACL Workshop on Very
Large Corpora, pages 82?94. Cambridge MA, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-2009, pages 147?155.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34(1-3):151?175, February.
Eldar Sadikov, Jayant Madhavan, Lu Wang, and Alon
Halevy. 2010. Clustering query refinements by user
intent. In Proceedings of the 19th international con-
ference on World wide web, pages 841?850. ACM.
Daisuke Sakamoto, Takanori Komatsu, and Takeo
Igarashi. 2013. Voice augmented manipulation: us-
ing paralinguistic information to manipulate mobile
devices. In Mobile HCI, pages 69?78.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142?147. Ed-
monton, Canada.
Zhiyong Zhang and Olfa Nasraoui. 2006. Mining
search engine query logs for query recommendation.
In Proceedings of the 15th international conference
on World Wide Web, pages 1039?1040. ACM.
1533
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 37?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Search right and thou shalt find ...
Using Web Queries for Learner Error Detection 
Michael Gamon Claudia Leacock 
Microsoft Research Butler Hill Group 
One Microsoft Way P.O. Box 935 
Redmond, WA 981052, USA Ridgefield, CT 06877, USA 
mgamon@microsoft.com Claudia.leacock@gmail.com 
 
 
Abstract
We investigate the use of web search queries 
for detecting errors in non-native writing. Dis-
tinguishing a correct sequence of words from 
a sequence with a learner error is a baseline 
task that any error detection and correction 
system needs to address. Using a large corpus 
of error-annotated learner data, we investigate 
whether web search result counts can be used 
to distinguish correct from incorrect usage. In 
this investigation, we compare a variety of 
query formulation strategies and a number of 
web resources, including two major search 
engine APIs and a large web-based n-gram 
corpus. 
1 Introduction 
Data-driven approaches to the detection and cor-
rection of non-native errors in English have been 
researched actively in the past several years. Such 
errors are particularly amenable to data-driven me-
thods because many prominent learner writing er-
rors involve a relatively small class of phenomena 
that can be targeted with specific models, in par-
ticular article and preposition errors. Preposition 
and determiner errors (most of which are article 
errors) are the second and third most frequent er-
rors in the Cambridge Learner Corpus (after the 
more intractable problem of content word choice). 
By targeting the ten most frequent prepositions 
involved in learner errors, more than 80% of pre-
position errors in the corpus are covered.  
Typically, data-driven approaches to learner er-
rors use a classifier trained on contextual informa-
tion such as tokens and part-of-speech tags within 
a window of the preposition/article (Gamon et al 
2008, 2010, DeFelice and Pulman 2007, 2008, Han 
et al 2006, Chodorow et al 2007, Tetreault and 
Chodorow 2008).  
Language models are another source of evidence 
that can be used in error detection. Using language 
models for this purpose is not a new approach, it 
goes back to at least Atwell (1987). Gamon et al 
(2008) and Gamon (2010) use a combination of 
classification and language modeling. Once lan-
guage modeling comes into play, the quantity of 
the training data comes to the forefront. It has been 
well-established that statistical models improve as 
the size of the training data increases (Banko and 
Brill 2001a, 2001b). This is particularly true for 
language models: other statistical models such as a 
classifier, for example, can be targeted towards a 
specific decision/classification, reducing the appe-
tite for data somewhat, while language models 
provide probabilities for any sequence of words - a 
task that requires immense training data resources 
if the language model is to consider increasingly 
sparse longer n-grams.  
Language models trained on data sources like 
the Gigaword corpus have become commonplace, 
but of course there is one corpus that dwarfs any 
other resource in size: the World Wide Web. This 
has drawn the interest of many researchers in natu-
ral language processing over the past decade. To 
mention just a few examples, Zhu and Rosenfeld 
(2001) combine trigram counts from the web with 
an existing language model where the estimates of 
the existing model are unreliable because of data 
sparseness. Keller and Lapata (2003) advocate the 
use of the web as a corpus to retrieve backoff 
probabilities for unseen bigrams. Lapata and Keller 
(2005) extend this method to a range of additional 
natural language processing tasks, but also caution 
that web counts have limitations and add noise. 
Kilgariff (2007) points out the shortcomings of 
37
accessing the web as a corpus through search que-
ries: (a) there is no lemmatization or part-of-speech 
tagging in search indices, so a linguistically mea-
ningful query can only be approximated, (b) search 
syntax, as implemented by search engine provid-
ers, is limited, (c) there is often a limit on the num-
ber of automatic queries that are allowed by search 
engines, (c) hit count estimates are estimates of 
retrieved pages, not of retrieved words. We would 
like to add to that list that hit count estimates on 
the web are just that -- estimates. They are com-
puted on the fly by proprietary algorithms, and ap-
parently the algorithms also access different slices 
of the web index, which causes a fluctuation over 
time, as Tetrault and Chodorow (2009) point out. 
In 2006, Google made its web-based 5gram lan-
guage model available through the Linguistic Data 
Consortium, which opens the possibility of using 
real n-gram statistics derived from the web direct-
ly, instead of using web search as a proxy. 
In this paper we explore the use of the web as a 
corpus for a very specific task: distinguishing be-
tween a learner error and its correction. This is ob-
viously not the same as the more ambitious 
question of whether a system can be built to detect 
and correct errors on the basis of web counts alone, 
and this is a distinction worth clarifying. Any sys-
tem that successfully detects and corrects an error 
will need to accomplish three tasks1: (1) find a part 
of the user input that contains an error (error de-
tection). (2) find one or multiple alternative 
string(s) for the alleged error (candidate genera-
tion) and (3) score the alternatives and the original 
to determine which alternative (if any) is a likely 
correction (error correction). Here, we are only 
concerned with the third task, specifically the 
comparison between the incorrect and the correct 
choice. This is an easily measured task, and is also 
a minimum requirement for any language model or 
language model approximation: if the model can-
not distinguish an error from a well-formed string, 
it will not be useful. 
                                                          
1 Note that these tasks need not be addressed by separate com-
ponents. A contextual classifier for preposition choice, for 
example, can generate a probability distribution over a set of 
prepositions (candidate generation). If the original preposition 
choice has lower probability than one or more other preposi-
tions, it is a potential error (error detection), and the preposi-
tions with higher probability will be potential corrections 
(error correction). 
We focus on two prominent learner errors in this 
study: preposition inclusion and choice and article 
inclusion and choice. These errors are among the 
most frequent learner errors (they comprise nearly 
one third of all errors in the learner corpus used in 
this study). 
In this study, we compare three web data 
sources: The public Bing API, Google API, and the 
Google 5-gram language model. We also pay close 
attention to strategies of query formulation. The 
questions we address are summarized as follows: 
Can web data be used to distinguish learner er-
rors from correct phrases? 
What is the better resource for web-data: the 
Bing API, the Google API, or the Google 5-
gram data? 
What is the best query formulation strategy 
when using web search results for this task? 
How much context should be included in the 
query? 
2 Related Work
Hermet et al (2008) use web search hit counts for 
preposition error detection and correction in 
French. They use a set of confusable prepositions 
to create a candidate set of alternative prepositional 
choices and generate queries for each of the candi-
dates and the original. The queries are produced 
using linguistic analysis to identify both a govern-
ing and a governed element as a minimum mea-
ningful context. On a small test set of 133 
sentences, they report accuracy of 69.9% using the 
Yahoo! search engine. 
Yi et al (2008) target article use and collocation 
errors with a similar approach. Their system first 
analyzes the input sentence using part-of-speech 
tagging and a chunk parser. Based on this analysis, 
potential error locations for determiners and verb-
noun collocation errors are identified. Query gen-
eration is performed at three levels of granularity: 
the sentence (or clause) level, chunk level and 
word level. Queries, in this approach, are not exact 
string searches but rather a set of strings combined 
with the chunk containing the potential error 
through a boolean operator. An example for a 
chunk level query for the sentence "I am learning 
economics at university" would be "[economics] 
AND [at university] AND [learning]". For article 
38
errors the hit count estimates (normalized for query 
length) are used directly. If the ratio of the norma-
lized hit count estimate for the alternative article 
choice to the normalized hit count estimate of the 
original choice exceeds a manually determined 
threshold, the alternative is suggested as a correc-
tion. For verb-noun collocations, the situation is 
more complex since the system does not automati-
cally generate possible alternative choices for 
noun/verb collocations. Instead, the snippets (doc-
ument summaries) that are returned by the initial 
web search are analyzed and potential alternative 
collocation candidates are identified. They then 
submit a second round of queries to determine 
whether the suggestions are more frequent than the 
original collocation. Results on a 400+ sentence 
corpus of learner writing show 62% precision and 
41% recall for determiners, and 30.7% recall and 
37.3% precision for verb-noun collocation errors. 
Tetreault and Chodorow (2009) make use of the 
web in a different way. Instead of using global web 
count estimates, they issue queries with a region-
specific restriction and compare statistics across 
regions. The idea behind this approach is that re-
gions that have a higher density of non-native 
speakers will show significantly higher frequency 
of erroneous productions than regions with a high-
er proportion of native speakers. For example, the 
verb-preposition combinations married to versus 
married with show very different counts in the UK 
versus France regions. The ratio of counts for mar-
ried to/married with in the UK is 3.28, whereas it 
is 1.18 in France. This indicates that there is signif-
icant over-use of married with among native 
French speakers, which serves as evidence that this 
verb-preposition combination is likely to be an er-
ror predominant for French learners of English. 
They test their approach on a list of known verb-
preposition errors. They also argue that, in a state-
of-the-art preposition error detection system, recall 
on the verb-preposition errors under investigation 
is still so low that systems can only benefit from 
increased sensitivity to the error patterns that are 
discoverable through the region web estimates. 
Bergsma et al(2009) are the closest to our work. 
They use the Google N-gram corpus to disambi-
guate usage of 34 prepositions in the New York 
Times portion of the Gigaword corpus. They use a 
sliding window of n-grams (n ranging from 2 to 5) 
across the preposition and collect counts for all 
resulting n-grams. They use two different methods 
to combine these counts. Their SuperLM model 
combines the counts as features in a linear SVM 
classifier, trained on a subset of the data. Their 
SumLM model is simpler, it sums all log counts 
across the n-grams. The preposition with the high-
est score is then predicted for the given context. 
Accuracy on the New York Times data in these ex-
periments reaches 75.4% for SuperLM and 73.7% 
for SumLM. 
Our approach differs from Bergsma et al in 
three crucial respects. First, we evaluate insertion, 
deletion, and substitution operations, not just subs-
titution, and we extend our evaluation to article 
errors. Second, we focus on finding the best query 
mechanism for each of these operations, which 
requires only a single query to the Web source. 
Finally, the focus of our work is on learner error 
detection, so we evaluate on real learner data as 
opposed to well-formed news text. This distinction 
is important: in our context, evaluation on edited 
text artificially inflates both precision and recall 
because the context surrounding the potential error 
site is error-free whereas learner writing can be, 
and often is, surrounded by errors. In addition, 
New York Times writing is highly idiomatic while 
learner productions often include unidiomatic word 
choices, even though the choice may not be consi-
dered an error. 
3 Experimental Setup 
3.1 Test Data 
Our test data is extracted from the Cambridge Uni-
versity Press Learners? Corpus (CLC). Our ver-
sion of CLC currently contains 20 million words 
from non-native English essays written as part of 
one of Cambridge?s English language proficiency 
tests (ESOL) ? at all proficiency levels. The essays 
are annotated for error type, erroneous span and 
suggested correction. We perform a number of 
preprocessing steps on the data. First, we correct 
all errors that were flagged as being spelling errors. 
Spelling errors that were flagged as morphology 
errors were left alone. We also changed confusable 
words that are covered by MS Word. In addition, 
we changed British English spelling to American 
English. We then eliminate all annotations for non-
pertinent errors (i.e. non-preposition/article errors, 
or errors that do not involve any of the targeted 
prepositions), but we retain the original (errone-
39
ous) text for these. This makes our task harder 
since we will have to make predictions in text con-
taining multiple errors, but it is more realistic giv-
en real learner writing. Finally, we eliminate 
sentences containing nested errors (where the an-
notation of one error contains an annotation for 
another error) and multiple article/preposition er-
rors. Sentences that were flagged for a replacement 
error but contained no replacement were also elim-
inated from the data. The final set we use consists 
of a random selection of 9,006 sentences from the 
CLC with article errors and 9,235 sentences with 
preposition errors. 
3.2 Search APIs and Corpora 
We examine three different sources of data to dis-
tinguish learner errors from corrected errors. First, 
we use two web search engine APIs, Bing and 
Google. Both APIs allow the retrieval of a page-
count estimate for an exact match query. Since 
these estimates are provided based on proprietary 
algorithms, we have to treat them as a "black box". 
The third source of data is the Google 5-gram cor-
pus (Linguistic Data Consortium 2006) which con-
tains n-grams with n ranging from 1 to 5. The 
count cutoff for unigrams is 200, for higher order 
n-grams it is 40. 
3.3 Query Formulation 
There are many possible ways to formulate an ex-
act match (i.e. quoted) query for an error and its 
correction, depending on the amount of context 
that is included on the right and left side of the er-
ror. Including too little context runs the risk of 
missing the linguistically relevant information for 
determining the proper choice of preposition or 
determiner. Consider, for example, the sentence we
rely most of/on friends. If we only include one 
word to the left and one word to the right of the 
preposition, we end up with the queries "most on 
friends" and "most of friends" - and the web hit 
count estimate may tell us that the latter is more 
frequent than the former. However, in this exam-
ple, the verb rely determines the choice of preposi-
tion and when it is included in the query as in "rely 
most on friends" versus "rely most of friends", the 
estimated hit counts might correctly reflect the in-
correct versus correct choice of preposition. Ex-
tending the query to cover too much of the context, 
on the other hand, can lead to low or zero web hit 
estimates because of data sparseness - if we in-
clude the pronoun we in the query as in "we rely 
most on friends" versus "we rely most of friends", 
we get zero web count estimates for both queries.  
Another issue in query formulation is what 
strategy to use for corrections that involve dele-
tions and insertions, where the number of tokens 
changes. If, for example, we use queries of length 
3, the question for deletion queries is whether we 
use two words to the left and one to the right of the 
deleted word, or one word to the left and two to the 
right. In other words, in the sentence we traveled 
to/0 abroad last year, should the query for the cor-
rection (deletion) be "we traveled abroad" or "tra-
veled abroad last"? 
Finally, we can employ some linguistic informa-
tion to design our query. By using part-of-speech 
tag information, we can develop heuristics to in-
clude a governing content word to the left and the 
head of the noun phrase to the right. 
The complete list of query strategies that we 
tested is given below. 
SmartQuery: using part-of-speech information 
to include the first content word to the left and the 
head noun to the right. If the content word on the 
left cannot be established within a window of 2 
tokens and the noun phrase edge within 5 tokens, 
select a fixed window of 2 tokens to the left and 2 
tokens to the right. 
FixedWindow Queries: include n tokens to the 
left and m tokens to the right. We experimented 
with the following settings for n and m: 1_1, 2_1, 
1_2, 2_2, 3_2, 2_3. The latter two 6-grams were 
only used for the API?s, because the Google corpus 
does not contain 6-grams. 
FixedLength Queries: queries where the length 
in tokens is identical for the error and the correc-
tion. For substitution errors, these are the same as 
the corresponding FixedWindow queries, but for 
substitutions and deletions we either favor the left 
or right context to include one additional token to 
make up for the deleted/inserted token. We expe-
rimented with trigrams, 4-grams, 5-grams and 6-
grams, with left and right preference for each, they 
are referred to as Left4g (4-gram with left prefe-
rence), etc. 
40
3.4 Evaluation Metrics 
For each query pair <qerror, qcorrection>, we produce 
one of three different outcomes: 
correct (the query results favor the correction of 
the learner error over the error itself):  
count(qcorrection) > count(qerror) 
incorrect (the query results favor the learner error 
over its correction):   
count(qerror) >= count(qcorrection) 
 where(count(qerror) &  0 OR 
 count(qcorrection) &  0) 
noresult:  
count(qcorrection) = count(qerror) = 0 
For each query type, each error (preposition or ar-
ticle), each correction operation (deletion, inser-
tion, substitution) and each web resource (Bing 
API, Google API, Google N-grams) we collect 
these counts and use them to calculate three differ-
ent metrics. Raw accuracy is the ratio of correct 
predictions to all query pairs: 
!"#$"%%&'"%( ) $
%*''
%*'' + ,-%*'' + -*'./&01 
We also calculate accuracy for the subset of query 
pairs where at least one of the queries resulted in a 
successful hit, i.e. a non-zero result. We call this 
metric Non-Zero-Result-Accurracy (NZRA), it is 
the ratio of correct predictions to incorrect predic-
tions, ignoring noresults: 
2*-3.'*!./&014%%&'"%( ) $
%*''
%*'' + ,-%*'' 
Finally, retrieval ratio is the ratio of queries that 
returned non-zero results: 
4 Results
We show results from our experiments in Table 1 -   
Table 6. Since space does not permit a full tabula-
tion of all the individual results, we restrict our-
selves to listing only those query types that achieve 
best results (highlighted) in at least one metric. 
Google 5-grams show significantly better results 
than both the Google and Bing APIs. This is good 
news in terms of implementation, because it frees 
the system from the vagaries involved in relying on 
search engine page estimates: (1) the latency, (2) 
query quotas, and (3) fluctuations of page esti-
mates over time. The bad news is that the 5-gram 
corpus has much lower retrieval ratio because, pre-
sumably, of its frequency cutoff. Its use also limits 
the maximum length of a query to a 5-gram (al-
though neither of the APIs outperformed Google 5-
grams when retrieving 6-gram queries). 
The results for substitutions are best, for fixed 
window queries. For prepositions, the SmartQue-
ries perform with about 86% NZRA while a fixed 
length 2_2 query (targeted word with a ?2-token 
window) achieves the best results for articles, at 
about 85% (when there was at least one non-zero 
match). Retrieval ratio for the prepositions was 
about 6% lower than retrieval ratio for articles ?
41% compared to 35%.  
The best query type for insertions was fixed-
length LeftFourgrams with about 95% NZRA and 
71% retrieval ratio for articles and 89% and 78% 
retrieval ratio for prepositions. However, Left-
Fourgrams favor the suggested rewrites because, 
by keeping the query length at four tokens, the 
original has more syntactic/semantic context. If the 
original sentence contains is referred as the and the 
annotator inserted to before as, the original query 
will be is referred as the and the correction query 
is referred to as.  
Conversely, with deletion, having a fixed win-
dow favors the shorter rewrite string. The best 
query types for deletions were: 2_2 queries for ar-
ticles (94% NZRA and 46% retrieval ratio) and 
SmartQueries for prepositions (97% NZRA and 
52% retrieval ratio). For prepositions the fixed 
length 1_1 query performs about the same as the 
SmartQueries, but that query is a trigram (or 
smaller at the edges of a sentence) whereas the av-
erage length of SmartQueries is 4.7 words for pre-
positions and 4.3 words for articles. So while the 
coverage for SmartQueries is much lower, the 
longer query string cuts the risk of matching on 
false positives.  
The Google 5-gram Corpus differs from search 
engines in that it is sensitive to upper and lower 
case distinctions and to punctuation. While intui-
tively it seemed that punctuation would hurt n-
gram performance, it actually helps because the 
punctuation is an indicator of a clause boundary. A 
recent Google search for have a lunch and have 
lunch produced estimates of about 14 million web 
pages for the former and only 2 million for the lat-
ter. Upon inspecting the snippets for have a lunch, 
the next word was almost always a noun such as 
menu, break, date, hour, meeting, partner, etc. The 
relative frequencies for have a lunch would be 
much different if a clause boundary marker were 
41
required. The 5-gram corpus also has sentence 
boundary markers which is especially helpful to 
identify changes at the beginning of a sentence. 
 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
SmartQuery 0.8637 0.9548 0.9742 0.8787 0.8562 0.5206 0.7589 0.8176 0.5071
1_1 0.4099 0.9655 0.9721 0.9986 0.9978 0.9756 0.4093 0.9634 0.9484
Table 1: Preposition deletions (1395 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
Left4g 0.7459 0.8454 0.8853 0.9624 0.9520 0.7817 0.7178 0.8048 0.6920
1_1 0.5679 0.2983 0.3550 0.9973 0.9964 0.9733 0.5661 0.2971 0.3456
Right3g 0.6431 0.8197 0.8586 0.9950 0.9946 0.9452 0.6399 0.8152 0.8116
Table 2: Preposition insertions (2208 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
SmartQuery 0.7396 0.8183 0.8633 0.7987 0.7878 0.4108 0.5906 0.6446 0.5071
1_1=L3g=R3g 0.4889 0.6557 0.6638 0.9870 0.9856 0.9041 0.4826 0.6463 0.6001
1_2=R4g 0.6558 0.7651 0.8042 0.9178 0.9047 0.6383 0.6019 0.6921 0.5133
Table 3: Preposition substitutions (5632 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
2_2 0.7678 0.9056 0.9386 0.8353 0.8108 0.4644 0.6414 0.7342 0.4359
1_1 0.3850 0.8348 0.8620 0.9942 0.9924 0.9606 0.3828 0.8285 0.8281
1_2 0.5737 0.8965 0.9097 0.9556 0.9494 0.7920 0.5482 0.8512 0.7205
Table 4: Article deletions (2769 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
Left4g 0.8292 0.9083 0.9460 0.9505 0.9428 0.7072 0.7880 0.8562 0.6690
1_1 0.5791 0.3938 0.3908 0.9978 0.9975 0.9609 0.5777 0.3928 0.3755
Left3g 0.6642 0.8983 0.8924 0.9953 0.9955 0.9413 0.6611 0.8942 0.8400
Table 5: Article insertions (5520 query pairs). 
Query type 
non-zero-result accuracy retrieval ratio raw accuracy 
B-API G-API G-Ngr B-API G-API G-Ngr B-API G-API G-Ngr 
2_2=Left5g= 
Right5g 0.6970 0.7842 0.8486 0.8285 0.8145 0.4421 0.5774 0.6388 0.3752
1_1=L3g=R3g 0.4385 0.7063 0.7297 0.9986 0.9972 0.9596 0.4379 0.7043 0.7001
1_2=R4g 0.5268 0.7493 0.7917 0.9637 0.9568 0.8033 0.5077 0.7169 0.6360
Table 6: Article substitutions (717 query pairs). 
 
42
5 Error Analysis 
We manually inspected examples where the 
matches on the original string were greater than 
matches on the corrected string. The results of this 
error analysis are shown in table 7. Most of the 
time, (1) the context that determined article or pre-
position use and choice was not contained within 
the query. This includes, for articles, cases where 
article usage depends either on a previous mention 
or on the intended sense of a polysemous head 
noun. Some other patterns also emerged. Some-
times (2) both and the original and the correction 
seemed equally good in the context of the entire 
sentence, for example it?s very important to us and 
it?s very important for us.  In other cases, (3) there 
was another error in the query string (recall that we 
retained all of the errors in the original sentences 
that were not the targeted error). Then there is a 
very subjective category (4) where the relative n-
gram frequencies are unexpected, for example 
where the corpus has 171 trigrams guilty for you 
but only 137 for guilty about you. These often oc-
cur when both of the frequencies are either low 
and/or close. This category includes cases where it 
is very likely that one of the queries is retrieving an 
n-gram whose right edge is the beginning of a 
compound noun (as in with the trigram have a 
lunch). Finally, (5) some of the ?corrections? either 
introduced an error into the sentence or the original 
and ?correction? were equally bad. In this catego-
ry, we also include British English article usage 
like go to hospital. For prepositions, (6) some of 
the corrections changed the meaning of the sen-
tence ? where the disambiguation context is often 
not in the sentence itself and either choice is syn-
tactically correct, as in I will buy it from you 
changed to I will buy it for you. 
 
 Articles Preps 
 freq ratio freq ratio
1.N-gram does not con-
tain necessary context 187 .58 183 .52
2.Original and correc-
tion both good 39 .12 51 .11
3.Other error in n-gram 30 .9 35 .10
4.Unexpected ratio 36 .11 27 .09
5.Correction is wrong 30 .9 30 .08
6.Meaning changing na na 24 .07
Table 7: Error analysis 
 
If we count categories 2 and 5 in Table 7 as not 
being errors, then the error rate for articles drops 
20% and the error rate for prepositions drops 19%. 
A disproportionately high subcategory of query 
strings that did not contain the disambiguating con-
text (category 1) was at the edges of the sentence ? 
especially for the LeftFourgrams at the beginning 
of a sentence where the query will always be a bi-
gram. 
6 Conclusion and Future Work 
We have demonstrated that web source counts can 
be an accurate predictor for distinguishing between 
a learner error and its correction - as long as the 
query strategy is tuned towards the error type. 
Longer queries, i.e. 4-grams and 5-grams achieve 
the best non-zero-result accuracy for articles, while 
SmartQueries perform best for preposition errors. 
Google N-grams across the board achieve the best 
non-zero-result accuracy, but not surprisingly they 
have the lowest retrieval ratio due to count cutoffs. 
Between the two search APIs, Bing tends to have 
better retrieval ratio, while Google achieves higher 
accuracy. 
In terms of practical use in an error detection 
system, a general "recipe" for a high precision 
component can be summarized as follows. First, 
use the Google Web 5-gram Corpus as a web 
source. It achieves the highest NZRA, and it avoids 
multiple problems with search APIs: results do not 
fluctuate over time, results are real n-gram counts 
as opposed to document count estimates, and a lo-
cal implementation can avoid the high latency as-
sociated with search APIs. Secondly, carefully 
select the query strategy depending on the correc-
tion operation and error type. 
We hope that this empirical investigation can 
contribute to a more solid foundation for future 
work in error detection and correction involving 
the web as a source for data. While it is certainly 
not sufficient to use only web data for this purpose, 
we believe that the accuracy numbers reported here 
indicate that web data can provide a strong addi-
tional signal in a system that combines different 
detection and correction mechanisms. One can im-
agine, for example, multiple ways to combine the 
n-gram data with an existing language model. Al-
ternatively, one could follow Bergsma et al (2009) 
and issue not just a single pair of queries but a 
43
whole series of queries and sum over the results. 
This would increase recall since at least some of 
the shorter queries are likely to return non-zero 
results. In a real-time system, however, issuing 
several dozen queries per potential error location 
and potential correction could cause performance 
issues. Finally, the n-gram counts can be incorpo-
rated as one of the features into a system such as 
the one described in Gamon (2010) that combines 
evidence from various sources in a principled way 
to optimize accuracy on learner errors. 
Acknowledgments 
We would like to thank Yizheng Cai for making 
the Google web ngram counts available through a 
web service and to the anonymous reviewers for 
their feedback. 
References  
Eric Steven Atwell. 1987. How to detect grammatical 
errors in a text without parsing it. Proceedings of the 
3rd EACL, Copenhagen, Denmark, pp 38 - 45. 
Michele Banko and Eric Brill. 2001a. Mitigating the 
paucity-of-data problem: Exploring the effect of 
training corpus size on classifier performance for 
natural language processing. In James Allan, editor, 
Proceedings of the First International Conference on 
Human Language Technology Research. Morgan 
Kaufmann, San Francisco. 
Michele Banko and Eric Brill. 2001b. Scaling to very 
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting of 
the Association for Computational Linguistics and 
the 10th Conference of the European Chapter of the 
Association for Computational Linguistics, pp. 26?
33, Toulouse, France. 
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009. 
Web-scale n-gram models for lexical disambiguation. 
In Proceedings for the 21st International Joint Confe-
rence on Artificial Intelligence, pp. 1507 ? 1512. 
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions, pp. 25-30.  
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In 
Proceedings of the Fourth ACL-SIGSEM Workshop 
on Prepositions, pp. 45-50. Prague. 
Rachele De Felice and Stephen Pulman.  2008.  A clas-
sifier-based approach to preposition and determiner 
error correction in L2 English.  COLING. Manches-
ter, UK. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, 
and Lucy Vanderwende. 2008. Using contextual 
speller techniques and language modeling for ESL 
error correction. In Proceedings of IJCNLP, Hydera-
bad, India.  
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing. In Proceedings of 
NAACL. 
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 
2006. Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Matthieu Hermet, Alain D?silets, Stan Szpakowicz. 
2008. Using the web as a linguistic resource to auto-
matically correct lexico-syntactic errors. In Proceed-
ings of the 6th Conference on Language Resources 
and Evaluation (LREC), pp. 874-878. 
Frank Keller and Mirella Lapata. 2003. Using the web 
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3): 459-484. 
Adam Kilgariff. 2007. Googleology is bad science. 
Computational Linguistics 33(1): 147-151. 
Mirella Lapata and Frank Keller. 2005. Web-Based 
Models for Natural Language Processing. ACM 
Transactions on Speech and Language Processing 
(TSLP), 2(1):1-31. 
Linguistic Data Consortium. 2006. Web 1T 5-gram 
version 1. 
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13 . 
Joel Tetreault and Martin Chodorow. 2008. The ups and 
downs of preposition error detection in ESL. 
COLING. Manchester, UK. 
Joel Tetreault and Martin Chodorow. 2009. Examining 
the use of region web counts for ESL error detection. 
Web as Corpus Workshop (WAC-5), San Sebastian, 
Spain. 
Xing Yi, Jianfeng Gao and Bill Dolan.  2008.  A web-
based English proofing system for English as a 
second language users.  In Proceedings of the Third 
International Joint Conference on Natural Language 
Processing (IJCNLP). Hyderabad, India. 
Zhu, X. and Rosenfeld, R. 2001. Improving trigram 
language modeling with the world wide web. In Pro-
ceedings of International Conference on Acoustics 
Speech and Signal Processing. Salt Lake City. 
44
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 180?189,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
High-Order Sequence Modeling for Language Learner Error Detection 
Michael Gamon 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
mgamon@microsoft.com 
  
 
Abstract 
We address the problem of detecting Eng-
lish language learner errors by using a dis-
criminative high-order sequence model. 
Unlike most work in error-detection, this 
method is agnostic as to specific error 
types, thus potentially allowing for higher 
recall across different error types.  The ap-
proach integrates features from many 
sources into the error-detection model, 
ranging from language model-based fea-
tures to linguistic analysis features. Evalua-
tion results on a large annotated corpus of 
learner writing indicate the feasibility of 
our approach on a realistic, noisy and in-
herently skewed set of data. High-order 
models consistently outperform low-order 
models in our experiments. Error analysis 
on the output shows that the calculation of 
precision on the test set represents a lower 
bound on the real system performance. 
1. Introduction 
Systems for automatic detection and correction of 
errors in native writing have been developed for 
many decades. Early in the development of these 
systems, the approach was exclusively based on 
knowledge engineering. Hand-crafted grammars 
would analyze a sentence and would contain spe-
cial mechanisms for rule or constraint relaxation 
that allow ungrammatical sentences to produce a 
parse, while at the same time indicating that a 
grammatical error is present. More recently, data-
driven methods have assumed prominence and 
there has been an emerging area of research into 
the challenge of detecting and correcting errors in 
learner language (for an overview see Leacock et 
al. 2010). Data-driven methods offer the familiar 
set of advantages: they can be more flexible than a 
manually maintained set of rules and they tend to 
cope better with noisy input. Drawbacks include 
the inability to handle linguistically more complex 
errors that involve long distance dependencies such 
as subject-verb agreement. Learner errors as a tar-
get for error detection and correction pose a partic-
ular challenge but also offer some unique 
opportunities. The challenge lies in the density of 
errors (much higher than in native writing), the 
variety of errors (a superset of typical native er-
rors) and the generally more non-idiomatic writing. 
On the other hand, the availability of annotated 
corpora, often comprised of manually corrected 
learner essays or scripts, provides a big advantage 
for the evaluation and training of data-driven sys-
tems.  
Data-driven systems for English learner error 
detection and correction typically target a specific 
set of error types and contain a machine learned 
component for each error type. For example, such 
a system may have a classifier that determines the 
correct choice of preposition given the lexical and 
syntactic part-of-speech (POS) context and hence 
can aid the learner with the notoriously difficult 
problem of identifying an appropriate preposition. 
Similarly, a classifier can be used to predict the 
correct choice of article in a given context. Such 
targeted systems have the advantage that they often 
achieve relatively high precision at, of course, the 
cost of recall. However, while there are a few ma-
jor learner error categories, such as prepositions 
and articles, there is also a long tail of content 
word and other errors that is not amenable to a tar-
geted approach. 
In this paper, we depart from the error-specific 
paradigm and explore a sequence modeling ap-
proach to general error detection in learner writing. 
This approach is completely agnostic as to the er-
ror type. It attempts to predict the location of an 
180
error in a sentence based on observations gathered 
from a supervised training phase on an error-
annotated learner corpus. Features used here are 
based on an n-gram language model, POS tags, 
simple string features that indicate token length 
and capitalization, and linguistic analysis by a con-
stituency parser. We train and evaluate the method 
on a sizeable subset of the corpus. We show the 
contribution of the different feature types and per-
form a manual error analysis to pinpoint shortcom-
ings of the system and to get a more accurate idea 
of the system?s precision. 
2. Related work 
Error-specific approaches comprise the majority of 
recent work in learner error detection. Two of the 
most studied error types in learner English are 
preposition and article errors since they make up a 
large percentage of errors in learner writing (16% 
and 13% respectively in the Cambridge Learner 
Corpus, without considering spelling and punctua-
tion errors). The most widely used approach for 
detecting and correcting these errors is classifica-
tion, with lexical and POS features gleaned from a 
window around the potential preposition/article 
site in a sentence. Some recent work includes Cho-
dorow et al (2007), De Felice and Pulman (2008), 
Gamon (2010), Han et al (2010), Izumi et al 
(2004), Tetreault and Chodorow (2008), Ro-
zovskaya and Roth (2010a, 2010b). Gamon et al 
(2008) and Gamon (2010) used a language model 
in addition to a classifier and combined the classi-
fier output and language model scores in a meta-
classifier. These error-specific methods achieve 
high precision (up to 80-90% on some corpora) but 
only capture highly constrained error types such as 
preposition and determiner errors. 
There has also been research on error-detection 
methods that are not designed to identify a specific 
error type. The basic idea behind these error-
agnostic approaches is to identify an error where 
there is a particularly unlikely sequence compared 
to the patterns found in a large well-formed corpus. 
Atwell (1986) used low-likelihood sequences of 
POS tags as indicators for the presence of an error. 
Sj?bergh (2005) used a chunker to detect unlikely 
chunks in native Swedish writing compared to the 
chunks derived from a large corpus of well-formed 
Swedish writing. Bigert and Knutsson (2002) em-
ployed a statistical method to identify a variety of 
errors in Swedish writing as rare sequences of 
morpho-syntactic tags. They significantly reduced 
false positives by using additional methods to de-
termine whether the unexpected sequence is due to 
phrase or sentence boundaries or due to rare single 
tags. Chodorow and Leacock (2000) utilized mutu-
al information and chi-square statistics to identify 
typical contexts for a small set of targeted words 
from a large well-formed corpus. Comparing these 
statistics to the ones found in a novel sentence, 
they could identify unlikely contexts for the target-
ed words that were often good indicators of the 
presence of an error. Sun et al (2007) mined for 
patterns that consist of POS tags and function 
words. The patterns are of variable length and can 
also contain gaps. Patterns were then combined in 
a classifier to distinguish correct from erroneous 
sentences. Wagner et al (2007) combined parse 
probabilities from a set of statistical parsers and 
POS tag n-gram probabilities in a classifier to de-
tect ungrammatical sentences. Okanohara and Tsu-
jii (2007) differed from the previous approaches in 
that they directly used discriminative language 
models to distinguish correct from incorrect sen-
tences, without the direct modeling of error-
indicating patterns. Park and Levy (2011) use a 
noisy channel model with a base language model 
and a set of error-specific noise models for error 
detection and correction. 
In contrast to previous work, we cast the task as 
a sequence modeling problem. This provides a 
flexible framework in which multiple statistical 
and linguistic signals can be combined and cali-
brated by supervised learning. The approach is er-
ror-agnostic and can easily be extended with 
additional statistical or linguistic features. 
3. Error detection by sequence modeling 
Errors consist of a sub-sequence of tokens in a 
longer token sequence. They can be identified by a 
combination of internal and contextual features, 
the latter requiring a notion of Markov window (a 
window around a token in which relevant infor-
mation is likely to be found). This is similar to 
tasks such as named entity recognition (NER) or 
part-of-speech tagging, where sequence modeling 
has proven to be successful.  
We choose a Maximum Entropy Markov Model 
(MEMM, McCallum et al 2000) as the modeling 
technique. In NER, the annotation convention uses 
181
three labels for a token ?O? (outside of NE), ?B? 
(beginning of NE), and ?I? (inside of NE). For our 
purpose we reduced the set of labels to just ?O? 
and ?I? since most of the errors are relatively short. 
Conditional Random Fields (Lafferty et al 
2001) are considered to be superior to MEMMs in 
learning problems affected by label bias (Bottou 
1991). In our scheme, however, there are only two 
states ?O? and ?I?, and both states can transition to 
each other. Since there are no states with asymmet-
ric transition properties that would introduce a bias 
towards states with fewer transitions, label bias is 
not a problem for us. 
Figure 1 shows the structure of our MEMM with 
a Markov order of five (the diagram only shows 
the complete set of arcs for the last state). The in-
put sentence contains the token sequence the past 
year I was stayed ? with the error was stayed. In-
stead of using the tokens themselves as observa-
tions, we chose to use POS tags assigned by an 
automatic tagger (Toutanova et al 2003). This 
choice was motivated by data sparseness. Learning 
a model that observes individual lexical items and 
predicts a sequence of error/non-error tags would 
be ideal, but given the many different error types 
and triggering contexts for an error, such a model 
would require much more training data. A large set 
of features that serve as constraints on the state 
transition models are extracted for each state. The-
se features are described in Section 5.  
Note that the model structure would lend itself 
to a factorial conditional random field (McCallum 
et al 2003) which allows the joint labeling of POS 
tags and state labels. This would, however, require 
training data that is labeled for both errors and 
POS tags. 
 
Figure 1: MEMM model for error detection, the 
full set of dependencies is only shown for the last 
state. 
4. Detecting errors in the Cambridge 
Learner Corpus  
The learner corpus used to train and evaluate the 
system is the Cambridge Learner Corpus (CLC). It 
consists of essays (scripts) written as part of the 
University of Cambridge English for Speakers of 
Other Languages (ESOL) examinations. The cor-
pus contains about 30 million words of learner 
English. All errors are annotated and include, when 
possible, a single suggested correction. Errors are 
categorized into 87 error types. 
We performed a number of preprocessing steps 
on the data. On the assumption that learners have 
access to a spell checker, errors that were marked 
as spelling errors were corrected based on the an-
notations. Confused words (their/there) were treat-
ed in the same way, given that they are corrected 
by a modern proofing tool such as the one in Mi-
crosoft Word. In addition, British English spelling 
conventions were changed to those of American 
English. Sentences containing errors that had no 
suggested rewrite were eliminated. Finally, only 
lexical errors are covered in this work. For punctu-
ation and capitalization we removed the error an-
notations, retaining the original (erroneous) 
punctuation and capitalization. 
We grouped the remaining 60 error classifica-
tions into eight categories: Content word, Inflec-
tional morphology, Noun phrase errors, 
Preposition errors, Multiple errors, Other errors 
involving content words, Other errors involving 
function words and Derivational morphology. The 
distribution of error categories is shown in Table 1. 
Error Class Freq Pct 
Content word insertion, dele-
tion or choice 
185,201 21% 
Inflectional morphology and 
agreement of content words 
157,660 18% 
Noun phrase formation: De-
terminers and quantifiers 
130,829 15% 
Preposition error 124,902 14% 
Multiple: Adjacent and nested 
annotations 
113,615 13% 
Other content word errors 79,596 9% 
Other function word errors: 
anaphors and conjunctions 
65,034 7% 
Derivational morphology of 
content words 
39,213 4% 
Table 1: Error types in the CLC. 
182
The multiple error class includes any combination 
of error types where the error annotations are either 
nested or adjacent. The other categories are more 
focused: the errors are of a particular class and 
their adjacent context is correct, although there 
may be another error annotation a single token 
away. Content word errors involve the insertion, 
deletion and substitution of nouns, verbs, adjec-
tives and adverbs. Further analysis of this error 
category on a random sample of 200 instances re-
veals that the majority (72%) of content word er-
rors involve substitutions, while deletions account 
for 10% of the errors and insertions for 18%. Most 
substitutions (63%) involve the wrong choice of a 
word that is somewhat semantically related to the 
correct choice. Inflectional morphology includes 
all inflection errors for content words as well as 
subject-verb agreement errors. The inflectional 
errors include many cases of what might be con-
sidered spelling errors, for example *dieing/dying. 
Similarly, the derivational morphology errors in-
clude all derivational errors for content words ? 
and also include many errors that may be consid-
ered as spelling errors. Noun formation errors in-
clude all annotations involving determiners and 
quantifiers: inflection, derivation, countability, 
word form and noun-phrase-internal agreement. 
Preposition errors include all annotations that in-
volve prepositions: insertion, deletion, substitution 
and a non-preposition being used in place of a 
preposition. There are two other categories: those 
involving the remaining function words (anaphors 
and conjunctions) and those involving remaining 
content words (collocation, idiom, negative for-
mation, argument structure, word order, etc.). 
It is important to highlight the challenges inher-
ent in this data set. First of all, the problem is high-
ly skewed since only 7.3% of tokens in the test set 
are involved in an error. Second, since we included 
correct learner sentences in the development and 
test sets in the proportion they occur in the overall 
corpus, only 47% of sentences in the test set con-
tain error annotations, greatly increasing the likeli-
hood of false positives. 
5. Features 
5.1 Language model features 
The language model (LM) features comprise a 
total of 29 features. Each of these features is calcu-
lated from n-gram probabilities observed at and 
around the current token. All LM features are 
based on scores from a 7-gram language model 
with absolute discount smoothing built from the 
Gigaword corpus (Gao et al 2001, Nguyen et al 
2007). 
We group the language model features concep-
tually into five categories: basic features, ratio fea-
tures, drop features, entropy delta features and 
miscellaneous. All probabilities are log probabili-
ties, and n in the n-grams ranges from 1 to 5. All 
features are calculated for each token w of the to-
kens w0?wi in a sentence. 
Basic LM features consist of two features: the 
unigram probability of w and the average n-gram 
probability of all n-grams in the sentence that con-
tain w. 
Ratio features are based on the intuition that er-
rors can be characterized as involving tokens that 
have a very low ratio of higher order n-gram prob-
abilities to lower order n-gram probabilities. In 
other words, these are tokens that are part of an 
unlikely combination of otherwise likely smaller n-
grams. These features are calculated as the ratio of 
the average x-gram probability of all x-grams con-
taining w to the average y-gram probability of all 
y-grams containing w. The values for x and y are: 5 
and 1, 4 and 1, 3 and 1, 2 and 1, 5 and 4, 4 and 3, 3 
and 2. 
Drop features measure either the drop or in-
crease in n-gram probability across token w. For 
example, the bigram drop at wi is the delta between 
the bigram probability of the bigram starting at i-1 
to the bigram probability of the bigram starting at i. 
Drop features are calculated for n-grams with 2 ? n 
? 5. 
Entropy delta features offer another way to look 
at the changes of n-gram probability across a token 
w. Forward entropy for wi is defined as the entropy 
of the string wi?wn where n is the index of the last 
token in the sentence. We calculate the entropy of 
an n-gram as the language model probability of 
string wi?wn divided by the number of tokens in 
that string. Backward entropy is calculated analo-
gously for w0?wi. For n-grams with 1 ? n ? 5, we 
also calculate, at each index i into the token array, 
the delta between the n-gram entropy of the n-gram 
starting at i and the n-gram starting at i+1 (forward 
sliding entropy). Similarly the delta between the n-
gram entropy of the n-gram starting at i and the n-
gram starting at i-1 (backward sliding entropy) is 
calculated.  
183
There are four miscellaneous language model 
features. Three of them, minimum ratio to random, 
average ratio to random, and overall ratio to ran-
dom address the fact that a ?good? n-gram is likely 
to have a much higher probability than an n-gram 
with the same tokens in random order. For all n-
grams where 2 ? n ? 5 we calculate the ratio be-
tween the n-gram probability and the sum of the 
unigram probabilities. For a token wi we produce 
the minimum ratio to random (the minimum ratio 
of all n-grams including w) and the average ratio 
to random (the average of all ratios of the n-grams 
including w). Overall ratio to random is obtained 
by looping through each n-gram where 2 ? n ? 5 
that includes wi and summing the n-gram proba-
bilities (sum1) as well as the unigram probabilities 
of all unigrams in these n-grams (sum2). The ratio 
feature is then sum1/sum2. The final feature ad-
dresses the intuition that an erroneous word may 
cause n-grams that contain the word to be less like-
ly than adjacent but non-overlapping n-grams. 
Overlap to adjacent ratio is the sum of probabili-
ties of n-grams including wi, divided by the sum of 
probabilities of n-grams that are adjacent to wi but 
do not include it.  
Note that this use of a host of language model 
features is substantially different from using a sin-
gle language model score on hypothesized error 
and potential correction to filter out unlikely cor-
rection candidates as in Gamon et al (2008) and 
Gamon (2010).  
5.2 String features 
String features capture information about the char-
acters in a token and the tokens in a sentence. Two 
binary features indicate whether a token is capital-
ized (initial capitalization or all capitalized), one 
feature indicates the token length in characters and 
one feature measures the number of tokens in the 
sentence. 
5.3 Linguistic Analysis features 
Each sentence is linguistically analyzed by a 
PCFG-LA parser (Petrov et al, 2006) trained on 
the Penn Treebank (Marcus et al, 1993). A num-
ber of features are extracted from the constituency 
tree to assess the syntactic complexity of the whole 
sentence, the syntactic complexity of the local en-
vironment of a token, and simple constituency in-
formation for each token. These features are: label 
of the parent and grandparent node, number of sib-
ling nodes, number of siblings of the parent, pres-
ence of a governing head node, label of the 
governing head node, and length of path to the 
root. An additional feature indicates whether the 
POS tag assigned by the parser does not match the 
tag assigned by the POS tagger, which may indi-
cate a tagging error. 
6. Experiments 
6.1 Design 
For our experiments we use three different mutual-
ly exclusive random subsets of CLC. 50K sentenc-
es are used for training of the models (larger data 
sets exceeded the capabilities of our MEMM train-
er). In this set, we only include sentences that con-
tain at least one annotated error. We also 
experimented using a mix of error-free and errone-
ous sentences, but the resulting models turned out 
to be extremely skewed towards always predicting 
the majority state ?O? (no error). 20K sentences 
(including both erroneous and correct sentences) 
are used for parameter tuning and testing, respec-
tively. 
Each token in the data is annotated with one of 
the states ?O? or ?I?. Performance is measured on 
a per token basis, i.e. each mismatch between the 
predicted state and the annotated state is counted as 
an error, each match is counted as a correct predic-
tion. 
We use the development set to tune two parame-
ters: the size of the Markov window and a prior to 
prevent overfitting. The latter is a Gaussian prior 
(or quadratic regularizer) where the mean is fixed 
to zero and the variance is left as a free parameter. 
We perform a grid search to find values for the 
parameters that optimize the model?s F1 score on 
the development data. 
In order to be able to report precision and recall 
curves, we use a technique similar to the one de-
scribed in Minkov et al (2010): we introduce an 
artificial feature with a constant value at training 
time. At test time we perform multiple runs, modi-
fying the weight on the artificial feature. This 
weight variation influences the model?s prior pro-
pensity to assign each of the two states, allowing 
us to measure a precision/recall tradeoff.  
184
6.2 Performance of feature sets 
Figure 2 illustrates the performance of three differ-
ent feature sets and combinations. The baseline is 
using only language model features and standard 
POS tags, which tops out at about 20% precision. 
Adding the string features discussed in the previ-
ous section, and partially lexicalized (PL) POS 
tags, where we used POS tags for content word 
tokens and the lexicalized token for function 
words, we get a small but consistent improvement. 
We obtain the best performance when all features 
are used, including the linguistic analysis features 
(DepParse). We found that a high-order model 
with a Markov window size of 14 performed best 
for all experiments with a top F1 score. F1 at low-
er orders was significantly worse. Training time for 
the best models was less than one hour. 
6.3 Predicting error types 
In our next experiment, we tried to determine how 
the sequence modeling approach performs for in-
dividual error types. Here we trained eight differ-
ent models, one for each of the error types in Table 
1. As in the previous experiments, the development 
and test files contained error-free sentences. The 
optimal Markov window size ranged from 8 to 15. 
Note that our general sequence model described in 
the previous sections does not recognize different 
error types, so it was necessary to train one model 
per error type for the experiments in this section. 
Figure 3 shows the results from this series of 
experiments. We omit the results for other content 
word error, other function word and multiple er-
rors in this graph since these relatively ill-defined 
error classes performed rather poorly. As Figure 3 
illustrates, derivational errors and preposition er-
rors achieve by far the best results. The fact that 
the individual precision never reaches the level of 
the general sequence model (Figure 2) can be at-
tributed to the much smaller overall set of errors in 
each of the eight training sets. In Figure 4 we com-
pare the sequence modeling results for prepositions 
with results from the preposition component of the 
current version of the system described in Gamon 
(2010) on the same test set. That system consists of 
a preposition-specific classifier, a language model 
and a meta-classifier that combines evidence from 
the classifier and the language model. The se-
quence model approach outperforms the classifier 
of that system, but the full system including lan-
guage model and meta-classifier achieves much 
higher precision than the sequence modeling ap-
proach. 
6.4 Learning curve experiments 
An obvious question that arises is how much train-
ing data we need for an error detection sequence 
model, i.e. how does performance degrade as we 
decrease the amount of training data from the 50K 
error-annotated sentences that were used in the 
previous experiments. To this end we produced 
random subsets of the training data in 20% incre-
ments. For each of these training sets, we deter-
mined the resulting F1 score by first performing 
parameter tuning on the development set and then 
measuring precision and recall of the best model 
on the test set. Results are shown in Figure 5: at 
20% of training data, precision starts to increase at 
the cost of recall. At 80% of the training data, re-
call starts to trend up as well. This upward trend of 
both precision and recall indicates that increasing 
the amount of training data is likely to further im-
prove results. 
6.5 Error  analysis 
The precision values obtained in our experi-
ments are low, but they are also based on the 
strictest possible measure of accuracy: an error 
prediction is only counted as correct if it exactly 
matches a location and annotation in the CLC. A 
manual analysis of 400 randomly selected sentenc-
es containing ?false positives?, where the system 
had 29% precision and 10% recall, by the strictest 
calculation, showed that 14% of the ?false posi-
tives? identified an error that was either not anno-
tated in CLC or was an error type not covered by 
the system such as punctuation or case (recall from 
Section 4 that for these errors we removed the er-
ror annotations but retained the original string). An 
additional 16% were adjacent to an error annota-
tion. 12% had error annotations within 2-4 tokens 
from the predicted error. Foreign language and 
other unknown proper names comprised an addi-
tional 6%. Finally, 9% were due to tokenization 
problems or all-upper case input that throws off the 
POS tagger. Thus the precision reported in Figure 
2 through Figure 6 is really a lower bound. 30% of 
the ?false positives? either identify, or are adjacent 
to, an error. 
185
Sentence length has a strong influence on the 
accuracy of the sequence model. For sentences less 
than 7 tokens long, average precision is approxi-
mately 7%, whereas longer sentences average at 
29% precision. This observation fits with the fact 
that high-order models perform best in the task, i.e. 
the more context a model can access, the more re-
liable its predictions are. Shorter sentences are also 
less likely to contain an error: only 12% of short 
sentences contain an error, as opposed to 46% of 
sentences of seven tokens or longer. 
For sentences that are at least 7 tokens long, er-
ror predictions on the first and last two tokens (the 
last token typically being punctuation) have an av-
erage precision of 22% as compared to an average 
of 30% at all other positions. Other unreliable error 
predictions include those involving non-alphabetic 
characters (quotes, parentheses, symbols, numbers) 
with 1% precision and proper name tags with 10% 
precision. Many of the predictions on NNP tags 
identify, by and large, unknown or foreign names 
(Cricklewood, Cajamarca). Ignoring system flags 
on short sentences, symbols and NNP tags would 
improve precision with little cost to recall. 
We also experimented with a precision/recall 
metric that is less harsh but at the same time realis-
tic for error detection. For this ?soft metric? we 
count correct and incorrect predictions at the error 
level instead of the token level. An error is defined 
as a consecutive sequence of n error tags, where n 
? 1.  
 
 
Figure 2: Precision and recall of different feature sets. 
 
Figure 3: Precision and recall of different error models. 
0
0.1
0.2
0.3
0.4
0.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p
re
ci
si
o
n
 
recall 
Precision and recall 
LM LM+ String + PL LM + String + PL + DepParse
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
p
re
ci
si
o
n
 
recall 
Precision and Recall per Error Type 
content deriv inflect nounphrase preposition
186
 
Figure 4: Preposition precision and recall. 
 
Figure 5: Learning curve. 
 
Figure 6: Precision and recall for adjacent annotated error 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
p
re
ci
si
o
n
 
recall 
Precision and Recall Prepositions 
sequence model full system Gamon (2010) classifier only Gamon (2010)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 10 20 30 40 50 60 70 80 90 100
percent of training data 
Precision, recall and amount of training data 
Precision Recall
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
p
re
ci
si
o
n
 
recall 
Precision and recall: soft metric and per sentence accuracy 
exact match soft metric
per sentence soft metric, short sentences excluded
per sentence, short sentences excluded exact match, short sentences excluded
187
A predicted error counts as being correct with re-
spect to an annotated error if the following two 
criteria are met: 
a) At least one predicted error token is part of 
an annotated error or is directly adjacent to 
an annotated error 
b) No more than two predicted error tokens 
fall outside the annotated error.  
Criterion (a) establishes that predicted and annotat-
ed error are overlapping or at least directly adja-
cent. Criterion (b) ensures that the predicted error 
is ?local? enough to the annotated error and does 
not include too much irrelevant context, but it still 
allows an annotated error to be flanked by predict-
ed error tokens. Figure 6 illustrates the preci-
sion/recall characteristics of the best model when 
using this soft metric as compared to the strict met-
ric. We also included a ?per sentence? metric in 
Figure 6, where we measure precision and recall at 
the level of identifying a sentence as containing an 
error or not, in other words when using the model 
as a detector for ungrammatical sentences. In addi-
tion we show for each of the three metrics how the 
results change if short sentences (shorter than 7 
tokens) are excluded from the evaluation. 
7. Conclusion and future work 
We have shown that a discriminative high order 
sequence model can be used to detect errors in 
English learner writing. This enables a general ap-
proach to error detection, at the cost of requiring 
annotated data. High-order models outperform 
lower order models significantly for this problem. 
It is obvious that there are several avenues to 
pursue in order to improve upon these initial re-
sults. Two possibilities that we would like to high-
light are the model structure and the feature set. As 
mentioned in Section 3, instead of using a separate 
POS tagger we could follow McCallum et al 
(2003) and design a model that jointly predicts two 
sequences: POS tags and error tags. As for feature 
sets, we conducted some preliminary additional 
experiments where we added a second set of lan-
guage model features, based on a different lan-
guage model, namely the Microsoft web n-gram 
model (Wang et al 2010). The addition of these 
features raised both precision and recall.  
Finally, an error detection system is only of 
practical use if it is combined with a component 
that suggests possible corrections. For future work, 
we envision a combination of generic error detec-
tion with a corpus-based lookup system that finds 
alternative strings that have been observed in simi-
lar contexts. All these alternatives can then be 
scored by a language model in the original context 
of the user input, allowing only those suggestions 
to be shown to the user that achieve a better lan-
guage model score than the original input. This 
combination of error detection and error correction 
has the advantage that the error detection compo-
nent can be used to provide recall, i.e. it can be 
allowed to operate at a lower precision level. The 
error correction component, on the other hand, 
then reduces the number of false flags by vetting 
potential corrections by language model scores. 
Acknowledgments 
We would like to thank Claudia Leacock for the 
manual error analysis, Michel Galley for detailed 
comments on an earlier draft and Chris Quirk for 
discussions and help around the MEMM model 
implementation. The idea of the ratio to random 
language model features is Yizheng Cai?s. We also 
greatly benefited from the comments of the anon-
ymous reviewers. 
References  
Eric Steven Atwell. 1986. How to detect grammatical 
errors in a text without parsing it. In Proceedings of 
EACL, pp. 38-45. 
L?on Bottou. 1991. Une approche th?orique de 
l?apprentissage connexionniste: Applications ? la re-
connaissance de la parole. Doctoral dissertation, 
Universit? de Paris XI. 
Johnny Bigert and Ola Knutsson. 2002. Robust error 
detection: a hybrid approach combining unsupervised 
error detection and linguistic knowledge. In Proceed-
ings of the Second Workshop on Robust Methods in 
Analysis of Natural Language Data, pp. 10-19. 
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors. 
In Proceedings of NAACL, pp. 140-147. 
Martin Chodorow, Joel Tetreault and Na-Rae Han. 
2007. Detection of grammatical errors involving 
prepositions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions, pp. 25-30. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifier-based approach to preposition and deter-
188
miner error correction in L2 English. In Proceedings 
of COLING, pp. 169-176. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William Dolan, Dmitriy Belenko 
and Lucy Vanderwende. 2008. Using Contextual 
Speller Techniques and Language Modeling for ESL 
Error Correction. In Proceedings of IJCNLP. 
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing. In Proceedings of 
NAACL. 
Jianfeng Gao, Joshua Goodman, and Jiangbo Miao. 
2001. The use of clustering techniques for language 
modeling--Application to Asian languages. Computa-
tional Linguistics and Chinese Language Processing, 
6(1), 27-60. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee and Jin-
Young Ha. 2010. Using error-annotated ESL data to 
develop an ESL error correction system. In Proceed-
ings of LREC. 
Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. SST speech corpus of Japanese learners? Eng-
lish and automatic detection of learners? errors. In-
ternational Computer Archive of Modern English 
Journal, 28:31-48. 
John Lafferty, Andrew McCallum and Fernando Perei-
ra. 2001. Conditional random fields: Probabilistic 
models for segmenting and labeling sequence data. In 
Proceedings of ICWSM, pp. 282-289. 
Claudia Leacock, Martin Chodorow, Michael Gamon 
and Joel Tetreault. 2010. Automated Grammatical 
Error Detection for Language Learners. Morgan and 
Claypool. 
Mitchell P. Marcus, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: The Penn Treebank. Computa-
tional Linguistics 19:313-330. 
Andrew McCallum, Dayne Freitag and Fernando Perei-
ra. 2000. Maximum entropy Markov models for in-
formation extraction and segmentation. In 
Proceedings of ICML, pp. 591-598. 
Andrew McCallum, Khashayar Rohanimanesh and 
Charles Sutton. 2003. Dynamic Conditional Random 
Fields for jointly labeling multiple sequences. In 
Proceedings of NIPS Workshop on Syntax, Semantics 
and Statistics. 
Einat Minkov, Richard C. Wang, Anthony Tomsaic and 
William C. Cohen. 2010. NER systems that suit us-
er?s preferences: Adjusting the Recall-Precision 
trade-off for entity extraction. In Proceedings of 
NAACL, pp. 93-96. 
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 
2007. MSRLM: A scalable language modeling 
toolkit (MSR-TR-2007-144). Redmond, WA: Mi-
crosoft. 
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative 
samples. In Proceedings of ACL, pp. 73-80. 
Y. Albert Park and Roger Levy. 2011. Automated 
whole sentence grammar correction using a Noisy 
Channel Model. In Proceedings of ACL 2011. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of 
COLING/ACL, pp. 443-440. 
Alla Rozovskaya and Dan Roth. 2010a. Training Para-
digms for correcting errors in grammar and usage. In 
Proceedings of NAACL-HLT. 
Alla Rozovskaya and Dan Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In 
Proceedings of EMNLP. 
Jonas Sj?bergh. 2005. Chunking: An unsupervised 
method to find errors in text. In Proceedings of the 
15th NODALIDA conference. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee and Chin-Yew Lin. 
2007. Detecting erroneous sentences using automati-
cally mined sequential patterns. In Proceedings of 
ACL, pp. 81-88. 
Joel Tetreault and Martin Chodorow. 2008. The ups and 
downs of preposition error detection in ESL writing. 
In Proceedings of COLING, pp. 865-872. 
Kristina Toutanova, Dan Klein, Chris Manning, and 
Yoram Singer. 2003. Feature-rich part-of-speech tag-
ging with a cyclic dependency network. In Proceed-
ings of NAACL, pp. 252-259. 
Joachim Wagner, Jennifer Foster, and Josef van 
Genabith. 2007. Judging grammaticality: Experi-
ments in sentence classification. In Proceedings of 
EMNLP & CONLL, pp 112-121. 
Kuansan Wang, Christopher Thrasher, Evelyne Viegas, 
Xialong Li, and Paul Hsu. 2010. An Overview of 
Microsoft web n-gram corpus and applications. In: 
Proceedings of NAACL 2010. 
189
Proceedings of BioNLP Shared Task 2011 Workshop, pages 155?163,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
MSR-NLP Entry in BioNLP Shared Task 2011 
 
 
Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{chrisq,pallavic,mgamon,lucyv}@microsoft.com 
 
 
Abstract 
We describe the system from the Natural 
Language Processing group at Microsoft 
Research for the BioNLP 2011 Shared 
Task. The task focuses on event extraction, 
identifying structured and potentially 
nested events from unannotated text. Our 
approach follows a pipeline, first 
decorating text with syntactic information, 
then identifying the trigger words of 
complex events, and finally identifying the 
arguments of those events. The resulting 
system depends heavily on lexical and 
syntactic features. Therefore, we explored 
methods of maintaining ambiguities and 
improving the syntactic representations, 
making the lexical information less brittle 
through clustering, and of exploring novel 
feature combinations and feature reduction. 
The system ranked 4th in the GENIA task 
with an F-measure of 51.5%, and 3rd in the 
EPI task with an F-measure of 64.9%. 
1 Introduction 
We describe a system for extracting complex 
events and their arguments as applied to the 
BioNLP-2011 shared task.  Our goal is to explore 
general methods for fine-grained information 
extraction, to which the data in this shared task is 
very well suited.  We developed our system using 
only the data provided for the GENIA task, but 
then submitted output for two of the tasks, GENIA 
and EPI, training models on each dataset 
separately, with the goal of exploring how general 
the overall system design is with respect to text 
domain and event types. We used no external 
knowledge resources except a text corpus used to 
train cluster features. We further describe several 
system variations that we explored but which did 
not contribute to the final system submitted. We 
note that the MSR-NLP system consistently is 
among those with the highest recall, but needs 
additional work to improve precision. 
2 System Description 
Our event extraction system is a pipelined 
approach, closely following the structure used by 
the best performing system in 2009 (Bj?rne et al, 
2009). Given an input sentence along with 
tokenization information and a set of parses, we 
first attempt to identify the words that trigger 
complex events using a multiclass classifier. Next 
we identify edges between triggers and proteins, or 
between triggers and other triggers. Finally, given 
a graph of proteins and triggers, we use a rule-
based post-processing component to produce 
events in the format of the shared task. 
2.1 Preprocessing and Linguistic Analysis 
We began with the articles as provided, with an 
included tokenization of the input and 
identification of the proteins in the input. However, 
we did modify the token text and the part-of-
speech tags of the annotated proteins in the input to 
be PROT after tagging and parsing, as we found 
that it led to better trigger detection. 
The next major step in preprocessing was to 
produce labeled dependency parses for the input. 
Note that the dependencies may not form a tree: 
there may be cycles and some words may not be 
connected. During feature construction, this 
parsing graph was used to find paths between 
155
words in the sentence. Since proteins may consist 
of multiple words, for paths we picked a single 
representative word for each protein to act as its 
starting point and ending point. Generally this was 
the token inside the protein that is closest to the 
root of the dependency parse. In the case of ties, 
we picked the rightmost such node. 
2.1.1 McClosky-Charniak-Stanford parses 
The organizers provide parses from a version of 
the McClosky-Charniak parser, MCCC (McClosky 
and Charniak, 2008), which is a two-stage 
parser/reranker trained on the GENIA corpus. In 
addition, we used an improved set of parsing 
models that leverage unsupervised data, MCCC-I 
(McClosky, 2010). In both cases, the Stanford 
Parser was used to convert constituency trees in the 
Penn Treebank format into labeled dependency 
parses: we used the collapsed dependency format. 
2.1.2 Dependency posteriors 
Effectively maintaining and leveraging the 
ambiguity present in the underlying parser has 
improved task accuracy in some downstream tasks 
(e.g., Mi et al 2008). McClosky-Charniak parses 
in two passes: the first pass is a generative model 
that produces a set of n-best candidates, and the 
second pass is a discriminative reranker that uses a 
rich set of features including non-local 
information. We renormalized the outputs from 
this log-linear discriminative model to get a 
posterior distribution over the 50-best parses. This 
set of parses preserved some of the syntactic 
ambiguity present in the sentence. 
The Stanford parser deterministically converts 
phrase-structure trees into labeled dependency 
graphs (de Marneffe et al, 2006). We converted 
each constituency tree into a dependency graph 
separately and retained the probability computed 
above on each graph. 
One possibility was to run feature extraction on 
each of these 50 parses, and weight the resulting 
features in some manner. However, this caused a 
significant increase in feature count. Instead, we 
gathered a posterior distribution over dependency 
edges: the posterior probability of a labeled 
dependency edge was estimated by the sum of the 
probability of all parses containing that edge. 
Gathering all such edges produced a single labeled 
graph that retained much of the ambiguity of the 
input sentence. Figure 1 demonstrates this process 
on a simple example. We applied a threshold of 0.5 
and retained all edges above that threshold, 
although there are many alternative ways to exploit 
this structure.  
 
Figure 1: Example sentence from the GENIA corpus. (a) Two of the top 50 constituency parses from the MCCC-I 
parser; the first had a total probability mass of 0.43 and the second 0.25 after renormalization. Nodes that differ 
between parses are shaded and outlined. (b) The dependency posteriors (labels omitted due to space) after 
conversion of 50-best parses. Solid lines indicate edges with posterior > 0.95; edges with posterior < 0.05 were 
omitted. Most of the ambiguity is in the attachment of ?elicited?. 
156
As above, the resulting graph is likely no longer 
a connected tree, though it now may also be cyclic 
and rather strange in structure. Most of the 
dependency features were built on shortest paths 
between words. We used the algorithm in Cormen 
et al (2002, pp.595) to find shortest paths in a 
cyclic graph with non-negative edge weights. The 
shortest path algorithm used in feature finding was 
supplied uniform positive edge weights. We could 
also weight edges by the negative log probability 
to find the shortest, most likely path. 
2.1.3 ENJU 
We also experimented with the ENJU parses 
(Miyao and Tsujii, 2008) provided by the shared 
task organizers. The distribution contained the 
output of the ENJU parser in a format consistent 
with the Stanford Typed Dependency 
representation . 
2.1.4 Multiple parsers 
We know that even the best modern parsers are 
prone to errors. Including features from multiple 
parsers helps mitigate these errors. When different 
parsers agree, they can reinforce certain 
classification decisions. The features that were 
extracted from a dependency parse have names 
that include an identifier for the parser that 
produced them. In this way, the machine learning 
algorithm can assign different weights to features 
from different parsers. For finding heads of multi-
word entities, we preferred the ENJU parser if 
present in that experimental condition, then fell 
back to MCCC parses, and finally MCCC-I. 
2.1.5 Dependency conversion rules 
We computed our set of dependency features (see 
2.2.1) from the collapsed, propagated Stanford 
Typed Dependency representation (see 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf and de Marneffe et al, 2006), made 
available by the organizers.  We chose this form of 
representation since we are primarily interested in 
computing features that hold between content 
words.  Consider, for example, the noun phrase 
?phosphorylation of TRAF2?. A dependency 
representation would specify head-modifier 
relations for the tuples (phosphorylation, of) and 
(of, TRAF2). Instead of head-modifier, a typed 
dependency representation specifies PREP and 
PPOBJ as the two grammatical relations: 
PREP(phosphorylation-1, of-2) and PPOBJ(of-2, 
TRAF2-3). A collapsed representation has a single 
triplet specifying the relation between the content 
words directly, PREP_OF(phosphorylation-1, 
TRAF2-3); we considered this representation to be 
the most informative.   
We experimented with a representation that 
further normalized over syntactic variation.  The 
system submitted for the GENIA subtask does not 
use these conversion rules, while the system 
submitted for the EPI subtask does use these rules.  
See Table 2 for further details. While for some 
applications it may be useful to distinguish 
whether a given relation was expressed in the 
active or passive voice, or in a main or a relative 
clause, we believe that for this application it is 
beneficial to normalize over these types of 
syntactic variation.  Accordingly, we had a set of 
simple renaming conversion rules, followed by a 
rule for expansion; this list was our first effort and 
could likely be improved.  We modeled this 
normalized level of representation on the logical 
form, described in Jensen (1993), though we were 
unable to explore NP-or VP-anaphora 
 
Renaming conversion rules: 
1. ABBREV -> APPOS 
2. NSUBJPASS -> DOBJ 
3. AGENT -> NSUBJ 
4. XSUBJ -> NSUBJ 
5. PARTMOD(head, modifier where last 3 
characters are "ing") -> NSUBJ(modifier, head) 
6. PARTMOD(head, modifier where last 3 
characters are "ed") -> DOBJ(modifier, head) 
Expansion: 
1. For APPOS, find all edges that point to the head 
(gene-20) and duplicate those edges, but 
replacing the modifier with the modifier of the 
APPOS relation (kinase-26).  
 
Thus, in the 2nd sentence in PMC-1310901-01-
introduction, ?... leading to expression of a bcr-abl 
fusion gene, an aberrant activated tyrosine kinase, 
....?, there are two existing grammatical relations: 
 
PREP_OF(expression-15, gene-20) 
APPOS(gene-20, kinase-26) 
 
to which this rule adds: 
 
PREP_OF(expression-15, kinase-26) 
157
2.2 Trigger Detection 
We treated trigger detection as a multi-class 
classification problem: each token should be 
annotated with its trigger type or with NONE if it 
was not a trigger. When using the feature set 
detailed below, we found that an SVM 
(Tsochantaridis et al, 2004) outperformed a 
maximum entropy model by a fair margin, though 
the SVM was sensitive to its free parameters. A 
large value of C, the penalty incurred during 
training for misclassifying a data point, was 
necessary to achieve good results. 
2.2.1 Features for Trigger Detection 
Our initial feature set for trigger detection was 
strongly influenced by features that were 
successful in Bj?rne et al, (2009).  
Token Features. We included stems of single 
tokens from the Porter stemmer (Porter, 1980), 
character bigrams and trigrams, a binary indicator 
feature if the token has upper case letters, another 
indicator for the presence of punctuation, and a 
final indicator for the presence of a number. We 
gathered these features for both the current token 
as well as the three immediate neighbors on both 
the left and right hand sides. 
We constructed a gazetteer of possible trigger 
lemmas in the following manner. First we used a 
rule-based morphological analyzer (Heidorn, 2000) 
to identify the lemma of all words in the training, 
development, and test corpora. Next, for each word 
in the training and development sets, we mapped it 
to its lemma. We then computed the number of 
times that each lemma occurred as a trigger for 
each type of event (and none). Lemmas that acted 
as a trigger more than 50% of the time were added 
to the gazetteer. 
During feature extraction for a given token, we 
found the lemma of the token, and then look up 
that lemma in the gazetteer. If found, we included 
a binary feature to indicate its trigger type. 
Frequency Features. We included as features 
the number of entities in the sentence, a bag of 
words from the current sentence, and a bag of 
entities in the current sentence. 
Dependency Features. We used primarily a set 
of dependency chain features that were helpful in 
the past (Bj?rne et al, 2009); these features walk 
the Stanford Typed Dependency edges up to a 
distance of 3. 
We also found it helpful to have features about 
the path to the nearest protein, regardless of 
distance. In cases of multiple shortest paths, we 
took only one, exploring the dependency tree 
generally in left to right order. For each potential 
trigger, we looked at the dependency edge labels 
leading to that nearest protein. In addition we had a 
feature including both the dependency edge labels 
and the token text (lowercased) along that path. 
Finally, we had a feature indicating whether some 
token along that path was also in the trigger 
gazetteer. The formulation of this set of features is 
still not optimal especially for the ?binding? events 
as the training data will include paths to more than 
one protein argument.  Nevertheless, in Table 3, 
 
Key Relation Value Key Relation Value 
quantities child(left, NNS?JJ) measurable measurable child-1(left, NNS?JJ) quantities 
found child(after, VBN?NNS) hours hours child-1(after, VBN?NNS) found 
found child(after, VBN?NN) ingestion ingestion child-1(after, VBN?NN) found 
 
Figure 2: A sample PubMed sentence along with its dependency parse, and some key/relation/value triples 
extracted from that parse for computation of distributional similarity. Keys with a similar distribution of values 
under the same relation are likely semantically related. Inverse relations are indicated with a superscript -1. 
Prepositions are handled specially: we add edges labeled with the preposition from its parent to each child 
(indicated by dotted edges). 
158
we can see that this set of features contributed to 
improved precision. 
Cluster Features. Lexical and stem features 
were crucial for accuracy, but were unfortunately 
sparse and did not generalize well. To mitigate 
this, we incorporated word cluster features. In 
addition to the lexical item and the stem, we added 
another feature indicating the cluster to which each 
word belongs. To train clusters, we downloaded all 
the PubMed abstracts (http://pubmed.gov), parsed 
them with a simple dependency parser (a 
reimplementation of McDonald, 2006 trained on 
the GENIA corpus), and extracted dependency 
relations to use in clustering: words that occur in 
similar contexts should fall into the same cluster. 
An example sentence and the relations that were 
extracted for distributional similarity computation 
are presented in Figure 2. We ran a distributional 
similarity clustering algorithm (Pantel et al, 2009) 
to group words into clusters. 
Tfidf features. This set of features was intended 
to capture the salience of a term in the medical and 
?general? domain, with the aim of being able to 
distinguish domain-specific terms from more 
ambiguous terms. We calculated the tf.idf score for 
each term in the set of all PubMed abstracts and 
did the same for each term in Wikipedia. For each 
token in the input data, we then produced three 
features: (i) the tf.idf value of the token in PubMed 
abstracts, (ii) the tf.idf value of the token in 
Wikipedia, and (iii) the delta between the two 
values. Feature values were rounded to the closest 
integer. We found, however, that adding these 
features did not improve results. 
2.2.2 Feature combination and reduction 
We experimented with feature reduction and 
feature combination within the set of features 
described here. For feature reduction we tried a 
number of simple approaches that typically work 
well in text classification. The latter is similar to 
the task at hand, in that there is a very large but 
sparse feature set. We tried two feature reduction 
methods: a simple count cutoff, and selection of 
the top n features in terms of log likelihood ratio 
(Dunning, 1993) with the target values. For a count 
cutoff, we used cutoffs from 3 to 10, but we failed 
to observe any consistent gains. Only low cutoffs 
(3 and occasionally 5) would ever produce any 
small improvements on the development set. Using 
log likelihood ratio (as determined on the training 
set), we reduced the total number of features to 
between 10,000 and 75,000. None of these 
experiments improved results, however. One 
potential reason for this negative result may be that 
there were a lot of features in our set that capture 
the same phenomenon in different ways, i.e. which 
correlate highly. By retaining a subset of the 
original feature set using a count cutoff or log 
likelihood ratio we did not reduce this feature 
overlap in any way. Alternative feature reduction 
methods such as Principal Component Analysis, on 
the other hand, would target the feature overlap 
directly. For reasons of time we did not experiment 
with other feature reduction techniques but we 
believe that there may well be a gain still to be had. 
For our feature combination experiments the 
idea was to find highly predictive Boolean 
combinations of features. For example, while the 
features a and b may be weak indicators for a 
particular trigger, the cases where both a and b are 
present may be a much stronger indicator. A linear 
classifier such as the one we used in our 
experiments by definition is not able to take such 
Boolean combinations into account. Some 
classifiers such as SVMs with non-linear kernels 
do consider Boolean feature combinations, but we 
found the training times on our data prohibitive 
when using these kernels. As an alternative, we 
decided to pre-identify feature combinations that 
are predictive and then add those combination 
features to our feature inventory. In order to pre-
identify feature combinations, we trained decision 
tree classifiers on the training set, and treated each 
path from the root to a leaf through the decision 
tree classifier as a feature combination. We also 
experimented with adding all partial paths through 
the tree (as long as they started from the root) in 
addition to adding all full paths. Finally, we tried 
to increase the diversity of our combination 
features by using a ?bagging? approach, where we 
trained a multitude of decision trees on random 
subsets of the data. Again, unfortunately, we did 
not find any consistent improvements. Two 
observations that held relatively consistently across 
our experiments with combination features and 
different feature sets were: (i) only adding full 
paths as combination features sometimes helped, 
while adding partial paths did not, and (ii) bagging 
hardly ever led to improvements. 
159
2.3 Edge Detection 
This phase of the pipeline was again modeled as 
multi-class classification. There could be an edge 
originating from any trigger word and ending in 
any trigger word or protein. Looking at the set of 
all such edges, we trained a classifier to predict the 
label of this edge, or NONE if the edge was not 
present. Here we found that a maximum entropy 
classifier performed somewhat better than an SVM, 
so we used an in-house implementation of a 
maximum entropy trainer to produce the models. 
2.3.1 Features for Edge Detection 
As with trigger detection, our initial feature set for 
edge detection was strongly influenced by features 
that were successful in Bj?rne et al (2009). 
Additionally, we included the same dependency 
path features to the nearest protein that we used for 
trigger detection, described in 2.2.1. Further, for a 
prospective edge between two entities, where the 
entities are either a trigger and a protein, or a 
trigger and a second trigger, we added a feature 
that indicates (i) if the second entity is in the path 
to the nearest protein, (ii) if the head of the second 
entity is in the path to the nearest protein, (iii) the 
type of the second entity.   
2.4 Post-processing 
Given the set of edges, we used a simple 
deterministic procedure to produce a set of events. 
This step is not substantially different from that 
used in prior systems (Bj?rne et al, 2009). 
2.4.1 Balancing Precision and Recall 
As in Bj?rne et al (2009), we found that the trigger 
detector had quite low recall. Presumably this is 
due to the severe class imbalance in the training 
data: less than 5% of the input tokens are triggers. 
Thus, our classifier had a tendency to overpredict 
NONE. We tuned a single free parameter ? ? ?? 
(the ?recall booster?) to scale back the score 
associated with the NONE class before selecting 
the optimal class. The value was tuned for whole-
system F-measure; optimal values tended to fall in 
the range 0.6 to 0.8, indicating that only a small 
shift toward recall led to the best results. 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Gene_expression 749 76.37 81.46 78.83 1002 73.95 73.22 73.58 
Transcription 158 49.37 73.58 59.09 174 41.95 65.18 51.05 
Protein_catabolism 23 69.57 80.00 74.42 15 46.67 87.50 60.87 
Phosphorylation 111 73.87 84.54 78.85 185 87.57 81.41 84.37 
Localization 67 74.63 75.76 75.19 191 51.31 79.03 62.22 
=[SVT-TOTAL]= 1108 72.02 80.51 76.03 1567 68.99 74.03 71.54 
Binding 373 47.99 50.85 49.38 491 42.36 40.47 41.39 
=[EVT-TOTAL]= 1481 65.97 72.73 69.18 2058 62.63 65.46 64.02 
Regulation 292 32.53 47.05 38.62 385 24.42 42.92 31.13 
Positive_Regulation 999 38.74 51.67 44.28 1443 37.98 44.92 41.16 
Negative_Regulation 471 35.88 54.87 43.39 571 41.51 42.70 42.10 
=[REG-TOTAL]= 1762 36.95 51.79 43.13 2399 36.64 44.08 40.02 
ALL-Total 3243 50.20 62.60 55.72 4457 48.64 54.71 51.50 
Table 1: Approximate span matching/approximate recursive matching on development and test data 
sets for GENIA Shared Task -1 with our system. 
Trigger 
Detection 
Features 
Trigger 
Loss Recall Prec. F1 
B 2.14 48.44 64.08 55.18 
B + TI 2.14 48.17 62.49 54.40 
B + TI + C 2.14 50.32 60.90 55.11 
B + TI + C + PI 2.03 50.20 62.60 55.72 
B + TI + C + PI 
+D 
2.02 49.21 62.75 55.16 
Table 2: Recall/Precision/F1 on the GENIA 
development set using MCCC-I + Enju parse; 
adding different features for Trigger Detection. 
B = Base set Features, TI = Trigger inflect 
forms, 
160
3 Results 
Of the five evaluation tracks in the shared task, we 
participated in two: the GENIA core task, and the 
EPI (Epigenetics and Post-translational 
modifications) task. The systems used in each track 
were substantially similar; differences are called 
out below. Rather than building a system 
customized for a single trigger and event set, our 
goal was to build a more generalizable framework 
for event detection. 
3.1 GENIA Task 
Using F-measure performance on the development 
set as our objective function, we trained the final 
system for the GENIA task with all the features 
described in section 2, but without the conversion 
rules and without either feature combination or 
reduction. Furthermore, we trained the cluster 
features using the full set of PubMed documents 
(as of  January 2011). The results of our final 
submission are summarized in Table 1. Overall, we 
saw a substantial degradation in F-measure when 
moving from the development set to the test set, 
though this was in line with past experience from 
our and other systems.  
We compared the results for different parsers in 
Table 3. MCCC-I is not better in isolation but does 
produce higher F-measures in combination with 
other parsers. Although posteriors were not 
particularly helpful on the development set, we ran 
Parser 
SVT-Total Binding REG-Total All-Total 
Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 
MCCC 70.94 82.72 76.38 45.04 55.26 49.63 34.39 51.88 41.37 48.10 64.39 55.07 
MCCC-I 68.59 82.59 74.94 42.63 58.67 49.38 32.58 52.76 40.28 46.06 65.50 54.07 
Enju 71.66 82.18 76.56 40.75 51.01 45.31 32.24 49.39 39.01 46.69 62.70 53.52 
MCCC-I + 
Posteriors 
70.49 78.87 74.44 47.72 51.59 49.58 35.64 50.40 41.76 48.94 61.47 54.49 
MCCC + 
Enju 
71.84 82.04 76.60 44.77 53.02 48.55 34.96 53.15 42.18 48.69 64.59 55.52 
MCCC-I + 
Enju 
72.02 80.51 76.03 47.99 50.85 49.38 36.95 51.79 43.13 50.20 62.60 55.72 
Table 3: Comparison of Recall/Precision/F1 on the GENIA Task-1 development set using various 
combinations of parsers: Enju, MCCC (Mc-Closky Charniak), and MCCC-I (Mc-Closky Charniak 
Improved self-trained biomedical parsing model) with Stanford collapsed dependencies were used for 
evaluation. Results on Simple, Binding and Regulation and all events are shown. 
 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Hydroxylation 31 25.81 61.54 36.36 69 30.43 84.00 44.68 
Dehydroxylation 0 100.00 100.00 100.00 0 100.00 100.00 100.00 
Phosphorylation 32 71.88 85.19 77.97 65 72.31 85.45 78.33 
Dephosphorylation 1 0.00 0.00 0.00 4 0.00 0.00 0.00 
Ubiquitination 76 63.16 75.00 68.57 180 67.78 81.88 74.16 
Deubiquitination 8 0.00 0.00 0.00 10 0.00 0.00 0.00 
DNA_methylation 132 72.73 72.18 72.45 182 71.43 73.86 72.63 
DNA_demethylation 9 0.00 0.00 0.00 6 0.00 0.00 0.00 
Glycosylation 70 61.43 67.19 64.18 169 39.05 69.47 50.00 
Deglycosylation 7 0.00 0.00 0.00 12 0.00 0.00 0.00 
Acetylation 65 89.23 75.32 81.69 159 87.42 85.28 86.34 
Deacetylation 19 68.42 92.86 78.79 24 62.50 93.75 75.00 
Methylation 65 64.62 75.00 69.42 193 62.18 73.62 67.42 
Demethylation 7 0.00 0.00 0.00 10 0.00 0.00 0.00 
Catalysis 60 3.33 15.38 5.48 111 4.50 33.33 7.94 
====[TOTAL]==== 582 57.22 72.23 63.85 1194 55.70 77.60 64.85 
Table 4: Approximate span matching/approximate recursive matching on development and test data 
sets for EPI CORE Task with our system 
161
a system consisting of MCCC-I with posteriors 
(MCCC-I + Posteriors) on the test set after the 
final results were submitted, and found that it was 
competitive with our submitted system (MCCC-I + 
ENJU). We believe that ambiguity preservation 
has merit, and hope to explore more of this area in 
the future. Diversity is important: although the 
ENJU parser alone was not the best, combining it 
with other parsers led to consistently strong results.  
Table 2 explores feature ablation: TI appears to 
degrade performance, but clusters regain that loss. 
Protein depth information was helpful, but 
dependency rule conversion was not.  Therefore 
the B+TI+C+PI combination was our final 
submission on GENIA.  
3.2 EPI Task 
We trained the final system for the Epigenetics 
task with all the features described in section 2. 
Further, we produced the clusters for the 
Epigenetics task using only the set of GENIA 
documents provided in the shared task. 
In contrast to GENIA, we found that the 
dependency rule conversions had a positive impact 
on development set performance. Therefore, we 
included them in the final system. Otherwise the 
system was identical to the GENIA task system.  
4 Discussion 
After two rounds of the BioNLP shared task, in 
2009 and 2011, we wonder whether it might be 
possible to establish an upper-bound on recall and 
precision. There is considerable diversity among 
the participating systems, so it would be interesting 
to consider whether there are some annotations in 
the development set that cannot be predicted by 
any of the participating systems1. If this is the case, 
then those triggers and edges would present an 
interesting topic for discussion. This might result 
either in a modification of the annotation protocols, 
or an opportunity for all systems to learn more. 
After a certain amount of feature engineering, 
we found it difficult to achieve further 
improvements in F1. Perhaps we need a significant 
shift in architecture, such as a shift to joint 
inference (Poon and Vanderwende, 2010). Our 
system may be limited by the pipeline architecture. 
                                                          
1 Our system output for the 2011development set can be 
downloaded from http://research.microsoft.com/bionlp/ 
MWEs (multi-word entities) are a challenge. 
Better multi-word triggers accuracy may improve 
system performance. Multi-word proteins often led 
to incorrect part-of-speech tags and parse trees. 
Cursory inspection of the Epigenetics task 
shows that some domain-specific knowledge 
would have been beneficial. Our system had 
significant difficulties with the rare inverse event 
types, e.g. ?demethylation? (e.g., there are 319 
examples for ?methylation? in the combined 
training/development set, but only 12 examples for 
?demethylation?). Each trigger type was treated 
independently, thus we did not share information 
between an event and its related inverse event type. 
Furthermore, our system also failed to identify 
edges for these rare events. One approach would 
be to share parameters between types that differ 
only in a prefix, e.g., ?de?. In general, some 
knowledge about the hierarchy of events may let 
the learner generalize among related events. 
5 Conclusion and Future Work 
We have described a system designed for fine-
grained information extraction, which we show to 
be general enough to achieve good performance 
across different sets of event types and domains.  
The only domain-specific characteristic is the pre-
annotation of proteins as a special class of entities. 
We formulated some features based on this 
knowledge, for instance the path to the nearest 
protein.  This would likely have analogues in other 
domains, given that there is often a special class of 
target items for any Information Extraction task. 
As the various systems participating in the 
shared task mature, it will be viable to apply the 
automatic annotations in an end-user setting.  
Given a more specific application, we may have 
clearer criteria for balancing the trade-off between 
recall and precision.  We expect that fully-
automated systems coupled with reasoning 
components will need very high precision, while 
semi-automated systems, designed for information 
visualization or for assistance in curating 
knowledge bases, could benefit from high recall.  
We believe that the data provided for the shared 
tasks will support system development in either 
direction. As mentioned in our discussion, though, 
we find that improving recall continues to be a 
major challenge. We seek to better understand the 
data annotations provided. 
162
Our immediate plans to improve our system 
include semi-supervised learning and system 
combination.  We will also continue to explore 
new levels of linguistic representation to 
understand where they might provide further 
benefit.  Finally, we plan to explore models of joint 
inference to overcome the limitations of pipelining 
and deterministic post-processing. 
Acknowledgments 
We thank the shared task organizers for providing 
this interesting task and many resources, the Turku 
BioNLP group for generously providing their 
system and intermediate data output, and Patrick 
Pantel and the MSR NLP group for their help and 
support. 
References  
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola, 
Tapio Pahikkala and Tapio Salakoski. 2009. 
Extracting Complex Biological Events with Rich 
Graph-Based Feature Sets. In Proceedings of  the 
Workshop on BioNLP: Shared Task. 
Thomas Cormen, Charles Leiserson, and Ronald Rivest. 
2002. Introduction to Algorithms. MIT Press. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational 
Linguistics, 19(1), pp. 61-74. 
George E. Heidorn, 2000. Intelligent Writing 
Assistance. In Handbook of Natural Language 
Processing, ed. Robert Dale, Hermann Moisl, and 
Harold Somers.  Marcel Dekker Publishers. 
Karen Jensen. 1993. PEGASUS: Deriving Argument 
Structures after Syntax. In Natural Language 
Processing: the PLNLP approach, ed. Jensen, K., 
Heidorn, G.E., and Richardson, S.D. Kluwer 
Academic Publishers. 
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. In 
LREC 2006. 
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest 
models for probabilistic HPSG parsing. 
Computational Linguistics 34(1): 35-80. 
David McClosky and Eugene Charniak. 2008.  Self-
Training for Biomedical Parsing. In Proceedings of 
the Association for Computational Linguistics 2008. 
David McClosky. 2010. Any Domain Parsing: 
Automatic Domain Adaptation for Natural Language 
Parsing. Ph.D. thesis, Department of Computer 
Science, Brown University.  
Ryan McDonald. 2006. Discriminative training and 
spanning tree algorithms for dependency parsing. Ph. 
D. Thesis. University of Pennsylvania. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation.  In Proceedings of ACL 2008, 
Columbus, OH. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009.  
Hoifung Poon and Lucy Vanderwende. 2010. Joint 
inference for knowledge extraction from biomedical 
literature. In Proceedings of NAACL-HLT 2010. 
Martin.F. Porter, 1980, An algorithm for suffix 
stripping, Program, 14(3):130?137. 
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims, and Yasemin Alton. 2004. Support vector 
machine learning for interdependent and structured 
output spaces. In ICML 2004. 
163
