Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?189,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Active Learning for Post-Editing Based Incrementally Retrained MT
Aswarth Dara Josef van Genabith Qun Liu John Judge Antonio Toral
School of Computing
Dublin City University
Dublin, Ireland
{adara,josef,qliu,jjudge,atoral}@computing.dcu.ie
Abstract
Machine translation, in particular statis-
tical machine translation (SMT), is mak-
ing big inroads into the localisation and
translation industry. In typical work-
flows (S)MT output is checked and (where
required) manually post-edited by hu-
man translators. Recently, a significant
amount of research has concentrated on
capturing human post-editing outputs as
early as possible to incrementally up-
date/modify SMT models to avoid repeat
mistakes. Typically in these approaches,
MT and post-edits happen sequentially
and chronologically, following the way
unseen data (the translation job) is pre-
sented. In this paper, we add to the ex-
isting literature addressing the question
whether and if so, to what extent, this
process can be improved upon by Active
Learning, where input is not presented
chronologically but dynamically selected
according to criteria that maximise perfor-
mance with respect to (whatever is) the re-
maining data. We explore novel (source
side-only) selection criteria and show per-
formance increases of 0.67-2.65 points
TER absolute on average on typical indus-
try data sets compared to sequential PE-
based incrementally retrained SMT.
1 Introduction and Related Research
Machine Translation (MT) has evolved dramati-
cally over the last two decades, especially since
the appearance of statistical approaches (Brown et
al., 1993). In fact, MT is nowadays succesfully
used in the localisation and translation industry,
as for many relevant domains such as technical
documentation, post-editing (PE) of MT output by
human translators (compared to human translation
from scratch) results in notable productivity gains,
as a number of industry studies have shown con-
vincingly, e.g. (Plitt and Masselot, 2010). Fur-
thermore, incremental retraining and update tech-
niques (Bertoldi et al., 2013; Levenberg et al.,
2010; Mathur et al., 2013; Simard and Foster,
2013) allow these PEs to be fed back into the MT
model, resulting in an MT system that is contin-
uously updated to perform better on forthcoming
sentences, which should lead to a further increase
in productivity.
Typically, post-editors are presented with MT
output units (sentences) in the order in which input
sentences appear one after the other in the trans-
lation job. Because of this, incremental MT re-
training and update models based on PE outputs
also proceed in the same chronological order de-
termined by the input data. This may be sub-
optimal. In this paper we study the application of
Active Learning (AL) to the scenario of PE MT
and subsequent PE-based incremental retraining.
AL selects data (here translation inputs and their
MT outputs for PE) according to criteria that max-
imise performance with respect to the remaining
data and may diverge from processing data items
in chronological order. This may allow incremen-
tally PE-based retrained MT to (i) improve more
rapidly than chronologically PE-based retrained
MT and (ii) result in overall productivity gains.
The main contributions of this paper include:
? Previous work (Haffari et al., 2009; Blood-
good and Callison-Burch, 2010) shows that,
given a (static) training set, AL can im-
prove the quality of MT. By contrast, here
we show that AL-based data selection for hu-
man PE improves incrementally and dynami-
cally retrained MT, reducing overall PE time
of translation jobs in the localisation industry
application scenarios.
? We propose novel selection criteria for AL-
based PE: we adapt cross-entropy difference
(Moore and Lewis, 2010; Axelrod et al.,
2011), originally used for domain adaptation,
and propose an extension to cross entropy
difference with a vocabulary saturation filter
(Lewis and Eetemadi, 2013).
? While much of previous work concentrates
on research datasets (e.g. Europarl, News
Commentary), we use industry data (techni-
185
cal manuals). (Bertoldi et al., 2013) shows
that the repetition rate of news is consider-
ably lower than that of technical documenta-
tion, which impacts on the results obtained
with incremental retraining.
? Unlike in previous research, our AL-based
selection criteria take into account only the
source side of the data. This supports se-
lection before translation, keeping costs to a
minimum, a priority in commercial PE MT
applications.
? Our experiments show that AL-based selec-
tion works for PE-based incrementally re-
trained MT with overall performance gains
around 0.67 to 2.65 TER absolute on average.
AL has been successfully applied to many tasks
in natural language processing, including pars-
ing (Tang et al., 2002), named entity recogni-
tion (Miller et al., 2004), to mention just a few. See
(Olsson, 2009) for a comprehensie overview of
the application of AL to natural language process-
ing. (Haffari et al., 2009; Bloodgood and Callison-
Burch, 2010) apply AL to MT where the aim is to
build an optimal MT model from a given, static
dataset. To the best of our knowledge, the most
relevant previous research is (Gonz?alez-Rubio et
al., 2012), which applies AL to interactive MT. In
addition to differences in the AL selection criteria
and data sets, our goals are fundamentally differ-
ent: while the previous work aimed at reducing
human effort in interactive MT, we aim at reduc-
ing the overall PE time in PE-based incremental
MT update applications in the localisation indus-
try.
In our experiments reported in Section 3 below
we want to explore a space consisting of a con-
siderable number of selection strategies and incre-
mental retraining batch sizes. In order to be able to
do this, we use the target side of our industry trans-
lation memory data to approximate human PE out-
put and automatic TER (Snover et al., 2006) scores
as a proxy for human PE times (O?Brien, 2011).
2 Methodology
Given a translation job, our goal is to reduce the
overall PE time. At each stage, we select sen-
tences that are given to the post editor in such a
way that uncertain sentences (with respect to the
MT system at hand)
1
are post-edited first. We then
translate the n top-ranked sentences using the MT
system and use the human PEs of the MT outputs
to retrain the system. Algorithm 1 describes our
1
The uncertainty of a sentence with respect to the model
can be measured according to different criteria, e.g. percent-
age of unknown n-grams, perplexity etc.
method, where s and t stand for source and target,
respectively.
Algorithm 1 Sentence Selection Algorithm
Input:
L?? Initial training data
M?? Initial MT model
for C ? (Random,Sequential,Ngram,CED,CEDN) do
U?? Translation job
while size(U) > 0 do
U1.s?? SelectTopSentences(C, U.s)
U1
1
.t?? Translate(M, U1.s)
U1.t?? PostEdit(U1
1
.t)
U?? U - U1
L?? L ? U1
M?? TrainModel (L)
end while
end for
We use two baselines, i.e. random and sequen-
tial. In the random baseline, the batch of sentences
at each iteration are selected randomly. In the se-
quential baseline, the batches of sentences follow
the same order as the data.
Aside from the Random and Sequential base-
lines we use the following selection criteria:
? N-gram Overlap. An SMT system will en-
counter problems translating sentences con-
taining n-grams not seen in the training data.
Thus, PEs of sentences with high number of
unseen n-grams are considered to be more in-
formative for updating the current MT sys-
tem. However, for the MT system to trans-
late unseen n-grams accurately, they need to
be seen a minimum number V times.
2
We
use an n-gram overlap function similar to
the one described in (Gonz?alez-Rubio et al.,
2012) given in Equation 1 where N(T
(i)
) and
N(S
(i)
) return i-grams in training data and
the sentence S, respectively.
unseen(S) =
n
?
i=1
{|N(T
(i)
) ?N(S
(i)
)|>V }
n
?
i=1
N(S
(i)
)
(1)
? Cross Entropy Difference (CED). This met-
ric is originally used in data selection (Moore
and Lewis, 2010; Axelrod et al., 2011).
Given an in-domain corpus I and a general
corpus O, language models are built from
both,
3
and each sentence in O is scored ac-
cording to the entropy H difference (Equation
2
Following (Gonz?alez-Rubio et al., 2012) we use V =
10.
3
In order to make the LMs comparable they have the same
size. As commonly the size of O is considerable bigger than
I, this means that the LM for O is built from a subset of the
same size as I.
186
2). The lower the score given to a sentence,
the more useful it is to train a system for the
specific domain I .
score(s) = H
I
(s)?H
O
(s) (2)
In our AL scenario, we have the current train-
ing corpus L and an untranslated corpus U.
CED is applied to select sentences from U
that are (i) different from L (as we would like
to add sentences that add new information to
the model) and (ii) similar to the overall cor-
pus U (as we would like to add sentences that
are common in the untranslated data). Hence
we apply CED and select sentences from U
that have high entropy with respect to L and
low entropy with respect to U (Equation 3).
score(s) = H
U
(s)?H
L
(s) (3)
? CED + n-gram (CEDN). This is an exten-
sion of the CED criterion inspired by the con-
cept of the vocabulary saturation filter (Lewis
and Eetemadi, 2013). CED may select many
very similar sentences, and thus it may be the
case that some of them are redundant. By
post-processing the selection made by CED
with vocabulary saturation we aim to spot
and remove redudant sentences. This works
in two steps. In the first step, all the sentences
from U are scored using the CED metric. In
the second step, we down-rank sentences that
are considered redundant. The top sentence is
selected, and its n-grams are stored in local-
ngrams. For the remaining sentences, one by
one, their n-grams are matched against local-
ngrams. If the intersection between them is
lower than a predefined threshold, the current
sentence is added and localngrams is updated
with the n-grams from the current sentence.
Otherwise the sentence is down-ranked to the
bottom. In our experiments, the value n = 1
produces best results.
3 Experiments and Results
We use technical documentation data taken from
Symantec translation memories for the English?
French (EN?FR) and English?German (EN?DE)
language pairs (both directions) for our experi-
ments. The statistics of the data (training and in-
cremental splits) are shown in Table 1.
All the systems are trained using the
Moses (Koehn et al., 2007) phrase-based sta-
tistical MT system, with IRSTLM (Federico et
al., 2008) for language modelling (n-grams up
to order five) and with the alignment heuristic
grow-diag-final-and.
For the experiments, we considered two settings
for each language pair in each direction. In the
first setting, the initial MT system is trained using
the training set (39,679 and 54,907 sentence pairs
for EN?FR and EN?DE, respectively). Then, a
batch of 500 source sentences is selected from the
incremental dataset according to each of the se-
lection criteria, and translations are obtained with
the initial MT system. These translations are post-
edited and the corrected translations are added to
the training data.
4
We then train a new MT sys-
tem using the updated training data (initial training
data plus PEs of the first batch of sentences). The
updated model will be used to translate the next
batch. The same process is repeated until the in-
cremental dataset is finished (16 and 20 iterations
for English?French and English?German, respec-
tively). For each batch we compute the TER score
between the MT output and the refererence trans-
lations for the sentences of that batch. We then
compute the average TER score for all the batches.
These average scores, for each selection criterion,
are reported in Table 2.
In the second setting, instead of using the whole
training data, we used a subset of (randomly se-
lected) 5,000 sentence pairs for training the initial
MT system and a subset of 20,000 sentences from
the remaining data as the incremental dataset.
Here we take batches of 1,000 sentences (thus 20
batches). The results are shown in Table 3.
The first setting aims to reflect the situation
where a translation job is to be completed for a do-
main for which we have a considerable amount of
data available. Conversely, the second setting re-
flects the situation where a translation job is to be
carried out for a domain with little (if any) avail-
able data.
Dir Random Seq. Ngram CED CEDN
EN?FR 29.64 29.81 28.97 29.25 29.05
FR?EN 27.08 27.04 26.15 26.63 26.39
EN?DE 24.00 24.08 22.34 22.60 22.32
DE?EN 19.36 19.34 17.70 17.97 17.48
Table 2: TER average scores for Setting 1
Dir Random Seq. Ngram CED CEDN
EN?FR 36.23 36.26 35.20 35.48 35.17
FR?EN 33.26 33.34 32.26 32.69 32.17
EN?DE 32.23 32.19 30.58 31.96 29.98
DE?EN 27.24 27.29 26.10 26.73 24.94
Table 3: TER average scores for Setting 2
For Setting 1 (Table 2), the best result is ob-
tained by the CEDN criterion for two out of the
four directions. For EN?FR, n-gram overlap
4
As this study simulates the post-editing, we use the ref-
erences of the translated segments instead of the PEs.
187
Type
EN?FR EN?DE
Sentences Avg. EN SL Avg. FR SL Sentences Avg. EN SL Avg. DE SL
Training 39,679 13.55 15.28 54,907 12.66 12.90
Incremental 8,000 13.74 15.50 10,000 12.38 12.61
Table 1: Data Statistics for English?French and English?German Symantec Translation Memory Data.
SL stands for sentence length, EN stands for English, FR stands for French and DE stands for German
performs slightly better than CEDN (0.08 points
lower) with a decrease of 0.67 and 0.84 points
when compared to the baselines (random and se-
quential, respectively). For FR?EN, n-gram
overlap results in a decrease of 0.93 and 0.89
points compared to the baselines. The decrease in
average TER score is higher for the EN?DE and
for DE?EN directions, i.e. 1.68 and 1.88 points
respectively for CEDN compared to the random
baseline.
In the scenario with limited data available be-
forehand (Table 3), CEDN is the best performing
criterion for all the language directions. For the
EN?FR and FR?EN language pairs, CEDN results
in a decrease of 1.06 and 1.09 points compared to
the random baseline. Again, the decrease is higher
for the EN?DE and DE?EN language pairs, i.e.
2.25 and 2.30 absolute points on average.
Figure 1 shows the TER scores per iteration for
each of the criteria, for the scenario DE?EN Set-
ting 2 (the trends are similar for the other scenar-
ios). The two baselines exhibit slight improve-
ment over the iterations, both starting at around
.35 TER points and finishing at around .25 points.
Conversely, all the three criteria start at very high
scores (in the range [.5,.6]) and then improve con-
siderably to arrive at scores below .1 for the last
iterations. Compared to Ngram and CED, CEDN
reaches better scores earlier on, being the criterion
with the lowest score up to iteration 13.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 RandomSeqNgramCEDCEDN
Iteration
TER
 sco
re
Figure 1: Results per iteration, DE?EN Setting 2
Figure 1 together with Tables 2 and 3 show
that AL for PE-based incremental MT retrain-
ing really works: all AL based methods (Ngram,
CED, CEDN) show strong improvements over
both baselines after the initial 8-9 iterations (Fig-
ure 1) and best performance on the complete incre-
mental data sets, resulting in a noticeable decrease
of the overall TER score (Tables 2 and 3). In six
out of eight scenarios, our novel metric CEDN ob-
tains the best result.
4 Conclusions and Future Work
This paper has presented an application of AL to
MT for dynamically selecting automatic transla-
tions of sentences for human PE, with the aim of
reducing overall PE time in a PE-based incremen-
tal MT retraining scenario in a typical industrial
localisation workflow that aims to capitalise on
human PE as early as possible to avoid repeat mis-
takes.
Our approach makes use of source side informa-
tion only, uses two novel selection criteria based
on cross entropy difference and is tested on indus-
trial data for two language pairs. Our best per-
forming criteria allow the incrementally retrained
MT systems to improve their performance earlier
and reduce the overall TER score by around one
and two absolute points for English?French and
English?German, respectively.
In order to be able to explore a space of selec-
tion criteria and batch sizes, our experiments sim-
ulate PE, in the sense that we use the target ref-
erence (instead of PEs) and approximate PE time
with TER. Given that TER correlates well with PE
time (O?Brien, 2011), we expect AL-based selec-
tion of sentences for human PE to lead to overall
reduction of PE time. In the future work, we plan
to do the experiments using PEs to retrain the sys-
tem and measuring PE time.
In this work, we have taken batches of sentences
(size 500 to 1,000) and do full retraining. As fu-
ture work, we plan to use fully incremental retrain-
ing and perform the selection on a sentence-by-
sentence basis (instead of taking batches).
Finally and importantly, a potential drawback of
our approach is that by dynamically selecting in-
dividual sentences for PE, the human post-editor
looses context, which they may use if processing
sentences sequentially. We will explore the trade
off between the context lost and the productivity
gain achieved, and ways of supplying context (e.g.
previous and following sentence) for real PE.
188
Acknowledgements
This work is supported by Science Foundation
Ireland (Grants 12/TIDA/I2438, 07/CE/I1142 and
12/CE/I2267) as part of the Centre for Next Gen-
eration Localisation (www.cngl.ie) at Dublin City
University. We would like to thank Symantec for
the provision of data sets used in our experiments.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42, Nice, France.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Jan
Hajic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 854?864. The Association for Computer
Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2012. Active learning for
interactive machine translation. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 245?254, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In HLT-NAACL, pages 415?
423. The Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL 2007, pages 177?180, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William Lewis and Sauleh Eetemadi. 2013. Dramati-
cally reducing training data size through vocabulary
saturation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 281?291,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online learning approaches in com-
puter assisted translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, ACL, pages 301?308, Sofia, Bulgaria.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT, pages
337?342.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sharon O?Brien. 2011. Towards predicting
post-editing productivity. Machine Translation,
25(3):197?215, September.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical Report T2009:06.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 191?198, Nice,
France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
MA.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 120?127, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
189
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
TMTprime: A Recommender System for MT and TM Integration
Aswarth Dara?, Sandipan Dandapat??, Declan Groves? and Josef van Genabith?
? Centre for Next Generation Localisation, School of Computing
Dublin City University, Dublin, Ireland
? Department of Computer Science and Engineering
IIT-Guwahati, Assam, India
{adara, dgroves, josef}@computing.dcu.ie, sdandapat@iitg.ernet.in
Abstract
TMTprime is a recommender system that fa-
cilitates the effective use of both transla-
tion memory (TM) and machine translation
(MT) technology within industrial language
service providers (LSPs) localization work-
flows. LSPs have long used Translation Mem-
ory (TM) technology to assist the translation
process. Recent research shows how MT sys-
tems can be combined with TMs in Computer
Aided Translation (CAT) systems, selecting
either TM or MT output based on sophis-
ticated translation quality estimation without
access to a reference. However, to date there
are no commercially available frameworks for
this. TMTprime takes confidence estimation
out of the lab and provides a commercially vi-
able platform that allows for the seamless inte-
gration of MT with legacy TM systems to pro-
vide the most effective (least effort/cost) trans-
lation options to human translators, based on
the TMTprime confidence score.
1 Introduction
Within the LSP community there is growing interest
in the use of MT as a means to increase automation
and reduce overall localisation project cost. When
high-quality MT output is available, translators see
significant productivity gains over translation from
scratch, but poor MT quality leads to frustration
and wasted time as suggested translations are dis-
carded in favour of providing a translation from
scratch. We present a commercially-relevant soft-
ware platform providing a translation confidence es-
timation metric and, based on this, a mechanism for
effectively integrating MT with TMs in localisation
workflows. The confidence metric ensures that only
?Author did this work during his post doctoral research at
CNGL.
those MT outputs that are guaranteed to require less
post-editing effort than the best corresponding TM
match are presented to the post-editor (He et al,
2010a). The MT is integrated seamlessly, and es-
tablished localisation cost estimation models based
on TM technologies still apply as upper bounds.
2 Related Work
MT confidence estimation and its relation to existing
TM scoring methods, together with how to make the
most effective use of both technologies, is an active
area of research.
(Specia, 2011) and (Specia et al, 2009, 2010) pro-
pose a confidence estimator that relates specifically
to the post-editing effort of translators. This re-
search uses regression on both the automatic scores
assigned to the MT and scores assigned by post-
editors and aims to model post-editors? judgements
of the translation quality between good and bad, or
among three levels of post-editing effort.
Our work is an extension of (He et al, 2010a,b,c),
and uses outputs and features relevant to the TM
and MT systems. We focus on using system exter-
nal features. This is important for cases where the
internals of the MT system are not available, as in
the use of MT as a service in a localisation work-
flow.1 Furthermore, instead of having to solve a
regression problem, our approach is based on solv-
ing an easier binary prediction problem (using Sup-
port Vector Machines) and can be easily integrated
into TMs. (He et al, 2010b) present a MT/TM seg-
ment recommender, (He et al, 2010c) a MT/TM n-
best list segment re-ranker and (He et al, 2010a) a
MT/TM integration method that can use matching
sub-segments in MT/TM combination. Importantly,
1(Specia et al, 2009) note that using glass-box features
when available, in addition to black-box features, offer only
small gains and also incur significant computational effort.
10
translators can tune the models for precision without
retraining the models.
Related research by (Simard and Isabelle., 2009)
focuses on combining TM information into an SMT
system for improving the performance of the MT
when a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editing
environment using information from the phrase-
based SMT system Moses.2 (Guerberof, 2009) com-
pares the post-editing effort required for TM and
MT output, respectively. (Tatsumi, 2009) studies the
correlation between automatic evaluation scores and
post-editing effort.
3 Translation Recommender
Figure 1: TMTprime Workflow
The workflow of the translation recommender is
shown in Figure 1. We train MT systems using a
significant portion of the training data and use these
models as well as TM outputs to obtain a recommen-
dation development data set. MT systems can be
either in-house, e.g. a Moses-based system, or ex-
ternally available systems, such as Microsoft Bing3
or Google Translate.4 For each sentence in the de-
velopment data set, we have access to the reference
as well as to the outputs for each of the MT and TM
systems. We then select the best MT (or TM) output
as the translation with the lowest TER score with
respect to the reference and label the data accord-
ingly. System-independent features for each trans-
lation output are fed as input to the SVM classi-
fier (Cortes and Vapnik, 1995). The SVM classi-
fier outputs class labels and the class labels are con-
verted into confidence scores using the techniques
given in (Lin et al, 2007). Relying on system inde-
pendent black-box features has allowed us to build
2http://www.statmt.org/moses/
3http://www.bing.com/translator
4http://translate.google.com/
a fully extendable platform that will allow any num-
ber of MT systems (or indeed TM systems) to be
plugged into the recommender with little effort.
4 Demo Description
Using the Amazon EC25 deployment as a back-end,
we have developed a front-end GUI for the system
(Figure 2). The interface allows the user to select
which of the available translation systems (whether
they be MT or TM) they wish to use within the rec-
ommender system. The user can input their own
pre-established estimated cost of post-editing, based
on error ranges. Typically the costs for post-editing
those translations which have a lower-error rate (i.e.
fewer errors) is less than the cost for post-editing
translations which have a greater number of errors,
as they are of lower quality. The user is requested to
upload a file for translation to the system.
Figure 2: TMTprime GUI
Once the user has selected their desired options,
the TMTprime platform provides various analysis
measures based on its recommendation engine, such
as how many segments from the input file are recom-
mended for translation by the various selected trans-
lation engines or TMs available. Based on the input
costs, it provides a visualisation of overall estimated
cost of either using an individual translation system
on its own, or using the recommender selecting the
best performing system on a segment-by-segment
basis. The TMTprime system is an implementa-
tion of a segment-based system selector selecting
the most appropriate available translation/TM sys-
tem for a given input. A snapshot of the results pro-
duced by TMTprime is given in Figure 3: the pie-
chart shows what percentage of segments are rec-
ommended from each of the translation systems; the
5http://aws.amazon.com/ec2/
11
bar-graph gives an estimated cost of using a single
translation system alone and the estimated cost when
using TMTprime?s combined recommendation. The
estimated cost using TMTprime is lower when com-
pared to using a single MT or TM system alone
(in the worst case, it will be the same as the best-
performing single translation engine or TM system).
This estimated cost includes both the cost for trans-
lation (currently uniform cost for each translation
system) and the cost required for post-editing. For
example, if the MT is an in-house system the cost
of translation will be (close to) zero whereas there is
potentially an additional base cost for using an exter-
nal MT engine. Finally, the interface provides statis-
tics related to various confidence levels for different
translation outputs across the various translation and
TM systems.
Figure 3: Results shown by TMTprime system
5 Experiments and Results
Evaluation targets two objectives and is described
below.
5.1 Correlation with Automatic Metrics
TER and METEOR are widely-used automatic met-
rics (Snover et al, 2006; Denkowski and Lavie,
2011) that calculate the quality of translation out-
put by comparing it against a human translation,
known as the reference translation. Our data sets
for the experiment consist of English-French trans-
lation memories from the IT domain. In all instances
MT was carried out for English-French translations.
As we have access to the reference target language
translations for our test set, we are able to calculate
the TER and METEOR scores for the three trans-
lation outputs (here TM, MaTrEx (Dandapat et al,
2010) and Microsoft Bing). For each sentence in the
test set, TMTprime recommends a particular transla-
tion output with a certain estimated confidence level
without access to a reference. We measure Pearson?s
correlation coefficient (Hollander and Wolfe, 1999)
between the recommendation scores, TER scores
and METEOR scores (for all system outputs) in or-
der to determine how well the TMTprime prediction
score correlates with the widely used automatic eval-
uation metrics. Results of these experiments are pro-
vided in Table 1 which shows there is a negative cor-
relation between TMTprime scores and TER scores.
This shows that both TMTprime scores and TER
scores are moving in opposite directions, supporting
the claim that the higher the recommendation scores,
the lower the TER scores. As TER is an error score,
the lower the TER score, the higher the quality of
the machine translation output compared to its refer-
ence. On the other hand, TMTprime scores are pos-
itively correlated with METEOR scores which sup-
ports the claim that the higher the recommendation
scores, the higher the METEOR scores.
Pearson?s r TER METEOR
TMTprime -0.402 0.447
Table 1: Correlation with automatic metrics
The evaluation has been performed on a test data
set of 2,500 sentences. Both the correlations are sig-
nificant at the (p<0.01) level.
5.2 Correlation with Post-Editing time
This is the most important and crucial metric for the
evaluation. For this experiment we made use of post-
editing data captured during a real-world translation
task, for English-French in the IT domain.
Pearson?s r TER METEOR PE Time
TMTprime -0.122 0.129 -0.132
Table 2: Correlation with Post-Editing times
For testing, we collect the post-editing times for
MT outputs from two different translators using a
commercial computer-aided translation (CAT tool)
in a real-world production scenario. The data set
consists of 1113 samples and is different from the
one used in the correlation with automatic metrics.
12
Post-editing times provide a real measure of the
amount of post-editing effort required to perfect the
output of the MT system. For this experiment, we
took the output of the MT system used in the task to-
gether with the post-editing times and measured the
Pearsons correlation coefficient between the TMT-
prime recommendation scores and the post-editing
(PE) times (only for MT output from a single sys-
tem since this data set does contain PE times for
other translation outputs). In addition, we also re-
peated the previous experiment setup for finding the
correlation between the TMTprime scores and the
automatically-produced TER, METEOR scores for
this data set. The results are given in Table 2.
The results show that the confidence scores do
correlate with automatic evaluation metrics and
post-editing times. Although the correlations do not
seem as strong as before, the results are statistically
significant (p<0.01).
6 Conclusions and Future Work
We present a commercially viable translation recom-
mender system which selects the best output from
multiple TM/MT outputs. We have shown that our
confidence score correlates with automatic metrics
and post-editing times. For future work, we are
looking into extending and evaluating the system for
different language pairs and data sets.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) as
part of the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We would also
like to thank Symantec, Autodesk and Welocalize for
their support and provision of data sets used in our ex-
periments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector
networks. In Machine Learning. pages 273?297.
Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-
gio Penkale, John Tinsley, and Andy Way. 2010. OpenMa-
TrEx: A free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th interna-
tional conference on Advances in natural language process-
ing. Springer-Verlag, Berlin, Heidelberg, IceTAL?10, pages
121?126.
Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation of ma-
chine translation systems. In Proceedings of the EMNLP
2011 Workshop on Statistical Machine Translation. Edin-
burgh, UK.
Guerberof, Ana. 2009. Productivity and quality in mt post-
editing. In Proceedings of Machine Translation Summit XII
- Workshop: Beyond Translation Memories: New Tools for
Translators. Ottawa, Canada.
He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van
Genabith. 2010a. Improving the post-editing experience us-
ing translation recommendation: A user study. In Proceed-
ings of the Ninth Conference of the Association for Ma-
chine Translation in the Americas. Denver, Colorado, AMTA
2010, pages 247?256.
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.
2010b. Bridging smt and tm with translation recommenda-
tion. In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for Com-
putational Linguistics, Uppsala, Sweden, ACL 2010, pages
622?630.
He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.
2010c. Integrating n-best smt outputs into a tm system. In
Proceedings of the 23rd International Conference on Com-
putational Linguistics: Posters. Association for Computa-
tional Linguistics, Beijing, China, COLING 2010, pages
374?382.
Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric
Statistical Methods. John Wiley and Sons.
Koehn, Philip and Barry Haddow. 2009. Interactive assis-
tance to human translators using statistical machine trans-
lation methods. In Proceedings of MT Summit XII. Ottawa,
Canada, pages 73?80.
Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A
note on platt?s probabilistic outputs for support vector ma-
chines. Machine Learning 68(3):267?276.
Simard, Michael and Pierre Isabelle. 2009. Phrase-based ma-
chine translation in a computer-assisted translation environ-
ment. In Proceedings of Machine Translation Summit XII.
Ottawa, Canada, pages 120?127.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of Asso-
ciation for Machine Translation in the Americas. Cambridge,
MA, pages 223?231.
Specia, Lucia. 2011. Exploiting objective annotations for mea-
suring translation post-editing effort. In Proceedings of the
15th Annual Conference of the European Association for
Machine Translation. Leuven, Belgium, EAMT 2011, pages
73?80.
Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A
dataset for assessing machine translation evaluation metrics.
In Proceedings of LREC 2010. Valletta, Malta.
Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the confidence
of machine translation quality estimates. In Proceedings
of Machine Translation Summit XII. Ottawa, Canada, pages
136?143.
Tatsumi, Midori. 2009. Correlation between automatic evalua-
tion scores, post-editing speed and some other factors. In
Proceedings of Machine Translation Summit XII. Ottawa,
Canada, pages 332?339.
13
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597?1606,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Partial Parsing from Bitext Projections
Prashanth Mannem and Aswarth Dara
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{prashanth,abhilash.d}@research.iiit.ac.in
Abstract
Recent work has shown how a parallel
corpus can be leveraged to build syntac-
tic parser for a target language by project-
ing automatic source parse onto the target
sentence using word alignments. The pro-
jected target dependency parses are not al-
ways fully connected to be useful for train-
ing traditional dependency parsers. In this
paper, we present a greedy non-directional
parsing algorithm which doesn?t need a
fully connected parse and can learn from
partial parses by utilizing available struc-
tural and syntactic information in them.
Our parser achieved statistically signifi-
cant improvements over a baseline system
that trains on only fully connected parses
for Bulgarian, Spanish and Hindi. It also
gave a significant improvement over pre-
viously reported results for Bulgarian and
set a benchmark for Hindi.
1 Introduction
Parallel corpora have been used to transfer in-
formation from source to target languages for
Part-Of-Speech (POS) tagging, word sense disam-
biguation (Yarowsky et al, 2001), syntactic pars-
ing (Hwa et al, 2005; Ganchev et al, 2009; Jiang
and Liu, 2010) and machine translation (Koehn,
2005; Tiedemann, 2002). Analysis on the source
sentences was induced onto the target sentence via
projections across word aligned parallel corpora.
Equipped with a source language parser and a
word alignment tool, parallel data can be used to
build an automatic treebank for a target language.
The parse trees given by the parser on the source
sentences in the parallel data are projected onto the
target sentence using the word alignments from
the alignment tool. Due to the usage of automatic
source parses, automatic word alignments and dif-
ferences in the annotation schemes of source and
target languages, the projected parses are not al-
ways fully connected and can have edges missing
(Hwa et al, 2005; Ganchev et al, 2009). Non-
literal translations and divergences in the syntax
of the two languages also lead to incomplete pro-
jected parse trees.
Figure 1 shows an English-Hindi parallel sen-
tence with correct source parse, alignments and
target dependency parse. For the same sentence,
Figure 2 is a sample partial dependency parse pro-
jected using an automatic source parser on aligned
text. This parse is not fully connected with the
words banaa, kottaige and dikhataa left without
any parents.
para bahuta hai
The cottage built on the hill looks very beautiful
pahaada banaa huaa kottaige sundara dikhataa
Figure 1: Word alignment with dependency
parses for an English-Hindi parallel sentence
To train the traditional dependency parsers (Ya-
mada and Matsumoto, 2003; Eisner, 1996; Nivre,
2003), the dependency parse has to satisfy four
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). Projectivity can be relaxed in some parsers
(McDonald et al, 2005; Nivre, 2009). But these
parsers can not directly be used to learn from par-
tially connected parses (Hwa et al, 2005; Ganchev
et al, 2009).
In the projected Hindi treebank (section 4) that
was extracted from English-Hindi parallel text,
only 5.9% of the sentences had full trees. In
1597
Spanish and Bulgarian projected data extracted by
Ganchev et al (2009), the figures are 3.2% and
12.9% respectively. Learning from data with such
high proportions of partially connected depen-
dency parses requires special parsing algorithms
which are not bound by connectedness. Its only
during learning that the constraint doesn?t satisfy.
For a new sentence (i.e. during inference), the
parser should output fully connected dependency
tree.
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
on cottage very beautifulbuild lookhill PastPart. Be.Pres.
Figure 2: A sample dependency parse with partial
parses
In this paper, we present a dependency pars-
ing algorithm which can train on partial projected
parses and can take rich syntactic information as
features for learning. The parsing algorithm con-
structs the partial parses in a bottom-up manner by
performing a greedy search over all possible rela-
tions and choosing the best one at each step with-
out following either left-to-right or right-to-left
traversal. The algorithm is inspired by earlier non-
directional parsing works of Shen and Joshi (2008)
and Goldberg and Elhadad (2010). We also pro-
pose an extended partial parsing algorithm that can
learn from partial parses whose yields are partially
contiguous.
Apart from bitext projections, this work can be
extended to other cases where learning from par-
tial structures is required. For example, while
bootstrapping parsers high confidence parses are
extracted and trained upon (Steedman et al, 2003;
Reichart and Rappoport, 2007). In cases where
these parses are few, learning from partial parses
might be beneficial.
We train our parser on projected Hindi, Bulgar-
ian and Spanish treebanks and show statistically
significant improvements in accuracies between
training on fully connected trees and learning from
partial parses.
2 Related Work
Learning from partial parses has been dealt in dif-
ferent ways in the literature. Hwa et al (2005)
used post-projection completion/transformation
rules to get full parse trees from the projections
and train Collin?s parser (Collins, 1999) on them.
Ganchev et al (2009) handle partial projected
parses by avoiding committing to entire projected
tree during training. The posterior regularization
based framework constrains the projected syntac-
tic relations to hold approximately and only in ex-
pectation. Jiang and Liu (2010) refer to align-
ment matrix and a dynamic programming search
algorithm to obtain better projected dependency
trees. They deal with partial projections by break-
ing down the projected parse into a set of edges
and training on the set of projected relations rather
than on trees.
While Hwa et al (2005) requires full projected
parses to train their parser, Ganchev et al (2009)
and Jiang and Liu (2010) can learn from partially
projected trees. However, the discriminative train-
ing in (Ganchev et al, 2009) doesn?t allow for
richer syntactic context and it doesn?t learn from
all the relations in the partial dependency parse.
By treating each relation in the projected depen-
dency data independently as a classification in-
stance for parsing, Jiang and Liu (2010) sacrifice
the context of the relations such as global struc-
tural context, neighboring relations that are crucial
for dependency analysis. Due to this, they report
that the parser suffers from local optimization dur-
ing training.
The parser proposed in this work (section 3)
learns from partial trees by using the available
structural information in it and also in neighbor-
ing partial parses. We evaluated our system (sec-
tion 5) on Bulgarian and Spanish projected depen-
dency data used in (Ganchev et al, 2009) for com-
parison. The same could not be carried out for
Chinese (which was the language (Jiang and Liu,
2010) worked on) due to the unavailability of pro-
jected data used in their work. Comparison with
the traditional dependency parsers (McDonald et
al., 2005; Yamada and Matsumoto, 2003; Nivre,
2003; Goldberg and Elhadad, 2010) which train on
complete dependency parsers is out of the scope of
this work.
3 Partial Parsing
A standard dependency graph satisfies four graph
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). In our work, we assume the dependency
graph for a sentence only satisfies the single-
1598
a)
parapahaada banaa huaa kottaige bahuta sundara dikhataa hai
hill on build PastPart. cottage very beautiful look Be.Pres.
b)
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
c)
para haibanaa huaa kottaige sundara dikhataapahaada
bahuta
d)
haibanaa huaa kottaige sundara dikhataapahaada
bahutapara
e)
haibanaa kottaige sundara dikhataapahaada
bahutapara huaa
f)
banaa kottaige sundara dikhataapahaada
bahutapara huaa hai
g)
sundara
bahuta haipahaada
para
banaa kottaige dikhataa
huaa
h)
haipahaada
para
sundara
bahuta
banaa kottaige dikhataa
huaa
Figure 3: Steps taken by GNPPA. The dashed arcs indicate the unconnected words in unConn. The
dotted arcs indicate the candidate arcs in candidateArcs and the solid arcs are the high scoring arcs that
are stored in builtPPs
headedness, acyclicity and projectivity constraints
while not necessarily being connected i.e. all the
words need not have parents.
Given a sentence W=w0 ? ? ? wn with a set of
directed arcs A on the words in W , wi ? wj de-
notes a dependency arc from wi to wj , (wi,wj) 
A. wi is the parent in the arc and wj is the child in
the arc. ??? denotes the reflexive and transitive clo-
sure of the arc. wi
??? wj says that wi dominates
wj , i.e. there is (possibly empty) path from wi to
wj .
A node wi is unconnected if it does not have
an incoming arc. R is the set of all such uncon-
nected nodes in the dependency graph. For the
example in Figure 2, R={banaa, kottaige,
dikhataa}. A partial parse rooted at node wi
denoted by ?(wi) is the set of arcs that can be tra-
versed from node wi. The yield of a partial parse
?(wi) is the set of nodes dominated by it. We
use pi(wi) to refer to the yield of ?(wi) arranged
in the linear order of their occurrence in the sen-
tence. The span of the partial tree is the first and
last words in its yield.
The dependency graph D can now be rep-
resented in terms of partial parses by D =
(W,R, %(R)) where W={w0 ? ? ? wn} is the sen-
tence, R={r1 ? ? ? rm} is the set of unconnected
nodes and %(R)= {?(r1) ? ? ? ?(rm)} is the set of
partial parses rooted at these unconnected nodes.
w0 is a dummy word added at the beginning of
W to behave as a root of a fully connected parse.
A fully connected dependency graph would have
only one element w0 in R and the dependency
graph rooted at w0 as the only (fully connected)
parse in %(R).
We assume the combined yield of %(R) spans
the entire sentence and each of the partial parses in
%(R) to be contiguous and non-overlapping with
one another. A partial parse is contiguous if its
yield is contiguous i.e. if a node wj  pi(wi), then
all the words between wi and wj also belong to
pi(wi). A partial parse ?(wi) is non-overlapping if
the intersection of its yield pi(wi) with yields of all
other partial parses is empty.
3.1 Greedy Non-directional Partial Parsing
Algorithm (GNPPA)
Given the sentence W and the set of unconnected
nodes R, the parser follows a non-directional
greedy approach to establish relations in a bottom
up manner. The parser does a greedy search over
all the possible relations and picks the one with
1599
the highest score at each stage. This process is re-
peated until parents for all the nodes that do not
belong to R are chosen.
Algorithm 1 lists the outline of the greedy non-
directional partial parsing algorithm (GNPPA).
builtPPs maintains a list of all the partial
parses that have been built. It is initialized
in line 1 by considering each word as a sep-
arate partial parse with just one node. can-
didateArcs stores all the arcs that are possi-
ble at each stage of the parsing process in a
bottom up strategy. It is initialized in line 2
using the method initCandidateArcs(w0 ? ? ? wn).
initCandidateArcs(w0 ? ? ? wn) adds two candidate
arcs for each pair of consecutive words with each
other as parent (see Figure 3b). If an arc has one
of the nodes in R as the child, it isn?t included in
candidateArcs.
Algorithm 1 Partial Parsing Algorithm
Input: sentence w0 ? ? ? wn and set of partial tree roots un-
Conn={r1 ? ? ? rm}
Output: set of partial parses whose roots are in unConn
(builtPPs = {?(r1) ? ? ? ?(rm)})
1: builtPPs = {?(r1) ? ? ? ?(rn)} ? {w0 ? ? ? wn}
2: candidateArcs = initCandidateArcs(w0 ? ? ? wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax
ci  candidateArcs
score(ci,??w )
5: builtPPs.remove(bestArc.child)
6: builtPPs.remove(bestArc.parent)
7: builtPPs.add(bestArc)
8: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
9: end while
10: return builtPPs
Once initialized, the candidate arc with the
highest score (line 4) is chosen and accepted
into builtPPs. This involves replacing the best
arc?s child partial parse ?(arc.child) and parent
partial parse ?(arc.parent) over which the arc
has been formed with the arc ?(arc.parent) ?
?(arc.child) itself in builtPPs (lines 5-7). In Figure
3f, to accept the best candidate arc ?(banaa) ?
?(pahaada), the parser would remove the nodes
?(banaa) and ?(pahaada) in builtPPs and add
?(banaa) ? ?(pahaada) to builtPPs (see Fig-
ure 3g).
After the best arc is accepted, the candidateArcs
has to be updated (line 8) to remove the arcs that
are no longer valid and add new arcs in the con-
text of the updated builtPPs. Algorithm 2 shows
the update procedure. First, all the arcs that end
on the child are removed (lines 3-7) along with
the arc from child to parent. Then, the immedi-
ately previous and next partial parses of the best
arc in builtPPs are retrieved (lines 8-9) to add pos-
sible candidate arcs between them and the partial
parse representing the best arc (lines 10-23). In
the example, between Figures 3b and 3c, the arcs
?(kottaige) ? ?(bahuta) and ?(bahuta)
? ?(sundara) are first removed and the arc
?(kottaige) ? ?(sundara) is added to can-
didateArcs. Care is taken to avoid adding arcs that
end on unconnected nodes listed in R.
The entire GNPPA parsing process for the ex-
ample sentence in Figure 2 is shown in Figure 3.
Algorithm 2 updateCandidateArcs(bestArc, can-
didateArcs, builtPPs, unConn)
1: baChild = bestArc.child
2: baParent = bestArc.parent
3: for all arc  candidateArcs do
4: if arc.child = baChild or
(arc.parent = baChild and
arc.child = baParent) then
5: remove arc
6: end if
7: end for
8: prevPP = builtPPs.previousPP(bestArc)
9: nextPP = builtPPs.nextPP(bestArc)
10: if bestArc.direction == LEFT then
11: newArc1 = new Arc(prevPP,baParent)
12: newArc2 = new Arc(baParent,prevPP)
13: end if
14: if bestArc.direction == RIGHT then
15: newArc1 = new Arc(nextPP,baParent)
16: newArc2 = new Arc(baParent,nextPP)
17: end if
18: if newArc1.parent /? unConn then
19: candidateArcs.add(newArc1)
20: end if
21: if newArc2.parent /? unConn then
22: candidateArcs.add(newArc2)
23: end if
24: return candidateArcs
3.2 Learning
The algorithm described in the previous section
uses a weight vector ??w to compute the best arc
from the list of candidate arcs. This weight vec-
tor is learned using a simple Perceptron like algo-
rithm similar to the one used in (Shen and Joshi,
2008). Algorithm 3 lists the learning framework
for GNPPA.
For a training sample with sentence w0 ? ? ? wn,
projected partial parses projectedPPs={?(ri) ? ? ?
?(rm)}, unconnected words unConn and weight
vector ??w , the builtPPs and candidateArcs are ini-
tiated as in algorithm 1. Then the arc with the
highest score is selected. If this arc belongs to
the parses in projectedPPs, builtPPs and candi-
dateArcs are updated similar to the operations in
1600
a)
para haipahaada banaa huaa kottaige bahuta sundara dikhataa
hill on build PastPart. cottage very beautiful look Be.Pres.
b)
para haipahaada banaa huaa kottaige bahuta sundara dikhataa
c)
hai
bahuta
pahaada para banaa huaa kottaige sundara dikhataa
d)
hai
para bahuta
pahaada banaa huaa kottaige sundara dikhataa
Figure 4: First four steps taken by E-GNPPA. The blue colored dotted arcs are the additional candidate
arcs that are added to candidateArcs
algorithm 1. If it doesn?t, it is treated as a neg-
ative sample and a corresponding positive candi-
date arc which is present both projectedPPs and
candidateArcs is selected (lines 11-12).
The weights of the positive candidate arc are in-
creased while that of the negative sample (best arc)
are decreased. To reduce over fitting, we use aver-
aged weights (Collins, 2002) in algorithm 1.
Algorithm 3 Learning for Non-directional Greedy
Partial Parsing Algorithm
Input: sentence w0 ? ? ? wn, projected partial parses project-
edPPs, unconnected words unConn, current ??w
Output: updated ??w
1: builtPPs = {?(r1) ? ? ? ?(rn)} ? {w0 ? ? ? wn}
2: candidateArcs = initCandidateArcs(w0 ? ? ? wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax
ci  candidateArcs
score(ci,??w )
5: if bestArc ? projectedPPs then
6: builtPPs.remove(bestArc.child)
7: builtPPs.remove(bestArc.parent)
8: builtPPs.add(bestArc)
9: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
10: else
11: allowedArcs = {ci | ci  candidateArcs && ci 
projectedArcs}
12: compatArc = argmax
ci  allowedArcs
score(ci,??w )
13: promote(compatArc,??w )
14: demote(bestArc,??w )
15: end if
16: end while
17: return builtPPs
3.3 Extended GNPPA (E-GNPPA)
The GNPPA described in section 3.1 assumes that
the partial parses are contiguous. The exam-
ple in Figure 5 has a partial tree ?(dikhataa)
which isn?t contiguous. Its yield doesn?t con-
tain bahuta and sundara. We call such non-
contiguous partial parses whose yields encompass
the yield of an other partial parse as partially con-
tiguous. Partially contiguous parses are common
in the projected data and would not be parsable by
the algorithm 1 (?(dikhataa)? ?(kottaige)
would not be identified).
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
hill on build cottage very beautiful lookPastPart. Be.Pres.
Figure 5: Dependency parse with a partially con-
tiguous partial parse
In order to identify and learn from relations
which are part of partially contiguous partial
parses, we propose an extension to GNPPA. The
extended GNPAA (E-GNPPA) broadens its scope
while searching for possible candidate arcs given
R and builtPPs. If the immediate previous or
the next partial parses over which arcs are to
be formed are designated unconnected nodes, the
parser looks further for a partial parse over which
it can form arcs. For example, in Figure 4b, the
arc ?(para) ? ?(banaa) can not be added to
the candidateArcs since banaa is a designated
unconnected node in unConn. The E-GNPPA
looks over the unconnected node and adds the arc
?(para) ? ?(huaa) to the candidate arcs list
candidateArcs.
E-GNPPA differs from algorithm 1 in lines 2
and 8. The E-GNPPA uses an extended initializa-
tion method initCandidateArcsExtended(w0) for
1601
Parent and Child par.pos, chd.pos, par.lex, chd.lex
Sentence Context
par-1.pos, par-2.pos, par+1.pos, par+2.pos, par-1.lex, par+1.lex
chd-1.pos, chd-2.pos, chd+1.pos, chd+2.pos, chd-1.lex, chd+1.lex
Structural Info
leftMostChild(par).pos, rightMostChild(par).pos, leftSibling(chd).pos,
rightSibling(chd).pos
Partial Parse Context previousPP().pos, previousPP().lex, nextPP().pos, nextPP().lex
Table 1: Information on which features are defined. par denotes the parent in the relation and chd the
child. .pos and .lex is the POS and word-form of the corresponding node. +/-i is the previous/next
ith word in the sentence. leftMostChild() and rightMostChild() denote the left most and right most
children of a node. leftSibling() and rightSibling() get the immediate left and right siblings of a node.
previousPP() and nextPP() return the immediate previous and next partial parses of the arc in builtPPs at
the state.
candidateArcs in line 2 and an extended proce-
dure updateCandidateArcsExtended to update the
candidateArcs after each step in line 8. Algorithm
4 shows the changes w.r.t algorithm 2. Figure 4
presents the steps taken by the E-GNPPA parser
for the example parse in Figure 5.
Algorithm 4 updateCandidateArcsExtended
( bestArc, candidateArcs, builtPPs,unConn )
? ? ? lines 1 to 7 of Algorithm 2 ? ? ?
prevPP = builtPPs.previousPP(bestArc)
while prevPP ? unConn do
prevPP = builtPPs.previousPP(prevPP)
end while
nextPP = builtPPs.nextPP(bestArc)
while nextPP ? unConn do
nextPP = builtPPs.nextPP(nextPP)
end while
? ? ? lines 10 to 24 of Algorithm 2 ? ? ?
3.4 Features
Features for a relation (candidate arc) are defined
on the POS tags and lexical items of the nodes in
the relation and those in its context. Two kinds
of context are used a) context from the input sen-
tence (sentence context) b) context in builtPPs i.e.
nearby partial parses (partial parse context). In-
formation from the partial parses (structural info)
such as left and right most children of the par-
ent node in the relation, left and right siblings of
the child node in the relation are also used. Ta-
ble 1 lists the information on which features are
defined in the various configurations of the three
language parsers. The actual features are combi-
nations of the information present in the table. The
set varies depending on the language and whether
its GNPPA or E-GNPPA approach.
While training, no features are defined on
whether a node is unconnected (present in un-
Conn) or not as this information isn?t available
during testing.
4 Hindi Projected Dependency Treebank
We conducted experiments on English-Hindi par-
allel data by transferring syntactic information
from English to Hindi to build a projected depen-
dency treebank for Hindi.
The TIDES English-Hindi parallel data con-
taining 45,000 sentences was used for this pur-
pose 1 (Venkatapathy, 2008). Word alignments
for these sentences were obtained using the widely
used GIZA++ toolkit in grow-diag-final-and mode
(Och and Ney, 2003). Since Hindi is a morpho-
logically rich language, root words were used in-
stead of the word forms. A bidirectional English
POS tagger (Shen et al, 2007) was used to POS
tag the source sentences and the parses were ob-
tained using the first order MST parser (McDon-
ald et al, 2005) trained on dependencies extracted
from Penn treebank using the head rules of Ya-
mada and Matsumoto (2003). A CRF based Hindi
POS tagger (PVS. and Gali, 2007) was used to
POS tag the target sentences.
English and Hindi being morphologically and
syntactically divergent makes the word alignment
and dependency projection a challenging task.
The source dependencies are projected using an
approach similar to (Hwa et al, 2005). While
they use post-projection transformations on the
projected parse to account for annotation differ-
ences, we use pre-projection transformations on
the source parse. The projection algorithm pro-
1The original data had 50,000 parallel sentences. It was
later refined by IIIT-Hyderabad to remove repetitions and
other trivial errors. The corpus is still noisy with typographi-
cal errors, mismatched sentences and unfaithful translations.
1602
duces acyclic parses which could be unconnected
and non-projective.
4.1 Annotation Differences in Hindi and
English
Before projecting the source parses onto the tar-
get sentence, the parses are transformed to reflect
the annotation scheme differences in English and
Hindi. While English dependency parses reflect
the PTB annotation style (Marcus et al, 1994),
we project them to Hindi to reflect the annotation
scheme described in (Begum et al, 2008). The
differences in the annotation schemes are with re-
spect to three phenomena: a) head of a verb group
containing auxiliary and main verbs, b) preposi-
tions in a prepositional phrase (PP) and c) coordi-
nation structures.
In the English parses, the auxiliary verb is the
head of the main verb while in Hindi, the main
verb is the head of the auxiliary in the verb group.
For example, in the Hindi parse in Figure 1,
dikhataa is the head of the auxiliary verb hai.
The prepositions in English are realized as post-
positions in Hindi. While prepositions are the
heads in a preposition phrase, post-positions are
the modifiers of the preceding nouns in Hindi. In
pahaada para (on the hill), hill is the head
of para. In coordination structures, while En-
glish differentiates between how NP coordination
and VP coordination structures behave, Hindi an-
notation scheme is consistent in its handling. Left-
most verb is the head of a VP coordination struc-
ture in English whereas the rightmost noun is the
head in case of NP coordination. In Hindi, the con-
junct is the head of the two verbs/nouns in the co-
ordination structure.
These three cases are identified in the source
tree and appropriate transformations are made to
the source parse itself before projecting the rela-
tions using word alignments.
5 Experiments
We carried out all our experiments on paral-
lel corpora belonging to English-Hindi, English-
Bulgarian and English-Spanish language pairs.
While the Hindi projected treebank was obtained
using the method described in section 4, Bulgar-
ian and Spanish projected datasets were obtained
using the approach in (Ganchev et al, 2009). The
datasets of Bulgarian and Spanish that contributed
to the best accuracies for Ganchev et al (2009)
Statistic Hindi Bulgarian Spanish
N(Words) 226852 71986 133124
N(Parent==-1) 44607 30268 54815
P(Parent==-1) 19.7 42.0 41.1
N(Full trees) 593 1299 327
N(GNPPA) 30063 10850 19622
P(GNPPA) 16.4 26.0 25.0
N(E-GNPPA) 35389 12281 24577
P(E-GNPPA) 19.3 29.4 30.0
Table 2: Statistics of the Hindi, Bulgarian and Spanish
projected treebanks used for experiments. Each of them has
10,000 randomly picked parses. N(X) denotes number of X
and P(X) denotes percentage of X. N(Words) is the number
of words. N(Parents==-1) is the number of words without a
parent. N(Full trees) is the number of parses which are fully
connected. N(GNPPA) is the number of relations learnt by
GNPPA parser and N(E-GNPPA) is the number of relations
learnt by E-GNPPA parser. Note that P(GNPPA) is calculated
as N(GNPPA)/(N(Words) - N(Parents==-1)).
were used in our work (7 rules dataset for Bulgar-
ian and 3 rules dataset for Spanish). The Hindi,
Bulgarian and Spanish projected dependency tree-
banks have 44760, 39516 and 76958 sentences re-
spectively. Since we don?t have confidence scores
for the projections on the sentences, we picked
10,000 sentences randomly in each of the three
datasets for training the parsers2. Other methods
of choosing the 10K sentences such as those with
the max. no. of relations, those with least no. of
unconnected words, those with max. no. of con-
tiguous partial trees that can be learned by GNPPA
parser etc. were tried out. Among all these, ran-
dom selection was consistent and yielded the best
results. The errors introduced in the projected
parses by errors in word alignment, source parser
and projection are not consistent enough to be ex-
ploited to select the better parses from the entire
projected data.
Table 2 gives an account of the randomly cho-
sen 10k sentences in terms of the number of words,
words without parents etc. Around 40% of the
words spread over 88% of sentences in Bulgarian
and 97% of sentences in Spanish have no parents.
Traditional dependency parsers which only train
from fully connected trees would not be able to
learn from these sentences. P(GNPPA) is the per-
centage of relations in the data that are learned by
the GNPPA parser satisfying the contiguous par-
tial tree constraint and P(E-GNPPA) is the per-
2Exactly 10K sentences were selected in order to compare
our results with those of (Ganchev et al, 2009).
1603
Parser
Hindi Bulgarian Spanish
Punct NoPunct Punct NoPunct Punct NoPunct
Baseline 78.70 77.39 51.85 55.15 41.60 45.61
GNPPA 80.03* 78.81* 77.03* 79.06* 65.49* 68.70*
E-GNPPA 81.10*? 79.94*? 78.93*? 80.11*? 67.69*? 70.90*?
Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained
on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates
without punctuation. * next to an accuracy denotes statistically significant (McNemar?s and p < 0.05)
improvement over the baseline. ? denotes significance over GNPPA
centage that satisfies the partially contiguous con-
straint. E-GNPPA parser learns around 2-5% more
no. of relations than GNPPA due to the relaxation
in the constraints.
The Hindi test data that was released as part of
the ICON-2010 Shared Task (Husain et al, 2010)
was used for evaluation. For Bulgarian and Span-
ish, we used the same test data that was used in
the work of Ganchev et al (2009). These test
datasets had sentences from the training section of
the CoNLL Shared Task (Nivre et al, 2007) that
had lengths less than or equal to 10. All the test
datasets have gold POS tags.
A baseline parser was built to compare learning
from partial parses with learning from fully con-
nected parses. Full parses are constructed from
partial parses in the projected data by randomly
assigning parents to unconnected parents, similar
to the work in (Hwa et al, 2005). The uncon-
nected words in the parse are selected randomly
one by one and are assigned parents randomly to
complete the parse. This process is repeated for all
the sentences in the three language datasets. The
parser is then trained with the GNPPA algorithm
on these fully connected parses to be used as the
baseline.
Table 3 lists the accuracies of the baseline,
GNPPA and E-GNPPA parsers. The accuracies
are unlabeled attachment scores (UAS): the per-
centage of words with the correct head. Table
4 compares our accuracies with those reported in
(Ganchev et al, 2009) for Bulgarian and Spanish.
5.1 Discussion
The baseline reported in (Ganchev et al, 2009)
significantly outperforms our baseline (see Table
4) due to the different baselines used in both the
works. In our work, while creating the data for
the baseline by assigning random parents to un-
connected words, acyclicity and projectivity con-
Parser Bulgarian Spanish
Ganchev-Baseline 72.6 69.0
Baseline 55.15 45.61
Ganchev-Discriminative 78.3 72.3
GNPPA 79.06 68.70
E-GNPPA 80.11 70.90
Table 4: Comparison of baseline, GNPPA and E-
GNPPA with baseline and discriminative model
from (Ganchev et al, 2009) for Bulgarian and
Spanish. Evaluation didn?t include punctuation.
straints are not enforced. Ganchev et al (2009)?s
baseline is similar to the first iteration of their dis-
criminative model and hence performs better than
ours. Our Bulgarian E-GNPPA parser achieved a
1.8% gain over theirs while the Spanish results are
lower. Though their training data size is also 10K,
the training data is different in both our works due
to the difference in the method of choosing 10K
sentences from the large projected treebanks.
The GNPPA accuracies (see table 3) for all the
three languages are significant improvements over
the baseline accuracies. This shows that learning
from partial parses is effective when compared to
imposing the connected constraint on the partially
projected dependency parse. Even while project-
ing source dependencies during data creation, it
is better to project high confidence relations than
look to project more relations and thereby intro-
duce noise.
The E-GNPPA which also learns from partially
contiguous partial parses achieved statistically sig-
nificant gains for all the three languages. The
gains across languages is due to the fact that in
the 10K data that was used for training, E-GNPPA
parser could learn 2 ? 5% more relations over
GNPPA (see Table 2).
Figure 6 shows the accuracies of baseline and E-
1604
 
30
 
40
 
50
 
60
 
70
 
80  0
 
1
 
2
 
3
 
4
 
5
 
6
 
7
 
8
 
9
 
10
Unlabeled Accuracy
Thousa
nds of 
senten
ces
Bulgari
an Hindi Spanis
h
hn-bas
eline
bg-bas
eline
es-bas
eline
Figure 6: Accuracies (without punctuation) w.r.t
varying training data sizes for baseline and E-
GNPPA parsers.
GNPPA parser for the three languages when train-
ing data size is varied. The parsers peak early with
less than 1000 sentences and make small gains
with the addition of more data.
6 Conclusion
We presented a non-directional parsing algorithm
that can learn from partial parses using syntac-
tic and contextual information as features. A
Hindi projected dependency treebank was devel-
oped from English-Hindi bilingual data and ex-
periments were conducted for three languages
Hindi, Bulgarian and Spanish. Statistically sig-
nificant improvements were achieved by our par-
tial parsers over the baseline system. The partial
parsing algorithms presented in this paper are not
specific to bitext projections and can be used for
learning from partial parses in any setting.
References
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation
scheme for indian languages. In In Proceedings of
The Third International Joint Conference on Natural
Language Processing (IJCNLP), Hyderabad, India.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
AAI9926110.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Morristown, NJ, USA. Association
for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, pages 340?345, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1, ACL-IJCNLP ?09, pages 369?
377, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Morristown, NJ,
USA. Association for Computational Linguistics.
Samar Husain, Prashanth Mannem, Bharath Ambati,
and Phani Gadde. 2010. Icon 2010 tools contest on
indian language dependency parsing. In Proceed-
ings of ICON 2010 NLP Tools Contest.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311?325, September.
Wenbin Jiang and Qun Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 12?20, Morristown, NJ, USA. Association for
Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Citeseer.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL on Main conference poster
sessions, pages 507?514, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
1605
Jens Nilsson and Joakim Nivre. 2008. Malteval:
an evaluation and visualization tool for dependency
parsing. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic. Association for
Computational Linguistics.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Eighth International
Workshop on Parsing Technologies, Nancy, France.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Avinesh PVS. and Karthik Gali. 2007. Part-Of-Speech
Tagging and Chunking using Conditional Random
Fields and Transformation-Based Learning. In Pro-
ceedings of the IJCAI and the Workshop On Shallow
Parsing for South Asian Languages (SPSAL), pages
21?24.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statisti-
cal parsers trained on small datasets. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 616?623,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495?504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the tenth conference on
European chapter of the Association for Computa-
tional Linguistics - Volume 1, EACL ?03, pages 331?
338, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Jrg Tiedemann. 2002. MatsLex - a multilingual lex-
ical database for machine translation. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC?2002), vol-
ume VI, pages 1909?1912, Las Palmas de Gran Ca-
naria, Spain, 29-31 May.
Sriram Venkatapathy. 2008. Nlp tools contest - 2008:
Summary. In Proceedings of ICON 2008 NLP Tools
Contest.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In In Proceedings of IWPT, pages 195?206.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, HLT ?01,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
1606
