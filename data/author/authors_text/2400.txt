Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112?120,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Human Evaluation of a German Surface Realisation Ranker
Aoife Cahill
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
70174 Stuttgart, Germany
aoife.cahill@ims.uni-stuttgart.de
Martin Forst
Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304, USA
mforst@parc.com
Abstract
In this paper we present a human-based
evaluation of surface realisation alterna-
tives. We examine the relative rankings of
naturally occurring corpus sentences and
automatically generated strings chosen by
statistical models (language model, log-
linear model), as well as the naturalness of
the strings chosen by the log-linear model.
We also investigate to what extent preced-
ing context has an effect on choice. We
show that native speakers do accept quite
some variation in word order, but there are
also clearly factors that make certain real-
isation alternatives more natural.
1 Introduction
An important component of research on surface
realisation (the task of generating strings for a
given abstract representation) is evaluation, espe-
cially if we want to be able to compare across sys-
tems. There is consensus that exact match with
respect to an actually observed corpus sentence is
too strict a metric and that BLEU score measured
against corpus sentences can only give a rough im-
pression of the quality of the system output. It is
unclear, however, what kind of metric would be
most suitable for the evaluation of string realisa-
tions, so that, as a result, there have been a range of
automatic metrics applied including inter alia ex-
act match, string edit distance, NIST SSA, BLEU,
NIST, ROUGE, generation string accuracy, gener-
ation tree accuracy, word accuracy (Bangalore et
al., 2000; Callaway, 2003; Nakanishi et al, 2005;
Velldal and Oepen, 2006; Belz and Reiter, 2006).
It is not always clear how appropriate these met-
rics are, especially at the level of individual sen-
tences. Using automatic evaluation metrics cannot
be avoided, but ideally, a metric for the evaluation
of realisation rankers would rank alternative real-
isations in the same way as native speakers of the
language for which the surface realisation system
is developed, and not only globally, but also at the
level of individual sentences.
Another major consideration in evaluation is
what to take as the gold standard. The easiest op-
tion is to take the original corpus string that was
used to produce the abstract representation from
which we generate. However, there may well be
other realisations of the same input that are as
suitable in the given context. Reiter and Sripada
(2002) argue that while we should take advantage
of large corpora in NLG, we also need to take care
that we do not introduce errors by learning from
incorrect data present in corpora.
In order to better understand what makes good
evaluation data (and metrics), we designed and im-
plemented an experiment in which human judges
evaluated German string realisations. The main
aims of this experiment were: (i) to establish how
much variation in German word order is accept-
able for human judges, (ii) to find an automatic
evaluation metric that mirrors the findings of the
human evaluation, (iii) to provide detailed feed-
back for the designers of the surface realisation
ranking model and (iv) to establish what effect
preceding context has on the choice of realisation.
In this paper, we concentrate on points (i) and (iv).
The remainder of the paper is structured as fol-
lows: In Section 2 we outline the realisation rank-
ing system that provided the data for the experi-
ment. In Section 3 we outline the design of the
experiment and in Section 4 we present our find-
ings. In Section 5 we relate this to other work and
finally we conclude in Section 6.
2 A Realisation Ranking System for
German
We take the realisation ranking system for German
described in Cahill et al (2007) and present the
output to human judges. One goal of this series
of experiments is to examine whether the results
112
based on automatic evaluation metrics published
in that paper are confirmed in an evaluation by hu-
mans. Another goal is to collect data that will al-
low us and other researchers1 to explore more fine-
grained and reliable automatic evaluation metrics
for realisation ranking.
The system presented by Cahill et al (2007)
ranks the strings generated by a hand-crafted
broad-coverage Lexical Functional Grammar
(Bresnan, 2001) for German (Rohrer and Forst,
2006) on the basis of a given input f-structure.
In these experiments, we use f-structures from
their held-out and test sets, of which 96% can
be associated with surface realisations by the
grammar. F-structures are attribute-value ma-
trices representing grammatical functions and
morphosyntactic features; roughly speaking,
they are predicate-argument structures. In LFG,
f-structures are assumed to be a crosslinguistically
relatively parallel syntactic representation level,
alongside the more surface-oriented c-structures,
which are context-free trees. Figure 1 shows
the f-structure2 associated with TIGER Corpus
sentence 8609, glossed in (1), as well as the 4
string realisations that the German LFG generates
from this f-structure. The LFG is reversible,
i.e. the same grammar is used for parsing as for
generation. It is a hand-crafted grammar, and
has been carefully constructed to only parse (and
therefore generate) grammatical strings.3
(1) Williams
Williams
war
was
in
in
der
the
britischen
British
Politik
politics
a?u?erst
extremely
umstritten.
controversial.
?Williams was extremely controversial in British
politics.?
The ranker consists of a log-linear model that
is based on linguistically informed structural fea-
tures as well as a trigram language model, whose
1The data is available for download from
http://www.ims.uni-stuttgart.de/projekte/pargram/geneval/data/
2Note that only grammatical functions are displayed;
morphosyntactic features are omitted due to space con-
straints. Also note that the discourse function TOPIC was
ignored in generation.
3A ranking mechanism based on so-called optimality
marks can lead to a certain ?asymmetry? between parsing and
generation in the sense that not all sentences that can be as-
sociated with a certain f-structure are necessarily generated
from this same f-structure. E.g. the sentence Williams war
a?u?erst umstritten in der britischen Politik. can be parsed
into the f-structure in Figure 1, but it is not generated because
an optimality mark penalizes the extraposition of PPs to the
right of a clause. Only few optimality marks were used in the
process of generating the data for our experiments, so that the
bias they introduce should not be too noticeable.
score is integrated into the model simply as an ad-
ditional feature. The log-linear model is trained on
corpus data, in this case sentences from the TIGER
Corpus (Brants et al, 2002), for which f-structures
are available; the observed corpus sentences are
considered as references whose probability is to
be maximised during the training process.
The output of the realisation ranker is evalu-
ated in terms of exact match and BLEU score,
both measured against the actually observed cor-
pus sentences. In addition to the figures achieved
by the ranker, the corresponding figures achieved
by the employed trigram language model on its
own are given as a baseline, and the exact match
figure of the best possible string selection is given
as an upper bound.4 We summarise these figures
in Table 1.
Exact Match BLEU score
Language model 27% 0.7306
Log-linear model 37% 0.7939
Upper bound 62% ?
Table 1: Results achieved by trigram LM ranker
and log-linear model ranker in Cahill et al (2007)
By means of these figures, Cahill et al (2007)
show that a log-linear model based on structural
features and a language model score performs con-
siderably better realisation ranking than just a lan-
guage model. In our experiments, presented in de-
tail in the following section, we examine whether
human judges confirm this and how natural and/or
acceptable the selection performed by the realisa-
tion ranker under consideration is for German na-
tive speakers.
3 Experiment Design
The experiment was divided into three parts. Each
part took between 30 and 45 minutes to complete,
and participants were asked to leave some time
(e.g. a week) between each part. In total, 24 par-
ticipants completed the experiment. All were na-
tive German speakers (mostly from South-Western
Germany) and almost all had a linguistic back-
ground. Table 2 gives a breakdown of the items
in each part of the experiment.5
4The observed corpus sentence can be (re)generated from
the corresponding f-structure for only 62% of the sentences
used, usually because of differences in punctuation. Hence
this exact match upper bound. An upper bound in terms
of BLEU score cannot be computed because BLEU score is
computed on entire corpora rather than individual sentences.
5Experiments 3a and 3b contained the same items as ex-
periments 1a and 1b.
113
"Williams war in der britischen Politik ?u?erst umstritten."
'sein<[378:umstritten]>[1:Williams]'PRED
'Williams'PRED1SUBJ
'umstritten<[1:Williams]>'PRED
[1:Williams]SUBJ
'?u?erst'PRED274ADJUNCT378
XCOMP-PRED
'in<[115:Politik]>'PRED
'Politik'PRED
'britisch<[115:Politik]>'PRED
[115:Politik]SUBJ171ADJUNCT
'die'PREDDETSPEC115
OBJ
88
ADJUNCT
[1:Williams]TOPIC65
Williams war in der britischen Politik a?u?erst umstritten.
In der britischen Politik war Williams a?u?erst umstritten.
?Au?erst umstritten war Williams in der britischen Politik.
?Au?erst umstritten war in der britischen Politik Williams.
Figure 1: F-structure associated with (1) and strings generated from it.
Exp 1a Exp 1b Exp 2
Num. items 44 52 41
Avg. sent length 14.4 12.1 9.4
Table 2: Statistics for each experiment part
3.1 Part 1
The aim of part 1 of the experiment was twofold.
First, to identify the relative rankings of the sys-
tems evaluated in Cahill et al (2007) according to
the human judges, and second to evaluate the qual-
ity of the strings as chosen by the log-linear model
of Cahill et al (2007). To these ends, part 1 was
further subdivided into two tasks: 1a and b.
Task 1a: During the first task, participants were
presented with alternative realisations for an input
f-structure (but not shown the original f-structure)
and asked to rank them in order of how natural
sounding they were, 1 being the best and 3 be-
ing the worst.6 Each item contained three alter-
natives, (i) the original string found in TIGER, (ii)
the string chosen as most likely by the trigram lan-
guage model, and (iii) the string chosen as most
likely by the log-linear model. Only items where
each system chose a different alternative were cho-
sen from the evaluation data of Cahill et al (2007).
The three alternatives were presented in random
order for each item, and the items were presented
in random order for each participant. Some items
were presented randomly to participants more than
6Joint rankings were not allowed, i.e. the participants
were forced to make strict ranking decisions, and in hindsight
this may have introduced some noise into the data.
once as a sanity check, and in total for Part 1a, par-
ticipants made 52 ranking judgements on 44 items.
Figure 2 shows a screen shot of what the partici-
pant was presented with for this task.
Task 1b: In the second task of part 1, partic-
ipants were presented with the string chosen by
the log-linear model as being the most likely and
asked to evaluate it on a scale from 1 to 5 on how
natural sounding it was, 1 being very unnatural
or marked and 5 being completely natural. Fig-
ure 3 shows a screen shot of what the participant
saw during the experiment. Again some random
items were presented to the participant more than
once, and the items themselves were presented in
random order. In total, the participants made 58
judgements on 52 items.
3.2 Part 2
In the second part of the experiment, participants
were presented between 4 and 8 alternative sur-
face realisations for an input f-structure, as well
as some preceding context. This preceding con-
text was automatically determined using informa-
tion from the export release of the TIGER treebank
and was not hand-checked for relevance.7 The par-
ticipants were then asked to choose the realisation
that they felt fit best given the preceding sentences.
7The export release of the TIGER treebank includes an
article ID for each sentence. Unfortunately, this is not com-
pletely reliable for determining relevant context, since an ar-
ticle can also contain several short news snippets which are
completely unrelated. Paragraph boundaries are not marked.
This leads to some noise, which unfortunately is difficult to
measure objectively
114
Figure 2: Screenshot of Part 1a of the Experiment
Figure 3: Screenshot of Part 1b of the Experiment
Total Average
Rank 1 Rank 2 Rank 3 Rank
Original String 817 366 65 1.40
LL String 303 593 352 2.04
LM String 128 289 831 2.56
Table 3: Task 1a: Ranks for each system
The items were presented in random order, and the
list of alternatives were presented in random order
to each participant. Some items were randomly
presented more than once, resulting in 50 judge-
ments on 41 items. Figure 4 shows a screen shot
of what the participant saw.
3.3 Part 3
Part 3 of the experiment was identical to Part 1,
except that now, rather than the participants being
presented with sentences in isolation, they were
given some preceding context. The context was
determined automatically, in the same way as in
Part 2. The items themselves were the same as in
Part 1. The aim of this part of the experiment was
to see what effect preceding context had on judge-
ments.
4 Results
In this section we present the result and analysis
of the experiments outlined above.
4.1 How good were the strings?
The data collected in Experiment 1a showed the
overall human relative ranking of the three sys-
tems. We calculate the total numbers of each
rank for each system. Table 3 summarises the re-
sults. The original string is the string found in the
Figure 5: Task 1b: Naturalness scores for strings
chosen by log-linear model, 1=worst
TIGER Corpus, the LM String is the string cho-
sen as being most likely by the trigram language
model and the LL String is the string chosen as
being most likely by the log-linear model.
Table 3 confirms the overall relative rankings
of the three systems as determined using BLEU
scores. The original TIGER strings are ranked best
(average 1.4), the strings chosen by the log-linear
model are ranked better than the strings chosen by
the language model (average 2.65 vs 2.04).
In Experiment 1b, the aim was to find out how
acceptable the strings chosen by the log-linear
model were, although they were not the same as
the original string. Figure 5 summarises the data.
The graph shows that the majority of strings cho-
sen by the log-linear model ranked very highly on
the naturalness scale.
4.2 Did the human judges agree with the
original authors?
In Experiment 2, the aim was to find out how of-
ten the human judges chose the same string as the
original author (given alternatives generated by the
LFG grammar). Most items had between 4 and 6
alternative strings. In 70% of all items, the human
judges chose the same string as the original au-
thor. However, the remaining 30% of the time, the
human judges picked an alternative as being the
115
Figure 4: Screenshot of Part 2 of the Experiment
most fitting in the given context.8 This suggests
that there is quite some variation in what native
German speakers will accept, but that this varia-
tion is by no means random, as indicated by 70%
of choices being the same string as the original au-
thor?s.
Figure 6 shows for each bin of possible alterna-
tives, the percentage of items with a given num-
ber of choices made. For example, for the items
with 4 possible alternatives, over 70% of the time,
the judges chose between only 2 of them. For the
items with 5 possible alternatives, in 10% of those
items the human judges chose only 1 of those al-
ternatives; in 30% of cases, the human judges all
chose the same 2 solutions, and for the remain-
ing 60% they chose between only 3 of the 5 pos-
sible alternatives. These figures indicate that al-
though judges could not always agree on one best
string, often they were only choosing between 2 or
3 of the possible alternatives. This suggests that,
on the one hand, native speakers do accept quite
some variation, but that, on the other hand, there
are clearly factors that make certain realisation al-
ternatives more preferable than others.
Figure 6: Exp 2: Number of Alternatives Chosen
8Recall that almost all strings presented to the judges were
grammatical.
The graph in Figure 6 shows that only in two
cases did the human judges choose from among
all possible alternatives. In one case, there were 4
possible alternatives and in the other 6. The origi-
nal sentence that had 4 alternatives is given in (2).
The four alternatives that participants were asked
to choose from are given in Table 4, with the fre-
quency of each choice. The original sentence that
had 6 alternatives is given in (3). The six alterna-
tives generated by the grammar and the frequen-
cies with which they were chosen is given in Table
5.
(2) Die
The
Brandursache
cause of fire
blieb
remained
zuna?chst
initially
unbekannt.
unknown.
?The cause of the fire remained unknown initially.?
Alternative Freq.
Zuna?chst blieb die Brandursache unbekannt. 2
Die Brandursache blieb zuna?chst unbekannt. 24
Unbekannt blieb die Brandursache zuna?chst. 1
Unbekannt blieb zuna?chst die Brandursache. 1
Table 4: The 4 alternatives given by the grammar
for (2) and their frequencies
Tables 4 and 5 tell different stories. On the one
hand, although each of the 4 alternatives was cho-
sen at least once from Table 4, there is a clear pref-
erence for one string (and this is also the origi-
nal string from the TIGER Corpus). On the other
hand, there is no clear preference9 for any one of
the alternatives in Table 5, and, in fact, the alterna-
tive that was selected most frequently by the par-
ticipants is not the original string. Interestingly,
out of the 41 items presented to participants, the
original string was chosen by the majority of par-
ticipants in 36 cases. Again, this confirms the
hypothesis that there is a certain amount of ac-
ceptable variation for native speakers but there are
clear preferences for certain strings over others.
9Although it is clear that alternative 2 is dispreferred.
116
(3) Die
The
Unternehmensgruppe
group of companies
Tengelmann
Tengelmann
fo?rdert
assists
mit
with
einem
a
sechsstelligen
6-figure
Betrag
sum
die
the
Arbeit
work
im
in
brandenburgischen
of-Brandenburg
Biospha?renreservat
biosphere reserve
Schorfheide.
Schorfheide.
?The Tengelmann group of companies is supporting the work at the biosphere reserve in Schorfheide, Brandenburg,
with a 6-figure sum.?
Alternative Freq.
Mit einem sechsstelligen Betrag fo?rdert die Unternehmensgruppe Tengelmann die Arbeit im brandenburgischen
Biospha?renreservat Schorfheide. 7
Mit einem sechsstelligen Betrag fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheide
die Unternehmensgruppe Tengelmann. 1
Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert die Unternehmensgruppe Tengelmann
mit einem sechsstelligen Betrag. 4
Die Arbeit im brandenburgischen Biospha?renreservat Schorfheide fo?rdert mit einem sechsstelligen Betrag
die Unternehmensgruppe Tengelmann. 5
Die Unternehmensgruppe Tengelmann fo?rdert die Arbeit im brandenburgischen Biospha?renreservat Schorfheide
mit einem sechsstelligen Betrag. 5
Die Unternehmensgruppe Tengelmann fo?rdert mit einem sechsstelligen Betrag die Arbeit im brandenburgischen
Biospha?renreservat Schorfheide. 5
Table 5: The 6 alternatives given by the grammar for (3) and their frequencies
4.3 Effects of context
As explained in Section 3.1, Part 3 of our exper-
iment was identical to Part 1, except that the par-
ticipants could see some preceding context. The
aim of this part was to investigate to what extent
discourse factors influence the way in which hu-
man judges evaluate the output of the realisation
ranker. In Task 3a, we expected the original strings
to be ranked (even) higher in context than out of
context; consequently, the ranks of the realisations
selected by the log-linear and the language model
would have to go down. With respect to Task 3b,
we had no particular expectation, but were just in-
terested in seeing whether some preceding context
would affect the evaluation results for the strings
selected as most probable by the log-linear model
ranker in any way.
Table 6 summarises the results of Task 3a. It
shows that, at least overall, our expectation that the
original corpus sentences would be ranked higher
within context than out of context was not borne
out. Actually, they were ranked a bit lower than
they were when presented in isolation, and the
only realisations that are ranked slightly higher
overall are the ones selected by the trigram LM.
The overall results of Task 3b are presented in
Figure 7. Interestingly, although we did not ex-
pect any particular effect of preceding context on
the way the participants would rate the realisa-
tions selected by the log-linear model, the natu-
ralness scores were higher in the condition with
context (Task 3b) than in the one without context
Total Average
Rank 1 Rank 2 Rank 3 Rank
Original String 810 365 71 1.41
(-7) (-1) (+6) (+0.01)
LL String 274 615 357 2.07
(-29) (+22) (+5) (+0.03)
LM String 162 266 818 2.53
(+34) (-23) (-13) (-0.03)
Table 6: Task 3a: Ranks for each system (com-
pared to ranks in Task 1a)
(Task 1b). One explanation might be that sen-
tences in some sort of default order are generally
rated higher in context than out of context, simply
because the context makes sentences less surpris-
ing.
Since, contrary to our expectations, we could
not detect a clear effect of context in the overall re-
sults of Task 3a, we investigated how the average
ranks of the three alternatives presented for indi-
vidual items differ between Task 1a and Task 3a.
An example of an original corpus sentence which
many participants ranked higher in context than in
isolation is given in (4a.). The realisations selected
by the the log-linear model and the trigram LM are
given in (4b.) and (4c.) respectively, and the con-
text shown to the participants is given above these
alternatives. We believe that the context has this
effect because it prepares the reader for the struc-
ture with the sentence-initial predicative partici-
ple entscheidend; usually, these elements appear
rather in clause-final position.
In contrast, (5a) is an example of a corpus
117
(4) -2 Betroffen
Concerned
sind
are
die
the
Antibabypillen
contraceptive pills
Femovan,
Femovan,
Lovelle,
Lovelle,
[...]
[...],
und
and
Dimirel.
Dimirel.
-1 Das
The
Bundesinstitut
federal institute
schlie?t
excludes
nicht
not
aus, da?
that
sich die
the
Thrombose-Warnung
thrombosis warning
als
as
grundlos
unfounded
erweisen
turn out
ko?nnte.
could.
a. Entscheidend
Decisive
sei
is
die
the
[...]
[...]
abschlie?ende
final
Bewertung,
evaluation,
sagte
said
Ju?rgen
Ju?rgen
Beckmann
Beckmann
vom
of the
Institut
institute
dem
the
ZDF.
ZDF.
b. Die [...] abschlie?ende Bewertung sei entscheidend, sagte Ju?rgen Beckmann vom Institut dem ZDF.
c. Die [...] abschlie?ende Bewertung sei entscheidend, sagte dem ZDF Ju?rgen Beckmann vom Institut.
(5) -2 Im
In the
konkreten
concrete
Fall
case
darf
may
der
the
Kurde
Kurd
allerdings
however
trotz
despite
der
the
Entscheidung
decision
der
of the
Bundesrichter
federal judges
nicht
not
in
to
die
the
Tu?rkei
Turkey
abgeschoben
deported
werden,
be
weil
because
ihm
him
dort
there
nach
according to
den
the
Feststellungen
conclusions
der
of the
Vorinstanz
court of lower instance
politische
political
Verfolgung
persecution
droht.
threatens.
-1 Es
It
besteht
exists
Abschiebeschutz
deportation protection
nach
according to
dem
the
Ausla?ndergesetz.
foreigner law.
a. Der
The
9.
9th
Senat
senate
[...]
[...]
a?u?erte
expressed
sich
itself
in
in
seiner
its
Entscheidung
decision
nicht
not
zur
to the
Verfassungsgema??heit
constitutionality
der
of the
Drittstaatenregelung.
third-country rule.
b. In seiner Entscheidung a?u?erte sich der 9. Senat [...] nicht zur Verfassungsgema??heit der Drittstaatenregelung.
c. Der 9. Senat [...] a?u?erte sich in seiner Entscheidung zur Verfassungsgema??heit der Drittstaatenregelung nicht.
Figure 7: Tasks 1b and 3b: Naturalness scores
for strings chosen by log-linear model, presented
without and with context
sentence which our participants tended to rank
lower in context than in isolation. Actually, the
human judges preferred the realisation selected
by the trigram LM to the original sentence and
the realisation chosen by the log-linear model in
both conditions, but this preference was even re-
inforced when context was available. One expla-
nation might be that the two preceding sentences
are precisely about the decision to which the ini-
tial phrase of variant (5b) refers, which ensures a
smooth flow of the discourse.
4.4 Inter-Annotator Agreement
We measure two types of annotator agreement.
First we measure how well each annotator agrees
with him/herself. This is done by evaluating what
percentage of the time an annotator made the same
choice when presented with the same item choices
(recall that as described in Section 3, a number of
items were presented randomly more than once to
each participant). The results are given in Table 7.
The results show that in between 70% and 74% of
cases, judges make the same decision when pre-
sented with the same data. We found this to be a
surprisingly low number and think that it is most
likely due to the acceptable variation in word or-
der for speakers. Another measure of agreement
is how well the individual participants agree with
each other. In order to establish this, we cal-
culate an average Spearman?s correlation coeffi-
cient (non-parametric Pearson?s correlation coef-
ficient) between each participant for each experi-
ment. The results are summarised in Table 8. Al-
though these figures indicate a high level of inter-
annotator agreement, more tests are required to es-
tablish exactly what these figures mean for each
experiment.
5 Related Work
The work that is most closely related to what is
presented in this paper is that of Velldal (2008). In
118
Experiment Agreement (%)
Part 1a 77.43
Part 1b 71.05
Part 2 74.32
Part 3a 72.63
Part 3b 70.89
Table 7: How often did a participant make the
same choice?
Experiment Spearman coefficient
Part 1a 0.62
Part 1b 0.60
Part 2 0.58
Part 3a 0.61
Part 3b 0.51
Table 8: Inter-Annotator Agreement for each ex-
periment
his thesis several models of realisation ranking are
presented and evaluated against the original cor-
pus text. Chapter 8 describes a small human-based
experiment, where 7 native English speakers rank
the output of 4 systems. One system is the orig-
inal text, another is a randomly chosen baseline,
another is a string chosen by a log-linear model
and the fourth is one chosen by a language model.
Joint rankings were allowed. The results presented
in Velldal (2008) mirror our findings in Exper-
iments 1a and 3a, that native speakers rank the
original strings higher than the log-linear model
strings which are ranked higher than the language
model strings. In both cases, the log-linear mod-
els include the language model score as a feature
in the log-linear model. Nakanishi et al (2005) re-
port that they achieve the best BLEU scores when
they do not include the language model score in
their log-linear model, but they also admit that
their language model was not trained on enough
data.
Belz and Reiter (2006) carry out a comparison
of automatic evaluation metrics against human do-
main experts and human non-experts in the do-
main of weather forecast statements. In their eval-
uations, the NIST score correlated more closely
than BLEU or ROUGE to the human judgements.
They conclude that more than 4 reference texts are
needed for automatic evaluation of NLG systems.
6 Conclusion and Outlook to Future
Work
In this paper, we have presented a human-based
experiment to evaluate the output of a realisation
ranking system for German. We evaluated the
original corpus text, and strings chosen by a lan-
guage model and a log-linear model. We found
that, at a global level, the human judgements mir-
rored the relative rankings of the three system ac-
cording to the BLEU score. In terms of natural-
ness, the strings chosen by the log-linear model
were generally given 4 or 5, indicating that al-
though the log-linear model might not choose the
same string as the original author had written, the
strings it was choosing were mostly very natural
strings.
When presented with all alternatives generated
by the grammar for a given input f-structure, the
human judges chose the same string as the origi-
nal author 70% of the time. In 5 out of 41 cases,
the majority of judges chose a string other than
the original string. These figures show that native
speakers accept some variation in word order, and
so caution should be exercised when using corpus-
derived reference data. The observed acceptable
variation was often linked to information struc-
tural considerations, and further experiments will
be carried out to investigate this relationship be-
tween word order and information structure.
In examining the effect of preceding context, we
found that overall context had very little effect. At
the level of individual sentences, however, clear
tendencies were observed, but there were some
sentences which were judged better in context and
others which were ranked lower. This again indi-
cates that corpus-derived reference data should be
used with caution.
An obvious next step is to examine how well
automatic metrics correlate with the human judge-
ments collected, not only at an individual sen-
tence level, but also at a global level. This can be
done using statistical techniques to correlate the
human judgements with the scores from the auto-
matic metrics. We will also examine the sentences
that were consistently judged to be of poor quality,
so that we can provide feedback to the developers
of the log-linear model in terms of possible addi-
tional features for disambiguation.
Acknowledgments
We are extremely grateful to all of our participants
for taking part in this experiment. This work was
partly funded by the Collaborative Research Cen-
tre (SFB 732) at the University of Stuttgart.
119
References
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation. In
Proceedings of the First International Natural Lan-
guage Generation Conference (INLG2000), pages
1?8, Mitzpe Ramon, Israel.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 313?320, Trento, Italy.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word Or-
der Language. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 17?24, Saarbru?cken, Germany, June. DFKI
GmbH. Document D-07-01.
Charles Callaway. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. In Proceedings
of the 18th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2003), pages 811?817, Aca-
pulco, Mexico.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic models for disambiguation
of an HPSG-based chart generator. In Proceedings
of IWPT 2005.
Ehud Reiter and Somayajulu Sripada. 2002. Should
Corpora Texts Be Gold Standards for NLG? In Pro-
ceedings of INLG-02, pages 97?104, Harriman, NY.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Erik Velldal and Stephan Oepen. 2006. Statistical
ranking in tactical generation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo.
120
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 264?272,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
TBL-Improved Non-Deterministic Segmentation and POS Tagging for a
Chinese Parser
Martin Forst & Ji Fang
Intelligent Systems Laboratory
Palo Alto Research Center
Palo Alto, CA 94304, USA
{mforst|fang}@parc.com
Abstract
Although a lot of progress has been made
recently in word segmentation and POS
tagging for Chinese, the output of cur-
rent state-of-the-art systems is too inaccu-
rate to allow for syntactic analysis based
on it. We present an experiment in im-
proving the output of an off-the-shelf mod-
ule that performs segmentation and tag-
ging, the tokenizer-tagger from Beijing
University (PKU). Our approach is based
on transformation-based learning (TBL).
Unlike in other TBL-based approaches to
the problem, however, both obligatory and
optional transformation rules are learned,
so that the final system can output multi-
ple segmentation and POS tagging anal-
yses for a given input. By allowing for
a small amount of ambiguity in the out-
put of the tokenizer-tagger, we achieve a
very considerable improvement in accu-
racy. Compared to the PKU tokenizer-
tagger, we improve segmentation F-score
from 94.18% to 96.74%, tagged word
F-score from 84.63% to 92.44%, seg-
mented sentence accuracy from 47.15%
to 65.06% and tagged sentence accuracy
from 14.07% to 31.47%.
1 Introduction
Word segmentation and tagging are the neces-
sary initial steps for almost any language process-
ing system, and Chinese parsers are no exception.
However, automatic Chinese word segmentation
and tagging has been recognized as a very difficult
task (Sproat and Emerson, 2003), for the follow-
ing reasons:
First, Chinese text provides few cues for word
boundaries (Xia, 2000; Wu, 2003) and part-of-
speech (POS) information. With the exception of
punctuation marks, Chinese does not have word
delimiters such as the whitespace used in English
text, and unlike other languages without whites-
paces such as Japanese, Chinese lacks morpholog-
ical inflections that could provide cues for word
boundaries and POS information. In fact, the lack
of word boundary marks and morphological in-
flection contributes not only to mistakes in ma-
chine processing of Chinese; it has also been iden-
tified as a factor for parsing miscues in Chinese
children?s reading behavior (Chang et al, 1992).
Second, in addition to the two problems de-
scribed above, segmentation and tagging also suf-
fer from the fact that the notion of a word is
very unclear in Chinese (Xu, 1997; Packard, 2000;
Hsu, 2002). While the word is an intuitive and
salient notion in English, it is by no means a
clear notion in Chinese. Instead, for historical
reasons, the intuitive and clear notion in Chinese
language and culture is the character rather than
the word. Classical Chinese is in general mono-
syllabic, with each syllable corresponding to an
independent morpheme that can be visually ren-
dered with a written character. In other words,
characters did represent the basic syntactic unit in
Classical Chinese, and thus became the sociolog-
ically intuitive notion. However, although collo-
quial Chinese quickly evolved throughout Chinese
history to be disyllabic or multi-syllabic, monosyl-
labic Classical Chinese has been considered more
elegant and proper and was commonly used in
written text until the early 20th century in China.
Even in Modern Chinese written text, Classical
Chinese elements are not rare. Consequently, even
if a morpheme represented by a character is no
264
longer used independently in Modern colloquial
Chinese, it might still appear to be a free mor-
pheme in modern written text, because it contains
Classical Chinese elements. This fact leads to a
phenomenon in which Chinese speakers have dif-
ficulty differentiating whether a character repre-
sents a bound or free morpheme, which in turn
affects their judgment regarding where the word
boundaries should be. As pointed out by Hoosain
(Hoosain, 1992), the varying knowledge of Classi-
cal Chinese among native Chinese speakers in fact
affects their judgments about what is or is not a
word. In summary, due to the influence of Classi-
cal Chinese, the notion of a word and the bound-
ary between a bound and free morpheme is very
unclear for Chinese speakers, which in turn leads
to a fuzzy perception of where word boundaries
should be.
Consequently, automatic segmentation and tag-
ging in Chinese faces a serious challenge from
prevalent ambiguities. For example 1, the string
????? can be segmented as (1a) or (1b), de-
pending on the context.
(1) a. ? ??
yo?u y?`jian
have disagreement
b. ?? ?
yo?uy?` jia`n
have the intention meet
The contrast shown in (2) illustrates that even a
string that is not ambiguous in terms of segmenta-
tion can still be ambiguous in terms of tagging.
(2) a. ?/a ?/n
ba?i hua?
white flower
b. ?/d ?/v
ba?i hua?
in vain spend
?spend (money, time, energy etc.) in vain?
Even Chinese speakers cannot resolve such am-
biguities without using further information from
a bigger context, which suggests that resolving
segmentation and tagging ambiguities probably
should not be a task or goal at the word level. In-
stead, we should preserve such ambiguities in this
level and leave them to be resolved in a later stage,
when more information is available.
1(1) and (2) are cited from (Fang and King, 2007)
To summarize, the word as a notion and hence
word boundaries are very unclear; segmentation
and tagging are prevalently ambiguous in Chinese.
These facts suggest that Chinese segmentation and
part-of-speech identification are probably inher-
ently non-deterministic at the word level. How-
ever most of the current segmentation and/or tag-
ging systems output a single result.
While a deterministic approach to Chinese seg-
mentation and POS tagging might be appropriate
and necessary for certain tasks or applications, it
has been shown to suffer from a problem of low
accuracy. As pointed out by Yu (Yu et al, 2004),
although the segmentation and tagging accuracy
for certain types of text can reach as high as 95%,
the accuracy for open domain text is only slightly
higher than 80%. Furthermore, Chinese segmenta-
tion (SIGHAN) bakeoff results also show that the
performance of the Chinese segmentation systems
has not improved a whole lot since 2003. This
fact also indicates that deterministic approaches
to Chinese segmentation have hit a bottleneck in
terms of accuracy.
The system for which we improved the output
of the Beijing tokenizer-tagger is a hand-crafted
Chinese grammar. For such a system, as proba-
bly for any parsing system that presupposes seg-
mented (and tagged) input, the accuracy of the
segmentation and POS tagging analyses is criti-
cal. However, as described in detail in the fol-
lowing section, even current state-of-art systems
cannot provide satisfactory results for our ap-
plication. Based on the experiments presented
in section 3, we believe that a proper amount
of non-deterministic results can significantly im-
prove the Chinese segmentation and tagging accu-
racy, which in turn improves the performance of
the grammar.
2 Background
The improved tokenizer-tagger we developed is
part of a larger system, namely a deep Chinese
grammar (Fang and King, 2007). The system
is hybrid in that it uses probability estimates for
parse pruning (and it is planned to use trained
weights for parse ranking), but the ?core? gram-
mar is rule-based. It is written within the frame-
work of Lexical Functional Grammar (LFG) and
implemented on the XLE system (Crouch et al,
2006; Maxwell and Kaplan, 1996). The input to
our system is a raw Chinese string such as (3).
265
(3)
?? ? ? ?
xia?owa?ng zo?u le .
XiaoWang leave ASP 2 .
?XiaoWang left.?
The output of the Chinese LFG consists of a
Constituent Structure (c-structure) and a Func-
tional Structure (f-structure) for each sentence.
While c-structure represents phrasal structure and
linear word order, f-structure represents various
functional relations between parts of sentences.
For example, (4) and (5) are the c-structure and f-
structure that the grammar produces for (3). Both
c-structure and f-structure information are carried
in syntactic rules in the grammar.
(4) c-structure of (3)
(5) f-structure of (3)
To parse a sentence, the Chinese LFG min-
imally requires three components: a tokenizer-
tagger, a lexicon, and syntactic rules. The
tokenizer-tagger that is currently used in the gram-
mar is developed by Beijing University (PKU)3
and is incorporated as a library transducer (Crouch
et al, 2006).
Because the grammar?s syntactic rules are ap-
plied based upon the results produced by the
tokenizer-tagger, the performance of the latter is
2ASP stands for aspect marker.
3http://www.icl.pku.edu.cn/icl res/
critical to overall quality of the system?s out-
put. However, even though PKU?s tokenizer-
tagger is one of the state-of-art systems, its per-
formance is not satisfactory for the Chinese LFG.
This becomes clear from a small-scale evaluation
in which the system was tested on a set of 101
gold sentences chosen from the Chinese Treebank
5 (CTB5) (Xue et al, 2002; Xue et al, 2005).
These 101 sentences are 10-20 words long and
all of them are chosen from Xinhua sources 4.
Based on the deterministic segmentation and tag-
ging results produced by PKU?s tokenizer-tagger,
the Chinese LFG can only parse 80 out of the
101 sentences. Among the 80 sentences that are
parsed, 66 received full parses and 14 received
fragmented parses. Among the 21 completely
failed sentences, 20 sentences failed due to seg-
mentation and tagging mistakes.
This simple test shows that in order for the
deep Chinese grammar to be practically useful,
the performance of the tokenizer-tagger must be
improved. One way to improve the segmentation
and tagging accuracy is to allow non-deterministic
segmentation and tagging for Chinese for the rea-
sons stated in Section 1. Therefore, our goal
is to find a way to transform PKU?s tokenizer-
tagger into a system that produces a proper amount
of non-deterministic segmentation and tagging re-
sults, one that can significantly improve the sys-
tem?s accuracy without a substantial sacrifice in
terms of efficiency. Our approach is described in
the following section.
3 FST5 Rules for the Improvement of
Segmentation and Tagging Output
For grammars of other languages implemented on
the XLE grammar development platform, the in-
put is usually preprocessed by a cascade of gener-
ally non-deterministic finite state transducers that
perform tokenization, morphological analysis etc.
Since word segmentation and POS tagging are
such hard problems in Chinese, this traditional
setup is not an option for the Chinese grammar.
However, finite state rules seem a quite natural ap-
proach to improving in XLE the output of a sep-
4The reason why only sentences from Xinhua sources
were chosen is because the version of PKU?s tokenizer-tagger
that was integrated into the system was not designed to han-
dle data from Hong Kong and Taiwan.
5We use the abbreviation ?FST? for ?finite-state trans-
ducer?. fst is used to refer to the finite-state tool called fst,
which was developed by Beesley and Karttunen (2003).
266
arate segmentation and POS tagging module like
PKU?s tokenizer-tagger.
3.1 Hand-Crafted FST Rules for Concept
Proving
Although the grammar developer had identified
PKU?s tokenizer-tagger as the most suitable for
the preprocessing of Chinese raw text that is
to be parsed with the Chinese LFG, she no-
ticed in the process of development that (i) cer-
tain segmentation and/or tagging decisions taken
by the tokenizer-tagger systematically go counter
her morphosyntactic judgment and that (ii) the
tokenizer-tagger (as any software of its kind)
makes mistakes. She therefore decided to develop
a set of finite-state rules that transform the output
of the module; a set of mostly obligatory rewrite
rules adapts the POS-tagged word sequence to the
grammar?s standard, and another set of mostly op-
tional rules tries to offer alternative segment and
tag sequences for sequences that are frequently
processed erroneously by PKU?s tokenizer-tagger.
Given the absence of data segmented and tagged
according to the standard the LFG grammar de-
veloper desired, the technique of hand-crafting
FST rules to postprocess the output of PKU?s
tokenizer-tagger worked surprisingly well. Re-
call that based on the deterministic segmentation
and tagging results produced by PKU?s tokenizer-
tagger, our system can only parse 80 out of the 101
sentences, and among the 21 completely failed
sentences, 20 sentences failed due to segmenta-
tion and tagging mistakes. In contrast, after the
application of the hand-crafted FST rules for post-
processing, 100 out of the 101 sentences can be
parsed. However, this approach involved a lot
of manual development work (about 3-4 person
months) and has reached a stage where it is dif-
ficult to systematically work on further improve-
ments.
3.2 Machine-Learned FST Rules
Since there are large amounts of training data that
are close to the segmentation and tagging standard
the grammar developer wants to use, the idea of
inducing FST rules rather than hand-crafting them
comes quite naturally. The easiest way to do this
is to apply transformation-based learning (TBL) to
the combined problem of Chinese segmentation
and POS tagging, since the cascade of transfor-
mational rules learned in a TBL training run can
straightforwardly be translated into a cascade of
FST rules.
3.2.1 Transformation-Based Learning and
?-TBL
TBL is a machine learning approach that has been
employed to solve a number of problems in nat-
ural language processing; most famously, it has
been used for part-of-speech tagging (Brill, 1995).
TBL is a supervised learning approach, since it re-
lies on gold-annotated training data. In addition,
it relies on a set of templates of transformational
rules; learning consists in finding a sequence of in-
stantiations of these templates that minimizes the
number of errors in a more or less naive base-line
output with respect to the gold-annotated training
data.
The first attempts to employ TBL to solve the
problem of Chinese word segmentation go back to
Palmer (1997) and Hockenmaier and Brew (1998).
In more recent work, TBL was used for the adap-
tion of the output of a statistical ?general pur-
pose? segmenter to standards that vary depend-
ing on the application that requires sentence seg-
mentation (Gao et al, 2004). TBL approaches to
the combined problem of segmenting and POS-
tagging Chinese sentences are reported in Florian
and Ngai (2001) and Fung et al (2004).
Several implementations of the TBL approach
are freely available on the web, the most well-
known being the so-called Brill tagger, fnTBL,
which allows for multi-dimensional TBL, and
?-TBL (Lager, 1999). Among these, we chose
?-TBL for our experiments because (like fnTBL)
it is completely flexible as to whether a sample
is a word, a character or anything else and (un-
like fnTBL) it allows for the induction of optional
rules. Probably due to its flexibility, ?-TBL has
been used (albeit on a small scale for the most part)
for tasks as diverse as POS tagging, map tasks, and
machine translation.
3.2.2 Experiment Set-up
We started out with a corpus of thirty gold-
segmented and -tagged daily editions of the Xin-
hua Daily, which were provided by the Institute
of Computational Linguistics at Beijing Univer-
sity. Three daily editions, which comprise 5,054
sentences with 129,377 words and 213,936 char-
acters, were set aside for testing purposes; the re-
maining 27 editions were used for training. With
the idea of learning both obligatory and optional
267
transformational rules in mind, we then split the
training data into two roughly equally sized sub-
sets. All the data were broken into sentences us-
ing a very simple method: The end of a para-
graph was always considered a sentence bound-
ary. Within paragraphs, sentence-final punctua-
tion marks such as periods (which are unambigu-
ous in Chinese), question marks and exclamation
marks, potentially followed by a closing parenthe-
sis, bracket or quote mark, were considered sen-
tence boundaries.
We then had to come up with a way of cast-
ing the problem of combined segmentation and
POS tagging as a TBL problem. Following a strat-
egy widely used in Chinese word segmentation,
we did this by regarding the problem as a charac-
ter tagging problem. However, since we intended
to learn rules that deal with segmentation and
POS tagging simultaneously, we could not adopt
the BIO-coding approach.6 Also, since the TBL-
induced transformational rules were to be con-
verted into FST rules, we had to keep our character
tagging scheme one-dimensional, unlike Florian
and Ngai (2001), who used a multi-dimensional
TBL approach to solve the problem of combined
segmentation and POS tagging.
The character tagging scheme that we finally
chose is illustrated in (6), where a. and b. show the
character tags that we used for the analyses in (1a)
and (1b) respectively. The scheme consists in tag-
ging the last character of a word with the part-of-
speech of the entire word; all non-final characters
are tagged with ?-?. The main advantages of this
character tagging scheme are that it expresses both
word boundaries and parts-of-speech and that, at
the same time, it is always consistent; inconsisten-
cies between BIO tags indicating word boundaries
and part-of-speech tags, which Florian and Ngai
(2001), for example, have to resolve, can simply
not arise.
(6)
? ? ?
a. v - n
b. - v v
Both of the training data subsets were tagged
according to our character tagging scheme and
6In this character tagging approach to word segmentation,
characters are tagged as the beginning of a word (B), inside
(or at the end) of a multi-character word (I) or a word of their
own (O). Their are numerous variations of this approach.
converted to the data format expected by ?-TBL.
The first training data subset was used for learn-
ing obligatory resegmentation and retagging rules.
The corresponding rule templates, which define
the space of possible rules to be explored, are
given in Figure 1. The training parameters of
?-TBL, which are an accuracy threshold and a
score threshold, were set to 0.75 and 5 respec-
tively; this means that a potential rule was only
retained if at least 75% of the samples to which it
would have applied were actually modified in the
sense of the gold standard and not in some other
way and that the learning process was terminated
when no more rule could be found that applied to
at least 5 samples in the first training data subset.
With these training parameters, 3,319 obligatory
rules were learned by ?-TBL.
Once the obligatory rules had been learned on
the first training data subset, they were applied to
the second training data subset. Then, optional
rules were learned on this second training data
subset. The rule templates used for optional rules
are very similar to the ones used for obligatory
rules; a few templates of optional rules are given in
Figure 2. The difference between obligatory rules
and optional rules is that the former replace one
character tag by another, whereas the latter add
character tags. They hence introduce ambiguity,
which is why we call them optional rules. Like in
the learning of the obligatory rules, the accuracy
threshold used was 0.75; the score theshold was
set to 7 because the training software seemed to
hit a bug below that threshold. 753 optional rules
were learned. We did not experiment with the ad-
justment of the training parameters on a separate
held-out set.
Finally, the rule sets learned were converted into
the fst (Beesley and Karttunen, 2003) notation for
transformational rules, so that they could be tested
and used in the FST cascade used for preprocess-
ing the input of the Chinese LFG. For evaluation,
the converted rules were applied to our test data set
of 5,054 sentences. A few example rules learned
by ?-TBL with the set-up described above are
given in Figure 3; we show them both in ?-TBL
notation and in fst notation.
3.2.3 Results
The results achieved by PKU?s tokenizer-tagger
on its own and in combination with the trans-
formational rules learned in our experiments are
given in Table 1. We compare the output of PKU?s
268
tag:m> - <- wd:???@[0] & wd:???@[1] & "/" m WS @-> 0 || ? _ ? [ ( TAG )
tag:q@[1,2,3,4] & {\+q=(-)}. CHAR ]?{0,3} "/" q WS
tag:r>n <- wd:???@[-1] & wd:???@[0]. "/" r WS @-> "/" n TB || ? ( TAG ) ? _
tag:add nr <- tag:(-)@[0] & wd:???@[1]. [..] (@->) "/" n r TB || CHAR _ ?
... ...
Figure 3: Sample rules learned in our experiments in ?-TBL notation on the left and in fst notation on
the right8
tag:A>B <- ch:C@[0].
tag:A>B <- ch:C@[1].
tag:A>B <- ch:C@[-1] & ch:D@[0].
tag:A>B <- ch:C@[0] & ch:D@[1].
tag:A>B <- ch:C@[1] & ch:D@[2].
tag:A>B <- ch:C@[-2] & ch:D@[-1] &
ch:E@[0].
tag:A>B <- ch:C@[-1] & ch:D@[0] &
ch:E@[1].
tag:A>B <- ch:C@[0] & ch:D@[1] & ch:E@[2].
tag:A>B <- ch:C@[1] & ch:D@[2] & ch:E@[3].
tag:A>B <- tag:C@[-1].
tag:A>B <- tag:C@[1].
tag:A>B <- tag:C@[1] & tag:D@[2].
tag:A>B <- tag:C@[-2] & tag:D@[-1].
tag:A>B <- tag:C@[-1] & tag:D@[1].
tag:A>B <- tag:C@[1] & tag:D@[2].
tag:A>B <- tag:C@[1] & tag:D@[2] &
tag:E@[3].
tag:A>B <- tag:C@[-1] & ch:W@[0].
tag:A>B <- tag:C@[1] & ch:W@[0].
tag:A>B <- tag:C@[1] & tag:D@[2] &
ch:W@[0].
tag:A>B <- tag:C@[-2] & tag:D@[-1] &
ch:W@[0].
tag:A>B <- tag:C@[-1] & tag:D@[1] &
ch:W@[0].
tag:A>B <- tag:C@[1] & tag:D@[2] &
ch:W@[0].
tag:A>B <- tag:C@[1] & tag:D@[2] &
tag:E@[3] & ch:W@[0].
tag:A>B <- tag:C@[-1] & ch:W@[1].
tag:A>B <- tag:C@[1] & ch:W@[1].
tag:A>B <- tag:C@[1] & tag:D@[2] &
ch:W@[1].
tag:A>B <- tag:C@[-2] & tag:D@[-1] &
ch:W@[1].
tag:A>B <- tag:C@[-1] & ch:D@[0] &
ch:E@[1].
tag:A>B <- tag:C@[-1] & tag:D@[1] &
ch:W@[1].
tag:A>B <- tag:C@[1] & tag:D@[2] &
ch:W@[1].
tag:A>B <- tag:C@[1] & tag:D@[2] &
tag:E@[3] & ch:W@[1].
tag:A>B <- tag:C@[1,2,3,4] & {\+C=?-?}.
tag:A>B <- ch:C@[0] & tag:D@[1,2,3,4] &
{\+D=?-?}.
tag:A>B <- tag:C@[-1] & ch:D@[0] &
tag:E@[1,2,3,4] & {\+E=?-?}.
tag:A>B <- ch:C@[0] & ch:D@[1] &
tag:E@[1,2,3,4] & {\+E=?-?}.
Figure 1: Templates of obligatory rules used in our
experiments
tag:add B <- tag:A@[0] & ch:C@[0].
tag:add B <- tag:A@[0] & ch:C@[1].
tag:add B <- tag:A@[0] & ch:C@[-1] &
ch:D@[0].
...
Figure 2: Sample templates of optional rules used
in our experiments
tokenizer-tagger run in the mode where it returns
only the most probable tag for each word (PKU
one tag), of PKU?s tokenizer-tagger run in the
mode where it returns all possible tags for a given
word (PKU all tags), of PKU?s tokenizer-tagger
in one-tag mode augmented with the obligatory
transformational rules learned on the first part of
our training data (PKU one tag + deterministic rule
set), and of PKU?s tokenizer-tagger augmented
with both the obligatory and optional rules learned
on the first and second parts of our training data re-
spectively (PKU one tag + non-deterministic rule
set). We give results in terms of character tag ac-
curacy and ambiguity according to our character
tagging scheme. Then we provide evaluation fig-
ures for the word level. Finally, we give results re-
ferring to the sentence level in order to make clear
how serious a problem Chinese segmentation and
POS tagging still are for parsers, which obviously
operate at the sentence level.
These results show that simply switching from
the one-tag mode of PKU?s tokenizer-tagger to its
all-tags mode is not a solution. First of all, since
the tokenizer-tagger always produces only one
segmentation regardless of the mode it is used in,
segmentation accuracy would stay completely un-
affected by this change, which is particularly seri-
ous because there is no way for the grammar to re-
cover from segmentation errors and the tokenizer-
tagger produces an entirely correct segmentation
only for 47.15% of the sentences. Second, the
improved tagging accuracy would come at a very
heavy price in terms of ambiguity; the median
number of combined segmentation and POS tag-
ging analyses per sentence would be 1,440.
269
In contrast, machine-learned transformation
rules are an effective means to improve the out-
put of PKU?s tokenizer-tagger. Applying only
the obligatory rules that were learned already im-
proves segmented sentence accuracy from 47.15%
to 63.14% and tagged sentence accuracy from
14.07% to 27.21%, and this at no cost in terms
of ambiguity. Adding the optional rules that were
learned and hence making the rule set used for
post-processing the output of PKU?s tokenizer-
tagger non-deterministic makes it possible to im-
prove segmented sentence accuracy and tagged
sentence accuracy further to 65.06% and 31.47%
respectively, i.e. tagged sentence accuracy is more
than doubled with respect to the baseline. While
this last improvement does come at a price in
terms of ambiguity, the ambiguity resulting from
the application of the non-deterministic rule set is
very low in comparison to the ambiguity of the
output of PKU?s tokenizer-tagger in all-tags mode;
the median number of analyses per sentences only
increases to 2. Finally, it should be noted that
the transformational rules provide entirely correct
segmentation and POS tagging analyses not only
for more sentences, but also for longer sentences.
They increase the average length of a correctly
segmented sentence from 18.22 words to 21.94
words and the average length of a correctly seg-
mented and POS-tagged sentence from 9.58 words
to 16.33 words.
4 Comparison to related work and
Discussion
Comparing our results to other results in the liter-
ature is not an easy task because segmentation and
POS tagging standards vary, and our test data have
not been used for a final evaluation before. Nev-
ertheless, there are of course systems that perform
word segmentation and POS tagging for Chinese
and have been evaluated on data similar to our test
data.
Published results also vary as to the evalua-
tion measures used, in particular when it comes
to combined word segmentation and POS tag-
ging. For word segmentation considered sepa-
rately, the consensus is to use the (segmentation)
F-score (SF). The quality of systems that perform
both segmentation and POS tagging is often ex-
pressed in terms of (character) tag accuracy (TA),
but this obviously depends on the character tag-
ging scheme adopted. An alternative measure is
POS tagging F-score (TF), which is the geomet-
ric mean of precision and recall of correctly seg-
mented and POS-tagged words. Evaluation mea-
sures for the sentence level have not been given in
any publication that we are aware of, probably be-
cause segmenters and POS taggers are rarely con-
sidered as pre-processing modules for parsers, but
also because the figures for measures like sentence
accuracy are strikingly low.
For systems that perform only word segmenta-
tion, we find the following results in the literature:
(Gao et al, 2004), who use TBL to adapt a ?gen-
eral purpose? segmenter to varying standards, re-
port an SF of 95.5% on PKU data and an SF of
90.4% on CTB data. (Tseng et al, 2005) achieve
an SF of 95.0%, 95.3% and 86.3% on PKU data
from the Sighan Bakeoff 2005, PKU data from
the Sighan Bakeoff 2003 and CTB data from the
Sighan Bakeoff 2003 respectively. Finally, (Zhang
et al, 2006) report an SF of 94.8% on PKU data.
For systems that perform both word segmenta-
tion and POS tagging, the following results were
published: Florian and Ngai (2001) report an SF
of 93.55% and a TA of 88.86% on CTB data.
Ng and Low (2004) report an SF of 95.2% and
a TA of 91.9% on CTB data. Finally, Zhang and
Clark (2008) achieve an SF of 95.90% and a TF
of 91.34% by 10-fold cross validation using CTB
data.
Last but not least, there are parsers that oper-
ate on characters rather than words and who per-
form segmentation and POS tagging as part of the
parsing process. Among these, we would like to
mention Luo (2003), who reports an SF 96.0%
on Chinese Treebank (CTB) data, and (Fung et
al., 2004), who achieve ?a word segmentation pre-
cision/recall performance of 93/94%?. Both the
SF and the TF results achieved by our ?PKU one
tag + non-deterministic rule set? setup, whose out-
put is slightly ambiguous, compare favorably with
all the results mentioned, and even the results
achieved by our ?PKU one tag + deterministic rule
set? setup are competitive.
5 Conclusions and Future Work
The idea of carrying some ambiguity from one
processing step into the next in order not to prune
good solutions is not new. E.g., Prins and van No-
ord (2003) use a probabilistic part-of-speech tag-
ger that keeps multiple tags in certain cases for
a hand-crafted HPSG-inspired parser for Dutch,
270
PKU PKU PKU one tag + PKU one tag +
one tag all tags det. rule set non-det. rule set
Character tag accuracy (in %) 89.98 92.79 94.69 95.27
Avg. number of tags per char. 1.00 1.39 1.00 1.03
Avg. number of words per sent. 26.26 26.26 25.77 25.75
Segmented word precision (in %) 93.00 93.00 96.18 96.46
Segmented word recall (in %) 95.39 95.39 96.84 97.02
Segmented word F-score (in %) 94.18 94.18 96.51 96.74
Tagged word precision (in %) 83.57 87.87 91.27 92.17
Tagged word recall (in %) 85.72 90.23 91.89 92.71
Tagged word F-score (in %) 84.63 89.03 91.58 92.44
Segmented sentence accuracy (in %) 47.15 47.15 63.14 65.06
Avg. nmb. of words per correctly segm. sent. 18.22 18.22 21.69 21.94
Tagged sentence accuracy (in %) 14.07 21.09 27.21 31.47
Avg. number of analyses per sent. 1.00 4.61e18 1.00 12.84
Median nmb. of analyses per sent. 1 1,440 1 2
Avg. nmb. of words per corr. tagged sent. 9.58 13.20 15.11 16.33
Table 1: Evaluation figures achieved by four different systems on the 5,054 sentences of our test set
and Curran et al (2006) show the benefits of us-
ing a multi-tagger rather than a single-tagger for
an induced CCG for English. However, to our
knowledge, this idea has not made its way into
the field of Chinese parsing so far. Chinese pars-
ing systems either pass on a single segmentation
and POS tagging analysis to the parser proper or
they are character-based, i.e. segmentation and
tagging are part of the parsing process. Although
several treebank-induced character-based parsers
for Chinese have achieved promising results, this
approach is impractical in the development of a
hand-crafted deep grammar like the Chinese LFG.
We therefore believe that the development of a
?multi-tokenizer-tagger? is the way to go for this
sort of system (and all systems that can handle a
certain amount of ambiguity that may or may not
be resolved at later processing stages). Our results
show that we have made an important first step in
this direction.
As to future work, we hope to resolve the prob-
lem of not having a gold standard that is seg-
mented and tagged exactly according to the guide-
lines established by the Chinese LFG developer
by semi-automatically applying the hand-crafted
transformational rules that were developed to the
PKU gold standard. We will then induce obliga-
tory and optional FST rules from this ?grammar-
compliant? gold standard and hope that these will
be able to replace the hand-crafted transformation
rules currently used in the grammar. Finally, we
plan to carry out more training runs; in particu-
lar, we intend to experiment with lower accuracy
(and score) thresholds for optional rules. The idea
is to find the optimal balance between ambigu-
ity, which can probably be higher than with our
current set of induced rules without affecting ef-
ficiency too adversely, and accuracy, which still
needs further improvement, as can easily be seen
from the sentence accuracy figures.
References
Kenneth R. Beesley and Lauri Karttunen. 2003. Fi-
nite State Morphology. CSLI Publications, Stan-
ford, CA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
J.M Chang, D.L. Hung, and O.J.L. Tzeng. 1992. Mis-
cue analysis of chinese children?s reading behavior
at the entry level. Journal of Chinese Linguistics,
20(1).
Dick Crouch, Mary Dalrymple, Ron Kaplan,
Tracy Holloway King, John Maxwell, and
Paula Newman. 2006. XLE documentation.
http://www2.parc.com/isl/groups/nltt/xle/doc/.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-Tagging for Lexicalized-Grammar
Parsing. In In Proceedings of COLING/ACL-06,
pages 697?704, Sydney, Australia.
Ji Fang and Tracy Holloway King. 2007. An lfg chi-
nese grammar for machine use. In Tracy Holloway
271
King and Emily M. Bender, editors, Proceedings of
the GEAF 2007 Workshop. CSLI Studies in Compu-
tational Linguistics ONLINE.
Radu Florian and Grace Ngai. 2001. Multidimen-
sional transformation-based learning. In CoNLL
?01: Proceedings of the 2001 workshop on Com-
putational Natural Language Learning, pages 1?8,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy Chinese
parser augmented by transformation-based learning.
ACM Transactions on Asian Language Information
Processing (TALIP), 3(2):159?168.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive Chinese word segmentation. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics, page 462, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Julia Hockenmaier and Chris Brew. 1998. Error-
Driven Segmentation of Chinese. International
Journal of the Chinese and Oriental Languages In-
formation Processing Society, 8(1):69??84.
R. Hoosain. 1992. Psychological reality of the
word in chinese. In H.-C. Chen and O.J.L. Tzeng,
editors, Language Processing in Chinese. North-
Holland and Elsevier, Amsterdam.
Kylie Hsu. 2002. Selected Issues in Mandarin Chinese
Word Structure Analysis. The Edwin Mellen Press,
Lewiston, New York, USA.
Torbjo?rn Lager. 1999. The ?-TBL System: Logic Pro-
gramming Tools for Transformation-Based Learn-
ing. In Proceedings of the Third International Work-
shop on Computational Natural Language Learning
(CoNLL?99), Bergen.
Xiaoqiang Luo. 2003. A Maximum Entropy Chinese
Character-Based Parser. In Michael Collins and
Mark Steedman, editors, Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 192?199.
John Maxwell and Ron Kaplan. 1996. An efficient
parser for LFG. In Proceedings of the First LFG
Conference. CSLI Publications.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based? . In Dekang
Lin and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 277?284, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics.
Jerome L. Packard. 2000. The Morphology of Chinese.
Cambridge University Press, Cambridge, UK.
David D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proceedings of the
35th annual meeting on Association for Computa-
tional Linguistics, pages 321?328, Morristown, NJ,
USA. Association for Computational Linguistics.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitement
Automatique des Langues, 44(3):121?139.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, pages 133?143.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A
Conditional Random Field Word Segmenter for
SIGHAN Bakeoff 2005. In Proceedings of Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
A.D. Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Interna-
tional Journal of Computational Linguistics and
Chinese Language Processing, 8(1):1?28.
Fei Xia. 2000. The segmentation guidelines for the
penn chinese treebank (3.0). Technical report, Uni-
versity of Pennsylvania.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Proceedings of the 19th. International Con-
ference on Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, pages 207?238.
Tongqiang Xu?????. 1997. On Language??
???. Dongbei Normal University Publishing,
Changchun, China.
Shiwen Yu?????, Baobao Chang ?????,
and Weidong Zhan ?????. 2004. An Intro-
duction of Computational Linguistics ?????
????. Shangwu Yinshuguan Press, Beijing,
China.
Yue Zhang and Stephen Clark. 2008. Joint Word Seg-
mentation and POS Tagging Using a Single Percep-
tron. In Proceedings of ACL-08, Columbus, OH.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based tagging for
confidence-dependent Chinese word segmentation.
In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 961?968, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
272
 
			Proceedings of the 5th Workshop on Important Unresolved Matters, pages 17?24,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Filling Statistics with Linguistics ?
Property Design for the Disambiguation of German LFG Parses
Martin Forst
Institute of Natural Language Processing
University of Stuttgart, Germany
forst@ims.uni-stuttgart.de
Abstract
We present a log-linear model for the disam-
biguation of the analyses produced by a Ger-
man broad-coverage LFG, focussing on the
properties (or features) this model is based
on. We compare this model to an initial
model based only on a part of the proper-
ties provided to the final model and observe
that the performance of a log-linear model
for parse selection depends heavily on the
types of properties that it is based on. In
our case, the error reduction achieved with
the log-linear model based on the extended
set of properties is 51.0% and thus com-
pares very favorably to the error reduction
of 34.5% achieved with the initial model.
1 Introduction
In the development of stochastic disambiguation
modules for ?deep? grammars, relatively much work
has gone into the definition of suitable probability
models and the corresponding learning algorithms.
Property design, on the contrary, has rather been un-
deremphasized, and the properties used in stochas-
tic disambiguation modules are most often presented
only superficially. This paper?s aim is to draw more
attention to property design by presenting linguisti-
cally motivated properties that are used for the dis-
ambiguation of the analyses produced by a German
broad-coverage LFG and by showing that property
design is of crucial importance for the quality of
stochastic models for parse selection.
We present, in Section 2, the system that the dis-
ambiguation module was developed for as well as
the initially used properties. In Section 3, we then
present a selection of the properties that were ex-
pressly designed for the resolution of frequent ambi-
guities in German LFG parses. Section 4 describes
experiments that we carried out with log-linear mod-
els based on the initial set of properties and on an
extended one. Section 5 concludes.
2 Background
2.1 The German ParGram LFG
The grammar for which the log-linear model for
parse selection described in this paper was devel-
oped is the German ParGram LFG (Dipper, 2003;
Rohrer and Forst, 2006). It has been developed with
and for the grammar development and processing
platform XLE (Crouch et al, 2006) and consists of
a symbolic LFG, which can be employed both for
parsing and generation, and a two-stage disambigua-
tion module, the log-linear model being the compo-
nent that carries out the final selection among the
parses that have been retained by an Optimality-
Theoretically inspired prefilter (Frank et al, 2001;
Forst et al, 2005).
The grammar has a coverage in terms of full
parses that exceeds 80% on newspaper corpora. For
sentences out of coverage, it employs the robust-
ness techniques (fragment parsing, ?skimming?) im-
plemented in XLE and described in Riezler et al
(2002), so that 100% of our corpus sentences receive
at least some sort of analysis. A dependency-based
evaluation of the analyses produced by the grammar
on the TiGer Dependency Bank (Forst et al, 2004)
results in an F-score between 80.42% on all gram-
17
matical relations and morphosyntactic features (or
72.59% on grammatical relations only) and 85.50%
(or 79.36%). The lower bound is based on an ar-
bitrary selection among the parses built up by the
symbolic grammar; the upper bound is determined
by the best possible selection.
2.2 Log-linear models for disambiguation
Since Johnson et al (1999), log-linear models of
the following form have become standard as disam-
biguation devices for precision grammars:
P?(x|y) = e
?m
j=1 ?j ?fj(x,y)
?
x??X(y) e
?m
j=1 ?j ?fj(x
?,y)
They are used for parse selection in the English Re-
source Grammar (Toutanova et al, 2002), the En-
glish ParGram LFG (Riezler et al, 2002), the En-
glish Enju HPSG (Miyao and Tsujii, 2002), the
HPSG-inspired Alpino parser for Dutch (Malouf
and van Noord, 2004; van Noord, 2006) and the
English CCG from Edinburgh (Clark and Curran,
2004).
While relatively much work has gone into the
question of how to estimate the property weights
?1 . . . ?m efficiently and accurately on the basis
of (annotated) corpus data, the question of how
to define suitable and informative property func-
tions f1 . . . fm has received relatively little attention.
However, we are convinced that property design is
the possibility of improving log-linear models for
parse selection now that the machine learning ma-
chinery is relatively well established.
2.3 Initially used properties for disambiguation
The first set of properties with which we conducted
experiments was built on the model of the property
set used for the disambiguation of English ParGram
LFG parses (Riezler et al, 2002; Riezler and Vasser-
man, 2004). These properties are defined with the
help of thirteen property templates, which are pa-
rameterized for c-structure categories, f-structure at-
tributes and/or their possible values. The templates
are hardwired in XLE, which allows for a very ef-
ficient extraction of properties based on them from
packed c-/f-structure representations. The downside
of the templates being hardwired, however, is that, at
least at first sight, the property developer is confined
to what the developers of the property templates an-
ticipated as potentially relevant for disambiguation
or, more precisely, for the disambiguation of English
LFG analyses.
The thirteen property templates can be subdi-
vided into c-structure-based property templates and
f-structure-based ones. The c-structure-based prop-
erty templates are:
? cs label <XP>: counts the number of XP
nodes in the c-structure of an analysis.
? cs num children <XP>: counts the num-
ber of children of allXP nodes in a c-structure.
? cs adjacent label <XP> <YP>:
counts the number of XP nodes that immedi-
ately dominate a Y P node.
? cs sub label <XP> <YP>: counts the
number ofXP nodes that dominate a Y P node
(at arbitrary depth).
? cs embedded <XP> <n>: counts the
number of XP nodes that dominate n other
distinct XP nodes (at arbitrary depth).
? cs conj nonpar <n>: counts the number
of coordinated constituents that are not parallel
at the nth level of embedding.
? cs right branch: counts the number of
right children in the c-structure of an analysis.
The f-structure-based property templates are:
? fs attrs <Attr1 ... Attrn>:
counts the number of times that attributes
Attr1 . . . Attrn occur in the f-structure of an
analysis.
? fs attr val <Attr> <Val>: counts
the number of times that the atomic attribute
Attr has the value V al.
? fs adj attrs <Attr1> <Attr2>:
counts the number of times that the com-
plex attribute Attr1 immediately embeds the
attribute Attr2.
? fs subattr <Attr1> <Attr2> counts
the number of times that the complex attribute
Attr1 embeds the attribute Attr2 (at arbitrary
depth).
? lex subcat <Lemma> <SCF1 ...
SCFn>: counts the number of times that
the subcategorizing element Lemma occurs
with one of the subcategorization frames
SCF1 . . . SCFn.
18
? verb arg <Lemma> <GF>: counts the
number of times that the element Lemma sub-
categorizes for the argument GF .
Automatically instantiating these templates for all
c-structure categories, f-structure attributes and val-
ues used in the German ParGram LFG as well as for
all lexical elements present in its lexicon results in
460,424 properties.
3 Property design for the disambiguation
of German LFG parses
Despite the very large number of properties that can
be directly constructed on the basis of the thirteen
property templates provided by XLE, many com-
mon ambiguities in German LFG parses cannot be
captured by any of these.
3.1 Properties that record the relative linear
order of functions
Consider, e.g., the SUBJ-OBJ ambiguity in (1).
(1) [. . . ]
[. . . ]
peilt
aims
[S/O das
the
Management]
management
[O/S ein
a
?sichtbar
?visibly
verbessertes?
improved?
Ergebnis]
result
an.
at.
?[. . . ] the management aims at a ?visibly im-
proved? result.? (TIGER Corpus s20834)
The c-structure is shared by the two readings
of the sentence, so that c-structure-based proper-
ties cannot contribute to the selection of the cor-
rect reading; the only f-structure-based proper-
ties that differ between the two analyses are of
the kinds fs adj attrs SUBJ ADJUNCT and
fs subattr OBJ ADJUNCT, which are only re-
motely, if at all, related to the observed SUBJ-OBJ
ambiguity. The crucial information from the in-
tended reading, namely that the SUBJ precedes the
OBJ, is not captured directly by any of the ini-
tial properties. We therefore introduce a new prop-
erty template that records the linear order of two
grammatical functions and instantiate it for all rel-
evant combinations. The new properties created
this way make it possible to capture the default
order of nominal arguments, which according to
Lenerz (1977) and Uszkoreit (1987) (among others),
is SUBJ, OBJ-TH, OBJ.
Similarly to the SUBJ-OBJ ambiguity just con-
sidered, the ADJUNCT-OBL ambiguity in (2) can-
not at all be resolved on the basis of c-structure-
based properties, and the f-structure-based proper-
ties whose values differ among the two readings
seem only remotely related to the observed ambi-
guity.
(2) [A/O Dagegen]
Against that/In contrast
sprach
spoke
sich
himself
[. . . ]
[. . . ]
Micha
Micha
Guttmann
Guttmann
[O/A fu?r
for
getrennte
separate
Gedenksta?tten]
memorials
aus.
out.
?In contrast, [. . . ] Michael Guttmann argued
for separate memorials.? (s2090)
However, the literature on constituent order in Ger-
man, e.g. Helbig and Buscha (2001), documents
the tendency of ADJUNCT PPs to precede OBL PPs,
which also holds in (2). We therefore introduced
properties that record the relative linear order of AD-
JUNCT PPs and OBL PPs.
3.2 Properties that consider the nature of a
constituent wrt. its function
Although linear order plays a major role in the func-
tional interpretation of case-ambiguous DPs in Ger-
man, it is only one among several ?soft? constraints
involved. The nature of such a DP may actually also
give hints to its grammatical function.
The tendency of SUBJs to be high on the defi-
niteness scale and the animacy scale as well as the
tendency of OBJs to be low on these scales has
mainly been observed in studies on differential ob-
ject/subject marking (see, e.g., Aissen (2003)). Nev-
ertheless, these tendencies also seem to hold in lan-
guages like German, which does not exhibit differ-
ential object/subject marking. In (3), the indefinite
inanimate DP is to be interpreted as the OBJ of the
sentence and the definite human DP, as its SUBJ al-
though the former precedes the latter.
(3) [O/S Nahezu
Nearly
stabile
stable
Preise]
prices
prognostizieren
forecast
[S/O die
the
bayerischen
Bavarian
Experten]
experts
[. . . ]
[. . . ].
?The Bavarian experts forecast nearly stable
prices [. . . ].? (s7357)
19
In order to allow these regularities to be
learned from corpus data, we defined addi-
tional property templates like isDef <GF> and
isHuman <GF>,1 which are instantiated for all rel-
evant grammatical functions.
3.3 Properties for the resolution of attachment
ambiguities concerning extraposed
constituents
A further common ambiguity in German con-
cerns the functional attachment of extraposed con-
stituents, such as relative clauses, dass clauses and
infinitival VPs. In (4), e.g., there is no hard con-
straint that would allow us to determine whether the
relative clause modifies Rolle or Autoversicherung.
(4) Eine
A
zentrale
central
Rolle
role
[. . . ]
[. . . ]
kommt
comes
der
the
Autoversicherung
car insurance
zu,
to,
die
which
ein
a
Fu?nftel
fifth
[. . . ]
[. . . ]
vereinnahmt.
receives.
?There is a central role for the car insurance,
which receives a fifth [. . . ].? (s27539)
In order to allow for an improved resolution of
this kind of attachment ambiguity, we introduced
properties that extract the surface distance of an ex-
traposed constituent to its functional head as well as
properties that record how the functional uncertainty
paths involved in these attachments were instanti-
ated. This way, we hope to extract the information
necessary to model the tendencies observed, e.g., in
Uszkoreit et al (1998).
3.4 Lexicalized properties capturing
dependencies
Inspired by Malouf and van Noord (2004),
we finally also introduced lexicalized proper-
ties capturing dependencies. These are built
on the following property templates: DEP12
<PoS1> <Dep> <PoS2> <Lemma2>, DEP21
<PoS1> <Lemma1> <Dep> <PoS2> and DEP22
<PoS1> <Lemma1> <Dep> <PoS2> <Lemma2>.
These are intended to capture information on the
subcategorization behavior of lexical elements and
on typical collocations.
1Humanness information is imported from GermaNet.
"Eine zentrale Rolle kommt der Autoversicherung zu, die ein F?nftel vereinnahmt."
'zu#kommen<[21:Rolle], [243:Versicherung]>'PRED
'Rolle'PRED
'zentral<[21:Rolle]>'PRED [21:Rolle]SUBJ107ADJUNCT
'vereinnahmen<[434:pro], [528:f?nftel]>'PRED
'pro'PRED434SUBJ
'f?nftel'PRED
'eine'PREDDETSPEC528OBJ [434:pro]PRON-REL [434:pro]TOPIC-REL633
ADJ-REL
'eine'PREDDETSPEC21
SUBJ
'Versicherung'PRED
'Auto'PRED-12MOD
'die'PREDDETSPEC243
OBJ-TH
[21:Rolle]TOPIC191
(a) evaluated as relatively improbable due to negative weight of
DISTANCE-TO-ANTECEDENT %X
"Eine zentrale Rolle kommt der Autoversicherung zu, die ein F?nftel vereinnahmt."
'zu#kommen<[21:Rolle], [243:Versicherung]>'PRED
'Rolle'PRED
'zentral<[21:Rolle]>'PRED [21:Rolle]SUBJ107ADJUNCT
'eine'PREDDETSPEC21
SUBJ
'Versicherung'PRED
'Auto'PRED-12MOD
'vereinnahmen<[434:pro], [528:f?nftel]>'PRED
'pro'PRED434SUBJ
'f?nftel'PRED
'eine'PREDDETSPEC528OBJ [434:pro]PRON-REL [434:pro]TOPIC-REL633
ADJ-REL
'die'PREDDETSPEC243
OBJ-TH
[21:Rolle]TOPIC191
(b) evaluated as more probable
Figure 1: Competing f-structures for (4)
In the case of (5), the property DEP21 common
Anwalt APP proper, which counts the num-
ber of occurrences of the common noun Anwalt
(?lawyer?) that govern a proper name via the depen-
dency APP (close apposition), contributes to the cor-
rect selection among the analyses illustrated in Fig-
ure 2 by capturing the fact that Anwalt is a prototyp-
ical head of a close apposition.2
(5) [. . . ],
[. . . ]
das
which
den
the
Anwalt
lawyer
Klaus
Klaus
Bollig
Bollig
zum
to the
vorla?ufigen
interim
Verwalter
administrator
bestellte.
appointed.
?[. . . ] which appointed lawyer Klaus Bollig as
interim administrator.? (s37596)
2Since we have a list of title nouns available, we might also
introduce a more general property that would count the number
of occurrences of title nouns in general that govern a proper
name via the dependency APP. Note, however, that the nouns
that be heads of APPs comprise not only title nouns, but also
nouns like Abteilung ?department?, Buch ?book?, etc.
20
"das den Anwalt Klaus Bollig zum vorl?ufigen Verwalter bestellte"
'bestellen<[1:pro], [82:Anwalt], [228:Bollig]>'PRED
'pro'PRED1SUBJ
'Anwalt'PRED
'die'PREDDETSPEC82OBJ
'Bollig'PRED
'Klaus'PRED188NAME-MOD228
OBJ-TH
'zu<[246:Verwalter]>'PRED
'Verwalter'PRED
'vorl?ufig<[246:Verwalter]>'PRED [246:Verwalter]SUBJ334ADJUNCT
'die'PREDDETSPEC246
OBJ
246
ADJUNCT
[1:pro]PRON-REL [1:pro]TOPIC-REL429
(a) evaluated as less probable
"das den Anwalt Klaus Bollig zum vorl?ufigen Verwalter bestellte"
'bestellen<[1:pro], [82:Anwalt]>'PRED
'pro'PRED1SUBJ
'Anwalt'PRED
'Bollig'PRED
'Klaus'PRED188NAME-MOD228
APP
'die'PREDDETSPEC82
OBJ
'zu<[246:Verwalter]>'PRED
'Verwalter'PRED
'vorl?ufig<[246:Verwalter]>'PRED [246:Verwalter]SUBJ334ADJUNCT
'die'PREDDETSPEC246
OBJ
246
ADJUNCT
[1:pro]PRON-REL [1:pro]TOPIC-REL429
(b) evaluated as relatively probable due to highly positive weight
of DEP21 common Anwalt APP proper
Figure 2: Competing f-structures for (5)
4 Experiments
4.1 Data
All the data we use are from the TIGER Corpus
(Brants et al, 2002), a treebank of German news-
paper texts comprising about 50,000 sentences. The
1,868 dependency annotations of the TiGer Depen-
dency Bank, which have been semi-automatically
derived from the corresponding treebank graphs, are
used for evaluation purposes; we split these into a
held-out set of 371 sentences (and corresponding de-
pendency annotations) and a test set of 1,497 sen-
tences. For training, we use packed, i.e. ambiguous,
c/f-structure representations where a proper subset
of the f-structures can be determined as compatible
with the TIGER graph annotations. Currently, these
are 8,881 pairs of labelled and unlabelled packed
c/f-structure reprentations.
From these 8,881 pairs of c/f-structure reprenta-
tions, we extract two sets of property forests, one
containing only the initially used properties, which
are based on the hardwired templates, and one con-
taining all properties, i.e. both the initially used and
the newly introduced ones.
4.2 Training
For training, we use the cometc software by Ste-
fan Riezler, which is part of XLE. Prior to train-
ing, however, we apply a frequency-based cutoff c
to the data that ensures that a property is discrimi-
native between the intended reading(s) and the un-
intended reading(s) in at least c sentences; c is set
to 4 on the basis of the evaluation results achieved
on our held-out set and following a policy of a ?con-
servative? cutoff whose only purpose is to prevent
that weights be learned for sparse properties. (For
a longer discussion of frequency-based cutoffs, see
Forst (2007).) For the actual estimation of prop-
erty weights, we then apply the combined method of
incremental property selection and l1 regularization
proposed in Riezler and Vasserman (2004), adjust-
ing the hyperparameters on our held-out set for each
of the two sets of properties. In order to compara-
tively evaluate the importance of property selection
and regularization, we also train models based on
each of the two sets of properties without applying
any kind of these techniques.
4.3 Evaluation
The overall results in terms of F-score and error re-
duction, defined as F? =
Factual?Flower
Fupper?Flower
, that the
four resulting systems achieve on our test set of
1,497 TiGer DB structures are shown in Table 1. In
order to give the reader an idea of the size of the dif-
ferent models, we also indicate the number of prop-
erties that they are based on. All of the F-scores
were calculated by means of the evaluation software
by Crouch et al (2002).
We observe that the models obtained using prop-
erty selection and regularization, in addition to be-
ing much more compact than their unregularized
counterparts, perform significantly better than these.
More importantly though, we can see that the most
important improvement, namely from an error re-
duction of 32.5% to one of 42.0% or from 34.8%
to 51.0% respectively, is achieved by adding more
informative properties to the model.
Table 2 then shows results broken down according
to individual dependencies that are achieved with,
on the one hand, the best-performing model based
on both the XLE template-based and the newly in-
21
# prop. F-sc. err. red.
XLE template-based properties,
unregularized MLE 14,263 82.07 32.5%
XLE templ.-based pr. that survive
a freq.-b. cutoff of 4, n-best
grafting with l1 regularization 3,400 82.19 34.8%
all properties,
unregularized MLE 57,934 82.55 42.0%
all properties that survive a
freq.-b. cutoff of 4, n-best
grafting with l1 regularization 4,340 83.01 51.0%
Table 1: Overall F-score and corresponding error re-
duction achieved by the four different systems on the
1,497 TiGer DB structures of our test set
troduced properties and, on the other hand, the best-
performing model based on XLE template-based
properties only. Furthermore, we indicate the re-
spective upper and lower bound F-scores, deter-
mined by the best possible parse selection and by
an arbitrary selection respectively.
We observe that the overall F-score is signifi-
cantly better with a selection based on the model that
includes the newly introduced properties than with a
selection based on the model that relies on the XLE
template-based properties only; overall error reduc-
tion increases from 34.5% to 51.0%. What is partic-
ularly interesting is the considerably better error re-
duction for the core grammatical functions sb (sub-
ject) and oa (accusative object). But also for rcs
(relative clauses) and mos (modifiers or adjuncts),
which are notoriously difficult for disambiguation
due to PP and ADVP attachment ambiguities, we
observe an improvement in F-score.
Our error reduction of 51.0% also compares fa-
vorably to the 36% error reduction on English LFG
parses reported in Riezler et al (2002). However,
it is considerably lower than the error reduction of
78% reported for the Dutch Alpino parser (Malouf
and van Noord, 2004), but this may be due to the
fact that our lower bound is calculated on the basis
of analyses that have already passed a prefilter and
is thus relatively high.
5 Conclusions
Our results show that property design is of crucial
importance in the development of a disambiguation
module for a ?deep? parser. They also indicate that it
is a good idea to carry out property design in a lin-
guistically inspired fashion, i.e. by referring to the
theoretical literature that deals with soft constraints
that are active in the language for which the system
is developed. Property design thus requires a pro-
found knowledge of the language under considera-
tion (and the theoretical literature that deals with its
syntax), and since the disambiguation module oper-
ates on the output of the symbolic grammar, a good
knowledge of the grammar is necessary as well.
Weighting against each other the contributions of
different measures taken for improving log-linear
models for parse selection, we can conclude that
property design is at least as important as prop-
erty selection and/or regularization, since even a
completely unregularized model based on all prop-
erties performs significantly better than the best-
adjusted model among the ones that are based on
the template-based properties only. Moreover, prop-
erty design can be carried out in a targeted way,
i.e. properties can be designed in order to improve
the disambiguation of grammatical relations that, so
far, are disambiguated particularly poorly or that
are of special interest for the task that the system?s
output is used for. By demonstrating that prop-
erty design is the key to good log-linear models for
?deep? syntactic disambiguation, our work confirms
that ?specifying the features of a SUBG [stochastic
unification-based grammar] is as much an empirical
matter as specifying the grammar itself? (Johnson et
al., 1999).
Acknowledgements
The work described in this paper has been carried
out in the DLFG project, which was funded by the
German Research Foundation (DFG).
Furthermore, I thank the audiences at several Par-
Gram meetings, at the Research Workshop of the
Israel Science Foundation on Large-scale Grammar
Development and Grammar Engineering at the Uni-
versity of Haifa and at the SFB 732 Opening Col-
loquium in Stuttgart for their important feedback on
earlier versions of this work.
References
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435?483.
22
upper stoch. select. stoch. select. lower
bound all properties templ.-based pr. bound
gramm. relation/morphosynt. feature F-sc. F-sc. err. red. F-sc. err. red. F-sc.
all 85.50 83.01 51.0 82.17 34.5 80.42
PREDs only 79.36 75.74 46.5 74.69 31.0 72.59
app (close apposition) 63 60 63 61 75 55
app cl (appositive clause) 53 53 100 52 86 46
cc (comparative complement) 28 19 -29 19 -29 21
cj (conjunct of coord.) 70 68 50 67 25 66
da (dative object) 67 63 67 62 58 55
det (determiner) 92 91 50 91 50 90
gl (genitive in spec. pos.) 89 88 75 88 75 85
gr (genitive attribute) 88 84 56 84 56 79
mo (modifier) 70 63 36 62 27 59
mod (non-head in compound) 94 89 29 89 29 87
name mod (non-head in compl. name) 82 80 33 81 67 79
number (number as determiner) 83 81 33 81 33 80
oa (accusative object) 78 75 77 69 31 65
obj (arg. of prep. or conj.) 90 88 50 87 25 86
oc fin (finite cl. obj.) 67 64 0 64 0 64
oc inf (infinite cl. obj.) 83 82 0 82 0 82
op (prepositional obj.) 57 54 40 54 40 52
op dir (directional argument) 30 23 13 23 13 22
op loc (local argument) 59 49 29 49 29 45
pd (predicative argument) 62 60 50 59 25 58
pred restr 92 87 62 84 38 79
quant (quantifying determiner) 70 68 33 68 33 67
rc (relative clause) 74 62 20 59 0 59
sb (subject) 76 73 63 71 38 68
sbp (logical subj. in pass. constr.) 68 63 62 61 46 55
case 87 85 75 83 50 79
comp form (complementizer form) 74 72 0 74 100 72
coord form (coordinating conj.) 86 86 100 86 100 85
degree 89 88 50 87 0 87
det type (determiner type) 95 95 ? 95 ? 95
fut (future) 86 86 ? 86 ? 86
gend (gender) 92 90 60 89 40 87
mood 90 90 ? 90 ? 90
num (number) 91 89 50 89 50 87
pass asp (passive aspect) 80 80 100 79 0 79
perf (perfect) 86 85 0 86 100 85
pers (person) 85 84 83 82 50 79
pron form (pronoun form) 73 73 ? 73 ? 73
pron type (pronoun type) 71 70 0 71 100 70
tense 92 91 0 91 0 91
Table 2: F-scores (in %) in the 1,497 TiGer DB examples of our test set
23
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCJ and Log-Linear Models. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL ?04), Barcelona,
Spain.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage parser. In Proceedings
of the LREC Workshop ?Beyond PARSEVAL?Towards
improved evaluation mesures for parsing systems?,
pages 67?74, Las Palmas, Spain.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE docu-
mentation. Technical report, Palo Alto Research Cen-
ter, Palo Alto, CA.
Stefanie Dipper. 2003. Implementing and Documenting
Large-scale Grammars ? German LFG. Ph.D. thesis,
IMS, University of Stuttgart. Arbeitspapiere des Insti-
tuts fu?r Maschinelle Sprachverarbeitung (AIMS), Vol-
ume 9, Number 1.
Martin Forst, Nu?ria Bertomeu, Berthold Crysmann, Fred-
erik Fouvry, Silvia Hansen-Schirra, and Valia Kordoni.
2004. Towards a dependency-based gold standard
for German parsers ? The TiGer Dependency Bank.
In Proceedings of the COLING Workshop on Lin-
guistically Interpreted Corpora (LINC ?04), Geneva,
Switzerland.
Martin Forst, Jonas Kuhn, and Christian Rohrer. 2005.
Corpus-based learning of OT constraint rankings for
large-scale LFG grammars. In Proceedings of the 10th
International LFG Conference (LFG?05), Bergen,
Norway. CSLI Publications.
Martin Forst. 2007. Disambiguation for a Linguistically
Precise German Parser. Ph.D. thesis, University of
Stuttgart.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFGGrammars. In
Peter Sells, editor, Formal and Empirical Issues in Op-
timality Theoretic Syntax, pages 367?397. CSLI Pub-
lications, Stanford, CA.
Gerhard Helbig and Joachim Buscha. 2001.
Deutsche Grammatik ? Ein Handbuch fu?r den
Ausla?nderunterricht. Langenscheidt, Berlin and
Munich, Germany.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics 1999, College Park, MD.
Ju?rgen Lenerz. 1977. Zur Abfolge nominaler Satzglieder
im Deutschen. Number 5 in Studien zur deutschen
Grammatik. Narr, Tu?bingen, Germany.
Robert Malouf and Gertjan van Noord. 2004. Wide Cov-
erage Parsing with Stochastic Attribute Value Gram-
mars. In Proceedings of the IJCNLP-04 Workshop
?Beyond Shallow Analyses - Formalisms and statisti-
cal modeling for deep analyses?.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings
of the Human Language Technology Conference, San
Diego, CA.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and l1 regularization for maximum
entropy parsing. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?04), Barcelona, Spain.
Stefan Riezler, Tracy Holloway King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discriminative Esti-
mation Techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics 2002, Philadelphia, PA.
Christian Rohrer and Martin Forst. 2006. Improv-
ing coverage and parsing quality of a large-scale
LFG for German. In Proceedings of the Language
Resources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Kristina Toutanova, Christopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT2002), pages 253?263.
Hans Uszkoreit, Thorsten Brants, Brigitte Krenn, Lars
Konieczny, Stephan Oepen, and Wojciech Skut. 1998.
Relative Clause Extraposition in German ? Evidence
from Corpus Studies and Acceptability Ratings. In
Proceedings of AMLaP-98, Freiburg, Germany.
Hans Uszkoreit. 1987. Word Order and Constituent
Structure in German. CSLI Publications, Stanford,
CA.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Piet Mertens, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e confe?rence sur le traite-
ment automatique des langues naturelles, pages 20?
42, Leuven, Belgium.
24
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 793?803,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Cascaded Classification Approach to Semantic Head Recognition
Lukas Michelbacher Alok Kothari Martin Forst?
Christina Lioma Hinrich Schu?tze
Institute for NLP
University of Stuttgart
{michells,kotharak,liomaca}@ims.uni-stuttgart.de
?Microsoft
martin.forst@microsoft.com
Abstract
Most NLP systems use tokenization as part
of preprocessing. Generally, tokenizers are
based on simple heuristics and do not recog-
nize multi-word units (MWUs) like hot dog
or black hole unless a precompiled list of
MWUs is available. In this paper, we propose
a new cascaded model for detecting MWUs
of arbitrary length for tokenization, focusing
on noun phrases in the physics domain. We
adopt a classification approach because ? un-
like other work on MWUs ? tokenization re-
quires a completely automatic approach. We
achieve an accuracy of 68% for recognizing
non-compositional MWUs and show that our
MWU recognizer improves retrieval perfor-
mance when used as part of an information re-
trieval system.
1 Introduction
Most NLP systems use tokenization as part of pre-
processing. Generally, tokenizers are based on sim-
ple heuristics and do not recognize multi-word units
(MWUs) like hot dog or black hole. Our long-term
goal is to build MWU-aware tokenizers that are used
as part of the standard toolkit for NLP preprocessing
alongside part-of-speech and named-entity tagging.
We define an MWU as a sequence of words that
has properties that cannot be inferred from the com-
ponent words (cf. e.g. Manning and Schu?tze (1999,
Ch. 5), Sag et al (2002)). The most important
of these properties is non-compositionality, the fact
that the meaning of a phrase cannot be predicted
from the meanings of its component words. For ex-
ample, a hot dog is not a hot animal but a sausage in
a bun and a black hole in astrophysics is a region of
space with special properties, not a dark cavity.
The correct recognition of MWUs is an important
building block of many NLP tasks. For example, in
information retrieval (IR) the query hot dog should
not retrieve documents that only contain the words
hot and dog individually, outside of the phrase hot
dog.
In this study, we focus on noun phrases in the
physics domain. For specialized domains such as
physics, adaptable and reliable MWU recognition
is of particular importance because comprehensive
and up-to-date lists of MWUs are not available
and would have to be created by hand. We chose
noun phrases because domain-specific terminology
is commonly encoded in noun phrase MWUs; other
types of phrases ? e.g., verb constructions ? rarely
give rise to fixed domain-specific multi-word se-
quences that should be treated as a unit.
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. The importance of
syntactic heads for many NLP tasks is generally ac-
cepted. For example, in coreference resolution iden-
tity of syntactic heads is predictive of coreference;
in parse disambiguation, the syntactic head of a noun
phrase is a powerful feature for resolving attachment
ambiguities. However, in all of these cases, the syn-
tactic head is only an approximation of the informa-
tion that is really needed; the underlying assumption
made when using the syntactic head as a substitute
for the entire phrase is that the syntactic head is rep-
resentative of the phrase. This is not the case when
the phrase is non-compositional.
We define the semantic head of a noun phrase as
the non-compositional part of a phrase. Semantic
heads would serve most NLP tasks better than syn-
tactic heads. For example, a coreference resolution
system is misled if it looks at syntactic heads to de-
793
termine possible coreference of a hot dog . . . the dog
in I first ate a hot dog and then fed the dog. This is
not the case for a system that makes the decision
based on the semantic heads hot dog of a hot dog
and dog of the dog.
The specific NLP application we evaluate in this
paper is information retrieval. We will show that se-
mantic head recognition improves the performance
of an information retrieval system.
We introduce a cascaded classification framework
for recognizing semantic heads that allows us to treat
noun phrases of arbitrary length. We use a number
of previously proposed features for recognizing non-
compositionality and semantic heads. In addition,
we compare three features that measure contextual
similarity.
Our main contributions in this paper are as fol-
lows. First, we introduce the notion of semantic
head, in analogy to syntactic head, and propose se-
mantic head recognition as a new component of NLP
preprocessing. Second, we develop a cascaded clas-
sification framework for semantic head recognition.
Third, we investigate the utility of contextual simi-
larity for detecting non-compositionality and show
that it significantly enhances a baseline semantic
head recognizer. However, we also identify a num-
ber of challenges of using contextual similarity in
high-confidence semantic head recognition. Fourth,
we show that our approach to semantic head recog-
nition improves the performance of an IR system.
Section 2 discusses previous work. In Section 3
we introduce semantic heads and present our cas-
caded model for semantic head recognition. In Sec-
tion 4, we describe our data and three different mea-
sures of contextual similarity. Section 5 introduces
the classifier and its features. Section 6 presents
classification results and discussion. Section 7 de-
scribes the information retrieval experiments. In
Section 8 we present our conclusions.
2 Related Work
While there is a large number of publications on
MWUs and collocation extraction, the general prob-
lem of automatic MWU detection for the specific
purpose of tokenization has not been investigated
before to our knowledge.
The classic approach to identifying collocations
and MWUs is to apply statistical association mea-
sures (AMs) to n-grams extracted from a corpus
? often combined with various linguistic heuris-
tics and other filters, resulting in candidate lists.
Choueka (1988) and the XTRACT system (Smadja,
1993) are well-known examples of this approach.
More recent approaches such as Pecina (2010)
and Ramisch et al (2010) combine classifiers with
association measures. Although our approach is
classification-based as well, our data set has a more
realistic size than Pecina (2010)?s (1 billion words
vs 1.5 million words) and we work on noun phrases
of arbitrary length (instead of just bigrams). The
mwetoolkit1 by Ramisch et al (2010) aims to
be a software package for lexicographers and its
features are limited to a small set of association
measures that do not consider marginal frequencies.
Neither of these two studies includes evaluation in
the context of an application.
Lin (1999) defines a decision criterion for non-
compositional phrases based on the change in the
mutual information of a phrase when substituting
one word for a similar one based on an automatically
constructed thesaurus. The method reaches 15.7%
precision and 13.7% recall.
In terms of the extraction of domain-specific
MWUs, cross-language methods have been pro-
posed that make use of the fact that an MWU in one
language might be expressed as a single word in an-
other. Caseli et al (2009) utilize word alignments
in a parallel corpus; Attia et al (2010) exploit the
links between article names of different-language
Wikipedias to search for many-to-one translations.
We did not pursue a cross-language approach be-
cause we strive for a self-contained method of MWU
recognition that operates on a single textual re-
source.
Non-compositionality and distributional se-
mantics. In recent years, a number of studies have
investigated the relationship between distributional
semantics and non-compositionality. These studies
compute the similarity between words and phrases
represented as semantic vectors in a word space
model. A semantic vector of a word is the accumu-
lation of the particular contexts in which the word
1http://sourceforge.net/projects/
mwetoolkit/
794
appears. The underlying idea is similar to Lin?s:
the meaning of a non-compositional phrase some-
how deviates from what one would expect given the
semantic vectors of parts of the phrase. The stan-
dard measure to compare semantic vectors is cosine
similarity. The questions that arise are (i) which
vectors to compare, (ii) how to combine the vectors
of the parts and (iii) from what point on a certain
dissimilarity indicates non-compositionality. To our
knowledge, there are no generally accepted answers
to these questions.
Regarding (i), Schone and Jurafsky (2001) com-
pare the semantic vector of a phrase p and the vec-
tors of its component words in two ways: one in-
cludes the contexts of p in the construction of the
semantic vectors of the parts and one does not. Re-
garding (ii), they suggest weighted or unweighted
sums of the semantic vectors of the parts.
Baldwin et al (2003) investigate semantic decom-
posability of noun-noun compounds and verb con-
structions. They address (i) by comparing the se-
mantic vectors of phrases with the vectors of their
parts individually to detect meaning changes; e.g.,
they compare vice president to vice and president.
We propose a new method that compares phrases
with their alternative phrases, in the spirit of Lin
(1999)?s substitution approach (see Section 4.3).
Our rationale is that context features should be
based on contexts that are syntactically similar to the
phrase in question.
With respect to (iii), the above-mentioned studies
use ad hoc thresholds to separate compositional and
non-compositional phrases but do not offer a princi-
pled decision criterion.2 In contrast, we train a sta-
tistical classifier to learn a decision criterion.
There is a larger body of work concerning non-
compositionality which revolves around the prob-
lem of literal (compositional) vs. non-literal (non-
compositional) usage of idiomatic verb construc-
tions like to break the ice or to spill the beans.
Some studies approach the problem with semantic
vector comparisons in the style of Schone and Ju-
rafsky (2001), e.g Katz and Giesbrecht (2006) and
Cook et al (2007). Other approaches use word-
alignment (e.g. Moiro?n and Tiedemann (2006)) or
2Lin (1999) uses a well-defined criterion but his approach is
not based on vector similarity.
a combination of heuristic and linguistic features
(e.g. Diab and Bhutada (2009), Li and Sporleder
(2010)). Even though there is some methodologi-
cal overlap between our approach and some of the
verb-oriented studies, we believe that verb construc-
tions have properties that are quite different from
noun phrases. For example, our definition of alter-
native vector relies on the fact that most noun phrase
MWUs are fixed and exhibit no syntactic variability.
In contrast, verb constructions are often discontinu-
ous.
The motivation for most work on MWU detec-
tion is lexicography, terminology extraction or the
creation of machine-readable dictionaries. Our mo-
tivation ? tokenization in a preprocessing setting ? is
different from this earlier work.
3 Semantic Heads and Cascaded Model
We cast the task of MWU tokenization as seman-
tic head recognition in this paper. We define the
semantic head of a noun phrase as the largest non-
compositional part of the phrase that contains the
syntactic head. For example, black hole is the se-
mantic head of unusual black hole and afterglow is
the semantic head of bright optical afterglow; in the
latter case syntactic and semantic heads coincide.
Semantic heads would serve most NLP tasks bet-
ter than syntactic heads. The attachment ambiguity
of the last noun phrase in he bought the hot dogs in a
packet can be easily resolved for the semantic head
hot dogs (food is often in a packet), but not as easily
for the syntactic head dogs (dogs are usually not in
packets). Indeed, we will show in Section 7 that se-
mantic head recognition improves the performance
of an IR system.
The semantic head is either a single noun or a non-
compositional noun phrase. In the latter case, the
modifier(s) introduce(s) a non-compositional, un-
predictable shift of meaning; hot shifts the mean-
ing of dog from live animal to food. In contrast,
the compositional meaning shift caused by small
in small dog is transparent. The semantic head al-
ways contains the syntactic head; for compositional
phrases, syntactic head and semantic head are iden-
tical.
To determine the semantic head of a phrase, we
use a cascaded classification approach. The cascade
795
(1) neutron star
(2) unusual black hole
(3) bright optical afterglow
(4) small moment of inertia
Figure 1: Example phrases with modifiers. Peripheral
elements are set in italics, syntactic heads in bold.
comes into play in all aspects of our study: the rat-
ing experiments with human subjects, data extrac-
tion, feature design and classification itself.
We need a cascade because we want to recog-
nize the semantic head in noun phrases of arbitrary
length. The starting point is a phrase of length n:
p = w1 . . . wn. We distinguish between the syntac-
tic head of a phrase and the remaining words, the
modifiers. Figure 1 shows phrases of varying syn-
tactic complexity. The syntactic head is marked in
bold. The model accommodates pre-nominal modi-
fiers as in examples (1) through (3) and post-nominal
modifiers like PPs in example (4).
Among the modifiers, there is a distinguished ele-
ment, the peripheral element u (italicized in the ex-
amples). The remaining words are called the rest
v. We can now represent any phrase p as p = uv.3
The element u is always the outermost modifier. of -
PPs are treated as a single modifier and they take
precedence over pre-nominal modification because
this analysis is dominant in our gold standard data.
This means that in the phrase small moment of iner-
tia, small (and not of inertia) is the peripheral ele-
ment u.
Cascaded classification then operates as shown in
Figure 2. In each iteration, the classifier decides
whether the relation between the current peripheral
element u and the rest v is compositional (C) or non-
compositional (NC). If the relation is NC, process-
ing stops and uv is returned as the semantic head
of p. If the relation is compositional, u is discarded
and classification continues with v as the new input
phrase, which again is represented in the form u?v?.
In case there is no more peripheral element u, i.e.
the new v is a single word, it is returned as the se-
mantic head of p.
Table 1 shows two examples. For the fully com-
positional phrase bright optical afterglow, the pro-
3We use the abstract representation p = uv even though u
can appear after v in the surface form of p.
function recognize semantic head(p)
u? peripheral(p)
v ? rest(p)
while decision(u, v) 6= NC do
u? peripheral(v)
if u = ? then
return v
v ? rest(v)
return uv
Figure 2: Cascaded classification of p
step u v decision
1 bright optical afterglow C
2 optical afterglow C
3 ? afterglow
1 small moment of inertia C
2 of inertia moment NC
Table 1: Cascaded decision processes
cess runs all the way down to the syntactic head af-
terglow which is also the semantic head. In the sec-
ond case, the process stops earlier, in step 2, because
the classifier finds that the relation between moment
and of inertia is NC. This means that the semantic
head of small moment of inertia is moment of iner-
tia.
4 Corpus and Feature Definitions
4.1 Candidate phrases
As our corpus, we use the iSearch collection, a
one billion word collection of documents from the
physics domain (Lykke et al, 2010). We tokenized
the collection by splitting on white space and adding
sentence boundaries and part-of-speech tags to the
output. With part-of-speech information, the iden-
tification of MWU candidates is easy, fast and reli-
able.
We extracted all noun phrases from the collection
that consist of a head noun with up to four modifiers
? almost all domain-specific terminology in our col-
lection is captured by this pattern. The pre-nominal
modifiers can be nouns, proper nouns, adjectives or
cardinal numbers.
The baseline accuracy of a classifier that always
chooses compositionality is very high (> 90%) for
796
V = v V 6= v
U = u O11 O12 = R1
U 6= u O21 O22 = R2
= C1 = C2 = N
Table 2: 2-by-2 contingency tables with observed and
marginal frequencies
phrases of the type [noun] of the/a [noun] (sg.)
(e.g. rest of the paper) and [noun] of [noun] (pl.)
(e.g. series of papers). We therefore restrict post-
nominal modifiers to prepositional phrases with the
word of followed by a non-modified, indefinite, sin-
gular noun, e.g., speed of light or moment of inertia.
Out of all phrases extracted with part-of-speech
patterns, we keep only the ones that appear more of-
ten than 50 times because it is hard to compute re-
liable features for less frequent phrases. All experi-
ments were carried out with lemmatized word forms.
We refer to lemmas as words if not noted otherwise.
4.2 Association measures
Statistical association measures are frequently used
for MWU detection and collocation extraction (e.g.
Schone and Jurafsky (2001), Evert and Krenn
(2001), Pecina (2010)).
We use all measures used by Schone and Jurafsky
(2001) that can be derived from a phrase?s contin-
gency table. These measures are Student?s t-score,
z-score, ?2, pointwise mutual information (MI),
Dice coefficient, frequency, log-likelihood (G2) and
symmetric conditional probability.
We define the AMs in Table 3 based on the no-
tation for the contingency table shown in Table 2
(cf. Evert (2004)). Oij is observed frequency and
Eij = RiCjN expected frequency.
The AMs are designed to deal with two random
variables U and V that traditionally represent single
words. In our model, we use U to represent periph-
eral elements u and V for rests v.
association measure formula
student?s t-score (amt) O11?E11?O11
z-score (amz) O11?E11?E11
chi-square (am?2)
?
i,j
(Oij?Eij)2
Eij
pointwise mutual infor-
mation (amMI ) log
O11
E11
Dice coefficient (amD) 2O11R1+C1
frequency (amf ) O11
log-likelihood (amG2) 2
?
i,j
Oij log OijEij
symmetric conditional
probability (amscp)
O112
R1C1
Table 3: Association measures
4.3 Word space model
As our baseline, we use two methods of compar-
ing semantic vectors: sj1 and sj2, both introduced
by Schone and Jurafsky (2001). They experimented
with variants of sj1 and sj2, but found no large differ-
ences. In addition, we introduce our own approach
alt.
Method sj1 compares the semantic vector of a
phrase p with the sum of the vectors of its parts.
Method sj2 is like sj1, except the contexts of p are
not part of the semantic vectors of the parts. Method
alt compares the semantic vector of a phrase with its
alternative vector. In the definitions below, s repre-
sents a vector similarity measure, w(p) a general se-
mantic vector of a phrase p and w?(wi) the semantic
vector of a partwi of a phrase p that does not include
the contexts of occurrences of wi that were part of p
itself.
sj1 s(w(black hole), w(black) + w(hole))
sj2 s(w(black hole), w?(black) + w?(hole))
alt s(w(black hole),?
u
w(u, hole)); u 6= black
For the third comparison, we build the alternative
vector as follows. For a phrase p = uv with pe-
ripheral element u and rest v, we call the phrase
797
p? = u?v an alternative phrase if the rest v is the
same and u? 6= u. E.g., giant star is an alternative
phrase of neutron star and isolated neutron star is
an alternative of young neutron star. The alterna-
tive vector of p is then the semantic vector that is
computed from the contexts of all of p?s alternative
phrases. The alternative vector is a representation
of the contexts of v except for those modified by u.
This technique bears resemblance to the substitution
approach of Lin (1999). The difference is that he
relies on a similarity thesaurus for substitution and
monitors the change in mutual information for each
substitution individually whereas we substitute with
general alternative modifiers and combine the alter-
native contexts into one vector for comparison.
Previous work has compared the semantic vector
of a phrase with the vectors of its components. Our
approach is more ?head-centric? and only compares
phrases in the same syntactic configuration. Our
question is: Is the typical context of the head hole
if it occurs with a modifier that is not black different
from when it occurs with the modifier black?
We used a bag-of-words model and a window of
?10 words for contexts to create semantic vectors.
We only kept the content words in the window which
we defined as words that are tagged as either a noun,
verb, adjective or adverb. To add information about
the variability of syntactic contexts in which phrases
occur, we add the words immediately before and af-
ter the phrase with positional markers (?1 and +1,
respectively) to the vector. These words were not
subject to the content-word filter. The dimension-
ality of the vectors is then 3V where V is the size
of the vocabulary: V dimensions each for bag-of-
words, left and right syntactic contexts. We did not
include vectors for the stop word of for sj1 and sj2.
4.4 Non-compositionality judgments
Since the domain of the corpus is physics, highly
specialized vocabulary had to be judged. We em-
ployed domain experts as raters (one engineering
and two physics graduate students).
In line with the cascaded model, the raters where
asked to identify the semantic head of each candi-
date phrase. If at least two raters agreed on a seman-
tic head of a phrase we made this choice the seman-
tic head in the gold standard. The final gold standard
comprises 1560 phrases.
We computed raw agreement of each rater with
the gold standard as the percentage of correctly rec-
ognized semantic heads ? this is the task that the
classifier addresses. Agreement is quite high at
86.5%, 88.3% and 88.5% for the three raters. In
addition, we calculated chance-corrected agreement
with Cohen?s ? on the first decision task against the
gold standard (see Section 6). As expected, agree-
ment decreases, but is still substantial at 74.0%,
78.2% and 71.8% for the three raters.
5 Classifier
We use the Stanford maximum entropy classifier for
our experiment.4 We randomly split the data into a
training set of 1300 and a held-out test set of 260
pairs.
We use the eight AMs and the cosine similari-
ties simsj1, simsj2 and simalt described in Sec-
tion 4.3 as features for the classifier. Cosine similar-
ity should be small if a phrase is non-compositional
and large if it is compositional. In other words, if the
contexts of the candidate phrase are too dissimilar to
the contexts of the sum of its parts or to the alterna-
tive phrases, then we suspect non-compositionality.
Feature values are binned into 5 bins. We ap-
plied a log transformation to the four AMs with large
values: amf , amG2 , am?2 and amz . For our ap-
plication there is little difference between statistical
significance at p < .001 and p < .00001. The
log transformation reduces the large gap in magni-
tude between high significance and very high signif-
icance. If co-occurrence of u and v in uv is below
chance, then we set the association scores to 0 since
this is an indication of compositionality (even if it is
highly significant).
Since AMs have been shown to be correlated (e.g.
Pecina (2010)), we first perform feature selection on
the AM features. We tested accuracy of all 2r ? 1
non-empty combinations of the r = 8 AM features
on the task of deciding whether the first decision
during the classification of a phrase was C or NC.
We then selected those AM features that were part
of at least one top 10 result in each fold. Those fea-
tures were amt, amf and amscp.
The main experiment combines these three se-
4http://nlp.stanford.edu/software/
classifier.shtml
798
lected AM features with all possible subsets of con-
text features. We train on the 1300-element training
set and test on the 260-element test set.
6 Results and Discussion
We ran three evaluation modes: dec-1st, dec-all, and
semh. Mode dec-1st only evaluates the first deci-
sion for each phrase; the baseline in this case is .554
since 55.4% of the first decisions are C. In mode
dec-all, we evaluate all decisions that were made in
the course of recognizing the semantic head. This
mode emphasizes the correct recognition of seman-
tic heads in phrases where multiple correct decisions
in a row are necessary. We define the confidence
for multi-decision classification as the product of
the confidence values of all intermediate decisions.
There is no obvious baseline for dec-all because the
number of decisions depends on the classifier ? a
classifier whose first decision on a four-word phrase
is NC makes one decision, another one may make
three. The mode semh evaluates how many semantic
heads were recognized correctly. This mode directly
evaluates the task of semantic head recognition. The
baseline for semh is the tokenizer that always returns
the syntactic head; this baseline is .488.5 Table 4
shows 8? 3 runs, corresponding to the three modes
tested on the AM features (amt, amf , and amscp)
and the eight possible subsets of the three context
features.
For all modes, the best result is achieved with base
AMs combined with the simalt feature; the accura-
cies are .692, .703 and .680. The improvements over
the baselines (for dec-1st and semh) are statistically
significant at p < .01 (binomial test, n = 260).
For semh, accuracy without any context features
is .603; this is significantly better than the .488 base-
line (p < .01). Performance with only the base AM
features is significantly lower than the best context
feature experiment (.680) at p < .01 and signifi-
cantly lower than the worst context feature exper-
iment (.653) at p < .1. However, the differences
between the context feature runs are not significant.
When the semantic head recognizer processes a
phrase, there are four possible results. Result rsemh:
5The baseline could be improved with simple heuristics, e.g.
?uv contains capital letter?? NC. However, this feature only
results in a 2% improvement compared to the baseline.
type freq definition
rsemh 92 sem. head correct (6= synt. head)
rsynth 85 sem. head correct (= synt. head)
r+ 48 sem. head too long
r? 35 sem. head too short
all 260
Table 5: Distribution of result types
the semantic head is correctly recognized and it is
distinct from the syntactic head. Result rsynth: the
semantic head is correctly recognized and it is iden-
tical to the syntactic head. Result r+: the semantic
head is not correctly recognized because the cascade
was stopped too early, i.e., a compositional modifier
that should have been removed was kept. Result r?:
the semantic head is not correctly recognized be-
cause the cascade was stopped too late, i.e., a modi-
fier causing a non-compositional meaning shift was
removed. Table 5 shows the distribution of result
types. It shows that r+ is the more common error:
the classifier more often regards compositional rela-
tions as non-compositional than vice versa.
Table 6 shows the top 20 classifications where
the semantic head was not the same as the syntac-
tic head sorted by confidence in descending order.
In the third column ?phrase . . . ? we list the candi-
dates with semantic heads in bold. The columns to
the right show the predicted semantic head and the
feature values. All five errors in the list are of type
r+.
Two r+ phrases are schematic view and many oth-
ers. The two phrases are clearly compositional and
the classifier failed even though the context feature
points in the direction of compositionality with a
value greater than .5. It can be argued that many oth-
ers is a trivial example that does not require complex
machinery to be identified as compositional, e.g. by
using a stop list. We included it in the analysis since
we want to be able to process arbitrary phrases with-
out additional hand-crafted resources.
Another incorrect classification occurs with the
phrase massive star birth6 for which star birth was
annotated as the semantic head. Here we have a case
where the peripheral element massive does not mod-
6i.e. the birth of a massive star, a certain type of star with
very high mass
799
mode baseline context feature context feature subsets
simalt - ? ? ? ? - - -
simsj1 - - ? ? ? ? - ?
simsj2 - - - ? ? - ? ?
dec-1st .554 .604 .692 .669 .685 .677 .654 .654 .662
dec-all - .615 .703 .681 .696 .688 .666 .669 .675
semh .488 .603 .680 .657 .673 .665 .653 .653 .661
Table 4: Performance for base AM features plus context feature subsets. A ??? indicates the use of the corresponding
context feature.
ify the syntactic head birth but massive star is itself
a complex modifier. In the test set, 5% of the phrases
exhibit structural ambiguities of this type. Our sys-
tem cannot currently deal with this phenomenon.
The remaining r+ phrases are peculiar velocity
and local group. However, Wikipedia lists both
phrases with an individual entry defining the former
as the true velocity of an object, relative to a rest
frame7 and the latter as the group of galaxies that
includes Earth?s galaxy, the Milky Way8. Both def-
initions provide evidence for non-compositionality
since the velocity is not peculiar (as in strange) and
the scope of local is not clear without further knowl-
edge. Arguably, in these cases our method chose a
justifiable semantic head, but the raters disagreed.9
For NLP preprocessing, it is acceptable to sacri-
fice recall and only make high-confidence decisions
on semantic heads. A tokenizer that reliably detects
a subset of MWUs is better than one that recognizes
none. However, our attempts to use the simalt rec-
ognizer (bold in Table 4) in this way were not suc-
cessful. Precision is .680 for confidence > .7 and
does not exceed .770 for higher confidence values.
To understand this effect, we analyzed the distri-
bution of simalt scores. Surprisingly, moderate sim-
ilarity between .4 and .6 is a more reliable indicator
for NC than low similarity < .3. Our intuition for
using distributional semantics in Section 2 was that
low similarity indicates non-compositionality. This
7http://en.wikipedia.org/wiki/Peculiar_
velocity
8http://en.wikipedia.org/wiki/Local_
group
9Further evidence that local group is non-compositional is
the fact that one of the domain experts annotated the phrase as
non-compositional but was overruled by the other two.
does not seem to hold for the lowest similarity val-
ues possibly because they are often extreme cases
in terms of distribution and frequency and then give
rise to unreliable decisions. This means that the con-
text features enhance the overall performance of the
classifier, but they are unreliable and do not support
the high-confidence decisions we need in NLP pre-
processing.
For comparison, the classifier that only uses AM
features achieves 90% precision at 14% recall with
confidence > .7 ? although it has lower overall ac-
curacy than the simalt recognizer. We are still in
the process of analyzing these results and decided to
use the AM-only recognizer for the IR experiment
because it has more predictable performance.
In summary, the results show that, for the recogni-
tion of semantic heads, basic AMs offer a significant
improvement over the baseline. We have shown that
some wrong decisions are defensible even though
the gold standard data suggests otherwise. Context
features further increase performance significantly,
but surprisingly, they are not of clear benefit for
a high-confidence classifier that is targeted towards
recognizing a smaller subset of semantic heads with
high confidence.
7 Information Retrieval Experiment
Typically, IR systems do not process non-
compositional phrases as one semantic entity,
missing out on potentially important information
captured by non-compositionality. This section
illustrates one way of adjusting the retrieval process
so that non-compositional phrases are processed as
semantic entities that may enhance retrieval perfor-
mance. The underlying hypothesis is that, given
800
c. type phrase (semantic head in bold) predicted semantic head amt amf amcp simalt
.99 rsemh ellipsoidal figure of equilibrium ellipsoidal figure of equilibrium 18.03 325 6.23e-01 .219
.99 rsemh point spread function point spread function 95.03 9056 2.33e-01 .529
.99 r+ massive star birth massive star birth 19.99 402 4.81e-03 .134
.98 rsemh high angular resolution imaging high angular resolution imaging 13.07 179 1.27e-03 .173
.98 rsemh integral field spectrograph integral field spectrograph 24.20 586 4.12e-02 .279
.98 r+ local group local group 153.54 24759 8.73e-03 .650
.98 rsemh neutral kaon system neutral kaon system 1.38 108 4.17e-03 .171
.97 rsemh IRAF task IRAF task 49.07 2411 2.96e-02 .517
.92 rsemh easy axis easy axis 44.66 2019 2.79e-03 .599
.89 r+ schematic view schematic view 40.56 1651 8.06e-03 .612
.87 rsemh differential resistance differential resistance 31.71 1034 6.38e-04 .548
.86 rsemh TiO band TiO band 36.84 1372 2.21e-03 .581
.86 r+ many others many others 97.76 9806 6.54e-03 .708
.86 rsemh VLBA observation VLBA observation 43.95 2004 9.35e-04 .648
.85 r+ peculiar velocity peculiar velocity 167.63 28689 2.37e-02 .800
.84 rsemh computation time computation time 43.80 1967 1.35e-03 .657
.83 rsemh Land factor Land factor 21.15 453 6.30e-04 .360
.83 rsemh interference filter interference filter 31.44 1002 1.27e-03 .574
.83 rsemh line formation calculations line formation calculations 14.20 203 1.96e-03 .381
.82 rsemh Wess-Zumino-Witten term Wess-Zumino-Witten term 9.60 94 8.12e-05 .291
Table 6: The 20 most confident classifications where the prediction is semantic head 6= syntactic head. ?c.? = confi-
dence
a query that contains a non-compositional phrase,
boosting the retrieval weight of documents that
contain this phrase will improve overall retrieval
performance.
We do this boosting using Indri?s10 combination
of the language modeling and inference network
approaches (Metzler and Croft, 2004), which al-
lows assigning different degrees of belief to differ-
ent parts of the query. This belief can be drawn from
any suitable external evidence of relevance. In our
case, this source of evidence is the knowledge that
certain query terms constitute a non-compositional
phrase. Under this approach, and using the #weight
and #combine operators for combining beliefs, the
relevance of a documentD to a queryQ is computed
as the probability that D generates Q, P (Q|D):
P (Q|D) =
?
t?Q
P (t|D)
wt
W (W =
?
t?Q
wt) (1)
where t is a term and wt is the belief weight as-
signed to t. The higher wt is, the higher the rank
of documents containing t. In this work, we dis-
10http://www.lemurproject.org/
tinguish between two types of query terms: terms
occurring in non-compositional phrases (Qnc), and
the remaining query terms (Qc). Terms t ? Qnc
receive belief weight wnc and terms t ? Qc belief
weight wc, (wnc + wc = 1 and wnc, wc ? [0, 1]).
To boost the ranking of documents containing non-
compositional phrases, we increase wnc at the ex-
pense of wc. We estimate P (t|D) in Eq. 1 using
Dirichlet smoothing (Zhai and Lafferty, 2002).
We use Indri for indexing and retrieval without
removing stopwords or stemming. This choice is
motivated by two reasons: (i) We do not have a
domain-specific stopword list or stemmer. (ii) Base-
line performance is higher when keeping stopwords
and without stemming, rather than without stop-
words and with stemming.
We use the iSearch collection discussed in Sec-
tion 4. It comprises 453,254 documents and a
set of 65 queries with relevance assessments. To
match documents to queries without any treat-
ment of non-compositionality (baseline run), we
use the Kullback-Leibler language model with
Dirichlet smoothing (KL-Dir) (Zhai and Lafferty,
2002). We applied the preprocessing described
801
run MAP REC P20
baseline 0.0663 770 0.1385
real NC 0.0718 844 0.1538
pseudo NC1 0.0664 788 0.1385
pseudo NC2 0.0658 782 0.1462
pseudo NC3 0.0671 777 0.1477
pseudo NC4 0.0681 807 0.1462
pseudo NC5 0.0670 783 0.1423
Table 7: IR performance without considering non-
compositionality (baseline), versus boosting real and
pseudo non-compositionality (real NC, pseudo NCi).
in Section 4 to the queries and identified non-
compositional phrases with the base AM classifier
from Section 5. Our approach for boosting the
weight of these non-compositional phrases uses
the same retrieval model enhanced with belief
weights as described in Eq. 1 (real NC run). In
addition, we include five runs that boost the weight
of pseudo non-compositional phrases that were
created randomly from the query text (pseudo NC
runs). These pseudo non-compositional phrases
have exactly the same length as the observed non-
compositional phrases for each query. We measure
retrieval performance in terms of mean average
precision (MAP), precision at 20 (P20), and recall
(REC, number of relevant documents retrieved
? total is 2878). For each evaluation measure
separately, we tune the following parameters and
report the best performance: (i) the smoothing
parameter ? of the KL-Dir retrieval model (? ?
{100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000}, following Zhai and Lafferty (2002)); (ii)
the belief weights wnc, wc ? {0.1, . . . , 0.9} in steps
of 0.1 while preserving wnc + wc = 1 at all times.
Table 7 displays retrieval performance of our
approach against the baseline and five runs with
pseudo non-compositional phrases. We see a 9.61%
improvement in the number of relevant retrieved
documents over the baseline. MAP and P20 also
show improvements. Our approach is better than
any of the 5 random runs on all three metrics ? the
probability of getting such a good result by chance
is 125 < .05, and thus the improvements are statis-tically significant. On doing a query-wise analysis
of MAP scores, we find that large improvements
over the baseline occur when a non-compositional
phrase aligns with what the user is looking for. The
system seems to retrieve more relevant documents
in that case. E.g., the improvement in MAP is
0.0977 for query #19. The user was looking for
?articles . . . on making tunable vertical cavity sur-
face emitting laser diodes? and laser diodes was
one of the non-compositional phrases recognized.
On the other hand, a decrease in MAP occurs for
non-compositional phrases unrelated to the infor-
mation need. In query #4 the user is looking for
?protein-protein interaction, the surface charge dis-
tribution of these proteins and how this has been in-
vestigated with Electrostatic Force Microscopy? and
though non-compositional phrases such as Force Mi-
croscopy are recognized, these do not reflect the core
information need ?The proteins of interest are the
Avidin-Biotin and IgG-anti-IgG systems?.
8 Conclusion
We have presented an approach to improving to-
kenization in NLP preprocessing that is based on
the notion of semantic head. Semantic heads are
? in analogy to syntactic heads ? the core meaning
units of phrases that cannot be further semantically
decomposed. To perform semantic head recogni-
tion for tokenization, we defined a novel cascaded
model and implemented it as a statistical classifier
that used previously proposed and new context fea-
tures. We have shown that the classifier significantly
outperforms the baseline and that context features
increase performance. We reached an accuracy of
68% and argued that even a semantic head recog-
nizer restricted to high-confidence decisions is use-
ful ? because reliably recognizing a subset of se-
mantic heads is better than recognizing none. We
showed that context features increase the accuracy
of the classifier, but undermine the confidence as-
sessments of the classifier, a result we are still ana-
lyzing. Finally, we showed that even in its prelim-
inary current form the semantic head recognizer is
able to improve the performance of an IR system.
Acknowledgments
This work was funded by DFG projects SFB 732 and
WordGraph. We also thank the anonymous review-
ers for their comments.
802
References
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel
Pecina, and Josef van Genabith. 2010. Automatic ex-
traction of arabic multiword expressions. In Proceed-
ings of the 2010 Workshop on Multiword Expressions,
pages 19?27, Beijing, China. Coling 2010 Organizing
Committee.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 Workshop on Multiword Expres-
sions, pages 89?96, Sapporo, Japan. Association for
Computational Linguistics.
Helena Caseli, Aline Villavicencio, Andre? Machado,
and Maria Jose? Finatto. 2009. Statistically-driven
alignment-based multiword expression identification
for technical domains. In Proceedings of the 2009
Workshop on Multiword Expressions, pages 1?8, Sin-
gapore. Association for Computational Linguistics.
Yaacov Choueka. 1988. Looking for needles in a
haystack. In Proceedings of RIAO88, pages 609?623.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the 2007 on Mul-
tiword Expressions, pages 41?48, Prague, Czech Re-
public. Association for Computational Linguistics.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction mwe token classification. In Proceedings of
the 2009 Workshop on Multiword Expressions, pages
17?22, Singapore. Association for Computational Lin-
guistics.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures.
In Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics, pages 188?195.
Association for Computational Linguistics.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung (IMS), Uni-
versita?t Stuttgart.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the 2006 Workshop on Multiword Expres-
sions, pages 12?19, Sydney, Australia. Association for
Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Linguistic cues
for distinguishing literal and non-literal usages. In
Coling 2010: Posters, pages 683?691, Beijing, China.
Coling 2010 Organizing Committee.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 317?324, College Park, Maryland,
USA. Association for Computational Linguistics.
Marianne Lykke, Birger Larsen, Haakon Lund, and Pe-
ter Ingwersen. 2010. Developing a test collection for
the evaluation of integrated search. In Advances in In-
formation Retrieval, 32nd European Conference on IR
Research, ECIR 2010, Milton Keynes, UK, March 28-
31, 2010. Proceedings, pages 627?630.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, MA.
Donald Metzler and W. Bruce Croft. 2004. Combining
the language model and inference network approaches
to retrieval. Inf. Process. Manage., 40(5):735?750.
B.V. Moiro?n and Jo?rg Tiedemann. 2006. Identify-
ing Idiomatic Expressions Using Automatic Word-
Alignment. In Multi-Word-Expressions in a Multilin-
gual Context, page 33.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Eval-
uation, 44(1-2):138?158.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for multiword
expression identification. In Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics, pages 1?
15, Mexico City.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the 2001 Conference on Empirical Methods in Natu-
ral Language Processing, pages 100?108, Pittsburgh,
Pennsylvania, USA. Association for Computational
Linguistics.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational linguistics, 19(1):143?177.
ChengXiang Zhai and John D. Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR,
pages 49?56. ACM.
803
