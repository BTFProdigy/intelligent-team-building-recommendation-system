Polarity sensitivity and evaluation order in type-logical grammar
Chung-chieh Shan
Division of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138
ccshan@post.harvard.edu
Abstract
We present a novel, type-logical analysis of po-
larity sensitivity: how negative polarity items
(like any and ever) or positive ones (like some)
are licensed or prohibited. It takes not just
scopal relations but also linear order into ac-
count, using the programming-language no-
tions of delimited continuations and evaluation
order, respectively. It thus achieves greater em-
pirical coverage than previous proposals.
1 Introduction
Polarity sensitivity (Ladusaw, 1979) has been a popu-
lar linguistic phenomenon to analyze in the categorial
(Dowty, 1994), lexical-functional (Fry, 1997, 1999), and
type-logical (Bernardi, 2002; Bernardi and Moot, 2001)
approaches to grammar. The multitude of these analy-
ses is in part due to the more explicit emphasis that these
traditions place on the syntax-semantics interface?be it
in the form of Montague-style semantic rules, the Curry-
Howard isomorphism, or linear logic as glue?and the
fact that polarity sensitivity is a phenomenon that spans
syntax and semantics.
On one hand, which polarity items are licensed or pro-
hibited in a given linguistic environment depends, by and
large, on semantic properties of that environment (Ladu-
saw, 1979; Krifka, 1995, inter alia). For example, to
a first approximation, negative polarity items can occur
only in downward-entailing contexts, such as under the
scope of a monotonically decreasing quantifier. A quan-
tifier q, of type (e ? t) ? t where e is the type of indi-
viduals and t is the type of truth values, is monotonically
decreasing just in case
(1) ?s1.?s2.
(
?x. s2(x) ? s1(x)
)
? q(s1) ? q(s2).
Thus (2a) is acceptable because the scope of nobody is
downward-entailing, whereas (2b?c) are unacceptable.
(2) a. Nobody saw anybody.
b. *Everybody saw anybody.
c. *Alice saw anybody.
On the other hand, a restriction on surface syntactic
form, such as that imposed by polarity sensitivity, is by
definition a matter of syntax. Besides, there are syntac-
tic restrictions on the configuration relating the licensor
to the licensee. For example, (2a) above is acceptable?
nobody manages to license anybody?but (3) below is
not. As the contrast in (4) further illustrates, the licen-
sor usually needs to precede the licensee.
(3) *Anybody saw nobody.
(4) a. Nobody?s mother saw anybody?s father.
b. *Anybody?s mother saw nobody?s father.
The syntactic relations allowed between licensor and li-
censee for polarity sensitivity purposes are similar to
those allowed between antecedent and pronoun for vari-
able binding purposes. To take one example, just as an
antecedent?s ability to bind a (c-commanded) pronoun
percolates up to a containing phrase (such as in (5), what
Bu?ring (2001) calls ?binding out of DP?), a licensor?s
ability to license a (c-commanded) polarity item perco-
lates up to a containing phrase (such as in (4a)).
(5) [Every boyi?s mother] saw hisi father.
Moreover, just as a bindee can precede a binder in a sen-
tence when the bindee sits in a clause that excludes the
binder (as in (6); see Williams, 1997, ?2.1), a licensee
can precede a licensor in a sentence when the licensee
sits in a clause that excludes the licensor (as in (7); see
Ladusaw, 1979, page 112).
(6) That hei would be arrested for speeding came as a
surprise to everyi motorist.
(7) That anybody would be arrested for speeding came
as a surprise to the district attorney.
This paper presents a new, type-logical account of po-
larity sensitivity that encompasses the semantic proper-
ties exemplified in (2) and the syntactic properties exem-
plified in (3?4). Taking advantage of the Curry-Howard
isomorphism, it is the first account of polarity sensitivity
in the grammatical frameworks mentioned above to cor-
rectly fault (3) for the failure of nobody to appear before
Axiom
A ` A
For each unary mode ? (blank, u, or p in this paper):
^?? ` A
??I
? ` ??A
? ` ??A ??E
^?? ` A
? ` A
^?I
^?? ` ^?A
? ` ^?A ?[^?A] ` B
^?E
?[?] ` B
For each binary mode ? (blank or c in this paper):
? ` B ? ` C
??I
? ?? ? ` B ?? C
? ` B ?C ?[B ?? C] ` A ??E
?[?] ` A
? ?? B ` C
/?I
? ` C/?B
? ` B/?A ? ` A
/?E
? ?? ? ` B
B ?? ? ` C \?I
? ` B\?C
? ` A ? ` A\?B \?E
? ?? ? ` B
Figure 1: Natural deduction rules for multimodal cate-
gorial grammar (Bernardi, 2002, pages 9 and 50). To
reduce notation, we do not distinguish structural punc-
tuation from logical connectives.
A a` A ?c 1(Root)
(B ?C) ?c K a` B ?c (C ? K)(Left)
(^B ?C) ?c K a` C ?c (K ? ^B)(Right)
A ` ^A(T)
^A ? ^B ` ^(A ? B)(K?)
^^uA ` ^uA(Unquote)
Figure 2: Structural postulates
anybody. The analysis makes further correct predictions,
as we will see at the end of ?3.
The analysis here borrows the concepts of delim-
ited continuations (Felleisen, 1988; Danvy and Filinski,
1990) and evaluation order from the study of program-
ming languages. Thus this paper is about computational
linguistics, in the sense of applying insights from com-
puter science to linguistics. The basic idea transfers to
other formalisms, but type-logical grammar?more pre-
cisely, multimodal categorial grammar?offers a frag-
ment NL^R? whose parsing problem is decidable using
proof-net technology (Moot, 2002, ?9.2), which is of
great help while developing and testing the theory.
2 Delimited continuations
Figure 1 shows natural deduction rules for multimodal
categorial grammar, a member of the type-logical fam-
ily of grammar formalisms (Moortgat, 1996a; Bernardi,
2002). Figure 2 lists our structural postulates. These two
figures form the logic underlying our account.
We use two binary modes: the default mode (blank)
for surface syntactic composition, and the continuation
mode c. As usual, a formula of the form A ? B can be
read as ?A followed by B?. By contrast, a formula of
the form A ?c B can be read as ?A in the context B?. In
programming-language terms, the formula A ?c B plugs
a subexpression A into a delimited continuation B. The
Root rule creates a trivial continuation: it says that 1 is
a right identity for the c mode, where 1 can be thought
of as a nullary connective, effectively enabling empty an-
tecedents for the c mode. The binary modes, along with
the first three postulates in Figure 2, provide a new way
to encode Moortgat?s ternary connective q (1996b) for in-
situ quantification. For intuition, it may help to draw log-
ical formulas as binary trees, distinguishing graphically
between the two modes.
To further capture the interaction between scope inver-
sion and polarity sensitivity exemplified in (3?4), we use
three unary modes: the value mode (blank), the unquota-
tion mode u, and the polarity mode p. The value mode
marks when an expression is devoid of in-situ quantifi-
cation, or, in programming-language terms, when it is a
pure value rather than a computation with control effects.
As a special case, any formula can be turned pure by em-
bedding it under a diamond using the T postulate, analo-
gous to quotation or staging in programming languages.
Quotations can be concatenated using the K? postulate.
The unquotation mode u marks when a diamond can be
canceled using the Unquote postulate. Unquotation is
also known as eval or run in programming languages.
The polarity mode p, and the empirical utility of these
unary modes, are explained in ?3.
A derivation is considered complete if it culminates in
a sequent whose antecedent is built using the default bi-
nary mode ? only, and whose conclusion is a type of the
form ^uA. Below is a derivation of Alice saw Bob.
(8) Alice ` np
saw ` (np\^us)/np Bob ` np
/E
saw ? Bob ` np\^us \E
Alice ? (saw ? Bob) ` ^us
Note that clauses take the type^us rather than the usual s,
so the Unquote rule can operate on clauses. We abbrevi-
ate ^us to s? below.
To illustrate in-situ quantification, Figure 3 on the fol-
lowing page shows a derivation of Alice saw a man?s
mother. For brevity, we treat a man as a single lexical
item. It is a quantificational noun phrase whose polarity
is neutral in a sense that contrasts with other quantifiers
considered below. The crucial part of this derivation is
the use of the structural postulates Root, Left, and Right
to divide the sentence into two parts: the subexpression
a man and its context Alice saw ?s mother. The type of
a man, s?/c(np\cs?), can be read as ?a subexpression that
produces a clause when placed in a context that can en-
close an np to make a clause?.
a man ` s?/c(np\cs?)
Alice ` np
saw ` (np\s?)/np
Axiom
np ` np ?s mother ` np\np
\E
np ? ?s mother ` np
/E
saw ? (np ? ?s mother) ` np\s?
\E
Alice ? (saw ? (np ? ?s mother)) ` s?
^I
^
(
Alice ? (saw ? (np ? ?s mother))
)
` ^s?
Unquote
^
(
Alice ? (saw ? (np ? ?s mother))
)
` s?
K? thrice
^Alice ? (^saw ? (^np ? ^?s mother)) ` s?
T
^Alice ? (^saw ? (np ? ^?s mother)) ` s?
Root(
^Alice ? (^saw ? (np ? ^?s mother))
)
?c 1 ` s? Right(
^saw ? (np ? ^?s mother)
)
?c (1 ? ^Alice) ` s? Right
(np ? ^?s mother) ?c
(
(1 ? ^Alice) ? ^saw
)
` s?
Left
np ?c
(
^?s mother ? ((1 ? ^Alice) ? ^saw)
)
` s?
\cI
^?s mother ? ((1 ? ^Alice) ? ^saw) ` np\cs?
/cEa man ?
(
^?s mother ? ((1 ? ^Alice) ? ^saw)
)
` s?
Left
(a man ? ^?s mother) ?c
(
(1 ? ^Alice) ? ^saw
)
` s?
Right(
^saw ? (a man ? ^?s mother)
)
?c (1 ? ^Alice) ` s? Right(
^Alice ? (^saw ? (a man ? ^?s mother))
)
?c 1 ` s? Root
^Alice ? (^saw ? (a man ? ^?s mother)) ` s?
T thrice
Alice ? (saw ? (a man ? ?s mother)) ` s?
Figure 3: In-situ quantification: deriving Alice saw a man?s mother
Quantifier Type
a man s?/c(np\cs?)
nobody s?/c(np\cs?)
anybody s?/c(np\cs?)
somebody s+/c(np\cs+)
everybody s?/c(np\cs+)
GFED@ABCs+
?

?
?
?
?
?
?
?
?
?
?
somebody

// GFED@ABCs?
?










anybody

ONMLHIJKGFED@ABCs? nobody
NN
a man
__
everybody
nn
//
Figure 4: Quantifier type assign-
ments, and a corresponding finite-
state machine
3 Polarity sensitivity and evaluation order
The pmode mediates polarity sensitivity. For every unary
mode ?, we can derive A ` ??^?A from the rules in Fig-
ure 1. This fact is particularly useful when ? = p, be-
cause we assign the types ^u
?
p^ps and 
?
p^p^us to pos-
itive and negative clauses, respectively, and can derive
s? ` ^u?p^ps, s
? ` ?p^p^us.(9)
In words, a neutral clause can be silently converted into a
positive or negative one. We henceforth write s+ and s?
for ^u
?
p^ps and 
?
p^p^us. By (9), both types are ?sub-
types? of s? (that is to say, entailed by s?).
The p mode is used in Figure 5 on the next page to
derive Nobody saw anybody. Unlike a man, the quan-
tifier anybody has the type s?/c(np\cs?), showing that it
takes scope over a negative clause to make another neg-
ative clause. Meanwhile, the quantifier nobody has the
type s?/c(np\cs?), showing that it takes scope over a neg-
ative clause to make a neutral clause. Thus nobody can
take scope over the negative clause returned by anybody
to make a neutral clause, which is complete.
The contrast between (2a) and (3) boils down to the
Right (but not Left) postulate?s requirement that the left-
most constituent be of the form ^B. (In programming-
language terms, a subexpression can be evaluated only if
all other subexpressions to its left are pure.) For nobody
to take scope over (and license) anybody in (3) requires
the context *Anybody saw . In other words, the sequent
(10) np ?c
(
(1 ? ^anybody) ? ^saw
)
` s?
must be derived, in which the Right rule forces the con-
stituents anybody and saw to be embedded under dia-
monds. Figure 6 shows an attempt at deriving (10), which
fails because the type s? for negative clauses cannot be
Unquoted (shown with question marks). The sequent
in (10) cannot be derived, and the sentence *Anybody saw
nobody is not admitted. Nevertheless, Somebody saw ev-
erybody is correctly predicted to have ambiguous scope,
because neutral and positive clauses can be Unquoted.
The quantifiers a man, nobody, and anybody in Figures
3 and 5 exemplify a general pattern of analysis: every
polarity-sensitive item, be it traditionally considered a li-
censor or a licensee, specifies in its type an input polarity
(of the clause it takes scope over) and an output polarity
(of the clause it produces). Figure 4 lists more quantifiers
and their input and output polarities. As shown there,
these type assignments can be visualized as a finite-state
machine. The states are the three clause types. The ?-
transitions are the two derivability relations in (9). The
non-? transitions are the quantifiers. The start states are
the clausal types that can be Unquoted. The final state is
the clausal type returned by verbs, namely neutral.
The precise pattern of predictions made by this the-
ory can be stated in two parts. First, due to the lexical
types in Figure 4 and the ?subtyping? relations in (9), the
quantifiers in a sentence must form a valid transition se-
quence, from widest to narrowest scope. This constraint
is standard in type-logical accounts of polarity sensitiv-
ity. Second, thanks to the unary modes in the structural
nobody ` s?/c(np\cs?)
anybody ` s?/c(np\cs?)
???
^np ? (^saw ? np) ` s?
Root,Right,Right
np ?c
(
(1 ? ^np) ? ^saw
)
` s?
^pI
^p
(
np ?c ((1 ? ^np) ? ^saw)
)
` ^ps?
?pInp ?c
(
(1 ? ^np) ? ^saw
)
` s?
\cI(1 ? ^np) ? ^saw ` np\cs?
/cEanybody ?c
(
(1 ? ^np) ? ^saw
)
` s?
Right,Right,Left
^np ?c
(
(^saw ? anybody) ? 1
)
` s?
T twice
np ?c
(
(saw ? anybody) ? 1
)
` s?
\cI(saw ? anybody) ? 1 ` np\cs?
/cEnobody ?c
(
(saw ? anybody) ? 1
)
` s?
Left,Root
nobody ? (saw ? anybody) ` s?
Figure 5: Polarity licensing: deriving Nobody saw anybody
???
anybody ? (saw ? np) ` s?
^I
^
(
anybody ? (saw ? np)
)
` ^s?
??? ???
^
(
anybody ? (saw ? np)
)
` s?
K? twice
^anybody ? (^saw ? ^np) ` s?
T
^anybody ? (^saw ? np) ` s?
Root(
^anybody ? (^saw ? np)
)
?c 1 ` s? Right
(^saw ? np) ?c (1 ? ^anybody) ` s? Right
np ?c
(
(1 ? ^anybody) ? ^saw
)
` s?
Figure 6: Linear order in polarity licensing: ruling
out Anybody saw nobody using left-to-right eval-
uation order
postulates in Figure 2, whenever two quantifiers take in-
verse rather than linear scope with respect to each other,
the transitions must pass through a start state (that is, a
clause type that can be Unquoted) in between. This con-
straint is an empirical advance over previous accounts,
which are oblivious to linear order.
The input and output polarities of quantifiers are highly
mutually constrained. Take everybody for example. If
we hold the polarity assignments of the other quantifiers
fixed, then the existence of a linear-scope reading for
A man introduced everybody to somebody forces every-
body to be input-positive and output-neutral. But then
our account predicts thatNobody introduced everybody to
somebody has a linear-scope reading, unlike the simpler
sentenceNobody introduced Alice to somebody. This pre-
diction is borne out, as observed by Kroch (1974, pages
121?122) and discussed by Szabolcsi (2004).
Acknowledgments
Thanks to Chris Barker, Raffaella Bernardi, William
Ladusaw, Richard Moot, Chris Potts, Stuart Shieber, Dy-
lan Thurston, and three anonymous referees. This work
is supported by the United States National Science Foun-
dation Grant BCS-0236592.
References
Raffaella Bernardi and Richard Moot. 2001. Generalized quan-
tifiers in declarative and interrogative sentences. Journal of
Language and Computation, 1(3):1?19.
Raffaella Bernardi. 2002. Reasoning with Polarity in Catego-
rial Type Logic. Ph.D. thesis, Utrecht Institute of Linguistics
(OTS), Utrecht University.
Daniel Bu?ring. 2001. A situation semantics for binding out of
DP. In Rachel Hastings, Brendan Jackson, and Zsofia Zv-
olensky, editors, SALT XI: Semantics and Linguistic Theory,
pages 56?75, Ithaca. Cornell University Press.
Olivier Danvy and Andrzej Filinski. 1990. Abstracting con-
trol. In Proceedings of the 1990 ACM Conference on Lisp
and Functional Programming, pages 151?160, New York,
March. ACM Press.
David R. Dowty. 1994. The role of negative polarity and con-
cord marking in natural language reasoning. In Mandy Har-
vey and Lynn Santelmann, editors, SALT IV: Semantics and
Linguistic Theory, Ithaca. Cornell University Press.
Matthias Felleisen. 1988. The theory and practice of first-class
prompts. In POPL ?88: Conference Record of the Annual
ACM Symposium on Principles of Programming Languages,
pages 180?190, New York. ACM Press.
John Fry. 1997. Negative polarity licensing at the syntax-
semantics interface. In Philip R. Cohen and Wolfgang
Wahlster, editors, Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics and 8th Con-
ference of the European Chapter of the Association for Com-
putational Linguistics, pages 144?150, San Francisco. Mor-
gan Kaufmann.
John Fry. 1999. Proof nets and negative polarity licensing.
In Mary Dalrymple, editor, Semantics and Syntax in Lexical
Functional Grammar: The Resource Logic Approach, chap-
ter 3, pages 91?116. MIT Press, Cambridge.
Manfred Krifka. 1995. The semantics and pragmatics of polar-
ity items. Linguistic Analysis, 25:209?257.
Anthony S. Kroch. 1974. The Semantics of Scope in En-
glish. Ph.D. thesis, Massachusetts Institute of Technology.
Reprinted by New York: Garland, 1979.
William A. Ladusaw. 1979. Polarity Sensitivity as Inherent
Scope Relations. Ph.D. thesis, Department of Linguistics,
University of Massachusetts, August. Reprinted by New
York: Garland, 1980.
Michael Moortgat. 1996a. Categorial type logics. In Johan van
Benthem and Alice ter Meulen, editors, Handbook of Logic
and Language, chapter 2. Elsevier Science, Amsterdam.
Michael Moortgat. 1996b. Generalized quantification and dis-
continuous type constructors. In Harry C. Bunt and Arthur
van Horck, editors, Discontinuous Constituency, pages 181?
207. de Gruyter, Berlin.
Richard Moot. 2002. Proof Nets for Linguistic Analysis. Ph.D.
thesis, Utrecht Institute of Linguistics (OTS), Utrecht Uni-
versity.
Anna Szabolcsi. 2004. Positive polarity?negative polarity.
Natural Language and Linguistic Theory, 22(2):409?452,
May.
Edwin Williams. 1997. Blocking and anaphora. Linguistic
Inquiry, 28(4):577?628.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23?32,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Entailment above the word level in distributional semantics
Marco Baroni
Raffaella Bernardi
University of Trento
name.surname@unitn.it
Ngoc-Quynh Do
Free University of Bozen-Bolzano
quynhdtn.hut@gmail.com
Chung-chieh Shan
Cornell University
University of Tsukuba
ccshan@post.harvard.edu
Abstract
We introduce two ways to detect entail-
ment using distributional semantic repre-
sentations of phrases. Our first experiment
shows that the entailment relation between
adjective-noun constructions and their head
nouns (big cat |= cat), once represented as
semantic vector pairs, generalizes to lexical
entailment among nouns (dog |= animal).
Our second experiment shows that a classi-
fier fed semantic vector pairs can similarly
generalize the entailment relation among
quantifier phrases (many dogs|=some dogs)
to entailment involving unseen quantifiers
(all cats|=several cats). Moreover, nominal
and quantifier phrase entailment appears to
be cued by different distributional corre-
lates, as predicted by the type-based view
of entailment in formal semantics.
1 Introduction
Distributional semantics (DS) approximates lin-
guistic meaning with vectors summarizing the
contexts where expressions occur. The success
of DS in lexical semantics has validated the hy-
pothesis that semantically similar expressions oc-
cur in similar contexts (Landauer and Dumais,
1997; Lund and Burgess, 1996; Sahlgren, 2006;
Schu?tze, 1997; Turney and Pantel, 2010). For-
mal semantics (FS) represents linguistic mean-
ings as symbolic formulas and assemble them via
composition rules. FS has successfully modeled
quantification and captured inferential relations
between phrases and between sentences (Mon-
tague, 1970; Thomason, 1974; Heim and Kratzer,
1998). The strengths of DS and FS have been
complementary to date: On one hand, DS has in-
duced large-scale semantic representations from
corpora, but it has been largely limited to the
lexical domain. On the other hand, FS has pro-
vided sophisticated models of sentence meaning,
but it has been largely limited to hand-coded mod-
els that do not scale up to real-life challenges by
learning from data.
Given these complementary strengths, we nat-
urally ask if DS and FS can address each other?s
limitations. Two recent strands of research are
bringing DS closer to meeting core FS chal-
lenges. One strand attempts to model compo-
sitionality with DS methods, representing both
primitive and composed linguistic expressions
as distributional vectors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011; Gue-
vara, 2010; Mitchell and Lapata, 2010). The
other strand attempts to reformulate FS?s notion
of logical inference in terms that DS can cap-
ture (Erk, 2009; Geffet and Dagan, 2005; Kotler-
man et al 2010; Zhitomirsky-Geffet and Dagan,
2010). In keeping with the lexical emphasis of
DS, this strand has focused on inference at the
word level, or lexical entailment, that is, discover-
ing from distributional vectors of hyponyms (dog)
that they entail their hypernyms (animal).
This paper brings these two strands of research
together by demonstrating two ways in which the
distributional vectors of composite expressions
bear on inference. Here we focus on phrasal vec-
tors harvested directly from the corpus rather than
obtained compositionally. In a first experiment,
we exploit the entailment properties of a class
of composite expressions, namely adjective-noun
constructions (ANs), to harvest training data for
an entailment recognizer. The recognizer is then
successfully applied to detect lexical entailment.
In short, since almost all ANs entail the noun they
contain (red car entails car), the distributional
vectors of AN-N pairs can train a classifier to de-
tect noun pairs that stand in the same relation (dog
23
entails animal). With almost no manual effort,
we achieve performance nearly identical with the
state-of-the-art balAPinc measure that Kotlerman
et al(2010) crafted, which detects feature inclu-
sion between the two nouns? occurrence contexts.
Our second experiment goes beyond lexical in-
ference. We look at phrases built from a quanti-
fying determiner1 and a noun (QNs) and use their
distributional vectors to recognize entailment re-
lations of the form many dogs |= some dogs, be-
tween two QNs sharing the same noun. It turns
out that a classifier trained on a set of Q1N |=Q2N
pairs can recognize entailment in pairs with a new
quantifier configuration. For example, we can
train on many dogs |= some dogs then correctly
predict all cats|=several cats. Interestingly, on the
QN entailment task, neither our classifier trained
on AN-N pairs nor the balAPinc method beat
baseline methods. This suggests that our success-
ful QN classifiers tap into vector properties be-
yond such relations as feature inclusion that those
methods for nominal entailment rely upon.
Together, our experiments show that corpus-
harvested DS representations of composite ex-
pressions such as ANs and QNs contain suffi-
cient information to capture and generalize their
inference patterns. This result brings DS closer
to the central concerns of FS. In particular, the
QN study is the first to our knowledge to show
that DS vectors capture semantic properties not
only of content words, but of an important class of
function words (quantifying determiners) deeply
studied in FS but of little interest until now in DS.
Besides these theoretical implications, our re-
sults are of practical import. First, our AN study
presents a novel, practical method for detect-
ing lexical entailment that reaches state-of-the-
art performance with little or no manual interven-
tion. Lexical entailment is in turn fundamental
for constructing ontologies and other lexical re-
sources (Buitelaar and Cimiano, 2008). Second,
our QN study demonstrates that phrasal entail-
ment can be automatically detected and thus paves
the way to apply DS to advanced NLP tasks such
as recognizing textual entailment (Dagan et al
2009).
1In the sequel we will simply refer to a ?quantifying de-
terminer? as a ?quantifier?.
2 Background
2.1 Distributional semantics above the word
level
DS models such as LSA (Landauer and Dumais,
1997) and HAL (Lund and Burgess, 1996) ap-
proximate the meaning of a word by a vector that
summarizes its distribution in a corpus, for exam-
ple by counting co-occurrences of the word with
other words. Since semantically similar words
tend to share similar contexts, DS has been very
successful in tasks that require quantifying se-
mantic similarity among words, such as synonym
detection and concept clustering (Turney and Pan-
tel, 2010).
Recently, there has been a flurry of interest
in DS to model meaning composition: How can
we derive the DS representation of a composite
phrase from that of its constituents? Although the
general focus in the area is to perform algebraic
operations on word semantic vectors (Mitchell
and Lapata, 2010), some researchers have also di-
rectly examined the corpus contexts of phrases.
For example, Baldwin et al(2003) studied vec-
tor extraction for phrases because they were inter-
ested in the decomposability of multiword expres-
sions. Baroni and Zamparelli (2010) and Gue-
vara (2010) look at corpus-harvested phrase vec-
tors to learn composition functions that should de-
rive such composite vectors automatically. Ba-
roni and Zamparelli, in particular, showed qual-
itatively that directly corpus-harvested vectors for
AN constructions are meaningful; for example,
the vector of young husband has nearest neigh-
bors small son, small daughter and mistress. Fol-
lowing up on this approach, we show here quanti-
tatively that corpus-harvested AN vectors are also
useful for detecting entailment. We find moreover
distributional vectors informative and useful not
only for phrases made of content words (such as
ANs) but also for phrases containing functional
elements, namely quantifying determiners.
2.2 Entailment from formal to distributional
semantics
Entailment in FS To characterize the condi-
tions under which a sentence is true, FS begins
with the lexical meanings of the words in the sen-
tence and builds up the meanings of larger and
larger phrases until it arrives at the meaning of the
whole sentence. The meanings throughout this
24
compositional process inhabit a variety of seman-
tic domains, depending on the syntactic category
of the expressions: typically, a sentence denotes a
truth value (true or false) or truth conditions,
a noun such as cat denotes a set of entities, and a
quantifier phrase (QP) such as all cats denotes a
set of sets of entities.
The entailment relation (|=) is a core notion of
logic: it holds between one or more sentences and
a sentence such that it cannot be that the former
(antecedent) are true and the latter (consequent)
is false. FS extends this notion from formal-logic
sentences to natural-language expressions. By as-
signing meanings to parts of a sentence, FS allows
defining entailment not only among sentences but
also among words and phrases. Each semantic
domain A has its own entailment relation |=A.
The entailment relation |=S among sentences is
the logical notion just described, whereas the en-
tailment relations |=N and |=QP among nouns
and quantifier phrases are the inclusion relations
among sets of entities and sets of sets of entities
respectively. Our results in Section 5 show that
DS needs to treat |=N and |=QP differently as well.
Empirical, corpus-based perspectives on en-
tailment Until recently, the corpus-based re-
search tradition has studied entailment mostly at
the word level, with applied goals such as clas-
sifying lexical relations and building taxonomic
WordNet-like resources automatically. The most
popular approach, first adopted by Hearst (1992),
extracts lexical relations from patterns in large
corpora. For instance, from the pattern N1 such
as N2 one learns that N2 |=N1 (from insects such
as beetles, derive beetles |= insects). Several stud-
ies have refined and extended this approach (Pan-
tel and Ravichandran, 2004; Snow et al 2005;
Snow et al 2006; Turney, 2008).
While empirically very successful, the pattern-
based method is mostly limited to single content
words (or frequent content-word phrases). We are
interested in entailment between phrases, where it
is not obvious how to use lexico-syntactic patterns
and cope with data sparsity. For instance, it seems
hard to find a pattern that frequently connects one
QP to another it entails, as in all beetles PATTERN
many beetles. Hence, we aim to find a more gen-
eral method and investigate whether DS vectors
(whether corpus-harvested or compositionally de-
rived) encode the information needed to account
for phrasal entailment in a way that can be cap-
tured and generalized to unseen phrase pairs.
Rather recently, the study of sentential entail-
ment has taken an empirical turn, thanks to the de-
velopment of benchmarks for entailment systems.
The FS definition of entailment has been modified
by taking common sense into account. Instead of
a relation from the truth of the consequent to the
truth of the antecedent in any circumstance, the
applied view looks at entailment in terms of plau-
sibility: ? |= ? if a human who reads (and trusts)
? would most likely infer that ? is also true. En-
tailment systems have been compared under this
new perspective in various evaluation campaigns,
the best known being the Recognizing Textual En-
tailment (RTE) initiative (Dagan et al 2009).
Most RTE systems are based on advanced NLP
components, machine learning techniques, and/or
syntactic transformations (Zanzotto et al 2007;
Kouleykov and Magnini, 2005). A few systems
exploit deep FS analysis (Bos and Markert, 2006;
Chambers et al 2007). In particular, the FS re-
sults about QP properties that affect entailment
have been exploited by Chambers et alwho com-
plement a core broad-coverage system with a Nat-
ural Logic module to trade lower recall for higher
precision. For instance, they exploit the mono-
tonicity properties of no that cause the follow-
ing reversal in entailment direction: some bee-
tles |= some insects but no insects |= no beetles.
To investigate entailment step by step, we ad-
dress here a much simpler and clearer type of
entailment than the more complex notion taken
up by the RTE community. While RTE is out-
side our present scope, we do focus on QP entail-
ment as Natural Logic does. However, our eval-
uation differs from Chambers et als, since we
rely on general-purpose DS vectors as our only
resource, and we look at phrase pairs with differ-
ent quantifiers but the same noun. For instance,
we aim to predict that all beetles |= many beetles
but few beetles 6|=all beetles. QPs, of course, have
many well-known semantic properties besides en-
tailment; we leave their analysis to future study.
Entailment in DS Erk (2009) suggests that it
may not be possible to induce lexical entailment
directly from a vector space representation, but it
is possible to encode the relation in this space af-
ter it has been derived through other means. On
the other hand, recent studies (Geffet and Dagan,
25
2005; Kotlerman et al 2010; Weeds et al 2004)
have pursued the intuition that entailment is the
asymmetric ability of one term to ?substitute? for
another. For example, baseball contexts are also
sport contexts but not vice versa, hence baseball
is ?narrower? than sport and baseball |=sport. On
this view, entailment between vectors corresponds
to inclusion of contexts or features, and can be
captured by asymmetric measures of distribution
similarity. In particular, Kotlerman et al(2010)
carefully crafted the balAPinc measure (see Sec-
tion 3.5 below). We adopt this measure because
it has been shown to outperform others in several
tasks that require lexical entailment information.
Like Kotlerman et al we want to capture the
entailment relation between vectors of features.
However, we are interested in entailment not only
between words but also between phrases, and we
ask whether the DS view of entailment as fea-
ture inclusion, which captures entailment between
nouns, also captures entailment between QPs. To
this end, we complement balAPinc with a more
flexible supervised classifier.
3 Data and methods
3.1 Semantic space
We construct distributional semantic vectors from
the 2.83-billion-token concatenation of the British
National Corpus (http://www.natcorp.
ox.ac.uk/), WackyPedia and ukWaC (http:
//wacky.sslmit.unibo.it/). We tok-
enize and POS-tag this corpus, then lemmatize
it with TreeTagger (Schmid, 1995) to merge sin-
gular and plural instances of words and phrases
(some dogs is mapped to some dog).
We process the corpus in two steps to compute
semantic vectors representing our phrases of in-
terest. We use phrases of interest as a general
term to refer to both multiword phrases and sin-
gle words, and more precisely to: those AN and
QN sequences that are in the data sets (see next
subsections), the adjectives, quantifiers and nouns
contained in those sequences, and the most fre-
quent (9.8K) nouns and (8.1K) adjectives in the
corpus. The first step is to count the content
words (more precisely, the most frequent 9.8K
nouns, 8.1K adjectives, and 9.6K verbs in the cor-
pus) that occur in the same sentence as phrases
of interest. In the second step, following standard
practice, the co-occurrence counts are converted
into pointwise mutual information (PMI) scores
(Church and Hanks, 1990). The result of this step
is a sparse matrix (with both positive and negative
entries) with 48K rows (one per phrase of interest)
and 27K columns (one per content word).
3.2 The AN |= N data set
To characterize entailment between nouns using
their semantic vectors, we need data exemplifying
which noun entails which. This section introduces
one cheap way to collect such a training data set
exploiting semantic vectors for composed expres-
sions, namely AN sequences. We rely on the lin-
guistic fact that ANs share a syntactic category
and semantic type with plain common nouns (big
cat shares syntactic category and semantic type
with cat). Furthermore, most adjectives are re-
strictive in the sense that, for every noun N, the
AN sequence entails the N alone (every big cat
is a cat). From a distributional point of view, the
vector for an N should by construction include the
information in the vector for an AN, given that the
contexts where the AN occurs are a subset of the
contexts where the N occurs (cat occurs in all the
contexts where big cat occurs). This ideal inclu-
sion suggests that the DS notion of lexical entail-
ment as feature inclusion (see Section 2.2 above)
should be reflected in the AN |= N pattern.
Because most ANs entail their head Ns, we can
create positive examples of AN |= N without any
manual inspection of the corpus: simply pair up
the semantic vectors of ANs and Ns. Furthermore,
because an AN usually does not entail another N,
we can create negative examples (AN1 6|=N2) just
by randomly permuting the Ns. Of course, such
unsupervised data would be slightly noisy, espe-
cially because some of the most frequent adjec-
tives are not restrictive.
To collect cleaner data and to be sure that we
are really examining the phenomenon of entail-
ment, we took a mere few moments of man-
ual effort to select the 256 restrictive adjectives
from the most frequent 300 adjectives in the cor-
pus. We then took the Cartesian product of these
256 adjectives with the 200 concrete nouns in the
BLESS data set (Baroni and Lenci, 2011). Those
nouns were chosen to avoid highly polysemous
words. From the Cartesian product, we obtain a
total of 1246 AN sequences, such as big cat, that
occur more than 100 times in the corpus. These
AN sequences encompass 190 of the 256 adjec-
26
tives and 128 of the 200 nouns.
The process results in 1246 positive instances
of AN |= N entailment, which we use as training
data. To create a comparable amount of negative
data, we randomly permuted the nouns in the pos-
itive instances to obtain pairs of AN1 6|= N2 (e.g.,
big cat 6|=dog). We manually double-checked that
all positive and negative examples are correctly
classified (2 of 1246 negative instances were re-
moved, leaving 1244 negative training examples).
3.3 The lexical entailment N1 |= N2 data set
For testing data, we first listed all WordNet nouns
in our corpus, then extracted hyponym-hypernym
chains linking the first synsets of these nouns. For
example, pope is found to entail leader because
WordNet contains the chain pope ? spiritual
leader ? leader. Eliminating the 20 hypernyms
with more than 180 hyponyms (mostly very ab-
stract nouns such as entity, object, and quality)
yields 9734 hyponym-hypernym pairs, encom-
passing 6402 nouns. Manually double-checking
these pairs leaves us with 1385 positive instances
of N1 |= N2 entailment.
We created the negative instances of again 1385
pairs by inverting 33% of the positive instances
(from pope|=leader to leader 6|=pope), and by ran-
domly shuffling the words across the positive in-
stances. We also manually double-checked these
pairs to make sure that they are not hyponym-
hypernym pairs.
3.4 The Q1N |= Q2N data set
We study 12 quantifiers: all, both, each, either,
every, few, many, most, much, no, several, some.
We took the Cartesian product of these quantifiers
with the 6402 WordNet nouns described in Sec-
tion 3.3. From this Cartesian product, we obtain
a total of 28926 QN sequences, such as every cat,
that occur at least 100 times in the corpus. These
are our QN phrases of interest to which the proce-
dure in Section 3.1 assigns a semantic vector.
Also, from the set of quantifier pairs (Q1,Q2)
where Q1 6= Q2, we identified 13 clear cases
where Q1 |=Q2 and 17 clear cases where Q1 6|=Q2.
These 30 cases are listed in the first column of
Table 1. For each of these 30 quantifier pairs
(Q1,Q2), we enumerate those WordNet nouns N
such that semantic vectors are available for both
Q1N and Q2N (that is, both sequences occur in
at least 100 times). Each such noun then gives
Quantifier pair Instances Correct
all |= some 1054 1044 (99%)
all |= several 557 550 (99%)
each |= some 656 647 (99%)
all |= many 873 772 (88%)
much |= some 248 217 (88%)
every |= many 460 400 (87%)
many |= some 951 822 (86%)
all |= most 465 393 (85%)
several |= some 580 439 (76%)
both |= some 573 322 (56%)
many |= several 594 113 (19%)
most |= many 463 84 (18%)
both |= either 63 1 (2%)
Subtotal 7537 5804 (77%)
some 6|= every 484 481 (99%)
several 6|= all 557 553 (99%)
several 6|= every 378 375 (99%)
some 6|= all 1054 1043 (99%)
many 6|= every 460 452 (98%)
some 6|= each 656 640 (98%)
few 6|= all 157 153 (97%)
many 6|= all 873 843 (97%)
both 6|= most 369 347 (94%)
several 6|= few 143 134 (94%)
both 6|= many 541 397 (73%)
many 6|= most 463 300 (65%)
either 6|= both 63 39 (62%)
many 6|= no 714 369 (52%)
some 6|= many 951 468 (49%)
few 6|= many 161 33 (20%)
both 6|= several 431 63 (15%)
Subtotal 8455 6690 (79%)
Total 15992 12494 (78%)
Table 1: Entailing and non-entailing quantifier pairs
with number of instances per pair (Section 3.4) and
SVMpair-out performance breakdown (Section 5).
rise to an instance of entailment (Q1N |= Q2N if
Q1 |=Q2; example: many dogs |= several dogs) or
non-entailment (Q1N 6|=Q2N if Q1 6|=Q2; example:
many dogs 6|=most dogs). The number of QN pairs
that each quantifier pair gives rise to in this way is
listed in the second column of Table 1. As shown
there, we have a total of 7537 positive instances
and 8455 negative instances of QN entailment.
3.5 Classification methods
We consider two methods to classify candidate
pairs as entailing or non-entailing, the balAPinc
measure of Kotlerman et al(2010) and a standard
Support Vector Machine (SVM) classifier.
27
balAPinc As discussed in Section 2.2, balAP-
inc is optimized to capture a relation of feature
inclusion between the narrower (entailing) and
broader (entailed) terms, while capturing other in-
tuitions about the relative relevance of features.
balAPinc averages two terms, APinc and LIN.
APinc is given by:
APinc(u |= v) =
?|Fu|
r=1
(
P (r) ? rel?(fr)
)
|Fu|
APinc is a version of the Average Precision
measure from Information Retrieval tailored to
lexical inclusion. Given vectors Fu and Fv rep-
resenting the dimensions with positive PMI val-
ues in the semantic vectors of the candidate pair
u |= v, the idea is that we want the features (that
is, vector dimensions) that have larger values in
Fu to also have large values in Fv (the opposite
does not matter because it is u that should be in-
cluded in v, not vice versa). The Fu features are
ranked according to their PMI value so that fr
is the feature in Fu with rank r, i.e., r-th high-
est PMI. Then the sum of the product of the two
terms P (r) and rel?(fr) across the features in Fu
is computed. The first term is the precision at r,
which is higher when highly ranked u features are
present in Fv as well. The relevance term rel?(fr)
is higher when the feature fr in Fu also appears
in Fv with a high rank. (See Kotlerman et alfor
how P (r) and rel?(fr) are computed.) The result-
ing score is normalized by dividing by the entail-
ing vector size |Fu| (in accordance with the idea
that having more v features should not hurt be-
cause the u features should be included in the v
features, not vice versa).
To balance the potentially excessive asymmetry
of APinc towards the features of the antecedent,
Kotlerman et alaverage it with LIN, the widely
used symmetric measure of distributional similar-
ity proposed by Lin (1998):
LIN(u, v) =
?
f?Fu?Fv [wu(f) + wv(f)]?
f?Fu wu(f) +
?
f?Fv wv(f)
LIN essentially measures feature vector overlap.
The positive PMI values wu(f) and wv(f) of a
feature f in Fu and Fv are summed across those
features that are positive in both vectors, normal-
izing by the cumulative positive PMI mass in both
vectors. Finally, balAPinc is the geometric aver-
age of APinc and LIN:
balAPinc(u|=v) =
?
APinc(u |= v) ? LIN(u, v)
To adapt balAPinc to recognize entailment, we
must select a threshold t above which we classify
a pair as entailing. In the experiments below, we
explore two approaches. In balAPincupper, we op-
timize the threshold directly on the test data, by
setting t to maximize the F-measure on the test
set. This gives us an upper bound on how well bal-
APinc could perform on the test set (but note that
optimizing F does not necessarily translate into a
good accuracy performance, as clearly illustrated
by Table 3 below). In balAPincAN |= N, we use the
AN |= N data set as training data and pick the t
that maximizes F on this training set.
We use the balAPinc measure as a refer-
ence point because, on the evidence provided by
Kotlerman et al it is the state of the art in various
tasks related to lexical entailment. We recognize
however that it is somewhat complex and specifi-
cally tuned to capturing the relation of feature in-
clusion. Consequently, we also experiment with
a more flexible classifier, which can detect other
systematic properties of vectors in an entailment
relation. We present this classifier next.
SVM Support vector machines are widely used
high-performance discriminative classifiers that
find the hyperplane providing the best separation
between negative and positive instances (Cristian-
ini and Shawe-Taylor, 2000). Our SVM classifiers
are trained and tested using Weka 3 and LIBSVM
2.8 (Chang and Lin, 2011). We use the default
polynomial kernel ((u ?v/600)3) with  (tolerance
of termination criterion) set to 1.6. This value was
tuned on the AN |=N data set, which we never use
for testing. In the same initial tuning experiments
on the AN |=N data set, SVM outperformed deci-
sion trees, naive Bayes, and k-nearest neighbors.
We feed each potential entailment pair to SVM
by concatenating the two vectors representing the
antecedent and consequent expressions.2 How-
ever, for efficiency and to mitigate data sparse-
ness, we reduce the dimensionality of the seman-
tic vectors to 300 columns using Singular Value
Decomposition (SVD) before feeding them to the
classifier.3 Because the SVD-reduced semantic
2We have tried also to represent a pair by subtracting and
by dividing the two vectors. The concatenation operation
gave more successful results.
3To keep a manageable parameter space, we picked 300
columns without tuning. This is the best value reported in
many earlier studies, including classic LSA. Since SVD
sometimes improves the semantic space (Landauer and Du-
28
vectors occupy a 300-dimensional space, the en-
tailment pairs occupy a 600-dimensional space.
An SVM with a polynomial kernel takes into
account not only individual input features but also
their interactions (Manning et al 2008, chapter
15). Thus, our classifier can capture not just prop-
erties of individual dimensions of the antecedent
and consequent pairs, but also properties of their
combinations (e.g., the product of the first dimen-
sions of the antecedent and the consequent). We
conjecture that this property of SVMs is funda-
mental to their success at detecting entailment,
where relations between the antecedent and the
consequent should matter more than their inde-
pendent characteristics.
4 Predicting lexical entailment from
AN |= N evidence
Since the contexts of AN must be a subset of the
contexts of N, semantic vectors harvested from
AN phrases and their head Ns are by construc-
tion in an inclusion relation. The first experiment
shows that these vectors constitute excellent train-
ing data to discover entailment between nouns.
This suggests that the vector pairs representing
entailment between nouns are also in an inclusion
relation, supporting the conjectures of Kotlerman
et al(2010) and others.
Table 2 reports the results we obtained with
balAPincupper, balAPincAN |= N (Section 3.5) and
SVMAN |= N (the SVM classifier trained on the
AN |= N data). As an upper bound for meth-
ods that generalize from AN |= N, we also re-
port the performance of SVM trained with 10-fold
cross-validation on the N1 |= N2 data themselves
(SVMupper). Finally, we tried two baseline classi-
fiers. The first baseline (fq(N1)< fq(N2)) guesses
entailment if the first word is less frequent than
the second. The second (cos(N1, N2)) applies a
threshold (determined on the test set) to the co-
sine similarity of the pair. The results of these
baselines shown in Table 2 use SVD; those with-
out SVD are similar. Both baselines outperformed
more trivial methods such as random guessing or
fixed response, but they performed significantly
worse than SVM and balAPinc.
Both methods that generalize entailment from
AN |= N to N1 |= N2 perform well, with 70%
mais, 1997; Rapp, 2003; Schu?tze, 1997), we tried balAPinc
on the SVD-reduced vectors as well, but results were consis-
tently worse than with PMI vectors.
P R F Accuracy
(95% C.I.)
SVMupper 88.6 88.6 88.5 88.6 (87.3?89.7)
balAPincAN |= N 65.2 87.5 74.7 70.4 (68.7?72.1)
balAPincupper 64.4 90.0 75.1 70.1 (68.4?71.8)
SVMAN |= N 69.3 69.3 69.3 69.3 (67.6?71.0)
cos(N1, N2) 57.7 57.6 57.5 57.6 (55.8?59.5)
fq(N1)< fq(N2) 52.1 52.1 51.8 53.3 (51.4?55.2)
Table 2: Detecting lexical entailment. Results ranked
by accuracy and expressed as percentages. 95% con-
fidence intervals around accuracy calculated by bino-
mial exact tests.
accuracy on the test set, which is balanced be-
tween positive and negative instances. Interest-
ingly, the balAPinc decision thresholds tuned on
the AN |= N set and on the test data are very
close (0.26 vs. 0.24), resulting in very similar per-
formance for balAPincAN |= N and balAPincupper.
This suggests that the relation captured by bal-
APinc on the phrasal entailment training data is
indeed the same that the measure captures when
applied to lexical entailment data.
The success of this first experiment shows that
the entailment relation present in the distribu-
tional representation of AN phrases and their
head Ns transfers to lexical entailment (entailment
among Ns). Most importantly, this result demon-
strates that the semantic vectors of composite ex-
pressions (such as ANs) are useful for lexical en-
tailment. Moreover, the result is in accordance
with the view of FS, that ANs and Ns have the
same semantic type, and thus they enter entail-
ment relations of the same kind. Finally, the hy-
pothesis that entailment among nouns is reflected
by distributional inclusion among their semantic
vectors (Kotlerman et al 2010) is supported both
by the successful generalization of the SVM clas-
sifier trained on AN |= N pairs and by the good
performance of the balAPinc measure.
5 Generalizing QN entailment
The second study is somewhat more ambitious,
as it aims to capture and generalize the entailment
relation between QPs (of shape QN) using only
the corpus-harvested semantic vectors represent-
ing these phrases as evidence. We are thus first
and foremost interested in testing whether these
vectors encode information that can help a power-
29
P R F Accuracy
(95% C.I.)
SVMpair-out 76.7 77.0 76.8 78.1 (77.5?78.8)
SVMquantifier-out 70.1 65.3 68.0 71.0 (70.3?71.7)
SVMQpair-out 67.9 69.8 68.9 70.2 (69.5?70.9)
SVMQquantifier-out 53.3 52.9 53.1 56.0 (55.2?56.8)
cos(QN1, QN2) 52.9 52.3 52.3 53.1 (52.3?53.9)
balAPincAN |= N 46.7 5.6 10.0 52.5 (51.7?53.3)
SVMAN |= N 2.8 42.9 5.2 52.4 (51.7?53.2)
fq(QN1)<fq(QN2) 51.0 47.4 49.1 50.2 (49.4?51.0)
balAPincupper 47.1 100 64.1 47.2 (46.4?47.9)
Table 3: Detecting quantifier entailment. Results
ranked by accuracy and expressed as percentages.
95% confidence intervals around accuracy calculated
by binomial exact tests.
ful classifier, such as SVM, to detect entailment.
To abstract away from lexical or other effects
linked to a specific quantifier, we consider two
challenging training and testing regimes. In the
first (SVMpair-out), we hold out one quantifier pair
as testing data and use the other 29 pairs in Table 1
as training data. Thus, for example, the classifier
must discover all dogs |= some dogs without see-
ing any all N |= some N instance in the training
data. In the second (SVMquantifier-out), we hold out
one of the 12 quantifiers as testing data (that is,
hold out every pair involving a certain quantifier)
and use the rest as training data. For example,
the quantifier must guess all dogs |= some dogs
without ever seeing all in the training data. We
expect the second training regime to be more dif-
ficult, not just because there is less training data,
but also because the trained classifier is tested on
a quantifier that it has never encountered within
any training QN sequence.4
Table 3 reports the results for SVMpair-out and
SVMquantifier-out, as well as for the methods we
tried in the lexical entailment experiments. (As
in the first study, the frequency- and cosine-based
4In our initial experiments, we added negative entail-
ment instances by blindly permuting the nouns, under the
assumption that Q1N1 typically does not entail Q2N2 when
Q1 6= Q2 and N1 6= N2. These additional instances turned
out to be much easier to classify: adding an equal proportion
of them to the training data and testing data, such that the
number of instances where N1 = N2 and where N1 6= N2
is equal, reduced every error rate roughly by half. The re-
ported results do not involve these additional instances.
baselines are only slightly better overall than more
trivial baselines.) We consider moreover an alter-
native approach that ignores the noun altogether
and uses vectors for the quantifiers only (e.g., the
decision about all dogs |=some dogs considers the
corpus-derived all and some vectors only). The
models resulting from this Q-only strategy are
marked with the superscript Q in the table.
The results confirm clearly that semantic vec-
tors for QNs contain enough information to allow
a classifier to detect entailment: SVMquantifier-out
performs as well as the lexical entailment classi-
fiers of our first study, and SVMpair-out does even
better. This success is especially impressive given
our challenging training and testing regimes.
In contrast to the first study, now SVMAN |= N,
the classifier trained on the AN |= N data set,
and balAPinc perform no better than the base-
lines. (Here balAPincupper and balAPincAN |= N
pick very different thresholds: the first settling
on a very low t = 0.01, whereas for the sec-
ond t = 0.26.) As predicted by FS (see Section
2.2 above), noun-level entailment does not gen-
eralize to quantifier phrase entailment, since the
two structures have different semantic types, cor-
responding to different kinds of entailment rela-
tions. Moreover, the failure of balAPinc suggests
that, whatever evidence the SVMs rely upon, it is
not simple feature inclusion.
Interestingly, even the Q vectors alone encode
enough information to capture entailment above
chance. Still, the huge drop in performance from
SVMQpair-out to SVM
Q
quantifier-out suggests that the Q-
only method learned ad-hoc properties that do not
generalize (e.g., ?all entails every Q2?).
Tables 1 and 4 break down the SVM results by
(pairs of) quantifiers. We highlight the remark-
able dichotomy in Table 4 between the good per-
formance on the universal-like quantifiers (each,
every, all, much) and the poor performance on the
existential-like ones (some, no, both, either).
In sum, the QN experiments show that seman-
tic vectors contain enough information to detect
a logical relation such as entailment not only be-
tween words, but also between phrases contain-
ing quantifiers that determine their entailment re-
lation. While a flexible classifier such as SVM
performs this task well, neither measuring fea-
ture inclusion nor generalizing nominal entail-
ment works. SVMs are evidently tapping into
other properties of the vectors.
30
Quantifier Instances Correct
|= 6|= |= 6|=
each 656 656 649 637 (98%)
every 460 1322 402 1293 (95%)
much 248 0 216 0 (87%)
all 2949 2641 2011 2494 (81%)
several 1731 1509 1302 1267 (79%)
many 3341 4163 2349 3443 (77%)
few 0 461 0 311 (67%)
most 928 832 549 511 (60%)
some 4062 3145 1780 2190 (55%)
no 0 714 0 380 (53%)
both 636 1404 589 303 (44%)
either 63 63 2 41 (34%)
Total 15074 16910 9849 12870 (71%)
Table 4: Breakdown of results with leaving-one-
quantifier-out (SVMquantifier-out) training regime.
6 Conclusion
Our main results are as follows.
1. Corpus-harvested semantic vectors repre-
senting adjective-noun constructions and
their heads encode a relation of entailment
that can be exploited to train a classifier
to detect lexical entailment. In particular,
a relation of feature inclusion between the
narrower antecedent and broader consequent
terms captures both AN |= N and N1 |= N2
entailment.
2. The semantic vectors of quantifier-noun con-
structions also encode information sufficient
to learn an entailment relation that general-
izes to QNs containing quantifiers that were
not seen during training.
3. Neither the entailment information encoded
in AN |= N vectors nor the balAPinc mea-
sure generalizes well to entailment detection
in QNs. This result suggests that QN vectors
encode a different kind of entailment, as also
suggested by type distinctions in Formal Se-
mantics.
In future work, we want first of all to conduct
an analysis of the features in the Q1N |=Q2N vec-
tors that are crucially exploited by our success-
ful entailment recognizers, in order to understand
which characteristics of entailment are encoded in
these vectors.
Very importantly, instead of extracting vectors
representing phrases directly from the corpus, we
intend to derive them by compositional operations
proposed in the literature (see Section 2.1 above).
We will look for composition methods producing
vector representations of composite expressions
that are as good as (or better than) vectors directly
extracted from the corpus at encoding entailment.
Finally, we would like to evaluate our entail-
ment detection strategies for larger phrases and
sentences, possibly containing multiple quanti-
fiers, and eventually embed them as core compo-
nents of an RTE system.
Acknowledgments
We thank the Erasmus Mundus EMLCT Program
for the student and visiting scholar grants to the
third and fourth author, respectively. The first
two authors are partially funded by the ERC 2011
Starting Independent Research Grant supporting
the COMPOSES project (nr. 283554). We are
grateful to Gemma Boleda, Louise McNally, and
the anonymous reviewers for valuable comments,
and to Ido Dagan for important insights into en-
tailment from an empirical point of view.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 Workshop on Multiword
Expressions, pages 89?96.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the Workshop on Geometrical Mod-
els of Natural Language Semantics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesn?t. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Paul Buitelaar and Philipp Cimiano. 2008. Bridging
the Gap between Text and Knowledge. IOS, Ams-
terdam.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
31
and Christopher D. Manning. 2007. Learning
alignments and leveraging natural logic. In ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2(3):27:1?27:27.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nal, evaluation and approaches. Natural Language
Engineering, 15:459?476.
Katrin Erk. 2009. Supporting inferences in semantic
space: representing words as regions. In Proceed-
ings of IWCS, pages 104?115, Tilburg, Netherlands.
Maayan Geffet and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of ACL, pages 107?114, Ann Arbor,
MI.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1395?1404, Edinburgh.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37, Uppsala, Sweden.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539?545, Nantes, France.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell, Oxford.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and
Maayan Zhitomirsky-Geffet. 2010. Directional
distributional similarity for lexical inference. Natu-
ral Language Engineering, 16(4):359?389.
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit sistance for textual entailment. In Proceed-
ings of RALNP-2005, International Conference on
Recent Advances in Natural Language Processing,
pages 271?278.
Thomas Landauer and Susan Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of ICML, pages
296?304, Madison, WI, USA.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?
208.
Chris Manning, Prabhakar Raghavan, and Hinrich
Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge.
Jeff Mitchell and Mirella Lapata. 2010. Composi-
tion in distributional models of semantics. Cogni-
tive Science, 34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theo-
ria, 36:373?398.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeliing semantic classes. In Proceed-
ings of HLT-NAACL 2004, pages 321?328.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA.
Magnus Sahlgren. 2006. The Word-Space Model.
Dissertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Proceedings of the EACL-SIGDAT Workshop,
Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Nat-
ural Language Learning. CSLI, Stanford, CA.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2005. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS 17.
Rion Snow, Daniel Juravsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from het-
erogenous evidence. In Proceedings of ACL 2006,
pages 801?808.
Richmond H. Thomason, editor. 1974. Formal Phi-
losophy: Selected Papers of Richard Montague.
Yale University Press, New York.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. In Proceed-
ings of COLING, pages 905?912, Manchester, UK.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference of Computational Linguistics,
COLING-2004, pages 1015?1021.
Fabio M. Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2007. Shallow semantics in fast tex-
tual entailment rule learners. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2010.
Bootstrapping distributional feature vector quality.
Computational Linguistics, 35(3):435?461.
32
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 57?65,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generating Quantifiers and Negation to Explain Homework Testing
Jason Perry and Chung-chieh Shan
Rutgers University
Department of Computer Science
Abstract
We describe Prograder, a software package for
automatic checking of requirements for pro-
gramming homework assignments. Prograder
lets instructors specify requirements in natural
language as well as explains grading results to
students in natural language. It does so using
a grammar that generates as well as parses to
translate between a small fragment of English
and a first-order logical specification language
that can be executed directly in Python. This
execution embodies multiple semantics?both
to check the requirement and to search for ev-
idence that proves or disproves the require-
ment. Such a checker needs to interpret and
generate sentences containing quantifiers and
negation. To handle quantifier and negation
scope, we systematically simulate continua-
tion grammars using record structures in the
Grammatical Framework.
1 Introduction
The typical programming assignment in a computer-
science course comes with not only a problem de-
scription but also correctness and stylistic require-
ments such as
(1) Every source file compiles and has comments,
and a text file mentions every source file and
every header file.
Although the requirements do not usually specify
exactly how the students? work will be judged, they
make it much easier to grade and respond to the sub-
mitted work. Many requirements can be checked
automatically by computer, and often they are?
perhaps even right after each student uploads their
work so that the student can revise their work using
the immediate feedback.
This common workflow is wanting in two aspects.
First, the requirements are both specified to the stu-
dents in English and coded into a testing harness by
the course staff. Keeping the two versions is a hassle
that involves much boilerplate text as well as boil-
erplate code. Second, students rightfully demand
comprehensible explanations when their work is re-
jected by the requirement tester. It is tricky to code
up a tester that produces error messages neither too
terse nor too verbose, when one student might forget
to comment just one file and another might not know
the lexical syntax of comments at all.
A natural approach to improve this workflow,
then, is to specify the requirements in a formal lan-
guage and to implement an interpreter for the lan-
guage that produces explanations. Because many
instructors, like students, are averse to learning new
languages, we pursue the use of a controlled subset
of English as that formal language. In short, we aim
to produce a programming-assignment tester that
allows instructors to specify requirements for pro-
gramming assignments in natural language, checks
those requirements automatically by executing com-
mands on the submitted assignment files, and gen-
erates for the student a natural-language explanation
of the results. For example, in an introductory pro-
gramming class, a professor may write the specifi-
cation sentence (1). The system would then grade
all students? programming assignments according to
these criteria, and return individualized explanations
57
to students like
(2) Credit was lost because bar.c did not compile
and no text file mentioned the files foo.h and
baz.h.
As this example illustrates, the tester needs to in-
terpret and generate sentences with quantifiers and
negation. Although the interpretation of quantifiers
and negation is a traditional research area in com-
putational linguistics (VanLehn, 1978; Hobbs and
Shieber, 1987; Moran, 1988), their generation is
much less studied (Gailly, 1988). Even if our system
were to compose explanations entirely from the in-
put specification sentences and their negation, it can-
not negate a specification sentence merely by adding
or removing verbal auxiliaries: the negation of ?a
source file defines main()? is not ?a source file does
not define main()?.
1.1 Contributions
We have built Prograder, a rudimentary program-
ming-assignment tester that correctly interprets and
generates a small fragment of English that includes
quantifiers and negation. This paper describes the
architecture of Prograder. Our implementation uses
a declarative grammar that simultaneously supports
interpretation and generation by relating English
phrase structure to type-logical semantics. This pa-
per details the new techniques we use in this gram-
mar to represent quantifier scope and De Morgan du-
ality in a tractable way.
Prograder also lets us investigate how to make a
computer program explain its own execution. The
concerns for a tester that must justify its output to
students are not merely grammatical, but involve the
semantics and pragmatics of summarization and jus-
tification. For example, when is it semantically cor-
rect to combine entities within an NP, as in ?foo.h
and baz.h? above? When is an explanation prag-
matically enough for a student who has a right to
know why she lost credit on a programming assign-
ment? Which pieces of evidence must be stated and
which are better left out? We plan to study these
questions within type-logical semantics as well.
1.2 Organization of This Paper
In Section 1, we have introduced the problem and
outlined the contributions of the research carried out
in building Prograder. In Section 2, we give a high-
level overview of the architecture of the system, in-
cluding the representation of natural-language syn-
tax and type-logical semantics, as well as the proce-
dure of operation. Section 3 describes Prograder?s
checking procedure and how it generates explana-
tions in tandem with the checking. In Section 4,
we begin to describe the details of the grammati-
cal framework that allows us to handle bidirectional
translation between English and the logical form.
Continuing in Section 5, we discuss the handling
of negation and quantifier scoping by means of a
record-based implementation of continuation gram-
mars. Section 6 reviews the architecture of the sys-
tem with additional implementation details, and Sec-
tion 7 concludes with future work.
2 System Overview
We explain the architecture of our Prograder sys-
tem using a simplified example. Suppose that the
instructor specifies the requirement that
(3) Every source file compiles.
Prograder begins by parsing this sentence into an ab-
stract syntax tree:
(4) ApplyS
EverySourceFile Compiles
Interpreting this tree compositionally gives the
meaning of the sentence:
(5) everysourcefile(lambda x:
compiles(x))
On one hand, this expression is a formula in pred-
icate logic. In general, each requirement specifi-
cation is a sentence, whose truth value is deter-
mined by checking a single student?s programming
assignment. We use the types of Montague gram-
mar (1974), which are the base type of entities e, the
base type of propositions t, and function types no-
tated by !. Our logical language is defined by a
domain-specific vocabulary of typed predicates and
entities (Hudak, 1996). Naturally, the files submit-
ted by the student are entities.
58
For example, the unary predicates compiles and
hascomments have the type e ! t, and the bi-
nary predicate mentions has the type e! e! t.
These predicates in our vocabulary represent vari-
ous properties of submitted files that instructors may
want to check. Logical connectives are also part
of the vocabulary; for instance, and and or have
the type t ! t ! t, not_b has the type t ! t, and
everysourcefile has the type (e! t)! t. There-
fore, the expression (5) has the type t, as do the ex-
pressions
(6) compiles("foo.c")
(7) and(compiles("foo.c"),
hascomments("bar.c"))
On the other hand, the expressions (5?7) are exe-
cutable Python code. Prograder includes a library
of Python functions such as everysourcefile,
which iterates over the source files submitted, and
compiles, which invokes gcc. As each student sub-
mits their homework, we evaluate the expression (5)
to check the requirement (3). We use Python?s own
lambda abstraction and first-class functions to ex-
press quantified statements. For example, evaluating
(5) invokes gcc on each source file submitted.
Evaluation not only computes a Boolean value but
also yields a tree of evidence statements to justify
the result. At the root of the tree is either the re-
quirement (5) or its negation:
(8) asourcefile(lambda x:
not_b(compiles(x)))
Each node?s children are the premises that together
justify the conclusion at that node. For example, if
every source file submitted by the student compiles
successfully except one, then (8) would be justified
by a sole child:
(9) not_b(compiles("bar.c"))
Prograder then parses each evidence statement into
abstract syntax:
(10) ApplyS
ASourceFile Not_VP
Compiles
ApplyS
bar.c Not_VP
Compiles
From these trees, Prograder finally renders each ev-
idence statement as an English clause:
(11) A source file does not compile because bar.c
does not compile.
To summarize, Prograder?s steps of operation are
as follows:
1. Parse the natural-language requirement state-
ment into a phrase-structure syntax tree.
2. Produce a logical semantic representation from
the syntax tree.
3. Execute the semantic representation to check
the requirement and produce a semantic repre-
sentation of each evidence statement.
4. Parse the evidence statements into phrase-
structure syntax trees.
5. Produce natural-language explanation from the
syntax trees.
From end to end, these steps operate on just three
levels of representation: the abstract syntax trees in
(4) and (10) can be spelled out either in natural lan-
guage (English) as in (3) and (11) or in predicate
logic (Python) as in (5?9). Thus, Prograder inter-
prets natural-language requirements and generates
natural-language explanations using the same exe-
cutable semantic representations. In fact, it uses the
same grammar, as described in Section 4.
3 Gathering Evidence while Testing Truth
As sketched above, evaluating a proposition gives a
Boolean result along with a tree of evidence propo-
sitions that justify that result. That is, we represent
the type t in Python as an ordered pair, not just a
Boolean value. This is straightforward for primitive
first-order predicates. For example, the compiles
function, when invoked with the argument "foo.c",
tries to compile foo.c, then either returns True
along with a trivial evidence tree containing just the
proposition compiles("foo.c"), or returns False
along with a trivial evidence tree containing just
the proposition not_b(compiles("foo.c")). In
short, we hold whether a file compiles to be self-
evident.
59
The picture is more complex for logical connec-
tives, especially higher-order functions such as the
quantifier everysourcefile. Ideally, the falsity
of (3) should be justified by an explanation like
(12) Not every source file compiles because foo.c
and bar.c do not compile.
Also, the explanation should be generated by com-
posing the implementations of everysourcefile
and of compiles, so that new quantifiers (?most
source files?) and predicates (?has comments?) can
be added as separate modules. For example, evaluat-
ing the proposition not_b(compiles("foo.c"))
first evaluates compiles("foo.c") then negates
the Boolean while keeping the evidence the same.
Prograder currently justifies quantified proposi-
tions in the following compositional way. When
the higher-order function everysourcefile is in-
voked with the predicate argument p, it invokes p
on each source file submitted and collects the result-
ing Boolean-evidence pairs. If any of the Boolean
results is False, then the overall Boolean result is
of course also False. This result is justified by an
evidence tree whose root proposition is
(13) not_b(everysourcefile(p))
and whose subtrees are the evidence trees for all the
False results. (After all, the primary mode of ex-
planation required in grading a programming assign-
ment is to say why credit has been subtracted.) Pro-
grader then produces the serviceable explanation
(14) Not every source file compiles because foo.c
does not compile and bar.c does not compile.
(We are working on simplifying this explanation
to (12).) This process generalizes immediately to
propositions with multiple quantifiers, such as
(15) Every text file mentions a source file.
everytextfile(lambda x:
asourcefile(lambda y:
mentions(y)(x)))
For example, Prograder might explain that
(16) Not every text file mentions a source file be-
cause README mentions no source file.
In sum, Prograder generates explanations in the
same process?the same sequence of function calls
and returns?as it computes Boolean values. In this
way, the checking process gains an additional inter-
pretation as a process of gathering evidence for the
explanation. The explanation itself is a tree structure
corresponding to the pattern of function calls in the
computation; in fact, it is a proof tree for the require-
ments specification statement (or its negation). Once
all the evidence has been gathered, a textual version
of the explanation can be generated by traversing the
tree and concatenating evidence statements, possi-
bly using summarization techniques to make the ex-
planation more concise.
We have so far glossed over how English sen-
tences, especially those with quantification and
negation such as (14) and (16), are parsed into and
generated from logical representations. The rest of
this paper describes our principled approach to this
problem, based on a consistent mapping between
syntactic categories and semantic types.
4 Using the Grammatical Framework
We relate natural language (English) to logical se-
mantics (Python) using the Grammatical Framework
(GF) of Ranta (2004). GF is a mature system that al-
lows linguists and logicians to define grammars for
both natural and formal languages using a Haskell-
like syntax. Once defined, the grammars can be used
for parsing as well as generation (linearization) with
no further programming.
In GF, grammars are specified in two parts: an ab-
stract grammar and a concrete grammar. An abstract
grammar specifies the set of well-formed abstract
syntax trees, whereas a concrete grammar specifies
how to spell out abstract syntax as strings. For exam-
ple, an abstract grammar can admit the abstract syn-
tax tree in (4) by specifying that EverySourceFile
is an NP, Compiles is a VP, and ApplyS combines
an NP and a VP into an S:
fun EverySourceFile: NP;
fun Compiles: VP;
fun ApplyS: NP -> VP -> S;
A concrete grammar for English can then specify
that the linearization of EverySourceFile is a sin-
gular (Sg) string,
lin EverySourceFile = {
s = "every source file"; n = Sg };
the linearization of Compiles is a pair of strings,
60
lin Compiles = {
s = { Sg => "compiles";
Pl => "compile" } };
and the linearization of ApplyS is a function that
combines these two linearizations into the string (3).
lin ApplyS NP VP = {
s = NP.s ++ (VP.s ! NP.n) };
Here ++ denotes string concatenation and ! denotes
table lookup.
Multiple concrete grammars can share the same
abstract grammar in GF, as in synchronous gram-
mars (Aho and Ullman, 1969) and abstract catego-
rial grammars (de Groote, 2002). This sharing is
meant to enable multilingual applications, in which
the same meaning representation defined by a sin-
gle abstract grammar can be rendered into various
natural languages by different concrete grammars.
This separation into abstract and concrete gram-
mars lets us use one concrete grammar to model En-
glish and another to model the Python syntax of Pro-
grader?s logical forms. For example, the lineariza-
tion of Compiles into Python is simply compiles,
which can be combined with "foo.c" by the lin-
earization of ApplyS to form compiles("foo.c").
The system can thus parse a natural-language spec-
ification into an abstract syntax tree using the first
concrete grammar, then produce the corresponding
semantic representation using the second concrete
grammar. Since each concrete grammar can be used
for both parsing and generation, the system can also
be run in the other direction, to parse the semantic
representations of an explanation, then generate that
explanation in natural language.
Since the linearization compiles("foo.c") in
Python is virtually isomorphic to its abstract syn-
tax tree, one may wonder why we bother with the
second concrete grammar at all. Why not just ex-
press the type-theoretical semantic structure in the
abstract syntax and relate it to English in a sin-
gle concrete grammar? The answer is that using
two concrete grammars lets us represent quantifier
meanings and scope preferences. Without the dis-
tinction between abstract syntax and logical seman-
tics, we would have to specify how to linearize
everysourcefile and asourcefile so that the
Python in (15) linearizes to the English. Moreover,
Prograder should disprefer the inverse-scope reading
(17) asourcefile(lambda y:
everytextfile(lambda x:
mentions(y)(x)))
for the same English sentence. We do not see a way
to achieve these goals given GF?s limited support for
higher-order abstract syntax. Instead, we keep our
grammars first-order and let our abstract grammar
express only the surface structure of English, where
quantifiers stay ?in situ? just like proper names.
Below we describe how even a first-order concrete
grammar for semantic representations can represent
quantifier meanings and scope preferences.
5 Quantifier Scope, Negation and
Continuation Grammars
Quantifier scoping has long been a key source of
difficulty in mapping natural language sentences to
logical forms. Scope ambiguities arise even in rela-
tively simple sentences such as (15), which any in-
structor might be expected to generate in specifying
programming assignment requirements. The scope
of negation is also problematic. An algorithm for
generating all possible quantifier scopings was de-
tailed by Hobbs and Shieber (1987). However, we
need a solution that prefers one highly likely default
scoping, that supports both interpretation and gener-
ation, and that is integrated with the type structure
of our semantic representation.
Compositional semantics based on continuations
(Barker, 2002) can represent preferred scoping of
quantifiers and negation without the semantic type-
shifting or syntactic underspecification (Hendriks,
1993; Steedman, 1996; Bos, 1995; Koller et al,
2003) that typically complicates interpreting and
generating quantification. The rough idea is to gen-
eralize Montague?s PTQ (1974), so that every con-
stituent?s semantic type has the form (   ! t)! t:
not only does every NP denote the type (e! t)! t
instead of e, but every VP also denotes the type
((e! t)! t)! t instead of e! t. For example
(as in PTQ),
(18) ?foo.c? denotes lambda c: c("foo.c")
(19) ?every text file? denotes
lambda c:
everytextfile(lambda x: c(x))
61
Analogously (but unlike in PTQ),
(20) ?compiles? denotes lambda c: c(compiles)
(21) ?mentions a source file? denotes
lambda c:
asourcefile(lambda x:
c(mentions(x)))
Recall from Section 4 that we model denotations as
the linearizations of abstract syntax trees. Therefore,
in GF, we want to specify linearizations like
lin EverySourceFile =
lambda c:
everysourcefile(lambda x: c(x));
lin Compiles = lambda c: c(compiles);
to be combined by the linearization of ApplyS
lin ApplyS NP VP =
NP(lambda n: VP(lambda v: v(n)));
into the expression (5).
In this last linearization, the innermost application
v(n) means to apply the VP?s predicate meaning
to the subject NP?s entity meaning. The surround-
ing NP(lambda n: VP(lambda v: ...)) lets a
quantificational subject take wide scope over the VP.
This general composition rule thus yields surface
scope to the exclusion of inverse scope. In particu-
lar, it equally well combines the denotations in (19)
and (21) into the expression (15), rather than (17).
In the present implementation of Prograder, the rules
all encode surface scope for the quantifiers. A simi-
lar linearization forms the VP denotation in (21) by
composing a transitive verb and its object NP:
lin ApplyVP VT NP =
lambda c: NP(lambda n: c(VT(n)))
The same machinery generalizes to handle posses-
sives, ditransitive verbs, relative clauses, and so on.
5.1 Simulating higher-order functions
As shown above, denotations using continuations
are higher-order functions. However, linearization
in GF does not allow higher-order functions?in
fact, the only ?functions? allowed in GF are record
or table structures indexed by a finite set of parame-
ters. To keep parsing tractable, GF only lets strings
be concatenated, not beta-reduced as lambda-terms;
the one extension that GF makes to the context-free
model is allowing argument suppression and repeti-
tion. In other words, GF cannot equate logical forms
by beta-equivalence. Therefore, we cannot just feed
the pseudocode above into GF to generate explana-
tions. This is an instance of the problem of logical-
form equivalence (Shieber, 1993).
Fortunately, because denotations using continua-
tions are always of a certain form (Danvy and Filin-
ski, 1992), we can simulate these higher-order func-
tions using first-order records. Specifically, we sim-
ulate a higher-order function of the form
(22) lambda c: sl c(sm) sr
by the triple of strings
(23) { s_l = sl; s_m = sm; s_r = sr }
in GF. The middle string sm corresponds to the
?core? of the phrase, and the left and right strings
sl;sr are those parts which may take scope over other
phrases. For example, following the pseudocode
above, we write
lin EverySourceFile = {
s_l = "everysourcefile ( lambda x :";
s_m = "x";
s_r = ")" };
lin Compiles = {
s_l = "";
s_m = "compiles";
s_r = "" };
in our GF concrete grammar. Continuing to follow
the pseudocode above, we can also implement the
linearizations of ApplyS and ApplyVP to operate on
triples rather than the functions they simulate:
lin ApplyS NP VP = {
s = NP.s_l ++ VP.s_l ++
VP.s_m ++ "(" ++ NP.s_m ++ ")" ++
VP.s_r ++ NP.s_r };
lin ApplyVP VT NP = {
s_l = NP.s_l;
s_m = VT.s ++ NP.s_m;
s_r = NP.s_r };
Here the linearization of the transitive verb VT con-
sists of a single string s.
This simulation is reminiscent of Barker and
Shan?s ?tower notation? (2008). In general, we
can simulate a n-level semantic tower by a tuple of
62
2n  1 strings. Overall, this simulation makes us
hopeful that linearization in GF can be extended to a
broad, useful class of higher-order expressions while
keeping parsing tractable.
5.2 Maintaining De Morgan duals
Both to interpret requirements and to generate ex-
planations, our system needs to deal with negation
correctly. Whether in the form of a negative quanti-
fier such as ?no source file? or a VP modifier such as
?don?t?, negation takes scope. (In the case of the de-
terminer ?no?, the scope of negation is linked to the
containing NP.) We use continuations to account for
the scope of negation, as for all scope-taking.
When scope ambiguities arise, negation exhibits
the same preference for surface scope as other quan-
tifiers. For example, all of the sentences below pre-
fer the reading where the subject takes wide scope.
(24) A source file doesn?t compile.
(25) A text file mentions no source file.
(26) No text file mentions a source file.
The linearization of ApplyS shown above already
captures this preference; we just need to specify new
linearizations with not_b in them. The pseudocode
lin NoSourceFile =
lambda c:
not_b(asourcefile(lambda x: c(x)));
lin Not_VP VP =
lambda c:
not_b(VP(lambda v: c(v)));
captures the fact that the negation of ?A source file
compiles? is not (24) but ?No source file compiles?.
To generate natural-sounding negations of sen-
tences containing quantification, some simple logi-
cal equivalences are necessary. To take an extreme
example, suppose that Prograder needs to negate the
requirement specification
(27) Every text file mentions no source file.
Strictly speaking, it would be correct to generate
(28) Not every text file mentions no source file.
However, it would be much more comprehensible
for Prograder to report instead
(29) A text file mentions a source file.
One heuristic for preferring (29) over (28) is to use
as few negations as possible. But to apply this
heuristic, Prograder must first realize that (29) is a
correct negation of (27). In general, our concrete
grammar ought to equate formulas that are equiv-
alent by De Morgan?s laws. Unfortunately, GF can
no more equate formulas by De Morgan equivalence
than by beta-equivalence.
In lieu of equating formulas, we normalize them:
we use De Morgan?s laws to move negations log-
ically as far ?inside? as possible. In other words,
we impose an invariant on our semantic represen-
tation, that not_b applies only to atomic formulas
(as in (8?9)). This invariant is easy to enforce in
our Python code for gathering evidence, because we
can rewrite each evidence statement after generating
it. It is trickier to enforce the invariant in our GF
concrete grammar for semantic representations, be-
cause (to keep parsing tractable) linearizations can
only be concatenated, never inspected and rewritten.
Therefore, our linearizations must maintain formu-
las alongside their De Morgan duals.
Specifically, we revise the simulation described in
the previous section as follows. The record
(30) { spl = s+l ; spm = s
+
m; spr = s
+
r ;
snl = s l ; snm = s
 
m; snr = s
 
r ;
switched = False }
represents a higher-order function of the form
(31) lambda c: s+l c(s
+
m) s
+
r
assuming that it is equivalent to
(32) lambda c: not_b(s l not_b(c(s
 
m)) s
 
r )
Dually, the record
(33) { spl = s+l ; spm = s
+
m; spr = s
+
r ;
snl = s l ; snm = s
 
m; snr = s
 
r ;
switched = True }
represents a higher-order function of the form
(34) lambda c: s+l not_b(c(s
+
m)) s
+
r
assuming that it is equivalent to
(35) lambda c: not_b(s l c(s
 
m) s
 
r )
For example, given the linearizations
63
lin NoSourceFile = {
spl = "everysourcefile ( lambda x :";
spm = "x"; spr = ")";
snl = "asourcefile ( lambda x :";
snm = "x"; snr = ")";
switched = True };
lin EverySourceFile = {
spl = "everysourcefile ( lambda y :";
spm = "y"; spr = ")";
snl = "asourcefile ( lambda y :";
snm = "y"; snr = ")";
switched = False };
lin ASourceFile = {
spl = "asourcefile ( lambda x :";
spm = "x"; spr = ")";
snl = "everysourcefile ( lambda x :";
snm = "x"; snr = ")";
switched = False };
(and an alphabetic variant of ASourcefile), Pro-
grader uses the switched flags to deduce that one
way to negate ?Every source file mentions no source
file? is ?A source file mentions a source file?.
Our solution demonstrates that an existing gram-
matical software package can express continuation
grammars. While the record-based implementation
is somewhat unwieldy when encoded in the gram-
mar by hand, restricting ourselves to GF?s context-
free rewrite grammars also ensures efficient parsing.
6 Putting It All Together
Currently, our English grammar supports declarative
sentences with a small vocabulary of transitive and
intransitive verbs (?exists?, ?compiles?, ?has com-
ments?, ?mentions?), proper noun phrases referring
to specific source files, noun phrases representing
quantified nouns, and negations.
Given that the grammars correctly specify logi-
cal scoping and natural language syntax for parsing
and generation, the Python code that implements re-
quirement checking and evidence gathering is rela-
tively straightforward. As described above, the log-
ical form of the requirements specification is exe-
cutable Python, and their execution emits an evi-
dence statement in the same logical language for
each check performed, in a tree structure. Thus
the execution of the checking code is an evidence-
gathering or proof-search process. The evidence
statements are then translated to natural language by
means of the two GF grammars.
A Python ?glue script? ties the Python and GF
components together and manages the dataflow of
the end-to-end system. This script provides a simple
scanner and symbol table to replace file names with
standardized placeholders from the grammar. The
variables used in lambda expressions also need to be
renamed in order to prevent conflicts.
Here is a sample output of the system as it cur-
rently runs:
./runPrograder.py 'every source file
compiles and every source file has
comments'
******************************
RESULT: False, because:
some source files don't compile and
some source files don't have
comments:
"nowork2.c" doesn't compile
"nowork.c" doesn't compile
"nowork2.c" doesn't have comments
"nowork.c" doesn't have comments
7 Conclusions and Future Work
To our knowledge, Prograder incorporates the first
implementation of continuation-based semantics
within a grammatical framework that supports ef-
ficient parsing and generation. Consequently, our
declarative grammar uniformly expresses quantifier
meanings and scope preferences.
We want to see how far we can stretch a record-
based grammar system such as GF to handle quan-
tifiers and negation using continuations. In the end,
the boilerplate ingredients of our solution ought to
be automated, so as to combine the expressivity of
continuation-based semantics with the usability and
efficiency of GF. This will also make it easier to ex-
pand the range of natural-language constructs that
the system handles. Of course, this development
should be driven by feedback from actual instructors
using the system, which we also intend to obtain.
Our second area of future work is the summa-
rization of explanations. We plan to use Prograder
to investigate the semantics and pragmatics of sum-
marization, and to search for underlying principles
based on proofs and types.
64
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syn-
tax directed translations and the pushdown assembler.
Journal of Computer and System Sciences, 3(1):37?
56, February.
Chris Barker and Chung-chieh Shan. 2008. Donkey
anaphora is in-scope binding. Semantics and Prag-
matics, 1(1):1?46.
Chris Barker. 2002. Continuations and the na-
ture of quantification. Natural Language Semantics,
10(3):211?242.
Johan Bos. 1995. Predicate logic unplugged. In Paul
Dekker and Martin Stokhof, editors, Proceedings of
the 10th Amsterdam Colloquium, pages 133?142. In-
stitute for Logic, Language and Computation, Univer-
siteit van Amsterdam.
Olivier Danvy and Andrzej Filinski. 1992. Representing
control: A study of the CPS transformation. Math-
ematical Structures in Computer Science, 2(4):361?
391, December.
Philippe de Groote. 2002. Towards abstract catego-
rial grammars. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 148?155, San Francisco, CA, July. Morgan
Kaufmann.
Pierre-Joseph Gailly. 1988. Expressing quantifier scope
in French generation. In COLING ?88: Proceedings of
the 12th International Conference on Computational
Linguistics, volume 1, pages 182?184.
Herman Hendriks. 1993. Studied Flexibility: Categories
and Types in Syntax and Semantics. Ph.D. thesis, In-
stitute for Logic, Language and Computation, Univer-
siteit van Amsterdam.
Jerry R. Hobbs and Stuart M. Shieber. 1987. An al-
gorithm for generating quantifier scopings. Compu-
tational Linguistics, 13(1?2):47?63.
Paul Hudak. 1996. Building domain-specific embedded
languages. ACM Computing Surveys, 28.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecification
formalisms: Hole semantics as dominance constraints.
In Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 195?202, Somerset, NJ. Association for
Computational Linguistics.
Richard Montague. 1974. The proper treatment of
quantification in ordinary English. In Richmond H.
Thomason, editor, Formal Philosophy: Selected Pa-
pers of Richard Montague, pages 247?270. Yale Uni-
versity Press, New Haven.
Douglas B. Moran. 1988. Quantifier scoping in the
SRI core language engine. In Proceedings of the 26th
Annual Meeting of the Association for Computational
Linguistics, pages 33?40, Somerset, NJ. Association
for Computational Linguistics.
Aarne Ranta. 2004. Grammatical Framework: A type-
theoretical grammar formalism. Journal of Functional
Programming, 14(2):145?189.
Stuart M. Shieber. 1993. The problem of logical-form
equivalence. Computational Linguistics, 19(1):179?
190.
Mark J. Steedman. 1996. Surface Structure and Inter-
pretation. MIT Press, Cambridge.
Kurt A. VanLehn. 1978. Determining the scope of
English quantifiers. Master?s thesis, Department of
Electrical Engineering and Computer Science, Mas-
sachusetts Institute of Technology.
65
