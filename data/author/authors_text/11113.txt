Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 466?474,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multimodal Subjectivity Analysis of Multiparty Conversation
Stephan Raaijmakers
TNO Information and
Communication Technology
Delft, The Netherlands
stephan.raaijmakers@tno.nl
Khiet Truong
TNO Defense, Security and Safety
Soesterberg, The Netherlands
khiet.truong@tno.nl
Theresa Wilson
School of Informatics
University of Edinburgh
Edingburgh, UK
twilson@inf.ed.ac.uk
Abstract
We investigate the combination of several
sources of information for the purpose of sub-
jectivity recognition and polarity classification
in meetings. We focus on features from two
modalities, transcribed words and acoustics,
and we compare the performance of three dif-
ferent textual representations: words, charac-
ters, and phonemes. Our experiments show
that character-level features outperform word-
level features for these tasks, and that a care-
ful fusion of all features yields the best perfor-
mance. 1
1 Introduction
Opinions, sentiments and other types of subjective
content are an important part of any meeting. Meet-
ing participants express pros and cons about ideas,
they support or oppose decisions, and they make
suggestions that may or may not be adopted. When
recorded and archived, meetings become a part of
the organizational knowledge, but their value is lim-
ited by the ability of tools to search and summa-
rize meeting content, including subjective content.
While progress has been made on recognizing pri-
marily objective meeting content, for example, in-
formation about the topics that are discussed (Hsueh
and Moore, 2006) and who is assigned to work on
given tasks (Purver et al, 2006), there has been
1This work was supported by the Dutch BSIK-project Mul-
timediaN, and the European IST Programme Project FP6-
0033812. This paper only reflects the authors? views and fund-
ing agencies are not liable for any use that may be made of the
information contained herein.
fairly little work specifically directed toward recog-
nizing subjective content.
In contrast, there has been a wealth of research
over the past several years on automatic subjectiv-
ity and sentiment analysis in text, including on-line
media. Partly inspired by the rapid growth of so-
cial media, such as blogs, as well as on-line news
and reviews, researchers are now actively address-
ing a wide variety of new tasks, ranging from blog
mining (e.g., finding opinion leaders in an on-line
community), to reputation management (e.g. find-
ing negative opinions about a company on the web),
to opinion-oriented summarization and question an-
swering. Yet many challenges remain, including
how best to represent and combine linguistic infor-
mation for subjectivity analysis. With the additional
modalities that are present when working with face-
to-face spoken communication, these challenges are
even more pronounced.
The work in this paper focuses on two tasks: (1)
recognizing subjective utterances and (2) discrimi-
nating between positive and negative subjective ut-
terances. An utterance may be subjective because
the speaker is expressing an opinion, because the
speaker is discussing someone else?s opinion, or be-
cause the speaker is eliciting the opinion of someone
else with a question.
We approach the above tasks as supervised ma-
chine learning problems, with the specific goal of
finding answers to the following research questions:
? Given a variety of information sources, such
as text arising from (transcribed) speech,
phoneme representations of the words in an ut-
terance, and acoustic features extracted from
466
the audio layer, which of these sources are par-
ticularly valuable for subjectivity analysis in
multiparty conversation?
? Does the combination of these sources lead to
further improvement?
? What are the optimal representations of these
information sources in terms of feature design
for a machine learning component?
A central tenet of our approach is that subword
representations, such as character and phoneme n-
grams, are beneficial for the tasks at hand.
2 Subword Features
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information for various
text classification tasks. An example of character
n-grams is the set of 3-grams {#se, sen, ent,
nti, tim, ime, men, ent, nt#, t#a,
#an, ana, nal, aly, lys, ysi, sis,
is#} for the two-word phrase sentiment analysis.
The special symbol # represents a word boundary.
While it is not directly obvious that there is much
information in these truncated substrings, character
n-grams have successfully been used for fine-
grained classification tasks, such as named-entity
recognition (Klein et al, 2003) and subjective
sentence recognition (Raaijmakers and Kraaij,
2008), as well as a variety of document-level tasks
(Stamatatos, 2006; Zhang and Lee, 2006; Kanaris
and Stamatatos, 2007).
The informativeness of these low-level features
comes in part from a form of attenuation (Eisner,
1996): a slight abstraction of the underlying data
that leads to the formation of string equivalence
classes. For instance, words in a sentence will in-
variably share many character n-grams. Since ev-
ery unique character n-gram in an utterance consti-
tutes a separate feature, this leads to the formation
of string classes, which is a form of abstraction. For
example, Zhang and Lee (2006) investigate similar
subword representations, called key substring group
features. By compressing substrings in a corpus in a
trie (a prefix tree), and labeling entire sets of distri-
butionally equivalent substrings with one group la-
bel, an attenuation effect is obtained that proves very
beneficial for a number of text classification tasks.
Aside from attenuation effects, character n-
grams, especially those that represent word bound-
aries, have additional benefits. Treating word
boundaries as characters captures micro-phrasal in-
formation: short strings that express the transition
of one word to another. Stemming occurs naturally
within the set of initial character n-grams of a word,
where the suffix is left out. Also, some part-of-
speech information is captured. For example, the
modals could, would, should can be represented by
the 4-gram, ould, and the set of adverbs ending in
-ly can be represented by the 3-gram ly#.
A challenging thought is to extend the use of n-
grams to the level of phonemes, which comprise
the first symbolic level in the process of sound to
grapheme conversion. If n-grams of phonemes com-
pare favorably to word n-grams for the purpose of
sentiment classification, then significant speedups
can be obtained for online sentiment classification,
since tokenization of the raw speech signal can make
a halt at the phoneme level.
3 Data
For this work we use 13 meetings from the AMI
Meeting Corpus (Carletta et al, 2005). Each meet-
ing has four participants and is approximately 30
minutes long. The participants play specific roles
(e.g., Project Manager, Marketing Expert) and to-
gether function as a design team. Within the set of
13 meetings, there are a total of 20 participants, with
each participant taking part in two or three meet-
ings as part of the same design team. Meetings with
the same set of participants represent different stages
in the design process (e.g., Conceptual Design, De-
tailed Design).
The meetings used in the experiments have been
annotated for subjective content using the AMIDA
annotation scheme (Wilson, 2008). Table 1 lists
the types of annotations that are marked in the data.
There are three main categories of annotations, sub-
jective utterances, subjective questions, and objec-
tive polar utterances. A subjective utterance is a
span of words (or possibly sounds) where a pri-
vate state is being expressed either through choice of
words or prosody. A private state (Quirk et al, 1985)
467
is an internal mental or emotional state, including
opinions, beliefs, sentiments, emotions, evaluations,
uncertainties, and speculations, among others. Al-
though typically when a private state is expressed
it is the private state of the speaker, as in example
(1) below, an utterance may also be subjective be-
cause the speaker is talking about the private state
of someone else. For example, in (2) the negative
opinion attributed to the company is what makes the
utterance subjective.
(1) Finding them is really a pain, you know
(2) The company?s decided that teletext is out-
dated
Subjective questions are questions in which the
speaker is eliciting the private state of someone else.
In other words, the speaker is asking about what
someone else thinks, feels, wants, likes, etc., and the
speaker is expecting a response in which the other
person expresses what he or she thinks, feels, wants,
or likes. For example, both (3) and (4) below are
subjective questions.
(3) Do you like the large buttons?
(4) What do you think about the large buttons?
Objective polar utterances are statements or phrases
that describe positive or negative factual information
about something without conveying a private state.
The sentence The camera broke the first time I used
it gives an example of negative factual information;
generally, something breaking the first time it is used
is not good.
For the work in this paper, we focus on recog-
nizing subjectivity in general and distinguishing be-
tween positive and negative subjective utterances.
Positive subjective utterances are those in which any
of the following types of private states are expressed:
agreements, positive sentiments, positive sugges-
tions, arguing for something, beliefs from which
positive sentiments can be inferred, and positive re-
sponses to subjective questions. Negative subjective
utterances express private states that are the oppo-
site of those represented by the positive subjective
category: disagreements, negative sentiments, nega-
tive suggestions, arguing against something, beliefs
from which negative sentiments can be inferred, and
negative responses to subjective questions. Example
(5) below contains two positive subjective utterances
Table 1: AMIDA Subjectivity Annotation Types
Subjective Utterances
positive subjective
negative subjective
positive and negative subjective
uncertainty
other subjective
subjective fragment
Subjective Questions
positive subjective question
negative subjective question
general subjective question
Objective Polar Utterances
positive objective
negative objective
and one negative subjective utterance. Each annota-
tion is indicated by a pair of angle brackets.
(5) Um ?POS-SUBJ it?s very easy to use?.
Um ?NEG-SUBJ but unfortunately it does
lack the advanced functions? ?POS-SUBJ
which I I quite like having on the controls?.
The positive and negative subjective category is for
marking cases of positive and negative subjectivity
that are so closely interconnected that it is difficult
or impossible to separate the two. For example, (6)
below is marked as both positive and negative sub-
jective.
(6) Um ?POS-AND-NEG-SUBJ they?ve also
suggested that we um we only use the remote
control to control the television, not the VCR,
DVD or anything else?.
In (Wilson, 2008), agreement is measured for each
class separately at the level of dialogue act segments.
If a dialogue act overlaps with an annotation of a
particular type, then the segment is considered to
be labelled with that type. Table 2 gives the Kappa
(Cohen, 1960) and % agreement for subjective seg-
ments, positive and negative subjective segments,2
and subjective questions.
2A positive subjective segment is any dialogue act segment
that overlaps with a positive subjective utterance or a positive-
and-negative subjective utterance. The negative subjective seg-
ments are defined similarly.
468
Table 2: Interannotator agreement for the AMIDA sub-
jectivity annotations
Kappa % Agree
Subjective 0.56 79
Pos Subjective 0.58 84
Neg Subjective 0.62 92
Subjective Question 0.56 95
4 Experiments
We conduct two sets of classification experiments.
For the first set of experiments (Task 1), we auto-
matically distinguish between subjective and non-
subjective utterances. For the second set of ex-
periments (Task 2), we focus on distinguishing be-
tween positive and negative subjective utterances.
For both tasks, we use the manual dialogue act seg-
ments available as part of the AMI Corpus as the unit
of classification. For Task 1, a segment is considered
subjective if it overlaps with either a subjective utter-
ance or subjective question annotation. For Task 2,
the segments being classified are those that overlap
with positive or negative subjective utterances. For
this task, we exclude segments that are both positive
and negative. Although limiting the set of segments
to be classified to just those that are positive or nega-
tive makes the task somewhat artificial, it also allows
us to focus in on the performance of features specifi-
cally for this task.3 We use 6226 subjective and 8707
non-subjective dialog acts for Task 1 (with an aver-
age duration of 1.9s, standard deviation of 2.0s), and
3157 positive subjective and 1052 negative subjec-
tive dialog acts for Task 2 (average duration of 2.6s,
standard deviation of 2.3s).
The experiments are performed using 13-fold
cross validation. Each meeting constitutes a separate
fold for testing, e.g., all the segments from meeting 1
make up the test set for fold 1. Then, for a given fold,
the segments from the remaining 12 meetings are
used for training and parameter tuning, with roughly
a 85%, 7%, and 8% split between training, tuning,
and testing sets for each fold. The assignment to
training versus tuning set was random, with the only
constraint being that a segment could only be in the
tuning set for one fold of the data.
3In practice, this excludes about 7% of the positive/negative
segments.
The experiments we perform involve two steps.
First, we train and optimize a classifier for each type
of feature using BoosTexter (Schapire and Singer,
2000) AdaBoost.MH. Then, we investigate the per-
formance of all possible combinations of features
using linear combinations of the individual feature
classifiers.
4.1 Features
The two modalities that are investigated, prosodic,
and textual, are represented by four different
sets of features: prosody (PROS), word n-
grams (WORDS), character n-grams (CHARS), and
phoneme n-grams (PHONES).
Based on previous research on prosody modelling
in a meeting context (Wrede and Shriberg, 2003)
and on the literature in emotion research (Banse and
Scherer, 1996) we extract PROS features that are
mainly based on pitch, energy and the distribution of
energy in the long-term averaged spectrum (LTAS)
(see Table 3). These features are extracted at the
word level and aggregated to the dialogue-act level
by taking the average over the words per dialogue
act. We then normalize the features per speaker per
meeting by converting the raw feature values to z-
scores (z = (x ? ?)/?).
Table 3: Prosodic features used in experiments.
pitch mean, standard deviation, min-
imum, maximum, range, mean
absolute slope
intensity (en-
ergy)
mean, standard deviation, min-
imum, maximum, range, RMS
energy
distribution en-
ergy in LTAS
slope, Hammerberg index, cen-
tre of gravity, skewness
The textual features, WORDS and CHARS, and
the PHONES features are based on a manual tran-
scription of the speech. The PHONES were pro-
duced through dictionary lookup on the words in the
reference transcription. Both CHARS and PHONES
representations include word boundaries as informa-
tive tokens. The textual features for a given seg-
ment are simply all the WORDS/CHARS/PHONES
in that segment. Selection of n-grams is performed
by the learning algorithm.
469
4.2 Single Source Classifiers
We train four single source classifiers using BoosT-
exter, one for each type of feature. For the WORDS,
CHARS, and PHONES, we optimize the classi-
fier by performing a grid search over the parame-
ter space, varying the number of rounds of boosting
(100, 500, 1000, 2000, 5000), the length of the n-
gram (1, 2, 3, 4, 5), and the type of n-gram. Boos-
Texter can be run with three different n-gram con-
figurations: n-gram, s-gram, and f -gram. For the
default configuration (n-gram), BoosTexter searches
for n-grams up to length n. For example, if n = 3,
BoosTexter will consider 1-grams, 2-grams, and 3-
grams. For the s-gram configuration, BoosTexter
will in addition consider sparse n-grams (i.e., n-
grams containing wildcards), such as the * idea. For
the f -gram configuration, BoosTexter will only con-
sider n-grams of a maximum fixed length, e.g., if
n = 3 BoosTexter will only consider 3-grams. For
the PROS classifier, only the number of rounds of
boosting was varied. The parameters are selected
for each fold separately; the parameter set that pro-
duces the highest subjective F1 score on the tuning
set for Task 1, and the highest positive subjective F1
score for Task 2, is used to train the final classifier
for that fold.
4.3 Classifier combination
After the single source classifiers have been trained,
they have to be combined into an aggregate classi-
fier. To this end, we decided to apply a simple linear
interpolation strategy. Linear interpolation of mod-
els is the weighted combination of simple models to
form complex models, and has its roots in generative
language models (Jelinek and Mercer, 1980). (Raai-
jmakers, 2007) has demonstrated its use for discrim-
inative machine learning.
In the present binary class setting, BoosTexter
produces two decision values, one for every class.
For every individual single-source classifier (i.e.,
PROS, WORDS, CHARS and PHONES), separate
weights are estimated that are applied to the decision
values for the two classes produced by these classi-
fiers. These weights express the relative importance
of the single-source classifiers.
The prediction of an aggregate classifier for a
class c is then simply the sum of all weights for
all participating single-source classifiers applied to
the decision values these classifiers produce for this
class. The class with the maximum score wins, just
as in the simple non-aggregate case.
Formally, then, this linear interpolation strategy
finds for n single-source classifiers n interpolation
weights ?1, . . . ?n that minimize the empirical loss
(measured by a loss function L), with ?j the weight
of classifier j (? ? [0, 1]), and C jc (xi) the decision
value of class c produced by classifier j for datum xi
(a feature vector). The two classes are denoted with
0, 1. The true class for datum xi is denoted with x?i.
The loss function is in our case based on subjective
F-measure (Task 1) or positive subjective F-measure
(Task 2) measured on heldout development training
and test data.
The aggregate prediction x?i for datum xi on the
basis of n single-source classifiers then becomes
x?i = arg maxc (
n
?
j=1
?j ? Cjc=0(xi),
n
?
j=1
?j ? Cjc=1(xi))
(1)
and the lambdas are defined as
?nj = arg min?nj ?[0,1]
k
?
i
L(x?i, x?i;?j , . . . , ?n) (2)
The search process for these weights can easily be
implemented with a simple grid search over admis-
sible ranges.
In the experiments described below, we investi-
gate all possible combinations of the four differ-
ent sets of features (PROS, WORDS, CHARS, and
PHONES) to determine which combination yields
the best performance for subjectivity and subjective
polarity recognition.
5 Results and Discussion
Results for the two tasks are given in Tables 4 and 5
and in Figures 1 and 2. We use two baselines, listed
at the top of each table. The bullets in a given row
indicate the features that are being evaluated for a
given experiment. In Table 4, subjective F1, recall,
and precision are reported as well as overall accu-
racy. In Table 4, the F1, recall, and precision scores
are for the positive subjective class. All values in the
tables are averages over the 13 folds.
470
Table 4: Results Task 1: Subjective vs. Non-Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-SUBJ always chooses subjective class 60.3 43.4 100 43.4
BASE-RAND randomly chooses a class based on priors 41.8 42.9 41.3 50.6
single
? 54.6 55.3 54.5 63.1
? 60.5 68.5 54.5 71.0
? 61.7 67.5 57.2 71.1
? 60.3 66.4 55.5 70.2
double
? ? 63.9 72.1 57.6 73.4
? ? 65.6 71.9 60.3 74.0
? ? 64.6 72.3 58.4 73.7
? ? 66.2 73.8 60.1 74.9
? ? 65.2 73.2 58.8 74.3
? ? 66.1 72.8 60.7 74.5
triple
? ? ? 66.5 74.3 60.3 75.1
? ? ? 65.5 73.5 59.0 74.5
? ? ? 66.5 73.3 60.8 74.8
? ? ? 66.9 74.3 60.9 75.3
quartet ? ? ? ? 67.1 74.5 61.2 75.4
Table 5: Results Task 2: Positive Subjective vs. Negative Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-POS-SUBJ always chooses positive subjective class 85.6 75.0 100 75.0
BASE-RAND randomly chooses a class based on priors 75.1 74.4 76.1 62.4
single
? 84.8 74.8 98.1 73.9
? 85.6 79.6 93.1 76.8
? 85.9 81.9 90.5 78.0
? 85.5 80.5 91.3 77.0
double
? ? 88.7 83.0 95.4 81.9
? ? 88.7 83.1 95.1 81.8
? ? 88.5 83.3 94.4 81.6
? ? 89.5 84.2 95.7 83.3
? ? 89.2 83.7 95.5 82.8
? ? 89.0 84.2 94.6 82.6
triple
? ? ? 89.6 84.0 96.1 83.4
? ? ? 89.3 83.6 95.8 82.8
? ? ? 89.2 83.7 95.5 82.7
? ? ? 89.8 84.4 96.0 83.8
quartet ? ? ? ? 89.9 84.4 96.2 83.8
It is quite obvious that the combination of differ-
ent sources of information is beneficial, and in gen-
eral, the more information the better the results. The
best performing classifier for Task 1 uses all the fea-
tures, achieving a subjective F1 of 67.1. For Task 2,
the best performing classifier also uses all the fea-
tures, although it does not perform significantly bet-
ter than the classifier using only WORDS, CHARS,
and PHONES.4 This classifier achieves a positive-
subjective F1 of 89.9.
We measured the effects of adding more infor-
mation to the single source classifiers. These re-
sults are listed in Table 6. Of the various feature
types, prosody seems to be the least informative for
both subjectivity and polarity classification. In ad-
dition to producing the single-source classifier with
the lowest performance for both tasks, Table 6 shows
that when prosody is added, of all the features it is
least likely to yield significant improvements.
4We measured significance with the non-parametric
Wilcoxon signed rank test, p < 0.05.
Throughout the experiments, adding an additional
type of textual feature always yields higher results.
In all cases but two, these improvements are sig-
nificant. The best performing of the features are
the character n-grams. Of the single-source exper-
iments, the character n-grams achieve the best per-
formance, with significant improvements in F1 over
the other single-source classifiers for both Task 1
and Task 2. Also, adding character n-grams to other
feature combinations always gives significant im-
provements in performance.
An obvious question that remains is what the ef-
fect is of classifier interpolation on the results. To
answer this question, we conducted two additional
experiments for both tasks. First, we investigated
the performance of an uninterpolated combination
of the four single-source classifiers. In essence, this
combines the separate feature spaces without explic-
itly weighting them. Second, we investigated the re-
sults of training a single BoosTexter model using all
the features, essentially merging all feature spaces
471
Table 6: Addition of features separately (for Task 1 and 2): ?+? for a row-column pair (r, c) means that the addition
of column feature c to the row features r significantly improved r?s F1; ?-? indicates no significant improvement; ?X?
means ?not applicable?
+PROS + WORDS +CHARS +PHONES
Task 1 2 1 2 1 2 1 2
PROS X X + + + + + +
WORDS - + X X + + + +
CHARS - + - + X X - +
PHONES - + + + + + X X
PROS+WORDS X X X X + + + +
PROS+CHARS X X + + X X + +
PROS+PHONES X X + + + + X X
WORDS+CHARS + - X X X X + +
WORDS+PHONES + - X X + + X X
CHARS+PHONES + + + + X X X X
PROS+WORDS+CHARS X X X X X X + +
PROS+WORDS+PHONES X X X X + + X X
PROS+CHARS+PHONES X X + + X X X X
WORDS+CHARS+PHONES + - X X X X X X
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
40
45
50
55
60
65
70
Figure 1: Results (F1) experiment 1: subjective vs. non-
subjective.
into one agglomerate feature space. The results for
these experiments are given in Table 7, along with
the results from the all-feature interpolated classifi-
cation for comparison.
The results in Table 7 show that interpolation
outperforms both the unweighted and single-model
combinations for both tasks. For Task 1, the ef-
fect of interpolation compared to a single model is
marginal (a .03 point difference in F1). However,
compared to the uninterpolated combination, inter-
polation gives a clear 3.1 points improvement of F1.
For Task 2, interpolation outperforms both the unin-
terpolated and single-model classifiers, with 2 and 3
points improvements in F1, respectively.
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
70
75
80
85
90
95
10
0
Figure 2: Results (F1) experiment 2: positive subjective
vs. negative subjective.
6 Related Work
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information. Character-
level models have successfully been used for named-
entity recognition (Klein et al, 2003), predicting
authorship (Keselj et al, 2003; Stamatatos, 2006),
text categorization (Zhang and Lee, 2006), web page
genre identification (Kanaris and Stamatatos, 2007),
and sentence-level subjectivity recognition (Raaij-
makers and Kraaij, 2008) In spoken-language data,
Hsueh (2008) achieves good results using chains
of phonemes to automatically segment meetings ac-
cording to topic. However, to the best of our knowl-
edge there has been no investigation to date on the
472
Table 7: Results of interpolated classifiers compared to
uninterpolated and single-model classifiers for all fea-
tures.
Task Combination ACC REC PREC F1
1
interpolated 75.4 61.2 74.5 67.1
uninterpolated 73.0 58.7 70.6 64.0
single model 74.7 62.1 72.7 66.8
2
interpolated 83.8 96.2 84.4 89.9
uninterpolated 79.8 98.0 79.7 87.9
single model 79.5 91.0 83.3 86.9
combination of character-level, phoneme-level, and
word-level models for any natural language classifi-
cation tasks.
In text, there has been a significant amount of
research on subjectivity and sentiment recognition,
ranging from work at the phrase level to work on
classifying sentences and documents. Sentence-
level subjectivity classification (e.g., (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and
sentiment classification (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Hu and Liu,
2004; Popescu and Etzioni, 2005)) is the research
in text most closely related to our work. Of the
sentence-level research, the most similar is work
by Raaijmakers and Kraaij (2008) comparing word-
spanning character n-grams to word-internal char-
acter n-grams for subjectivity classification in news
data. They found that character n-grams spanning
words perform the best.
Research on recognizing subjective content in
multiparty conversation includes work by Somasun-
daran et al (2007) on recognizing sentiments and
arguing in meetings, work by Neiberg el al. (2006)
on recognizing positive, negative, and neutral emo-
tions in meetings, work on recognizing agreements
and disagreements in meetings (Hillard et al, 2003;
Galley et al, 2004; Hahn et al, 2006), and work
by Wrede and Shriberg (2003) on recognizing meet-
ing hotspots. Somasundaran et al use lexical and
discourse features to recognize sentences and turns
where meeting participants express sentiments or ar-
guing. They also use the AMI corpus in their work;
however, the use of different annotations and task
definitions makes it impossible to directly compare
their results and ours. Neiberg et al use acoustic?
prosodic features (Mel-frequency Cepstral Coeffi-
cients (MFCCs) and pitch features) and lexical n-
grams for recognizing emotions in the ISL Meeting
Corpus (Laskowski and Burger, 2006).
Agreements and disagreements are a subset of the
private states represented by the positive and neg-
ative subjective categories used in this work. To
recognise agreements and disagreements automati-
cally, Hillard et al train 3-way decision tree clas-
sifiers (agreement, disagreement, other) using both
word-based and prosodic features. Galley et al
model this task as a sequence tagging problem, and
investigate whether features capturing speaker inter-
actions are useful for recognizing agreements and
disagreements. Hahn et al investigate the use of
contrast classifiers (Peng et al, 2003) for the task,
using only lexical features.
Hotspots are places in a meeting in which the par-
ticipants are highly involved in the discussion. Al-
though high involvement does not necessarily equate
subjective content, in practice, we expect more sen-
timents, opinions, and arguments to be expressed
when participants are highly involved in the discus-
sion. In their work on recognizing meeting hotspots,
Wrede and Shriberg focus on evaluating the contri-
bution of various prosodic features, ignoring lexi-
cal features completely. The results of their study
helped to inform our choice of prosodic features for
the experiments in this paper.
7 Conclusions
In this paper, we investigated the use of prosodic
features, word n-grams, character n-grams, and
phoneme n-grams for subjectivity recognition and
polarity classification of dialog acts in multiparty
conversation. We show that character n-grams
outperform prosodic features, word n-grams and
phoneme n-grams in subjectiviy recognition and po-
larity classification. Combining these features sig-
nificantly improves performance. Comparing the
additive value of the four information sources avail-
able, prosodic information seem to be least in-
formative while character-level information indeed
proves to be a very valuable source. For subjectiv-
ity recognition, a combination of prosodic, word-
level, character-level, and phoneme-level informa-
tion yields the best performance. For polarity clas-
sification, the best performance is achieved with a
473
combination of words, characters and phonemes.
References
R. Banse and K. R. Scherer. 1996. Acoustic profiles in
vocal emotion expression. Journal of Personality and
Social Psychology, pages 614?636.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
the Measuring Behavior Symposium on ?Annotating
and Measuring Meeting Behavior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
J. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. In Technical Report
IRCS-96-11, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings
of HLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In
Proceedings of IEEE/ACM Workshop on Spoken Lan-
guage Technology.
P. Hsueh. 2008. Audio-based unsupervised segmentation
of meeting dialogue. In Proceedings of ICASSP.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of KDD.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Proceedings, Workshop on Pattern Recognition in
Practice, pages 381?397.
I. Kanaris and E. Stamatatos. 2007. Webpage genre iden-
tification using variable-length character n-grams. In
Proceedings of ICTAI.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of PACLING.
S. Kim and Eduard Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Coling.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003.
Named entity recognition with character-level models.
In Proceedings of CoNLL.
K. Laskowski and S. Burger. 2006. Annotation and anal-
ysis of emotionally relevant behavior in the ISL meet-
ing corpus. In Proceedings of LREC 2006.
D. Neiberg, K. Elenius, and K. Laskowski. 2006. Emo-
tion recognition in spontaneous speech using GMMs.
In Proceedings of INTERSPEECH.
K. Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic.
2003. Exploiting unlabeled data for improving ac-
curacy of predictive data mining. In Proceedings of
ICDM.
A. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting
action items in multi-party meetings: Annotation and
initial experiments. In Proceedings of MLMI.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
S. Raaijmakers and W. Kraaij. 2008. A shallow ap-
proach to subjectivity classification. In Proceedings
of ICWSM.
S. Raaijmakers. 2007. Sentiment classification with in-
terpolated information diffusion kernels. In Proceed-
ings of the First International Workshop on Data Min-
ing and Audience Intelligence for Advertising (AD-
KDD?07).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In Pro-
ceedings of SIGdial.
E. Stamatatos. 2006. Ensemble-based author identifica-
tion using character n-grams. In Proceedings of TIR.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proceedings of LREC.
B. Wrede and E. Shriberg. 2003. Spotting ?hot spots?
in meetings: Human judgments and prosodic cues. In
Proceedings of EUROSPEECH.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
D. Zhang and W. S. Lee. 2006. Extracting key-substring-
group features for text classification. In Proceedings
of KDD.
474
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 61?68,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
Applying prosodic speech features in mental health care: 
An exploratory study in a life-review intervention for depression 
  
Sanne M.A. Lamers 
University of Twente 
Psychology, Health, & 
Technology 
the Netherlands 
s.m.a.lamers@ 
utwente.nl 
 
Khiet P. Truong 
University of Twente 
Human Media Interaction 
the Netherlands 
k.p.truong@ 
utwente.nl 
Bas Steunenberg 
UMC Utrecht 
the Netherlands 
 b.steunenberg@ 
umcutrecht.nl 
Franciska de Jong  
University of Twente 
Human Media Interaction 
the Netherlands 
f.m.g.dejong@ 
utwente.nl  
 
Gerben J. Westerhof 
University of Twente 
Psychology, Health, & Technology 
the Netherlands 
g.j.westerhof@ 
utwente.nl 
 
Abstract 
The present study aims to investigate the   
application of prosodic speech features in a 
psychological intervention based on life-
review. Several studies have shown that 
speech features can be used as indicators of 
depression severity, but these studies are 
mainly based on controlled speech recording 
tasks instead of natural conversations. The 
present exploratory study investigated speech 
features as indicators of depression in con-
versations of a therapeutic intervention. The 
changes in the prosodic speech features pitch, 
duration of pauses, and total duration of the 
participant?s speaking time were studied over 
four sessions of a life-review intervention for 
three older participants. The ecological valid-
ity of the dynamics observed for prosodic 
speech features could not be established in 
the present study. The changes in speech fea-
tures differed from what can be expected in 
an intervention that is effective in decreasing 
depression and were inconsistent with each 
other for each of the participants. We suggest 
future research to investigate changes within 
the intervention sessions, to relate the chang-
es in feature values to the topical content of 
the speech, and to relate the speech features 
directly to depression scores. 
1 Introduction 
Depression is a mood disorder that is mainly 
characterized by a sad mood or the loss of in 
terest and pleasure in nearly all activities in a 
period of at least two weeks (American Psychia- 
tric Association, 2000). Depression disorders are 
the leading cause of disability and contribute 
largely to the burden of disease in middle- and 
high-income countries worldwide (?stun et al., 
2004). In 2012, more than 350 million people 
around the world suffered from depression symp-
toms (World Health Organization, 2012). To de-
crease the onset of depression disorders, early 
psychological interventions, i.e., psychological 
methods targeting behavioral change to reduce 
limitations or problems (Vingerhoets, Kop, & 
Soons, 2002), aiming at adults with depression 
symptoms or mild depression disorders are    
necessary. Meta-analytic findings show that psy-
chological interventions reduce the incidence of 
depression disorders by 22%, indicating that pre-
vention of new cases of depression disorders is 
indeed possible (Cuijpers et al., 2008).  
To evaluate the effectiveness of interventions 
for depression and changes during the interven-
tions, reliable and valid measures of depression 
severity are necessary. Depression severity is 
mostly measured by self-report questionnaires 
such as the Center for Epidemiologic Studies 
Depression scale (CES-D; Radloff, 1977), the 
Hamilton Depression Rating Scale (HAM-D; 
Hamilton, 1960), and the Beck Depression In-
ventory (Beck, Steer, & Brown, 1996). These 
self-report questionnaires often include items on 
mood and feelings. Moreover, questionnaire 
items may cover physical depression symptoms 
such as sleep disturbances, changes in weight 
61
and appetite, and loss in energy. However, in 
some target groups such as older adults these 
items can confound with health problems and 
physical diseases, which increase in old age. For 
these reasons, there is a need for valid and objec-
tive measures of depression severity. Not only to 
assess depression severity before and after thera-
py, but also to detect the dynamics during the 
therapy (Elliot, 2010). 
1.1 Computational linguistics, speech ana-
lysis, and mental health care 
It is commonly assumed  and confirmed in    
several studies that emotions and mood can in-
fluence the speaking behavior of a person and the 
characteristics of the sound in speech (Kuny & 
Stassen, 1993; Scherer, Johnstone, & Klasmeyer, 
2003). Already in 1954, Moses concluded that 
the voice and speech patterns of psychiatric pa-
tients differed from those of people without a 
psychiatric diagnosis. Clinicians observe the 
speech of depressed patients frequently as uni-
form, monotonous, slow, and with a low voice 
(Kuny & Stassen, 1993). A review by Sobin and 
Sackeim (1997) showed that depressed people 
differ from normal and other psychiatric groups 
on psychomotor symptoms such as speech. The 
speech of depressed patients is characterized by a 
longer pause duration, that is, an increased 
amount of time between speech utterances as 
well as by a reduced variability in mean vocal 
pitch.  
More recently these insights have led to      
collaborative and multidisciplinary work be-
tween researchers from the fields of computa-
tional linguistics and mental health care. With 
the growing availability of models and algo-
rithms for automated natural language processing 
that can be put to use in clinical scenarios, de-
pression can now increasingly be measured 
based on the characteristics of the language used 
by patients, such as the frequency of verbal ele-
ments in a narrative that express a certain mood 
or sentiment (Pennebaker & Chung, 2011), and  
acoustic speech features.  Because vocal acoustic 
features such as pause durations and pitch are 
biologically based, it has even been argued that 
they can serve as biomarkers of depression se-
verity (Mundt et al., 2012). As a consequence, 
speech features such as pitch and pause durations 
can be used to estimate the severity of a depres-
sion. 
To date, several studies investigated the validi-
ty of several speech features as indicators  of de-
pression. Indeed, the speech features pitch and 
speech loudness correlate significantly with 
global depression scores during recovery (Kuny 
& Stassen, 1993; Stassen, Kuny, & Hell, 1998). 
After recovery from depression, the speech pause 
time of depressed adults was no longer elongated 
(Hardy et al., 1984). These results indicate that  
prosodic speech features are valid measures of 
depression.  
However, these studies have the limitation that 
the speech analyses are based on the recording of 
controlled speech based on tasks such as count-
ing and reading out loud. Such speech recording 
tasks take place under ideal voice recording con-
ditions (Cannizzaro, Harel, Reilly, Chappell, & 
Snyder, 2004), while speech analysis is more 
difficult when conducted outside a controlled 
setting, because of so-called noisy channel     
effects (Janssen, Tacke, de Vries, van den Broek, 
Westerink, Haselager, & IJsselsteijn, 2013). 
Moreover, controlled speech tasks are cognitive-
ly less demanding than free speech tasks (Alpert 
et al., 2011). This evokes the question whether 
speech features are also ecological valid, i.e., 
whether they can be used as indicators of depres-
sion severity, when measured during natural 
conversations instead of during the recording of 
controlled speech tasks (Bronfenbrenner, 1977).  
A study on speech samples from video recor- 
dings of structured interviews revealed promis-
ing results: speaking rate and pitch variation, but 
not the percentage of pauses, showed a large cor-
relation with depression rating scores (Canni-
zaro, Harel, Reilly, Chappell, & Snyder, 2004). 
Additional studies on the ecological validity of 
using prosodic speech features as indicator for  
depression are necessary. 
1.2 Speech features as mood markers in a 
life-review intervention 
In the present study the speech of older adults 
will be measured in four sessions of a psycholo- 
gical intervention, combining knowledge in the 
fields of computational linguistics and psycho-
logical interventions in mental health care. Be-
cause psychological interventions of depression 
have shown to be effective (e.g., Cuijpers, van 
Straten, & Smit, 2006) and are broadly imple-
mented in mental health care, the measurement 
of speech features in psychological interventions 
is a promising application for the field of compu-
tational linguistics. For example, speech features 
can be used to provide direct feedback to both 
the therapist and patient on the severity and 
changes in severity of depression during the psy-
chological intervention. Clinicians do not have 
62
the ability to differentiate precisely the duration 
of for example the patient?s utterances and paus-
es (Alpert et al. 2001). There is also ample evi-
dence that text mining techniques based on the 
frequency of certain terms can be applied to nar-
ratives from patients in order to monitor changes 
in mood (Pennebaker & Chung, 2011), and a re-
cent study has shown that machines can better 
recognize certain emotions than lay people 
(Janssen et al., 2013), underlining once again the 
added value  of automated speech analysis. To 
pave the way for future applications that would 
enable the use of  speech features as a direct 
feedback mechanism, the first step is to gain 
more knowledge on the patterns in speech fea-
tures and on how changes in these features can 
be considered as meaningful signals of patterns 
in   psychological interventions.  
The psychological intervention in the present 
study is based on life-review: the structured re-
collection of autobiographical memories. De-
pressed people have difficulties in retrieving spe-
cific, positive memories. Their autobiographical 
memory is characterized by more negative and 
general memories (e.g., Williams et al., 2007), 
for example memories that reflect a period or 
recurrent event (e.g., the period of a marriage) 
rather than a specific event (e.g., the ceremony 
on the wedding day). The present life-review 
course targets the recollection of specific, posi-
tive memories in older adults with depression 
symptoms. In four weekly sessions, the inter-
viewer stimulates the recollection by asking 
questions on the depressed person?s childhood, 
adolescence, adulthood and life in general. An 
advantage of life-review in comparison to other 
therapies such as Cognitive Behavioral Therapy, 
is that it fits in with a natural activity of older 
adults to recollect memories and tell stories 
about their lives (Bluck & Levine, 1998). Life-
review has shown to be an effective method to 
decrease depression symptoms (Korte, Bohl-
meijer, Cappeliez, Smit, & Westerhof, 2012; 
Pinquart & Forstmeier, 2012) and is considered 
an evidence-based intervention for depression in 
older adults (Scogin, Welsh, Hanson, Stump, & 
Coates, 2005).  
Our study is one of the first to investigate pro-
sodic speech features during a psychological in-
tervention. The study is exploratory and aims to 
gain insight into the ecological validity of pro-
sodic speech features in a psychological life-
review intervention. The life-review intervention 
offers the opportunity to investigate the prosodic 
speech features over time. Life-review is highly 
suitable to investigate speech features during an 
intervention, since the speech from the recall of 
autobiographical memories provides strong pro-
sodic speech changes (Cohen, Hong, & Guevara, 
2010) and the expression of emotions charac-
terized by speech characteristics is stronger after 
open and meaning-questions as compared to 
closed and fact-questions (Truong, Westerhof, 
Lamers, & de Jong, under review). Our paper is a 
first step to gain insight into the methods that are 
necessary to evaluate the application of prosodic 
speech features in mental health care. In the pre-
sent study into the role of prosodic speech fea-
tures,  vocal pitch and pause duration will be in-
vestigated in three participants across all four 
weekly sessions. Because the life-review inter-
vention is effective in decreasing depression 
symptoms (Korte et al., 2011; Serrano, Latorre, 
Gatz, & Montanes, 2004), we expect that the 
prosodic features change accordingly. Therefore, 
we hypothesize (a) an increase in average vocal 
pitch, (b) an increase in the variation in vocal 
pitch, (c) a decrease in average pause duration, 
(d) a decrease in the ratio between the total pause 
time and total speech time (pause speech ratio), 
and (e) an increase in the ratio between the par-
ticipant?s speech and total duration of the session 
(speech total duration ratio) during the interven-
tion.  
2 Method 
In this section we will describe the methodology 
applied in the design of the psychological inter-
ventions during which the research data sets 
were generated, the procedure for selecting the 
participants and the corresponding data sets, the 
data preparations steps and the analyses per-
formed.  
2.1 Intervention ?Precious memories? 
The life-review intervention ?Precious memories? 
(Bohlmeijer, Serrano, Cuijpers, & Steunenberg, 
2007) targets the recollection of specific, positive 
memories. The intervention is developed for  
older adults with depression symptoms living in 
a nursing home. Each of the four weekly sessions 
focuses on a different theme: childhood, ado-
lescence, adulthood, and life in general. The ses-
sions are individual and guided by a trained in-
terviewer. The sessions take place at the partici-
pant?s home and last approximately 45 minutes. 
Each of the sessions is structured by fourteen 
main questions that stimulate the participant to 
recollect and tell specific positive memories 
63
about his or her life. The interviewers are in-
structed to ask for lively details about each of the 
positive memories of the participants, for        
example the colors, smells and people that were 
involved in the memory. Table 1 shows an ex-
ample question for each of the four sessions. 
 
Session  Example question 
1: Childhood Can you remember an 
event in which your father 
or mother did something 
when you were a child that 
made you very happy? 
2: Adolescence Do you remember a special 
moment of getting your 
first kiss or falling in love 
with someone? 
3: Adulthood What has been a very im-
portant positive experience 
in your life between the 
ages of 20 and 60? 
4: Life in general What is the largest gift you 
ever received in your life? 
Tabel 1. Example questions for the four sessions 
of the life-review intervention ?Precious  
memories? 
2.2 Procedure and participants 
Participants with depression symptoms were re-
cruited in nursing homes in the area of Amster-
dam, the Netherlands. Participation in the life-
review intervention was voluntary. Three partici-
pants were selected for whom audio recordings 
of the four sessions were available, which resul-
ted in a dataset of twelve life-review sessions. 
The three participants (below labeled as P1, P3 
and P5) were females with an age between 83 
and 90 years. The educational background varied 
from low to high and the marital status from 
married to never married. The participants signed 
an informed consent form for the use of the au-
dio-tapes  for scientific purposes.  
2.3 Data preparation and analysis 
All acoustic features were automatically extract-
ed with Praat (Boersma, 2001). Because the 
speech of both the interviewer and the partici-
pants were recorded on one mixed audio channel, 
some manual interventions had to be applied in 
order to determine the segments in which the 
participant is talking. First, for each session, the 
segments in which the participant is the main 
speaker were selected. These so-called ?turns? 
were then labeled in more detail; utterances pro-
duced by the interviewer were marked and dis-
carded in the speech analysis. For each turn, 
mean pitch, standard deviation pitch, pause dura-
tion, the ratio between total pause time and total 
speech time, and the ratio between total speech 
time and total duration of the session were ex-
tracted. Pause durations were automatically ex-
tracted by applying silence detection where the 
minimal silence duration was set at 500 ms. All 
features were normalized per speaker by trans-
forming the raw feature values to z-scores (mean 
and standard deviation were calculated over all 4 
sessions, z = ((x-m)/sd)). The ratio between total 
speech time and total duration time was not nor-
malized because this feature was calculated over 
a whole session instead of a turn. Subsequently, 
averages over all turns per session were taken in 
order to obtain one value per session. 
3 Results 
The results of the prosodic speech features over 
the four sessions of the life-review intervention 
are graphically presented separately for each fea-
ture, in the Figures 1 to 5. We hypothesized an 
increase in the average pitch during the interven-
tion. As shown in Figure 1, the patterns in ave-
rage pitch during the intervention differs across 
the three participants. Only in Participant 3, the 
pattern is in line with our expectations, showing 
an increase in the sessions 3 and 4. In both Par-
ticipant 1 and 5, there was a decrease in average 
pitch in the sessions 3 and 4. 
 
 
 
Figure 1. Average pitch of the participants 
(P1,P3,P5) during the four sessions. 
 
We expected the variation in pitch to increase 
during the intervention. Figure 2 shows the par-
ticipants? patterns of the standard deviation of 
pitch during the intervention. The changes in 
standard deviation do not confirm our hypothe-
sis. Although the speech of Participant 3 shows 
64
an increase in session 4, the standard deviation is 
lower in session 4 than in session 1 of the inter-
vention. The standard deviation of Participant 5 
is relatively stable during the intervention. Par-
ticipant 1 mainly shows a large variation in pitch 
in session 2. 
 
 
 
Figure 2. Standard deviation in pitch of the par-
ticipants (P1,P3,P5) during the four sessions. 
 
It was hypothesized that the average pause du-
ration would decrease during the four sessions of 
the intervention. Figure 3 shows that the average 
pause duration was relatively stable over the first 
three sessions in all three participants. Only in 
Participant 1 the average pause duration de-
creased in session 4, in line with our expec-
tations.  
 
 
 
Figure 3. Pause duration of the participants 
(P1,P3,P5) during the four sessions. 
 
In agreement with our hypothesis on average 
pause duration, we also expected a decrease dur-
ing the intervention in the ratio between the total 
pause time and total speech time. Although there 
was a large decrease in the pause speech ratio of 
Participant 1 between the sessions 2 and 3, the 
ratio in session 4 was similar to the pause speech 
ratio in the first session (see Figure 4). In both 
Participant 2 and 3, the ratio was relatively stable 
in the sessions 1 to 3, but in session 4 the pause 
speech ratio showed an increase in Participant 3 
and a slight decrease in Participant 2. 
 
 
 
Figure 4. Pause speech ratio of the participants 
(P1,P3,P5) during the four sessions. 
 
Last, we investigated the ratio between the 
participant?s speech and total duration of the ses-
sion. We hypothesized an increase in the speech 
total duration ration during the intervention. Fig-
ure 5 shows the differences between the partici-
pants in the speech total duration ratio over the 
four sessions. The ratio is relatively stable, and 
high, in Participant 5. The ratio in both Partici-
pant 1 and 3 in general decreases during the in-
tervention, with a lower speech total duration 
ratio in session 4 as compared to session 1.  
 
 
 
Figure 5. Speech total duration ratio of the par-
ticipants (P1,P3,P5) during the four sessions. 
 
4 Conclusion 
The aim of the present study was to investigate 
the suitability of applying  an analysis of proso-
dic speech features in the speech recordings    
65
collected in psychological intervention based on 
life-review. Because several studies have shown 
that speech features can be used as indicators of 
depression severity (e.g., Kuny & Stassen, 1993; 
Stassen, Kuny, & Hell, 1998), the application of 
speech analyses in mental health care is promi- 
sing. However, the measurement of speech fea-
tures is often based on speech recording tasks 
and the ecological validity within psychological 
interventions is not yet established. The study is 
a first exploratory step to gain insight into the 
ecological validity of prosodic speech features in 
a psychological life-review intervention.  
We expected to measure a change during the 
intervention in the prosodic speech features that 
could be  related to depression symptoms, and 
hypothesized an increase in average pitch and 
pitch variation, a decrease in average pause dura-
tion, and an increase in the amount of speech by 
the participant during the intervention. However, 
we could not establish  the ecological validity of 
these speech indicators in the present study. In 
general, the patterns of the prosodic speech indi-
cators differ from our expectations. The dyna-
mics  in the speech indicators was different from 
what can be expected in an intervention that is 
effective in decreasing depression (Korte et al., 
2011; Serrano et al., 2004). Moreover, the speech 
indicators were inconsistent with each other for  
the participants in the pool. For example, Partici-
pant 3 showed an increase in pitch during the 
intervention, which indicates a decrease in de-
pression, and an increase in average pause dura-
tion and pause speech ratio, which indicates an 
increase in depression.  
Taken together, the findings from the present 
study indicate that the prosodic speech features 
that have been validated for controlled settings, 
are not directly applicable for the spontaneous 
type of conversation that is typical for a mental 
health care setting. More research is needed to 
establish the ecological validity of prosodic 
speech features such as pitch, pauses, and speech 
duration as indicators of depression severity. A 
few suggestions can be made. First, each of the 
four sessions in the life-review intervention in 
the present study focused on a different theme. 
Although we aimed to evaluate the development 
of the speech features during the intervention, the 
differences across the session may be the conse-
quence of differences in session theme. More-
over, not all parts of the session consisted of  
life-review, and participants were talking about a 
variety of subjects, for example about  their 
caregivers. The goal of the life-review interven-
tion is to stimulate the retrieval of specific posi-
tive memories. In a next step, we aim to select 
the parts in which the participant is recollecting 
such memories and to evaluate the patterns in 
prosodic speech features only for  these parts. 
Second, the prosodic speech indicators were 
averaged per session to provide a clear overview 
of the changes over the four sessions. However, 
changes can also occur within the session. For 
example, vocal pitch may increase during the 
session, which would indicate a decrease in de-
pression symptoms. Furthermore, within each 
session, the interaction between the interviewer 
and participant may play a role. For instance, 
when the interviewer speaks with a higher pitch 
and more variation in pitch, the participant may 
unconsciously take over some of this speaking 
behavior. We suggest future studies to investi-
gate not only the average session, but to include 
changes during the session the interviewer?s 
speech features. 
Third, the present research was conducted in 
line with the assumption that life-review is effec-
tive as an intervention for mood disorder, as is 
shown in several studies (Korte et al., 2011; Ser-
rano et al., 2004). However, we due to lack of 
data on depression severity we do not know 
whether the life-review intervention was fully 
effective for the participants in the present study. 
To validate the patterns prosodic speech features 
as a reliable indicator for depressions that can be 
used in mental health care, it is necessary to 
demonstrate  that  the dynamics in speech fea-
tures can be related directly to changes in depres-
sion scores. As argued in earlier studies, in order 
to  conclude that speech features correlate signi-
ficantly with global depression scores during re-
covery (Kuny & Stassen, 1993; Stassen, Kuny, & 
Hell, 1998), these correlations need to be inves-
tigated in psychological interventions.  
In sum, the study  of how prosodic speech fea-
tures such as pitch and pauses relate to the kind 
of spoken narratives that play a role in mental 
health care settings is a promising field. How-
ever, the ecological validity of prosodic speech 
features could not be established in the present 
study. More research based on  larger data sam-
ples the establishment of a direct relation to de-
pression scores is  necessary before the tech-
niques from the field of computational linguistics 
can be applied as a basis for the collection of in-
dicators that can be used  in psychological inter-
ventions in a meaningful and effective way. 
 
66
References 
Alpert, M., Pouget, E. R., & Silva, R. R. (2001). Re-
flections of depression in acoustic measures of the 
patient?s speech. Journal of Affective Disorders, 
66, 59-69. 
 
American Psychiatric Association (2000). Diagnostic 
and statistical manual of mental disorders (4th ed., 
text rev.). Washington, DC: Author.  
 
Beck, A. T., Steer, R. A., & Brown, G. K. (1996). 
Manual for the Beck Depression Inventory-II. San 
Antoinio, TX: Psychological Corporation. 
 
Bluck, S., & Levine, L. J. (1998). Reminiscence as 
autobiographical memory: A catalyst for reminis-
cence theory development. Ageing and Society, 
18, 185-208. 
 
Boersma, P. (2001). Praat, a sysem for doing pho- 
netics by computer. Glot International, 5(9/10), 
341-345. 
 
Bohlmeijer, E. T., Serrano, J., Cuijpers, P., & Steu-
nenberg, B. (2007). Dierbare herinneringen. Pro-
tocol voor life-review bij ouderen met depressieve 
klachten in verzorgings- en verpleeghuizen [Pre-
cious memories. Protocol for life-review in older 
people with depressive symptoms in nursing 
homes]. 
 
Bronfenbrenner, U. (1977). Toward an experimental 
ecology of human development. American Psy-
chologist, 32, 513-531. 
Cannizzaro, M., Harel, B., Reilly, N., Chappell, P., & 
Snyder, P. J. (2004). Voice acoustical measure-
ment of the severity of major depression. Brain 
and Cognition, 56, 30-35. 
 
Chien, J.-T., & Chueh, C-H. (2010). Joint acoustic 
and language modeling for speech recognition. 
Speech Communication, 52, 223-235. 
 
Cohen, A. S., Hong, S. L., & Guevara, A. (2010). 
Understanding emotional expression using prosod-
ic analysis of natural speech: refining the method-
ology. Journal of Behavioral Therapy & Experi-
mental Psychiatry, 41, 150-157. 
 
Cuijpers, P., van Straten, A., Smit, F. (2006). Psycho-
logical treatment of late-life depression: A meta-
analysis of randomized controlled trials. Interna-
tional Journal of Geriatric Psychiatry, 21, 1139-
1149. 
 
Cuijpers, P., van Straten, A., Smit, F., Mihalopoulos, 
C., & Beekman, M. D. (2008). Preventing the on-
set of depressive disorders: a meta-analytic review 
of psychological interventions. American Journal 
of Psychiatry, 165, 1272-1280. 
 
Elliot, R. (2010). Psychotherapy change process re-
search: Realizing the promise. Psychotherapy Re-
search, 20, 123-135. 
 
Hamilton, M. (1960). A rating scale for depression. 
Journal of Neurology, Neurosurgery, & Psychia-
try, 23, 56-62. 
 
Hardy, P., Jouvant, R., & Widl?cher, D. (1984). 
Speech pause time and the retardation rating scale 
for depresstion (ERD): towards a reciprocal vali-
dation. Journal of Affective Disorders, 6, 123-127. 
 
Janssen, J. H., Tacke, P., de Vries, J. J. G., van den 
Broek, E. L., Westerink, J. H. D. M., Haselager, 
P., & IJsselsteijn, W. A. (2013). Machines outper-
form laypersons in recognizing emotions elicited 
by autobiographical recollection. Human-
Computer Interaction, 28, 479-517. 
 
Koolagudi, S. G., & Sreenivasa, K. S. (2012). Emo-
tion recognition from speech: a review. Interna-
tional Journal of Speech Technology, 15, 99-117. 
 
Korte, J., Bohlmeijer, E. T., Cappeliez, P., Smit, F., & 
Westerhof G. J. (2012). Life-review therapy for 
older adults with moderate depressive sympto-
matology: A pragmatic randomized controlled tri-
al. Psychological Medicine, 42, 1163-1172.  
 
Kuny, S., & Stassen, H. H. (1993). Speaking behavior 
and voice sound characteristics in depressive pa-
tients during recovery. Journal of Psychiatric Re-
search, 27, 289-307. 
 
Moses, J. P. (1954). The voice of neurosis. Oxford, 
UK: Grune and Stratton. 
 
Mundt, J. C., Vogel, A. P., Feltner, D. E., & Lender-
king, W. R. (2012). Vocal acoustic biomarkers of 
depression severity and treatment response. Bio-
logical Psychiatry, 72, 580-587. 
 
Pennebaker, J. W. and Chung, C. K. 
(2011). Expressive Writing and its Links to Men-
tal and Physical Health. In H. S. Friedman 
(Ed.), Oxford Handbook of Health Psycholo-
gy. New York, NY: Oxford University Press, 417-
437. 
 
Pinquart, M., & Forstmeier, S. (2012). Effects of rem-
iniscence interventions on psychosocial outcomes: 
A meta-analysis. Aging & Mental Health, 16, 514-
558. 
 
Radloff, L. S. (1977). The CES-D Scale: A Self-
Report Depression Scale for Research in the Gen-
eral Population. Applied Psychological Measure-
ment, 1, 385-401. 
67
 
Scherer, K. R., Johnstone, T., & Klasmeyer, G. 
(2003). Vocal expression of emotion. In R. J. Da-
vidson, K. R. Scherer, & H. Goldsmith (Eds.), 
Handbook of the Affective Sciences (pp. 433?456). 
New York and Oxford: Oxford University Press. 
 
Scogin, F., Welsh, D., Hanson, A., Stump, J., & 
Coates, A. (2005). Evidence-based psychothera-
pies for depression in older adults. Clinical Psy-
chology: Science and Practice, 12, 222-237. 
 
Serrano, J., Latorre, J., Gatz, M., & Montanes, J. 
(2004). Life review therapy using autobiograph-
ical retrieval practice for older adults with depres-
sive symptomatology. Psychology & Aging, 19, 
272-277. 
 
Sobin, C., & Alpert, M. (1999). Emotion in speech: 
the acoustic attributes of fear, anger, sadness, and 
joy. Journal of Psycholinguistic Research, 28, 
347-365. 
 
Sobin, C., & Sackeim, H. A. (1997). Psychomotor 
symptoms of depression. American Journal of 
Psychiatry, 154, 4-17. 
 
Stassen, H. H., Kuny, S., & Hell, D. (1998). The 
speech analysis approach to determining onset of 
improvement under antidepressants. European 
Neuropsychopharmacology, 8, 303-310. 
 
Truong, K., Westerhof, G. J., Lamers, S. M. A., & de 
Jong, F. (under review). Towards modeling ex-
pressed emotions in oral history interviews: using 
verbal and non-verbal signals to track personal 
narratives. Literary and Linguistic Computing. 
 
?st?n, T. B., Ayuso-Mateos, J. L., Chatterji, S., 
Mathers, C., & Murray, C. J. L. (2004). Global 
burden of depressive disorders in the year 2000. 
British Journal of Psychiatry, 184, 386-392. 
 
Vervedis, D., & Kotropoulos, C. (2006). Emotional 
speech recognition: Resources, features, and 
methods. Speech Communication, 48, 1162-1181. 
 
Vingerhoets, A. J. J. M., Kop, P. F. M., & Soons, P. 
H. G. M. (2002). Psychologie in de gezondheids-
zorg: een praktijkori?ntatie [Psychology in health 
care: a practical orientation]. Houten, the Nether-
lands: Bohn Stafleu van Loghum. 
 
Williams, J. M., Barnhofer, T., Crane, C., Herman, D, 
Raes, F., Watkins, E., & Dalgleish, T. (2007). Au-
tobiographical memory specificity and emotional 
disorder. Psychological Bulletin, 133, 122-148. 
 
World Health Organization. (2012). Depression Fact 
Sheet. Retrieved at March, 5, 2014:  
www.who.int/mediacentre/factsheets/fs369/en/ 
 
68
