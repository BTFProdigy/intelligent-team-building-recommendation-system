Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 123?130,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Speech to speech machine translation:
Biblical chatter from Finnish to English
David Ellis
Brown University
Providence, RI 02912
Mathias Creutz Timo Honkela
Helsinki University of Technology
FIN-02015 TKK, Finland
Mikko Kurimo
Abstract
Speech-to-speech machine translation is in
some ways the peak of natural language pro-
cessing, in that it deals directly with our
original, oral mode of communication (as
opposed to derived written language). As
such, it presents challenges that are not to be
taken lightly. Although existing technology
covers each of the steps in the process, from
speech recognition to synthesis, deriving a
model of translation that is effective in the
domain of spoken language is an interesting
and challenging task. If we could teach our
algorithms to learn as children acquire lan-
guage, the result would be useful both for
language technology and cognitive science.
We propose several potential approaches, an
implementation of a multi-path model that
translates recognized morphemes alongside
words, and a web-interface to test our speech
translation tool as trained for Finnish to En-
glish. We also discuss current approaches to
machine translation and the problems they
face in adapting simultaneously to morpho-
logically rich languages and to the spoken
modality.
1 Introduction
Effective and fluent machine translation poses many
challenges, and often requires a variety of resources.
Some are language-specific, some domain-specific,
and others manage to be relatively independent (one
might even say context-free), and thus generally ap-
plicable in a wide variety of circumstances. There
are still untapped resources, however, that might
benefit machine translation systems. Most statistical
approaches do not take into account any similarities
in word forms, so words that share a common root,
(like ?blanche? and ?bianca?, meaning ?white? in
French and Italian respectively) are no more likely to
be aligned than others (like ?vache? and ?guardare?,
meaning ?cow? and ?to watch? respectively). Such
a root is sometimes subject to vowel shift and conso-
nant gradation, and may not be reflected in orthog-
raphy, since it is often purely phonetic.
This means we are not taking advantage of every-
thing that normally benefits human speakers, hear-
ers and translators. It may be that a more natural
approach to translation would first involve under-
standing of the input, stored in some mental rep-
resentation (an interlingua), and then generation of
an equivalent phrase in the target language, directly
from the knowledge sources.
In order to allow for more dramatic differences
in grammar like agglutinativity, it seems that the
statistical machine translation (SMT) system must
be more aware of sub-word units (morphemes) and
features (phonetic similarity). This general sort of
morphological approach could potentially benefit
any language pair, but might be crucial for a sys-
tem that handles Finnish, Turkish, Hungarian, Es-
tonian or any other highly inflectional language. In
the following section we discuss the confounds pre-
sented by agglutinative languages, and how aware-
ness of morphemes might improve the situation.
This is followed by a brief foray into semantics
and natural language generation as a component of
123
SMT. Capturing phonetic features is most applicable
to speech-to-speech translation, which will be dis-
cussed in the penultimate section. A description of
the Bible conversation experiment and some of its
results can be found in the final section.
2 Agglutinative Confounds
Traditional n-gram language models and phrase-
based translation models do not work terribly well
for Finnish because each lexical item can appear in
dozens of inflected or declined forms. If an SMT
system is presented with ?taloosi? (to your house), it
will not know if that is another form of a word it saw
in training (like ?taloissaan?, in their houses). Align-
ment data are thus unnaturally sparse and test sen-
tences often contain several unknown items, which
share their stems with trained words. It has been
assumed that morphological analysis would be es-
sential for handling agglutinative languages. How-
ever, although several effective segmenters and an-
alyzers for specific languages exist, and even unsu-
pervised language-neutral versions such as Morfes-
sor (Creutz and Lagus, 2007), only recently have
similar approaches been successfully used in the
context of machine translation to improve the BLEU
score (Oflazer and El-Kahlout, 2007), and none yet
in Finnish.
In our experience, building a translation model
through stemmed (truncated) word-alignment out-
performs full-form alignment, or any morph-
segmented alignment. But once one has generated
such a translation model, including phrase tables
where stemmed forms (keys in source language)
are associated with full forms (values in target lan-
guage), is there anything to be gained from induction
of morphology? Our research in this area has yet to
reveal any positive results, but we are still working
on it. It is also worth considering the effectiveness of
the evaluation metrics. Does BLEU accurately cap-
ture the accuracy of a translation, particularly in an
agglutinative language? Unfortunately not.
We think the word segmentation in the BLEU
metric is biased against progress in morpheme-level
translation. Some other metrics have been set forth,
but none is widely accepted, in part due to inertia,
but also because translation cannot be objectively
evaluated, unless both the communicative intent and
its effectiveness can be quantified. The same prob-
lem occurs for teachers grading essays ? what was
the student intending to convey, was the phrasing
correct, the argument sound, and where does all this
diverge from the underlying power of words, written
or well said, to transmit information? Translation is
an art, and maybe in addition to human evaluation
by linguists and native speakers of the language, we
should consider the equivalent of an art or literary
critic. On the other hand, that might only be worth-
while for poetry, wherein automated translation is
perhaps not the best approach.
One might think that the stemmed model de-
scribed above would lose track of closed-class func-
tion items (like prepositions), particularly when they
are represented as inflectional morphemes in one
language but as separate words in the other. How-
ever, it seems that the language model for the target
takes care of that quite well in most cases. There
are some languages (like Japanese) with underspec-
ified noun phrases, in which efforts to preserve def-
initeness (i.e., the book, kirjan; a book, kirjaa) seem
futile, but given the abundance of monolingual data
to train LM?s on, these are contextually inferred and
corrected at the tail end of the production line. Ag-
glutinative confounds are thus very closely related to
other issues found throughout machine translation,
and perhaps an integrated solution (including a new
evaluation metric) is necessary.
3 Knowledge-Based Approaches
Incorporating statistical natural language generation
into a machine translation system involves some
modifications to the above. First, the source lan-
guage is translated or parsed into ontological rep-
resentations. This is similar to sentence parsing
techniques that can be used to induce a context-free
grammar for a language (Charniak, 1997), and could
in fact be considered one of their more useful appli-
cations. The parsing generally depends on a proba-
bilistic model trained on sentences aligned with their
syntactic and semantic representations, often in a
tree that could be generated by a context-free gram-
mar. The resulting semantic representation can then
be used as the source of a target-language generation
process.
The algorithm that generates such a representa-
124
tion from raw input could be trained on a tree-
bank, and an annotated form of the same corpus
(where the derivations in the generation space are
associated with counts for each decision made) can
be used to train the output component to generate
language. (Belz, 2005) To incorporate the statisti-
cal component, which allows for robust generaliza-
tion, per (Knight and Hatzivassiloglou, 1995), the
NLG on the target side is filtered through a language
model (described above). This helps address many
of the knowledge gap problems introduced by lin-
guistic differences or in a component of the system
- the analyzer or generator.
This approach does have significant advantages,
particularly in that it is more focused on semantics
(as opposed to statistical cooccurrence), so it may
be less likely to distort meaning. On the other hand,
it could misinterpret or miscommunicate (or both),
just like a human translator. Perhaps the crucial dif-
ference is that, while machine learning often has lit-
tle to do with our understanding of cognitive pro-
cesses, this sort of machine translation has greater
potential for illuminating mysterious areas of the hu-
man process. It is not an ersatz brain, nor neural
network, but in many ways it has more in common
with those technologies (particularly in that they
model cognition) than many natural language pro-
cessing algorithms. That is because, if we can get
a semantically-aware machine translation system to
work, it may more closely mirror human cognition.
Humans certainly do not ignore meaning when they
translate, but today?s statistical machine translation
has no awareness of it at all.
Potential disadvantages of the system include its
dependence on more resources. However, this is
less of a problem with WordNet(Miller, 1995) and
other such semantic webs. It is also worth men-
tioning again that humans always have an incred-
ible amount of information at their disposal when
translating. Not only all of their past experience and
word-knowledge, but their interlocutor?s demeanor,
manner, intonation, facial expressions, gestures, and
so on. There are often things that would be obvi-
ous in the context of a conversation, but are missing
from the transcribed text. For instance, the referent
of many pronouns is ambiguous, but usually there is
a unique individual or item picked out by the speak-
ers? shared information. This is true for simple sen-
tences like ?He hit him,? which are normally dis-
ambiguated by conversational context, but a purely
statistical, pseudo-syntactic interpretation would get
little of the meaning a human would glean from that
utterance.
4 Spoken Features
Speech-to-speech machine translation is in some
ways the peak of natural language processing, in that
it deals directly with our (humans?) original, oral
mode of communication (as opposed to derived writ-
ten language). As such, it presents challenges that
are not to be taken lightly. Much of the pipeline in-
volved is at least relatively straightforward: acoustic
modeling and language modeling on the input side
can take advantage of the latest advances without
extensive adaptation; similarly, speech synthesis on
the output can be directly connected with the system
(i.e., not work with text output, but a richer repre-
sentation).
Although such a system might seem quite com-
plicated, it could better take advantage of all the
available data. Natural language understanding and
generation could even be incorporated to an extent,
perhaps to add further confidence measures based
on semantic equivalence. Designing it in this way
also allows for a variety of methods to be tried with
ease, in a modular fashion. It may be that yet an-
other source of information can be found to improve
the translation by adding features to the translation
model ? perhaps leveraging multilingual corpora in
other languages, segmenting into morphemes earlier
in the process, or even incorporating intonation in
some fashion. Weights for all such features could
be learned during training, such that no language-
specific tuning would be necessary. This framework
would certainly not make speech-to-speech transla-
tion simple, but its flexibility might make research
and improvement in this area more tractable.
Efficiency is crucial in online translation of con-
versation, so a word alignment model with collapsed
Gibbs sampling, rather than EM, at its core is worth
experimenting with. We have written up a bare-
bones IBM Model 1 in both C++ and Python, us-
ing the standard EM approach and a Gibbs sampling
one. The latter allows for optimizations using lin-
ear algebra, and although it does not quite match the
125
perplexity or log-likelihood achieved by EM, it is
significantly faster, particularly on longer sentences.
Since morpheme segmentation is at least somewhat
helpful in speech recognition (Creutz, 2006; Creutz
et al, 2007), it should still be considered a potential
component in speech-to-speech translation. In terms
of incorporating the knowledge-based approach into
such a system, we think it may yet be too early,
but if existing understanding-and-generation frame-
works for machine translation could be adapted to
this use, it could be very fruitful, in particular since
spoken language generation might be more effective
from a knowledge base, since it would know what
it was trying to say, instead of relying on statistics
alone, hoping the phonemes end up in a meaningful
order.
The critical step of SST is, of course, transla-
tion. In an integrated system, as described above,
the translation model could be trained on a parallel
spoken corpus (perhaps tokenized into phonemes, or
segmented into morphemes), since there might be
advantages to limiting the intermediate steps in the
process. The Bible is a massively multilingual publi-
cation, and as it happens, its text is available aligned
between Finnish and English, and it is possible to
find corresponding recordings in both languages.
So, this corpus would enable a direct approach
to speech-to-speech translation. Alternatively, one
could treat the speech recognition and synthesis as
distinct from the translation, in which case text cor-
pora corresponding to the style and genre of speech
would be necessary. This would be particularly fea-
sible when, for instance, translating UN parliamen-
tary proceedings from a recording, for which trans-
lated transcripts are readily available. For a more
general and robust solution, we might advocate a
combined approach, in the hope that some potential
weaknesses of one might be avoided or compensated
for by using whatever limited resources are available
to add features from the other. Thus, a direct trans-
lation from speech to speech could be informed, in a
sense, by a derived translation from the recognized
text.
5 Biblical Chatter
Here, we present a system for translating Finnish to
English speech, in a restricted and ancient domain:
the Bible.
5.1 Introduction
Speech to speech translation attacks a variety of
problems at once, from speech recognition to syn-
thesis, and can similarly be used for several pur-
poses. If a system is efficient enough to be used
without introducing significant delay, it can trans-
late conversational speech online, acting as an in-
terpreter in place of (or in cooperation with) a hu-
man professional. On the other hand, a slow speech
translation system is still useful because it can make
news broadcasts (radio or television) accessible to
wider audiences through offline multilingual dub-
bing, allowing international viewers to enjoy a de-
layed broadcast.
5.2 System Description
The domain selected for our experiments was heav-
ily influenced by the available data. We needed a
bilingual (Finnish and English) and bimodal (text
and speech) corpus, and unfortunately none is read-
ily available, but we put one together using the
Bible. Both Old and New Testaments were used,
with one book from each left out for testing pur-
poses. We used multiple editions of the Bible to
train the translation model: the American Standard
Version (first published in 1901, updated 1997),
and Finnish translations (from 1992 and 1933,38).
The spoken recordings used were the World English
Bible (1997) and Finnish Bible (Raamattu) readings
(recorded at TKK 2004).
Our approach was to use existing components,
and try weaving them together in an optimal way.
First, there is the open vocabulary automatic speech
recognition (ASR) task, where the goal is to de-
tect phonemes in an acoustic signal and map them
to words. Here, we use an ?unlimited vocabu-
lary? continuous speech recognizer (Hirsima?ki et al,
2006), trained on a multi-speaker Finnish acoustic
model with a varigram (Siivola et al, 2007) lan-
guage model that includes Bible n-grams. Then,
for translation, Moses (Koehn et al, 2007) is trained
on words and morphemes (derived from Morfessor
Baseline (Creutz and Lagus, 2005)). For speech syn-
thesis, we used Festival (Taylor, 1999), including the
built-in English voice and a Finnish voice developed
at Helsinki University.
126
5.3 Results
The following is an example fragment, taken from
the test corpus.
Niin Daavid meni David slept with his
lepoon isiensa? luo, fathers, and was
ja ha?nethaudattiin buried in the
Daavidin kaupunkiin. city of David. The days
Nelja?kymmenta? vuotta that David reigned
ha?n oli ollut over Israel were
Israelin kuninkaana. forty years; seven
Hebronissa ha?n years reigned he
hallitsi seitsema?n in Hebron, and
vuotta, Jerusalemissa thirty-three years
kolmenkymmenenkolmen reigned he
vuoden ajan. in Jerusalem.
Salomo nousi Solomon sat on
isa?nsa? Daavidin the throne of David
valtaistuimelle,ja his father; and
ha?nen kuninkuutensa his kingdom was
vahvistui lujaksi. established greatly.
A translation of the reference text skips recogni-
tion, and runs the system from translation to synthe-
sis. The following shows how the sample text was
translated by our system (BLEU = 0.735):
Niin Daavid meni so david slept with his
lepoon isiensa? luo, fathers and was
ja ha?net haudattiin buried in the
Daavidin kaupunkiin. city of david
Nelja?kymmenta? vuotta forty years he
ha?n oli ollut was king over
Israelin kuninkaana. israel and in
Hebronissa ha?n hebron he reigned
hallitsi seitsema?n seven years
vuotta, Jerusalemissa in jerusalem
kolmenkymmenenkolmen thirty and three
vuoden ajan. years solomon
Salomo nousi went up to
isa?nsa? Daavidin the throne of
valtaistuimelle, ja david his father
ha?nen kuninkuutensa and his kingdom
vahvistui lujaksi. was strong for luja
The following recognized translation (BLEU =
0.541) represents a complete run of the system. The
recognition (on the left) had a LER of 12.9% and a
WER of 56.8%.
niintaa meni niintaa went
lepoon isiensa?lla isiensa?lla rest and was
ja ha?net haudattiin buried in the
daavidin kaupunkiin city of david the king
nelja?kymmenta? of israel was
vuotta ha?n oli ollut forty years he was
israelin kuninkaan in hebron he
hebronissa ha?n reigned seven years
hallitsi seitsema?n in jerusalem
vuotta jerusalemissa kymmenenkolmen
kolmen kymmenenkolmen three years
vuoden ajan after the new
salomon uusi on the throne of david
isa?nsa? daavidin and solomon
valtaistuimelle ja his father
ha?nenkuninkuutensa ha?nenkuninkuutensa
valmistulujaksi valmistulujaksi
Here we have an alternative path through the sys-
tem, which uses Morfessor on the recognized text,
and then translates using a model trained on the
morpheme-segmented corpus. This results in a re-
duced score (BLEU = 0.456), but fewer unknown
words.
n iin taa# meni# iin behind went to
lepo on# isi ensa? lla# the sabbath that
ja# ha?n et# hauda ttiin# is with ensa? and he
daavid in# kaupunki in# shall not at the grave of abner
nelja?kymmenta?# vuotta# was forty years of the
ha?n# oli# ollut# city of david and
israeli n kuninkaan# he was israeli to
hebron issa# ha?n# the king of hebron
hallitsi# seitsema?n# and he reigned
vuotta# jerusalem seven years in
issa# kolmen# jerusalem three tenth
kymmenen kolmen# three years of
vuoden# ajan# the new solomon his
salomo n# uusi# isa? istuim to david
nsa?# daavid in# my father of the
valta istuim elle# kingdoms of
ja# ha?n en kun ink ink and he
uutensa# valmistu luja ksi# uutensa valmistu to luja
The morphemes might have been more effective
in translation if they had been derived through rule-
based morphological analysis. Or, they could still be
statistical, but optimized for the translation phase by
minimizing perplexity during word alignment.
A significant barrier to thorough and concrete
evaluation of our system involves segmentation of
the speech stream into sentences (or verses) to match
the text. In the above examples, we manually
clipped the audio files. Evaluating performance on
the entire test set reduced the BLEU score if the
data were streamed through each component unseg-
mented. When the recognizer was set to insert a pe-
riod for detected pauses of a certain length, or at sen-
tence boundaries identified by its language model,
127
input to the translation phase became considerably
more problematic. In particular, the lattice input
ought to be split into sentences, but there would usu-
ally be a period in every time slice (but with low
probability).
5.4 Discussion
There were significant difficulties in the process,
particularly in the English to Finnish direction.
Whereas Finnish speech recognition is relatively
straightforward, since its orthography is consistent,
English speech recognition is more dependent on
a pronunciation dictionary. Although many such
dictionaries are available, and the pronunciation of
novel words can be estimated, neither of these re-
sources is terribly effective within the Bible domain,
where there are many archaic words and names. In
the second step, translation into Finnish is demon-
strably difficult from any source language, and re-
sults in consistently lower BLEU scores (Virpioja
et al, 2007). And although using morphemes can
reduce the frequency of unknown words, it also re-
duces the BLEU score.
It might improve translation quality if we use the
recognizer lattice as translator input, since acous-
tically improbable segments may lead to the most
fluent translation. Having access to many possibili-
ties might help the translation model, but then again,
second-guessing the recognizer might not be help-
ful. There were some difficulties with the Moses in-
tegration, in part because the word-sausage format
varies from SRILM?s. Also, the recognizer output
indicates word boundaries as <w>, not string-final
hash-marks (#). This is problematic since the for-
mer are separate symbols, occupying a node in the
lattice, whereas the latter are appended to another
symbol (e.g., ?<w> morph eme </w>?, 4 nodes,
versus ?morph eme#?, 2 nodes). Using the lattice,
final output from Moses tends to be more fluent,
but less on-topic, and often truncated. Although we
have no improvements thus far, it is likely that with
further parameter tuning, we could achieve better re-
sults. On the other hand, we seek a general, robust,
domain-independent solution, so focusing on Bible
translation may not be worthwhile.
Our speech-to-speech translation system is
accessible through a web interface.
http://www.cis.hut.fi/projects/speech/
translate/
It accepts a sound file, with recorded Finnish
bible-style chatter, an optional reference text and
translation, and within a half hour (usually much
less) sends a detailed report, including a sound file
with the synthesized English.
Ideas for future research include online speech-
to-speech translation, which must be efficient, light-
weight and robust. A potential barrier to this and
other applications is the lack of spoken language
training texts. It might be possible to adaptively train
to new speakers and contexts, perhaps taking advan-
tage of an efficient alternative to EM in word align-
ment (see discussion of Gibbs sampling). As men-
tioned elsewhere, it might be worth using prosodic
features captured during recognition as factors in
translation. Adapting existing resources to new lan-
guage pairs is particularly essential in an area where
so much is necessary, and so little available.
6 Conclusion
We cannot yet say for sure whether linguistic or
statistically optimized morphemes derived from text
corpora could be useful somehow in machine trans-
lation, but it has been demonstrated helpful in
speech recognition. Awareness of sub-word units
could benefit a speech-to-speech translation system,
and it may in fact help to maintain information
from the speech recognizer about morpheme seg-
mentation throughout the translation process, even
in speech generation. Incorporating natural lan-
guage understanding may also be fruitful, but for
compact, efficient systems (like a handheld transla-
tor) might not have access to the necessary resources
or computational power to support that. On the other
hand, it is our duty as researchers to stay ahead of the
technology and push its limits.
We are by no means the first to imagine this, but
perhaps we will soon be speaking into wrist watches
that understand our query, seemingly instantly shift
through more information than Google has currently
indexed, and reply in fluent English, Finnish, or Pun-
jabi with as much detail as could be hoped for after
hours of painstaking research with current technol-
ogy. In this case (and computational linguists must
always be optimistic), knowledge-based natural lan-
guage processing certainly has a crucial place.
128
Morphemes and agglutinative languages do pose
unique problems for computational linguists, but
many of the general techniques developed for lan-
guages like Arabic and Chinese, which are equally
far from English in grammar (and even orthogra-
phy), might surmount those problems without any
manual adaptation. Discriminative training of fea-
tures used in the translation model allows for such
solutions to be molded automatically to whatever
language pair (and set of corpora) they are being
used for. There is, as always, much more to be done
in this area, and we hope our research into efficient,
online Bible-conversational translation ? a modern
Babelfish in an ancient genre? is fruitful, and helps
to shed light on lemmatization.
Acknowledgments
Many thanks to Teemu Hirsima?ki, Antti Puurula,
Sami-Virpioja and Jaakko J. Va?yrynen for their
help with components of the system and for their
thoughts and comments at various stages of the
project.
References
Anja Belz. 2005. Statistical generation: Three methods
compared and evaluated. In Proceedings of the 10th
European Workshop on Natural Language Generation
(ENLG05), pages 15?23.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI/IAAI, pages 598?603.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical Re-
port A81, Publications in Computer and Information
Science, Helsinki University of Technology.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1), January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkknen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraclar, and Andreas
Stolcke. 2007. Analysis of morph-based speech
recognition and the modeling of out-of-vocabulary
words across languages. In Proceedings of Hu-
man Language Technologies / The Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL 2007),
Rochester, NY, USA.
Mathias Creutz. 2006. Morfessor in the morpho chal-
lenge. In Mikko Kurimo, Mathias Creutz, and Krista
Lagus, editors, Proceedings of the PASCAL Challenge
Workshop on Unsupervised Segmentation of Words
into Morphemes, Venice, Italy.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkko?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language models
applied to Finnish. Computer Speech and Language,
20(4):515?541.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, pages 252?260, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Ondrej Bojar, Alexandra Constantin, and Evan
Herb. 2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the ACL 2007
Demo and Poster Sessions, pages 177?180.
George A. Miller. 1995. Wordnet: a lexical database for
English. Commun. ACM, 38(11):39?41.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages 25?
32.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Transactions on Audio, Speech
and Language Processing, 15(5):1617?1624.
Paul Taylor. 1999. The Festival Speech Architecture.
Web page.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology?aware statis-
tical machine translation based on morphs induced in
an unsupervised manner. In Proceedings of the Ma-
chine Translation Summit XI, Copenhagen, Denmark.
To appear.
129
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
130
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 487?494,
New York, June 2006. c?2006 Association for Computational Linguistics
Unlimited vocabulary speech recognition for agglutinative languages
Mikko Kurimo1, Antti Puurula1, Ebru Arisoy2, Vesa Siivola1,
Teemu Hirsim?ki1, Janne Pylkk?nen1, Tanel Alum?e3, Murat Saraclar2
1 Adaptive Informatics Research Centre, Helsinki University of Technology
P.O.Box 5400, FIN-02015 HUT, Finland
{Mikko.Kurimo,Antti.Puurula,Vesa.Siivola}@tkk.fi
2 Bogazici University, Electrical and Electronics Eng. Dept.
34342 Bebek, Istanbul, Turkey
{arisoyeb,murat.saraclar}@boun.edu.tr
3 Laboratory of Phonetics and Speech Technology,
Institute of Cybernetics, Tallinn Technical University, Estonia
tanel.alumae@phon.ioc.ee
Abstract
It is practically impossible to build a
word-based lexicon for speech recogni-
tion in agglutinative languages that would
cover all the relevant words. The prob-
lem is that words are generally built by
concatenating several prefixes and suffixes
to the word roots. Together with com-
pounding and inflections this leads to mil-
lions of different, but still frequent word
forms. Due to inflections, ambiguity and
other phenomena, it is also not trivial to
automatically split the words into mean-
ingful parts. Rule-based morphological
analyzers can perform this splitting, but
due to the handcrafted rules, they also suf-
fer from an out-of-vocabulary problem. In
this paper we apply a recently proposed
fully automatic and rather language and
vocabulary independent way to build sub-
word lexica for three different agglutina-
tive languages. We demonstrate the lan-
guage portability as well by building a
successful large vocabulary speech recog-
nizer for each language and show superior
recognition performance compared to the
corresponding word-based reference sys-
tems.
1 Introduction
Speech recognition for dictation or prepared radio
and television broadcasts has had huge advances
during the last decades. For example, broadcast
news (BN) in English can now be recognized with
about ten percent word error rate (WER) (NIST,
2000) which results in mostly quite understandable
text. Some rare and new words may be missing but
the result has proven to be sufficient for many im-
portant applications, such as browsing and retrieval
of recorded speech and information retrieval from
the speech (Garofolo et al, 2000). However, besides
the development of powerful computers and new al-
gorithms, a crucial factor in this development is the
vast amount of transcribed speech and suitable text
data that has been collected for training the mod-
els. The problem faced in porting the BN recogni-
tion systems to conversational speech or to other lan-
guages is that almost as much new speech and text
data have to be collected again for the new task.
The reason for the need for a vast amount of train-
ing texts is that the state-of-the-art statistical lan-
guage models contain a huge amount of parameters
to be estimated in order to provide a proper probabil-
ity for any possible word sequence. The main reason
for the huge model size is that for an acceptable cov-
erage in an English BN task, the vocabulary must
be very large, at least 50,000 words, or more. For
languages with a higher degree of word inflections
than English, even larger vocabularies are required.
This paper focuses on the agglutinative languages in
which words are frequently formed by concatenat-
ing one or more stems, prefixes, and suffixes. For
these languages in which the words are often highly
inflected as well as formed from several morphemes,
even a vocabulary of 100,000 most common words
would not give sufficient coverage (Kneissler and
487
Klakow, 2001; Hirsim?ki et al, 2005). Thus, the
solution to the language modeling clearly has to in-
volve splitting of words into smaller modeling units
that could then be adequately modeled.
This paper focuses on solving the vocabulary
problem for several languages in which the speech
and text database resources are much smaller than
for the world?s main languages. A common fea-
ture for the agglutinative languages, such as Finnish,
Estonian, Hungarian and Turkish is that the large
vocabulary continuous speech recognition (LVCSR)
attempts so far have not resulted comparable perfor-
mance to the English systems. The reason for this
is not only the language modeling difficulties, but,
of course, the lack of suitable speech and text train-
ing data resources. In (Geutner et al, 1998; Sii-
vola et al, 2001) the systems aim at reducing the
active vocabulary and language models to a feasi-
ble size by clustering and focusing. In (Szarvas and
Furui, 2003; Alum?e, 2005; Hacioglu et al, 2003)
the words are split into morphemes by language-
dependent hand-crafted morphological rules. In
(Kneissler and Klakow, 2001; Arisoy and Arslan,
2005) different combinations of words, grammati-
cal morphemes and endings are utilized to decrease
the OOV rate and optimize the speech recognition
accuracy. However, constant large improvements
over the conventional word-based language models
in LVCSR have been rare.
The approach presented in this paper relies on a
data-driven algorithm called Morfessor (Creutz and
Lagus, 2002; Creutz and Lagus, 2005) which is a
language independent unsupervised machine learn-
ing method to find morpheme-like units (called sta-
tistical morphs) from a large text corpus. This
method has several advantages over the rule-based
grammatical morphemes, e.g. that no hand-crafted
rules are needed and all words can be processed,
even the foreign ones. Even if good grammatical
morphemes are available, the language modeling re-
sults by the statistical morphs seem to be at least as
good, if not better (Hirsim?ki et al, 2005). In this
paper we evaluate the statistical morphs for three
agglutinative languages and describe three different
speech recognition systems that successfully utilize
the n-gram language models trained for these units
in the corresponding LVCSR tasks.
2 Building the lexicon and language
models
2.1 Unsupervised discovery of morph units
Naturally, there are many ways to split the words
into smaller units to reduce a lexicon to a tractable
size. However, for a subword lexicon suitable
for language modeling applications such as speech
recognition, several properties are desirable:
1. The size of the lexicon should be small enough
that the n-gram modeling becomes more feasi-
ble than the conventional word based modeling.
2. The coverage of the target language by words
that can be built by concatenating the units
should be high enough to avoid the out-of-
vocabulary problem.
3. The units should be somehow meaningful, so
that the previously observed units can help in
predicting the next one.
4. In speech recognition one should be able to de-
termine the pronunciation for each unit.
A common approach to find the subword units
is to program the language-dependent grammatical
rules into a morphological analyzer and utilize that
to then split the text corpus into morphemes as in
e.g. (Hirsim?ki et al, 2005; Alum?e, 2005; Ha-
cioglu et al, 2003). There are some problems re-
lated to ambiguous splits and pronunciations of very
short inflection-type units, but also the coverage in,
e.g., news texts may be poor because of many names
and foreign words.
In this paper we have adopted a similar approach
as (Hirsim?ki et al, 2005). We use unsupervised
learning to find the best units according to some cost
function. In the Morfessor algorithm the minimized
cost is the coding length of the lexicon and the words
in the corpus represented by the units of the lexicon.
This minimum description length based cost func-
tion is especially appealing, because it tends to give
units that are both as frequent and as long as possi-
ble to suit well for both training the language models
and also decoding of the speech. Full coverage of
the language is also guaranteed by splitting the rare
words into very short units, even to single phonemes
if necessary. For language models utilized in speech
488
recognition, the lexicon of the statistical morphs can
be further reduced by omitting the rare words from
the input of the Morfessor algorithm. This operation
does not reduce the coverage of the lexicon, because
it just splits the rare words then into smaller units,
but the smaller lexicon may offer a remarkable speed
up of the recognition.
The pronunciation of, especially, the short units
may be ambiguous and may cause severe problems
in languages like English, in which the pronuncia-
tions can not be adequately determined from the or-
thography. In most agglutinative languages, such as
Finnish, Estonian and Turkish, rather simple letter-
to-phoneme rules are, however, sufficient for most
cases.
2.2 Building the lexicon for open vocabulary
The whole training text corpus is first passed through
a word splitting transformation as in Figure 1. Based
on the learned subword unit lexicon, the best split
for each word is determined by performing a Viterbi
search with the unigram probabilities of the units. At
this point the word break symbols are added between
each word in order to incorporate that information in
the statistical language models, as well. Then the n-
gram models are trained similarly as if the language
units were words including word and sentence break
symbols as additional units.
2.3 Building the n-gram model over morphs
Even though the required morph lexicon is much
smaller than the lexicon for the corresponding word
n-gram estimation, the data sparsity problem is still
important. Interpolated Kneser-Ney smoothing is
utilized to tune the language model probabilities in
the same way as found best for the word n-grams.
The n-grams that are not very useful for modeling
the language can be discarded from the model in
order to keep the model size down. For Turkish,
we used the entropy based pruning (Stolcke, 1998),
where the n-grams, that change the model entropy
less than a given treshold, are discarded from the
model. For Finnish and Estonian, we used n-gram
growing (Siivola and Pellom, 2005). The n-grams
that increase the training set likelihood enough with
respect to the corresponding increase in the model
size are accepted into the model (as in the minimum
description length principle). After the growing pro-
Morph lexicon
+ probabilities
word forms
Distinct
Text with words
segmented into
morphs
model
Language
Text corpus
segmentation
Viterbi
segmentation
MorphExtract
vocabulary
Train
n?grams
Figure 1: The steps in the process of estimating a
language model based on statistical morphs from a
text corpus (Hirsim?ki et al, 2005).
cess the model is further pruned with entropy based
pruning. The method allows us to train models with
higher order n-grams, since the memory consump-
tion is lower and also gives somewhat better mod-
els. Both methods can also be viewed as choosing
the correct model complexity for the training data to
avoid over-learning.
3 Statistical properties of Finnish,
Estonian and Turkish
Before presenting the speech recognition results,
some statistical properties are presented for the three
agglutinative languages studied. If we consider
choosing a vocabulary of the 50k-70k most common
words, as usual in English broadcast news LVCSR
systems, the out-of-vocabulary (OOV) rate in En-
glish is typically smaller than 1%. Using the lan-
guage model training data the following OOV rates
can be found for a vocabulary including only the
most common words: 15% OOV for 69k in Finnish
(Hirsim?ki et al, 2005), 10% for 60k in Estonian
and 9% for 50k in Turkish. As shown in (Hacioglu et
al., 2003) this does not only mean the same amount
of extra speech recognition errors, but even more,
because the recognizer tends to lose track when un-
known words get mapped to those that are in the vo-
cabulary. Even doubling the vocabulary is not a suf-
489
0 1 2 3
x 106
0
2
4
6
8 x 10
5
Number of sentences
N
um
be
r o
f d
ist
in
ct
 u
ni
ts
0 1 2 3
x 106
2.6
2.8
3
3.2
3.4
3.6 x 10
4
Number of sentences
N
um
be
r o
f d
ist
in
ct
 m
or
ph
s
Morphs
Words Morphs
Figure 2: Vocabulary growth of words and morphs
for Turkish language
ficient solution, because a vocabulary twice as large
(120k) would only reduce the OOV rate to 6% in
Estonian and 5% in Turkish. In Finnish even a 400k
vocabulary of the most common words still gives 5%
OOV in the language model training material.
Figure 2 illustrates the vocabulary explosion en-
countered when using words and how using morphs
avoids this problem for Turkish. The figure on the
left shows the vocabulary growth for both words and
morphs. The figure on the right shows the graph
for morphs in more detail. As seen in the figure,
the number of new words encountered continues to
increase as the corpus size gets larger whereas the
number of new morphs encountered levels off.
4 Speech recognition experiments
4.1 About selection of the recognition tasks
In this work the morph-based language models have
been applied in speech recognition for three differ-
ent agglutinative languages, Finnish, Estonian and
Turkish. The recognition tasks are speaker depen-
dent and independent fluent dictation of sentences
taken from newspapers and books, which typically
require very large vocabulary language models.
4.2 Finnish
Finnish is a highly inflected language, in which
words are formed mainly by agglutination and com-
pounding. Finnish is also the language for which the
algorithm for the unsupervised morpheme discovery
(Creutz and Lagus, 2002) was originally developed.
The units of the morph lexicon for the experiments
in this paper were learned from a joint corpus con-
taining newspapers, books and newswire stories of
totally about 150 million words (CSC, 2001). We
obtained a lexicon of 25k morphs by feeding the
learning algorithm with the word list containing the
160k most common words. For language model
training we used the same text corpus and the re-
cently developed growing n-gram training algorithm
(Siivola and Pellom, 2005). The amount of resulted
n-grams are listed in Table 4. The average length
of a morph is such that a word corresponds to 2.52
morphs including a word break symbol.
The speech recognition task consisted of a book
read aloud by one female speaker as in (Hirsim?ki et
al., 2005). Speaker dependent cross-word triphone
models were trained using the first 12 hours of data
and evaluated by the last 27 minutes. The models
included tied state hidden Markov models (HMMs)
of totally 1500 different states, 8 Gaussian mixtures
(GMMs) per state, short-time mel-cepstral features
(MFCCs), maximum likelihood linear transforma-
tion (MLLT) and explicit phone duration models
(Pylkk?nen and Kurimo, 2004). The real-time fac-
tor of recognition speed was less than 10 xRT with
a 2.2 GHz CPU. However, with the efficient LVCSR
decoder utilized (Pylkk?nen, 2005) it seems that by
making an even smaller morph lexicon, such as 10k,
the decoding speed could be optimized to only a few
times real-time without an excessive trade-off with
recognition performance.
4.3 Estonian
Estonian is closely related to Finnish and a similar
language modeling approach was directly applied
to the Estonian recognition task. The text corpus
used to learn the morph units and train the statis-
tical language model consisted of newspapers and
books, altogether about 55 million words (Segakor-
pus, 2005). At first, 45k morph units were obtained
as the best subword unit set from the list of the 470k
most common words in the corpora. For speed-
ing up the recognition, the morph lexicon was after-
wards reduced to 37k by splitting the rarest morphs
(occurring in only one or two words) further into
smaller ones. Corresponding growing n-gram lan-
guage models as in Finnish were trained from the
Estonian corpora resulting the n-grams in Table 4.
The speech recognition task in Estonian consisted
of long sentences read by 50 randomly picked held-
out test speakers, 7 sentences each (a part of (Meister
490
et al, 2002)). Unlike the Finnish and Turkish micro-
phone data, this data was recorded from telephone,
i.e. 8 kHz sampling rate and narrow band data in-
stead of 16 kHz and normal (full) bandwidth. The
phoneme models were trained for speaker indepen-
dent recognition using windowed cepstral mean sub-
traction and significantly more data (over 200 hours
and 1300 speakers) than for the Finnish task. The
speaker independence, together with the telephone
quality and occasional background noises, made this
task still a considerably more difficult one. Other-
wise the acoustic models were similar cross-word
triphone GMM-HMMs with MFCC features, MLLT
transformation and the explicit phone duration mod-
eling, except larger: 5100 different states and 16
GMMs per state. Thus, the recognition speed is
also slower than in Finnish, about 20 xRT (2.2GHz
CPU).
4.4 Turkish
Turkish is another a highly-inflected and agglutina-
tive language with relatively free word order. The
same Morfessor tool (Creutz and Lagus, 2005) as in
Finnish and Estonian was applied to Turkish texts
as well. Using the 360k most common words from
the training corpus, 34k morph units were obtained.
The training corpus consists of approximately 27M
words taken from literature, law, politics, social
sciences, popular science, information technology,
medicine, newspapers, magazines and sports news.
N-gram language models for different orders with
interpolated Kneser-Ney smoothing as well as en-
tropy based pruning were built for this morph lexi-
con using the SRILM toolkit (Stolcke, 2002). The
number of n-grams for the highest order we tried (6-
grams without entropy-based pruning) are reported
in Table 4. In average, there are 2.37 morphs per
word including the word break symbol.
The recognition task in Turkish consisted of ap-
proximately one hour of newspaper sentences read
by one female speaker. We used decision-tree state
clustered cross-word triphone models with approx-
imately 5000 HMM states. Instead of using letter
to phoneme rules, the acoustic models were based
directly on letters. Each state of the speaker inde-
pendent HMMs had a GMM with 6 mixture compo-
nents. The HTK frontend (Young et al, 2002) was
used to get the MFCC based acoustic features. The
explicit phone duration models were not applied.
The training data contained 17 hours of speech from
over 250 speakers. Instead of the LVCSR decoder
used in Finnish and Estonian (Pylkk?nen, 2005), the
Turkish evaluation was performed using another de-
coder (AT&T, 2003), Using a 3.6GHz CPU, the real-
time factor was around one.
5 Results
The recognition results for the three different tasks:
Finnish, Estonian and Turkish, are provided in Ta-
bles 1 ? 3. In each task the word error rate (WER)
and letter error rate (LER) statistics for the morph-
based system is compared to a corresponding word-
based system. The resulting morpheme strings are
glued to words according to the word break symbols
included in the language model (see Section 2.2) and
the WER is computed as the sum of substituted, in-
serted and deleted words divided by the correct num-
ber of words. LER is included here as well, because
although WER is a more common measure, it is not
comparable between languages. For example, in ag-
glutinative languages the words are long and contain
a variable amount of morphemes. Thus, any incor-
rect prefix or suffix would make the whole word in-
correct. The n-gram language model statistics are
given in Table 4.
Finnish lexicon WER LER
Words 400k 8.5 1.20
Morphs 25k 7.0 0.95
Table 1: The LVCSR performance for the speaker-
dependent Finnish task consisting of book-reading
(see Section 4.2). For a reference (word-based) lan-
guage model a 400k lexicon was chosen.
Estonian lexicon WER LER
Words 60k 56.3 22.4
Morphs 37k 47.6 18.9
Table 2: The LVCSR performance for the speaker-
independent Estonian task consisting of read sen-
tences recorded via telephone (see Section 4.3). For
a reference (word-based) language model a 60k lex-
icon was used here.
491
Turkish lexicon WER LER
Words
3-gram 50k 38.8 15.2
Morphs
3-gram 34k 39.2 14.8
4-gram 34k 35.0 13.1
5-gram 34k 33.9 12.4
Morphs, rescored by morph 6-gram
3-gram 34k 33.8 12.4
4-gram 34k 33.2 12.3
5-gram 34k 33.3 12.2
Table 3: The LVCSR performance for the speaker-
independent Turkish task consisting of read news-
paper sentences (see Section 4.4). For the refer-
ence 50k (word-based) language model the accuracy
given by 4 and 5-grams did not improve from that of
3-grams.
In the Turkish recognizer the memory constraints
during network optimization (Allauzen et al, 2004)
allowed the use of language models only up to 5-
grams. The language model pruning thresholds were
optimized over a range of values and the best re-
sults are shown in Table 3. We also tried the same
experiments with two-pass recognition. In the first
pass, instead of the best path, lattice output was gen-
erated with the same language models with prun-
ing. Then these lattices were rescored using the non-
pruned 6-gram language models (see Table 4) and
the best path was taken as the recognition output.
For the word-based reference model, the two-pass
recognition gave no improvements. It is likely that
the language model training corpus was too small to
train proper 6-gram word models. However, for the
morph-based model, we obtained a slight improve-
ment (0.7 % absolute) by two-pass recognition.
6 Discussion
The key result of this paper is that we can success-
fully apply the unsupervised statistical morphs in
large vocabulary language models in all the three ex-
perimented agglutinative languages. Furthermore,
the results show that in all the different LVCSR
tasks, the morph-based language models perform
very well and constantly dominate the reference lan-
guage model based on words. The way that the lexi-
# morph-based models
ngrams Finnish Estonian Turkish
1grams 24,833 37,061 34,332
2grams 2,188,476 1,050,127 655,621
3grams 17,064,072 7,133,902 1,936,263
4grams 25,200,308 8,201,543 3,824,362
5grams 7,167,021 3,298,429 4,857,125
6grams 624,832 691,899 5,523,922
7grams 23,851 55,363 -
8grams 0 1045 -
Sum 52,293,393 20,469,369 16,831,625
Table 4: The amount of different n-grams in each
language model based on statistical morphs. Note
that the Turkish language model was not prepared
by the growing n-gram algorithm as the others and
the model was limited to 6-grams.
con is built from the word fragments allows the con-
struction of statistical language models, in practice,
for almost an unlimited vocabulary by a lexicon that
still has a convenient size.
The recognition was here restricted to agglutina-
tive languages and tasks in which the language used
is both rather general and matches fairly well with
the available training texts. Significant performance
variation in different languages can be observed
here, because of the different tasks and the fact that
comparable recognition conditions and training re-
sources have not been possible to arrange. However,
we believe that the tasks are still both difficult and
realistic enough to illustrate the difference of per-
formance when using language models based on a
lexicon of morphs vs. words in each task. There are
no directly comparable previous LVCSR results on
the same tasks and data, but the closest ones which
can be found are slightly over 20% WER for the
Finnish task (Hirsim?ki et al, 2005), slightly over
40 % WER for the Estonian task (Alum?e, 2005)
and slightly over 30 % WER for the Turkish task
(Erdogan et al, 2005).
Naturally, it is also possible to prepare a huge lex-
icon and still succeed in recognition fairly well (Sar-
aclar et al, 2002; McTait and Adda-Decker, 2003;
Hirsim?ki et al, 2005), but this is not a very con-
venient approach because of the resulting huge lan-
guage models or the heavy pruning required to keep
492
them still tractable. The word-based language mod-
els that were constructed in this paper as reference
models were trained as much as possible in the same
way as the corresponding morph language models.
For Finnish and Estonian the growing n-grams (Sii-
vola and Pellom, 2005) were used including the op-
tion of constructing the OOV words from phonemes
as in (Hirsim?ki et al, 2005). For Turkish a con-
ventional n-gram was built by SRILM similarly as
for the morphs. The recognition approach taken for
Turkish involves a static decoding network construc-
tion and optimization resulting in near real time de-
coding. However, the memory requirements of net-
work optimization becomes prohibitive for large lex-
icon and language models as presented in this paper.
In this paper the recognition speed was not a ma-
jor concern, but from the application point of view
that is a very important factor to be taken into a ac-
count in the comparison. It seems that the major fac-
tors that make the recognition slower are short lexi-
cal units, large lexicon and language models and the
amount of Gaussian mixtures in the acoustic model.
7 Conclusions
This work presents statistical language models
trained on different agglutinative languages utilizing
a lexicon based on the recently proposed unsuper-
vised statistical morphs. To our knowledge this is
the first work in which similarly developed subword
unit lexica are developed and successfully evaluated
in three different LVCSR systems in different lan-
guages. In each case the morph-based approach con-
stantly shows a significant improvement over a con-
ventional word-based LVCSR language models. Fu-
ture work will be the further development of also
the grammatical morph-based language models and
comparison of that to the current approach, as well
as extending this evaluation work to new languages.
8 Acknowledgments
We thank the Finnish Federation of the Visually Im-
paired for providing the Finnish speech data and the
Finnish news agency (STT) and the Finnish IT cen-
ter for science (CSC) for the text data. Our work was
supported by the Academy of Finland in the projects
New information processing principles, Adaptive In-
formatics and New adaptive and learning methods in
speech recognition. This work was supported in part
by the IST Programme of the European Community,
under the PASCAL Network of Excellence, IST-
2002-506778. The authors would like to thank Sa-
banci and ODTU universities for the Turkish acous-
tic and text data and AT&T Labs ? Research for
the software. This research is partially supported
by SIMILAR Network of Excellence and TUBITAK
BDP (Unified Doctorate Program of the Scientific
and Technological Research Council of Turkey).
References
Cyril Allauzen, Mehryar Mohri, Michael Riley, and Brian
Roark. 2004. A generalized construction of integrated
speech recognition transducers. In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), Montreal, Canada.
Tanel Alum?e. 2005. Phonological and morphologi-
cal modeling in large vocabulary continuous Estonian
speech recognition system. In Proceedings of Second
Baltic Conference on Human Language Technologies,
pages 89?94.
Mehryar Mohri and Michael D. Riley. DCD Library ?
Speech Recognition Decoder Library. AT&T Labs ?
Research. http://www.research.att.com/
sw/tools/dcd/.
Ebru Arisoy and Levent Arslan. 2005. Turkish dictation
system for broadcast news applications. In 13th Euro-
pean Signal Processing Conference - EUSIPCO 2005,
Antalya, Turkey, September.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of the Work-
shop on Morphological and Phonological Learning of
ACL-02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Unsuper-
vised morpheme segmentation and morphology in-
duction from text corpora using Morfessor. Techni-
cal Report A81, Publications in Computer and Infor-
mation Science, Helsinki University of Technology.
URL: http://www.cis.hut.fi/projects/
morpho/.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of Content Based Multimedia
Information Access Conference, April 12-14.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adap-
tive vocabularies for transcribing multilingual broad-
cast news. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), Seattle, WA, USA, May.
493
H. Erdogan, O. Buyuk, K. Oflazer. 2005. Incorporating
language constraints in sub-word based speech recog-
nition. IEEE Automatic Speech Recognition and Un-
derstanding Workshop, Cancun, Mexico.
Kadri Hacioglu, Brian Pellom, Tolga Ciloglu, Ozlem Oz-
turk, Mikko Kurimo, and Mathias Creutz. 2003. On
lexicon creation for Turkish LVCSR. In Proceedings
of 8th European Conference on Speech Communica-
tion and Technology, pages 1165?1168.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen. 2005.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language. (accepted for publication).
Jan Kneissler and Dietrich Klakow. 2001. Speech recog-
nition for huge vocabularies by using optimized sub-
word units. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology
(Eurospeech), pages 69?72, Aalborg, Denmark.
CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-
guage Text Bank: Corpora Books, Newspapers,
Magazines and Other. http://www.csc.fi/
kielipankki/.
Kevin McTait and Martine Adda-Decker. 2003. The
300k LIMSI German Broadcast News Transcription
System. In Proceedings of 8th European Conference
on Speech Communication and Technology.
Einar Meister, J?rgen Lasn, and Lya Meister. 2002. Esto-
nian SpeechDat: a project in progress. In Proceedings
of the Fonetiikan P?iv?t ? Phonetics Symposium 2002
in Finland, pages 21?26.
NIST. 2000. Proceedings of DARPA workshop on Auto-
matic Transcription of Broadcast News. NIST, Wash-
ington DC, May.
Janne Pylkk?nen. 2005. New pruning criteria for effi-
cient decoding. In Proceedings of 9th European Con-
ference on Speech Communication and Technology.
Janne Pylkk?nen and Mikko Kurimo. 2004. Duration
modeling techniques for continuous speech recogni-
tion. In Proceedings of the International Conference
on Spoken Language Processing.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, CO, USA.
Segakorpus ? Mixed Corpus of Estonian. Tartu Uni-
versity. http://test.cl.ut.ee/korpused/
segakorpus/.
Vesa Siivola and Bryan Pellom. 2005. Growing an n-
gram language model. In Proceedings of 9th European
Conference on Speech Communication and Technol-
ogy.
Vesa Siivola, Mikko Kurimo, and Krista Lagus. 2001.
Large vocabulary statistical language modeling for
continuous speech recognition. In Proceedings of 7th
European Conference on Speech Communication and
Technology, pages 737?747, Aalborg, Copenhagen.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Mate Szarvas and Sadaoki Furui. 2003. Evaluation of the
stochastic morphosyntactic language model on a one
million word Hungarian task. In Proceedings of the
8th European Conference on Speech Communication
and Technology (Eurospeech), pages 2297?2300.
S. Young, D. Ollason, V. Valtchev, and P. Woodland.
2002. The HTK book (for HTK version 3.2.), March.
494
Proceedings of NAACL HLT 2007, pages 380?387,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis of Morph-Based Speech Recognition and the Modeling of
Out-of-Vocabulary Words Across Languages
Mathias Creutz?, Teemu Hirsima?ki?, Mikko Kurimo?, Antti Puurula?, Janne Pylkko?nen?,
Vesa Siivola?, Matti Varjokallio?, Ebru Ar?soy?, Murat Sarac?lar?, and Andreas Stolcke?
? Helsinki University of Technology, <firstname>.<lastname>@tkk.fi,
? Bog?azic?i University, arisoyeb@boun.edu.tr, murat.saraclar@boun.edu.tr,
? SRI International / International Computer Science Institute, stolcke@speech.sri.com
Abstract
We analyze subword-based language
models (LMs) in large-vocabulary
continuous speech recognition across
four ?morphologically rich? languages:
Finnish, Estonian, Turkish, and Egyptian
Colloquial Arabic. By estimating n-gram
LMs over sequences of morphs instead
of words, better vocabulary coverage
and reduced data sparsity is obtained.
Standard word LMs suffer from high
out-of-vocabulary (OOV) rates, whereas
the morph LMs can recognize previously
unseen word forms by concatenating
morphs. We show that the morph LMs
generally outperform the word LMs and
that they perform fairly well on OOVs
without compromising the accuracy
obtained for in-vocabulary words.
1 Introduction
As automatic speech recognition systems are being
developed for an increasing number of languages,
there is growing interest in language modeling ap-
proaches that are suitable for so-called ?morpholog-
ically rich? languages. In these languages, the num-
ber of possible word forms is very large because
of many productive morphological processes; words
are formed through extensive use of, e.g., inflection,
derivation and compounding (such as the English
words ?rooms?, ?roomy?, ?bedroom?, which all stem
from the noun ?room?).
For some languages, language modeling based on
surface forms of words has proven successful, or at
least satisfactory. The most studied language, En-
glish, is not characterized by a multitude of word
forms. Thus, the recognition vocabulary can sim-
ply consist of a list of words observed in the training
text, and n-gram language models (LMs) are esti-
mated over word sequences. The applicability of the
word-based approach to morphologically richer lan-
guages has been questioned. In highly compounding
languages, such as the Germanic languages German,
Dutch and Swedish, decomposition of compound
words can be carried out to reduce the vocabulary
size. Highly inflecting languages are found, e.g.,
among the Slavic, Romance, Turkic, and Semitic
language families. LMs incorporating morphologi-
cal knowledge about these languages can be applied.
A further challenging category comprises languages
that are both highly inflecting and compounding,
such as the Finno-Ugric languages Finnish and Es-
tonian.
Morphology modeling aims to reduce the out-
of-vocabulary (OOV) rate as well as data sparsity,
thereby producing more effective language mod-
els. However, obtaining considerable improvements
in speech recognition accuracy seems hard, as is
demonstrated by the fairly meager improvements
(1?4 % relative) over standard word-based models
accomplished by, e.g., Berton et al (1996), Ordel-
man et al (2003), Kirchhoff et al (2006), Whit-
taker and Woodland (2000), Kwon and Park (2003),
and Shafran and Hall (2006) for Dutch, Arabic, En-
glish, Korean, and Czech, or even the worse perfor-
mance reported by Larson et al (2000) for German
and Byrne et al (2001) for Czech. Nevertheless,
clear improvements over a word baseline have been
achieved for Serbo-Croatian (Geutner et al, 1998),
Finnish, Estonian (Kurimo et al, 2006b) and Turk-
ish (Kurimo et al, 2006a).
In this paper, subword language models in the
recognition of speech of four languages are ana-
380
lyzed: Finnish, Estonian, Turkish, and the dialect
of Arabic spoken in Egypt, Egyptian Colloquial
Arabic (ECA). All these languages are considered
?morphologically rich?, but the benefits of using
subword-based LMs differ across languages. We at-
tempt to discover explanations for these differences.
In particular, the focus is on the analysis of OOVs:
A perceived strength of subword models, when con-
trasted with word models, is that subword models
can generalize to previously unseen word forms by
recognizing them as sequences of shorter familiar
word fragments.
2 Morfessor
Morfessor is an unsupervised, data-driven, method
for the segmentation of words into morpheme-like
units. The general idea is to discover as com-
pact a description of the input text corpus as possi-
ble. Substrings occurring frequently enough in sev-
eral different word forms are proposed as morphs,
and the words in the corpus are then represented
as a concatenation of morphs, e.g., ?hand, hand+s,
left+hand+ed, hand+ful?. Through maximum a pos-
teriori optimization (MAP), an optimal balance is
sought between the compactness of the inventory of
morphs, i.e., the morph lexicon, versus the compact-
ness of the representation of the corpus.
Among others, de Marcken (1996), Brent (1999),
Goldsmith (2001), Creutz and Lagus (2002), and
Creutz (2006) have shown that models based on
the above approach produce segmentations that re-
semble linguistic morpheme segmentations, when
formulated mathematically in a probabilistic frame-
work or equivalently using the Minimum Descrip-
tion Length (MDL) principle (Rissanen, 1989).
Similarly, Goldwater et al (2006) use a hierarchical
Dirichlet model in combination with morph bigram
probabilities.
The Morfessor model has been developed over
the years, and different model versions exist. The
model used in the speech recognition experiments of
the current paper is the original, so-called Morfes-
sor Baseline algorithm, which is publicly available
for download.1. The mathematics of the Morfessor
Baseline model is briefly outlined in the following;
consult Creutz (2006) for details.
1http://www.cis.hut.fi/projects/morpho/
2.1 MAP Optimization Criterion
In slightly simplified form, the optimization crite-
rion utilized in the model corresponds to the maxi-
mization of the following posterior probability:
P (lexicon | corpus) ?
P (lexicon) ? P (corpus | lexicon) =
?
letters ?
P (?) ?
?
morphs ?
P (?). (1)
The lexicon consists of all distinct morphs spelled
out; this forms a long string of letters ?, in which
each morph is separated from the next morph using
a morph boundary character. The probability of the
lexicon is the product of the probability of each let-
ter in this string. Analogously, the corpus is repre-
sented as a sequence of morphs, which corresponds
to a particular segmentation of the words in the cor-
pus. The probability of this segmentation equals the
product of the probability of each morph token ?.
Letter and morph probabilities are maximum likeli-
hood estimates (empirical Bayes).
2.2 From Morphs to n-Grams
As a result of the probabilistic (or MDL) approach,
the morph inventory discovered by the Morfessor
Baseline algorithm is larger the more training data
there is. In some speech recognition experiments,
however, it has been desirable to restrict the size of
the morph inventory. This has been achieved by set-
ting a frequency threshold on the words on which
Morfessor is trained, such that the rarest words will
not affect the learning process. Nonetheless, the
rarest words can be split into morphs in accordance
with the model learned, by using the Viterbi algo-
rithm to select the most likely segmentation. The
process is depicted in Figure 1.
2.3 Grapheme-to-Phoneme Mapping
The mapping between graphemes (letters) and
phonemes is straightforward in the languages stud-
ied in the current paper. More or less, there is
a one-to-one correspondence between letters and
phonemes. That is, the spelling of a word indicates
the pronunciation of the word, and when splitting the
word into parts, the pronunciation of the parts in iso-
lation does not differ much from the pronunciation
of the parts in context. However, a few exceptions
381
Morph
inventory
+ probs
n?grams
Train
cut?off
Frequency
Viterbi
segm.
Text with words
segmented into
LM
morphs
MorfessorExtractwords
Text corpus
Figure 1: How to train a segmentation model using
the Morfessor Baseline algorithm, and how to fur-
ther train an n-gram model based on morphs.
have been treated more rigorously in the Arabic ex-
periments: e.g., in some contexts the same (spelled)
morph can have multiple possible pronunciations.
3 Experiments and Analysis
The goal of the conducted experiments is to com-
pare n-gram language models based on morphs to
standard word n-gram models in automatic speech
recognition across languages.
3.1 Data Sets and Recognition Systems
The results from eight different tests have been an-
alyzed. Some central properties of the test config-
urations are shown in Table 1. The Finnish, Esto-
nian, and Turkish test configurations are slight vari-
ations of experiments reported earlier in Hirsima?ki
et al (2006) (Fin1: ?News task?, Fin2: ?Book task?),
Kurimo et al (2006a) (Fin3, Tur1), and Kurimo et
al. (2006b) (Fin4, Est, Tur2).
Three different recognition platforms have been
used, all of which are state-of-the-art large vocab-
ulary continuous speech recognition (LVCSR) sys-
tems. The Finnish and Estonian experiments have
been run on the HUT speech recognition system de-
veloped at Helsinki University of Technology.
The Turkish tests were performed using the
AT&T decoder (Mohri and Riley, 2002); the acous-
tic features were produced using the HTK front end
(Young et al, 2002). The experiments on Egyptian
Colloquial Arabic (ECA) were carried out using the
SRI DecipherTM speech recognition system.
3.1.1 Speech Data and Acoustic Models
The type and amount of speech data vary from
one language to another. The Finnish data con-
sists of news broadcasts read by one single female
speaker (Fin1), as well as an audio book read by an-
other female speaker (Fin2, Fin3, Fin4). The Finnish
acoustic models are speaker dependent (SD). Mono-
phones (mon) were used in the earlier experiments
(Fin1, Fin2), but these were later replaced by cross-
context triphones (tri).
The Estonian speech data has been collected from
a large number of speakers and consists of sen-
tences from newspapers as well as names and dig-
its read aloud. The acoustic models are speaker-
independent triphones (SI tri) adapted online using
Cepstral Mean Subtraction and Constrained Maxi-
mum Likelihood Linear Regression. Also the Turk-
ish acoustic training data contains speech from hun-
dreds of speakers. The test set is composed of news-
paper text read by one female speaker. Speaker-
independent triphones are used as acoustic models.
The Finnish, Estonian, and Turkish data sets con-
tain planned speech, i.e., written text read aloud.
By contrast, the Arabic data consists of transcribed
spontaneous telephone conversations,2 which are
characterized by disfluencies and by the presence
of ?non-speech?, such as laugh and cough sounds.
There are multiple speakers in the Arabic data, and
online speaker adaptation has been performed.
3.1.2 Text Data and Language Models
The n-gram language models are trained using
the SRILM toolkit (Stolcke, 2002) (Fin1, Fin2,
Tur1, Tur2, ECA) or similar software developed
at HUT (Siivola and Pellom, 2005) (Fin3, Fin4,
Est). All models utilize the Modified Interpolated
Kneser-Ney smoothing technique (Chen and Good-
man, 1999). The Arabic LM is trained on the
same corpus that is used for acoustic training. This
data set is regrettably small (160 000 words), but it
matches the test set well in style, as it consists of
transcribed spontaneous speech. The LM training
corpora used for the other languages contain fairly
large amounts of mainly news and book texts and
conceivably match the style of the test data well.
In the morph-based models, words are split into
morphs using Morfessor, and statistics are collected
for morph n-grams. As the desired output of the
2LDC CallHome corpus of Egyptian Colloquial Ara-
bic: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC97S45
382
Table 1: Test configurations
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA
Recognizer HUT HUT HUT HUT HUT AT&T AT&T SRI
Speech data
Type of speech read read read read read read read spont.
Training set [kwords] 20 49 49 49 790 230 110 160
Speakers in training set 1 1 1 1 1300 550 250 310
Test set [kwords] 4.3 1.9 1.9 1.9 3.7 7.0 7.0 16
Speakers in test set 1 1 1 1 50 1 1 57
Text data
LM training set [Mwords] 36 36 32 150 53 17 27 0.16
Models
Acoustic models SD mon SD mon SD tri SD tri SI tri SI tri SI tri SI tri
Morph lexicon [kmorphs] 66 66 120 25 37 52 34 6.1
Word lexicon [kwords] 410 410 410 ? 60 120 50 18
Out-of-vocabulary words
OOV LM training set [%] 5.0 5.0 5.9 ? 14 5.3 9.6 0.61
OOV test set [%] 5.0 7.2 7.3 ? 19 5.5 12 9.9
New words in test set [%] 2.7 3.0 3.1 1.5 3.4 1.6 1.5 9.8
speech recognizer is a sequence of words rather than
morphs, the LM explicitly models word breaks as
special symbols occurring in the morph sequence.
For comparison, word n-gram models have been
tested. The vocabulary cannot typically include ev-
ery word form occurring in the training set (because
of the large number of different words), so the most
frequent words are given priority; the actual lexicon
sizes used in each experiment are shown in Table 1.
Any word not contained in the lexicon is replaced by
a special out-of-vocabulary symbol.
As words and morphs are units of different length,
their optimal performance may occur at different or-
ders of the n-gram. The best order of the n-gram
has been optimized on development test sets in the
following cases: Fin1, Fin2, Tur1, ECA (4-grams
for both morphs and words) and Tur2 (5-grams for
morphs, 3-grams for words). The models have ad-
ditionally been pruned using entropy-based pruning
(Tur1, Tur2, ECA) (Stolcke, 1998). In the other
experiments (Fin3, Fin4, Est), no fixed maximum
value of n was selected. n-Gram growing was per-
formed (Siivola and Pellom, 2005), such that those
n-grams that maximize the training set likelihood
are gradually added to the model. The unrestricted
growth of the model is counterbalanced by an MDL-
type complexity term. The highest order of n-grams
accepted was 7 for Finnish and 8 for Estonian.
Note that the optimization procedure is neutral
with respect to morphs vs. words. Roughly the
same number of parameters are allowed in the result-
ing LMs, but typically the morph n-gram LMs are
smaller than the corresponding word n-gram LMs.
3.1.3 Out-of-Vocabulary Words
Table 1 further shows statistics on out-of-
vocabulary rates in the data sets. This is relevant
for the assessment of the word models, as the OOV
rates define the limits of these models.
The OOV rate for the LM training set corresponds
to the proportion of words replaced by the OOV
symbol in the LM training data, i.e., words that were
not included in the recognition vocabulary. The high
OOV rates for Estonian (14 %) and Tur2 (9.6 %) in-
dicate that the word lexicons have poor coverage of
these sets. By contrast, the ECA word lexicon cov-
ers virtually the entire training set vocabulary.
Correspondingly, the test set OOV rate is the pro-
portion of words that occur in the data sets used
for running the speech recognition tests, but that are
missing from the recognition lexicons. This value
is thus the minimum error that can be obtained by
the word models, or put differently, the recognizer
is guaranteed to get at least this proportion of words
wrong. Again, the values are very high for Estonian
(19 %) and Tur2 (12 %), but also for Arabic (9.9 %)
because of the insufficient amount of training data.
Finally, the figures labeled ?new words in test set?
denote the proportion of words in the test set that do
not occur in the LM training set. Thus, these values
indicate the minimum error achievable by any word
model trained on the training sets available.
383
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
76.6
71.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
W
or
d 
ac
cu
ra
cy
 [%
]
 
 
Morphs
Words
Figure 2: Word accuracies for the different speech
recognition test configurations.
3.2 Results and Analysis
The morph-based and word-based results of the con-
ducted speech recognition experiments are shown in
Figure 2 (for Fin4, no comparable word experiment
has been carried out). The evaluation measure used
is word accuracy (WAC): the number of correctly
recognized words minus the number of incorrectly
inserted words divided by the number of words in
the reference transcript. (Another frequently used
measure is the word error rate, WER, which relates
to word accuracy as WER = 100 % ? WAC.)
Figure 2 shows that the morph models perform
better than the word models, with the exception
of the Arabic experiment (ECA), where the word
model outperforms the morph model. The statisti-
cal significance of these differences is confirmed by
one-tailed paired Wilcoxon signed-rank tests at the
significance level of 0.05.
Overall, the best performance is observed for the
Finnish data sets, which is explained by the speaker-
dependent acoustic models and clean noise condi-
tions. The Arabic setup suffers from the insufficient
amount of LM training data.
3.2.1 In-Vocabulary Words
For a further investigation of the outcome of the
experiments, the test sets have been partitioned into
regions based on the types of words they contain.
The recognition output is aligned with the refer-
ence transcript, and the regions aligned with in-
vocabulary (IV) reference words (words contained
in the vocabulary of the word model) are put in
one partition and the remaining words (OOVs) are
put in another partition. Word accuracies are then
computed separately for the two partitions. Inserted
words, i.e., words that are not aligned with any word
in the reference, are put in the IV partition, unless
they are adjacent to an OOV region, in which case
they are put in the OOV partition.
Figure 3a shows word accuracies for the in-
vocabulary words. Without exception, the accuracy
for the IVs is higher than that of the entire test set vo-
cabulary. One could imagine that the word models
would do better than the morph models on the IVs,
since the word models are totally focused on these
words, whereas the morph models reserve modeling
capacity for a much larger set of words. The word
accuracies in Fig. 3a also partly seem to support this
view. However, Wilcoxon signed-rank tests (level
0.05) show that the superiority of the word model is
statistically significant only for Arabic and for Fin3.
With few exceptions, it is thus possible to draw
the conclusion that morph models are capable of
modeling a much larger set of words than word
models without, however, compromising the perfor-
mance on the limited vocabulary covered by the
word models in a statistically significant way.
3.2.2 Out-of-Vocabulary Words
Since the word model and morph model perform
equally well on the subset of words that are included
in the lexicon of the word model, the overall supe-
riority of the morph model needs to come from its
successful coping with out-of-vocabulary words.
In Figure 3b, word accuracies have been plot-
ted for the out-of-vocabulary words contained in the
test set. It is clear that the recognition accuracy for
the OOVs is much lower than the overall accuracy.
Also, negative accuracy values are observed. This
happens when the number of insertions exceeds the
number of correctly recognized units.
In Figure 3b, if speaker-dependent and speaker-
independent setups are considered separately (and
Arabic is left out), there is a tendency for the morph
models to recognize the OOVs more accurately, the
higher the OOV rate is. One could say that a morph
model has a double advantage over a correspond-
ing word model: the larger the proportion of OOVs
384
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA0
10
20
30
40
50
60
70
80
90
100
79.979.781.977.9
92.594.6
73.374.771.872.671.771.9
45.6
48.1
W
or
d 
ac
cu
ra
cy
 fo
r i
n?
vo
ca
bu
la
ry
 w
or
ds
 [%
]
 
 
Morphs
Words
(a)
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
15.1
?74.8
43.2
?62.6
55.1
?76.1
38.0
?47.7
13.2
?21.8
29.2
?19.4
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r O
O
Vs
 [%
]
 
 
Morphs
Words
(b)
Figure 3: Word accuracies computed separately for those words in the test sets that are (a) included in and
(b) excluded from the vocabularies of the word vocabulary; cf. figures listed on the row ?OOV test set? in
Table 1. Together these two partitions make up the entire test set vocabulary. For comparison, the results for
the entire sets are shown using gray-shaded bars (also displayed in Figure 2).
in the word model is, the larger the proportion of
words that the morph model can recognize but the
word model cannot, a priori. In addition, the larger
the proportion of OOVs, the more frequent and more
?easily modelable? words are left out of the word
model, and the more successfully these words are
indeed learned by the morph model.
3.2.3 New Words in the Test Set
All words present in the training data (some of
which are OOVs in the word models) ?leave some
trace? in the morph models, in the n-gram statistics
that are collected for morph sequences. How, then,
about new words that occur only in the test set, but
not in the training set? In order to recognize such
words correctly, the model must combine morphs in
ways it has not observed before.
Figure 4 demonstrates that the new unseen words
are very challenging. Now, also the morph mod-
els mostly obtain negative word accuracies, which
means that the number of insertions adjacent to new
words exceeds the number of correctly recognized
new words. The best results are obtained in clean
acoustic conditions (Fin2, Fin3, Fin4) with only few
foreign names, which are difficult to get right using
typical Finnish phoneme-to-grapheme mappings (as
the negative accuracy of Fin1 suggests).
3.3 Vocabulary Growth and Arabic
Figure 5 shows the development of the size of
the vocabulary (unique word forms) for growing
amounts of text in different corpora. The corpora
used for Finnish, Estonian, and Turkish (planned
speech/text), as well as Arabic (spontaneous speech)
are the LM training sets used in the experiments.
Additional sources have been provided for Arabic
and English: Arabic text (planned) from the FBIS
corpus of Modern Standard Arabic (a collection
of transcribed radio newscasts from various radio
stations in the Arabic speaking world), as well as
text from the New York Times magazine (English
planned) and spontaneous transcribed English tele-
phone conversations from the Fisher corpus.
The figure illustrates two points: (1) The faster
the vocabulary growth is, the larger the potential ad-
vantage of morph models is in comparison to stan-
dard word models, because of OOV and data spar-
sity problems. The obtained speech recognition re-
sults seem to support this hypothesis; the applied
morph LMs are clearly beneficial for Finnish and
Estonian, mostly beneficial for Turkish, and slightly
detrimental for ECA. (2) A more slowly growing
vocabulary is used in spontaneous speech than in
planned speech (or written text). Moreover, the
Arabic ?spontaneous? curve is located fairly close
385
Fin1 Fin2 Fin3 Fin4 Est Tur1 Tur2 ECA?100
?80
?60
?40
?20
0
20
40
60
80
100
76.671.9
79.1
67.8
89.8
82.2
92.8
66.7
51.9
68.667.466.7
61.2
40.141.8
?8.7
?93.0
20.7
?75.9
31.0
?81.0
17.9
?32.3
?64.6
?6.1
?24.6
?5.9
?24.5
?10.1
?14.6
W
or
d 
ac
cu
ra
cy
 fo
r u
ns
ee
n 
wo
rd
s 
[%
]
 
 
Morphs
Words
Figure 4: Word accuracies computed for the words
in the test sets that do not occur at all in the train-
ing sets; cf. figures listed on the row ?new words
in test set? in Table 1. For comparison, the gray-
shaded bars show the corresponding results for the
entire test sets (also displayed in Figure 2).
to the English ?planned? curve and much below
the Finnish, Estonian, and Turkish curves. Thus,
even though Arabic is considered a ?morphologi-
cally rich? language, this is not manifested through
a considerable vocabulary growth (and high OOV
rate) in the Egyptian Colloquial Arabic data used in
the current speech recognition experiments. Conse-
quently, it may not be that surprising that the morph
model did not work particularly well for Arabic.
Arabic words consist of a stem surrounded by pre-
fixes and suffixes, which are fairly successfully seg-
mented out by Morfessor. However, Arabic also
has templatic morphology, i.e., the stem is formed
through the insertion of a vowel pattern into a ?con-
sonantal skeleton?.
Additional experiments have been performed us-
ing the ECA data and Factored Language Models
(FLMs) (Kirchhoff et al, 2006). The FLM is a
powerful model that makes use of several sources
of information, in particular a morphological lexi-
con of ECA. The FLM incorporates mechanisms for
handling templatic morphology, but despite its so-
phistication, it barely outperforms the standard word
model: The word accuracy of the FLM is 42.3 % and
that of the word model is 41.8 %. The speech recog-
nition implementation of both the FLM and the word
0 20 40 60 80 100 120 140 160 1800
5
10
15
20
25
30
35
40
45
50
Corpus size [1000 words]
Un
iq
ue
 w
or
ds
 [1
00
0 w
ord
s]
Fin
nish
 (pla
nned
)
Est
onia
n (pl
anne
d)
Turk
ish (pl
anned
)
Arabic 
(spontan
eous)Ara
bic (plann
ed)
English (plann
ed)
English (spontaneous)
Figure 5: Vocabulary growth curves for the differ-
ent corpora of spontaneous and planned speech (or
written text). For growing amounts of text (word
tokens) the number of unique different word forms
(word types) occurring in the corpus are plotted.
model is based on whole words (although subword
units are used for assigning probabilities to word
forms in the FLM). This contrasts these models with
the morph model, which splits words into subword
units also in the speech recognition implementation.
It seems that the splitting is a source of errors in this
experimental setup with very little data available.
4 Discussion
Alternative morph-based and word-based ap-
proaches exist. We have tried some, but none of
them has outperformed the described morph models
for Finnish, Estonian, and Turkish, or the word and
FLM models for Egyptian Arabic (in a statistically
significant way). The tested models comprise
more linguistically accurate morph segmentations
obtained using later Morfessor versions (Categories-
ML and Categories-MAP) (Creutz, 2006), as well
as analyses obtained from morphological parsers.
Hybrids, i.e., word models augmented with
phonemes or other subword units have been pro-
posed (Bazzi and Glass, 2000; Galescu, 2003;
Bisani and Ney, 2005). In our experiments, such
models have outperformed the standard word mod-
els, but not the morph models.
Simply growing the word vocabulary to cover the
386
entire vocabulary of large training corpora could be
one (fairly ?brute-force?) approach, but this is hardly
feasible for languages such as Finnish. The en-
tire Finnish LM training data of 150 million words
(used in Fin4) contains more than 4 million unique
word forms, a value ten times the size of the rather
large word lexicon currently used. And even if a 4-
million-word lexicon were to be used, the OOV rate
of the test set would still be relatively high: 1.5 %.
Judging by the Arabic experiments, there seems
to be some potential in Factored Language Models.
The FLMs might work well also for the other lan-
guages, and in fact, to do justice to the more ad-
vanced morph models from later versions of Mor-
fessor, FLMs or some other refined techniques may
be necessary as a complement to the currently used
standard n-grams.
Acknowledgments
We are most grateful to Katrin Kirchhoff and Dimitra Vergyri
for their valuable help on issues related to Arabic, and to the EU
AMI training program for funding part of this work. The work
was also partly funded by DARPA under contract No. HR0011-
06-C-0023 (approved for public release, distribution is unlim-
ited). The views herein are those of the authors and do not nec-
essarily reflect the views of the funding agencies.
References
I. Bazzi and J. R. Glass. 2000. Modeling out-of-vocabulary
words for robust speech recognition. In Proc. ICSLP, Bei-
jing, China.
A. Berton, P. Fetter, and P. Regel-Brietzmann. 1996. Com-
pound words in large-vocabulary German speech recognition
systems. In Proc. ICSLP, pp. 1165?1168, Philadelphia, PA,
USA.
M. Bisani and H. Ney. 2005. Open vocabulary speech recog-
nition with flat hybrid models. In Proc. Interspeech, Lisbon,
Portugal.
M. R. Brent. 1999. An efficient, probabilistically sound algo-
rithm for segmentation and word discovery. Machine Learn-
ing, 34:71?105.
W. Byrne, J. Hajic?, P. Ircing, F. Jelinek, S. Khudanpur, P. Kr-
bec, and J. Psutka. 2001. On large vocabulary continuous
speech recognition of highly inflectional language ? Czech.
In Proc. Eurospeech, pp. 487?489, Aalborg, Denmark.
S. F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Computer
Speech and Language, 13:359?394.
M. Creutz and K. Lagus. 2002. Unsupervised discovery of
morphemes. In Proc. ACL SIGPHON, pp. 21?30, Philadel-
phia, PA, USA.
M. Creutz. 2006. Induction of the Morphology of Natural
Language: Unsupervised Morpheme Segmentation with Ap-
plication to Automatic Speech Recognition. Ph.D. thesis,
Helsinki University of Technology. http://lib.tkk.fi/
Diss/2006/isbn9512282119/.
C. G. de Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
L. Galescu. 2003. Recognition of out-of-vocabulary words
with sub-lexical language models. In Proc. Eurospeech, pp.
249?252, Geneva, Switzerland.
P. Geutner, M. Finke, and P. Scheytt. 1998. Adaptive vocabu-
laries for transcribing multilingual broadcast news. In Proc.
ICASSP, pp. 925?928, Seattle, WA, USA.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153?198.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. Coling/ACL, pp. 673?680, Sydney, Australia.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and
J. Pylkko?nen. 2006. Unlimited vocabulary speech recogni-
tion with morph language models applied to Finnish. Com-
puter Speech and Language, 20(4):515?541.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for Ara-
bic speech recognition. Computer Speech and Language,
20(4):589?608.
M. Kurimo, M. Creutz, M. Varjokallio, E. Ar?soy, and
M. Sarac?lar. 2006a. Unsupervised segmentation of words
into morphemes ? Morpho Challenge 2005, Application to
automatic speech recognition. In Proc. Interspeech, Pitts-
burgh, PA, USA.
M. Kurimo, A. Puurula, E. Ar?soy, V. Siivola, T. Hirsima?ki,
J. Pylkko?nen, T. Aluma?e, and M. Sarac?lar. 2006b. Un-
limited vocabulary speech recognition for agglutinative lan-
guages. In Proc. NAACL-HLT, New York, USA.
O.-W. Kwon and J. Park. 2003. Korean large vocabulary con-
tinuous speech recognition with morpheme-based recogni-
tion units. Speech Communication, 39(3?4):287?300.
M. Larson, D. Willett, J. Koehler, and G. Rigoll. 2000. Com-
pound splitting and lexical unit recombination for improved
performance of a speech recognition system for German par-
liamentary speeches. In Proc. ICSLP.
M. Mohri and M. D. Riley. 2002. DCD library, Speech
recognition decoder library. AT&T Labs Research. http:
//www.research.att.com/sw/tools/dcd/.
R. Ordelman, A. van Hessen, and F. de Jong. 2003. Compound
decomposition in Dutch large vocabulary speech recogni-
tion. In Proc. Eurospeech, pp. 225?228, Geneva, Switzer-
land.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15:79?93.
I. Shafran and K. Hall. 2006. Corrective models for speech
recognition of inflected languages. In Proc. EMNLP, Syd-
ney, Australia.
V. Siivola and B. Pellom. 2005. Growing an n-gram model. In
Proc. Interspeech, pp. 1309?1312, Lisbon, Portugal.
A. Stolcke. 1998. Entropy-based pruning of backoff language
models. In Proc. DARPA BNTU Workshop, pp. 270?274,
Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. ICSLP, pp. 901?904. http://www.speech.
sri.com/projects/srilm/.
E. W. D. Whittaker and P. C. Woodland. 2000. Particle-based
language modelling. In Proc. ICSLP, pp. 170?173, Beijing,
China.
S. Young, D. Ollason, V. Valtchev, and P. Woodland. 2002.
The HTK book (for version 3.2 of HTK). University of Cam-
bridge.
387
Proceedings of NAACL HLT 2009: Short Papers, pages 73?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimum Bayes Risk Combination of Translation Hypotheses from
Alternative Morphological Decompositions
Adria` de Gispert? Sami Virpioja?
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
? Helsinki University of Technology. Adaptive Informatics Research Centre
P.O.Box 5400, 02015 TKK, Finland
{sami.virpioja,mikko.kurimo}@tkk.fi
Mikko Kurimo? William Byrne?
Abstract
We describe a simple strategy to achieve trans-
lation performance improvements by combin-
ing output from identical statistical machine
translation systems trained on alternative mor-
phological decompositions of the source lan-
guage. Combination is done by means of Min-
imum Bayes Risk decoding over a shared N-
best list. When translating into English from
two highly inflected languages such as Ara-
bic and Finnish we obtain significant improve-
ments over simply selecting the best morpho-
logical decomposition.
1 Introduction
Morphologically rich languages pose significant
challenges for natural language processing. The ex-
tensive use of inflection, derivation, and composi-
tion leads to a huge vocabulary, and sparsity in mod-
els estimated from data. Statistical machine transla-
tion (SMT) systems estimated from parallel text are
affected by this. This is particularly acute when ei-
ther the source or the target language, or both, are
morphologically complex.
Owing to these difficulties and to the natural in-
terest researchers take in complex linguistic phe-
nomena, many approaches to morphological anal-
ysis have been developed and evaluated. We fo-
cus on applications to SMT in Section 1.1, but we
note the recent general survey (Roark and Sproat,
2007) and the Morpho Challenge competitive evalu-
ations1. Prior evaluations of morphological analyz-
ers have focused on determining which analyzer was
1See http://www.cis.hut.fi/morphochallenge2009/ and links
best suited for some particular task. For translation,
we take a different approach and investigate whether
competing analyzers might have complementary in-
formation. Our method is straightforward. We train
two identical SMT systems with two versions of
the same parallel corpus, each with a different mor-
phological decomposition of the source language.
We combine their translation hypotheses perform-
ing Minimum Bayes Risk decoding over merged N-
best lists. Results are reported in the NIST 2008
Arabic-to-English MT task and an European Parlia-
ment Finnish-to-English task, with significant gains
over each individual system.
1.1 Prior Work
Several earlier works investigate word segmenta-
tion and transformation schemes, which may include
Part-Of-Speech or other information, to alleviate
the effect of morphological variation on translation
models. With different training corpus sizes, they
focus on translation into English from Arabic (Lee,
2004; Habash and Sadat, 2006; Zollmann et al,
2006), Czech (Goldwater and McClosky, 2005; Tal-
bot and Osborne, 2006), German (Nie?en and Ney,
2004) or Catalan, Spanish and Serbian (Popovic
and Ney, 2004). Some address the generation
challenge when translating from English into Span-
ish (Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). Unsupervised morphology learning is pro-
posed as a language-independent solution to reduce
the problems of rich morphology in (Virpioja et al,
there to earlier workshops. The combination scheme described
in this paper will be one of the evaluation tracks in the upcoming
workshop.
73
Arabic wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn
MADA D2 w+ qrrt >n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn
SAKHR w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn
English a preparatory committee of the whole of the general assembly is to be established at its fifty-second session
Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration.
2007). Factored models are introduced in (Koehn
and Hoang, 2007) for better integration of morpho-
syntactic information.
Gime?nez and Ma`rquez (2005) merge mul-
tiple word alignments obtained from several
linguistically-tagged versions of a Spanish-English
corpus, but only standard tokens are used in decod-
ing. Dyer et al (2008) report improvements from
multiple Arabic segmentations in translation to En-
glish translation, but their goal was to demonstrate
the value of lattice-based translation. From a model-
ing perspective their approach is unwieldy: multiple
analyses of the parallel text collections are merged
to create a large, heterogeneous training set; a sin-
gle set of models and alignments is produced; lattice
translation is then performed using a single system
to translate all morphological analyses. We find that
similar gains can be obtained much more easily.
The approach we take is Minimum Bayes Risk
(MBR) System Combination (Sim et al, 2007). N-
best lists from multiple SMT systems are merged;
the posterior distributions over the individual lists
are interpolated to form a new distribution over the
merged list. MBR hypotheses selection is then per-
formed using sentence-level BLEU score (Kumar
and Byrne, 2004). It is very likely that even greater
gains can be achieved by more complicated combi-
nation schemes (Rosti et al, 2007), although signif-
icantly more effort in tuning would be required.
2 Arabic-to-English Translation
For Arabic-to-English translation, we consider two
alternative segmentations of the Arabic words. We
first use the MADA toolkit (Habash and Rambow,
2005). After tagging, we split word prefixes and suf-
fixes according to scheme ?D2? (Habash and Sadat,
2006). Secondly, we take the segmentation gener-
ated by Sakhr Software in Egypt using their Arabic
Morphological Tagger, as an alternative segmenta-
tion into subword units. This scheme generates more
tokens as it segments all Arabic articles which other-
wise remain attached in the MADA D2 scheme (Ta-
ble 1).
Translation experiments are based on the NIST
MT08 Arabic-to-English translation task, includ-
ing all allowed parallel data as training material
(?150M English words, and 153M or 178M Arabic
words for MADA-segmented and Sakhr-segmented
text, respectively). In addition to the MT08 set itself,
we take the NIST MT02 through MT05 evaluation
sets and divide them into a development set (odd-
numbered sentences) and a test set (even-numbered
sentences), each containing ?2k sentences.
The SMT system used is HiFST, a hierarchical
phrase-based system implemented with Weighted
Finite-State Transducers (Iglesias et al, 2009). Two
identical systems are trained from each parallel cor-
pus, i.e. MADA-based and SAKHR-based. Both
systems use the same standard features and share
the first-pass English language model, a 4-gram es-
timated over the parallel text and a 965 million word
subset of monolingual data from the English Giga-
word Third Edition. Minimum Error Training pa-
rameter estimation under IBM BLEU is performed
on the development set (mt02-05-tune), and the out-
put translation lattice is rescored with large language
models estimated using ?4.7B words of English
newswire text, in the same fashion as (Iglesias et
al., 2009). Finally, the first 1000-best hypotheses
are rescored with MBR, taking the negative sentence
level BLEU score as the loss function to minimise.
For system combination, we obtain two sets of N-
best lists of depth N=500, one from each system.
Both lists are obtained after large-LM lattice rescor-
ing, i.e. prior to individual MBR. A joint MBR de-
coding is then carried out on the aggregated 1000-
best list with equal weight assigned to the posterior
distribution assigned to the hypotheses by each sys-
tem. Results are shown in Table 2.
As shown, the scores obtained via MBR combi-
nation outperform significantly those achieved via
MBR for the best-performing system (MADA). The
74
mt02-05-
-tune -test mt08
MADA-based 53.3 52.7 43.7
+MBR 53.7 53.3 44.0
SAKHR-based 52.7 52.8 43.3
+MBR 53.2 53.2 43.8
MBR-combined 54.6 54.6 45.6
Table 2: Arabic-to-English translation results. Lower-
cased IBM BLEU reported.
mixed case BLEU-4 for the MBR-combined system
on mt08 is 44.1. This is directly comparable to the
official MT08 Constrained Training Track evalua-
tion results.2
3 Finnish-to-English Translation
Finnish is a highly-inflecting, agglutinative lan-
guage. It has dozens of both inflectional and
derivational suffixes, that are concatenated together
with only moderately small changes in the sur-
face forms. For instance, one can inflect the
word ?kauppa? (shop) into ?kaupa+ssa+mme+kin?
(also in our shop) by glueing the suffixes to the
end. In addition, Finnish has many compound
words, sometimes consisting of several parts, such
as ?ulko+maa+n+kauppa+politiikka? (foreign trade
policy). Due to these properties, the number of dif-
ferent word forms that can be observed is enormous.
Morfessor (Creutz and Lagus, 2007) is a method
for modeling concatenative morphology in an un-
supervised manner. It tries to find morpheme-like
units, morphs, that are segments of the words. In-
spired by the minimum description length principle,
Morfessor tries to find a concise lexicon of morphs
that can effectively code the words in the train-
ing data. Unlike other unsupervised methods (e.g.,
Goldsmith (2001)), there is no restrictions on how
many morphs a word can have. After training the
model, the most likely segmentation of new words
to morphs can be found using the Viterbi algorithm.
There exist a few different versions of Morfessor.
The baseline algorithm has been found to be very
useful in automatic speech recognition of agglutina-
tive languages (Kurimo et al, 2006). However, it
2Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html
often oversegments morphemes that are rare or not
seen at all in the training data. Following the ap-
proach in (Virpioja et al, 2007), we use the Morfes-
sor Categories-MAP algorithm (Creutz and Lagus,
2005). It applies a hierarchical model with three sur-
face categories (prefix, stem and suffix), that allow
the algorithm to treat out-of-vocabulary words in a
convenient manner. For instance, if we encounter a
new name with a known suffix, it can usually sepa-
rate the suffix and leave the actual name intact.
Similarly to the Arabic-to-English task, we train
two identical HiFST systems. In this case, whereas
one is trained on Finnish morphs decomposed by
Morfessor (morph-based), the other is trained on
standard, unprocessed Finnish (word-based). For
this task we use the EuParl parallel corpus . Portions
from Q4/2000 was reserved for testing and Septem-
ber 2000 for development, both containing around
3,000 sentences. The training data comprised 23M
English words, and 17M or 27M Finnish tokens for
word-based or morph-based text, respectively.
The training set was also used to train the mor-
phological segmentation. The quality of the seg-
mentation is evaluated in (Virpioja et al, 2007). A
precision of 78.72% and recall of 52.29% was mea-
sured for the segmentation boundaries with respect
to a linguistic reference segmentation. As the recall
is not very high, the segmentation is more conserva-
tive than the linguistic reference. Table 4 shows an
example for a phrase in the training data.
Results are shown in Table 3, where again signifi-
cant gains are achieved when simply combining out-
put N-best lists via MBR. Only one reference was
available for scoring. In this case we did not ap-
ply large-LM rescoring, as no large additional par-
liamentary data was available. Individual MBR did
not yield gains for each of the systems.
devel test
Word-based 30.2 27.9
Morph-based 29.4 27.4
MBR-combined 30.5 28.9
Table 3: Finnish-to-English translation results. Lower-
cased IBM BLEU reported.
75
Finnish vaarallisten aineiden kuljetusten turvallisuusneuvonantaja
Morfessor vaaraSTM llistenSTM aineSTM idenSUF kuljetusPRE tenSTM turvallisuusPRE neuvoSTM nSUF antajaSTM
Linguistic vaara llis t en aine i den kuljet us t en turva llis uus neuvo n anta ja
English safety adviser for the transport of dangerous goods
Table 4: Example of Morfessor Categories-MAP segmentation and linguistic segmentation for a Finnish phrase. Sub-
scripts show the morph categories given by Morfessor: stem (STM), prefix (PRE) and suffix (SUF).
4 Conclusions
We demonstrated that multiple morphological anal-
yses can be the basis for SMT system combination.
These results will be of interest to researchers devel-
oping morphological analyzers, as it provides a new,
and potentially profitable way to evaluate compet-
ing analysers. The results should also interest SMT
researchers. SMT system combination is an active
area of research, but good gains from combination
usually require very different system architectures;
this can be a barrier to developing competitive sys-
tems. We find that the same architecture trained on
two different analyses is adequate to generate the di-
verse hypotheses needed for system combination.
Acknowledgments. This work was supported by the GALE
program of DARPA (HR0011-06-C-0022), the GSLT and AIRC
in the Academy of Finland, and the EMIME project and PAS-
CAL2 NoE in the EC?s FP7.
References
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Conf. on Adaptive Knowledge Representation
and Reasoning (AKRR).
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learning.
ACM Trans. Speech and Language Processing, 4(1).
A. de Gispert and J.B. Marin?o. 2008. On the impact
of morphology in English to Spanish statistical MT.
Speech Communication, 50.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In ACL-HLT.
J. Gime?nez and Ll. Ma`rquez. 2005. Combining linguis-
tic data views for phrase-based SMT. In ACL Work-
shop on Building and Using Parallel Texts.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
S. Goldwater and D. McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In HLT-
EMNLP.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-
NAACL: Short Papers.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In HLT-NAACL.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In EMNLP.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
M. Kurimo, A. Puurula, E. Arisoy, V. Siivola, T. Hir-
sima?ki, J. Pylkko?nen, T. Aluma?e, and M. Saraclar.
2006. Unlimited vocabulary speech recognition for
agglutinative languages. In HLT-NAACL.
Y.-S. Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL: Short Papers.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2).
M. Popovic and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In LREC.
B. Roark and R. Sproat. 2007. Computational Ap-
proaches to Morphology and Syntax. Oxford Univer-
sity Press.
A.V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In ACL.
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. C. Wood-
land. 2007. Consensus network decoding for sta-
tistical machine translation system combination. In
ICASSP, volume 4.
D. Talbot and M. Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In ACL.
N. Ueffing and H. Ney. 2003. Using POS information for
SMT into morphologically rich languages. In EACL.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. In MT Summit XI.
A. Zollmann, A. Venugopal, and S. Vogel. 2006. Bridg-
ing the inflection morphology gap for Arabic statistical
machine translation. In HLT-NAACL: Short Papers.
76
Proceedings of NAACL HLT 2009: Short Papers, pages 193?196,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition
Teemu Hirsima?ki and Mikko Kurimo
Adaptive Informatics Research Centre
Helsinki University of Technology
P.O. Box 5400, 02015, TKK, Finland
teemu.hirsimaki@tkk.fi
Abstract
We analyze the recognition errors made by
a morph-based continuous speech recognition
system, which practically allows an unlim-
ited vocabulary. Examining the role of the
acoustic and language models in erroneous
regions shows how speaker adaptive training
(SAT) and discriminative training with mini-
mum phone frame error (MPFE) criterion de-
crease errors in different error classes. An-
alyzing the errors with respect to word fre-
quencies and manually classified error types
reveals the most potential areas for improving
the system.
1 Introduction
Large vocabulary speech recognizers have become
very complex. Understanding how the parts of the
system affect the results separately or together is far
from trivial. Still, analyzing the recognition errors
may suggest how to reduce the errors further.
There exist previous work on analyzing recogni-
tion errors. Chase (1997) developed error region
analysis (ERA), which reveals whether the errors
are due to acoustic or language models. Greenberg
et al (2000) analyzed errors made by eight recog-
nition systems on the Switchboard corpus. The er-
rors correlated with the phone misclassification and
speech rate, and conclusion was that the acoustic
front ends should be improved further. Duta et al
(2006) analyzed the main errors made by the 2004
BBN speech recognition system. They showed that
errors typically occur in clusters and differ between
broadcast news (BN) and conversational telephone
speech (CTS) domains. Named entities were a com-
mon cause for errors in the BN domain, and hesita-
tion, repeats and partially spoken words in the CTS
domain.
This paper analyzes the errors made by a Finnish
morph-based continuous recognition system (Hir-
sima?ki et al, 2009). In addition to partitioning the
errors using ERA, we compare the number of let-
ter errors in different regions and analyze what kind
of errors are corrected when speaker adaptive train-
ing and discriminative training are taken in use. The
most potential error sources are also studied by par-
titioning the errors according to manual error classes
and word frequencies.
2 Data and Recognition System
The language model training data used in the experi-
ments consist of 150 million words from the Finnish
Kielipankki corpus. Before training the n-gram
models, the words of the training data were split
into morphs using the Morfessor algorithm, which
has been shown to improve Finnish speech recogni-
tion (Hirsima?ki et al, 2006). The resulting morph
lexicon contains 50 000 distinct morphs. A growing
algorithm (Siivola et al, 2007) was used for training
a Kneser-Ney smoothed high-order variable-length
n-gram model containing 52 million n-grams.
The acoustic phoneme models were trained on the
Finnish SpeechDat telephone speech database: 39
hours from 3838 speakers for training, 46 minutes
from 79 speakers for development and another simi-
lar set for evaluation. Only full sentences were used
and sentences with severe noise or mispronuncia-
tions were removed.
193
AM score
LM score
LM score
AM score
Hyp.
Ref. tiedon
tiedon valta
valta tie
tien
mullista
mullista
a
a
#
#
#
#
?423
?127
?10.8
?6.62
?136
?39.7
?114
?33.0
?15.3
?0.01
?269
?181
?36.5
?18.7
?36.5
?18.7
?242
?203
?11.1
?1.55
?133
?12.9
?136
?39.7
?10.8
?6.62
?423
?127
AM: ?398.3  LM: ?214.01  TOT: ?612.31
AM: ?386.1  LM: ?217.45  TOT: ?603.55
Figure 1: An example of a HYP-AM error region. The
scores are log probabilities. Word boundaries are denoted
by ?#?. The error region only contains one letter error (an
inserted ?n?).
The acoustic front-end consist of 39-dimensional
feature vectors (Mel-frequency cepstral coefficients
with first and second time-derivatives), global max-
imum likelihood linear transform, decision-tree tied
HMM triphones with Gaussian mixture models, and
cepstral mean subtraction.
Three models are trained: The first one is a max-
imum likelihood (ML) model without any adap-
tation. The second model (ML+SAT) enhances
the ML model with three iterations of speaker
adaptive training (SAT) using constrained maxi-
mum likelihood linear regression (CMLLR) (Gales,
1998). In recognition, unsupervised adaptation
is applied in the second pass. The third model
(ML+SAT+MPFE) adds four iterations of discrim-
inative training with minimum phone frame error
(MPFE) criterion (Zheng and Stolcke, 2005) to the
ML+SAT model.
3 Analysis
3.1 Error Region Analysis
Error Region Analysis (Chase, 1997) can be used
to find out whether the language model (LM), the
acoustic model (AM) or both can be blamed for
an erroneous region in the recognition output. Fig-
ure 1 illustrates the procedure. For each utter-
ance, the final hypothesis is compared to the forced
alignment of the reference transcript and segmented
into correct and error regions. An error region is
a contiguous sequence of morphs that differ from
the corresponding reference morphs with respect to
morph identity, boundary time-stamps, AM score,
Letter errors
Region ML ML+SAT ML+SAT+MPFE
HYP-BOTH 962 909 783
HYP-AM 1059 709 727
HYP-LM 623 597 425
REF-TOT 82 60 15
Total 2726 2275 1950
LER (%) 6.8 5.6 4.8
Table 1: SpeechDat: Letter errors for different training
methods and error regions. The reference transcript con-
tains 40355 letters in total.
LM score, or n-gram history1.
By comparing the AM and LM scores in the hy-
pothesis and reference regions, the regions can be
divided in classes. We denote the recognition hy-
pothesis as HYP, and the reference transcript as REF.
The relevant classes for the analysis are the follow-
ing. REF-TOT: the reference would have better to-
tal score, but it has been erroneously pruned. HYP-
AM: the hypothesis has better score, but only AM
favors HYP over REF. HYP-LM: the hypothesis has
better score, but only LM favors HYP over REF.
HYP-BOTH: both the AM and LM favor HYP.
Since the error regions are independent, the let-
ter error rate2 (LER) can be computed separately for
each region. Table 1 shows the error rates for three
different acoustic models: ML training, ML+SAT,
andML+SAT+MPFE.We see that SAT decreases all
error types, but the biggest reduction is in the HYP-
AM class. This should be expected. In the ML case,
the Gaussian mixtures contain much variance due to
different unnormalized speakers, and since the test
set contains only unseen speakers, many errors are
expected for some speakers. Adapting the models to
the test set is expected to increase the acoustic score
of the reference transcript, and since in the HYP-AM
regions the LM already prefers REF, corrections be-
cause of SAT are most probable there.
On the other hand, adding MPFE after SAT seems
1A region may be defined as an error region even if the tran-
scription is correct (only the segmentation differs). However,
since we are going to analyze the number of letter errors in the
error regions, the ?correct? error regions do not matter.
2The words in Finnish are often long and consist of several
morphs, so the performance is measured in letter errors instead
of word errors to have finer resolution for the results.
194
Letter errors
Class label Total HYP-BOTH HYP-AM HYP-LM REF-TOT Class description
Foreign 156 89 61 6 Foreign proper name
Inflect 143 74 26 43 Small error in inflection
Poor 131 37 84 10 Poor pronunciation or repair
Noise 124 21 97 6 Error segment contains some noise
Name 81 29 29 23 Finnish proper name
Delete 65 29 9 27 Small word missing
Acronym 53 44 6 3 Acronym
Compound 42 11 8 23 Word boundary missing or inserted
Correct 37 15 19 3 Hypothesis can be considered correct
Rare 27 11 3 13 Reference contains a very rare word
Insert 9 3 6 Small word inserted incorrectly
Other 1082 421 379 277 5 Other error
Table 2: Manual error classes and the number of letter errors for the ML+SAT+MPFE system.
to reduce HYP-BOTH and HYP-LM errors, but not
HYP-AM errors. The number of search errors (REF-
TOT) also decreases.
All in all, for all models, there seems to be more
HYP-AM errors than HYP-LM errors. Chase (1997)
lists the following possible reasons for the HYP-
AM regions: noise, speaker pronounces badly, pro-
nunciation model is poor, some phoneme models
not trained to discriminate, or reference is plainly
wrong. The next section studies these issues further.
3.2 Manual Error Classification
Next, the letter errors in the error regions were
manually classified according to the most probable
cause. Table 2 shows the classes, the total number
of letter errors for each class, and the errors divided
to different error region types.
All errors that did not seem to have an obvious
cause are put under the class Other. Some of the er-
rors were a bit surprising, since the quality of the
audio and language seemed perfectly normal, but
still the recognizer got the sentences wrong. On the
other hand, the class also contains regions where the
speech is very fast or the signal level is quite low.
The largest class with a specific cause is Foreign,
which contains about 8 % of all letter errors. Cur-
rently, the morph based recognizer does not have
any foreign pronunciation modeling, so it is natural
that words like Ching, Yem Yung, Villeneuve, Schu-
macher, Direct TV, Thunderbayssa are not recog-
nized correctly, since the mapping between the writ-
ten form and pronunciation does not follow the nor-
mal Finnish convention. In Table 2 we see, that the
acoustic model prefers the incorrect hypothesis in al-
most all cases. A better pronunciation model would
be essential to improve the recognition. However,
integrating exceptions in pronunciation to morph-
based recognition is not completely straightforward.
Another difficulty with foreign names is that they
are often rare words, so they will get low language
model probability anyway.
The errors in the Acronym class are pretty much
similar to foreign names. Since the letter-by-letter
pronunciation is not modelled, the acronyms usually
cause errors.
The next largest class is Inflect, which contains
errors where the root of the word is correctly rec-
ognized, but the inflectional form is slightly wrong
(for example: autolla/autolle, kirjeeksi/kirjeiksi). In
these errors, it is usually the language model that
prefers the erroneous hypothesis.
The most difficult classes to improve are perhaps
Poor and Noise. For bad pronunciations and repairs
it is not even clear what the correct answer should
be. Should it be the word the speaker tried to say,
or the word that was actually said? As expected, the
language model would have preferred the correct hy-
pothesis in most cases, but the acoustic model have
chosen the wrong hypothesis.
The Name and Rare are also difficult classes.
Contrary to the foreign names and acronyms, the
pronunciation model is not a problem.
195
05000
10000
Le
tte
rs
 in
 re
fe
re
nc
e
0
200
400
Le
tte
r e
rro
rs
0?1 1?3 3?7 7?15  ?31  ?63  ?127  ?255  ?511 ?4116 New
0
5
10
15
Le
tte
r e
rro
r r
at
e 
(%
)
Subset of training data vocabulary (x 1000)
Figure 2: Frequency analysis of the SAT+MPFE system.
Number of letters in reference (top), number of letter er-
rors (middle), and letter error rate (bottom) partitioned
according to word frequencies. The leftmost bar corre-
sponds to the 1000 most frequent words, the next bar to
the 2000 next frequent words, and so on. The rightmost
bar corresponds to words not present in the training data.
The Compound errors are mainly in HYP-LM re-
gions, which is natural since there is usually lit-
tle acoustic evidence at the word boundary. Fur-
thermore, it is sometimes difficult even for humans
to know if two words are written together or not.
Sometimes the recognizer made a compound word
error because the compound word was often written
incorrectly in the language model training data.
3.3 Frequency Analysis
In order to study the effect of rare words in more de-
tail, the words in the test data were grouped accord-
ing their frequencies in the LM training data: The
first group contained all the words that were among
the 1000 most common words, the next group con-
tained the next 2000 words, then 4000, and so on,
until the final group contained all words not present
in the training data.
Figure 2 shows the number of letters in the ref-
erence (top), number of letter errors (middle), and
letter error rate (bottom) for each group. Quite ex-
pectedly, the error rates (bottom) rise steadily for the
infrequent words and is highest for the new words
that were not seen in the training data. But looking
at the absolute number of letter errors (middle), the
majority occur in the 1000 most frequent words.
4 Conclusions
SAT and MPFE training seem to correct different
error regions: SAT helps when the acoustic model
dominates and MPFE elsewhere. The manual error
classification suggests that improving the pronunci-
ation modeling of foreign words and acronyms is a
potential area for improvement. The frequency anal-
ysis shows that a major part of the recognition errors
occur still in the 1000 most common words. One
solution might be to develop methods for detecting
when the problem is in acoustics and to trust the lan-
guage model more in these regions.
Acknowledgments
This work was partly funded from the EC?s FP7
project EMIME (213845).
References
Lin Chase. 1997. Error-Responsive Feedback Mecha-
nisms for Speech Recognizers. Ph.D. thesis, Robotics
Institute, Carnegie Mellon University.
Nicolae Duta, Richard Schwartz, and John Makhoul.
2006. Analysis of the errors produced by the 2004
BBN speech recognition system in the DARPA EARS
evaluations. IEEE Trans. Audio, Speech Lang. Pro-
cess., 14(5):1745?1753.
M. J. F. Gales. 1998. Maximum likelihood linear trans-
formations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
Steven Greenberg, Shuangyu Chang, and Joy Hollen-
back. 2000. An introduction to the diagnostic eval-
uation of the Switchboard-corpus automatic speech
recognition systems. In Proc. NIST Speech Transcrip-
tion Workshop.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen. 2006.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language, 20(4):515?541.
Teemu Hirsima?ki, Janne Pylkko?nen, and Mikko Kurimo.
2009. Importance of high-order n-gram models in
morph-based speech recognition. IEEE Trans. Audio,
Speech Lang. Process., 17(4):724?732.
Vesa Siivola, Teemu Hirsima?ki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Trans. Audio, Speech Lang. Pro-
cess., 15(5):1617?1624.
Jing Zheng and Andreas Stolcke. 2005. Improved dis-
criminative training using phone lattices. In Proc. In-
terspeech, pages 2125?2128.
196
Proceedings of NAACL HLT 2009: Demonstrations, pages 13?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Morpho Challenge - Evaluation of algorithms for unsupervised learning of
morphology in various task s and languages
Mik k o K urimo, S ami V irpioja, V ille T urunen, T eemu H irsima?k i
Adaptive Informatics Research Centre
H elsink i U niversity of T echnolog y
F I- 0 2 0 15 , T K K , F inland
Firstname.Lastname@tkk.fi
A b strac t
After the release of the open sou rce softw are
implementation of M orfessor alg orithm, a se-
ries of several open evalu ations has b een or-
g aniz ed for u nsu pervised morpheme analy -
sis and morpheme-b ased speech recog nition
and information retrieval. T he u nsu pervised
morpheme analy sis is a particu larly attrac-
tive approach for speech and lang u ag e tech-
nolog y for the morpholog ically complex lan-
g u ag es. W hen the amou nt of distinct w ord
forms b ecomes prohib itive for the constru c-
tion of a su ffi cient lex icon, it is important
that the w ords can b e seg mented into smaller
meaning fu l lang u ag e modeling u nits. In this
presentation w e w ill demonstrate the resu lts
of the evalu ations, the b aseline sy stems b u ilt
u sing the open sou rce tools, and invite re-
search g rou ps to participate in the nex t eval-
u ation w here the task is to enhance statistical
machine translation b y morpheme analy sis.
A proposal for a T y pe I I D emo
1 Ex tended A b strac t
1 .1 T he segmentation of w ords into
morphemes
O ne of the fu ndamental task s in natu ral lang u ag e
processing applications, su ch as larg e-vocab u lary
speech recog nition (L V CS R), statistical machine
translation (S M T ) and information retrieval (IR),
is the morpholog ical analy sis of w ords. It is par-
ticu larly important for the morpholog ically com-
plex lang u ag es, w here the amou nt of different
w ord forms is su b stantially increased b y infl ection,
derivation and composition. T he decomposition of
w ords is req u ired not only for u nderstanding the sen-
tence, b u t in many lang u ag es also for ju st represent-
ing the lang u ag e b y any tractab le and trainab le sta-
tistical model and lex icon. T he manu ally composed
ru le-b ased morpholog ical analy z ers can solve these
prob lems to some ex tent, b u t only a fraction of the
ex isting lang u ag es have b een covered so far, and for
many the coverag e of the relevant content is insu ffi -
cient.
T he ob jective of the M orpho Challeng e1 is to de-
sig n and evalu ate new u nsu pervised statistical ma-
chine learning alg orithms that discover w hich mor-
phemes (smallest individu ally meaning fu l u nits of
lang u ag e) w ords consist of. T he g oal is to discover
b asic vocab u lary u nits su itab le for different task s,
su ch as L V CS R, S M T and IR. In u nsu pervised learn-
ing the list of morphemes is not pre-specifi ed for
each lang u ag e, b u t the optimal morpheme lex icon
and morpheme analy sis of all different w ord forms
is statistically optimiz ed from a larg e tex t corpu s in
a completely data-driven manner.
T he evalu ation of the morpheme analy sis alg o-
rithms is performed b oth b y a ling u istic and an ap-
plication oriented task . T he analy sis ob tained for
a long list of w ords is fi rst compared to the lin-
g u istic g old standard representing a g rammatically
correct analy sis b y verify ing that the morpheme-
sharing w ord pairs are the correct ones (K u rimo et
al., 2 0 0 7 ) . T his is repeated in different lang u ag es
and then the ob tained decomposition of w ords is
applied in state-of-the-art sy stems ru nning variou s
1S ee http://w w w .cis.hu t.fi /morphochalleng e2 0 0 9 /
13
NLP applications. The suitability of the morphemes
is v erifi ed by comparing the performance of the sys-
tems to each other and to systems using unprocessed
w ord s or conv entional w ord processing alg orithms
lik e stemming or rule-based d ecompositions.
A s a baseline method in all application, w e hav e
built systems by applying the M orfessor alg orithm,
w hich is an unsuperv ised w ord d ecomposition alg o-
rithm d ev eloped at our research g roup (C reutz and
Lag us, 20 0 2) and released as open source softw are
implementation2.
1.2 Morphemes in Information Retrieval
In information retriev al ( I R ) from tex t d ocuments a
typical task is to look for the most relev ant d ocu-
ments for a g iv en q uery. O ne of the k ey challeng es
is to red uce all the infl ected w ord forms to a common
root or stem for effectiv e ind ex ing . F rom the mor-
pheme analysis point of v iew this task is to d ecom-
pose all the w ord s in the q uery and tex t d ocuments
and fi nd out those common morphemes w hich form
the most relev ant link s.
In M orpho C halleng e the IR systems built using
the unsuperv ised morpheme analysis alg orithms are
compared in state-of-the-art C LE F task s in F innish,
G erman and E ng lish (K urimo and Turunen, 20 0 8 )
using the mean av erag e precision metric. The results
are also compared to those obtained by the g rammat-
ical morphemes as w ell as the stemming and w ord
normaliz ation method s conv entionally used in IR .
1.3 Morphemes in S peec h Rec og nition
In larg e-v ocabulary continuous speech recog nition
(LV C S R ) one k ey part of the process is the statis-
tical lang uag e mod eling w hich d etermines the prior
probabilities of all the possible w ord seq uences. A n
especially challeng ing task is to cov er all the pos-
sible w ord forms w ith suffi cient accuracy, because
any out-of-v ocabulary w ord s w ill not only be nev er
correctly recog niz ed , but also sev erely d eg rad e the
mod eling of the other nearby w ord s. B y d ecompos-
ing the w ord s into meaning ful sub-w ord units, such
as morphemes, larg e-v ocabulary lang uag e mod els
can be successfully built ev en for the most d iffi cult
ag g lutinativ e lang uag es, lik e F innish, E stonian and
Turk ish (K urimo et al, 20 0 6 b).
2S ee http://w w w .cis.hut.fi /projects/morpho/
In M orpho C halleng e the unsuperv ised mor-
pheme alg orithms hav e been compared by using
the morphemes to train statistical lang uag e mod els
and applying the mod els in state-of-the-art LV C S R
task s in F innish and Turk ish (K urimo et al, 20 0 6 a) .
B enchmark s for the same task s w ere obtained by
mod els that utiliz e the g rammatical morphemes as
w ell as trad itional w ord -based lang uag e mod els.
1.4 Morphemes in Mac hine T ranslation
The state-of-the-art statistical machine translation
(S M T) systems are affected by the morpholog ical
v ariation of w ord s at tw o d ifferent stag es (V irpi-
oja et al, 20 0 7 ) . In the fi rst stag e, the alig nment
of the source and targ et lang uag e w ord s in a par-
allel training corpus and the training of the transla-
tion mod el can benefi t from the d ecomposition of
complex w ord s into morphemes. This is particularly
important w hen either the targ et or the source lan-
g uag e, or both, are morpholog ically complex . The
fi nal stag e w here the targ et lang uag e tex t is g ener-
ated , may also req uire morpheme-based mod els, be-
cause the larg e-v ocabulary statistical lang uag e mod -
els are applied in the same w ay as in LV C S R .
In the on-g oing M orpho C halleng e 20 0 9 compe-
tition, the morpheme analysis alg orithms are com-
pared in S M T task s, w here the analysis is need ed
for the source lang uag e tex ts. The E uropean Par-
liament parallel corpus (K oehn, 20 0 5 ) is used in
the ev aluation. The source lang uag es are F innish
and G erman and the targ et in both task s is E ng lish.
To obtain a state-of-the-art performance in the task s
the morpheme-based S M T w ill be combined w ith a
w ord -based S M T using the M inimum B ayes R isk
( M B R ) interpolation of the N-best translation hy-
pothesis of both systems (d e G ispert et al, 20 0 9 ) .
1.5 Morpho C halleng e 20 0 9
A s its pred ecessors, the M orpho C halleng e 20 0 9
competition is open to all and free of charg e. The
participants? are ex pected to use their unsuperv ised
machine learning alg orithms to analyz e the w ord
lists of d ifferent lang uag es prov id ed by the org aniz -
ers and submit the results of their morpheme analy-
sis. The org aniz ers w ill then run the ling uistic ev al-
uations and build the IR and S M T systems and pro-
v id e all the results and comparisons of the d ifferent
systems. The participated alg orithms and ev aluation
14
results will be presented at the Morpho Challenge
work shop that is c urrently planned to tak e plac e
within the H L T - N A A CL 2 0 1 0 c onferenc e.
Acknowledgments
T he Morpho Challenge c om petitions and work shops
are part of the E U N etwork of E x c ellenc e P A S CA L
Challenge program and organiz ed in c ollaboration
with CL E F . W e are grateful to Mathias Creutz , E bru
A risoy , S tefan B ordag, N iz ar H abash and Majdi
S awalha for c ontributions in proc essing the training
data and c reating the gold standards. T he A c adem y
of F inland has supported the work in the projec ts
Adaptive Informatics and N ew adaptive and learn-
ing meth ods in speech recog nition.
R efer ences
M. Creutz and K . L agus. 2 0 0 2 . U nsuperv ised disc ov ery
of m orphem es. In W ork sh op on M orph olog ical and
P h onolog ical L earning of AC L - 0 2 .
A . de G ispert, S . V irpioja, M. K urim o, and W . B y rne.
2 0 0 9 . Minim um B ay es risk c om bination of translation
hy potheses from alternativ e m orphologic al dec om po-
sitions. S ubm itted to H L T - N AAC L .
P . K oehn. 2 0 0 5 . E uroparl: A parallel c orpus for statisti-
c al m ac hine translation. In M T S u mmit X .
M. K urim o and V . T urunen. 2 0 0 8 . U nsuperv ised m or-
phem e analy sis ev aluation by IR ex perim ents ? Mor-
pho Challenge 2 0 0 8 . In C L E F .
M. K urim o, M. Creutz , M. V arjok allio, E . A risoy , and M.
S arac lar. 2 0 0 6 a. U nsuperv ised segm entation of words
into m orphem es - Challenge 2 0 0 5 , an introduc tion and
ev aluation report. In P AS C AL C h alleng e W ork sh op on
U nsu pervised seg mentation of w ords into morph emes.
M. K urim o, A . P uurula, E . A risoy , V . S iiv ola, T . H ir-
sim a?k i, J . P y lk k o?nen, T . A lum a?e, and M. S arac lar.
2 0 0 6 b. U nlim ited v oc abulary speec h rec ognition for
agglutinativ e languages. In H L T - N AAC L .
M. K urim o, M. Creutz , and M. V arjok allio. 2 0 0 7 . Mor-
pho Challenge ev aluation using a linguistic G old S tan-
dard. In C L E F .
S . V irpioja, J . J . V a?y ry nen, M. Creutz , and M. S adeniem i.
2 0 0 7 . Morphology -aware statistic al m ac hine transla-
tion based on m orphs induc ed in an unsuperv ised m an-
ner. In M T S u mmit X I. D enm ark .
2 S cr ip t ou tline for th e demo p r esenta tion
In this dem o we will present the ac hiev em ents of the
Morpho Challenge 2 0 0 5 - 2 0 0 8 c om petition in graphs
and the baseline sy stem s for v arious languages de-
v eloped using the Morfessor algorithm for word de-
c om position, I R , L V CS R and S MT . T he audienc e
will also be welc om e to try their own input for these
baseline sy stem s and v iew the results.
T he sc ript is presented below for a poster-sty le
and try -it- y ourself on laptop dem o, but it will work
well as a lec ture-sty le show, too, if needed.
In the poster we illustrate the following points:
1 . B asic c harac teristic s of the unsuperv ised learn-
ing algorithm s and m orphem e analy sis results
in different languages (F innish, T urk ish, G er-
m an, E nglish, A rabic ) as in T able 1 , dem o:
h ttp://w w w .cis.h u t.fi /projects/morph o/.
2 . T he results of the ev aluations against the lin-
guistic gold standard m orphem es in different
languages, see e.g. F igure 1 .
3 . T he results of the IR ev aluations and c om par-
isons to the perform anc e of gram m atic al m or-
phem es, word-based m ethods and stem m ing in
different languages, see e.g. F igure 2 .
4 . T he results of the L V CS R ev aluations with
c om parisons to gram m atic al m orphem es and
word-based m ethods, see e.g. F igure 3 .
5 . T he c all for partic ipation in the Morpho Chal-
lenge 2 0 0 9 c om petition where the new ev alua-
tion task is using m orphem es in S MT .
F igure 1 : F - m easures for the T urk ish m orphem e analy sis.
T he laptop is used to dem onstrate the baseline
sy stem s we hav e rec ently dev eloped for different
task s that are all based on unsuperv ised m orphem es:
15
Example word M orfes s or an aly s is G old S tan dard
Finnish: lin u xiin lin u x + iin lin u x N + I L L
T u r k ish: popU lerliG in i pop + U + ler + liG in i popU ler + D ER lH g + P O S 2 S + A C C ,
popU ler + D ER lH g + P O S 3 + A C C 3
A r a b ic : A lmtH dp A l+ mtH d + p mu t aH idap P O S :P N A l+ + S G ,
mu t aH id P O S :A J A l+ + S G
G e r m a n: z u ru ec k z u b eh alten z u ru ec k + z u + b e+ h alten z u ru ec k B z u b e h alt V + I N F
E ng lish: b ab y - s itters b ab y - + s itter + s b ab y N s it V er s + P L
T ab le 1 : M orph eme an aly s is examples in differen t lan g u ag es .
F ig u re 2 : P rec is ion performan c es for th e G erman IR .
F ig u re 3 : L V C S R error rates for th e T u rk is h tas k .
1 . O n lin e L V C S R s y s tem for h ig h ly ag g lu tin ativ e
lan g u ag es , s ee e.g . s c reen s h ot in F ig u re 4 .
2 . O n lin e I R s y s tem for h ig h ly ag g lu tin ativ e lan -
g u ag es .
3 . O n lin e S M T s y s tem wh ere th e s ou rc e lan g u ag e
is a h ig h ly ag g lu tin ativ e lan g u ag e, s ee e.g .
s c reen s h ot in F ig u re 5 .
F ig u re 4 : S c reen s h ot of th e morph eme-b as ed s peec h rec -
og n iz er in ac tion for F in n is h . A n offl in e v ers ion c an b e
tried in http://www.cis.hut.fi/projects/speech/.
F ig u re 5 : S c reen s h ot of th e morph eme-b as ed mac h in e
tran s lator in ac tion for F in n is h -En g lis h . A s implifi ed web
in terfac e to th e s y s tem is als o av ailab le (pleas e email to
th e au th ors for a lin k ) .
16
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 89?95,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Vocabulary Decomposition for Estonian Open Vocabulary Speech
Recognition
Antti Puurula and Mikko Kurimo
Adaptive Informatics Research Centre
Helsinki University of Technology
P.O.Box 5400, FIN-02015 HUT, Finland
{puurula, mikkok}@cis.hut.fi
Abstract
Speech recognition in many morphologi-
cally rich languages suffers from a very high
out-of-vocabulary (OOV) ratio. Earlier work
has shown that vocabulary decomposition
methods can practically solve this problem
for a subset of these languages. This pa-
per compares various vocabulary decompo-
sition approaches to open vocabulary speech
recognition, using Estonian speech recogni-
tion as a benchmark. Comparisons are per-
formed utilizing large models of 60000 lex-
ical items and smaller vocabularies of 5000
items. A large vocabulary model based on
a manually constructed morphological tag-
ger is shown to give the lowest word er-
ror rate, while the unsupervised morphol-
ogy discovery method Morfessor Baseline
gives marginally weaker results. Only the
Morfessor-based approach is shown to ade-
quately scale to smaller vocabulary sizes.
1 Introduction
1.1 OOV problem
Open vocabulary speech recognition refers to au-
tomatic speech recognition (ASR) of continuous
speech, or ?speech-to-text? of spoken language,
where the recognizer is expected to recognize any
word spoken in that language. This capability is a re-
cent development in ASR, and is required or benefi-
cial in many of the current applications of ASR tech-
nology. Moreover, large vocabulary speech recogni-
tion is not possible in most languages of the world
without first developing the tools needed for open
vocabulary speech recognition. This is due to a fun-
damental obstacle in current ASR called the out-of-
vocabulary (OOV) problem.
The OOV problem refers to the existence of words
encountered that a speech recognizer is unable to
recognize, as they are not covered in the vocabu-
lary. The OOV problem is caused by three inter-
twined issues. Firstly, the language model training
data and the test data always come from different
samplings of the language, and the mismatch be-
tween test and training data introduces some OOV
words, the amount depending on the difference be-
tween the data sets. Secondly, ASR systems always
use finite and preferably small sized vocabularies,
since the speed of decoding rapidly slows down as
the vocabulary size is increased. Vocabulary sizes
depend on the application domain, sizes larger than
60000 being very rare. As some of the words en-
countered in the training data are left out of the vo-
cabulary, there will be OOV words during recogni-
tion. The third and final issue is the fundamental
one; languages form novel sentences not only by
combining words, but also by combining sub-word
items called morphs to make up the words them-
selves. These morphs in turn correspond to abstract
grammatical items called morphemes, and morphs
of the same morpheme are called allomorphs of that
morpheme. The study of these facets of language
is aptly called morphology, and has been largely ne-
glected in modern ASR technology. This is due to
89
ASR having been developed primarily for English,
where the OOV problem is not as severe as in other
languages of the world.
1.2 Relevance of morphology for ASR
Morphologies in natural languages are character-
ized typologically using two parameters, called in-
dexes of synthesis and fusion. Index of synthesis
has been loosely defined as the ratio of morphs per
word forms in the language(Comrie, 1989), while
index of fusion refers to the ratio of morphs per mor-
pheme. High frequency of verb paradigms such as
?hear, hear + d, hear + d? would result in a high syn-
thesis, low fusion language, whereas high frequency
of paradigms such as ?sing, sang, sung? would re-
sult in almost the opposite. Counting distinct item
types and not instances of the types, the first ex-
ample would have 2 word forms, 2 morphs and 2
morphemes, the second 3 word forms, 3 morphs and
1 morpheme. Although in the first example, there
are 3 word instances of the 2 word forms, the lat-
ter word form being an ambiguous one referring to
two distinct grammatical constructions. It should
also be noted that the first morph of the first ex-
ample has 2 pronunciations. Pronunciational bound-
aries do not always follow morphological ones, and
a morph may and will have several pronunciations
that depend on context, if the language in question
has significant orthographic irregularity.
As can be seen, both types of morphological com-
plexity increase the amount of distinct word forms,
resulting in an increase in the OOV rate of any fi-
nite sized vocabulary for that language. In prac-
tice, the OOV increase caused by synthesis is much
larger, as languages can have thousands of differ-
ent word forms per word that are caused by addi-
tion of processes of word formation followed by in-
flections. Thus the OOV problem in ASR has been
most pronounced in languages with much synthesis,
regardless of the amount of fusion. The morpheme-
based modeling approaches evaluated in this work
are primarily intended for fixing the problem caused
by synthesis, and should work less well or even ad-
versely when attempted with low synthesis, high fu-
sion languages. It should be noted that models based
on finite state transducers have been shown to be ad-
equate for describing fusion as well(Koskenniemi,
1983), and further work should evaluate these types
of models in ASR of languages with higher indexes
of fusion.
1.3 Approaches for solving the OOV problem
The traditional method for reducing OOV would be
to simply increase the vocabulary size so that the rate
of OOV words becomes sufficiently low. Naturally
this method fails when the words are derived, com-
pounded or inflected forms of rarer words. While
this approach might still be practical in languages
with a low index of synthesis such as English, it
fails with most languages in the world. For exam-
ple, in English with language models (LM) of 60k
words trained from the Gigaword Corpus V.2(Graff
et al, 2005), and testing on a very similar Voice
of America -portion of TDT4 speech corpora(Kong
and Graff, 2005), this gives a OOV rate of 1.5%.
It should be noted that every OOV causes roughly
two errors in recognition, and vocabulary decompo-
sition approaches such as the ones evaluated here
give some benefits to word error rate (WER) even
in recognizing languages such as English(Bisani and
Ney, 2005).
Four different approaches to lexical unit selec-
tion are evaluated in this work, all of which have
been presented previously. These are hence called
?word?, ?hybrid?, ?morph? and ?grammar?. The
word approach is the default approach to lexical
item selection, and is provided here as a baseline for
the alternative approaches. The alternatives tested
here are all based on decomposing the in-vocabulary
words, OOV words, or both, in LM training data into
sequences of sub-word fragments. During recogni-
tion the decoder can then construct the OOV words
encountered as combinations of these fragments.
Word boundaries are marked in LMs with tokens so
that the words can be reconstructed from the sub-
word fragments after decoding simply by removing
spaces between fragments, and changing the word
boundaries tokens to spaces. As splitting to sub-
word items makes the span of LM histories shorter,
higher order n-grams must be used to correct this.
Varigrams(Siivola and Pellom, 2005) are used in
this work, and to make LMs trained with each ap-
proach comparable, the varigrams have been grown
to roughly sizes of 5 million counts. It should be
noted that the names for the approaches here are
somewhat arbitrary, as from a theoretical perspec-
90
tive both morph- and grammar-based approaches try
to model the grammatical morph set of a language,
difference being that ?morph? does this with an un-
supervised data-driven machine learning algorithm,
whereas ?grammar? does this using segmentations
from a manually constructed rule-based morpholog-
ical tagger.
2 Modeling approaches
2.1 Word approach
The first approach evaluated in this work is the tra-
ditional word based LM, where items are simply the
most frequent words in the language model training
data. OOV words are simply treated as unknown
words in language model training. This has been
the default approach to selection of lexical items in
speech recognition for several decades, and as it has
been sufficient in English ASR, there has been lim-
ited interest in any alternatives.
2.2 Hybrid approach
The second approach is a recent refinement of the
traditional word-based approach. This is similar to
what was introduced as ?flat hybrid model?(Bisani
and Ney, 2005), and it tries to model OOV-words
as sequences of words and fragments. ?Hybrid?
refers to the LM histories being composed of hy-
brids of words and fragments, while ?flat? refers to
the model being composed of one n-gram model in-
stead of several models for the different item types.
The models tested in this work differ in that since
Estonian has a very regular phonemic orthography,
grapheme sequences can be directly used instead
of more complex pronunciation modeling. Subse-
quently the fragments used are just one grapheme in
length.
2.3 Morph approach
The morph-based approach has shown superior re-
sults to word-based models in languages of high
synthesis and low fusion, including Estonian. This
approach, called ?Morfessor Baseline? is described
in detail in (Creutz et al, 2007). An unsupervised
machine learning algorithm is used to discover the
morph set of the language in question, using mini-
mum description length (MDL) as an optimization
criterion. The algorithm is given a word list of the
language, usually pruned to about 100 000 words,
that it proceeds to recursively split to smaller items,
using gains in MDL to optimize the item set. The
resulting set of morphs models the morph set well in
languages of high synthesis, but as it does not take
fusion into account any manner, it should not work
in languages of high fusion. It neither preserves in-
formation about pronunciations, and as these do not
follow morph boundaries, the approach is unsuitable
in its basic form to languages of high orthographic
irregularity.
2.4 Grammar approach
The final approach applies a manually constructed
rule-based morphological tagger(Alum?e, 2006).
This approach is expected to give the best results,
as the tagger should give the ideal segmentation
along the grammatical morphs that the unsupervised
and language-independent morph approach tries to
find. To make this approach more comparable to
the morph models, OOV morphs are modeled as
sequences of graphemes similar to the hybrid ap-
proach. Small changes to the original approach
were also made to make the model comparable to
the other models presented here, such as using the
tagger segmentations as such and not using pseudo-
morphemes, as well as not tagging the items in any
manner. This approach suffers from the same handi-
caps as the morph approach, as well as from some
additional ones: morphological analyzers are not
readily available for most languages, they must be
tailored by linguists for new datasets, and it is an
open problem as to how pronunciation dictionaries
should be written for grammatical morphs in lan-
guages with significant orthographic irregularity.
2.5 Text segmentation and language modeling
For training the LMs, a subset of 43 mil-
lion words from the Estonian Segakorpus was
used(Segakorpus, 2005), preprocessed with a mor-
phological analyzer(Alum?e, 2006). After selecting
the item types, segmenting the training corpora and
generation of a pronunciation dictionary, LMs were
trained for each lexical item type. Table 1 shows
the text format for LM training data after segmen-
tation with each model. As can be seen, the word-
based approach doesn?t use word boundary tokens.
To keep the LMs comparable between each model-
91
model text segmentation
word 5k voodis reeglina loeme
word 60k voodis reeglina loeme
hybrid 5k v o o d i s <w> reeglina <w> l o e m e
hybrid 60k voodis <w> reeglina <w> loeme
morph 5k voodi s <w> re e g lina <w> loe me
morph 60k voodi s <w> reegli na <w> loe me
grammar 5k voodi s <w> reegli na <w> loe me
grammar 60k voodi s <w> reegli na <w> loe me
Table 1. Sample segmented texts for each model.
ing approach, growing varigram models(Siivola and
Pellom, 2005) were used with no limits as to the or-
der of n-grams, but limiting the number of counts to
4.8 and 5 million counts. In some models this grow-
ing method resulted in the inclusion of very frequent
long item sequences to the varigram, up to a 28-
gram. Models of both 5000 and 60000 lexical items
were trained in order to test if and how the model-
ing approaches would scale to smaller and therefore
much faster vocabularies. Distribution of counts in
n-gram orders can be seen in figure 1.
Figure 1. Number of counts included for each n-
gram order in the 60k varigram models.
The performance of the statistical language mod-
els is often evaluated by perplexity or cross-entropy.
However, we decided to only report the real ASR
performance, because perplexity does not suit well
to the comparison of models that use different lex-
ica, have different OOV rates and have lexical units
of different lengths.
3 Experimental setup
3.1 Evaluation set
Acoustic models for Estonian ASR were trained on
the Estonian Speechdat-like corpus(Meister et al,
2002). This consists of spoken newspaper sentences
and shorter utterances, read over a telephone by
1332 different speakers. The data therefore was
quite clearly articulated, but suffered from 8kHz
sample rate, different microphones, channel noises
and occasional background noises. On top of this
the speakers were selected to give a very broad cov-
erage of different dialectal varieties of Estonian and
were of different age groups. For these reasons, in
spite of consisting of relatively common word forms
from newspaper sentences, the database can be con-
sidered challenging for ASR.
Held-out sentences were from the same corpus
used as development and evaluation set. 8 different
sentences from 50 speakers each were used for eval-
uation, while sentences from 15 speakers were used
for development. LM scaling factor was optimized
for each model separately on the development set.
On total over 200 hours of data from the database
was used for acoustic model training, of which less
than half was speech.
3.2 Decoding
The acoustic models were Hidden Markov Models
(HMM) with Gaussian Mixture Models (GMM)
for state modeling based on 39-dimensional
MFCC+P+D+DD features, with windowed cepstral
mean subtraction (CMS) of 1.25 second window.
Maximum likelihood linear transformation (MLLT)
was used during training. State-tied cross-word
triphones and 3 left-to-right states were used, state
durations were modeled using gamma distributions.
On total 3103 tied states and 16 Gaussians per state
were used.
Decoding was done with the decoder developed
at TKK(Pylkk?nen, 2005), which is based on a one-
pass Viterbi beam search with token passing on a
lexical prefix tree. The lexical prefix tree included a
cross-word network for modeling triphone contexts,
and the nodes in the prefix tree were tied at the tri-
phone state level. Bigram look-ahead models were
92
used in speeding up decoding, in addition to prun-
ing with global beam, history, histogram and word
end pruning. Due to the properties of the decoder
and varigram models, very high order n-grams could
be used without significant degradation in decoding
speed.
As the decoder was run with only one pass, adap-
tation was not used in this work. In preliminary
experiments simple adaptation with just constrained
maximum likelihood linear regression (CMLLR)
was shown to give as much as 20 % relative word
error rate reductions (RWERR) with this dataset.
Adaptation was not used, since it interacts with the
model types, as well as with the WER from the first
round of decoding, providing larger RWERR for the
better models. With high WER models, adaptation
matrices are less accurate, and it is also probable that
the decomposition methods yield more accurate ma-
trices, as they produce results where fewer HMM-
states are misrecognized. These issues should be in-
vestigated in future research.
After decoding, the results were post-processed
by removing words that seemed to be sequences of
junk fragments: consonant-only sequences and 1-
phoneme words. This treatment should give very
significant improvements with noisy data, but in pre-
liminary experiments it was noted that the use of
sentence boundaries resulted in almost 10% RW-
ERR weaker results for the approaches using frag-
ments, as that almost negates the gains achieved
from this post-processing. Since sentence bound-
ary forcing is done prior to junk removal, it seems
to work erroneously when it is forced to operate on
noisy data. Sentence boundaries were nevertheless
used, as in the same experiments the word-based
models gained significantly from their use, most
likely because they cannot use the fragment items
for detection of acoustic junk, as the models with
fragments can.
4 Results
Results of the experiments were consistent with ear-
lier findings(Hirsim?ki et al, 2006; Kurimo et al,
2006). Traditional word based LMs showed the
worst performance, with all of the recently proposed
alternatives giving better results. Hybrid LMs con-
sistently outperformed traditional word-based LMs
in both large and small vocabulary conditions. The
two morphology-driven approaches gave similar and
clearly superior results. Only the morph approach
seems to scale down well to smaller vocabulary
sizes, as the WER for the grammar approach in-
creased rapidly as size of the vocabulary was de-
creased.
size word hybrid morph grammar
60000 53.1 47.1 39.4 38.7
5000 82.0 63.0 43.5 47.6
Table 2. Word error rates for the models (WER %).
Table 2 shows the WER for the large (60000) and
small (5000) vocabulary sizes and different mod-
eling approaches. Table 3 shows the correspond-
ing letter error rates (LER). LERs are more compa-
rable across some languages than WERs, as WER
depends more on factors such as length, morpho-
logical complexity, and OOV of the words. How-
ever, for within-language and between-model com-
parisons, the RWERR should still be a valid met-
ric, and is also usable in languages that do not use a
phonemic writing system. The RWERRs of differ-
ent novel methods seems to be comparable between
different languages as well. Both WER and LER are
high considering the task. However, standard meth-
ods such as adaptation were not used, as the inten-
tion was only to study the RWERR of the different
approaches.
size word hybrid morph grammar
60000 17.8 15.8 12.4 12.3
5000 35.5 20.8 14.4 15.4
Table 3. Letter error rates for the models (LER %).
5 Discussion
Four different approaches to lexical item selection
for large and open vocabulary ASR in Estonian
were evaluated. It was shown that the three ap-
proaches utilizing vocabulary decomposition give
substantial improvements over the traditional word
based approach, and make large vocabulary ASR
technology possible for languages similar to Esto-
nian, where the traditional approach fails due to very
93
high OOV rates. These include memetic relatives
Finnish and Turkish, among other languages that
have morphologies of high fusion, low synthesis and
low orthographic irregularity.
5.1 Performance of the approaches
The morpheme-based approaches outperformed the
word- and hybrid-based approaches clearly. The re-
sults for ?hybrid? are in in the range suggested by
earlier work(Bisani and Ney, 2005). One possi-
ble explanation for the discrepancy between the hy-
brid and morpheme-based approaches would be that
the morpheme-based approaches capture items that
make sense in n-gram modeling, as morphs are items
that the system of language naturally operates on.
These items would then be of more use when try-
ing to predict unseen data(Creutz et al, 2007). As
modeling pronunciations is much more straightfor-
ward in Estonian, the morpheme-based approaches
do not suffer from erroneous pronunciations, result-
ing in clearly superior performance.
As for the superiority of the ?grammar? over the
unsupervised ?morph?, the difference is marginal in
terms of RWERR. The grammatical tagger was tai-
lored by hand for that particular language, whereas
Morfessor method is meant to be unsupervised and
language independent. There are further arguments
that would suggest that the unsupervised approach
is one that should be followed; only ?morph? scaled
well to smaller vocabulary sizes, the usual practice
of pruning the word list to produce smaller morph
sets gives better results than here and most impor-
tantly, it is questionable if ?grammar? can be taken
to languages with high indexes of fusion and ortho-
graphic irregularity, as the models have to take these
into account as well.
5.2 Comparison to previous results
There are few previous results published on Estonian
open vocabulary ASR. In (Alum?e, 2006) a WER of
44.5% was obtained with word-based trigrams and
a WER of 37.2% with items similar to ones from
?grammar? using the same speech corpus as in this
work. Compared to the present work, the WER
for the morpheme-based models was measured with
compound words split in both hypothesis and ref-
erence texts, making the task slightly easier than
here. In (Kurimo et al, 2006) a WER of 57.6% was
achieved with word-based varigrams and a WER of
49.0% with morphs-based ones. This used the same
evaluation set as this work, but had slightly different
LMs and different acoustic modelling which is the
main reason for the higher WER levels. In summary,
morpheme-based approaches seem to consistently
outperform the traditional word based one in Esto-
nian ASR, regardless of the specifics of the recogni-
tion system, test set and models.
In (Hirsim?ki et al, 2006) a corresponding com-
parison of unsupervised and grammar-based morphs
was presented in Finnish, and the grammar-based
model gave a significantly higher WER in one of the
tasks. This result is interesting, and may stem from a
number of factors, among them the different decoder
and acoustic models, 4-grams versus varigrams, as
well as differences in post-processing. Most likely
the difference is due to lack of coverage for domain-
specific words in the Finnish tagger, as it has a 4.2%
OOV rate on the training data. On top of this the
OOV words are modeled simply as grapheme se-
quences, instead of modeling only OOV morphs in
that manner, as is done in this work.
5.3 Open problems in vocabulary
decomposition
As stated in the introduction, modeling languages
with high indexes of fusion such as Arabic will re-
quire more complex vocabulary decomposition ap-
proaches. This is verified by recent empirical re-
sults, where gains obtained from simple morpholog-
ical decomposition seem to be marginal(Kirchhoff
et al, 2006; Creutz et al, 2007). These languages
would possibly need novel LM inference algorithms
and decoder architectures. Current research seems
to be heading in this direction, with weighted finite
state transducers becoming standard representations
for the vocabulary instead of the lexical prefix tree.
Another issue in vocabulary decomposition is or-
thographic irregularity, as the items resulting from
decomposition do not necessarily have unambigu-
ous pronunciations. As most modern recognizers
use the Viterbi approximation with vocabularies of
one pronunciation per item, this is problematic. One
solution to this is expanding the different items with
tags according to pronunciation, shifting the prob-
lem to language modeling(Creutz et al, 2007). For
example, English plural ?s? would expand to ?s#1?
94
with pronunciation ?/s/?, and ?s#2? with pronunci-
ation ?/z/?, and so on. In this case the vocabulary
size increases by the amount of different pronunci-
ations added. The new items will have pronuncia-
tions that depend on their language model context,
enabling the prediction of pronunciations with lan-
guage model probabilities. The only downside to
this is complicating the search for optimal vocabu-
lary decomposition, as the items should make sense
in both pronunciational and morphological terms.
One can consider the originally presented hybrid
approach as an approach to vocabulary decompo-
sition that tries to keep the pronunciations of the
items as good as possible, whereas the morph ap-
proach tries to find items that make sense in terms
of morphology. This is obviously due to the meth-
ods having been developed on very different types
of languages. The morph approach was developed
for the needs of Finnish speech recognition, which
is a high synthesis, moderate fusion and very low or-
thographic irregularity language, whereas the hybrid
approach in (Bisani and Ney, 2005) was developed
for English, which has low synthesis, moderate fu-
sion, and very high orthographic irregularity. A uni-
versal approach to vocabulary decomposition would
have to take all of these factors into account.
Acknowledgements
The authors would like to thank Dr. Tanel Alum?e
from Tallinn University of Technology for help in
performing experiments with Estonian speech and
text databases. This work was supported by the
Academy of Finland in the project: New adaptive
and learning methods in speech recognition.
References
Bernard Comrie. 1972. Language Universals and Lin-
guistic Typology, Second Edition. Athen?um Press
Ltd, Gateshead, UK.
Kimmo Koskenniemi. 1983. Two-level Morphol-
ogy: a General Computational Model for Word-Form
Recognition and Production. University of Helsinki,
Helsinki, Finland.
Tanel Alum?e. 2006. Methods for Estonian Large Vo-
cabulary Speech Recognition. PhD Thesis. Tallinn
University of Technology. Tallinn, Estonia.
Maximilian Bisani, Hermann Ney. 2005. Open Vocab-
ulary Speech Recognition with Flat Hybrid Models.
INTERSPEECH-2005, 725?728.
Janne Pylkk?nen. 2005. An Efficient One-pass Decoder
for Finnish Large Vocabulary Continuous Speech
Recognition. Proceedings of The 2nd Baltic Con-
ference on Human Language Technologies, 167?172.
HLT?2005. Tallinn, Estonia.
Vesa Siivola, Bryan L. Pellom. 2005. Growing an n-
Gram Language Model. INTERSPEECH-2005, 1309?
1312.
David Graff, Junbo Kong, Ke Chen and Kazuaki
Maeda. 2005. LDC Gigaword Corpora: En-
glish Gigaword Second Edition. In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
Junbo Kong and David Graff. 2005. TDT4 Multilin-
gual Broadcast News Speech Corpus. In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
Segakorpus. 2005. Segakorpus - Mixed Corpus of Esto-
nian. Tartu University. http://test.cl.ut.ee/korpused/.
Einar Meister, J?rgen Lasn and Lya Meister 2002. Esto-
nian SpeechDat: a project in progress. In Proceedings
of the Fonetiikan P?iv?t - Phonetics Symposium 2002
in Finland, 21?26.
Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, Kevin
Duh and Andreas Stolcke 2006. Morphology-
based language modeling for conversational Arabic
speech recognition. Computer Speech & Language
20(4):589?608.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkk?nen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraclar and Andreas
Stolcke 2007. Analysis of Morph-Based Speech
Recognition and the Modeling of Out-of-Vocabulary
Words Across Languages To appear in Proceedings
of Human Language Technologies: The Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics. NAACL-HLT
2007, Rochester, NY, USA
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola,
Teemu Hirsim?ki, Janne Pylkk?nen, Tanel Alumae
and Murat Saraclar 2006. Unlimited vocabulary
speech recognition for agglutinative languages. In Hu-
man Language Technology, Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. HLT-NAACL 2006. New York,
USA
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja and Janne Pylkk?nen 2006.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
& Language 20(4):515?541.
95
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1177?1185, Dublin, Ireland, August 23-29 2014.
Morfessor FlatCat: An HMM-Based Method for Unsupervised and
Semi-Supervised Learning of Morphology
Stig-Arne Gr
?
onroos
1
stig-arne.gronroos@aalto.fi
Sami Virpioja
2
sami.virpioja@aalto.fi
Peter Smit
1
peter.smit@aalto.fi
Mikko Kurimo
1
mikko.kurimo@aalto.fi
1
Department of Signal Processing and Acoustics, Aalto University
2
Department of Information and Computer Science, Aalto University
Abstract
Morfessor is a family of methods for learning morphological segmentations of words based
on unannotated data. We introduce a new variant of Morfessor, FlatCat, that applies a hid-
den Markov model structure. It builds on previous work on Morfessor, sharing model compo-
nents with the popular Morfessor Baseline and Categories-MAP variants. Our experiments show
that while unsupervised FlatCat does not reach the accuracy of Categories-MAP, with semi-
supervised learning it provides state-of-the-art results in the Morpho Challenge 2010 tasks for
English, Finnish, and Turkish.
1 Introduction
Morphological analysis is essential for automatic processing of compounding and highly-inflecting lan-
guages, for which the number of unique word forms may be very large. Apart from rule-based analyzers,
the task has been approached by machine learning methodology. Especially unsupervised methods that
require no linguistic resources have been studied widely (Hammarstr?om and Borin, 2011). Typically
these methods focus on morphological segmentation, i.e., finding morphs, the surface forms of the mor-
phemes.
For language processing applications, unsupervised learning of morphology can provide decent-
quality analyses without resources produced by human experts. However, while morphological ana-
lyzers and large annotated corpora may be expensive to obtain, a small amount of linguistic expertise is
more easily available. A well-informed native speaker of a language can often identify the different pre-
fixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference.
One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually
segmented words provide significant improvements to the quality of the output when comparing to a
linguistic gold standard.
The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the sim-
plest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor
Baseline is simply a categorical distribution of morphs?a unigram model in the terms of statistical lan-
guage modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semi-
supervised methods evaluated in the Morpho Challenge competitions (Kurimo et al., 2010a) so far, the
next question is how the approach works for more complex models.
Another popular variant of Morfessor, Categories-MAP (CatMAP) (Creutz and Lagus, 2005), models
word formation using a hidden Markov model (HMM). The context-sensitivity of the model improves
the precision of the segmentation. For example, it can prevent splitting a single s, a common English
suffix, from the beginning of a word. Moreover, it can disambiguate between identical morphs that are
actually surface forms of different morphemes. Finally, separation of stems and affixes in the output
makes it simple to use the method as a stemmer.
In contrast to Morfessor Baseline, the lexicon of CatMAP is hierarchical: a morph that is already in
the lexicon may be used to encode the forms of other morphs. This has both advantages and drawbacks.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1177
One downside is that it mixes the prior and likelihood components of the cost function, so that the semi-
supervised approach presented by Kohonen et al. (2010) is not usable.
1.1 Hierarchical versus flat lexicons
From the viewpoint of data compression and following the two-part Minimum Description Length prin-
ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model
parameters and the training data. Equivalently, the cost function L can be derived from the Maximum a
Posteriori (MAP) estimate:
?
? = argmax
?
P(? |D) = argmin
?
(
? log P(?)? log P(D | ?)
)
= argmin
?
L(?,D), (1)
where ? are the model parameters, D is the training corpus, P(?) is the prior of the parameters and
P(D | ?) is the data likelihood.
In context-independent models such as Morfessor Baseline, the parameters include only the forms and
probabilities of the morphs in the lexicon of the model. Morfessor Baseline and Categories-ML (CatML)
(Creutz and Lagus, 2004) use a flat lexicon, in which the forms of the morphs are encoded directly as
strings: each letter requires a certain number of bits to encode. Thus longer morphs are more expensive.
Encoding a long morph is worthwhile only if the morph is referred to frequently enough from the words
in the training data. If a certain string, let us say segmentation, is common enough in the training data, it
is cost-effective to have it as a whole in the lexicon. Splitting it into two items, segment and ation, would
double the number of pointers from the data, even if those morphs were already in the lexicon. The
undersegmentation of frequent words becomes evident especially if the training data is a corpus instead
of a list of unique word forms.
In contrast, Morfessor CatMAP applies a hierarchical lexicon, which makes use of the morphs that
are already in the lexicon. Instead of encoding the form of segmentation by its 12 letters, we could just
encode the form with two references to the forms of the morphs segment and ation. This may also cause
errors, for example encoding station with st and ation.
The lexicon of Morfessor CatMAP allows but does not force hierarchical encoding for the forms:
each morph has an extra parameter that indicates whether it has a hierarchical representation or not. The
problem of oversegmentation, as in st + ation, is solved using the morph categories. The categories,
which are states of the HMM, include stem, prefix, suffix, and a special non-morpheme category. The
non-morpheme category is intended to catch segments that do not fit well into the three proper morph
categories because they are fragments of a larger morph. In our example, the morph st cannot be a suffix
as it starts the word, it is unlikely to be a prefix as it directly precedes a common suffix ation, and it is
unlikely to be a stem as it is very short. Thus the algorithm is likely to use the non-morpheme state. The
hierarchy is expanded only up to the level in which there are no non-morphemes, so the final analysis is
still station. Without the hierarchy, the non-morphemes have to be removed heuristically, as in CatML
(Creutz and Lagus, 2004).
A hierarchical lexicon presents some challenges to model training. For a standard unigram or HMM
model, if you know the state and emission sequence of the training data, you can directly derive the
maximum likelihood (ML) parameters of the model: a probability of a morph is proportional to the
number of times it is referred to, conditional on the state in the HMM. But if the lexicon is partly
hierarchical, also the references within the lexicon add to the reference counts, and there is no direct way
to find the ML parameters even if the encoding of the training data is known. Similarly, semi-supervised
learning cannot be accomplished simply by adding the counts from an annotated data set, as it is not
clear when to use hierarchy instead of segmenting a word directly in the data.
Moreover, for a flat lexicon, the cost function divides into two parts that have opposing optima: the
cost of the data (likelihood) is optimal when there is minimal splitting and the lexicon consists of the
words in the training data, whereas the cost of the model (prior) is optimal when the lexicon is minimal
and consists only of the letters. In consequence, the balance of precision and recall of the segmentation
boundaries can be directly controlled by setting a weight for the data likelihood. Tuning this hyper-
parameter is a very simple form of supervision, but it has drastic effects on the segmentation results
1178
(Kohonen et al., 2010). A direct control of the balance may also be useful for some applications: Virpioja
et al. (2011) found that the performance of the segmentation algorithms in machine translation correlates
more with the precision than the recall. The weighting approach does not work for hierarchical lexicons,
for which changing the weight does not directly affect the decision whether to encode the morph with
hierarchy or not.
1.2 Morfessor FlatCat
In this paper, we introduce a new member to the Morfessor family, Morfessor FlatCat. As indicated by its
name, FlatCat uses a flat lexicon. Our hypothesis is that enabling semi-supervised learning is effective in
compensating for the undersegmentation caused by the lack of hierarchy. In particular, semi-supervised
learning can improve modeling of suffixation. In the examined languages, suffixes tend to serve syntactic
purposes, such as marking case, tense, person or number. Examples are the suffix s marking tense and
person in she writes and number in stations. Thus the suffix class is closed and has only a small number
of morphemes compared to the prefix and stem categories. As a consequence, a large coverage of suffixes
can be achieved already with a relatively small annotated data set.
The basic model of morphotactics in FlatCat is the same as in the CatML and CatMAP variants: a
hidden Markov model with states that correspond to a word boundary and four morph categories: stem,
prefix, suffix, and non-morpheme. As in CatML, we apply heuristics for removal of non-morphemes
from the final segmentation. However, because FlatCat uses MAP estimation of the parameters, these
heuristics are not necessary during the training for controlling the model complexity, but merely used as
a post-processing step to get meaningful categories.
Modeling of morphotactics improves the segmentation of compound words, by allowing the overall
level of segmentation to be increased without increasing the number of correct morphs used in incorrect
positions. As the benefits of semi-supervised learning and improved morphotactics are likely to com-
plement each other, we can expect improved performance over the semi-supervised Morfessor Baseline
method. By experimental comparison to the previous Morfessor variants, we are able to shed more light
on the effects of using an HMM versus unigram model for morphotactics, using a hierarchical versus flat
lexicon, and exploiting small amounts of annotated training data.
2 FlatCat model and algorithms
Morfessor FlatCat uses components from the older Morfessor variants. Instead of going through all the
details, we refer to the previous work and highlight only the differences. Common components between
Morfessor methods are summarized in Table 1.
As a generative model, Morfessor FlatCat describes the joint distribution P(A,W | ?) of words and
their analyses. The wordsW are observed, but their analyses, A, is a latent variable in the model. An
analysis of a word contains its morphs and morph categories: prefix, stem, suffix, and non-morpheme.
As marginalizing over all possible analyses is generally infeasible, point estimates are used during the
training. The likelihood conditioned on the current analyses is
P(D |A, ?) =
|D|
?
j=1
P(A
j
| ?). (2)
If m
i
are the morphs in A
j
, c
i
are the hidden states of the HMM corresponding to the categories of the
morphs, and # is the word boundary, P(A
j
| ?) is
P(c
1
|#)
|A
j
|
?
i=1
[
P(m
i
| c
i
) P(c
i+1
| c
i
)
]
P(# | c
|A
j
|
). (3)
Morfessor FlatCat applies an MDL-derived prior designed to control the number of non-zero param-
eters. The prior is otherwise the same as in Morfessor Baseline, but it includes the usage properties
from Morfessor CatMAP: the length of the morph and its right and left perplexity. The perplexity mea-
sures describe the predictability of the contexts in which the morph occurs. The emission probability of
1179
Morfessor method
Component Baseline CatMAP CatML FlatCat
Lexicon type Flat Hierarchy Flat Flat
Morphotactics Unigram HMM HMM HMM
Estimation MAP MAP ML MAP
Semi-supervised Implemented Not implemented Not implemented Implemented
Table 1: Overview of similarities and differences between Morfessor methods.
a morph conditioned on the morph category, P(m | c), is calculated from the properties of the morphs
similarly as in CatMAP.
2.1 Training algorithms
The parameters are optimized using a local search. Only a part of the parameters are optimized in each
step: the parameters that are used in calculating the likelihood of a certain part, unit, of the corpus. Units
vary in complexity, from all occurrences of a certain morph to the occurrences of a morph bigram whose
context fits to certain criteria.
The algorithm tries to simultaneously find the optimal segmentation for the unit and the optimal pa-
rameters consistent with that segmentation:
(A, ?) = argmin
OP(A,?)
{
L(?,A,D)
}
. (4)
The training operators OP define the units changed by the local search and the alternative segmentations
tried for each unit. There are three training operators: split, join and resegment, analogous to the similarly
named stages in CatMAP.
The split operator is applied first. It targets all occurrences of a specific morph in the corpus simultane-
ously, attempting to split it into two parts. The whole corpus is processed by sorting the current morphs
by length from shortest to longest.
The second operator attempts to join morph bigrams, grouped by the position of the bigram in the
word. The position grouped bigram counts are sorted by frequency, from most to least common.
Finally, resegmenting uses the generalized Viterbi algorithm to find the currently optimal segmentation
for one whole word at a time. This operator targets each corpus word in increasing order of frequency.
The heuristics used in FlatCat to remove non-morphemes from the final segmentation are the fol-
lowing: All consequent non-morphemes are joined together. If the resulting morph is longer than 4
characters, it is accepted as a stem. All non-morphemes preceded by a suffix and followed by only suf-
fixes or other non-morphemes are recategorized as suffixes without joining with their neighbors. If any
short non-morphemes remain, they are joined either to the preceding or following morphs (the latter only
for those in the initial position).
2.2 Semi-supervised learning
Kohonen et al. (2010) found that semi-supervised learning of Morfessor models was not effective by
only fixing the values of the analysis A for the annotated samplesD
A
. Their solution was to introduce
corpus likelihood weights ? and ?, one for the unannotated data set and one for the annotated data set.
Thus, instead of optimizing the MAP estimate, Kohonen et al. (2010) minimize the cost
L(?,A,D,D
A
) = ? log P(?)? ? log P(D |A, ?)? ? log P(D
A
|A, ?). (5)
The weights can be tuned on a development set. We use the same scheme for FlatCat.
The likelihood of the annotated data is calculated using the same HMM that is used for the unannotated
data. The morph properties are estimated only from the unannotated data. To ensure that the morphs
required for the annotated data can be emitted, a copy of each word in the annotations is added to the
1180
(a) English.
Method ? ? Pre Rec F
U Baseline 1.0 ? .88 .59 .71
U CatMAP ? ? .89 .51 .65
U FlatCat 1.0 ? .90 .57 .69
W Baseline 0.7 ? .83 .62 .71
W FlatCat 0.5 ? .84 .60 .70
SS Baseline 1.0 3000 .83 .77 .80
SS FlatCat 0.9 2000 .86 .76 .81
SS CRF+FlatCat 0.9 2000 .87 .77 .82
S CRF ? ? .92 .73 .81
(b) Finnish.
Method ? ? Pre Rec F
U Baseline 1.0 ? .84 .38 .53
U CatMAP ? ? .76 .51 .61
U FlatCat 1.0 ? .84 .38 .52
W Baseline .02 ? .62 .54 .58
W FlatCat .015 ? .66 .52 .58
SS Baseline .1 15000 .75 .72 .73
SS FlatCat .2 1500 .79 .71 .75
SS CRF+FlatCat .2 2500 .82 .76 .79
S CRF ? ? .88 .74 .80
Table 2: Boundary Precision and Recall results in comparison to gold standard segmentation. Abbrevi-
ations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) and fully
supervised (S) methods. Best results for each measure have been hilighted using boldface.
unannotated data. This unannotated copy is loosely linked to the annotated word: operations that would
result in the removal of a morph required for the annotations from the lexicon cannot be selected, as such
an operation would have infinite cost.
3 Experiments
We compare Morfessor FlatCat
1
to two previous Morfessor methods and a fully supervised discrimi-
native segmentation method. The Morfessor methods used as references are the CatMAP
2
and Base-
line
3
implementations by Creutz and Lagus (2005) and Virpioja et al. (2013), respectively. Virpioja et
al. (2013) implements the semi-supervised method described by Kohonen et al. (2010). For a super-
vised discriminative model, we use a character-level conditional random field (CRF) implementation by
Ruokolainen et al. (2013)
4
.
We use the English, Finnish and Turkish data sets from Morpho Challenge 2010 (Kurimo et al.,
2010b). They include large unannotated word lists, one thousand annotated words for training, 700?
800 annotated words for parameter tuning, and 10? 1000 annotated words for testing.
For evalution, we use the BPR score by Virpioja et al. (2011). The score calculates the precision (Pre),
recall (Rec), and F
1
-score (F) of the predicted morph boundaries compared to a linguistic gold standard.
In the presence of alternative gold standard analyses, we weight each alternative equally.
We also report the mean average precision from the English and Finnish information retrieval (IR)
tasks of the Morpho Challenge. The Lemur Toolkit (Ogilvie and Callan, 2001) with Okapi BM25 rank-
ing was used. The Finnish data consists of 55K documents, 50 test queries and 23K binary relevance
assessments. The English data consists of 170K documents, 50 test queries and 20K binary relevance as-
sessments. The domain of both data sets is short newspaper articles. All word forms in both the corpora
and the queries were replaced by the morphological segmentation to be evaluated.
Morfessor FlatCat is a pipeline method that refines an initial segmentation given as input. We try two
different initializations for the semi-supervised setting: initializing with the segmentation produced by
semi-supervised Morfessor Baseline, and initializing with the CRF segmentation. All unsupervised and
likelihood-weighted results are initialized with the corresponding Baseline output.
All methods were trained using word types. The weight and perplexity threshold parameters were
optimized separately for each method, using a grid search with the held-out data set. The supervised
CRF method was trained using the one thousand word annotated training data set.
1
Available at https://github.com/aalto-speech/flatcat
2
Available at http://www.cis.hut.fi/projects/morpho/morfessorcatmap.shtml
3
Available at https://github.com/aalto-speech/morfessor
4
Available at http://users.ics.aalto.fi/tpruokol/
1181
Method ? ? Pre Rec F
U Baseline 1.0 ? .85 .36 .51
U CatMAP ? ? .83 .50 .62
U FlatCat 1.0 ? .87 .36 .51
W Baseline 0.1 ? .71 .41 .52
W FlatCat 0.3 ? .88 .38 .53
SS Baseline 0.4 2000 .86 .60 .71
SS FlatCat 0.8 2666 .87 .59 .70
SS CRF+FlatCat 1.0 3000 .87 .61 .72
S CRF ? ? .89 .58 .70
Table 3: Boundary Precision and Recall results in comparison to gold standard segmentation for Turkish.
Abbreviations have been used for Unsupervised (U), likelihood weighted (W), semi-supervised (SS) and
fully supervised (S) methods. Best results for each measure have been hilighted using boldface.
3.1 Comparison to linguistic gold standards
The results of the BPR evaluations are shown in Tables 2 (English, Finnish) and 3 (Turkish). Semi-
supervised FlatCat initialized using CRF achieves the highest F-score for both the English and Turkish
data sets. The difference between the highest and second-highest scoring methods is statistically signifi-
cant for Finnish and Turkish, but not for English (Wilcoxon signed-rank test, p < 0.01).
Table 4 shows BPR for subsets of words consisting of different morph category patterns. Each subset
consists of 500 words from the English or Finnish gold standard, with one of five selected morph patterns
as the only valid analysis. The subsets consist of words with the following morph patterns: words that
should not be segmented (STM), compound words consisting of exactly two stems (STM + STM), a
prefix followed by a stem (PRE + STM), a stem followed by a single suffix (STM + SUF) and a stem
and exactly two suffixes (STM + SUF + SUF). For the STM pattern only precision is reported, as recall
is not defined for an empty set of true boundaries.
The fact that semi-supervised FlatCat compares well against CatMAP in recall, for all morph patterns
and for the test set as a whole, indicates that supervision indeed is effective in compensating for the
undersegmentation caused by the lack of hierarchy in the lexicon. The benefit of modeling morphotactics
can be seen in improved precision for the STM + STM (for English and Finnish) and PRE + STM (for
Finnish) patterns when comparing against semi-supervised Baseline. The more aggressive segmentation
of Baseline gives better results for the English PRE + STM subset than for Finnish due to the shortness
of the English prefixes (on average 3.6 letters for the English and 5.3 for the Finnish subset). While
not directly observable in Table 4, a large part of the improvement over semi-supervised Baseline is
explained by that FlatCat does not use suffix-like morphs in incorrect positions.
Initializing the FlatCat model with CRF segmentation improves the F-scores in all subsets compared
to the initialization with Morfessor Baseline. While FlatCat cannot keep the accuracy of the suffix
boundaries at as high level as CRF, it clearly improves the stem splitting.
3.2 Information retrieval
Stemming has been shown to improve IR results (Kurimo et al., 2009), by removing inflection that is
often not relevant to the query. The morph categories make it possible to simulate stemming by removing
morphs categorized as prefixes or suffixes. As longer affixes are more likely to be meaningful, we limited
the affix removal to morphs of at most 3 letters. For methods that use morph categories, we report two
IR results: the first using all the data and the second with short affix removal (SAR) applied.
In the IR results, we include the topline methods from Morpho Challenge: Snowball Porter stemmer
(Porter, 1980) for English and ?TWOL first? for Finnish. The latter selects the lemma from the first
of the possible analyses given by the morphological analyzer FINTWOL (Lingsoft, Inc.) based on the
1182
(a) English.
STM STM + STM PRE + STM STM + SUF STM + SUF + SUF
Method Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec F
U CatMAP .90 .94 .63 .75 .91 .64 .75 .87 .45 .59 .90 .51 .65
SS Baseline .64 .93 .77 .84 .82 .74 .77 .83 .86 .84 .91 .79 .85
SS FlatCat .68 .94 .65 .77 .78 .62 .69 .86 .88 .87 .94 .79 .86
SS CRF+FlatCat .68 .95 .78 .86 .78 .66 .72 .87 .89 .88 .94 .80 .87
S CRF .78 .94 .72 .81 .85 .59 .69 .92 .91 .91 .95 .82 .88
(b) Finnish.
STM STM + STM PRE + STM STM + SUF STM + SUF + SUF
Method Pre Pre Rec F Pre Rec F Pre Rec F Pre Rec F
U CatMAP .77 .90 .97 .94 .88 .96 .92 .67 .46 .54 .68 .38 .49
SS Baseline .50 .82 .88 .85 .73 .83 .78 .64 .85 .73 .76 .78 .77
SS FlatCat .49 .91 .95 .93 .80 .89 .85 .67 .84 .75 .77 .75 .76
SS CRF+FlatCat .53 .91 .96 .94 .84 .94 .88 .71 .88 .79 .80 .79 .79
S CRF .68 .88 .91 .89 .90 .91 .91 .83 .91 .87 .91 .85 .88
Table 4: Results of BPR experiments with different morph category patterns. Best results for each
measure have been hilighted using boldface.
two-level model by Koskenniemi (1983). As baseline results we also include unsegmented word forms
and truncating each word after the first five letters (First 5).
The results of the IR experiment are shown in Table 5. FlatCat provides the highest score for Finnish.
The English scores are similar to those of the semi-supervised Baseline. FlatCat performs better than
CRF for both languages. This is explained by the higher level of consistency in the segmentations
produced by FlatCat, which makes the resulting morphs more useful as query terms. The number of
morphs in the lexicons of FlatCat initialized using CRF are 108 391 (English), 46 123 (Finnish) and
74 193 (Turkish), which is much smaller than the respective morph lexicon sizes counted from the CRF
segmentation: 339 682 (English), 396 869 (Finnish) and 182 356 (Turkish). This decrease in lexicon
size indicates a more structured segmentation.
The IR performance of semi-supervised FlatCat benefits from the removal of short affixes for English
when initialized by CRF, and Finnish for both initializations. It also improves the results of unsupervised
FlatCat and CatMAP for Finnish, but lowers the precision for English. A possible explanation is that the
unsupervised methods do not analyze the suffixes with a high enough accuracy.
4 Conclusions
We have introduced a new variant of the Morfessor method, Morfessor FlatCat. It predicts both morphs
and their categories based on unannotated data, but also annotated training data can be provided. It was
shown to outperform earlier Morfessor methods in the semi-supervised learning task for English, Finnish
and Turkish.
The purely supervised CRF-based segmentation method proposed by Ruokolainen et al. (2013) outper-
forms FlatCat for Finnish and reaches the same level for English. However, we show that a discriminative
model such as CRF gives inconsistent segmentations that do not work as well in a practical application:
In English and Finnish information retrieval tasks, FlatCat clearly outperformed the CRF-based segmen-
tation.
We see two major directions for future work. Currently Morfessor FlatCat, like most Morfessor meth-
ods, assumes that words in a sentence occur independently. Making use of the sentence context in which
words occur would, however, allow making Part-Of-Speech -like distinctions. These distinctions could
1183
(a) English.
Rank Method SAR MAP
1 ? Snowball Porter ? 0.4092
2 SS Baseline ? 0.3855
3 SS FlatCat No 0.3837
4 SS FlatCat Yes 0.3821
5 SS CRF+FlatCat Yes 0.3810
6 SS CRF+FlatCat No 0.3788
7 S CRF ? 0.3771
8 W Baseline ? 0.3761
9 U Baseline ? 0.3695
10 U CatMAP No 0.3682
11 U CatMAP Yes 0.3653
12 W FlatCat No 0.3651
13 ? (First 5) ? 0.3648
14 W FlatCat Yes 0.3606
15 U FlatCat No 0.3486
16 U FlatCat Yes 0.3451
17 ? (Words) ? 0.3303
(b) Finnish.
Rank Method SAR MAP
1 W FlatCat No 0.5057
2 W FlatCat Yes 0.5029
3 SS FlatCat Yes 0.4987
4 ? TWOL first ? 0.4973
5 SS CRF+FlatCat Yes 0.4912
6 U CatMAP Yes 0.4884
7 U CatMAP No 0.4865
8 SS CRF+FlatCat No 0.4826
9 SS FlatCat No 0.4821
10 ? (First 5) ? 0.4757
11 SS Baseline ? 0.4722
12 S CRF ? 0.4660
13 W Baseline ? 0.4582
14 U Baseline ? 0.4378
15 U FlatCat Yes 0.4349
16 U FlatCat No 0.4334
17 ? (Words) ? 0.3483
Table 5: Information Retrieval results. Results of the method presented in this paper are hilighted using
boldface. Mean Average Precision is abbreviated as MAP. Short affix removal is abbreviated as SAR.
help disambiguate inflections of different lexemes that have the same surface form but should be analyzed
differently (Can and Manandhar, 2013).
The second direction is removal of the assumption that a morphology consists only of concatenative
processes. Introducing transformations to model allomorphy in a similar manner as Kohonen et al.
(2009) would allow finding the shared abstract morphemes underlying different allomorphs. This could
be especially beneficial in information retrieval and machine translation applications.
Acknowledgments
This research has been supported by European Community?s Seventh Framework Programme
(FP7/2007?2013) under grant agreement n?287678 and the Academy of Finland under the Finnish Cen-
tre of Excellence Program 2012?2017 (grant n?251170) and the LASTU Programme (grants n?256887
and 259934). The experiments were performed using computer resources within the Aalto University
School of Science ?Science-IT? project. We thank Teemu Ruokolainen for his help with the experiments.
References
Burcu Can and Suresh Manandhar. 2013. Dirichlet processes for joint learning of morphology and PoS tags. In
Proceedings of the International Joint Conference on Natural Language Processing, pages 1087?1091, Nagoya,
Japan, October.
Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02Workshop onMorphological and Phonological Learning, pages 21?30, Philadelphia,
PA, USA, July. Association for Computational Linguistics.
Mathias Creutz and Krista Lagus. 2004. Induction of a simple morphology for highly-inflecting languages. In
Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages
43?51, Barcelona, Spain, July. Association for Computational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unanno-
tated text. In Timo Honkela, Ville K?on?onen, Matti P?oll?a, and Olli Simula, editors, Proceedings of AKRR?05,
1184
International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June. Helsinki University of Technology, Laboratory of Computer and Information
Science.
Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language Processing, 4(1):3:1?3:34, January.
Harald Hammarstr?om and Lars Borin. 2011. Unsupervised learning of morphology. Computational Linguistics,
37(2):309?350, June.
Oskar Kohonen, Sami Virpioja, and Mikaela Klami. 2009. Allomorfessor: Towards unsupervised morpheme
analysis. In Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17?19, 2008, Revised Selected
Papers, volume 5706 of Lecture Notes in Computer Science, pages 975?982. Springer Berlin / Heidelberg,
September.
Oskar Kohonen, Sami Virpioja, and Krista Lagus. 2010. Semi-supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and
Phonology, pages 78?86, Uppsala, Sweden, July. Association for Computational Linguistics.
Kimmo Koskenniemi. 1983. Two-level morphology: A general computational model for word-form recognition
and production. Ph.D. thesis, University of Helsinki.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and
results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.
Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus. 2010a. Morpho Challenge 2005-2010: Eval-
uations and results. In Jeffrey Heinz, Lynne Cahill, and Richard Wicentowski, editors, Proceedings of the
11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 87?95,
Uppsala, Sweden, July. Association for Computational Linguistics.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen. 2010b. Overview and results of Morpho Challenge 2010. In
Proceedings of the Morpho Challenge 2010 Workshop, pages 7?24, Espoo, Finland, September. Aalto Univer-
sity School of Science and Technology, Department of Information and Computer Science. Technical Report
TKK-ICS-R37.
Paul Ogilvie and James P Callan. 2001. Experiments using the Lemur toolkit. In TREC, volume 10, pages
103?108.
Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130?137.
Jorma Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465?471.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo. 2013. Supervised morphological seg-
mentation in a low-resource learning setting using conditional random fields. In Proceedings of the Seventeenth
Conference on Computational Natural Language Learning, pages 29?37, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Sami Virpioja, Ville T Turunen, Sebastian Spiegler, Oskar Kohonen, and Mikko Kurimo. 2011. Empirical com-
parison of evaluation methods for unsupervised learning of morphology. Traitement Automatique des Langues,
52(2):45?90.
Sami Virpioja, Peter Smit, Stig-Arne Gr?onroos, and Mikko Kurimo. 2013. Morfessor 2.0: Python implementation
and extensions for Morfessor Baseline. Report 25/2013 in Aalto University publication series SCIENCE +
TECHNOLOGY, Department of Signal Processing and Acoustics, Aalto University.
1185
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 21?24,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Morfessor 2.0: Toolkit for statistical morphological segmentation
Peter Smit
1
peter.smit@aalto.fi
Sami Virpioja
2
sami.virpioja@aalto.fi
Stig-Arne Gr
?
onroos
1
stig-arne.gronroos@aalto.fi
Mikko Kurimo
1
mikko.kurimo@aalto.fi
1
Department of Signal Processing and Acoustics, Aalto University
2
Department of Information and Computer Science, Aalto University
Abstract
Morfessor is a family of probabilistic ma-
chine learning methods for finding the
morphological segmentation from raw text
data. Recent developments include the de-
velopment of semi-supervised methods for
utilizing annotated data. Morfessor 2.0
is a rewrite of the original, widely-used
Morfessor 1.0 software, with well docu-
mented command-line tools and library in-
terface. It includes new features such as
semi-supervised learning, online training,
and integrated evaluation code.
1 Introduction
In the morphological segmentation task, the goal
is to segment words into morphemes, the small-
est meaning-carrying units. Morfessor is a family
of methods for unsupervised morphological seg-
mentation. The first version of Morfessor, called
Morfessor Baseline, was developed by Creutz and
Lagus (2002) its software implementation, Mor-
fessor 1.0, released by Creutz and Lagus (2005b).
A number of Morfessor variants have been devel-
oped later, including Morfessor Categories-MAP
(Creutz and Lagus, 2005a) and Allomorfessor
(Virpioja et al., 2010). Even though these algo-
rithms improve Morfessor Baseline in some areas,
the Baseline version has stayed popular as a gener-
ally applicable morphological analyzer (Spiegler
et al., 2008; Monson et al., 2010).
Over the past years, Morfessor has been used
for a wide range of languages and applications.
The applications include large vocabulary contin-
uous speech recognition (e.g. Hirsim?aki et al.,
2006), machine translation (e.g. Virpioja et al.,
2007), and speech retrieval (e.g. Arisoy et al.,
2009). Morfessor is well-suited for languages with
concatenative morphology, and the tested lan-
guages include Finnish and Estonian (Hirsim?aki
et al., 2009), German (El-Desoky Mousa et al.,
2010), and Turkish (Arisoy et al., 2009).
Morfessor 2.0 is a new implementation of the
Morfessor Baseline algorithm.
1
It has been writ-
ten in a modular manner and released as an open
source project with a permissive license to encour-
age extensions. This paper includes a summary of
the Morfessor 2.0 software and a description of the
demonstrations that will be held. An extensive de-
scription of the features in Morfessor 2.0, includ-
ing experiments, is available in the report by Vir-
pioja et al. (2013).
2 Morfessor model and algorithms
Models of the Morfessor family are generative
probabilistic models that predict compounds and
their analyses (segmentations) given the model pa-
rameters. We provide a brief overview of the
methodology; Virpioja et al. (2013) should be re-
ferred to for the complete formulas and description
of the model and its training algorithms.
Unlike older Morfessor implementations, Mor-
fessor 2.0 is agnostic in regard to the actual data
being segmented. In addition to morphological
segmentation, it can handle, for example, sentence
chunking. To reflect this we use the following
generic terms: The smallest unit that can be split
will be an atom (letter). A compound (word) is a
sequence of atoms. A construction (morph) is a
sequence of atoms contained inside a compound.
2.1 Model and cost function
The cost function of Morfessor Baseline is derived
using maximum a posteriori estimation. That is,
the goal is to find the most likely parameters ?
1
Morfessor 2.0 can be downloaded from the Mor-
pho project website (http://www.cis.hut.fi/
projects/morpho/) or GitHub repository (https:
//github.com/aalto-speech/morfessor).
21
given the observed training dataD
W
:
?
MAP
= argmax
?
p(?)p(D
W
|?) (1)
Thus we are maximizing the product of the model
prior p(?) and the data likelihood p(D
W
|?). As
usual, the cost function to minimize is set as the
minus logarithm of the product:
L(?,D
W
) = ? log p(?)? log p(D
W
|?). (2)
During training, the data likelihood is calcu-
lated using a hidden variable that contains the cur-
rent chosen analyses. Secondly, it is assumed that
the constructions in a compound occur indepen-
dently. This simplifies the data likelihood to the
product of all construction probabilities in the cho-
sen analyses. Unlike previous versions, Morfes-
sor 2.0 includes also the probabilities of the com-
pound boundaries in the data likelihood.
For prior probability, Morfessor Baseline de-
fines a distribution over the lexicon of the model.
The prior assigns higher probability to lexicons
that store fewer and shorter constructions. The
lexicon prior consists of to parts, a product over
the form probabilities and a product over the usage
probabilities. The former includes the probability
of a sequence of atoms and the latter the maxi-
mum likelihood estimates of the constructions. In
contrast to Morfessor 1.0, Morfessor 2.0 currently
supports only an implicit exponential length prior
for the constructions.
2.2 Training and decoding algorithms
A Morfessor model can be trained in multiple
ways. The standard batch training uses a local
search utilizing recursive splitting. The model is
initialized with the compounds and the full model
cost is calculated. The data structures are designed
in such way that the cost is efficient compute dur-
ing the training.
In one epoch of the algorithm, all compounds
in the training data are processed. For each com-
pound, all possible two-part segmentations are
tested. If one of the segmentations yields the low-
est cost, it is selected and the segmentation is tried
recursively on the resulting segments. In each step
of the algorithm, the cost can only decrease or stay
the same, thus guaranteeing convergence. The al-
gorithm is stopped when the cost decreases less
than a configurable threshold value in one epoch.
An extension of the Viterbi algorithm is used
for decoding, that is, finding the optimal segmen-
tations for new compound forms without changing
the model parameters.
3 New features in Morfessor 2.0
3.1 Semi-supervised extensions
One important feature that has been implemented
in Morfessor 2.0 are the semi-supervised exten-
sions as introduced by Kohonen et al. (2010)
Morfessor Baseline tends to undersegment
when the model is trained for morphological seg-
mentation using a large corpus (Creutz and Lagus,
2005b). Oversegmentation or undersegmentation
of the method are easy to control heuristically
by including a weight parameter ? for the likeli-
hood in the cost function. A low ? increases the
priors influence, favoring small construction lexi-
cons, while a high value increases the data likeli-
hood influence, favoring longer constructions.
In semi-supervised Morfessor, the likelihood of
an annotated data set is added to the cost function.
As the amount of annotated data is typically much
lower than the amount of unannotated data, its ef-
fect on the cost function may be very small com-
pared to the likelihood of the unannotated data.
To control the effect of the annotations, a sepa-
rate weight parameter ? can be included for the
annotated data likelihood.
If separate development data set is available for
automatic evaluation of the model, the likelihoods
weights can be optimized to give the best out-
put. This can be done by brute force using a grid
search. However, Morfessor 2.0 implementation
includes a simple heuristic for automatically tun-
ing the value of ? during the training, trying to
balance precision and recall. A simple heuristic,
which gives an equivalent contribution to the an-
notated data, is used for ?.
3.2 On-line training
In addition to the batch training mode, Morfes-
sor 2.0 supports on-line training mode, in which
unannotated text is processed one compound at a
time. This makes it simple to, for example, adapt
pre-trained models for new type of data. As fre-
quent compounds are encountered many times in
running text, Morfessor 2.0 includes an option for
randomly skipping compounds and constructions
that have been recently analyzed. The random
22
Figure 1: Screenshot from the Morfessor 2.0 demo.
skips can also be used to speed up the batch train-
ing.
3.3 Integrated evaluation code
One common method for evaluating the perfor-
mance of a Morfessor model is to compare it
against a gold standard segmentation using seg-
mentation boundary precision and recall. To make
the evaluation easy, the necessary tools for calcu-
lating the BPR metric by (Virpioja et al., 2011)
are included in Morfessor 2.0. For significance
testing when comparing multiple models, we have
included the Wilcoxon signed-rank test. Both the
evaluation code and statistical testing code are ac-
cessible from both the command line and the li-
brary interface.
3.4 N-best segmentation
In order to generate multiple segmentations for a
single compound, Morfessor 2.0 includes a n-best
Viterbi algorithm. It allows extraction of all possi-
ble segmentations for a compound and the proba-
bilities of the segmentations.
4 Demonstration
In the demonstration session, multiple features
and usages of Morfessor will be shown.
4.1 Web-based demonstration
A live demonstration will be given of segmenting
text with Morfessor 2.0 for different language and
training data options. In a web interface, the user
can choose a language, select the size of the train-
ing corpus and other options. After that a word
can be given which will be segmented using n-best
Viterbi, showing the 5 best results.
A list of planned languages can be found in Ta-
ble 1. A screen shot of the demo interface is shown
in Figure 1.
Languages # Words # Word forms
English 62M 384.903
Estonian 212M 3.908.820
Finnish 36M 2.206.719
German 46M 1.266.159
Swedish 1M 92237
Turkish 12M 617.298
Table 1: List of available languages for Morfessor
2.0 demonstration.
4.2 Command line interface
The new command line interface will be demon-
strated to train and evaluate Morfessor models
from texts in different languages. A diagram of
the tools is shown in Figure 2
4.3 Library interface
Interfacing with the Morfessor 2.0 Python library
will be demonstrated for building own scientific
experiments, as well as integrating Morfessor in
23
Training data
Annotation data
morfessor-train
Morfessor
model
Corpus
Gold standard
morfessor-
segment
morfessor-
evaluate
Segmented corpus
BPR-scores
Figure 2: The standard workflow for Morfessor
command line tools
bigger project. Also the code of the Web based
demonstration will be shown as an example.
Acknowledgements
The authors have received funding from the EC?s
7th Framework Programme (FP7/2007?2013) un-
der grant agreement n?287678 and the Academy
of Finland under the Finnish Centre of Excel-
lence Program 2012?2017 (grant n?251170) and
the LASTU Programme (grants n?256887 and
259934). The experiments were performed us-
ing computer resources within the Aalto Univer-
sity School of Science ?Science-IT? project.
References
E. Arisoy, D. Can, S. Parlak, H. Sak, and M. Saraclar.
2009. Turkish broadcast news transcription and re-
trieval. Audio, Speech, and Language Processing,
IEEE Transactions on, 17(5):874?883.
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Mike Maxwell, editor, Pro-
ceedings of the ACL-02 Workshop on Morphological
and Phonological Learning, pages 21?30. Associa-
tion for Computational Linguistics, July.
M. Creutz and K. Lagus. 2005a. Inducing the mor-
phological lexicon of a natural language from unan-
notated text. In Proceedings of AKRR?05, Interna-
tional and Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning, pages
106?113, Espoo, Finland, June. Helsinki University
of Technology.
M. Creutz and K. Lagus. 2005b. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.
A. El-Desoky Mousa, M. Ali Basha Shaik, R. Schluter,
and H. Ney. 2010. Sub-lexical language models for
German LVCSR. In Spoken Language Technology
Workshop (SLT), 2010 IEEE, pages 171?176. IEEE.
T. Hirsim?aki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkk?onen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech & Lan-
guage, 20(4):515?541.
T. Hirsim?aki, J. Pylkk?onen, and M. Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. Audio, Speech, and
Language Processing, IEEE Transactions on,
17(4):724?732.
O. Kohonen, S. Virpioja, and K. Lagus. 2010. Semi-
supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Spe-
cial Interest Group on Computational Morphology
and Phonology, pages 78?86, Uppsala, Sweden,
July. Association for Computational Linguistics.
C. Monson, K. Hollingshead, and B. Roark. 2010.
Simulating morphological analyzers with stochastic
taggers for confidence estimation. In Multilingual
Information Access Evaluation I. Text Retrieval Ex-
periments, pages 649?657. Springer.
S. Spiegler, B. Gol?enia, K. Shalonova, P. Flach, and
R. Tucker. 2008. Learning the morphology of zulu
with different degrees of supervision. In Spoken
Language Technology Workshop, 2008. SLT 2008.
IEEE, pages 9?12. IEEE.
S. Virpioja, J. V?ayrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine trans-
lation based on morphs induced in an unsupervised
manner. In Proceedings of the Machine Translation
Summit XI, pages 491?498, Copenhagen, Denmark,
September.
S. Virpioja, O. Kohonen, and K. Lagus. 2010. Unsu-
pervised morpheme analysis with Allomorfessor. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, volume 6241 of LNCS, pages
609?616. Springer Berlin / Heidelberg.
S. Virpioja, V. Turunen, S. Spiegler, O. Kohonen, and
M. Kurimo. 2011. Empirical comparison of evalua-
tion methods for unsupervised learning of morphol-
ogy. TAL, 52(2):45?90.
S. Virpioja, P. Smit, S. Gr?onroos, and M. Kurimo.
2013. Morfessor 2.0: Python implementation and
extensions for Morfessor Baseline. Report 25/2013
in Aalto University publication series SCIENCE +
TECHNOLOGY, Aalto University, Finland.
24
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74?78,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Accelerated Estimation of Conditional Random Fields using a
Pseudo-Likelihood-inspired Perceptron Variant
Teemu Ruokolainen
a
Miikka Silfverberg
b
Mikko Kurimo
a
Krister Lind?n
b
a
Department of Signal Processing and Acoustics, Aalto University, firstname.lastname@aalto.fi
b
Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi
Abstract
We discuss a simple estimation approach
for conditional random fields (CRFs). The
approach is derived heuristically by defin-
ing a variant of the classic perceptron al-
gorithm in spirit of pseudo-likelihood for
maximum likelihood estimation. The re-
sulting approximative algorithm has a lin-
ear time complexity in the size of the la-
bel set and contains a minimal amount of
tunable hyper-parameters. Consequently,
the algorithm is suitable for learning CRF-
based part-of-speech (POS) taggers in
presence of large POS label sets. We
present experiments on five languages.
Despite its heuristic nature, the algorithm
provides surprisingly competetive accura-
cies and running times against reference
methods.
1 Introduction
The conditional random field (CRF) model (Laf-
ferty et al., 2001) has been successfully applied
to several sequence labeling tasks in natural lan-
guage processing, including part-of-speech (POS)
tagging. In this work, we discuss accelerating the
CRF model estimation in presence of a large num-
ber of labels, say, hundreds or thousands. Large la-
bel sets occur in POS tagging of morphologically
rich languages (Erjavec, 2010; Haverinen et al.,
2013).
CRF training is most commonly associated with
the (conditional) maximum likelihood (ML) crite-
rion employed in the original work of Lafferty et
al. (2001). In this work, we focus on an alternative
training approach using the averaged perceptron
algorithm of Collins (2002). While yielding com-
petitive accuracy (Collins, 2002; Zhang and Clark,
2011), the perceptron algorithm avoids extensive
tuning of hyper-parameters and regularization re-
quired by the stochastic gradient descent algo-
rithm employed in ML estimation (Vishwanathan
et al., 2006). Additionally, while ML and percep-
tron training share an identical time complexity,
the perceptron is in practice faster due to sparser
parameter updates.
Despite its simplicity, running the perceptron al-
gorithm can be tedious in case the data contains
a large number of labels. Previously, this prob-
lem has been addressed using, for example, k-best
beam search (Collins and Roark, 2004; Zhang and
Clark, 2011; Huang et al., 2012) and paralleliza-
tion (McDonald et al., 2010). In this work, we
explore an alternative strategy, in which we mod-
ify the perceptron algorithm in spirit of the classic
pseudo-likelihood approximation for ML estima-
tion (Besag, 1975). The resulting novel algorithm
has linear complexity w.r.t. the label set size and
contains only a single hyper-parameter, namely,
the number of passes taken over the training data
set.
We evaluate the algorithm, referred to as the
pseudo-perceptron, empirically in POS tagging
on five languages. The results suggest that the
approach can yield competitive accuracy com-
pared to perceptron training accelerated using a
violation-fixed 1-best beam search (Collins and
Roark, 2004; Huang et al., 2012) which also pro-
vides a linear time complexity in label set size.
The rest of the paper is as follows. In Section 2,
we describe the pseudo-perceptron algorithm and
discuss related work. In Sections 3 and 4, we
describe our experiment setup and the results, re-
spectively. Conclusions on the work are presented
in Section 5.
2 Methods
2.1 Pseudo-Perceptron Algorithm
The (unnormalized) CRF model for input and
output sequences x = (x
1
, x
2
, . . . , x
|x|
) and
74
y = (y
1
, y
2
, . . . , y
|x|
), respectively, is written as
p (y |x;w) ? exp
(
w ??(y, x)
)
=
|x|
?
i=n
exp
(
w ? ?(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where w denotes the model parameter vector, ?
the vector-valued global feature extracting func-
tion, ? the vector-valued local feature extracting
function, and n the model order. We denote the
tag set as Y . The model parameters w are esti-
mated based on training data, and test instances
are decoded using the Viterbi search (Lafferty et
al., 2001).
Given the model definition (1), the param-
eters w can be estimated in a straightforward
manner using the structured perceptron algo-
rithm (Collins, 2002). The algorithm iterates
over the training set a single instance (x, y) at
a time and updates the parameters according
to the rule w
(i)
= w
(i?1)
+ ??(x, y, z), where
??(x, y, z) for the ith iteration is written as
??(x, y, z) = ?(x, y)??(x, z). The predic-
tion z is obtained as
z = arg max
u?Y(x)
w ??(x, u) (2)
by performing the Viterbi search over
Y(x) = Y ? ? ? ? ? Y , a product of |x| copies
of Y . In case the perceptron algorithm yields
a small number of incorrect predictions on the
training data set, the parameters generalize well
to test instances with a high probability (Collins,
2002).
The time complexity of the Viterbi search is
O(|x| ? |Y|
n+1
). Consequently, running the per-
ceptron algorithm can become tedious if the la-
bel set cardinality |Y| and/or the model order n
is large. In order to speed up learning, we define
a variant of the algorithm in the spirit of pseudo-
likelihood (PL) learning (Besag, 1975). In anal-
ogy to PL, the key idea of the pseudo-perceptron
(PP) algorithm is to obtain the required predictions
over single variables y
i
while fixing the remaining
variables to their true values. In other words, in-
stead of using the Viterbi search to find the z as in
(2), we find a z
?
for each position i ? 1..|x| as
z
?
= arg max
u?Y
?
i
(x)
w ??(x, u) , (3)
with Y
?
i
(x) = {y
1
}?? ? ??{y
i?1
}?Y?{y
i+1
}?
? ? ? ? {y
|x|
}. Subsequent to training, test instances
are decoded in a standard manner using the Viterbi
search.
The appeal of PP is that the time complexity
of search is reduced to O(|x| ? |Y|), i.e., linear
in the number of labels in the label set. On the
other hand, we no longer expect the obtained pa-
rameters to necessarily generalize well to test in-
stances.
1
Consequently, we consider PP a heuris-
tic estimation approach motivated by the rather
well-established success of PL (Kor
?
c and F?rstner,
2008; Sutton and McCallum, 2009).
2
Next, we study yet another heuristic pseudo-
variant of the perceptron algorithm referred to as
the piecewise-pseudo-perceptron (PW-PP). This
algorithm is analogous to the piecewise-pseudo-
likelihood (PW-PL) approximation presented by
Sutton and McCallum (2009). In this variant, the
original graph is first split into smaller, possibly
overlapping subgraphs (pieces). Subsequently, we
apply the PP approximation to the pieces. We em-
ploy the approach coined factor-as-piece by Sut-
ton and McCallum (2009), in which each piece
contains n + 1 consecutive variables, where n is
the CRF model order.
The PW-PP approach is motivated by the results
of Sutton and McCallum (2009) who found PW-
PL to increase stability w.r.t. accuracy compared
to plain PL across tasks. Note that the piecewise
approximation in itself is not interesting in chain-
structured CRFs, as it results in same time com-
plexity as standard estimation. Meanwhile, the
PW-PP algorithm has same time complexity as PP.
2.2 Related work
Previously, impractical running times of percep-
tron learning have been addressed most notably
using the k-best beam search method (Collins and
Roark, 2004; Zhang and Clark, 2011; Huang et
al., 2012). Here, we consider the ?greedy? 1-best
beam search variant most relevant as it shares the
time complexity of the pseudo search. Therefore,
in the experimental section of this work, we com-
pare the PP and 1-best beam search.
We are aware of at least two other learning ap-
proaches inspired by PL, namely, the pseudo-max
and piecewise algorithms of Sontag et al. (2010)
and Alahari et al. (2010), respectively. Com-
pared to these approaches, the PP algorithm pro-
vides a simpler estimation tool as it avoids the
1
We leave formal treatment to future work.
2
Meanwhile, note that pseudo-likelihood is a consistent
estimator (Gidas, 1988; Hyv?rinen, 2006).
75
hyper-parameters involved in the stochastic gradi-
ent descent algorithms as well as the regularization
and margin functions inherent to the approaches of
Alahari et al. (2010) and Sontag et al. (2010). On
the other hand, Sontag et al. (2010) show that the
pseudo-max approach achieves consistency given
certain assumptions on the data generating func-
tion. Meanwhile, as discussed in previous section,
we consider PP a heuristic and do not provide any
generalization guarantees. To our understanding,
Alahari et al. (2010) do not provide generalization
guarantees for their algorithm.
3 Experimental Setup
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The first data set we consider
is the classic Penn Treebank. The complete tree-
bank is divided into 25 sections of newswire text
extracted from the Wall Street Journal. We split
the data into training, development, and test sets
using the sections 0-18, 19-21, and 22-24, accord-
ing to the standardly applied division introduced
by Collins (2002).
Multext-East. The second data we consider is
the multilingual Multext-East (Erjavec, 2010) cor-
pus. The corpus contains the novel 1984 by
George Orwell. From the available seven lan-
guages, we utilize the Czech, Estonian and Ro-
manian sections. Since the data does not have a
standard division to training and test sets, we as-
sign the 9th and 10th from each 10 consecutive
sentences to the development and test sets, respec-
tively. The remaining sentences are assigned to the
training sets.
Turku Dependency Treebank. The third data
we consider is the Finnish Turku Dependency
Treebank (Haverinen et al., 2013). The treebank
contains text from 10 different domains. We use
the same data split strategy as for Multext East.
3.2 Reference Methods
We compare the PP and PW-PP algorithms with
perceptron learning accelerated using 1-best beam
search modified using the early update rule
(Huang et al., 2012). While Huang et al. (2012)
experimented with several violation-fixing meth-
ods (early, latest, maximum, hybrid), they ap-
peared to reach termination at the same rate in
lang. train. dev. test tags train. tags
eng 38,219 5,527 5,462 45 45
rom 5,216 652 652 405 391
est 5,183 648 647 413 408
cze 5,402 675 675 955 908
fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
POS tagging. Our preliminary experiments using
the latest violation updates supported this. Conse-
quently, we employ the early updates.
We also provide results using the CRFsuite
toolkit (Okazaki, 2007), which implements a 1st-
order CRF model. To best of our knowledge,
CRFsuite is currently the fastest freely available
CRF implementation.
3
In addition to the averaged
perceptron algorithm (Collins, 2002), the toolkit
implements several training procedures (Nocedal,
1980; Crammer et al., 2006; Andrew and Gao,
2007; Mejer and Crammer, 2010; Shalev-Shwartz
et al., 2011). We run CRFsuite using these algo-
rithms employing their default parameters and the
feature extraction scheme and stopping criterion
described in Section 3.3. We then report results
provided by the most accurate algorithm on each
language.
3.3 Details on CRF Training and Decoding
While the methods discussed in this work are ap-
plicable for nth-order CRFs, we employ 1st-order
CRFs in order to avoid overfitting the relatively
small training sets.
We employ a simple feature set including word
forms at position t? 2, . . . , t+ 2, suffixes of word
at position t up to four letters, and three ortho-
graphic features indicating if the word at position
t contains a hyphen, capital letter, or a digit.
All the perceptron variants (PP, PW-PP, 1-best
beam search) initialize the model parameters with
zero vectors and process the training instances in
the order they appear in the corpus. At the end
of each pass, we apply the CRFs using the latest
averaged parameters (Collins, 2002) to the devel-
opment set. We assume the algorithms have con-
verged when the model accuracy on development
3
See benchmark results at http://www.chokkan.
org/software/crfsuite/benchmark.html
76
has not increased during last three iterations. Af-
ter termination, we apply the averaged parameters
yielding highest performance on the development
set to test instances.
Test and development instances are decoded us-
ing a combination of Viterbi search and the tag
dictionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer. We use our own C++-based implemen-
tation of the methods discussed in Section 2.
4 Results
The obtained training times and test set accuracies
(measured using accuracy and out-of-vocabulary
(OOV) accuracy) are presented in Table 2. The
training CPU times include the time (in minutes)
consumed by running the perceptron algorithm
variants as well as evaluation of the development
set accuracy. The column labeled it. corresponds
to the number of passes over training set made by
the algorithms before termination.
We summarize the results as follows. First, PW-
PP provided higher accuracies compared to PP on
Romanian, Czech, and Finnish. The differences
were statistically significant
4
on Czech. Second,
while yielding similar running times compared
to 1-best beam search, PW-PP provided higher
accuracies on all languages apart from Finnish.
The differences were significant on Estonian and
Czech. Third, while fastest on the Penn Treebank,
the CRFsuite toolkit became substantially slower
compared to PW-PP when the number of labels
were increased (see Czech and Finnish). The dif-
ferences in accuracies between the best perform-
ing CRFsuite algorithm and PP and PW-PP were
significant on Czech.
5 Conclusions
We presented a heuristic perceptron variant for
estimation of CRFs in the spirit of the classic
4
We establish significance (with confidence level 0.95)
using the standard 1-sided Wilcoxon signed-rank test per-
formed on 10 randomly divided, non-overlapping subsets of
the complete test sets.
method it. time (min) acc. OOV
English
PP 9 6 96.99 87.97
PW-PP 10 7 96.98 88.11
1-best beam 17 8 96.91 88.33
Pas.-Agg. 9 1 97.01 88.68
Romanian
PP 9 8 96.81 83.66
PW-PP 8 7 96.91 84.38
1-best beam 17 10 96.88 85.32
Pas.-Agg. 13 9 97.06 84.69
Estonian
PP 10 8 93.39 78.10
PW-PP 8 6 93.35 78.66
1-best beam 23 15 92.95 75.65
Pas.-Agg. 15 12 93.27 77.63
Czech
PP 11 26 89.37 70.67
PW-PP 16 41 89.84 72.52
1-best beam 14 19 88.95 70.90
Pegasos 15 341 90.42 72.59
Finnish
PP 11 58 87.09 58.58
PW-PP 11 56 87.16 58.50
1-best beam 21 94 87.38 59.29
Pas.-Agg. 16 693 87.17 57.58
Table 2: Results. We report CRFsuite results pro-
vided by most accurate algorithm on each lan-
guage: the Pas.-Agg. and Pegasos refer to the al-
gorithms of Crammer et al. (2006) and Shalev-
Shwartz et al. (2011), respectively.
pseudo-likelihood estimator. The resulting ap-
proximative algorithm has a linear time complex-
ity in the label set cardinality and contains only
a single hyper-parameter, namely, the number of
passes taken over the training data set. We eval-
uated the algorithm in POS tagging on five lan-
guages. Despite its heuristic nature, the algo-
rithm provided competetive accuracies and run-
ning times against reference methods.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language stud-
ies) and the Academy of Finland under the grant
no 251170 (Finnish Centre of Excellence Pro-
gram (2012-2017)). We would like to thank Dr.
Onur Dikmen for the helpful discussions during
the work.
77
References
Karteek Alahari, Chris Russell, and Philip H.S. Torr.
2010. Efficient piecewise learning for conditional
random fields. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on,
pages 895?901.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th international conference on Ma-
chine learning, pages 33?40.
Julian Besag. 1975. Statistical analysis of non-lattice
data. The statistician, pages 179?195.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551?585.
Toma? Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Basilis Gidas. 1988. Consistency of maximum like-
lihood and pseudo-likelihood estimators for Gibbs
distributions. In Stochastic differential systems,
stochastic control theory and applications, pages
129?145.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Aapo Hyv?rinen. 2006. Consistency of pseudolike-
lihood estimation of fully visible Boltzmann ma-
chines. Neural Computation, 18(10):2283?2292.
Filip Kor
?
c and Wolfgang F?rstner. 2008. Approximate
parameter learning in conditional random fields: An
empirical investigation. Pattern Recognition, pages
11?20.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 conference on
empirical methods in natural language processing,
pages 971?981.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of computa-
tion, 35(151):773?782.
Naoaki Okazaki. 2007. CRFsuite: a fast implemen-
tation of conditional random fields (CRFs). URL
http://www.chokkan.org/software/crfsuite.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natural
language processing, volume 1, pages 133?142.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal esti-
mated sub-gradient solver for SVM. Mathematical
Programming, 127(1):3?30.
David Sontag, Ofer Meshi, Tommi Jaakkola, and Amir
Globerson. 2010. More data means less inference:
A pseudo-max approach to structured learning. In
Advances in Neural Information Processing Systems
23, pages 2181?2189.
Charles Sutton and Andrew McCallum. 2009. Piece-
wise training for structured prediction. Machine
learning, 77(2):165?194.
S.V.N. Vishwanathan, Nicol Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23rd in-
ternational conference on Machine learning, pages
969?976.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
78
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Painless Semi-Supervised Morphological Segmentation using Conditional
Random Fields
Teemu Ruokolainen
a
Oskar Kohonen
b
Sami Virpioja
b
Mikko Kurimo
a
a
Department of Signal Processing and Acoustics, Aalto University
b
Department of Information and Computer Science, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, that is the surface
forms of morphemes. We extend a re-
cent segmentation approach based on con-
ditional random fields from purely super-
vised to semi-supervised learning by ex-
ploiting available unsupervised segmenta-
tion techniques. We integrate the unsu-
pervised techniques into the conditional
random field model via feature set aug-
mentation. Experiments on three di-
verse languages show that this straight-
forward semi-supervised extension greatly
improves the segmentation accuracy of the
purely supervised CRFs in a computation-
ally efficient manner.
1 Introduction
We discuss data-driven morphological segmenta-
tion, in which word forms are segmented into
morphs, the surface forms of morphemes. This
type of morphological analysis can be useful for
alleviating language model sparsity inherent to
morphologically rich languages (Hirsim?ki et al.,
2006; Creutz et al., 2007; Turunen and Kurimo,
2011; Luong et al., 2013). Particularly, we focus
on a low-resource learning setting, in which only
a small amount of annotated word forms are avail-
able for model training, while unannotated word
forms are available in abundance.
We study morphological segmentation using
conditional random fields (CRFs), a discrimina-
tive model for sequential tagging and segmenta-
tion (Lafferty et al., 2001). Recently, Ruoko-
lainen et al. (2013) showed that the CRFs can
yield competitive segmentation accuracy com-
pared to more complex, previous state-of-the-
art techniques. While CRFs yielded generally
the highest accuracy compared to their reference
methods (Poon et al., 2009; Kohonen et al., 2010),
on the smallest considered annotated data sets of
100 word forms, they were outperformed by the
semi-supervised Morfessor algorithm (Kohonen et
al., 2010). However, Ruokolainen et al. (2013)
trained the CRFs solely on the annotated data,
without any use of the available unannotated data.
In this work, we extend the CRF-based ap-
proach to leverage unannotated data in a straight-
forward and computationally efficient manner via
feature set augmentation, utilizing predictions of
unsupervised segmentation algorithms. Experi-
ments on three diverse languages show that the
semi-supervised extension substantially improves
the segmentation accuracy of the CRFs. The ex-
tension also provides higher accuracies on all the
considered data set sizes and languages compared
to the semi-supervised Morfessor (Kohonen et al.,
2010).
In addition to feature set augmentation, there
exists numerous approaches for semi-supervised
CRF model estimation, exemplified by minimum
entropy regularization (Jiao et al., 2006), gen-
eralized expectations criteria (Mann and McCal-
lum, 2008), and posterior regularization (He et al.,
2013). In this work, we employ the feature-based
approach due to its simplicity and the availabil-
ity of useful unsupervised segmentation methods.
Varying feature set augmentation approaches have
been successfully applied in several related tasks,
such as Chinese word segmentation (Wang et al.,
2011; Sun and Xu, 2011) and chunking (Turian et
al., 2010).
The paper is organized as follows. In Section 2,
we describe the CRF-based morphological seg-
mentation approach following (Ruokolainen et al.,
2013), and then show how to extend this approach
to leverage unannotated data in an efficient man-
ner. Our experimental setup and results are dis-
cussed in Sections 3 and 4, respectively. Finally,
84
we present conclusions on the work in Section 5.
2 Methods
2.1 Supervised Morphological Segmentation
using CRFs
We present the morphological segmentation task
as a sequential labeling problem by assigning each
character to one of three classes, namely {be-
ginning of a multi-character morph (B), middle
of a multi-character morph (M), single character
morph (S)}. We then perform the sequential label-
ing using linear-chain CRFs (Lafferty et al., 2001).
Formally, the linear-chain CRF model distribu-
tion for label sequence y = (y
1
, y
2
, . . . , y
T
) and
a word form x = (x
1
, x
2
, . . . , x
T
) is written as a
conditional probability
p (y |x;w) ?
T
?
t=2
exp
(
w ? ?(y
t?1
, y
t
, x, t)
)
,
(1)
where t indexes the character positions,w denotes
the model parameter vector, and ? the vector-
valued feature extracting function. The model pa-
rameters w are estimated discrimatively based on
a training set of exemplar input-output pairs (x, y)
using, for example, the averaged perceptron algo-
rithm (Collins, 2002). Subsequent to estimation,
the CRF model segments test word forms using
the Viterbi algorithm (Lafferty et al., 2001).
We next describe the feature set
{?
i
(y
t?1
, y
t
, x, t)}
|?|
i=1
by defining emission
and transition features. Denoting the label set {B,
M, S} as Y , the emission feature set is defined as
{?
m
(x, t)1(y
t
= y
?
t
) |m ? 1..M ,?y
?
t
? Y} ,
(2)
where the indicator function 1(y
t
= y
?
t
) returns
one if and only if y
t
= y
?
t
and zero otherwise, that
is
1(y
t
= y
?
t
) =
{
1 if y
t
= y
?
t
0 otherwise
, (3)
and {?
m
(x, t)}
M
m=1
is the set of functions describ-
ing the character position t. Following Ruoko-
lainen et al. (2013), we employ binary functions
that describe the position t of word x using all left
and right substrings up to a maximum length ?.
The maximum substring length ?
max
is considered
a hyper-parameter to be adjusted using a develop-
ment set. While the emission features associate
the input to labels, the transition feature set
{1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) | y
?
t
, y
?
t?1
? Y} (4)
captures the dependencies between adjacent labels
as irrespective of the input x.
2.2 Leveraging Unannotated Data
In order to utilize unannotated data, we explore a
straightforward approach based on feature set aug-
mentation. We exploit predictions of unsupervised
segmentation algorithms by defining variants of
the features described in Section 2.1. The idea is
to compensate the weaknesses of the CRF model
trained on the small annotated data set using the
strengths of the unsupervised methods that learn
from large amounts of unannotated data.
For example, consider utilizing predictions of
the unsupervised Morfessor algorithm (Creutz and
Lagus, 2007) in the CRF model. In order to ac-
complish this, we first learn the Morfessor model
from the unannotated training data, and then ap-
ply the learned model on the word forms in the
annotated training set. Assuming the annotated
training data includes the English word drivers,
the Morfessor algorithm might, for instance, re-
turn a (partially correct) segmentation driv + ers.
We present this segmentation by defining a func-
tion ?(t), which returns 0 or 1, if the position t is
in the middle of a segment or in the beginning of a
segment, respectively, as in
t 1 2 3 4 5 6 7
x
t
d r i v e r s
?(t) 1 0 0 0 1 0 0
Now, given a set of U functions {?
u
(t)}
U
u=1
, we
define variants of the emission features in (2) as
{?
u
(x, t)?
m
(x, t)1(y
t
= y
?
t
) |
?u ? 1..U ,?m ? 1..M ,?y
?
t
? Y} . (5)
By adding the expanded features of form (5), the
CRF model learns to associate the output of the
unsupervised algorithms in relation to the sur-
rounding substring context. Similarly, an ex-
panded transition feature is written as
{?
u
(x, t)1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) |
?u ? 1..U ,?y
?
t
, y
?
t?1
? Y} . (6)
After defining the augmented feature set, the
CRF model parameters can be estimated in a stan-
dard manner on the small, annotated training data
set. Subsequent to CRF training, the Morfessor
model is applied on the test instances in order to
allow the feature set augmentation and standard
decoding with the estimated CRF model. We ex-
pect the Morfessor features to specifically improve
85
segmentation of compound words (for example,
brain+storm), which are modeled with high ac-
curacy by the unsupervised Morfessor algorithm
(Creutz and Lagus, 2007), but can not be learned
from the small number of annotated examples
available for the supervised CRF training.
As another example of a means to augment the
feature set, we make use of the fact that the output
of the unsupervised algorithms does not have to be
binary (zeros and ones). To this end, we employ
the classic letter successor variety (LSV) scores
presented originally by (Harris, 1955).
1
The LSV
scores utilize the insight that the predictability of
successive letters should be high within morph
segments, and low at the boundaries. Conse-
quently, a high variety of letters following a prefix
indicates a high probability of a boundary. We use
a variant of the LSV values presented by ??ltekin
(2010), in which we first normalize the scores by
the average score at each position t, and subse-
qently logarithmize the normalized value. While
LSV score tracks predictability given prefixes, the
same idea can be utilized for suffixes, providing
the letter predecessor variety (LPV). Subsequent
to augmenting the feature set using the functions
LSV (t) and LPV (t), the CRF model learns to
associate high successor and predecessor values
(low predictability) to high probability of a seg-
ment boundary. Appealingly, the Harris features
can be obtained in a computationally inexpensive
manner, as they merely require counting statistics
from the unannotated data.
The feature set augmentation approach de-
scribed above is computationally efficient, if the
computational overhead from the unsupervised
methods is small. This is because the CRF param-
eter estimation is still based on the small amount
of labeled examples as described in Section 2.1,
while the number of features incorporated in the
CRF model (equal to the number of parameters)
grows linearly in the number of exploited unsu-
pervised algorithms.
3 Experimental Setup
3.1 Data
We perform the experiments on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al., 2009; Ku-
1
We also experimented on modifying the output of the
Morfessor algorithm from binary to probabilistic, but these
soft cues provided no consistent advantage over the standard
binary output.
English Finnish Turkish
Train (unann.) 384,903 2,206,719 617,298
Train (ann.) 1,000 1,000 1,000
Devel. 694 835 763
Test 10,000 10,000 10,000
Table 1: Number of word types in the Morpho
Challenge data set.
rimo et al., 2010) consisting of manually prepared
morphological segmentations in English, Finnish
and Turkish. We follow the experiment setup, in-
cluding data partitions and evaluation metrics, de-
scribed by Ruokolainen et al. (2013). Table 1
shows the total number of instances available for
model estimation and testing.
3.2 CRF Feature Extraction and Training
The substring features included in the CRF model
are described in Section 2.1. We include all sub-
strings which occur in the training data. The Mor-
fessor and Harris (successor and predecessor va-
riety) features employed by the semi-supervised
extension are described in Section 2.2. We ex-
perimented on two variants of the Morfessor al-
gorithm, namely, the Morfessor Baseline (Creutz
and Lagus, 2002) and Morfessor Categories-MAP
(Creutz and Lagus, 2005), CatMAP for short. The
Baseline models were trained on word types and
the perplexity thresholds of the CatMAP models
were set equivalently to the reference runs in Mor-
pho Challenge 2010 (English: 450, Finnish: 250,
Turkish: 100); otherwise the default parameters
were used. The Harris features do not require any
hyper-parameters.
The CRF model (supervised and semi-
supervised) is trained using the averaged
perceptron algorithm (Collins, 2002). The num-
ber of passes over the training set made by the
perceptron algorithm, and the maximum length of
substring features are optimized on the held-out
development sets.
The experiments are run on a standard desktop
computer using a Python-based single-threaded
CRF implementation. For Morfessor Baseline, we
use the recently published implementation by Vir-
pioja et al. (2013). For Morfessor CatMAP, we
used the Perl implementation by Creutz and La-
gus (2005).
86
3.3 Reference Methods
We compare our method?s performance with
the fully supervised CRF model and the semi-
supervised Morfessor algorithm (Kohonen et al.,
2010). For semi-supervised Morfessor, we use the
Python implementation by Virpioja et al. (2013).
4 Results
Segmentation accuracies for all languages are pre-
sented in Table 2. The columns titled Train (ann.)
and Train (unann.) denote the number of anno-
tated and unannotated training instances utilized
by the method, respectively. To summarize, the
semi-supervised CRF extension greatly improved
the segmentation accuracy of the purely super-
vised CRFs, and also provided higher accuracies
compared to the semi-supervised Morfessor algo-
rithm
2
.
Appealingly, the semi-supervised CRF exten-
sion already provided consistent improvement
over the supervised CRFs, when utilizing the com-
putationally inexpensive Harris features. Addi-
tional gains were then obtained using the Morfes-
sor features. On all languages, highest accuracies
were obtained using a combination of Harris and
CatMAP features.
Running the CRF parameter estimation (includ-
ing hyper-parameters) consumed typically up to a
few minutes. Computing statistics for the Harris
features also took up roughly a few minutes on
all languages. Learning the unsupervised Mor-
fessor algorithm consumed 3, 47, and 20 min-
utes for English, Finnish, and Turkish, respec-
tively. Meanwhile, CatMAP model estimation
was considerably slower, consuming roughly 10,
50, and 7 hours for English, Finnish and Turkish,
respectively. Training and decoding with semi-
supervised Morfessor took 21, 111, and 47 hours
for English, Finnish and Turkish, respectively.
5 Conclusions
We extended a recent morphological segmenta-
tion approach based on CRFs from purely super-
vised to semi-supervised learning. We accom-
plished this in an efficient manner using feature set
augmentation and available unsupervised segmen-
tation techniques. Experiments on three diverse
2
The improvements over the supervised CRFs and semi-
supervised Morfessor were statistically significant (confi-
dence level 0.95) according to the standard 1-sided Wilcoxon
signed-rank test performed on 10 randomly divided, non-
overlapping subsets of the complete test sets.
Method Train (ann.) Train (unann.) F1
English
CRF 100 0 78.8
S-MORF. 100 384,903 83.7
CRF (Harris) 100 384,903 80.9
CRF (BL+Harris) 100 384,903 82.6
CRF (CM+Harris) 100 384,903 84.4
CRF 1,000 0 85.9
S-MORF. 1,000 384,903 84.3
CRF (Harris) 1,000 384,903 87.6
CRF (BL+Harris) 1,000 384,903 87.9
CRF (CM+Harris) 1,000 384,903 88.4
Finnish
CRF 100 0 65.5
S-MORF. 100 2,206,719 70.4
CRF (Harris) 100 2,206,719 78.9
CRF (BL+Harris) 100 2,206,719 79.3
CRF (CM+Harris) 100 2,206,719 82.0
CRF 1,000 0 83.8
S-MORF. 1,000 2,206,719 76.4
CRF (Harris) 1,000 2,206,719 88.3
CRF (BL+Harris) 1,000 2,206,719 88.9
CRF (CM+Harris) 1,000 2,206,719 89.4
Turkish
CRF 100 0 77.7
S-MORF. 100 617,298 78.2
CRF (Harris) 100 617,298 82.6
CRF (BL+Harris) 100 617,298 84.9
CRF (CM+Harris) 100 617,298 85.5
CRF 1,000 0 88.6
S-MORF. 1,000 617,298 87.0
CRF (Harris) 1,000 617,298 90.1
CRF (BL+Harris) 1,000 617,298 91.7
CRF (CM+Harris) 1,000 617,298 91.8
Table 2: Results on test data. CRF (BL+Harris)
denotes semi-supervised CRF extension using
Morfessor Baseline and Harris features, while
CRF (CM+Harris) denotes CRF extension em-
ploying Morfessor CatMAP and Harris features.
languages showed that this straightforward semi-
supervised extension greatly improves the seg-
mentation accuracy of the supervised CRFs, while
being computationally efficient. The extension
also outperformed the semi-supervised Morfessor
algorithm on all data set sizes and languages.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
87
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8. Association for Computational
Linguistics.
?agr? ??ltekin. 2010. Improving successor variety
for morphological segmentation. In Proceedings of
the 20th Meeting of Computational Linguistics in the
Netherlands.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 21?30,
Philadelphia, PA, USA, July. Association for Com-
putational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Timo Honkela, Ville K?n?nen,
Matti P?ll?, and Olli Simula, editors, Proceedings of
AKRR?05, International and Interdisciplinary Con-
ference on Adaptive Knowledge Representation and
Reasoning, pages 106?113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of
Computer and Information Science.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3:1?3:34, January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo,
Antti Puurula, Janne Pylkk?nen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Sara?lar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):3:1?3:29, December.
Zellig Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 38?46,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541,
October.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 209?216. Association for Computational Lin-
guistics.
Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology, pages 78?86, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Mikko Kurimo, Sami Virpioja, Ville Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Mikko Kurimo, Sami Virpioja, and Ville Turunen.
2010. Overview and results of Morpho Chal-
lenge 2010. In Proceedings of the Morpho Chal-
lenge 2010 Workshop, pages 7?24, Espoo, Finland,
September. Aalto University School of Science and
Technology, Department of Information and Com-
puter Science. Technical Report TKK-ICS-R37.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Carla E. Brodley and Andrea Po-
horeckyj Danyluk, editors, Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282?289, Williamstown, MA, USA. Mor-
gan Kaufmann.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 29?37. Association for Computa-
tional Linguistics, August.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878. Association for
Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 209?217.
Association for Computational Linguistics.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
88
the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 29?37. As-
sociation for Computational Linguistics, August.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970?979. As-
sociation for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Ville Turunen and Mikko Kurimo. 2011. Speech re-
trieval from unsegmented Finnish audio using statis-
tical morpheme-like units for segmentation, recog-
nition, and retrieval. ACM Transactions on Speech
and Language Processing, 8(1):1:1?1:25, October.
Sami Virpioja, Peter Smit, Stig-Arne Gr?nroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python im-
plementation and extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication se-
ries SCIENCE + TECHNOLOGY, Department of
Signal Processing and Acoustics, Aalto University.
Yiou Wang, Yoshimasa Tsuruoka Jun?ichi Kazama,
Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,
and Kentaro Torisawa. 2011. Improving Chinese
word segmentation and POS tagging with semi-
supervised methods using large auto-analyzed data.
In IJCNLP, pages 309?317.
89
Proceedings of the ACL 2010 Conference Short Papers, pages 301?306,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Domain Adaptation of Maximum Entropy Language Models
Tanel Aluma?e?
Adaptive Informatics Research Centre
School of Science and Technology
Aalto University
Helsinki, Finland
tanel@cis.hut.fi
Mikko Kurimo
Adaptive Informatics Research Centre
School of Science and Technology
Aalto University
Helsinki, Finland
Mikko.Kurimo@tkk.fi
Abstract
We investigate a recently proposed
Bayesian adaptation method for building
style-adapted maximum entropy language
models for speech recognition, given a
large corpus of written language data
and a small corpus of speech transcripts.
Experiments show that the method con-
sistently outperforms linear interpolation
which is typically used in such cases.
1 Introduction
In large vocabulary speech recognition, a language
model (LM) is typically estimated from large
amounts of written text data. However, recogni-
tion is typically applied to speech that is stylisti-
cally different from written language. For exam-
ple, in an often-tried setting, speech recognition is
applied to broadcast news, that includes introduc-
tory segments, conversations and spontaneous in-
terviews. To decrease the mismatch between train-
ing and test data, often a small amount of speech
data is human-transcribed. A LM is then built
by interpolating the models estimated from large
corpus of written language and the small corpus
of transcribed data. However, in practice, differ-
ent models might be of different importance de-
pending on the word context. Global interpola-
tion doesn?t take such variability into account and
all predictions are weighted across models identi-
cally, regardless of the context.
In this paper we investigate a recently proposed
Bayesian adaptation approach (Daume III, 2007;
Finkel and Manning, 2009) for adapting a con-
ditional maximum entropy (ME) LM (Rosenfeld,
1996) to a new domain, given a large corpus of
out-of-domain training data and a small corpus
of in-domain data. The main contribution of this
?Currently with Tallinn University of Technology, Esto-
nia
paper is that we show how the suggested hierar-
chical adaptation can be used with suitable pri-
ors and combined with the class-based speedup
technique (Goodman, 2001) to adapt ME LMs
in large-vocabulary speech recognition when the
amount of target data is small. The results outper-
form the conventional linear interpolation of back-
ground and target models in both N -grams and
ME models. It seems that with the adapted ME
models, the same recognition accuracy for the tar-
get evaluation data can be obtained with 50% less
adaptation data than in interpolated ME models.
2 Review of Conditional Maximum
Entropy Language Models
Maximum entropy (ME) modeling is a framework
that has been used in a wide area of natural lan-
guage processing (NLP) tasks. A conditional ME
model has the following form:
P (x|h) =
e
?
i ?ifi(x,h)
?
x? e
?
j ?jfj(x
?,h)
(1)
where x is an outcome (in case of a LM, a word),
h is a context (the word history), and x? a set of all
possible outcomes (words). The functions fi are
(typically binary) feature functions. During ME
training, the optimal weights ?i corresponding to
features fi(x, h) are learned. More precisely, find-
ing the ME model is equal to finding weights that
maximize the log-likelihood L(X; ?) of the train-
ing data X . The weights are learned via improved
iterative scaling algorithm or some of its modern
fast counterparts (i.e., conjugate gradient descent).
Since LMs typically have a vocabulary of tens
of thousands of words, the use of a normalization
factor over all possible outcomes makes estimat-
ing a ME LM very memory and time consuming.
Goodman (2001) proposed a class-based method
that drastically reduces the resource requirements
for training such models. The idea is to cluster
301
words in the vocabulary into classes (e.g., based
on their distributional similarity). Then, we can
decompose the prediction of a word given its his-
tory into prediction of its class given the history,
and prediction of the word given the history and
its class :
P (w|h) = P (C(w)|h)?P (w|h,C(w)) (2)
Using such decomposition, we can create two ME
models: one corresponding to P (C(w)|h) and the
other corresponding to P (w|h,C(w)). It is easy to
see that computing the normalization factor of the
first component model now requires only looping
over all classes. It turns out that normalizing the
second model is also easier: for a context h,C(w),
we only need to normalize over words that belong
to class C(w), since other words cannot occur in
this context. This decomposition can be further
extended by using hierarchical classes.
To avoid overfitting, ME models are usually
smoothed (regularized). The most widely used
smoothing method for ME LMs is Gaussian pri-
ors (Chen and Rosenfeld, 2000): a zero-mean
prior with a given variance is added to all feature
weights, and the model optimization criteria be-
comes:
L?(X; ?) = L(X; ?)?
F?
i=1
?2i
2?2i
(3)
where F is the number of feature functions. Typi-
cally, a fixed hyperparameter ?i = ? is used for
all parameters. The optimal variance is usually
estimated on a development set. Intuitively, this
method encourages feature weights to be smaller,
by penalizing weights with big absolute values.
3 Domain Adaptation of Maximum
Entropy Models
Recently, a hierarchical Bayesian adaptation
method was proposed that can be applied to a large
family of discriminative learning tasks (such as
ME models, SVMs) (Daume III, 2007; Finkel and
Manning, 2009). In NLP problems, data often
comes from different sources (e.g., newspapers,
web, textbooks, speech transcriptions). There are
three classic approaches for building models from
multiple sources. We can pool all training data and
estimate a single model, and apply it for all tasks.
The second approach is to ?unpool? the data, i.e,
only use training data from the test domain. The
third and often the best performing approach is to
train separate models for each data source, apply
them to test data and interpolate the results.
The hierarchical Bayesian adaptation method
is a generalization of the three approaches de-
scribed above. The hierarchical model jointly
optimizes global and domain-specific parameters,
using parameters built from pooled data as priors
for domain-specific parameters. In other words,
instead of using smoothing to encourage param-
eters to be closer to zero, it encourages domain-
specific model parameters to be closer to the
corresponding global parameters, while a zero
mean Gaussian prior is still applied for global pa-
rameters. For processing test data during run-
time, the domain-specific model is applied. Intu-
itively, this approach can be described as follows:
the domain-specific parameters are largely deter-
mined by global data, unless there is good domain-
specific evidence that they should be different.
The key to this approach is that the global and
domain-specific parameters are learned jointly, not
hierarchically. This allows domain-specific pa-
rameters to influence the global parameters, and
vice versa. Formally, the joint optimization crite-
ria becomes:
Lhier(X; ?) =
?
d
(
Lorig(Xd,?d)?
F?
i=1
(?d,i ? ??,i)2
2?2d
)
?
F?
i=1
?2?,i
2?2?
(4)
where Xd is data for domain d, ??,i the global
parameters, ?d,i the domain-specific parameters,
?2? the global variance and ?
2
d the domain-specific
variances. The global and domain-specific vari-
ances are optimized on the heldout data. Usually,
larger values are used for global parameters and
for domains with more data, while for domains
with less data, the variance is typically set to be
smaller, encouraging the domain-specific parame-
ters to be closer to global values.
This adaptation scheme is very similar to the ap-
proaches proposed by (Chelba and Acero, 2006)
and (Chen, 2009b): both use a model estimated
from background data as a prior when learning
a model from in-domain data. The main differ-
ence is the fact that in this method, the models are
estimated jointly while in the other works, back-
302
ground model has to be estimated before learning
the in-domain model.
4 Experiments
In this section, we look at experimental results
over two speech recognition tasks.
4.1 Tasks
Task 1: English Broadcast News. This recog-
nition task consists of the English broadcast news
section of the 2003 NIST Rich Transcription Eval-
uation Data. The data includes six news record-
ings from six different sources with a total length
of 176 minutes.
As acoustic models, the CMU Sphinx open
source triphone HUB4 models for wideband
(16kHz) speech1 were used. The models have
been trained using 140 hours of speech.
For training the LMs, two sources were used:
first 5M sentences from the Gigaword (2nd ed.)
corpus (99.5M words), and broadcast news tran-
scriptions from the TDT4 corpus (1.19M words).
The latter was treated as in-domain data in the
adaptation experiments. A vocabulary of 26K
words was used. It is a subset of a bigger 60K
vocabulary, and only includes words that occurred
in the training data. The OOV rate against the test
set was 2.4%.
The audio used for testing was segmented
into parts of up to 20 seconds in length.
Speaker diarization was applied using the
LIUM SpkDiarization toolkit (Dele?glise et al,
2005). The CMU Sphinx 3.7 was used for
decoding. A three-pass recognition strategy was
applied: the first pass recognition hypotheses
were used for calculating MLLR-adapted models
for each speaker. In the second pass, the adapted
acoustic models were used for generating a
5000-best list of hypotheses for each segment. In
the third pass, the ME LM was used to re-rank the
hypotheses and select the best one. During decod-
ing, a trigram LM model was used. The trigram
model was an interpolation of source-specific
models which were estimated using Kneser-Ney
discounting.
Task 2: Estonian Broadcast Conversations.
The second recognition task consists of four
recordings from different live talk programs from
1http://www.speech.cs.cmu.edu/sphinx/
models/
three Estonian radio stations. Their format con-
sists of hosts and invited guests, spontaneously
discussing current affairs. There are 40 minutes
of transcriptions, with 11 different speakers.
The acoustic models were trained on various
wideband Estonian speech corpora: the BABEL
speech database (9h), transcriptions of Estonian
broadcast news (7.5h) and transcriptions of radio
live talk programs (10h). The models are triphone
HMMs, using MFCC features.
For training the LMs, two sources were used:
about 10M sentences from various Estonian news-
papers, and manual transcriptions of 10 hours of
live talk programs from three Estonian radio sta-
tions. The latter is identical in style to the test data,
although it originates from a different time period
and covers a wider variety of programs, and was
treated as in-domain data.
As Estonian is a highly inflective language,
morphemes are used as basic units in the LM.
We use a morphological analyzer (Kaalep and
Vaino, 2001) for splitting the words into mor-
phemes. After such processing, the newspaper
corpus includes of 185M tokens, and the tran-
scribed data 104K tokens. A vocabulary of 30K
tokens was used for this task, with an OOV rate
of 1.7% against the test data. After recognition,
morphemes were concatenated back to words.
As with English data, a three-pass recognition
strategy involving MLLR adaptation was applied.
4.2 Results
For both tasks, we rescored the N-best lists in
two different ways: (1) using linear interpolation
of source-specific ME models and (2) using hi-
erarchically domain-adapted ME model (as de-
scribed in previous chapter). The English ME
models had a three-level and Estonian models a
four-level class hierarchy. The classes were de-
rived using the word exchange algorithm (Kneser
and Ney, 1993). The number of classes at each
level was determined experimentally so as to op-
timize the resource requirements for training ME
models (specifically, the number of classes was
150, 1000 and 5000 for the English models and
20, 150, 1000 and 6000 for the Estonian models).
We used unigram, bigram and trigram features that
occurred at least twice in the training data. The
feature cut-off was applied in order to accommo-
date the memory requirements. The feature set
was identical for interpolated and adapted models.
303
Interp. models Adapted models
Adapta-
tion data
(No of
words)
?2OD ?
2
ID ?
2
? ?
2
OD ?
2
ID
English Broadcast News
147K 2e8 3e5 5e7 2e7 2e6
292K 2e8 5e5 5e7 2e7 2e6
591K 2e8 1e6 5e7 2e7 2e6
1119K 2e8 2e6 5e7 2e7 5e6
Estonian Broadcast Conversations
104K 5e8 3e5 5e7 1e7 2e6
Table 1: The unnormalized values of Gaus-
sian prior variances for interpolated out-of-domain
(OD) and in-domain (ID) ME models, and hierar-
chically adapted global (*), out-of-odomain (OD)
and in-domain (ID) models that were used in the
experiments.
For the English task, we also explored the ef-
ficiency of these two approaches with varying
size of adaptation data: we repeated the exper-
iments when using one eighth, one quarter, half
and all of the TDT4 transcription data for interpo-
lation/adaptation. The amount of used Gigaword
data was not changed. In all cases, interpolation
weights were re-optimized and new Gaussian vari-
ance values were heuristically determined.
The TADM toolkit2 was used for estimating ME
models, utilizing its implementation of the conju-
gate gradient algorithm.
The models were regularized using Gaussian
priors. The variance parameters were chosen
heuristically based on light tuning on develop-
ment set perplexity. For the source-specific ME
models, the variance was fixed on per-model ba-
sis. For the adapted model, that jointly models
global and domain-specific data, the Gaussian pri-
ors were fixed for each hierarchy node (i.e., the
variance was fixed across global, out-of-domain,
and in-domain parameters). Table 1 lists values
for the variances of Gaussian priors (as in equa-
tions 3 and 4) that we used in the experiments. In
other publications, the variance values are often
normalized to the size of the data. We chose not
to normalize the values, since in the hierarchical
adaptation scheme, also data from other domains
have impact on the learned model parameters, thus
2http://tadm.sourceforge.net/
it?s not possible to simply normalize the variances.
The experimental results are presented in Table
2. Perplexity and word error rate (WER) results of
the interpolated and adapted models are compared.
For the Estonian task, letter error rate (LER) is
also reported, since it tends to be a more indicative
measure of speech recognition quality for highly
inflected languages. In all experiments, using the
adapted models resulted in lower perplexity and
lower error rate. Improvements in the English ex-
periment were less evident than in the Estonian
system, with under 10% improvement in perplex-
ity and 1-3% in WER, against 15% and 4% for the
Estonian experiment. In most cases, there was a
significant improvement in WER when using the
adapted ME model (according to the Wilcoxon
test), with and exception of the English experi-
ments on the 292K and 591K data sets.
The comparison between N -gram models and
ME models is not entirely fair since ME models
are actually class-based. Such transformation in-
troduces additional smoothing into the model and
can improve model perplexity, as also noticed by
Goodman (2001).
5 Discussion
In this paper we have tested a hierarchical adapta-
tion method (Daume III, 2007; Finkel and Man-
ning, 2009) on building style-adapted LMs for
speech recognition. We showed that the method
achieves consistently lower error rates than when
using linear interpolation which is typically used
in such scenarios.
The tested method is ideally suited for language
modeling in speech recognition: we almost always
have access to large amounts of data from written
sources but commonly the speech to be recognized
is stylistically noticeably different. The hierarchi-
cal adaptation method enables to use even a small
amount of in-domain data to modify the parame-
ters estimated from out-of-domain data, if there is
enough evidence.
As Finkel and Manning (2009) point out, the
hierarchical nature of the method makes it possi-
ble to estimate highly specific models: we could
draw style-specific models from general high-level
priors, and topic-and-style specific models from
style-specific priors. Furthermore, the models
don?t have to be hierarchical: it is easy to gen-
eralize the method to general multilevel approach
where a model is drawn from multiple priors. For
304
Perplexity WER LER
Adaptation
data (No.
of words)
Pooled
N-
gram
Interp.
N-
gram
Interp.
ME
Adapted
ME
Interp.
N-
gram
Interp.
ME
Adapted
ME
Interp.
N-
gram
Interp.
ME
Adapted
ME
English Broadcast News
147K 290 255 243 230 27.2 26.3 25.9
292K 286 250 236 223 26.7 25.8 25.6
591K 280 243 228 215 26.6 25.9 25.6
1119K 272 232 217 204 26.2 25.6 24.9
Estonian Broadcast Conversations
104K 237 197 200 169 40.5 38.9 37.4 17.7 17.3 16.6
Table 2: Perplexity, WER and LER results comparing pooled and interpolated N -gram models and
interpolated and adapted ME models, with changing amount of available in-domain data.
instance, we could build a model for recognizing
computer science lectures, given data from text-
books, including those about computer science,
and transcripts of lectures on various topics (which
don?t even need to include lectures about computer
science).
The method has some considerable shortcom-
ings from the practical perspective. First, train-
ing ME LMs in general has much higher resource
requirements than training N -gram models which
are typically used in speech recognition. More-
over, training hierarchical ME models requires
even more memory than training simple ME mod-
els, proportional to the number of nodes in the hi-
erarchy. However, it should be possible to allevi-
ate this problem by profiting from the hierarchi-
cal nature of n-gram features, as proposed in (Wu
and Khudanpur, 2002). It is also difficult to deter-
mine good variance values ?2i for the global and
domain-specific priors. While good variance val-
ues for simple ME models can be chosen quite re-
liably based on the size of the training data (Chen,
2009a), we have found that it is more demand-
ing to find good hyperparameters for hierarchical
models since weights for the same feature in dif-
ferent nodes in the hierarchy are all related to each
other. We plan to investigate this problem in the
future since the choice of hyperparameters has a
strong impact on the performance of the model.
Acknowledgments
This research was partly funded by the Academy
of Finland in the project Adaptive Informatics,
by the target-financed theme No. 0322709s06 of
the Estonian Ministry of Education and Research
and by the National Programme for Estonian Lan-
guage Technology.
References
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. Computer Speech & Language, 20(4):382?399,
October.
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Trans-
actions on Speech and Audio Processing, 8(1):37?
50.
S. F. Chen. 2009a. Performance prediction for expo-
nential language models. In Proceedings of HLT-
NAACL, pages 450?458, Boulder, Colorado.
Stanley F. Chen. 2009b. Shrinking exponential lan-
guage models. In Proceedings of HLT-NAACL,
pages 468?476, Boulder, Colorado.
H. Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256?263.
P. Dele?glise, Y. Este?ve, S. Meignier, and T. Merlin.
2005. The LIUM speech transcription system: a
CMU Sphinx III-based system for French broadcast
news. In Proceedings of Interspeech, Lisboa, Portu-
gal.
J. R. Finkel and Ch. Manning. 2009. Hierarchi-
cal Bayesian domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, Colorado.
J. Goodman. 2001. Classes for fast maximum entropy
training. In Proceedings of ICASSP, Utah, USA.
H.-J. Kaalep and T. Vaino. 2001. Complete morpho-
logical analysis in the linguist?s toolbox. In Con-
gressus Nonus Internationalis Fenno-Ugristarum
Pars V, pages 9?16, Tartu, Estonia.
R. Kneser and H. Ney. 1993. Improved clustering
techniques for class-based statistical language mod-
elling. In Proceedings of the European Conference
305
on Speech Communication and Technology, pages
973?976.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
J. Wu and S. Khudanpur. 2002. Building a topic-
dependent maximum entropy model for very large
corpora. In Proceedings of ICASSP, Orlando,
Florida, USA.
306
Proceedings of the ACL 2010 System Demonstrations, pages 48?53,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Personalising speech-to-speech translation in the EMIME project
Mikko Kurimo1?, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,
Yong Guan5, Teemu Hirsima?ki1, Reima Karhila1, Simon King2, Hui Liang3, Keiichiro
Oura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,
Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi2
1 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,
Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,
6 University of Cambridge, UK
?Corresponding author: Mikko.Kurimo@tkk.fi
Abstract
In the EMIME project we have studied un-
supervised cross-lingual speaker adapta-
tion. We have employed an HMM statisti-
cal framework for both speech recognition
and synthesis which provides transfor-
mation mechanisms to adapt the synthe-
sized voice in TTS (text-to-speech) using
the recognized voice in ASR (automatic
speech recognition). An important ap-
plication for this research is personalised
speech-to-speech translation that will use
the voice of the speaker in the input lan-
guage to utter the translated sentences in
the output language. In mobile environ-
ments this enhances the users? interaction
across language barriers by making the
output speech sound more like the origi-
nal speaker?s way of speaking, even if she
or he could not speak the output language.
1 Introduction
A mobile real-time speech-to-speech translation
(S2ST) device is one of the grand challenges in
natural language processing (NLP). It involves
several important NLP research areas: auto-
matic speech recognition (ASR), statistical ma-
chine translation (SMT) and speech synthesis, also
known as text-to-speech (TTS). In recent years
significant advance have also been made in rele-
vant technological devices: the size of powerful
computers has decreased to fit in a mobile phone
and fast WiFi and 3G networks have spread widely
to connect them to even more powerful computa-
tion servers. Several hand-held S2ST applications
and devices have already become available, for ex-
ample by IBM, Google or Jibbigo1, but there are
still serious limitations in vocabulary and language
selection and performance.
When an S2ST device is used in practical hu-
man interaction across a language barrier, one fea-
ture that is often missed is the personalization of
the output voice. Whoever speaks to the device in
what ever manner, the output voice always sounds
the same. Producing high-quality synthesis voices
is expensive and even if the system had many out-
put voices, it is hard to select one that would sound
like the input voice. There are many features in the
output voice that could raise the interaction expe-
rience to a much more natural level, for example,
emotions, speaking rate, loudness and the speaker
identity.
After the recent development in hidden Markov
model (HMM) based TTS, it has become possi-
ble to adapt the output voice using model trans-
formations that can be estimated from a small
number of speech samples. These techniques, for
instance the maximum likelihood linear regres-
sion (MLLR), are adopted from HMM-based ASR
where they are very powerful in fast adaptation of
speaker and recording environment characteristics
(Gales, 1998). Using hierarchical regression trees,
the TTS and ASR models can further be coupled
in a way that enables unsupervised TTS adaptation
(King et al, 2008). In unsupervised adaptation
samples are annotated by applying ASR. By elimi-
nating the need for human intervention it becomes
possible to perform voice adaptation for TTS in
almost real-time.
The target in the EMIME project2 is to study
unsupervised cross-lingual speaker adaptation for
S2ST systems. The first results of the project have
1http://www.jibbigo.com
2http://emime.org
48
been, for example, to bridge the gap between the
ASR and TTS (Dines et al, 2009), to improve
the baseline ASR (Hirsima?ki et al, 2009) and
SMT (de Gispert et al, 2009) systems for mor-
phologically rich languages, and to develop robust
TTS (Yamagishi et al, 2010). The next step has
been preliminary experiments in intra-lingual and
cross-lingual speaker adaptation (Wu et al, 2008).
For cross-lingual adaptation several new methods
have been proposed for mapping the HMM states,
adaptation data and model transformations (Wu et
al., 2009).
In this presentation we can demonstrate the var-
ious new results in ASR, SMT and TTS. Even
though the project is still ongoing, we have an
initial version of mobile S2ST system and cross-
lingual speaker adaptation to show.
2 Baseline ASR, TTS and SMT systems
The baseline ASR systems in the project are devel-
oped using the HTK toolkit (Young et al, 2001)
for Finnish, English, Mandarin and Japanese. The
systems can also utilize various real-time decoders
such as Julius (Kawahara et al, 2000), Juicer at
IDIAP and the TKK decoder (Hirsima?ki et al,
2006). The main structure of the baseline sys-
tems for each of the four languages is similar and
fairly standard and in line with most other state-of-
the-art large vocabulary ASR systems. Some spe-
cial flavors for have been added, such as the mor-
phological analysis for Finnish (Hirsima?ki et al,
2009). For speaker adaptation, the MLLR trans-
formation based on hierarchical regression classes
is included for all languages.
The baseline TTS systems in the project utilize
the HTS toolkit (Yamagishi et al, 2009) which
is built on top of the HTK framework. The
HMM-based TTS systems have been developed
for Finnish, English, Mandarin and Japanese. The
systems include an average voice model for each
language trained over hundreds of speakers taken
from standard ASR corpora, such as Speecon
(Iskra et al, 2002). Using speaker adaptation
transforms, thousands of new voices have been
created (Yamagishi et al, 2010) and new voices
can be added using a small number of either su-
pervised or unsupervised speech samples. Cross-
lingual adaptation is possible by creating a map-
ping between the HMM states in the input and the
output language (Wu et al, 2009).
Because the resources of the EMIME project
have been focused on ASR, TTS and speaker
adaptation, we aim at relying on existing solu-
tions for SMT as far as possible. New methods
have been studied concerning the morphologically
rich languages (de Gispert et al, 2009), but for the
S2ST system we are currently using Google trans-
late3.
3 Demonstrations to show
3.1 Monolingual systems
In robust speech synthesis, a computer can learn
to speak in the desired way after processing only a
relatively small amount of training speech. The
training speech can even be a normal quality
recording outside the studio environment, where
the target speaker is speaking to a standard micro-
phone and the speech is not annotated. This differs
dramatically from conventional TTS, where build-
ing a new voice requires an hour or more careful
repetition of specially selected prompts recorded
in an anechoic chamber with high quality equip-
ment.
Robust TTS has recently become possible us-
ing the statistical HMM framework for both ASR
and TTS. This framework enables the use of ef-
ficient speaker adaptation transformations devel-
oped for ASR to be used also for the TTS mod-
els. Using large corpora collected for ASR, we can
train average voice models for both ASR and TTS.
The training data may include a small amount of
speech with poor coverage of phonetic contexts
from each single speaker, but by summing the ma-
terial over hundreds of speakers, we can obtain
sufficient models for an average speaker. Only a
small amount of adaptation data is then required to
create transformations for tuning the average voice
closer to the target voice.
In addition to the supervised adaptation us-
ing annotated speech, it is also possible to em-
ploy ASR to create annotations. This unsu-
pervised adaptation enables the system to use a
much broader selection of sources, for example,
recorded samples from the internet, to learn a new
voice.
The following systems will demonstrate the re-
sults of monolingual adaptation:
1. In EMIME Voice cloning in Finnish and En-
glish the goal is that the users can clone their
own voice. The user will dictate for about
3http://translate.google.com
49
Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.
Blue markers show male speakers and red markers show female speakers. Available online via
http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1
10 minutes and then after half an hour of
processing time, the TTS system has trans-
formed the average model towards the user?s
voice and can speak with this voice. The
cloned voices may become especially valu-
able, for example, if a person?s voice is later
damaged in an accident or by a disease.
2. In EMIME Thousand voices map the goal is
to browse the world?s largest collection of
synthetic voices by using a world map in-
terface (Yamagishi et al, 2010). The user
can zoom in the world map and select any
voice, which are organized according to the
place of living of the adapted speaker, to ut-
ter the given sentence. This interactive ge-
ographical representation is shown in Figure
1. Each marker corresponds to an individual
speaker. Blue markers show male speakers
and red markers show female speakers. Some
markers are in arbitrary locations (in the cor-
rect country) because precise location infor-
mation is not available for all speakers. This
geographical representation, which includes
an interactive TTS demonstration of many of
the voices, is available from the URL pro-
vided. Clicking on a marker will play syn-
thetic speech from that speaker4. As well as
4Currently the interactive mode supports English and
Spanish only. For other languages this only provides pre-
being a convenient interface to compare the
many voices, the interactive map is an attrac-
tive and easy-to-understand demonstration of
the technology being developed in EMIME.
3. The models developed in the HMM frame-
work can be demonstrated also in adapta-
tion of an ASR system for large-vocabulary
continuous speech recognition. By utilizing
morpheme-based language models instead of
word-based models the Finnish ASR system
is able to cover practically an unlimited vo-
cabulary (Hirsima?ki et al, 2006). This is
necessary for morphologically rich languages
where, due to inflection, derivation and com-
position, there exists so many different word
forms that word based language modeling be-
comes impractical.
3.2 Cross-lingual systems
In the EMIME project the goal is to learn cross-
lingual speaker adaptation. Here the output lan-
guage ASR or TTS system is adapted from speech
samples in the input language. The results so far
are encouraging, especially for TTS: Even though
the cross-lingual adaptation may somewhat de-
grade the synthesis quality, the adapted speech
now sounds more like the target speaker. Sev-
eral recent evaluations of the cross-lingual speaker
synthesised examples, but we plan to add an interactive type-
in text-to-speech feature in the near future.
50
Figure 2: All English HTS voices can be used as online TTS on the geographical map.
adaptation methods can be found in (Gibson et al,
2010; Oura et al, 2010; Liang et al, 2010; Oura
et al, 2009).
The following systems have been created to
demonstrate cross-lingual adaptation:
1. In EMIME Cross-lingual Finnish/English
and Mandarin/English TTS adaptation the
input language sentences dictated by the user
will be used to learn the characteristics of her
or his voice. The adapted cross-lingual model
will be used to speak output language (En-
glish) sentences in the user?s voice. The user
does not need to be bilingual and only reads
sentences in their native language.
2. In EMIME Real-time speech-to-speech mo-
bile translation demo two users will interact
using a pair of mobile N97 devices (see Fig-
ure 3). The system will recognize the phrase
the other user is speaking in his native lan-
guage and translate and speak it in the native
language of the other user. After a few sen-
tences the system will have the speaker adap-
tation transformations ready and can apply
them in the synthesized voices to make them
sound more like the original speaker instead
of a standard voice. The first real-time demo
version is available for the Mandarin/English
language pair.
3. The morpheme-based translation system for
Finnish/English and English/Finnish can be
compared to a word based translation for
arbitrary sentences. The morpheme-based
approach is particularly useful for language
pairs where one or both languages are mor-
phologically rich ones where the amount and
complexity of different word forms severely
limits the performance for word-based trans-
lation. The morpheme-based systems can
learn translation models for phrases where
morphemes are used instead of words (de
Gispert et al, 2009). Recent evaluations (Ku-
rimo et al, 2009) have shown that the perfor-
mance of the unsupervised data-driven mor-
pheme segmentation can rival the conven-
tional rule-based ones. This is very useful if
hand-crafted morphological analyzers are not
available or their coverage is not sufficient for
all languages.
Acknowledgments
The research leading to these results was partly
funded from the European Communitys Seventh
51
  
ASR SMT TTS
Cross-lingualSpeaker adaptation
Speakeradaptation
input outputspeech
Figure 3: EMIME Real-time speech-to-speech
mobile translation demo
Framework Programme (FP7/2007-2013) under
grant agreement 213845 (the EMIME project).
References
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of transla-
tion hypotheses from alternative morphological de-
compositions. In Proc. NAACL-HLT.
J. Dines, J. Yamagishi, and S. King. 2009. Measur-
ing the gap between HMM-based ASR and TTS. In
Proc. Interspeech ?09, Brighton, UK.
M. Gales. 1998. Maximum likelihood linear transfor-
mations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
M. Gibson, T. Hirsima?ki, R. Karhila, M. Kurimo,
and W. Byrne. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis using two-pass decision tree construction. In
Proc. of ICASSP, page to appear, March.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S.
Virpioja, and J. Pylkko?nen. 2006. Unlimited vo-
cabulary speech recognition with morph language
models applied to finnish. Computer Speech & Lan-
guage, 20(4):515?541, October.
T. Hirsima?ki, J. Pylkko?nen, and M Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. IEEE Trans. Audio,
Speech, and Language Process., 17:724?732.
D. Iskra, B. Grosskopf, K. Marasek, H. van den
Heuvel, F. Diehl, and A. Kiessling. 2002.
SPEECON speech databases for consumer devices:
Database specification and validation. In Proc.
LREC, pages 329?333.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda,
N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-
mamoto, A. Yamada, T. Utsuro, and K. Shikano.
2000. Free software toolkit for japanese large vo-
cabulary continuous speech recognition. In Proc.
ICSLP-2000, volume 4, pages 476?479.
S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008.
Unsupervised adaptation for HMM-based speech
synthesis. In Proc. Interspeech 2008, pages 1869?
1872, September.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
H. Liang, J. Dines, and L. Saheer. 2010. A
comparison of supervised and unsupervised cross-
lingual speaker adaptation approaches for HMM-
based speech synthesis. In Proc. of ICASSP, page
to appear, March.
Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-
jam Wester, and Keiichi Tokuda. 2009. Unsuper-
vised speaker adaptation for speech-to-speech trans-
lation system. In Proc. SLP (Spoken Language Pro-
cessing), number 356 in 109, pages 13?18.
K. Oura, K. Tokuda, J. Yamagishi, S. King, and
M. Wester. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ICASSP, page to appear, March.
Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ISCSLP, pages 1?4, December.
Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State
mapping based method for cross-lingual speaker
adaptation in HMM-based speech synthesis. In
Proc. of Interspeech, pages 528?531, September.
J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,
K. Tokuda, S. King, and S. Renals. 2009. Robust
speaker-adaptive HMM-based text-to-speech syn-
thesis. IEEE Trans. Audio, Speech and Language
Process., 17(6):1208?1230. (in press).
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,
J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and
M. Kurimo. 2010. Thousands of voices for hmm-
based speech synthesis. IEEE Trans. Speech, Audio
& Language Process. (in press).
52
S. Young, G. Everman, D. Kershaw, G. Moore, J.
Odell, D. Ollason, V. Valtchev, and P. Woodland,
2001. The HTK Book Version 3.1, December.
53
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 259?264,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Part-of-Speech Tagging using Conditional Random Fields: Exploiting
Sub-Label Dependencies for Improved Accuracy
Miikka Silfverberg
a
Teemu Ruokolainen
b
Krister Lind?n
a
Mikko Kurimo
b
a
Department of Modern Languages, University of Helsinki,
firstname.lastname@helsinki.fi
b
Department of Signal Processing and Acoustics, Aalto University,
firstname.lastname@aalto.fi
Abstract
We discuss part-of-speech (POS) tagging
in presence of large, fine-grained la-
bel sets using conditional random fields
(CRFs). We propose improving tagging
accuracy by utilizing dependencies within
sub-components of the fine-grained labels.
These sub-label dependencies are incor-
porated into the CRF model via a (rela-
tively) straightforward feature extraction
scheme. Experiments on five languages
show that the approach can yield signifi-
cant improvement in tagging accuracy in
case the labels have sufficiently rich inner
structure.
1 Introduction
We discuss part-of-speech (POS) tagging using
the well-known conditional random field (CRF)
model introduced originally by Lafferty et al
(2001). Our focus is on scenarios, in which the
POS labels have a rich inner structure. For exam-
ple, consider
PRON+1SG V+NON3SG+PRES N+SG
I like ham
,
where the compound labels PRON+1SG,
V+NON3SG+PRES, and N+SG stand for pro-
noun first person singular, verb non-third singular
present tense, and noun singular, respectively.
Fine-grained labels occur frequently in mor-
phologically complex languages (Erjavec, 2010;
Haverinen et al, 2013).
We propose improving tagging accuracy by uti-
lizing dependencies within the sub-labels (PRON,
1SG, V, NON3SG, N, and SG in the above ex-
ample) of the compound labels. From a technical
perspective, we accomplish this by making use of
the fundamental ability of the CRFs to incorporate
arbitrarily defined feature functions. The newly-
defined features are expected to alleviate data spar-
sity problems caused by the fine-grained labels.
Despite the (relative) simplicity of the approach,
we are unaware of previous work exploiting the
sub-labels to the extent presented here.
We present experiments on five languages (En-
glish, Finnish, Czech, Estonian, and Romanian)
with varying POS annotation granularity. By uti-
lizing the sub-labels, we gain significant improve-
ment in model accuracy given a sufficiently fine-
grained label set. Moreover, our results indi-
cate that exploiting the sub-labels can yield larger
improvements in tagging compared to increasing
model order.
The rest of the paper is organized as follows.
Section 2 describes the methodology. Experimen-
tal setup and results are presented in Section 3.
Section 4 discusses related work. Lastly, we pro-
vide conclusions on the work in Section 5.
2 Methods
2.1 Conditional Random Fields
The (unnormalized) CRF model (Lafferty et al,
2001) for a sentence x = (x
1
, . . . , x
|x|
) and a POS
sequence y = (y
1
, . . . , y
|x|
) is defined as
p (y |x;w) ?
|x|
?
i=n
exp
(
w??(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where n denotes the model order,w the model pa-
rameter vector, and ? the feature extraction func-
tion. We denote the tag set as Y , that is, y
i
? Y
for i ? 1 . . . |x|.
2.2 Baseline Feature Set
We first describe our baseline feature set
{?
j
(y
i?1
, y
i
, x, i)}
|?|
j=1
by defining emission and
transition features. The emission feature set as-
sociates properties of the sentence position i with
259
the corresponding label as
{?
j
(x, i)1(y
i
= y
?
i
) | j ? 1 . . . |X | , ?y
?
i
? Y} ,
(2)
where the function 1(q) returns one if and only if
the proposition q is true and zero otherwise, that is
1(y
i
= y
?
i
) =
{
1 if y
i
= y
?
i
0 otherwise
, (3)
and X = {?
j
(x, i)}
|X |
j=1
is the set of functions
characterizing the word position i. Following the
classic work of Ratnaparkhi (1996), our X com-
prises simple binary functions:
1. Bias (always active irrespective of input).
2. Word forms x
i?2
, . . . , x
i+2
.
3. Prefixes and suffixes of the word form x
i
up
to length ?
suf
= 4.
4. If the word form x
i
contains (one or more)
capital letter, hyphen, dash, or digit.
Binary functions have a return value of either zero
(inactive) or one (active). Meanwhile, the transi-
tion features
{1(y
i?k
= y
?
i?k
) . . .1(y
i
= y
?
i
) |
y
?
i?k
, . . . , y
?
i
? Y ,?k ? 1 . . . n} (4)
capture dependencies between adjacent labels ir-
respective of the input x.
2.2.1 Expanded Feature Set Leveraging
Sub-Label Dependencies
The baseline feature set described above can yield
a high tagging accuracy given a conveniently sim-
ple label set, exemplified by the tagging results
of Collins (2002) on the Penn Treebank (Mar-
cus et al, 1993). (Note that conditional random
fields correspond to discriminatively trained hid-
den Markov models and Collins (2002) employs
the latter terminology.) However, it does to some
extent overlook some beneficial dependency infor-
mation in case the labels have a rich sub-structure.
In what follows, we describe expanded feature sets
which explicitly model the sub-label dependen-
cies.
We begin by defining a function P(y
i
) which
partitions any label y
i
into its sub-label compo-
nents and returns them in an unordered set. For
example, we could define P(PRON+1+SG) =
{PRON, 1, SG}. (Label partitions employed in
the experiments are described in Section 3.2.) We
denote the set of all sub-label components as S.
Subsequently, instead of defining only (2), we
additionally associate the feature functionsX with
all sub-labels s ? S by defining
{?
j
(x, i)1(s ? P(y
i
)) | ?j ? 1 . . . |X | ,?s ? S} ,
(5)
where 1(s ? P(y
i
)) returns one in case s is in
P(y
i
) and zero otherwise. Second, we exploit sub-
label transitions using features
{1(s
i?k
? P(y
i?k
)) . . .1(s
i
? P(y
i
)) |
?s
i?k
, . . . , s
i
? S ,?k ? 1 . . .m} . (6)
Note that we define the sub-label transitions up
to order m, 1 ? m ? n, that is, an nth-order
CRF model is not obliged to utilize sub-label tran-
sitions all the way up to order n. This is be-
cause employing high-order sub-label transitions
may potentially cause overfitting to training data
due to substantially increased number of features
(equivalent to the number of model parameters,
|w| = |?|). For example, in a second-order
(n = 2) model, it might be beneficial to em-
ploy the sub-label emission feature set (5) and
first-order sub-label transitions while discarding
second-order sub-label transitions. (See the exper-
imental results presented in Section 3.)
In the remainder of this paper, we use the fol-
lowing notations.
1. A standard CRF model incorporating (2) and
(4) is denoted as CRF(n,-).
2. A CRF model incorporating (2), (4), and (5)
is denoted as CRF(n,0).
3. A CRF model incorporating (2), (4), (5), and
(6) is denoted as CRF(n,m).
2.3 On Linguistic Intuition
This section aims to provide some intuition on the
types of linguistic phenomena that can be captured
by the expanded feature set. To this end, we con-
sider an example on the plural number in Finnish.
First, consider the plural nominative word form
kissat (cats) where the plural number is denoted
by the 1-suffix -t. Then, by employing the features
(2), the suffix -t is associated solely with the com-
pound label NOMINATIVE+PLURAL. However,
by incorporating the expanded feature set (5), -t
260
will also be associated to the sub-label PLURAL.
This can be useful because, in Finnish, also adjec-
tives and numerals are inflected according to num-
ber and denote the plural number with the suffix
-t (Hakulinen et al, 2004, ?79). Therefore, one
can exploit -t to predict the plural number also in
words such as mustat (plural of black) with a com-
pound analysis ADJECTIVE+PLURAL.
Second, consider the number agreement (con-
gruence). For example, in the sentence fragment
mustat kissat juoksevat (black cats are running),
the words mustat and kissat share the plural num-
ber. In other words, the analyses of both mustat
and kissat are required to contain the sub-label
PLURAL. This short-span dependency between
sub-labels will be captured by a first-order sub-
label transition feature included in (6).
Lastly, we note that the feature expansion sets
(5) and (6) will, naturally, capture any short-span
dependencies within the sub-labels irrespective if
the dependencies have a clear linguistic interpre-
tation or not.
3 Experiments
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The English Penn Treebank
(Marcus et al, 1993) is divided into 25 sections
of newswire text extracted from the Wall Street
Journal. We split the data into training, develop-
ment, and test sets using the sections 0-18, 19-21,
and 22-24, according to the standardly applied di-
vision introduced by Collins (2002).
Turku Depedency Treebank. The Finnish
Turku Depedendency Treebank (Haverinen et al,
2013) contains text from 10 different domains.
The treebank does not have default partition to
training and test sets. Therefore, from each 10
consecutive sentences, we assign the 9th and 10th
to the development set and the test set, respec-
tively. The remaining sentences are assigned to
the training set.
Multext-East. The third data we consider is the
multilingual Multext-East (Erjavec, 2010) corpus,
from which we utilize the Czech, Estonian and Ro-
manian sections. The corpus corresponds to trans-
lations of the novel 1984 by George Orwell. We
apply the same data splits as for Turku Depen-
dency Treebank.
lang. train. dev. test tags train. tags
Eng 38,219 5,527 5,462 45 45
Rom 5,216 652 652 405 391
Est 5,183 648 647 413 408
Cze 5,402 675 675 955 908
Fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
3.2 Label Partitions
This section describes the employed compound la-
bel splits. The label splits for all data sets are sub-
mitted as data file attachments. All the splits are
performed a priori to model learning, that is, we
do not try to optimize them on the development
sets.
The POS labels in the Penn Treebank are split
in a way which captures relevant inflectional cat-
egories, such as tense and number. Consider, for
example, the split for the present tense third sin-
gular verb label P(VBZ) = {VB, Z}.
In the Turku Dependency Treebank, each
morphological tag consists of sub-labels mark-
ing word-class, relevant inflectional categories,
and their respective values. Each inflec-
tional category, such as case or tense, com-
bined with its value, such as nominative or
present, constitutes one sub-label. Consider,
for example, the split for the singular, adessive
noun P(N+CASE_ADE+NUM_SG) = {POS_N,
CASE_ADE, NUM_SG}.
The labeling scheme employed in the Multext-
East data set represents a considerably different
annotation approach compared to the Penn and
Turku Treebanks. Each morphological analysis is
a sequence of feature markers, for example Pw3?
r. The first feature marker (P) denotes word class
and the rest (w, 3, and r) encode values of inflec-
tional categories relevant for that word class. A
feature marker may correspond to several differ-
ent values depending on word class and its posi-
tion in the analysis. Therefore it becomes rather
difficult to split the labels into similar pairs of in-
flectional category and value as we are able to do
for the Turku Dependency Treebank. Since the in-
terpretation of a feature marker depends on its po-
sition in the analysis and the word class, the mark-
ers have to be numbered and appended with the
261
word class marker. For example, consider the split
P(Pw3?r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}.
3.3 CRF Model Specification
We perform experiments using first-order and
second-order CRFs with zeroth-order and first-
order sub-label features. Using the notation
introduced in Section 2, the employed mod-
els are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0),
and CRF(2,1). We do not report results us-
ing CRF(2,2) since, based on preliminary exper-
iments, this model overfits on all languages.
The CRF model parameters are estimated using
the averaged perceptron algorithm (Collins, 2002).
The model parameters are initialized with a zero
vector. We evaluate the latest averaged parameters
on the held-out development set after each pass
over the training data and terminate training if no
improvement in accuracy is obtained during three
last passes. The best-performing parameters are
then applied on the test instances.
We accelerate the perceptron learning using
beam search (Zhang and Clark, 2011). The beam
width, b, is optimized separately for each lan-
guage on the development sets by considering b =
1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy
does not improve by at least 0.01 (absolute).
Development and test instances are decoded us-
ing Viterbi search in combination with the tag dic-
tionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer (Intel Xeon E5450 with 3.00 GHz and
64 GB of memory). The methods discussed in
Section 2 are implemented in C++.
3.5 Results
The obtained tagging accuracies and training
times are presented in Table 2. The times in-
clude running the averaged perceptron algorithm
and evaluation of the development sets. The col-
umn labeled it. corresponds to the number of
passes over the training data made by the percep-
tron algorithm before termination. We summarize
the results as follows.
First, compared to standard feature extraction
approach, employing the sub-label transition fea-
tures resulted in improved accuracy on all lan-
guages apart from English. The differences were
statistically significant on Czech, Estonian, and
Finnish. (We establish statistical significance
(with confidence level 0.95) using the standard 1-
sided Wilcoxon signed-rank test performed on 10
randomly divided, non-overlapping subsets of the
complete test sets.) This results supports the in-
tuition that the sub-label features should be most
useful in presence of large, fine-grained label sets,
in which case the learning is most affected by data
sparsity.
Second, on all languages apart from English,
employing a first-order model with sub-label fea-
tures yielded higher accuracy compared to a
second-order model with standard features. The
differences were again statistically significant on
Czech, Estonian, and Finnish. This result suggests
that, compared to increasing model order, exploit-
ing the sub-label dependencies can be a preferable
approach to improve the tagging accuracy.
Third, applying the expanded feature set in-
evitably causes some increase in the computa-
tional cost of model estimation. However, as
shown by the running times, this increase is not
prohibitive.
4 Related Work
In this section, we compare the approach pre-
sented in Section 2 to two prior systems which at-
tempt to utilize sub-label dependencies in a similar
manner.
Smith et al (2005) use a CRF-based system
for tagging Czech, in which they utilize expanded
emission features similar to our (5). However, they
do not utilize the full expanded transition features
(6). More specifically, instead of utilizing a sin-
gle chain as in our approach, Smith et al employ
five parallel structured chains. One of the chains
models the sequence of word-class labels such as
noun and adjective. The other four chains model
gender, number, case, and lemma sequences, re-
spectively. Therefore, in contrast to our approach,
their system does not capture cross-dependencies
between inflectional categories, such as the de-
pendence between the word-class and case of ad-
jacent words. Unsurprisingly, Smith et al fail
to achieve improvement over a generative HMM-
based POS tagger of Haji
?
c (2001). Meanwhile,
our system outperforms the generative trigram tag-
ger HunPos (Hal?csy et al, 2007) which is an im-
262
model it. time (min) acc. OOV.
English
CRF(1, -) 8 9 97.04 88.65
CRF(1, 0) 6 17 97.02 88.44
CRF(1, 1) 8 22 97.02 88.82
CRF(2, -) 9 15 97.18 88.82
CRF(2, 0) 11 36 97.17 89.23
CRF(2, 1) 8 27 97.15 89.04
Romanian
CRF(1, -) 14 29 97.03 85.01
CRF(1, 0) 13 68 96.96 84.59
CRF(1, 1) 16 146 97.24 85.94
CRF(2, -) 7 19 97.08 85.21
CRF(2, 0) 18 99 97.02 85.42
CRF(2, 1) 12 118 97.29 86.25
Estonian
CRF(1, -) 15 28 93.39 78.66
CRF(1, 0) 17 66 93.81 80.44
CRF(1, 1) 13 129 93.77 79.37
CRF(2, -) 15 30 93.48 77.13
CRF(2, 0) 13 53 93.78 79.60
CRF(2, 1) 16 105 94.01 79.53
Czech
CRF(1, -) 6 28 89.28 70.90
CRF(1, 0) 10 112 89.94 74.44
CRF(1, 1) 10 365 90.78 76.83
CRF(2, -) 19 91 89.81 72.44
CRF(2, 0) 13 203 90.35 76.37
CRF(2, 1) 24 936 91.00 77.75
Finnish
CRF(1, -) 10 80 87.37 59.29
CRF(1, 0) 13 249 88.58 63.46
CRF(1, 1) 12 474 88.41 62.63
CRF(2, -) 11 106 86.74 56.96
CRF(2, 0) 13 272 88.52 63.46
CRF(2, 1) 12 331 88.68 63.62
Table 2: Results.
proved open-source implementation of the well-
known TnT tagger of Brants (2000). The obtained
HunPos results are presented in Table 3.
Eng Rom Est Cze Fin
HunPos 96.58 96.96 92.76 89.57 85.77
Table 3: Results using a generative HMM-based
HunPos tagger of Halacsy et al (2007).
Ceaus?u (2006) uses a maximum entropy
Markov model (MEMM) based system for tag-
ging Romanian which utilizes transitional behav-
ior between sub-labels similarly to our feature set
(6). However, in addition to ignoring the most in-
formative emission-type features (5), Ceaus?u em-
beds the MEMMs into the tiered tagging frame-
work of Tufis (1999). In tiered tagging, the full
morphological analyses are mapped into a coarser
tag set and a tagger is trained for this reduced tag
set. Subsequent to decoding, the coarser tags are
mapped into the original fine-grained morpholog-
ical analyses. There are several problems associ-
ated with this tiered tagging approach. First, the
success of the approach is highly dependent on a
well designed coarse label set. Consequently, it
requires intimate knowledge of the tag set and lan-
guage. Meanwhile, our model can be set up with
relatively little prior knowledge of the language
or the tagging scheme (see Section 3.2). More-
over, a conversion to a coarser label set is neces-
sarily lossy (at least for OOV words) and poten-
tially results in reduced accuracy since recovering
the original fine-grained tags from the coarse tags
may induce errors. Indeed, the accuracy 96.56, re-
ported by Ceaus?u on the Romanian section of the
Multext-East data set, is substantially lower than
the accuracy 97.29 we obtain. These accuracies
were obtained using identical sized training and
test sets (although direct comparison is impossible
because Ceaus?u uses a non-documented random
split).
5 Conclusions
We studied improving the accuracy of CRF-based
POS tagging by exploiting sub-label dependency
structure. The dependencies were included in the
CRF model using a relatively straightforward fea-
ture expansion scheme. Experiments on five lan-
guages showed that the approach can yield signif-
icant improvement in tagging accuracy given suf-
ficiently fine-grained label sets.
In future work, we aim to perform a more
fine-grained error analysis to gain a better under-
standing where the improvement in accuracy takes
place. One could also attempt to optimize the
compound label splits to maximize prediction ac-
curacy instead of applying a priori partitions.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the grant no
251170 (Finnish Centre of Excellence Program
(2012-2017)). We would like to thank the anony-
mous reviewers for their useful comments.
263
References
Thorsten Brants. 2000. Tnt: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224?231.
A. Ceausu. 2006. Maximum entropy tiered tagging.
In The 11th ESSLI Student session, pages 173?179.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8.
Toma?z Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Jan Haji?c, Pavel Krbec, Pavel Kv?eto?n, Karel Oliva, and
Vladim?r Petkevi?c. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of the 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 268?
275.
Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, Vesa
Koivisto, Tarja Riitta Heinonen, and Irja Alho.
2004. Iso suomen kielioppi. Suomalaisen Kirjal-
lisuuden Seura, Helsinki, Finland.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz.
2007. Hunpos: An open source trigram tagger. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 209?212.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguis-
tics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natu-
ral language processing, volume 1, pages 133?142.
Philadelphia, PA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
475?482.
Dan Tufis. 1999. Tiered tagging and combined lan-
guage models classifiers. In Proceedings of the Sec-
ond International Workshop on Text, Speech and Di-
alogue, pages 28?33.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
264
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195?200,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Applying morphological decomposition to statistical machine translation
Sami Virpioja and Jaakko Va?yrynen and Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science and Technology
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fi
Abstract
This paper describes the Aalto submission
for the German-to-English and the Czech-
to-English translation tasks of the ACL
2010 Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR.
Statistical machine translation has focused
on using words, and longer phrases con-
structed from words, as tokens in the sys-
tem. In contrast, we apply different mor-
phological decompositions of words using
the unsupervised Morfessor algorithms.
While translation models trained using the
morphological decompositions did not im-
prove the BLEU scores, we show that the
Minimum Bayes Risk combination with
a word-based translation model produces
significant improvements for the German-
to-English translation. However, we did
not see improvements for the Czech-to-
English translations.
1 Introduction
The effect of morphological variation in languages
can be alleviated by using word analysis schemes,
which may include morpheme discovery, part-of-
speech tagging, or other linguistic information.
Words are very convenient and even efficient rep-
resentation in statistical natural language process-
ing, especially with English, but morphologically
rich languages can benefit from more fine-grained
information. For instance, statistical morphs dis-
covered with unsupervised methods result in bet-
ter performance in automatic speech recognition
for highly-inflecting and agglutinative languages
(Hirsima?ki et al, 2006; Kurimo et al, 2006).
Virpioja et al (2007) applied morph-based
models in statistical machine translation (SMT)
between several language pairs without gaining
improvement in BLEU score, but obtaining re-
ductions in out-of-vocabulary rates. They uti-
lized morphs both in the source and in the tar-
get language. Later, de Gispert et al (2009)
showed that Minimum Bayes Risk (MBR) com-
bination of word-based and morph-based trans-
lation models improves translation with Arabic-
to-English and Finnish-to-English language pairs,
where only the source language utilized morph-
based models. Similar results have been shown for
Finnish-to-English and Finnish-to-German in per-
formance evaluation of various unsupervised mor-
pheme analysis algorithms in Morpho Challenge
2009 competition (Kurimo et al, 2009).
We continue the research described above and
examine how the level of decomposition affects
both the individual morph-based systems and
MBR combinations with the baseline word-based
model. Experiments are conducted with the
WMT10 shared task data for German-to-English
and Czech-to-English language pairs.
2 Methods
In this work, morphological analyses are con-
ducted on the source language data, and each dif-
ferent analysis is applied to create a unique seg-
mentation of words into morphemes. Translation
systems are trained with the Moses toolkit (Koehn
et al, 2007) from each differently segmented ver-
sion of the same source language to the target lan-
guage. Evaluation with BLEU is performed on
both the individual systems and system combina-
tions, using different levels of decomposition.
2.1 Morphological models for words
Morfessor (Creutz and Lagus, 2002; Creutz and
Lagus, 2007, etc.) is a family of methods for
unsupervised morphological segmentation. Mor-
fessor does not limit the number of morphemes
for each word, making it suitable for agglutina-
tive and compounding languages. An analysis of a
single word is a list of non-overlapping segments,
195
morphs, stored in the model lexicon. We use both
the Morfessor Baseline (Creutz and Lagus, 2005b)
and the Morfessor Categories-MAP (Creutz and
Lagus, 2005a) algorithms.1 Both are formulated
in a maximum a posteriori (MAP) framework, i.e.,
the learning algorithm tries to optimize the prod-
uct of the model prior and the data likelihood.
The generative model applied by Morfessor
Baseline assumes that the morphs are independent.
The resulting segmentation can be influenced by
using explicit priors for the morph lengths and
frequencies, but their effect is usually minimal.
The training data has a larger effect on the re-
sults: A larger data set alows a larger lexicon,
and thus longer morphs and less morphs per word
(Creutz and Lagus, 2007). Moreover, the model
can be trained with or without taking into account
the word frequencies. If the frequencies are in-
cluded, the more frequent words are usually un-
dersegmented compared to a linguistic analysis,
whereas the rare words are oversegmented (Creutz
and Lagus, 2005b). An easy way to control the
amount of segmentation is to weight the training
data likelihood by a positive factor ?. If ? > 1,
the increased likelihood results in longer morphs.
If ? < 1, the morphs will be shorter and the words
more segmented.
Words that are not present in the training data
can be segmented using an algorithm similar to
Viterbi. The algorithm can be modified to allow
new morphs types to be used by using an approx-
imative cost of adding them into the lexicon (Vir-
pioja and Kohonen, 2009). The modification pre-
vents oversegmentation of unseen word forms. In
machine translation, this is important especially
for proper nouns, for which there is usually no
need for translation.
The Morfessor Categories-MAP algorithm ex-
tends the model by imposing morph categories of
stems, prefixes and suffixes, as well as transition
probabilities between them. In addition, it applies
a hierarchical segmentation model that allows it to
construct new stems from smaller pieces of ?non-
morphemes? (Creutz and Lagus, 2007). Due to
these features, it can provide reasonable segmen-
tations also for those words that contain new mor-
phemes. The drawback of the more sophisticated
model is the slower and more complex training al-
gorithm. In addition, the amount of the segmenta-
1The respective software is available at http://www.
cis.hut.fi/projects/morpho/
tion is harder to control.
Morfessor Categories-MAP was applied to sta-
tistical machine translation by Virpioja et al
(2007) and de Gispert et al (2009). However,
Kurimo et al (2009) report that Morfessor Base-
line outperformed Categories-MAP in Finnish-to-
English and German-to-English tasks both with
and without MBR combination, although the dif-
ferences were not statistically significant. In all
the previous cases, the models were trained on
word types, i.e., without using their frequencies.
Here, we also test models trained on word tokens.
2.2 Statistical machine translation
We utilize the Moses toolkit (Koehn et al, 2007)
for statistical machine translation. The default pa-
rameter values are used except with the segmented
source language, where the maximum sentence
length is increased from 80 to 100 tokens to com-
pensate for the larger number of tokens in text.
2.3 Morphological model combination
For combining individual models, we apply Min-
imum Bayes Risk (MBR) system combination
(Sim et al, 2007). N-best lists from multiple
SMT systems trained with different morpholog-
ical analysis methods are merged; the posterior
distributions over the individual lists are interpo-
lated to form a new distribution over the merged
list. MBR hypotheses selection is then performed
using sentence-level BLEU score (Kumar and
Byrne, 2004).
In this work, the focus of the system combina-
tion is not to combine different translation systems
(e.g., Moses and Systran), but to combine systems
trained with the same translation algorithm using
the same source language data with with different
morphological decompositions.
3 Experiments
The German-to-English and Czech-to-English
parts of the ACL WMT10 shared task data were
investigated. Vanilla SMT models were trained
with Moses using word tokens for MBR combi-
nation and comparison purposes. Several different
morphological segmentation models for German
and Czech were trained with Morfessor. Each seg-
mentation model corresponds to a morph-based
SMT model trained with Moses. The word-based
vanilla Moses model is compared to each morph-
based model as well as to several MBR com-
196
binations between word-based translation models
and morph-based translation models. Quantitative
evaluation is carried out using the BLEU score
with re-cased and re-tokenized translations.
4 Data
The data used in the experiments consisted
of Czech-to-English (CZ-EN) and German-to-
English (DE-EN) parallel language data from
ACL WMT10. The data was divided into distinct
training, development, and evaluation sets. Statis-
tics and details are shown in Table 1.
Aligned data from Europarl v5 and News
Commentary corpora were included in training
German-to-English SMT models. The English
part from the same data sets was used for train-
ing a 5-gram language model, which was used in
all translation tasks. The Czech-to-English trans-
lation model was trained with CzEng v0.9 (train-
ing section 0) and News Commentary data. The
monolingual German and Czech parts of the train-
ing data sets were used for training the morph seg-
mentation models with Morfessor.
The data sets news-test2009, news-
syscomb2009 and news-syscombtune2010
from the ACL WMT 2009 and WMT 2010,
were used for development. The news-test2008,
news-test2010, and news-syscombtest2010 data
sets were used for evaluation.
4.1 Preprocessing
All data sets were preprocessed before use. XML-
tags were removed, text was tokenized and char-
acters were lowercased for every training, devel-
opment and evaluation set.
Morphological models for German and Czech
were trained using a corpus that was a combina-
tion of the respective training sets. Then the mod-
els were used for segmenting all the data sets, in-
cluding development and evaluation sets, with the
Viterbi algorithm discussed in Section 2.1. The
modification of allowing new morph types for out-
of-vocabulary words was not applied.
The Moses cleaning script performed additional
filtering on the parallel language training data.
Specifically, sentences with over 80 words were
removed from the vanilla Moses word-based mod-
els. For morph-based models the limit was set
to 100 morphs, which is the maximum limit of
the Giza++ alignment tool. After filtering with a
threshold of 100 tokens, the different morph seg-
mentations for DE-EN training data from com-
bined Europarl and News Commentary data sets
ranged from 1 613 556 to 1 624 070 sentences.
Similarly, segmented CZ-EN training data ranged
from 896 163 to 897 744 sentences. The vanilla
words-based model was trained with 1 609 998
sentences for DE-EN and 897 497 sentences for
CZ-EN.
5 Results
The details of the ACL WMT10 submissions are
shown in Table 2. The results of experiments with
different morphological decompositions and MBR
system combinations are shown in Table 3. The
significances of the differences in BLEU scores
between the word-based model (Words) and mod-
els with different morphological decompositions
was measured by dividing each evaluation data set
into 49 subsets of 41?51 sentences, and using the
one-sided Wilcoxon signed rank test (p < 0.05).
5.1 Segmentation
We created several word segmentations with Mor-
fessor baseline and Morfessor Categories-MAP
(CatMAP). Statistics for the different segmenta-
tions are given in Table 3. The amount of seg-
mentation was measured as the average number of
morphs per word (m/w) and as the percentage of
segmented words (s-%) in the training data. In-
creasing the data likelihood weight ? in Morfes-
sor Baseline increases the amount of segmentation
for both languages. However, it had little effect
on the proportion of segmented words in the three
evaluation data sets: The proportion of segmented
word tokens was 10?11 % for German and 8?9 %
for Czech, whereas the out-of-vocabulary rate was
7.5?7.8 % for German and 4.8?5.6 % for Czech.
Disregarding the word frequency information
in Morfessor Baseline (nofreq) produced more
morphs per word type and segmented nearly
all words in the training data. The Morfessor
CatMAP algorithm created segmentations with the
largest number of morphs per word, but did not
segment as many words as the Morfessor Baseline
without the frequencies.
5.2 Morph-based translation systems
The models with segmented source language per-
formed worse individually than the word-based
models. The change in the BLEU score was statis-
tically significant in almost all segmentations and
197
Data set Statistics Training Development Evaluation
Sentences Words per sentence SM LM TM
DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN
Europarl v5 1 540 549 23.2 25.2 x x x
News Commentary 100 269 21.9 18.9 21.5 x x x x x
CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x
news-test2009 2 525 21.7 18.8 23.2 x
news-syscomb2009 502 19.7 17.2 21.1 x
news-syscombtune2010 455 20.2 17.3 21.0 x
news-test2008 2 051 20.3 17.8 21.7 x
news-test2010 2 489 21.7 18.4 22.3 x
news-syscombtest2010 2 034 22.0 18.6 22.6 x
Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the
number of aligned sentences and the average number of words per sentence in each language. The data
sets used for model training, development and evaluation are marked. Training is divided into German
(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training
and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.
Submission Segmentation model for source language BLEU-cased
(news-test2010)
aalto DE-EN WMT10 Morfessor Baseline (? = 0.5) 17.0
aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5
aalto CZ-EN WMT10 Morfessor Baseline (? = 0.5) 16.2
aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9
Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are
trained from the segmented source language into unsegmented target language with Moses.
all evaluation sets. Morfessor Baseline (? = 0.5)
was the best individual segmented model for both
German and Czech in the sense that it had the
lowest number of significant decreases the BLEU
score compared to the word-based model. Remov-
ing word frequency information with Morfessor
Baseline and using Morfessor CatMAP gave the
lowest BLEU scores with both source languages.
5.3 Translation system combination
For the DE-EN language pair, all MBR system
combinations between each segmented model and
the word-based model had slightly higher BLUE
scores than the individual word-based model.
Nearly all improvements were statistically signifi-
cant.
The BLEU scores for the MBR combinations
in the CZ-EN language pair were mostly not sig-
nificantly different from the individual word-based
model. Two scores were significantly lower.
6 Discussion
We have applied concatenative morphological
analysis, in which each original word token is seg-
mented into one or more non-overlapping morph
tokens. Our results with different levels of seg-
mentation with Morfessor suggest that the optimal
level of segmentation is language pair dependent
in machine translation.
Our approach for handling rich morphology has
not been able to directly improve the translation
quality. We assume that improvements might still
be possible by carefully tuning the amount of seg-
mentation. The experiments in this paper with
different values of the ? parameter for Morfes-
sor Baseline were conducted with the word fre-
quencies. The parameter had little effect on the
proportion of segmented words in the evaluation
data sets, as frequent words were not segmented
at all, and out-of-vocabulary words were likely to
be oversegmented by the Viterbi algorithm. Fu-
ture work includes testing a larger range of val-
ues for ?, also for models trained without the
word frequencies, and using the modification of
the Viterbi algorithm proposed in Virpioja and Ko-
honen (2009).
It might also be helpful to only segment selected
words, where the selection would be based on the
potential benefit in the translation process. In gen-
eral, the direct segmentation of words into morphs
is problematic because it increases the number
of tokens in the text and directly increases both
model training and decoding complexity. How-
ever, an efficient segmentation decreases the num-
ber of types and the out-of-vocabulary rate (Virpi-
oja et al, 2007).
We have replicated here the result that an MBR
combination of a morph-based MT system with
198
Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 16.37 - 17.28 13.22 -
Morfessor Baseline (? = 0.5) 1.82 72.4% 15.19? 16.47+ 17.04? 13.28? 13.70+
Morfessor Baseline (? = 1.0) 1.65 61.0% 15.14? 16.54+ 16.87? 11.95? 13.66+
Morfessor Baseline (? = 5.0) 1.24 23.7% 15.04? 16.44? 16.63? 11.78? 13.43+
Morfessor CatMAP 2.25 67.5% 14.21? 16.42? 16.53? 11.15? 13.61+
Morfessor Baseline nofreq 2.24 91.6% 13.98? 16.47+ 16.36? 10.66? 13.58+
Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)
news-test2008 news-test2010 news-syscombtest2010
m/w s-% No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 14.91 - 16.73 12.75 -
Morfessor Baseline (? = 0.5) 1.19 17.7% 13.22? 14.87? 16.01? 12.60? 12.53?
Morfessor Baseline (? = 1.0) 1.09 8.1% 13.33? 14.88? 16.10? 11.29? 12.84?
Morfessor Baseline (? = 5.0) 1.03 2.9% 13.53? 14.83? 15.92? 11.17? 12.85?
Morfessor CatMAP 2.29 71.9% 11.93? 14.86? 15.79? 10.12? 10.79?
Morfessor Baseline nofreq 2.18 90.3% 12.43? 14.96? 15.82? 10.13? 12.89?
Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.
The source language is segmented with the shown algorithms. The amount of segmentation in the train-
ing data is measured with the average number of morphs per word (m/w) and as proportion of segmented
words (s-%) against the word-based model (Words). The trained translation systems are evaluated in-
dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translation
systems (MBR). Unchanged (?), significantly higher (+) and lower (?) BLEU scores compared to the
word-based translation model (Words) are marked. The best morph-based model for each column is
emphasized.
a word-based MT system can produce a BLEU
score that is higher than from either of the indi-
vidual systems (de Gispert et al, 2009; Kurimo
et al, 2009). With the DE-EN language pair, the
improvement was statistically significant with all
tested segmentation models. However, the im-
provements were not as large as those obtained
before and the results for the CZ-EN language
pair were not significantly different in most cases.
Whether this is due to the different languages,
training data sets, the domain of the evaluation
data sets, or some problems in the model training,
is currently uncertain.
One very different approach for applying dif-
ferent levels of linguistic analysis is factor mod-
els for SMT (Koehn and Hoang, 2007), where
pre-determined factors (e.g., surface form, lemma
and part-of-speech) are stored as vectors for each
word. This provides better integration of mor-
phosyntactic information and more control of the
process, but the translation models are more com-
plex and the number and factor types in each word
must be fixed.
Our submissions to the ACL WMT10 shared
task utilize unsupervised morphological decompo-
sition models in a straightforward manner. The
individual morph-based models trained with the
source language words segmented into morphs
did not improve the vanilla word-based models
trained with the unsegmented source language.
We have replicated the result for the German-
to-English language pair that an MBR combina-
tion of a word-based and a segmented morph-
based model gives significant improvements to the
BLEU score. However, we did not see improve-
ments for the Czech-to-English translations.
Acknowledgments
This work was supported by the Academy of
Finland in the project Adaptive Informatics, the
Finnish graduate school in Language Technology,
and the IST Programme of the European Commu-
nity, under the FP7 project EMIME (213845).
References
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL?02, pages 21?30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005a. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the AKRR?05,
Espoo, Finland.
199
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using Morfessor 1.0.
Technical Report A81, Publications in Computer
and Information Science, Helsinki University of
Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 73?76, Boul-
der, USA, June. Association for Computational Lin-
guistics.
Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkko?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the EMNLP 2007,
pages 868?876, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of ACL, demonstration ses-
sion, pages 177?180, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the HLT-NAACL 2004, pages
169?176.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsima?ki, Janne Pylkko?nen, Tanel
Aluma?e, and Murat Saraclar. 2006. Unlimited vo-
cabulary speech recognition for agglutinative lan-
guages. In Proceedings of the HLT-NAACL 2006,
pages 487?494, New York, USA.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodl. 2007. Consensus network decoding
for statistical machine translation system combina-
tion. In IEEE Int. Conf. on Acoustics, Speech, and
Signal Processing.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings
of the Machine Translation Summit XI, pages 491?
498, Copenhagen, Denmark, September.
200
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 87?95,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Morpho Challenge competition 2005-2010: Evaluations and results
Mikko Kurimo, Sami Virpioja, Ville Turunen, Krista Lagus
Adaptive Informatics Research Centre
Aalto University, Espoo, Finland
Firstname.Lastname@tkk.fi
Abstract
Morpho Challenge is an annual evalu-
ation campaign for unsupervised mor-
pheme analysis. In morpheme analysis,
words are segmented into smaller mean-
ingful units. This is an essential part in
processing complex word forms in many
large-scale natural language processing
applications, such as speech recognition,
information retrieval, and machine trans-
lation. The discovery of morphemes is
particularly important for morphologically
rich languages where inflection, deriva-
tion and composition can produce a huge
amount of different word forms. Morpho
Challenge aims at language-independent
unsupervised learning algorithms that can
discover useful morpheme-like units from
raw text material. In this paper we de-
fine the challenge, review proposed algo-
rithms, evaluations and results so far, and
point out the questions that are still open.
1 Introduction
Many large-scale natural language processing
(NLP) applications, such as speech recognition,
information retrieval and machine translation, re-
quire that complex word forms are analyzed into
smaller, meaningful units. The discovery of these
units called morphemes is particularly important
for morphologically rich languages where the in-
flection, derivation and composition makes it im-
possible to even list all the word forms that are
used. Various tools have been developed for mor-
pheme analysis of word forms, but they are mostly
based on language-specific rules that are not eas-
ily ported to other languages. Recently, the per-
formance of tools based on language-independent
unsupervised learning from raw text material has
improved significantly and rivaled the language-
specific tools in many applications.
The unsupervised algorithms proposed so far in
Morpho Challenge typically first generate various
alternative morphemes for each word and then se-
lect the best ones based on relevant criteria. The
statistical letter successor variation (LSV) analy-
sis (Harris, 1955) and its variations are quite com-
monly used as generation methods. LSV is based
on the observation that the segment borders be-
tween the sub-word units often co-occur with the
peaks of variation for the next letter. One popu-
lar selection approach is to minimize a cost func-
tion that balances between the size of the corpus
when coded by the morphemes and the size of
the morpheme codebook needed. Selection cri-
teria that produce results resembling the linguis-
tic morpheme segmentation include, for example,
the Minimum Description Length (MDL) princi-
ple and maximum a posteriori (MAP) probability
optimization (de Marcken, 1996; Creutz and La-
gus, 2005).
The Morpho Challenge competition was
launched in 2005 to encourage the machine
learning people, linguists and specialists in NLP
applications to study this field and come together
to compare their best algorithms against each
other. The organizers selected evaluation tasks,
data and metric and performed all the evaluations.
Thus, participation was made easy for people
who were not specialists in the chosen NLP
applications. Participation was open to everybody
with no charge. The competition became popular
right from the beginning and has gained new
participants every year.
Although not all the authors of relevant mor-
pheme analysis algorithms have yet submitted
their algorithms for this evaluation campaign,
more than 50 algorithms have already been eval-
uated. After the first five years of Morpho Chal-
lenge, a lot has been learned on the various pos-
sible ways to solve the problem and how the dif-
ferent methods work in various NLP tasks. How-
87
ever, there are still open questions such as: how to
find meaning for the obtained unsupervised mor-
phemes, how to disambiguate among the alterna-
tive analyses of one word, and how to use context
in the analysis. Another recently emerged ques-
tion that is the special topic in 2010 competition
is how to utilize small amounts of labeled data
and semi-supervised learning to further improve
the analysis.
2 Definition of the challenge
2.1 Morphemes and their evaluation
Generally, the morphemes are defined as the
smallest meaningful units of language. Rather
than trying to directly specify which units are
meaningful, the Morpho Challenge aims at find-
ing units that would be useful for various practical
NLP applications. The goal is to find automatic
methods that can discover suitable units using un-
supervised learning directly on raw text data. The
methods should also not be restricted to certain
languages or include many language and applica-
tion dependent parameters that needed to be hand
tuned for each task separately. The following three
goals have been defined as the main scientific ob-
jectives for the challenge: (1) To learn of the phe-
nomena underlying word construction in natural
languages. (2) To discover approaches suitable for
a wide range of languages. (3) To advance ma-
chine learning methodology.
The evaluation tasks, metrics and languages
have been designed based on the scientific objec-
tives of the challenge. It can not be directly ver-
ified how well an obtained analysis reflects the
word construction in natural languages, but intu-
itively, the methods that split everything into let-
ters or pre-specified letter n-grams, or leave the
word forms unanalyzed, would not be very in-
teresting solutions. An interesting thing that can
be evaluated, however, is how close the obtained
analysis is to the linguistic gold standard mor-
phemes that can be obtained from CELEX or
various language-dependent rule-based analyzers.
The exact definition of the morphemes, tags, or
features available in the gold standard to be uti-
lized in the comparison should be decided and
fixed for each language separately.
To verify that a proposed algorithm works in
various languages would, ideally, require running
the evaluations on a large number of languages
that would be somehow representative of various
important language families. However, the re-
sources available for both computing and evalu-
ating the analysis in various applications and lan-
guages are limited. The suggested and applicable
compromise is to select morphologically rich lan-
guages where the morpheme analysis is most use-
ful and those languages where interesting state-of-
the-art evaluation tasks are available. By including
German, Turkish, Finnish and Arabic, many inter-
esting aspects of concatenative morphology have
already been covered.
While the comparison against the linguistic
gold standard morphemes is an interesting sub-
goal, the main interest in running the Morpho
Challenge is to find out how useful the proposed
morpheme analyses are for various practical NLP
applications. Naturally, this is best evaluated
by performing evaluations in several state-of-the-
art application tasks. Due to the limitations of
the resources, the applications have been selected
based on the importance of the morpheme analy-
sis for the application, on the availability of open
state-of-the-art evaluation tasks, and on the effort
needed to run the actual evaluations.
2.2 Unsupervised and semi-supervised
learning
Unsupervised learning is the task of learning with-
out labeled data. In the context of morphology dis-
covery, it means learning without knowing where
morpheme borders are, or which morphemes exist
in which words. Unsupervised learning methods
have many attractive features for morphological
modeling, such as language-independence, inde-
pendence of any particular linguistic theory, and
easy portability to a new language.
Semi-supervised learning can be approached
from two research directions, namely unsuper-
vised and supervised learning. In an essentially
unsupervised learning task there may exist some
labeled (classified) data, or some known links be-
tween data items, which might be utilized by the
(typically generative) learning algorithms. Turned
around, an essentially supervised learning task,
such as classification or prediction, may benefit
also from unlabeled data which is typically more
abundantly available.
In morphology modeling one might consider
the former setup to be the case: the learning task
is essentially that of unsupervised modeling, and
morpheme labels can be thought of as known links
88
between various inflected word forms.
Until 2010 the Morpho Challenge has been de-
fined only as an unsupervised learning task. How-
ever, since small samples of morphologically la-
beled data can be provided already for quite many
languages, also the semi-supervised learning task
has become of interest.
Moreover, while there exists a fair amount of
research and now even books on semi-supervised
learning (Zhu, 2005; Abney, 2007; Zhu, 2010),
it has not been as widely studied for structured
classification problems like sequence segmenta-
tion and labeling (cf. e.g. (Jiao et al, 2006)). The
semi-supervised learning challenge introduced for
Morpho Challenge 2010 can thus be viewed as an
opportunity to strengthen research in both mor-
phology modeling as well as in semi-supervised
learning for sequence segmentation and labeling
in general.
3 Review of Morpho Challenge
competitions so far
3.1 Evaluation tasks, metrics, and languages
The evaluation tasks and languages selected for
Morpho Challenge evaluations are shown in Fig-
ure 1. The languages where evaluations have been
prepared are Finnish (FIN), Turkish (TUR), En-
glish (ENG), German (GER), and Arabic (ARA).
First the morphemes are compared to linguis-
tic gold standards in direct morpheme segmen-
tation (2005) and full morpheme analysis (since
2007). The practical NLP application based eval-
uations are automatic speech recognition (ASR),
information retrieval (IR) and statistical machine
translation (SMT). Morphemes obtained by semi-
supervised learning can be evaluated in parallel
with the unsupervised morphemes. For IR, eval-
uation has also been extended for full sentences,
where the morpheme analysis can based on con-
text. The various suggested and tested evaluations
are defined in this section.
year new languages new tasks
2005 FIN, TUR, ENG segmentation, ASR
2007 GER full analysis, IR
2008 ARA context IR
2009 - SMT
2010 - semi-supervised
Table 1: The evolution of the evaluations. The
acronyms are explained in section 3.1.
3.1.1 Comparisons to linguistic gold standard
The first Morpho Challenge in 2005 (Kurimo et
al., 2006) considered unsupervised segmentation
of words into morphemes. The evaluation was
based on comparing the segmentation boundaries
given by the competitor?s algorithm to the bound-
aries obtained from a gold standard analysis.
From 2007 onwards, the task was changed to
full morpheme analysis, that is, the algorithm
should not only locate the surface forms (i.e., word
segments) of the morphemes, but find also which
surface forms are realizations (allomorphs) of the
same underlying morpheme. This generalizes the
task for finding more meaningful units than just
the realizations of morphemes that may be just in-
dividual letters or even empty strings. In applica-
tions this is useful when it is important to identify
which units carry the same meaning even if they
have different realizations in different words.
As an unsupervised algorithm cannot find the
morpheme labels that would equal to the labels in
the gold standard, the evaluation has to be based
on what word forms share the same morphemes.
The evaluation procedure samples a large num-
ber of word pairs, such that both words in the
pair have at least one morpheme in common, from
both the proposed analysis and the gold standard.
The first version of the method was applied in
2007 (Kurimo et al, 2008) and 2008 (Kurimo et
al., 2009a), and minor modifications were done in
2009 (Kurimo et al, 2009b). However, the orga-
nizers have reported the evaluation results of the
2007 and 2008 submissions also with the new ver-
sion, thus allowing a direct comparison between
them. A summary of these results for English,
Finnish, German and Turkish for the best algo-
rithms is presented in Table 2. The evaluations
in 2008 and 2009 were also performed on Arabic,
but these results and not comparable, because the
database and the gold standard was changed be-
tween the years. The exact annual results for all
participants as well as the details of the evaluation
in each year can be reviewed in the annual evalu-
ation reports (Kurimo et al, 2006; Kurimo et al,
2008; Kurimo et al, 2009a; Kurimo et al, 2009b).
Already the linguistic evaluation of Morpho
Challenge 2005 applied some principles that have
been used thereafter: (1) The evaluation is based
on a subset of the word forms given as training
data. This not only makes the evaluation proce-
dure lighter, but also allows changing the set when
89
English Finnish
Method P R F Method P R F
2009 2009
Allomorfessor 68.98 56.82 62.31 Monson PMU 47.89 50.98 49.39
Monson PMU 55.68 62.33 58.82 Monson PMM 51.75 45.42 48.38
Lignos 83.49 45.00 58.48 Spiegler PROMODES C 41.20 48.22 44.44
2008 2008
Monson P+M 69.59 65.57 67.52 Monson P+M 65.21 50.43 56.87
Monson ParaMor 63.32 51.96 57.08 Monson ParaMor 49.97 37.64 42.93
Zeman 1 67.13 46.67 55.06 Monson Morfessor 79.76 24.95 38.02
2007 2007
Monson P+M 70.09 67.38 68.71 Bernhard 2 63.92 44.48 52.45
Bernhard 2 67.42 65.11 66.24 Bernhard 1 78.11 29.39 42.71
Bernhard 1 75.61 57.87 65.56 Bordag 5a 72.45 27.21 39.56
German Turkish
Method P R F Method P R F
2009 2009
Monson PMU 52.53 60.27 56.14 Monson PMM 48.07 60.39 53.53
Monson PMM 51.07 57.79 54.22 Monson PMU 47.25 60.01 52.88
Monson PM 50.81 47.68 49.20 Monson PM 49.54 54.77 52.02
2008 2008
Monson P+M 64.06 61.52 62.76 Monson P+M 66.78 57.97 62.07
Monson Morfessor 70.73 38.82 50.13 Monson ParaMor 57.35 45.75 50.90
Monson ParaMor 56.98 42.10 48.42 Monson Morfessor 77.36 33.47 46.73
2007 2007
Monson P+M 69.96 55.42 61.85 Bordag 5a 81.06 23.51 36.45
Bernhard 2 54.02 60.77 57.20 Bordag 5 81.19 23.44 36.38
Bernhard 1 66.82 42.48 51.94 Zeman 77.48 22.71 35.13
Table 2: The summary of the best three submitted methods for years 2009, 2008 and 2007 using the
linguistic evaluation of Morpho Challenge 2009. The complete results tables by the organizers are avail-
able from http://www.cis.hut.fi/morphochallenge2009/. The three columns numbers
are precision (P), recall (R), and F-measure (F). The best F-measure for each language is in boldface,
and the best result that is not based on a direct combination of two other methods is underlined.
the old one is considered to be ?overlearned?. (2)
The frequency of the word form plays no role in
evaluation; rare and common forms are equally
likely to be selected, and have equal weight to
the score. (3) The evaluation score is balanced F-
measure, the harmonic mean of precision and re-
call. Precision measures how many of the choices
made by the algorithm are matched in gold stan-
dard; recall measures how many of the choices
in the gold standard are matched in the proposed
analysis. (4) If the linguistic gold standard has
several alternative analysis for one word, for full
precision, it is enough that one of the alternatives
is equivalent to the proposed analysis. The same
holds the other way around for recall.
All of the principles can be also criticized. For
example, evaluation based on the full set would
provide more trustworthy estimates, and common
word forms are more significant in any practical
application. However, the third and the fourth
principle have problems that can be considered to
be more serious.
Balanced F-measure favors methods that are
able to get near-to-equal precision and recall. As
many algorithms can be tuned to give either more
or less morphemes per word than in the default
case, this encourages using developments sets to
optimize the respective parameters. The winning
methods in Challenge 2009?Monson?s ParaMor-
Morfessor Union (PMU) and ParaMor-Morfessor
90
Mimic (PMM) (Monson et al, 2009), and Al-
lomorfessor (Virpioja and Kohonen, 2009)?did
this, more or less explicitly.1 Moreover, it can
be argued that the precision would be more im-
portant than recall in many applications, or, more
generally, that the optimal balance between preci-
sion and recall is application dependent. We see
two solutions for this: Either the optimization for
F-measure should be allowed with a public devel-
opment set, which means moving towards semi-
supervised direction, or precision-recall curves
should be compared, which means more complex
evaluations.
The fourth principle causes problems, if the
evaluated algorithms are allowed to have alterna-
tive analyses for each word. If several alternative
analyses are provided, the obtained precision is
about the average over the individual analyses, but
the recall is based on the best of the alternatives.
This property have been exploited in Challenges
2007 and 2008 by combining the results of two
algorithms as alternative analyses. The method,
Monson?s ParaMor+Morfessor (P+M) holds still
the best position measured in F-measures in all
languages. Combining even better-performing
methods in a similar manner would increase the
scores further. To fix this problem, either the eval-
uation metric should require matching number of
alternative analyses to get the full points, or the
symmetry of the precision and recall measures has
to be removed.
Excluding the methods that combine the anal-
yses of two other methods as alternative ones, we
see that the best F-measure (underlined in Table 2)
is held by Monson?s ParaMor-Morfessor Mimic
from 2009 (Monson et al, 2009) in Turkish and
Bernhard?s method 2 from 2007 (Bernhard, 2006)
in all the other three languages. This means that
except for Turkish, there is no improvement in the
results over the three years. Furthermore, both
of the methods are based purely on segmentation,
and so are all the other top methods presented
in Table 2 except for Bordag?s methods (Bordag,
2006) and Allomorfessor (Virpioja and Kohonen,
2009).
3.1.2 Speech recognition
A key factor in the success of large-vocabulary
continuous speech recognition is the system?s abil-
1Allomorfessor was trained with a pruned data to obtain
a higher recall, whereas ParaMor-Morfessor is explicitly op-
timized for F-measure with a separate Hungarian data set.
ity to limit the search space using a statistical lan-
guage model. The language model provides the
probability of different recognition hypothesis by
using a model of the co-occurence of its words
and morphemes. A properly smoothed n-gram is
the most conventional model. The n-gram should
consist of modeling units that are suitable for the
language, typically words or morphemes.
In Morpho Challenge state-of-the-art large-
vocabulary speech recognizers have been built for
evaluations in Finnish and Turkish (Kurimo et al,
2006). The various morpheme analysis algorithms
have been compared by measuring the recogni-
tion accuracy with different language models each
trained and optimized based on units from one of
the algorithms. The best results were quite near
to each other, but Bernhard (Bernhard, 2006) and
Morfessor Categories MAP were at the top for
both languages.
3.1.3 Information retrieval
In the information retrieval task, the algorithms
were tested by using the morpheme segmentations
for text retrieval. To return all relevant documents,
it is important to match the words in the queries to
the words in the documents irrespective of which
word forms are used. Typically, a stemming al-
gorithm or a morphological analyzer is used to re-
duce the inflected forms to their stem or base form.
The problem with these methods is that specific
rules need to be crafted for each language. How-
ever, these approaches were also tested for com-
parison purposes. The IR experiments were car-
ried out by replacing the words in the corpora and
queries by the suggested morpheme segmenta-
tions. Test corpora, queries and relevance assess-
ments were provided by Cross-Language Evalua-
tion Forum (CLEF) (Agirre et al, 2008).
To test the effect of the morpheme segmen-
tation, the number of other variables will have
to be minimized, which poses some challenges.
For example, the term weighting method will af-
fect the results and different morpheme analyz-
ers may perform optimally with different weight-
ing approaches. TFIDF and Okapi BM25 term
weighting methods have been tested. In the 2007
Challenge, it was noted that Okapi BM25 suffers
greatly if the corpus contains a lot of frequent
terms. These terms are often introduced when the
algorithms segment suffixes from stems. To over-
come this problem, a method for automatically
generating stop lists of frequent terms was intro-
91
duced. Any term that occurs more times in the cor-
pus than a certain threshold is added to the stop list
and excluded from indexing. The method is quite
simple, but it treats all morpheme analysis meth-
ods equally as it does not require the algorithm
to tag which morphemes are stems and which are
suffixes. The generated stoplists are also reason-
able sized and the results are robust with respect
to the stop list cutoff parameter. With a stop list,
Okapi BM25 clearly outperformed TFIDF rank-
ing method for all algorithms. However, the prob-
lem of choosing the term weighting approach that
treats all algorithms in an optimal way remains
open.
Another challenge is analyzing the results as it
is hard to achieve statistically significant results
with the limited number of queries (50-60) that
were available. In fact, in each language 11-17 of
the best algorithms belonged to the ?top group?,
that is, had no statistically different result to the
top performer of the language. To improve the
significance of the results, the number of queries
should be increased. This is a known problem in
the field of IR. However, it is important to test the
methods in a real life application and if an algo-
rithm gives good results across languages, there is
evidence that it is doing something useful.
Some conclusions can be drawn from the re-
sults. The language specific reference methods
(Porter stemming for English, two-layer morpho-
logical analysis for Finnish and German) give the
best results, but the best unsupervised algorithms
are almost at par and the differences are not signif-
icant. For German and Finnish, the best unsuper-
vised methods can also beat in a statistically sig-
nificant way the baseline of not doing any segmen-
tation or stemming. The best algorithms that per-
formed well across languages are ParaMor (Mon-
son et al, 2008), Bernhard (Bernhard, 2006), Mor-
fessor Baseline, andMcNamee (McNamee, 2008).
Comparing the results to the linguistic evalua-
tion (section 3.1.1), it seems that methods that per-
form well at the IR task tend to have good preci-
sion in the linguistic task, with exceptions. Thus,
in the IR task it seems important not to overseg-
ment words. One exception is the method (Mc-
Namee, 2008) which simply splits the words into
equal length letter n-grams. The method gives sur-
prisingly good results in the IR task, given the sim-
plicity, but suffers from low precision in the lin-
guistic task.
3.1.4 Machine translation
In phrase-based statistical machine translation
process there are two stages where morpheme
analysis and segmentation of the words into mean-
ingful sub-word units is needed. The first stage
is the alignment of the parallel sentences in the
source and target language for training the transla-
tion model. The second one is training a statistical
language model for the production of fluent sen-
tences in a morphologically rich target language.
In the machine translation tasks used in the
Morpho Challenge, the focus has so far been in
the alignment problem. In the evaluation tasks in-
troduced in 2009 the language-pairs were Finnish-
English and German-English. To obtain state-of-
the-art results, the evaluation consists of minimum
Bayes risk (MBR) combination of two transla-
tion systems trained on the same data, one us-
ing words and the other morphemes as the ba-
sic modeling units (de Gispert et al, 2009). The
various morpheme analysis algorithms are com-
pared by measuring the translation performance
for different two-model combinations where the
word-based model is always the same, but the
morpheme-based model is trained based on units
from each of the algorithms in turns.
Because the machine translation evaluation has
yet been tried only in 2009, it is difficult to draw
conclusions about the results yet. However, the
Morfessor Baseline algorithm seems to be partic-
ularly difficult to beat both in Finnish-German and
German-English task. The differences between
the best results are small, but the ranking in both
tasks was the same: 1. Morfessor Baseline, 2. Al-
lomorfessor, 3. The linguistic gold standard mor-
phemes (Kurimo et al, 2009b).
3.2 Evaluated algorithms
This section attempts to describe very briefly some
of the individual morpheme analysis algorithms
that have been most successful in the evaluations.
Morfessor Baseline (Creutz and Lagus, 2002):
This is a public baseline algorithm based on jointly
minimizing the size of the morph codebook and
the encoded size of the all the word forms using
the minimum description length MDL cost func-
tion. The performance is above average for all
evaluated tasks in most languages.
Allomorfessor (Kohonen et al, 2009; Virpi-
oja and Kohonen, 2009): The development of
this method was based on the observation that the
92
Finnish German English
0.25
0.3
0.35
0.4
0.45
0.5
0.55
 
 
Morfessor baseline
2007 Bernhard
2008 McNamee 4?gram
2008 Monson P+M
2009 Monson PMU
2009 Lignos
2009 Allomorfessor
Figure 1: Mean Average Precision (MAP) values for some of the best algorithms over the years in the IR
task. The upper horizontal line shows the ?goal level? for each language, i.e. the performance of the best
language specific reference method. The lower line shows the baseline reference of doing no stemming
or analysis.
morph level surface forms of one morpheme are
often very similar and the differences occur close
to the morpheme boundary. Thus, the allomor-
phemes could be modeled by simple mutations.
It has been implemented on top of the Morfessor
Baseline using maximum a posteriori (MAP) opti-
mization. This model slightly improves the perfor-
mance in the linguistic evaluation in all languages
(Kurimo et al, 2009b), but in IR and SMT there is
no improvement yet.
Morfessor Categories MAP (Creutz and La-
gus, 2005): In this method hidden Markov models
are used to incorporate morphotactic categories for
theMorfessor Baseline. The structure is optimized
by MAP and yields slight improvements in the lin-
guistic evaluation for most languages, but not for
IR or SMT tasks.
Bernhard (Bernhard, 2006): This has been one
of the best performing algorithms in Finnish, En-
glish and German linguistic evaluation and in IR
(Kurimo et al, 2008). First a list of the most likely
prefixes and suffixes is extracted and alternative
segmentations are generated for the word forms.
Then the best ones are selected based on cost func-
tions that favour most frequent analysis and some
basic morphotactics.
Bordag (Bordag, 2006): This method applies
iterative LSV and clustering of morphs into mor-
phemes. The performance in the linguistic eval-
uation is quite well for Turkish and decent for
Finnish (Kurimo et al, 2008).
ParaMor (Monson et al, 2008): This method
applies an unsupervised model for inflection rules
and suffixation for the stems by building linguisti-
cally motivated paradigms. It has obtained one of
the top performances for all languages when com-
bined with the Morfessor Baseline (Kurimo et al,
2009a). Various combination methods have been
tested: union, weighted probabilistic average and
proposing both the analyses (Monson et al, 2009).
Lignos (Lignos et al, 2009): This method is
based on the observation that the derivation of
the inflected forms can be modeled as transfor-
mations. The best transformations can be found
by optimizing the simplicity and frequency. This
method performs much better in English than in
the other languages (Kurimo et al, 2009b).
Promodes (Spiegler et al, 2009): This method
presents a probabilistic generative model that ap-
plies LSV and combines multiple analysis using a
committee. It seems to generate a large amount
of short morphemes, which is difficult for many
of the practical applications. However, it obtained
the best performance for the linguistic evaluation
in Arabic 2009 (Kurimo et al, 2009b), but did not
survive as well in other languages, and particularly
not in the IR application.
4 Open questions and challenges
Although more than 50 algorithms have already
been tested in the Morpho Challenge evaluations
and many lessons have been learned from the re-
sults and discussions, many challenges are still
open and untouched. In fact, the attempts to solve
the problem have perhaps produced even more
open questions than there were in the beginning.
93
The main new and open challenges are described
in this section.
What is the best analysis algorithm? Some
of the suggested algorithms have produced good
test results and some even in several tasks and lan-
guages, such as Bernhard (Bernhard, 2006), Mon-
son ParaMor+Morfessor (Monson et al, 2008)
and Allomorfessor (Virpioja and Kohonen, 2009).
However, none of the methods perform really well
in all the evaluation tasks and languages and their
mutual performance differences are often rather
small, even though the morphemes and the al-
gorithmic principles are totally different. Thus,
no dominant morpheme analysis algorithm have
been found. Furthermore, reaching the perfor-
mance level that rivals, or even sometimes domi-
nates, the rule-based and language-dependent ref-
erence methods does not mean that the solutions
are sufficient. Often the limited coverage or un-
suitable level of details in the analysis for the task
in the reference methods just indicates that they
are not sufficient either and better solutions are
needed. Another observation which complicates
the finding and determination of the best algorithm
is that in some tasks, such as statistical language
models for speech recognition, very different al-
gorithms can reach the same performance, because
advanced modelling methods can compensate for
unsuitable morpheme analysis.
What is the meaning of the morphemes? In
some of the fundamental applications of mor-
pheme analysis, such as text understanding, mor-
pheme segmentation alone is only part of the solu-
tion. Even more important is to find the meaning
for the obtained morphemes. The extension of the
segmentation of words into smaller units to iden-
tification of the units that correspond to the same
morpheme is a step taken to this direction, but the
question of the meaning of the morpheme is still
open. However, in the unsupervised way of learn-
ing, solutions to this may be so tightly tied to the
applications that much more complex evaluations
would be needed.
How to evaluate the alternative analyses? It
is clear that when a word form is separated from
the sentence context where it was used, the mor-
pheme analysis easily becomes ambiguous. In the
Morpho Challenge evaluations this has been taken
into account by allowing multiple alternative anal-
yses. However, in some evaluations, for exam-
ple, in the measurement of the recall of the gold
standard morphemes, this leads to unwanted re-
sults and may favour methods that always provide
a large number of alternative analysis.
How to improve the analysis using context?
A natural way to disambiguate the analysis in-
volves taking the sentence context into account.
Some of the Morpho Challenge evaluations, for
example, the information retrieval, allow this op-
tion when the source texts and queries are given.
However, this has not been widely tried yet by
the participants, probably because of the increased
computational complexity of the modelling task.
How to effectively apply semi-supervised
learning? In semi-supervised learning, a small set
of labeled data in the form of gold standard anal-
ysis for the word forms are provided. This data
can be used for improving the unsupervised solu-
tions based on unlabeled data in several ways: (1)
The labeled data is used for tuning some learning
parameters, followed by an unsupervised learning
process for the unlabeled data. (2) The labeled
morphemes are used as an ideal starting point
to bootstrap the learning on the unlabeled words
(self-training). (3) Using the EM algorithm for es-
timating a generative model, the unlabeled cases
can be treated as missing data.
The best and most practical way of using the
partly labeled data will be determined in future
when the semi-supervised task has been evaluated
in the future Morpho Challenge evaluations. For
the first time this task will be evaluated in the on-
going Morpho Challenge 2010.
Acknowledgments
We are grateful to the University of Leipzig,
University of Leeds, Computational Linguistics
Group at University of Haifa, Stefan Bordag,
Ebru Arisoy, Nizar Habash, Majdi Sawalha, Eric
Atwell, and Mathias Creutz for making the data
and gold standards in various languages available
to the Challenge. This work was supported by the
Academy of Finland in the project Adaptive In-
formatics, the graduate schools in Language Tech-
nology and Computational Methods of Informa-
tion Technology, in part by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and in part by
the IST Programme of the European Community,
under the FP7 project EMIME (213845) and PAS-
CAL Network of Excellence.
94
References
Steven Abney. 2007. Semisupervised Learning
for Computational Linguistics. Chapman and
Hall/CRC.
Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. 2008. CLEF
2008: Ad hoc track overview. In Working Notes for
the CLEF 2008 Workshop.
Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proc. PASCAL
Challenge Workshop on Unsupervised segmentation
of words into morphemes, Venice, Italy. PASCAL
European Network of Excellence.
Stefan Bordag. 2006. Two-step approach to unsuper-
vised morpheme segmentation. In Proc. of the PAS-
CAL Challenge Workshop on Unsupervised segmen-
tation of words into morphemes, Venice, Italy. PAS-
CAL European Network of Excellence.
Mathias Creutz and Krista Lagus. 2002. Unsu-
pervised discovery of morphemes. In Proc. SIG-
PHON/ACL?02, pages 21?30.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proc. AKRR?05, pages 106?
113.
Adria de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum bayes risk
combination of translation hypothesis from alter-
native morphological decompositions. In Proc.
NAACL?09, pages 73-76.
C. G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222. Reprinted 1970 in Pa-
pers in Structural and Transformational Linguistics,
Reidel Publishing Company, Dordrecht, Holland.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proc.
ACL?06, pages 209?216.
Oskar Kohonen, Sami Virpioja, and Mikaela Klami.
2009. Allomorfessor: Towards unsupervised mor-
pheme analysis. In Evaluating systems for Mul-
tilingual and MultiModal Information Access, 9th
Workshop of the Cross-Language Evaluation Forum,
CLEF 2008, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5706. Springer.
Mikko Kurimo, Mathias Creutz, and Krista Lagus.
2006. Unsupervised segmentation of words into
morphemes - challenge 2005, an introduction and
evaluation report. In Proc. PASCAL Challenge
Workshop on Unsupervised segmentation of words
into morphemes, Venice, Italy. PASCAL European
Network of Excellence.
Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864?873. Springer.
Mikko Kurimo, Ville Turunen, and Matti Varjokallio.
2009a. Overview of Morpho Challenge 2008.
In Evaluating systems for Multilingual and Mul-
tiModal Information Access, 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5706. Springer.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009b.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece.
Constantine Lignos, Erwin Chan, Mitchell P. Marcus,
and Charles Yang. 2009. A rule-based unsupervised
morphology learning framework. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece.
Paul McNamee. 2008. Retrieval experiments at mor-
pho challenge 2008. InWorking Notes for the CLEF
2008 Workshop, Aarhus, Denmark, September.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5152. Springer.
Christian Monson, Kristy Hollingshead, and Brian
Roard. 2009. Probabilistic paraMor. In Working
Notes for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sebastian Spiegler, Bruno Golenia, and Peter Flach.
2009. PROMODES: A probabilistic generative
model for word decomposition. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece,
September.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme discovery with Allomorfessor. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
Xiaojin Zhu. 2010. Semi-supervised learning. In En-
cyclopedia of Machine Learning. To appear.
95
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 37?40,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Vocabulary Adaptation for Morph-based Language Models
Andre? Mansikkaniemi and Mikko Kurimo
Aalto University School of Science
Department of Information and Computer Science
PO BOX 15400, 00076 Aalto, Finland
{andre.mansikkaniemi,mikko.kurimo}@aalto.fi
Abstract
Modeling of foreign entity names is an im-
portant unsolved problem in morpheme-based
modeling that is common in morphologically
rich languages. In this paper we present an
unsupervised vocabulary adaptation method
for morph-based speech recognition. Foreign
word candidates are detected automatically
from in-domain text through the use of letter
n-gram perplexity. Over-segmented foreign
entity names are restored to their base forms in
the morph-segmented in-domain text for eas-
ier and more reliable modeling and recogni-
tion. The adapted pronunciation rules are fi-
nally generated with a trainable grapheme-to-
phoneme converter. In ASR performance the
unsupervised method almost matches the abil-
ity of supervised adaptation in correctly rec-
ognizing foreign entity names.
1 Introduction
Foreign entity names (FENs) are difficult to rec-
ognize correctly in automatic speech recognition
(ASR). Pronunciation rules that cover native words
usually give incorrect pronunciation for foreign
words. More often the foreign entity names encoun-
tered in speech are out-of-vocabulary words, previ-
ously unseen words not present in neither the lexicon
nor background language model (LM).
An in-domain LM trained on a smaller corpus re-
lated to the topic of the speech, can be used to adapt
the background LM to give more suitable probabil-
ities to rare or unseen foreign words. Proper pro-
nunciation rules for foreign entity names are needed
to increase the probability of their correct recogni-
tion. These can either be obtained from a hand-made
lexicon or by generating pronunciation rules auto-
matically using for example a trainable grapheme-
to-phoneme (G2P) converter.
In morph-based speech recognition words are
segmented into sub-word units called morphemes.
When using statistical morph-segmentation algo-
rithms such as Morfessor (Creutz and Lagus, 2005)
new foreign entity names encountered in in-domain
text corpora are often over-segmented (e.g. mcdow-
ell ? mc do well). To guarantee reliable pronuncia-
tion modeling, it?s preferable to keep the lemma in-
tact. Restoring over-segmented foreign entity names
back in to their base forms is referred to as mor-
pheme adaptation in this paper.
This work describes an unsupervised approach to
language and pronunciation modeling of foreign en-
tity names in morph-based speech recognition. We
will study an adaptation framework illustrated below
in Figure 1.
In-domain text
Find FENs
Stemmer
Morpheme
adaptation
LM adaptation
Morph
segmentation
Background
corpus
Adapted LM
FEN lexicon
G2P converter
Adapted
lexicon
Figure 1: Adaptation framework.
37
The adaptation framework is centered around the
following automated steps: 1. Find foreign words in
adaptation texts, 2. Convert foreign word candidates
into their base forms, 3. Generate pronunciation
variants for the retrieved foreign entity name can-
didates using a G2P converter. Additionally, to fa-
cilitate easier and more reliable pronunciation adap-
tation, the foreign entity names are restored to their
base forms in the segmented in-domain text.
The adaptation framework will be compared to a
supervised method where the adaptation steps are
done manually. The evaluation will be done on
Finnish radio news segments.
2 Methods
2.1 Foreign Word Detection
Unsupervised detection of foreign words in text
has previously been implemented for English using
word n-ngram models (Ahmed, 2005).
Finnish has a rich morphology and using word n-
gram models or dictionaries for the detection of for-
eign words would not be practical. Many of the for-
eign words occurring in written Finnish texts could
be identified from unusual letter sequences that are
not common in native words. A letter n-gram model
trained on Finnish words could be used to identify
foreign words by calculating the average perplexity
of the letter sequence in a word normalized by its
length.
A two-step algorithm is implemented for the au-
tomatic detection of foreign words. First, all words
starting in uppercase letters in the unprocessed adap-
tation text are held out as potential foreign entity
names. The perplexity for each foreign word candi-
date is calculated using a letter-ngram model trained
on Finnish words. Words with the highest perplexity
values are the most probable foreign entity names. A
percentage threshold T for the top perplexity words
can be determined from prior information.
The most likely foreign words are fi-
nally converted into their base forms using
a Finnish stemming algorithm (Snowball -
http://snowball.tartarus.org/).
2.2 Lexicon Adaptation
For Finnish ASR systems the pronunciation dictio-
nary can easily be constructed for arbitrary words
by mapping letters directly to phonemes. Foreign
names are often pronounced according to their orig-
inal languages, which can have more complicated
pronunciation rules. These pronunciation rules
can either be manually added to a lookup dictio-
nary or generated automatically with a grapheme-
to-phoneme converter. Constructing a foreign word
lexicon through manual input involves a lot of te-
dious work and it will require a continuous effort to
keep it updated.
In this work Sequitur G2P is used, a data-
driven grapheme-to-phoneme converter based on
joint-sequence models (Bisani and Ney, 2008). A
pronunciation model is trained on a manually con-
structed foreign word lexicon consisting of 2000 for-
eign entity names with a manually given pronuncia-
tion hand-picked from a Finnish newswire text col-
lection. The linguistic origins of the foreign words
are mixed but Germanic and Slavic languages are
the most common.
The pronunciation model is used to generate the
most probable pronunciation variants for the foreign
entity name candidates found in the adaptation text.
2.3 Morpheme Adaptation
In current state of the art Finnish language model-
ing words are segmented into sub-word units (mor-
phemes) (Hirsima?ki et. al, 2009). This allows the
system to cover a large number of words which re-
sult from the highly agglutinative word morphology.
Over-segmentation usually occurs for previously
unseen words found in adaptation texts. To en-
sure reliable pronunciation modeling of foreign
entity names it?s preferable to keep the lemma
intact. Mapping a whole word pronunciation
rule onto separate morphemes is a non-trivial
task for non-phonetic languages such as English.
The morphemes in the in-domain corpus will be
adapted such that all foreign words are restored
into their base forms and the base forms are added
to the morpheme vocabulary. Below is an exam-
ple. Word boundaries are labeled with the <w>-tag.
<w> oilers <w> ha?visi <w> edmonton in <w> com mon
we al th <w> sta dium illa <w>
?
<w> oilers <w> ha?visi <w> edmonton in <w> com-
monwealth <w> stadium illa <w>
38
2.4 Language Model Adaptation
The in-domain adaptation text is segmented differ-
ently depending on the foreign entity name can-
didates that are included. A separate in-domain
LM Pi(w|h) is trained for each segmentation of the
text. Linear interpolation is used to the adapt the
background LM PB(w|h) with the in-domain LM
Pi(w|h).
Padapi(w|h) = ?Pi(w|h) + (1? ?)PB(w|h) (1)
3 Experiments
3.1 Speech Data
Evaluation data consisted of two sets of Finnish ra-
dio news segments in 16 kHz audio. All of the
recordings were collected in 2011-2012 from YLE
Radio Suomi news and sports programs.
The first data set consisted of 32 general news
segments. The total transcription length was 8271
words. 4.8% of the words were categorized as for-
eign entity names (FEN). The second data set con-
sisted of 43 sports news segments. The total tran-
scription length 6466 was words. 7.9% of the words
were categorized as foreign entity names.
3.2 System and Models
All speech recognition experiments were run on the
Aalto speech recognizer (Hirsima?ki et. al, 2009).
The background LM was trained on the Kieli-
pankki corpus (70 million words). A lexicon of 30k
morphs and a model of morph segmentation was
learnt from the same corpus as the LM using Mor-
fessor (Creutz and Lagus, 2005). The baseline lex-
icon was adapted with a manually transcribed pro-
nunciation dictionary of 2000 foreign entity names
found in Finnish newswire texts. A Kneser-Ney
smoothed varigram LM (n=12) was trained on the
segmented corpus with the variKN language model-
ing toolkit (Siivola et al, 2007).
LM adaptation data was manually collected from
the Web. On average 2-3 articles were gathered per
topic featured in the evaluation data sets. 120 000
words of text were gathered for LM adaptation on
the general news set. 60 000 words were gathered
for LM adaptation on the sports news set.
The foreign word detection algorithm and a letter
trigram model trained on the Kielipankki word list
were used to automatically find foreign entity names
in the adaptation texts and convert them into their
base forms. Different values were used as percent-
age threshold T (30, 60, and 100%).
The adaptation texts were segmented into morphs
with the segmentation model learnt from the back-
ground corpus. Morpheme adaptation was per-
formed by restoring the foreign entity name candi-
dates into their base forms. Separate in-domain vari-
gram LMs (n=6) were trained for adaptation data
segmented into morphs using each choice of T in
the foreign name detection. The background LM
was adapted with each in-domain LM separately us-
ing linear interpolation with weight ? = 0.1 chosen
based on preliminary experiments.
A pronunciation model was trained with Sequitur
G2P on the manually constructed foreign word lexi-
con. The number of the most probable pronunciation
variants m for one word to be used in lexicon adap-
tation, was tested with different values (1, 4, and 8).
4 Results
The word error rate (WER), letter error rate (LER),
and the foreign entity name error rate (FENER) are
reported in the results. All the results are presented
in Table 1.
The first experiment was run on the baseline sys-
tem. The average WER is 21.7% for general news
and 34.0% for sports. The average FENER is signif-
icantly higher for both (76.6% and 80.7%).
Supervised vocabulary adaptation was imple-
mented by manually retrieving the foreign entity
names from the adaptation text and adding their pro-
nunciation rules to the lexicon. Morpheme adapta-
tion was also applied. Compared to only using linear
interpolation (? = 0.1) supervised vocabulary adap-
tation reduces WER by 4% (general news) and 6%
(sports news). Recognition of foreign entity names
is also improved with FENER reductions of 18%
and 24%.
Unsupervised vocabulary adaptation was imple-
mented through automatic retrieval and pronuncia-
tion generation of foreign entity names. The pa-
rameters of interest are the foreign name percentage
threshold T, determining how many foreign word
candidates are included for lexicon and morpheme
adaptation and m, the number of pronunciation vari-
39
Adaptation method Results
LM Lexicon General News Sports NewsAdaptation T[%] m WER[%] LER[%] FENER[%] WER[%] LER[%] FENER[%]
Background Baseline 21.7 5.7 76.6 34.0 11.4 80.7
Background + Adaptation
Baseline 20.6 5.3 67.8 32.0 10.7 69.4
Supervised - 1 19.8 5.0 55.7 30.1 9.8 53.1
Unsupervised
30
1 20.4 5.2 64.0 31.5 10.4 64.1
4 20.2 5.2 58.7 31.6 10.4 60.4
8 20.4 5.3 56.9 31.5 10.4 56.8
60
1 20.7 5.3 63.7 32.3 10.4 63.7
4 20.7 5.3 59.4 31.1 9.9 59.8
8 21.1 5.5 58.2 31.0 9.9 55.6
100
1 21.1 5.4 62.7 33.2 10.7 66.1
4 21.2 5.5 58.2 32.6 10.4 60.7
8 22.1 5.9 59.2 33.2 10.6 57.0
Table 1: Results of adaptation experiments on the two test sets. Linear interpolation is tested with supervised and
unsupervised vocabulary adaptation. T is the top percentage of foreign entity name candidates used in unsupervised
vocabulary adaptation, and m is the number of pronunciation variants for each word.
ants generated for each word. The best performance
is reached on the general news set with T = 30% and
m = 4 (WER = 20.2%, FENER = 58.7%), and on the
sports news set with T = 60% and m = 8 (WER =
31.0%, FENER = 55.6%).
5 Conclusion and Discussion
In this work we presented an unsupervised approach
to pronunciation and language modeling of foreign
entity names in morph-based speech recognition.
In the context of LM adaptation, foreign en-
tity name candidates were retrieved from in-domain
texts using a foreign word detection algorithm. Pro-
nunciation variants were generated for the foreign
word candidates using a grapheme-to-phoneme con-
verter. Morpheme adaptation was also applied by
restoring the foreign entity names into their base
forms in the morph-segmented adaptation texts.
The results indicate that unsupervised pronun-
ciation and language modeling of foreign entity
names is feasible. The unsupervised approach al-
most matches supervised adaptation in correctly rec-
ognizing foreign entity names. Average WER is also
very close to the supervised adaptation one despite
the increased acoustic confusability when introduc-
ing more pronunciation variants. The percentage of
foreign word candidates included for adaptation af-
fects performance of the algorithm. Including all
words starting in uppercase letters significantly de-
grades ASR results. The optimal threshold value
is dependent on the adaptation text and its foreign
word frequency and similarity to the evaluation data.
The composition of likely pronunciations of for-
eign names by Finnish speakers is not a straight-
forward task. While the native pronunciation of the
name is the favored one, the origin of the name is not
always clear, nor the definition of the pronunciation.
Additionally, the mapping of the native pronuncia-
tion to the phoneme set used by the Finnish ASR
system can only be an approximation, as well as the
pronunciations that the Finnish speakers are able to
produce. In future work we will study new methods
to model the pronunciation of the foreign names and
perform evaluations also in speech retrieval where
the recognition of names have particular importance.
References
B. Ahmed. 2005. Detection of Foreign Words and Names
in Written Text. Doctoral thesis, Pace University.
M. Bisani and H. Ney. 2008. Joint-Sequence Models for
Grapheme-to-Phoneme Conversion. Speech Commu-
nication, vol. 50, Issue 5, pp. 434-451.
M. Creutz and K. Lagus. 2005. Unsupervised Mor-
pheme Segmentation and Morphology Induction from
Text Corpora using Morfessor 1.0. Technical Report
A81, Publications in Computer and Information Sci-
ence, Helsinki University of Technology.
T. Hirsima?ki, J. Pylkko?nen, and M. Kurimo 2009. Im-
portance of High-order N-gram Models in Morph-
based Speech Recognition. IEEE Trans. Audio,
Speech and Lang., pp. 724-732, vol. 17.
V. Siivola, T. Hirsima?ki and S. Virpioja. 2007. On Grow-
ing and Pruning Kneser-Ney Smoothed N-Gram Mod-
els. IEEE Trans. Audio, Speech and Lang., Vol. 15,
No. 5.
40
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 29?37,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Supervised Morphological Segmentation in a Low-Resource Learning
Setting using Conditional Random Fields
Teemu Ruokolainena Oskar Kohonena Sami Virpiojaa Mikko Kurimob
a Department of Information and Computer Science, Aalto University
b Department of Signal Processing and Acoustics, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, the surface forms
of morphemes. Our focus is on a low-
resource learning setting, in which only a
small amount of annotated word forms are
available for model training, while unan-
notated word forms are available in abun-
dance. The current state-of-art methods
1) exploit both the annotated and unan-
notated data in a semi-supervised man-
ner, and 2) learn morph lexicons and sub-
sequently uncover segmentations by gen-
erating the most likely morph sequences.
In contrast, we discuss 1) employing only
the annotated data in a supervised man-
ner, while entirely ignoring the unanno-
tated data, and 2) directly learning to pre-
dict morph boundaries given their local
sub-string contexts instead of learning the
morph lexicons. Specifically, we em-
ploy conditional random fields, a popular
discriminative log-linear model for seg-
mentation. We present experiments on
two data sets comprising five diverse lan-
guages. We show that the fully super-
vised boundary prediction approach out-
performs the state-of-art semi-supervised
morph lexicon approaches on all lan-
guages when using the same annotated
data sets.
1 Introduction
Modern natural language processing (NLP) appli-
cations, such as speech recognition, information
retrieval and machine translation, perform their
tasks using statistical language models. For mor-
phologically rich languages, estimation of the lan-
guage models is problematic due to the high num-
ber of compound words and inflected word forms.
A successful means of alleviating this data sparsity
problem is to segment words into meaning-bearing
sub-word units (Hirsim?ki et al, 2006; Creutz et
al., 2007; Turunen and Kurimo, 2011). In lin-
guistics, the smallest meaning-bearing units of a
language are called morphemes and their surface
forms morphs. Thus, morphs are natural targets
for the segmentation.
For most languages, existing resources contain
large amounts of raw unannotated text data, only
small amounts of manually prepared annotated
training data, and no freely available rule-based
morphological analyzers. The focus of our work is
on performing morphological segmentation in this
low-resource scenario. Given this setting, the cur-
rent state-of-art methods approach the problem by
learning morph lexicons from both annotated and
unannotated data using semi-supervised machine
learning techniques (Poon et al, 2009; Kohonen
et al, 2010). Subsequent to model training, the
methods uncover morph boundaries for new word
forms by generating their most likely morph se-
quences according to the morph lexicons.
In contrast to learning morph lexicons (Poon et
al., 2009; Kohonen et al, 2010), we study mor-
phological segmentation by learning to directly
predict morph boundaries based on their local sub-
string contexts. Specifically, we apply the linear-
chain conditional random field model, a popular
discriminative log-linear model for segmentation
presented originally by Lafferty et al (2001). Im-
portantly, we learn the segmentation model from
solely the small annotated data in a supervised
manner, while entirely ignoring the unannotated
data. Despite not using the unannotated data, we
show that by discriminatively learning to predict
the morph boundaries, we are able to outperform
the previous state-of-art.
We present experiments on Arabic and Hebrew
using the data set presented originally by Snyder
and Barzilay (2008), and on English, Finnish and
29
Turkish using the Morpho Challenge 2009/2010
data sets (Kurimo et al, 2009; Kurimo et al,
2010). The results are compared against two state-
of-art techniques, namely the log-linear model-
ing approach presented by Poon et al (2009) and
the semi-supervised Morfessor algorithm (Koho-
nen et al, 2010). We show that when employ-
ing the same small amount of annotated train-
ing data, the CRF-based boundary prediction ap-
proach outperforms these reference methods on
all languages. Additionally, since the CRF model
learns from solely the small annotated data set, its
training is computationally much less demanding
compared to the semi-supervised methods, which
utilize both the annotated and the unannotated data
sets.
The rest of the paper is organized as follows. In
Section 2, we discuss related work in morpholog-
ical segmentation and methodology. In Section 3,
we describe our segmentation method. Our exper-
imental setup is described in Section 4, and the
obtained results are presented in Section 5. In Sec-
tion 6, we discuss the method and the results. Fi-
nally, we present conclusions on the work in Sec-
tion 7.
2 Related work
The CRF model has been widely used in NLP seg-
mentation tasks, such as shallow parsing (Sha and
Pereira, 2003), named entity recognition (McCal-
lum and Li, 2003), and word segmentation (Zhao
et al, 2006). Recently, CRFs were also employed
successfully in morphological segmentation for
Arabic by Green and DeNero (2012) as a com-
ponent of an English to Arabic machine trans-
lation system. While the segmentation method
of Green and DeNero (2012) and ours is very sim-
ilar, our focuses and contributions differ in sev-
eral ways. First, while in our work we consider
the low-resource learning setting, in which a small
annotated data set is available (up to 3,130 word
types), their model is trained on the Arabic Tree-
bank (Maamouri et al, 2004) constituting sev-
eral times larger training set (588,244 word to-
kens). Second, we present empirical comparison
between the CRF approach and two state-of-art
methods (Poon et al, 2009; Kohonen et al, 2010)
on five diverse languages. Third, due to being a
component of a larger system, their presentation
on the method and experiments is rather undersp-
eficied, while here we are able to provide a more
thorough description.
In the experimental section, we compare the
CRF-based segmentation approach with two state-
of-art methods, the log-linear modeling approach
presented by Poon et al (2009) and the semi-
supervised Morfessor algorithm (Kohonen et al,
2010). As stated previously, the CRF-based seg-
mentation approach differs from these methods in
that it learns to predict morph boundaries from
a small amount of annotated data, in contrast to
learning morph lexicons from both annotated and
large amounts of unannotated data.
Lastly, there exists ample work on varying un-
supervised (and semi-supervised) morphological
segmentation methods. A useful review is given
by Hammarstr?m and Borin (2011). The funda-
mental difference between our approach and these
techniques is that our method necessarily requires
manually annotated training data.
3 Methods
In this section, we describe in detail the CRF-
based approach for supervised morphological seg-
mentation.
3.1 Morphological segmentation as a
classification task
We represent the morphological segmentation task
as a structured classification problem by assign-
ing each character to one of four classes, namely
{beginning of a multi-character morph (B), mid-
dle of a multi-character morph (M), end of a multi-
character morph (E), single character morph (S)}.
For example, consider the English word form
drivers
with a corresponding segmentation
driv + er + s .
Using the classification notation, this segmenta-
tion is represented as
START B M M E B E S STOP
<w> d r i v e r s </w>
where we have assumed additional word start
and end markers <w> and </w> with respective
classes START and STOP. As another example,
consider the Finnish word form
autoilla (with cars)
with a corresponding segmentation
auto + i + lla .
Using the classification notation, this segmenta-
tion is represented as
30
START B M M E S B M E STOP
<w> a u t o i l l a </w>
Intuitively, instead of the four class set {B, M,
E, S}, a segmentation could be accomplished us-
ing only a set of two classes {B, M} as in (Green
and DeNero, 2012). However, similarly to Chi-
nese word segmentation (Zhao et al, 2006), our
preliminary experiments suggested that using the
more fine-grained four class set {B, M, E, S} per-
formed slightly better. This result indicates that
morph segments of differerent lengths behave dif-
ferently.
3.2 Linear-chain conditional random fields
We perform the above structured classification us-
ing linear-chain conditional random fields (CRFs),
a discriminative log-linear model for tagging and
segmentation (Lafferty et al, 2001). The central
idea of the linear-chain CRF is to exploit the de-
pendencies between the output variables using a
chain structured undirected graph, also referred to
as a Markov random field, while conditioning the
output globally on the observation.
Formally, the model for input x (characters in a
word) and output y (classes corresponding to char-
acters) is written as
p (y |x;w) ?
T?
t=2
exp
(
w>f(yt?1, yt,x, t)
)
,
(1)
where t indexes the characters, T denotes word
length, w the model parameter vector, and f the
vector-valued feature extracting function.
The purpose of the feature extraction function
f is to capture the co-occurrence behavior of the
tag transitions (yt?1, yt) and a set of features de-
scribing character position t of word form x. The
strength of the CRF model lies in its capability to
utilize arbitrary, non-independent features.
3.3 Feature extraction
The quality of the segmentation depends heavily
on the choice of features defined by the feature
extraction function f . We will next describe and
motivate the feature set used in the experiments.
Our feature set consists of binary indicator func-
tions describing the position t of word x using
all left and right substrings up to a maximum
length ?. For example, consider the problem
of deciding if the letter e in the word drivers
is preceded by a morph boundary. This deci-
sion is now based on the overlapping substrings
to the left and right of this potential bound-
ary position, that is {v, iv, riv, driv, <w>driv} and
{e, er, ers, ers</w>}, respectively. The substrings
to the left and right are considered indepen-
dently. Naturally, if the maximum allowed sub-
string length ? is less than five, the longest sub-
strings are discarded accordingly. In general, the
optimum ? depends on both the amount of avail-
able training data and the language.
In addition to the substring functions, we use a
bias function which returns value 1 independent
of the input x. The bias and substring features are
combined with all the possible tag transitions.
To motivate this choice of feature set, consider
formulating an intuitive segmentation rule for the
English words talked, played and speed with the
correct segmentations talk + ed, play + ed and
speed, respectively. Now, as a right context ed
is generally a strong indicator of a boundary, one
could first formulate a rule
position t is a segment boundary
if its right context is ed.
This rule would indeed correctly segment the
words talked and played, but would incorrectly
segment speed as spe + ed. This error can be re-
solved if the left contexts are utilized as inhibitors
by expanding the above rule as
position t is a segment boundary
if its right context is ed
and the left context is not spe.
Using the feature set defined above, the CRF
model can learn to perform segmentation in this
rule-like manner according to the training data.
For example, using the above example words and
segmentations for training, the CRFs could learn
to assign a high score for a boundary given that
the right context is ed and a high score for a non-
boundary given the left context spe. Subsequent to
training, making segmentation decisions for new
word forms can then be interpreted as voting based
on these scores.
3.4 Parameter estimation
The CRF model parameters w are estimated based
on an annotated training data set. Common train-
ing criteria include the maximum likelihood (Laf-
ferty et al, 2001; Peng et al, 2004; Zhao et al,
2006), averaged structured perceptron (Collins,
2002), and max-margin (Szummer et al, 2008).
In this work, we estimate the parameters using the
perceptron algorithm (Collins, 2002).
31
In perceptron training, the required graph infer-
ence can be efficiently performed using the stan-
dard Viterbi algorithm. Subsequent to training, the
segmentations for test instances are acquired again
using Viterbi search.
Compared to other training criteria, the struc-
tured perceptron has the advantage of employing
only a single hyperparameter, namely the number
of passes over training data, making model esti-
mation fast and straightforward. We optimize the
hyperparameter using a separate development set.
Lastly, we consider the longest substring length ?
a second hyperparameter optimized using the de-
velopment set.
4 Experimental setup
This section describes the data sets, evaluation
metrics, reference methods, and other details con-
cerning the evaluation of the methods.
4.1 Data sets
We evaluate the methods on two different data sets
comprising five languages in total.
S&B data. The first data set we use is the He-
brew Bible parallel corpus introduced by Snyder
and Barzilay (2008). It contains 6,192 parallel
phrases in Hebrew, Arabic, Aramaic, and English
and their frequencies (ranging from 5 to 3517).
The phrases have been extracted using automatic
word alignment. The Hebrew and Arabic phrases
have manually annotated morphological segmen-
tations, and they are used in our experiments. The
phrases are sorted according to frequency, and ev-
ery fifth phrase starting from the first phrase is
placed in the test set, every fifth starting from the
second phrase in the development set (up to 500
phrases), and the rest of the phrases in the train-
ing set. 1 The total numbers of word types in the
sets are shown in Table 1. Finally, the word forms
in the training set are randomly permuted, and the
first 25%, 50%, 75%, and 100% of them are se-
lected as subsets to study the effect of training data
size.
MC data. The second data set is based on the
Morpho Challenge 2010 (Kurimo et al, 2010).
It includes manually prepared morphological seg-
mentations in English, Finnish and Turkish. The
1We are grateful to Dr. Hoifung Poon for providing us
instructions for dividing of the data set.
Arabic Hebrew
Training 3,130 2,770
Development 472 450
Test 1,107 1,040
Table 1: The numbers of word types in S&B data
sets (Snyder and Barzilay, 2008).
English Finnish Turkish
Unannot. 384,903 2,206,719 617,298
Training 1,000 1,000 1,000
Develop. 694 835 763
Test 10?1,000 10?1,000 10?1,000
Table 2: The numbers of word types in the MC
data sets (Kurimo et al, 2009; Kurimo et al,
2010).
additional German corpus does not have segmen-
tation annotation and is therefore excluded. The
annotated data sets include training, development,
and test sets for each language. Following Virpi-
oja et al (2011), the test set results are based on
ten randomly selected 1,000 word sets. Moreover,
we divide the annotated training sets into ten par-
titions with respective sizes of 100, 200, . . . , 1000
words so that each partition is a subset of the all
larger partitions. The data is divided so that the
smallest set had every 10th word of the original
set, the second set every 10th word and the fol-
lowing word, and so forth. For reference methods
that require unannotated data, we use the English,
Finnish and Turkish corpora from Competition 1
of Morpho Challenge 2009 (Kurimo et al, 2009).
Table 2 shows the sizes of the MC data sets.
4.2 Evaluation measures
The word segmentations are evaluated by compar-
ison with linguistic morphs using precision, recall,
and F-measure. The F-measure equals the geo-
metric mean of precision (the percentage of cor-
rectly assigned boundaries with respect to all as-
signed boundaries) and recall (the percentage of
correctly assigned boundaries with respect to the
reference boundaries). While using F-measure is
a standard procedure, the prior work differ at least
in three details: (1) whether precision and recall
are calculated as micro-average over all segmenta-
tion points or as macro-average over all the word
forms, (2) whether the evaluation is based on word
types or word tokens in a corpus, and (3) if the
32
reference segmentations have alternative correct
choices for a single word type, and how to deal
with them.
For the experiments with the S&B data sets,
we follow Poon et al (2009) and apply token-
based micro-averages. For the experiments with
the MC data sets, we follow Virpioja et al (2011)
and use type-based macro-averages. However, dif-
fering from their boundary measure, we take the
best match over the alternative reference analyses
(separately for precision and recall), since none of
the methods considered here provide multiple seg-
mentations per word type. For the models trained
with the full training set, we also report the F-
measures of the boundary evaluation method by
Virpioja et al (2011) in order to compare to the
results reported in the Morpho Challenge website.
4.3 CRF feature extraction and training
The features included in the feature vector in the
CRF model (1) are described in Section 3.3. We
include all substring features which occur in the
training data.
The CRF model is trained using the averaged
perceptron algorithm as described in Section 3.4.
The algorithm initializes the model parameters
with zero vectors. The model performance, mea-
sured using F-measure, is evaluated on the devel-
opment set after each pass over the training set,
and the training is terminated when the perfor-
mance has not improved during last 5 passes. The
maximum length of substrings ? is optimized by
considering ? = 1, 2, 3, . . . , and the search is ter-
minated when the performance has not improved
during last 5 values. Finally, the algorithm returns
the parameters yielding the highest F-measure on
the development set.
For some words, the MC training sets include
several alternative segmentations. We resolve this
ambiguity by using the first given alternative and
discarding the rest. During evaluation, the alter-
native segmentations are taken into account as de-
scribed in Section 4.2.
The experiments are run on a standard desktop
computer using our own single-threaded Python-
based implementation2.
4.4 Reference methods
We compare our method?s performance on Arabic
and Hebrew data with semi-supervised Morfessor
2Available at http://users.ics.aalto.fi/
tpruokol/
(Kohonen et al, 2010) and the results reported by
Poon et al (2009). On Finnish, English and Turk-
ish data, we compare the method only with semi-
supervised Morfessor as we have no implementa-
tion of the model by Poon et al (2009).
We use a recently released Python implemen-
tation of semi-supervised Morfessor3. Semi-
supervised Morfessor was trained separately for
each training set size, always using the full unan-
notated data sets in addition to the annotated sets.
The hyperparameters, the unannotated data weight
? and the annotated data weight ?, were optimized
with a grid search on the development set. For the
S&B data, there are no separate unannotated sets.
When the annotated training set size is varied, the
remaining parts are utilized as unannotated data.
The log-linear model described in (Poon et al,
2009) and the semi-supervised Morfessor algo-
rithm are later referred to as POON-2009 and S-
MORFESSOR for brevity.
5 Results
Method performances for Arabic and Hebrew on
the S&B data are presented in Tables 3 and 4, re-
spectively. The results for the POON-2009 model
are extracted from (Poon et al, 2009). Perfor-
mances for English, Finnish and Turkish on the
MC data set are presented in Tables 5, 6 and 7,
respectively.
On the Arabic and Hebrew data sets, the CRFs
outperform POON-2009 and S-MORFESSOR
substantially on all the considered data set sizes.
On Finnish and Turkish data, the CRFs outper-
form S-MORFESSOR except for the smallest sets
of 100 instances. On English data, the CRFs out-
perform S-MORFESSOR when the training set is
500 instances or larger.
Using our implementation of the CRF model,
obtaining the results for Arabic, Hebrew, English,
Finnish, and Turkish consumed 10, 11, 22, 32,
and 28 minutes, respectively. These CPU times
include model training and hyperparameter opti-
mization. In comparison, S-MORFESSOR train-
ing is considerably slower. For Arabic and He-
brew, the S-MORFESSOR total training times
were 24 and 22 minutes, respectively, and for En-
glish, Finnish, and Turkish 4, 22, and 10 days,
respectively. The higher training times of S-
MORFESSOR are partly because of the larger
3Available at https://github.com/
aalto-speech/morfessor
33
grids in hyperparameter optimization. Further-
more, the S-MORFESSOR training time for each
grid point grows linearly with the size of the
unannotated data set, resulting in particularly slow
training on the MC data sets. All reported times
are total CPU times for single-threaded runs, while
in practice grid searches can be parallelized.
The perceptron algorithm typically converged
after 10 passes over the training set, and never re-
quired more than 40 passes to terminate. Depend-
ing on the size of the training data, the optimized
maximum lengths of substrings varied in ranges
{3,5}, {2,7}, {3,9}, {3,6}, {3,7}, for Arabic, He-
brew, English, Finnish and Turkish, respectively.
Method %Lbl. Prec. Rec. F1
CRF 25 95.5 93.1 94.3
S-MORFESSOR 25 78.7 79.7 79.2
POON-2009 25 84.9 85.5 85.2
CRF 50 96.5 94.6 95.5
S-MORFESSOR 50 87.5 91.5 89.4
POON-2009 50 88.2 86.2 87.5
CRF 75 97.2 96.1 96.6
S-MORFESSOR 75 92.8 83.0 87.6
POON-2009 75 89.6 86.4 87.9
CRF 100 98.1 97.5 97.8
S-MORFESSOR 100 91.4 91.8 91.6
POON-2009 100 91.7 88.5 90.0
Table 3: Results for Arabic on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
Finally, Table 8 shows the results of the CRF
and S-MORFESSOR models trained with the full
English, Finnish, and Turkish MC data sets and
evaluated with the boundary evaluation method of
Virpioja et al (2011). That is, these numbers are
directly comparable to the BPR-F column in the
result tables presented at the Morpho Challenge
website4. For each of the three languages, CRF
clearly outperforms all the Morpho Challenge sub-
missions that have provided morphological seg-
mentations.
4http://research.ics.aalto.fi/events/
morphochallenge/
Method %Lbl. Prec. Rec. F1
CRF 25 90.5 90.6 90.6
S-MORFESSOR 25 71.5 85.3 77.8
POON-2009 25 78.7 73.3 75.9
CRF 50 94.0 91.5 92.7
S-MORFESSOR 50 82.1 81.8 81.9
POON-2009 50 82.8 74.6 78.4
CRF 75 94.0 92.7 93.4
S-MORFESSOR 75 84.0 88.1 86.0
POON-2009 75 83.1 77.3 80.1
CRF 100 94.9 94.0 94.5
S-MORFESSOR 100 85.3 91.1 88.1
POON-2009 100 83.0 78.9 80.9
Table 4: Results for Hebrew on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
6 Discussion
Intuitively, the CRF-based supervised learning ap-
proach should yield high segmentation accuracy
when there are large amounts of annotated train-
ing data available. However, perhaps surprisingly,
the CRF model yields state-of-art results already
using very small amounts of training data. This
result is meaningful since for most languages it is
infeasible to acquire large amounts of annotated
training data.
The strength of the discriminatively trained
CRF model is that overlapping, non-independent
features can be naturally employed. Importantly,
we showed that simple, language-independent
substring features are sufficient for high perfor-
mance. However, adding new, task- and language-
dependent features is also easy. One might, for ex-
ample, explore features capturing vowel harmony
in Finnish and Turkish.
The CRFs was estimated using the structured
perceptron algorithm (Collins, 2002), which has
the benefit of being computationally efficient and
easy to implement. Other training criteria, such
as maximum likelihood (Lafferty et al, 2001)
or max-margin (Szummer et al, 2008), could
also be employed. Similarly, other classifiers,
such as the Maximum Entropy Markov Models
(MEMMs) (McCallum et al, 2000), are applica-
ble. However, as the amount of information in-
34
Method Train. Prec. Rec. F1
CRF 100 80.2 74.6 77.3
S-MORFESSOR 100 88.1 79.7 83.7
CRF 200 84.7 79.2 81.8
S-MORFESSOR 200 88.1 79.5 83.6
CRF 300 86.7 79.8 83.1
S-MORFESSOR 300 88.4 80.6 84.3
CRF 400 86.5 80.6 83.4
S-MORFESSOR 400 84.6 83.6 84.1
CRF 500 88.6 80.7 84.5
S-MORFESSOR 500 86.3 82.7 84.4
CRF 600 88.1 82.6 85.3
S-MORFESSOR 600 86.7 82.5 84.5
CRF 700 87.9 83.4 85.6
S-MORFESSOR 700 86.0 82.9 84.4
CRF 800 89.1 83.2 86.1
S-MORFESSOR 800 87.1 82.5 84.8
CRF 900 89.0 82.9 85.8
S-MORFESSOR 900 86.4 82.6 84.5
CRF 1000 89.8 83.5 86.5
S-MORFESSOR 1000 88.8 80.1 84.3
Table 5: Results for English on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 384,903 word types.
corporated in the model would be unchanged, the
choice of parameter estimation criterion and clas-
sifier is unlikely to have a dramatic effect on the
method performance.
In CRF training, we focused on the supervised
learning scenario, in which no unannotated data is
exploited in addition to the annotated training sets.
However, there does exist ample work on extend-
ing CRF training to the semi-supervised setting
(for example, see Mann and McCallum (2008)
and the references therein). Nevertheless, our re-
sults strongly suggest that it is crucial to use the
few available annotated training instances as ef-
ficiently as possible before turning model train-
ing burdensome by incorporating large amounts of
unannotated data.
Following previous work (Poon et al, 2009;
Kohonen et al, 2010; Virpioja et al, 2011), we
applied the boundary F-score evaluation measure,
while Green and DeNero (2012) reported charac-
ter accuracy. We consider the boundary F-score a
better measure than accuracy, since the boundary-
Method Train. Prec. Rec. F1
CRF 100 71.4 66.0 68.6
S-MORFESSOR 100 69.8 71.0 70.4
CRF 200 76.4 71.3 73.8
S-MORFESSOR 200 75.5 68.6 71.9
CRF 300 80.4 73.9 77.0
S-MORFESSOR 300 73.1 71.8 72.5
CRF 400 81.0 76.6 78.7
S-MORFESSOR 400 73.3 74.3 73.8
CRF 500 82.9 77.9 80.3
S-MORFESSOR 500 73.5 75.1 74.3
CRF 600 82.6 80.6 81.6
S-MORFESSOR 600 76.1 73.7 74.9
CRF 700 84.3 81.4 82.8
S-MORFESSOR 700 75.0 76.6 75.8
CRF 800 85.1 83.4 84.2
S-MORFESSOR 800 74.1 78.2 76.1
CRF 900 85.2 83.8 84.5
S-MORFESSOR 900 74.2 78.5 76.3
CRF 1000 86.0 84.7 85.3
S-MORFESSOR 1000 74.2 78.8 76.4
Table 6: Results for Finnish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 2,206,719 word
types.
tag distribution is strongly skewed towards non-
boundaries. Nevertheless, for completeness, we
computed the character accuracy for our Arabic
data set, obtaining the accuracy 99.1%, which is
close to their reported accuracy of 98.6%. How-
ever, these values are not directly comparable due
to our use of the Bible corpus by Snyder and Barzi-
lay (2008) and their use of the Penn Arabic Tree-
bank (Maamouri et al, 2004).
7 Conclusions
We have presented an empirical study in data-
driven morphological segmentation employing
supervised boundary prediction methodology.
Specifically, we applied conditional random fields,
a discriminative log-linear model for segmentation
and tagging. From a methodological perspective,
this approach differs from the previous state-of-art
methods in two fundamental aspects. First, we uti-
lize a discriminative model estimated using only
annotated data. Second, we learn to predict morph
35
Method Train. Prec. Rec. F1
CRF 100 72.4 79.6 75.8
S-MORFESSOR 100 77.9 78.5 78.2
CRF 200 83.2 82.3 82.8
S-MORFESSOR 200 80.0 83.2 81.6
CRF 300 83.9 85.9 84.9
S-MORFESSOR 300 80.1 85.6 82.8
CRF 400 86.4 86.5 86.4
S-MORFESSOR 400 80.7 87.1 83.8
CRF 500 87.5 86.4 87.0
S-MORFESSOR 500 81.0 87.2 84.0
CRF 600 87.8 88.1 87.9
S-MORFESSOR 600 80.5 89.9 85.0
CRF 700 89.1 88.3 88.7
S-MORFESSOR 700 80.9 90.7 85.5
CRF 800 88.6 90.3 89.4
S-MORFESSOR 800 81.2 91.0 85.9
CRF 900 89.2 89.8 89.5
S-MORFESSOR 900 81.4 91.2 86.0
CRF 1000 89.9 90.4 90.2
S-MORFESSOR 1000 83.0 91.5 87.0
Table 7: Results for Turkish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 617,298 word types.
boundaries based on their local character substring
contexts instead of learning a morph lexicon.
We showed that our supervised method yields
improved results compared to previous state-of-
art semi-supervised methods using the same small
amount of annotated data, while not utilizing the
unannotated data used by the reference methods.
This result has two implications. First, supervised
methods can provide excellent results in morpho-
logical segmentation already when there are only
a few annotated training instances available. This
is meaningful since for most languages it is infea-
sible to acquire large amounts of annotated train-
ing data. Second, performing morphological seg-
mentation by directly modeling segment bound-
aries can be advantageous compared to modeling
morph lexicons.
A potential direction for future work includes
evaluating the morphs obtained by our method in
real world applications, such as speech recognition
and information retrieval. We are also interested
in extending the method from fully supervised to
Method English Finnish Turkish
CRF 82.0 81.9 71.5
S-MORFESSOR 79.6 73.5 70.5
Table 8: F-measures of the Morpho Chal-
lenge boundary evaluation for CRF and S-
MORFESSOR using the full annotated training
data set.
semi-supervised learning.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
References
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), volume 10,
pages 1?8. Association for Computational Linguis-
tics.
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A Stolcke. 2007. Morph-
based speech recognition and modeling of out-of-
vocabulary words across languages. ACM Transac-
tions on Speech and Language Processing, 5(1):3:1?
3:29, December.
S. Green and J. DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 146?155.
Association for Computational Linguistics.
H. Hammarstr?m and L. Borin. 2011. Unsupervised
learning of morphology. Computational Linguistics,
37(2):309?350, June.
T. Hirsim?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkk?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech and Lan-
guage, 20(4):515?541, October.
O. Kohonen, S. Virpioja, and K. Lagus. 2010. Semi-
supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Spe-
cial Interest Group on Computational Morphology
36
and Phonology, pages 78?86, Uppsala, Sweden,
July. Association for Computational Linguistics.
M. Kurimo, S. Virpioja, V. Turunen, G. W. Blackwood,
and W. Byrne. 2009. Overview and results of Mor-
pho Challenge 2009. In Working Notes for the CLEF
2009 Workshop, Corfu, Greece, September.
M. Kurimo, S. Virpioja, and V. Turunen. 2010.
Overview and results of Morpho Challenge 2010. In
Proceedings of the Morpho Challenge 2010 Work-
shop, pages 7?24, Espoo, Finland, September. Aalto
University School of Science and Technology, De-
partment of Information and Computer Science.
Technical Report TKK-ICS-R37.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, pages 282?289.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The penn arabic treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Con-
ference on Arabic Language Resources and Tools,
pages 102?109.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of con-
ditional random fields. In Proceedings of ACL-
08: HLT, pages 870?878. Association for Compu-
tational Linguistics.
A. McCallum and W. Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 188?191. Association for Computational Lin-
guistics.
A. McCallum, D. Freitag, and F. Pereira. 2000. Max-
imum entropy Markov models for information ex-
traction and segmentation. In Pat Langley, editor,
Proceedings of the Seventeenth International Con-
ference on Machine Learning (ICML 2000), pages
591?598, Stanford, CA, USA. Morgan Kaufmann.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics (COLING 2004), page 562. Association for
Computational Linguistics.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 209?217. Association for
Computational Linguistics.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 134?
141. Association for Computational Linguistics.
B. Snyder and R. Barzilay. 2008. Crosslingual prop-
agation for morphological analysis. In Proceedings
of the AAAI, pages 848?854.
M. Szummer, P. Kohli, and D. Hoiem. 2008. Learn-
ing CRFs using graph cuts. Computer Vision?ECCV
2008, pages 582?595.
V. Turunen and M. Kurimo. 2011. Speech retrieval
from unsegmented Finnish audio using statistical
morpheme-like units for segmentation, recognition,
and retrieval. ACM Transactions on Speech and
Language Processing, 8(1):1:1?1:25, October.
S. Virpioja, V. Turunen, S. Spiegler, O. Kohonen, and
M. Kurimo. 2011. Empirical comparison of eval-
uation methods for unsupervised learning of mor-
phology. Traitement Automatique des Langues,
52(2):45?90.
H. Zhao, C.N. Huang, and M. Li. 2006. An improved
chinese word segmentation system with conditional
random field. In Proceedings of the Fifth SIGHAN
Workshop on Chinese Language Processing, volume
1082117. Sydney: July.
37
